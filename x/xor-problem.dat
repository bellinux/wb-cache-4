4|1|Public
40|$|In {{this paper}} {{we present a}} method which assigns to each layer of a {{multilayer}} neural network, whose network dynamics is governed by a noisy winner-take-all mechanism, a neural temperature. This neural temperature is obtained by a least mean square fit of the probability distribution of the noisy winner-take-all mechanism to the distribution of a softmax mechanism, which has a well defined temperature as free parameter. We call this approximated temperature resulting from the optimization step the neural temperature. We apply this method to a multilayer neural network during learning the <b>XOR-problem</b> with a Hebb-like learning rule and show that after a transient the neural temperature decreases in each layer according to a power law. This indicates a self-organized annealing behavior induced by the learning rule itself instead of an external adjustment of a control parameter as in physically motivated optimization methods e. g. simulated annealing. Comment: 10 pages, 4 figure...|$|E
40|$|For {{a network}} of spiking neurons that encodes {{information}} in the timing of individual spike-times, we derive a supervised learning rule, SpikeProp, akin to traditional error-backpropagation and show how to overcome the discontinuities introduced by thresholding. With this algorithm, we demonstrate how networks of spiking neurons with biologically reasonable action potentials can perform complex non-linear classification in fast temporal coding {{just as well as}} rate-coded networks. We perform experiments for the classical <b>XOR-problem,</b> when posed in a temporal setting, as well as for a number of other benchmark datasets. Comparing the (implicit) number of spiking neurons required for the encoding of the interpolated XOR problem, it is demonstrated that temporal coding requires significantly less neurons than instantaneous rate-coding. 1991 Mathematics Subject Classification: 82 C 32, 68 T 05, 68 T 10, 68 T 30, 92 J 40, 92 B 20. 1998 ACM Computing Classification System: C. 1. 3, F. 1. 1, I. 2. 6, I. 5. 1. Ke [...] ...|$|E
40|$|In {{this paper}} {{we present a}} method which assigns to each layer of a {{multilayer}} neural network, whose network dynamics is governed by a noisy winner-take-all mechanism, an approximated temperature β − 1. This approximated temperature is obtained by comparison of a softmax mechanism where a temperature is well defined with the noisy winner-take-all mechanism. We apply this method to a multilayer neural network during learning the <b>XOR-problem</b> with a Hebb-like learning rule and show that after a transient the approximated temperature decreases in each layer according to a power law β − 1 ∼ t −γ. This indicates a self-organized annealing behaviour induced by the learning rule itself instead of an external adjustment of a control parameter as in physically motivated optimization methods e. g. simulated annealing. As a consequence of this method one can connect the functional learning bebaviour of a neural network, objectively observable by the network error, to the neuronal level using the approximated temperature {{which serves as a}} kind of order parameter for the network performance. Key words: neural networks; Hebb-like learning; reinforcement learning...|$|E
40|$|Agent-based {{methods have}} {{provided}} economics with {{new ways of}} modeling economic processes. We have found these possibilities useful with respect to modeling endogenous growth, in particular the modeling of learning agents that must cope with real world growth constraints such as time, space, and complexity. Time and space are immediately given once a two-dimensional space with agents moving in time is introduced. Complexity is modeled by using single-cell neural nets {{as a metaphor for}} production units and <b>XOR-problems</b> as metaphors for demand signals. The production units learn by changing the weights in their neural nets, but their structure is too simple to learn to respond correctly to demand signals. However, as entrepreneurs, production units may connect more production units into more complex neural nets and thus improve the efficiency. We thus have an artificial economy where economic growth is driven by demand but limited by both demand and supply sides. On the supply side, neural nets must receive demand signals to learn and thus become more efficient. Besides time and space constraints in the search for efficient production, the demand side is constrained by the distribution of the generated wealth since agents have limited access to credit. An interesting feature of the model is how it produces cycles around the growth path. ...|$|R
40|$|There {{has been}} much {{evidence}} to show that single precise spikes, transfer information among biological neurons. Based on this encoding scheme various spiking neural networks have been proposed to solve computational problems. One such algorithm, a spike time error-backpropagation algorithm for temporally encoded networks of spiking neurons, has been successfully applied to the problem of complex non-liner data classification. There are however, certain features of the algorithm that can be further improved. In this paper, synaptic weight limitation is introduced into the algorithm and a novel solution for the problem raised by non-firing neurons is presented. In addition a square cosine encoder is applied to the input neurons and thus the number of input neurons can be reduced. The improved algorithm becomes more reliable, robust, efficient and reduces the implementation costs. The classical <b>XOR-problem,</b> a function approximation experiment and the Iris benchmark data have been applied to validate the improved algorithm. The experimental results reported show that the modified algorithm produces comparable accuracy in classification with the original approach utilising a smaller spiking neural network. 1...|$|E

