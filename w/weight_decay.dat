215|154|Public
2500|$|Tikhonov regularization, {{named for}} Andrey Tikhonov, {{is the most}} {{commonly}} used method of regularization of ill-posed problems. [...] In statistics, the method is known as ridge regression, in machine learning it is known as <b>weight</b> <b>decay,</b> and with multiple independent discoveries, it is also variously known as the Tikhonov–Miller method, the Phillips–Twomey method, the constrained linear inversion method, and the method of linear regularization. It is related to the Levenberg–Marquardt algorithm for non-linear least-squares problems.|$|E
5000|$|Then, for , , and [...] {{we get the}} Hebbian rule, and for , , , and , where [...] is an {{identity}} matrix, introduce <b>weight</b> <b>decay.</b> The formula then reduces to: ...|$|E
5000|$|It {{also has}} [...] class for {{training}} a neural network using Stochastic gradient descent, although the Optim package provides much more options in this respect, like momentum and <b>weight</b> <b>decay</b> regularization.|$|E
3000|$|... for u = 10. Results {{obtained}} from the <b>weight</b> function <b>decaying</b> as the inverse of the seventh power are added as a more quickly <b>decaying</b> <b>weight</b> function, which {{is the case of}} NIED. In this case, [...]...|$|R
30|$|Intuitively, their {{notion of}} edge {{transition}} <b>weight</b> <b>decays</b> exponentially {{with the number of}} possible continuations of the temporal walk at node ui− 1. The more edges appear before (ui− 1,ui,ti), in their model it is exponentially less likely that the information is sent along the given edge—and not another edge that appears earlier.|$|R
3000|$|Comparing weights {{in these}} methods {{with those of}} our {{simulation}} indicates that NIED, using a <b>weight</b> function <b>decaying</b> faster than the inverse fourth power, probably succeeds in reducing the bias by systematic errors (Table 2). But, JMA may fail to reduce bias with a <b>weight</b> function <b>decaying</b> inversely with the second power of distance. Both NIED and JMA must underestimate standard errors of hypocenters, which are calculated based on [...]...|$|R
50|$|A simple form {{of added}} regularizer is <b>weight</b> <b>decay,</b> which simply adds an {{additional}} error, {{proportional to the}} sum of weights (L1 norm) or squared magnitude (L2 norm) of the weight vector, to the error at each node. The level of acceptable model complexity can be reduced by increasing the proportionality constant, thus increasing the penalty for large weight vectors.|$|E
50|$|DNNs {{are prone}} to {{overfitting}} because of the added layers of abstraction, which allow them to model rare dependencies in the training data. Regularization methods such as Ivakhnenko's unit pruning or <b>weight</b> <b>decay</b> (-regularization) or sparsity (-regularization) can be applied during training to combat overfitting. Alternatively dropout regularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies.|$|E
50|$|Tikhonov regularization, {{named for}} Andrey Tikhonov, {{is the most}} {{commonly}} used method of regularization of ill-posed problems. In statistics, the method is known as ridge regression, in machine learning it is known as <b>weight</b> <b>decay,</b> and with multiple independent discoveries, it is also variously known as the Tikhonov-Miller method, the Phillips-Twomey method, the constrained linear inversion method, and the method of linear regularization. It is related to the Levenberg-Marquardt algorithm for non-linear least-squares problems.|$|E
40|$|Constructing active sets is a {{key part}} of the Multivariate Decomposition Method. An {{algorithm}} for constructing optimal or quasi-optimal active sets is proposed in the paper. By numerical experiments, it is shown that the new method can provide sets that are significantly smaller than the sets constructed by the already existing method. The experiments also show that the superposition dimension could surprisingly be very small, at most 3, when the error demand is not smaller than $ 10 ^{- 3 }$ and the <b>weights</b> <b>decay</b> sufficiently fast...|$|R
40|$|It {{has long}} been known that the {{differential}} operator D represents a typical examples of unbounded operators in many Banach spaces including the classical Fock spaces, the Fock [...] Sobolev spaces, and the generalized Fock spaces where the <b>weight</b> <b>decays</b> faster than the Gaussian weight. In this note we identify Fock type spaces where the operator admits some basic spectral structures including compactness and membership in the Schatten S_p classes. We also showed that its nontrivial spectrum while acting on such spaces is precisely the closed unit disk in the complex plane...|$|R
40|$|We {{consider}} a heterostructure of a metal and a paramagnetic Mott insulator using {{an adaptation of}} dynamical mean field theory to describe inhomogeneous systems. The metal can penetrate into the insulator via the Kondo effect. We investigate the scaling properties of the metal-insulator interface close to the critical point of the Mott insulator. At criticality, the quasiparticle <b>weight</b> <b>decays</b> as 1 /x^ 2 with distance x from the metal within our mean field theory. Our numerical results (using the numerical renormalization group as an impurity solver) show that the prefactor of this power law is extremely small. Comment: 4 pages, 3 figure...|$|R
5000|$|The theory {{mentioned}} above gives an obvious strategy: create {{a set of}} experts with low bias and high variance, and then average them. Generally, {{what this means is}} to create a set of experts with varying parameters; frequently, these are the initial synaptic weights, although other factors (such as the learning rate, momentum etc.) may be varied as well. Some authors recommend against varying <b>weight</b> <b>decay</b> and early stopping. The steps are therefore: ...|$|E
40|$|We {{note that}} common {{implementations}} of adaptive gradient algorithms, such as Adam, limit the potential benefit of <b>weight</b> <b>decay</b> regularization, because the weights do not decay multiplicatively (as {{would be expected}} for standard <b>weight</b> <b>decay)</b> but by an additive constant factor. We propose {{a simple way to}} resolve this issue by decoupling <b>weight</b> <b>decay</b> and the optimization steps taken w. r. t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of <b>weight</b> <b>decay</b> factor from the setting of the learning rate for both standard SGD and Adam, and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). We also demonstrate that longer optimization runs require smaller <b>weight</b> <b>decay</b> values for optimal results and introduce a normalized variant of <b>weight</b> <b>decay</b> to reduce this dependence. Finally, we propose a version of Adam with warm restarts (AdamWR) that has strong anytime performance while achieving state-of-the-art results on CIFAR- 10 and ImageNet 32 x 32. Our source code is available at [URL]...|$|E
40|$|<b>Weight</b> <b>decay</b> was {{proposed}} to reduce overfitting as it often {{appears in the}} learning tasks of artificial neural networks. In this paper <b>weight</b> <b>decay</b> is applied to a well defined model system based on a single layer perceptron, which exhibits strong overfitting. Since the optimal non-overfitting solution is known for this system, we can compare {{the effect of the}} <b>weight</b> <b>decay</b> with this solution. A strategy to find the optimal <b>weight</b> <b>decay</b> strength is proposed, which leads to the optimal solution for any number of examples. 1 The Model Overfitting is a problem in neural network learning which can reduce the performance drastically. Simply defined, overfitting means that the networks learns too many details from the examples and neglects its generalization ability. In this paper we study how <b>weight</b> <b>decay,</b> see [3], can help to avoid overfitting. In previous works [1] and [2], we have established a model system, which can be handled analytically, but shows already many of the characteristi [...] ...|$|E
40|$|This paper {{examines}} sparse grid quadrature on weighted tensor products (wtp) of reproducing kernel Hilbert spaces {{on products}} of the unit sphere S 2. We describe a wtp quadrature algorithm based on an algorithm of Hegland [1], and also formulate a version of Wasilkowski and Wo´zniakowski’s wtp algorithm [2], here called the ww algorithm. We prove that our algorithm is optimal and therefore lower in cost than the ww algorithm, and therefore both algorithms have the optimal asymptotic rate of convergence given by Theorem 3 of Wasilkowski and Wo´zniakowski [2]. Even so, the initial rate of convergence can be very slow, if the dimension <b>weights</b> <b>decay</b> slowly enough...|$|R
40|$|In this article, {{we study}} linearly edge-reinforced random walk on general {{multi-level}} ladders for large initial edge weights. For infinite ladders, {{we show that}} the process can be represented as a random walk in a random environment, given by random weights on the edges. The edge <b>weights</b> <b>decay</b> exponentially in space. The process converges to a stationary process. We provide asymptotic bounds for {{the range of the}} random walker up to a given time, showing that it localizes much more than an ordinary random walker. The random environment is described in terms of an infinite-volume Gibbs measure. Comment: Published at [URL] in the Annals of Probability ([URL] by the Institute of Mathematical Statistics ([URL]...|$|R
40|$|We {{study the}} {{relaxation}} dynamics of a nonequilibrium Luttinger liquid after a sudden interaction switch-on ("quench"), focussing on a double-step initial momentum distribution function. In {{the framework of}} the non-equilibrium bosonization, the results are obtained in terms of singular Fredholm determinants that are evaluated numerically and whose asymptotics are found analytically. While the quasi-particle <b>weights</b> <b>decay</b> exponentially with time after the quench, this is not a relaxation into a thermal state, in view of the integrability of the model. The steady-state distribution emerging at infinite times retains two edges which support Luttinger-liquid-like power-law singularities smeared by dephasing. The obtained critical exponents and the dephasing length are found to depend on the initial nonequilibrium state. Comment: 11 pages, 5 figure...|$|R
30|$|The {{learning}} {{parameters are}} set as follows, the <b>weight</b> <b>decay</b> parameter λ= 1 × 10 − 4, {{the weight of}} the penalty term β= 3, and the sparsity parameter ρ= 4 × 10 − 3. The maximum number of iterations is set to 300. The parameters for the training of the softmax classifier are set as follows. The <b>weight</b> <b>decay</b> parameter λ= 1 × 10 − 4 and the maximum number of iterations was set to 200. In the fine-tuning phase, the <b>weight</b> <b>decay</b> parameter was changed to λ= 3 × 10 − 3.|$|E
40|$|Although <b>weight</b> <b>decay</b> {{learning}} {{has been proposed}} to improve generalization ability of a neural network, many simulated {{studies have demonstrated that}} it is able to improve fault tolerance. To explain the underlying reason, this paper presents an analytical result showing the equivalence between adding <b>weight</b> <b>decay</b> and adding explicit regularization on training a RBF to tolerate multiplicative weight noise. Under a mild condition, it is proved that explicit regularization will be reduced to <b>weight</b> <b>decay.</b> Index Terms-Weight decay, Explicit regularization, Radial basis function (RBF), Multiplicative weight noise (MWN). 1...|$|E
40|$|Abstract — In this paper, the regularization of {{employing}} the forgetting recursive least square (FRLS) training technique on feedforward neural networks is studied. We derive our {{result from the}} corresponding equations for the expected prediction error and the expected training error. By comparing these error equations with other equations obtained previously from the <b>weight</b> <b>decay</b> method, {{we have found that}} the FRLS technique has an effect which is identical to that of using the simple <b>weight</b> <b>decay</b> method. This new finding suggests that the FRLS technique is another on-line approach for the realization of the <b>weight</b> <b>decay</b> effect. Besides, we have shown that, under certain conditions, both the model complexity and the expected prediction error of the model being trained by the FRLS technique are better than the one trained by the standard RLS method. Index Terms—Feedforward neural network, forgetting recursive least square, model complexity, prediction error, regularization, <b>weight</b> <b>decay.</b> I...|$|E
40|$|In {{the present}} paper, linearly edge-reinforced random walk is studied {{on a large}} class of {{one-dimensional}} periodic graphs satisfying a certain reflection symmetry. It is shown that the edge-reinforced random walk is recurrent. Estimates for {{the position of the}} random walker are given. The edge-reinforced random walk has a unique representation as a random walk in a random environment, where the random environment is given by random weights on the edges. It is shown that these <b>weights</b> <b>decay</b> exponentially in space. The distribution of the random weights equals the distribution of the asymptotic proportion of time spent by the edge-reinforced random walker on the edges of the graph. The results generalize work of the authors in [MR 05 c], [Rol 06], and [MR 05 a] to a large class of graphs and to periodic initial weights with a reflection symmetry. 4 5...|$|R
40|$|International audienceThis paper {{introduces}} {{an explicit}} covert communication code for binary-input asynchronous discrete memoryless channels based on binary polar codes, in which legitimate parties exploit uncertainty created {{by both the}} channel noise {{and the time of}} transmission to avoid detection by an adversary. The proposed code jointly ensures reliable communication for a legitimate receiver and low probability of detection with respect to the adversary, both observing noisy versions of the codewords. Binary polar codes are used to shape the weight distribution of codewords and ensure that the average <b>weight</b> <b>decays</b> as the block length grows. The performance of the proposed code is severely limited by the speed of polarization, which in turn controls the decay of the average codeword weight with the block length. Although the proposed construction falls largely short of achieving the performance of random codes, it inherits the low-complexity properties of polar codes...|$|R
40|$|It {{has been}} {{established}} that matrix product states {{can be used to}} compute the ground state and single-particle excitations and their properties of lattice gauge theories at the continuum limit. However, by construction, in this formalism the Hilbert space of the gauge fields is truncated to a finite number of irreducible representations of the gauge group. We investigate quantitatively the influence of the truncation of the infinite number of representations in the Schwinger model, one-flavor QED 2, with a uniform electric background field. We compute the two-site reduced density matrix of the ground state and the weight of each of the representations. We find that this <b>weight</b> <b>decays</b> exponentially with the quadratic Casimir invariant of the representation which justifies the approach of truncating the Hilbert space of the gauge fields. Finally, we compute the single-particle spectrum of the model {{as a function of the}} electric background field...|$|R
40|$|We {{investigate}} {{the use of}} a regularization prior that we show has pruning properties. Analyses are conducted both using a Bayesian framework and with the generalization method, on a simple toy problem. Results are thoroughly compared with those obtained with a traditional <b>weight</b> <b>decay.</b> Keywords: Regularization, Generalization method, Bayesian learning, Evidence framework, Laplace prior, Comparison with <b>weight</b> <b>decay.</b> 1 Gauss and Laplace priors Regularization by <b>weight</b> <b>decay,</b> and the associated Bayesian interpretation involving a Gaussian prior is a well established part of modern neural computation, see e. g., [KH 91]. In [HR 94] it was shown that if the <b>weight</b> <b>decay</b> parameter is determined by data, either using MacKay's Evidence procedure, or by minimizing generalization error, pruning may result. In a recent communication, [Wil 95] elucidated the interesting properties of the Laplacian prior, in particular he showed that this prior directly leads to pruning of small weights, for any fixe [...] ...|$|E
40|$|Several {{authors have}} {{suggested}} viewing boosting as a gradient descent {{search for a}} good fit in function space. At each iteration observations are re-weighted using the gradient of the underlying loss function. We present an approach of <b>weight</b> <b>decay</b> for observation weights which is equivalent to “robustifying ” the underlying loss function. At the extreme end of decay this approach converges to Bagging, which {{can be viewed as}} boosting with a linear underlying loss function. We illustrate the practical usefulness of <b>weight</b> <b>decay</b> for improving prediction performance and present an equivalence between one form of <b>weight</b> <b>decay</b> and “Huberizing ” — a statistical method for making loss functions more robust. 1...|$|E
40|$|We {{present a}} simple trick {{to get an}} {{approximate}} estimate of the <b>weight</b> <b>decay</b> parameter. The method combines early stopping and <b>weight</b> <b>decay,</b> into the estimate = krE(W es) k=k 2 W es k, where W es is the set of weights at the early stopping point, and E(W) is the training data fit error. The estimate is demonstrated and compared to the standard cross-validation procedure for selection on one synthetic and four real life data sets. The result is that is as good an estimator for the optimal <b>weight</b> <b>decay</b> parameter value as the standard search estimate, but orders of magnitude quicker to compute. The results also show that <b>weight</b> <b>decay</b> can produce solutions that are significantly superior to committees of networks trained with early stopping. 1 Introduction A regression problem which does not put constraints on the model used is illposed [21], because there are infinitely many functions that can fit a finite set of training data perfectly. Furthermore, real life data sets tend to h [...] ...|$|E
40|$|This paper {{introduces}} {{an explicit}} covert communication code for binary-input asynchronous discrete memoryless channels based on binary polar codes, in which legitimate parties exploit uncertainty created {{by both the}} channel noise {{and the time of}} transmission to avoid detection by an adversary. The proposed code jointly ensures reliable communication for a legitimate receiver and low probability of detection with respect to the adversary, both observing noisy versions of the codewords. Binary polar codes are used to shape the weight distribution of codewords and ensure that the average <b>weight</b> <b>decays</b> as the block length grows. The performance of the proposed code is severely limited by the speed of polarization, which in turn controls the decay of the average codeword weight with the block length. Although the proposed construction falls largely short of achieving the performance of random codes, it inherits the low-complexity properties of polar codes...|$|R
40|$|Second order {{properties}} of cost functions for recurrent networks are investigated. We analyze a layered fully recurrent architecture, {{the virtue of}} this architecture is that it features the conventional feedforward architecture as a special case. A detailed description of recursive computation of the full Hessian of the network cost function is provided. We discuss the possibility of invoking simplifying approximations of the Hessian and show how <b>weight</b> <b>decays</b> iron the cost function and thereby greatly assist training. We present tentative pruning results, using Hassibi et al. 's Optimal Brain Surgeon, demonstrating that recurrent networks can construct an efficient internal memory. 1 LEARNING IN RECURRENT NETWORKS Time series processing is an important application area for neural networks and numerous architectures have been suggested, see e. g. (Weigend and Gershenfeld, 94). The most general structure is a fully recurrent network {{and it may be}} adapted using Real Time Rec [...] ...|$|R
40|$|International audienceHankel {{operators}} with anti-holomorphic {{symbols are}} studied {{for a large}} class of weighted Fock spaces on. The weights defining these Hilbert spaces are radial and subject to a mild smoothness condition. In addition, {{it is assumed that}} the <b>weights</b> <b>decay</b> at least as fast as the classical Gaussian weight. The main result of the paper says that a Hankel operator on such a Fock space is bounded if and only if the symbol belongs to a certain BMOA space, defined via the Berezin transform. The latter space coincides with a corresponding Bloch space which is defined by means of the Bergman metric. This characterization of boundedness relies on certain precise estimates for the Bergman kernel and the Bergman metric. Characterizations of compact Hankel operators and Schatten class Hankel operators are also given. In the latter case, results on Carleson measures and Toeplitz operators along with Hörmander's L^ 2 estimates for the ∂̅ operator are key ingredients in the proof...|$|R
3000|$|The Multi-Layer Perception with <b>Weight</b> <b>Decay</b> (MLPWD) {{prediction}} model to forecast the workload in the environments with the periodic workload pattern [...]...|$|E
40|$|Abstract—While weight noise {{injection}} {{during training}} {{has been adopted}} in attaining fault tolerant neural networks (NNs), theoretical and empirical studies on the online algorithms developed based on these strategies {{have yet to be}} complete. In this paper, we present results on two important aspects in online learning algorithms based on combining weight noise injection and <b>weight</b> <b>decay.</b> Through intensive computer simulations, the convergence behaviors of those algorithms and the performance of the NNs generated by these algorithms are elucidated. It is found that (i) the online learning algorithm based on purely multiplicative weight noise injection does not converge, (ii) the algorithms combining weight noise injection and <b>weight</b> <b>decay</b> exhibit better convergence behaviors than their pure weight noise injection counterparts, and (iii) the neural networks attained by those algorithms combining weight noise injection and <b>weight</b> <b>decay</b> show better fault tolerance abilities than the neural networks attained by the pure weight noise injection-based algorithms. These empirical results not just can supplement our recent work done on the convergence behaviors of the weight noise injection-based learning algorithms [16], but also provide new information on effect of weight noise and <b>weight</b> <b>decay</b> on the neural networks that are generated by these algorithms...|$|E
40|$|Deep neural {{networks}} have proved {{very successful in}} domains where large training sets are available, but {{when the number of}} training samples is small, their performance suffers from overfitting. Prior methods of reducing overfitting such as <b>weight</b> <b>decay,</b> Dropout and DropConnect are data-independent. This paper proposes a new method, GraphConnect, that is data-dependent, and is motivated by the observation that data of interest lie close to a manifold. The new method encourages the relationships between the learned decisions to resemble a graph representing the manifold structure. Essentially GraphConnect is designed to learn attributes that are present in data samples in contrast to <b>weight</b> <b>decay,</b> Dropout and DropConnect which are simply designed to {{make it more difficult to}} fit to random error or noise. Empirical Rademacher complexity is used to connect the generalization error of the neural network to spectral properties of the graph learned from the input data. This framework is used to show that GraphConnect is superior to <b>weight</b> <b>decay.</b> Experimental results on several benchmark datasets validate the theoretical analysis, and show that when the number of training samples is small, GraphConnect is able to significantly improve performance over <b>weight</b> <b>decay.</b> Comment: Theorems need more validatio...|$|E
40|$|Recall that an {{integration}} rule {{is said to}} have a trigonometric {{degree of}} exactness m if it integrates exactly all trigonometric polynomials of degree > 6. We introduce three notions of weighted degree of exactness, where we use weights to characterize the anisotropicness of the integrand with respect to successive coordinate directions. Unlike in the classical unweighted setting, the minimal number of integration points needed to achieve a prescribed weighted degree of exactness no longer grows exponentially with d provided that the <b>weights</b> <b>decay</b> sufficiently fast. We present a component-by-component algorithm {{for the construction of a}} rank-$ lattice rule such that (i) it has a prescribed weighted degree of exactness, and (ii) its worst case error achieves the optimal rate of convergence in a weighted Korobov space. Then we introduce a modified, more practical, version of this algorithm which maximizes the weighted degree of exactness in each step of the construction. Both algorithms are illustrated by numerical results. status: publishe...|$|R
40|$|Hankel {{operators}} with anti-holomorphic {{symbols are}} studied {{for a large}} class of weighted Fock spaces on. The weights defining these Hilbert spaces are radial and subject to a mild smoothness condition. In addition, {{it is assumed that}} the <b>weights</b> <b>decay</b> at least as fast as the classical Gaussian weight. The main result of the paper says that a Hankel operator on such a Fock space is bounded if and only if the symbol belongs to a certain BMOA space, defined via the Berezin transform. The latter space coincides with a corresponding Bloch space which is defined by means of the Bergman metric. This characterization of boundedness relies on certain precise estimates for the Bergman kernel and the Bergman metric. Characterizations of compact Hankel operators and Schatten class Hankel operators are also given. In the latter case, results on Carleson measures and Toeplitz operators along with Hörmander's L^ 2 estimates for the ∂̅ operator are key ingredients in the proof...|$|R
40|$|Strong {{consistency}} and asymptotic normality of the Gaussian pseudo-maximum likelihood {{estimate of the}} parameters in a wide class of ARCH(1) processes are established. We require the ARCH <b>weights</b> to <b>decay</b> at least hyperbolically, with a faster rate needed for the central limit theorem than for the law of large numbers. Various rates are illustrated in examples of particular parameteriza- tions in which our conditions are shown to be satis ed...|$|R
