215|9|Public
2500|$|This {{command is}} {{identical}} to a generic memory write, but comes with the guarantee that one or more whole cache lines will be written, with all byte selects enabled. [...] This is an optimization for <b>write-back</b> caches snooping the bus. [...] Normally, a <b>write-back</b> cache holding dirty data must interrupt the write operation long enough to write its own dirty data first. [...] If the write is performed using this command, the data to be written back is guaranteed to be irrelevant, and may simply be invalidated in the <b>write-back</b> cache.|$|E
2500|$|PCI {{originally}} included optional {{support for}} <b>write-back</b> cache coherence. [...] This required support by cacheable memory targets, which {{would listen to}} two pins from the cache on the bus, SDONE (snoop done) and SBO# (snoop backoff).|$|E
50|$|The current {{version of}} CAS for Linux {{supports}} write-through, <b>write-back,</b> and write-around caching. The Windows versions of CAS support write-through and <b>write-back</b> caching.|$|E
5000|$|Modern systems use {{variants}} of the MSI protocol {{to reduce the}} amount of traffic in the coherency interconnect. The MESI protocol adds an [...] "Exclusive" [...] state to reduce the traffic caused by writes of blocks that only exist in one cache. The MOSI protocol adds an [...] "Owned" [...] state to reduce the traffic caused by <b>write-backs</b> of blocks that are read by other caches. The MOESI protocol does both of these things.|$|R
50|$|From the start, the NetWare design {{focused on}} servers with copious amounts of RAM. The entire file {{allocation}} table (FAT) was read into RAM when a volume was mounted, thereby requiring a minimum amount of RAM proportional to online disk space; adding a disk to a server would often require a RAM upgrade as well. Unlike most competing network operating systems prior to Windows NT, NetWare automatically used all otherwise unused RAM for caching active files, employing delayed <b>write-backs</b> to facilitate re-ordering of disk requests (elevator seeks). An unexpected shutdown could therefore corrupt data, making an uninterruptible power supply practically a mandatory part of a server installation.|$|R
40|$|High-performance {{multiprocessor}} {{systems are}} built around out-of-order processors with aggressive branch predictors. Despite their relatively high branch prediction accuracies, these processors execute many memory instructions on mispredicted paths. Previous studies {{that focused on}} uniprocessors systems showed that these wrong-path memory references may pollute the caches by bringing in data that are not needed on the correct execution path and by evicting useful data or instructions. Additionally, they may also {{increase the amount of}} cache and memory traffic. On the positive side, however, they may have a prefetching effect for memory references on the correct path. For multiprocessor systems, the performance implications of these wrong-path memory references are more widespread, including, but not limited to, more cache-to-cache transfers, <b>write-backs,</b> and cache block state transitions. In this thesis, we investigated the effects of wrong path memor...|$|R
50|$|The {{enhanced}} Am486 series supported {{new features}} like extended power-saving modes and an 8 KiB <b>Write-Back</b> L1-Cache, later versions even got an upgrade to 16 KiB <b>Write-Back</b> L1-Cache.|$|E
5000|$|Highly {{efficient}} <b>write-back</b> implementation dirty data {{is always}} written out in sorted order, and optionally background <b>write-back</b> is smoothly throttled down to keeping configured {{percentage of the}} cache dirty ...|$|E
5000|$|This {{command is}} {{identical}} to a generic memory write, but comes with the guarantee that one or more whole cache lines will be written, with all byte selects enabled. This is an optimization for <b>write-back</b> caches snooping the bus. Normally, a <b>write-back</b> cache holding dirty data must interrupt the write operation long enough to write its own dirty data first. If the write is performed using this command, the data to be written back is guaranteed to be irrelevant, and may simply be invalidated in the <b>write-back</b> cache.|$|E
40|$|This paper {{presents}} a new concurrent multiplethreaded architectural model, called superthreading, for exploiting thread-level parallelism on a processor. This architectural model adopts a thread pipelining execution model that allows threads with data dependences and control dependences {{to be executed}} in parallel. The basic idea of thread pipelining is to compute and forward recurrence data and possible dependent store addresses to the next thread as soon as possible, so the next thread can start execution and perform runtime data dependence checking. Thread pipelining also forces contiguous threads to perform their memory <b>write-backs</b> in order, which enables the compiler to fork threads with control speculation. With run-time support for data dependence checking and control speculation, the superthreaded architectural model can exploit loop-level parallelism from {{a broad range of}} applications. 1 Introduction As the rapid progress of VLSI technology allows microprocessors to have more [...] ...|$|R
50|$|Spring {{used two}} sorts of file systems, a local file system which {{was similar to}} most common Unix systems, {{as well as a}} caching file system for network devices. The caching system {{demonstrates}} the utility of Spring's VM/pager split, using the same physical memory from the VM which it would have to use normally, the CFS short-circuited all read requests to the local cache, and did lazy <b>write-backs</b> every 30 seconds to the source file system. This would be particularly notable if common Unix directories were being loaded over the network, the normal setup for labs of workstations. Most Unix systems use similar caching mechanisms for the same performance reasons, but would end up using RAM twice, once in the cache, and again in the programs using it. The CFS also cached names from the remote system, making the initial directory traversal and open requests much faster.|$|R
40|$|Uniprocessor {{studies have}} shown that wrong-path memory {{references}} pollute the caches by bringing in data that are not needed for the correct execution path and by evicting useful data or instructions. Additionally, they also increase the amount of cache and memory traffic. On the positive side, however, they may have a prefetching effect for loads and instructions on the correct path. While the wrong-path effects are well studied for uniprocessors, there is no work on its effects on multiprocessor systems. In this paper, we explore the effects of wrong-path memory references on the memory system behavior of sharedmemory multiprocessor (SMP) systems with broadcast (snoop-based) and directory-based cache coherence. We show that in contrast to uniprocessor systems, these wrong-path memory references can increase the amount of cache-to-cache transfers by 32 %, invalidations by 8 % and 20 % for broadcast and directory-based SMPs, respectively, and the number of <b>write-backs</b> by up to 67 % for both systems. In addition to the extra coherence traffic, wrong-path memory references also increases the number of cache line state transitions by 21 % and 32 % for broadcast and directory-based SMPs, respectively. ...|$|R
5000|$|Write-through (which is the default), <b>write-back</b> and write-around {{policies}} ...|$|E
5000|$|WT = Write-Through cache {{strategy}}, WB = <b>Write-Back</b> cache strategy ...|$|E
5000|$|L1 {{cache size}} 16 KB <b>write-back</b> 4-way set {{associative}} unified I/D cache.|$|E
40|$|This thesis {{presents}} a concurrent multiple-threaded architectural model, called superthreading, for exploiting fine-grained thread-level parallelism on a processor. This architectural model adopts a thread pipelining execution model that allows threads with data dependences and control dependences {{to be executed}} in parallel. The basic idea of thread pipelining is to compute and forward recurrence data and possible dependent store addresses to the next thread as soon as possible, so the next thread can start execution and perform run-time data dependence checking. Thread pipelining also forces contiguous threads to perform their memory <b>write-backs</b> in order, which enables the compiler to fork threads with control speculation. With run-time support for data dependence checking and control speculation, the superthreaded architecture can exploit loop-level and instruction-level parallelism from {{a broad range of}} applications. In this thesis we also present the compiler techniques for superthreaded processors. Many existing compiler techniques used in traditional parallelizing compilers for multiprocessors as well as some specific compiler techniques for superthreaded processors are needed for generating su-perthreaded codes and enhancing parallelism between threads. We evaluate the performance of the superthreaded architecture with a trace-driven, cycle-by-cycle superthreaded processor simulator by using codes transformed by hand and codes generated by our superthreading compiler proto-type. The simulation results show that a superthreaded processor can achieve good performance by exploiting both thread-level and instruction-level parallelism in programs...|$|R
40|$|Emerging {{non-volatile}} memory {{technologies such as}} phase-change memory,resistive random access memory, spin-torque transfer memory and 3 D XPoint memorypromise to significantly increase the I/O sub-system performance. But, current disk-centricsystems fall short in {{taking advantage of the}} bandwidth and latency characteristicsof such memories. This dissertation presents three systems that address: hardware, systemsoftware and micro-architecture support for faster-than-flash non-volatile memories. First, we explore system design for using emerging non-volatile memories (NVM) as a persistent cache that bridges the price and density gap between NVMs and denserstorage. Bankshot is a prototype PCIe-based intelligent cache with access latencies anorder of magnitude lower than conventional SSDs. Unlike previous designs of SSDcaches, Bankshot relies on the OS for heavyweight operations such as servicing missesand <b>write-backs</b> while allows cache hits to bypass the operating system (OS) and itsassociated software overhead entirely. Second, we extend the ability to define application specific interface to emergingNVM SSDs such that a broad range of applications can benefit from low-latency,high-bandwidth access to the SSDâ€™s data. Our prototype system, called Willow, supportsconcurrent execution of an application and trusted code within the SSD without compromisingon file system protections. We present three SSD apps - Caching, Appendand zero-out that showcase Willows capabilities. Caching extends Willows semanticsto use the SSD storage as a persistent cache while file-append and zero-out extends thesemantics for file system operations. Finally, we address the challenge of accessing byte-addressable, emerging NVMswith higher than DRAM latency when attached to the processor memory bus; specificallyfor loads. We propose Non-Blocking Load (NBLD), an instruction set extension tomitigate pipeline stalls from long-latency memory accesses. NBLD is a non-blockinginstruction that brings data into the upper levels of the cache hierarchy, however, unlikeprefetch instructions, NBLD triggers the execution of application-specific code once datais resident in the cache, effectively hiding the latency of the memory...|$|R
40|$|The {{trend of}} {{increasing}} processor performance by boosting frequency has been halted due to excessive power dissipation. However, transistor density {{has continued to}} grow which has enabled integration of many cores on a single chip to meet the performance requirements of future applications. Scaling to hundreds of cores on a single chip present a number of challenges, mainly efficient data access and on-chip communication. Near-threshold voltage (NTV) operation {{has been identified as}} the most energy efficient region to operate in. Running at NTV can facilitate efficient data access, however, it introduces bit-cell faults in the SRAMs which needs to be dealt with. Another avenue to extract data access efficiency is by improving on-chip data locality. Shared memory abstraction dominates the traditional small computer and embedded space due to its ease of programming. For efficiency, shared memory is often implemented with hardware support for synchronization and cache coherence among the cores. However, accesses to shared data with frequent writes results in wasteful invalidations, synchronous <b>write-backs,</b> and cache line ping-pong leading to low spatio-temporal locality. Moreover, communication through coherent caches and shared memory primitives is inefficient because it can take many instructions to coordinate between cores. This thesis focuses on mitigating the effects of the data access and communication challenges and make architectural contributions to enable efficient and scalable many-core processors. The main idea is to minimize data movement and make each necessary data access more efficient. In this regard, a novel private level- 1 cache architecture is presented to enable efficient and fault-free operation at near-threshold voltages. To better exploit data locality, a last-level cache (LLC) data replication scheme is proposed that co-optimizes data locality and off-chip miss rate. It utilizes an in-hardware predictive mechanism to classify data and only replicate high reuse data in the local LLC bank. Finally, a hybrid shared memory, explicit messaging architecture is proposed to enable efficient on-chip communication. In this architecture the shared memory model is retained, however, a set of lightweight in-hardware explicit message passing style instructions are introduced in the instruction set architecture (ISA) that enable efficient movement of computation to where data is located...|$|R
5000|$|L1 cache {{has been}} changed from write-through to <b>write-back,</b> {{allowing}} for lower latency and higher bandwidth ...|$|E
5000|$|Both write-through and <b>write-back</b> {{policies}} can {{use either}} of these write-miss policies, but usually they are paired in this way: ...|$|E
5000|$|A <b>write-back</b> cache uses write allocate, {{hoping for}} {{subsequent}} writes (or even reads) {{to the same}} location, which is now cached.|$|E
5000|$|A {{portion of}} SSD space {{is used as}} a <b>write-back</b> buffer to absorb {{incoming}} write traffic, which hides perceivable latencies and boosts write performance.|$|E
5000|$|Reserved: The {{block is}} the only copy of the memory, {{but it is still}} coherent. No <b>write-back</b> is needed if the block is replaced.|$|E
50|$|The Dragon cache {{coherence}} protocol is the schema {{used in the}} Xerox Dragon multiprocessor workstation, developed by Xerox PARC. This protocol uses a <b>write-back</b> policy.|$|E
50|$|Memory type range {{registers}} (MTRRs) are {{a set of}} processor supplementary capabilities control registers {{that provide}} system software with control of how accesses to memory ranges by the CPU are cached. It uses a set of programmable model-specific registers (MSRs) which are special registers provided by most modern CPUs. Possible access modes to memory ranges can be uncached, write-through, write-combining, write-protect, and <b>write-back.</b> In <b>write-back</b> mode, writes are written to the CPU's cache and the cache is marked dirty, so that its contents are written to memory later.|$|E
50|$|Both <b>write-back</b> and write-through (which is the default) {{policies}} are supported for caching write operations. In {{case of the}} <b>write-back</b> policy, written data is stored inside the SSD caches first, and propagated to the HDDs later in a batched way while performing seek-friendly operations making bcache to act also as an I/O scheduler. For the write-through policy, which ensures that no write operation is marked as finished until the data requested to be written has reached both SSDs and HDDs, performance improvements are reduced by effectively performing only caching of the written data.|$|E
50|$|Other {{policies}} {{may also}} trigger data <b>write-back.</b> The client may make many changes to {{data in the}} cache, and then explicitly notify the cache to write back the data.|$|E
5000|$|Multiple SSDs in a cache set only dirty data (for the <b>write-back</b> policy) and {{metadata}} {{would be}} mirrored, without wasting SSD {{space for the}} clean data and read caches ...|$|E
50|$|If data {{is written}} to the cache, at some point {{it must also be}} written to main memory; the timing of this write is known as the write policy. In a write-through cache, every write to the cache causes a write to main memory. Alternatively, in a <b>write-back</b> or copy-back cache, writes are not {{immediately}} mirrored to the main memory, and the cache instead tracks which locations have been written over, marking them as dirty. The data in these locations is written back to the main memory only when that data is evicted from the cache. For this reason, a read miss in a <b>write-back</b> cache may sometimes require two memory accesses to service: one to first write the dirty location to main memory, and then another to read the new location from memory. Also, a write to a main memory location that is not yet mapped in a <b>write-back</b> cache may evict an already dirty location, thereby freeing that cache space for the new memory location.|$|E
5000|$|A [...] "Disk Cache" [...] or [...] "File Cache" [...] keeps {{statistics}} on the data contained within it and commits data within a time-out period in <b>write-back</b> modes. A buffer does none of this.|$|E
50|$|PCI {{originally}} included optional {{support for}} <b>write-back</b> cache coherence. This required support by cacheable memory targets, which {{would listen to}} two pins from the cache on the bus, SDONE (snoop done) and SBO# (snoop backoff).|$|E
50|$|The 82497 Cache Controller {{implements}} the MESI <b>write-back</b> {{protocol for}} full multiprocessing support. Dual ported buffers and registers allow the 82497 to concurrently handle CPU bus, memory bus, and internal cache operation for maximum performance.|$|E
5000|$|Lustre 1.2.0, {{released}} in March 2004, worked on Linux kernel 2.6, {{and had a}} [...] "size glimpse" [...] feature to avoid lock revocation on files undergoing write, and client side data <b>write-back</b> cache accounting (grant).|$|E
5000|$|<b>Write-back</b> (also called write-behind): initially, {{writing is}} done {{only to the}} cache. The write to the backing store is {{postponed}} until the cache blocks containing the data {{are about to be}} modified/replaced by new content.|$|E
50|$|Because of {{the bubble}} (the blue ovals in the illustration), the processor's Decode {{circuitry}} is idle during cycle 3. Its Execute circuitry is idle during cycle 4 and its <b>Write-back</b> circuitry is idle during cycle 5.|$|E
50|$|A <b>write-back</b> cache is {{more complex}} to implement, since it needs to track which of its {{locations}} have been written over, and mark them as dirty for later writing to the backing store. The data in these locations are written back to the backing store only when they are evicted from the cache, an effect {{referred to as a}} lazy write. For this reason, a read miss in a <b>write-back</b> cache (which requires a block to be replaced by another) will often require two memory accesses to service: one to write the replaced data from the cache back to the store, and then one to retrieve the needed data.|$|E
