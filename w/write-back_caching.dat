10|3|Public
50|$|The current {{version of}} CAS for Linux {{supports}} write-through, write-back, and write-around caching. The Windows versions of CAS support write-through and <b>write-back</b> <b>caching.</b>|$|E
50|$|Mariah was {{a revised}} version of the Rigel chip set {{fabricated}} by DEC in their 1 µm CMOS-3 process, with higher clock frequencies between 62 and 71 MHz. The Mariah CPU, FPU and cache controller were designated DC595, DC596 and DC597 respectively. Enhancements over Rigel included a 4 kB first-level cache and 32-bit physical memory addressing in the Mariah CPU, and <b>write-back</b> <b>caching</b> implemented in the cache controller chip. Mariah {{was used in the}} VAX 6000 Model 500, MicroVAX 3100 Model 80 and VAXstation 4000 Model 60.|$|E
40|$|As {{the size}} of cloud systems {{and the number of}} hosted VMs rapidly grow, the {{scalability}} of shared VM storage systems becomes a serious issue. Client-side flash-based caching has the potential to improve the performance of cloud VM stor-age by employing flash storage available on the client-side of the storage system to exploit the locality inherent in VM IOs. However, because of the limited capacity and durabil-ity of flash storage, it is important to determine the proper size and configuration of the flash caches used in cloud sys-tems. This paper provides answers to the key design ques-tions of cloud flash caching based on dm-cache, a block-level caching solution customized for cloud environments, and a large amount of long-term traces collected from real-world public and private clouds. The study first validates that cloud workloads have good cacheability and dm-cache-based flash caching incurs low overhead with respect to commod-ity flash devices. It further reveals that <b>write-back</b> <b>caching</b> substantially outperforms write-through caching in typical cloud environments due to the reduction of server IO load. It also shows that there is a tradeoff on making a flash cache persistent across client restarts which saves hours of cache warm-up time but incurs considerable overhead from com-mitting every metadata update persistently. Finally, to re-duce the data loss risk from using <b>write-back</b> <b>caching,</b> the pa-per proposes a new cache-optimized RAID technique, which minimizes the RAID overhead by introducing redundancy of cache dirty data only, and shows to be significantly faster than traditional RAID and write-through caching. 1...|$|E
40|$|Choices, an {{object-oriented}} operating system, {{has been}} ported to the ARM architecture. The porting effort {{was based on}} the exising code-base for the SPARC processor based SPARCStation 1. Choices supports two ARM-based machine configurations: the Texas Instruments OMAP 1610 and the ARM Integrator as emulated by QEMU. Choices on ARM supports virtual memory, <b>write-back</b> data <b>caching</b> and interrupt management with support for serial I/O and timers. An environment consisting of X 10 -based remote power control devices, networked serial ports and comprehensive build scripts enable short development cycles with the OMAP hardware. The QEMU based virtual hardware presented powerful debug capabilities useful for developing an operating system for the ARM. ...|$|R
40|$|Digital Equipment Corporation Extensive caching is a {{key feature}} of the Echo {{distributed}} file system. Echo client machines maintain coherent caches of file and directory data and properties, with write-behind (delayed <b>write-back)</b> of all <b>cached</b> information. Echo specifies ordering constraints on this write-behind, enabling applications to store and maintain consistent data structures in the file system even when crashes or network faults prevent some writes from being completed. In this paper we describe the Echo cache’s coherence and ordering semantics, show how they can improve the performance and consistency of applications, and explain how they are implemented. We also discuss the general problem of reliably notifying applications and users when write-behind is 10 SE we addressed this problem {{as part of the}} Echo design, but did not find a fully satisfactory solution...|$|R
40|$|Extensive caching is a {{key feature}} of the Echo {{distributed}} file system. Echo client machines maintain coherent caches of file and directory data and properties, with write-behind (delayed <b>write-back)</b> of all <b>cached</b> information. Echo specifies ordering constraints on this write-behind, enabling applications to store and maintain consistent data structures in the file system even when crashes or network faults prevent some writes from being completed. In this paper we describe the Echo cache's coherence and ordering semantics, show how they can improve the performance and consistency of applications, and explain how they are implemented. We also discuss the general problem of reliably notifying applications and users when write-behind is lost; we addressed this problem {{as part of the}} Echo design but did not find a fully satisfactory solution. Contents 1 Introduction 1 2 Design motivation 2 3 Coherence and ordering semantics 4 3. 1 Ordering constraints : : : : : : : : : : : : : : : : [...] ...|$|R
40|$|Modern {{data centers}} are {{increasingly}} using shared storage solutions {{for ease of}} management. Data is cached on the client side on inexpensive and high-capacity flash devices, helping improve performance and reduce contention on the storage side. Currently, write-through caching is used because it ensures consistency and durability under client failures, but it offers poor performance for write-heavy workloads. In this work, we propose two write-back based caching policies, called write-back flush and write-back persist, that provide strong reliability guarantees, under two different client failure models. These policies rely on storage applications such as file systems and databases issuing write barriers to persist their data, because these barriers are the only reliable method for storing data durably on storage media. Our evaluation shows that these policies achieve performance close to <b>write-back</b> <b>caching,</b> while providing stronger guarantees than vanilla write-though caching. i...|$|E
40|$|All server storage environments {{depend on}} disk arrays {{to satisfy their}} capacity, reliability, and {{availability}} requirements. In order to manage these storage systems efficiently, {{it is necessary to}} understand the behavior of disk arrays and predict their performance. We develop an analytical model that estimates mean performance measures of disk arrays under a synchronous I/O workload. Synchronous I/O requests are generated by jobs that each block while their request is serviced. Upon I/O service completion, a job may use other computer resources before issuing another I/O request. Our disk array model considers the effect of workload sequentiality, read-ahead caching, <b>write-back</b> <b>caching,</b> and other complex optimizations incorporated into most disk arrays. The model is validated against a mid-range disk-array for a variety of synthetic I/O workloads. The model is computationally simple and scales easily as the number of jobs issuing requests increases, making it potentially useful to performance engineers...|$|E
40|$|This paper investigates issues {{involving}} writes and caches. First, tmdeoffs on writes that miss in the cache are inves-tigated. In particular, whether the missed cache block is fetched on a write miss, whether the missed cache block is allocated in the cache, {{and whether the}} cache line is written before hit or miss is known are considered. Depending on {{the combination of these}} polices chosen, the entire cache miss rate can vary by a factor of two on some applications. The combination of no-fetch-on-write and write-allocate can provide better performance than cache line allocation instructions. Second, tradeoffs between write-through and <b>write-back</b> <b>caching</b> when writes hit in a cache are con-sidered. A mixture of these two altematives, called write caching is proposed. Write caching places a small fully-associative cache behind a write-through cache. A write cache can eliminate almost as much write traffic as a write-back cache. 1...|$|E
40|$|Is it {{possible}} for a distributed filesystem to perform at the same speed as local disk filesystems, at least in important cases? This paper is trying to answer this question for traditional client-server distributed filesystems, not for distributed filesystems exploiting very fast networks and disk striping techniques. We claim the answer is "yes". Systems such as AFS, Sprite, Coda, Arla and DFS showed what can be achieved by eliminating much RPC traffic, while NFS showed how aggressive kernel optimizations can help. Performance analysis of Coda and NFS shows that in order to achieve local disk performance on read traffic the kernel needs more autonomy. In Coda this leads to satisfactory results, both in micro benchmarks and in a http server benchmark. For read/write traffic performance is worse. Many operations lead to synchronous RPCs, but a new <b>write-back</b> <b>caching</b> model can permit a client to proceed without server interference and will eliminate most of the remaining RPC traffic. Co [...] ...|$|E
40|$|File system {{consistency}} frequently {{involves a}} choice between raw performance and integrity guarantees. A few software-based solutions for this problem have appeared and are currently being used on some commercial operating systems; these include log-structured file systems, journaling file systems, and soft updates. In this paper, we propose meta-data snapshotting as a low-cost, scalable, and simple mechanism that provides file system integrity. It allows the safe use of <b>write-back</b> <b>caching</b> by making successive snapshots of the meta-data using copy-onwrite, and atomically committing the snapshot to stable storage without interrupting file system availability. In the presence of system failures, no file system checker or any other operation is necessary to mount the file system, therefore it greatly improves system availability. This paper describes meta-data snapshotting, and its incorporation into a file system available for the Linux and K 42 operating systems. We show that metadata snapshotting has low overhead: for a microbenchmark, and two macrobenchmarks, the measured overhead is of at most 4 %, when compared to a completely asynchronous file system, with no consistency guarantees. Our experiments also show that it induces less overhead then a write-ahead journaling file system, and it scales much better {{when the number of}} clients and file system operations grows. Furthermore...|$|E
40|$|Host-side flash-based caching {{offers a}} {{promising}} new direction for optimizing access to networked storage. Current work has argued for using host-side flash {{primarily as a}} read cache and employing a write-through policy which provides the strictest consistency and durability guarantees. However, write-through requires synchronous updates over the network for every write. For write-mostly or write-intensive workloads, it significantly under-utilizes the high-performance flash cache layer. The write-back policy, on the other hand, better utilizes the cache for workloads with significant write I/O requirements. However, conventional write-back performs out-of-order eviction of data and unacceptably sacrifices data consistency at the network storage. We develop and evaluate two consistent <b>write-back</b> <b>caching</b> policies, ordered and journaled, {{that are designed to}} perform increasingly better than write-through. These policies enable new trade-off points across performance, data consistency, and data staleness dimensions. Using benchmark workloads such as PostMark, TPC-C, Filebench, and YCSB we evaluate the new write policies we propose alongside conventional write-through and write-back. We find that ordered write-back performs better than write-through. Additionally, we find that journaled write-back can trade-off staleness for performance, approaching, and in some cases, exceeding conventional write-back performance. Finally, a variant of journaled write-back that utilizes consistency hints from the application can provide straightforward application-level storage consistency, a stricter form of consistency than the transactional consistency provided by write-through. ...|$|E
40|$|This thesis {{presents}} the Distributed Active Resource Architecture (DARC), a modular, dynamic {{system for the}} flexible construction of distributed storage and I/O infrastructure. DARC is comprised of a component-oriented framework and distributed runtime system, inspired by the Actor Model. Within DARC, components form the fundamental unit through which all functionality is provided and all communication occurs via asynchronous message-passing. This component model extends to the mechanisms through which DARC exposes its services and makes use of external storage resources. Components are modular, lightweight and can be dynamically deployed, configured and composed. The composition of DARC components is based on directed graphs which enable arbitrary topologies of distributed I/O down to per-message granularity. These graphs are mutable through the lifecycle of each message and thus allow distributed, nested compositions. Composition is facilitated {{at a high level}} by an embedded domain-specific language for the generation and modification of graphs. The system is implemented in Java, which provides the code mobility required for dynamic component deployment. Network communication occurs via an open platformindependent protocol, allowing interoperability. Prototype interfaces to the system include a REST-oriented web service and user-level filesystem module. Interfaces to external storage services include filesystems, object stores and cloud storage. Example functional component implementations include name-and content-based data distribution, <b>write-back</b> <b>caching,</b> parallel transfers, optimistic replication and aggregated I/O. The implementation and evaluation described in this thesis show that the overhead of graph processing is not an impediment to high performance and illustrate the feasibility and flexibility of the design. The contribution of this work is widely applicable to a range of systems and especially dynamic cloud computing infrastructure. Thesis (Ph. D.) [...] University of Adelaide, School of Computer Science, 201...|$|E

