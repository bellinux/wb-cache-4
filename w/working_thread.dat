8|54|Public
5000|$|Background {{processes}} are independent processes that primarily perform time-consuming disk operations at specified intervals {{or at the}} request of a <b>working</b> <b>thread</b> or another background process.|$|E
5000|$|A <b>working</b> <b>thread</b> {{communicates}} {{directly with}} a single client process. It receives and handles messages from a client process and returns the result. It handles most DBMS jobs such as SQL parsing and optimization. Even after a <b>working</b> <b>thread</b> is disconnected from a client, it does not disappear. It is created when Tibero starts and is removed when Tibero terminates. This improves system performance as threads {{do not need to}} be created or removed even if connections to clients need to be made frequently.|$|E
5000|$|Control thread Creates as many working threads as {{specified}} in the initialization parameter when Tibero is started, allocates new client connection requests to an idle <b>working</b> <b>thread,</b> and Checks signal processing.|$|E
5000|$|One {{working process}} {{consists}} of one control <b>thread</b> and multiple <b>working</b> <b>threads.</b> A <b>working</b> process contains one control <b>thread</b> and ten <b>working</b> <b>threads</b> by default. default.The number of <b>working</b> <b>threads</b> per {{process can be}} set using the initialization parameter, and after Tibero begins, this number cannot be changed.|$|R
5000|$|This process writes changed data {{blocks to}} disk. The written data blocks are usually read {{directly}} by <b>working</b> <b>threads.</b>|$|R
40|$|The {{main purpose}} of this paper is to present the impact of pCoR thread level mes-sage passing {{facilities}} on applications running in a Myrinet cluster. To exploit Myrinet technology we use the GM library, which provides a lim-ited number of ports as abstractions to name communication end-points. pCoR communication layer multiplexes GM ports by using a dispatcher thread to handle messages to/from a large number of communication entities (<b>working</b> <b>threads).</b> Our approach combines polling operations executed by multiple threads to avoid unnecessary context switching; a simple mechanism is used to engage <b>working</b> <b>threads</b> into the polling scheme. We aim to reduce the total number of polling operations required to hold message passing system performance. ...|$|R
5000|$|Listener {{receives}} {{requests for}} new connections from clients and assigns {{them to an}} available <b>working</b> <b>thread.</b> Listener plays an intermediate role between clients and working threads using an independent executable file, tblistener.|$|E
50|$|Tambour lace {{refers to}} a family of lace made by {{stretching}} a fine net over a frame (the eponymous Tambour, from the French for drum) and creating a chain stitch using a fine hook to reach through the net and draw the <b>working</b> <b>thread</b> through the net.|$|E
5000|$|Nålebinding (Danish: {{literally}} [...] "binding with a needle" [...] or [...] "needle-binding", also naalbinding, nålbinding, nålbindning or naalebinding) is {{a fabric}} creation technique predating both knitting and crochet. Also known in English as [...] "knotless netting," [...] "knotless knitting," [...] or [...] "single needle knitting," [...] {{the technique is}} distinct from crochet in that it involves passing {{the full length of}} the <b>working</b> <b>thread</b> through each loop, unlike crochet where the work is formed only of loops, never involving the free end. It also differs from knitting in that lengths must be pieced together during the process of nålebinding, rather than a continuous strand of yarn that can easily be pulled out. Archaeological specimens of fabric made by nålebinding can be difficult to distinguish from knitted fabric.|$|E
50|$|Example: A {{software}} application executed on a Hexa-core processor creates three Unix processes for fulfilling the user requirement. Each {{of these three}} processes creates two threads, enumerating a total of 6 <b>working</b> <b>threads.</b> Computation is distributed evenly on the 6 independent threads. If no wait for resources is involved, total CPU time {{is expected to be}} six times the elapsed real time.|$|R
40|$|This paper emphasises on load {{balancing}} issues associated 0 with hybrid parallelisation of tiled algorithms onto SMP clusters. The intrinsic load imbalance in hybrid parallelisation {{derives from the}} fact that message passing libraries often provide limited multi-threading support, thus allowing only the master thread to perform inter-node message passing communication. In order to mitigate this effect, we propose a generic method for the application of {{load balancing}} on the coarse-grain hybrid model for the appropriate load distribution among the <b>working</b> <b>threads.</b> We investigate both static and dynamic load balancing, and experimentally evaluate three balancing variations against kernel benchmarks. Copyright © 2009, Inderscience Publishers...|$|R
30|$|The {{zygomatic}} bone was adequately exposed. Implants from Noris Medical Ltd. (Nesher, Israel) were chosen. The <b>working,</b> <b>threaded</b> {{part of the}} implant is 13  mm long, while the remaining, fully smooth shaft has 4 -mm diameter and variable length. In all, length ranges from 35 to 57.5  mm. Implant drilling was performed using both straight and angled handpieces. The fixtures were placed at 35  rpm for the 2 / 3 of the apical and manually for the most coronal 1 / 3 working part. Palatal-alveolar repair was attained with soft tissue, local flaps: these were also wrapped around the implants. In order to obtain a durable watertight seal between oral and nasal/antral cavities, implant uncovering and loading were planned to be deferred by 3  months.|$|R
40|$|Abstract—Power {{dissipation}} {{is one of}} {{the most}} imminent limitation factors influencing the development of High Per-formance Computing (HPC). Toward power-efficient HPC on CPU-GPU hybrid platform, we are investigating software methodologies to achieve optimized power utilization by al-gorithm design and programming technique. In this paper we discuss power measurements of GPU, propose a method of automatic extraction of power data of CUDA kernels from long measurement sequence, and execute an exactitude and effective power analysis on CUDA kernels. By using the proposed method above, we measured a sample kernel that performs single precision floating point additions on GeForce 8800 GTS. Our results suggest that the power consumption by a non-working thread in underoccupied half-warp is 71 % of the power consumed by a <b>working</b> <b>thread.</b> Keywords-GPGPU, CUDA, power dissipation, power model-ing. I...|$|E
40|$|This paper {{presents}} a multi-threaded socket server allowing access to shared hash tables. It is implemented using Tcl 8. 1 multi-threading capabilities and runs multiple Tcl interpreters to service client requests. The application is {{designed as a}} pre-threaded server which allows a single <b>working</b> <b>thread</b> to handle many requests. The central shared data object is a hash table with structured values which allows access by all threads. Synchronization {{is based on a}} reader/writer lock implementation using the synchronization primitives available in Tcl, i. e., mutexes and condition variables. The application achieves insert rates that are significantly higher than what current commercial database management systems achieve. The usage of third-level language programming in C and application-specific scripting in Tcl allows a design based on a light-weight, robust kernel {{on the one hand and}} easily modifiable application-domain code on the other. The experiences with thread-safety and other threading features in Tcl 8. 1 have been largely positive in this real-world application...|$|E
40|$|Abstract This paper {{emphasizes}} on load balancing issues associ-ated with hybrid programming {{models for the}} parallelization of fully permutable nested loops onto SMP clusters. Hybrid parallel programming models usually suffer from intrinsic load imbalance between threads, mainly becausemost existing message passing libraries generally provide limited multi-threading support, allowing only the masterthread to perform inter-node message passing communication. In order to mitigate this effect, we propose a genericmethod {{for the application of}} static load balancing on the coarse-grain hybrid model for the appropriate distributionof the computational load to the <b>working</b> <b>threads.</b> We experimentally evaluate the efficiency of the proposed schemeagainst a micro-kernel benchmark, and demonstrate the potential of such load balancing schemes for the extraction ofmaximum performance out of hybrid parallel programs. ...|$|R
40|$|This paper {{emphasizes}} on load balancing {{issues associated with}} hybrid programming models for the parallelization of fully permutable nested loops onto SMP clusters. Hybrid parallel programming models usually suffer from intrinsic load imbalance between threads, mainly because most existing message passing libraries generally provide limited multi-threading support, allowing only the master thread to perform inter-node message passing communication. In order to mitigate this effect, we propose a generic method {{for the application of}} static load balancing on the coarse-grain hybrid model for the appropriate distribution of the computational load to the <b>working</b> <b>threads.</b> We experimentally evaluate the efficiency of the proposed scheme against a micro-kernel benchmark, and demonstrate the potential of such load balancing schemes for the extraction of maximum performance out of hybrid parallel programs. ...|$|R
50|$|Flat or sew-through buttons have holes {{through which}} thread is sewn {{to attach the}} button. Flat buttons may be {{attached}} by sewing machine rather than by hand, and may be used with heavy fabrics by <b>working</b> a <b>thread</b> shank to extend {{the height of the}} button above the fabric.|$|R
40|$|The {{reachability}} {{analysis of}} recursive programs that communicate asynchronously over reliable Fifo channels calls for restrictions to ensure decidability. We extend here a model proposed by La Torre, Madhusudan and Parlato [LMP 08], based on communicating pushdown systems that can dequeue with empty stack only. Our extension adds the dual modality, which allows to dequeue with non-empty stack, and thus models interrupts for <b>working</b> <b>threads.</b> We study (possibly cyclic) network architectures under a semantic assumption on communication that ensures the decidability of reachability for finite state systems. Subsequently, we determine precisely how pushdowns {{can be added}} to this setting while preserving the decidability; in the positive case we obtain exponential time as the exact complexity bound of reachability. A second result is a generalization of the doubly exponential time algorithm of [LMP 08] for bounded context analysis to our symmetric queueing policy. We provide here a direct and simpler algorithm...|$|R
5000|$|The {{round turn}} bowline {{is made by}} the {{addition}} of an extra turn {{in the formation of the}} [...] "rabbit hole" [...] before the <b>working</b> end is <b>threaded</b> through.|$|R
40|$|Histogramming is a {{technique}} by which input datasets are mined to extract features and patterns. Histograms have wide range of uses in computer vision, machine learning, database processing, quality control for manufacturing, and many applications benefit from advance knowledge about the distribution of data. Computing a histogram is, essentially, the antithesis of parallel processing. Without the use of slow atomic operations or serial execution when contributing data to a histogram bin in an input-driven method, there would likely be inaccuracies in the resulting output. An output-driven method would {{eliminate the need for}} atomic operations but would amplify read bandwidth requirements, reduce overall throughput, and result in a zero or negative gain in performance. We introduce a method to pack multiple bins into a memory word with the goal of better utilizing GPU resources. This method improves GPU occupancy relative to earlier histogram kernel implementations, increases the number of <b>working</b> <b>threads</b> to better hide the latency of atomic operations and collisions while maintainin...|$|R
40|$|The {{number of}} cores {{in a single}} chip {{multiprocessor}} {{is expected to grow}} in coming years. Likewise, aggregate on-chip cache capacity is increasing fast and its effective utilization is becoming ever more important. Furthermore, available cores are expected to be underutilized due to the power wall and highly heterogeneous future workloads. This trend makes existing L 2 cache management techniques less effective for two problems: increased capacity interference between working cores and longer L 2 access latency. We propose a novel scalable cache management framework called CloudCache that creates dynamically expanding and shrinking L 2 caches for <b>working</b> <b>threads</b> with fine-grained hardware monitoring and control. The key architectural components of CloudCache are L 2 cache chaining, inter- and intra-bank cache partitioning, and a performance-optimized coherence protocol. Our extensive experimental evaluation demonstrates that CloudCache significantly improves performance {{of a wide range of}} workloads when all or a subset of cores are occupied. 1...|$|R
40|$|International audienceThe {{reachability}} {{analysis of}} recursive programs that commu- nicate asynchronously over reliable Fifo channels calls for restrictions to ensure decidability. We extend here a model proposed by [La Torre et al. 2008], based on communicating pushdown sys- tems that can dequeue with empty stack only. Our extension adds the dual modality, which allows to dequeue with non-empty stack, and thus models interrupts for <b>working</b> <b>threads.</b> We study (possibly cyclic) net- work architectures under a semantic assumption on communication that ensures the decidability of reachability for finite state systems. Subse- quently, we determine precisely how pushdowns {{can be added}} to this setting while preserving the decidability; in the positive case we obtain exponential time as the exact complexity bound of reachability. A sec- ond result is a generalization of the doubly exponential time algorithm of [La Torre et al. 2008] for bounded context analysis to our symmetric queueing policy. We provide here a direct and simpler algorithm...|$|R
50|$|Crab {{locomotives}} were {{equipped with}} a winch for pulling cars out of the un-powered tracks. This approach allowed use of temporary track that was too light to carry {{the weight of the}} a cable-reel or battery locomotive. The disadvantage of a crab locomotive was that someone had to pull the haulage cable from the winch to the <b>working</b> face, <b>threading</b> it over pulleys at any sharp turns.|$|R
5000|$|... process.h is a C header file which {{contains}} function declarations and macros used in <b>working</b> with <b>threads</b> and processes. Most C compilers that target DOS, Windows 3.1x, Win32, OS/2, Novell NetWare or DOS extenders supply this header {{and the library}} functions in their C library. Neither the header file nor most of the functions are defined by either the ANSI/ISO C standard or by POSIX.|$|R
50|$|Similar to C#, Java {{has since}} version 5 {{a higher level}} {{replacement}} for <b>working</b> with <b>threads</b> directly. Executors are capable of running asynchronous tasks and typically manage a pool of threads. All threads of the internal pool will be reused under the hood for revenant tasks, so we can run as many concurrent tasks as we want throughout the life-cycle of our application with a single executor service.|$|R
40|$|SEcure Neighbor Discovery (SEND) is {{proposed}} to counter IPv 6 Neighbor Discovery Protocol (NDP) security threats. However, SEND is compute-intensive. Fulfilling Hash 2 condition in Cryptographically Generated Addresses (CGA) {{is the main}} heavy part of SEND. Unfortunately, CGA computation cannot see significant speed improvement when it runs on multicore machine because CGA generation algorithm is sequential. In this paper, we propose a multicore-based high performance SEND implementation for Windows families to speed up SEND computations. The proposed approach automatically detects the number of processors available on a machine and creates equivalent number of <b>working</b> <b>threads</b> to compute Hash 2 condition. The parallelization mechanism is implemented to assign CGA computation to all the cores. When one thread satisfies CGA Hash 2 condition, the others stop. With the parallel approach, the speedup time has been increased extremely by {{increasing the number of}} cores in the computing device. Besides the parallelization, we extend SEND implementation to generate the key pair for CGA algorithm onthe-fly to enhance the security and to protect the privacy...|$|R
50|$|Similar to {{the double}} bowline, the water bowline {{is made by}} forming a clove hitch before the <b>working</b> end is <b>threaded</b> through. It {{is said to be}} {{stronger}} and also more resistant to jamming than the other variations, especially when wet.|$|R
40|$|Garbage {{collection}} (GC) is a {{well known}} approach to simplify soft-ware development. Recently, it has started to gain acceptance also in the real-time community. Several hard real-time GC algorithms have been proposed for uniprocessors. However, the growing pop-ularity of multi-processors for real-time systems entails that algo-rithms and techniques have to be developed that allow hard real-time GC on multi-processors as well. We propose a novel root cache, which aggregates information of the processor-local root sets in multi-processor systems. It allows that the root scanning phase of the garbage collector is decoupled from the root scanning phase of <b>working</b> <b>threads.</b> Thread-local root scanning can be scheduled flexibly, without impeding the garbage collector. Additionally, the new cache lowers both the blocking time and the memory bandwidth consumption due to the root scan-ning phase of GC. The proposed solution has been implemented for evaluation in a chip multi-processor system based on the Java Optimized Proces-sor. We show how bounds on the garbage collector period can be extended {{to take into account}} the root cache. We also present ex-perimental results, which highlight the advantages and limits of the proposed approach...|$|R
40|$|Conventional {{workload}} distribution {{schemes for}} software distributed shared mem-ory (DSM) systems simply distribute the program threads {{in accordance with}} the CPU power of the individual processors or the data-sharing characteristics of the application. Although these schemes aim to minimize the program execution time by reducing the computation and communication costs, memory access costs also have a major influence on the overall program performance. If a processor has insufficient physical memory space to cache all of the data required by its local <b>working</b> <b>threads,</b> it must perform a se-ries of page replacements if it is to complete its thread executions. Although these page replacements enable the threads to complete their tasks, thread execution is inevitably delayed by the latency of the page swapping operations. Consequently, the current study proposes a novel workload distribution scheme for DSM systems which considers not only the CPU power and data-sharing characteristics, but also the physical memory ca-pabilities of the individual processors. The present results confirm the importance of considering memory resources when establishing an appropriate workload distribution for DSM systems and indicate that the proposed scheme is more effective than schemes which consider only CPU resources or memory resources, respectively...|$|R
40|$|The 2008 Wenchuan {{earthquake}} {{is one of}} {{the largest}} and deadliest events of the last century, both in terms of victims and seismically-triggered geo-environmental hazards. The combined effects resulting from the seismic activity of 2008 led to major damages in several domains of the local society, including the built environment and the social component, highlighting the extensive gap related to resilience measures aimed at both prevention and recovery in case of a disruption. Thus, it is imperative for the development of a multi-objective and comprehensive formulation for resilience combining holistically all the diverse facets of a local community. To this regard, this paper presents a wide-spectrum analysis and preliminary findings, being structured in <b>working</b> <b>threads</b> specifically addressed in the context of an ongoing project (REACH). The focus is on the built environment damage analysis following the field work which took place in December 2016 in the Wenchuan territory, involving 3 D laser scanning activity of 7 high-risk areas and the related audit and preliminary structural analysis of the damage. As a result, a set of external and internal factors have been identified as possible key causes of the surveyed damages. Of these, external factors mainly relate to environmental and hazardous properties, whereas internal indicators involve structural features as well as building regulations...|$|R
40|$|Trace-level {{speculative}} multithreaded processors exploit trace-level speculation {{by means}} of two <b>threads</b> <b>working</b> cooperatively. One <b>thread,</b> called the speculative thread, executes instructions ahead of the other by speculating on the result of several traces. The other thread executes speculated traces and verifies the speculation made by the first thread. In this paper, we propose a static program analysis for identifying candidate traces to be speculated. This approach identifies large regions of code whose live-output values may be successfully predicted. We present several heuristics {{to determine the best}} opportunities for dynamic speculation, based on compiler analysis and program profiling information. Simulation results show that the proposed trace recognition techniques achieve on average a speed-up close to 38 % for a collection of SPEC 2000 benchmarks. Peer ReviewedPostprint (published version...|$|R
40|$|Multithreaded {{applications}} are faster {{as they can}} effectively exploit the capabilities of modern computer systems. The purpose of this thesis is to present the efficiency of multithreaded applications {{with the use of}} a simple Windows console application which copies files from one directory to another using different thread pools. In this thesis we describe the ways of developing multithreaded applications, some limitations which should be taken into account when developing multithreaded applications, and the principles of protecting the access to shared data from different threads simultaneously. A console application for the copying of files was developed in Visual Studio 2010 in C++ programming language. Based on the input parameter, the application determines the number of <b>working</b> <b>threads</b> in a thread pool and then assigns a specific task to each thread. The efficiency of the application was tested with various test cases and the results are suitably presented. We learned that multithreaded {{applications are}} more efficient for the copying of larger files although the hard disk presents a certain limitation. A higher number of worker threads does not increase the efficiency of the application when copying smaller files which is a faster operation. The time needed for creating new worker threads and for checking whether any threads are free decreases the efficiency of the application if the number of worker threads is too high...|$|R
30|$|In the {{multi-thread}} approach, {{a number}} of threads in a process are <b>working</b> simultaneously. <b>Thread</b> is a small action unit which is generated in a process. Thread in mobile device is not directly operated in a running application, but it is used for background process to perform data loading, network communication, and file I/O in the application. The most recent mobile devices contain multiple CPU cores and their calculation speed is also being rapidly upgraded. However, most main logic parts of mobile applications don’t efficiently utilize these multiple CPU cores {{and they have been}} implemented based on performing in one flow of CPU.|$|R
40|$|Abstract. Trace-Level Speculative Multithreaded Processors exploit trace-level {{speculation}} {{by means}} of two <b>threads</b> <b>working</b> cooperatively. One <b>thread,</b> called the speculative thread, executes instructions ahead of the other by speculating on the result of several traces. The other thread executes speculated traces and verifies the speculation made by the first thread. Speculated traces are validated by verifying their live-output values. Every time a trace misspeculation is detected, a thread synchronization is fired. This recovery action involves flushing the pipeline and reverting to a safe point in a program, which results in some performance penalties. This paper proposes a new thread synchronization scheme based on the observation {{that a significant number}} of instructions whose control and data are independent of the mispredicted instruction. This scheme significantly increases the performance potential of the architecture at less cost. Our experimental results show that the mechanism cuts the number of executed instructions by 8 % and achieves on average speed-up of almost 9 % for a collection of SPEC 2000 benchmarks. 1...|$|R
50|$|Anderson is {{currently}} <b>working</b> on Phantom <b>Thread,</b> {{a film about}} the London fashion industry in the 1950s. It will star Daniel Day-Lewis in his first acting role since Lincoln in 2012, Lesley Manville, and Richard Graham. In September 2016, the U.S. distribution rights were acquired by Focus Features, with Universal handling international distribution. Principal photography began in January 2017, with an intended release in late 2017.|$|R
50|$|Linen fabric {{has been}} used for table {{coverings}}, bed coverings and clothing for centuries. The significant cost of linen derives not only from the difficulty of <b>working</b> with the <b>thread,</b> but also because the flax plant itself requires a great deal of attention. In addition flax thread is not elastic, and therefore it is difficult to weave without breaking threads. Thus linen is considerably more expensive to manufacture than cotton.|$|R
