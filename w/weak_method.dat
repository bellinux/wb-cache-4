15|578|Public
5000|$|Throughout its history, Soar {{has been}} used to {{implement}} a wide variety of classic AI puzzles and games, such as Tower of Hanoi, Water Jug, Tic Tac Toe, Eight Puzzle, Missionaries and Cannibals, and variations of the Blocks World. One of the initial achievements of Soar was showing that many different weak methods would naturally arise from the task knowledge that was encoded in it, a property called, the Universal <b>Weak</b> <b>Method.</b>|$|E
50|$|LAN Manager {{authentication}} uses {{a particularly}} <b>weak</b> <b>method</b> of hashing a user's password {{known as the}} LM hash algorithm, stemming from the mid 1980s when floppy viruses were the major concern as opposed to potentially high-frequency attacks with feedback over a (high-bandwidth) network. This makes such hashes crackable {{in a matter of}} seconds using rainbow tables, or in few hours using brute force. Its use in Windows NT was replaced by NTLM, of which older versions are still vulnerable to rainbow tables, but less vulnerable to brute force attacks. NTLM is used for logon with local accounts except on domain controllers since Windows Vista and later versions no longer maintain the LM hash by default. Kerberos is used in Active Directory Environments.|$|E
50|$|The second {{movement}} is written with the clarinet {{playing in the}} clarino register and chalumeau with minimal leaps in the exposition, but later Brahms turns to his favored leaps and arpeggiation in order to transition to a new theme. Many consider this a <b>weak</b> <b>method</b> of composition; however, {{it has also been}} noted that Brahms does this to accentuate the capabilities of the clarinet. Similar to the first movement, the cello and clarinet have intermingled parts, with the piano mimicking this interplay throughout. Many criticize this style of composition because it lacks the certain depth that has been achieved with other trio groupings; however, supporters of Brahms praise this method of writing because the clarinet and cello voices interact so well, giving the piece a certain texture that is not achieved by any other type of chamber ensemble. Brahms uses interesting harmonic and rhythmic modulations {{in the latter half of}} this movement, and, combined with the already-askew format of the movement, creates a unique work and places a certain dignity on the Piano, Clarinet, and Cello trio that was a stark and refreshing interest for this time period.|$|E
40|$|Recently, we {{proposed}} a weak Galerkin finite element method for the Laplace eigenvalue problem. In this paper, we present two-grid and two-space skills to accelerate the <b>weak</b> Galerkin <b>method.</b> By choosing parameters properly, the two-grid and two-space <b>weak</b> Galerkin <b>method</b> not only doubles the convergence rate, but also maintains the asymptotic lower bounds property of the <b>weak</b> Galerkin <b>method.</b> Some numerical examples are provided to validate our theoretical analysis. Comment: 4 figure 2, 20 page...|$|R
40|$|Logarithmic {{amplifier}} is {{used for}} reducing the dynamic range of the input signal. Logarithmic amplifier is implemented using two different techniques. One of the methods is a parallel summation based method {{and the other one}} is a <b>weak</b> inversion based <b>method.</b> In parallel summation based method the transistors are maintained in saturation whereas in <b>weak</b> inversion based <b>method</b> the transistors are maintained in <b>weak</b> inversion. Both <b>methods</b> are simulated in 50 nm CMOS technology using HSPICE. Considering power, area and dynamic range <b>weak</b> inversion <b>method</b> is more efficient compared to parallel summation method...|$|R
40|$|An {{empirical}} work {{is described}} which compares the optimization levels {{produced by a}} group of economic agents versus those of a similar group of economic agents which additionally employ a genetic algorithm (GA) to attain a higher level of optimization. The problem domain is multimodal. It incorporates multiple hard and soft constraints and dynamical behaviors. It also has areas of infeasibility and non-linear behaviors. The simulated model environment provides several types of sensors, actuators and opportunities for inter-agent resource mediation. Evidence is offered to support the theory that multiple <b>weak</b> <b>methods</b> operating in concert, on a shared problem, can produce better results than the individual <b>weak</b> <b>methods</b> acting alone. The problem area is resistant to the use of strong methods. Introduction Many real-world domains require the ability to deal with environments and problems which include multimodal features, non-linear functions, environmental interactions, fe [...] ...|$|R
40|$|The weak methods occur pervasively in Al {{systems and}} may form the basic methods for all {{intelligent}} systems. The {{purpose of this}} paper is to characterize the weak methods and to explain how and why they arise in intelligent systems. We propose an organization, called a universal <b>weak</b> <b>method,</b> that provides functionality of all the weak methods. A universal <b>weak</b> <b>method</b> is an organizational scheme for knowledge that produces the appropriate search behavior given the available task-domain knowledge. We present a problem solving architecture in which we realize a universal <b>weak</b> <b>method.</b> We also demonstrate the universal <b>weak</b> <b>method</b> with a variety of weak methods on a set of tasks. 1 1...|$|E
40|$|There are two {{important}} classes of numerical methods for stochastic differential equations (SDEs) : strong methods and weak methods. Strong methods construct numerical approximations to trajectories of the SDEs directly, using the Brownian motions driving the SDEs. Weak methods compute a numerical trajectory using {{a sequence of}} random variables independent of the Brownian motions. The convergence of weak methods is usually expressed indirectly {{in terms of the}} convergence of expected values of test functions of the trajectories. Here we present an alternative formulation of convergence for weak methods in terms of the well-known Prokhorov metric on spaces of random variables. For a general class of weak methods, we establish bounds on the rates of convergence in terms of the Prokhorov metric. In doing so, we revisit the original proofs of convergence for weak methods and show explicitly how the bounds on the error depend on the smoothness of the test functions. As an application of our result, we use the Strassen–Dudley theorem to show that the true solution to the system of SDEs and the approximation from the <b>weak</b> <b>method</b> can be embedded in the same probability space {{in such a way that}} values generated by the <b>weak</b> <b>method</b> converge there in a strong sense. We conclude with a review of the existing results for pathwise convergence of weak methods and the corresponding strong results available under embedding. stochastic differential equations, convergence in distribution, weak convergence, Prokhorov metric, Strassen–Dudley Theore...|$|E
40|$|We {{propose a}} <b>weak</b> <b>method</b> for {{handling}} the fluidstructure interface in finite element fluidstructure interaction based on Nitsche's method [Abh. Math. Univ. Hamburg 36 (1971) 9]. We assume transient incompressible Newtonian flow and, for the structure, undamped linear elasticity. For the time-discretization, {{we use the}} time-continuous (energy conserving) Galerkin method for the structure, and for the fluid we employ the time-discontinuous Galerkin method. This means that the velocity becomes piecewise constant on each timestep for the fluid, matching the time-derivative of the displacements in the solid which is also piecewise constant over a time step. We formulate the method and report some numerical examples using spacetime oriented elements for the fluid in order to mimic Lagrangian or ALE-type simulations...|$|E
40|$|A {{variant of}} the Tau <b>method,</b> called the <b>weak</b> Tau <b>method,</b> is {{developed}} {{on the basis of}} the weak form of the PDE for use in least-squares parameter estimation; also presented is a suitable abstract convergence framework. The emphasis is on the theoretical framework that allows treatment of the <b>weak</b> Tau <b>method</b> when it is applied to a wide class of inverse problems, including those for diffusion-advection equations, the Fokker-Planck model for population dynamics, and damped beam equations. Extensive numerical testing of the <b>weak</b> Tau <b>method</b> has demonstrated that it compares quite favorably with existing methods...|$|R
40|$|We {{propose a}} {{modification}} of the <b>weak</b> Galerkin <b>methods</b> and show its equivalence to {{a new version of}} virtual element methods. We also show the original <b>weak</b> Galerkin <b>method</b> is equivalent to the non-conforming virtual element method. As a consequence, ideas and techniques used for one method can be transferred to another. The key of the connection is the degree of freedoms...|$|R
40|$|For a {{stationary}} nonlinear Boltzmann equation in a slab {{with a particular}} trun-cation in the collision operator, the Milne problem for the boundary layer together with a weak type of hydrodynamic behavior {{in the interior of}} the slab are studied by nonperturbative methods in the small-mean-free-path limit. KEY WORDS: Boltzmann asymptotics; boundary layer; hydrodynamic limit; Maxwellian limit; Milne problem; stationary gas kinetics; <b>weak</b> <b>methods...</b>|$|R
40|$|Following Tesauro's work on TD-Gammon, {{we used a}} 4000 {{parameter}} feed-forward {{neural network}} to develop a competitive backgammon evaluation function. Play proceeds by a roll of the dice, application of the network to all legal moves, and choosing the move with the highest evaluation. However, no back-propagation, reinforcement or temporal difference learning methods were employed. Instead we apply simple hill-climbing in a relative fitness environment. We start with an initial champion of all zero weights and proceed simply by playing the current champion network against a slightly mutated challenger and changing weights if the challenger wins. Surprisingly, this worked rather well. We investigate how the peculiar dynamics of this domain enabled a previously discarded <b>weak</b> <b>method</b> to succeed, by preventing suboptimal equilibria in a "meta-game" of self-learning. Keywords: coevolution, backgammon, reinforcement, temporal difference learning, self-learning Running Head: CO-EVOLUTIONARY LEA [...] ...|$|E
40|$|USING NEURALNETWORKSAND GENETIC ALGORITHMS AS HEURISTICS FOR NP-COMPLETE PROBLEMS William M. Spears, M. S. George Mason University, 1989 Thesis Director: Dr. Kenneth A. De Jong Paradigms {{for using}} neural {{networks}} (NNs) and genetic algorithms (GAs) to heuristically solve boolean satisfiability (SAT) problems are presented. Results are presented for two-peak and false-peak SAT problems. Since SAT is NPComplete, any other NP-Complete {{problem can be}} transformed into an equivalent SAT problem in polynomial time, and solved via either paradigm. This technique is illustrated for hamiltonian circuit (HC) problems. INTRODUCTION One approach to discussing and comparing AI problem solving methods is to categorize them using the terms strong or weak. Generally, a <b>weak</b> <b>method</b> {{is one that has}} the property of wide applicability but, because it makes few assumptions about the problem domain, can suffer from combinatorially explosive solution costs when scaling to larger problems. State space search [...] ...|$|E
40|$|Personal {{information}} and organizational information {{need to be}} protected, which requires that only authorized users {{gain access to the}} information. The most commonly used method for authenticating users to determine access to such information is through the use of username-password combinations. However, this is a <b>weak</b> <b>method</b> of authentication because users tend to generate passwords that are easy to remember but also easy to crack. Proactive password checking, for which passwords must satisfy certain criteria, is one method for improving the security of user-generated passwords. The present study evaluated the time and number of attempts needed to generate unique passwords satisfying different restrictions for multiple accounts, as well as the login time and accuracy for recalling those passwords. Imposing password restrictions alone did not necessarily lead to more secure passwords. However, the use of a technique for which the first letter of each word of a sentence was used, coupled with a requirement to insert a special character and digit, yielded secure passwords that were more memorable...|$|E
40|$|The {{state of}} the art in {{collision}} prevention for manipulators with revolute joints, showing that it is a particularly computationally hard problem, is discussed. Based on the analogy with other hard or undecidable problems such as theorem proving, an extensible multi-resolution architecture for path planning, based on a collection of <b>weak</b> <b>methods</b> is proposed. Finally, the role that sensors can play for an on-line use of sensor data is examined...|$|R
40|$|Theories of {{transformational}} {{and charismatic}} leadership {{provide important insights}} {{about the nature of}} effective leadership, but most of the theories have weaknesses in the conceptualization and measurement of leadership processes. The limitations include use of simplistic two-factor models, omission of relevant behaviours, focus on dyadic processes, assumption of heroic leadership, and overreliance on <b>weak</b> <b>methods.</b> I discuss these weaknesses and present results from a study on leader behaviour dimensions to clarify some of my concerns...|$|R
40|$|Using a {{recently}} developed <b>method,</b> <b>weak</b> convergence <b>method,</b> {{in dealing with}} the large deviation principle, we demonstrate the large deviation principle property for mild solutions of stochastic evolution equations with monotone nonlinearity and multiplicative noise. An Itô-type inequality is a main tool in the proofs. We also give two examples to illustrate the applications of the theorems...|$|R
40|$|An {{enzyme-linked}} immunosorbent assay {{specific to}} outer membrane protein P 6 (P 6 -ELISA) was applied for detecting Haemophilus influenzae in middle ear fluids (MEFs) from acute otitis media (AOM) patients and in nasopharyngeal secretions (NPSs) from acute rhinosinusitis patients. P 6 -ELISA had a sensitivity of 83. 3 % for MEFs and 71. 5 % for NPSs and a specificity of 85. 6 % for MEFs and 92. 5 % for NPSs, respectively. Real-time PCR exhibited {{significant differences in the}} number of ompP 1 gene copies among samples determined by P 6 -ELISA to be positive and negative for H. influenzae. However, because the P 6 -ELISA test has the reactivity in Haemophilus species include two commensals H. haemolyticus and H. parainfluenzae, it is thus a <b>weak</b> <b>method</b> in order to detect only NTHi correctly. Consequently, diagnosis using the P 6 -ELISA should be based on an overall evaluation, including the results of other related examinations and clinical symptoms to prevent misleading conclusions in clinical setting...|$|E
40|$|Merapi {{eruption}} in 2010 causing major damage {{impact on}} that region. Post-disaster damage assessment {{that has been}} done by the government have not been supported with a good spatial data so that validation is relatively <b>weak.</b> <b>Method</b> of post-disaster damage assessment, particularly assessment of building damage using geotagged photos, remote sensing and GIS is expected to improve the method of damage assessment by the government of Indonesia. Geojot Applications for Android Smartphone/Tablet allows the assessment of building damage {{to be included in the}} photo attribute. Interpretation of satellite imagery of building damage is done by using three indications: building visibility, building collapse, and building roof. Geotagged photograph can complement the needs of building damage assessment from satellite images because it can describe the structural and non-structural damage to buildings clearly. Geotagged photograph with GPS Lock-Off mode requiring information on the direction and distance of the object being photographed. Geotagged photograph with the QR code is the most profitable because the identity of the building is already known and can be matched with an existing database...|$|E
40|$|This article {{explores the}} idea of {{learning}} efficient strategies for solving problems by searching for macro-operators. A macro-operator, or macro for short, is simply a sequence of operators chosen from the primitive operators of a problem. The technique is particularly useful for problems with non-serializable subgoals, such as Rubik's Cube, for which other weak methods fail. Both a problem-solving program and a learning program are described in detail. The performance of these programs is analyzed {{in terms of the}} number of macros required to solve all problem instances, the length of the resulting solutions (expressed as the number of primitive moves), and the amount of time necessary to learn the macros. In addition, a theory of why the method works, and a characterization of the range of problems for which it is useful are presented. The theory introduces a new type of problem structure called operator decomposability. Finally, it is concluded that the macro technique is a new kind of <b>weak</b> <b>method,</b> a method for learning as opposed to problem solving...|$|E
40|$|Abstract. A general {{procedure}} to construct <b>weak</b> <b>methods</b> for the numerical solution of stochas-tic differential systems is presented. As in the deterministic case, the procedure consists of comparing the stochastic {{expansion of the}} approximation with the corresponding Taylor scheme. In this way the authors obtain the order conditions that a stochastic Runge–Kutta method must satisfy to have weak order two. Explicit examples of generalizations of the classical family of second order two-stage explicit Runge–Kutta methods are shown. Also numerical examples are presented...|$|R
40|$|Inspired {{by recent}} {{advances}} {{in the theory of}} modified differential equations, we propose a new methodology for constructing numerical integrators with high weak order for the time integration of stochastic differential equations. This approach is illustrated with the constructions of new high order <b>weak</b> <b>methods,</b> in particular, implicit integrators well suited for stiff stochastic problems, and integrators that exactly conserve all quadratic first integrals of a stochastic dynamical system. Numerical examples confirm the theoretical results and show the versatility of the methodology...|$|R
40|$|AbstractWe {{propose a}} {{mathematical}} limit of L 1 -stable <b>weak</b> asymptotic <b>methods.</b> A family of L 1 -stable approximate solutions {{is transformed into}} a normal family of holomorphic functions defined in a complex domain having the real space on its boundary. This provides a holomorphic function which is the same mathematical object as the solutions from explicit calculations. The weak limit of the approximate solutions from <b>weak</b> asymptotic <b>methods</b> in the space of bounded Radon measures is recovered as a boundary value of this holomorphic function...|$|R
40|$|The work {{covers the}} {{mathematical}} {{models of the}} physical systems described by the differential equations expressing the conservation properties. The aim is to reveal and analyse the basic mathematical structures connected {{with the problems of}} approximated method convergence; to prove the theorems about global solvability of the Cauchy problem for the quasi-linear and semi-linear systems having application in the mathematical physics. The convergence of regular approximated methods to the functional solution of conservation laws at condition of the weak approximation and <b>weak</b> <b>method</b> stability has been substantiated. The possible classes of correctness have been described. The convergence of difference method to the functional solution for the Boltzmann type equations has been substantiated. The solvability on a whole has been proved, and the convergence of approximated methods for the general type non-linear conservation lay systems and for concrete models has been substantiated. The results are used in the Moscow State University, in the Physical-Energetical Institute, in the Institute of Nuclear Power Engineering, in the Institute of Hydrodynamics a. o. The application field: gas dynamics, physical kineticsAvailable from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|E
40|$|Background Examinations of {{the role}} of {{demographic}} characteristics in quality of life (QOL) in psychiatric samples are not new. However, serious limitations of previous research have been that (1) QOL was not assessed according to current recommendations, (2) assessment of QOL was often hampered by a substantial overlap in content between symptoms and QOL measures, and (3) the majority of the study samples had quite specific characteristics hampering the generalizability of results, as a result of which clinical implications of the results remained unclear. The aim {{of the present study was}} to investigate explicitly the relationships between demographics and QOL in a sample reflecting the general population of psychiatric outpatients, QOL being assessed in a comprehensive, culturally sensitive, and subjective way, paying attention to the relative importance of its various facets. The main hypothesis was that these relationships would be rather <b>weak.</b> <b>Method</b> From a population of 533 adult Dutch psychiatric outpatients, 495 participants completed the World Health Organization Quality of Life (WHOQOL) -Bref for assessing QOL. Furthermore, several demographic characteristics were recorded. Results Statistically significant correlations were found between partner relationship, habitual status, work, and sick leave and the WHOQOL-Bref domains social relationships and environment. Psychological health was associated to partner relationship, educational level, and sick leave. The total amount of QOL variance explained by demographics was rather low. Conclusion Amongst factors determining QOL, demographic characteristics are relatively unimportant. Therefore, paying attention to demographics during psychiatric treatment will probably have little effect on improvement of QOL...|$|E
40|$|Doctor of PhilosophyDepartment of Counseling and Educational PsychologyFred O. BradleyThis study {{examines}} {{the validity of}} the College Learning Effectiveness Inventory (CLEI). The CLEI is a new instrument designed to assess issues that college students face that affect their performance, including academic success and persistence. The CLEI serves diagnostic and prescriptive functions. Academic advisors, counselors and others whose work involves supporting student success and retention can use the CLEI to assess an individual student’s strengths and weaknesses and use the results to counsel students and provide appropriate remedial activities. This study compares the following six scales of the College Learning Effectiveness Inventory (CLEI) with instruments that have already been established. The six scales of the CLEI are as follows: (1) Academic Self-Efficacy, (2) Organization and Attention to Study, (3) Stress and Time Pressure, (4) Involvement with College Activity, (5) Emotional Satisfaction, and (6) Class Communication. The validation instruments for this cross-validation study included the Concentration, Self-Testing, Study Aids, and Time Management scales from the Learning and Study Strategies Inventory (LASSI), the Time Organization and Study Environment Management subscale of the Motivated Strategies for Learning Questionnaire (MSLQ), the College Adjustment Questionnaire (CAQ), the Rosenberg Self-Esteem Scale (RSES), and the Student Propensity to Ask Questions (SPAQ) scale. 	This study answers the following research questions: 1.) Are the CLEI scales reliable measures of the constructs they purport to assess? 2.) Are the CLEI scales valid measures of the dimensions they purport to assess? 3.) What are the CLEI scales attributes for this sample, and how do they compare with those from an earlier normative sample? 4.) How are the CLEI scales related to one another? 5.) Are the CLEI scales gender neutral? and 6.) Does the CLEI differentiate between students who are successful and those who may be at risk? Finally, this study cross-validates the CLEI. The reason for a cross-validation study of new scales is to demonstrate that these new measures actually measure what they purport to assess. Without cross validation, we would have to rely on a scale’s face validity, which is a comparatively <b>weak</b> <b>method</b> of assessing validity...|$|E
40|$|The weak Galerkin {{finite element}} method is a novel {{numerical}} method that was first proposed and analyzed by Wang and Ye for general second order elliptic problems on triangular meshes. The goal {{of this paper is}} to conduct a computational investigation for the <b>weak</b> Galerkin <b>method</b> for various model problems with more general finite element partitions. The numerical results confirm the theory established by Wang and Ye. The results also indicate that the <b>weak</b> Galerkin <b>method</b> is efficient, robust, and reliable in scientific computing. Comment: 19 page...|$|R
5000|$|... {{that other}} e-systems, such as Exxon Mobil’s Speedpass keychain payment device, used <b>weak</b> {{encryption}} <b>methods</b> {{which could be}} compromised by a half-hour or so of computing time; ...|$|R
40|$|The {{relationship}} between theoretically-grounded hints, strategy selection, and solution {{performance in the}} Toads and Frogs puzzle, a well-structured problem in which <b>weak</b> <b>methods</b> are difficult to apply, is investigated through an experiment and an ACT-R simulation. The main results show that providing a state specific hint is useful in speeding up {{the adoption of a}} hybrid solution strategy, comprising both the retrieval of previous moves and the proceduralization of a domain-specific heuristic that avoids any kind of forward search. The implications of the results for the problem solving theory are discussed...|$|R
50|$|For digital implementation, {{note that}} none of the Threshold or <b>Weak</b> Sync <b>methods</b> {{actually}} synthesize the waveform in a way different from Hard Sync (rather, they selectively deactivate it).|$|R
40|$|Let (Yn) be a {{sequence}} of random variables whose probabilitydistributions depend on x 2 [a; b]: It is well-known that ifnE(Yn ¡ x) 2 oconverges uniformly to zero on [a; b]; then, for all f 2 C[a; b]; fE (f(Yn)) g isuniformly convergent to f on [a; b]; where E denotes the mathematical expecta-tion. In this paper, we mainly improve this result via the concept of statisticalconvergence from the summability theory, which is a <b>weaker</b> <b>method</b> than theusual convergence. Furthermore, we construct an example such that our newresult is applicable while the classical one is not...|$|R
40|$|Despite an {{increasing}} interest in reinsurance and environmental manage ment systems (EMSs) concerning pollution, {{little research has}} been conducted on the subject. This paper presents the findings from a survey of reinsurers' views on this issue. Reinsurers give EMSs a cursory consideration in their pollution risk assessments because they inadequately appraise pollution liability, have no explicit requirements regarding risk management and there is little evidence to show that they are effective at reducing environmental risks. Reinsurers' underwriting assessments and post-loss investigations are poorly developed concerning public liability insurance. Reinsurance and EMSs are currently <b>weak</b> <b>methods</b> to prevent and control pollution risks. ...|$|R
30|$|In this paper, we {{establish}} a large deviation principle for a mean reflected stochastic differential equation driven by both Brownian motion and Poisson random measure. The <b>weak</b> convergence <b>method</b> {{plays an important}} role.|$|R
50|$|This method allows every {{bridge to}} {{potentially}} earn full credit if {{the standards are}} met. In situations where competition is <b>weak,</b> this <b>method</b> {{runs the risk of}} having many competitors receive no credit.|$|R
40|$|Fluid {{structure}} interaction (FSI) {{analysis is}} of great significance with the advance of computing technology and numerical algorithms in the last decade. This multidisciplinary problem has been expanded to engineering applications such as offshore structures, dam-reservoirs and other industrial applications. The motivation {{of this research is}} to investigate the fundamental physics involved in the complex interaction of fluid and structural domains by numerical simulations and to tackle the multiple surface interactions of a one-way coupling FSI GBS engineering case. To solve such problem, the partitioned method has been adopted and the approach is to utilise the advantage of the existing numerical algorithms in solving the complex fluid and structural interactions. The suitability has been validated for both strong and <b>weak</b> coupling <b>methods</b> which are the distinctive partitioned coupling approach. Therefore, with the computational platform of ANSYS FEA, the coupled field methods were adopted in this numerical analysis. Comparisons were made with the results obtained to justify the ability of both strong and <b>weak</b> <b>methods</b> in resolving the one-way coupling example with the potential applications in the field of ocean and marine engineering...|$|R
