3|96|Public
40|$|Recent {{results on}} {{particle}} momentum and spin correlations {{are discussed in}} view of the role played by the effects of quantum statistics, including multiboson and coherence phenomena, and final state interaction. Particularly, it is demonstrated that the latter allows for (i) correlation femtoscopy with unlike particles; (ii) study of the relative space [...] time asymmetries in the production of different particle species (e. g., relative time delays or spatial shifts due to collective flows); (iii) study of the particle strong interaction hardly accessible by other means (e. g., in two-lambda system). Comment: Extended contribution to Proceedings of the XXXII International Symposium on Multiparticle Dynamics, Alushta, Ukraine, September 8 - 12, 2002, nucl-th/ 0212089; 17 pages. A version to appear in Phys. At. Nucl. 67 (2004) with some <b>wording</b> <b>corrections</b> and updated reference...|$|E
40|$|We reconsider in {{some detail}} a {{construction}} allowing (Borel) convergence of an alternative perturbative expansion, for specific physical quantities of asymptotically free models. The usual perturbative expansions (with an explicit mass dependence) are transmuted into expansions in 1 /F, where F ∼ 1 /g(m) for m ≫Λ while F ∼ (m/Λ) ^α for m Λ, Λ being the basic scale and α given by renormalization group coefficients. (Borel) convergence holds {{in a range of}} F which corresponds to reach unambiguously the strong coupling infrared regime near m→ 0, which can define certain "non-perturbative" quantities, such as the mass gap, from a resummation of this alternative expansion. Convergence properties can be further improved, when combined with δ expansion (variationally improved perturbation) methods. We illustrate these results by re-evaluating, from purely perturbative informations, the O(N) Gross-Neveu model mass gap, known for arbitrary N from exact S matrix results. Comparing different levels of approximations that can be defined within our framework, we find reasonable agreement with the exact result. Comment: 33 pp., RevTeX 4, 6 eps figures. Minor typos, notation and <b>wording</b> <b>corrections,</b> 2 references added. To appear in Phys. Rev. ...|$|E
40|$|Completed element {{characteristics}} page Added element state page Revision 2 – Moved Page Code {{field in}} CDB Corrected Opcode in CDB table and command table Removed complete descriptors requirement and added note about difference from RES Added requirement that element address be ignored if the supported pages list is requested Renamed Element Characteristics page to Element Static Information page Added requirement for supported pages order in return data Added length fields to pages with fixed length descriptors Removed “Offline ” concept and all bits – disabled with UA’s {{will be used}} instead Added Import and Operator Intervention bits to element state Change Exception Cause field to ASC/ASCQ as in RES Added support column to page code list Revision 3 – Changes requested in Feb. 2007 T 10 meeting Moved “see SPC- 3 ” for allocation and control field to the correct location. Changed ELEMENT TYPE CODE field to reference 6. 10. 1 directly Moved truncated descriptors not an error statement to normative text in allocation length description <b>Wording</b> <b>corrections</b> on supported element information pages description Moved location of single supported pages descriptor for each element type requirement Changed return data to allow grouping of elements by starting address and number of elements Changed location length descriptor to 4 bytes and location parameter length to 4 bytes Removed list of proposed location types Corrected location of PARAMETERS LENGTH in element location descriptor. Changed supported volume types list to supported volume types parameters Added ability to report a supported volume type as read only when in the described data transfer element Added a volume index to the state information. Related Documents smc 3 r 04 – SCSI Media Changer Commands- 3 revision 0...|$|E
50|$|Among Maliit’s {{features}} are a plugin-based architecture, <b>word</b> <b>correction</b> and prediction, multitouch, and context sensitive layouts.|$|R
5000|$|Fleksy {{is limited}} to <b>word</b> <b>corrections</b> and does not offer next word predictions, as found in competitors, such as Swype, SwiftKey and other keyboards.|$|R
50|$|Designed {{mostly for}} {{touchscreen}} devices, Maliit allows the inputting of text without {{the presence of}} a physical keyboard. More advanced features such as <b>word</b> <b>correction</b> and prediction are also available.|$|R
5000|$|Fleksy also {{utilizes}} a gesture-based interface {{that can}} be used for some common functions, such space, backspace, and choosing a <b>word</b> <b>correction.</b> The gesture system has elicited a mixed response, with criticism centered around the learning curve created for many users.|$|R
40|$|In this paper, we take {{a pattern}} {{recognition}} approach to correcting errors in text generated from printed documents using optical character recognition (OCR). We apply a very general, theoretically optimal model {{to the problem of}} OCR <b>word</b> <b>correction,</b> introduce practical methods for parameter estimation, and evaluate performance on real data...|$|R
50|$|The {{design of}} error {{detection}} and correction circuits is {{helped by the}} fact that soft errors usually are localised to a very small area of a chip. Usually, only one cell of a memory is affected, although high energy events can cause a multi-cell upset. Conventional memory layout usually places one bit of many different <b>correction</b> <b>words</b> adjacent on a chip. So, even a multi-cell upset leads to only a number of separate single-bit upsets in multiple <b>correction</b> <b>words,</b> rather than a multi-bit upset in a single <b>correction</b> <b>word.</b> So, an error correcting code needs only to cope with a single bit in error in each <b>correction</b> <b>word</b> in order to cope with all likely soft errors. The term 'multi-cell' is used for upsets affecting multiple cells of a memory, whatever <b>correction</b> <b>words</b> those cells happen to fall in. 'Multi-bit' is used when multiple bits in a single <b>correction</b> <b>word</b> are in error.|$|R
50|$|Before the {{invention}} of <b>word</b> processors, <b>correction</b> fluid greatly facilitated the production of typewritten documents.|$|R
40|$|The {{difficulty}} with information retrieval for OCR documents {{lies in the}} fact that OCR documents comprise of a significant amount of erroneous words and unfortunately most information retrieval techniques rely heavily on word matching between documents and queries. In this paper, we propose a general content-based correction model that can work on top of an existing OCR correction tool to "boost" retrieval performance. The basic idea of this correction model is to exploit the whole content of a document to supplement any other useful information provided by an existing OCR <b>correction</b> tool for <b>word</b> <b>correction.</b> Instead of making an explicit correction decision for each erroneous word as typically done in a traditional approach, we consider the uncertainties in such correction decisions and compute an estimate of the original "uncorrupted" document language model accordingly. The document language model can then be used for retrieval with a language modeling retrieval approach. Evaluation using a TREC standard testing collection indicates that our method significantly improves the performance when compared with simple <b>word</b> <b>correction</b> approaches such as using only the top ranked correction...|$|R
40|$|The {{momentum}} {{distribution for}} the η' meson produced through the inclusive decay B→η'X is investigated under two decay mechanisms, b→η's and b→η' s g. Although all these two mechanisms can explain the recently observed decay rate of B→η'X, the momentum spectrums for η' meson predicted by them are strongly different. Thus detailed experiment is proposed to distinguish the two cases. Comment: 7 pages, 1 figure in PS file(not changed), minor <b>words</b> <b>corrections,</b> to appear in Phys. Rev. D, Rapid Communication...|$|R
40|$|Some edits {{were made}} {{outside of the}} joint review that {{included}} deletion of <b>word(s),</b> <b>correction</b> of spelling & grammar, changes to bolded text, etc. that did not change the intent, meaning or requirements of the OGRs, but rather to provide clarification. These changes are not documented in this table. Authorization of the Annual Operating Plan (AOP) does not constitute waiver or exemption from the ground rules or any statutory requirement, nor is authorization of the AOP verification of compliance with the ground rules or any statutory requirement...|$|R
40|$|Two <b>word</b> slant <b>correction</b> {{methods are}} {{evaluated}} {{in this work}} using handwritten numeral strings. A method originally proposed to <b>word</b> slant <b>correction</b> is adapted {{in order to improve}} the results for handwritten numeral strings. The original and modified methods are evaluated by means of some interesting analyses on the NIST SD 19 database. In these analyses the difference, in terms of slant estimated, between two slant correction techniques of isolated numerals extracted from strings, and the impact of slant correction on the number of overlapping numerals in the strings are evaluated. The results look promising...|$|R
50|$|To {{speak to}} Manakas <b>words,</b> a <b>correction</b> in the {{software}} - {{in this case the}} muscle apparatus - could cause changes in the somatic condition of the related segment.|$|R
40|$|We {{describe}} techniques {{based on}} {{natural language generation}} which allow a user to author a document in controlled language for multiple natural languages. The author {{is expected to be}} an expert in the application domain but not in the controlled language or in more than one of the supported natural languages. Because the system can produce multiple expressions of the same input in multiple languages, the author can choose among alternative expressions satisfying the constraints of the controlled language. Because the system offers only legitimate choices of <b>wording,</b> <b>correction</b> is unnecessary. Consequently, acceptance of error reports and corrections by trained authors are non-issues...|$|R
5000|$|In 2007 the Tennessee Department of Correction banned prison {{visitors}} from wearing thong or g-string underwear. In the <b>words</b> of <b>Correction</b> Commissioner George Little, prisoners [...] "don't {{need any help}} getting turned on." ...|$|R
50|$|Fleksy is a {{third party}} virtual {{keyboard}} extension and input method for touchscreen devices, which attempts to improve traditional tap-typing input speed and accuracy through enhanced auto-correct and gesture control. It uses error-correcting algorithms that analyze the region where the user touches the keyboard and feeds this through a language model, which calculates and identifies the intended word. Swiping gestures are used to control common functions, such as space, delete, and <b>word</b> <b>correction.</b> Blind and visually impaired users have utilized Fleksy for eyes-free typing through muscle memory. It was first commercially available on the iPhone as a download from Apple's App Store in July 2012, and is available in more than 40 languages.|$|R
40|$|In this paper, {{we aim to}} {{understand}} whether current language and vision (LaVi) models truly grasp {{the interaction between the}} two modalities. To this end, we propose an extension of the MSCOCO dataset, FOIL-COCO, which associates images with both correct and "foil" captions, that is, descriptions of the image that are highly similar to the original ones, but contain one single mistake ("foil word"). We show that current LaVi models fall into the traps of this data and perform badly on three tasks: a) caption classification (correct vs. foil); b) foil word detection; c) foil <b>word</b> <b>correction.</b> Humans, in contrast, have near-perfect performance on those tasks. We demonstrate that merely utilising language cues is not enough to model FOIL-COCO and that it challenges the state-of-the-art by requiring a fine-grained understanding of the relation between text and image. Comment: To appear at ACL 201...|$|R
40|$|The searching of data Process {{end users}} search their data needs using query representation, by using {{this way of}} {{retrieving}} data may not meet their expectations. To achieve end users goal, developers implement several techniques. Previously end users follow greedy algorithm with IQp [1]. But {{in this paper we}} will work forward with n-gram Language model. In this approach, end user selects the searchable keyword with the length of minimum n+ 1 data units. With n data units users failed to retrieve their expectations. This approach includes Probabilistic algorithms used for large vocabulary <b>word</b> <b>correction</b> system with language model. This paper explains data search using n gram data model [5], web server and allows users to interact via browser front end. We outline the challenges and discuss the implementation of our system including results of extensive experimental evaluation. Keywords: N-Gram, Spell Checker, Search Engine, Query construction...|$|R
40|$|The {{importance}} of learning Chinese is increasing {{in the latest}} decades. However, the learning of Chinese is not easy for foreigners {{as a second language}} learning. Sometimes they write some text or document, but there always have many error words. So, how to detect the error word in document is becoming more then more important. This issue is very extensive, not only can help foreigners to learning Chinese but also can detect the error word. This paper had proposed method can divide five sections of structure: First sections are input sentence; second sections are parsing and word segmentation; third sections are fine the wrong word; forth sections are remove duplicate; fifth sections are final output. In this paper we use language model to detect Chinese spelling. It is had four part, E-Hownet, CKIP, similar pronunciation and shape dictionary, use the preset word to compare the <b>word</b> <b>correction</b> which in database. We use the bi-gram to promote our performance. ...|$|R
2500|$|Correction fluid is an opaque, white fluid {{applied to}} paper to mask errors in text. It was very {{important}} when material was typed with a typewriter, but has become less so {{since the advent of}} the <b>word</b> processor. <b>Correction</b> fluid was invented by Bette Nesmith Graham in 1951. Originally called by the brand name [...] "Mistake Out", Graham began selling correction fluid in 1956.|$|R
40|$|The {{notion of}} infix {{probability}} has been intro-duced {{in the literature}} as a generalization {{of the notion of}} prefix (or initial substring) prob-ability, motivated by applications in speech recognition and <b>word</b> error <b>correction.</b> For the case where a probabilistic context-free gram-mar is used as language model, methods for the computation of infix probabilities have been presented in the literature, based on vari-ous simplifying assumptions. Here we present a solution that applies to the problem in its full generality. ...|$|R
40|$|The Phonetic and the Homophone Error {{problem in}} a {{language}} have been characterized as a symbol substitution problem. Phonetically equivalent symbols or symbol combinations in the language are grouped together. Each group or a number of related groups give(s) rise to a dictionary or a number of dictionaries. A new design methodology for Orthographic Dictionaries in alphabetic languages has been described. The dictionaries include the root words. The meanings are stored only in case of Homophone words. Words are sorted {{on the basis of}} a Phonetic Ordering Scheme. The dictionaries are being used to detect and correct the Phonetic Error and the Homophone Error in isolated words of Bengali. The methodologies described in this paper can be used in developing spell-checkers not only for other Indian languages which have evolved from the ancient Brahmi script and have a common phonetic structure but also for other alphabetic languages with suitable modifications. The practical NLP systems must deal with erroneous or ill-formed inputs. The nature of errors basically depends on the source of input as well as on the user's exceeding the systems limited grammatical, conceptual or functional coverage. Automatic <b>word</b> <b>correction</b> research may be viewed [7] as focusing o...|$|R
40|$|This paper {{explores the}} use of a {{character}} segment based character correction model, language modeling, and shallow morphology for Arabic OCR error correction. Experimentation shows that character segment based correction is superior to single character correction and that language modeling boosts correction, by improving the ranking of candidate corrections, while shallow morphology had a small adverse effect. Further, given sufficiently large corpus to extract a dictionary and to train a language model, <b>word</b> based <b>correction</b> works well for a morphologically rich language such as Arabic. ...|$|R
40|$|This paper {{presents}} a novel reversible data hiding (RDH) algorithm for gray-scaled images, {{in which the}} prediction-error of prediction error (PPE) of a pixel is used to carry the secret data. In the proposed method, the pixels to be embedded are firstly predicted with their neighboring pixels to obtain the corresponding prediction errors (PEs). Then, by exploiting the PEs of the neighboring pixels, the prediction of the PEs of the pixels can be determined. And, a sorting technique based on the local complexity of a pixel is used to collect the PPEs to generate an ordered PPE sequence so that, smaller PPEs will be processed first for data embedding. By reversibly shifting the PPE histogram (PPEH) with optimized parameters, the pixels corresponding to the altered PPEH bins can be finally modified to carry the secret data. Experimental results have implied that the proposed method can benefit from the prediction procedure of the PEs, sorting technique as well as parameters selection, and therefore outperform some state-of-the-art works in terms of payload-distortion performance when applied to different images. Comment: There has no technical difference to previous versions, but rather some minor <b>word</b> <b>corrections.</b> A 2 -page summary {{of this paper was}} accepted by ACM IH&MMSec' 16 "Ongoing work session". My homepage: hzwu. github. i...|$|R
5000|$|... {{pronounced}} as [...] "tesh", is a <b>word</b> of French <b>Correction</b> note: Not named Teche {{because of}} a big snake in the region; but, because the Bayou Teche has a snakelike appearance as it meanders through the land curving this way and then that way, similar to snakelike movements.|$|R
40|$|Using a <b>word</b> order <b>correction</b> paradigm, we {{assessed}} syntactic awareness skills {{in children with}} good and poor reading comprehension, matched for age, decoding skill, and nonverbal ability. Poor comprehenders performed less well than normal readers, {{and the performance of}} both groups was influenced by the syntactic complexity and semantic ambiguity of the sentences. These findings support the view that poor comprehenders have language processing difficulties encompassing grammatical as well as semantic weaknesses, although their phonological processing skills are normal. The implications of such language weaknesses for the development of skilled reading are discussed...|$|R
50|$|Rosemary finally {{arrives and}} is greeted by sharp <b>words</b> of <b>correction.</b> Despite {{the effort to}} {{reprimand}} Rosemary the women fall quickly back into a lighter air with one another. Throughout scene one we see a consistency of playful jests among the women. The teasing and prodding brings a familial nature among the group. We learn about each woman’s current or past love lives, with particular mention on Vera’s current abusive relationship with her husband. The scene closes with Ellen and Una jabbing at one another and discussing the lengths the women go to guide Rosemary.|$|R
40|$|Convolutional neural {{networks}} (ConvNets) {{have been shown}} to be effective at a variety of natural language processing tasks. To date, their utility for correcting errors in writing has not been investigated. Writing error correction is important for a variety of computer-based methods for the assessment of writing. In this thesis, we apply ConvNets to a number of tasks pertaining to writing errors – including non-word error detection, isolated non-word correction, context-dependent non-word correction, and context-dependent real <b>word</b> <b>correction</b> – and find them to be competitive with or superior to a number of existing approaches. On these tasks, ConvNets function as discriminative language models, so on several tasks we compare ConvNets to probabilistic language models. Non-word error detection, for instance, is usually performed with a dictionary that provides a hard, Boolean answer to a word query. We evaluate ConvNets as a soft dictionary that provides soft, probabilistic answers to word queries. Our results indicate that ConvNets perform better in this setting than traditional probabilistic language models trained with the same examples. Similarly, in context-dependent non-word error correction, high-performing systems often make use of a probabilistic language model. We evaluate ConvNets and other neural architectures on this task and find that all neural network models outperform probabilistic language models, even though the networks were trained with two orders of magnitude fewer examples...|$|R
40|$|The paper {{presents}} {{a new language}} processing toolkit developed at Adam Mickiewicz University. Its functionality includes currently tokenization, sentence splitting, dictionary-based- morphological analysis, heuristic morphological analysis of unknown <b>words,</b> spelling <b>correction,</b> pattern search, and generation of concordances. It is organized {{as a collection of}} command-line programs, each performing one operation. The components may be connected in various ways to provide various text processing services. Also new user-defined components may be easily incorporated into the system. The toolkit is destined for processing raw (not annotated) text corpora. The system was originally intended for Polish, but its adaptation to other languages is possible. 1...|$|R
40|$|In {{this paper}} the Damerau-Levenshtein string {{difference}} metric is generalized {{in two ways}} to more accurately compensate for the types of errors that {{are present in the}} script recognition domain. First, the basic dynamic programming method for computing such a measure is extended to allow for merges, splits and two-letter substitutions. Second, edit operations are refined into categories according to the effect they have on the visual "appearance" of words. A set of recognizer-independent constraints is developed to reflect the severity of the information lost due to each operation. These constraints are solved to assign specific costs to the operations. Experimental results on 2, 335 corrupted strings and a lexicon of 21, 299 words show higher correcting rates than with the original form. Keywords: string distance, string matching, spelling error <b>correction,</b> <b>word</b> recognition and <b>correction,</b> text editing, script recognition and post-processing 1 INTRODUCTION Since the goal of text recog [...] ...|$|R
40|$|Abstract: Error {{analysis}} involves detecting, diagnosing and correcting {{discrepancies between}} the text produced so far (TPSF) and the writers mental representation {{of what the}} text should be. While many factors determine the choice of strategy, cognitive effort is {{a major contributor to}} this choice. This research shows how cognitive effort during error analysis affects strategy choice and success as measured by a series of online text production measures. Text production is shown to be influenced most by error span, i. e. whether or not the error spans more or less than two characters. Next, it is influenced by input mode, that {{is whether or not the}} error has been generated by speech recognition or keyboard, and finally by lexicality, i. e. whether or not the error comprises an existing <b>word.</b> <b>Correction</b> of larger error spans are corrected more successful smaller errors. Writers impose a wise speed accuracy tradeoff during large error spans since correction is better, but preparation times and production times take longer, and interference reaction times are slower. During large error spans, there is a tendency to opt for error correction first, especially when errors occurred in the condition in which the TPSF is not preceded by speech. In general the addition of speech frees the cognitive demands of writing: shorter preparation and reaction times. Writers also opt more often to continue text production when the TPSF is presented auditory first...|$|R
40|$|The {{individual}} {{cognitive science}} disciplines all have contributions {{to make to}} the understanding and modelling of human learning. Our previous research has explored unsupervised learning of phonology, morphology and low-level syntax, as well as basic noun, verb and preposition ontology and semantics, plus musical and speech prosody. Successful applications using a mix of supervised and unsupervised techniques include speech control of equipment, deep web search, confused <b>word</b> spelling <b>correction,</b> multi-lingual semantic models and audio-visual speech recognition. Our current research is focused on doing simultaneous learning of ontology, syntax and semantics by embedding the learner in realistic situations and by developing lowlevel biologically-plausible models of perceptual and cognitive processing...|$|R
40|$|Error {{analysis}} involves detecting, diagnosing and correcting {{discrepancies between}} the text produced so far (TPSF) and the writers mental representation {{of what the}} text should be. While many factors determine the choice of strategy, cognitive effort is {{a major contributor to}} this choice. This research shows how cognitive effort during error analysis affects strategy choice and success as measured by a series of online text production measures. Text production is shown to be influenced most by error span, i. e. whether or not the error spans more or less than two characters. Next, it is influenced by input mode, that {{is whether or not the}} error has been generated by speech recognition or keyboard, and finally by lexicality, i. e. whether or not the error comprises an existing <b>word.</b> <b>Correction</b> of larger error spans are corrected more successful smaller errors. Writers impose a wise speed accuracy tradeoff during large error spans since correction is better, but preparation times and production times take longer, and interference reaction times are slower. During large error spans, there is a tendency to opt for error correction first, especially when errors occurred in the condition in which the TPSF is not preceded by speech. In general the addition of speech frees the cognitive demands of writing: shorter preparation and reaction times. Writers also opt more often to continue text production when the TPSF is presented auditory first. Cognitive effort, Dictation, Dual task technique, Error analysis, Speech recognition, Text Produced So Far (TPSF), Text production, Technology of writing, Working memory...|$|R
50|$|The {{most common}} letters (the large {{letters in the}} {{illustration}} below) are accessed by a tap. Less common letters are accessed by a slide. Example: Tapping the center square generates an 'o'. Sliding to the left from the same square generates a 'c'. A green trail shows {{the path of the}} finger. The keyboard supports multiple user dictionaries, used for <b>word</b> prediction and <b>correction.</b>|$|R
