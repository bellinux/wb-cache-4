1392|942|Public
50|$|Alternatively the coefficients, C, {{could be}} {{calculated}} in a spreadsheet, employing a built-in matrix inversion routine to obtain the inverse of the normal equations matrix. This set of coefficients, once calculated and stored, {{can be used with}} all calculations in which the same <b>weighting</b> <b>scheme</b> applies. A different set of coefficients is needed for each different <b>weighting</b> <b>scheme.</b>|$|E
50|$|A typical <b>{{weighting}}</b> <b>scheme</b> is:The weighting schemes {{vary depending}} on the variables being measured.|$|E
5000|$|The optimal <b>weighting</b> <b>scheme</b> , that {{balances}} the two {{terms in the}} display above, is given as follows: set , ...|$|E
40|$|Abstract. Term <b>weighting</b> <b>schemes</b> play a {{vital role}} in the {{performance}} of many Information Retrieval models. The vector space model is one such model in which the weights applied to the document terms are of crucial importance to the accuracy of the retrieval system. This paper outlines a procedure using genetic programming to automatically determine term <b>weighting</b> <b>schemes</b> that achieve a high average precision. The schemes are tested on standard test collections and are shown to perform consistantly better than the traditional tf-idf <b>weighting</b> <b>schemes.</b> We present an analysis of the evolved <b>weighting</b> <b>schemes</b> to explain their increase in performance. These term <b>weighting</b> <b>schemes</b> are shown to be general across various collections and are shown to adhere to Luhn’s theory as both high and low frequency terms are assigned a low weight. ...|$|R
3000|$|We use words’ {{weights to}} guide neural models {{to focus on}} {{important}} words. Various supervised <b>weighting</b> <b>schemes</b> are explored in this work. We discover that better text features are learned for sentiment analysis when suitable <b>weighting</b> <b>schemes</b> are applied upon neural models.|$|R
40|$|How {{to assign}} {{appropriate}} weights to terms {{is one of}} the critical issues in information retrieval. Many term <b>weighting</b> <b>schemes</b> are unsupervised. They are either based on the empirical observation in information retrieval, or based on generative approaches for language modeling. As a result, the existing term <b>weighting</b> <b>schemes</b> are usually insufficient in distinguishing informative words from the uninformative ones, which is crucial to the performance of information retrieval. In this paper, we present supervised term <b>weighting</b> <b>schemes</b> that automatically learn term weights based on the correlation between word frequency an...|$|R
5000|$|It is {{customary}} {{to define}} the average opacity, calculated using a certain <b>weighting</b> <b>scheme.</b> Planck opacity uses the normalized Planck black body radiation energy density distribution, , as the weighting function, and averages [...] directly: ...|$|E
50|$|To {{address these}} and other {{concerns}} there are some developed models from the probabilistic relevance framework. The Binary Independence Model for one, as {{it is from the}} same author. The most known derivative of this framework is the Okapi(BM25) <b>weighting</b> <b>scheme</b> and its BM25F brother.|$|E
50|$|Variations of the tf-idf <b>weighting</b> <b>scheme</b> {{are often}} used by search engines as a central tool in scoring and ranking a document's {{relevance}} given a user query. tf-idf can be successfully used for stop-words filtering in various subject fields including text summarization and classification.|$|E
30|$|In this paper, we {{introduce}} <b>weighting</b> <b>schemes</b> into Paragraph Vector (PV). The weighting information guides PV {{to focus}} on important words while ignores unimportant ones. Inspired by the line of models ‘word 2 vec–PV-weighted PV,’ we further introduce text embedding and <b>weighting</b> <b>schemes</b> into GloVe, which are PV-GloVe and weighted PV-GloVe, respectively.|$|R
30|$|Various {{supervised}} <b>weighting</b> <b>schemes</b> are {{explored in}} this work. Significant and consistent improvements are witnessed {{with the introduction}} of <b>weighting</b> information. <b>Weighting</b> <b>schemes</b> are effective on PV and PV-GloVe, just as <b>weighting</b> <b>schemes</b> on sparse BOW representation. Besides that, we discover that PV-GloVe is useful in generating text representations. PV-GloVe and weighted PV-GloVe achieve comparable results with PV and weighted PV, respectively. Finally, we compare our methods with newly proposed neural models. In terms of accuracy, our methods do not perform as well as them. Nevertheless, the gap is not big and our models are simple, efficient, and do not require additional resources.|$|R
40|$|In {{order to}} achieve {{relevant}} scholarly information from the biomedical databases, researchers generally use technical terms as queries such as proteins, genes, diseases, and other biomedical descriptors. However, the technical terms have limits as query terms {{because there are so}} many indirect and conceptual expressions denoting them in scientific literatures. Combinatorial <b>weighting</b> <b>schemes</b> are proposed as an initial approach to this problem, which utilize various indexing and weighting methods and their combinations. In the experiments based on the proposed system and previously constructed evaluation collection, this approach showed promising results in that one could continually locate new relevant expressions by combining the proposed <b>weighting</b> <b>schemes.</b> Furthermore, it could be ascertained that the most outperforming binary combinations of the <b>weighting</b> <b>schemes,</b> showing the inherent traits of the <b>weighting</b> <b>schemes,</b> could be complementary to each other and it is possible to find hidden relevant documents based on the proposed methods...|$|R
5000|$|... where [...] is the {{coordinate}} of {{the particle}} and [...] the observation point. Perhaps {{the easiest and}} most used choice for the shape function is the so-called cloud-in-cell (CIC) scheme, which is a first order (linear) <b>weighting</b> <b>scheme.</b> Whatever the scheme is, the shape function has to satisfy the following conditions: ...|$|E
50|$|Both for {{classification}} and regression, it can {{be useful}} to assign weight to {{the contributions of the}} neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common <b>weighting</b> <b>scheme</b> consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor.|$|E
50|$|When the Google {{search engine}} became popular, search engine optimizers learned that Google's ranking {{algorithm}} depended {{in part on}} a link <b>weighting</b> <b>scheme</b> called PageRank. Rather than simply count all inbound links equally, the PageRank algorithm determines that some links may be more valuable than others, and therefore assigns them more weight than others. Link farming was adapted to help increase the PageRank of member pages.|$|E
30|$|In {{the context}} of kNN and K-means, we compare the {{performance}} of PDSM with that of other four measures, Euclidean distance, Cosine similarity, EJ, and Manhattan. Moreover, to see the effect of different term <b>weighting</b> <b>schemes</b> {{on the performance of}} PDSM, we employ some popular <b>weighting</b> <b>schemes,</b> including tf and tf-idf. We also apply the Cosine normalization factor (16) [33] to tf and tf-idf (tf-normal and tf-idf-normal).|$|R
3000|$|Section  2 {{reviews the}} related work of {{sentiment}} analysis. Section  3 discusses how to integrate <b>weighting</b> <b>schemes</b> into Paragraph Vector. In Sect.   4, PV-GloVe and weighted PV-GloVe are proposed, where text embedding and weighting information are introduced into GloVe. The <b>weighting</b> <b>schemes</b> {{used in this}} paper are discussed in Sect.   5. Experimental results are given in Sect.   6, followed by the conclusion in Sect.   7.1 [...]...|$|R
40|$|It {{has been}} known that using {{different}} representations of either queries or documents, or different retrieval techniques retrieves different sets of documents. Recent work suggests that significant improvements in retrieval performance {{can be achieved by}} combining multiple representations or multiple retrieval techniques. In this paper we propose a simple method for retrieving different documents within a single query representation, a single document representation and a single retrieval technique. We classify the types of documents, and describe the properties of <b>weighting</b> <b>schemes.</b> Then, we explain that different properties of <b>weighting</b> <b>schemes</b> may retrieve different types of documents. Experimental results show that significant improvements can be obtained by combining the retrieval results from different properties of <b>weighting</b> <b>schemes...</b>|$|R
5000|$|The LEED 2009 {{performance}} {{credit system}} aims to allocate points [...] "based {{on the potential}} environmental impacts and human benefits of each credit." [...] These are weighed using the environmental impact categories of the United States Environmental Protection Agency's Tools for the Reduction and Assessment of Chemical and Other Environmental Impacts (TRACI). and the environmental-impact <b>weighting</b> <b>scheme</b> developed by the National Institute of Standards and Technology (NIST).|$|E
50|$|Do Pitch class mapping {{with respect}} to the {{estimated}} reference frequency. This is a procedure for determining the pitch class value from frequency values. A <b>weighting</b> <b>scheme</b> with cosine function is used. It considers the presence of harmonic frequencies (harmonic summation procedure), taking account a total of 8 harmonics for each frequency. To map the value on a one-third of a semitone, the size of the pitch class distribution vectors must be equal to 36.|$|E
50|$|An {{assumption}} {{underlying the}} treatment given above {{is that the}} independent variable, x, is free of error. In practice, the errors on the measurements of the independent variable are usually {{much smaller than the}} errors on the dependent variable and can therefore be ignored. When this is not the case, total least squares or more generally errors-in-variables models, or rigorous least squares, should be used. This can be done by adjusting the <b>weighting</b> <b>scheme</b> to take into account errors on both the dependent and independent variables and then following the standard procedure.|$|E
40|$|Abstract:In this paper, an {{improved}} weighted least squares (WLS), together with autoregressive (AR) model, is {{proposed to improve}} prediction accuracy of earth rotation parameters(ERP). Four <b>weighting</b> <b>schemes</b> are developed and the optimal power e for determination of the weight elements is studied. The {{results show that the}} improved WLS-AR model can improve the ERP prediction accuracy effectively, and for different prediction intervals of ERP, different <b>weight</b> <b>scheme</b> should be chosen...|$|R
30|$|A {{direction}} {{that remains to}} be explored is using attention mechanism to learn the weight of word automatically. Existing <b>weighting</b> <b>schemes</b> are proposed on the basis of people’s prior knowledge on textual data. It is a more elegant way to integrate the word weighting process into neural models. In our future work, we will explore attention-based neural models on sentiment analysis and compare them with <b>weighting</b> <b>schemes</b> designed by hands.|$|R
40|$|In this paper, an {{improved}} weighted least squares (WLS), together with autoregressive (AR) model, is {{proposed to improve}} prediction accuracy of earth rotation parameters(ERP). Four <b>weighting</b> <b>schemes</b> are developed and the optimal power e for determination of the weight elements is studied. The {{results show that the}} improved WLS-AR model can improve the ERP prediction accuracy effectively, and for different prediction intervals of ERP, different <b>weight</b> <b>scheme</b> should be chosen...|$|R
5000|$|... {{where the}} {{subscript}} [...] labels the grid point. To {{ensure that the}} forces acting on particles are self-consistently obtained, the way of calculating macro-quantities from particle positions on the grid points and interpolating fields from grid points to particle positions has to be consistent, too, since they both appear in Maxwell's equations. Above all, the field interpolation scheme should conserve momentum. This {{can be achieved by}} choosing the same <b>weighting</b> <b>scheme</b> for particles and fields and by ensuring the appropriate space symmetry (i.e. no self-force and fulfilling the action-reaction law) of the field solver at the same time ...|$|E
50|$|She {{worked at}} the Cambridge Language Research Unit from the late 1950s, then at Cambridge's Computer Laboratory from 1974, and retired in 2002, holding the post of Professor of Computers and Information, which she was awarded in 1999. She {{continued}} {{to work in the}} Computer Laboratory until shortly before her death. Her main research interests, since the late 1950s, were natural language processing and information retrieval. One of her most important contributions was the concept of inverse document frequency (IDF) weighting in information retrieval, which she introduced in a 1972 paper. IDF is used in most search engines today, usually as part of the tf-idf <b>weighting</b> <b>scheme.</b>|$|E
50|$|The Animal in You {{features}} a personality test of nine questions that collapses {{to one of}} 45 possible personality types. After readers answer the questions about their personality and physical attributes, the test returns a number corresponding {{to one of the}} 45 animal personality types, appearing in a look-up table. The underlying mechanisms for these types of tests are trivial for modern software based Internet tests, {{but this is the first}} known example of a book-based test able to resolve over 20 categories. The test is augmented by an interdependent <b>weighting</b> <b>scheme</b> wherein each question is assigned a different weight depending on how the other questions are answered.|$|E
40|$|We {{present a}} {{molecular}} phylogeny including most {{species of the}} genus Medicago L. (Fabaceae). Based on the consensus of the 48 most parsimonious trees, life-history and mating-system characters are mapped, and a putative history of the genus is suggested. The most parsimonious reconstruction suggests an ancestral annual and selfing state, and recurrent evolution towards perenniality and outcrossing. Based on theoretical predictions and classical hypotheses {{of the history of}} the genus, different assumptions about the ancestral state and different <b>weighting</b> <b>schemes</b> of evolution between the character states are made. Assuming an outcrossing, perennial ancestral state (partly supported by morphological features) does not fundamentally change the reconstruction. To meet theoretical expectations, various <b>weighting</b> <b>schemes</b> favouring evolution towards annuality and selfing are applied. Influence and validity of such <b>weighting</b> <b>schemes</b> are discussed with regard to other studies...|$|R
40|$|Pairwise {{similarity}} judgement {{correlations between}} humans and Latent Semantic Analysis (LSA) were explored {{on a set of}} 50 news documents. LSA is a modern and commonly used technique for automatic determination of document similarity. LSA users must choose local and global <b>weighting</b> <b>schemes,</b> the number of factors to be retained, stop word lists and whether to background. Global <b>weighting</b> <b>schemes</b> had more effect than local <b>weighting</b> <b>schemes.</b> Use of a stop word list almost always improved performance. Introduction of a background set of similar documents increased larger correlations and reduced smaller ones. The correlations ranged between approximately 0 and 0. 6 depending on the LSA settings indicating the importance of correct settings. The low maximum correlation indicates that information presentation schemes based on LSA may often be at variance with visualisation...|$|R
30|$|One {{problem of}} neural bag-of-words models {{is that they}} treat each word equally. Intuitively, some words are more {{important}} for sentiment analysis task, such as ‘amazing’ and ‘best.’ To capture better features, we use <b>weighting</b> <b>schemes</b> to guide the training of these models. To this end, the new models are able {{to pay more attention}} to words that reflect polarities of texts. When suitable <b>weighting</b> <b>schemes</b> are used, significant improvements over traditional neural bag-of-words models are witnessed.|$|R
50|$|The {{description}} {{above is}} representative of a receiver start-up situation. Most receivers have a track algorithm, sometimes called a tracker, that combines sets of satellite measurements collected at different times—in effect, {{taking advantage of the}} fact that successive receiver positions are usually close to each other. After a set of measurements are processed, the tracker predicts the receiver location corresponding to the next set of satellite measurements. When the new measurements are collected, the receiver uses a <b>weighting</b> <b>scheme</b> to combine the new measurements with the tracker prediction. In general, a tracker can (a) improve receiver position and time accuracy, (b) reject bad measurements, and (c) estimate receiver speed and direction.|$|E
5000|$|The {{sampling}} bias of Kashtan et al. [...] provided great impetus for designing better algorithms for the NM discovery problem. Although Kashtan et al. tried {{to settle this}} drawback {{by means of a}} <b>weighting</b> <b>scheme,</b> this method imposed an undesired overhead on the running time as well a more complicated implementation. This tool {{is one of the most}} useful ones, as it supports visual options and also is an efficient algorithm with respect to time. But, it has a limitation on motif size as it does not allow searching for motifs of size 9 or higher because of the way the tool is implemented.|$|E
50|$|Example of how over-bundling causes big {{problems}} (permutations {{and evaluation of}} total price in source selection): Lets say a requiring activity wants to get polling services. Acquisition planning reveals there are five polls in ten different regions. However, {{it turns out that}} the Government will only be ordering one of the five polls in any real numbers and that particular poll is much more expensive in actual cost than the other four. If a <b>weighting</b> <b>scheme</b> is not applied to this bundled requirement, a vendor can make the four lightly ordered polls very cheap in their offer and the high volume poll very expensive, based on their knowledge of the ordering patterns of the Government in past acquisitions. Thus, on its face, the overall price of a bid when each poll is added together to arrive at a total price (used in source selection) would look attractive but in practice, the Government will burn through its budget very quickly given the vast majority of the actually ordered polls are extremely expensive (even though the actual cost of the most frequently ordered poll is far less than what was in the offer). To avoid the headache of a <b>weighting</b> <b>scheme,</b> all five polls should be broken apart and contracted for separately so they can be judged on their merits. This is an example of what is frequently done on major indefinite duration, indefinite quantity (IDIQ) contracts and explains why some acquisitions are appallingly expensive and require additional funding to achieve the requiring activity's objectives.|$|E
40|$|This study proposes an {{adaptive}} algorithm for tracking objects {{based on the}} dynamic <b>weights</b> <b>scheme</b> and gaze density. The proposed adaptive algorithm is to amend the deficiency on tracking moving objects that are not driven by inertia. Using the dynamic <b>weights</b> <b>scheme</b> and the information of the gaze point density, possible areas where the objects may appear are constantly researched to target and trace the aimed objects. It is proven to be effective on tracking non-inertia driven moving objects...|$|R
40|$|Research {{evaluations}} {{based on}} quality weighted publication output are often criticized {{on account of}} the employed journal quality weights. This study shows that evaluations of entire research organizations are very robust with respect to the choice of readily available <b>weighting</b> <b>schemes.</b> We document this robustness by applying rather different <b>weighting</b> <b>schemes</b> to otherwise identical rankings. Our unit of analysis consists of German, Austrian and Swiss university departments in business administration and economics. Research evaluation, university management...|$|R
40|$|Recommender systems apply machine {{learning}} and data mining techniques for filtering unseen information and can predict whether a user {{would like a}} given resource. To date, a number of recommender system algorithms have been proposed, where collaborative filtering is the most famous and adopted recommendation algorithm. Collaborative filtering recommender systems recommend items by identifying other similar users, in case of user-based collaborative filtering, or similar items, in case of item-based collaborative filtering. Significance <b>weighting</b> <b>schemes</b> assign different <b>weights</b> to neighbouring users/items found against an active user/item. Several significance <b>weighting</b> <b>schemes</b> have been proposed [1], [2], [3], [4]. In this paper, we claim that these proposed schemes are flawed {{by the fact that}} they can not be applied to general recommender system datasets. We provide the correct generalized significance <b>weighting</b> <b>schemes</b> using different novel heuristics, and by extensive experimental results on three different data sets, show how significance <b>weighting</b> <b>schemes</b> affect the performance of a recommender system. Furthermore, we claim that the conventional weighted sum prediction formula used in item-based [5] collaborative filtering is not correct for very sparse datasets. We provide the correct prediction formula and empirically evaluate it...|$|R
