4|11|Public
50|$|A {{personal}} trademark is {{that she}} always runs wearing long <b>white</b> <b>compression</b> socks.|$|E
40|$|Abstract: Many {{established}} and emerging image processing applications rely on quantum-limited imaging, i. e., imaging in extremely poor illumination. At this, images are corrupted by severe signal-dependent Poisson noise. For optimal noise reduction the noise characteristics must be estimated and {{integrated into the}} method. Common noise estimators, however, assume Gaussian noise which is not signal-dependent. In this paper, we describe the modeling process exemplarily for low-dose medical X-ray imaging. In this context, we formulate functional models for detector images and images which have undergone nonlinear <b>white</b> <b>compression</b> prior to further processing. Furthermore, we present a robust estimator for signal-dependent noise suited for real-time applications. Key–Words: Quantum noise, noise estimation, X-ray imaging, <b>white</b> <b>compression...</b>|$|E
40|$|Fluoroscopic images exhibit severe signal-dependent quantum noise, due to {{the reduced}} X-ray dose {{involved}} in image formation, that is generally modelled as Poisson-distributed. However, image gray-level transformations, commonly applied by fluoroscopic device to enhance contrast, modify the noise statistics {{and the relationship between}} image noise variance and expected pixel intensity. Image denoising is essential to improve quality of fluoroscopic images and their clinical information content. Simple average filters are commonly employed in real-time processing, but they tend to blur edges and details. An extensive comparison of advanced denoising algorithms specifically designed for both signal-dependent noise (AAS, BM 3 Dc, HHM, TLS) and independent additive noise (AV, BM 3 D, K-SVD) was presented. Simulated test images degraded by various levels of Poisson quantum noise and real clinical fluoroscopic images were considered. Typical gray-level transformations (e. g. <b>white</b> <b>compression)</b> were also applied in order to evaluate their effect on the denoising algorithms. Performances of the algorithms were evaluated in terms of peak-signal-to-noise ratio (PSNR), signal-to-noise ratio (SNR), mean square error (MSE), structural similarity index (SSIM) and computational time. On average, the filters designed for signal-dependent noise provided better image restorations than those assuming additive white Gaussian noise (AWGN). Collaborative denoising strategy was found to be the most effective in denoising of both simulated and real data, also in the presence of image gray-level transformations. <b>White</b> <b>compression,</b> by inherently reducing the greater noise variance of brighter pixels, appeared to support denoising algorithms in performing more effectively. © 2012 Elsevier Ltd. All rights reserve...|$|E
40|$|Rapid {{development}} of DNA sequencing technologies exponentially increases {{the amount of}} publicly available genomic data. Whole genome multiple sequence alignments represent a particularly voluminous, frequently downloaded static dataset. In this work we propose an asymmetric source coding scheme for such alignments using evolutionary prediction in combination with lossless black and <b>white</b> image <b>compression.</b> Compared to the Lempel-Ziv algorithm used so far the compression rates are almost halved. ...|$|R
40|$|Image {{compression}} algorithms {{have evolved}} {{into some of}} the most complex structures in computing. Yet at the core of these algorithms are simple techniques that have been long known about data compression. This paper explores some of the fundamental techniques behind bi-level (black and <b>white)</b> image <b>compression</b> that have set the bar for much of the compression standards in existence today. More specifically, the paper focuses on compression techniques developed for the fax machine, for that development marked {{a turning point in the}} standardization of image compression techniques...|$|R
40|$|Abstract. In {{this paper}} we extend the {{recently}} proposed DCT-mod 2 feature extraction technique (which utilizes polynomial coefficients derived from 2 D DCT coefficients obtained from horizontally & vertically neighbouring blocks) via {{the use of}} various windows and diagonally neighbouring blocks. We also propose enhanced PCA, where traditional PCA feature extraction is combined with DCT-mod 2. Results using test images corrupted by a linear and a non-linear illumination change, white Gaussian noise and compression artefacts, show that use of diagonally neighbouring blocks and windowing is detrimental to robustness against illumination changes while being useful for increasing robustness against <b>white</b> noise and <b>compression</b> artefacts. We also show that the enhanced PCA technique retains all {{the positive aspects of}} traditional PCA (that is, robustness against <b>white</b> noise and <b>compression</b> artefacts) while also being robust to illumination changes; moreover, enhanced PCA outperforms PCA with histogram equalisation pre-processing. ...|$|R
40|$|Background The working posture {{affects the}} {{peripheral}} venous circulation, although the current literature does not report any correlation between working posture and the abnormalities of the jugular veins flow. The {{purpose of this}} preliminary research is to study, in female workers, the prevalence of Venous Compressive Syndrome (VCS) caused by total block of the internal jugular veins flow, so-called “White Compression”. Due to complete compression by postural, muscular, fascial, anatomical or bone anomalies, <b>White</b> <b>Compression</b> is not visible by EchoColorDoppler (ECD) and its flow can only be detected by the rotational movements of the head or by Valsalva’s maneuver. Methods We studied a sample of female workers with ECD (n= 128), in supine and upright position divided into subgroups according to the obliged posture maintained during working hours: group A, seated work (n = 61; 47. 7...|$|E
40|$|A {{nonlinear}} {{mathematical model}} for the human visual system (HVS) is selected as a preprocessing stage for monochrome and color digital image compression. Rate distortion curves and derived power spectra are used to develop coding algorithms in the preprocessed "perceptual space. " Black and <b>white</b> image <b>compressions</b> down to. 1 bit per pixel are obtained. In addition, color images are compressed to 1 bit per pixel (1 / 3 bit per pixel per color) with less than 1 % mean square error and no visible degradations. Minor distor-tions are incurred with compressions down to 1 / 4 bit per pixel (1 / 12 bit per pixel per color). Thus, {{it appears that the}} perceptual power spectrum coding technique "puts " the noise where one can not see it. The result is bit rates up to an order of magnitude lower than those previously obtained with comparable quality...|$|R
30|$|Although the {{robustness}} of this steganalysis algorithm is not ideal when speech is attacked by Gaussian white noise or G. 729 compression encoding. But in the practical application, the probability is low for the blended speech {{to be attacked}} by strong <b>white</b> noise or <b>compression</b> encoding. There are two main reasons. First, the Internet channel has a high-quality. Second, the hidden capacity of the blending-based speech hiding algorithm is high, which results that the blended speech has a low compressibility. In order to extract the secret speech correctly, for {{both sides of the}} covert communication, it is less likely to transmit the compression blended speech.|$|R
40|$|Intrinsic {{nonlinearity}} {{complicates the}} modeling of perceived quality of digital images, especially when using feature-based objective methods. The research {{described in this}} paper indicates that models from Computational Intelligence can predict quality and cope with multi-dimensional data characterized by complex perceptual relationships. A reduced-reference scheme exploits Support Vector Machines (SVMs) to assess the degradation in perceived image quality induced by three different distortion types: JPEG <b>compression,</b> <b>white</b> noise, and Gaussian blur. First, an objective description of the images is obtained by exploiting the co-occurrence matrix and its features; then, the SVM supports the nonlinear mapping between the objective description and the quality evaluation. Experimental results confirm {{the validity of the}} approac...|$|R
40|$|Lung {{injuries}} are common {{among those who}} suffer an impact or trauma. The relative severity of injuries up to physical tearing of tissue have been documented in clinical studies. However, the specific details of energy required to cause visible damage to the lung parenchyma are lacking. Furthermore, the limitations of lung tissue under simple mechanical loading are also not well documented. This study aimed to collect mechanical test data from freshly excised lung, obtained from both Sprague-Dawley rats and New Zealand <b>White</b> rabbits. <b>Compression</b> and tension tests were conducted at three different strain rates: 0. 25, 2. 5 and 25 [*]min− 1. This study aimed to characterise the quasi-static behaviour of the bulk tissue prior to extending to higher rates. A nonlinear viscoelastic analytical model {{was applied to the}} data to describe their behaviour. Results exhibited asymmetry in terms of differences between tension and compression. The rabbit tissue also appeared to exhibit stronger viscous behaviour than the rat tissue. As a narrow strain rate band is explored here, no conclusions are being drawn currently regarding the rate sensitivity of rat tissue. However, this study does highlight both the clear differences between the two tissue types and the important role that composition and microstructure can play in mechanical response...|$|R
5000|$|G4 {{offers a}} small {{improvement}} over G3-2D {{by removing the}} end of line (EOL) codes. G3 and G4 compression both treat an image {{as a series of}} horizontal black strips on a <b>white</b> page. Better <b>compression</b> is achieved when there are fewer unique black dots/lines on the page. Both G3-2D and G4 add a two dimensional feature to achieve greater compression by taking advantage of vertical symmetry. A worst-case image would be an alternating pattern of single-pixel black and white dots offset by one pixel on even/odd lines. G4 compression would actually increase the file size on this type of image. G4 typically achieves a 20:1 compression ratio. For an 8.5"×11" [...] page scanned at 200 DPI, this equates to a reduction from 467.5 kB to 23.4 kB (95% compression ratio).|$|R
40|$|This paper {{presents}} a composite blind digital watermarking technique, CompMark, {{to hide a}} visually meaningful grayscale logo in a host image. The multi-resolution fusion principles are used to embed the grayscale logo in perceptually significant blocks in wavelet subbands of the host image. A modulus approach is further used to embed a binary counterpart of the logo in the approximation subband. The extraction process combines both extracted grayscale and binary logos to obtain a better, recognizable logo at the receiver. The watermark detection decision is based on either {{the correlation between the}} thresholded extracted and embedded logos or the visual similarity. Experimental results demonstrate our scheme is robust against <b>compression,</b> <b>white</b> noise addition, histogram equalization, and image filtering techniques. In addition, it performs better than a peer blind scheme, XFuseMark. Index Terms — Grayscale logo watermarking, modulus embedding, additive fusion principle...|$|R
40|$|Context. With the {{increasing}} demand for image processing applications in multimedia applications, the importance for research on image quality assessment subject has received great interest. While the goal of Image Quality Assessment {{is to find the}} efficient Image Quality Metrics that are closely relative to human visual perception, from the last three decades much effort has been put by the researchers and numerous papers and literature has been developed with emerging Image Quality Assessment techniques. In this regard, emphasis is given to Full-Reference Image Quality Assessment research where analysis of quality measurement algorithms is done based on the referenced original image as that is much closer to perceptual visual quality. Objectives. In this thesis we investigate five mostly used Image Quality Metrics which were selected (which includes Peak Signal to Noise Ratio (PSNR), Structural SIMilarity Index (SSIM), Feature SIMilarity Index (FSIM), Visual Saliency Index (VSI), Universal Quality Index (UQI)) to perform an experiment on a chosen image dataset (of images with different types of distortions due to different image processing applications) and find the most efficient one with respect to the dataset used. This research analysis could possibly be helpful to researchers working on big image data projects where selection of an appropriate Image Quality Metric is of major significance. Our study details the use of dataset taken and the experimental results where the image set highly influences the results.  Methods. The goal of this study is achieved by conducting a Literature Review to investigate the existing Image Quality Assessment research and Image Quality Metrics and by performing an experiment. The image dataset used in the experiment is prepared by obtaining the database from LIVE Image Quality Assessment database. Matlab software engine was used to experiment for image processing applications. Descriptive analysis (includes statistical analysis) was employed to analyze the results obtained from the experiment. Results. For the distortion types involved (JPEG 2000, JPEG <b>compression,</b> <b>White</b> Gaussian Noise, Gaussian Blur) SSIM was efficient to measure the image quality after distortion for JPEG 2000 compressed and white Gaussian noise images and PSNR was efficient for JPEG compression and Gaussian blur images with respect to the original image.  Conclusions. From this study it is evident that SSIM and PSNR are efficient in Image Quality Assessment for the dataset used. Also, that the level of distortions in the image dataset highly influences the results, where in our case SSIM and PSNR perform efficiently for the used database. ...|$|R

