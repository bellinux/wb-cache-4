552|2490|Public
5|$|<b>Web</b> <b>document</b> markup {{languages}} frequently use URI {{references to}} point to other resources, such as external documents or specific portions of the same logical document.|$|E
25|$|In {{areas of}} {{language}} modeling, the Web {{has been used}} to address data sparseness. Lexical statistics have been gathered for resolving prepositional phrase attachments, while <b>Web</b> <b>document</b> were used to seek a balance in the corpus.|$|E
50|$|In general, Annotea {{associates}} text strings to a <b>web</b> <b>document</b> or selected {{parts of}} a <b>web</b> <b>document</b> without actually needing to modify the original document.|$|E
40|$|To {{overcome}} {{the limitations of}} conventional Web search engines in retrieving <b>Web</b> <b>documents</b> relevant to users' queries, one has to exploit semantic structures embedded in <b>Web</b> <b>documents.</b> We propose a Web Information Retrieval (WebIR) model for <b>Web</b> <b>documents</b> containing semantic elements which are text segments enclosed by special tags. These special tags, known as semantic tags, can either be independently created for individual <b>Web</b> <b>documents,</b> or be standardized for a collection of <b>Web</b> <b>documents</b> sharing common types of semantic elements. The WebIR model supports queries on both intra-document semantic elements and inter-document links, and returns directed graphs as query results. Each directed graph represents a cluster of connected <b>Web</b> <b>documents</b> satisfying the given query. The collection of directed graphs in a WebIR query result is further ranked. In this paper, we describe the WebIR model and an ongoing implementation effort to realize the model. 1 Introduction In recent years, the [...] ...|$|R
40|$|The {{automatic}} categorisation of <b>web</b> <b>documents</b> is be-coming {{crucial for}} organising the {{huge amount of}} infor-mation available in the Internet. We are facing a new challenge {{due to the fact}} that <b>web</b> <b>documents</b> have a rich structure and are highly heterogeneous. Two ways to respond to this challenge are (1) using a represen-tation of the content of <b>web</b> <b>documents</b> that captures these two characteristics and (2) using more eective classiers. Our categorisation approach is based on a probabilistic description-oriented representation of <b>web</b> <b>documents,</b> and a probabilistic interpretation of the k-nearest neigh-bour classier. With the former, we provide an en-hanced document representation that incorporates the structural and heterogeneous nature of <b>web</b> <b>documents.</b> With the latter, we provide a theoretical sound justi-cation for the various parameters of the k-nearest neigh-bour classier. Experimental results show that (1) using an enhanced representation of <b>web</b> <b>documents</b> is crucial for an eec-tive categorisation of <b>web</b> <b>documents,</b> and (2) a theo-retical interpretation of the k-nearest neighbour classi-er gives us improvement over the standard k-nearest neighbour classier. ¤This work has been carried out in the framework of the Eu-roSearch project, LE 4 - 8303...|$|R
40|$|The World Wide Web has {{huge amount}} of {{information}} that is retrieved using information retrieval tool like Search Engine. Page repository of Search Engine contains the <b>web</b> <b>documents</b> downloaded by the crawler. This repository contains variety of <b>web</b> <b>documents</b> from different domains. In this paper, a technique called “Retrieval of <b>Web</b> <b>documents</b> using a fuzzy hierarchical clustering ” is being proposed that creates the clusters of we...|$|R
5000|$|... #Caption: The {{unclassified}} [...] "Military Working Dogs" [...] <b>web</b> <b>document,</b> marked Distribution Restricted circa 2011 ...|$|E
50|$|The W3C's Cool URIs for the Semantic <b>Web</b> <b>document</b> {{recommends}} {{using one}} or other {{of these two}} methods, depending on {{the requirements of the}} project.|$|E
50|$|<b>Web</b> <b>document</b> markup {{languages}} frequently use URI {{references to}} point to other resources, such as external documents or specific portions of the same logical document.|$|E
25|$|The {{template}} {{and content}} resources are processed and combined by the template engine to mass-produce <b>web</b> <b>documents.</b> For {{purposes of this}} article, <b>web</b> <b>documents</b> include any of various output formats for transmission over the web via HTTP, or another Internet protocol.|$|R
50|$|Inserting {{music into}} <b>web</b> <b>documents.</b>|$|R
50|$|Users {{that access}} any <b>web</b> <b>documents</b> can also load the {{metadata}} {{associated with it}} from a selected annotation server (or groups of servers) and see a peer group's comments on the document. Similarly shared metadata tags can be attached to <b>web</b> <b>documents</b> to help in future retrieval.|$|R
50|$|In {{areas of}} {{language}} modeling, the Web {{has been used}} to address data sparseness. Lexical statistics have been gathered for resolving prepositional phrase attachments, while <b>Web</b> <b>document</b> were used to seek a balance in the corpus.|$|E
50|$|Templates are {{processed}} by a template processor, a template engine which produces a <b>web</b> <b>document</b> and a style sheet used for page {{layout of the}} document. This enables {{the design of the}} template to be separated from the content it manipulates.|$|E
50|$|Document {{clustering}} {{involves the}} use of descriptors and descriptor extraction. Descriptors are sets of words that describe the contents within the cluster. Document clustering is generally considered to be a centralized process. Examples of document clustering include <b>web</b> <b>document</b> clustering for search users.|$|E
40|$|Internet {{which is}} {{included}} plenty of huge data source is now rapidly increasing in all domains. It is considered as valuable data sources if the {{data can be}} processed that results in information. Data mining techniques are widely utilized in <b>web</b> <b>documents</b> in order to extract information. In this paper a data mining approach based on Ontology is proposed to classify <b>web</b> <b>documents</b> {{in order to facilitate}} applications based on classified text documents like search engines. The proposed approach is implemented and applied on several <b>web</b> <b>documents.</b> The experimental results show considerable progress...|$|R
40|$|In {{this paper}} we {{investigate}} {{the applicability of}} on-line learning algorithms to the real-world problem of web search. Consider that <b>web</b> <b>documents</b> are indexed using Boolean features. We first present a practically efficient on-line learning algorithm TW 2 to search for <b>web</b> <b>documents</b> represented relevant features. We then design and implement WebSail, a real-time adaptive web search learner, with TW 2 as its learning component. WebSail learns from the user's relevance feedback in real-time and helps the user {{to search for the}} desired <b>web</b> <b>documents.</b> The architecture and performance of WebSail are also discussed. 1...|$|R
40|$|The {{semantic}} web {{will not be}} a static collection of formats, data and meta-data but highly dynamic in each aspect. This paper proposes a personalized event notification system for semantic <b>web</b> <b>documents</b> (ENS-SW). The system can intelligently detect and filter changes in semantic <b>web</b> <b>documents</b> by exploiting the semantic structure of those documents. In our prototype, we combine the functionalities of user profiles and distributed authoring systems. Typically, both approaches would lack the ability to handle semantic <b>web</b> <b>documents.</b> This paper introduces the design and implementation of our event notification system for semantic <b>web</b> <b>documents</b> that handles the XML representation of RDF. We analyzed our prototype regarding accuracy and efficiency in change detection. Our system supports sophisticated change detection including partial deletion, awareness for document restructuring, and approximate filter matches...|$|R
50|$|In 1996 {{he founded}} Stigma OnLine. The company built {{one of the}} first Intranet {{products}} in the market, SolWeb Intra, a <b>web</b> <b>document</b> management product which became the backbone for the Intranet of Kraft, Novartis, the Italian Broadcasting Television and the Italian Stock Exchange.|$|E
5000|$|For <b>web</b> <b>document</b> retrieval, if the user's {{objectives}} are not clear, the precision and recall can't be optimized. As summarized by Lopresti, Browsing is a comfortable and powerful paradigm (the serendipity effect).* Search results don't {{have to be}} very good.* Recall? Not important (as long as you get at least some good hits).* Precision? Not important (as long as {{at least some of the}} hits on the first page you return are good).|$|E
50|$|The Curl {{language}} {{attempts to}} address a long-standing problem: the different building blocks that make up any modern <b>web</b> <b>document</b> most often require wildly different methods of implementation: different languages, different tools, different frameworks, often completely different teams. The final — and often most difficult — hurdle has been getting all of these blocks {{to communicate with each}} other in a consistent manner. Curl attempts to side-step these problems by providing a consistent syntactic and semantic interface at all levels of web content creation: from simple HTML to complex object-oriented programming.|$|E
40|$|Abstract. Many <b>Web</b> <b>documents</b> such as HTML {{files and}} XML files have no rigid {{structure}} and are called semistructured data. In general, such semistructured <b>Web</b> <b>documents</b> {{are represented by}} rooted trees with ordered children. We propose a new method for discovering frequent tree structured patterns in semistructured <b>Web</b> <b>documents</b> by using a tag tree pattern as a hypothesis. A tag tree pattern is an edge labeled tree with ordered children which has structured variables. An edge label is a tag or a keyword in such <b>Web</b> <b>documents,</b> and a variable can be substituted by an arbitrary tree. So a tag tree pattern is suited for representing tree structured patterns in such <b>Web</b> <b>documents.</b> First we show {{that it is hard}} to compute the optimum frequent tag tree pattern. So we present an algorithm for generating all maximally frequent tag tree patterns and give the correctness of it. Finally, we report some experimental results on our algorithm. Although this algorithm is not efficient, experiments show that we can extract characteristic tree structured patterns in those data. ...|$|R
40|$|The {{number of}} World-Wide <b>Web</b> (WWW) <b>documents</b> {{available}} to {{users of the}} Internet is growing at an incredible rate. Therefore, {{it is becoming increasingly}} important to develop systems that aid users in searching, filtering, and retrieving information from the Internet. Currently, only a few prototype systems catalog and index images in <b>Web</b> <b>documents.</b> To greatly improve the cataloging and indexing of images on the Web, we have developed a prototype rule-based system that detects the content images in <b>Web</b> <b>documents.</b> Content images are images that are associated with the main content of <b>Web</b> <b>documents,</b> as opposed to a multitude of other images that exist in <b>Web</b> <b>documents</b> for different purposes, such as decorative, advertisement and logo images. We present a system that uses decision tree learning for automated rule induction for the content image detection system. The system uses visual features, text-related features and the document context of images in concert for fast and effective conte [...] ...|$|R
40|$|<b>Web</b> <b>documents</b> present new {{challenges}} to conventional Information Retrieval (IR) technologies. This paper describes how these challenges are faced in FameIR, a multilingual multimedia IR shell. In this shell Cross-Language IR (CLIR) and query expansion are performed using EuroWordNet (EWN), the best developed and {{most widely used}} lexical resource for several languages. Techniques to extract information from <b>Web</b> <b>documents,</b> Wrapper Generation (WG) techniques, are used to access a finer information granularity than the whole Web page. By combining IR and WG techniques {{with the use of}} EWN, FameIR provides a powerful facility to perform CLIR from multimedia <b>Web</b> <b>documents.</b> 1...|$|R
5000|$|Haml (HTML Abstraction Markup Language) is a {{templating}} {{system that}} is designed to avoid writing inline code in a <b>web</b> <b>document</b> and make the HTML easy and clean. Haml gives the flexibility to have some dynamic content in HTML. Similar to other web languages like PHP, ASP, JSP and template systems like eRuby, Haml also embeds some code that gets executed during runtime and generates HTML code in order to provide some dynamic content. In order to run Haml code, files need to have [...]haml extension. These files are similar to [...]erb or eRuby files which also help to embed Ruby code while developing a web application.|$|E
50|$|On Wikipedia, an infobox is {{transcluded}} into {{an article}} by enclosing its name and attribute-value pairs within a double set of braces. The MediaWiki software on which Wikipedia operates then parses the document, for which the infobox and other templates are processed by a template processor. This is a template engine which produces a <b>web</b> <b>document</b> and a style sheet used for presentation of the document. This enables {{the design of the}} infobox to be separated from the content it manipulates; that is, the design of the template may be updated without affecting the information within it, and the new design will automatically propagate to all articles that transclude the infobox. Usually, infoboxes are formatted to appear in the top-right corner of a Wikipedia article in the desktop view, or at the top in the mobile view.|$|E
40|$|In {{this work}} several {{semantic}} approaches to concept-based query expansion and re-ranking schemes are studied and compared with different ontology-based expansion methods in <b>web</b> <b>document</b> search and retrieval. In particular, {{we focus on}} concept-based query expansion schemes where, in order to effectively increase the precision of <b>web</b> <b>document</b> retrieval and to decrease the users' browsing time, the main goal is to quickly provide users with the most suitable query expansion. Two key tasks for query expansion in <b>web</b> <b>document</b> retrieval are to find the expansion candidates, as the closest concepts in <b>web</b> <b>document</b> domain, and to rank the expanded queries properly. The approach we propose aims at improving the expansion phase for better <b>web</b> <b>document</b> retrieval and precision. The basic idea is to measure the distance between candidate concepts using the PMING distance, a collaborative semantic proximity measure, i. e. a measure which can be computed using statistical results from a web search engine. Experiments show that the proposed technique can provide users with more satisfying expansion results and {{improve the quality of}} <b>web</b> <b>document</b> retrieval...|$|E
40|$|ABSTRACT In {{this paper}} we present HearSay, {{a system for}} {{browsing}} hyper-text <b>Web</b> <b>documents</b> via audio. The HearSay system is based on our novel approach to automatically creating audio browsable con-tent from hypertext <b>Web</b> <b>documents.</b> It combines two key technologies: (1) automatic partitioning of <b>Web</b> <b>documents</b> throughtightly coupled structural and semantic analysis, which transforms raw HTML documents into semantic structures so as to facilitateaudio browsing; and (2) VoiceXML, an already standardized technology which we adopt to represent voice dialogs automaticallycreated from the XML output of partitioning. This paper describes the software components of HearSay and presents an initial systemevaluation...|$|R
40|$|In {{this paper}} we present Deiush, a {{multimodal}} system for browsing hypertext <b>Web</b> <b>documents.</b> The Deiush system {{is based on}} our novel approach to automatically annotate hypertext <b>Web</b> <b>documents</b> (i. e. XHTML pages) with browsable audio components. It combines two key technologies: (1) middleware automatic separation of <b>Web</b> <b>documents</b> through structural and semantic analysis which is annotated with audio components, transforming them into XHTML+VoiceXML format to represent multimodal dialog; and (2) Opera Browser, an already standardized browser which we adopt as an interface of the XHTML+VoiceXML output of annotating. This paper describes the annotation technology of Deiush and presents an initial system evaluation...|$|R
40|$|In {{this paper}} we present HearSay, {{a system for}} {{browsing}} hypertext <b>Web</b> <b>documents</b> via audio. The HearSay system is based on our novel approach to automatically creating audio browsable content from hypertext <b>Web</b> <b>documents.</b> It combines two key technologies: (1) automatic partitioning of <b>Web</b> <b>documents</b> through tightly coupled structural and semantic analysis, which transforms raw HTML documents into semantic structures so as to facilitate audio browsing; and (2) VoiceXML, an already standardized technology which we adopt to represent voice dialogs automatically created from the XML output of partitioning. This paper describes the software components of HearSay and presents an initial system evaluation...|$|R
40|$|Exponential {{growth of}} the web {{increased}} the importance of <b>web</b> <b>document</b> classification and data mining. To get the exact information, {{in the form of}} knowing what classes a <b>web</b> <b>document</b> belongs to, is expensive. Automatic classification of <b>web</b> <b>document</b> is of great use to search engines which provides this information at a low cost. In this paper, we propose an approach for classifying the <b>web</b> <b>document</b> using the frequent item word sets generated by the Frequent Pattern (FP) Growth which is an association analysis technique of data mining. These set of associated words act as feature set. The final classification obtained after Naïve Bayes classifier used on the feature set. For the experimental work, we use Gensim package, as it is simple and robust. Results show that our approach can be effectively classifying the <b>web</b> <b>document.</b> Comment: 9 Page...|$|E
40|$|Abstract- Exponential {{growth of}} the web {{increased}} the importance of <b>web</b> <b>document</b> classification and data mining. To get the exact information, {{in the form of}} knowing what classes a <b>web</b> <b>document</b> belongs to, is expensive. Automatic classification of <b>web</b> <b>document</b> is of great use to search engines which provides this information at a low cost. In this paper, we propose an approach for classifying the <b>web</b> <b>document</b> using the frequent item word sets generated by the Frequent Pattern (FP) Growth which is an association analysis technique of data mining. These set of associated words act as feature set. The final classification obtained after Naïve Bayes classifier used on the feature set. For the experimental work, we use Gensim package, as it is simple and robust. Results show that our approach can be effectively classifying the <b>web</b> <b>document.</b> Keywords- Classification, FP-growth, Gensim, Naïve Bayes, Vector space model I...|$|E
40|$|Abstract—In {{order to}} save time in extracting {{specific}} information from high volume of data in Thai <b>Web</b> <b>document,</b> this paper proposes a Thai <b>Web</b> <b>document</b> classification system by using centroid and neural network classifiers. The initial training sets in this experiment consist of five categories (100 Web for eac...|$|E
3000|$|... ● Keyword Extractor: The module is {{responsible}} to extract semantic of content in <b>Web</b> <b>documents.</b> Based on keyword extraction methods, keywords reflecting certain concepts can be extracted from the content of <b>Web</b> <b>documents.</b> In other words, the module is the extractor of object resources and performing the Step 3 to Step 5 of the proposed mechanism.|$|R
40|$|Web Graph is a directed, unlabeled graph EVG,, which {{represents}} connections between <b>web</b> <b>documents</b> on the Internet. It {{consists of two}} sets: V – vertices (nodes) for individual documents, and E – edges between nodes, i. e., the links between <b>web</b> <b>documents.</b> The complete <b>Web</b> Graph is a huge structure. As of May 2011, it is estimate...|$|R
40|$|Web users rely on World Wide Web search engines, such as Yahoo! and AltaVista, to {{retrieve}} <b>Web</b> <b>documents</b> of interest. Whether {{a search engine}} provides categories for a user to click on or a query facility for a user to type in keywords, the <b>Web</b> <b>documents</b> retrieved still suffer from poor precision (i. e., too many irrelevant documents ar...|$|R
