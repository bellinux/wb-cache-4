18|54|Public
5000|$|Knight's Spider <b>Web</b> <b>Farm</b> {{is an art}} studio, {{retail outlet}} and {{residence}} owned by Will Knight, an artist who uses spider webs to produce art. It is located in Williamstown, Vermont. Knight calls his farm [...] "The Original Web Site". [...] The finished artwork uses whole spider webs {{as a form of}} textile art over painted or stained wood.|$|E
50|$|An {{application}} delivery controller (ADC) is a computer network device in a datacenter, often part of an {{application delivery}} network (ADN), that helps perform common tasks, such as those done by web sites to remove load from the web servers themselves. Many also provide load balancing. ADCs are often placed in the DMZ, between the outer firewall or router and a <b>web</b> <b>farm.</b>|$|E
50|$|Server {{farms are}} {{commonly}} used for cluster computing. Many modern supercomputers comprise giant server farms of high-speed processors connected by either Gigabit Ethernet or custom interconnects such as Infiniband or Myrinet. Web hosting is a common use of a server farm; such a system is sometimes collectively {{referred to as a}} <b>web</b> <b>farm.</b> Other uses of server farms include scientific simulations (such as computational fluid dynamics) and the rendering of 3D computer generated imagery (also see render farm).|$|E
40|$|Web hosting service, <b>web</b> server <b>farm,</b> <b>web</b> server cluster, load balancing, scalability, {{performance}} analysis, SpecWeb Web hosting is {{an infrastructure}} service that allows to design, integrate, {{operate and maintain}} all of the infrastructure components required to run web-based applications. It includes <b>Web</b> server <b>farms,</b> network access, data staging tools and security firewalls. <b>Web</b> server <b>farms</b> are used in a Web hosting infrastructure {{as a way to}} create scalable and highly availabl...|$|R
40|$|The {{widespread}} use of clusters and <b>web</b> <b>farms</b> has increased the importance of data replication. In this paper, we show how to implement consistent and scalable data replication at the middleware level. We do this by combining transactional concurrency control with group communication primitives. The paper presents different replication protocols, argues their correctness, describes their implementation {{as part of a}} generic middleware tool, and proves their feasibility with an extensive performance evaluation. The solution proposed is well suited for a variety of applications including <b>web</b> <b>farms</b> and distributed object platforms...|$|R
40|$|In a {{commercial}} website or portal, Web information fusion is usually {{from the following}} two approaches, one is to integrate the Web content, structure, and usage data for surfing behavior analysis; {{the other is to}} integrate Web usage data with traditional customer, product, and transaction data for purchasing behavior analysis. In this paper, we propose a unified model based on <b>Web</b> <b>farming</b> technology for collecting clickstream logs in the whole user interaction process. We emphasize that collecting clickstream logs at the application layer will help to seamlessly integrate Web usage data with other customer-related data sources. In this paper, we extend the Web log standard to modeling clickstream format and Web mining to <b>Web</b> <b>farming</b> from passively collecting data and analyzing the customer behavior to actively influence the customer's decision making. The proposed model can be developed as a common plugin for most existing commercial websites and portals. <b>Web</b> <b>farming,</b> clickstream log, <b>Web</b> information fusion, e-Business portal...|$|R
50|$|Another {{solution}} {{is to keep the}} per-session data in a database. Generally this is bad for performance because it increases the load on the database: the database is best used to store information less transient than per-session data. To prevent a database from becoming a single point of failure, and to improve scalability, the database is often replicated across multiple machines, and load balancing is used to spread the query load across those replicas. Microsoft's ASP.net State Server technology {{is an example of a}} session database. All servers in a <b>web</b> <b>farm</b> store their session data on State Server and any server in the farm can retrieve the data.|$|E
50|$|The {{first to}} utilize Layer 7 load {{balancing}} to scale. At the time, round-robin DNS with weighted nodes {{to account for}} servers with more or less power or connectivity {{was the only way}} to scale a <b>web</b> <b>farm.</b> CNNfn was working closely with Netscape by also helping the write portions of their software which was incorporated into their 2.x product line. Round robin was not fast enough at taking a server out of rotation when Netscape would crash. Thus CNNfn worked with a few vendors to implement a Layer 7 solution that would instantly know when a server was up or down. This dramatically increased the speed and quality of the CNNfn and CNN websites.|$|E
5000|$|WSE 3.0 was {{released}} in October 2005 and has design time support with Visual Studio 2005. It includes policy framework enhancements including security based on policy assertions (associating CLR client proxies with policy files), turnkey security scenarios for securing end to end messages, extensibility mechanisms for user-defined policies in code and a simplified policy model applied to a message exchange instead of on a per-message level. It supports updated web services specifications and a native 64-bit runtime. WS-SecureConversation sessions can be canceled explicitly and sessions are reliable and usable in <b>web</b> <b>farm</b> scenarios as Security Context Tokens can contain the original client authentication token when sent from the client to the service, which enable sessions to be re-established if lost, e.g. when a service's appdomain is reset. WSE 3.0 is wire-level interoperable over HTTP with Windows Communication Foundation (WCF) and supports the same version of the WS-* specifications as WCF (WS-Security 1.1, SOAP 1.2, MTOM).|$|E
40|$|Abstract: The {{evolution}} of the eFinance Market presents new and changing requirements on <b>web</b> server <b>farm</b> architecture. <b>Web</b> server <b>farms</b> built during the boom years of the web were designed to provide service to {{a very large number}} of users, where each request however placed relatively little load on the system. Further, the requests displayed a large amount of statistical similarity, so that caching mechanisms could be successfully applied. However, as professional eFinance tools migrate to web-based ASP technology, and as different companies cooperate to build these tools using the Web Services paradigm, these basic assumptions no longer hold: The number of users decreases, while the demands placed by each user increases. Further, each user places highly specialized demands on the application, decreasing the similarities between different requests. The success of conventional caching techniques drops even further if the <b>web</b> server <b>farm</b> is now called upon to provide XML-based Web Services instead of HTML web pages. This paper analyzes these new requirements on the basis of a case study: We present the design process behind the addition of an On The Fly Calculation Server to IS Innovative Software's <b>Web</b> Server <b>Farm</b> architecture. This new server has been developed to provide advanced financial (MPT) calculations to professional users. The resulting design represents a form of intelligent memory, optimized to minimize the movement of data and thus request latency. The stages in the development of this new server, and the integration of this server into the existing <b>web</b> server <b>farm,</b> are presented...|$|R
40|$|Web hosting is an {{infrastructure}} service that allows to design, integrate, {{operate and maintain}} all of the infrastructure components required to run web-based applications. It includes <b>Web</b> server <b>farms,</b> network access, data staging tools and security rewalls. <b>Web</b> server <b>farms</b> are used in a Web hosting infrastructure {{as a way to}} create scalable and highly available solutions. One of the main problems in <b>web</b> server <b>farm</b> management iscontent management and load balancing. In this paper, we analyze several hardware/software solutions on the market and demonstrate their scalability problems. We outline a new scalable solution FLEX for design and management of an e cient Web hosting service. This solution can be applied to a Web hosting service implemented on di erent architecture platforms such asweb server farms with replicated disk content, web server clusters having access to a shared le system or multi-computer systems using a global (shared) le system. A preliminary performance analysis provides a comparison of the current solutions and FLEX using a synthetic workload generator based on SpecWeb' 96 benchmark...|$|R
5000|$|Kern, Louis (1993). The Farm Midwives. Retrieved February 1, 2008, from The <b>Farm</b> <b>Web</b> site ...|$|R
40|$|A {{common problem}} that sales {{consultants}} {{face in the}} field is the selection of an appropriate hardware and software configuration for web farms. Over-provisioning means that the tender will be expensive while under-provisioning will lead to a configuration that does not meet the customer criteria. Indy is a performance modeling environment which allows developers to create custom modeling applications. We have constructed an Indy-based application for defining <b>web</b> <b>farm</b> workloads and topologies. The paper presents an optimization framework that allows the consultant to easily find configurations that meet customers’ criteria. The system searches the solution space creating possible configurations, using the <b>web</b> <b>farm</b> models to predict their performance. The optimization tool is then employed to select an optimal configuration. Rather than using a fixed algorithm, the framework provides an infrastructure for implementing multiple optimization algorithms. In this way, the appropriate algorithm can be selected to match the requirements of different types of problem. The framework incorporates a number of novel techniques, including caching results between problem runs, an XML based configuration language, and an effective method of comparing configurations. We have applied the system to a typical <b>web</b> <b>farm</b> configuration problem and results have been obtained for three standard optimization algorithms...|$|E
40|$|With web {{applications}} {{being more}} and more complex the need for data caching grows larger. When running ASP. NET applications under Internet Information Services in Microsoft Windows it is recommended to run the application in a <b>web</b> <b>farm.</b> This is because it allows multiple requests to be handled simultaneous. Problems emerge when these processes needs to share common data with each other since two processes cannot read each other’s memory in Microsoft Windows. Since each process has to allocate memory for the data caching individually this creates {{a negative impact on}} scalability. This study concludes that a shared memory solution using memory mapped files is the most efficient solution regarding scalability and overall performance...|$|E
40|$|Amaethon is a web {{application}} {{that is designed}} for enterprise farm management. It takes a job typically performed with spreadsheets, paper, or custom software {{and puts it on}} the <b>web.</b> <b>Farm</b> administration personnel may use it to schedule farm operations and manage their resources and equipment. A survey was con- ducted to assess Amaethon’s user interface design. Participants in the survey were two groups of students and a small group of agriculture professionals. Among other results, the survey indicated that a calendar interface inside Amaethon was preferred, and statistically no less effective, than a map interface. This is despite the fact that a map interface was viewed by some users as a potentially important and effective component of Amaethon...|$|E
40|$|A novel locking {{protocol}} maintains data {{consistency in}} distributed and clustered file {{systems that are}} used as scalable infrastructure for <b>Web</b> server <b>farms.</b> High-performance <b>Web</b> sites rely on Web server “farms”—hundreds of computers serving the same content—for scalability, reliability, and low-latency access to Internet content. Deploying these scalable farms typically requires the power of distributed or clustered file systems. Building <b>Web</b> server <b>farms</b> on file systems complements hierarchical proxy caching. 1 Proxy caching replicates Web content throughout the Internet, thereby reducing latency from network delays and off-loading traffic from the primary servers. <b>Web</b> server <b>farms</b> scale resources at a single site, reducing latency from queuing delays. Both technologies are essential when building a high-performance infrastructure for content delivery. In this article, we present a cache consistency model and locking protocol customized for file systems that are used as scalable infrastructure for <b>Web</b> server <b>farms.</b> The protocol {{takes advantage of the}} Web’s relaxed consistency semantics to reduce latencies and network overhead. Our hybrid approach preserves strong consistency for concurrent write sharing with time-based consistency and push caching for readers (Web servers). Using simulation, we compare our approach to the Andrew file system and the sequential consistency file system protocols we propose to replace. Data Consistency File system design carries assumptions about workloads and application consistency needs that, when applied to Web serving, lead to inferior performance. File system designers assume that data are shared infrequently and that such data require strong (sequential) consistency. 2 Consistency describes how processors in a parallel or distributed system view shared data; the sidebar on related work in data consistency presents examples. For Web serving, data are widely shared among many servers, as Figure 1 shows, and strong consistency guarantee...|$|R
40|$|During {{recent years}} web servers evolved from {{providing}} simple, static content to offering different services {{and a variety}} of dynamically generated pages and objects. Consequently, scalability and load-balancing have emerged as main requirements for modern <b>web</b> <b>farms.</b> A common solution is based on placing a Web Switch in front of the web servers; the switch acts as a dispatcher that redirects user requests according to desired criteria. In this paper we describe a novel web switching architecture, based on the MPLS technology and on Open-Source software. The switching decision is primarily made considering Layer- 7 information, thus achieving flexible content-based routing to the most appropriate server. State information from the web servers- such as, for example, load average and resource availability- is considered as well. The architecture here proposed has been implemented as a free software project using MPLS-enabled Linux workstations...|$|R
40|$|The {{widespread}} use of clusters and <b>web</b> <b>farms</b> has increased the importance of data replication. In existing protocols, typical distributed system solutions emphasize fault tolerance {{at the price of}} performance while database solutions emphasize performance at the price of consistency. In this paper, we explore the use of data replication in a cluster con guration with the objective ofproviding both fault tolerance and good performance without compromising consistency. We do this by combining transactional concurrency control with group communication primitives. In our approach, transactions are executed at only one site so that not all nodes incur in the overhead of parsing, optimizing, and producing results. To further reduce latency, we use an optimistic multicast approach that overlaps transaction execution with the total order message delivery. The techniques we present in the paper provide correct executions while minimizing overhead and providing higher scalability...|$|R
40|$|Distributed Denial Of Service (DDoS) {{attacks are}} {{familiar}} threats to Internet users {{for more than}} ten years. Such attacks are carried out by a “Bot net”, an army of zombie hosts spread around the Internet, that overwhelm the bandwidth toward their victim Web server, by sending traffic upon command. This paper introduces WDA, a novel architecture to attenuate the DDoS attacker’s bandwidth. WDA is especially designed to protect Web farms. WDA is asymmetric and only monitors and protects the uplink toward the <b>Web</b> <b>farm,</b> which is the typical bottleneck in DDoS attacks. Legitimate traffic toward Web farms is very distinctive since it is produced by humans using Web browsing software. Specifically, such upload traffic has low volume, and more importantly, has long off times that correspond to human view time. WDA utilizes these properties of legitimate client traffic to distinguish it from attack traffic, which tends to be continuous and heavy. A key feature of WDA is in its use of randomized thresholds that trap and penalize deterministic zombie traffic that tries to mimic human client patterns. WDA’s heart is WDAQ, a novel active queue management mechanism aimed to prefer legitimate client traffic over attacker traffic. With WDA installed, the attacker traffic toward the victim is attenuated. Extensive simulation results show that WDA can defeat simple flooding attacks, and can attenuate the bandwidth usable by sophisticated WDA-aware attacks by orders of magnitude. As a consequence, the attacker must increase his “bot-net ” size by the same factor, to compensate for the effects of WDA. Our simulations show that WDA can defend a typical <b>Web</b> <b>farm</b> from DDoS attacks launched by hundreds of thousands zombies, while keeping legitimate clients ’ service degradation under ten percent...|$|E
40|$|The {{increasing}} size of web-server {{farms and}} {{the sheer volume of}} HTTP requests, makes hard to collect performance measurements and monitor the state of a farm in real time. Further, it increases the cost of a bad algorithmic or architectural decision, while predicting the performance of new algorithms and architectures is also hard. We propose a way to side-step these problems, by intelligently combining small-scale experiments and analysis. Our hypothesis is this: if we take a sample of the incoming requests, and feed it into a suitably scaled version of the <b>web</b> <b>farm,</b> we can extrapolate from the performance of the scaled system to that of the original. Our main nding is that when we suitably scale a web-server farm, then performance measures such as mean response time and throughput are left virtually unchanged. We show this using experiments and simple analysis...|$|E
40|$|Abstract. Web Farms are {{clustered}} systems {{designed to provide}} high availability and high performance web services. A <b>web</b> <b>farm</b> {{is a group of}} replicated HTTP servers that reply web requests forwarded by a single point of access to the service. To deal with this task the point of access executes a load balancing algorithm to distribute web request among the group of servers. The present algorithms provides a short-term dynamic configuration for this operation, but some corrective actions (granting different session priorities or distributed WAN forwarding) cannot be achieved without a long-term estimation of the future web load. On this paper we propose a method to forecast web service work load. Our approach also includes an innovative segmentation method for the web pages using EDAs (estimation of distribution algorithms) and the application of semi-naïve Bayes classifiers to predict future web load several minutes before. All our analysis has been performed using real data from a world-wide academic portal...|$|E
50|$|The first {{versions}} introduced {{basic functionality}} such as order processing, attributes, plugins, discounts, tier pricing, news, blogs, private messages, forums, tax and shipping support.In June 2010, a new data access layer {{was introduced in}} version 1.70.Version 2.00 (August 2011) launched nopCommerce as an ASP.NET MVC based solution.Later in 2011 nopCommerce moved to ASP.NET MVC 4.Versions 3.00 and 3.10 were extended to include multi-store and multi-vendor features and to simplify the product logic. In versions 3.50, 3.60 and 3.70 a modern and responsive template were included. Version 3.80 was released with a brand new and responsive admin area with basic and advanced views and with the capability to run in <b>web</b> <b>farms.</b> Version 3.90 contains significant improvements in marketing and content management functionality, in performance optimization, and in the admin area UI and UX.The version release cycle is 5-6 months.|$|R
50|$|In 2014, Wise {{starred in}} an {{original}} Chipotle <b>web</b> series titled <b>Farmed</b> and Dangerous; he played Buck Marshall in the satirical black comedy-type situation comedy.|$|R
50|$|In 1995 GEN hired Raymond Karrenbauer, {{who became}} {{instrumental}} {{in designing the}} first Microsoft IIS-based <b>web</b> server <b>farm,</b> which was a strategic decision that led to GEN becoming the first web hosting provider to offer Microsoft Frontpage hosting in 1996 upon Microsoft's launch of the product, with its $9.95/month Start Smart Frontpage hosting plan.|$|R
40|$|Abstract. Current service-level {{agreements}} (SLAs) {{offered by}} cloud providers make guarantees about quality attributes such as availability. However, although {{one of the}} most important quality attributes from the perspective of the users of a cloud-based Web application is its re-sponse time, current SLAs do not guarantee response time. Satisfying a maximum average response time guarantee for Web applications is dif-ficult due to unpredictable traffic patterns, but in this paper we show how it can be accomplished through dynamic resource allocation in a virtual <b>Web</b> <b>farm.</b> We present the design and implementation of a work-ing prototype built on a EUCALYPTUS-based heterogeneous compute cloud that actively monitors the response time of each virtual machine assigned to the farm and adaptively scales up the application to satisfy a SLA promising a specific average response time. We demonstrate the feasibility of the approach in an experimental evaluation with a testbed cloud and a synthetic workload. Adaptive resource management has the potential to increase the usability of Web applications while maximizing resource utilization. ...|$|E
40|$|The {{traffic and}} the number of users on the Internet are increasing, and service {{providers}} must respond to this demand if they are not to loose both customers and revenue. In order to provide a satisfactory level of QoS, the providers must increase server performance by aggregating multiple physical host into a <b>web</b> <b>farm,</b> which by working together, sharing the total load of the requests, will act as one unified server. Various algorithms and technology exists for performing this load sharing, and this thesis will introduce a new approach in this field. In the traditional load balancing (LB) strategies, the requests are pushed to the individual servers, which are passively forced to accept the requests. The new approach presented in this paper, differs from the traditional techniques, in that the servers themselves are more involved in the decision making associated with load balancing. Requests are stored in a central queue, and the individual requests is processed by the servers at the server’s own convenience, leaving the server with full control over it’s own resources. The paperwill present and compare this algorithmwith the traditional load balancing algorithms, and investigate possible benefits of this strategy. Master i nettverks- og systemadministrasjo...|$|E
40|$|Web Services {{are widely}} used to provide {{services}} and exchange data among business units, customers, partners and suppliers for enterprises. Although Web Services significantly improve the interaction and development of processes in the business world, they raise several security concerns, since they greatly increase the exposure of critical enterprise data. Web Services exchange data using SOAP messages {{that are based on}} the interoperable XML language. We have previously introduced XPRIDE as an enhanced security architecture for assuring confidentiality and integrity of SOAP messages. XPRIDE uses contentbased encryption to secure SOAP messages based on their XML content, and depends on security policies to define the parts of the SOAP message that need to be encrypted. Security policies are defined by administrators for each Web Service that needs to be secured. This paper extends XPRIDE using a modular design approach to ensure extensibility, such that new modules can be developed and deployed to handle the security of different types of data. In addition, we show a new implementation of XPRIDE as a gateway capable of applying content-based security on attachments of SOAP messages, where a single gateway serves several web servers in a <b>web</b> <b>farm.</b> These new features significantly improve the security, scalability, and deployability of XPRIDE...|$|E
5000|$|To ensure {{acceptable}} {{performance of}} widely dispersed distributed services, large organizations typically implement edge computing by deploying <b>Web</b> server <b>farms</b> with clustering. Previously {{available only to}} very large corporate and government organizations, edge computing has utilized technology advances and cost reductions for large-scale implementations have made the technology available to [...] small and medium-sized businesses.|$|R
40|$|The recent {{liberalisation}} of the German {{energy market}} forced {{the energy industry}} to develop and install new information systems to support agents on the energy trading floors in their analytical tasks. Besides classical approaches of building a data warehouse to give insight into the time series to understand market and pricing mechanisms {{it is crucial to}} provide a variety of external data from the web. Weather information as well as political news or market rumors are relevant to give the right interpretation to the variables of a volatile energy market. Starting from a multidimensional data model and a collection of buy and sell transactions a data warehouse is built that gives analytical support to the agents. Following the idea of <b>web</b> <b>farming</b> we harvest the web, match the external information sources after a filtering and evaluation process to the data warehouse objects and present this qualified information on a user interface where market values are correlated with those external sources over the time axis. 1...|$|R
40|$|Over {{the last}} ten years we have {{witnessed}} a shift from large mainframe computing to commodity, off-the-shelf clusters of servers. The cluster paradigm allows organizations to acquire resources at a fine granularity, scaling the cluster to provide the resources necessary for increasingly-powerful simulations, multimedia rendering, and <b>web</b> <b>farms.</b> As yesterday’s clusters grow into today’s data centers, we are seeing a push back towards the centralized placement of servers, driven by high initial costs of ownership and complex cooling and power infrastructure demands. Today’s data centers contain thousands or tens of thousands of servers, providing services and computation for tens or hundreds of thousands of users. In addition to traditional IT challenges such as server management, security, and performance, data center owners now must deal with power and thermal issues, previously the domain of facilities management. These trends will continue to accelerate as organizations acquire bladed servers and consolidate multiple, smaller clusters into centrally-located data centers. Even as individual organizations grow their data centers, we are seeing the growth o...|$|R
40|$|Multiprocessor-based servers {{are often}} used for {{building}} popular Web sites which have to guarantee an acceptable Quality of Web Service. In common multi-node systems, namely Web server farms, a Web switch (say, Dispatcher) routes client requests among the server nodes. This architecture resembles a traditional cluster in which a global scheduler dispatches parallel applications among the server nodes. The main {{difference is that the}} load reaching Web server farms tends to occur in waves with intervals of heavy peaks. These heavy-tailed characteristics have motivated the use of policies based on dynamic state information for global scheduling in Web server farms. This paper presents an accurate comparison between static and dynamic policies for different classes of Web sites. The goal is to identify main features of architectures and load management algorithms that guarantee scalable Web services. We verify that a <b>Web</b> <b>farm</b> with a Dispatcher with full control on client connections is a very robust architecture. Indeed, we demonstrate that if the Web site provides only HTML pages or simple database searches, the Dispatcher does not need to use sophisticated scheduling algorithms even if the load occurs in heavy bursts. Dynamic scheduling policies appears to be necessary for scalability only when most requests are for Web services of three or more orders of magnitude higher than providing HTML pages with some embedded objects...|$|E
40|$|A Web-server farm is a {{specialized}} facility {{designed specifically for}} housing Web servers catering {{to one or more}} Internet facing Web sites. In this dissertation, stochastic dynamic programming technique is used to obtain the optimal admission control policy with different classes of customers, and stochastic uid- ow models are used to compute the performance measures in the network. The two types of network traffic considered in this research are streaming (guaranteed bandwidth per connection) and elastic (shares available bandwidth equally among connections). We first obtain the optimal admission control policy using stochastic dynamic programming, in which, {{based on the number of}} requests of each type being served, a decision is made whether to allow or deny service to an incoming request. In this subproblem, we consider a xed bandwidth capacity server, which allocates the requested bandwidth to the streaming requests and divides all of the remaining bandwidth equally among all of the elastic requests. The performance metric of interest in this case will be the blocking probability of streaming traffic, which will be computed in order to be able to provide Quality of Service (QoS) guarantees. Next, we obtain bounds on the expected waiting time in the system for elastic requests that enter the system. This will be done at the server level in such a way that the total available bandwidth for the requests is constant. Trace data will be converted to an ON-OFF source and fluid- flow models will be used for this analysis. The results are compared with both the mean waiting time obtained by simulating real data, and the expected waiting time obtained using traditional queueing models. Finally, we consider the network of servers and routers within the <b>Web</b> <b>farm</b> where data from servers flows and merges before getting transmitted to the requesting users via the Internet. We compute the waiting time of the elastic requests at intermediate and edge nodes by obtaining the distribution of the out ow of the upstream node. This out ow distribution is obtained by using a methodology based on minimizing the deviations from the constituent in flows. This analysis also helps us to compute waiting times at different bandwidth capacities, and hence obtain a suitable bandwidth to promise or satisfy the QoS guarantees. This research helps in obtaining performance measures for different traffic classes at a Web-server farm so as to be able to promise or provide QoS guarantees; while at the same time helping in utilizing the resources of the server farms efficiently, thereby reducing the operational costs and increasing energy savings...|$|E
50|$|Because {{of their}} recent {{appearance}} on the <b>Web,</b> content <b>farms</b> have not yet {{received a lot of}} explicit attention from the research community. The model of hiring inexpensive freelancers to produce content of marginal or questionable quality was first discussed as an alternative strategy to generating fake content automatically; this was discussed together with an example of the infrastructure necessary to make content-farm-based sites profitable through online ads, along with techniques to detect social spam that promotes such content.|$|R
5000|$|After her graduation, Stacey {{became a}} hairdresser at Ali Barbers and {{eventually}} started her own salon, Absolutely Fabulous Hair Design, in 1997. That same year, she joined Hagley Theatre Company where she remained a regular cast member {{for over two}} years and acted in various stage productions including Charlotte's <b>Web,</b> Animal <b>Farm</b> and The Cherry Orchard. Appearing in Heavenly Creatures (1994) and The Frighteners (1995) during this time, she decided to become a full-time actress and closed her salon after 8 months.|$|R
40|$|In [1] {{we present}} the first {{analysis}} of Join-the-Shortest-Queue (JSQ) routing in <b>Web</b> server <b>farms,</b> with Processor-Sharing (PS) servers. Please see that paper for {{details about the}} JSQ routing policy and its analysis, and for the relevant references. The analytical work in [1] was backed up by many simulation experiments, not all of which {{made it into the}} paper. This document is meant to serve as an online supplement to [1], providing the full set of simulation results and comparisons with analytical results. We hope that these will be useful for those who wish to extend our work on this problem...|$|R
40|$|Abstract: In {{this paper}} we {{describe}} a project whose {{purpose is to}} provide a framework for developing remote intermediary applications on the Web. This framework ensures high availability and tolerance to faults like a distributed <b>Web</b> Server <b>Farm</b> that has redundant, loadbalanced servers that provide the highest availability and will easily scale in forecast of future needs. The platform is based on Java distributed objects so that it inherits the benefits offered by the language, such as being type-safe, code mobility, reusability and independence from hardware/software infrastructure. Our project is based on Web Based Intermediaries (WBI) [1], developed at IBM Almaden Research Center. 1...|$|R
