487|2|Public
2500|$|The online {{public access}} catalog (OPAC) {{is a basic}} module, part of the library’s {{integrated}} library system. Earlier, the OPAC has been limited to searching physical texts, and sometimes digital copies but has only limited special features. Caplan argues {{that they are in}} process of replacement by newer [...] "discovery tools" [...] allowing more customization. Yang and Hofmann suggest that vendors see money in building either separate discovery tools or Next-Generation OPACs to be purchased as an add-on feature. A problem with vocabulary arises here. Yang and Wagner (2010, in Yang and Hofmann, 2011) refer to discovery tools by many names, including [...] "stand-alone OPAC, discovery layer, and next-generation catalog [...] " [...] This contrasts Bair, Boston, and Garrison, who differentiate between next-generation catalogues and <b>web-scale</b> discovery services. [...] Despite any confusion, {{it is clear that the}} OPAC as it currently stands is outdated, and will be replaced by more modern, user-friendly tools. The next-generation OPAC as described by Yang and Hofmann will ideally have the following 12 features (although not all features are currently available in any single discovery product): ...|$|E
50|$|RocksDB {{is used in}} {{production}} systems at various <b>web-scale</b> enterprises including Facebook, Yahoo!, and LinkedIn.|$|E
50|$|In February 2010, Convera Corporation {{merged with}} Firstlight ERA to become NTENThttp://www.NTENT.com/, {{bringing}} with it its <b>web-scale</b> semantic search engine.|$|E
5000|$|Amazon Elastic MapReduce (EMR) Provides a PaaS service {{delivering}} Hadoop {{for running}} MapReduce queries framework {{running on the}} <b>web-scale</b> infrastructure of EC2 and Amazon S3.|$|E
5000|$|Scalability: how to factorize million-by-billion matrices, {{which are}} commonplace in <b>Web-scale</b> data mining, e.g., see Distributed Nonnegative Matrix Factorization (DNMF) and Scalable Nonnegative Matrix Factorization (ScalableNMF) ...|$|E
50|$|OnRamp {{exchange}} is a <b>web-scale,</b> hosted technology platform (SaaS) {{that can be}} deployed as a stand-alone application or in support of MOLA services. It supports document management, collaboration features, productivity features, reporting, notifications and alerts, and permission controls.|$|E
50|$|Since the {{introduction}} of the Semantic Web concept by Tim Berners-Lee in 1999, there has been growing interest and application of the W3C (World Wide Web Consortium, WWWC) standards to provide <b>web-scale</b> semantic data exchange, federation, and inferencing capabilities.|$|E
50|$|After {{the merger}} with Firstlight ERA in February 2010, Convera's {{remaining}} assets and technology became {{the property of}} NTENT. NTENT continues to provide <b>web-scale</b> semantic search for vertical applications, and also provides context-sensitive advertising services for both web pages and search results.|$|E
5000|$|Intota <b>web-scale</b> {{collection}} {{management system}} aims {{to address the}} needs of today's libraries that are still using systems designed for print, while library collections have become increasingly electronic. It has been described as a total reconceptualization of library management systems, addressing the complete resource lifecycle including selection, acquisition, cataloging, discovery and fulfillment. It includes the following elements: ...|$|E
50|$|Ramakrishnan {{received}} a bachelor's degree from IIT Madras in 1983, and a Ph.D. from the University of Texas at Austin in 1987. He {{has been selected}} as a Fellow of the ACM and a Packard fellow, and has done pioneering research {{in the areas of}} deductive databases, data mining, exploratory data analysis, data privacy, and <b>web-scale</b> data integration. The focus of his work in 2007 was community-based information management.|$|E
50|$|The company's CloudView {{product is}} search and {{information}} access software used for both online and enterprise search-based applications {{as well as}} enterprise search. CloudView combines <b>Web-scale</b> semantic technologies, rapid drag-and-drop application development and hybrid quantitative/qualitative analytics to deliver a consumer-style information experience to mission-critical business processes. In the case of structured data, the SBA index replaces a traditional relational database structure as the primary vehicle for information access and reporting.|$|E
5000|$|Bisciglia's primary {{contribution}} to computer science {{has been the}} introduction of hands-on large-scale computing into the undergraduate computer science curriculum originally developed at the University of Washington. In 2008, along with co-authors, Aaron Kimball and Sierra Michels-Slettvet,Bisciglia published a paper titled [...] "Cluster Computing for <b>Web-Scale</b> Data Processing." [...] This paper details the first MapReduce based large-scale computing course ever offered to undergraduate students, and has provided the foundation for similar courses at CMU, MIT, and Tsinghua University.|$|E
50|$|Revenues {{from the}} company's high speed {{products}} have grown rapidly {{due to the}} rapid expansion of telecom backbone and content provider networks accommodating increased mobile traffic. Initial adoption of the company's 100G coherent products were in the Long Haul market sector, {{over the next several years}} it is expect that growth in 100G and beyond will be mainly driven by adoption of 100G coherent products in the much larger Metro market sector and the datacenter market for large <b>web-scale</b> data network market.|$|E
5000|$|The Summon Service - a <b>web-scale</b> {{discovery}} service, {{described as}} similar to Google but for accessing a library's academic content. It provides a single search box for finding all items in a library's collection including books, electronic {{resources such as}} e-books, e-journals and academic databases, multimedia items and other holdings. It executes searches against a database of preharvested content, preferably by indexing the full text, and offers {{the ability to use}} facets to refine searches by restricting results to properties such as date range or content type.|$|E
5000|$|For modern (<b>Web-scale)</b> {{information}} retrieval, recall is {{no longer}} a meaningful metric, as many queries have thousands of relevant documents, and few users will be interested in reading all of them. Precision at k documents (P@k) is still a useful metric (e.g., P@10 or [...] "Precision at 10" [...] corresponds to the number of relevant results on the first search results page), but fails {{to take into account the}} positions of the relevant documents among the top k. Another shortcoming is that on a query with fewer relevant results than k, even a perfect system will have a score less than 1. [...] It is easier to score manually since only the top k results need to be examined to determine if they are relevant or not.|$|E
50|$|The Library website was {{completely}} re-designed {{and the content}} refreshed in 2011, and a further update completed summer 2014. A new Library Management System, from Innovative Interfaces went live in July 2012, and upgraded to the newer Sierra platform in May 2015. In 2013 the Library piloted the LibGuides product as a complement to the website and decided to mainstream this from 2014 for subject and thematic guides. In August 2014 the Library launched the Summon 2.0 <b>web-scale</b> discovery service, badged locally as OneSearch. The Library also maintains {{a presence in the}} key social media. These include an instant messaging chat service, a Twitter feed, an Instagram presence, YouTube channels, a blog and a page on Facebook.A mobile website and catalogue are available, with integration of these into the University app.|$|E
5000|$|The online {{public access}} catalog (OPAC) {{is a basic}} module, part of the library’s {{integrated}} library system. Earlier, the OPAC has been limited to searching physical texts, and sometimes digital copies but has only limited special features. Caplan argues {{that they are in}} process of replacement by newer [...] "discovery tools" [...] allowing more customization. Yang and Hofmann suggest that vendors see money in building either separate discovery tools or Next-Generation OPACs to be purchased as an add-on feature. A problem with vocabulary arises here. Yang and Wagner (2010, in Yang and Hofmann, 2011) refer to discovery tools by many names, including [...] "stand-alone OPAC, discovery layer, and next-generation catalog sic." [...] This contrasts Bair, Boston, and Garrison, who differentiate between next-generation catalogues and <b>web-scale</b> discovery services. [...] Despite any confusion, {{it is clear that the}} OPAC as it currently stands is outdated, and will be replaced by more modern, user-friendly tools. The next-generation OPAC as described by Yang and Hofmann will ideally have the following 12 features (although not all features are currently available in any single discovery product): ...|$|E
5000|$|Over {{the last}} fifteen years, Broder pioneered several {{algorithms}} systems and concepts fundamental to the science and technology of the WWW. Some of the highlights are as follows:In 1997, Broder led {{the development of the}} first practical solution for finding near-duplicate documents on <b>web-scale</b> using [...] "shingling" [...] to reduce the problem to a set-intersection problem and [...] "min-hashing" [...] or to construct [...] "sketches" [...] of sets. This was a pioneering effort in the area of locality-sensitive hashing. In 1998, he co-invented the first practical test to prevent robots from masquerading as human and access web sites, often referred to as CAPTCHA In 2000, Broder, then at AltaVista, together with colleagues from IBM and DEC SRC, conducted the first large-scale analysis of the Web graph, and identified the bow-tie model of the web graph. Around 2001-02, Broder published an opinion piece where he qualified the differences between classical information retrieval and Web search and introduced a now widely accepted classification of web queries intro navigational, information, and transactional, ...|$|E
50|$|Scality’s {{principal}} {{product is}} a scale-out object storage software platform {{known as the}} RING. Scality’s RING delivers petabyte-scale software-defined storage designed to use commodity hardware and characterized by cost-effective scaling, performance, and auto-recovery. Scality’s RING is a multitier architecture and can scale up to thousands of servers and many 100s of petabytes under a single global namespace. It allows customers to deploy both performance-optimized and capacity-optimized storage, varied data durability levels, and small to large object and file support in a single global namespace. Object Storage {{is one of the}} fastest growing segments of the Enterprise Storage Market. According to the IDC report “Worldwide File - and Object - Based Storage Forecast, 2016 - 2020” (IDC #US41685816, September 2016), it is estimated that object-based storage capacity is expected to grow at a CAGR of 30.7 percent from 2016 to 2020, reaching 293.7EB in 2020.The Scality RING is software that turns any standard x86 servers into <b>web-scale</b> storage. With the RING, you can store any amount of data, of any type, with incredible efficiency and 100% availability, guaranteed—all while reducing costs by as much as 90% over legacy systems.|$|E
40|$|We use <b>web-scale</b> N-grams in a base NP parser that {{correctly}} analyzes 95. 4 % of {{the base}} NPs in natural text. <b>Web-scale</b> data improves performance. That is, there is no data like more data. Performance scales log-linearly {{with the number of}} parameters in the model (the number of unique N-grams). The <b>web-scale</b> N-grams are particularly helpful in harder cases, such as NPs that contain conjunctions. ...|$|E
40|$|Content-based image {{retrieval}} {{has been}} studied more than two decades, but surprisingly has not been widely deployed for <b>web-scale</b> image databases. This is mainly caused by two factors: low scalability and lack of commercial applications. In this poster we review our recent research activities for developing web-scalable image retrieval and its novel applications to address various problems. First we discuss scalability issues to be addressed for handling <b>web-scale</b> image databases. We also describe our hashing technique to drastically lower down the memory and computational requirements for <b>web-scale</b> image retrieval. Second we explain its applications to the fields of image copyright and editing, and discuss benefits that it brings over prior techniques developed without considering or utilizing <b>web-scale</b> image databases. Finally, we discuss remained problems towards our goal. ...|$|E
40|$|This {{panel will}} review how <b>web-scale</b> {{discovery}} is being applied by academic libraries and examine evidence of its impact. The session will feature librarian pioneers in deploying <b>web-scale</b> discovery sharing their experiences {{as well as}} qualitative and quantitative results. Included will be a readout on the immediate rise in its content usage and heightened its presence among students and faculty. The speakers will offer advice on how <b>web-scale</b> discovery can best be leveraged to help libraries increase their return on content investments and strengthen their library brand...|$|E
40|$|In this paper, we {{systematically}} {{assess the}} value of using <b>web-scale</b> N-gram data in state-of-the-art supervised NLP classifiers. We compare classifiers that include or exclude features for the counts of various N-grams, where the counts are obtained from a <b>web-scale</b> auxiliary corpus. We show that including N-gram count features can advance the state-of-the-art accuracy on standard data sets for adjective ordering, spelling correction, noun compound bracketing, and verb part-of-speech disambiguation. More importantly, when operating on new domains, or when labeled training data is not plentiful, we show that using <b>web-scale</b> N-gram features is essential for achieving robust performance...|$|E
40|$|While the web {{provides}} a fantastic linguistic resource, collecting and processing data at <b>web-scale</b> {{is beyond the}} reach of most academic laboratories. Previous research has relied on search engines to collect online information, but this is hopelessly inefficient for building large-scale linguistic resources, such as lists of named-entity types or clusters of distributionally-similar words. An alternative to processing <b>web-scale</b> text directly is to use the information provided in an N-gram corpus. An N-gram corpus is an efficient compression of large amounts of text. An N-gram corpus states how often each sequence of words (up to length N) occurs. We propose tools for working with enhanced <b>web-scale</b> N-gram corpora that include richer levels of source annotation, such as part-of-speech tags. We describe a new set of search tools that make use of these tags, and collectively lower the barrier for lexical learning and ambiguity resolution at <b>web-scale.</b> The tools will allow novel sources of information to be applied to long-standing natural language challenges. 1...|$|E
40|$|Selecting a <b>web-scale</b> {{discovery}} {{service is a}} large and important undertaking that involves a significant investment of time, staff, and resources. Finding the right match begins with a thorough and carefully planned evaluation process. In order to be successful, this process should be inclusive, goal-oriented, data-driven, user-centered, and transparent. The following article offers a step-by-step guide for developing a <b>web-scale</b> discovery evaluation plan rooted in these five key principles based on best practices synthesized from the literature {{as well as the}} author’s own experiences coordinating the evaluation process at Rutgers University. The goal is to offer academic libraries that are considering acquiring a <b>web-scale</b> {{discovery service}} a blueprint for planning a structured and comprehensive evaluation process...|$|E
40|$|We {{propose a}} novel way of {{incorporating}} dependency parse and word co-occurrence information into a state-of-the-art <b>web-scale</b> n-gram model for spelling correction. The syntactic and distributional information provides extra evidence {{in addition to}} that provided by a <b>web-scale</b> n-gram corpus and especially helps with data sparsity problems. Experimental results show that introducing syntactic features into n-gram based models significantly reduces errors by up to 12. 4 % over the current state-of-the-art. The word co-occurrence information shows potential but only improves overall accuracy slightly. ...|$|E
40|$|In {{the fall}} of 2010, Illinois Wesleyan University {{reviewed}} all the major <b>web-scale</b> discovery tools available to libraries. We chose to be a beta-test site for EBSCO’s Discovery Service (EDS) and conducted usability testing with students. We eventually purchased EDS and did a full roll-out this past fall semester. This presentation will address the philosophy behind <b>web-scale</b> discovery along with our experiences regarding selection, testing, implementation, evaluation, and teaching. The presentation will also include live search demonstrations using Wesleyan’s EDS interface...|$|E
40|$|Traditional Naive Bayes Classifier {{performs}} miserably on <b>web-scale</b> taxonomies. In this paper, {{we investigate}} {{the reasons behind}} such bad performance. We discover that the low performance are not completely caused by the intrinsic limitations of Naive Bayes, but mainly comes from two largely ignored problems: contradiction pair problem and discriminative evidence cancelation problem. We propose modifications that can alleviate the two problems while preserving the advantages of Naive Bayes. The experimental results show our modified Naive Bayes can significantly improve the performance on real <b>web-scale</b> taxonomies...|$|E
40|$|<b>Web-scale</b> {{knowledge}} bases typically consist {{entirely of}} predicates over entities. However, the distributional properties of how those entities appear in text are equally {{important aspects of}} knowledge. If noun phrases mapped unambiguously to knowledge base entities, adding this knowledge would simply require counting. The many-to-many relationship between noun phrase mentions and knowledgebase entitiesmakesaddingdistributional knowledge about entities difficult. In this paper, we argue that this information should be explicitly included in <b>web-scale</b> knowledge bases. We propose a generative model that learns these distributional semantics by performingentitylinkingontheweb,andwegive some preliminary results that point to its usefulness. ...|$|E
40|$|In this paper, we {{analyze the}} nature and {{distribution}} of structured data on the Web. <b>Web-scale</b> information extraction, or the problem of creating structured tables using extraction from the entire web, is gathering lots of research interest. We perform a study to understand and quantify the value of <b>Web-scale</b> extraction, and how structured information is distributed amongst top aggregator websites and tail sites for various interesting domains. We {{believe this is the}} first study of its kind, and gives us new insights for information extraction over the Web. Comment: VLDB 201...|$|E
40|$|<b>Web-scale</b> {{discovery}} {{services for}} libraries provide deep discovery to a library’s local and licensed content, and represent an evolution, perhaps a revolution, for end user information discovery as pertains to library collections.   This article frames {{the topic of}} <b>web-scale</b> discovery, and begins by illuminating <b>web-scale</b> discovery from an academic library’s perspective – that is, the internal perspective seeking widespread staff participation in the discovery conversation.   This included {{the creation of a}} discovery task force, a group which educated library staff, conducted internal staff surveys, and gathered observations from early adopters.   The article next addresses the substantial research conducted with library vendors which have developed these services.   Such work included drafting of multiple comprehensive question lists distributed to the vendors, onsite vendor visits, and continual tracking of service enhancements.   Together, feedback gained from library staff, insights arrived at by the Discovery Task Force, and information gathered from vendors collectively informed the recommendation of a service for the UNLV Libraries...|$|E
40|$|Abstract. Recent {{research}} and development have created the necessary ingredients for a major push in <b>web-scale</b> language understanding: large repositories of structured knowledge (DBpedia, the Google knowledge graph, Freebase, YAGO) progress in language processing (parsing, information extraction, computational semantics), linguistic knowledge resources (Treebanks, WordNet, BabelNet, UWN) and new powerful techniques for machine learning. A major goal is the automatic aggregation of knowledge from textual data. A central component of this endeavor is relation extraction (RE). In this paper, we will outline {{a new approach to}} connecting repositories of world knowledge with linguistic knowledge (syntactic and lexical semantics) via <b>web-scale</b> relation extraction technologies...|$|E
40|$|Abstract—While Linked Open Data (LOD) {{has gained}} much {{attention}} in the recent years, requirements and the challenges concerning its usage from a database perspective are lacking. We argue that such a perspective is crucial for increasing acceptance of LOD. In this paper, we compare the characteristics and constraints of relational databases with LOD, {{trying to understand the}} latter as a <b>Web-scale</b> database. We propose LOD-specific requirements beyond the established database rules and highlight research challenges, aiming to combine future efforts of the database research community and the Linked Data research community in this area. Keywords-linked data; <b>Web-scale</b> database; applications. I...|$|E
40|$|Enabling entity {{search and}} ranking at <b>Web-scale</b> {{is fraught with}} many challenges: annotating the corpus with {{entities}} and types, query language design, index design, query processing logic, and answer consolidation. We describe a Webscale entity search engine we are building to handle over a billion Web pages, over 200, 000 types, over 1, 500, 000 entities, and hundreds of entity annotations per page. We describe the design of compressed, token span oriented indices for entity and type annotations. Our prototype demonstrates the practicality of <b>Web-scale</b> entity-relation search. Categories and Subject Descriptors: H. 3. 3 [Information Search and Retrieval]: Search process...|$|E
40|$|Grand Valley State University Libraries {{implemented}} Serials Solutions 2 ̆ 7 <b>web-scale</b> discovery tool, Summon, {{during the}} fall of 2009. This case study explores whether Summon {{had an impact on}} the use of the library 2 ̆ 7 s resources during its first semester of implementation. An examination of usage statistics showed a dramatic decrease in the use of traditional abstracting and indexing databases and an equally dramatic increase in the use of full-text resources from full-text database and online journal collections. The author concludes that the increase in full-text use is linked to the implementation of a <b>web-scale</b> discovery tool...|$|E
40|$|This paper {{describes}} Project Kittyhawk, an undertaking at IBM Research {{to explore}} {{the construction of a}} nextgeneration platform capable of hosting many simultaneous <b>web-scale</b> workloads. We hypothesize that for a large class of <b>web-scale</b> workloads the Blue Gene/P platform is an order of magnitude more efficient to purchase and operate than the commodity clusters in use today. Driven by scientific computing demands the Blue Gene designers pursued an aggressive system-on-a-chip methodology that led to a scalable platform composed of air-cooled racks. Each rack contains more than a thousand independent computers with highspeed interconnects inside and between racks. We postulate that the same demands of efficiency and density apply to <b>web-scale</b> platforms. This project aims to develop the system software to enable Blue Gene/P as a generic platform capable of being used by heterogeneous workloads. We describe our firmware and operating system work to provide Blue Gene/P with generic system software, one of the results of which is the ability to run thousands of heterogeneous Linux instances connected by TCP/IP networks over the high-speed internal interconnects...|$|E
