7|237|Public
5000|$|IBM Tivoli Workload Scheduler - Now {{known as}} IBM <b>Workload</b> <b>Schedule</b> from release 9.3 ...|$|E
50|$|As the {{activation}} of Skylab progressed, the astronauts complained of being pushed too hard. Ground crews disagreed; {{they felt that}} the astronauts were not working long enough or hard enough, {{during the course of}} the mission. On December 28, this culminated in turning off radio communications with NASA ground control for a full day. Subsequently, there was a radio conference to air frustrations which led to the <b>workload</b> <b>schedule</b> being modified, and by the end of their mission the crew had completed even more work than originally planned. The experiences of the crew and ground controllers provided important lessons in planning subsequent manned spaceflight work schedules.|$|E
30|$|There {{is also an}} {{important}} class of problems in this family that have not been addressed in the literature; cases where the <b>workload</b> <b>schedule</b> is not known in advance, and instead only a probability distribution of workloads is known.|$|E
40|$|Abstract—The {{cloud based}} {{multimedia}} applications {{have been widely}} adopted in recent years. Due to the large-scale and time-varying workload, an effective <b>workload</b> <b>scheduling</b> scheme is becoming a challenge faced by multimedia application providers. In this paper, we study the <b>workload</b> <b>scheduling</b> schemes for multimedia cloud. Specifically, we examine and solve the response time minimization problem and the resource cost minimization problem, respectively. Moreover, we propose a greedy algorithm to efficiently <b>schedule</b> <b>workload</b> for practical multimedia cloud. Simulation results demonstrate that the proposed <b>workload</b> <b>scheduling</b> schemes can optimally balance workload to achieve the minimal response time or the minimal resource cost for multimedia application providers. I...|$|R
40|$|Abstract—Renewable (or green) energy, such as solar or wind, has {{at least}} {{partially}} powered data centers to reduce {{the environmental impact of}} traditional energy sources (brown energy with high carbon footprint). In this paper, we propose a holistic <b>workload</b> <b>scheduling</b> algorithm to minimize the brown energy consumption across multiple geographically distributed data centers with renewable energy sources. While green energy supply for a single data center is intermittent due to daily/seasonal effects, our <b>workload</b> <b>scheduling</b> algorithm is aware of different amounts of green energy supply and dynamically <b>schedules</b> the <b>workload</b> across data centers. The scheduling decision adapts to workload and data center cooling dynamics. Our experiments with real workload traces demonstrate that our scheduling algorithm greatly reduces brown energy consumption by up to 40 % in comparison with other scheduling policies. Keywords-Green data centers, renewable energy, <b>workload</b> <b>scheduling,</b> geographically distributed data centers. I...|$|R
40|$|Clusters of {{commodity}} microprocessors have overtaken custom-designed systems {{as the high}} performance computing (HPC) platform of choice. The design and optimization of <b>workload</b> <b>scheduling</b> systems for clusters has been an active research area. This paper surveys some examples of <b>workload</b> <b>scheduling</b> methods used in large-scale applications such as Google, Yahoo, and Amazon that use a MapReduce parallel processing framework. It examines a specific MapReduce framework, Hadoop, in some detail. It describes a novel dynamic prioritization, self-tuning workload scheduler, and provides simulation results that suggest the approach will improve performance compared to standard Hadoop scheduling...|$|R
40|$|Research {{suggests}} {{that women who}} adopt alternative work arrangements may be viewed less favorably than women who work a regular schedule. This study examined whether those negative perceptions persist even after the woman returns to a regular schedule. One hundred twenty-five employed MBA students participated in an experimental study in which work schedule was manipulated. Participants reviewed a personnel file for a female employee who was either on a regular schedule or who had previously been on a reduced-workload schedule. They then completed a questionnaire assessing {{their perceptions of the}} target employee. Contrary to the authors 2 ̆ 7 expectations, results revealed that the female employee who had previously been on an reduced <b>workload</b> <b>schedule</b> was actually viewed as having significantly greater advancement motivation and advancement capability than a female employee who had always used a regular schedule. She was also somewhat more likely to be recommended for a promotion...|$|E
40|$|Matrix {{factorization}} (MF) {{has been}} widely used in e. g., recommender systems, topic modeling and word embedding. Stochastic gradient descent (SGD) is popular in solving MF problems because it can deal with large data sets and is easy to do incremental learning. We observed that SGD for MF is memory bound. Meanwhile, single-node CPU systems with caching performs well only for small data sets; distributed systems have higher aggregated memory bandwidth but suffer from relatively slow network connection. This observation inspires us to accelerate MF by utilizing GPUs's high memory bandwidth and fast intra-node connection. We present cuMF_SGD, a CUDA-based SGD solution for large-scale MF problems. On a single CPU, we design two <b>workload</b> <b>schedule</b> schemes, i. e., batch-Hogwild! and wavefront-update that fully exploit the massive amount of cores. Especially, batch-Hogwild! as a vectorized version of Hogwild! overcomes the issue of memory discontinuity. We also develop highly-optimized kernels for SGD update, leveraging cache, warp-shuffle instructions and half-precision floats. We also design a partition scheme to utilize multiple GPUs while addressing the well-known convergence issue when parallelizing SGD. On three data sets with only one Maxwell or Pascal GPU, cuMF_SGD runs 3. 1 X- 28. 2 X as fast compared with state-of-art CPU solutions on 1 - 64 CPU nodes. Evaluations also show that cuMF_SGD scales well on multiple GPUs in large data sets...|$|E
40|$|Data center (DC) {{electricity}} use is increasing {{at an annual}} rate of over 20 % and presents a concern for the Information Technology (IT) industry, governments, and the society. A large fraction of the energy use is consumed by the compressor cooling to maintain the recommended operating conditions for IT equipment. The most common way to improve the DC efficiency is achieved by optimally provisioning the cooling power to match the global heat dissipation in the DC. However, at a more granular level, the large range of heat densities of today's IT equipment makes the task of provisioning cooling power optimized to the level of individual computer room air conditioning (CRAC) units much more challenging. Distributed sensing within a DC enables the development of new strategies to improve energy efficiency, such as hot spot elimination through targeted cooling, matching power consumption at rack level with <b>workload</b> <b>schedule,</b> and minimizing power losses. The scope of Measurement and Management Technologies (MMT) is to develop a software tool and the underlying sensing technology to provide critical decision support and control for DC and telecommunication facilities (TF) operations. A key aspect of MMT technology is integration of modeling tools to understand how changes in one operational parameter affect the overall DC response. It is demonstrated that reduced ordered models for DC can generate, in less than 2 seconds computational time, a three dimensional thermal model in a 50 kft{sup 2 } DC. This rapid modeling enables real time visualization of the DC conditions and enables 'what if' scenarios simulations to characterize response to 'disturbances'. One such example is thermal zone modeling that matches the cooling power to the heat generated at a local level by identifying DC zones cooled by a specific CRAC. Turning off a CRAC unit can be simulated to understand how the other CRAC utilization changes and how server temperature responds. Several new sensing technologies were added to the existing MMT platform: (1) air contamination (corrosion) sensors, (2) power monitoring, and (3) a wireless environmental sensing network. All three technologies are built on cost effective sensing solutions that increase the density of sensing points and enable high resolution mapping of DCs. The wireless sensing solution enables Air Conditioning Unit (ACU) control while the corrosion sensor enables air side economization and can quantify the risk of IT equipment failure due to air contamination. Validation data for six test sites demonstrate that leveraging MMT energy efficiency solutions combined with industry best practices results in an average of 20 % reduction in cooling energy, without major infrastructure upgrades. As an illustration of the unique MMT capabilities, a data center infrastructure efficiency (DCIE) of 87 % (industry best operation) was achieved. The technology is commercialized through IBM System and Technology Lab Services that offers MMT as a solution to improve DC energy efficiency. Estimation indicates that deploying MMT in existing DCs can results in an 8 billion kWh savings and projection indicates that constant adoption of MMT can results in obtainable savings of 44 billion kWh in 2035. Negotiations are under way with business partners to commercialize/license the ACU control technology and the new sensor solutions (corrosion and power sensing) to enable third party vendors and developers to leverage the energy efficiency solutions...|$|E
40|$|Abstract — Traditionally CPU <b>workload</b> <b>scheduling</b> and {{fan control}} in {{multi-socket}} {{systems have been}} designed sep-arately leading to less efficient solutions. In this paper we present Cool and Save, a cooling aware dynamic workload management strategy that is significantly more energy ef-ficient than state-of-the art solutions in multi-socket CPU systems because it performs <b>workload</b> <b>scheduling</b> in tan-dem with controlling socket fan speeds. Our experimental results indicate that applying our scheme gives average fan energy savings of 73 % concurrently with reducing the max-imum fan speed by 53 %, thus leading to lower vibrations and noise levels. I...|$|R
50|$|Sysload Software, was a {{computer}} software company specializing in systems measurement, performance and capacity management solutions for servers and data centers, based in Créteil, France. It has been acquired in September 2009 by ORSYP, {{a computer}} software company specialist in <b>workload</b> <b>scheduling</b> and IT Operations Management, based in La Défense, France.|$|R
40|$|The Monarch Chemistry system, a {{centrifugal}} analyser incorporating sophisticated robotics for analytical rotor {{transfer and}} flexible software for <b>workload</b> <b>scheduling,</b> has been evaluated. The optical system {{is capable of}} monitoring absorbance, fluorescence and light scattering reactions. In addition, an ion selective electrode unit may be incorporated for the measurement of sodium, potassium and chloride...|$|R
40|$|In {{evolutionary}} biology, ageing {{is usually}} {{defined as a}} persistent decline in the age-specific fitness components of an organism due to internal physiological deterioration. This definition integrates effects on reproduction and survival. Gerontologists simply define ageing as {{an increase in the}} likelihood that an individual will die in a certain time interval. As we age, intracellular processes degenerate and ultimately fail. This can lead to age-related diseases, such as cardiovascular disease, Parkinson’s disease etc., and ultimately to death. There has been much speculation on the role of energy metabolism in the causation of these processes. This has {{led to the formation of}} several intriguing theories which attribute the causation of death ultimately to the very motor of life itself; the rate of living theory (Pearl, 1928; Rubner, 1908) and free radical theory of ageing (Harman, 1956). This idea was summarized by Murray (1926) in the statement: ‘If aliveness is measured by the velocity of chemical activity (heat production) an organism may in this sense be said to dig its own grave. The more abundant its manifestations of life, the greater will be its rate of senescence’. The primary aim of this thesis was to investigate the relationship between ageing and metabolic rate. In two large-scale experiments I manipulated the energy expenditure of a group of animals by either increasing their physical activity or exposing them to cold. Survival curves were created for different experimental groups and I looked at changes that occurred in several physiological parameters that might be involved in ageing to explain differences that occurred in life span (Part II: Metabolism & Ageing). In addition, I explored the behavioural and physiological consequences of changes in energy balance in mice that had been selectively bred for high levels of spontaneous physical activity (Part I: Activity & Metabolism). PART I: Activity & Metabolism Life history theory The evolution of life histories has been explained by the presence of limited resources that results in trade-offs between survival (maintenance of the body) and reproduction (Stearns, 2000). In times of plenty, resources can be allocated to growth and reproduction, but when resources are scarce, energy has to be allocated to enable survival of the individual and future success. In many species the reproductive season is tuned to coincide with the peak in food availability. When food is scarce, reproduction ceases and energy is allocated to increase the chance of survival into the next year. There is a large variation in the way animals deal with such trade-offs. When there is a genetic basis for these decisions, natural selection favours life-history traits that result in a higher fitness. The main environmental factors influencing the available resources for endothermic animals are environmental temperature and food availability. In part I of this thesis we investigated the effects of low ambient temperatures or food availability on metabolism and the amount of voluntary activity mice were willing to engage. We used mice that had been selected by T. Garland Jr. for high wheel-running activity and their random-bred controls. Detailed description of the selection protocol and the main characteristics of these mice is provided in box 1. 1. The amount of wheel-running activity was the selection criteria. After 31 generations of selection the mice ran approximately 2, 7 times as much as control animals (Rhodes et al., 2005). With the selection for wheel-running activity other traits have been co-selected (i. e. small body size) and much research has been undertaken to uncover these co-selected traits. In chapter 2 we investigate mice exposed to various ambient temperatures. We measured wheel-running activity and metabolic rate simultaneously to determine whether high-activity mice have evolved to have a lower running economy and whether they would more likely use heat generated by activity to substitute for thermoregulatory heat at low ambient temperatures than control mice do. In chapter 3 we manipulated food availability using a system in which animals had to run in a running wheel for a set number of revolutions to obtain a food pellets. This approach was used to study effects of food availability on physiological and behavioural responses in control and selected mice. Previous studies in rats by T. Adage showed that rats with low spontaneous levels of wheel-running activity have more difficulties to cope with a <b>workload</b> <b>schedule</b> than rats with high spontaneous levels of wheel-running activity. Similar effects were expected between control and high-activity mice. Exercise & obesity Obesity is becoming an increasingly prevalent health problem in affluent societies. It is often associated with metabolic derangements such as impaired glucose tolerance, insulin resistance, high blood pressure, dyslipidemia, and abdominal obesity. When these metabolic abnormalities are displayed in concert (often referred to as the “metabolic syndrome”), they entail a high risk of developing into life-threatening conditions such as cardiovascular disease and diabetes mellitus type 2 (for review see (Carroll and Dudfield, 2004; Moller and Kaufman, 2005)). Increased dietary fat intake in combination with a sedentary existence are factors precipitating the development of obesity and the associated metabolic syndrome. Adipose tissue produces several hormones, such as leptin and adiponectin, that are important for energy homeostasis. Levels of these hormones are associated with metabolic risk factors. Adiponectin levels are decreased and leptin levels increased in obese compared with lean subjects (Park et al., 2004). Mice that have been selected for high wheel-running activity (for detailed description see box 1. 1.) have decreased levels of leptin even when correcting for fat mass (Girard et al., 2006). Leptin is produced by adipose tissue and informs the body about its available fat stores and is involved in regulating food intake. Selected mice have a high food intake to cover the increased costs of wheel-running activity (Swallow et al., 2001 a), and lowering levels of leptin may be an adaptation to increase food intake and maintain energy balance. High-activity mice have a lean phenotype (Dumke et al., 2001; Swallow et al., 1999 a) and adiponectin levels are thus expected to be increased in high-activity mice. This together with low levels of leptin might make high-activity mice less prone to develop metabolic derangements on a high-fat diet and would make these mice a suitable model to study the metabolic syndrome. In chapter 4 we measured hormone levels of leptin, adiponectin and corticosterone in aging male mice selected for high wheel-running activity and their random-bred controls. We studied correlations between the hormones and body composition. In chapter 5 we describe a study in which we exposed selected males and females to a high fat diet. Body composition, food efficiency, energy metabolism and glucose tolerance were tested to determine whether high-activity mice responded differently to a high-fat diet than controls. Part II: Metabolism & Ageing “Rate of living” and “free radical” theory of ageing Instinctively we know that things (cars, machines) break down faster if you use them more often and more intensively. The same might be applicable to animal (and human) life. This notion that the rate of energy turnover determines the rate of breakdown is known as the “rate of living” theory (Pearl, 1928). In 1908, Rubner noted that food intake per gram decreased with increasing life span among five domestic animals (guinea pig, cat, dog, cow and horse). He calculated the energy intake per gram per life span (life-time energy potential, LEP) and found that the variation in LEP between species was small (1, 5 fold), although the variation in body mass was very large. Including data for men the variation in life-time energy expenditure was slightly larger, but still only 5 -fold. He concluded that mass-specific energy metabolism times the maximal lifespan was a constant (Rubner, 1908). Energy metabolism might thus be the factor that determines our life span. In 1928 Pearl postulated the “rate of living theory” that states that there is an inverse relationship between energy expenditure and life span (Pearl, 1928). An extensive body of evidence exists that is consistent with this theory. Comparative studies have shown that energy expenditure tends to show an inverse relationship with body size and longevity when compared across mammalian or bird species (Ku et al., 1993; Speakman, 2005 a). Also evidence from intra-specific studies show evidence for the rate of living theory. Increasing ambient temperature (and thereby energy expenditure) decreased life span of nematodes proportionally (Van Voorhies and Ward, 1999). Honey bees that were forced to fly with extra loads had decreased life spans (Wolf and Schmid-Hempel, 1989), and flies prohibited to fly (thereby decreasing metabolic rate) had increased life spans (Yan and Sohal, 2000). Brood size increases in kestrels resulted in increased energy turnover and a subsequent decrease in the survival of parents that had enlarged broods (Daan et al., 1996). In hibernating hamsters survival was higher than in hamsters that did not hibernate (Lyman et al., 1981). A moderate increase of the level of basal metabolism of young adult rats adapted to hypergravity compared to controls in normal gravity was accompanied by a roughly similar increase in the rate of organ aging and reduction of survival (Economos et al., 1982). In contrast there are also numerous studies that showed no relationship or a positive relationship between energy expenditure and life span (in mammals (Holloszy and Smith, 1987; Holloszy and Smith, 1986; Navarro et al., 2004; 2003; Speakman et al., 2004)), and comparative studies show that for a certain body mass birds expend up to 4 times more energy than a mammal, and live longer (Speakman, 2005 b). Another line of evidence comes from experiments on calorically restricted animals. Caloric restriction (CR; decreasing energy intake) is widely recognized as the only (non-genetic) manipulation that increases mean and maximum life span in mammals (first shown by (McCay et al., 1935)). In 1977 Sacher proposed that CR extended life span by decreasing metabolic rate. A study by Masoro et al. found that following the initiation of CR there was a brief period of reduced food intake per gram body mass, but this was followed by a lifetime where the intake per gram body mass was higher in CR rats than ad-libitum fed rats (Masoro et al., 1982). In a study where mass-specific 24 -h metabolic rates were measured mass-specific (based on lean mass) metabolic rates were reduced upon the initiation of CR, but increased to levels higher than ad-libitum fed animals later on (McCarter and Palmer, 1992). Similar results were shown in rhesus monkeys (Ramsey et al., 1996). These studies disagree with a role for metabolic rate in the life extending effect of CR. Interpretation of the results is confounded because metabolic rate is usually normalized for body mass or lean mass, whereas the relative sizes of organs are not the same for animals that are CR or fed ad libitum (Greenberg, 1999 b; Greenberg and Boozer, 2000). A related theory of ageing was suggested by Harman in 1956 known as the “free radical theory” (Harman, 1956). This theory specifies the reason why there should be a direct link between energy metabolism and the rate of ageing. Free radicals or radical oxygen species (ROS) are produced as by-products of normal oxidative phosphorylation, and can cause damage to macromolecules which may result in malfunction and eventually cell death (for review see (Beckman and Ames, 1998)). The body has evolved defense systems against these radicals in the form of antioxidant enzymes (e. g. superoxide dismutase, catalase and gluthione peroxidase) that scavenge ROS and transform them into less toxic products. A small amount of ROS escape conversion. If damage to macromolecules has occurred the processes of DNA repair and protein synthesis can repair most of this damage. Despite these defense systems a small amount of damage still occurs and this accumulates with age resulting in malfunction of cells and eventually death (see Figure 1. 1. for a graphical representation of the process). When energy expenditure (and oxidative phosphorylation) increases, the production of ROS will also increase. This would explain the relationship between ageing and metabolism proposed by the rate of living theory. The relationship between oxidative phosphorylation and ROS production is not linear. Oxidative phosphorylation takes place on the inner membrane of mitochondria as a result of the transport of electrons over the membrane (electron transport chain; ETC). The ETC consists of 4 complexes. NADH and FADH 2 that have been formed in the tricarboxylic acid cycle (TCA) donate their electrons to subsequently complex I or II which are then passed on to ubiquinone (Q). Q moves across the membrane to complex III and the electrons are passed on to cytochrome C that moves on to complex IV where the electrons are accepted by molecular oxygen and combined with protons to form water (for a more detailed description see (Brand, 2000 a)). During this process protons are pumped across the membrane into the inner membrane space and a proton motive force builds up. When oxidative phosphorylation is coupled these protons are pumped back to the matrix via an ATP-ase pump resulting in the phosphorylation of ADP to ATP (ATP synthesis). Free radicals are generated during oxidative phosphorylation when an oxygen molecule promiscuously reacts with one of the transported electrons before it reaches complex IV. This can for instance occur when the supply of ADP is limited thereby blocking up the system. Agents that increase respiration rate and thereby lower proton motive force (i. e., ATP synthesis) thus lower the rate of ROS production. ROS production is thus not linearly related to the rate of electron transport. The flow of electrons in the ETC is usually tightly coupled to the production of ATP, and it does not occur unless the phosphorylation of ADP can proceed. This prevents a waste of energy, because high-energy electrons do not flow unless ATP can be produced. If electron flow is uncoupled from the phosphorylation of ADP there would be no production of ATP, and the energy of the electrons would be wasted as heat. Uncoupling agents abolish the link between oxidation and phosphoryalation, allowing electron transport to proceed without coupled ATP synthesis, thereby increasing the respiration rate and lowering ROS production (Brand, 2000 b). Therefore, metabolic rate and free radical production are not necessarily linearly related. Many studies support the importance of antioxidants, oxidative stress and repair of oxidative damage for the ageing process. For instance, the importance of antioxidants enzymes is clear from studies with over-expression or knocking out of these enzymes. Overexpression of catalase and superoxide dismutase in Drosophila melanogaster increased median and maximum lifespan up to 30 % (Orr and Sohal, 1994; Sohal et al., 1995), and mice lacking manganese superoxide dismutase died within 10 days (Li et al., 1995), whereas administration of superoxide dismutase-catalase mimetics increased lifespan up to three times in mice (Melov et al., 2001). In CR animals life span extending effects have also been attributed to differences in oxidative stress. Increased antioxidant enzyme activity, DNA repair and protein synthesis, and decreased numbers of oxidatively damaged molecules have been shown in CR animals (for reviews see (Gredilla and Barja, 2005; Tavernarakis and Driscoll, 2002; Yu, 1996)). Whereas the “free radical” theory has gained much support in recent years, the rate of living theory has been discarded as invalid by many researchers based on inter-specific comparisons and the lack of effects on (or increases in) energy metabolism in CR animals. This is remarkable since the free radical theory of ageing is itself the main theory postulating the mechanism connecting energy turnover and ageing. As argued by Speakman (2002; 2005 c) the reasons to dispute the theory may not always be valid, because the arguments that are used to test the theory are fraught with problems. Firstly, maximum life span is not a good measure of ageing. Maximal life span is determined by a single point in every data base and is highly affected by the sample size used and also by the conditions in which animals are housed (i. e. laboratory or natural conditions). Secondly, basal metabolic rates have been used in most studies to estimate life-time energy potential. Basal metabolic rate is the metabolism of an animal when fasting and resting at thermo-neutral temperatures and contributes only 40 % to the total daily energy expenditure. The latter is a better measure of metabolism. Using a single measure of metabolic rate in the life time of an animal might not be sufficient to make an accurate estimate of life-time energy potential. Thirdly, testing for consistency in life-time energy expenditure per gram of tissue by inter-specific comparisons between birds and mammals is not the best way to test the rate of living theory and inter-specific comparisons are complicated by the fact that animals from different species may reflect adaptive or genetic differences in free-radical production or differences in defence and repair mechanisms. Therefore, intra-specific comparisons are more convincing when looking at associations between energy expenditure and ageing. A fourth argument relates to the scaling of energy expenditure to body mass. Greenberg has shown that in cases where no relation was found between life-time energy expenditure per gram body mass and life span, a relationship does exist when one calculates the energy expenditure for certain metabolically active organs and relate this to life span (Greenberg, 1999 a). Life-time energy expenditure per gram dry lean body mass instead of total body mass might be a better measure to test the rate of living theory since this contains the tissue that is metabolically most active. A stronger correlation between energy metabolism and dry lean mass is usually found then between body mass and energy expenditure. In studies on energy expenditure and life span almost never the body composition and energy turnover are followed throughout life. In order to resolve some of the confusion in this area we carried out two large scale experiments. We manipulated energy metabolism by either increasing activity through selection (chapter 6) or by decreasing environmental temperature (chapter 8) and looked at the relationship between energy metabolism and survival in intra-specific comparison. Mice selectively bred for high wheel-running activity were used to investigate the effects of increased voluntarily exercise. In the cold experiment, c 57 Bl/ 6 J mice were used that were subjected to 10 C compared to 22 C in control mice. An additional group that was exposed to cold early in life was added. This for the first time tests one basic implicit proposition in the rate of living and free radical theories: that the effects of energy turnover are cumulative. Energy turnover increase in youth should still have and effect in old age. In both experiments we paid specific attention to the effects of age and experimental manipulation (i. e. cold or activity) on two systems that are involved in defending the body against ROS, the antioxidant defence system and protein turnover (chapter 7 and chapter 9) ...|$|E
40|$|Divisible {{workload}} applications {{arise in}} many fields {{of science and}} engineering. They can be parallelized in master-worker fashion and relevant scheduling strategies have been proposed to reduce application makespan. Our goal {{is to develop a}} practical divisible <b>workload</b> <b>scheduling</b> strategy. This requires that previous work be revisited as several usual assumptions about the computing platform do not hold in practice. We have partially addressed this concern in a previous paper via an algorithm that achieves high performance with realistic resource latency models. In this paper we extend our approach to account for performance prediction errors, which are expected for most realworld performance and applications. In essence, we combine ideas from multi-round divisible <b>workload</b> <b>scheduling,</b> for performance, and from factoring-based scheduling, for robustness. We present simulation results to quantify the benefits of our approach compared to our original algorithm and to other previously proposed algorithms. ...|$|R
5000|$|In May 2016, Univa {{announced}} a new business unit, Navops with a product line based on its Grid Engine cluster <b>workload</b> <b>scheduling</b> technology originally at Sun Microsystems and later acquired by Univa from Oracle. The Navops line differs from Univa Grid Engine in that it enables enterprises {{to take full advantage}} of Kubernetes and run containerized microservices applications and non-containerized workloads on Kubernetes.|$|R
50|$|The System Display and Search Facility (SDSF) {{feature of}} IBM {{mainframes}} running z/OS allows users and administrators {{to be able}} to view and control various aspects of the mainframe's operation and system resources. Batch job output, status of running Unix processes, system information, <b>workload</b> <b>scheduling,</b> external device monitoring such as printers and initiators. SDSF is primarily used to access the batch and system log files and dumps.|$|R
30|$|This paper adopts {{parallel}} computing techniques to help speed up large CA. In the master/slave architecture, the I/O workload is minimal and is handled with the master reading the data, and then broadcasting to the slaves. It {{focuses on the}} challenges of <b>workload</b> <b>scheduling,</b> <b>workload</b> balance, and limiting the communication cost of CA computation. Our contribution is to address this large-scale massive {{parallel computing}} problem of CA, apply the hierarchical task scheduling, which efficiently limits the synchronization penalty along with the dynamic task scheduling strategy, and introduce the method to N −  2 large-scale contingency analysis with robust scalability.|$|R
40|$|Research Overview My {{research}} centers around data center {{energy efficiency and}} cluster performance, particularly capacity provisioning and <b>workload</b> <b>scheduling</b> taking energy and performance into consideration. I am interested {{in the development and}} utilization of analytical tools combined with implementation experience to improve system design and management. In addition to the data center area, my research contributes new analytical tools to the fields of convex optimization, online algorithms and stochastic modeling...|$|R
40|$|Reliability, {{security}} and stability of cloud services without sacrificing too much resources have become a desired feature {{in the area of}} workload management in clouds. The paper proposes and evaluates a lightweight framework for <b>scheduling</b> a <b>workload</b> which part could be unreliable. This unreliability could be caused by various types of failures or attacks. Our framework for robust <b>workload</b> <b>scheduling</b> efficiently combines classic fault-tolerant and security tools, such as packet/job scanning, with <b>workload</b> <b>scheduling,</b> and it does not use any heavy resource-consuming tools, e. g., cryptography or non-linear optimization. More specifically, the framework uses a novel objective function to allocate jobs to servers and constantly decides which job to scan based on a formula associated with the objective function. We show how to set up the objective function and the corresponding scanning procedure to make the system provably stable, provided it satisfies a specific stability condition. As a result, we show that our framework assures cloud stability even if naive scanning-all and scanning-none strategies are not stable. We extend the framework to decentralized scheduling and evaluate it under several popular routing procedures...|$|R
30|$|The {{processors}} {{are assigned}} to several groups, and the master in the group communicates with the individual slave processors, receiving results and feedbacks on task assignments. Such a communications workload is within the master’s capacity. Therefore, the workload is balanced within groups. Moreover, {{because the number of}} slaves in a group is limited such that the number of requests being served by the master is limited as well, both <b>workload</b> <b>scheduling</b> and result collection will less degrade the performance.|$|R
40|$|This paper {{presents}} a tractable optimization strategies of sensing <b>workload</b> <b>scheduling</b> in {{wireless sensor networks}} using Divisible Load Theory (DLT). Because of the limited battery power of sensors, it is desirable that the sensor network can complete a task as quickly as possible. Given a task, each source node processes an appropriate fraction of the entire workload and then transmits the result to the sink node via a set of routing nodes. Two representative network models are presented to illustrate how the <b>workload</b> is <b>scheduled</b> among the sensor nodes so that the finish time of the workload is minimized. Furthermore, we present the energy model for nodes in a two-level hierarchical network topology. Finally, simulation results are presented to demonstrate the effects of selected system parameters such {{as the number of}} sensor nodes, measurement, processing, and communication speed on the performance of the sensor network...|$|R
40|$|Abstract—In {{state of}} the art systems, <b>workload</b> <b>scheduling</b> and server fan speed operate {{independently}} leading to cooling inefficiencies. In this work we propose GentleCool, a proactive multi-tier approach for significantly lowering the fan cooling costs without compromising the performance. Our technique manages the fan speed through intelligently allocating the workload across different machines. The experimental results show our approach delivers average cooling energy savings of 72 % and improves the mean time between failures (MTBF) of the fans by 2. 3 X compared to the {{state of the}} art. I...|$|R
30|$|Resource {{estimation}} underlies various workload {{management strategies}} including dynamic provisioning, <b>workload</b> <b>scheduling,</b> and admission control. All these approaches possess a prediction module in common which provides estimations to determine respectively {{whether or not}} to add more resources, rearrange the order of query execution, and admit or reject a new incoming query [89]. Prediction of the future resource behavior is a crucial process for efficient resource utilization in dynamic cloud computing environment because workload forecasting for short or long periods will be necessary to real-time control, resource allocation, capacity planning and data centre energy saving in cloud computing [90].|$|R
5000|$|... {{organize}} work <b>schedules,</b> <b>workload</b> assignments, employee demographic information, training, {{and financial}} information ...|$|R
40|$|Multithreaded {{programs}} can deliver higher throughput than single-threaded {{programs on the}} chip multiprocessors that have become the industry standard. However, increasing numbers of threads may not improve per-formance when memory speed and bandwidth cannot increase proportionally {{with the number of}} cores. We in-vestigate scaling properties of high-performance, multithreaded programs, and study how they can be scheduled concurrently on CMPs to provide both higher throughput and performance. By design, multithreaded programs should finish fastest by using the maximum number of threads, but we find results to the contrary. We derive analytic metrics and use local search heuristics for creating efficient multiprogrammed, multithreaded <b>workload</b> <b>schedules.</b> Our schedules divide cores among programs based on application scaling properties, achieving an av-erage 19 % performance gain over separately running programs at the maximum number of threads. In so doing, we reduce total energy consumption by 20 %. We verify our approach via experiments on an AMD Phenom 9500...|$|R
40|$|Pricing {{models for}} virtualized (cloud) {{resources}} {{are meant to}} reflect the operational costs and profit margins for providers to deliver specific resources or services to customers subject to an underlying Service Level Agreements (SLAs). While the operational costs incurred by cloud providers are dynamic they vary over time, depending on factors such as energy cost, cooling strategies, and overall utilization the pricing models extended to customers are typically fixed they are static over time and independent of aggregate demand. This disconnect between the cost incurred by a provider and the price paid by a customer results in an inefficient marketplace. In particular, it does not provide incentives for customers to express <b>workload</b> <b>scheduling</b> flexibilities that may benefit {{them as well as}} cloud providers. In this paper, we propose a new dynamic pricing model that aims to address this marketplace inefficiency by giving customers the opportunity and incentive to take advantage of any tolerances they may have regarding the <b>scheduling</b> of their <b>workloads.</b> We present the architecture and algorithmic blueprints of a framework for workload colocation, which provides customers with the ability to formally express <b>workload</b> <b>scheduling</b> flexibilities using Directed Acyclic Graphs (DAGs), optimizes the use of cloud resources to collocate clients’ workloads, and utilizes Shapley valuation to rationally and thus fairly in a game-theoretic sense attribute costs to customer workloads. In a thorough experimental evaluation we show the practical utility of our dynamic pricing mechanism and the efficacy of the resulting marketplace in terms of cost savings. National Science Foundation (0720604, 0735974, 0820138, 0952145, 1012798...|$|R
50|$|OpenLava is an {{open source}} <b>workload</b> job <b>scheduling</b> {{software}} for a cluster of computers. OpenLava was derived from {{an early version of}} Platform LSF. Its configuration file syntax, API, and CLI have been kept unchanged. Therefore, OpenLava is mostly compatible with Platform LSF.|$|R
40|$|Power {{dissipation}} {{is reduced}} by predicting future workload values and pre-emptively regulating supply voltage. Current microprocessors are complex systems with multiple cores and non-trivial architectures, employing millions of sub- 100 nm transistors designed for extreme power, speed, and reliability constraints. 1 An efficient control scheme for dynamic power, reliability, error detection-correction, <b>workload</b> <b>scheduling,</b> and thermal management is required. However, neuromorphic architectures are currently more suitable for complex tasks, such as noisy nonlinear data classification and prediction, relative to Boolean logic architecture (see Figure 1). We use a dynamic voltage scaling (DVS) control scheme as a test bench to demonstrate neuromorphic controller efficiency. 2 Microprocessors are typically designed to operate at a target frequency (f) and supply voltage (VDD) for peak performance...|$|R
40|$|Grid {{computing}} systems utilize distributive {{owned and}} geographically dispersed resources for providing {{a wide variety}} of services for various applications. It is possible that the job submission for the resource request by resource consumers can be large owing to wide area distribution of grid. Key services such as resource discovery, monitoring and scheduling are inherently more complicated in a grid environment. In this paper, we present a method of applying Particle Swarm Optimization (PSO) algorithm to the problem of Grid <b>workload</b> <b>scheduling.</b> Swapping mechanism is used to improve the performance of the Discrete Particle Swarm Optimization algorithm. Furthermore, we compare the scheduling algorithm with other scheduling mechanisms, namely, FCFS and EDF algorithm. The computational results reveal that the proposed Discrete PSO approach is more effective. 1...|$|R
40|$|In {{this paper}} we {{measured}} and analyzed the workload on Yahoo! Video, the 2 nd largest U. S. video sharing site, {{to understand its}} nature and the impact on online video data center design. We discovered interesting statistical properties on both static and temporal dimensions of the workload including file duration and popularity distributions, arrival rate dynamics and predictability, and workload stationarity and burstiness. Complemented with queueing-theoretic techniques, we further extended our understanding on the measurement data with a virtual design on the workload and capacity management components of a data center assuming the same workload as measured, which reveals key results regarding the impact of Service Level Agreements (SLAs) and <b>workload</b> <b>scheduling</b> schemes on the design and operations of such large-scale video distribution systems...|$|R
40|$|Modern {{computing}} systems comprise heterogeneous designs which combine {{multiple and}} diverse architectures {{on a single}} system. These designs provide potentials for high performance under reduced power requirements but require advanced resource management and <b>workload</b> <b>scheduling</b> across the available processors. Programmability frameworks, such as OpenCL and CUDA, enable resource management and <b>workload</b> <b>scheduling</b> on heterogeneous systems. These frameworks fully assign the control of resource allocation and scheduling to the application. This design sufficiently serves the needs of dedicated application systems but introduces significant challenges for multi-tasking environments where multiple users and applications compete for access to system resources. This thesis considers these challenges and presents three major contributions that enable efficient multi-tasking on heterogeneous systems. The presented contributions are compatible with existing systems, remain portable across vendors and do not require application changes or recompilation. The first contribution of this thesis is an optimization technique that reduces host-device communication overhead for OpenCL applications. It does this without modification or recompilation of the application source code and is portable across platforms. This work enables efficiency and performance improvements for diverse application workloads found on multi-tasking systems. The second contribution is the design and implementation of a secure, user-space virtualization layer that integrates the accelerator resources of a system with the standard multi-tasking and user-space virtualization facilities of the commodity Linux OS. It enables fine-grained sharing of mixed-vendor accelerator resources and targets heterogeneous systems found in data center nodes and requires no modification to the OS, OpenCL or application. Lastly, the third contribution is a technique and software infrastructure that enable resource sharing control on accelerators, while supporting software managed scheduling on accelerators. The infrastructure remains transparent to existing systems and applications and requires no modifications or recompilation. In enforces fair accelerator sharing which is required for multi-tasking purposes...|$|R
40|$|Static task {{scheduling}} in computational grids {{is very important}} because of the optimal usage of computing time for scheduling algorithms. Given a set of resources, a static scheduler computes the execution schedule before runtime. In static {{task scheduling}} resource information and performance parameters {{are assumed to be}} known depending on how a job can be divided, relevant research can be categorized into two different areas: divisible <b>workload,</b> <b>scheduling</b> where they can divided workload into arbitraty-sizes. Solving this problem dynamically needs more time. Therefore an attempt is made to solve it by meta-heuristic techniques. A new GA scheduler, GASAScheduler is presented whose run-time depends on the number of tasks in scheduling problem. The computation time to find sub-optimal function improved. The result shows the computation time of the proposed algorithms is better...|$|R
40|$|As {{computation}} is outsourced to the "clouds" and Mobile platforms, {{the differences}} in latency and responsiveness across these platforms presents new challenges in scheduling. To ensure the quality of service (QoS), a variety of <b>workload</b> <b>scheduling</b> techniques have been adopted by the service providers. However, these techniques often ignore the flexibility available in the service level agreements (SLAs) and focus on improving the performance rather than energy efficiency. This thesis explores opportunities for energy savings. For instance, the slackness in the job execution presents a great potential for energy savings. To achieve energy efficiency, we exploit the available slack opportunities from the SLAs to <b>schedule</b> <b>workload</b> under bounded latency requirements. We devise spatio-temporal deferral techniques for load balancing in the Cloud spanning the three layers of geographically distributed data centers, capacity provisioning inside a data center and job scheduling inside a server. Globally, we devise deferral techniques for load balancing by dynamic assignment and migration of jobs to the more available and cheaper sources of energy. Inside a data center, we devise server capacity provisioning techniques utilizing the flexibility of temporal-deferral to dynamically ̀right-size' the cluster of servers for energy proportionality. We also investigate the impact of dynamic deferral on user satisfaction and devise techniques to capture the loss in value of deferred execution by utility functions and make a trade-off between user satisfaction and energy efficiency. In addition, we devise path consolidation techniques to achieve power-proportionality in the data center networks. Integration with renewable energy sources presents another important means to improve energy efficiency. It allows us to not only tradeoff energy use quantity against costs but also makes the "quality" of energy an important consideration. The quality cost {{may be due to}} the time energy is used or the source of energy with or without carbon-cost. We devise workload shaping techniques to increase renewable energy usage for computing workload in data centers and EV charging/discharging load for smart grid. We investigate how much renewable power to store and how much workload to delay for increasing renewable usage while meeting latency constraints. In this dissertation, we analytically prove the quality of our proposed techniques for <b>workload</b> <b>scheduling</b> and experimentally show that significant energy savings can be achieved via dynamic deferra...|$|R
50|$|On the June 7, 2008, Lee {{announced}} that Shawn Elliott had left 1UP Yours to become exclusive to GFW Radio due to <b>workload</b> and <b>schedule</b> issues. It was also {{announced that}} longtime producer Andrew Pfister {{would join the}} cast in a speaking role, with John Davison returning permanently after a few guest appearances.|$|R
40|$|With {{the growth}} of {{cloud-based}} services, cloud data centers are experiencing large growth. A key component in a cloud data center is the network technology deployed. In particular, Ethernet technology, commonly deployed in cloud data centers, is already envisioned for 10 Tbps Ethernet. In this paper, we study and analyze the makespan, workload execution times, and virtual machine migrations as the network speed increases. In particular, we consider homogeneous and heterogeneous data centers, virtual machine <b>scheduling</b> algorithms, and <b>workload</b> <b>scheduling</b> algorithms. Results obtained from our study indicate {{that the increase in}} a network's speed reduces makespan and workloads execution times, while aiding in the increase of the number of virtual machine migrations. We further observed that the number of migrations' behaviors in relation to the speed of the networks also depends on the employed virtual machines scheduling algorithm...|$|R
40|$|As {{clusters}} {{are being}} deployed {{to support a}} wide range of parallel <b>workloads,</b> <b>scheduling</b> becomes a challenging research issue because these workloads exhibit diverse characteristics and impose vary-ing quality-of-service requirements. Many scheduling strategies are thus proposed, each intended for a different application/system setting. Due to the lack of a uniform simulation platform, a significant amount of research effort is spent in building a unique simulator for each algorithm, which may lead to false conclusions. This article presents ClusterSchedSim, which is a unifying simulation frame-work of cluster scheduling strategies. The core of ClusterSchedSim includes the node model and an interconnect model. ClusterSchedSim has implemented variations of popular cluster scheduling schemes, and it is flexible enough to add on new schemes. Using ClusterSchedSim, one can con-veniently compare different scheduling schemes, profile their executions, and understand the impact of different application and system configuration parameters...|$|R
