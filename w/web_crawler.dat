599|382|Public
5|$|To {{convert the}} {{backlink}} data gathered by BackRub's <b>web</b> <b>crawler</b> into {{a measure of}} importance for a given web page, Brin and Page developed the PageRank algorithm, and realized {{that it could be}} used to build a search engine far superior to existing ones. The new algorithm relied on a new kind of technology that analyzed the relevance of the backlinks that connected one Web page to another, and allowed the number of links and their rank, to determine the rank of the page. Combining their ideas, the pair began utilizing Page's dormitory room as a machine laboratory, and extracted spare parts from inexpensive computers to create a device that they used to connect the nascent search engine with Stanford's broadband campus network. After filling Page's room with equipment, they then converted Brin's dorm room into an office and programming center, where they tested their new search engine designs on the Web. The rapid growth of their project caused Stanford's computing infrastructure to experience problems.|$|E
25|$|Xenon is a <b>web</b> <b>crawler</b> used by {{government}} tax authorities to detect fraud.|$|E
25|$|Open Search Server is {{a search}} engine and <b>web</b> <b>crawler</b> {{software}} release under the GPL.|$|E
5000|$|The {{most common}} web {{archiving}} technique uses <b>web</b> <b>crawlers</b> to automate {{the process of}} collecting <b>web</b> pages. <b>Web</b> <b>crawlers</b> typically access <b>web</b> pages {{in the same manner}} that users with a browser see the Web, and therefore provide a comparatively simple method of remote harvesting web content. Examples of <b>web</b> <b>crawlers</b> used for <b>web</b> archiving include: ...|$|R
2500|$|As {{noted by}} Koster, {{the use of}} <b>Web</b> <b>crawlers</b> is useful {{for a number of}} tasks, but comes with a price for the general community. The costs of using <b>Web</b> <b>crawlers</b> include: ...|$|R
40|$|<b>Web</b> <b>crawlers</b> visit {{internet}} applications, collect data, {{and learn}} about new web pages from visited pages. <b>Web</b> <b>crawlers</b> have a long and interesting history. Early <b>web</b> <b>crawlers</b> collected statistics about the web. In addition to collecting statistics about the web and indexing the applications for search engines, modern crawlers {{can be used to}} perform accessibility and vulnerability checks on the application. Quick expansion of the web, and the complexity added to web applications have made the process of crawling a very challenging one. Throughout the history of web crawling many researchers and industrial groups addressed different issues and challenges that <b>web</b> <b>crawlers</b> face. Different solutions have been proposed to reduce the time and cost of crawling. Performing an exhaustive crawl is a challenging question. Additionally capturing the model of a modern web application and extracting data from it automatically is another open question. What follows is a brief history of different technique and algorithms used from the early days of crawling up to the recent days. We introduce criteria to evaluate the relative performance of <b>web</b> <b>crawlers.</b> Based on these criteria we plot the evolution of <b>web</b> <b>crawlers</b> and compare their performanc...|$|R
25|$|Coffman et al. {{worked with}} a {{definition}} of the objective of a <b>Web</b> <b>crawler</b> that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They {{also noted that the}} problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the <b>Web</b> <b>crawler</b> is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the <b>Web</b> <b>crawler.</b>|$|E
25|$|Frontera is web {{crawling}} framework implementing crawl frontier component and providing scalability primitives for <b>web</b> <b>crawler</b> applications.|$|E
25|$|A <b>Web</b> <b>crawler</b> {{may also}} be called a Web spider, an ant, an {{automatic}} indexer, or (in the FOAF software context) a Web scutter.|$|E
50|$|Since Lynx {{will take}} keystrokes from a text file, {{it is still}} very useful for {{automated}} data entry, web page navigation, and web scraping, thus Lynx is used in some <b>web</b> <b>crawlers.</b> <b>Web</b> designers may use Lynx to determine the way search engines and <b>web</b> <b>crawlers</b> see the sites they develop. Online services that provide Lynx's view of a given web page are available.|$|R
2500|$|... a parallelization {{policy that}} states how to {{coordinate}} distributed <b>web</b> <b>crawlers.</b>|$|R
5000|$|Focused <b>crawlers,</b> {{which are}} <b>Web</b> <b>crawlers</b> guided by page topic classifiers.|$|R
25|$|HTTrack uses a <b>Web</b> <b>crawler</b> {{to create}} a mirror of a web site for {{off-line}} viewing. It is written in C and released under the GPL.|$|E
25|$|A <b>Web</b> <b>crawler,</b> {{sometimes}} called a spider, is an Internet bot that systematically browses the World Wide Web, typically {{for the purpose}} of Web indexing (web spidering).|$|E
25|$|A {{recent study}} {{based on a}} large scale {{analysis}} of robots.txt files showed that certain web crawlers were preferred over others, with Googlebot being the most preferred <b>web</b> <b>crawler.</b>|$|E
5000|$|Robots, whether <b>web</b> <b>crawlers</b> from valid sources or {{malicious}} internet bots.|$|R
5000|$|... a parallelization {{policy that}} states how to {{coordinate}} distributed <b>web</b> <b>crawlers.</b>|$|R
40|$|<b>Web</b> <b>crawlers</b> have a {{long and}} {{interesting}} his-tory. Early <b>web</b> <b>crawlers</b> collected statistics about the web. In addition to collecting statistics about the web and indexing the applications for search engines, modern crawlers {{can be used to}} perform accessibility and vulnerability checks on the appli-cation. Quick expansion of the web, and the complex-ity added to web applications have made the pro-cess of crawling a very challenging one. Through-out the history of web crawling many researchers and industrial groups addressed different issues and challenges that <b>web</b> <b>crawlers</b> face. Different solu-tions have been proposed to reduce the time and cost of crawling. Performing an exhaustive crawl is a challenging question. Additionally, capturing the model of a modern web application and extracting data from it automatically is another open question. What follows is a brief history of different tech-niques and algorithms used from the early days of crawling up to the recent days. We introduce cri-teria to evaluate the relative performance and ob-jective of <b>web</b> <b>crawlers.</b> Based on these criteria we plot the evolution of <b>web</b> <b>crawlers.</b> Copyright c © IBM Canada Ltd., 2013. Permission to copy is hereby granted provided the original copyright notice is re-produced in copies made. ...|$|R
25|$|Apache Nutch is {{a highly}} {{extensible}} and scalable <b>web</b> <b>crawler</b> written in Java and released under an Apache License. It is based on Apache Hadoop {{and can be used}} with Apache Solr or Elasticsearch.|$|E
25|$|The Web {{has a very}} dynamic nature, and {{crawling}} {{a fraction}} of the Web can take weeks or months. By the time a <b>Web</b> <b>crawler</b> has finished its crawl, many events could have happened, including creations, updates, and deletions.|$|E
25|$|Swiftbot is Swiftype's <b>web</b> <b>crawler,</b> {{designed}} specifically for indexing a single or small, defined group of web sites to create a highly customized search engine. It enables unique features such as real-time indexing that are unavailable to other enterprise search providers.|$|E
5000|$|... an {{integration}} of a SQL database (using the MySQL) for i.e. <b>web</b> <b>crawlers</b> ...|$|R
40|$|Abstract — To search any {{information}} on the web users extensively use the search engines. As {{the growth of the}} World Wide Web exceeded all expectations, the search engines rely on <b>web</b> <b>crawlers</b> to maintain the index of billions of pages for efficient searching. The <b>web</b> <b>crawlers</b> have to interact with millions of hosts and retrieve the pages continuously to keep the index up-to-date. According to literature survey, most of the network traffic and bandwidth is consumed by <b>web</b> <b>crawlers</b> so instead of using them, we are proposing mobile crawler developed using JINI technology with the help of remote page selection, filtration and compression at web servers and not search engine...|$|R
25|$|StormCrawler, a {{collection}} of resources for building low-latency, scalable <b>web</b> <b>crawlers</b> on Apache Storm (Apache License).|$|R
25|$|The {{main problem}} in focused {{crawling}} {{is that in}} the context of a <b>Web</b> <b>crawler,</b> we would like to be able to predict the similarity of the text of a given page to the query before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton in the first <b>web</b> <b>crawler</b> of the early days of the Web. Diligenti et al. propose using the complete content of the pages already visited to infer the similarity between the driving query and the pages that have not been visited yet. The performance of a focused crawling depends mostly on the richness of links in the specific topic being searched, and a focused crawling usually relies on a general Web search engine for providing starting points.|$|E
25|$|To {{convert the}} {{backlink}} data gathered by BackRub's <b>web</b> <b>crawler</b> into {{a measure of}} importance for a given web page, Brin and Page developed the PageRank algorithm, and realized {{that it could be}} used to build a search engine far superior to existing ones. The new algorithm relied on a new kind of technology that analyzed the relevance of the backlinks that connected one Web page to another.|$|E
25|$|Robozilla is a <b>Web</b> <b>crawler</b> {{written to}} check the status of all sites listed in DMOZ. Periodically, Robozilla will flag sites which appear to have moved or {{disappeared}} and editors follow up {{to check the}} sites and take action. This process is critical for the directory in striving to achieve one of its founding goals: to reduce the link rot in web directories. Shortly after each run, the sites marked with errors are automatically moved to the unreviewed queue where editors may investigate them when time permits.|$|E
5000|$|Web search engines&rsquo; <b>web</b> <b>crawlers</b> may pay {{particular}} attention to the words used in the title.|$|R
25|$|<b>Web</b> <b>crawlers</b> {{typically}} {{identify themselves}} to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out {{more information about the}} <b>crawler.</b> Examining <b>Web</b> server log is tedious task, and therefore some administrators use tools to identify, track and verify <b>Web</b> <b>crawlers.</b> Spambots and other malicious <b>Web</b> <b>crawlers</b> are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler.|$|R
40|$|Abstract: This is {{the world}} of information. The size of world wide web [4, 5] is growing at an {{exponential}} rate day by day. The information on the web is accessed through search engine. These search engines [8] uses <b>web</b> <b>crawlers</b> to prepare the repository and update that index at an regular interval. These <b>web</b> <b>crawlers</b> [3, 6] are the heart of search engines. <b>Web</b> <b>crawlers</b> continuously keep on crawling the web and find any new web pages that {{have been added to}} the web, pages that have been removed from the web and reflect all these changes in the repository of the search engine so that the search engines produce the most up to date results...|$|R
25|$|A <b>Web</b> <b>crawler</b> {{starts with}} a list of URLs to visit, called the seeds. As the crawler visits these URLs, it {{identifies}} all the hyperlinks in the page and adds them to the list of URLs to visit, called the crawl frontier. URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as they were on the live web, but are preserved as ‘snapshots'.|$|E
2500|$|Octoparse, [...] a free {{client-side}} Windows <b>web</b> <b>crawler</b> {{written in}} [...]NET.|$|E
2500|$|The {{behavior}} of a <b>Web</b> <b>crawler</b> is {{the outcome of a}} combination of policies: ...|$|E
50|$|<b>Web</b> <b>crawlers</b> {{typically}} {{identify themselves}} to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out {{more information about the}} <b>crawler.</b> Examining <b>Web</b> server log is tedious task, and therefore some administrators use tools to identify, track and verify <b>Web</b> <b>crawlers.</b> Spambots and other malicious <b>Web</b> <b>crawlers</b> are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler.|$|R
50|$|BotSeer {{had also}} {{had set up}} a {{honeypot}} to test the ethics, performance and behavior of <b>web</b> <b>crawlers.</b>|$|R
50|$|<b>Web</b> <b>crawlers</b> on {{different}} hosts need to query {{each other to}} synchronize an indexed URI. Whereas one approach is to program each crawler to receive and respond to such queries, the client-queue-client approach is to store the indexed content from both crawlers in a passive queue, such as a relational database, on another host. Both <b>web</b> <b>crawlers</b> read and write to the database, but never communicate with each other.|$|R
