15|10000|Public
40|$|This paper {{describes}} a positioning method for wearable augmented reality systems. To realize augmented reality systems, {{the position and}} orientation of user’s viewpoint should be obtained in real time. In the proposed method, the system measures the orientation of user’s viewpoint by an inertial sensor and the user’s position by combining speci-fication of the user’s absolute position and dead reckoning. This paper also describes prototype of <b>wearable</b> <b>augmented</b> <b>reality</b> <b>system</b> using the proposed positioning method. ...|$|E
40|$|We {{describe}} an experimental <b>wearable</b> <b>augmented</b> <b>reality</b> <b>system</b> that enables users to experience hypermedia presentations that are {{integrated with the}} actual outdoor locations {{to which they are}} are relevant. Our mobile prototype uses a tracked see-through head-worn display to overlay 3 D graphics, imagery, and sound on top of the real world, and presents additional, coordinated material on a hand-held pen computer. We have used these facilities to create several situated documentaries that tell the stories of events that took place on our campus. We describe the software and hardware that underly our prototype system and explain the user interface that we have developed for it...|$|E
40|$|This paper {{presents}} {{methodology for}} integrating a small, singlepoint laser range finder into a <b>wearable</b> <b>augmented</b> <b>reality</b> <b>system.</b> We introduce a method using the laser range finder to incrementally build 3 D panoramas from a fixed observer’s location. To build a 3 D panorama semi-automatically, we track the system’s orientation {{and use the}} sparse range data acquired as the user looks around in conjunction with real-time image processing to construct geometry around the user’s position. Using full 3 D panoramic geometry, {{it is possible for}} new virtual objects to be placed in the scene with proper lighting and occlusion by real world objects, which increases the expressivity of the AR experience. 1...|$|E
40|$|This paper {{describes}} a <b>wearable</b> <b>augmented</b> <b>reality</b> (AR) <b>system</b> using invisible visual markers. Some AR systems use visual markers {{in order to}} measure the position and ori-entation of user’s viewpoint. However, conventional visual markers usually impair the scenery. This paper proposes a system using invisible visual markers consisting of translu-cent retro-reflectors. The system can be realized without undesirable visual effects and power supply for infrastruc-tures. ...|$|R
40|$|Abstract. We {{have been}} researching a {{software}} framework to facili-tate {{the development of}} a networked <b>wearable</b> <b>augmented</b> <b>reality</b> (AR) <b>system.</b> We developed the simple toolkit to provide three fundamental functionalities: database management, communication, and rendering. We then improved the communication functionality further and devel-oped a dynamic priority control technique for filtering AR annotations. This technique is able to give higher priority to the more necessary an-notations according to a user’s position and the angular velocity of the viewing direction. ...|$|R
40|$|This paper {{describes}} the prototype {{implementation of a}} pervasive, <b>wearable</b> <b>augmented</b> <b>reality</b> (AR) <b>system</b> based on a full body-motion-capture system using low-power wireless sensors. The system uses body motion to visualize and interact with virtual objects populating AR settings. Body motion is used to implement a whole body gesture-driven interface to manipulate the virtual objects. Gestures are mapped to correspondent behaviors for virtual objects, such as controlling the playback and volume of virtual audio players or displaying a virtual object’s metadata...|$|R
40|$|This paper {{discusses}} {{the potential for}} a hypothetical <b>wearable</b> <b>augmented</b> <b>reality</b> <b>system</b> entitled ‘Xuni AR’. By combining a number of separate devices and their individual capabilities, this system attempts explore a possible interactive mixed-reality experience. The {{purpose of this paper is}} to investigate the potential for this mobile-focused system to exist, and furthermore a potential visualization of the system in action. To do this a series of investigative experiments was conducted to test each device in turn and their possible integration and compatibility with a Unity 3 D software environment. Following this investigation a visualization was created. This was based on a range of academic research surrounding human computer interaction, gesture recognition and augmented user interface design...|$|E
40|$|This paper {{describes}} a <b>wearable</b> <b>augmented</b> <b>reality</b> <b>system</b> using positioning infrastructures and a pedometer. To realize augmented reality systems, {{the position and}} orientation of user's viewpoint should be obtained in real time. The proposed system measures the orientation of user's viewpoint by an inertial sensor and the user's position using positioning infrastructures in environments and a pedometer. The system specifies the user's position using the position ID received from RFID tags or IrDA markers which are the components of positioning infrastructures. When the user goes away from them, the user's position is alternatively estimated by using a pedometer. We have developed a navigation system using the proposed techniques and have proven the feasibility of the system with experiments...|$|E
40|$|To realize an {{augmented}} reality (AR) system using a wearable computer, the exact position and orientation of a user are required. We propose a localization method {{which is based}} on using an IR camera and invisible visual markers consisting of translucent retro-reflectors. In the method, to stably extract the regions of markers from the captured images, the camera captures the reflection of IR LEDs that are flashed on and off continuously. In experiments, we first describe the quantitative evaluation in computer simulation to decide the alignment of markers in real environments. We then carry out the localization experiments in real environments in which markers are installed based on the marker alignment decided by simulations. Finally, we develop a <b>wearable</b> <b>augmented</b> <b>reality</b> <b>system</b> using the proposed localization system...|$|E
40|$|This paper {{presents}} {{a set of}} pinch glove-based user interface tools for an outdoor <b>wearable</b> <b>augmented</b> <b>reality</b> computer <b>system.</b> The main form of user interaction {{is the use of}} hand and head gestures. We have developed a set of <b>augmented</b> <b>reality</b> information presentation techniques. To support direct manipulation, the following three selection techniques have been implemented: two-handed framing, line of sight and laser beam. A new glove-based text entry mechanism has been developed to support symbolic manipulation. A scenario for a military logistics task is described to illustrate the functionality of this form of interaction...|$|R
40|$|In this paper, {{we propose}} {{a method of}} {{personal}} positioning for a <b>wearable</b> <b>Augmented</b> <b>Reality</b> (AR) <b>system</b> that allows a user to freely move around indoors and outdoors. The user is equipped with selfcontained sensors, a wearable camera, an inertial head tracker and display. The method is based on sensor fusion of estimates for relative displacement caused by human walking locomotion and estimates for absolute position and orientation within a Kalman filtering framework. The former is based on intensive analysis of human walking behavior using self-contained sensors. The latter is based on image matching of video frames from a wearable camera with an image database that was prepared beforehand...|$|R
40|$|This paper {{presents}} {{a set of}} large object manipulation techniques implemented in a <b>wearable</b> <b>augmented</b> <b>reality</b> computer <b>system</b> that are optimised for the outdoor setting. These techniques supplement the current image plane approach, to provide a comprehensive solution to 3 D object manipulation in an <b>augmented</b> <b>reality</b> outdoor environment. The three extended manipulation techniques, Revolve, Xscale, and Ground plane translation, are focused on using what we determined {{to be the best}} coordinate system for object rotation, scaling and translation. This paper goes on to present the generalised plane technique for the constrained translation of graphical objects on arbitrary planes to enable more complex translation operations. The paper presents the techniques from both the user interface and software development perspectives. ...|$|R
40|$|This paper {{describes}} an augmented reality {{system with a}} wearable computer to overlay an annotation relative to the real world. To realize an augmented reality system, a position of user's viewpoint is needed for acquiring {{a relationship between the}} real and virtual coordinate systems. However, it is difficult that the user's viewpoint is accurately measured in both in- and out-door scene. In this paper, a <b>wearable</b> <b>augmented</b> <b>reality</b> <b>system</b> which can overlay an annotation on real scene is developed by using RFID tag and tag reader. The system can measure user's position by RFID tag set up at appointed position where user thinks that it need an information relative to real scene; for example, a front of a guide plate and a blanch point. The feasibility of the system has been successfully demonstrated with experimental...|$|E
40|$|ISMAR 2003 : IEEE / ACM International Symposium on Mixed and Augmented Reality, Oct 7 - 10, 2003, Tokyo, JapanThis paper {{describes}} a <b>wearable</b> <b>augmented</b> <b>reality</b> <b>system</b> using positioning infrastructures and a pedometer. To realize augmented reality systems, {{the position and}} orientation of user's viewpoint should be obtained in real time. The proposed system measures the orientation of user's viewpoint by an inertial sensor and the user's position using positioning infrastructures in environments and a pedometer. The system specifies the user's position using the position ID received from RFID tags or IrDA markers which are the components of positioning infrastructures. When the user goes away from them, the user's position is alternatively estimated by using a pedometer. We have developed a navigation system using the proposed techniques and have proven the feasibility of the system with experiments...|$|E
40|$|This paper {{describes}} a new visible portion estimation method of moving target object for networked wearable augmented reality (AR) system. In annotation overlay applications using AR systems, {{it is important}} to improve readability and intelligibility of annotations in a user’s view. View management makes it possible to appropriately generate annotation overlay images for users so as to intuitively understand annotations. For instance, overlap-pings of annotations and other objects can be prevented by using a view management technique. View management requires visible portions of 3 D target objects in a 2 D view plane. This paper proposes a visible portion estima-tion method for moving target objects based on positions and shapes of the moving target objects. The <b>wearable</b> <b>augmented</b> <b>reality</b> <b>system</b> obtains positions of target objects via wireless network for estimating visible portions of target objects in the user’s view. Annotations are able to be overlaid by using view management techniques with our proposed visible portion estimation...|$|E
40|$|Abstract- In this paper, we {{describe}} a novel natural feature based 3 -D object tracking method. Our method determines geometric relation between known 3 -D objects and a camera, not using fiducial markers. Since our method only uses a camera to determine this geometric relation, it {{is suitable for}} <b>wearable</b> <b>augmented</b> <b>reality</b> (AR) <b>systems.</b> Our method combines {{two different types of}} approaches for tracking: a bottom up approach (BUA) and a top down approach (TDA). We mainly use a BUA, because it acquires accurate results with small calculation cost. When BUA cannot output an accurate result, our method starts TDA to avoid mistracking. An experimental result shows an accuracy and integrity of our method. I...|$|R
40|$|In this paper, {{we present}} a <b>wearable</b> <b>Augmented</b> <b>Reality</b> (AR) <b>system</b> with {{personal}} positioning based on walking locomotion analysis that allows a user to freely move around indoors and outdoors. The user is equipped with self-contained sensors, a wearable camera, an inertial head tracker and display. The system {{is based on the}} sensor fusion of estimates for relative displacement caused by human walking locomotion and estimates for absolute position and orientation within a Kalman filtering framework. The former is based on intensive analysis of human walking behavior using self-contained sensors. The latter is based on image matching of video frames from a wearable camera with an image database that was prepared beforehand...|$|R
40|$|This paper {{deals with}} {{extending}} the AR Toolkit’s functionality to allow not only small stationary setups but wide range tracking applications as well. Based on a visionary scenario of a mobile user {{walking around in}} a building with an intelligent environment, we split the AR Toolkit’s functionality in several components based on the DWARF (Distributed <b>Wearable</b> <b>Augmented</b> <b>Reality</b> Framework) <b>system.</b> Using a thin mobile client we describe a mechanism for a dynamic configuration according to the location context during runtime. This means that the system triggers a context change with different strategies and then gets reconfigured by the environment. We discuss {{the architecture of the}} location component, evaluate several strategies for a context change and show how environmental conditions can influence th...|$|R
40|$|EGVE Symposium 2008 : 14 th Eurographics Symposium on Virtual Environments, May 29 - 30, 2008, Eindhoven, The NetherlandsThis paper {{describes}} a new visible portion estimation method of moving target object for networked wearable augmented reality (AR) system. In annotation overlay applications using AR systems, {{it is important}} to improve readability and intelligibility of annotations in a user's view. View management makes it possible to appropriately generate annotation overlay images for users so as to intuitively understand annotations. For instance, overlappings of annotations and other objects can be prevented by using a view management technique. View management requires visible portions of 3 D target objects in a 2 D view plane. This paper proposes a visible portion estimation method for moving target objects based on positions and shapes of the moving target objects. The <b>wearable</b> <b>augmented</b> <b>reality</b> <b>system</b> obtains positions of target objects via wireless network for estimating visible portions of target objects in the user's view. Annotations are able to be overlaid by using view management techniques with our proposed visible portion estimation...|$|E
40|$|Abstract—This paper {{describes}} {{the development of}} vision-aided navigation (i. e., pose estimation) for a <b>wearable</b> <b>augmented</b> <b>reality</b> <b>system</b> operating in natural outdoor environments. This system combines a novel pose estimation capability, a helmet-mounted see-through display, and a wearable processing unit to accurately overlay geo-registered graphics on the user’s view of reality. Accurate pose estimation is achieved through integration of inertial, magnetic, GPS, terrain elevation data, and computer-vision inputs. Specifically, a helmet-mounted forward-looking camera and custom computer vision algorithms are used to provide measurements of absolute orientation (i. e., orientation of the helmet {{with respect to the}} Earth). These orientation mea-surements, which leverage mountainous terrain horizon geometry and/or known landmarks, enable the system to achieve significant improvements in accuracy compared to GPS/INS solutions of similar size, weight, and power, and to operate robustly in the presence of magnetic disturbances. Recent field testing activities, across a variety of environments where these vision-based signals of opportunity are available, indicate that high accuracy (less than 10 mrad) in graphics geo-registration can be achieved. This paper presents the pose estimation process, the methods behind the generation of vision-based measurements, and representative experimental results. Index Terms—inertial navigation, augmented reality, computer vision...|$|E
40|$|Figure 1 : An example color {{panorama}} and semi-automatically generated {{depth map}} pair. Darker {{regions of the}} depth map are closer to the user. To generate both images the user simply has to look around the scene. Both images are composed of the four surrounding faces of a cube map, and are not warped to a cylindrical projection. This cube projection causes the strange peak on the roof line {{in the center of}} the images. This paper presents methodology for integrating a small, singlepoint laser range finder into a <b>wearable</b> <b>augmented</b> <b>reality</b> <b>system.</b> We first present a way of creating object-aligned annotations with very little user effort. Second, we describe techniques to segment and pop-up foreground objects. Finally, we introduce a method using the laser range finder to incrementally build 3 D panoramas from a fixed observer’s location. To build a 3 D panorama semiautomatically, we track the system’s orientation and use the sparse range data acquired as the user looks around in conjunction with real-time image processing to construct geometry around the user’s position. Using full 3 D panoramic geometry, it is possible for ne...|$|E
40|$|A <b>wearable</b> <b>augmented</b> <b>reality</b> (AR) <b>system</b> has {{received}} {{a great deal of}} attention as a new method for displaying location-based information in the real world. In wearable AR, it is required to precisely measure position and orientation of a user for merging the real and virtual worlds. This paper proposes a user localization system for wearable AR in indoor environments. To realize a localization system, it is necessary to easily construct environments for localization without producing undesirable visual effects. In the proposed system, wallpapers containing printed invisible markers are pasted on ceilings or walls. To construct environments for localization, this system contains a tool which calibrates the alignment of the markers from photos of the markers with digital still camera. The user's position and orientation are estimated by recognizing the markers using an infrared camera with infrared LEDs...|$|R
40|$|Recent {{developments}} in computing hardware {{have begun to}} make mobile and <b>wearable</b> <b>Augmented</b> <b>Reality</b> (AR) <b>systems</b> a <b>reality.</b> With this new freedom, AR systems can now {{be used in a}} very wide range of applications including disaster relief, localization and repair of utilities and even as an assistant for tourists walking through unfamiliar historical sites. This paper considers the use of AR to assist with the task of warfighting in an urban environment. Urban environments are compact, complicated, and can be highly dynamic. Any successful AR system must overcome a host of challenges including the need for a robust tracking systems and wearable hardware and must present the information to the user an intuitive and informative manner. This paper considers the problem of designing a user interface which avoids information overload through automatically managing the grahpical context which is displayed to the user. We describe the paradigm of an information filter - a decision mechanism that uses the user's location, the user's current goal and the properties of objects within the environment to deduce what information should be displayed...|$|R
40|$|Mobile {{services}} are already widespread used. It {{is often the}} first step for cell phones to become ubiquitous when {{they are able to}} submit location based information. Another application for mobile spatial aware systems is the combination of location with the computation of the head direction on a <b>wearable</b> computer. These <b>Augmented</b> <b>Reality</b> <b>systems</b> are mostly isolated applications. They offer a specific service in a specialized region, even in outdoor areas. When combining them with more widespread used localization techniques a whole of a city can be covered by a mobile service. This paper presents an approach, where several tracking techniques with different accuracies and reliability information, are offered within one system. A first implementation of this idea {{can be found in the}} research project GEIST, which allows an independent tour through a city...|$|R
40|$|This paper {{describes}} a <b>wearable</b> <b>augmented</b> <b>reality</b> <b>system</b> with an IrDA device and a passometer. To realize augmented reality systems, {{the position and}} orientation of user's viewpoint should be obtained in real time for aligning the real and virtual coordinate systems. In the proposed system, the orientation of user's viewpoint is measured by an inertial sensor attached to the user's glasses, and the position is measured by using an IrDA device and a passometer. First, the user's position is specified exactly when the user comes into the infrared ray range of the IrDA markers which are set up to the appointed points. When the user goes out of the infrared ray range, the user's position is estimated by using a passometer. The passometer is constructed of an electronic compass and acceleration sensors. The former can detect the user's walking direction. The latter can count how many steps the user walks. These data and the user's pace {{make it possible to}} estimate the user's position in the neighborhood of the IrDA markers. We have developed a navigation system based on using the techniques above and have proven the feasibility of the system with experiments...|$|E
40|$|Stereoscopic Displays and Virtual Reality Systems XII, Jan 21 - 23, 2003, Santa Clara, CA, USAThis paper {{describes}} a <b>wearable</b> <b>augmented</b> <b>reality</b> <b>system</b> with an IrDA device and a passometer. To realize augmented reality systems, {{the position and}} orientation of user's viewpoint should be obtained in real time for aligning the real and virtual coordinate systems. In the proposed system, the orientation of user's viewpoint is measured by an inertial sensor attached to the user's glasses, and the position is measured by using an IrDA device and a passometer. First, the user's position is specified exactly when the user comes into the infrared ray range of the IrDA markers which are set up to the appointed points. When the user goes out of the infrared ray range, the user's position is estimated by using a passometer. The passometer is constructed of an electronic compass and acceleration sensors. The former can detect the user's walking direction. The latter can count how many steps the user walks. These data and the user's pace {{make it possible to}} estimate the user's position in the neighborhood of the IrDA markers. We have developed a navigation system based on using the techniques above and have proven the feasibility of the system with experiments...|$|E
40|$|We {{are proposing}} to develop {{computational}} tools for researchers {{and students to}} model, visualize, and analyze historic and ancient sites. This proposal addresses four major scientific components to support this research. First, we are proposing new methods of creating complex, 3 -D, photorealistic models of large sites. This includes a mobile robot sensing system {{that can be used}} as an intelligent sensing device over a large scale. Second, we are proposing to develop new methods to image below-ground data accurately and efficiently. These methods are especially suited to modeling the wealth of subsurface information at archaeological sites. Third, we will be developing new database technology to catalog and access a site’s structures, artifacts, objects, and historical references. This will significantly improve a user’s ability to query and analyze a site’s information. Fourth, we have created a <b>wearable</b> <b>augmented</b> <b>reality</b> <b>system</b> for presenting georegistered information to mobile users, using overlaid graphics and sound. We will extend this system to create a new class of information visualization systems that integrate 3 D above- and below-ground models, 2 D images, text and other web-based resources to annotate the physical environment We will apply this system to support scientists in the field, as well to allow on-site and remote tours of historic and ancient sites. We are intrigued by the possibility of an historic site serving as the nexus for introducing complex ideas from a variety of disciplines: mathematics, earth sciences, economics, geography, literature, languages, politics, and history. Our ability to model, visualize and analyze the site {{is at the center of}} this endeavor. We have assembled a team of seasoned researchers from the Columbia community with scientific and historical preservation backgrounds who will work together to achieve our goal. Prof. Peter Allen provides experience in 3 D modeling, computer vision and robotics. Prof. Roelof Versteeg specializes in below-ground and noninvasive sensing. Prof. Kenneth Ross is an expert on databases and new methods of accessing data. Prof. Steven Feiner is at the forefront of user interface design and augmented reality. Prof. Lynn Meskell is an authority on Ancient Egyptian Archaeolog...|$|E
40|$|Advances in {{miniaturization}} and {{computing power}} {{have set the}} scene {{for the emergence of}} powerful wearable computer systems capable of active computer vision. A lightweight miniature multimedia computer can be worn by a user, receive input from a camera to sense the local environment, present virtual annotations on a heads up display, and network with others through a wireless modem. Applications include field repair, on site tele-medicine, and augmented tourism. The very notion of such a <b>wearable,</b> networked, <b>augmented</b> <b>reality</b> <b>system</b> has an inherent appeal, as it liberates computing from the desktop and integrates computing with everyday activities. The objective of this research is to demonstrate that <b>augmented</b> <b>reality</b> interfaces can be achieved using basic 1 -D or 2 -D imaging methods. The notion of Personal Context is introduced to address information overload in <b>augmented</b> <b>reality</b> by taking a user-centric model in gathering awareness and context information. Two functional personal context prototypes were built and discussed, showing potential for piano and dance applications. Balancing the body-centric focus of personal context, the idea of mosaicing as a world model for <b>augmented</b> <b>reality</b> registration and telecollaboration is also presented, and realized in another working proof-of-concept system. Finally, a systematic study on accuracy, reliability, and speed of existing and new mosaicing methods (including the method used in the mosaicing prototype) was conducted, identifying their strengths and weaknesses as engines for <b>augmented</b> <b>reality</b> world modelling...|$|R
40|$|Thesis (Ph. D.) [...] Memorial University of Newfoundland, 2002. Engineering and Applied ScienceBibliography: leaves 167 - 196 Advances in {{miniaturization}} and {{computing power}} {{have set the}} scene {{for the emergence of}} powerful wearable computer systems capable of active computer vision. A lightweight miniature multimedia computer can be worn by a user, receive input from a camera to sense the local environment, present virtual annotations on a heads up display, and network with others through a wireless modem. Applications include field repair, on site tele-medicine, and augmented tourism. The very notion of such a <b>wearable,</b> networked, <b>augmented</b> <b>reality</b> <b>system</b> has an inherent appeal, as it liberates computing from the desktop and integrates computing with everyday activities. The objective of this research is to demonstrate that <b>augmented</b> <b>reality</b> interfaces can be achieved using basic 1 -D or 2 -D imaging methods. The notion of "Personal Context" is introduced to address information overload in <b>augmented</b> <b>reality</b> by taking a user-centric model in gathering awareness and context information. Two functional personal context prototypes were built and discussed, showing potential for piano and dance applications. Balancing the body-centric focus of personal context, the idea of mosaicing as a world model for <b>augmented</b> <b>reality</b> registration and telecollaboration is also presented, and realized in another working proof-of-concept system. Finally, a systematic study on accuracy, reliability, and speed of existing and new mosaicing methods (including the method used in the mosaicing prototype) was conducted, identifying their strengths and weaknesses as engines for <b>augmented</b> <b>reality</b> world modelling...|$|R
40|$|In the past, {{architectures}} of <b>Augmented</b> <b>Reality</b> <b>systems</b> {{have been}} widely different and taylored to specific tasks. In this paper, we use {{the example of the}} SHEEP game to show how the structural flexibility of DWARF, our componentbased Distributed <b>Wearable</b> <b>Augmented</b> <b>Reality</b> Framework, facilitates a rapid prototyping and online development process for building, debugging and altering a complex, distributed, highly interactive AR system...|$|R
40|$|Usability {{engineering}} is a cost-effective, user-centered process that ensures {{a high level}} of effectiveness, efficiency, and safety in complex interactive systems. This paper presents a brief description of usability engineering activities, and discusses our experiences with leading usability engineering activities for three very different types of interactive applications: a responsive workbench-based command and control application called Dragon, a <b>wearable</b> <b>augmented</b> <b>reality</b> application for urban warfare called Battlefield <b>Augmented</b> <b>Reality</b> <b>System</b> (BARS), and a head-mounted hardware device, called Nomad, for dismounted soldiers. For each application, we present our approach to usability engineering, how we tailored the usabilit...|$|R
40|$|ISWC 2003 : IEEE International Symposium on Wearable Computers, Oct 20 - 23, 2003, Sanibel Island, Florida, USAThis paper {{describes}} a <b>wearable</b> <b>augmented</b> <b>reality</b> systemusing positioning infrastructures and a pedometer. Torealize <b>augmented</b> <b>reality</b> <b>systems,</b> {{the position and}} orientationof user's viewpoint should be obtained in real time. Theproposed system measures the orientation of user's viewpointby an inertial sensor and the user's position using positioninginfrastructures in environments and a pedometer. The system specifies the user's position using the positionID received from RFID tags or IrDA markers which are thecomponents of positioning infrastructures. When the usergoes away from them, the user's position is alternatively estimatedby using a pedometer. We have developed a navigationsystem using the proposed techniques and have proventhe feasibility of the system with experiments...|$|R
40|$|We {{present a}} mobile <b>augmented</b> <b>reality</b> <b>system</b> for outdoor {{annotation}} {{of the real}} world. To reduce user burden, we use aerial photographs {{in addition to the}} wearable system’s usual data sources (position, orientation, camera and user input). This allows the user to accurately annotate 3 D features with only a few simple interactions from a single position by aligning features in both their firstperson viewpoint and in the aerial view. We examine three types of aerial photograph features – corners, edges, and regions – that are suitable {{for a wide variety of}} useful mobile <b>augmented</b> <b>reality</b> applications, and are easily visible on aerial photographs. By using aerial photographs in combination with <b>wearable</b> <b>augmented</b> <b>reality,</b> we are able to achieve much higher accuracy 3 D annotation positions than was previously possible from a single user location...|$|R
40|$|Several <b>wearable</b> <b>augmented</b> <b>reality</b> {{devices have}} emerged in recent years. Although these devices target users with 20 / 20 vision, they have also been {{explored}} as low vision aids. However, such devices are still relatively inaccessible and expensive. This study explores one of the inexpensive commercial head-mounted see-through display, google cardboard, and a simple homemade <b>wearable</b> <b>augmented</b> <b>reality</b> display. The experimentation reveals that, although not perfect, the homemade device built using a smartphone and common household scrap items is the most promising platform for experimenting with visual aids...|$|R
50|$|In December 2016, ODG {{closed a}} $58 million Series A funding round with {{strategic}} investors including 21st Century Fox. The Series A {{is the largest}} {{ever in the history}} of <b>wearables,</b> <b>augmented</b> <b>reality,</b> and virtual reality.|$|R
40|$|Our goal is {{to further}} {{evaluate}} and formalize techniques and choices of input and output modalities for interaction in <b>wearable</b> outdoor <b>augmented</b> <b>reality.</b> Currently most outdoor <b>augmented</b> <b>reality</b> <b>systems</b> treat choices on interaction techniques and devices in an adhoc per application basis. We hope to formalize more general best practices for these fields to guide future researchers in developing their own systems. To address interaction techniques we will evaluate which techniques developed for virtual reality are still applicable for outdoor <b>augmented</b> <b>reality</b> for selection, manipulation and navigation, and develop and evaluate new techniques for annotation. Along with this we will evaluate and formulate best practices for use of devices in outdoor <b>augmented</b> <b>reality,</b> and modalities of input to the interaction system. ...|$|R
