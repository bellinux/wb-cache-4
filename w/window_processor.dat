10|53|Public
40|$|Methods of data {{transaction}} acceleration in global-bus multi-processor systems {{based on}} multiple <b>window</b> <b>processor</b> local memory access are proposed. Possibility of transaction synchronization through data and control buses is considered. ???????????? ?????? ????????? ?????? ??????? ? ?????????????????? ???????? ? ????? ???????????, ?????????? ?? ????????????? ??????? ??????? ? ????????? ?????? ???????????. ??????????????? ??????????? ????????????? ???????? ?????? ?? ???? ?????? ? ???? ??????????...|$|E
40|$|Large {{instruction}} window processors achieve {{high performance}} by exposing {{large amounts of}} instruction level parallelism. However, accessing large hardware structures typically required to buffer and process such instruction window sizes significantly degrade the cycle time. This paper proposes a novel Checkpoint Processing and Recovery (CPR) microarchitecture, and shows how to implement a large instruction <b>window</b> <b>processor</b> without requiring large structures thus permitting a high clock frequency...|$|E
40|$|While {{the central}} window {{implementation}} in a superscalar processor {{is an effective}} approach to waking up ready instructions, this implementation does not scale to large instruction window sizes as those that are required by wide issue superscalars of the future. We propose a new wake-up algorithm that dynamically associates explicit wake-up lists with executing instructions according to the dependences between instructions. Instead of repeatedly examining a waiting instruction for wake-up till it can be issued, this algorithm identifies and considers for wake-up a fresh subset of waiting instructions from the instruction window in each cycle. This subset of instructions are the ones present in the wake-up lists of completing instructions. The direct wake-up microarchitecture (DWMA) that we present is able to achieve approximately 80 %, 75 % and 63 % {{of the performance of}} a central <b>window</b> <b>processor</b> at high issue widths of 8, 16 and 32 respectively when an effective memory disambiguation mechanism [13] is employed for load speculation. In contrast, the wake-up algorithm proposed in constructing the dependence-based microarchitecture (DBMA) [15] can only achieve approximately 62 %, 46 % and 45 % of the performance of a central <b>window</b> <b>processor</b> at issue widths of 8, 16 and 32. Thus the DWMA processor greatly outperforms the DBMA processor...|$|E
50|$|With earlier {{versions}} of <b>Windows,</b> <b>processor</b> drivers along with Cool'n'Quiet software {{also need to be}} installed.|$|R
3000|$|... {{configurable}} <b>window</b> <b>processors.</b> The architecture {{was implemented}} on an FPGA to execute algorithms with window sizes up to [...]...|$|R
5000|$|Joe Guthridge - Led the {{development}} of Samna Amí, the first <b>Windows</b> word <b>processor,</b> later renamed Lotus Word Pro ...|$|R
40|$|A design {{methodology}} for {{the definition of}} parallel architectures for image processing is presented. The architectural design is based on functional blocks, operating on limited regions of neighboring pixels (windows) in the image plane. This window-based computational granularity allows the coverage {{of a wide range}} of image processing algorithms, with particular reference to image enhancement and analysis. The window-based formulation and decomposition of the algorithms is derived, making possible the identification of the architectural characteristics of the <b>window</b> <b>processor</b> and of the required interconnection structures. Evaluations of the architectural complexity guaranteed by this approach are given in terms of temporal and spatial requirements...|$|E
40|$|We {{consider}} {{a variety of}} dynamic, hardware-based methods for exploiting load/store parallelism, including mechanisms that use memory dependence speculation. While previous work has also investigated such methods [19, 4], this has been done primarily for split, distributed <b>window</b> <b>processor</b> models. We focus on centralized, continuous-window processor models (the common configuration today). We confirm that exploiting load/ store parallelism can greatly improve performance. Moreover, we show that much of this performance potential can be captured if addresses of the memory locations accessed by both loads and stores {{can be used to}} schedule loads. However, using addresses to schedule load execution may not always be an option due to complexity, latency, and cost considerations. For thi...|$|E
40|$|Checkpoint {{prediction}} {{and intelligent}} management have been recently proposed {{for reducing the}} number of coarse-grain checkpoints needed to achieve high performance through speculative execution. In this work, we {{take a closer look at}} various checkpoint prediction and management alternatives, comparing their performance and requirements as the scheduler window size increases. We also study a few additional design choices. The key contribution of this work is BranchTap, a novel checkpoint-aware speculation strategy that temporarily throttles speculation to reduce recovery cost while allowing speculation to proceed when it is likely to boost performance. BranchTap dynamically adapts to application behavior. We demonstrate that for a 1 K-entry <b>window</b> <b>processor</b> with a FIFO of just four checkpoints, our adaptive speculation control mechanism leads to an average performance degradation of just 1. 49 % compared to a processor that has an infinite number of checkpoints. This represents an improvement of 28. 3 % over using just predictionbased checkpoint allocation. Average performance degradation without BranchTap is 2. 08 %. For the same configuration, BranchTap decreases the worst case deterioration from 8. 99 % t...|$|E
5000|$|In a {{multiprocessor}} system running Microsoft <b>Windows,</b> a <b>processor</b> may interrupt another processor {{for the following}} reasons, {{in addition to the}} ones listed above: ...|$|R
40|$|Large {{instruction}} <b>window</b> <b>processors</b> {{can achieve}} high performance by supplying more instructions during long latency load misses, thus effectively hiding these latencies. Continual Flow Pipeline (CFP) architectures provide high-performance by effectively {{increasing the number}} of actively executing instructions without increasing the size of the cycle-critical structures. A CFP consists of a Slice Processing Unit which stores missed loads and their forward slice inside a Slice Data Buffer. This makes it possible to open up the resources occupied by these idle instructions to new instructions. In this project, we have designed and implemented CFP on top of Simplescalar. Further, we have compared conventional pipelines to CFPs by running them on various benchmarks in SPEC integer benchmarks suite. We also studied the behavior of mispredicted branches dependent on load misses, which {{turn out to be the}} main bottleneck in CFPs. We also compare the performance of CFPs with ideal and non-ideal fetch mechanisms...|$|R
5000|$|PAQ8JD was {{released}} on December 30, 2006 by Bill Pettis. This version has since been ported to 32 bit <b>Windows</b> for several <b>processors,</b> and 32 and 64 bit Linux.|$|R
40|$|Building {{processors}} {{with large}} instruction windows {{has been proposed}} as a mechanism for overcoming the memory wall, but finding a feasible and implementable design has been an elusive goal. Traditional processors are composed of structures that do not scale to large instruction windows because of timing and power constraints. However, the behavior of programs executed with large instruction windows {{gives rise to a}} natural and simple alternative to scaling. We characterize this phenomenon of execution locality and propose a microarchitecture to exploit it to achieve the benefit of a large instruction <b>window</b> <b>processor</b> with low implementation cost. Execution locality is the tendency of instructions to exhibit high or low latency based on their dependence on memory operations. In this paper we propose a decoupled microarchitecture that executes low latency instructions on a Cache Processor and high latency instructions on a Memory Processor. We demonstrate that such a design, using small structures and many in-order components, can achieve the same performance as much more aggressive proposals while minimizing design complexity. ...|$|E
40|$|Large {{instruction}} window processors achieve {{high performance}} by exposing {{large amounts of}} instruction level parallelism. However, accessing large hardware structures typically required to buffer and process such instruction window sizes significantly degrade the cycle time. This paper proposes a novel checkpoint processing and recovery (CPR) microarchitecture, and shows how to implement a large instruction <b>window</b> <b>processor</b> without requiring large structures thus permitting a high clock frequency. We focus on four critical aspects of a microarchitecture: (1) scheduling instructions, (2) recovering from branch mispredicts, (3) buffering {{a large number of}} stores and forwarding data from stores to any dependent load, and (4) reclaiming physical registers. While scheduling window size is important, we show the performance of large instruction windows to be more sensitive to the other three design issues. Our CPR proposal incorporates novel microarchitectural schemes for addressing these design issues—a selective checkpoint mechanism for recovering from mispredicts, a hierarchical store queue organization for fast store-load forwarding, and an effective algorithm for aggressive physical register reclamation. Our proposals allow a processor to realize performance gains due to instruction windows of thousands of instructions without requiring large cycle-critical hardware structures...|$|E
40|$|The design, fabrication, {{and testing}} of a state-of-the-art, {{high-throughput}} on-focal plane infrared-image signal processor is described. The processing functions performed are frame differencing and thresholding. The final focal plane array will consist of a 128 x 128 -pixel platinum-suicide detector bump-mounted to an onchip CCD multiplexer. The processor is in a 128 -channel parallel-pipeline format. Eh channel consists of a pixel regenerator (charge differencer), 128 -pixel frame store CCD memory, pixel differencer, second pixel regenerator, thresholder (analog comparator), and digital latch. Four parallel analog outputs and four parallel digital outputs are included. The digital outputs provide a bit map of the image. All analog clock signals (128 KHz, 256 KHz, and 5 MHz) are generated by on-chip TFL-input clock drivers. TTL clock driver inputs are generated off-chip. The technology is low-temperature surface and buried channel CCD,MOS/inthum bump. It is believed {{that this is the}} first time that all of these technologies are integrated into one device. The design goal was 8 -bit resolution at 77 K and 1000 frames per second. The chip is 890 x 900 mils 2. Applications include point- or extended-target motion detection with thresholding. Design trade-offs and enhancements (such as on-chip detector gain compensation and a simple <b>window</b> <b>processor)</b> are discussed. I...|$|E
40|$|Image {{processing}} {{requires more}} computational power and data throughput than most conventional processors can provide. Designing specific hardware can improve execution time and achieve better performance {{per unit of}} silicon area. A field-programmable-gate-array- (FPGA-) based configurable systolic architecture specially tailored for real-time window-based image operations is presented in this paper. The architecture {{is based on a}} 2 D systolic array of 7 ÃƒÂ— 7 configurable <b>window</b> <b>processors.</b> The architecture was implemented on an FPGA to execute algorithms with window sizes up to 7 ÃƒÂ— 7, but the design is scalable to cover larger window sizes if required. The architecture reaches a throughput of 3. 16 GOPs at a 60 MHz clock frequency and a processing time of 8. 35 milliseconds for 7 ÃƒÂ— 7 generic window-based operators on 512 ÃƒÂ— 512 gray-level images. The architecture compares favorably with other architectures in terms of performance and hardware utilization. Theoretical and experimental results are presented to demonstrate the architecture effectiveness...|$|R
50|$|Word Pro {{was based}} upon Ami Pro (originally {{published}} by Samna), but was substantially rewritten (including a new native document format). The predecessor to Ami Pro, Amí, was released in 1988, {{and was the first}} fully functional <b>Windows</b> word <b>processor.</b> (The <b>Windows</b> version of Microsoft Word would not debut until early 1989.) Shortly after the release of Amí, the development team added support for tables and renamed the product Ami Pro.|$|R
3000|$|A set {{of tests}} were {{performed}} {{using the following}} default values: 100 instantiation requests, 8 <b>processors,</b> <b>window</b> size of 10 requests, generation cycles of 10 and population size of 10. As instantiation requests' length (t [...]...|$|R
40|$|Graduation date: 1997 A {{relatively}} recent {{development in the}} late 1980 s in processors has been the superscalar processor. Superscalar processors use multiple pipelines {{in an attempt to}} achieve higher performance than previous generations of processors. Having multiple pipelines makes it possible to execute more than one instruction per cycle. However, since instructions are not independent of one another, but are interdependent, {{there is no guarantee that}} any given sequence of instruction will take advantage of the wider pipeline. One major factor that governs the ability of a processor to discover parallel instructions is the processor's mechanism for decoding and executing instruction. For superscalar processors with the central window design, the number of parallel instructions discovered is dependent on the size of the window. With a large window, the probability that the processor can find more parallel instructions is higher because there are more instruction to choose from. However, the larger the window the longer the critical path and thus lower clock speed. The major theme of this thesis is to find ways to have a large instruction window but still have clock speed comparable to a small instruction <b>window</b> <b>processor.</b> One way to achieve this is to apply the idea of memory interleaving to the processor's instruction window or reservation station design. With interleaving, there are multiple small instruction windows instead of one large window. In the first cycle the first window is used, and the second window is used in the second clock cycle. After all windows are used, the processor returns to the first window. Therefore with the interleaved design only a small portion of the whole instruction window is active at one time. In this way, there can be a large virtual window. Furthermore since the size of individual window is kept small, the clock speed is not affected. The rest of this thesis will explain how this interleaved instruction window scheme works and also list some simulation results to show its performance...|$|E
50|$|HP {{encouraged}} independent software vendors {{to produce}} versions of applications which {{took advantage of}} NewWave functionality, allowing their data to be handled as objects instead of files. One early example was Samna Corporation (later acquired by Lotus) who produced an edition of their Microsoft <b>Windows</b> word <b>processor</b> Ami Pro entitled ‘’Ami Pro for NewWave’’. On June 20, 1988 Microsoft Corporation and Hewlett-Packard issued a press release announcing the inclusion of NewWave support in an up-coming release Microsoft Excel.|$|R
25|$|MacWrite, Microsoft Word, {{and other}} word {{processing}} {{programs for the}} bit-mapped Apple Macintosh screen, introduced in 1984, were probably the first true WYSIWYG word processors to become known to many people until the introduction of Microsoft <b>Windows.</b> Dedicated word <b>processors</b> eventually became museum pieces.|$|R
40|$|Multicore {{processors}} {{have emerged}} as a powerful platform on which to efficiently exploit thread-level parallelism (TLP). However, due to Amdahl’s Law, such designs will be increasingly limited by the remaining sequential components of applications. To overcome this limitation {{it is necessary to}} design processors with many lower–performance cores for TLP and some high-performance cores designed to execute sequential algorithms. Such cores will need to address the memory-wall by implementing kilo-instruction <b>windows.</b> Large <b>window</b> <b>processors</b> require large Load/Store Queues that would be too slow if implemented using current CAMbased designs. This paper proposes an Epoch-based Load Store Queue (ELSQ), a new design based on Execution Locality. It is integrated into a large-window processor that has a fast, out-of-order core operating only on L 1 /L 2 cache hits and N slower cores that process L 2 misses and their dependent instructions. The large LSQ is coupled with the slow cores and is partitioned into N small and local LSQs, one per core. We evaluate ELSQ in a large-window environment, finding that it enables high performance at low power. By exploiting locality among loads and stores, ELSQ outperforms even an idealized central LSQ when implemented on top of a decoupled processor design. ...|$|R
50|$|Microsoft has {{distributed}} Times New Roman {{with every}} copy of Microsoft Windows since version 3.1, and the typeface {{is used as}} the default in many applications for MS <b>Windows,</b> especially word <b>processors</b> and Web browsers. (Calibri became the default font for Microsoft Word beginning with Microsoft Office 2007).|$|R
2500|$|... /NOEXECUTE={OPTIN|OPTOUT|ALWAYSON|ALWAYSOFF} [...] This {{option is}} only {{available}} on 32-bit versions of Windows when running on processors supporting Data Execution Prevention (DEP). It enables DEP, {{which results in}} the memory manager marking pages containing data as no-execute so that they cannot be executed as code. This can be useful for preventing malicious code from exploiting buffer overflow bugs with unexpected program input in order to execute arbitrary code. No-execute protection is always enabled on 64-bit versions of <b>Windows</b> on <b>processors</b> that support no-execute protection. There are several options the user can specify with this switch: ...|$|R
50|$|<b>Windows</b> NT <b>processor</b> {{scheduling}} {{refers to}} {{the process by which}} Windows NT determines which job (task) should be run on the computer processor at which time. Without scheduling, the processor would give attention to jobs based on when they arrived in the queue, which is usually not optimal. As part of the scheduling, the processor gives a priority level to different processes running on the machine. When two processes are requesting service at the same time, the processor performs the jobs for the one with the higher priority.|$|R
40|$|Hard-to-predict {{branches}} {{depending on}} longlatency cache-misses {{have been recognized}} as a major performance obstacle for modern microprocessors. With the widening speed gap between memory and microprocessors, such long-latency branch mispredictions also waste substantial power/energy in executing instructions on wrong paths, especially for large instruction <b>window</b> <b>processors.</b> This paper presents a novel program locality that can be exploited to handle long-latency hard-topredict branches. The locality {{is a result of}} an interesting program execution behavior: for some applications, major data structures or key components of the data structures tend to remain stable for a long time. If a hard-to-predict branch depends on such stable data, the address of the data rather than the data value is sufficient to determine the branch outcome. This way, a misprediction can be resolved much more promptly when the data access results in a long-latency cache miss. We call such locality addressbranch correlation and we show that certain memoryintensive benchmarks, especially those with heavy pointer chasing, exhibit this locality. We then propose a low-cost auxiliary branch predictor to exploit address-branch correlation. Our experimental results show that the proposed scheme reduces the execution time by 6. 3 % (up to 27 %) and energy consumption by 5. 2 % (up to 24 %) for a set of memory-intensive benchmarks with a 9 kB prediction table when used with a state-of-art 16 kB TAGE predictor. 1...|$|R
40|$|Conventional {{processors}} use a fully-associative store queue (SQ) {{to implement}} store-load forwarding. Associative search latency does not scale well to capacities and bandwidths required by wide-issue, large <b>window</b> <b>processors.</b> In this work, we improve SQ scalability by implementing store-load forwarding using speculative indexed access rather than associative search. Our design uses prediction {{to identify the}} single SQ entry from which each dynamic load {{is most likely to}} forward. When a load executes, it either obtains its value from the predicted SQ entry (if the address of the entry matches the load address) or the data cache (otherwise). A forwarding mis-prediction — detected by pre-commit filtered load re-execution — results in a pipeline flush. SQ index prediction is generally accurate, but for some loads it cannot reliably identify a single SQ entry. To avoid flushes on these difficult loads while keeping the single-SQ-access-per-load invariant, a second predictor delays difficult loads until all but the youngest of their 2 ̆ 2 candidate 2 ̆ 2 stores have committed. Our predictors are inspired by store-load dependence predictors for load scheduling (Store Sets and the Exclusive Collision Predictor) and unify load scheduling and forwarding. Experiments on the SPEC 2000 and MediaBench benchmarks show that on an 8 -way issue processor with a 512 -entry reorder buffer, our technique performs within 3. 3...|$|R
5000|$|... /NOEXECUTE={OPTIN|OPTOUT|ALWAYSON|ALWAYSOFF} [...] - [...] This {{option is}} only {{available}} on 32-bit versions of Windows when running on processors supporting Data Execution Prevention (DEP). It enables DEP, {{which results in}} the memory manager marking pages containing data as no-execute so that they cannot be executed as code. This can be useful for preventing malicious code from exploiting buffer overflow bugs with unexpected program input in order to execute arbitrary code. No-execute protection is always enabled on 64-bit versions of <b>Windows</b> on <b>processors</b> that support no-execute protection. There are several options the user can specify with this switch: ...|$|R
50|$|With a superscalar <b>processor,</b> the {{instruction}} <b>window</b> of the <b>processor</b> fills {{up with a}} number of instructions (known as the issue rate). Depending on the scheme that the superscalar processor uses to dispatch these instruction from the window to the execution core of the CPU, we may encounter problems if there is a dependency not unlike the one shown above.|$|R
40|$|Branch mispredictions {{are a major}} {{obstacle}} to exploiting instruction-level parallelism, {{at least in part}} because all instructions after a mispredicted branch are squashed. However, instructions that are control independent of the branch must be fetched regardless of the branch outcome, and do not necessarily have to be squashed and re-executed. Control independence exists when the two paths following a branch re-converge. A trace processor microarchitecture is developed to exploit control independence and thereby reduce branch misprediction penalties. There are three major contributions. 1) Trace-level re-convergence is not guaranteed despite re-convergence at the instruction-level. Novel trace selection techniques are developed to expose control independence at the trace-level. 2) Control independence 's potential complexity stems from insertion and removal of instructions {{from the middle of the}} instruction <b>window.</b> Trace <b>processors</b> manage control flow hierarchically (traces [...] ...|$|R
5000|$|Although AMD [...] "verified" [...] {{the ability}} for {{computers}} with Ryzen <b>processors</b> to boot <b>Windows</b> 7 and Windows 10, Microsoft only officially supports Ryzen on computers running Windows 10 per support policies. Windows Update blocks updates from being installed on Ryzen systems running versions older than Windows 10. In support of this position, AMD only provides drivers for <b>Windows</b> 10. Ryzen <b>processors</b> are compatible with Linux; the full performance of Ryzen is enabled in kernel version 4.10 or newer.|$|R
25|$|The IBM System i {{includes}} an extensive library-based operating system, IBM i, {{and is also}} capable of supporting multiple instances of AIX, Linux, Lotus Domino, Microsoft Windows 2000 and Windows Server 2003. While IBM i, AIX, Linux and Lotus Domino are supported on the POWER <b>processors,</b> <b>Windows</b> is supported with either single-processor internal blade servers (IXS) or externally linked multiple-processor servers (IXA and iSCSI). iSCSI also provides support for attachment of IBM Bladecenters. Windows, Linux, and VMware ESX(VI3) are supported on iSCSI attached servers.|$|R
5000|$|By {{the early}} 1980s, {{the chaos and}} {{incompatibility}} that was rife in the early microcomputer market {{had given way to}} a smaller number of de facto industry standards, including the S-100 bus, CP/M, the Apple II, Microsoft BASIC in read-only memory (ROM), and the [...] inch floppy drive. No single firm controlled the industry, and fierce competition ensured that innovation in both hardware and software was the rule rather than the exception. Microsoft <b>Windows</b> and Intel <b>processors</b> gained ascendance and their ongoing alliance gave them market dominance.|$|R
50|$|The IBM System i {{includes}} an extensive library-based operating system, IBM i, {{and is also}} capable of supporting multiple instances of AIX, Linux, Lotus Domino, Microsoft Windows 2000 and Windows Server 2003. While IBM i, AIX, Linux and Lotus Domino are supported on the POWER <b>processors,</b> <b>Windows</b> is supported with either single-processor internal blade servers (IXS) or externally linked multiple-processor servers (IXA and iSCSI). iSCSI also provides support for attachment of IBM Bladecenters. Windows, Linux, and VMware ESX(VI3) are supported on iSCSI attached servers.|$|R
5000|$|Released in 2013. Includes a 13.3" [...] LED backlit {{touch screen}} display with 1920×1080 {{resolution}} and Corning Gorilla Glass, Intel Core fourth generation i5-4200U or i7-4500U <b>processor,</b> <b>Windows</b> 8.1 (64-bit), Intel HD Graphics 4400, 4GB or 8GB Dual Channel DDR3 1600 MHz RAM, 128GB or 256GB mSATA SSD, Intel Dual Band Wireless-AC 7260 + Bluetooth wireless, and 55 WHr 6-Cell Battery. Note: Intel Chipset is 4th generation I series. However, Dell has not fixed the issue with this generation - a high pitched noise emitting from the keyboard area to the right.|$|R
50|$|In June 2008, Impinj {{sold its}} {{non-volatile}} memory business to Virage Logic.Also in 2008, Impinj acquired the Intel RFID division, including an Intel-developed RFID reader chip. Impinj renamed the chip Indy R1000.In 2009, Coca-Cola unveiled their Freestyle soda machine that gives users one hundred different possible drink combinations. The Freestyle soda machine uses Impinj Monza tag chips and Indy reader chips to determine user preferences and {{to monitor the}} dispensers.Mexico has certified the Impinj Speedway reader {{to be used by}} state agencies in the electronic vehicle registration initiative beginning in Mexico in July 2010.In 2005, Impinj began working with Intel to develop RFID chips that would allow for “Processor Secured Storage.”Impinj created two new chips for the project: Monza X-2K Dura and Monza X-8K Dura,which allow for increased theft deterrence and wireless configuration of electronic devices. The chips will be used in Intel's Microsoft <b>Windows</b> 8-based <b>processors</b> for tablet computers, which will be released {{in the second half of}} 2012.|$|R
