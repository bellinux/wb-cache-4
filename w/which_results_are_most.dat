3|10000|Public
40|$|Simulations on liquid {{loads and}} flow rates which the Taitel-Dukler model predicts to have {{multiple}} solutions {{have been performed}} with ANSYS Fluent and LedaFlow. Both steady state and transient results in one, two and three dimensional flows are reported in this work. The hypothesis that the holdup of a pipe operated in the multiple solution region {{will be determined by}} the downstream holdup is investigated. Some results indicate that the hypothesized interface level gradients effects are correct. The Fluent steady state simulations had mass imbalance issues in addition to being both grid and geometry dependent, but produced results consistent with the independent Fluent transient simulations. The one dimension LedaFlow solver illustrated the effect shear stress modeling have on the multivalued solution region. The solver chose the intermediate solution for some flow rates, which by physical arguments can be excluded. The novel solver LedaFlow Q 3 D produced transient results displaying the wavy surface of the high holdup solutions. The results from the different models are deviating, {{but it is hard to}} predict <b>which</b> <b>results</b> <b>are</b> <b>most</b> the accurate since no comparison with experimental results have been conducted. </p...|$|E
40|$|Many anomaly {{detection}} methods, depending on various parameters, {{have been proposed}} in literature. Given the diversity of available anomaly detectors, from an operational viewpoint {{it is interesting to}} determine an efficient strategy to find the best suited detector for a given application. This is not obvious, especially in scenes with a highly structured background. The work presented here proposes a generic approach to the problem by examining the following questions: How different are the results of the various anomaly detectors? Are the parameters influencing the results significantly? Are there classes of methods sufficiently similar so that one can test only one of each class and see <b>which</b> <b>results</b> <b>are</b> <b>most</b> adequate for a given application? What are the spectral/spatial characteristics of the differences between methods? Can one predict which detector will give the best results for a given application? The current paper tries to answer the first three questions by comparing results of different types of anomaly detectors applied to different complex (urban, industrial and harbor) scenes. The comparison is not in absolute terms because it does not rely on a priori ground truth. In stead the detectors are compared relative to one another, the aim being to evaluate the similarities between the performance of the detectors and the dependency of their results on the used parameters, i. e. the inter- and intra method consistency. Index Terms — Anomaly detection, hyperspectral, clustering, segmentatio...|$|E
40|$|Word {{similarity}} is {{a semantic}} measure that evaluates {{the similarity of}} words. The goal of the master 2 ̆ 7 s thesis is to implement a graph-based knowledgebase and compute similarity between two word forms based on their connections in the graph. The graph is built {{as a foundation for}} the similarity calculations. As primary resources for the graph construction, we use natural text descriptions (senses and example use) from each phrase. The main source that we use for extracting similarity knowledge is WordNet. The dataset Google Books Ngram Viewer is integrated to enhance the connections between similar word forms. As a consequence, we build a data structure that connects not only words that have mutual relationships (synonym, hyponym and hypernym), but also words that appear in the same context. ^ The phrase graph that we build has words from the English language as vertices. Vertices are interconnected to each other with edges which carry a similarity weight. The advantage of this approach is that the search algorithm utilizes a robust approach that connects phrases that are similar in meaning with high coefficient values. The similarity of the meanings is not only calculated by the synonym, hyponym, and hypernym relationships, but also from {{the context in which the}} words appear. It is assumed that words which appear in senses together, or are mentioned in an example use sentence, have a higher probability of being similar. This way, it is expected that words that are different in their basic meaning, but are commonly related to each other in everyday use will have high similarity coefficients. ^ Measuring word similarity is a technique that can be used for engineering ranking algorithms. The ranking algorithms decide <b>which</b> <b>results</b> <b>are</b> <b>most</b> relevant and should be displayed first in the results list. This approach has the advantage of recognizing similar meanings of words and ranking them high in the results list, despite the difference in their natural text descriptions. ...|$|E
3000|$|The <b>result</b> <b>which</b> <b>is</b> <b>most</b> {{closely related}} to that given here was shown by Grotowski [8]. Grotowski {{obtained}} boundary partial regularity results for more general systems: [...]...|$|R
30|$|The {{quantitative}} {{evidence for}} prospect-refuge theory remains inconsistent. It is especially problematic that the <b>results</b> <b>which</b> <b>are</b> <b>most</b> commonly cited in architecture relate to studies of natural environments, not interiors or urban environments. As this paper demonstrates, the <b>results</b> <b>are</b> <b>most</b> valid in specific venues.|$|R
30|$|In {{this section}} we present some {{analytical}} <b>results</b> <b>which</b> {{end with the}} proof of uniqueness of the solution to (1.1)-(1.2). In the following lemma we introduce a positivity <b>result</b> <b>which</b> <b>is</b> the <b>most</b> important to establish our main results.|$|R
40|$|Adaptive query {{expansion}} (QE) {{allows users}} to better define their search domain by supplementing the original query with additional terms related to their preferences and information needs. The system we present {{is an extension of}} the traditional QE techniques, which rely on the computation of two-dimensional co-occurrence matrices. Our system makes use of three-dimensional co-occurrence matrices, where the added dimension is represented by semantic classes (i. e., categories comprising all the terms that share a semantic property) related to the folksonomy extracted from social bookmarking services such as delicious, Digg, and StumbleUpon. The generation of the user profile occurs through the creation of a model that is dynamically updated using the information gleaned from the searches (visited pages and corresponding search queries). The system analyzes the input queries and, if they actually reflect the interests already shown by the user in previous searches, it returns different QEs involving different semantic fields. The output of the system is structured in different blocks categorized through keywords, thus helping the user judge <b>which</b> <b>result</b> <b>is</b> <b>most</b> relevant to him. The results of an experimental evaluation involving real users are reported...|$|R
50|$|Labour {{held the}} seat, {{but with a}} {{significantly}} reduced majority, as the Conservatives picked up votes. While the National Front only won 6.2% of the vote, this enabled them to beat the Liberals into an embarrassing fourth place. Three of the socialist parties won around 1% of the votes each, a <b>result</b> <b>which</b> <b>was</b> <b>most</b> disappointing for the Workers Revolutionary Party, who had stood in the constituency in previous years and had stood a well-known candidate.|$|R
5000|$|... or some {{reflection}} or {{rotation of}} one of these. These patterns cannot be killed off by their surroundings. It is the [...] "only if" [...] part of this <b>result</b> <b>which</b> <b>is</b> <b>most</b> interesting. If none of these patterns is present at t = 0, then in any bounded region the pattern eventually settles down to all zeros. [...] The key tool in the proof in [...] is a [...] "winding number" [...] which is shown to be invariant for this model.|$|R
30|$|Personality in {{this matter}} is a {{continuum}} and we labelled (as AMN did) {{one end of the}} continuum as positive and summed scores. A higher score does not mean more personality, but another personality. The pooled estimation <b>results</b> (<b>which</b> <b>are</b> <b>most</b> precise) indicate for study programs that require the most developed personality traits (the reference PT programs) that students whose personality traits are conductive to success tend to dropout relatively more often. The programs that fall within this category are for example nursing and social work.|$|R
40|$|The {{potential}} radiological {{consequences to}} humans resulting from aqueous releases at the Savannah River Site (SRS) have usually been assessed using the computer code LADTAP or deterministic variations of this code. The advancement of LADTAP {{over the years}} included LADTAP II (a computer program that still resides on the mainframe at SRS) [1], LADTAP XL{copyright} (Microsoft Excel{reg_sign} Spreadsheet) [2], and other versions specific to SRS areas such as [3]. The spreadsheet variations of LADTAP contain two worksheets: LADTAP and IRRIDOSE. The LADTAP worksheet estimates dose for environmental pathways including ingestion of water and fish and external exposure resulting from recreational activities. IRRIDOSE estimates potential dose to individuals from irrigation of food crops with contaminated water. A new version of this deterministic methodology, LADTAP-PROB, was developed at Savannah River National Laboratory (SRNL) to (1) consider the complete range of the model parameter values (not just maximum or mean values), (2) determine the influences of parameter uncertainties within the LADTAP methodology, to perform a sensitivity analysis of all model parameters (to identify the input parameters to <b>which</b> model <b>results</b> <b>are</b> <b>most</b> sensitive), and (3) probabilistically assess radiological consequences from contaminated water. This study presents the methodology applied in LADTAP-PROB...|$|R
40|$|Two {{separate}} {{field studies}} of chemists and physicists were made which {{were concerned with}} how scientists spend their time, especially the amount and kind of communication in which they engage. Approximately 2, 200 chemists and physicists were observed. The studies were also concerned with finding out what journals scientists read, what parts of journals, the purposes {{for which they are}} read, and the variance of costs of disseminating information through journals, as well as the effect which characteristics of employing organizations have on the way a scientist spends his time. Outstanding features of the research design and those <b>results</b> <b>which</b> <b>are</b> <b>most</b> likely to <b>be</b> of general scientific interest are presented in this article. ...|$|R
40|$|The {{aim is to}} {{investigate}} the effect of availability and level of detail of data on cost estimates of alcohol consumption. Using the recent Swedish cost of alcohol study as baseline, limitations on data are applied, forming two models. The costs of alcohol in Sweden are re-estimated in the two models and compared to the baseline, to establish the magnitude {{and direction of the}} bias resulting from limited data, and <b>which</b> <b>results</b> <b>are</b> the <b>most</b> sensitive to variations in data availability and level of detail. Almost all differences between the baseline and the two limited models stem from reduced availability and not the level of detail of data. However, the level of detail plays an important role for the prevalence of consumption and the alcohol-attributable fractions (AAFs) for injuries. The conclusion is that {{it is more important to}} estimate more cost components than to improve existing estimates, as differences between cost estimates are mainly driven by the availability rather than the level of detail of data. However, the level of detail in the prevalence rates of consumption and in the AAFs for injuries is likely to considerably affect the cost estimates and obtaining the best possible data in these areas should therefore be prioritized...|$|R
30|$|The <b>results</b> <b>which</b> <b>are</b> <b>most</b> {{closely related}} to that given here were shown in [12] and [13]. In this paper, we would get the desired {{conclusions}} by the method of A-harmonic approximation. The A-harmonic approximation technique is {{a natural extension of}} harmonic approximation technique. In [14] Simon used harmonic approximation method to simplify Allard’s [15] regularity theorem and later on Schoen and Uhlenbeck’s [16] regularity result for harmonic maps. The idea was generalized to more general linear operators by Duzaar and Steffen [17], {{in order to deal with}} the regularity of almost minimizers to elliptic variational integrals in the setting of geometric measure theory. As a by-product Duzaar and Grotowski [18] were able to use the idea of A-harmonic approximation to deal with elliptic systems under quadratic growth, even to the boundary points for nonlinear elliptic systems [19] and variational problems [20].|$|R
40|$|Although many Americans {{assume that}} the {{education}} provided by public schools will prepare them for higher learning and/ or the workforce, {{recent studies have shown}} that American students' test scores lag behind their counterparts around the world and a growing number are not prepared for graduation. School choice, in the form of publicly funded vouchers for low-income students, has caused significant debate as a form of education reform. Proponents argue that vouchers will induce competition between schools, help low-income students obtain a better education, and increase parental satisfaction. Opponents of school vouchers argue that publicly funded vouchers will drain public schools of much needed resources, leave the most difficult to educate students in public schools, and violate the constitution by funding sectarian institutions. School voucher programs have been implemented in New York, Florida, Cleveland and Milwaukee yet there is no consensus in terms of the effect of vouchers on achievement scores. This thesis looks at four school choice programs, evaluates and compares the design/ methodologies of the program evaluations, and draws conclusion about <b>which</b> <b>results</b> <b>are</b> the <b>most</b> reliable and why. (cont.) Once the methodologies and findings of the evaluations were analyzed, I found that most programs did not {{have a significant effect on}} achievement test scores of voucher recipients and did not induce competition between public and private schools. by Shawntel B. Hines. Thesis (M. C. P.) [...] Massachusetts Institute of Technology, Dept. of Urban Studies and Planning, 2007. Includes bibliographical references (p. 63 - 64) ...|$|R
40|$|Abstract—Internet {{measurement}} studies {{often require}} pro-longed probing of remote targets to collect information, yet almost all such studies {{of which we}} are aware were undertaken without considering the polluting effects of their unconstrained probing behavior. To help researchers conduct experiments in a more responsible fashion, we present a framework and technique that enables efficient execution of large-scale periodic probing without exceeding pre-set limits on probing rates. Our technique employs a novel scheduling algorithm and leverages knowledge of diurnal traffic patterns to make data collection more efficient (e. g., by probing servers only during periods in <b>which</b> probe <b>results</b> will <b>be</b> <b>most</b> useful). We evaluate our technique {{in the context of a}} real-world study and show that it substantially outperforms naı̈ve probing strategies for accomplishing the same goal, sending more probes during useful periods, fewer probes overall, and probing at more precise intervals as required by our measurement applications. Keywords-probe scheduling; responsible network experiments. I...|$|R
40|$|Biological soil {{attributes}} {{have shown}} to be good indicators of soil changes {{as a result of}} the management function. The aim of this study was to evaluate the effect of using cover crops, as well as planting and tillage systems on the biological attributes of a yellowish red latosol soil. Soil samples were taken at 0 to 0. 10 m depth, seven days before the bean harvest. Microbial biomass carbon and nitrogen, basal soil respiration, metabolic ratio and total enzyme activity were evaluated in this study. The best agroecological management was achieved under the association of the ground cover with millet and in direct seeding because they showed higher soil microbial biomass carbon and nitrogen content and lower metabolic quotient, being pork bean the best plant coverage. All biological soil attributes were sensitive to the tillage system, which showed the best results of the total enzyme activity and of the soil metabolic quotient <b>which</b> <b>resulted</b> to <b>be</b> the <b>most</b> efficient...|$|R
40|$|Approximately 5 % of all breast cancers {{are due to}} one of {{the high-risk}} breast cancer genes BRCA 1 and BRCA 2, or {{possibly}} to a third or fourth moderate- to high-risk gene(s). A further proportion of cases arise {{in the presence of a}} less striking family history, with later average age at onset and lower penetrance: familial breast cancer. Bilaterality is a recognized feature of hereditary breast cancer. Cancers often present at an early age, with the contralateral risk high within 10 years. Proof that bilateral malignancies are separate primaries can be difficult histologically, however, especially within 3 years. The recent finding of specific pathological features related to BRCA 1 and, to a lesser extent, BRCA 2 mutations means that, in addition to bilaterality and family history, a pathological element can be entered into the risk calculation for the presence of BRCA 1 /BRCA 2 mutations. This will facilitate the targeting of mutation testing to families in <b>which</b> a positive <b>result</b> <b>is</b> <b>most</b> likely, and may subsequently influence the clinical management of these families...|$|R
40|$|The ADAPT-VPA {{assessment}} methodology of Butterworth et al. (1999) {{is applied to}} abundance estimates (from both IDCR/SOWER and JARPA surveys) and catch at age data (both commercial and scientific) for Areas IV and V. The methodology is extended {{to be able to}} take account of inter-annual differences in the distribution of the population between the two Areas when they are assessed jointly. An important feature of these updated <b>results</b> <b>is</b> that revised JARPA estimates of abundance are shown to be statistically comparable with estimates from the IDCR/SOWER programme (i. e. calibration factor not significantly different from 1). The general pattern shown by <b>results</b> <b>is</b> of a minke whale abundance trend that increased over the middle decades of the 20 th Century to peak at about 1970, and then declined for the next three decades. The recruitment trend is similar, though with its peak slightly earlier. The factor to <b>which</b> the <b>results</b> <b>are</b> <b>most</b> sensitive <b>is</b> the value of natural mortality M. The assessments do show retrospective patterns, primarily related to changes in the best estimate of M as time has progressed. This in turn seems linked to the IDCR/SOWER survey trends suggesting higher, and the JARPA survey trends lower estimates of M. For the assessment of the two Areas combined, M is estimated at 0. 068 with a CV of 0. 12; this compares with CVs of typically 0. 35 for the Area-specific assessments of Butterworth et al. (1999), which were based on eight seasons’ fewer data. The paper reflects an account of work in progress, and suggestions are made of areas where further analyses would be desirable...|$|R
40|$|International audienceFrench {{environmental}} law for nature protection requires {{that all the}} facilities, works, and development projects that may affect the environment should {{be the subject of}} an impact study to evaluate their consequences, including on human health. For this analysis, the risk assessment approach is used and the population's exposure is estimated with the aid of multimedia models. The CalTOX model is frequently used for this kind of study. Unfortunately, the analysis of these studies shows that the model is often badly understood and poorly used. The difficulties encountered by the users, the errors and the problems met in the interpretation of <b>results,</b> <b>which</b> <b>are</b> the <b>most</b> commonly found in the human exposure assessment, are listed and their consequences illustrated. CalTOX has been shown to have many advantages (adaptability, speed in carrying out calculations, transparency), but it ought not {{to be used as a}} "black box" because such a use may lead to many errors and a loss of confidence in the studies...|$|R
40|$|The Daya Bay reactor {{neutrino}} experiment {{aimed to}} precisely measure the least known mixing angle θ 13. In March 2012, Daya Bay announced the non-zero value {{with more than}} 5 σ. With more statistics, less background and better control of systematics, the Daya Bay experiment is continuously improving the precision of sin 2 2 θ 13 {{as well as the}} effective neutrino mass squared differences. In this paper, I will report the recent oscillation <b>results,</b> <b>which</b> <b>are</b> the <b>most</b> precise measurement of the oscillation parameter θ 13 and |Δmee 2 |. With 1230 days of data, sin 2 2 θ 13 is measured to be [8. 41 ± 0 : 27 (stat.) ± 0. 19 (syst.) ]× 10 - 2, and |Δmee 2 | = [2. 50 ± 0. 06 (stat.) ± 0. 06 (syst.) ]× 10 - 3 eV 2 with X 2 /NDF = 232. 6 / 263. An independent measurement with the inverse beta decay neutron captured on hydrogen will also be presented...|$|R
40|$|Sleep {{and emotion}} are closely linked, however {{the effects of}} sleep on socio-emotional task {{performance}} have only recently been investigated. Sleep loss and insomnia {{have been found to}} affect emotional reactivity and social functioning, although <b>results,</b> taken together, <b>are</b> somewhat contradictory. Here we review this advancing literature, aiming to 1) systematically review the relevant literature on sleep and socio-emotional functioning, with reference to the extant literature on emotion and social interactions, 2) summarize results and outline ways in which emotion, social interactions, and sleep may interact, and 3) suggest key limitations and future directions for this field. From the reviewed literature, sleep deprivation is associated with diminished emotional expressivity and impaired emotion recognition, and this has particular relevance for social interactions. Sleep deprivation also increases emotional reactivity; <b>results</b> <b>which</b> <b>are</b> <b>most</b> apparent with neuro-imaging studies investigating amygdala activity and its prefrontal regulation. Evidence of emotional dysregulation in insomnia and poor sleep has also been reported. In general, limitations of this literature include how performance measures are linked to self-reports, and how <b>results</b> <b>are</b> linked to socio-emotional functioning. We conclude by suggesting some possible future directions for this field...|$|R
40|$|Objectives: This study {{explored}} {{the views of}} healthcare-professionals (HCPs) in the UK about what information should be disclosed; when; and whether women/parents {{should be given a}} choice as to what they wish to know. Methods: Q-methodology was used to assess the views of forty HCPs (genetic healthcare professionals, fetal medicine experts, lab-scientists). Results: Most participants agreed that variants of unknown clinical significance should not be disclosed. Participants were divided between those who considered variants of uncertain clinical significance helpful for parents and clinicians, and those who considered them harmful. Although recognising the potential disadvantages of disclosing risks for adult-onset conditions, participants thought {{it would be difficult to}} withhold such information once identified. Participants largely supported some parental involvement in determining <b>which</b> <b>results</b> should <b>be</b> returned. <b>Most</b> participants believed that information obtained via CMA testing in pregnancy should either be disclosed during pregnancy, or not at all. Conclusion: HCPs taking part in the study largely believed that variants that will inform the management of the pregnancy, or are relevant to other family members, should be reported. Recent UK guidelines, published after this research was completed, reflect these opinions...|$|R
40|$|Expansion of the {{completeness}} of {{examination of}} the investment appeal of the enterprises considering all set of interests of the specific investor in an integral indicator, and providing sufficiency of reliability represents an actual task. On the basis of refining of the concept “investment appeal of the enterprise” the economic-mathematical model of an assessment of investment appeal of industrial enterprises providing possibility of step-by-step selection of options of forming of its investment image at decrease in the risks decision making corresponding to interests of the specific investor is developed. Methodical approach, the implementing its algorithm and the software product at the choice of options of forming of investment image or a choice of investees with the subsequent assessment of attractive options and receipts of the aggregate <b>result</b> <b>which</b> <b>is</b> <b>most</b> corresponding to preferences of the specific investor is developed. The practical recommendations about a choice of acceptable ranges of change of critical parameters of an integral indicator for the recommended assessment model allowing to prove decisions on the accepted options on management of investment appeal of the enterprises or the chosen investees are developed...|$|R
40|$|Objectives This study {{explored}} {{the views of}} healthcare professionals (HCPs) in the UK about what information should be disclosed, when; and whether women/parents {{should be given a}} choice as to what they wish to know. Methods Q-methodology was used to assess the views of 40 HCPs (genetic HCPs, fetal medicine experts, lab-scientists). Results Most participants agreed that variants of unknown clinical significance should not be disclosed. Participants were divided between those who considered variants of uncertain clinical significance helpful for parents and clinicians, and those who considered them harmful. Although recognising the potential disadvantages of disclosing risks for adult-onset conditions, participants thought {{it would be difficult to}} withhold such information once identified. Participants largely supported some parental involvement in determining <b>which</b> <b>results</b> should <b>be</b> returned. <b>Most</b> participants believed that information obtained via CMA testing in pregnancy should either be disclosed during pregnancy, or not at all. Conclusion HCPs taking part in the study largely believed that variants that will inform the management of the pregnancy, or are relevant to other family members, should be reported. Recent UK guidelines, published after this research was completed, reflect these opinions. © 2016 The Authors. Prenatal Diagnosis published by John Wiley & Sons, Ltd...|$|R
40|$|Abstract—The two-tier {{architecture}} {{consisting of a}} small number of resource-abundant storage nodes in the upper tier and a large number of sensors in the lower tier could be promising for large-scale sensor networks in terms of resource efficiency, network capacity, network management complexity, etc. In this architecture, each sensor having multiple sensing capabilities periodically forwards the multidimensional sensed data to the storage node, which responds to the queries, such as range query, top- query, and skyline query. Unfortunately, node compromises pose the great challenge of securing the data collection; the sensed data could be leaked to or could be manipulated by the com-promised nodes. Furthermore, chunks of the sensed data could <b>be</b> dropped maliciously, <b>resulting</b> in an incomplete query <b>result,</b> <b>which</b> <b>is</b> the <b>most</b> difficult security breach. Here, we propose a simple yet effective hash tree-based framework, under which data confidentiality, query result authenticity, and query <b>result</b> completeness can <b>be</b> guaranteed simultaneously. In addition, the subtree sampling technique, which could be of independent interest to the other applications, is proposed to efficiently identify the compromised nodes. Last, analytical and extensive simulation studies are conducted to evaluate the performance and security of our methods. Prototype implementation on TelosB mote demon-strates the practicality of our proposed methods. Index Terms—Multidimensional query, secure query, sensor net-work...|$|R
30|$|Despite {{what seems}} to be {{widespread}} acceptance in the architectural, urban and landscape design fields, the quantitative evidence for prospect-refuge theory remains inconsistent. It is especially problematic that the <b>results</b> <b>which</b> <b>are</b> <b>most</b> commonly cited in architecture relate to studies of natural environments, not interiors or urban environments. As this paper demonstrates, the <b>results</b> <b>are</b> <b>most</b> valid in specific venues. For example, the benefits of a close visual connection to nature and of inhabiting a space that offers both an open area for outlook and a more private area for being hidden, have been broadly supported by past research in natural settings. However, the same spatio-visual configuration (the same volume of outlook and enclosure) in an interior overlooking a city skyline, will trigger a different psychological reaction. In particular, enclosure appears to be primarily significant in natural and urban settings. These two <b>results</b> <b>are</b> especially challenging for architectural arguments about the need for prospect and refuge, as the evidence for the latter is far weaker than for the former, and especially so for interiors. More specifically for designers, the results for complexity seem to confirm that a degree of complexity in interior space is preferred, but they are unclear about how much or where it should <b>be.</b> The <b>results</b> for mystery <b>are</b> less emphatic (75  % neutral or negative) although few of the spaces tested in these studies appear to possess a high level of mystery. Moreover, very few of the hypothesised triggers for mystery have ever been examined using quantitative means. For example, visual connections, changing levels of light and varying ceiling heights (Hildebrand 1991, 1999) have been claimed to be significant components of preferred spaces, but only two of the studies examined here consider them and both are neutral in their findings.|$|R
40|$|The rapid {{industrial}} development and urbanization processes {{that occurred in}} China over the past 30 years has increased dramatically the consumption {{of natural resources and}} raw materials, thus exacerbating the human pressure on environmental ecosystems. In result, large scale environmental pollution of soil, natural waters and urban air were recorded. The development of effective industrial planning to support regional sustainable economy development has become an issue of serious concern for local authorities which need to select safe sites for new industrial settlements (i. e. industrial plants) according to assessment approaches considering cumulative impacts, synergistic pollution effects and risks of accidental releases. In order to support decision makers in the development of efficient and effective regional land-use plans encompassing the identification of suitable areas for new industrial settlements and areas in need of intervention measures, this study provides a spatial regional risk assessment methodology which integrates relative risk assessment (RRA) and socioeconomic assessment (SEA) and makes use of spatial analysis (GIS) methodologies and multicriteria decision Analysis (MCDA) techniques. The proposed methodology was applied to the Chinese region of Hulunbeier which is located in eastern Inner Mongolia Autonomous Region, adjacent to the Republic of Mongolia. The application results demonstrated the effectiveness of the proposed methodology in the identification of the most hazardous and risky industrial settlements, the most vulnerable regional receptors and the regional districts <b>which</b> <b>resulted</b> to <b>be</b> the <b>most</b> relevant for intervention measures since they are characterized by high regional risk and excellent socio-economic development conditions...|$|R
40|$|Abstract. We have derived abundances of 33 {{elements}} and upper limits for 6 additional elements for the metal-poor ([Fe/H]= − 2. 42) turn-off star HE 0338 − 3945 from high-quality VLT-UVES spectra. The star is heavily enriched, {{by about a}} factor of 100 relative to iron and the Sun, in the heavy s-elements (Ba, La, [...] ). It is also heavily enriched in Eu, which is generally considered an r-element, and in other similar elements. It is less enriched, by about a factor of 10, in the lighter s-elements (Sr, Y and Zr). C is also strongly enhanced and, to a somewhat lesser degree, N and O. These abundance estimates are subject to severe uncertainties due to NLTE and thermal inhomogeneities which are not taken into detailed consideration. However, an interesting <b>result,</b> <b>which</b> <b>is</b> <b>most</b> probably robust {{in spite of these}} uncertainties, emerges: the abundances derived for this star are very similar to those of other stars with an overall enhancement of all elements beyond the iron peak. We have defined criteria for this class of stars, r+s stars, and discuss nine different scenarios to explain their origin. None of these explanations is found to be entirely convincing. The most plausible hypotheses involve a binary system in which the primary component goes through its giant branch and asymptotic giant branch phases and produces CNO and s-elements which are dumped onto the observed star. Whether the r-element Eu is produced by supernovae before the star was forme...|$|R
40|$|AbstractWorld Wide Web {{is a vast}} {{resource}} of data growing continuously. Nowadays, it becomes increasingly hard for users to retrieve useful data due to the continually rapid growth in data volume. This vast amount of data is making search {{more and more difficult}} with traditional search engine as they return huge data for a given query which is consisting of relevant as well as irrelevant data. This <b>is</b> not only <b>results</b> in wastage of user time but also lead to data overload problem. So, users are not satisfied with searching the information by traditional search engine. So the problem of re-ranking search pages or results {{has become one of the}} main problems in IR field. Currently searching methods are mainly based on keyword matching technique but this technique has some weaknesses. The first weakness is that web users cannot express their search intention accurately or properly using several keywords. So most of the time, the exactly matched results do not satisfy the web users. Second weakness is that keyword matching cannot sure the selected candidates have high correlation with the user's query, given the different meanings of the keywords. Another problem about traditional search engines is their ranking methods. To fulfil the requirement of users we are using Semantic search engine with page ranking algorithm which will search the data semantically and holds the capability to re-rank search results effectively and try the best to arrange the web <b>results</b> <b>which</b> <b>are</b> <b>most</b> relevant for the users. The proposed algorithm for page ranking <b>is</b> based on <b>result</b> of semantic web with user attention time...|$|R
40|$|A {{model of}} the grain surface {{chemistry}} involving the accretion of atoms of two different elements, X and Y, and their reactions to form species X_ 2, XY, and Y_ 2 was examined {{for a wide range}} of choices for the values of its three free parameters - the accretion rate of X and Y, the desorption rate of X and the grain surface sweeping time of Y, all considered relative to the grain surface sweeping rate of X. Relative production rates of the diatomics were calculated with five methods involving, respectively, a high-order truncation of the master equation, a low-order truncation of the master equation, the standard deterministic rate equation approach, a modified rate equation approach and a set of approximations which are in some cases appropriate for accretion dominated chemistry. The accuracies of the relative production rates calculated with the different methods were assessed for the wide range of model parameters. The more accurate of the low-truncation master equation calculations and the standard deterministic rate equation approach gives <b>results</b> <b>which</b> <b>are</b> in <b>most</b> cases within ten or twenty per cent of the results given by the high-truncation master equation calculations. For many cases, the more accurate of the low order truncation and the standard deterministic rate equation approaches is indicated by a consideration of the average number of atoms of the two species on the grain's surface. Comment: 10 pages, 7 figures. Accepted for publication in Astronomy & Astrophysics. Additional figures can be found at [URL]...|$|R
60|$|These few {{examples}} show, I think, {{that in every}} department of nature there occur instances of the instability of specific form, which the increase of materials aggravates rather than diminishes. And {{it must be remembered}} that the naturalist is rarely likely {{to err on the side}} of imputing greater indefiniteness to species than really exists. There is a completeness and satisfaction to the mind in defining and limiting and naming a species, which leads us all to do so whenever we conscientiously can, and which we know has led many collectors to reject vague intermediate forms as destroying the symmetry of their cabinets. We must therefore consider these cases of excessive variation and instability as being thoroughly well established; and to the objection that, after all, these cases are but few compared with those in which species can be limited and defined, and are therefore merely exceptions to a general rule, I reply that a true law embraces all apparent exceptions, and that to the great laws of nature there are no real exceptions--that what appear to be such <b>are</b> equally <b>results</b> of law, and are often (perhaps indeed always) those very <b>results</b> <b>which</b> <b>are</b> <b>most</b> important as revealing the true nature and action of the law. It is for such reasons that naturalists now look upon the study of varieties as more important than that of well-fixed species. It is in the former that we see nature still at work, in the very act of producing those wonderful modifications of form, that endless variety of colour, and that complicated harmony of relations, which gratify every sense and give occupation to every faculty of the true lover of nature.|$|R
40|$|Background: Researchers in {{the field}} of spatial {{psychology}} and environmental preference theory have tested a range of claims about the capacity of certain spatial configurations to evoke a positive sense of wellbeing in observers. In parallel, across the landscape, urban, architectural and interior design disciplines, there has been a growing acceptance that a balance of spatial characteristics-including prospect, refuge, mystery and complexity-is desirable in a natural, urban or interior environment. Yet, the evidence that the design disciplines cite for the desirability of these characteristics is often entirely qualitative and only rarely acknowledges the results from the fields of spatial psychology and environmental preference theory. Methods: The {{purpose of this paper is}} to provide a critical overview of the results of quantitative research which has been undertaken into the veracity of prospect-refuge theory and closely associated aspects of environmental preference theory. This meta-analysis not only involves a review of the results, but also their broad classification to develop a more holistic picture of the field, its findings and any gaps. The purpose of this process is not, explicitly at least, to assess the believability or rigour of this past research, but rather to examine and classify the findings, both for and against prospect-refuge theory, in a way that is useful for the design disciplines. Results: Urban and interior studies supported the significance of prospect, and were more neutral about refuge. Studies related to natural environments provided evidence for the significance of both prospect and refuge, which has been linked to comfort, but also included evidence against and a neutral finding. More specifically for designers, the results for complexity seem to confirm that a degree of complexity in interior space is preferred, but they are unclear about how much or where it should <b>be.</b> The <b>results</b> for mystery <b>are</b> less emphatic with the majority being neutral or contrary. Discussion and Conclusions: The quantitative evidence for prospect-refuge theory remains inconsistent. It is especially problematic that the <b>results</b> <b>which</b> <b>are</b> <b>most</b> commonly cited in architecture relate to studies of natural environments, not interiors or urban environments. As this paper demonstrates, the <b>results</b> <b>are</b> <b>most</b> valid in specific venues...|$|R
40|$|Significantly {{different}} results attained {{from the use}} of three Finite Element codes used in the analysis of a large complex model are discussed. Building on previous work by the authors regarding the comparison of stress results from several commercial FE codes used on a simple model, this paper recommends steps for an investigation methodology to aid in ascertaining <b>results</b> <b>which</b> <b>are</b> <b>most</b> representative, useful and correct. DESCRIPTION OF PROBLEM The current commercial finite element (FE) codes represent an effective analysis tool for the investigation of critical aspects of pressure containment equipment. The need to formulate an adequate model, apply the correct loadings and confirm the results represents a major portion of the engineer’s analysis task. Once a valid model is constructed, the variability of results produced by the different FE codes remains a concern. To evaluate the magnitude of these variations, we will examine a relatively large and complex model. This model will be “solved ” using three commercial FE codes. The results, in terms of the indicated Stress Intensity, will be compared to investigate the variability of the results. For this comparison, the same model was used with each FE code, except for some minor variations required to adapt to the specific code. The results derived from the three codes were compared to closed-form solutions for pressure and thermal displacements. The variability of the indicated displacements and stresses were then compared to assess the ability of the commercial codes to give reasonably accurate and consistent results. This methodology of establishing a valid model of the problem, verifying the model by closed-form solutions, and comparing and interpreting the results forms a basic protocol that can be extended to other FE analysis work...|$|R
40|$|The {{purpose of}} this {{research}} was to identify factors that are related to the perceived financial well-being of adults in South Dakota, specifically delivery methods of financial information and sources of financial education. This quantitative study used the eight-question Personal Financial Wellness Scale (PFW scale, also known as the InCharge Financial Distress/Financial Well-Being Scale) to measure perceived financial well-being. A random sample of 3, 000 individuals was mailed a survey that elicited 814 completed questionnaires. The survey consisted of the PFW Scale, demographics, delivery methods of financial information, and sources of informal and formal financial education. The PFW scale scores were calculated for all individuals, and the mean score was used as the dependent variable in all analysis. Independent variables included: demographic factors, delivery methods of financial information, sources of informal financial education, sources of formal financial education, and having formal or informal financial education. A block regression of the total sample was used with financial well-being as the dependent variable and all other items as independent variables to test for possible linear relationships. Reliability statistics of the sample were acceptable, and assumptions for the model were tested. An analysis of variance (ANOVA) with Bonferroni post-hoc test was used to identify pair-wise differences between mean perceived financial well-being for individual significant variables and for the variable of having formal or informal financial education. Data were analyzed using SPSS statistical software. The mean perceived financial well-being for adult South Dakotans in the study was 6. 24 (SD= 2. 18) on a ten-point scale. Demographic variables as a group did have a significant association with perceived financial well-being, and five individual demographic variables emerged as being related to perceived financial well-being. Delivery methods of financial information as a group significantly impact the variance of perceived financial well-being, and one individual delivery method, television, was found to have a significant negative impact. Sources of informal or formal financial education variables as a group were not significant in explaining the variance in perceived financial well-being. However, individuals having neither informal nor formal financial education (M= 6. 06, SD= 2. 31) had significantly lower perceived financial well-being than individuals having both informal and formal financial education (M= 6. 67, SD= 2. 15). This study shows {{that there may be a}} positive relationship between individuals receiving financial education in both the informal and formal setting and PFW scale scores. The delivery method used to deliver financial information may have a significant impact on financial well-being and should be considered. Future research may consider including delivery methods of financial information into a conceptual model of financial well-being. Financial planners, counselors, educators, psychologists, and extension educators can use the information to better serve their clients by targeting those individuals that may have low perceived financial well being: female, younger age, with dependent children in the home, working, or lower income. Targeting financial education resources using appropriate delivery methods as described is especially true for South Dakota, to <b>which</b> these <b>results</b> <b>are</b> <b>most</b> appropriate...|$|R
40|$|We have derived abundances of 33 {{elements}} and upper limits for 6 additional elements for the metal-poor ([Fe/H] = - 2. 42) turn-off star HE 0338 - 3945 from high-quality VLT-UVES spectra. The star is heavily enriched, {{by about a}} factor of 100 relative to iron and the Sun, in the heavy s-elements (Ba, La, [...] ). It is also heavily enriched in Eu, which is generally considered an r-element, and in other similar elements. It is less enriched, by about a factor of 10, in the lighter s-elements (Sr, Y and Zr). C is also strongly enhanced and, to a somewhat lesser degree, N and O. These abundance estimates are subject to severe uncertainties due to NLTE and thermal inhomogeneities which are not taken into detailed consideration. However, an interesting <b>result,</b> <b>which</b> <b>is</b> <b>most</b> probably robust {{in spite of these}} uncertainties, emerges: the abundances derived for this star are very similar to those of other stars with an overall enhancement of all elements beyond the iron peak. We have defined criteria for this class of stars, r+s stars, and discuss nine different scenarios to explain their origin. None of these explanations is found to be entirely convincing. The most plausible hypotheses involve a binary system in which the primary component goes through its giant branch and asymptotic giant branch phases and produces CNO and s-elements which are dumped onto the observed star. Whether the r-element Eu is produced by supernovae before the star was formed (perhaps triggering the formation of a low-mass binary), by a companion as it explodes as a supernova (possibly triggered by mass transfer), or whether it is possibly produced in a high-neutron-density version of the s-process is still unclear. Several suggestions are made on how to clarify this situation. Comment: Accepted for A&A; 22 pages, 9 figures, 2 tables. Table 2 is in electronic form and available at [URL] with description at [URL]...|$|R
