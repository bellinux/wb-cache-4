9|10000|Public
3000|$|... “Self-correction and {{self-checking}} {{allow you}} to think about it. You don’t really depend on teacher to see <b>where</b> <b>is</b> <b>the</b> <b>problem.</b> Teacher will first at least let you try…let you do correction on your own using just some more clues…(Chemistry student 4).” [...]...|$|E
40|$|Conditional Kolmogorov {{complexity}} can {{be understood}} as the complexity of the problem “ ”, <b>where</b> <b>is</b> <b>the</b> <b>problem</b> “construct ” and is the problem “construct ”. Other logical operations () can be interpreted in a similar way, extending Kolmogorov interpretation of intuitionistic logic and Kleene realizability. This leads to interesting problems in algorithmic information theory. Some of these questions are discussed...|$|E
40|$|This work {{is focused}} on {{motivating}} and evaluating employees in a gastro company, analysis of these activities and their effect on staff behavior to corporate finance and property in general. The first part describes theoretical background issues, basic concepts and their relationships. Then <b>where</b> <b>is</b> <b>the</b> <b>problem</b> thoroughly analyzed and practical examples of problems and their current solution are included. After this comprehensive introduction to the topic, new solution will be proposed, which would have the maximum possible effect through theoretical knowledge, logical reasoning, and ultimately practical experience and analytical calculations...|$|E
40|$|Colloque sans acte à {{diffusion}} restreinte. nationale. National audienceThis presentation {{outlines the}} application context of (m,k) -firm real-time paradigm, shows <b>where</b> <b>are</b> <b>the</b> <b>problems</b> when applying <b>the</b> DBP algorithm to the real-time context and then details our solutions {{based on the}} new Matrix-DBP algorithm. Future research work related to the (m,k) -firm paradigm is indicate...|$|R
40|$|The aim of {{this paper}} is to show how to analyze and {{correctly}} chose measuring device specially applied on mass flow meter based on Coriolis principle. In the beginning short description of Coriolis based mass flow meter is given. Furthermore, comparison analysis of two flow meters shows <b>where</b> <b>are</b> <b>the</b> <b>problems</b> of wrongly applied method in slurry fluid measurement in production of powder detergents plant. Analysis is made using RS Logix 500 program. In given diagrams which show causes of wrong measurements and position of mass flow device <b>the</b> few malfunctions <b>are</b> determined Finally, using comparison measurement with new mass flow meter shows that measurement method is applicable for slurry fluids which are specially problematic and complex for measuring...|$|R
40|$|AbstractConditional Kolmogorov {{complexity}} K(x|y) can <b>be</b> {{understood as}} <b>the</b> complexity of <b>the</b> <b>problem</b> “Y→X”, <b>where</b> X <b>is</b> <b>the</b> <b>problem</b> “construct x” and Y <b>is</b> <b>the</b> <b>problem</b> “construct y”. Other logical operations (∧,∨,↔) {{can be interpreted}} in a similar way, extending Kolmogorov interpretation of intuitionistic logic and Kleene realizability. This leads to interesting problems in algorithmic information theory. Some of these questions are discussed...|$|R
40|$|The Gene {{expression}} messy {{genetic algorithm}} (GEMGA) {{is a new}} generation of messy genetic algorithms (GAs) that pays careful attention to linkage learning and in a broader context the search for appropriate relations [6]. This paper revisits the earlier work on the GEMGA [8], [9], [7] and proposes {{a new version of the}} algorithm that brings back the time complexity to O(A()), <b>where</b> <b>is</b> <b>the</b> <b>problem</b> length, A is the alphabet set of the representation, and k is the order of delineability [6] of the problem. This paper also reports the scalable linear performance of the GEMGA for various difficult, large optimization problems...|$|E
40|$|The thesis {{deals with}} the {{active labour market policy}} in the Frýdek-Místek district, <b>where</b> <b>is</b> <b>the</b> <b>problem</b> of higher rates of {{unemployment}} and long-term unemployment. Active labour policy has an important role in solving the problem of unemployment in the region. The thesis describes the nature of the local labour market and assesses the active employment policy programs used in the years 2006 - 2010 in terms of their impact on employment and employability of participants. In particular, the thesis focuses on retraining, {{which is one of the}} most used tools of active employment policy, and monitors the creation of new jobs that directly affect the situation of individuals in the labour market. The thesis evaluates the targeting of retraining, which significantly affects their results. Using statistical data analysis methods are examined gross effects of retraining and monitored the effects of active employment policy to selected target groups of unemployed people...|$|E
40|$|Nowadays, {{there is}} a lot of {{information}} in the internet, personal computer, mobile phone and everywhere around us. How we exchange this information? There are many decisions, but the most popular exchange form is using XML document standards. What is more, it will be everything good, but people are, who wants to use them, often faced with certain problems. They are trying get this information from these documents, but they cannot read it or very hard without any special software. Some people do not trust XML documents, because they didn’t use any time of them. <b>Where</b> <b>is</b> <b>the</b> <b>problem?</b> Some people could not understand XML document methodology; others did not use special software for special XML documents. It is software for storing and migrating signed XML documents. Special tools can save us time. People can quickly access the documents and information they need. It can help people work together in ways that are most effective for them...|$|E
40|$|Emergency {{response}} <b>is</b> <b>the</b> {{disaster management}} phase {{with the most}} extreme requirements. During the crisis management, several organizations coordinate their work based not only on well-defined policies and procedures (product of careful preparation) {{but also on the}} outcomes of the decision-making process. Decision-making is a highly complicated process in crisis situations. Good support in decision-making when disaster occurs is of critical importance to react accurately, fast and effectively. Good decision-making helps to control damage, save lives and resources, and reduce unwanted consequences of a crisis. However, the current decision-making process does not always result in optimal and adequate crisis response. <b>Where</b> <b>are</b> <b>the</b> <b>problems?</b> What <b>are</b> <b>the</b> bottlenecks? How to improve the cooperation between the different units involved in emergency response activities. To give answers to these and many more questions, a group of researchers has investigated the work of the different actors (police, fire brigade, ambulance, municipalities and other institutions) involved in emergency response organising workshops, collecting interviews and participating in various trainings. ...|$|R
40|$|International audienceActive Covariance Matrix Adaptation and Mirrored Mutations {{have been}} {{independently}} proposed as improved variants of the well-known optimization algorithm Covariance Matrix Adaptation Evolution Strategy (CMA-ES) for numerical optimization. This paper investigates {{the impact of}} the algorithm's population size when both active covariance matrix adaptation and mirrored mutation <b>are</b> used in <b>the</b> CMA-ES. To this end, we compare the CMA-ES with standard population size λ, i. e., λ = 4 + 3 (D) with a version with half this population size <b>where</b> D <b>is</b> <b>the</b> <b>problem</b> dimension...|$|R
40|$|The {{publication}} of the Cooley-Tukey fast Fourier transform (FIT) algorithm in 1965 has opened a new area in digital signal processing by reducing the order of complexity of some crucial computational tasks like Fourier transform and convolution from N 2 to N log 2 N, <b>where</b> N <b>is</b> <b>the</b> <b>problem</b> size. <b>The</b> development of the major algorithms (Cooley-Tukey and split-radix FFT, prime factor algorithm and Winograd fast Fourier transform) is reviewed. Then, an attempt is made to indicate {{the state of the}} art on the subject, showing the standing of research, open problems and implementations...|$|R
40|$|This work {{compares the}} {{operating}} system of anaerobic fermentation of agricultural biogas plants with realization using biowaste. It deals with the operation system of anaerobic fermentation of agricultural biogas plants and implement an appropriate system to enable the use of biowaste. Based on the comparison of technological solutions and operational parameters of specific sites {{has been designed to}} allow the system biowaste were made a practical experiment to verify the assumption of increased biogas production. In experiments used substrates, which are industrially produced from the available bio-wastes, treated and then provide to operator of biogas plants. The work was carried out practical measurements to verify the production of biogas from different substrates. Utilize of nominal electric power using biowaste amounted up to 97. 66 %. Processing of such modified substrates in anaerobic digestion technology can greatly affect the amount of energy crops. Benefit of waste is governed primarily by such projects, <b>where</b> <b>is</b> <b>the</b> <b>problem</b> of ensuring sufficient energy crops. The proposed composition of raw materials also allows the implementation of the existing ope­ra­tion of anaerobic digestion. Operational data on real operations demonstrate the real possibility of further development of the area and secondly the use of biogas plants {{as well as in the}} preparation of suitably prepared substrates for the operators. The entire data set underwent a complete statistical analysis. Differences between variants were statistically significant...|$|E
40|$|It {{is almost}} twenty years after GNU/Linux has beenoverblown. Still the market figures of Linux use are at veryabysmal levels. In spite of the {{maturity}} and adoption, Linux isstill remained as a second hand option for many users across theworld. Linux has many advantages compared to its counterpartsin the market and unlike any other proprietary operating system;some of the distributions are available for users as full featuredsoftware (wholesome OS). Even though Linux is proved to be bestsoftware in server market, the desktop market remained feeble. The market share of Linux is only laying less than 2 % but rest isshared by all other operating systems. Windows alone attribute toapproximately 85 % of market share. Under the commonmarketing tenet that if a product is not adopted by market then itmight be that, the users in the market either might not be awareof it or if they are aware, might not like it, or if they like it andstill don’t use then it {{is the problem of}} availability, but this is notthe problem to Linux. It is available freely (as a source code) from the respective websites and users across the world arefamiliar. Then <b>where</b> <b>is</b> <b>the</b> <b>problem?</b> Exactly here the need forthis study arises. User perception is one of the importantattributes which characterizes market share. In this study asurvey was done to know if expertise of individuals influencestheir perception towards Linux. A hypothesis was formulated totest if any dependency exists in between these two variables(namely individual expertise and their perception towardsLinux). It was found that the two variables are not significantlydifferent; which means expertise of individuals significantlyinfluences their perception towards Linux...|$|E
40|$|The Four Columned Approach to Clinical Diagnosis: One of {{the real}} {{problems}} in making clinical diagnosis {{is the amount of}} clinical information we have to digest, such that {{it is often difficult to}} see the wood for the trees. Accordingly, it is often hard to make a diagnosis "all-of-a-piece" by any "inductive" or other process of reasoning. With experience, you will learn to recognize different disease patterns, and readily be able to make a clinical diagnosis in those cases. But pattern-recognition is fraught with pitfalls, because so often individual cases depart significantly from the classical. Besides, pattern recognition requires experience, and so is of little value to those in learning. Also, it is often less than fully descriptive, e. g. 'myocardial infarction'. Because of these factors, the approach here is to handle the clinical information within a small number of separate categories, each largely independent, but which, when put together, severally describe all aspects of the condition or diagnosis. The following are the four categories: <b>WHERE</b> <b>is</b> <b>the</b> <b>problem?</b> = ANATOMICAL DIAGNOSIS = the anatomical system involved. WHAT is the general pathological nature of the condition? = PATHOLOGICAL DIAGNOSIS HOW does it affect the patient? = PHYSIOLOGICAL DIAGNOSIS = functional consequences of the condition. WHY did the patient get it? = AETIOLOGICAL DIAGNOSIS = background cause. Subsumed under this category is also the question of WHO has the condition, and why it occurred WHEN it did. The WHO relates to the type of patient concerned, in particular to the social and psychological contexts of the presentation. The WHEN aims to focus on its precipitating factors. The value of the four-column approach is that it allows you to focus down within each category on much less than the total body of information, and thereby give you a much greater chance of reaching the correct conclusion. Each of the categories contributes and, when taken together, they severally describe all the elements of clinical diagnosis. By contrast, when the pattern-recogniser pins some off-the-rack diagnostic label on a patient, it so often lacks one or other of these categories, particularly the Functional and Aetiological ones. Thus, we often hear that a patient has had an 'acute myocardial infarct' (Anatomical and general Pathological diagnoses), but where is the comment about how this has affected him functionally (did he have secondary heart failure or ventricular dysrhythmias for example?); and what were the long-term factors predisposing to his condition, as well as the more recent ones precipitating it (Aetiological diagnosis) - in this case was there preexisting hypertension predisposing to atheroma, and some stressful life event precipitating the episode itself? The present approach allows you to 'tailor-make' a diagnosis (Dx) to the individual patient as you go along, rather than force him into some pre-conceived diagnostic pigeon hole read about in some text book...|$|E
40|$|We {{consider}} derivative-free algorithms for stochastic optimization {{problems that}} use only noisy function values rather than gradients, analyzing their finite-sample convergence rates. We show that if pairs of function values are available, algorithms that use gradient estimates based on random perturbations suffer {{a factor of}} at most √ d in convergence rate over traditional stochastic gradient methods, <b>where</b> d <b>is</b> <b>the</b> <b>problem</b> dimension. We complement our algorithmic development with information-theoretic lower bounds on the minimax convergence rate of such problems, which show that our bounds are sharp with respect to all problemdependent quantities: they cannot be improved by more than constant factors. ...|$|R
40|$|The goal of my PhD {{thesis is}} to develop an {{evolutionary}} tool, the evolutionary requirements analyser (ERA), for searching an optimal set of component combinations {{in the use of}} requirements engineering. <b>The</b> evolutionary tool <b>is</b> demonstrated in evolving system requirements, which <b>are</b> symbolised by <b>the</b> component-based approach. The chanlleges of this research <b>are</b> fourfold: <b>the</b> transformation of <b>the</b> <b>problem</b> domain, <b>the</b> method to assess chromosome fitness, constraint satisfaction in the RE domain, and the optimisation of multi-objective problems. Two major fields: requirements engineering (RE) and evolutionary computation (EC), are involved in my research <b>where</b> RE <b>is</b> <b>the</b> <b>problem</b> domain and EC <b>is</b> <b>the</b> solution domain...|$|R
40|$|AbstractLet L(N, n) <b>be</b> <b>the</b> set of all N × n {{matrices}} X = (xij with xij = − 1, 0, 1. <b>The</b> <b>problem</b> considered <b>is</b> <b>the</b> {{finding of}} X 0 in L(N, n) which minimizes tr(XTX) − 1, X ϵ L(N, n). This <b>problem</b> corresponds to <b>the</b> finding of certain optimal designs in statistics <b>where</b> n <b>is</b> small. <b>The</b> <b>problem</b> <b>is</b> solved for n ⩽ 6...|$|R
40|$|This {{project is}} dealing with {{creating}} of system for e-learning support. It describes problematics of e-learning, explains meaning of this term and describes its advantages and disadvantages. <b>The</b> objectives <b>were</b> to create an application suitable for electronic learning, which could illustrate specifications of restrictions of the system objects. This means to create an application, <b>where</b> <b>is</b> explained <b>the</b> <b>problem</b> of OCL {{in that kind of}} format, which reacts the structure and the form of application used for e-learning. I meet the program languages of OCL and UML 2. 0. I read over the CASE Rational Rose environment and meet <b>the</b> e-learning <b>problem,</b> too. These <b>are</b> <b>the</b> knowledges, which I used while I was creating my e-learning application...|$|R
40|$|AbstractChannel routing {{is a vital}} task in {{the layout}} design of VLSI circuits. Multiterminal channel routing is {{different}} from two-terminal one. While <b>the</b> later <b>is</b> quite understood, <b>the</b> former still poses the difficulty. In this paper, we investigate the multiterminal channel routing problem in a hexagonal model, whose grid is composed of horizontal tracks, right tracks (with slope + 60 °), and left tracks (with slope − 60 °). We present an efficient algorithm for routing multiterminal nets on a channel of width d + 3, <b>where</b> d <b>is</b> <b>the</b> <b>problem</b> density. Furthermore, we can wire the layout produced by the router using four layers {{and there are no}} overlaps among different layers. This improves the previous known results [15, 19]...|$|R
40|$|Research into {{programming}} methodology usually {{focuses on}} `Programming in the Large', as this <b>is</b> <b>where</b> <b>the</b> perceived <b>problems</b> <b>are,</b> and <b>where</b> <b>the</b> expensive mistakes <b>are</b> usually made by experienced programmers. Such problems are far removed, however, from <b>the</b> hurdles to <b>be</b> climbed by inexperienced programmers. Their difficulty is more with `Programming in the Small', where they must manage constructs {{of only a}} few lines, but conforming to complex rules...|$|R
40|$|In the cyclic Barzilai–Borwein (CBB) method, {{the same}} Barzilai–Borwein (BB) stepsize is reused for m {{consecutive}} iterations. It is proved that CBB is locally linearly convergent {{at a local}} minimizer with positive definite Hessian. Numerical evidence indicates that when m> n/ 2 � 3, <b>where</b> n <b>is</b> <b>the</b> <b>problem</b> dimension, CBB <b>is</b> locally superlinearly convergent. In the special case m = 3 and n = 2, it <b>is</b> proved that <b>the</b> convergence rate <b>is</b> no better than linear, in general. An implementation of the CBB method, called adaptive cyclic Barzilai–Borwein (ACBB), combines a non-monotone line search and an adaptive choice for the cycle length m. In numerical experiments using <b>the</b> CUTEr test <b>problem</b> library, ACBB performs better than the existing BB gradient algorithm, while it <b>is</b> competitive with <b>the</b> well-known PRP+ conjugate gradient algorithm...|$|R
40|$|It {{is shown}} that several {{recursive}} least squares (RLS) type equalization algorithms such as, e. g., decision-directed schemes and `orthogonalized' constant modulus algorithms, possess a common algorithmic structure {{and are therefore}} rather straightforwardly implemented on an triangular array (filter structure) for RLS estimation with inverse updating. While the computational complexity for such algorithms <b>is</b> O(N 2), <b>where</b> N <b>is</b> <b>the</b> <b>problem</b> size, <b>the</b> troughput rate for <b>the</b> array implementation <b>is</b> O(1), i. e., independent of <b>the</b> <b>problem</b> size. Such a throughput rate cannot be achieved with standard (Gentleman-Kung-type) RLS/QR-updating arrays because of feedback loops in the computational schemes. 1 Introduction The paper deals with channel equalization, i. e. signal recovery after digital data transmission over a communication channel. In mobile communications, for example, multipath propagation introduces intersymbol interference, which means that <b>the</b> signal that <b>is</b> received, e. g., by [...] ...|$|R
50|$|A college boy starts {{suffering}} with <b>the</b> <b>problem</b> of {{hair loss}} {{but does not}} take it seriously, later it gets serious because {{his career as a}} model was getting affected due to lesser hair.First half of <b>the</b> film <b>is</b> mostly about youth and college life, this <b>is</b> <b>where</b> <b>the</b> <b>problem</b> of hair fall starts. Second half shows tricks and techniques of growing hair and is presented very sarcastically, slowly moving towards a love plot ending it all on a positive note.|$|R
40|$|The derandomized {{evolution}} strategy (ES) with covariance matrix adaptation (CMA), <b>is</b> modified with <b>the</b> goal {{to speed up}} the algorithm in terms of needed number of generations. The idea of the modification of <b>the</b> algorithm <b>is</b> to adapt <b>the</b> covariance matrix in a faster way than in the original version by using a larger amount of the information contained in large populations. The original version of <b>the</b> CMA <b>was</b> designed to reliably adapt the covariance matrix in small populations and turned out to be highly efficient in this case. The modification scales up the efficiency to population sizes of up to 10 n, <b>where</b> n <b>is</b> <b>the</b> <b>problem</b> dimension. If enough processors <b>are</b> available, <b>the</b> use of large populations and thus of evaluating a large number of search points per generation is not a <b>problem</b> since <b>the</b> algorithm can <b>be</b> easily parallelized...|$|R
40|$|Simple, polynomial-time, {{heuristic}} algorithms {{for finding}} approximate solutions to various polynomial complete optimization problems are analyzed {{with respect to}} their worst case behavior, measured by the ratio of the worst solution value that can <b>be</b> chosen by <b>the</b> algorithm to the optimal value. For certain problems, such as a simple form of <b>the</b> kanpsack <b>problem</b> and an optimization problem based on satisfiability testing, there are algorithms for which this ratio is bounded by a constant, independent of <b>the</b> <b>problem</b> size. For a number of set covering problems, simple algorithms yield worst case ratios which can grow with the log of <b>the</b> <b>problem</b> size. And for <b>the</b> <b>problem</b> of finding <b>the</b> maximum clique in a graph, no algorithm has been found for which the ratio does not grow at least as fast as nε, <b>where</b> n <b>is</b> <b>the</b> <b>problem</b> size and ε> 0 depends on the algorithm...|$|R
40|$|This paper {{analyzes}} the relative advantages between crossover and mutation on {{a class of}} deterministic and stochastic additively separable problems with substructures of non-uniform salience. This study assumes that the recombination and mutation operators have {{the knowledge of the}} building blocks (BBs) and effectively exchange or search among competing BBs. Facetwise models of convergence time and population sizing have been used to determine the scalability of each algorithm. The analysis shows that for deterministic exponentially-scaled additively separable, <b>problems,</b> <b>the</b> BB-wise mutation <b>is</b> more efficient than crossover yielding a speedup of o(ℓ log ℓ), <b>where</b> ℓ <b>is</b> <b>the</b> <b>problem</b> size. For <b>the</b> noisy exponentially-scaled <b>problems,</b> <b>the</b> outcome depends on whether scaling on noise is dominant. When scaling dominates, mutation is more efficient than crossover yielding a speedup of o(ℓ log ℓ). On the other hand, when noise dominates, crossover is more efficient than mutation yielding a speedup of o(ℓ) ...|$|R
40|$|This paper {{describes}} {{some preliminary}} {{results from a}} 20 -week study {{on the use of}} Compaq iPAQ Personal Digital Assistants (PDAs) by 10 senior developers, analysts, technical managers, and senior organisational managers. The goal of <b>the</b> study <b>was</b> to identify what applications were used, how and <b>where</b> they <b>were</b> used, <b>the</b> <b>problems</b> and issues that arose, and how use of the iPAQs changed over the study period. The paper highlights some interesting uses of the iPAQs, and identifies some of the characteristics of successful mobile applications. 1...|$|R
40|$|The main aim of {{randomized}} search heuristics is {{to produce}} good approximations of optimal solutions within {{a small amount of}} time. In contrast to numerous experimental results, {{there are only a few}} theoretical ones on this subject. We consider the approximation ability of randomized search heuristics for the class of covering problems and compare single-objective and multi-objective models for such <b>problems.</b> For <b>the</b> VertexCover <b>problem,</b> we point out situations where the multi-objective model leads to a fast construction of optimal solutions while in the single-objective case even no good approximation can be achieved within expected polynomial time. Examining the more general SetCover problem we show that optimal solutions can be approximated within a factor of log n, <b>where</b> n <b>is</b> <b>the</b> <b>problem</b> dimension, using <b>the</b> multi-objective approach while the approximation quality obtainable by the single-objective approach in expected polynomial time may be arbitrarily bad...|$|R
40|$|The authors {{show that}} there is a {{generalization}} of Rodrigues' formula for computing the exponential map exp: so(n) from skewsymmetric matrices to orthogonal matrices when n 4, and give a method for computing some determination of the (multivalued) function log: SO(n) so(n). <b>The</b> key idea <b>is</b> <b>the</b> decomposition of a skew-symmetric nn matrix B in terms of (unique) skew-symmetric matrices B 1, [...] .,B p obtained from the diagonalization of B and satisfying some simple algebraic identities. A subproblem arising in computing log R, <b>where</b> R#SO(n), <b>is</b> <b>the</b> <b>problem</b> of finding a skewsymmetric and 0. The authors also consider the exponential map <b>where</b> se(n) <b>is</b> <b>the</b> Lie algebra of the Lie group SE(n) of (a#ne) rigid motions. The authors {{show that there}} is a Rodrigues-like formula for computing this exponential map, and give a method for computing some determination of the (multivalued) se(n). This yields a direct proof of the surjectivity of exp: se(n) #SE(n) ...|$|R
50|$|In 1906, he {{graduated}} from Tulane University and got his Ph.D. from Cornell University Medical College. In 1909, {{he came back to}} Sweden where he applied for doctoral program at University of Uppsala. The university declined his application, and Anderson discouragly went to the University of Berlin <b>where</b> he <b>was</b> accepted to Emil Fischer's laboratory. Unfortunatelly, <b>the</b> labaratory <b>was</b> already full, and he was transported to Hermann Leuchs labaratory <b>where</b> he <b>was</b> solving <b>the</b> <b>problem</b> on <b>the</b> chemistry of color reaction of combination of brucine with nitric acid.|$|R
40|$|We {{present the}} first {{space and time}} optimal {{parallel}} algorithm for the pairwise sequence alignment problem, a fundamental problem in computational biology. This problem can be solved sequentially in O(mn) time and O(m + n) space, where m and n <b>are</b> <b>the</b> lengths of <b>the</b> sequences to <b>be</b> aligned. <b>The</b> fastest known parallel space-optimal algorithm for space but suboptimal O � (m+n) 2 � time, <b>where</b> p <b>is</b> <b>the</b> this <b>problem</b> takes optimal O � m+n p number of processors. The most space-economical optimal time parallel algorithm take...|$|R
40|$|Generating fully 3 D terrains is a dificult task, {{meaning that}} we need to store a lot of data or do a lot of {{computing}} or both. We can reduce or completly eliminate the data srorage by using a procedural approch, but this <b>is</b> <b>where</b> <b>the</b> <b>problem</b> gets realy computationaly costly and the CUDA platform comes in. CUDA kernels runinng parallely on graphic accelerators can rapidly decrease time needed for computation, allowing even these complex calculations to work in real time or even better. Finding its use in game or movie industry...|$|R
40|$|Thesis <b>is</b> {{focused on}} <b>the</b> future {{possibilities}} of industrial town Šumperk. <b>The</b> main <b>problem</b> {{of this town}} <b>is</b> <b>the</b> departure of young generation which causes the lack of qualified workforce. The result of <b>the</b> thesis <b>is</b> making some future variants. They are made by exploratory development scenarios. According the thematic analysis focused on structure and character of the town we identified the driving powers which might affect the town in its future. In this case <b>the</b> driving powers <b>are</b> <b>the</b> automation and return of the natives. The exploratory scenarios can make an important part for making strategic plan for town Šumperk. These results can <b>be</b> inspirational for <b>the</b> similar industrial towns <b>where</b> <b>is</b> <b>the</b> same <b>problem</b> with lack of the qualified workforce. There {{is a need to}} create conditions and measures which can lead to positive development. It helps to imagine the town structure after a few years so we can reduce the impact of threats or use the offered opportunities...|$|R
40|$|The {{original}} recursive {{least squares}} (RLS) computational scheme basically consists of triangular updates and triangular backsolves, {{and it is}} well known that pipelining these two separate steps on a parallel architecture as such is impossible. As a result of this, a straightforward systolic implementation with, e. g. O(n²) processors for O(n²) operations per update, will have a throughput which is only O(n^- 1), <b>where</b> n <b>is</b> <b>the</b> <b>problem</b> size. Several alternative schemes have been developed, but as far as we know, up till now none of these have been worked into an efficient (stable) systolic implementation. Here, we focus on an RLS algorithm by Pan and Plemmons [12], and we show how an efficient systolic implementation can be derived. We get around <b>the</b> critical path <b>problem</b> by introducing a few additional computational steps. Such a strategy is apparently new. As <b>the</b> different steps <b>are</b> then perfectly pipelined, <b>the</b> obtained throughput <b>is</b> <b>the</b> highest possible, i. e. O(n°). The [...] ...|$|R
40|$|We show enough {{evidence}} that a structured version of Adiabatic Quantum Computation (AQC) is efficient for most satisfiability problems. More precisely, when <b>the</b> success probability <b>is</b> fixed beforehand, <b>the</b> computational resources grow subexponentially {{in the number of}} qubits. Our study focuses on random satisfiability and exact cover problems, developing a multi-step algorithm that solves clauses one by one. Relating the computational cost to classical properties of <b>the</b> <b>problem,</b> we collect significant statistics with up to N= 140 qubits, around the phase transitions, which <b>is</b> <b>where</b> <b>the</b> hardest <b>problems</b> appear...|$|R
