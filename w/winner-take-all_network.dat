28|47|Public
40|$|Implementation of the Koch-Ullman model Based on the Feature Integration Theory Key ideas: • compute color, intensity, and orientations in parallel, • collect {{conspicuous}} {{points in}} a saliency map, • generate focus by a <b>winner-take-all</b> <b>network,</b> • inhibit attended regions by inhibition of return method. The visual attention model by Itti, Koch and Niebu...|$|E
40|$|Medium Spiny {{projection}} neurons, {{the sole}} output neurons of the striatum, are connected by GABAergic synapses. Such connections are classically {{assumed to be}} inhibitory, thus {{it has been suggested}} that one of the main functions of the striatum is to detect and classify cortical representations of sensory events to trigger appropriate motor responses through a <b>winner-take-all</b> <b>network</b> dynamic. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|E
40|$|We {{present and}} rigorously analyze a {{generalization}} of the Winner-Take-All Network: the K-Winners-Take-All Network. This net-work identifies the K largest {{of a set}} of N real numbers. The network model used is the continuous Hopfield model. The <b>Winner-Take-All</b> <b>Network</b> is a network which identifies the largest of N real numbers. Winner-Take-All Networks have been developed using various neural networks models (Grossberg- 73, Lippman- 87, Feldman- 82, Lazzaro- 89). We present here a generalization of the Winner-Take-All Network: the K-Winners-Take-Al...|$|E
40|$|An exact {{solution}} {{of a system}} of coupled differential equations describing the dynamics of a special class of <b>winner-take-all</b> <b>networks</b> is given. From the solution two properties of the short-term-memory traces are derived: (1) information preservation and (2) a discrimination measure. These properties justify a biologically inspired fault tolerant extension of the network using differentiating neurons...|$|R
40|$|This upload {{contains}} the source {{code of the}} simulations in this paper: Marx S, Gruenhage G, Walper D, Rutishauser U, Einhäuser W. Competition with and without priority control: linking rivalry to attention through <b>winner-take-all</b> <b>networks</b> with memory. Ann N Y Acad Sci. 2015 Mar; 1339 : 138 - 53. doi: 10. 1111 /nyas. 12575. Please see the readme. txt file for further instructions...|$|R
40|$|Quantitative {{studies of}} the {{connectivity}} in the superficial layers of visual cortex have revealed {{a high degree of}} recurrence between pyramidal neurons. This is consistent with the presence of <b>winner-take-all</b> <b>networks,</b> which are thought to {{play a crucial role in}} cortical processing (Douglas and Martin 2004). We investigate, by simulation, how these networks can self-organize and self-calibrate in a biologically plausible way...|$|R
40|$|Abstract. This paper investigates a {{mechanism}} for reliable generation of sparse code in a sparsely connected, hierarchical, learning memory. Activity reduction is accomplished with local competitions that suppress activities of unselected neurons so that costly global competition is avoided. The learning ability and the memory characteristics of the proposed <b>winner-take-all</b> <b>network</b> and an oligarchy-take-all network are demonstrated using experimental results. The proposed models have the features of a learning memory essential {{to the development of}} machine intelligence...|$|E
40|$|We {{consider}} {{a layer of}} excitatory neurons with small asymmetric excitatory connections and strong coupling to a single inhibitory interneuron. If the inhibition is fast, the network behaves as a <b>winner-take-all</b> <b>network</b> in which one cell fires {{at the expense of}} all others. As the inhibition slows down, oscillatory behavior begins. This is followed by a symmetric rotating solution in which neurons share the activity in a round-robin fashion. Finally, if the inhibition is sufficiently slower than excitation the neurons completely synchronize to a global periodic solution. Conditions guaranteeing stable synchrony are given...|$|E
40|$|A modular transconductance-mode (T-mode) design {{approach}} is presented for analog hardware implementations of neural networks. This design {{approach is}} used to build a modular bidirectional associative memory network. The authors show {{that the size of}} the whole system can be increased by interconnecting more modular chips. It is also shown that by changing the interconnection strategy different neural network systems can be implemented, such as a Hopfield network, a <b>winner-take-all</b> <b>network,</b> a simplified ART 1 network, or a constrained optimization network. Experimentally measured results from CMOS 2 -μm double-metal, double-polysilicon prototypes (MOSIS) are presented. Peer Reviewe...|$|E
40|$|A {{mathematical}} {{analysis of a}} special class of <b>winner-take-all</b> <b>networks</b> is given. Starting with a solution in closed form describing {{the dynamics of the}} network, we show that an inhibitory winner-take-all net efficiently discriminates input patterns with respect to a canonical measure. This result in combination with further properties suggests an upgrading of the system by incorporating fault tolerance and efficiently generated evidential response into the system...|$|R
50|$|<b>Winner-take-all</b> <b>networks</b> are {{commonly}} used in computational models of the brain, particularly for distributed decision-making or action selection in the cortex. Important examples include hierarchical models of vision (Riesenhuber et al. 1999), and models of selective attention and recognition (Carpenter and Grossberg, 1987; Itti et al. 1998). They are also common in artificial neural networks and neuromorphic analog VLSI circuits. It has been formally proven that the winner-take-all operation is computationally powerful compared to other nonlinear operations, such as thresholding (Maass 2000).|$|R
50|$|In {{the theory}} of {{artificial}} neural <b>networks,</b> <b>winner-take-all</b> <b>networks</b> are a case of competitive learning in recurrent neural networks. Output nodes in the network mutually inhibit each other, while simultaneously activating themselves through reflexive connections. After some time, only one node in the output layer will be active, namely the one corresponding to the strongest input. Thus the network uses nonlinear inhibition {{to pick out the}} largest of a set of inputs. Winner-take-all is a general computational primitive that can be implemented using different types of neural network models, including both continuous-time and spiking networks (Grossberg, 1973; Oster et al. 2009).|$|R
40|$|Abstract — Device {{mismatch}} in neuromorphic VLSI implementations of spiking {{neural networks}} can be a serious and limiting problem. Classical engineering solutions can reduce the effect of mismatch, but require increasing layout sizes or using additional precious silicon real-estate. Here we propose a complementary strategy which exploits the Address-Event Representation used in neuromorphic systems and {{does not affect the}} device layout. We propose a method that selectively changes the connectivity profile in the neural network to normalize its response. We provide a theoretical analysis of the approach proposed and demonstrate its effectiveness with experimental data obtained from a VLSI Soft <b>Winner–Take–All</b> <b>network.</b> I...|$|E
40|$|Abstract–A modular transconductance-mode (T-mode) design {{approach}} is presented for analog hardware implementations of neural networks. This design {{approach is}} used to build a modular bidirectional associative memory (BAM) network. We will show {{that the size of}} the whole system can be increased by interconnecting more modular chips together. Also, we will show that by changing the interconnection strategy different neural network systems can be implemented, such as a Hopfield network, a <b>winner-take-all</b> <b>network,</b> a simplified ART 1 network, or a constrained optimization network. Experimentally measured results from CMOS 2 -pm double-metal, doublepolysilicon prototypes (MOSIS) are presented...|$|E
40|$|An {{optoelectronic}} p-n-p-n <b>winner-take-all</b> <b>network</b> {{is presented}} that {{can identify the}} location of the maximum intensity in two-dimensional spatially distributed light. An input pattern of illumination is incident upon an array of p-n-p-n devices that are connected in parallel to a common load resistor and therefore have identical voltage biases. If the light intensity at one or several elements exceeds a threshold value, only one p-n-p-n element stays on and emits light when the input light pattern is turned off. The element that is turned on is the one that receives the maximum illumination. status: publishe...|$|E
40|$|Abstract — Recently we have {{proposed}} a simple circuit of <b>winner-take-all</b> (WTA) neural <b>network.</b> Assuming no external input, we have derived an analytic equation for its network response time. In this paper, we further analyze the network response time for a class of winner-take-all circuits involving selfdecay and show that the network response time of such a class of WTA {{is the same as}} that of the simple WTA model. Index Terms — Inputless <b>winner-take-all</b> neural <b>network,</b> network response time, self-decay...|$|R
40|$|In {{this paper}} we analyse several {{approaches}} {{to the design of}} Cooperative Algorithms for solving a general problem: That of computing the values of some property over a spatial domain, when these values are constrained (but not uniquely determined) by some observations, and by some a priori knowledge {{about the nature of the}} solution (smoothness, for example). Specifically, we discuss the use of: Variational techniques; stochastic approximation methods for global optimization, and linear threshold networks. Finally, we present a new approach, based on the interconnection of <b>Winner-take-all</b> <b>networks,</b> for which it is possible to establish precise convergence results, including bounds on the rate of convergence. MIT Artificial Intelligence Laborator...|$|R
50|$|With Richard Didday, he {{developed}} {{one of the}} first <b>winner-take-all</b> neural <b>networks</b> in 1970. More recently, with Giacomo Rizzolatti, the leader of research team that discovered mirror neurons, he proposed an evolutionary link between mirror neurons, imitation, and the evolution of language.|$|R
40|$|We {{present a}} novel {{self-organizing}} classification network for structured objects. The system {{consists of two}} layers, an upper layer {{which serves as a}} classifier and a lower layer as a feature extractor. The classifier is an inhibitory <b>winner-take-all</b> <b>network</b> where each unit corresponds to a unique class while the lower layer consists of Hopfield-style networks solving the problem of finding maximum common parts of the input structure and the particular prototypes. The self-organizing architecture is able to outperform traditional classification models using a maximum selector as a classifier. The improvements arise from deactivating networks in the lower layer which indicate large dissimilarities between the input structure and the corresponding prototypes...|$|E
40|$|Recurrent {{networks}} that perform a winner-take-all computation {{have been studied}} extensively. Although some of these studies include spiking networks, they consider only analog input rates. We present results of this winner-take-all computation on a network of integrate-and-fire neurons which receives spike trains as inputs. We show how we can configure the connectivity in the network so that the winner is selected after a pre-determined number of input spikes. We discuss spiking inputs with both regular frequencies and Poisson-distributed rates. The robustness of the computation was tested by implementing the <b>winner-take-all</b> <b>network</b> on an analog VLSI array of 64 integrate-and-fire neurons which have an innate variance in their operating parameters. ...|$|E
40|$|An {{asynchronous}} PDM (Pulse-Density-Modulating) digital {{neural network}} system has been developed in our laboratory. It consists of one thousand neurons that are physically interconnected via one million 7 -bit synapses. It can solve one thousand simultaneous nonlinear first-order differential equations in a fully parallel and continuous fashion. The performance of this system was measured by a <b>winner-take-all</b> <b>network</b> with one thousand neurons. Although {{the magnitude of the}} input and network parameters were identi-cal for each competing neuron, one of them won in 6 milliseconds. This processing speed amounts to 360 billion connections per sec-ond. A broad range of neural networks including spatiotemporal filtering, feedforward, and feedback networks can be run by loading appropriate network parameters from a host system. ...|$|E
40|$|In this paper, {{we propose}} a fast <b>winner-take-all</b> (WTA) neural <b>network.</b> The fast <b>winner-take-all</b> neural <b>network</b> with the dynamic ratio in mutual-inhibition is {{developed}} from the general mean-based neural network (GEMNET), which adopts {{the mean of the}} active neurons as the threshold of mutual inhibition. Furthermore, the other win-ner-take-all neural network enhances the convergence speed to become a decimal system. The proposed WTA neural networks statistically achieve the large ratio of mutual inhibi-tion. The new WTA Neural Networks converge faster than the existing WTA neural networks for a large number of competitors based on both theoretical analyses and simu-lation results...|$|R
40|$|A neural network-based {{controller}} {{is presented}} for the real-time arbitration of routing paths in large crossbar switches constructed from one-sided crosspoint chips. This controller {{is suitable for}} a synchronous environment where a number of connection-requests are presented simultaneously to the switch. The controller aims to maximize the effective bandwidth of the switch and to minimize the simultaneous-switching noise in the individual chips. The controller uses multiple <b>winner-take-all</b> <b>networks</b> coupled with some competitive-cooperative mechanisms to achieve the joint optimization. The effect of various network parameters are studied through simulation, and cases leading to non-optimal solutions analyzed. The {{results show that the}} arbitration complexity and time scale well with the size of the switches, and the throughput achieved is close to the theoretically maximum attainable. We also introduce a hierarchical neural network controller for a packet-switched environment where conne [...] ...|$|R
40|$|ABSTRACT: In {{this paper}} we analyse several {{approaches}} {{to the design of}} Cooperative Algorithms for solving a general problem: That of computing the values of some property over a spatial domain, when these values are constrained (but not uniquely determined) by some observations, and by some a priori knowledge {{about the nature of the}} solution (smoothness, for example). Specifically, we discuss the use of: Variational techniques; stochastic approximation methods for global optimization, and linear threshold net-works. Finally, we present a new approach, based on the interconnection of <b>Winner-take-all</b> <b>networks,</b> for which it is possible to establish precise convergence results, including bounds on the rate of convergence. A. I. Laboratory Working Papers are produced for internal circulation, and may contain information that is, for example, too preliminary or too detailed for formal publication. It is not intended that they should be considered papers to which reference can be made in the literature...|$|R
40|$|Recurrent {{networks}} that perform a winner-take-all computation {{have been studied}} extensively. Although some of these studies include spiking networks, they consider only analog inputs. We present results from an analog VLSI implementation of a <b>winner-take-all</b> <b>network</b> that receives spike trains as input. We show how we can configure the connectivity in the network so that the winner will be selected after a pre-determined number of input spikes. To reduce the effect of transistor mismatch on the network operation, we use bursts of input spikes to compensate for this mismatch. The chip with a network of 64 integrate-andfire neurons can reliably detect the winning neuron, that is, the neuron that receives spikes with the shortest inter-spike interval. 1...|$|E
40|$|Event-driven spike-based {{processing}} systems offer new possibilities for real-time vision. Signals are encoded asynchronously in time thus preserving the time information of {{the occurrence of}} an event. We examine this form of coding using experimental data from a multi-layered multi-chip system which consists of an artificial retina, a convolution filterbank and a <b>winner-take-all</b> <b>network</b> which detect {{the position of a}} moving object. The spike outputs of the convolution stage can be described by an inhomogeneous Poisson distribution of Gaussian profile, although the underlying building blocks are completely deterministic and exhibit only a small amount of variation. We discuss a method for measuring the accuracy of the asynchronous spiking representation in both time and value, thereby quantifying the performance of the winner-takeall network in determining the position of a ball rotating in front of the system...|$|E
40|$|We {{present an}} {{approach}} to texture analysis that uses spatially localized filters and cooperativecompetitive mechanisms for determining emergent boundaries. Gabor filters that closely resemble cortical receptive fields are used to activate columns of cells that selectively respond to localized frequency and orientation attributes of an image. For each cell column, a <b>winner-take-all</b> <b>network</b> that is moderated by the activities of neighboring columns is used to gradually segment the image. All the mechanisms used are biologically plausible and typically yield results that are superior to those reported previously for real images. 1. Texture-based Segmentation. Both biological systems and computational vision models use texture for perceptual tasks such as segmentation of surfaces, classification of surface materials and computation of shape. An image texture {{can be interpreted as}} a pattern of image intensities projected from a surface of uniform surface radiance attributes. Several mod [...] ...|$|E
40|$|Abstract. An exact {{solution}} {{of a system}} of coupled differential equations describing the dynamics of a special class of <b>winner-take-all</b> <b>networks</b> is given. From the solution two properties of the short-termmemory traces are derived: (1) information preservation and (2) a discrimination measure. These properties justify a biologically inspired fault tolerant extension of the network using differentiating neurons. 1 Introduction One {{way to deal with the}} maximum selection from a set of inputs within a connectionist framework are <b>winner-take-all</b> (WTA) <b>networks</b> ([4]). The operation of these networks is a mode of contrast enhancement and pattern normalization where only the unit with the highest activation fires and all other units in the network are inhibited after some setting time. References to common competitive architectures to select the maximum or minimum from a set of data can be found in [10]. Exemplary for a practical application field of WTA nets we mention classification tasks and knowledge discovery of structured objects ([12] and references therein). Results on the computational power of competitive nets can be found in [11]. There is a growing body of mathematical results on competitive neural systems describing the dynamics of competition. Well known series of articles concerning the stability analysis includes the work of Wilson and Cowan, Grossberg et al., Amari et al. ([9] and references therein), and the Lyapunov method in the Cohen-Grossberg ([1]) and Hopfield-Tank ([7]) models. Further interesting results on the dynamics of competitive models are given in [2], [3], [6], and [8]...|$|R
40|$|In [1] we {{proposed}} a simple circuit of <b>Winner-Take-All</b> neural <b>network</b> and derived an analytical equation for its network response time [2]. In this paper, we explore this analytical equation for a more general class of winner-take-all circuits. We show that this equation for network response time is indeed an upper bound for a general class of WTA. 1 Introduction Since the beginning of neural <b>network</b> research, the <b>Winner-Take-All</b> (WTA) <b>network</b> has played {{a very important role}} in the design of most of the unsupervised learning neural networks [3] such as competitive learning and Hamming network. Lippman first {{proposed a}} discrete-time algorithm called Maxnet to realize the Hamming network [4]. Recently, Dempsey and McVey designed an alternative one called peak detector neural network (PDNN) based on the Hopfield network topology [7], and provided a hardware implementation for it [6]. Seiler and Nossek [8] independently proposed an inputless WTA cellular neural network based on Chua's C [...] ...|$|R
40|$|It {{has long}} been known that lateral {{inhibition}} in neural networks can lead to a winner-take-all competition, so that only a single neuron is active at a steady state. Here we show how to organize lateral inhibition so that groups of neurons compete to be active. Given a collection of potentially overlapping groups, the inhibitory connectivity is set by a formula that can be interpreted as arising from a simple learning rule. Our analysis demonstrates that such inhibition generally results in winner-take-all competition between the given groups, with the exception of some degenerate cases. In a broader context, the network serves as a particular illustration of the general distinction between permitted and forbidden sets, which was introduced recently. From this viewpoint, the computational function of our network is to store and retrieve memories as permitted sets of coactive neurons. In traditional <b>winner-take-all</b> <b>networks,</b> lateral inhibition is used to enforce a localize [...] ...|$|R
40|$|Abstract—Selective {{attention}} {{is a very}} efficient strategy for engineering active vision systems that need to extract relevant information from the scene in real-time. We propose an im-plementation of a saliency-map based active vision system in which Address-Event sensors and neuromorphic winner-take-all devices complement conventional imagers and machine vision components. A standard imager is mounted next to a Dynamic Vision Sensor (DVS) on a Pan-Tilt Unit. The output of the DVS is fed to an event-based Selective Attention Chip that implements a <b>Winner-Take-All</b> <b>network</b> with inhibition of return, to identify and sequentially select the most salient regions in the visual input space, and drive the Pan-Tilt Unit accordingly. We characterize the system with experiments using real-world scenarios and natural scenes, and interface it to a workstation to implement models of top-down attention used to influence the decision making process. I...|$|E
40|$|Hardware {{implementation}} of neuromorphic computing is attractive as a computing paradigm beyond the conventional digital computing. In this work, {{we show that}} the SET (off-to-on) transition of metal oxide resistance switching memory becomes probabilistic under a weak programming condition. The switching variability of the binary synaptic device implements a stochastic learning rule. Such stochastic SET transition was statistically measured and modeled for a simulation of a <b>winner-take-all</b> <b>network</b> for competitive learning. The simulation illustrates that with such stochastic learning, the orientation classification function of input patterns can be effectively realized. The system performance metrics were compared between the conventional approach using the analog synapse and the approach in this work that employs the binary synapse utilizing the stochastic learning. The feasibility of using binary synapse in the neurormorphic computing may relax the constraints to engineer continuous multilevel intermediate states and widens the material choice for the synaptic device design...|$|E
40|$|A {{simple method}} is {{presented}} to derive the complete solution of the Maxnet network dynamics. Besides, the exact response time of the network is deduced. 1 Introduction Since the beginning of neural network research, the <b>Winner-Take-All</b> <b>network</b> has played {{a very important role}} in the design of learning algorithms, in particular, most of the unsupervised learning algorithms (Pao 1989) such as competitive learning, self organizing map and Adaptive Resonance Theory model. Conventionally, an N-neurons winner-take-all (WTA) network is defined as following: lim t! 1 h i (v i (t)) = 8 ? ? ? ! ? ? ? : 1 if v i (0) ? v j (0) 8 j 6 = i 0 otherwise. (1) Many researchers have attempted to design and realize the WTA. In (Lippman 1987), a discrete time algorithm called Maxnet is proposed. Maxnet is a fully connected neural network. Each neuron's output is positively fed back to its input and negatively fed back to other neurons' inputs. Suppose there are N neurons. We denote their state variables [...] ...|$|E
40|$|Abstract—In this paper, we {{show that}} noise {{injection}} into inputs in un-supervised learning neural networks does not improve their performance {{as it does in}} supervised learning neural networks. Specifically, {{we show that}} training noise degrades the classification ability of a sparsely connected version of the Hopfield neural network, whereas the performance of a sparsely connected <b>winner-take-all</b> neural <b>network</b> does not depend on the injected training noise. I...|$|R
40|$|In {{this paper}} new single-electron {{tunneling}} (SET) based circuits are proposed: SET <b>winner-take-all</b> (WTA) <b>networks.</b> Two network configurations have been studied: a fully interconnected architecture and a lateral inhibition architecture. Their performance was evaluated using a dot pattern recognition task. Critical aspects in SET-based circuits {{were taken into}} account, such as cotunnelling, temperature dependence and background charges. Power consumption and area estimations (critical for GSI and TSI realizations) were also taken into account...|$|R
40|$|A {{new way of}} {{measuring}} generalization in unsupervised learning is presented. The measure {{is based on an}} exclusive allocation, or credit assignment, criterion. In a classifier that satisfies the criterion, input patterns are parsed so that the credit for each input feature is assigned exclusively to one of multiple, possibly overlapping, output categories. Such a classifier achieves context-sensitive, global representations of pattern data. Two additional constraints, sequence masking and uncertainty multiplexing, are described; these can be used to refine the measure of generalization. The generalization performance of EXIN <b>networks,</b> <b>winner-take-all</b> competitive learning <b>networks,</b> linear decorrelator networks, and Nigrin's SONNET [...] 2 network is compared. Keywords Generalization, Exclusive allocation, Credit assignment, Binding, Unsupervised learning, Pattern classification, Distributed coding, EXIN (excitatory+inhibitory) learning, Sparse coding, Rule extraction, Regularization, Blind [...] ...|$|R
