335|347|Public
25|$|Deep <b>web</b> <b>crawling</b> also multiplies {{the number}} of web links to be crawled. Some {{crawlers}} only {{take some of the}} URLs in <ahref="URL"> form. In some cases, such as the Googlebot, <b>Web</b> <b>crawling</b> is done on all text contained inside the hypertext content, tags, or text.|$|E
25|$|Frontera is <b>web</b> <b>crawling</b> {{framework}} implementing crawl frontier component {{and providing}} scalability primitives for web crawler applications.|$|E
25|$|Web {{search engines}} {{and some other}} sites use <b>Web</b> <b>crawling</b> or {{spidering}} software to update their web content or indices of others sites' web content. Web crawlers can copy all the pages they visit for later processing by a search engine which indexes the downloaded pages so the users can search much more efficiently.|$|E
40|$|International audienceThis paper {{proposes a}} random <b>Web</b> <b>crawl</b> model. A <b>Web</b> <b>crawl</b> is a (biased and partial) {{image of the}} Web. This paper deals with the {{hyperlink}} structure, i. e. a <b>Web</b> <b>crawl</b> is a graph, whose vertices are the pages and whose edges are the hypertextual links. Of course a <b>Web</b> <b>crawl</b> has a very special structure; we recall some known results about it. We then propose a model generating similar structures. Our model simply simulates a crawling, i. e. builds and crawls the graph at the same time. The graphs generated have lot of known properties of <b>Web</b> <b>crawls.</b> Our model is simpler than most random Web graph models, but captures the same properties. Notice that it models the crawling process instead of the page writing process of Web graph models...|$|R
50|$|In February 2014 the Common Crawl project adopted Nutch for its open, {{large-scale}} <b>web</b> <b>crawl.</b>|$|R
50|$|The <b>web</b> <b>crawls</b> from Cuil were {{donated to}} the Internet Archive where they were {{converted}} into arc files, which are available for download at https://archive.org/details/cuilcrawl.|$|R
25|$|While most of {{the website}} owners are keen to have their pages indexed as broadly as {{possible}} to have strong presence in search engines, <b>web</b> <b>crawling</b> can also have unintended consequences and lead to a compromise or data breach if search engine indexes resources that shouldn't be publicly available or pages revealing potentially vulnerable versions of software.|$|E
25|$|Cho et al. {{made the}} first study on {{policies}} for crawling scheduling. Their data set was a 180,000-pages crawl from the stanford.edu domain, in which a crawling simulation was done with different strategies. The ordering metrics tested were breadth-first, backlink count and partial Pagerank calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his Ph.D. dissertation at Stanford on <b>web</b> <b>crawling.</b>|$|E
25|$|Coffman et al. {{worked with}} a {{definition}} of the objective of a Web crawler that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They {{also noted that the}} problem of <b>Web</b> <b>crawling</b> can be modeled as a multiple-queue, single-server polling system, on which the Web crawler is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web crawler.|$|E
50|$|Google Video also searched other non-affiliated video {{sites from}} <b>web</b> <b>crawls.</b> Sites searched by Google Video in {{addition}} to their own videos and YouTube included GoFish, ExposureRoom, Vimeo, Myspace, Biku, and Yahoo! Video.|$|R
50|$|Large scale <b>web</b> <b>crawls</b> isn't {{the only}} Frontera's purpose. Its {{flexibility}} allows to run crawls of moderate size {{on a single}} machine with few cores by leveraging single process and distributed spiders run modes.|$|R
5000|$|Alexa {{continues}} {{to supply the}} Internet Archive with <b>Web</b> <b>crawls.</b> In 1999, as the company moved away from its original vision of providing an [...] "intelligent" [...] search engine, Alexa was acquired by Amazon.com for approximately US$250 million in Amazon stock.|$|R
5000|$|Frontera (<b>web</b> <b>crawling)</b> is an {{open source}} crawl {{frontier}} implementation written purely in Python.|$|E
50|$|Distributed <b>web</b> <b>crawling</b> is a {{distributed}} computing technique whereby Internet search engines employ many computers to index the Internet via <b>web</b> <b>crawling.</b> Such systems may allow for users to voluntarily offer their own computing and bandwidth resources towards crawling web pages. By spreading the load of these tasks across many computers, costs {{that would otherwise}} be spent on maintaining large computing clusters are avoided.|$|E
5000|$|Frontera is <b>web</b> <b>crawling</b> {{framework}} implementing crawl frontier component {{and providing}} scalability primitives for web crawler applications.|$|E
40|$|The Environment MW Italian Lexicon is a lexicon of noun-noun {{multiword}} expressions automatically /nextracted from a 36 Mio word <b>web</b> <b>crawled</b> corpus in {{the environmental}} domain. The lexicon was produced at CNR-ILC, Pisa, Italy as an outcome of the PANACEA EU-FP 7 Funded Project under Grant Agreement 248064 ([URL]...|$|R
40|$|In this paper, {{we present}} GLB, yet another open source and free system {{to create and}} exploit {{linguistic}} corpora gathered from the web. A simple, robust <b>web</b> <b>crawl</b> algorithm, a multi-dimensional information retrieval tool, and a crude parallelization mechanism are proposed, especially for researchers working in resource-limited environments...|$|R
50|$|Splash Island Adventure: A tropical-themed {{collection}} of 75 activities - including four slides, water cannons, <b>web</b> <b>crawl</b> tunnels, spiral cargo nets and swinging bridges. Splash Island’s centerpiece is a 1,000-gallon bucket atop a five-story tower that tips hundreds {{of gallons of}} water over the whole attraction every few minutes.|$|R
5000|$|... 80legs is a <b>web</b> <b>crawling</b> {{service that}} allows its users {{to create and}} run web crawls through its {{software}} as a service platform.|$|E
5000|$|Web {{archives}} which rely on <b>web</b> <b>crawling</b> {{as their}} primary means of collecting the Web {{are influenced by the}} difficulties of web crawling: ...|$|E
50|$|Some of the {{techniques}} for indexing, and caching are trade secrets, whereas <b>web</b> <b>crawling</b> is a straightforward process of visiting all sites on a systematic basis.|$|E
40|$|The Labour MW Italian Lexicon is a lexicon of noun-noun {{multiword}} expressions automatically /nextracted from a 70 Mio word <b>web</b> <b>crawled</b> corpus in {{the labour}} law domain. The lexicon was produced at CNR-ILC, Pisa, Italy as {{an outcome of}} the PANACEA EU-FP 7 Funded Project under Grant Agreement 248064 ([URL]...|$|R
40|$|Abstract. In {{this paper}} we {{describe}} the development of e-Research tools enabling remote access and analysis of large-scale <b>web</b> <b>crawl</b> data. The tools are being developed {{in the context of}} a planned research project titled the “. au Census”, the aim of which is to gain new insights into Australian commerce and society using data from large-scale crawls of the Australian public web...|$|R
40|$|This paper {{examines}} the average page quality over time of pages downloaded during a <b>web</b> <b>crawl</b> of 328 million unique pages. We use the connectivity-based metric PageRank {{to measure the}} quality of a page. We show that traversing the web graph in breadth-first search order is a good crawling strategy, as it tends to discover high-quality pages early on in the crawl...|$|R
5000|$|There exist various free {{services}} {{which may}} be used to archive web resources [...] "on-demand", using <b>web</b> <b>crawling</b> techniques. These services include the Wayback Machine and WebCite.|$|E
5000|$|Legal aspects {{surrounding}} web scraping {{of alternative}} data {{have yet to}} be defined. Current best practices address the following issues when determining legal compliance of <b>web</b> <b>crawling</b> operations: ...|$|E
5000|$|The school {{holds an}} Annual Competitive Event [...] "Expressions". Through Expressions, many schools in Delhi come {{together}} to compete in categories including Graffiti Painting, <b>Web</b> <b>Crawling,</b> Folk Singing, Best Out of Waste and Band Competitions.|$|E
5000|$|... "You {{may have}} seen stories today {{reporting}} on a new product that we're testing, and speculating about our plans. Here's what's really going on. We are testing a new way for content owners to submit their content to Google, which we hope will complement existing methods such as our <b>web</b> <b>crawl</b> and Google Sitemaps. We think it's an exciting product, and we'll {{let you know when}} there's more news." ...|$|R
40|$|We {{propose to}} use MapReduce to quickly test new {{retrieval}} approaches on {{a cluster of}} machines by sequentially scanning all documents. We present a small case study in which we use a cluster of 15 low cost ma- chines to search a <b>web</b> <b>crawl</b> of 0. 5 billion pages showing that sequential scanning is a viable approach to running large-scale information retrieval experiments with little effort. The code is available to other researchers at: [URL]...|$|R
40|$|The {{bachelor}} {{thesis is}} about utilization of semantic model schema. org. The theoretical part provides {{an analysis of}} using schema. org with data standards Microdata, RDFa and JSON-LD. Furthermore the theoretical part is describing search engine optimization and posibilities of schema. org in its improving. In the practical part there is an analysis of utilization of schema. org in data formats Microdata, RDFa and JSON-LD, based on data extracted from a <b>web</b> <b>crawl</b> in October 2016...|$|R
50|$|In March 2000, {{ruling on}} Tickets.com's {{motion to dismiss}} U.S. District Judge Harry Lindley Hupp found that deep linking was not {{prohibited}} by the Copyright Act because no direct copying had occurred. In August 2000, Hupp denied Ticketmaster's motion for a preliminary injunction against Tickets.com's linking and <b>web</b> <b>crawling.</b> For linking, he wrote that uniform resource locators (URLs) were not copyrightable because they contained only factual and function features, and for <b>web</b> <b>crawling,</b> he wrote that it passed legal muster under the fair use doctrine and did not pose an undue burden on Ticketmaster's servers. The United States Court of Appeals for the Ninth Circuit affirmed in a one-paragraph unpublished opinion.|$|E
5000|$|Automated <b>web</b> <b>crawling</b> tools {{can use a}} {{simplified}} form, where an important field is contact information in case of problems. By convention the word [...] "bot" [...] {{is included in the}} name of the agent. For example: ...|$|E
50|$|As a Web-wide video search engine, Truveo {{competes with}} Google Video, Bing Video, and Blinkx among others. The site claims that its <b>web</b> <b>crawling</b> {{technology}} can find more videos and better metadata than conventional web crawlers for video.|$|E
40|$|Many Natural Language Processing {{applications}} nowadays rely on pre-trained word representations estimated {{from large}} text corpora such as news collections, Wikipedia and <b>Web</b> <b>Crawl.</b> In this paper, we show how to train high-quality word vector representations {{by using a}} combination of known tricks that are however rarely used together. The main result of our work is the new set of publicly available pre-trained models that outperform {{the current state of the}} art by a large margin on a number of tasks...|$|R
40|$|We have {{developed}} a text summarization system that can generate summaries over time from <b>web</b> <b>crawls</b> on disasters. We show that our method of identifying exemplar sentences for a summary using affinity propagation clustering produces better summaries than clustering based on K-medoids as measured using Rouge on a small set of examples. A key component of our approach is the prediction of salient in-formation using event related features based on location, temporal changes in topic, and two different language mod-els...|$|R
50|$|In November 2012, PADICAT has {{preserved}} 58,122 <b>webs,</b> 249.609 <b>crawls,</b> 349 million {{files and}} 13 TB of disk space. All {{of them are}} freely available.|$|R
