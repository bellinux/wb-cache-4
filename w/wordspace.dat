9|4|Public
50|$|It {{also runs}} a {{programme}} called Writers in Schools which takes New Zealand writers and illustrators into schools throughout the country. School students {{who live in}} geographically isolated areas can also speak to writers through a programme called <b>WordSpace,</b> which {{is a series of}} video-conferences between writers and students held annually.|$|E
50|$|Letter-spacing {{may also}} {{refer to the}} {{insertion}} of fixed spaces, as was commonly done in hand-set metal type to achieve letter-spacing. This is a more mechanical method which relies less upon spacing and kerning tables resident in each typeface and accessed and used when letterspacing is applied universally. Fixed spaces include a hair space, thin space, <b>wordspace,</b> en-space, and em-space. An en-space is equal to half the current point size, and an em-space is the same width as the current point size.|$|E
40|$|Abstract. Distributional {{approaches}} {{are based on}} a simple hypothesis: the meaning of a word can be inferred from its usage. The application of that idea to the vector space model makes possible the construction of a <b>WordSpace</b> in which words are represented by mathematical points in a geometric space. Similar words are represented close in this space and the definition of “word usage ” depends on the definition of the context used to build the space, which can be the whole document, the sentence in which the word occurs, a fixed window of words, or a specific syntactic context. However, in its original formulation <b>WordSpace</b> can take into account only one definition of context at a time. We propose an approach based on vector permutation and Random Indexing to encode several syntactic contexts in a single <b>WordSpace.</b> We adopt WaCkypedia EN corpus to build our <b>WordSpace</b> that is a 2009 dump of the English Wikipedia (about 800 million tokens) annotated with syntactic information provided by a full dependency parser. The effectivenes...|$|E
40|$|This paper proposes an {{approach}} {{to the construction of}} <b>WordSpaces</b> which takes into account temporal information. The proposed method is able to build a geometrical space considering several periods of time. This methodology enables the analysis of the time evolution of the meaning of a word. Exploiting this approach, we build a framework, called Temporal Random Indexing (TRI) that provides all the necessary tools for building <b>WordSpaces</b> and performing such linguistic analysis. We propose some examples of usage of our tool by analysing word meanings in two corpora: a collection of Italian books and English scientific papers about computational linguistics...|$|R
40|$|Program {{appearance}} {{has changed}} little {{since the first}} high-level languages were developed in the 1960 s. With a few exceptions, most notably APL, programs have been expressed and composed almost entirely out of alphanumeric symbols (“ASCII text”). They are typically presented in a single typeface, often without even the use of boldface or italic; in a single point size, with fixed-width characters, fixed <b>wordspacing,</b> and fixed linespacing; and {{without the benefit of}} symbols, rules, grids, gray scale, diagrammatic elements, and pictures. Program Appearance Why is program appearance so impoverished? In part, this is an artifact of the composing and printing technologies of the early days of computing — keypunch, teletype, and line printer. These obsolete technologies have been superseded by the interactive raster display and the laser printer. Programs can now easily be represented using those elements heretofore omitted, such as multiple typefaces; variable weights, slants, point sizes, <b>wordspacing,</b> and linespacing; and rules, gray scale, and pictures...|$|R
5000|$|When typing a document, the {{technical}} communicator {{should make the}} text flush left because: [...] "it’s the easiest to read because we read it most often". Keeping the text flush left instead of justifying it: [...] "gives the text a more harmonious appearance and {{makes it easier to}} read, since all <b>wordspaces</b> have the same width". The reason justified text should be avoided is because of the: [...] "hideously stretched and squished words and spaces".|$|R
40|$|Distributional {{approaches}} {{are based on}} a simple hypothesis: the meaning of a word can be inferred from its usage. The application of that idea to the vector space model makes possible the construction of a <b>WordSpace</b> in which words are represented by mathematical points in a geometric space. Similar words are represented close in this space and the definition of "word usage" depends on the definition of the context used to build the space, which can be the whole document, the sentence in which the word occurs, a fixed window of words, or a specific syntactic context. However, in its original formulation <b>WordSpace</b> can take into account only one definition of context at a time. We propose an approach based on vector permutation and Random Indexing to encode several syntactic contexts in a single <b>WordSpace.</b> We adopt WaCkypedia EN corpus to build our <b>WordSpace</b> that is a 2009 dump of the English Wikipedia (about 800 million tokens) annotated with syntactic information provided by a full dependency parser. The effectiveness of our approach is evaluated using the GEometrical Models of natural language Semantics (GEMS) 2011 Shared Evaluation data...|$|E
40|$|Here is {{discussed}} how to construct domain ontologies with both taxonomic and non-taxonomic conceptual relationships, exploiting a machinereadable dictionary (MRD) and domain-specific texts. The taxonomic relationships come from WordNet in the interaction with a domain expert, {{using the following}} two strategies: match result analysis and trimmed result analysis. The non-taxonomic relationships come from domainspecific texts with the analysis of lexical cooccurrence statistics; based on <b>WordSpace</b> to represent lexical items according to how semantically close they are to one another. We have done case studies {{in the field of}} law. The empirical results show us that our environment can support a user in constructing domain ontologies...|$|E
40|$|This paper explores two {{hypotheses}} regarding {{vector space}} models that predict the compositionality of German noun-noun compounds: (1) Against our intuition, we demonstrate that window-based rather than syntax-based distributional features perform better predictions, and that not adjectives or verbs but nouns represent {{the most salient}} part-of-speech. Our overall best result is state-of-the-art, reaching Spearman’s ρ = 0. 65 with a <b>wordspace</b> model of nominal features from a 20 word window of a 1. 5 billion word web corpus. (2) While there are {{no significant differences in}} predicting compound–modifier vs. compound–head ratings on compositionality, we show that the modifier (rather than the head) properties predominantly influence the degree of compositionality of the compound...|$|E
40|$|Adobe 2 ̆ 7 s newest {{page layout}} program, InDesign, {{includes}} a 2 ̆ 2 multi-line composing engine. 2 ̆ 2 This feature has been highlighted in presentations and preliminary literature {{about the program}} by Adobe. The claim being made by Adobe is that the multi-line composing method will produce visible improvements over traditional line-by-line justification methods, such as that employed by the current most-popular page layout program, QuarkXPress. Text produced using line-by-line justification methods tends to exhibit significant variances in interword spacing from one line to the next. Text often appears too loose or too tight in parts, and visual effects such as rivers of white space running through a column are often present. A multi-line method of justification should produce markedly better results, as interword spacing should be mostly consistent throughout an entire paragraph. The idea for a multi-line justification method {{is based on the}} hand-compositor 2 ̆ 7 s practice of resetting previous lines of text when a line cannot be acceptably justified on its own. This practice became very difficult {{with the arrival of the}} Linotype in 1886 and practically impossible with the Monotype in 1887. First- and second- generation phototypesetters also did not allow any form of multi-line justification. Only with the arrival of typographical technology to desktop systems has it again become possible to employ a multi-line justification method. Two notable programs that were able to perform multi-line justification before the arrival of InDesign are Donald Knuth 2 ̆ 7 s page description language, _fX, and the /zz-program, developed by Peter Karow and Hermann Zapf. InDesign 2 ̆ 7 s multi-line composition engine is in fact based on TX and the /zz-program, and it employs similar algorithms. Although there have been comparisons done between TeX or the /tz-program and line-by-line justification methods, there have been no extensive comparisons between InDesign 2 ̆ 7 s multi-linecomposing engine and a program using a line-by-line justification method, such as QuarkXPress. The hypothesis of this thesis, then, is that InDesign 2 ̆ 7 s multi-line justification method will indeed produce a significant improvement over the line-by-line justification method used by QuarkXPress. This hypothesis is tested through flowing text into three templates that are designed to be representative of a book layout, a newspaper layout, and an magazine layout. Identical versions of these templates were created in both InDesign and QuarkXPress 3. 32. A challenging, yet not extraordinary text was flowed into all templates. <b>Wordspacing</b> and hyphenation are evaluated...|$|R
40|$|Abstract. In this paper, we {{describe}} how to exploit a machine-readable dictionary (MRD) and domain-specific text corpus {{in supporting the}} construction of domain on-tologies that specify taxonomic and non-taxonomic relationships among given domain concepts. A) In building taxonomic relationships (hierarchically structure) of domain concepts, some hierarchically structure can be extracted from a MRD with marked sub-trees that may be modified by a domain expert, using matching result analysis and trimmed result analysis. B) In building non-taxonomic relationships (specifica-tion templates) of domain concepts, we construct concept specification templates that come from pairs of concepts extracted from text corpus, using <b>WordSpace</b> and an association rule algorithm. A domain expert modifies taxonomic and non-taxonomic relationships later. Through the case study with CISG, we {{make sure that our}} system can work to support the process of constructing domain ontologies with a MRD and text corpus. ...|$|E
40|$|Semantic {{networks}} have become key components in many {{natural language processing}} applications. This paper presents an automatic construction of Amharic semantic networks using Amharic WordNet as initial knowledge base where intervening word patterns between pairs of concepts in the WordNet are extracted for a specific relation from a given text. For each pair of concepts which we know the relationship contained in Amharic WordNet, we search the corpus for some text snapshot between these concepts. The returned text snapshot is processed to extract all the patterns having n-gram words between the two concepts. We use the <b>WordSpace</b> model for extraction of semantically related concepts and relation identification among these concepts utilizes the extracted text patterns. The system is designed to extract “part-of ” and “type-of” relations between concepts which are very popular and frequently found between concepts in any corpus. The system was tested in three phases with text corpus collected from news outlets, and experimental results are reported. ...|$|E
40|$|Preamble IV Abstract; This thesis {{explores the}} problem of {{inferring}} meaning from un-annotated language: arguably a key problem {{in the pursuit of}} strong AI. We take pains to tackle the problem from the ground up, re-examining ingrained devices such as eo-occurrence and <b>wordspace,</b> in search of insights into their known limitations. We pay particular attention to the pervasive problem of the poverty-of-the-stimulus, and how this can be tackled without sacrificing specificity. All of the while we adhere to a purely distributional paradigm. Our work results in three main contributions to the field: Firstly, taking a cue from statistical biogerography, we explore and develop distance-based (windowless) association measures which re-interpret the notion of eo-occurrence introduced by Harris (1954). While there has been some experimentation with distance-based devices in the past, we prove though both intrinsic analyses and psycholinguistic evaluations that they provide a particularly robust foundation for distributional analy,ses. Secondly, taking our cue from semiotics, we investigate an alternative vector space model of words-in-context which derives from the notion of reader expectation. The model combines the advantages of spaces built from high-order eo- occurrence vectors (intuitive geometric interpretations, and dense generalising vectors), with those of arbitrarily sophisticated language models (sensitivity to high- arity language structure, and the ability to exploit diverse heterogeneous feature- " sets). We test a simple implementation in a word sense disambiguation setting with very encouraging results, showing - importantly - that such models represent plausible accounts of meaning. Thirdly, we show how the resultant expectation vectors lead to an implicit compositional account of meaning. While the implementation arises trivially from the vectors, our experiments allude to some surprisingly sophisticated behaviour which indicates a sensitivity to both structural and lexical aspects of phrases. During the course of these investigations we make several subordinate contributions to the field. Among these are a formulation of distance-based predictive language models, and particularly robust vector-similarity measures based on fuzzy rough sets. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|E

