6|11|Public
40|$|Abstract—The focused <b>web-harvesting</b> is {{deployed}} to realize an automated and comprehensive index databases {{as an alternative}} way for virtual topical data integration. The <b>web-harvesting</b> has been implemented and extended by not only specifying the targeted URLs, but also predefining human-edited harvesting parameters to improve the speed and accuracy. The harvesting parameter set comprises three main components. First, the depthscale of being harvested final pages containing desired information counted from the first page at the targeted URLs. Secondly, the focus-point number to determine the exact box containing relevant information. Lastly, the combination of keywords to recognize encountered hyperlinks of relevant images or full-texts embedded in those final pages. All parameters are accessible and fully customizable for each target by the administrators of participating institutions over an integrated web interface. A real implementation to the Indonesian Scientific Index which covers all scientific information across Indonesia is also briefly introduced. I...|$|E
40|$|The focused <b>web-harvesting</b> is {{deployed}} to realize an automated and comprehensive index databases {{as an alternative}} way for virtual topical data integration. The <b>web-harvesting</b> has been implemented and extended by not only specifying the targeted URLs, but also predefining human-edited harvesting parameters to improve the speed and accuracy. The harvesting parameter set comprises three main components. First, the depth-scale of being harvested final pages containing desired information counted from the first page at the targeted URLs. Secondly, the focus-point number to determine the exact box containing relevant information. Lastly, the combination of keywords to recognize encountered hyperlinks of relevant images or full-texts embedded in those final pages. All parameters are accessible and fully customizable for each target by the administrators of participating institutions over an integrated web interface. A real implementation to the Indonesian Scientific Index which covers all scientific information across Indonesia is also briefly introduced. Comment: 6 pages, 4 figures, Proceeding of the International Conference on Advanced Computational Intelligence and Its Applications 200...|$|E
40|$|This paper {{describes}} an automatic method {{for creating a}} domain-independent senseannotated corpus harvested from the web. As a proof of concept, this method {{has been applied to}} German, a language for which sense-annotated corpora are still in short supply. The sense inventory is taken from the German wordnet GermaNet. The <b>web-harvesting</b> relies on an existing mapping of GermaNet to the German version of the web-based dictionary Wiktionary. The data obtained by this method constitut...|$|E
40|$|This chapter reviews {{existing}} {{data mining}} tools for scraping data from heterogeneous online social networks. It introduces {{not only the}} complexities of scraping data from these sources (which include diverse data forms), but also presents currently available tools including their strengths and weaknesses. The chapter introduces our solution to effectively mining online social networks {{through the development of}} VoyeurServer, a tool we designed which builds upon the open-source <b>Web-Harvest</b> framework. We have shared details of how VoyeurServer was developed and how it works so that data mining developers can develop their own customized data mining solutions built upon the <b>Web-Harvest</b> framework. We conclude the chapter with future directions of our data mining project so that developers can incorporate relevant features into their data mining applications...|$|R
40|$|We {{present a}} new methodological {{approach}} which combines both naturally-occurring speech harvested {{on the web}} and speech data elicited in the laboratory. This proof-of-concept study examines the phenomenon of focus sensitivity in English, in which the interpretation of particular grammatical constructions (e. g., the comparative) is sensitive {{to the location of}} prosodic prominence. Machine learning algorithms (support vector machines and linear discriminant analysis) and human perception experiments are used to cross-validate the <b>web-harvested</b> and lab-elicited speech. Results confirm the theoretical predictions for location of prominence in comparative clauses and the advantages using both <b>web-harvested</b> and lab-elicited speech. The most robust acoustic classifiers include paradigmatic (i. e., un-normalized), non-intonational acoustic measures (duration and relative formant frequencies from single segments). These acoustic cues are also significant predictors of human listeners’ classification, offering new evidence in the debate whether prominence is mainly encoded by pitch or by other cues, and the role that utterance-normalization plays when looking at non-pitch cues such as duration...|$|R
30|$|In this work, the context-based metric {{was applied}} with H = 1 over a <b>web-harvested</b> corpus, while the contextual {{features}} were weighted using a binary scheme. The word affective ratings were estimated using as seeds 600 entries {{selected from the}} ANEW lexicon [94]. More details about the corpus, seed selection, and the training of λ weights {{can be found in}} [92]. The three affective ratings plus the POS tagging values constitute the four features for the modeling of text saliency.|$|R
40|$|Abstract. Today, a {{historically}} unprecedented {{volume of data}} {{is available in the}} public domain with the potential of becoming useful for researchers. More {{than at any other time}} before, political parties and governments are making data available such as speeches, legislative bills and acts. However, as the size of available data increases, the need for sophisticated tools for <b>web-harvesting</b> and data analysis simultaneously grows. Yet, for the most part researchers who are developing these tools come from a computer science background, while researchers in the social and behavior sciences who have an interest in using such tools often lack the necessary training to apply these tools themselves. In order to provide a bridge between these two communities we propose a new tool called PolicyMiner. The objective of this tool is twofold: First, to provide a general purpose <b>web-harvesting</b> and data clean-up tool which can be used with relative ease by researchers with limited technical backgrounds. The second objective is to implement knowledge discovery algorithms that can be applied to textual data, such as legislative acts. With our paper we present a technical document which details the steps of data processing that have been implemented in the PolicyMiner. First, the PolicyMiner harvests the raw html data from publically available websites, such as governmental sites, and provides a unique integrated view for the data. Second, it cleans the data by removing irrelevant items, such as html tags and non-informative terms. Third, it classifies the harvested data according to a pre-defined standard conceptual hierarchy relying on the Eurovoc thesaurus. Fourth, it applies different knowledge discovery algorithms such as time series and correlation-based analysis to capture the temporal and substantive policy dependencies of the textual data across countries. ...|$|E
40|$|Web Archive Switzerland is a {{pilot project}} {{undertaken}} {{in collaboration with the}} Swiss Cantonal libraries as part of the e-Helvetica Project at the Swiss National Library (SNL). The mission of the SNL and the Swiss Cantonal libraries is to collect and archive all Swiss publications, both printed and electronic. The objective of the e-Helvetica Project is to fulfill this mission for electronic publications. The Web Archive Switzerland pilot project recently designed and tested a shared workflow for selecting, collecting, cataloguing, archiving and disseminating non-commercial Swiss web resources. Within the workflow the Cantonal libraries are responsible for the identification, selection and announcement of data on the Web. The SNL is responsible for the <b>web-harvesting</b> (objects and rights), cataloguing, archiving and dissemination of those identified web resources. The aim of this co-operation is to share resources and knowledge. This presentation will give some historical context to Web Archive Switzerland and discuss the collaboration with the Cantonal libraries and the shared workflow. The approach chosen for selecting, cataloguing and harvesting the web resources will be highlighted as well as our experiences to date. The presentation will conclude with some ideas on how we are planning to disseminate the web resources. For further information on the e-Helvetica Project (in German and French), see: <[URL]...|$|E
40|$|Faced in 2009 {{with the}} loss of its {{microfilming}} program the Minnesota Historical Society has sought to rethink how it will provide access to the rich and popular content found in Minnesota newspapers. Several linked digital projects are emerging to both mitigate the loss and to move historical news content access to Web space, perhaps to the ultimate benefit of public audiences. Three speakers from MHS will summarize the emergent projects and will offer an early evaluation of the benefits and liabilities of each. These projects include the repository s National Digital Newspaper Program participation (and a couple related local projects using very similar technical and access models); a partnership with Minnesota newspaper publishers to acquire, preserve, and provide public access to the digital representation of daily and weekly newspapers; and a project to provide access to the news content appearing on media websites (newspaper publishers, as well as broadcast media) using a <b>web-harvesting</b> service. Panelists will consider, for each of these projects, the challenges and opportunities it creates in the areas of technical tools manage and preserve content, collaborations with a diverse set of partners, and mechanisms for affording public access. ABOUT THE PRESENTERS: Dennis Meisner is Head of Collections Management; Jane Wong is Central Collection Services Manager; Sarah Quimby is Library Processing Manager; and Noah Skogerboe is Project Associate, Newspaper Digitization at the Minnesota Historical Society...|$|E
40|$|We {{present a}} new methodological {{approach}} which combines both naturally-occurring speech “harvested” {{on the web}} and speech data elicited in the laboratory. This proof-of-concept study examines the phenomenon of focus sensitivity, in which the interpretation of particular grammatical constructions (e. g. the English comparative) is sensitive {{to the location of}} prosodic prominence. Machine learning algorithms (support vector machines and linear discriminant analysis) and human perception experiments are used to cross-validate the <b>web-harvested</b> and lab-elicited speech. Results confirm the theoretical predictions for location of prominence in comparative clauses and the advantages using both <b>web-harvested</b> and lab-elicited speech. The most robust acoustic classifiers include paradigmatic (i. e. un-normalized), non-intonational acoustic measures (duration and relative formant frequencies from single segments). These acoustic cues are also significant predictors of human listeners’ classification, offering new evidence in the decades-old debate surrounding the role of syntagmatic (i. e. utterance-normalized) and intonational acoustic cues of semantic focus. This research was supported by the following grants: NSF 1035151 RAPID: Harvesting Speech Datasets for Linguistic Research on the Web (Digging into Data Challenge), SSHRC Digging into Data Challenge Grant 869 - 2009 - 0004, the SSHRC CRC program and SSHRC 410 - 2011 - 1062 Relative prosodic boundary strength and its role in encoding syntactic structure...|$|R
40|$|We {{investigate}} {{the creation of}} corpora from <b>web-harvested</b> data following a scalable approach that has linear query complexity. Individual web queries are posed for a lexicon that includes thousands of nouns and the retrieved data are aggregated. A lexical network is constructed, in which the lexicon nouns are linked according to their context-based similarity. We introduce the notion of semantic neighborhoods, which are exploited for the computation of semantic similarity. Two types of normalization are proposed and evaluated on the semantic tasks of: (i) similarity judgement, and (ii) noun categorization and taxonomy creation. The created corpus along {{with a set of}} tools and noun similarities are made publicly available...|$|R
40|$|Abstract. Irony is an {{effective}} but challenging mode of communication that allows a speaker to express sentiment-rich viewpoints with concision, sharpness and humour. Irony is especially common in online documents that express subjective and deeply-felt opinions, and thus represents a significant obstacle to the accurate analysis of sentiment in web texts. In this paper we look at one commonly used framing device for linguistic irony – the simile – to show how irony is often marked in ways that make it computationally feasible to detect. We conduct a very large corpus analysis of <b>web-harvested</b> similes to identify the most interesting characteristics of ironic comparisons, and provide an empirical evaluation of a new algorithm for separating ironic from non-ironic similes. ...|$|R
40|$|Irony is an {{effective}} but challenging mode of communication that allows a speaker to express sentiment-rich viewpoints with concision, sharpness and humour. Creative irony is especially common in online documents that express subjective and deeply-felt opinions, and thus represents a significant obstacle to the accurate analysis of sentiment in web texts. In this paper we look at one commonly used framing device for linguistic irony – the simile – to show how even the most creative uses of irony are often marked in ways that make them computationally feasible to detect. We conduct a very large corpus analysis of <b>web-harvested</b> similes to identify the most interesting characteristics of ironic comparisons, and provide an empirical evaluation of a new algorithm for separating ironic from non-ironic similes...|$|R
40|$|This chapter {{identifies}} {{a number}} of the most common data mining toolkits and evaluates their utility in the extraction of data from heterogeneous online social networks. It introduces not only the complexities of scraping data from the diverse forms of data manifested in these sources, but also critically evaluates currently available tools. This analysis is followed by a presentation and discussion on the development of a hybrid system, which builds upon the work of the open-source <b>Web-Harvest</b> framework, for the collection of information from online social networks. This tool, VoyeurServer, attempts to address the weaknesses of tools identified in earlier sections, as well as prototype the implementation of key functionalities thought to be missing from commonly available data extraction toolkits. The authors conclude the chapter with a case study and subsequent evaluation of the VoyeurServer system itself. This evaluation presents future directions, remaining challenges, and additional extensions thought to be important to the effective development of data mining tools for the study of online social networks...|$|R
40|$|The present paper explores a {{wide range}} of word sense {{disambiguation}} (WSD) algorithms for German. These WSD algorithms are based on a suite of semantic relatedness measures, including path-based, information-content-based, and gloss-based methods. Since the individual algorithms produce diverse results in terms of precision and thus complement each other well in terms of coverage, a set of combined algorithms is investigated and compared in performance to the individual algorithms. Among the single algorithms considered, a word overlap method derived from the Lesk algorithm that uses Wiktionary glosses and GermaNet lexical fields yields the best F-score of 56. 36. This result is outperformed by a combined WSD algorithm that uses weighted majority voting and obtains an F-score of 63. 59. The WSD experiments utilize the German wordnet GermaNet as a sense inventory as well as WebCAGe (short for: <b>Web-Harvested</b> Corpus Annotated with GermaNet Senses), a newly constructed, sense-annotated corpus for this language. The WSD experiments also confirm that WSD performance is lower for words with fine-grained sense distinctions compared to words with coarse-grained senses...|$|R
40|$|I {{present a}} new {{approach}} to research on meaning and prosody, using speech "harvested" from the web. I advocate a pluralistic view of linguistic data and methodology, within which <b>web-harvested</b> speech plays a vital role. I show that webharvested speech can be used effectively with computational and experimental methods on the one hand, and qualitative, impressionistic study on the other. My domain of inquiry is the well-known correlation between (i) which information in a discourse is most important (e. g. new or contrastive); and (ii) which material in an utterance is realized with prosodic prominence (e. g. stress, accent) which I refer to as "focus". In Chapter 2, I describe the method of harvesting speech data from the web, quantify its efficacy and discuss possible improvements. In Chapter 3, I investigate the location and acoustic realization of focus in comparative clauses (e. g. than I did). Using machine learning and human classifiers, I discover a robust correlation between particular acoustic cues of prosodic prominence and the location of focus predicted by linguistic theory. From the robustness of nonintonational acoustic cues, I hypothesis that focus may be realized by discrete, paradigmatic (i. e. cross-utterance) categories of stress. Results obtained from the <b>web-harvested</b> speech are cross-validated in a laboratory production experiment with stimuli modeled on the web data. Experimental results also confirm a distinct, but ambiguous prosodic realization of "second occurrence focus", which has been central to debates surrounding the semantics of focus. In Chapters 4 and 5, I investigate the adnominal emphatic reflexive (ER; e. g. himself in Jane met Chomsky himself). I argue that it is an instance of a theoretically predicted but poorly attested focus-sensitive operator having sub-propositional scope. Using constructed data and personal introspection, I argue that the adnominal ER exhibits the expected pragmatic, semantic, syntactic and prosodic properties of focus sensitive constructions, and I reconcile opposing approaches to its semantics. Finally, I debunk a deterministic view of focus, according to which certain linguistic constructions in a language are inherently or obligatorily focused, through the careful investigation of the intonation and discourse context of individual examples of the adnominal ER. NSF 1035151 RAPID: Harvesting Speech Datasets for Linguistic Research on the Web (Digging into Data Challenge) SSHRC Digging into Data Challenge Grant 869 - 2009 - 0004 Project: Harvesting Speech Datasets for Linguistic Research on the We...|$|R
40|$|Having {{received}} a lively {{response to our}} call for papers on the lexicography of Japanese as a second language, the editorial board decided to dedicate two issues of this year's ALA to this theme, and I am happy to introduce {{the second round of}} papers, after the first thematic issue published in October this year. This issue is again divided into two parts. The first two papers offer analyses of two aspects of existing dictionaries {{from the point of view}} of Japanese language learners, while the following four papers present particular lexicographic projects for learners of Japanese as a foreign language. The first paper, by Kanako Maebo, entitled A survey of register labelling in Japanese dictionaries - Towards the labelling of words in dictionaries for learners of Japanese, analyses register labelling in existing dictionaries of Japanese, both in those expressly intended for learners of Japanese as a second language and those intended for native speakers, pointing out how register information provided by such dictionaries is not sufficient for L 2 language production. After stressing the usefulness of usage examples for learners trying to write in Japanese, she offers an example of a corpus-based register analysis and proposes a typology of labels to be assigned to dictionary entries, calling for the development of corpora of different genres to be used for lexical analysis. In the second paper, An analysis of the efficiency of existing kanji indexes and development of a coding-based index, Galina N. Vorobеva and Victor M. Vorobеv tackle one of the most time-consuming tasks learners of Japanese are confronted with: looking up unknown Chinese characters. After a comprehensive description of existing indexes, including less known indexing systems developed by Japanese, Chinese, Russian and German researchers, they compare the efficiency of these systems using the concept of selectivity, and propose their own coding-based system. Although searching for unknown characters is becoming increasingly easy with the use of optical character recognition included in portable electronic dictionaries, tablets and smart-phones, not all learners have yet access to such devices. Efficient indexes for accessing information on Chinese characters are therefore still a valuable tool to support language learners in this most tedious task, while the ability to decompose a character into component parts remains an important basis for character memorisation. The second part of this issue presents four projects aimed at supporting particular lexical needs of learners of Japanese as a second language. In the first paper, Development of a learners' dictionary of polysemous Japanese words and some proposals for learners’ lexicography, Shingo Imai presents a new lexicographic approach to the description of polysemous words. As Imai rightfully stresses, the most basic and common words learned by beginning language learners are actually often very polysemous; being deceivingly simple at first glance, they are often introduced with simple glosses or basic prototypical examples at the first stages of learning, and later treated as known words in intermediate or advanced textbooks, even if used for less common senses which are still unknown to the learners, causing much confusion. In the dictionary series presented here, polysemous headwords are thoroughly and systematically described within their semantic networks, where the connections between core and derived meanings are schematically visualised and exemplified. The following two papers present two of the first and most popular web-based systems for Japanese language learning support, both of which have been developing for more than a decade, supporting Japanese language learners all over the world. Reading Tutor, a reading support system for Japanese language learners, presented by Yoshiko Kawamura, is a widely known and used system based at Tokyo International University, which offers automatic glossing of Japanese text with Japanese definitions and examples, and translations into 28 languages. After introducing the system, its development, functionalities and its tools for signalling the level of difficulty of single words, characters, or whole Japanese texts, the author describes its possible uses in language instruction and autonomous learning, and one concrete example of its application to the development of learning material for a specific segment of learners, foreign candidates to the Japanese national examination for certified care workers, mostly Filipino and Indonesian nurses working in Japan. The author concludes with suggestions for fostering autonomous vocabulary learning. The other Japanese language learning support system with an equally long and successful tradition, developed at Tokyo Institute of Technology, is presented by its initiator, Kikuko Nishina, and one of its younger developers, Bor Hodošček, in Japanese Learning Support Systems: Hinoki Project Report. The article presents the many components of this successful system, including Asunaro, a reading support system aimed especially at science and engineering students and speakers of underrepresented Asian languages, Natsume, a writing assistance system using large-scale corpora to support collocation search, Natane, a learner corpus, and Nutmeg, an automatic error correction system for learners' writing. The last project report, by Tomaž Erjavec and myself, introduces resources and tools being developed at the University of Ljubljana and at Jožef Stefan Institute: JaSlo: Integration of a Japanese-Slovene Bilingual Dictionary with a Corpus Search System. The dictionary, corpora and search tools are being developed primarily for Slovene speaking learners of Japanese, but part of the tools, particularly the corpus of sentences from the <b>web-harvested</b> texts, divided into five difficulty levels, can be used by any learner or teacher of Japanese. I hope you will enjoy reading these articles as much as I did, and wish you a peaceful New Year.    </p...|$|R

