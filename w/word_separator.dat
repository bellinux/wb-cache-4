15|20|Public
2500|$|A colon-like symbol (⁚) is {{sometimes}} {{used as a}} <b>word</b> <b>separator.</b> [...] In some cases a ring (⸰) is used instead.|$|E
5000|$|The {{letters are}} encoded U+10900 [...] aleph through to U+10915 [...] taw,U+10916 ,U+10917 ,U+10918 [...] andU+10919 [...] encode the numerals 1, 10, 20 and 100 {{respectively}} andU+1091F [...] is the <b>word</b> <b>separator.</b>|$|E
50|$|Such usage {{is similar}} to {{multiword}} file names written for operating systems and applications that are confused by embedded space codes - such file names instead use an underscore (_) as a <b>word</b> <b>separator,</b> as_in_this_phrase.|$|E
50|$|Word {{splitting}} is {{the process}} of parsing concatenated text (i.e. text that contains no spaces or other <b>word</b> <b>separators)</b> to infer where word breaks exist.|$|R
50|$|In {{languages}} with {{a literary}} tradition, there is interrelation between orthography {{and the question}} of what is considered a single <b>word.</b> <b>Word</b> <b>separators</b> (typically spaces) are common in modern orthography of languages using alphabetic scripts, but these are (excepting isolated precedents) a relatively modern development (see also history of writing).|$|R
5000|$|The use of {{underscores}} as <b>word</b> <b>separators</b> in identifiers in programming languages is old, {{dating to}} the late 1960s. It is particularly associated with C, being found in The C Programming Language (1978), and contrasted with Pascal case (a type of camel case). However, the convention traditionally had no specific name: the Python style guide refers to it simply as [...] "lower_case_with_underscores".|$|R
5000|$|Text {{processing}} {{support for}} SBCS and UTF-8 encodings, stopwords, indexing of words known not {{to appear in}} the database ("hitless"), stemming, word forms, tokenizing exceptions, and [...] "blended characters" [...] (dual-indexing as both a real character and a <b>word</b> <b>separator).</b>|$|E
50|$|According to Vékony, the {{inscription}} {{was made by}} a Khavar leader, whose religion was Karaite. The first symbol of the first row is a ligature, its transcription: atlïɣ. The first symbol (from left) in the second row is a Khazarian <b>word</b> <b>separator.</b>|$|E
50|$|Also {{coming from}} Biblical Hebrew, a paseq ⟨⟩ {{is used as}} a <b>word</b> <b>separator.</b> Also not on a {{standard}} Hebrew keyboard, a vertical bar ⟨|⟩ is often used instead. The vertical bar, a standard key on any keyboard, is used in English for such applications as mathematics and computing.|$|E
25|$|Scripts {{can also}} contain any other general {{category}} character such as marks (diacritic and otherwise), numbers (numerals), punctuation, <b>separators</b> (<b>word</b> <b>separators</b> such as spaces), symbols and non-graphical format characters. These {{are included in}} a particular script when they are unique to that script. Other such characters are generally unified and included in the punctuation or diacritic blocks. However, the bulk of characters in any script (other than the common and inherited scripts) are letters.|$|R
50|$|As noted above, {{the single}} and double interpunct {{were used in}} manuscripts (on paper) {{throughout}} the ancient world. For example, Ethiopic inscriptions used a vertical line, whereas manuscripts used double dots (፡) resembling a colon. The latter practice continues today, though the space is making inroads. Classical Latin used the interpunct in both paper manuscripts and stone inscriptions (Wingo 1972:16). Ancient Greek orthography used between two and five dots as <b>word</b> <b>separators,</b> {{as well as the}} hypodiastole.|$|R
25|$|Exacerbating the problem, common {{punched card}} {{character}} sets {{of the time}} were uppercase only and lacked other special characters. It {{was only in the}} late 1960s that the widespread adoption of the ASCII character set made both lower case and the underscore character _ universally available. Some languages, notably C, promptly adopted underscores as <b>word</b> <b>separators,</b> and identifiers such as end_of_file are still prevalent in C programs and libraries (as well as in later languages influenced by C, such as Perl and Python). However, some languages and programmers chose to avoid underscores—among other reasons to prevent confusing them with whitespace—and adopted camel case instead.|$|R
5000|$|Based upon stylistic analysis, the runologist Sven B. F. Jannson has {{suggested}} that the unnamed runemaster who carved Sm 39 also carved inscriptions Sm 5 in Transjö and Sm 44 in Ivla. For example, the rune forms for both Sm 39 and Sm 5 are somewhat unusual as the m-runes are dotted (...) and the k-runes have a stroke to the left instead of to the right, and the runic text on both stones uses two dots (:) as a <b>word</b> <b>separator</b> punctuation mark.|$|E
50|$|The {{most common}} <b>word</b> <b>separator</b> is a space (U+0020). However, {{there are other}} word joiners and {{separators}} that also indicate a break between words and participate in line-breaking algorithms. The No-Break Space (U+00A0) also produces a baseline advance without a glyph but inhibits rather than enabling a line-break. The Zero Width Space (U+200B) allows a line-break but provides no space: in a sense joining, rather than separating, two words. Finally, the Word Joiner (U+2060) inhibits line breaks and also involves none of the white space produced by a baseline advance.|$|E
5000|$|The space {{character}} (U+0020) typically input by the space bar on a keyboard serves semantically as a <b>word</b> <b>separator</b> in many languages. For legacy reasons, the UCS also includes spaces of varying sizes that are compatibility equivalents for the {{space character}}. While these spaces of varying width are important in typography, the Unicode processing model calls for such visual effects to be handled by rich text, markup and other such protocols. They {{are included in the}} Unicode repertoire primarily to handle lossless roundtrip transcoding from other character set encodings. These spaces include: ...|$|E
40|$|This paper {{describes}} a novel algorithm for the unsupervised learning of <b>word</b> <b>separators</b> in raw text. The algorithm requires no language-specific knowledge regarding the text being processed. It relies solely on distributional {{properties of the}} text and uses the minimum description length (MDL) principle in order to partition characters into two subsets that correspond well with the traditional notion of letters and separators. The distinction between these types of characters emerges as an optimal {{solution to the problem of}} simultaneously compressing two elements: the lexicon that is obtained by tokenizing the text using the hypothesized separators, and the representation of the text under this lexicon. The performance of the proposed algorithm is evaluated on the basis of electronic text in English, French and German...|$|R
40|$|The {{usage of}} {{specific}} language codes and chat and SMS-like messages {{is a major}} trend in electronic communications. This fact makes Natrual Language Processing quite hard, even at the simplest step fo text message tokenization, due to the widespread usage of non-alphanumeric symbols, frequent typos and non-standard <b>word</b> <b>separators.</b> In this work we present a new approach for text message tokenization, specific for the Spanish language as used in Social Networks and in electronic communications. Our system has been integrated in a more general application for age-detection in Social Networks developed in {{the research and development}} project WENDY, and it has been quantitatively evaluated both in a direct fashion, and indirectly by its impact on the genearl age-detection application, showing very promising results. </p...|$|R
40|$|Hummingbird {{submitted}} ranked result {{sets for}} all Monolingual Information Re-trieval tasks of the Cross-Language Evaluation Forum (CLEF) 2002. Enabling stem-ming in SearchServer increased average precision by 16 points in Finnish, 9 points in German, 4 points in Spanish, 3 points in Dutch, 2 points in French and Italian, and 1 point in Swedish and English. Accent-indexing increased average precision by 3 points in Finnish and 2 points in German, but decreased it by 2 points in French and 1 point in Italian and Swedish. Treating apostrophes as <b>word</b> <b>separators</b> increased average pre-cision by 3 points in French and 1 point in Italian. Confidence intervals produced using the bootstrap percentile method {{were found to}} be very similar to those produced using the standard method; both were of similar width to rank-based intervals for differences in average precision, but substantially narrower for differences i...|$|R
50|$|When {{alphabetic}} writing {{began in}} Greece, the letter forms used were similar but not {{identical to the}} Phoenician ones and vowels were added, because the Phoenician alphabet did not contain any vowels. There were also distinct variants of the writing system {{in different parts of}} Greece, primarily in how those Phoenician characters that did not have an exact match to Greek sounds were employed. The Ionic variant evolved into the standard Greek alphabet, and the Cumae variant into the Latin alphabet, which accounts for many {{of the differences between the}} two. Occasionally, Phoenician used a short stroke or dot symbol as a <b>word</b> <b>separator.</b>|$|E
50|$|The {{separators}} (File, Group, Record, and Unit: FS, GS, RS and US) {{were made}} to structure data, usually on a tape, in order to simulate punched cards.End of medium (EM) warns that the tape (or other recording medium) is ending.While many systems use CR/LF and TAB for structuring data, {{it is possible to}} encounter the separator control characters in data that needs to be structured. The separator control characters are not overloaded; there is no general use of them except to separate data into structured groupings. Their numeric values are contiguous with the space character, which can be considered a member of the group, as a <b>word</b> <b>separator.</b>|$|E
5000|$|In C and C++, {{keywords}} {{and standard}} library identifiers are mostly lowercase. In the C standard library, abbreviated names {{are the most}} common (e.g. [...] for a function testing whether a character is alphanumeric), while the C++ standard library often uses an underscore as a <b>word</b> <b>separator</b> (e.g. [...] ). Identifiers representing macros are, by convention, written using only upper case letters and underscores (this is related to the convention in many programming languages of using all-upper-case identifiers for constants). Names containing double underscore or beginning with an underscore and a capital letter are reserved for implementation (compiler, standard library) and should not be used (e.g. [...] or [...] ). This is superficially similar to stropping, but the semantics differ: the underscores are part {{of the value of the}} identifier, rather than being quoting characters (as is stropping): the value of [...] is [...] (which is reserved), not [...] (but in a different namespace).|$|E
40|$|Sumo is a {{formalism}} {{for universal}} segmentation of text. Its {{purpose is to}} provide a framework for the creation of segmentation applications. It is called #universal# as the formalism itself is independent of the language of the documents to process and independent of the levels of segmentation #e. g. words, sentences, paragraphs, morphemes [...] . # considered by the target application. This framework relies on a layered structure representing the possible segmentations of the document. This structure and the tools to manipulate it are described, followed by detailed examples highlighting some features of Sumo. Introduction Tokenization, or word segmentation, is a fundamental task of almost all NLP systems. In languages that use <b>word</b> <b>separators</b> in their writing, tokenization seems easy: every sequence of characters between two whitespaces or punctuation marks is a word. This works reasonably well, but exceptions are handled in a cumbersome way. On the other hand, there are languages that do [...] ...|$|R
40|$|Abstract. Self-indexes for natural-language texts, {{where these}} are {{regarded}} as token (<b>word</b> or <b>separator)</b> sequences, achieve very attractive space and search time. However, they suffer from a space penalty due to their large vocabulary. In this paper we show that by replacing the Huffman encoding they implicitly use by the slightly weaker Hu-Tucker encoding, which respects the lexical order of the vocabulary, both their space and time are improved. ...|$|R
40|$|Chinese texts do {{not contain}} spaces as <b>word</b> <b>separators</b> like Eng-lish and many {{alphabetic}} languages. To use Moses to train transla-tion models, we must segment Chinese texts into sequences of Chinese words. Increasingly more software tools for Chinese segmentation are populated on the Internet in recent years. How-ever, some of these tools were trained with general texts, so might not handle domain-specific terms in patent documents very well. Some machine-learning based tools require us to provide seg-mented Chinese to train segmentation models. In both cases, providing segmented Chinese texts to refine a pre-trained model or {{to create a new}} model for segmentation is an important basis for successful Chinese-English machine translation systems. Ide-ally, high-quality segmented texts should be created and verified by domain experts, but doing so would be quite costly. We ex-plored an approach to algorithmically generate segmented texts with parallel texts and lexical resources. Our scores in NTCIR- 10 PatentMT indeed improved from our scores in NTCIR- 9 PatentMT with the new approach. Categories and Subject Descriptor...|$|R
40|$|Although a word-based {{method is}} {{commonly}} used in document retrieval, it cannot be directly applicable to languages that have no obvious <b>word</b> <b>separator.</b> Given a lexicon, {{it is possible to}} identify words in documents, but a large lexicon is troublesome to maintain and makes retrieval systems large and complicated. This paper proposes an effective and efficient ranking that does not use a large lexicon; words need not be identified during document registration because a characterbased signature file is used for the access structure. A user request, during document retrieval, is statistically analyzed to generate an appropriate query, and the query is evaluated efficiently in a word-based manner using the character-based index. We also propose two optimizing techniques to accelerate retrieval...|$|E
40|$|Text Summarization is extracting the {{important}} {{information from the}} document by leaving out the irrelevant information, and to reduce the details and collects them in a compressed way. Normally text summarization is done in single or multi documents. The advantage on processing time can be achieved in the text summarization. Converting English sentences into expressions or Interlingua is called Universal Networking Language (UNL). The given source document is preprocessed by eliminating tables and images. The preprocessed document is fed into sentence splitter and then to <b>word</b> <b>separator.</b> The given word is sent to Morphological Analyzer to find the root word. This root word is fed into UNL dictionary for finding the corresponding concepts and attributes. By using the heuristic rules, we identify the relations between concepts. UNL represents knowledge {{in the form of}} graphical format, where nodes represent concepts and links represent relations between concepts. It represents the whole document not the sentences in particular. The graph algorithm is used to find the weight age of links connected to the Universal Word. According to the highest weight age the document is summarized...|$|E
40|$|This paper {{addresses}} a technique {{using a combination}} of known words, and methods from information theory, to attempt decipherment of additional words in the extinct and as yet undeciphered phonetic script, Meroitic. A short history of the script, and the problems translating it, will be followed by a description of the statistical techniques, their results and implications. A Short History of Meroitic Meroitic was the written phonetic script of the ancient civilization of Kush, located for centuries in what is now Northern Sudan. The word ‘Meroitic ’ derives from the name of the city Meroë, which was located on the east bank of the Nile, south of where the Atbara River flows off to the east. It is the oldest written script in Africa other than Egyptian hieroglyphs and the related hieratic and demotic scripts. It has a hieroglyphic form using some adopted Egyptian signs and a cursive form similar to demotic. The script had one innovation uncommon in ancient written scripts, such as Egyptian hieroglyphics or Greek, in that there was a <b>word</b> <b>separator,</b> similar in function to spaces in modern scripts, that looks similar to a colon (fig. 1). Meroitic is first attested in the 2 nd century BC and was continuousl...|$|E
500|$|Each COBOL {{program is}} made up of four basic lexical items: words, literals, picture character-strings (see [...] ) and <b>separators.</b> <b>Words</b> include {{reserved}} words and user-defined identifiers. They are up to 31 characters long and may include letters, digits, hyphens and underscores. Literals include numerals (e.g. [...] ) and strings (e.g. [...] ). Separators include the space character and commas and semi-colons followed by a space.|$|R
40|$|This paper {{describes}} a new method for unsupervised grammar induction {{based on the}} automatic extraction of certain patterns in the texts. Our starting hypothesis is that there exist some classes of words that function as separators, marking the beginning {{or the end of}} new constituents. Among these separators we distinguish those which trigger new levels in the parse tree. If we are able to detect these separators we can follow a very simple procedure to identify the constituents of a sentence by taking the classes of <b>words</b> between <b>separators.</b> This paper is devoted to describe the process that we have followed to automatically identify the set of separators from a corpus only annotated with Part-of-Speech (POS) tags. The proposed approach has allowed us to improve the results of previous proposals when parsing sentences from the Wall Street Journal corpus. ...|$|R
5000|$|The {{implicit}} grammatical divisions {{can be made}} explicit by <b>separator</b> <b>words</b> such as cu and vau, {{which are}} often elidable but sometimes need to be present to avoid ambiguity::le nixli cu melbi(This instance shows that the left-hand gismu is sumti and the right-hand gismu is selbri. Without cu the two gismu would be grammatically undistinguishable.):mi dunda le cukta gie lebna lo rupnu vau do(vau indicates that the two bridi, dunda le cukta and lebna lo rupnu, sharing the same first sumti mi, together terminate at that position, enabling them to have the subsequent do as their mutual second sumti. Compare it with its longer equivalent: mi dunda le cukta do [...]ije mi lebna lo rupnu do.) ...|$|R
40|$|Abstract. How small can a graph be that {{contains}} as subgraphs all trees on n vertices with maximum degree d? In this paper, {{this question is}} answered by constructing such universal graphs that have n vertices and bounded degree (depending only on d). Universal graphs with n vertices and O(n log n) edges are also constructed that contain all bounded-degree planar graphs on n vertices as subgraphs. In general, it is shown that the minimum universal graph containing all bounded-degree graphs on n vertices with separators of size n has O(n) edges ifa 1 / 2. Key <b>words,</b> universal graphs, <b>separators,</b> trees, planar graphs AMS(MOS) subject classifications. 05 C, 65 W, 68 Q 1. Introduction. Given a familyFofgraphs, a graph G {{is said to be}} F-universal if G contains every graph in F as a subgraph. A fundamental problem of interest is to determine how few edges a universal graph can have. Such problems are of interest in circuit design [Vl, data representation [CRS], [RSS], and parallel computin...|$|R
40|$|AbstractProtein name {{recognition}} aims to detect {{each and every}} protein names appearing in a PubMed abstract. The task is not simple, as the graphic <b>word</b> boundary (space <b>separator)</b> assumed in conventional preprocessing does not necessarily coincide with the protein name boundary. Such boundary disagreement caused by tokenization ambiguity has usually been ignored in conventional preprocessing of general English. In this paper, we argue that boundary disagreement poses serious limitations in biomedical English text processing, not to mention protein {{name recognition}}. Our key idea {{for dealing with the}} boundary disagreement is to apply techniques used in Japanese morphological analysis where there are no word boundaries. Having evaluated the proposed method with GENIA corpus 3. 02, we obtain F-measure of 69. 01 on a strict criterion and 79. 32 on a relaxed criterion. The result is comparable to other published work in protein name recognition, without resorting to manually prepared ad hoc feature engineering. Further, compared to the conventional preprocessing, the use of morphological analysis as preprocessing improves the performance of protein name recognition and reduces the execution time...|$|R

