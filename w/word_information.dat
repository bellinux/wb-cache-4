88|1185|Public
25|$|The book, co-authored with Warren Weaver, The Mathematical Theory of Communication, reprints Shannon's 1948 {{article and}} Weaver's popularization of it, which is {{accessible}} to the non-specialist. Warren Weaver {{pointed out that the}} <b>word</b> <b>information</b> in communication theory is not related to what you do say, but to what you could say. That is, information is a measure of one's freedom of choice when one selects a message. Shannon's concepts were also popularized, subject to his own proofreading, in John Robinson Pierce's Symbols, Signals, and Noise.|$|E
2500|$|This theory {{suggests}} {{there is a}} lag in the brain's ability to recognize {{the color of the}} word since the brain reads words faster than it recognizes colors. [...] This is {{based on the idea that}} word processing is significantly faster than color processing. In a condition where there is a conflict regarding words and colors (e.g., Stroop test), if the task is to report the color, the <b>word</b> <b>information</b> arrives at the decision-making stage before the color information which presents processing confusion. [...] Conversely, if the task is to report the word, because color information lags after <b>word</b> <b>information,</b> a decision can be made ahead of the conflicting information.|$|E
2500|$|Prior to this paper, limited information-theoretic ideas {{had been}} {{developed}} at Bell Labs, all implicitly assuming events of equal probability. [...] Harry Nyquist's 1924 paper, Certain Factors Affecting Telegraph Speed, contains a theoretical section quantifying [...] "intelligence" [...] and the [...] "line speed" [...] at which it can be transmitted by a communication system, giving the relation [...] (recalling Boltzmann's constant), where W is the speed of transmission of intelligence, m {{is the number of}} different voltage levels to choose from at each time step, and K is a constant. [...] Ralph Hartley's 1928 paper, Transmission of Information, uses the <b>word</b> <b>information</b> as a measurable quantity, reflecting the receiver's ability to distinguish one sequence of symbols from any other, thus quantifying information as , where S was the number of possible symbols, and n the number of symbols in a transmission. The unit of information was therefore the decimal digit, which has since sometimes been called the hartley in his honor as a unit or scale or measure of information. Alan Turing in 1940 used similar ideas as part of the statistical analysis of the breaking of the German second world war Enigma ciphers.|$|E
2500|$|The U.S. Naval Academy defines {{plagiarism}} as [...] "the {{use of the}} <b>words,</b> <b>information,</b> insights, {{or ideas}} of another without crediting that person through proper citation." ...|$|R
40|$|WO 200141389 A UPAB: 20010801 NOVELTY - The method {{involves}} {{selecting a}} candidate transmission sequence that meets a defined criterion {{from a number}} of candidate transmission sequences generated from an <b>information</b> <b>word.</b> The <b>information</b> <b>word</b> and a tag for identifying the transmission sequence are processed using an invertable feedback combination algorithm to obtain a combined <b>information</b> <b>word</b> that is identified by the tag. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following: an arrangement for generating a transmission sequence from an <b>information</b> <b>word</b> and for methods of generating a transmission sequence from an <b>information</b> <b>word</b> and detecting an <b>information</b> <b>word</b> from a received transmission sequence. USE - For reduction of par/out-of-band radiation in an OFDM system ADVANTAGE - Provides a practical {{solution to the problem of}} out-of-band radiation...|$|R
40|$|Interleaver for {{scrambling}} an <b>information</b> <b>word,</b> the <b>information</b> <b>word</b> {{having a}} multitude of digits, for obtaining a permuted <b>information</b> <b>word.</b> The interleaver includes a first interleaver stage for a row-by-row arranging of the digits of the <b>information</b> <b>word</b> in a plurality of first rows and first columns, and a second interleaver stage for scrambling the digits {{of one of the}} first rows by interchanging at least two digits of the one first row in order to obtain a first scrambled row, and for replacing the one of the first rows by the first scrambled row. The first interleaver stage is configured for reading the first row, which is replaced based on the first scrambled row, in a column-by-column manner in order to obtain the permuted <b>information</b> <b>word...</b>|$|R
5000|$|Antenne Saar - Spoken <b>word</b> <b>information</b> with Franco-German character.|$|E
5000|$|Although VP {{showed no}} {{evidence}} for transfer of color, shape or size, there was evidence for transfer of <b>word</b> <b>information.</b> This {{is consistent with the}} speculation that the transfer of <b>word</b> <b>information</b> involves fibres in the ventroposterior region of the splenium—the same region in which V.P. had callosal sparing. V.P. is able to integrate words presented to both visual fields, creating a concept that is not suggested by either word. For example, she combines [...] "head" [...] and [...] "stone" [...] to form the integrated concept of a tombstone.|$|E
5000|$|This theory {{suggests}} {{there is a}} lag in the brain's ability to recognize {{the color of the}} word since the brain reads words faster than it recognizes colors. [...] This is {{based on the idea that}} word processing is significantly faster than color processing. In a condition where there is a conflict regarding words and colors (e.g., Stroop test), if the task is to report the color, the <b>word</b> <b>information</b> arrives at the decision-making stage before the color information which presents processing confusion. Conversely, if the task is to report the word, because color information lags after <b>word</b> <b>information,</b> a decision can be made ahead of the conflicting information.|$|E
40|$|This paper {{describes}} a generalized translation memory system, which {{takes advantage of}} sentence level matching, sub-sentential matching, and pattern-based machine translation technologies. All of the three techniques generate translation suggestions {{with the assistance of}} <b>word</b> alignment <b>information.</b> For the sentence level matching, the system generates the translation suggestion by modifying the translations of the most similar example with <b>word</b> alignment <b>information.</b> For sub-sentential matching, the system locates the translation fragments in several examples with <b>word</b> alignment <b>information,</b> and then generates the translation suggestion by combining these translation fragments. For pattern-based machine translation, the system first extracts translation patterns from examples using <b>word</b> alignment <b>information</b> and then generates translation suggestions with pattern matching. This system is compared with a traditional translation memory system without <b>word</b> alignment <b>information</b> in terms of translation efficiency and quality. Evaluation results indicate that our system improves the translation quality and saves about 20 % translation time. ...|$|R
40|$|DE 19840835 A UPAB: 20000508 NOVELTY - The {{system has}} an {{allocation}} device (12), for a reversible code word {{from a group}} of such <b>words,</b> to an <b>information</b> <b>word</b> located in a region of such words. The group of reversible words is so designed that for each <b>information</b> <b>word</b> in the region, an individual reversible code word is available. There is an extra value generator (14) for an <b>information</b> <b>word</b> outside the word region. The decoder comprises a detector of reversible code words from a word sequence, and an allocation device of a certain <b>information</b> <b>word</b> to the code word, based on a code table. The detector picks up a preset code in the code word sequence, and the extra <b>information</b> <b>word</b> is then determined. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the coding processes. USE - For modern audio signal coding and decoding according to the standard moving picture expert group (MPEG) layer 3. ADVANTAGE - Improved fault recognition if the entropy coded <b>information</b> <b>words</b> are transmitted over defective channel...|$|R
30|$|Law {{enforcement}}: Law {{enforcement agencies}} can subpoena {{records from the}} credit card Issuer {{to find out the}} time, date, and place of a credit card purchase, in other <b>words</b> <b>information</b> that may be helpful in determining the last known location of a crime victim or suspect. National security agencies also track terrorist activity by monitoring certain purchases.|$|R
5000|$|The name Info {{comes from}} the <b>word</b> <b>information,</b> which {{represents}} all the new breed needs for being smart and powerful in the future: there are no limits for information in nowadays... "Internet, media, networks exist for the human being evolution and reached by everyone" [...] (John C. on interview for Radioactiva 97.9 FM, Colombia - March 2005).|$|E
5000|$|Hartley's 1928 paper, called simply [...] "Transmission of Information", {{went further}} {{by using the}} <b>word</b> <b>information</b> (in a {{technical}} sense), and making explicitly clear that information in this context was a measurable quantity, reflecting only the receiver's ability to distinguish that one sequence of symbols had been intended by the sender rather than any other—quite regardless of any associated meaning or other psychological or semantic aspect the symbols might represent. This amount of information he quantified as ...|$|E
50|$|The book, co-authored with Warren Weaver, The Mathematical Theory of Communication, reprints Shannon's 1948 {{article and}} Weaver's popularization of it, which is {{accessible}} to the non-specialist. Warren Weaver {{pointed out that the}} <b>word</b> <b>information</b> in communication theory is not related to what you do say, but to what you could say. That is, information is a measure of one's freedom of choice when one selects a message. Shannon's concepts were also popularized, subject to his own proofreading, in John Robinson Pierce's Symbols, Signals, and Noise.|$|E
50|$|Commentaries (≤ 5,000 <b>words)</b> present <b>information</b> and {{personal}} insight {{on a particular}} topic.|$|R
40|$|Abstract. By using part {{of speech}} {{information}} to similarity calculation of paraphrase sentences, the similarity calculation method is improved. The improved method includes multi information of paraphrase text, such as, matching <b>words</b> <b>information</b> and {{part of speech}} information. A small-scale comparing experiment of double negation sentence paraphrase for two method of similarity calculation has been done, and the experimental results indicated effect of improvement method...|$|R
30|$|A {{realistic}} {{model for}} information propagation {{may be a}} state dependent variant of the model where propagation events (attempts of influence) occur only for new information (or probability for new information is higher). In other <b>words,</b> <b>information</b> is mediated to neighbouring nodes only in cases when the node is unaware of the information before the propagation event. Nodes are less willing to propagate known information than new information.|$|R
5000|$|The English {{word was}} {{apparently}} {{derived from the}} Latin stem (information-) of the nominative (informatio): this noun {{is derived from the}} verb informare (to inform) in the sense of [...] "to give form to the mind", [...] "to discipline", [...] "instruct", [...] "teach". Inform itself comes (via French informer) from the Latin verb informare, which means to give form, or to form an idea of. Furthermore, Latin itself already contained the word informatio meaning concept or idea, but the extent to which this may have influenced the development of the <b>word</b> <b>information</b> in English is not clear.|$|E
5000|$|From 1987 to April 1991, {{the theme}} {{music for the}} newscasts was a {{modified}} version of [...] "Also sprach Zarathustra" [...] (which would later be used on the channel's final newscast on 12 April 1992). During that time, the openings to all of the newscasts featured a rotating globe and a satellite, before showing the channel's logo and the newscast's title. When La Cinq's logo was changed in April 1991, the newscasts' openings were changed to a variant of the channel's ident with the <b>word</b> <b>Information</b> superimposed onto the channel's logo; this would be used until shortly before the channel's closure one year later. It was accompanied by a hard-hitting news music package.|$|E
5000|$|The ancient Greek {{word for}} {{information}} is πληροφορία, which transliterates (plērophoria) from πλήρης (plērēs) [...] "fully" [...] and φέρω (phorein) frequentative of (pherein) to carry-through. It literally means [...] "fully bears" [...] or [...] "conveys fully". In modern Greek language the word Πληροφορία {{is still in}} daily use and has the same meaning as the <b>word</b> <b>information</b> in English. In addition to its primary meaning, the word Πληροφορία as a symbol has deep roots in Aristotle's semiotic triangle. In this regard it can be interpreted to communicate information to the one decoding that specific type of sign. This is something that occurs frequently with the etymology of many words in ancient and modern Greek language {{where there is a}} very strong denotative relationship between the signifier, e.g. the word symbol that conveys a specific encoded interpretation, and the signified, e.g. a concept whose meaning the interpreter attempts to decode.|$|E
5000|$|Initial clickstream or click path data had to {{be gleaned}} from server log files. Because human and machine traffic were not differentiated, the study of human clicks took a {{substantial}} effort. Subsequently, Javascript technologies were developed which use a tracking cookie to generate a series of signals from browsers. In other <b>words,</b> <b>information</b> was only collected from [...] "real humans" [...] clicking on sites through browsers.It {{was not possible to}} identify the clickpath.|$|R
40|$|This paper {{presents}} the co-occurrence dictionary based on Thai phenomena. The theoretical background, the data structure, the dictionary development and <b>word</b> collocation <b>information</b> {{are described in}} details. At present, 75, 000 word collocations have been added in the co-occurrence dictionary {{with the help of}} linguists who made much effort in encoding the linguistic <b>information.</b> Hopefully, the <b>word</b> collocation <b>information</b> presented in this paper will be the useful resources for the natural language processing studies, and second language acquisition...|$|R
40|$|This {{paper offers}} a {{discussion}} of the types of heterogeneity that can occur among processes that share information. The author attempts to establish areas of similarity and difference between the airline, financial and manufacturing industries and the health-care industry. Ways by which information can be shared are discussed, given this heterogeneity, and some recommendations for system design are offered. Key <b>Words</b> <b>Information</b> transfer, semantics, medical informatics, virtual enterprise, medicalinformatics standards, ontology, distributed information systemsThe Nature of Heterogeneit...|$|R
5000|$|Thus, the Fisher {{information}} is the negative of {{the expectation of}} the second derivative {{with respect to the}} parameter α of the log likelihood function. Therefore, Fisher {{information is}} a measure of the curvature of the log likelihood function of α. A low curvature (and therefore high radius of curvature), flatter log likelihood function curve has low Fisher information; while a log likelihood function curve with large curvature (and therefore low radius of curvature) has high Fisher information. When the Fisher information matrix is computed at the evaluates of the parameters ("the observed Fisher information matrix") it is equivalent to the replacement of the true log likelihood surface by a Taylor's series approximation, taken as far as the quadratic terms. [...] The <b>word</b> <b>information,</b> in the context of Fisher information, refers to information about the parameters. Information such as: estimation, sufficiency and properties of variances of estimators. The Cramér-Rao bound states that the inverse of the Fisher information is a lower bound on the variance of any estimator of a parameter α: ...|$|E
50|$|In reading, {{information}} within 2° (approximately 6-8 characters) of {{the point}} of fixation is processed in foveal vision, while information up to 5° of visual angle benefits from parafoveal preview. Studies have shown that people {{can tell the difference}} in the letters of a word in the fovea and near-parafovea (the part of the parafovea closest to the fovea), but not in the outer edges of the parafovea. In languages that read from left to right, the word immediately {{to the right of the}} fixated word is known as the parafoveal <b>word.</b> <b>Information</b> present in the parafovea can interact with information present in the fovea. The benefit the parafoveal preview has is also mediated by how common the word in the parafovea is, with less common words providing less of a reduction in fixation duration when they reach foveal fixation. As the clarity of information in the parafovea is not as great as in the fovea, the SWIFT model of eye movements in reading, while allowing for parallel processing, accounts for this difference by assigning the parafoveal less processing power the further away it is from the foveal fixation.|$|E
5000|$|Prior to this paper, limited information-theoretic ideas {{had been}} {{developed}} at Bell Labs, all implicitly assuming events of equal probability. Harry Nyquist's 1924 paper, Certain Factors Affecting Telegraph Speed, contains a theoretical section quantifying [...] "intelligence" [...] and the [...] "line speed" [...] at which it can be transmitted by a communication system, giving the relation [...] (recalling Boltzmann's constant), where W is the speed of transmission of intelligence, m {{is the number of}} different voltage levels to choose from at each time step, and K is a constant. Ralph Hartley's 1928 paper, Transmission of Information, uses the <b>word</b> <b>information</b> as a measurable quantity, reflecting the receiver's ability to distinguish one sequence of symbols from any other, thus quantifying information as , where S was the number of possible symbols, and n the number of symbols in a transmission. The unit of information was therefore the decimal digit, much later renamed the hartley in his honour as a unit or scale or measure of information. Alan Turing in 1940 used similar ideas as part of the statistical analysis of the breaking of the German second world war Enigma ciphers.|$|E
40|$|EP 1041397 A UPAB: 20001106 NOVELTY - The {{method has}} a coded signal sampled and the sample values stored, with a decoded {{version of the}} coded signal {{compared}} with a given <b>information</b> <b>word,</b> for detection of the <b>information</b> <b>word</b> and estimated of its reception time point. The stored sample values are correlated with a reference signal incorporating the coded <b>information</b> <b>word,</b> for correction of the estimated reception time point via the correlation maximum. DETAILED DESCRIPTION - Also included are INDEPENDENT CLAIMS for the following: (a) a device for determining the received time point of an <b>information</b> <b>word</b> in a coded signal; (b) a device for determining the propagation time of a coded signal USE - The method is used for detecting the reception time point of an <b>information</b> <b>word</b> in a received coded signal, for measurement of a signal propagation time, e. g. for 3 -dimensional location of an earth satellite position. ADVANTAGE - The method eliminates errors caused by non-deterministic delays in the decoding method...|$|R
40|$|We {{propose a}} novel way of {{incorporating}} dependency parse and <b>word</b> co-occurrence <b>information</b> into a state-of-the-art web-scale n-gram model for spelling correction. The syntactic and distributional information provides extra evidence {{in addition to}} that provided by a web-scale n-gram corpus and especially helps with data sparsity problems. Experimental results show that introducing syntactic features into n-gram based models significantly reduces errors by up to 12. 4 % over the current state-of-the-art. The <b>word</b> co-occurrence <b>information</b> shows potential but only improves overall accuracy slightly. ...|$|R
40|$|This work {{implements}} {{a state-of-the-art}} term extraction technique C-value/NC-value method which combines linguistic and statistical information. C-value method {{is intended to}} enhance the common statistic measure of frequency of occurrence for term extraction. NC-value method incorporates the context <b>words</b> <b>information</b> into C-value method to improve the accuracy of extracted terms. We evaluated the performance of both methods on a computer science corpus and compared to that on a medical corpus, and confirmed that similar performance was achieved on both corpora. ...|$|R
5000|$|While the {{emotional}} Stroop {{test and the}} classic Stroop effect elicit similar behavioral outcomes (a slowing in response times to colored words), these tests engage different mechanisms of interference. The classic Stroop test creates a conflict between an incongruent color and word (the word [...] "RED" [...] in font color blue) but {{the emotional}} Stroop involves only emotional and neutral words—color does not affect slowing {{because it does not}} conflict with word meaning. In other words, studies show the same effects of slowing for emotional words relative to neutral even if all the words are black. Thus, the emotional Stroop does not involve an effect of conflict between a word meaning and a color of text, but rather appears to capture attention and slow response time due to the emotional relevance of the word for the individual. The emotional Stroop test has been used broadly in clinical studies using emotional words related to a particular individual's area of concern, such as alcohol-related words for someone who is alcoholic, or words involving a particular phobia for someone with anxiety or phobic disorders. Both the classic and the emotional Stroop tests, however, involve the need to suppress responses to distracting <b>word</b> <b>information,</b> while selectively maintaining attention on the color of the word to complete the task.|$|E
50|$|Cue-dependent forgetting (also, context-dependent forgetting) or {{retrieval}} failure, is {{the failure}} to recall a memory due to missing stimuli or cues that were present {{at the time the}} memory was encoded. Encoding {{is the first step in}} creating and remembering a memory. How well something has been encoded in the memory can be measured by completing specific tests of retrieval. Examples of these tests would be explicit ones like cued recall or implicit tests like word fragment completion. Cue-dependent forgetting is one of five cognitive psychology theories of forgetting. This theory states that a memory is sometimes temporarily forgotten purely because it cannot be retrieved, but the proper cue can bring it to mind. A good metaphor for this is searching for a book in a library without the reference number, title, author or even subject. The information still exists, but without these cues retrieval is unlikely. Furthermore, a good retrieval cue must be consistent with the original encoding of the information. If the sound of the word is emphasized during the encoding process, the cue that should be used should also put emphasis on the phonetic quality of the <b>word.</b> <b>Information</b> is available however, just not readily available without these cues. Depending on the age of a person, retrieval cues and skills may not work as well. This is usually common in older adults but that is not always the case. When information is encoded into the memory and retrieved with a technique called spaced retrieval, this helps older adults retrieve the events stored in the memory better. There is also evidence from different studies that show age related changes in memory. These specific studies have shown that episodic memory performance does in fact decline with age and have made known that older adults produce vivid rates of forgetting when two items are combined and not encoded.|$|E
40|$|We {{propose a}} Named Entity (NE) {{recognition}} method using rules acquired from unlabeled data. Rules are acquired from automatically labeled data with an NE recognizer. These rules {{are used to}} identify NEs, the beginning of NEs, {{or the end of}} NEs. The application results of rules are used as features for machine learning based NE recognizers. In addition, we use <b>word</b> <b>information</b> acquired from unlabeled data as in a previous work. The <b>word</b> <b>information</b> includes the candidate NE classes of each word, the candidate NE classes of co-occurring words of each word, and so on. We evaluate our method with IREX data set for Japanese NE recognition and unlabeled data consisting of more than one billion words. The experimental results show that our method using rules and <b>word</b> <b>information</b> achieves the best accuracy on the GENERAL and ARREST tasks of IREX. ...|$|E
5000|$|... "words" [...] are {{mentioned}} explicitly in {{the description of}} the problem, so it is reasonable for the designer to treat <b>words</b> as <b>information</b> packets (IPs) ...|$|R
40|$|Abstract. The {{main goal}} of current Web {{navigation}} languages is to retrieve set of nodes reachable from a given node. No information is provided about {{the fragments of}} the Web navigated to reach these nodes. In other <b>words,</b> <b>information</b> about their connections is lost. This paper presents an efficient algorithm to extract relevant parts of these Web fragments and shows the importance of producing subgraphs besides of sets of nodes. We discuss examples with real data using an implementation of the algorithm in the EXpRESs tool...|$|R
40|$|Eye {{movements}} were monitored in 4 experiments that explored {{the role of}} parafoveal word length in reading. The experiments employed a type of compound word where the deletion of a letter results in 2 short words (e. g., backhand, back and). The boundary technique (K. Rayner, 1975) was employed to manipulate <b>word</b> length <b>information</b> in the parafovea. Accuracy of the parafoveal word length preview significantly affected landing positions and fixation durations. This disruption was larger for 2 -word targets, but the results demonstrated that this interaction was not due to the morphological status of the target words. Manipulation of sentence context also demonstrated that parafoveal <b>word</b> length <b>information</b> {{can be used in}} combination with sentence context to narrow down lexical candidates. The 4 experiments converge in demonstrating that an important role of parafoveal <b>word</b> length <b>information</b> is to direct the eyes {{to the center of the}} parafoveal word...|$|R
