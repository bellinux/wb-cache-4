32|195|Public
50|$|ECM was {{developed}} as a traditional software application that companies implemented on their own corporate networks. In this scenario, each individual company manages and maintains both the ECM application, and the network storage devices that store the data. Many on-premises ECM systems are highly customized for individual organizational needs.Since paper document capture {{requires the use of}} physical scanning devices, like scanners or multi-function devices, it is typically performed on-premises. However, it can be outsourced to businesses that provide scanning services. Known as Service Bureaus, these companies complete high-volume scanning and indexing and return the electronic files to organizations via <b>Web</b> <b>transfer</b> or on CDs, DVDs, or other external storage devices.|$|E
40|$|The {{objective}} of the project is to provide fundamental knowledge and diagnostic tools needed to design new technologies that will allow ultra high speed <b>web</b> <b>transfer</b> from press rolls to dryer cylinders. A brief summary is given for the progress made {{on each of the}} following research tasks: Task 1 [...] identify composition of contaminants and topology of press and dryer roll surfaces at commercial mills; Task 2 [...] develop facilities to simulate contaminant deposition process under controlled experimental conditions; Task 3 [...] develop facilities to simulate <b>web</b> <b>transfer</b> from contaminant surfaces and measure work of adhesion; Task 4 [...] develop models to predict contaminant deposition and work of adhesion; Task 5 [...] develop and verify model to predict <b>web</b> <b>transfer</b> at ultra high paper machine speeds; and Task 6 [...] develop and demonstrate new roll surface conditioning technologies...|$|E
40|$|Using the {{existing}} internet protocol suite of HTTP over TCP, the user {{has little or}} no control over the progress of a <b>web</b> <b>transfer</b> he requested. We are developing a new protocol |WebTP | that includes the user in the transmission loop for a <b>web</b> <b>transfer.</b> WebTP optimizes the transport of a document from the sender to the receiver by taking into account factors like the contents of the document, the state of the network, the available resources at the client, and even the preferences of the user. We set up a model that captures the above information. Using the algorithms we developed, the receiver determines the optimal subset of objects to request and an optimal order of transport for these objects to maximize his utility. ...|$|E
5000|$|... {{delivers}} applications over a <b>web</b> Hypertext <b>Transfer</b> Protocol (HTTP) connection ...|$|R
40|$|Many applications, {{including}} <b>web</b> <b>transfers,</b> software distribution, video-on-demand, and peer-to-peer data downloads, {{require the}} retrieval of structured documents consisting of multiple components like images, video, and text. Large systems using these applications {{may be made}} more scalable by using efficient data distribution techniques like multicast, and by enabling clients to retrieve data from multiple servers in parallel...|$|R
40|$|Short <b>web</b> <b>transfers</b> that {{dominate}} the Internet suffer from TCP’s inadequate provisioning for short flows. This paper surveys ten proposals that attempt to solve {{one or more of}} the problems of short flows, and suggests general criteria to evaluate them. The proposals range from T/TCP in 1992 to RIO-PS in 2001. We classify the proposals into three categories: (1) those that reduce connection setup/teardown overheads, (2) those that use different network state sharing mechanisms, and (3) those that improve performance during slow start. ...|$|R
40|$|The {{objective}} of the project is to provide fundamental knowledge and diagnostic tools needed to design new technologies that will allow ultra high speed <b>web</b> <b>transfer</b> from press rolls and dryer cylinders. From a fundamental standpoint, we expect that roll surface performance depends on the composition of contaminants that deposit on those surfaces during use, {{as well as the}} materials and finishing techniques used in manufacturing these surfaces. We need to understand; the contamination process, the influence of contamination on work of adhesion, the roles of surface topology, film splitting, and process conditions on <b>web</b> <b>transfer.</b> The purpose of this quarterly report is to provide an overview of the project including key tasks, schedules for completion of tasks and to document accomplishments...|$|E
40|$|TCP over {{bandwidth}} asymmetric networks such as Cable TV, Asymmetric Digital Subscriber Loop (ADSL) and Wireless Networks exhibits different characteristics from TCP on symmetric links. A {{number of}} techniques {{have been proposed}} to address this problem. However previous research has been largely focused on bulk transfers. This paper investigates the effects of bandwidth asymmetry on Weblike short-lived transfers. A close-form prediction model is presented for TCP transfers over asymmetric links. The <b>Web</b> <b>transfer</b> model is then derived from it. Simulations based on ns- 2 show that the model can give predictions for TCP transfers {{with a high degree}} of accuracy...|$|E
30|$|A rate-constant based first-order multi-compartment {{model was}} used for {{predicting}} bioavailability, bioaccumulation and food <b>web</b> <b>transfer.</b> The combined results of modelling, lab- and field investigations revealed {{that a combination of}} modelling partitioning to different organic carbon phases and applying assessment methods to estimate freely dissolved (pore) water concentrations or exchangeable sediment concentrations seems to be the most cost-effective approach for underpinning management decisions. Despite the still considerable uncertainties in current methods to assess bioavailability, MODELKEY investigations clearly showed that retrospective risk assessment (of supposedly contaminated water bodies) shall be based on extractable concentrations and freely dissolved concentrations rather than on total water or total sediment concentrations. These methods can also be useful for investigative monitoring studies.|$|E
40|$|Improving the {{performance}} of data transfers in the Internet (such as <b>Web</b> <b>transfers)</b> requires a detailed understanding of when and how delays are introduced. Unfortunately, the complexity of data transfers like those using HTTP is great enough that identifying the precise causes of delays is dif- cult. In this paper we describe a method for pinpointing where delays are introduced into applications like HTTP by using critical path analysis. By constructing and proling the critical path, {{it is possible to}} determine what fraction of total transfer latency is due to packet propagation, network variation (e. g., queuing at routers or route uctuation), packet losses, and delays at the server and at the client. We have implemented our technique in a tool called ####### that automates critical path analysis for Web transactions. We show that our analysis method is robust enough to analyze traces taken for two dierent TCP implementations (Linux and FreeBSD). To demonstrate the utility of our approach, we present the results of critical path analysis for a set of Web transactions taken over 14 days under a variety of server and network conditions. The results show that critical path analysis can shed considerable light on the causes of delays in <b>Web</b> <b>transfers,</b> and can expose subtleties in the behavior of the entire end-to-end system. 1...|$|R
40|$|Abstract — Wireless {{broadband}} networks are an attractive way of connecting users to the Internet. The services offered by such networks typically are point-to-point services {{that do not}} utilize the broadcast capabilities of the wireless networks. Multicast Web Caching is a readily-available and deployed method that embraces the broadcast capabilities of wireless links and largely improves <b>web</b> <b>transfers.</b> In this paper, we analyze the efficiency of our Multicast Web Caching solution. The suitabulity is proven by trace-based simulation {{that focus on the}} reductions in connection latency as well as bandwith savings on both forward an return link. I...|$|R
50|$|While XUL serves {{primarily}} for constructing Mozilla applications and their extensions, {{it may also}} feature in <b>Web</b> applications <b>transferred</b> over HTTP. The Mozilla Amazon Browser, a former XUL application of this type, provided a rich interface for searching books at Amazon.com.|$|R
40|$|We propose an {{architecture}} and several mechanisms for providing "Best Effort Differentiated Services " (BEDS), {{a set of}} services similar to Best Effort in that the QoS provided depends on the network conditions, but differentiated in their tradeoff between packet delay and packet loss probability. This proposal starts from the observation that a single Best Effort service does not fit {{the needs of all}} types of elastic applications. In our proposal, the "loss-conservative" service has smaller packet loss probability but larger delay than the "delay-conservative" one. The former is suited for file transfer applications, whereas VoIP can significantly benefit from the delay-conservative service. We provide experimental results that confirm our models' predictions that file and <b>web</b> <b>transfer</b> and interactive voice applications can simultaneously benefit from this service. The experiments were conducted using networks of Nortel routers and switches implementing our proposed mechanism [...] ...|$|E
40|$|In this paper, {{we propose}} and study a new technique, {{which we call}} cache-based {{compaction}} for reducing the latency of Web browsing over a slow link. Our compaction technique trades computation for bandwidth. The key observation is that an object can be coded in a highly compact form for transfer if similar objects that have been transferred earlier {{can be used as}} references. The contributions of this paper are: (1) an efficient selection algorithm for selecting similar objects as references, and (2) an encoding /decoding algorithm that reduces the size of a Web object by exploiting its similarities with the reference objects. We verify the efficacy of our proposal through detailed experimental evaluations. Our compaction technique significantly generalizes previous work on optimizing <b>Web</b> <b>transfer</b> using compression or differencing, and provides a systematic foundation that ties together caching, compression and prefetching...|$|E
40|$|The Constrained Application Protocol (CoAP) is a {{specialized}} <b>Web</b> <b>transfer</b> protocol for resource-oriented applications intended {{to run on}} constrained devices, typically part of the Internet of Things. In this paper we leverage Information-Centric Networking (ICN), deployed within the domain of a network provider that interconnects, {{in addition to other}} terminals, CoAP endpoints in order to provide enhanced CoAP services. We present various CoAP-specific communication scenarios and discuss how ICN can provide benefits to both network providers and CoAP applications, even though the latter are not aware of the existence of ICN. In particular, the use of ICN results in smaller state management complexity at CoAP endpoints, simpler implementation at CoAP endpoints, and less communication overhead in the network. Comment: Proc. of the 8 th IFIP International Conference on New Technologies, Mobility and Security (NTMS), Larnaca, Cyprus, November, 201...|$|E
40|$|One of {{the most}} in uential {{achievements}} of the last sanctuary was the emer-gence of a decentralized, highly heterogenous, global communication network, the so called Internet. The current Internet supports various services of <b>web</b> browsing, <b>transferring</b> of les, interactive shell and multimedia applications...|$|R
5000|$|One popular {{trend is}} client/server computing. This is the {{principle}} that a client computer can provide certain capabilities for a user and request others from other computers that provide services for the clients. (The <b>Web's</b> Hypertext <b>Transfer</b> Protocol {{is an example of}} this idea.) ...|$|R
40|$|Many {{emerging}} smart {{applications and}} services employ Web technology, and users nowadays surf the Web from any device via {{any kind of}} access network. Typically, high page latencies trigger users to abort ongoing transfers, resulting in the abrupt terminations of the TCP connections. This paper presents a systematic study of the termination process of the TCP connections and identifies the reasons behind the observed sequences of termination flags. Monitoring and classification of the termination behavior of the TCP connections can provide indications about the user-perceived performance of <b>Web</b> <b>transfers.</b> From the results, {{it is observed that}} the TCP termination behavior is heavily-dependent on the client-side application. Therefore, a set of criteria is required to identify the abortions made by the user...|$|R
40|$|In the Internet of Things (IoT) {{research}} arena, many {{efforts are}} devoted to adapt the existing IP standards to emerging IoT nodes. This is the direction followed by three Internet Engineering Task Force (IETF) Working Groups, which {{paved the way for}} research on IP-based constrained networks. Through a simplification of the whole TCP/IP stack, resource constrained nodes become direct interlocutors of application level entities in every point of the network. In this paper we analyze some side effects of this solution, when in the presence of large amounts of data to transmit. In particular, we conduct a performance analysis of the Constrained Application Protocol (CoAP), a widely accepted <b>web</b> <b>transfer</b> protocol for the Internet of Things, and propose a service management enhancement that improves the exploitation of the network and node resources. This is specifically thought for constrained nodes in the abovementioned conditions and proves to be able to significantly improve the node energetic performance when in the presence of large resource representations (hence, large data transmissions) ...|$|E
40|$|Total mercury (THg) and {{methylmercury}} (MeHg) {{were recorded}} in the commercial demersal fish Lethrinus nebulosus, caught from six locations in Qatar EEZ (Exclusive Economic Zone). Concentrations of THg decreased in the order: liver ˃ muscle ˃ gonad. THg concentrations in fish tissue ranged from 0. 016 ppm in gonad to 0. 855 ppm (mg kg− 1 w/w) in liver tissues, while concentrations in muscle tissue ranged from 0. 24 to 0. 49 ppm (mg kg− 1 w/w) among sampling sites. MeHg concentrations were used to validate food <b>web</b> <b>transfer</b> rate calculations. Intake rates were calculated to assess the potential health impact of the fish consumption. There is no major threat to human health from the presence of Hg in L. nebulosus, based upon reasonable consumption patterns, limited {{to no more than}} three meals of L. nebulosus per week. Qatar National Research Fund (QNRF) under the National Priorities Research Program (NPRP) award number NPRP 09 - 505 - 1 - 08...|$|E
40|$|Multiple {{representation}} of spatial data, which means obtaining different detailed {{representation of}} geographic phenomena {{based on the}} same spatial database or different data versions, {{is one of the key}} technologies for digital earth. It plays an important role in such applications as seamless data navigation, progressive <b>web</b> <b>transfer</b> and self-adaptable visualization. Multiple representation technology meets the contradiction between data volume and high granularity representation. This paper aims at the objective by small size of data volume to realize high granularity representation, presenting a concept, namely representation lifespan over scale space to describe that the spatial data has different representation scene in scale space. The scale transformation is represented as t ij: , where f i is the transformation function, [si 1, si 2] the scale range controlled by two key scale points si 1 and si 2 and {g ij} the base representation status associated with the key scale point of si 1 or si 2. The study summaries that there are four transformations, namely generalization, LOD accumulation, morphing and extraction...|$|E
40|$|One of {{the major}} {{problems}} in Internet today is Network Congestion. TCP {{is used as a}} Transport Protocol for most of the applications on the Internet and is also responsible for adjusting to network congestion dynamically. The Active Queue Management algorithms (AQM) are used to reduce congestion, and in this paper two such AQM algorithms are considered. Here we look at the existing Explicit Congestion Notification (ECN) algorithm and a Queue Management Backward Congestion Control Algorithm (QMBCCA). A comparison is made between the two algorithms in terms of Fairness and Percentage Loss for short lived <b>web</b> <b>transfers.</b> We have found that there is a significant reduction in packet loss and improvement in fairness index when using QMBCCA when there were more number of TCP flows...|$|R
40|$|In recent years, various {{researchers}} {{have shown that}} network traffic that is due to world-wide <b>web</b> <b>transfers</b> shows characteristics of self-similarity {{and it has been}} argued that this can be explained by the heavy-tailedness of many of the involved distributions. Considering these facts, developing methods that are able to handle self-similarity and heavy-tailedness is of great importance for network capacity planning purposes. However, heavy-tailed distributions cannot be used so easily for analytical or numerical evaluation studies. To overcome this problem, in this paper, we approximate the empirical distributions by analytically more tractable, that is, hyper-exponential distributions. For that purpose, we present a new fitting algorithm based on the expectation-maximisation and show it to perform well both for pure traffic statistics as well as in queuing studies. ...|$|R
40|$|This paper {{addresses}} Web performance over satellite {{using the}} SPDY protocol. SPDY {{is a new}} application technology, introduced by Google, to accelerate <b>Web</b> <b>transfers</b> over common terrestrial links. Most of the SPDY techniques (i. e. header compression, pushing and multiplexing) have been usually included in satellite Performance Enhancing Proxies (PEPs) to optimize performance. Therefore, SPDY over satellite is expected to provide good performance without requiring any specific modification over the network. Proof of such an improvement is revolutionary {{for the role of}} satellite in the future Internet, since it could be considered as a transparent link, which does not need of ad-hoc protocol adaptations. Performance assessment relies on a satellite emulator that reproduces in software a DVB-RCS link while running real implementations of both TCP/IP stacks and SPDY...|$|R
40|$|This paper reviews current {{knowledge}} of biogeochemical cycles of pollutant organic chemicals in aquatic ecosystems {{with a focus}} on coastal ecosystems. There is a bias toward discussing chemical and geochemical aspects of biogeochemical cycles and an emphasis on hydrophobic organic compounds such as polynuclear aromatic hydrocarbons, polychlorinated biphenyls, and chlorinated organic compounds used as pesticides. The complexity of mixtures of pollutant organic compounds, their various modes of entering ecosystems, and their physical chemical forms are discussed. Important factors that influence bioavailability and disposition (e. g., organism-water partitioning, uptake via food, food <b>web</b> <b>transfer)</b> are reviewed. These factors include solubilities of chemicals; partitioning of chemicals between solid surfaces, colloids, and soluble phases; variables rates of sorption, desorption; and physiological status of organism. It appears that more emphasis on considering food as a source of uptake and bioaccumulation is important in benthic and epibenthic ecosystems when sediment-associated pollutants are a significant source of input to an aquatic ecosystem. Progress with mathematical models for exposure and uptake of contaminant chemicals is discussed briefly...|$|E
40|$|The {{progressive}} {{transmission of}} map data over World Wide Web provides the users with a self-adaptive strategy to access remote data. It not only speeds up the <b>web</b> <b>transfer</b> but also offers an efficient navigation guide in information acquisition. The key technology in this transmission is the efficient multiple representation of spatial data and pre-organization on server site. This paper aims at offering {{a new model}} for the multiple representations of vector data, called changes accumulation model, which considers the spatial representation from one scale to another as an accumulation of the set of changes. The difference between two consecutive representations is recorded in a linear order and through gradually addition or subtraction of “change patches ” the progressive transmission is realized. As an example, the progressive transmission of area features based on this model is investigated in the project. The model is built upon the hierarchical decomposition of polygon into series of convex hulls or bounding rectangles and the progressive transmission is accomplished through component of the decomposed elements...|$|E
40|$|The {{networking}} {{community has}} made a significant investment in GMPLS networks, which are connection-oriented networks that support dynamic call-by-call bandwidth sharing. Currently, GMPLS switches are call blocking and GMPLS control-plane protocols only support immediate requests for bandwidth. This thesis first addresses the question of suitability for different types of applications for GMPLS networks. Using the Erlang-B formula, we reason that GMPLS net-works are well suited for applications in which the required per-circuit bandwidth is {{on the order of}} one-hundredth the shared link capacity. Then, we propose two applications for the GMPLS network, CHEETAH, which we have de-ployed as part of an NSF-sponsored project. The first is a <b>web</b> <b>transfer</b> application, for which we design and implement a software package called WebFT. We integrate the CHEETAH end-host software modules into WebFT to provide deterministic data-transfer services transparently to users. The CHEETAH network provides connection-oriented services in addition to the connectionless service offered by the Internet. This “add-on ” design allows the WebFT package to provide normal web access to non–CHEETAH clients through the Internet while simultaneously serving CHEE...|$|E
5000|$|Model-view-controller: an HTTP- and servlet-based {{framework}} providing hooks for {{extension and}} customization for web applications and RESTful (representational state <b>transfer)</b> <b>Web</b> services.|$|R
40|$|AbstractRapid {{development}} of telecommunication technologies and the ever growing network users demands have made network congestion a prominent problem in today's Internet. Congestion brings significant performance degradation {{to the network}} the Quality-of-Service (QoS). The Active Queue Management algorithms (AQM) are used to reduce congestion. In this paper, two AQM algorithms Extended Queue Management Backward Congestion Control Algorithm (EQMBCCA) and QMBCCA, which we have proposed earlier are taken in to consideration. QMBCCA makes use of ISQ (Internet Control Messaging Protocol (ICMP) Source Quench) signaling mechanism and also the CE (Congestion Experienced) bit whenever the average queue size crosses minimum threshold value. Both of them are redundantly used even for mild congestions. EQMBCCA introduces a configurable intermediate threshold value IntThres between the minimum and maximum values and generates ISQ signals only if the congestion crosses this threshold value. Therefore the generation of ISQ messages is significantly reduced here. There is also reliability since CE bits are also set in the packets once congestion occurs. The two algorithms are compared in terms of ISQ traffic, HTTP packet loss, fairness and average <b>web</b> object <b>transfer</b> delay for short lived <b>web</b> <b>transfers.</b> We {{have found that the}} performance of EQMBCCA is almost equal to that of QMBCCA and there is a significant reduction in the ISQ traffic in the reverse direction...|$|R
50|$|Currently, {{logging on}} the RS Music Group <b>Web</b> site is <b>transferred</b> to Def Jam Recordings, {{confirming}} the {{rumors that the}} label has been sold.|$|R
40|$|The Constrained Application Protocol (CoAP) is a {{specialized}} <b>web</b> <b>transfer</b> protocol {{for use with}} constrained nodes and constrained (e. g., low-power, lossy) networks. The nodes often have 8 -bit microcontrollers with small amounts of ROM and RAM, while constrained networks such as IPv 6 over Low-Power Wireless Personal Area Networks (6 LoWPANs) often have high packet error rates and a typical throughput of 10 s of kbit/s. The protocol is designed for machine-to-machine (M 2 M) applications such as smart energy and building automation. CoAP provides a request/response interaction model between application endpoints, supports built-in discovery of services and resources, and includes key concepts of the Web such as URIs and Internet media types. CoAP is designed to easily interface with HTTP for integration with the Web while meeting specialized requirements such as multicast support, very low overhead, and simplicity for constrained environments. Status of This Memo This is an Internet Standards Track document. This document {{is a product of}} the Internet Engineering Task Force (IETF). It represents the consensus of the IETF community. It has received public review and has been approved for publication by th...|$|E
40|$|This paper {{presents}} a silver-nanoparticle-based, screen-printed, high-performance, dual-band, bandstop filter (DBBSF) on a flexible polyethylene terephthalate (PET) substrate. Using screen-printing techniques to process a highly viscous silver printing ink, high-conductivity printed lines were implemented at a <b>web</b> <b>transfer</b> speed of 5 [*]m/min. Characterized by X-ray diffraction (XRD), optical microscopy, {{atomic force microscopy}} (AFM), and scanning electron microscopy (SEM), the printed lines were shown to be characterized by smooth surfaces with a {{root mean square roughness}} of 7. 986 [*]nm; a significantly higher thickness (12. 2 [*]μm) than the skin depth; and a high conductivity of 2 × 107 [*]S/m. These excellent printed line characteristics enabled the implementation of a high-selectivity DBBSF using shunt-connected uniform impedance resonators (UIRs). Additionally, the inductive loading effect of T-shaped stubs on the UIRs, which were analyzed using S-parameters based on lumped parameter calculations, was used to improve the return losses of the geometrically optimized DBBSF. The measured minimum return loss and maximum insertion loss of 28. 26 and 1. 58 [*]dB, respectively, at the central frequencies of 2. 56 and 5. 29 [*]GHz of a protocol screen-printed DBBSF demonstrated the excellent performance of the material and its significant potential for use in future cost-effective, flexible WiMax and WLAN applications...|$|E
30|$|Both IETF {{groups have}} {{realized}} the interconnectivity between tiny objects {{and the current}} internet in a standardized way. However, this connectivity is merely an enabler required to unlock all potential of the IoT {{in the form of}} novel applications and services. Web service technology made the success of the current internet. Now, it is expected that an embedded counterpart of web service technology is needed in order to exploit all great opportunities offered by the internet of things, since existing application layer protocols, such as hypertext transfer protocol (HTTP), SOAP, and XML are even heavier than the protocols defined in layers below. Therefore, the constrained RESTful environments (CoRE) working group was established to specifically work on the standardization of a framework for resource-oriented applications, allowing the realization of RESTful embedded web services in a similar way as traditional web services [4]. Their work resulted in the Constrained Application Protocol (CoAP), a specialized RESTful <b>web</b> <b>transfer</b> protocol for use with constrained networks and nodes. It uses the same RESTful principles as HTTP, but it is much lighter {{so that it can be}} run on constrained devices [4]. In addition, the group designed observe functionality in order to allow a device to publish a value or event to another device that has subscribed to be notified of changes in the resource representation [5].|$|E
40|$|We propose {{extending}} the traditional Best-Effort service with several Qualify-of-Service classes named "Best-Effort Differentiated Services (BEDS) ". Each class {{has a different}} policy for degrading the quality of packet forwarding during congestion. For example, one class may constrain the packet queuing delay (delay-conservative), while another class may constrain the packet drop probability (drop-conservative). Applications can choose among such new flavors of Best Effort according to their needs. It is expected that time-sensitive but drop-tolerant applications such as voice over IP would benefit most when using a delay-conservative class, while file and <b>web</b> <b>transfers</b> may benefit from drop-conservative class. BEDS does not require admission control and does not provide hard QoS guarantees. Therefore, BEDS is most useful to elastic applications that currently use Best-Effort. An important benefit of BEDS consists in providing a set of enhanced services without the important overhead [...] ...|$|R
40|$|Internet traffic {{primarily}} {{consists of}} packets from elastic flows, i. e. <b>Web</b> <b>transfers,</b> file transfers (FTP), and e-mail, whose transfers are mediated via the Transmission Control Protocol. We develop a conditional sampling technique to analyze throughput correlations among elastic flow classes based on flow level measurements from current network traffic monitoring tools. The primary contributions {{of this paper}} are: (1) a demonstration of throughput correlation among temporally overlapping flows on congested resources by using analytical/simulation models, and (2) application of a multivariate statistical method (principal components) to infer network properties, such {{as the number of}} shared resources by flows in the network from non-intrusive, flow level measurements collected at a single site. Our proposal for using flow level measurements to infer network properties differs significantly from previous network tomography research that has employed end-to-end packet level measurements for making inferences...|$|R
40|$|In this paper, we have {{presented}} a {{statistical analysis of}} through-puts of <b>Web</b> <b>transfers.</b> This analysis was performed using a large packet-level trace of traffic at the Web site for the Atlanta Summer Olympic Games. Observed mean throughputs for these transfers vary widely {{as a function of}} end-host location and time of day, confirming that a large amount of heterogeneity exists in the Inter-net. Our analysis showed that despite these wide variations the transfer throughput exhibited significant temporal and spatial sta-bility. In particular, we found that: 1. Throughputs to several individual internet hosts can be mod-eled using a log-normal distribution regardless of location. More precisely, we found that for over 50 % of the hosts in our study, the log-normal hypothesis could not be rejected at any level of significance, and that it characterized observed throughputs better than the other analytic distributions w...|$|R
