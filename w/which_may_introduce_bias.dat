18|10000|Public
5000|$|In statistics, {{imputation}} is {{the process}} of replacing missing data with substituted values. When substituting for a data point, it is known as [...] "unit imputation"; when substituting for a component of a data point, it is known as [...] "item imputation". There are three main problems that missing data causes: missing data can introduce a substantial amount of bias, make the handling and analysis of the data more arduous, and create reductions in efficiency. Because missing data can create problems for analyzing data, imputation is seen as a way to avoid pitfalls involved with listwise deletion of cases that have missing values. That is to say, when one or more values are missing for a case, most statistical packages default to discarding any case that has a missing value, <b>which</b> <b>may</b> <b>introduce</b> <b>bias</b> or affect the representativeness of the results. Imputation preserves all cases by replacing missing data with an estimated value based on other available information. Once all missing values have been imputed, the data set can then be analysed using standard techniques for complete data. Imputation theory is constantly developing and thus requires consistent attention to new information regarding the subject. There have been many theories embraced by scientists to account for missing data but the majority of them introduce large amounts of bias. A few of the well known attempts to deal with missing data include: hot deck and cold deck imputation; listwise and pairwise deletion; mean imputation; regression imputation; last observation carried forward; stochastic imputation; and multiple imputation.|$|E
30|$|As {{observed}} {{from the}} above, a few gait databases {{can be used}} for the evaluation of gait-based age estimation, and are inadequate {{in terms of the number}} of subjects, in addition to gender and age balances, when compared with existing face databases (e.g., MORPH database [53]), <b>which</b> <b>may</b> <b>introduce</b> <b>bias</b> into the evaluation results [45]. Therefore, one of the important contributions of this study is to construct a significantly large-scale gait database with a wide age variation and good balance of genders and age groups to allow the performance evaluation of gait-based age estimation in a more statistically reliable manner.|$|E
30|$|As in any research, {{there are}} some {{limitations}} that {{should be taken into}} account in the interpretation of these findings. First, the environment of this study may limit its findings; future research may be able to offer new findings in new and different contexts. Second, the instruments used to measure the variables limit the results of this study, though the validity and reliability of the instruments have been verified and they are tenable. However, in future studies, deeper research may lead to the adoption of new standards and represent a new approach to this topic. Finally, all data was based on self-reports, <b>which</b> <b>may</b> <b>introduce</b> <b>bias.</b>|$|E
40|$|Abstract. Prior {{work has}} shown that a robot which uses polite-ness {{modifiers}} in its speech is perceived more favorably by human interactants, as compared to a robot using more direct instructions. However, the findings to-date have been based soley on data aquired from the standard university pool, <b>which</b> <b>may</b> <b>introduce</b> <b>biases</b> into the results. Moreover, the work {{does not take into}} account the po-tential modulatory effects of a person’s age and gender, despite the influence these factors exert on perceptions of both natural language interactions and social robots. Via a set of two experimental studies, the present work thus explores how prior findings translate, given a more diverse subject population recruited via Amazon’s Mechani-cal Turk. The results indicate that previous implications regarding a robot’s politeness hold even with the broader sampling. Further, they reveal several gender-based effects that warrant further attention. ...|$|R
50|$|A {{number of}} medical and life science {{journals}} require authors submitting papers on reviews, meta-analyses and systematic reviews to include a QUOROM flow chart. QUOROM is the acronym used for the Quality of Reporting of Meta-analyses standards developed by QUOROM group. This is a flow chart that graphically describes the sequence of steps defined for exclusion of studies under review. The rationale behind the inclusion of such a diagram {{is to increase the}} transparency of decisions made by the researcher for including or excluding certain studies, <b>which</b> <b>may</b> subsequently <b>introduce</b> <b>biases</b> in the overall measure of effect.|$|R
40|$|Abstract The aim of {{this work}} is to develop an {{empirical}} expression for diurnal mean martian surface pressure {{in support of the}} landing of Mars Science Laboratory. We eval-uate the consistency of surface pressure measurements from four landers, Viking Lander 1, Viking Lander 2, Mars Pathfinder, and Phoenix, and one radio occultation experiment, Mars Global Surveyor. With the exception of Mars Pathfinder, whose measurements are 0. 1 mbar smaller than expected, all are consistent. We assume that the diurnal mean surface pres-sure is a separable function of altitude and season, neglecting dependences on time of day, latitude, and longitude, and use the Viking Lander 1 dataset to characterize the sea-sonal dependence as a harmonic function of season with annual and semi-annual periods. We characterize the exponential dependence of surface pressure on altitude using Mars Global Surveyor radio occultation measurements widely-distributed below + 1 km altitude and within 45 degrees of the equator. These measurements have local times of 3 – 5 hours, <b>which</b> <b>may</b> <b>introduce</b> <b>biases</b> into our estimates for diurnal mean surface pressure. Our em-pirical expression for diurnal mean surface pressure, pdm, is p 0,V L 1 exp(−(z − z 0,V L 1) /H 0) (1 + s 1,V L 1 sin(1 Ls) + c 1,V L 1 cos(1 Ls) + s 2,V L 1 sin(2 Ls) + c 2,V L 1 cos(2 Ls)) where z is al...|$|R
40|$|In the Humanities and Social Sciences (HSS), the {{monograph}} is {{an important}} means of communicating scientific results. As {{in the field of}} STM, the value of scholarly output needs to be assessed. This is done by bibliometric measures and qualitative methods. Bibliometric measures based on articles do not function well in the field of HSS, where monographs are the norm. The qualitative methods which take into account several stakeholders are labour intensive and the results are dependent on self-assessment of the respondents, <b>which</b> <b>may</b> <b>introduce</b> <b>bias.</b> In the case of humanities, the picture becomes even less clear due to uncertainties about the stakeholders. This dataset consists of over 25, 000 downloads by more than 1, 500 providers, spread over 859 monographs...|$|E
40|$|In this {{discussion}} paper multiple points of view, and effects {{and sources of}} bias in qualitative research results in organizational information systems are explored. A generic qualitative research process is traced starting from {{the ways in which}} researchers obtain permission to gather data in organizations. Multiple points of view in the research process, all of <b>which</b> <b>may</b> <b>introduce</b> <b>bias</b> into research results, are considered next. These include researcher/s, research institution, funding body, client organization, and individual participants. Finally bias effects from these multiple points of view are considered at analysis, interpretation and reporting of research results for organizational information systems. The position taken in this paper is that there is inevitable bias in qualitative research that may have many effects. The appropriateness of a `scientific' approach to bias in qualitative research in organizational information systems is also considered...|$|E
30|$|Although this cohort shows {{heterogeneity}} regarding some aspects, it {{is representative}} for the clinical practice in hospitals that treat large numbers of patients with prostate cancer. As an example of heterogeneity this cohort includes 38 patients that received other imaging studies for prostate cancer staging prior to 18 F-DCFPyL PET/CT, <b>which</b> <b>may</b> <b>introduce</b> <b>bias</b> in the cohort. Most of those prior studies had equivocal or negative results for metastases, especially for bone metastases. The pre-test likelihood of presence of metastases in these patients is probably lower than in patients without any prior imaging study, which {{is supported by the}} fact that bone metastases were found in 18.5 % of patients that had prior imaging procedures and in 29.2 % of patients without prior imaging. Therefore, {{it is likely that the}} detection rates of 18 F-DCFPyL PET/CT in a non-biased cohort will be at least as high as found in the present cohort.|$|E
40|$|Abridged) We {{present a}} {{systematic}} {{study of the}} X-ray and multiwavelength properties of a sample of 17 highly radio-loud quasars (HRLQs) at z > 4 with sensitive X-ray coverage from new Chandra and archival Chandra, XMM-Newton, and Swift observations. Eight of the new and archival observations are reported in this work for the first time. New Chandra observations of two moderately radio-loud and highly optically luminous quasars at z > 4 are also reported. Our HRLQ sample represents the top ~ 5 % of radio-loud quasars in terms of radio loudness. We found that our HRLQs have an X-ray emission enhancement over HRLQs at lower redshifts (by a typical factor of ~ 3), and this effect, after controlling for several factors <b>which</b> <b>may</b> <b>introduce</b> <b>biases,</b> has been solidly estimated to be significant at the 3 - 4 sigma level. HRLQs at z= 3 - 4 are also {{found to have a}} similar X-ray emission enhancement over z < 3 HRLQs, which supports further the robustness of our results. We discuss models for the X-ray enhancement's origin including a fractional contribution from inverse Compton scattering of cosmic microwave background photons. No strong correlations are found between the relative X-ray brightness and optical/UV emission-line rest-frame equivalent widths (REWs) for radio-loud quasars. However, the line REWs are positively correlated with radio loudness, which suggests that relativistic jets make a negligible contribution to the optical/UV continua of these HRLQs (contrary to the case where the emission lines are diluted by the relativistically boosted continuum). Our HRLQs are generally consistent with the known anti-correlation between radio loudness and X-ray power-law photon index. We also found that the two moderately radio-loud quasars appear to have the hardest X-ray spectra among our objects, suggesting that intrinsic X-ray absorption (N_H~ 10 ^ 23 cm^- 2) may be present. Comment: ApJ in press; 9 figures, 13 table...|$|R
30|$|Interpreting our {{findings}} should be contextualized {{in view of}} its strengths and limitations. Limitations of our study include the following: (i) we did not examine gender effects due to insufficiency of data. Nevertheless, Posthumus et al. [30] examined gender differences of the CC genotype among ACLR participants; (ii) A 1 and A 2 alleles for BstUI were not examined separately, but instead, we combined them <b>which</b> <b>may</b> have sacrificed precision of outcomes, given the reported contrasting effects of these two alleles in the literature [26, 28]; (iii) linkage disequilibrium was reported for the COL 5 A 1 polymorphisms in the component studies [25, 31, 32], <b>which</b> <b>may</b> have <b>introduced</b> <b>bias</b> [37] by masking identity of the true causal variant; and (iv) the low number of studies (n =[*] 2 – 3) and underpowered status (58 % and 44 %) warrant caution in interpreting the significant effects of rs 71746744 and rs 16399.|$|R
30|$|It is {{sometimes}} convenient {{to represent a}} mobile call network by an undirected network, arguing that communication during a single phone call goes both ways, and set {{the weight of the}} link as the sum of the weights from both directions. However, who initiates the call might be important in other contexts than the passing of information, depending on the aim of the research, and Kovanen et al. have showed that reciprocal calls are often strongly imbalanced [17]. In the interacting pair, one user is often initiating most of the calls, so how can this be represented in an undirected network by a representative link weight? In a closely related question, most CDRs contain both information on voice calls and text messages, but so far {{it is not clear how}} to incorporate both pieces of information into one simple measure. Moreover, there seems to be a generational difference in the use of text messages or preference between texts and voice calls <b>which</b> <b>may</b> <b>introduce</b> a <b>bias</b> in measures that only take one type of communication into account [18].|$|R
40|$|Microarray {{has become}} a popular {{biotechnology}} in biological and medical research. However, systematic and stochastic variabilities in microarray data are expected and unavoidable, resulting in the problem that the raw measurements have in-herent “noise ” within microarray experiments. Currently, logarithmic ratios are usually analyzed by various clustering methods directly, <b>which</b> <b>may</b> <b>introduce</b> <b>bias</b> interpretation in identifying groups of genes or samples. In this paper, a statistical method based on mixed model approaches was proposed for microarray data clus-ter analysis. The underlying rationale of this method is to partition the observed total gene expression level into various variations caused by different factors using an ANOVA model, and to predict the differential effects of GV (gene by variety) interaction using the adjusted unbiased prediction (AUP) method. The predicted GV interaction effects can then {{be used as the}} inputs of cluster analysis. We illus-trated the application of our method with a gene expression dataset and elucidated the utility of our approach using an external validation. Key words: gene expression, clustering analysis, predicting GV interaction effect...|$|E
40|$|Conventional {{longitudinal}} data analysis methods assume that outcomes {{are independent of}} the data-collection schedule. However, the independence assumption may be violated, for example, when adverse events trigger additional physician visits in between prescheduled follow-ups. Observation times may therefore be associated with outcome values, <b>which</b> <b>may</b> <b>introduce</b> <b>bias</b> when estimating the eect of covariates on outcomes using standard longitudinal regression methods. Existing semi-parametric methods that accommodate outcome-dependent observation times are limited {{to the analysis of}} continuous outcomes. We develop new methods for the analysis of binary outcomes, while retaining the exibility of semi-parametric models. Our methods are based on counting process approaches, rather than relying on possibly intractable likelihood-based or pseudo-likelihood-based approaches, and provide marginal, population-level inference. In simulations, we evaluate the statistical properties of our proposed methods. Comparisons are made to 2 ̆ 7 naive 2 ̆ 7 GEE approaches that either do not account for outcome-dependent observation times or incorporate weights based on the observation-time process. We illustrate the utility of our proposed methods using data from a randomized controlled trial of interventions designed to improve adherence to warfarin therapy. We show that our method performs well in the presence of outcome-dependent observation times, and provide identical inference to 2 ̆ 7 naive 2 ̆ 7 approaches when observation times are not associated with outcomes...|$|E
40|$|Self-reported diet {{is prone}} to {{measurement}} error. Analytical models of diet may include several foods or nutrients to avoid confounding. Such multivariate models of diet may be affected by errors correlated among the dietary covariates, <b>which</b> <b>may</b> <b>introduce</b> <b>bias</b> of unpredictable direction and magnitude. The authors used 1993 – 1998 data from the European Prospective Investigation into Cancer and Nutrition in Norfolk, United Kingdom, to explore univariate and multivariate regression models relating nutrient intake estimated from a 7 -day diet record or a food frequency questionnaire to plasma levels of vitamin C. The purpose was to provide an empirical examination {{of the effect of}} two different multivariate error structures in the assessment of dietary intake on multivariate regression models, in a situation where the underlying relation between the independent and dependent variables is approximately known. Emphasis was put on the control for confounding and the effect of different methods of controlling for estimated energy intake. The results for standard multivariate regression models were consistent with considerable correlated error, introducing spurious associations between some nutrients and the dependent variable and leading to instability of the parameter estimates if energy was included in the model. Energy adjustment using regression residuals or energy density models led to improved parameter stability. collinearity; confounding; diet; linear regression; measurement error; multivariate analysis; nutrition assessmen...|$|E
40|$|Abstract Background Retaining {{participants}} in cohort studies with multiple follow-up waves is difficult. Commonly, researchers {{are faced with}} the problem of missing data, <b>which</b> <b>may</b> <b>introduce</b> <b>biased</b> results as well as a loss of statistical power and precision. The STROBE guidelines von Elm et al. (Lancet, 370 : 1453 - 1457, 2007); Vandenbroucke et al. (PLoS Med, 4 :e 297, 2007) and the guidelines proposed by Sterne et al. (BMJ, 338 :b 2393, 2009) recommend that cohort studies report on the amount of missing data, the reasons for non-participation and non-response, and the method used to handle missing data in the analyses. We have conducted a review of publications from cohort studies in order to document the reporting of missing data for exposure measures and to describe the statistical methods used to account for the missing data. Methods A systematic search of English language papers published from January 2000 to December 2009 was carried out in PubMed. Prospective cohort studies with a sample size greater than 1, 000 that analysed data using repeated measures of exposure were included. Results Among the 82 papers meeting the inclusion criteria, only 35 (43 %) reported the amount of missing data according to the suggested guidelines. Sixty-eight papers (83 %) described how they dealt with missing data in the analysis. Most of the papers excluded participants with missing data and performed a complete-case analysis (n[*]=[*] 54, 66 %). Other papers used more sophisticated methods including multiple imputation (n[*]=[*] 5) or fully Bayesian modeling (n[*]=[*] 1). Methods known to produce biased results were also used, for example, Last Observation Carried Forward (n[*]=[*] 7), the missing indicator method (n[*]=[*] 1), and mean value substitution (n[*]=[*] 3). For the remaining 14 papers, the method used to handle missing data in the analysis was not stated. Conclusions This review highlights the inconsistent reporting of missing data in cohort studies and the continuing use of inappropriate methods to handle missing data in the analysis. Epidemiological journals should invoke the STROBE guidelines as a framework for authors so that the amount of missing data and how this was accounted for in the analysis is transparent in the reporting of cohort studies. </p...|$|R
40|$|Background: Retaining {{participants}} in cohort studies with multiple follow-up waves is difficult. Commonly, researchers {{are faced with}} the problem of missing data, <b>which</b> <b>may</b> <b>introduce</b> <b>biased</b> results as well as a loss of statistical power and precision. The STROBE guidelines von Elm et al. (Lancet, 370 : 1453 - 1457, 2007); Vandenbroucke et al. (PLoS Med, 4 :e 297, 2007) and the guidelines proposed by Sterne et al. (BMJ, 338 :b 2393, 2009) recommend that cohort studies report on the amount of missing data, the reasons for non-participation and non-response, and the method used to handle missing data in the analyses. We have conducted a review of publications from cohort studies in order to document the reporting of missing data for exposure measures and to describe the statistical methods used to account for the missing data. Methods: A systematic search of English language papers published from January 2000 to December 2009 was carried out in PubMed. Prospective cohort studies with a sample size greater than 1, 000 that analysed data using repeated measures of exposure were included. Results: Among the 82 papers meeting the inclusion criteria, only 35 (43 %) reported the amount of missing data according to the suggested guidelines. Sixty-eight papers (83 %) described how they dealt with missing data in the analysis. Most of the papers excluded participants with missing data and performed a complete-case analysis (n = 54, 66 %). Other papers used more sophisticated methods including multiple imputation (n = 5) or fully Bayesian modeling (n = 1). Methods known to produce biased results were also used, for example, Last Observation Carried Forward (n = 7), the missing indicator method (n = 1), and mean value substitution (n = 3). For the remaining 14 papers, the method used to handle missing data in the analysis was not stated. Conclusions: This review highlights the inconsistent reporting of missing data in cohort studies and the continuing use of inappropriate methods to handle missing data in the analysis. Epidemiological journals should invoke the STROBE guidelines as a framework for authors so that the amount of missing data and how this was accounted for in the analysis is transparent in the reporting of cohort studies. © 2012 Karahalios et al.; licensee BioMed Central Ltd...|$|R
40|$|Previously it {{has been}} proved that the {{geometry}} of a multi-pinhole SPECT system with circular orbit can be uniquely determined from a measurement of two point sources, without {{the knowledge of the}} distance between them. In this paper, we report that this conclusion only holds if the motion of the camera is perfectly circular. In reality, the detector heads systematically slightly deviate from the circular orbit, <b>which</b> <b>may</b> <b>introduce</b> non-negligible <b>bias</b> in the calibrated parameters. An analytical linear model was extended to estimate the influence of both data noise and systematic deviations on the accuracy of the calibration and the image quality of the reconstruction. It turns out that applying the knowledge of the distances greatly reduces the reconstruction error, especially in the presence of systematic deviations. In addition, we propose that instead of using the information about the distances between the point sources, it is more straightforward to use the knowledge about the distances between the pinhole apertures during multi-pinhole calibration. The two distance-fixing approaches yield similar calibration accuracy, but fixing the inter-pinhole distances is more preferable since it facilitates simultaneous animal-calibration data acquisition. Our theoretical results are supported by reconstruction images of a Jaszczak-type phantom. status: accepte...|$|R
40|$|Abstract Background Data on {{survival}} endpoints {{are usually}} summarised using either hazard ratio, cumulative number of events, or median survival statistics. Network meta-analysis, {{an extension of}} traditional pairwise meta-analysis, is typically based on a single statistic. In this case, studies which do not report the chosen statistic are excluded from the analysis <b>which</b> <b>may</b> <b>introduce</b> <b>bias.</b> Methods In this paper we present a tutorial illustrating how network meta-analyses of survival endpoints can combine count and hazard ratio statistics in a single analysis on the hazard ratio scale. We also describe methods for accounting for the correlations in relative treatment effects (such as hazard ratios) that arise in trials with more than two arms. Combination of count and hazard ratio data in a single analysis is achieved by estimating the cumulative hazard for each trial arm reporting count data. Correlation in relative treatment effects in multi-arm trials is preserved by converting the relative treatment effect estimates (the hazard ratios) to arm-specific outcomes (hazards). Results A worked example of an analysis of mortality data in {{chronic obstructive pulmonary disease}} (COPD) is used to illustrate the methods. The data set and WinBUGS code for fixed and random effects models are provided. Conclusions By incorporating all data presentations in a single analysis, we avoid the potential selection bias associated with conducting an analysis for a single statistic and the potential difficulties of interpretation, misleading results and loss of available treatment comparisons associated with conducting separate analyses for different summary statistics. </p...|$|E
40|$|Participant {{recruitment}} is {{a difficult}} and time consuming aspect of clinical trials, often resulting in delays and budget overruns. Having reached recruitment targets the next challenge is participant retention. Some weight-loss studies have attrition rates around 60 % <b>which</b> <b>may</b> <b>introduce</b> <b>bias</b> in the results. It {{may be possible to}} reduce attrition rates if known predictors can be found but to date few studies produced consistent results. The aim of this exploratory study was to determine whether start date could be a predictor of attrition for participants involved in the SMART weight loss clinical trial (ACTRN 12608000425392). Recruitment for the trial occurred as two cohorts at different times of the year. The relationship between attrition and cohort was determined using a Pearson chi square analysis. At three months 31 % of Cohort 1 had withdrawn compared with 18 % of Cohort 2. Cohort 1 participants, (enrolled July to November) {{were significantly more likely to}} withdraw within 6 months than Cohort 1 (January to March) (P= 0. 049). In each cohort most withdrawals occurred within the first three months. For Cohort 1, this coincided with the period leading up to Christmas and summer holidays. Changes in routines and increased food temptations at this time of year are known challenges of weight management and this may explain the significant difference between cohort withdrawal rates. The period around Easter and autumn holidays did not seem to have the same effect. Adjusting start dates and developing strategies to minimise the effect of seasonal holidays may help reduce attrition rates. ...|$|E
40|$|Conventional {{longitudinal}} data analysis methods typically assume that outcomes {{are independent of}} the data-collection schedule. However, the independence assumption may be violated when an event triggers outcome assessment in between prescheduled follow-up visits. For example, patients initiating warfarin therapy who experience poor anticoagulation control may have extra physician visits to monitor the impact of necessary dose changes. Observation times may therefore be associated with outcome values, <b>which</b> <b>may</b> <b>introduce</b> <b>bias</b> when estimating the effect of covariates on outcomes using standard longitudinal regression methods. We consider a joint model approach with two components: a semi-parametric regression model for longitudinal outcomes and a recurrent event model for observation times. The semi-parametric model includes a parametric specification for covariate effects, but allows the effect of time to be unspecified. We formulate a framework of outcome-observation dependence mechanisms to describe conditional independence between the outcome and observation-time processes given observed covariates or shared latent variables. ^ We generalize existing methods for continuous outcomes by accommodating any combination of mechanisms {{through the use of}} observation-level weights and/or patient-level latent variables. We develop new methods for binary outcomes, while retaining the flexibility of a semi-parametric approach. We extend these methods to account for discontinuous risk intervals in which patients enter and leave the at-risk set multiple times during the study. Our methods are based on counting process approaches, rather than relying on possibly intractable likelihood-based or pseudo-likelihood-based approaches, and provide marginal, population-level inference. In simulations, we evaluate the statistical properties of our proposed methods. Comparisons are made to 2 ̆ 7 naïve 2 ̆ 7 approaches that do not account for outcome-dependent observation times. We illustrate the utility of our proposed methods using data from a randomized trial of interventions designed to improve adherence to warfarin therapy and a randomized trial of malaria vaccines among children in Mali. ...|$|E
40|$|In the subrenal capsule assay, the immuno-competent mice used have a host immune {{response}} to the graft <b>which</b> <b>may</b> <b>introduce</b> a <b>bias</b> in the determination of tumour size. A study was therefore conducted to verify {{the quality of the}} correlation that could exist between 'macroscopical' and 'microscopical' evaluation. A histological study of 12 different tumours treated with cis-Pt, L-PAM, DTIC, vindesine, BCNU or TGU demonstrated that a cellular host {{immune response}} was observed in 11 out of the 12 control groups and was uneven in the treated groups. The 'microscopical' and 'macroscopical' evaluation of tumor-take were 100 and 95 % respectively. Although there was a fair correlation between the 'microscopic' and 'macroscopic' parameters, tumor variations in absence of host reaction - which reflect pure chemosensitivity of the treated tissue - could be measured in 50 % of the cases. It was possible to determine a base-line for rejection of the test results when the control group showed a decrease in mean tumor size exceeding 20 % of its initial size on day 0. If the decrease is less than 20 %, then the histological evaluation appears to be of considerable additional value. SCOPUS: ar. jFLWNAinfo:eu-repo/semantics/publishe...|$|R
40|$|THE {{relatively}} recent introduction of statistical methods {{in the analysis}} of problems concerned with the inherit-ance of quantitative traits has made the literature on the subject somewhat fore-boding to those unfamiliar with the tech-niques involved. If practical applications of these investigations are to be made, some effort must be spent in bridging the gap between the theoretical investigators {{and the people who are}} in a position to make use of their results. It is the intent of this paper to present in detail a method of estimating herit-ability which is applicable to many of the heritability problems confronting poultry geneticists. Lerner (1950) has indicated in general the procedure to be used {{in the analysis of}} variance components, but a considerable knowledge of statistics is required to carry through the analysis correctly. The analysis would be quite simple, if the subclass numbers were equal, but this usually is not the case, so the more difficult unequal subclass numbers case must be considered. Many previous users of the variance component method have left out of their analyses those dams having less than a certain minimum num-ber of full sib progeny, a procedure <b>which</b> <b>may</b> <b>introduce</b> some <b>bias</b> to the results. The procedure to be described does not suffer from this fault...|$|R
40|$|The {{geometry}} {{of a single}} pinhole SPECT system with circular orbit can be uniquely determined from a measurement of three point sources, provided {{that at least two}} inter-point distances are known. In contrast, it has been shown mathematically that, for a multi-pinhole SPECT system with circular orbit, only two point sources are needed, and the knowledge of the distance between them is not required. In this paper, we report that this conclusion only holds if the motion of the camera is perfectly circular. In reality, the detector heads systematically slightly deviate from the circular orbit, <b>which</b> <b>may</b> <b>introduce</b> non-negligible <b>bias</b> in the estimated parameters and degrade the reconstructed image. An analytical linear model was extended to estimate the influence of both data noise and systematic deviations on the accuracy of the calibration and on the image quality of the reconstruction. It turns out that applying the knowledge of the distances greatly reduces the reconstruction error, especially in the presence of systematic deviations. In addition, we propose that instead of using the information about the distances between the point sources, it is more straightforward to use the knowledge about the distances between the pinhole apertures during multi-pinhole calibration. The two distance-fixing approaches yield similar reconstruction accuracy. Our theoretical results are supported by reconstruction images of a Jaszczak-type phantom scan. status: publishe...|$|R
40|$|Musculoskeletal {{disorders}} are very common and affects the individual by pain and functional impairment, {{and the society}} through work disability and healthcare utilisation. To what extent is less studied. Routinely collected healthcare registers is a potential resource for epidemiological studies of musculoskeletal disorders. Skåne region, as opposed to nationally in Sweden, has healthcare registers, covering all care including primary care. he overall aim of this thesis was to incorporate healthcare registers in the epidemiological research of consultation prevalence, healthcare consultation and sick leave patterns in patients with musculoskeletal disorders. he thesis comprises four studies in which the Skåne Healthcare Register was linked with national registers on sick leave, prescribed drugs, and socioeconomic status. Additionally, the thesis include a comparative study between data from the Skåne Healthcare Register with that of an United Kingdom (UK) consultation database. he main variables under study are in Paper I–III disease, consultations, and sick leave and in Paper IV education, income, and work status. he consultation prevalence of low back pain was estimated to 3 – 4 % in the Skåne region while the figure was larger in the UK. People having low back pain had increased levels of healthcare consultations in general, and more pain diagnoses in particular. People diagnosed with whiplash associated neck injury had higher healthcare consultation rates already three years before the neck injury and the postinjury consultation level {{was associated with the}} preinjury consultation levels. Low socioeconomic status was associated with being diagnosed with chronic pain. It was feasible to use routinely collected databases in the studies of the burden of disease from musculoskeletal disorders, thus potentially also for other public health disorders. While there are vast potentials with register data, {{it is also important to}} bear in mind limitations due to e. g., missing data and misclassification <b>which</b> <b>may</b> <b>introduce</b> <b>bias.</b> My main findings indicate a need for early interventions after initial pain and neck injury to prevent the pain to becoming chronic. Stratification-based management is especially suggested...|$|E
40|$|Scholars {{within the}} Process school assume that {{cognitive}} and motivational aspects within {{the decision and}} integration processes may systematically effect the outcome of mergers and acquisitions. More specifically, {{it has been suggested}} that decision makers use heuristics for simplifying information processing <b>which</b> <b>may</b> <b>introduce</b> <b>bias.</b> The main aim of the present thesis is to add to the understanding of how offers and counteroffers, and information about a fair price affects the process and outcome of price negotiations in mergers and acquisitions. The results from five prestudies and one case study showed that norms of fairness may influence takeover negotiations. The head negotiatiors emphasized that they did not tried to buy (sell) to lowest (highest) possible price, but at a juste price. However, it was not possible to disentangle if the parties had an interest in being ”perfectly” fair, or if they had a more ”egoistic” interpretation of fairness. In order to further investigate whether or not fairness matters in corporate takeovers, simulation experiments was conducted with MBA students. By letting subjects rate their degree of satisfaction with the offered selling prices, it was possible to establish a ”social utility function” which specifies the level of satisfaction with outcomes to oneself and others. The results clearly showed that buyers’ social utility function was affected by information of a fair price. However, it seems like that they were more interested in that them selves were not being treated unfair (steeper decreasing social utility for buying at a price higher than a fair price) than treating others unfairly (flatter increasing social utility for buying at a price lower than a fair price). The behavioral predictions was tested in two additional simulated takeover negotiations. These results clearly showed that the negotiation process and outcome were affected by information about a fair price. More specifically, selling prices were lower when the subjects had received information about a low fair price, and higher when they had information about a high fair price. In addition, as if subjects anchored there counteroffer on the opponents initial offer and insufficiently adjusted from it, buyers first counteroffers were higher and sellers’ first counteroffer lower when the opponent started the negotiation. Furthermore, was the negotiated outcome lower when the buyer started the negotiation...|$|E
30|$|Several {{authors have}} applied {{footprint}} analysis {{in an effort}} to correct for non-homogenous environments, and horizontal advection, for example due to daily upslope or downslope winds, <b>which</b> <b>may</b> <b>introduce</b> <b>bias</b> into the eddy-covariance measurements (Baldocchi 2008). For example, Ehman et al. (2002) used footprints to calculate an estimate of sensor bias given using a vegetation index, and this gave confidence that the sensor measurements were representative of the inventory measurements. Gielen et al. (2013) used a more complex footprint model applied to the processing of NEP data, were fluxes originating outside of the area where inventory measurements were sampled were removed from the NEP estimates. In this study, footprint weighting by ecological attributes and footprint probability density was evaluated where it facilitated comparison between inventory-based measurements of ecosystem ΔC stocks and tower ΣNEP by accounting for fine scale spatial variability in forest structure. The effect of K-MSN classification and footprint weighting had an effect on the estimated site-level values; however, this effect was smaller than differences due to measurement methodology. Notably, applying footprint weighting to the estimation of CBM-CFS 3 ΣNEP at the regenerating clearcut increased convergence with EC tower ΣNEP; however, footprint weighting did not have the same effect for ΔC 2. The largest differences observed among methods were seen at the regenerating clearcut, where the inventory approach indicated the site was a much lower source than the ΣNEP methods. A possible cause is that the inventory methods did not properly account for C losses from decomposition of coarse roots from stumps and large woody debris at recently harvested sites, which can be a substantial source of respiration (Janisch et al. 2005). Therefore, if an estimate of stump coarse root decomposition were included in ΔCD 2 it may have increased convergence with the ΣNEP estimates. Additionally, the original sample locations were designed to capture the range of conditions in near proximity of the tower (based on interpretation of aerial photographs), and the relationship was similar when scaled up to the site level. Future studies may seek to derive accurate footprint estimates prior to establishing sample plots to ensure they are representative of the broader site conditions. Another more costly alternative is to establish a much greater number of plots in a systematic basis around the tower, to ensure the spatial variation within the tower footprint is adequately captured.|$|E
40|$|Purpose – The {{purpose of}} this paper is to show the {{applicability}} of data envelopment analysis (DEA) in arriving at an unbiased account of relative performance in a set of companies, using the pharmaceutical industry as an example. Design/methodology/approach – A DEA-based income efficiency measure of business performance for the pharmaceutical industry is computed. The pharmaceutical industry, which includes many multinational corporations with complex governance problems, and the strategies that allowed firm efficiency rankings to change over time, over ten recent years, are analyzed. Findings – The analyses indicate that the inclines and declines in DEA efficiency rankings are related to the strategic choices made by the upper management. Research limitations/implications – The paper attempted to trace firm behavior post hoc to validate the DEA rankings. All relevant firm behavior may not have been captured; the paper only attempted to capture behavior reported in the respectable business press, <b>which</b> <b>may</b> <b>introduce</b> a <b>bias.</b> Practical implications – The approach may be ideal to evaluate strategic managers (CEOs, general managers, and presidents) by board of directors, since it relates multiple performance indices to a meta-measure of performance. Another group of beneficiaries include sector financial analysts. The approach adds a new dimension to sector analysis, to compare specific industries and identify the relative rankings of firms on multiple performance indices. Originality/value – The paper demonstrates the usefulness of DEA in performance governance measurement by applying it to the pharmaceuticals industry. Corporate governance, Data analysis, Manangement accountability, Performance measures, Pharmaceuticals industry...|$|R
40|$|Summary Forest {{transpiration}} {{estimates are}} frequently based on xylem sap flux measurements {{in the outer}} sections of the hydro-active stem sapwood. We used Granier’s constant-heating technique with heating probes at various xylem depths to analyze radial patterns of sap flux density in the sapwood of seven broad-leaved tree species differing in wood density and xylem structure. Study aims were to (1) compare radial sap flux density profiles between diffuse- and ring-porous trees and (2) analyze the relationship between hydro-active sapwood area and stem diameter. In all investigated species except the dif-fuse-porous beech (Fagus sylvatica L.) and ring-porous ash (Fraxinus excelsior L.), sap flux density peaked {{at a depth of}} 1 to 4 cm beneath the cambium, revealing a hump-shaped curve with species-specific slopes. Beech and ash reached maximum sap flux densities immediately beneath the cambium in the youngest annual growth rings. Experiments with dyes showed that the hydro-active sapwood occupied 70 to 90 % of the stem cross-sectional area in mature trees of diffuse-porous species, whereas it occupied only about 21 % in ring-porous ash. Dendrochronological analyses indicated that vessels in the older sapwood may remain functional for 100 years or more in diffuse-porous species and for up to 27 years in ring-porous ash. We conclude that radial sap flux density patterns are largely dependent on tree species, <b>which</b> <b>may</b> <b>introduce</b> serious <b>bias</b> in sap-flux-derived forest transpiration estimates, if non-specific sap flux profiles are assumed...|$|R
40|$|Accurate {{earthquake}} {{locations are}} of primary importance when studying the seismicity {{of a given}} area, they allow important inferences on the ongoing seismo-tectonics. Both, for standard, {{as well as for}} earthquake relative location techniques, the velocity parameters are kept fixed to a-priori values, that are assumed to be correct, and the observed traveltime residuals are minimised by adjusting the hypocentral parameters. However, the use of an unsuitable velocity model, can introduce systematic errors in the hypocentre location. Precise hypocentre locations and error estimate, therefore, require the simultaneous solution of both velocity and hypocentral parameters. We perform a simultaneous inversion of both the velocity structure and the hypocentre location in NE-Sicily and SW-Calabria (Italy). Since the density of the network is not sufficient for the identification of the 3 D structure with a resolution of interest here, we restrict ourselves to a 1 D inversion using the well-known code VELEST. A main goal of the paper is the analysis of the stability of the inverted model parameters. For this purpose we carry out a series of tests concerning the initial guesses of the velocity structure and locations used in the inversion. We further assess the uncertainties which originate from the finiteness of the available datasets carrying out resampling experiments. From these tests we conclude that the data catalogue is sufficient to constrain the inversion. We note that the uncertainties of the inverted velocities increases with depth. On the other hand the inverted velocity structure depends decisively on the initial guess as they tend to maintain the overall shape of the starting model. In order to derive an improved starting model we derive a guess for the probable depth of the MOHO. For this purpose we exploit considerations of the depth distribution of earthquake foci and of the shear strength of rock depending on its rheological behaviour at depth. In a second step we derived a smooth starting model and repeated the inversion. Strong discontinuities tend to attract hypocentre locations <b>which</b> <b>may</b> <b>introduce</b> <b>biases</b> to the earthquake location. Using the smooth starting model we obtained again a rather smooth model as final solution which gave the best travel-time residuals among all models discussed in this paper. This poses severe questions as to the significance of velocity discontinuities inferred from rather vague a-priori information. Besides this, the use of those smooth models widely avoids the problems of hypocentre locations being affected by sudden velocity jumps, an effect which can be extremely disturbing in relative location procedures. The differences of the velocity structure obtained with different starting models is larger than those encountered during the bootstrap test. This underscores the importance of the choice of the initial guess. Fortunately the effects of the uncertainties discussed here on the final locations turned out as limited, i. e., less than 1 km for the horizontal coordinates and less than 2 km for the depth...|$|R
40|$|International audienceThe {{study of}} mass and energy {{transfer}} across landscapes has recently evolved to comprehensive considerations acknowledging {{the role of}} biota and humans as geomorphic agents, {{as well as the}} importance of small-scale landscape features. A contributing and supporting factor to this evolution is the emergence {{over the last two decades}} of technologies able to acquire high resolution topography (HRT) (meter and sub-meter resolution) data. Landscape features can now be captured at an appropriately fine spatial resolution at which surface processes operate; this has revolutionized the way we study Earth-surface processes. The wealth of information contained in HRT also presents considerable challenges. For example, selection of the most appropriate type of HRT data for a given application is not trivial. No definitive approach exists for identifying and filtering erroneous or unwanted data, yet inappropriate filtering can create artifacts or eliminate/distort critical features. Estimates of errors and uncertainty are often poorly defined and typically fail to represent the spatial heterogeneity of the dataset, <b>which</b> <b>may</b> <b>introduce</b> <b>bias</b> or error for many analyses. For ease of use, gridded products are typically preferred rather than the more information-rich point cloud representations. Thus many users take advantage of only a fraction of the available data, which has furthermore been subjected to a series of operations often not known or investigated by the user. Lastly, standard HRT analysis work-flows are yet to be established for many popular HRT operations, which has contributed to the limited use of point cloud data. In this review, we identify key research questions relevant to the Earth-surface processes community within the theme of mass and energy transfer across landscapes and offer guidance on how to identify the most appropriate topographic data type for the analysis of interest. We describe the operations commonly performed from raw data to raster products and we identify key considerations and suggest appropriate work-flows for each, pointing to useful resources and available tools. Future research directions should stimulate further development of tools that take advantage of the wealth of information contained in the HRT data and address the present and upcoming research needs such as the ability to filter out unwanted data, compute spatially variable estimates of uncertainty and perform multi-scale analyses. While we focus primarily on HRT applications for mass and energy transfer, we envision this review to be relevant beyond the Earth-surface processes community for a much broader range of applications involving the analysis of HRT...|$|E
40|$|AIM: To {{investigate}} {{the association between}} hepatitis C infection and type 2 diabetes mellitus. METHODS: Observational studies assessing the relationship between hepatitis C infection and type 2 diabetes mellitus were identified via electronic and hand searches. Studies published between 1988 to March 2011 were screened, according to the inclusion criteria set for the present analysis. Authors performed separate analyses for the comparisons between hepatitis C virus (HCV) infected and not infected, and HCV infected and hepatitis B virus infected. The included studies were further subgrouped {{according to the study}} design. Heterogenity was assessed using I 2 statistics. The summary odds ratios with their corresponding 95 % CIs were calculated based on a random-effects model. The included studies were subgrouped according to the study design. To assess any factor that could potentially affect the outcome, results were further stratified by age group (proportion of &#x 2265; 40 years), gender (proportion of male gender), body mass index (BMI) (proportion of BMI &#x 2265; 27), and family history of diabetes (i. e., self reported). For stability of results, a sensitivity analysis was conducted including only prospective studies. RESULTS: Combining the electronic database and hand searches, a total of 35 observational studies (in 31 articles) were identified for the final analysis. Based on random-effects model, 17 studies (n = 286 &#x 2005; 084) compared hepatitis C-infected patients with those who were uninfected [summary odds ratio (OR) : 1. 68, 95 % CI: 1. 15 - 2. 45]. Of these 17 studies, 7 were both a cross-sectional design (41. 2 %) and cohort design (41. 2 %), while 3 were case-control studies (17. 6 %). Nineteen studies (n = 51 &#x 2005; 156) compared hepatitis C-infected participants with hepatitis B-infected (summary OR: 1. 92, 95 % CI: 1. 41 - 2. 62). Of these 19 studies, 4 (21. 1 %), 6 (31. 6 %) and 9 (47. 4 %) were cross-sectional, cohort and case-control studies, respectively. A sensitivity analysis with 3 prospective studies indicated that hepatitis C-infected patients had a higher risk of developing type 2 diabetes compared with uninfected controls (summary odds ratio: 1. 41, 95 % CI: 1. 17 - 1. 7; I 2 = 0 %). Among hepatitis C-infected patients, male patients (OR: 1. 26, 95 % CI: 1. 03 - 1. 54) with age over 40 years (summary OR: 7. 39, 95 % CI: 3. 82 - 9. 38) had an increased frequency of type 2 diabetes. Some caution must be taken in the interpretation of these results because there may be unmeasured confounding factors <b>which</b> <b>may</b> <b>introduce</b> <b>bias.</b> CONCLUSION: The findings support the association between hepatitis C infection and type 2 diabetes mellitus. The direction of association remains to be determined, however. Prospective studies with adequate sample sizes are recommended...|$|E
40|$|Due to {{instrument}} sensitivities and algorithm detection limits, Level 2 (L 2) Cloud-Aerosol Lidar with Orthogonal Polarization (CALIOP) 532  nm aerosol extinction profile retrievals are often populated with retrieval fill values (RFVs), which indicate {{the absence of}} detectable levels of aerosol within the profile. In this study, using four years (2007 – 2008 and 2010 – 2011) of CALIOP Version 3 L 2 aerosol data, the occurrence frequency of daytime CALIOP profiles containing all RFVs (all-RFV profiles) is studied. In the CALIOP data products, the aerosol optical thickness (AOT) of any all-RFV profile is reported as being zero, <b>which</b> <b>may</b> <b>introduce</b> a <b>bias</b> in CALIOP-based AOT climatologies. For this study, we derive revised estimates of AOT for all-RFV profiles using collocated Moderate Resolution Imaging Spectroradiometer (MODIS) Dark Target (DT) and, where available, Aerosol Robotic Network (AERONET) data. Globally, all-RFV profiles comprise roughly 71  % of all daytime CALIOP L 2 aerosol profiles (i. e., including completely attenuated profiles), accounting for nearly half (45  %) of all daytime cloud-free L 2 aerosol profiles. The mean collocated MODIS DT (AERONET) 550  nm AOT {{is found to be}} near 0. 06 (0. 08) for CALIOP all-RFV profiles. We further estimate a global mean aerosol extinction profile, a so-called noise floor, for CALIOP all-RFV profiles. The global mean CALIOP AOT is then recomputed by replacing RFV values with the derived noise floor values for both all-RFV and non-all-RFV profiles. This process yields an improvement in the agreement of CALIOP and MODIS over-ocean AOT...|$|R
40|$|Background: Little {{information}} is available to inform clinical assessments on risk of self-harm repetition in ethnic minority groups. Methods: In a prospective cohort study, using data collected from six hospitals in England for self-harm presentations occurring between 2000 and 2007, we investigated risk factors for repeat self-harm in South Asian and Black people in comparison to Whites. Results: During the study period, 751 South Asian, 468 Black and 15, 705 White people presented with self-harm in the study centres. Repeat self-harm occurred in 4379 individuals, which included 229 suicides (with eight of these fatalities being in the ethnic minority groups). The risk ratios for repetition in the South Asian and Black groups compared to the White group were 0. 6, 95 % CI 0. 5 - 0. 7 and 0. 7, 95 % CI 0. 5 - 0. 8, respectively. Risk factors for repetition were similar across all three groups, although excess risk versus Whites was seen in Black people presenting with mental health symptoms, and South Asian people reporting alcohol use and not having a partner. Additional modelling of repeat self-harm count data showed that alcohol misuse was especially strongly linked with multiple repetitions in both BME groups. Limitations: Ethnicity was not recorded in a third of cases <b>which</b> <b>may</b> <b>introduce</b> selection <b>bias.</b> Differences <b>may</b> exist due to cultural diversity within the broad ethnic groups. Conclusion: Known social and psychological features that infer risk were present in South Asian and Black people who repeated self-harm. Clinical assessment in these ethnic groups should ensure recognition and treatment of mental illness and alcohol misuse. © 2012 Elsevier B. V...|$|R
40|$|Background Controlling bias {{is key to}} {{successful}} randomized controlled trials for behaviour change. Bias can be generated at multiple points during a study, for example, when participants are allocated to different groups. Several methods of allocations exist to randomly distribute participants over the groups such that their prognostic factors (e. g., socio-demographic variables) are similar, {{in an effort to}} keep participants’ outcomes comparable at baseline. Since it is challenging to create such groups when all prognostic factors are taken together, these factors are often balanced in isolation or only the ones deemed most relevant are balanced. However, the complex interactions among prognostic factors may lead to a poor estimate of behaviour, causing unbalanced groups at baseline, <b>which</b> <b>may</b> <b>introduce</b> accidental <b>bias.</b> Methods We present a novel computational approach for allocating participants to different groups. Our approach automatically uses participants’ experiences to model (the interactions among) their prognostic factors and infer how their behaviour is expected to change under a given intervention. Participants are then allocated based on their inferred behaviour rather than on selected prognostic factors. Results In order to assess the potential of our approach, we collected two datasets regarding the behaviour of participants (n =  430 and n =  187). The potential of the approach on larger sample sizes was examined using synthetic data. All three datasets highlighted that our approach could lead to groups with similar expected behavioural changes. Conclusions The computational approach proposed here can complement existing statistical approaches when behaviours involve numerous complex relationships, and quantitative data is not readily available to model these relationships. The software implementing our approach and commonly used alternatives is provided at no charge to assist practitioners in the design of their own studies and to compare participants 2 ̆ 7 allocations...|$|R
40|$|Investment is {{the most}} {{volatile}} component of aggregate demand and it is often considered central to business cycles fluctuations. The responsiveness of business investment to changes in its price is thus crucial {{to our understanding of}} economic activity. In spite of the key role played by the user cost of capital in economic analysis, there is little empirical support for the existence of substantial user cost elasticity. However, most of the evidence to date is based on aggregate user cost data, <b>which</b> <b>may</b> have <b>introduced</b> downward <b>biases</b> in the estimated user cost. This paper contributes to the literature by constructing a disaggregated, industry-specific micro user cost variable and focusing on a special class of firms - the high-tech firms. To provide a benchmark for the results, the user cost estimates for the high-tech sector are compared with those {{for the rest of the}} manufacturing sector. The results suggest that there is little response of investment to variations in its user cost. The findings also suggest that high-tech firms' investment behavior is not, after all, that different from the rest of the manufacturing sector. business investment, user cost of capital, high-tech investment,...|$|R
