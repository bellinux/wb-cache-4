5|23|Public
40|$|Abstract: Forecasts of the 8 January Denmark Windstorm are compared. In a <b>wrong</b> <b>forecast,</b> the Greenland-lee low is far to shallow, {{there is}} less outflow of cold air from west of Greenland and {{consequently}} a poor development of the upper trough that fed the windstorm. The analysis of the forecasts and an ETKF analysis support that a correct analysis of {{the atmosphere in the}} region between Iceland and Greenland would have been of importance to get a correct forecast of the windstorm over Denmark 3 days later...|$|E
40|$|The paper {{tests for}} the {{presence}} of heteroscedasticity in income, food importation and exportation in Nigeria using white heteroscedasticity and Arch Model as analytical procedure. This is done to create awareness, deepen knowledge and understanding the danger involved in given <b>wrong</b> <b>forecast</b> or prediction that may lead to greater problems in the economy. Once this is adhered to by researchers, most research conclusions will become realistic. The result of the white heteroscedasticity reveals the presence of heteroscedasticity in the analysis because the value of the critical chi- square is less than the computed or observed value of the R- squared obtained. This decision is dictated by the rule of thumb. The same situation also holds with the testing of the heteroscedasticity and specification bias. However, the error is corrected by the Arch model in which the result of the conditional heteroscedasticity reveals that the process generating the disturbance is weak. The paper, therefore, recommends that researchers should make their analysis to be heteroscedastic consistent so that their forecast or predictions will become more realistic in Nigeria...|$|E
40|$|As {{someone who}} has {{traumatic}} experiences, Lieutenant Tatsuya deals {{with a lot of}} negatives effects which occur as the result of his traumatic experiences. Therefore, the writer wants to reveal Lieutenant Tatsuya?s traumatic experiences in the past and the ways his traumatic experiences affect his life and to reveal his efforts in coping with his trauma. The writer uses literary approach which consists of theory of characterization, conflict and symbol in order to analyze Lieutenant Tatsuya. The writer also uses the concept of traumatic event which talks about Post Traumatic Stress Disorder to analyze Lieutenant Tatsuya?s traumatic experiences, their effects, and his efforts to cope with his trauma. His first trauma is the death of his parents. The second is the earthquake which crushes his hometown. The third is his trauma toward a seaquake forecaster who makes a <b>wrong</b> <b>forecast.</b> The last is his trauma toward money. Moreover, the writer finds that his trauma causes negative effects such as intrusive imagery, avoiding stimuli, and increased arousal in his present life. It creates problem and conflicts between him and other characters. Furthermore, the writer finds that Lieutenant Tatsuya tries to confront with his trauma and finally, he is able to cope with his trauma...|$|E
5000|$|... c) Marketing and Sickness : This {{is another}} part which always affects {{the health of}} any sector as well as SSI. This {{including}} <b>wrong</b> demand <b>forecasting,</b> selection of inappropriate product mix, absence of product planning, wrong market research methods, and bad sales promotions.|$|R
5000|$|... "It {{is unusual}} {{for most of}} the detail to be {{completely}} correct, but equally it is rare for nearly everything to be <b>wrong</b> ... Some <b>forecasts</b> are clearly very good, and a few are very poor, but the majority fall in the gray area in between, where an optimistic assessor would find merit, but a critical assessor would find fault." ...|$|R
6000|$|... "Sure it isn't me as is judge. It's the widdy. She {{says to me}} this mornin', says she, `The'll be a stiff breeze afore night, Teddy,' an' I nivver {{found the}} widdy <b>wrong</b> in her <b>forecasts</b> o' the weather." ...|$|R
40|$|Capital market {{forecasts}} sometimes {{suffer from}} a too intensive orientation towards the current market situation and lose their future-orientated character. This phenomenon {{can be referred to}} as Topical Orientated Trend Adjustment (GOVA). For an individual statistical proof we develop a new instrument called GOVA coefficient. In the second step we combine the GOVA coefficient with the conventional forecast quality measure of Theil's inequality coefficient. The result is the Forecast Quality Matrix. With this instrument, it is possible to verify whether the examined forecast is better or worse than the naive forecast and to exclude Topical Orientated Trend Adjustment. This results in four different possibilities: 1. The forecast considering the future is an ideal case. This forecast is significantly better than a naive forecast and is not influenced by the market situation during the making of the forecast. 2. The quasi-naive forecast is characterized ba a Topical Orientated Trend Adjustment, but is not better than the naive forecast. 3. The <b>wrong</b> <b>forecast</b> does not reflect the past, but unfortunately not even the future. 4. The direction-orientated forecast follows the past, but mostly indicates the correct market trend. This classification facilitates the evaluation of the practicability of forecasts and points out the reasons for a possibly lacking forecast accuracy. Forecast quality measures, topical orientated trend adjustment, forecast quality matrix, GOVA-coefficient, quasi-naive forecast...|$|E
40|$|Problem of this {{research}} is started that there is over deviation sales budget (10 %). The purpose of {{this research}} is deciding of acceptable sales forecast method {{which is going to be}} applied at PT Huma Indah Mekar and to arrange sales budget for year 2009. This research methodology is comparative and assosiative with the variable such as sales forecast (dependent variable) and sales budget (independent variable). This research use secondary data taken from marketing data and selling budget for the lateks 5 years (2004 - 2008). Base on that data, it is made to predict market sell uses least square, method moment, medium average and curve and also is counted by standard deviation of <b>wrong</b> <b>forecast</b> from each method. The result of hypothesis used forecast on marketing year 2009 with moment method and least square to produce think lateks with Y= 4. 455. 114, 8 + 362. 745, 7 (X) and Y = 5. 180. 606, 2 + 362. 745 (X) is Y 2009 = 6. 268. 843 kg with SKP 488. 759 the method quadrate linier line. Y= 4. 658. 237, 8 + 362. 745, 7 (X) + 261. 184, 2 (X) 2 is Y 2009 = 8. 097. 132, 7 kg with SKP 218. 809, 84. based on that forecast cause, curve method is more appropriate in apply marketing budget because it has smaller SKP. Selling forecast of rubber smoke sheet Y = 1. 100. 394 - 203. 015, 8 (X) and Y= 694. 362, 4 - 203. 015, 8 (X) is Y 2009 = 85. 315 kg with SKP 218. 166 while the curve linear line Y= 830. 350, 4 – 203. 015, 8 (X) – 67. 994 (X) 2 is Y 2009 = - 390. 643 kg with SKP 186. 149, 25. The arrangement before using marketing forecast, there is over budget deviation more than 10 % up to 72, 07 %, and on the contrary after using this method there is less 10 % deviation 0, 6 %- 8, 35 %. From this statement, it can be concluded that it is not appropriate in determining sales budget which is applied by company...|$|E
40|$|Demand {{variation}} {{is a key}} issue in processed food industry, because demands for processed food products vary daily. The organizations in this situation face challenges to meet customer demand. Their products have a definite shelf life and prone to be obsolete. Obsolete products are totally wastes. So, there exists a producer risk. This study has been conducted {{with the aim of}} identifying the root causes of demand variation and its impact on sales volume. For this purpose an exploratory study involving two food processing organizations and their forty eight points of sales had been performed. Each food item has different causes and consequences for demand variation. In this regard, three food items having limited shelf life had been selected to find out the causes of their demand variation. The study identified eleven causes and twelve consequences such as special occasion, duration of shelf life, <b>wrong</b> <b>forecasting</b> and so on. Then some root causes are figured out that dominates over others. The impacts of these causes on sales volume are also shown with six months demand data. The research concludes with the level of impact of the significant causes like price, occasion. Lastly some recommendations are mentioned to minimize those root causes of demand variation...|$|R
40|$|This paper investigates opinion {{contagion}} {{in collective}} behaviour, using {{an extension of}} Granovetter’s (1978) and Krassa’s (1988) threshold models. The theoretical background is the spiral of silence concept developed by Noelle-Neumann (1974), arguing that people only assert their opinions if they perceive a minimal support from a relevant proportion of others. We apply the model to explain the <b>wrong</b> electoral <b>forecasts</b> of the Polish parliamentary and presidential elections in 2005. It is shown that the minority opinions were more widely-held than was declared in opinion polls {{as a consequence of}} different distributions of the threshold values of opinion assertion...|$|R
40|$|Abstract. An {{exponential}} {{improvement of}} {{numerical weather prediction}} (NWP) models was observed {{during the last decade}} (Lynch, 2008). Civil Protection (CP) systems exploited Meteo services in order to redeploy their actions towards the prediction and prevention of events rather than towards an exclusively response-oriented mechanism 1. Nevertheless, experience tells us that NWP models, even if assisted by real time observations, are far from being deterministic. Complications frequently emerge in medium to long range forecasting, which are subject to sudden modifications. On the other hand, short term forecasts, if seen through the lens of criminal trials 2, are to the same extent, scarcely reliable (Molini et al., 2009). One particular episode related with <b>wrong</b> <b>forecasts,</b> in the Italian panorama, has deeply frightened CP operators as the NWP model in force missed a meteorological adversity which, in fact, caused death and dealt severe damage in the province of Vibo Valentia (2006). This event turned into a very discussed trial, lasting over three years, and intended against whom assumed the legal position of guardianship within the CP. A first set of data is now available showing that in concomitance with the trial of Vibo Valentia the number of alerts issued raised almost three folds. We sustain the hypothesis that the beginning of the process of overcriminalization (Husak, 2008) of CPs is currently increasing the number of false alerts with the consequent effect of weakening alert perception and response by th...|$|R
40|$|Planning stock {{portfolios}} is {{a challenging}} task, because investors have to forecast stock market trends. To limit losses due to <b>wrong</b> <b>forecasts</b> a common strategy is diversification, which consists in buying stocks belonging to different sectors/markets to spread bets across different assets. Since {{the amount of}} stock market data is continuously growing, an appealing research strategy is to first apply data mining algorithms to discover significant patterns from potentially large stock datasets and then exploit them to support investor decision-making. This article presents an itemset-based approach to supporting buy-and-hold investors in technical anal- yses by automatically identifying promising sets of high-yield yet diversified stocks to buy. Specifically, it investigates the use of itemsets to generate stock portfolios from historical stock data and recommend them for buy-and-hold investments. To achieve this goal, it analyzes stock market datasets, which contain for each stock the closing prices on different trading days. Datasets are enriched with (analyst-provided) taxonomies, which are used to classify stocks as the corresponding sectors. Unlike previous approaches, it generates a model composed of a subset of potentially interesting itemsets, which are then used to support investors in decision-making. The selected itemsets represent promptly usable stock portfolios satisfying expert's requirements on minimal average return and minimal level of diversification across sectors. The experiments performed on real stock datasets acquired under different market conditions demon- strate {{the effectiveness of the}} proposed approach compared to real stock funds...|$|R
40|$|Recent {{findings}} suggest that we neglect our personality to predict our future emotional reactions to specific events by focusing only on the events, inducing <b>wrong</b> <b>forecasts.</b> An interesting question to investigate is that our friends could predict better our affective states about future events because they take into account our personal dispositions. In the present study, sixty-nine pairs of students (participant/friend) were asked to predict their emotional reactions and those for a friend on a 7 -point Likert scale, ranging from 1 (very bad) to 7 (very good) two months before they will obtain their results for one examination. All the participants were contacted by SMS {{the day when the}} results were available, and were requested to rate their present affective states. Results showed that emotional predictions were different as compared to the current emotions for extreme results (upper than 8 / 10, and below 4 / 10) for the main participants but also for their friends, meaning that all the subjects predicted more positive emotions than current ones for very good results, and more negative emotions than current ones for very bad results. In contrast, the predictions were right for middle results (between 5 / 10 and 7 / 10) for both groups. These findings do not show that our friends are better predictors of our future emotional states than us. One possible explanation is that in present study, both participants and friends were concerned about the future event (i. e., academic results). So, our friends could be better forecasters only in the case of future events not shared. Peer reviewe...|$|R
40|$|Malthusian {{population}} theory was {{developed as a}} result of the rapid population growth rate and diminishing return in agricultural sector. Malthus observed geometric ratio growth in population vis-a-vis arithmetic ratio growth in food production and envisaged world “misery” or “vice ” if not checked. Subsequent development in the world however, proved the theory wrong. But this work discovers that the predicted doom of {{population theory}} is manifesting in Nigeria- rapid population growth rate, food crises, large scale poverty, ethnic and religious conflict, HIV/AIDS epidemics, etc. Although, the aforementioned are in line with the theory‟s predictions, Nigerian government operational modus favors these manifestations over the years. The work therefore, recommended that the judicial arm of government be made more efficient at law-enforcement, education sector be given appropriate budgetary attention to subdue poverty, diseases and health care predicaments. Thus, conclude that although, the theory is looked upon as primitive and <b>wrong,</b> the <b>forecasted</b> melancholies still exist in the 21 st century Nigeria...|$|R
50|$|It is {{difficult}} to determine the accuracy of any forecast, as it represents an attempt to predict future events, which is always challenging. To help improve and test forecast accuracy researchers use many different checking methods. A simple checking method involves the use of several different forecasting methods and comparing the results to see if they are more or less equal. Another method can involve statistically calculating the errors in the forecasting calculation and expressing them in terms of the root mean squared error, thereby providing an indication of the overall error in the method. A sensitivity analysis can also be useful, as it determines what will happen if some of the original data upon which the forecast was developed turned out to be <b>wrong.</b> Determining <b>forecast</b> accuracy, like forecasting itself, can never be performed with certainty and so it is advisable to ensure that input data is measured and obtained as accurately as possible, the most appropriate forecasting methods are selected, and the forecasting process is conducted as rigorously as possible.|$|R
40|$|Two decades ago, I {{was engaged}} {{in a study of}} {{corporate}} forecasting practices in Australia. On one occasion, I walked into an appointment with the Director of Marketing for a prominent and sizable international computer manufacturer. My first question for him was related to his company’s forecast error rates. To my utter astonishment, he replied that their <b>forecasts</b> were never <b>wrong.</b> He added that it was not within the culture of his organisation to permit error. Fortunately, forecasters know all about error and being wrong. We even expect to be <b>wrong</b> in our <b>forecasts</b> – it’s just a matter of how wrong we’re going to be, and how t...|$|R
40|$|November 28, 1995 (First draft: June 1992) This paper {{introduces}} {{the concept of}} Rational Belief Equilibrium (RBE) {{as a basis for}} a new theory of asset pricing. Rational Beliefs are probability beliefs about future economic variables which cannot be contradicted by the data generated by the economy. RBE is an equilibrium in which the diverse beliefs of all the agents induce an equilibrium stochastic process of prices and quantities and these beliefs are, in general, wrong {{in the sense that they}} are different from the true probability of the equilibrium process. These beliefs are, however, Rational. Consequently, in an RBE agents use the <b>wrong</b> <b>forecasting</b> functions and their forecasting mistakes play a crucial role in the analysis. First, we show that these mistakes are the reason why stock returns are explainable in retrospect and forecastable whenever the environment remains unchanged over a long enough time interval for agents to learn the forecasting function. Second, the aggregation of these mistakes generates Endogenous Uncertainty: it is that component of the variability of stock prices and returns which is endogenously induced by the beliefs and actions of the agents rather than by the standard exogenous state variables. The paper develops some basic propositions and empirical implications of the theory of RBE. Based on the historical background of the post world war II era, we formulate an econometric model of stock returns which allows non-stationarity in the form of changing environments ("regimes"). A sequence of econometric hypotheses are then formulated as implications of the theory of RBE and tested utilizing data on common stock returns in the post war period. Apart from confirming the validity of our theory, the empirical analysis shows that (i) common stock returns are forecastable within each environment but it takes time for agents to learn and approximate the forecasting functions. For some agents the time is too short so that it is too late to profit from such learning; (ii) the equilibrium forecasting functions change from one environment to the other in an unforecastable manner so that learning the parameters of one environment does not improve the ability to forecast in the subsequent environments. (iii) more than 2 / 3 of the variability of stock returns is due to endogenous uncertainty rather than exogenous causes. The paper analyzes one example of a gross market overvaluation which was induced in the 1960 's by an aggregation of agent's Mistakes. JEL Classification: D 5, E 17, G 12...|$|R
40|$|Predicting life {{expectancy}} 75 {{years into the}} future today is equivalent to having a forecaster in 1929 predict {{life expectancy}} for 2004. The most accurate prediction about such a forecast is probably that it will be <b>wrong.</b> Each <b>forecast</b> in this review involves assumptions {{about the extent to which}} the past will represent the future and the appropriate definition of the relevant past. Most demographers who produce forecasts prefer to fit a trend line to historical data, treating fluctuations in gains in life expectancy as random fluctuations around an upwardly trending mean. Most actuaries prefer to adjust past trends based on their interpre-tations of the causes of these past trends and the likelihood of these past events being repeated in the future. In addition, some experts in the field of aging believe that the pace of mortality decline will slow down because of the biological limits of the human body; others believe that science can overcome current limits to life expectancy. Some experts believe that changes in lifestyle and the adoption of healthier behaviors could increase life expectancy significantly, even without further medical breakthroughs. This literature review compares the 2004 forecasts of the Board of Trustees of the Old-Age and Survivors Insurance and Disability Insurance (Social Security) Trust Funds with the recommendation...|$|R
40|$|Things {{that are}} normal or common to {{people are often}} {{considered}} to be not interesting or simply not paid attention to because they are there anyway, everyday. Especially the air suffers from that mental attitude- its not only surrounding everyone, its even invisible and can thus be easily overseen. But air is interesting. It not only consists of different elements of the periodic system such as Oxygen, Nitrogen or even inert gases like Argon or Helium, but also of molecules like methane or water. It is a complex mix-ture {{of a lot of}} different liquids, gases and solid particles, i. e. dust. Thus its behaviour sometimes is complex as well. A <b>wrong</b> weather <b>forecast</b> is mostly not a result of a bad meteorologist’s work but of this strange behaviour. As leaving an umbrella at home and needing it afterwards might be in some cases a dissatisfying experience there is an interest to provide more reliable weather forecasts. Moreover although climate is obviously changing, some weather and climate effects are not well understood yet in some fields of research. In {{order to be able to}} judge on how much the climate is affected by humans at first it has to be found out what exactl...|$|R
40|$|An {{exponential}} {{improvement of}} {{numerical weather prediction}} (NWP) models was observed {{during the last decade}} (Lynch, 2008). Civil Protection (CP) systems exploited Meteo services in order to redeploy their actions towards the prediction and prevention of events rather than towards an exclusively response-oriented mechanism 1 . Nevertheless, experience tells us that NWP models, even if assisted by real time observations, are far from being deterministic. Complications frequently emerge in medium to long range forecasting, which are subject to sudden modifications. On the other hand, short term forecasts, if seen through the lens of criminal trials 2 , are to the same extent, scarcely reliable (Molini et al., 2009). One particular episode related with <b>wrong</b> <b>forecasts,</b> in the Italian panorama, has deeply frightened CP operators as the NWP model in force missed a meteorological adversity which, in fact, caused death and dealt severe damage in the province of Vibo Valentia (2006). This event turned into a very discussed trial, lasting over three years, and intended against whom assumed the legal position of guardianship within the CP. A first set of data is now available showing that in concomitance with the trial of Vibo Valentia the number of alerts issued raised almost three folds. We sustain the hypothesis that the beginning of the process of overcriminalization (Husak, 2008) of CPs is currently increasing the number of false alerts with the consequent effect of weakening alert perception and response by the citizenship (Brezntiz, 1984). The common misunderstanding of such an issue, i. e. the inherent uncertainty in weather predictions, mainly by prosecutors and judges, and generally by whom deals with law and justice, is creating the basis for a defensive behaviour 3 within CPs. This paper intends, thus, to analyse the social and legal relevance of uncertainty in the process of issuing meteo-hydrological alerts by CPs. Footnotes: 1 The Italian Civil Protection is working in this direction since 1992 (L. 225 / 92). An example of this effort is clearly given by the Prime Minister Decree (DPCM 20 / 12 / 2001 "Linee guida relative ai piani regionali per la programmazione delle attivita' di previsione, prevenzione e lotta attiva contro gli incendi boschivi – Guidelines for regional plans for the planning of prediction, prevention and forest fires fighting activities") that, already in 2001, emphasized "the most appropriate approach to pursue the preservation of forests is to promote and encourage prediction and prevention activities rather than giving priority to the emergency-phase focused on fire-fighting". 2 Supreme Court of the United States, In re Winship (No. 778), No. 778 argued: 20 January 1970, decided: 31 March 1970 : Proof beyond a reasonable doubt, which is required by the Due Process Clause in criminal trials, is among the "essentials of due process and fair treatment" 3 In Kessler and McClellan (1996) : "Defensive medicine is a potentially serious social problem: if fear of liability drives health care providers to administer treatments that do not have worthwhile medical benefits, then the current liability system may generate inefficiencies much larger than the costs of compensating malpractice claimants". ...|$|R
6000|$|Behind {{it was his}} grey-toned, intelligent, resentful face, his {{smouldering}} eyes, his slightly frayed {{collar and}} vivid, ill-chosen tie. At times Trafford could almost hear his flat insistent voice, his measured h-less speech. Dowd was so penetratingly right,--and so ignorant of certain essentials, so <b>wrong</b> in his <b>forecasts</b> and ultimates. It was true beyond disputing that Trafford as compared with Dowd had opportunity, power of a sort, the prospect and possibility of leisure. He admitted the liability that followed on that advantage. It expressed so entirely the spirit of his training that with Trafford the noble maxim of the older socialists; [...] "from each according to his ability, to each according to his need," [...] received an intuitive acquiescence. He had no more doubt than Dowd that Dowd {{was the victim of}} a subtle evasive injustice, innocently and helplessly underbred, underfed, cramped and crippled, and that all his own surplus made him in a sense Dowd's debtor.|$|R
40|$|The Federalist Founding Fathers {{would not}} {{recognize}} the modern American judiciary. Far from being the 2 ̆ 2 least dangerous 2 ̆ 2 branch and even farther from being 2 ̆ 2 beyond comparison the weakest of the three departments of power 2 ̆ 2 [...] as the Federalist Papers famously predicted [...] the judiciary today wields much greater influence than the Federalists originally envisioned. The Federalists were <b>wrong</b> in their <b>forecasts</b> {{of the reach of}} the American judiciary. But the Anti-Federalists were right. They correctly predicted the role of the modern American judiciary. The Anti-Federalists cautioned that judicial encroachments into the public square would undermine the American project of democracy and its promise of popular participation in public discourse. This Article explores the use of several constitutional devices in the service of American popular democracy. These devices have two purposes: first, to restore balance to the American constitutional order, and second, to bring the modern American judiciary into conformity with the more modest vision the Founding Fathers had when they created it...|$|R
40|$|This {{tutorial}} {{will show}} you how to do basic assembly work using parts in CAD ➲ This tutorial assumes you know basic CAD, including sketches, extrusions, cuts, and how to make basic parts ➲ This tutorial uses SolidWorks, {{but it can also be}} followed in AutoDesk Inventor, which is free to students Why are Assemblies Important? ➲ Assemblies are collections of parts put together to create and entire machine. The important thing about assemblies is that they allow us to see how parts interact with each other, if they will fit, how they cause each other to move. Without being able to do assemblies, we really can't do any design work in CAD To demonstrate how to make assemblies we will be making a basic crank from some simple parts you should be able to make on your own. Heres the first, a plate with a hole in it. Development up to present ➲ Development made up to the current situation ➲ Important background information ➲ Original forecasts which turned out to be <b>wrong</b> ➲ Original <b>forecasts</b> which turned out to be true Potential Alternatives ➲ State the alternative strategies ➲ List the pros and cons of each strategy ➲ Give a forecast of cost...|$|R
40|$|Wind {{predictions}} and bids in Denmark Wind {{power producers}} need a prediction method to bid their energy to day-ahead market. They pay balancing {{costs for the}} energy that has been <b>forecasted</b> <b>wrong.</b> If the prices for up- and down-regulation are asymmetric, it is worthwhile to bid a different amount than the forecast. Minimising the costs results in different bid than minimising the errors in energy. In this way the balancing costs are minimised. This is the case now in Denmark (both in West and in East), as the price difference of down-regulation and spot price is higher than for up-regulation. The question is, whether this kind of behaviour could result in wrong information for the TSO and cause more critical situations with larger errors in wind power production. This has been studied by a case of 12 months, for 2 wind farms in Denmark. They are both 20 MW wind farms, one is in East Denmark (Middelgrunden) and one in West Denmark (Klim). The results for the analysis are described in this document and more detailed results are in table 1. It is shown that for large errors downward (forecast production larger than realised production) there will be larger errors when using the bidding strategy. In those situations the information that the TSO receives from the bids would result in larger errors and more up-regulation would be needed...|$|R
40|$|Consider the {{following}} problem: at each {{date in the}} future, a given event {{may or may not}} occur, and you will be asked to forecast, at each date, the probability that the event will occur in the next date. Unless you make degenerate forecasts (zero or one), the fact that the event does or does not occur does not prove your <b>forecast</b> <b>wrong.</b> But, in the long run, if your forecasts are accurate, the conditional relative frequencies of occurrence of the event should approach your forecast. [4] has presented an algorithm that, whatever the sequence of realizations of the event, will meet the long-run accuracy criterion, even though it is completely ignorant about the real probabilities of occurrence of the event, or about the reasons why the event occurs or fails to occur. It is an adaptive algorithm, that reacts to the history of forecasts and occurrences, but does not learn from the history anything about the future: indeed, the past need not say anything about the future realizations of the event. The algorithm only looks at its own past inaccuracies and tries to make up for them in the future. The amazing result is that this (making up for past inaccuracies) can be done with arbitrarily high probability! Alternative arguments for this result have been proposed in the literature, remarkably by [3], where a very simple algorithm has been proved to work, using a classical result in game theory: Blackwell´s approachability result, [1]. Very recently, [2] has especialized Blackwell´s theorem in a way that (under a minor modification of the algorithm) simplifies the argument of [3]. Here I present such modification and argument. ...|$|R
40|$|The {{topic of}} this paper has been {{inspired}} by something {{of particular interest to}} Michael Mussa, something to which he made major and important contribu-tions while at the IMF. The biannual World Economic Outlook, published since the 1980 s {{under the auspices of the}} Research Department, has, as its purpose, the pro-vision of “analysis and projections [...] . [as] integral elements of the IMF’s surveil-lance of economic developments and policies in its member countries, developments in international financial markets, and the global economic system. ” In policy-oriented institutions (and in most businesses and individual decision making), poli-cymaking decisions are often guided by projections and forward-looking indicators. [JEL B 10, B 20] When forecasts are accurate, rarely does someone comment, “That is what weshould have expected to happen. ” However, when forecasts are inaccurate, they attract attention, usually critical, often in conjunction with an attack not only on the forecaster but also on the entire profession to which he or she belongs. To understand the nature of forecasting, and what can go right or <b>wrong,</b> I exam-ine <b>forecasts</b> in different times and places. This is not a complete survey; forecasts and forecasting have an exceptionally long history, so for the sake of brevity, I will focus on various forecasts relevant to the history of economics and economic his-tory, to see what we can learn about this type of exercise. *Stanley Engerman is John H. Munro Professor of Economics and Professor of History at the University of Rochester. This paper was presented at the IMF conference Mussa Fest, held in honor of Michael Mussa. I benefited from the comments of conference participants, particularly Michael Bordo and James Boughton, as well as Seymour Drescher and David Eltis...|$|R
40|$|September 9, 1997 (Updated November 28, 1998) The {{theory of}} Rational Belief Equilibria (RBE) offers a unified {{paradigm}} for explaining market volatility by {{the effect of}} "Endogenous Uncertainty" on financial markets. This uncertainty is propagated within the economy (hence "endogenous") by the beliefs of asset traders. The theory of RBE was developed in a sequence of papers assembled in a recently published book (Kurz [1997]) and the present paper provides a non-mathematical exposition of both the main ideas {{of the theory of}} RBE as well as a summary of the main results of the book regarding market volatility. Section I starts by reviewing the standard assumptions underlying models of Rational Expectations Equilibria (REE) and their implications to market volatility. The paper then reviews four basic problems which have constituted puzzles or anomalies in relation to the assumptions of REE : (i) Why are asset prices much more volatile than their underlying fundamentals? (ii) The equity premium puzzle: why under REE the predicted riskless rate is so high and the equity risk premium so low? (iii) Why do asset prices exhibit the "GARCH" behavior without exogenous fundamental variables to explain it? (iv) the "Forward Discount Bias" in foreign exchange: why are interest rate differentials poor predictors of future changes in the exchange rates? Section II outlines the basic assumptions of the theory of RBE and the main propositions which it implies for market volatility. Section III develops the simulation models which are used to study the four problems above and explains that the domestic economy is calibrated, as in Mehra and Prescott [1985], to the U. S. economy. Then {{for each of the four}} problems the relevant simulation results of the RBE are presented and compared to the results predicted by a corresponding REE and to the actual empirical observations in the U. S. The paper concludes that the main cause of market volatility is the dynamics of beliefs of agents. The theory of RBE shows that if agents disagree then the state of belief of each agent, represented by his conditional probability, must fluctuate over time. Hence the distribution of the individual states of belief in the market is the root cause of all phenomena of market volatility. The GARCH phenomenon of time varying variance of asset prices is explained in the simulation model by the presence of both persistence in the states of beliefs of agents as well as correlation among these states. Correlation makes beliefs either narrowly distributed (i. e. "consensus") or widely distributed (i. e. "non-consensus"). In a belief regime of consensus (due to persistence it remains in place for a while) agents seek to buy or sell the same portfolio leading to high volatility. In a belief regime of non-consensus there is a widespread disagreement which cause a balance between sellers and buyers leading to low market volatility. In short, the GARCH phenomenon is the result of shifts in the distribution of beliefs in the market induced by the dynamics of the individual states of belief. Turning to the equity risk premium, the key question is what are the distributions of beliefs which ensure that the average riskless rate is low and the average equity risk premium is high. It turns out that the only circumstances when the mean riskless rate falls to around 1 % and the mean equity premium rises to around 5. 5 % arise when, on the average, the majority of agents are relatively optimistic about the prospects of capital gains in the subsequent period. In such a circumstance the rationality of belief conditions imply that the pessimists (who are in the minority) must have a higher intensity of pessimism than the intensity of the optimists. In a large economy with this property the state of belief of any one agent may fluctuate but on the average there will be a minority of intensely pessimistic agents. This asymmetry between optimists and pessimists flows directly from the rationality conditions of beliefs and implies that at most dates the pessimists have a stronger impact on the bill market. At those dates the pessimists protect their wealth by increasing their purchases of the riskless bill. This bids up the price of the bill, lowers the riskless rate and results in a higher equity risk premium. In sum, the theory of Rational Belief offers a very simple explanation to the observed riskless rate and equity premium. It says that the riskless rate is, on average, low and the premium high because at most dates there is a minority of pessimist who, by the rationality of belief conditions, have the higher intensity level of belief about high stock prices in the future. These agents drive the riskless rate lower and the equity premium higher. The "Forward Discount Bias" in foreign exchange markets is the result of the fact that in an RBE agents often make the <b>wrong</b> <b>forecasts</b> although they are right on the average. Hence, in an RBE the exchange rate fluctuates excessively due to the errors of the agents and hence at almost no date is the interest differential between two countries an unbiased estimate of the rate of depreciation of the exchange rate one period later. The bias is positive since agents who invest in foreign currency demand a risk premium on endogenous uncertainty which is above and beyond the risk which exists in an REE. The size of the bias is equal to the added risk premium due to endogenous uncertainty. JEL Classification Numbers: D 58, D 84, G 12. Key Words: rational expectation equilibrium (REE), rational beliefs, rational belief equilibrium (RBE), endogenous uncertainty, state of belief, market volatility, equity risk premium, riskless rate, GARCH, forward discount bias, foreign exchange rates, OLG economy, correlation among beliefs, simulations. rational expectation equilibrium (REE), rational beliefs, rational belief equilibrium (RBE), endogenous uncertainty, state of belief, market volatility, equity risk premium, riskless rate, GARCH, forward discount bias, foreign exchange rates, OLG economy, correlation among beliefs, simulations...|$|R
40|$|Reservoir {{engineering}} has {{the task}} {{to study the}} behavior and the characteristics of an oil or gas reservoir so as to determine the strategy of future development and production that would maximize the profit. History matching represents a fundamental step in view of reservoir production forecasting and quantification of the associated uncertainty and it is definitely the most challenging phase of reservoir simulation. In fact, the team performing the calibration {{has to deal with}} the non-uniqueness issue, as history matching is an ill-posed inverse problem due to the insufficient constraints and data. Some years ago, a new methodology called automatic history matching was approached by the scientific community. The idea consisted in treating history matching as an optimization process, i. e. defining an objective function representative of the discrepancy between measured (real) and simulated data, and in minimizing the objective function. In the last years, the scientific community has taken a great leap forward in the automation of the history matching process to calibrate dynamic reservoir models. Some of the methods have been inherited from other scientific disciplines; others have been constructed ad-hoc for the history matching problem. But at the moment, there is no clear winner. The workflow of reservoir studies has simultaneously evolved boosting the integration among disciplines and somewhat reducing the time needed for the calibration of a model. Therefore, one would expect that assisted history matching would be the standard {{but the truth is that}} history matching is still a very knotty problem. Yet, the possibility to exploit assisted history matching procedures is a key step to find multiple calibrated reservoir models. The importance of obtaining a number of solution to the inversion problem is that they can be used for field performance prediction so as to obtain a representative evaluation of the risks associated to any given reservoir development scenario. The aim of this research consisted in discussing benefits, limitations and drawbacks of assisted history matching when using different techniques such as such as evolutionary multi-objective optimization and data assimilation methodologies. Furthermore, the research was devoted to prove that a single calibrated model approach can lead to large errors in the production forecast and that only a true probabilistic approach, based on multiple calibrated reservoir models relying on different geological configurations, can assess the uncertainty affecting the reservoir productivity and final hydrocarbon recovery. Three reservoir cases subject to assisted history were studied. The case studies have increasing complexity and were selected to solve problems progressively more challenging in terms of historical data to be assimilated and number of parameters to be calibrated. An innovative framework for the calibration of reservoir numerical models was conceived and applied to the first case study. The study was triggered by the need for an algorithm capable of taking advantage of a network of computers to process the large number of simulations required by a multi-objective optimization process. The multi-objective optimization is a process for simultaneously optimizing two or more conflicting objectives subject to certain constraints. A multi-objective evolutionary algorithm (SPEA 2) for assisted history matching was implemented. Evolutionary algorithms are based on the theory of the evolution and behavior of living species. The interaction of the users with the optimization algorithm was addressed by the introduction of an innovative fitness function definition that includes a "social" or user contribution to facilitate the exploration of the solution space in the neighborhood of the user-selected candidate solutions. The developed algorithm was coupled to a collective computation network. In order to overcome the licensing restrictions of the available commercial reservoir simulators a black-oil two-phase numerical simulator based on the finite volume method was also developed. The results showed that the SPEA 2 algorithm was capable of finding a representative set of optimal solutions that matched the historical data. The implemented workflow also provided the possibility, for competent users, to steer the selection of the fittest individuals or solutions by selecting preferred or "liked" solutions. Furthermore, the optimization method showed a good degree of scalability, demonstrating the importance of parallelization for speeding up the calibration process. In the second case study the calibration process was carried out by using both the adaptive Gaussian mixture filter (AGM) and the ensemble Kalman filter (EnKF). The adaptive Gaussian mixture filter is an ensemble-based approach for sequential data assimilation where different weights are associated to each ensemble member, the ensemble Kalman filter is characterized by uniform member weights. The methodology was applied to the PUNQ-S 3 model, a medium-complexity synthetic reservoir model constructed from a real field study and made available to the scientific community by the oil companies. More than 5000 parameters were calibrated so as to match the historical production data coming from 5 different wells. The tested methods showed a remarkably low computational cost compared with global optimization methods. Both the EnKF and AGM methods provided a fairly good estimate of the cumulative oil production in the forecast phase and effectively reduced the distribution spread of the observed data from the initial to the final models. AGM proved to be better suited than the EnKF for the estimation of the petrophysical distributions. However, the bandwidth parameter of the AGM should be tuned for the method to provide outstanding results. In the third case study the analysis was focused at the assessment of the uncertainty associated with the facies distribution in a synthetic hydrocarbon reservoir comprising sand channels and clayey floodplain as in a typical fluvial depositional environment. Three different conceptual models of the internal reservoir geometry were defined. A conceptual model is intended as a given set of geometrical parameters that are used to stochastically generate different facies realizations by distributing the hydrocarbon-bearing facies within the model domain. A total of nearly 80, 000 parameters were calibrated by the assimilation of the data coming from the producers and injectors wells. Results proved the importance of achieving multiple solutions so as to obtain a meaningful quantification of the uncertainty associated to the production forecast for successive technical and economic risk analysis. The proposed AHM workflow efficiently reduced the uncertainty associated to the initial set of generated models by conditioning them to the dynamic data and significantly shifted the ensemble median towards the true value. It was also demonstrated that a good fit of the production data does not necessarily provide a good estimation of the parameters of the reservoir. This result further confirmed that a deterministic history matching approach can be very misleading because <b>wrong</b> production <b>forecast</b> can be obtained from a nicely calibrated model. Although the application to the three case studies proved the efficiency of the conceived workflow, it only seems to be the end of the beginning rather than the beginning of the end. The user interaction to apply the new tools is even more important and more interdisciplinary than in manual HM workflows because the parameters of the algorithms for assisted history matching need to be tuned for the methods to work efficiently. The evolution of the technology and the "market" should guarantee a smooth transition between manual and assisted history matching expected in the following years. In this research novel solutions were presented to address the main known issues limiting the application of the AHM to real cases. With the proposed workflow reservoir studies can be significantly enhanced by generating several solutions that truly take into account the uncertainty of the data used for the construction and calibration of the model and, at the same, the time necessary for the calibration of the models can be reduced. Therefore, the application of the investigated techniques can lead to more representative production and economic forecasts. All the professionals involved in a reservoir study (i. e. geologists, geophysicists, reservoir engineers, etc.) can benefit from the proposed workflow because it adds additional information extracted from dynamic data to the modeling process, closing the loop and leading to the ultimate integrated approach. In the thesis novel techniques and methods were developed. The innovations were focused on three main areas: parallel computation, algorithm selection and uncertainty assessment. Grid computing is considered one of the most important technological development of this decade. In fact, there is a growing interest of big companies such Amazon (Amazon EC 2 elastic cloud computing) and Google (Google Cloud computing) to provide huge amount of CPU cores in order to deal with computational intensive tasks. In this thesis parallel computing was used to accelerate the calibration of reservoir numerical models using heuristic algorithms guided by user interactions. This approach provides the possibility to handle high resolution reservoir models that can be calibrated in a fraction of the time required by the traditional approach. The traditional gradient based methods have the advantage of converge rapidly towards a minimum; however, they can provide only one solution which can be significantly far from the "real" solution. In this thesis a new generation of sequential filters were tested, namely the Ensemble Kalman Filter and Adaptive Gaussian Mixture. Both algorithms provide multiple calibrated models that can better assess the uncertainty of the input parameters and offer a solution to the non-uniqueness issue related to history matching. The novel workflow, including the implemented algorithms, was then tested to calibrate a set of fluvial-depositional reservoir models under multiple facies distribution...|$|R

