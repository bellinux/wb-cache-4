315|380|Public
50|$|There {{are many}} {{branches}} and many research groups working on word embeddings. In 2013, a team at Google led by Tomas Mikolov created word2vec, a <b>word</b> <b>embedding</b> toolkit which can train vector space models {{faster than the}} previous approaches. Most new <b>word</b> <b>embedding</b> techniques rely on a neural network architecture instead of more traditional n-gram models and unsupervised learning.|$|E
50|$|Other key {{techniques}} {{in this field}} are negative sampling and <b>word</b> <b>embedding.</b> <b>Word</b> <b>embedding,</b> such as word2vec, {{can be thought of}} as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using <b>word</b> <b>embedding</b> as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar {{can be thought of as}} probabilistic context free grammar (PCFG) implemented by an RNN. Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing. Deep neural architectures have achieved state-of-the-art results in natural language processing tasks such as constituency parsing, sentiment analysis, information retrieval, spoken language understanding, machine translation, contextual entity linking, writing style recognition and others.|$|E
50|$|Quality of <b>word</b> <b>embedding</b> {{increases}} with higher dimensionality. But after reaching some point, marginal gain will diminish. Typically, the dimensionality of the vectors {{is set to}} be between 100 and 1,000.|$|E
40|$|The {{present study}} looks at {{manipulating}} audience design using different priming techniques. We {{were trying to}} test the effectiveness of different priming techniques (priming <b>words</b> <b>embedded</b> in a story versus priming <b>words</b> <b>embedded</b> in a <b>word</b> search) on audience design by making people more or less helpful in a story retelling task. A time constraint was also introduced {{to see if the}} effect of word search priming would be cancelled out. In order to answer these questions two experiments were run with one looking at the effectiveness of <b>embedding</b> <b>words</b> in a story to prime helpfulness or unhelpfulness with the hypothesis that these stories would prime people {{in such a way that}} would affect the quality of their story retelling. The second experiment’s hypothesis was that a strict time constraint would cancel out the effect of the priming from the word search task. Finally, our results pointed to a possible effect of priming using <b>words</b> <b>embedded</b> in a story on quality of stories written. However we cannot be sure that the priming was the source of the differences in audience design. The word search version of the experiment did not give conclusive results but pointed to a general trend of strict time constraints cancelling out the effects of priming...|$|R
40|$|In a 50, 000 -word {{corpus of}} spoken British English the {{occurrence}} of <b>words</b> <b>embedded</b> within other <b>words</b> is reported. Within-word embedding in this real speech sample is common, and analogous {{to the extent of}} embedding observed in the vocabulary. Imposition of a syllable boundary matching constraint reduces but by no means eliminates spurious <b>embedding.</b> <b>Embedded</b> <b>words</b> are most likely to overlap with the beginning of matrix words, and thus may pose serious problems for speech recognisers...|$|R
40|$|<b>Words</b> <b>embedded</b> in {{unrelated}} <b>words</b> are {{a frequent}} source of word play. When the <b>embedded</b> <b>word</b> occurs in reverse order, the surprise factor is further increased. For example, a recent cryptic crossword included the punning clue 2 ̆ 2 We sent up plant monster 2 ̆ 2. The answer was WEREWOLF, i. e., WE followed by FLOWER reversed. Another {{answer in the}} same puzzle was DAIRYMAID. It 2 ̆ 7 s clue dissected the answer into AID preceded by the reversal of MYRIAD...|$|R
50|$|<b>Word</b> <b>embedding</b> is the {{collective}} {{name for a}} set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with much lower dimension.|$|E
5000|$|The {{reasons for}} {{successful}} <b>word</b> <b>embedding</b> {{learning in the}} word2vec framework are poorly understood. Goldberg and Levy {{point out that the}} word2vec objective function causes words that occur in similar contexts to have similar embeddings (as measured by cosine similarity) and note that this is in line with J. R. Firth's distributional hypothesis. However, they note that this explanation is [...] "very hand-wavy" [...] and argue that a more formal explanation would be preferable.|$|E
50|$|A {{recursive}} {{neural network}} (RNN) {{is a kind}} of deep neural network created by applying the same set of weights recursively over a structure, to produce a structured prediction over variable-size input structures, or a scalar prediction on it, by traversing a given structure in topological order. RNNs have been successful for instance in learning sequence and tree structures in natural language processing, mainly phrase and sentence continuous representations based on <b>word</b> <b>embedding.</b> RNNs have first been introduced to learn distributed representations of structure, such as logical terms.Models and general frameworks have been developed in further works since the 90s.|$|E
40|$|The cross modal {{repetition}} priming paradigm {{was used}} to investigate how potential lexically ambiguous no-release variants are processed. In particular we focus on segmental regularities that affect the variant’s frequency of occurrence (voicing of the critical segment) and phonological {{context in which the}} variant occurs (status of the following word-initial segment). Primes consisted of carrier words ending in a segment likely (voiced; e. g., BAND) or unlikely (voiceless; PLANT) to be produced in no-release form followed by a consonant or vowel onset context word. Each carrier <b>word</b> had an <b>embedded</b> lexical competitor (<b>embedded</b> <b>word)</b> formed by the segments prior to its final consonant (e. g., plan in plant). Productions of these <b>embedded</b> <b>words</b> (true <b>embedded</b> <b>word)</b> were also used as primes. Both true <b>embedded</b> <b>words</b> (Experiments 1 a– 1 c) and carrier words (Experiments 2 a– 2 c) were used as visual targets. The results are discussed in terms of the contribution of probabilistic speech events to theories of spoken word recognition and lexical competition...|$|R
40|$|This study {{investigates the}} effect of sentence-level prosody on {{production}} of English lexical stress, comparing L 1 English and L 1 Taiwan Mandarin speaker groups. 4 L 1 North American English speakers and 9 L 1 Taiwan Mandarin speakers were asked to produce a set of 20 disyllabic and multisyllabic <b>words</b> <b>embedded</b> in three different prosodic contexts: neutral broad focus, at a phrase/sentence boundary, and in narrow focus. Results suggest that production of the prosodic cues to mark lexical stress (F 0, duration and amplitude) becomes {{much more difficult for}} L 2 speakers when disyllabic and multisyllabic <b>words</b> are <b>embedded</b> in higher-level prosodic contexts...|$|R
40|$|This paper {{presents}} lexical {{statistics on}} the pattern of occurrence of <b>words</b> <b>embedded</b> in other <b>words.</b> We report {{the results of an}} analysis of 25000 words, varying in length from two to six syllables, extracted from a phonetically-coded English dictionary (The Longman Dictionary of Contemporary English). Each syllable, and each string of syllables within each word was checked against the dictionary. Two analyses are presented: the first used a complete list of polysyllables, with look-up on the entire dictionary; the second used a sublist of content <b>words,</b> counting only <b>embedded</b> <b>words</b> which were themselves content words. The results have important implications for models of human speech recognition. The efficiency of these models depends, in different ways, on the number and location of words within words...|$|R
5000|$|The <b>word</b> <b>embedding</b> {{technique}} {{began development}} in 2000. Bengio et al. provided {{in a series}} of papers the [...] "Neural probabilistic language models" [...] to reduce the high dimensionality of words representations in contexts by [...] "learning a distributed representation for words". (Bengio et al, 2003). Roweis and Saul published in Science how to use [...] "locally linear embedding" [...] (LLE) to discover representations of high dimensional data structure. The area developed gradually and really took off after 2010, partly because important advances had been made since then on the quality of vectors and the training speed of the model.|$|E
50|$|Textual {{entailment}} measures {{natural language}} understanding as it asks for a semantic interpretation of the text, and due to its generality remains an active area of research. Many approaches and refinements of approaches have been considered, such as <b>word</b> <b>embedding,</b> logical models, graphical models, rule systems, contextual focusing, and machine learning. Practical or large-scale solutions avoid these complex methods and instead use only surface syntax or lexical relationships, but are correspondingly less accurate. However, even state-of-the-art systems are still far from human performance; a study found humans to be in agreement on the dataset 95.25% of the time, while algorithms from 2016 had not yet achieved 90%.|$|E
5000|$|Instead {{of using}} neural net {{language}} models to produce actual probabilities, {{it is common}} to instead use the distributed representation encoded in the networks' [...] "hidden" [...] layers as representations of words; each word is then mapped onto an -dimensional real vector called the <b>word</b> <b>embedding,</b> where [...] is the size of the layer just before the output layer. The representations in skip-gram models have the distinct characteristic that they model semantic relations between words as linear combinations, capturing a form of compositionality. For example, in some such models, if [...] is the function that maps a word [...] to its -d vector representation, then ...|$|E
5|$|File {{artifacts}} and meta-data {{can be used}} to identify the origin of a particular piece of data; for example, older versions of Microsoft <b>Word</b> <b>embedded</b> a Global Unique Identifer into files which identified the computer it had been created on. Proving whether a file was produced on the digital device being examined or obtained from elsewhere (e.g., the Internet) can be very important.|$|R
40|$|Derived words udy e d in d com the u suffix (-ing) {{differ in}} the amount of {{information}} carried by their morphological families lingu comp age (The present paper aims at adding to the current knowledge on both topics by reporting an eye-tracking regression study of Dutch derived <b>words</b> <b>embedded</b> in sentential contexts. mine processing costs of derived word recognition (e. g., Ber...|$|R
40|$|In the November 2002 Kickshaws, Dave Morice {{introduced}} {{the idea of}} using the Fibonacci sequence to generate all 676 bigrams in fourteen closed loops. Ross Eckler in the current issue points out the lovely symmetry and underlying mathematics of the fascinating pattern Dave found. Dave also noted several <b>words</b> <b>embedded</b> in six of the loops, mostly three-letter words with three of four letters...|$|R
5000|$|The <b>word</b> <b>embedding</b> {{approach}} {{is able to}} capture multiple different degrees of similarity between words. Mikolov et al. (2013) found that semantic and syntactic patterns can be reproduced using vector arithmetic. Patterns such as “Man is to Woman as Brother is to Sister” can be generated through algebraic operations on the vector representations of these words such that the vector representation of “Brother” - ”Man” + ”Woman” produces a result which is closest to the vector representation of “Sister” in the model. Such relationships can be generated {{for a range of}} semantic relations (such as Country—Capital) as well as syntactic relations (e.g. present tense—past tense) ...|$|E
30|$|RNN: <b>Word</b> <b>embedding</b> layer → Dropout → RNN layer → Dropout.|$|E
40|$|Word embeddings {{have been}} found to provide {{meaningful}} representations for words in an efficient way; therefore, they have become common in Natural Language Processing sys- tems. In this paper, we evaluated different <b>word</b> <b>embedding</b> models trained on a large Portuguese corpus, including both Brazilian and European variants. We trained 31 <b>word</b> <b>embedding</b> models using FastText, GloVe, Wang 2 Vec and Word 2 Vec. We evaluated them intrinsically on syntactic and semantic analogies and extrinsically on POS tagging and sentence semantic similarity tasks. The obtained results suggest that word analogies are not appropriate for <b>word</b> <b>embedding</b> evaluation; task-specific evaluations appear to be a better option. Comment: 7 pages, STIL 2017 Full pape...|$|E
40|$|Several {{models of}} spoken word {{recognition}} postulate that recognition is achieved via a process of competition between lexical hypotheses. Competition not only provides a mechanism for isolated word recognition, it also assists in continuous speech recognition, since it offers a means of segmenting continuous input into individual words. We present statistics on the pattern of occurrence of <b>words</b> <b>embedded</b> in the polysyllabic words of the English vocabulary, showing that an overwhelming majority (84 %) of polysyllables have shorter <b>words</b> <b>embedded</b> within them. Positional analyses show that these embeddings are most common at the onsets of the longer word. Although both phonological and syntactic constraints could rule out some <b>embedded</b> <b>words,</b> they do not remove the problem. Lexical competition provides a means of dealing with lexical embedding. It is also supported by {{a growing body of}} experimental evidence. We present results which indicate that competition operates both between word candidates that begin at the same point in the input and candidates that begin at different points (McQueen, Norris, & Cutler, 1994, Noms, McQueen, & Cutler, in press). We conclude that lexical competition is an essential component in models of continuous speech recognition...|$|R
5000|$|Maestro Notation for MS <b>Word</b> - <b>embeds</b> and edits music {{notation}} {{directly in}} Word documents ...|$|R
40|$|Japanese {{listeners}} detected Japanese <b>words</b> <b>embedded</b> {{at the end}} {{of nonsense}} sequences (e. g., kaba 'hippopotamus' in gyachikaba). When the final portion of the preceding context together with the initial portion of the word (e. g., here, the sequence chika) was compatible with many lexical competitors, recognition of the <b>embedded</b> <b>word</b> was more difficult than when such a sequence was compatible with few competitors. This clear effect of competition, established here for preceding context in Japanese, joins similar demonstrations, in other languages and for following contexts, to underline that the functional architecture of the human spoken-word recognition system is a universal one. ...|$|R
40|$|This paper {{proposes a}} model to learn word embeddings with {{weighted}} contexts based on part-of-speech (POS) relevance weights. POS is a fundamental element in natural language. However, state-of-the-art <b>word</b> <b>embedding</b> models fail to consider it. This paper proposes to use position-dependent POS relevance weighting matrices to model the inherent syntactic relationship among words within a context window. We utilize the POS relevance weights to model each word-context pairs during the <b>word</b> <b>embedding</b> training process. The model proposed in this paper paper jointly optimizes word vectors and the POS relevance matrices. Experiments conducted on popular word analogy and word similarity tasks all demonstrated {{the effectiveness of the}} proposed method. Comment: <b>Word</b> <b>embedding...</b>|$|E
40|$|We {{introduce}} language-driven image generation, {{the task}} of generating an image visualizing the semantic contents of a <b>word</b> <b>embedding,</b> e. g., given the <b>word</b> <b>embedding</b> of grasshopper, we generate a natural image of a grasshopper. We implement a simple method based on two mapping functions. The first takes as input a <b>word</b> <b>embedding</b> (as produced, e. g., by the word 2 vec toolkit) and maps it onto a high-level visual space (e. g., the space defined {{by one of the}} top layers of a Convolutional Neural Network). The second function maps this abstract visual representation to pixel space, in order to generate the target image. Several user studies suggest that the current system produces images that capture general visual properties of the concepts encoded in the <b>word</b> <b>embedding,</b> such as color or typical environment, and are sufficient to discriminate between general categories of objects. Comment: A 6 -page version to appear at the Multimodal Machine Learning NIPS 2015 Worksho...|$|E
40|$|Sponsored {{search is}} at the center of a multibil-lion dollar market {{established}} by search tech-nology. Accurate ad click prediction is a key component for this market to function since the pricing mechanism heavily relies on the estimation of click probabilities. Lexical fea-tures derived from the text of both the query and ads play a significant role, complementing features based on historical click information. The purpose of this paper is to explore the use of <b>word</b> <b>embedding</b> techniques to generate ef-fective text features that can capture not only lexical similarity between query and ads but also the latent user intents. We identify several potential weaknesses of the plain application of conventional <b>word</b> <b>embedding</b> methodolo-gies for ad click prediction. These observa-tions motivated us to propose a set of novel joint <b>word</b> <b>embedding</b> methods by leveraging implicit click feedback. We verify the effec-tiveness of these new <b>word</b> <b>embedding</b> models by adding features derived from the new mod-els to the click prediction system of a com-mercial search engine. Our evaluation results clearly demonstrate the effectiveness of the proposed methods. To the best of our knowl-edge this work is the first successful applica-tion of <b>word</b> <b>embedding</b> techniques for the task of click prediction in sponsored search. ...|$|E
40|$|The paper {{describes}} {{the challenges of}} modeling embedded hybrid control systems at a higher abstraction level. It discusses the problems of modeling such systems and suggests the use of hybrid Petri nets. Modeling an exemplary embedded control system with a special hybrid Petri net class using an objectoriented modeling and simulation tool shows the potential of hybrid Petri nets. Key <b>Words.</b> <b>Embedded</b> Control Systems, Hybrid Petri Nets 1...|$|R
5000|$|Additionally, Spanish {{influences}} {{are frequently}} {{encountered in the}} languages of Nabuenos. Some examples of Spanish <b>words</b> <b>embedded</b> in the local dialect are: “Abreyā raw iton puertan.” This is a command statement, meaning “Open the door” in English or “Buksan ang pintuan” in the Filipino language. The word “abreyā” is an inflection of the Spanish verb “abrir” (to open), and “puertan” is a shorten word of [...] "puertāhan" [...] which is from the Spanish word “puerta”.|$|R
40|$|Studies of spoken-word {{recognition}} {{have revealed}} that competition from <b>embedded</b> <b>words</b> differs in strength {{as a function of}} where in the carrier <b>word</b> the <b>embedded</b> <b>word</b> is found and have further shown embedding patterns to be skewed such that embeddings in initial position in carriers outnumber embeddings in final position. Lexico-statistical analyses show that this skew is highly attenuated in Japanese, a noninflectional language. Comparison of the extent of the asymmetry in the three Germanic languages English, Dutch, and German allows the source to be traced to a combination of suffixal morphology and vowel reduction in unstressed syllables. 6 page(s...|$|R
40|$|International audienceIn this paper, {{we explore}} {{the usage of}} <b>Word</b> <b>Embedding</b> {{semantic}} resources for Information Retrieval (IR) task. This embedding, produced by a shallow neural network, {{have been shown to}} catch semantic similarities between words (Mikolov et al., 2013). Hence, our goal is to enhance IR Language Models by addressing the term mismatch problem. To do so, we applied the model presented in the paper Integrating and Evaluating Neural <b>Word</b> <b>Embedding</b> in Information Retrieval by Zuccon et al. (2015) that proposes to estimate the translation probability of a Translation Language Model using the cosine similarity between <b>Word</b> <b>Embedding.</b> The results we obtained so far did not show a statistically significant improvement compared to classical Language Model...|$|E
40|$|Do word embeddings {{converge}} {{to learn}} similar things over different initializations? How repeatable are experiments with word embeddings? Are all <b>word</b> <b>embedding</b> techniques equally reliable? In this {{paper we propose}} evaluating methods for learning word representations by their consistency across initializations. We propose a measure to quantify the similarity of the learned word representations under this setting (where {{they are subject to}} different random initializations). Our preliminary results illustrate that our metric not only measures a intrinsic property of <b>word</b> <b>embedding</b> methods but also correlates well with other evaluation metrics on downstream tasks. We believe our methods are is useful in characterizing robustness [...] an important property to consider when developing new <b>word</b> <b>embedding</b> methods. Comment: RepEval @ ACL 201...|$|E
40|$|Semantic {{similarity}} {{measures are}} an important part in Natural Language Processing tasks. However Semantic similarity measures built for general use do not perform well within specific domains. Therefore in this study we introduce a domain specific semantic similarity measure that was created by the synergistic union of word 2 vec, a <b>word</b> <b>embedding</b> method that is used for semantic similarity calculation and lexicon based (lexical) semantic similarity methods. We prove that this proposed methodology out performs <b>word</b> <b>embedding</b> methods trained on generic corpus and methods trained on domain specific corpus but do not use lexical semantic similarity methods to augment the results. Further, we prove that text lemmatization can improve the performance of <b>word</b> <b>embedding</b> methods. Comment: 6 Pages, 3 figure...|$|E
40|$|Three word-spotting {{experiments}} {{assessed the}} role of syllable onsets and offsets in lexical segmentation. Participants detected CVC <b>words</b> <b>embedded</b> initially or finally in bisyllabic nonwords with aligned (CVC. CVC) or misaligned (CV. CCVC) syllabic structure. A misalign- ment between word and syllable onsets (Experiment 1) produced a greater perceptual cost than a misalignment between word and syllable offsets (Experiments 2 and 3). These results suggest that listeners rely on syllable onsets to locate the beginning of words. The implications for theories of lexical access in continuous speech are discussed...|$|R
50|$|The club's colours {{are white}} and green. The Club's kit is a White Vest with an emerald Green Band across the chest with the <b>word</b> Raheny <b>embedded.</b> Shorts are emerald green.|$|R
40|$|Three cross-modal {{associative}} priming experiments investigated whether {{speech input}} activates <b>words</b> that are <b>embedded</b> in other <b>words.</b> When the <b>embedded</b> <b>word</b> {{corresponded to the}} final syllable of a bisyllabic carrier (boos, meaning angry embedded in framboos, meaning raspberry), facilitatory priming effects were observed for related targets of the <b>embedded</b> <b>word.</b> No effects were found when the end-embedded word did not start {{at the onset of}} a syllable (wijn meaning wine in zwijn meaning swine). Beginning-embedded words were activated only if the carrier was a nonword (vel meaning skin in velk), but not when the carrier was a word (vel in velg, meaning rim). The results support the joint operation of metric segmentation and lexical competition: Words are activated if their onset matches the onset of a strong syllable; words are then excluded on the basis of interword competition. SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|R
