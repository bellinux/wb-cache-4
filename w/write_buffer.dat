69|105|Public
25|$|Shader storage buffer objects, {{allowing}} shaders to {{read and}} <b>write</b> <b>buffer</b> objects like image load/store from 4.2, but through the language rather than function calls.|$|E
2500|$|Mudge was {{responsible}} for early research into a type of security vulnerability known as the buffer overflow. In 1995 he published [...] "How to <b>Write</b> <b>Buffer</b> Overflows", {{one of the first}} papers on the topic. He published some of the first security advisories and research demonstrating early vulnerabilities in Unix such as code injection, side-channel attacks, and information leaks, and was a leader in the full disclosure movement. He was the initial author of security tools L0phtCrack, AntiSniff, and l0phtwatch.|$|E
50|$|Use of a <b>write</b> <b>buffer</b> in {{this manner}} frees the cache to service read {{requests}} while the write is taking place. It is especially useful for very slow main memory in that subsequent reads are able to proceed without waiting for long main memory latency. When the <b>write</b> <b>buffer</b> is full (i.e. all buffer entries are occupied), subsequent writes still {{have to wait until}} slots are freed. Subsequent reads could be served from the <b>write</b> <b>buffer.</b> To further mitigate this stall, one optimization called <b>write</b> <b>buffer</b> merge may be implemented. <b>Write</b> <b>buffer</b> merge combines writes that have consecutive destination addresses into one buffer entry. Otherwise, they would occupy separate entries which increases the chance of pipeline stall.|$|E
5000|$|Writing {{many small}} {{blocks of data}} can even lead to {{negative}} compression rates, so {{it is essential for}} applications to use large <b>write</b> <b>buffers.</b>|$|R
5000|$|A device error {{may occur}} on the media where the object is stored while <b>writing</b> <b>buffered</b> data, the {{completion}} structure or updating meta data related to the object (for example last access time).|$|R
40|$|In {{this paper}} we {{categorize}} the coherence traffic in update-based protocols and show that, for most applications, more than 90 % of all updates {{generated by the}} protocol are unnecessary. We identify application characteristics that generate useless update traffic, and compare the isolated and combined effects of several software and hardware techniques for eliminating useless updates. These techniques include dynamic and static hybrid protocols, a data re-mapping strategy, and coalescing <b>write</b> <b>buffers.</b> Our simulations show that these techniques are effective for different types of useless updates. Overall, software caching (where dynamic data re-mapping is performed under programmer or compiler control) {{has the potential to}} significantly increase the percentage of useful traffic in applications. When software caching is not applicable, either the static or the dynamic protocol generates the least useless traffic. Although coalescing <b>write</b> <b>buffers</b> provide great reductions in the total nu [...] ...|$|R
50|$|Store {{instructions}} {{result in}} data buffered in a 4-entry by 32-byte <b>write</b> <b>buffer.</b> The <b>write</b> <b>buffer</b> improved performance {{by reducing the}} number of writes on the system bus by merging data from adjacent stores and by temporarily delaying stores, enabling loads to be serviced quicker as the system bus is not utilized as often.|$|E
50|$|Status {{requests}} {{are performed}} as READ BUFFER SCSI commands, enclosure action requests as <b>WRITE</b> <b>BUFFER</b> commands.|$|E
5000|$|<b>Write</b> <b>buffer</b> - {{a buffer}} used to {{facilitate}} apparently simultaneous reads and writes {{by the memory}} system ...|$|E
50|$|Hard disks may default {{to using}} their own {{volatile}} <b>write</b> cache to <b>buffer</b> <b>writes,</b> which greatly improves performance while introducing a potential for lost writes. (Tools such as hdparm -F will instruct the HDD controller to flush the on-drive <b>write</b> cache <b>buffer.)</b> The performance impact of turning caching off is so large that even the normally conservative FreeBSD community rejected disabling write caching by default in FreeBSD 4.3.|$|R
40|$|We {{evaluate}} the performance impact {{of two different}} <b>write</b> [...] <b>buffer</b> configurations (one word per buffer entry and one block per buffer entry) and two different write [...] policies (write [...] through and write [...] back), when using the partial block invalidation coherence mechanism [3] in a shared [...] memory multiprocessor. Using an execution [...] driven simulator, {{we find that the}} one word per entry buffer configuration with a write [...] back policy is preferred for small <b>write</b> [...] <b>buffer</b> sizes when both buffers have an equal number of data words, and when they have equal hardware cost. Furthermore, when partial block invalidation is supported, we find that a write [...] through policy is preferred over a write [...] back policy due to its simpler cache hit detection mechanism, its elimination of write [...] back transactions, and its competitive performance when the <b>write</b> [...] <b>buffer</b> is relatively large. Keywords: cache coherence, <b>write</b> [...] <b>buffer,</b> <b>write</b> [...] merging, write [...] through, shared [...] memory multiprocessors 1 Introduction O [...] ...|$|R
5000|$|Cycles {{needed to}} {{complete}} current CPU activities. To minimize those costs, microcontrollers {{tend to have}} short pipelines (often three instructions or less), small <b>write</b> <b>buffers,</b> and ensure that longer instructions are continuable or restartable. RISC design principles ensure that most instructions take {{the same number of}} cycles, helping avoid the need for most such continuation/restart logic.|$|R
50|$|A victim buffer {{is a type}} of <b>write</b> <b>buffer</b> that stores dirty evicted {{lines in}} {{write-back}} caches so that they get written back to main memory. Besides reducing pipeline stall by not waiting for dirty lines to write back as a simple <b>write</b> <b>buffer</b> does, a victim buffer may also serves as a temporary backup storage when subsequent cache accesses exhibit locality, requesting those recently evicted lines, which are still in the victim buffer.|$|E
5000|$|Shader storage buffer objects, {{allowing}} shaders to {{read and}} <b>write</b> <b>buffer</b> objects like image load/store from 4.2, but through the language rather than function calls.|$|E
50|$|In a CPU cache, a <b>write</b> <b>buffer</b> {{can be used}} to hold data {{being written}} from the cache to main memory or to the next cache in the memory hierarchy. This is a {{variation}} of write-through caching called buffered write-through.|$|E
50|$|Another example {{concerns}} some functions {{carried out}} by graphics processing units (GPUs). For instance, a GPU might be carrying out shader processing on the contents of graphics memory. It can be faster and more efficient for the GPU to take input for the shader process from read-only locations and write the shader output to different write-only locations without having to copy data between the read and <b>write</b> <b>buffers</b> after each iteration.|$|R
50|$|Other {{changes to}} the core include a 6-stage {{pipeline}} (vs. 5 on P5) with a return stack (first done on Cyrix 6x86) and better parallelism, an improved instruction decoder, 32 KB L1 cache with 4-way associativity (vs. 16 KB with 2-way on P5), 4 <b>write</b> <b>buffers</b> (vs. 2 on P5) and an improved branch predictor taken from the Pentium Pro, with a 512 entry buffer (vs. 256 on P5).|$|R
50|$|Write {{combining}} (WC) allows {{data to be}} temporarily {{stored in}} <b>write</b> combine <b>buffers</b> (WCB) and released in burst mode instead of writing single bits.|$|R
5000|$|Part of the SSD {{space is}} used as a <b>write</b> <b>buffer</b> for {{incoming}} writes. In the stable state, a minimum 4 GB space is reserved for buffering writes. A small spare area is set aside on the SSD for performance consistency.|$|E
50|$|A <b>write</b> <b>buffer</b> {{is a type}} of {{data buffer}} used in certain CPU cache {{architectures}} like Intel's x86 and AMD64. In multi-core systems, write buffers destroy sequential consistency. Some software disciplines, like C11's data-race-freedom, are sufficient to regain a sequentially consistent view of memory.|$|E
50|$|The 32-bit {{processor}} featured microprogramming control, two 256-entry caches, a 32-entry <b>write</b> <b>buffer</b> and {{the capability}} unit itself, which had 64 registers for holding evaluated capabilities. Floating point operations were available using a single 72-bit accumulator. The instruction set featured over 200 instructions, including basic ALU and memory operations, to capability- and process-control instructions.|$|E
25|$|Other {{changes to}} the core include a 6-stage {{pipeline}} (vs. 5 on P5) with a return stack (first done on Cyrix 6x86) and better parallelism, an improved instruction decoder, 32 KB L1 cache with 4-way associativity (vs. 16 KB with 2-way on P5), 4 <b>write</b> <b>buffers</b> that could now be used by either pipeline (vs. one corresponding to each pipeline on P5) and an improved branch predictor taken from the Pentium Pro, with a 512 entry buffer (vs. 256 on P5).|$|R
5000|$|... <b>writes</b> the <b>buffer</b> to {{the file}} [...] "text" [...] making ed respond with 65, {{the number of}} {{characters}} written to the file. [...] will end an ed session.|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedSACS is a cache simulator {{that provides the}} user {{with a wide range}} of timing information, in addition to providing typical information such as hit and miss rates. The SACS model includes read and <b>write</b> <b>buffers,</b> main memory, and cache memory. In addition. SACS supports a number of buffer and data forwarding policies, as well as the traditional block replacement, write. and write miss policies. SACS also includes a self-testing mode which can be used to debug the program after source-code modification[URL] United States Naval Reserv...|$|R
50|$|In addition, CPUs {{often have}} write buffers which let CPUs proceed after writes to non-cached regions. The von Neumann nature of memory is then visible when {{instructions}} are written as data by the CPU and software {{must ensure that}} the caches (data and instruction) and <b>write</b> <b>buffer</b> are synchronized before trying to execute those just-written instructions.|$|E
50|$|The IMMU and DMMU are memory {{management}} units for instructions and data, respectively. Each MMU contained a 32-entry fully associative {{translation lookaside buffer}} (TLB) that can map 4 KB, 64 KB or 1 MB pages. The <b>write</b> <b>buffer</b> (WB) has eight 16-byte entries. It enables the pipelining of stores. The bus interface unit (BIU) provided the SA-110 with an external interface.|$|E
50|$|In {{order to}} avoid the problem of read/write order {{described}} above, the <b>write</b> <b>buffer</b> can be treated as a fully associative cache and added into the memory hierarchy of the device in which it is implemented.Adding complexity slows down the memory hierarchy so this technique is often only used for memory which does not need strong ordering (always correct) like the frame buffers of video cards.|$|E
2500|$|... w text <b>writes</b> the <b>buffer</b> to {{the file}} [...] "text" [...] making ed respond with 65, {{the number of}} {{characters}} written to the file. q will end an ed session.|$|R
40|$|Superscalar {{microprocessors}} obtain {{high performance}} by exploiting parallelism at the instruction level. To effec-tively use the instruction-level parallelism found in gen-eral purpose, non-numeric code, future processors {{will need to}} speculatively execute far beyond instruction fetch limiting conditional branches. One result of this deep speculation {{is an increase in}} the number of instruction and data memory references due to the execution of mispre-dicted paths. Using a tool we developed to generate specu-lative traces from Intel architecture Unix binaries, we examine the differences in cache performance between speculative and non-speculative execution models. The results pertaining to increased memory traffic, mispre-dicted path reference effects, allocation strategies, and speculative <b>write</b> <b>buffers</b> are discussed. ...|$|R
50|$|Intel {{improved}} 16-bit code execution {{performance on}} the Pentium II, {{an area in which}} the Pentium Pro was at a notable handicap; this was achieved with the addition of segment register caches. Most consumer software of the day was still using at least some 16-bit code, because of a variety of factors. The Pentium II featured 32 KB of L1 cache, double that of the Pentium Pro, as well as deeper <b>write</b> <b>buffers</b> for a slight L1 performance increase. The Pentium II was also the first P6-based CPU to implement the Intel MMX integer SIMD instruction set which had already been introduced on the Pentium MMX.|$|R
50|$|The DECstation 3100 and 2100 uses a R2000 processor, a R2010 {{floating}} point coprocessor and four R2020 write buffers. The R2000 uses an external 64 KB direct-mapped instruction cache and a 64 KB direct-mapped write-through data cache with a cache line size of four bytes. Four R2020 implement a four-stage <b>write</b> <b>buffer</b> {{to improve performance}} by permitting the R2000 to write to its write-through data cache without stalling.|$|E
5000|$|Mudge was {{responsible}} for early research into a type of security vulnerability known as the buffer overflow. In 1995 he published [...] "How to <b>Write</b> <b>Buffer</b> Overflows", {{one of the first}} papers on the topic. He published some of the first security advisories and research demonstrating early vulnerabilities in Unix such as code injection, side-channel attacks, and information leaks, and was a leader in the full disclosure movement. He was the initial author of security tools L0phtCrack, AntiSniff, and l0phtwatch.|$|E
50|$|Since the caches {{intermediate}} accesses {{to memory}} addresses, data written to different addresses may reach the peripherals' memory or registers {{out of the}} program order, i.e. if software writes data to an address and then writes data to another address, the cache <b>write</b> <b>buffer</b> does not guarantee that the data will reach the peripherals in that order. It {{is the responsibility of the}} software to include memory barrier instructions after the first write, to ensure that the cache buffer is drained before the second write is executed.|$|E
50|$|Write {{combining}} (WC) is {{a computer}} bus technique for allowing data to be combined and temporarily stored in a <b>buffer</b> — the <b>write</b> combine <b>buffer</b> (WCB) — to be released together later in burst mode instead of writing (immediately) as single bits or small chunks.|$|R
40|$|Multiple {{instruction}} rollback (MIR) is {{a technique}} that has been implemented in mainframe computers to provide rapid recovery from transient processor failures. Hardware-based MIR designs eliminate rollback data hazards by providing data redundancy implemented in hardware. Compiler-based MIR designs have also been developed which remove rollback data hazards directly with data-flow transformations. This paper describes compiler-assisted techniques to achieve multiple instruction rollback recovery. We observe that some data hazards resulting from instruction rollback can be resolved efficiently by providing an operand read buffer while others are resolved more efficiently with compiler transformations. The compiler-assisted scheme presented consists of hardware that is less complex than shadow files, history files, history <b>buffers,</b> or delayed <b>write</b> <b>buffers,</b> while experimental evaluation indicates performance improvement over compiler-based schemes...|$|R
40|$|Modem high {{performance}} disk systems make {{extensive use of}} non-volatile RAM (NVRAM) write caches. A single-copy NVRAM cache creates a {{single point of failure}} while a dual-copy NVRAM cache is very expensive because of the high cost of NVRAM. This paper presents a new cache architecture called RAPID-Cache for Redundant, Asymmetrically Parallel, and Inexpensive Disk Cache. A typical RAPID-Cache consists of two redundant <b>write</b> <b>buffers</b> on top of a disk system. One of the buffers is a primary cache made of RAM or NVRAM and the other is a backup cache containing a two level hierarchy: a small NVRAM buffer on top of a log disk. The small NVRAM <b>buffer</b> combines small <b>write</b> data and writes them into the log disk in large sizes. B...|$|R
