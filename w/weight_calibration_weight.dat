0|169|Public
40|$|In survey statistics, {{the usual}} {{technique}} for estimating a population total consists in summing appropriately weighted variable {{values for the}} units in the sample. Different weighting systems exit: sampling <b>weights,</b> GREG <b>weights</b> or <b>calibration</b> <b>weights</b> for example. In this article, we propose to use the inverse of conditional inclusion probabilities as weighting system. We study examples where an auxiliary information enables to perform an a posteriori stratification of the population. We show that, in these cases, exact computations of the conditional weights are possible. When the auxiliary information consists in the knowledge of a quantitative variable for all the units of the population, then we show that the conditional weights can be estimated via Monte-Carlo simulations. This method is applied to outlier and strata-Jumper adjustments...|$|R
3000|$|... dev, {{from the}} {{development}} database {{are used to}} train the <b>calibration</b> <b>weights,</b> i.e. the intercept and regression coefficient of the logistic regression model, and subsequently these <b>calibration</b> <b>weights</b> can then be used to calibrate scores from the test database. The pooled procedure for calculating the <b>calibration</b> <b>weights</b> was adopted (refer to [19] for details) in this paper. For a detailed tutorial on logistic regression calculation in converting a score to an interpretable likelihood ratio, refer to [12].|$|R
5000|$|Kott, P. (2006). Using <b>calibration</b> <b>weighting</b> {{to adjust}} for nonresponse and {{coverage}} errors. Survey Methodology, 133-142 ...|$|R
50|$|A mass used to {{calibrate}} a weighing scale {{is sometimes called}} a <b>calibration</b> mass or <b>calibration</b> <b>weight.</b>|$|R
40|$|<b>Calibration</b> <b>weighting</b> is a {{methodology}} under which probability-sample weights are adjusted {{in such a}} way that when applied to survey data they can produce model-unbiased estimators for a number of different target variables. This paper briefly reviews the history of <b>calibration</b> <b>weighting</b> before the term was coined and some major developments since then. A change in the definition of a calibration estimator is recommended. This change expands the class to include such special cases as, 1, randomization-optimal estimators, and, 2, randomization-consistent estimators incorporating local polynomial regression. Although originally developed as a method for reducing sampling errors, <b>calibration</b> <b>weighting</b> has also been applied to adjust for unit nonresponse and for coverage errors. A variant of the jackknife variance estimator proposed here should prove computationally convenient for these applications...|$|R
5000|$|Kott, P., & Chang, T. (2010). Using <b>calibration</b> <b>weighting</b> {{to adjust}} for nonignorable unit nonresponse. Journal of the American Statistical Association, 105, 1265-1275.|$|R
40|$|In the {{introductory}} chapter {{of this work}} is caught organizational structure of the national metrology system in the Czech Republic and its links to international organizations. There is indicated the basic terminology of metrology, particularly {{in the area of}} classification instruments. The following sections approaching the issue of measurement uncertainties, their classification, sources of uncertainty determined by the type A and B, their specifics and calculation. The above linked area already dealing with themselves calibrations, first of all <b>calibration</b> <b>weights,</b> classification of weights according accuracy classes, established procedures, and finally determining uncertainty in <b>calibration</b> <b>weights.</b> Then, immediately followed by a chapter dealing with calibration balances, performed tests and measurement uncertainties. The main part is of course directed towards the application of acquired knowledge to practical examples, thus performing the <b>calibration</b> <b>weight</b> class F 2 using a high-precision weights, both in the premises of the Technical University in Brno, both in the laboratory weighing the Czech Metrological Institute. Further calibration was performed school balances Ohaus Explorer EX 224...|$|R
40|$|The work is {{justified}} {{from the standpoint}} of system analysis using multimedia factors to improve rehabilitation patients at different stages of the disease. The first time the formation of the methodology vector priorities specialists in various fields of <b>calibration</b> <b>weight</b> judgments are used according to their professional expertise...|$|R
40|$|In {{the paper}} main {{approaches}} of rich people identification are analyzed, the classifications of estimation methods of mentioned social group are given. The method of rich threshold {{identification with the}} use of Household Living Conditions Survey Data and System of National Accounts Data by the method of statistical <b>weight</b> <b>calibration</b> is proposed...|$|R
40|$|Weighting {{procedures}} are commonly applied in surveys {{to compensate for}} nonsampling errors such as nonresponse errors and coverage errors. Two types of weight-adjustment {{procedures are}} commonly used {{in the context of}} unit nonresponse: (i) nonresponse propensity <b>weighting</b> followed by <b>calibration,</b> also known as the two-step approach and (ii) nonresponse <b>calibration</b> <b>weighting,</b> also known as the one-step approach. In this article, we discuss both approaches and warn against the potential pitfalls of the one-step procedure. Results from a simulation study, evaluating the properties of several point estimators, are presented...|$|R
3000|$|... [...]), and {{molecular}} weight distribution (MWD) were determined using a Waters GPC instrument (Milford, MA, USA) equipped with a 2414 differential refractive index detector and two (HR 2 and HR 4) Waters μ-Styragel columns. Spectral grade THF {{was used as an}} eluent at a flow rate of 1.0  ml min− 1, and the molecular <b>weight</b> <b>calibrations</b> were carried out using polystyrene standards.|$|R
40|$|Background and Purpose. The {{purpose of}} this study was to {{evaluate}} the reli-ability cf measurements of weight distribution among the wheels of wheelchairs using a commercial balance testing system. Reliable data may be useful in the wheelchair evaluation and adjustment process. Subjects. Three male full-time manual wheelchair users aged 30, 26, and 2 7 years with cervical spinal cord injuries 7. 5, 65, and 10 years in duration participated. Metbods. <b>Calibration</b> <b>weights,</b> unoccupied wheelchairs, and occupied wheelchairs were repeatedly placed o n the force transducers of the balance testing system to obtain measure-ments o f weight distribution. Results The intraclass correlation coeficients of the measurements were. 99 for <b>calibration</b> <b>weights,.</b> 96 for unoccupied wheelchairs, and. 98 for wheelchairs occupied by the subjects. Conclusion and Discussion. The described use of this instrumentation appears to generate reliable measure-ments o f static weight distribution. With further testing, this system may provide useful information related to manual wheelchair prescription and adjustment...|$|R
40|$|Sample <b>weight</b> <b>calibration,</b> also {{referred}} to as calibration estimation, is a widely applied technique in the analysis of survey data. This method borrows strength from a set of auxiliary variables and can produce weighted estimates with smaller mean square errors than those estimators that do not use the calibration adjustments. Poststratification is a well-known calibration method that forces weighted counts within cells generated by cross-classifying the categorical (or categorized) auxiliary variables to equal the corresponding population control totals. Several assumptions are critical to the theory developed to date for <b>weight</b> <b>calibration.</b> Two assumptions relevant to this research include: (i) the control totals calculated from the population of interest and known without (sampling) error; and (ii) the sample units selected for the survey are taken from a sampling frame that completely covers the population of interest (e. g., no problems with frame undercoverage). With a few exceptions, research to date generally is conducted as if these assumptions hold, or that any violation does not affect estimation. Our researc...|$|R
40|$|Fractional {{hot deck}} imputation, {{considered}} in Fuller and Kim (2005), is extended to multivariate missing data. The joint {{distribution of the}} study items is nonparametrically estimated using a discrete approximation, where the discrete transformation also serves to define imputation cells. The procedure first estimates the probabilities for the cells and then imputes real observations for missing items. <b>Calibration</b> <b>weighting</b> is used to reduce the imputation variance. Replication variance estimation is discussed...|$|R
40|$|Sometimes {{benchmark}} constraints in a calibration problem {{cannot be}} met {{if there are}} range restric-tions on the <b>calibration</b> <b>weights.</b> There are various approaches to this problem that involve either allow-ing the benchmark constraints to be adjusted within a specified tolerance, or to determine a minimal lin-ear adjustment of the benchmark constraints. In this paper we propose an optimization problem that explicitly incorporates into the objective function {{a measure of the}} amount by which the benchmark con-straints are missed...|$|R
40|$|Given a {{randomly}} drawn sample, <b>calibration</b> <b>weighting</b> {{can provide}} double {{protection against the}} selection bias resulting from unit nonresponse. This means that if either an assumed linear prediction model or an implied unit selection model holds, the resulting estimator will be asymptotically unbiased in some sense. The functional form of the selection model when using linear alibration adjustment is dubious. The authors discuss an alternative, nonlinear calibration-weighting procedure and software that can, among other things, implicitly estimate a logistic-response model. " (author's abstract...|$|R
40|$|We {{extend the}} problem of obtaining an {{estimator}} for the finite population mean parameter incorporating complete auxiliary information through calibration estimation in survey sampling but considering a functional data framework. The functional <b>calibration</b> sampling <b>weights</b> of the estimator are obtained by matching the calibration estimation problem with the maximum entropy on the mean principle. In particular, the calibration estimation is viewed as an infinite dimensional linear inverse problem following {{the structure of the}} maximum entropy on the mean approach. We give a precise theoretical setting and estimate the functional <b>calibration</b> <b>weights</b> assuming, as prior measures, the centered Gaussian and compound Poisson random measures. Additionally, through a simple simulation study, we show that our functional calibration estimator improves its accuracy compared with the Horvitz-Thompson estimator...|$|R
40|$|Parametric {{fractional}} imputation {{is proposed}} {{as a general}} tool for missing data analysis. Using fractional weights, the observed likelihood can be approximated by the weighted mean of the imputed data likelihood. Computational efficiency can be achieved using the idea of importance sampling and <b>calibration</b> <b>weighting.</b> The proposed imputation method provides efficient parameter estimates for the model parameters specified in the imputation model and also provides reasonable estimates for parameters that {{are not part of}} the imputation model. Variance estimation is discussed and results from a limited simulation study are presented. Copyright 2011, Oxford University Press. ...|$|R
40|$|Main abstract: Fluctuation scaling {{reports on}} all {{processes}} producing a data set. Some fluctuation scaling relationships, {{such as the}} Horwitz curve, follow exponential dispersion models which have useful properties. The mean-variance method applied to Poisson distributed data is a special case of these properties allowing the gain of a system to be measured. Here, a general method is described for investigating gain (G), dispersion (β), and process (α) in any system whose fluctuation scaling follows a simple exponential dispersion model, a segmented exponential dispersion model, or complex scaling following such a model locally. When gain and dispersion cannot be obtained directly, relative parameters, GR and βR, may be used. The method was demonstrated on data sets conforming to simple, segmented, and complex scaling. These included mass, fluorescence intensity, and absorbance measurements and specifications for classes of <b>calibration</b> <b>weights.</b> Changes in gain, dispersion, and process were observed in the scaling of these data sets in response to instrument parameters, photon fluxes, mathematical processing, and <b>calibration</b> <b>weight</b> class. The process parameter which limits the type of statistical process that can be invoked to explain a data set typically exhibited 0 4 possible. With two exceptions, <b>calibration</b> <b>weight</b> class definitions only affected β. Adjusting photomultiplier voltage while measuring fluorescence intensity changed all three parameters (0 <α< 0. 8; 0 <βR< 3; 0 <GR< 4. 1). The method provides a framework for calibrating and interpreting uncertainty in chemical measurement allowing robust compar ison of specific instruments, conditions, and methods. Supporting information abstract: On first inspection, fluctuation scaling data may appear to approximate a straight line when log transformed. The data presented in figure 5 of the main text gives a reasonable approximation to a straight line and for many purposes this would be sufficient. The {{purpose of the study}} of fluorescence intensity was to determine whether adjusting the voltage of a photomultiplier tube while measuring a fluorescent sample changes the process (α), the dispersion (β) and/or the gain (G). In this regard, the linear model established that PMT setting affects more than the gain. However, a detailed analysis beginning with testing for model mis-specification provides additional information. Specifically, Poisson behavior is only seen over a limited wavelength range in the 600 V and 700 V data sets...|$|R
40|$|Abstract — Problems {{associated}} with the characterization by size exclusion chranatography (SEC) of complex polymers, polymers for which a unique relationship between size and molecular weight does not exist, are considered. An attempt is made to develop a general methodology for the SEC of these polymers. This includes generalized universal molecular <b>weight</b> <b>calibration,</b> corrections for imperfect resolution {{and the use of}} multidetector systems, including low angle laser light scattering photo— metry, viscometry and UV and IR spectrophotcinetry. Included are examples of the estimation of branching frequency and sequence length across the SEC chrcinatograns of branched hc*nopoliners and copolners...|$|R
40|$|We {{show that}} the pseudo {{empirical}} maximum likelihood estimator can be recast as a calibration estimator. The process of estimating the probabilities pk of the distribution function can be done also in a maximum entropy framework. We suggest that a minimum cross-entropy estimator has attractive theoretical properties. A Monte Carlo simulation suggests that this estimator outperforms the PEMLE and the Horvitz-Thompson estimator. This is a joint SALDRU/DataFirst Working Paper {{as part of the}} Mellon Data Quality Project. For more information about the project visit www. datafirst. uct. ac. za. sample <b>weights,</b> <b>calibration,</b> pseudo-empirical maximum likelihood estimation, cross entropy...|$|R
40|$|This brief {{analyzes}} {{the effect of}} capacitor variation {{on the design of}} high-resolution nonbinary-weighted successive-approximation-register analog-to-digital converters in terms of radix, conversion steps, and accuracy. Moreover, the limitation caused by the one-side redundancy of the nonbinary-weighted network is addressed and a corresponding solution with a mathematical derivation is provided. In order to relax the mismatch requirement on the capacitor sizing while still ensuring enough linearity, a bottom-up <b>weight</b> <b>calibration</b> technique accounting for noise and offset errors is proposed, and its effectiveness is demonstrated. This calibration approach can be easily incorporated into a charge-redistribution converter without modifying its main architecture and conversion sequence...|$|R
50|$|Phillip S. Kott (born 1952) is an American statistician. He {{has worked}} in the field of survey {{statistics}} for more than 25 years, and is regarded as a leader in this field. His areas of expertise include survey sampling design, analysis of survey data, and <b>calibration</b> <b>weighting,</b> among other areas. He revolutionized sampling design and estimation strategies with the Agricultural Resource Management Survey, which uses survey information more efficiently. He has taught at George Mason University, and USDA Graduate School. He is currently an Associate Editor for the Journal of Official Statistics and the scientific journal Survey Methodology.|$|R
40|$|Although {{survey data}} are {{sometimes}} weighted by their selection weights, {{it is often}} preferable to use auxiliary information available on the whole population to improve estimation. <b>Calibration</b> <b>weighting</b> (Deville and Sarndal, 1992, Journal of the American Statistical Association 87 : 376 - 382) {{is one of the}} most common methods of doing this. This method adjusts the selection weights so that known population totals for the auxiliary variables are reproduced exactly, while ensuring that the calibrated weights are as close as possible to the original sampling weight. The simplest example of calibration is poststratification. This is the special case where the auxiliary variable is a single categorical variable. General calibration extends this to deal with more than one auxiliary variable and allows the user to include both categorical and numerical variables. A typical example might occur in a population survey, where the selection weights could be calibrated to ensure that the sample weighted by the <b>calibration</b> <b>weights</b> has exactly the same distribution as the population on variables such as age, sex, and region. Many packages have routines for calibration. SAS has the macro CALMAR; GenStat has the procedure SVCALIBRATE; and R has the function calibrate. However, no such routine is publicly available in Stata. I will introduce a user-written Stata program for calibration and will also discuss a simple extension to show how it can incorporate a nonresponse correction. I will also briefly discuss the program's strengths and limitations when compared to rival packages. ...|$|R
40|$|Quantitative expert {{judgement}} {{is used in}} many areas of risk analysis to provide assessments of uncertainty. A leading method is Cooke’s classical model, which provides a way of weighting experts depending on their performance in answering so-called calibration questions. The moment method for {{expert judgement}} combination is an alternative to Cooke’s method which provides a different way of calibrating experts. It has better theoretical properties in that it uses a strictly proper scoring rule, that is, an expert who wishes to maximise his score can only do so by stating what he actually believes. The method also requires the specification of <b>weights</b> for the <b>calibration</b> process. In this paper we consider a special case of the moment model that scores location and variability assessments. We provide an approach to setting the <b>calibration</b> <b>weights</b> and repeat the comparison of this moment model with Cooke’s classical model we conducted earlier for this special case...|$|R
40|$|Purpose To achieve high {{temporal}} and spatial resolution for contrast-enhanced time-resolved MR angiography exams (trMRAs), fast imaging techniques such as non-Cartesian parallel imaging must be used. In this study, the three-dimensional (3 D) through-time radial generalized autocalibrating partially parallel acquisition (GRAPPA) method is used to reconstruct highly accelerated stack-of-stars data for time-resolved renal MRAs. Materials and Methods Through-time radial GRAPPA has been recently introduced as a method for non-Cartesian GRAPPA <b>weight</b> <b>calibration,</b> and a similar concept {{can also be used}} in 3 D acquisitions. By combining different sources of calibration information, acquisition time can be reduced. Here, different GRAPPA <b>weight</b> <b>calibration</b> schemes are explored in simulation, and the results are applied to reconstruct undersampled stack-of-stars data. Results Simulations demonstrate that an accurate and efficient approach to 3 D calibration is to combine a small number of central partitions with as many temporal repetitions as exam time permits. These findings were used to reconstruct renal trMRA data with an in-plane acceleration factor as high as 12. 6 with respect to the Nyquist sampling criterion, where the lowest root mean squared error value of 16. 4 % was achieved when using a calibration scheme with 8 partitions, 16 repetitions, and a 4 projection ⁽×⁾ 8 read point segment size. Conclusion 3 D through-time radial GRAPPA can be used to successfully reconstruct highly accelerated non-Cartesian data. By using in-plane radial undersampling, a trMRA can be acquired with a temporal footprint less than 4 s/frame with a spatial resolution of approximately 1. 5 mm ⁽×⁾ 1. 5 mm ⁽×⁾ 3 mm...|$|R
40|$|An {{experiment}} was designed (a) {{to examine the}} weight losses caused by Sitophilus feeding on maize cultures which contained different proportions (0 %, 5 % and 15 %) of broken grains and (b) to assess and compare the efficiency of different weight loss assessment methods on the different cultures. The weight loss assessment methods examined were the Count and Weigh, the Converted Percentage Damage, the Simple and Multiple Thousand Grain Mass (TGM) methods and the Standard Volume Weight (SVW) methods by direct comparison and by reference to a baseline <b>calibration.</b> <b>Weight</b> losses in cultures containing different proportions of broken grains were not significantly different. Further work is recommended using pests with secondary status...|$|R
40|$|An X-ray {{fluorescence}} spectrometry (XRF) {{method to}} determine 10 major and 18 trace elements in silicate rocks has been investigated. The analysis {{is carried out}} on fused glass beads, containing one part rock powder, two parts of lithium metaborate/tetraborate flux and 0. 3 parts of lithium n itrate by <b>weight.</b> <b>Calibration</b> lines were established using international silicate rock reference materials. The low dilution fused glass technique effectively eliminates particle size effect, and allows accurate determination of both major and trace elements from single glass beads. Analyses of standard rocks agree well with the recommended values of Imai et al. (1995) for GSJ reference materials, and with those of Potts et al. (1992) for USGS standards...|$|R
40|$|Calibration estimation, {{which can}} be roughly {{described}} as adjusting the original design weights to incorporate the known population totals of the auxiliary variables, has become very popular in sample surveys. The <b>calibration</b> <b>weights</b> are chosen to minimize a given distance measure while satisfying a set of constraints related to the auxiliary variable information. Under simple random sampling, Chen and Qin (1993) suggested that the calibration estimator maximizing the constrained empirical likelihood can make efficient use of the auxiliary variables. We extend the result to unequal probability sampling and propose an algorithm to implement the proposed procedure. Asymptotic properties of the proposed calibration estimator are discussed. The proposed method is extended to the stratified sampling. Results from a limited simulation study are presented...|$|R
40|$|This paper {{focuses on}} the {{interaction}} between sample design, data collection mode and nonresponse handling of the ISAE consumer survey, as these factors may affect sample efficiency. For calculating sample weights and obtaining more reliable estimates, the calibration methodology has been adopted. Outcomes seem encouraging inasmuch they confirm the survey quality. In fact, the differences between calibrated and unweighted series are very small even they are not randomly distributed. From the EDP point of view, the new processing system utilises new technologies and data maintenance support. The renewed procedure allows wider aggregation capabilities, offering new sectoral/regional breakdowns. The new microdata database, {{which is part of}} the complete EDP project, also enables to carry out microdata analysis. Complex sample design, <b>weighting,</b> <b>calibration,</b> post-stratification, non-responses. ...|$|R
40|$|The use of nonresponse <b>calibration</b> <b>weighting</b> is {{considered}} {{in a complete}} design-based frameworkto account for the cases in which nonresponse is a fixed characteristic of the units, just like the interest variable. Approximate expressions of design-based bias and variance of the calibration estimator are derived and some estimators of the sampling variance are proposed. The choice of auxiliary variables is discussed from theoretical and practical point of view. The results of an extensive simulation study demonstrate how {{the reliability of the}} procedure is mainly determined by the capability of selecting auxiliary variables {{in such a way that}} their relationship with the interest variable is similar for both the respondent and nonrespondent sub-populations. auxiliary variables, calibration estimator, variance estimator, simulation study. ...|$|R
30|$|In {{order to}} elicit a stretch reflex {{the force of}} the pull and {{subsequent}} strech velocity of the tibial translation must be sufficient to activate the proprioceptors. The velocity of the pulls was generated as fast as possible by the tester (JA). For the force magnitude, Friemert et al. (2005 b) demonstrated that a force of[*]≥[*] 140  N and stretch velocity of 30  mm/s would increase the chance to elicit stretch reflex to 100  %. In a pilot experiment, the force of pull on the handle of the KT- 2000 measured using a strain gauge. The force output of the KT- 2000 was determined statically with <b>calibration</b> <b>weights</b> within the range of forces expected to be generated by the user. Similarly, the position output of the KT- 2000 was calibrated using ceramic test gauge blocks.|$|R
40|$|Credit risk models used in {{quantitative}} {{risk management}} treat credit risk analysis conceptually like {{a single person}} decision problem. From this perspective an exogenous source of risk drives the fundamental parameters of credit risk: probability of default, exposure at default and the recovery rate. In reality these parameters {{are the result of}} the interaction of many market participants: They are endogenous. The authors develop a general equilibrium model with endogenous credit risk that can be viewed {{as an extension of the}} capital asset pricing model. They analyze equilibrium prices of securities as well as equilibrium allocations in the presence of credit risk. The authors use the model to discuss the conceptual underpinnings of the approach to risk <b>weight</b> <b>calibration</b> for credit risk taken by the Basel Committee. Credit Risk, Endogenous Risk, Systemic Risk, Banking Regulation...|$|R
40|$|Weighting samples is {{important}} to reflect not only sample design decisions made at the planning stage, but also practical issues that arise during data collection and cleaning that necessitate weighting adjustments. Adjustments to base weights are used to account for these planned and unplanned eventualities. Often these adjustments lead to variations in the survey weights from the original selection weights (i. e., the weights {{based solely on the}} sample units' probabilities of selection). Large variation in survey weights can cause inferential problems for data users. A few extremely large weights in a sample dataset can produce unreasonably large estimates of national- and domain-level estimates and their variances in particular samples, even when the estimators are unbiased over many samples. Design-based and model-based methods have been developed to adjust such extreme weights; both approaches aim to trim weights such that the overall mean square error (MSE) is lowered by decreasing the variance more than increasing the square of the bias. Design-based methods tend to be ad hoc, while Bayesian model-based methods account for population structure but can be computationally demanding. I present three research papers that expand the current weight trimming approaches under the goal of developing a broader framework that connects gaps and improves the existing alternatives. The first paper proposes more in-depth investigations of and extensions to a newly developed method called generalized design-based inference, where we condition on the realized sample and model the survey weight {{as a function of the}} response variables. This method has potential for reducing the MSE of a finite population total estimator in certain circumstances. However, there may be instances where the approach is inappropriate, so this paper includes an in-depth examination of the related theory. The second paper incorporates Bayesian prior assumptions into model-assisted penalized estimators to produce a more efficient yet robust calibration-type estimator. I also evaluate existing variance estimators for the proposed estimator. Comparisons to other estimators that are in the literature are also included. In the third paper, I develop summary- and unit-level diagnostic tools that measure the impact of variation of weights and of extreme individual weights on survey-based inference. I propose design effects to summarize the impact of variable <b>weights</b> produced under <b>calibration</b> <b>weighting</b> adjustments under single-stage and cluster sampling. A new diagnostic for identifying influential, individual points is also introduced in the third paper...|$|R
40|$|A {{new snow}} layer probing device with a {{rectangular}} section {{is described in}} the paper. This device allows receiving snow samples with the undisturbed structures of snow layers, and more accurately reflects the actual value of the snow water equivalent. Application of a new snow probing device in snow measurements provides {{a new way to}} organize the monitoring dynamics of snow density layer variability, to estimate density of certain snow types in the snow cover, including the total or vertically averaged one, and to visualize the features of the snow stratigraphy structure. Snow layer probing device with a rectangular section as opposed to the cylindrical weight snow devices has constructive advantages. It also can be used in the <b>calibration</b> <b>weight</b> snow measurement devices that will ensure the comparability of the snow shooting results made in different climatic zones. </p...|$|R
40|$|Abstract: Work {{with sample}} surveys often makes {{extensive}} use of measures of size. Two prominent examples are the use of “probability proportional to size ” sampling; and use of size measures in adjustment of survey weights through, e. g., ratio estimation, post-stratification or <b>calibration</b> <b>weighting.</b> However, many survey applications use size variables that are imperfect approximations to the idealized size measures that would produce optimal efficiency results. This paper explores the effects that alternative size measures may have on the efficiency of some standard design-estimator pairs. Principal {{emphasis is placed on}} numerical results of a simulation study that uses size measures and economic variables available through the Quarterly Census of Employment and Wages of the Bureau of Labor Statistics. Key words: measures of size; ratio estimation; regression estimation; sampling with probabilities proportional to size; unequal-probability sampling. 1...|$|R
