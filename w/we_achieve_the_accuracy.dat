3|10000|Public
40|$|VALLEX is a valency lexicon of Czech verbs. We briefly {{introduce}} VALLEX {{and then}} describe {{and evaluate the}} VALEVAL experiment: annotation of 10256 corpus instances of 109 Czech verbs with valency frames. The inter-annotator agreement of three parallel annotations ranges from 61 % to 74 % and κ from 0. 52 to 0. 62. More than 8000 sentences are now available as the “golden VALEVAL ” for word-sense disambiguation experiments. In out first attempts using morphological and syntax-based information, <b>we</b> <b>achieve</b> <b>the</b> <b>accuracy</b> of 70 % to 80 %. ...|$|E
40|$|Artificial Neural Networks (ANNs) {{have been}} widely {{advocated}} as tools for solving many decision modeling problems. In this paper, we use ANNs for the prediction of coronary artery disease. Real data from four major international medical organizations {{are used in the}} training and testing of the ANN algorithm. To speed up the training time, we implemented the algorithm in parallel on an Intel Paragon parallel computer. We have achieved an accuracy of ? 76 %, a comparable performance to probabilistic and statistical techniques. Furthermore, with parallel implementation, <b>we</b> <b>achieve</b> <b>the</b> <b>accuracy</b> in ! 5 minutes of training time. Comparing with statistical approach, such savings in time is substantial. We, therefore, conclude that ANN is a fast alternative to classical statistical techniques for prediction and modeling of experimental data. Two popular weight-adaptation algorithms, RPROP and Delta-Bar-Delta rules are compared. The effect of network architecture and how to treat missing values [...] ...|$|E
40|$|Recent {{advances}} in spaceborne GPS technology have shown significant advantages in many aspects over conventional technologies. For instance, spaceborne GPS can realize autonomous orbit determination with significant savings in spacecraft life cycle, in power, and in mass. At present, the onboard orbit determination {{in real time}} or near-real time can typically achieve 3 D orbital accuracy of metres to tens metres with Kalman filtering process, but 21 st century space engineering requires onboard orbit accuracy of better than 5 metres, and even sub-metre for some space applications. The research focuses {{on the development of}} GPS-based autonomous orbit determination techniques for spacecraft. Contributions are made to the field of GPS-based orbit determination in the following five areas: Techniques to simplify the orbital dynamical models for onboard processing have been developed {{in order to reduce the}} computional burden while retaining full model accuracy. The Earth gravity acceleration approximation method was established to replace the traditional recursive acceleration computations. Results have demonstrated that with the computation burden for a 55 × spherical harmonic gravity model, <b>we</b> <b>achieve</b> <b>the</b> <b>accuracy</b> of a 7070 × model. Efforts were made for the simplification of solar & lunar ephemerides, atmosphere density model and orbit integration. All these techniques together enable a more accurate orbit integrator to operate onboard. Efficient algorithms for onboard GPS measurement outlier detection and measurement improvement have been developed. In addition, a closed-form single point position method was implemented to provide an initial orbit solution without any a priori information. The third important contribution was made to the development of sliding-window short-arc orbit filtering techniques for onboard processing. With respect to the existing Kalman recursive filtering, the short-arc method is more stable because more measurements are used. On the other hand, the short-arc method requires less accurate orbit dynamical model information compared to the long-arc method, thus it is suitable for onboard processing. Our results have demonstrated that by using the 1 2 revolutions of LEO code GPS data we can achieve an orbit accuracy of 1 2 metres. Sliding-window techniques provide sub-metre level orbit determination solutions with 5 20 minutes delay. A software platform for the GPS orbit determination studies has been established. Methods of orbit determination in near-real time have been developed and tested. The software system includes orbit dynamical modelling, GPS data processing, orbit filtering and result analysis modules, providing an effective technical basis for further studies. Furthermore a ground-based near-real time orbit determination system has been established for FedSat, Australia's first satellite in 30 years. The system generates 10 -metre level orbit solution with half-day latency on an operational basis. This system has supported the scientific missions of FedSat such as Ka-band tracking and GPS atmosphere studies within the Cooperative Research Centre for Satellite System (CRCSS) community. Though it is different from the onboard orbit determination, it provides important test-bed for the techniques described in previous section. This thesis focuses on the onboard orbit determination techniques that were discussed in Chapter 2 through Chapter 6. The proposed onboard orbit determination algorithms were successfully validated using real onboard GPS data collected from Topex/Poseidon, CHAMP and SAC-C satellites...|$|E
40|$|Cough is {{the most}} common symptom of the several {{respiratory}} diseases containing diagnostic information. It is the best suitable candidate to develop a simplified screening technique for the management of respiratory diseases in timely manner, both in developing and developed countries, particularly in remote areas where medical facilities are limited. However, major issue hindering the development is the non-availability of reliable technique to automatically identify cough events. Medical practitioners still rely on manual counting, which is laborious and time consuming. In this paper we propose a novel method, based on the neural network to automatically identify cough segments, discarding other sounds such a speech, ambient noise etc. <b>We</b> <b>achieved</b> <b>the</b> <b>accuracy</b> of 98...|$|R
40|$|Detecting the {{position}} {{and the level of}} joint impingement is often a key to a computer-aided surgical plan to normalize joint kinematics. So far for ball-and-socket joints most of the existing impingement detection methods are not efficient or only consider the collided points as the detection result. In this thesis, we present two efficient and accurate collision detection systems to detect and evaluate the impingement on ball-and-socket joints. Through the first system, we rapidly non-uniformly sample the object surface in the spherical coordinate system based on polygon rasterization and then globally check the distance differences between the objects to approximately estimate the impingement level; the second system has a similar design but is based on a uniform sampling method which can sample the object more adequately and reduce the memory cost by 20 %. <b>We</b> <b>achieved</b> <b>the</b> <b>accuracy,</b> <b>the</b> efficiency, and also the feature of providing overall impingement estimation and visualization on the joint surfaces. The first system is further applied in a real-time ROM observation simulation to flexibly check the location and the level of the impingement occurred during the motion...|$|R
40|$|Abstract—We compare three {{different}} methods of Word Sense Disambiguation {{applied to the}} disambiguation of a selected set of 13 Polish words. The selected words express different problems for sense disambiguation. As {{it is hard to}} find works for Polish in this area, our goal was to analyse applicability and limitations of known methods in relation to Polish and Polish language resources and tools. The obtained results are very positive, as using limited resources, <b>we</b> <b>achieved</b> <b>the</b> <b>accuracy</b> of sense disambiguation greatly exceeding the baseline of the most frequent sense. For the needs of experiments a small corpus of representative examples was manually collected and annotated with senses drawn from plWordNet. Different representations of context of word occurrences were also experimentally tested. Examples of limitations and advantages of the applied methods are discussed. I...|$|R
40|$|Part of Speech tagging in Indian Languages {{is still}} an open problem. We still lack a clear {{approach}} in implementing a POS tagger for Indian Languages. In this paper we describe our efforts to build a Hidden Markov Model based Part of Speech Tagger. We have used IL POS tag set {{for the development of}} this tagger. <b>We</b> have <b>achieved</b> <b>the</b> <b>accuracy</b> of 92 %...|$|R
40|$|Abstract We {{describe}} our {{participation in}} the Author Profiling task of the PAN 2013 competition. The task objective {{is to determine the}} age and the gender of an author of a document. We applied the Common N-Gram (CNG) classifier (Kešelj et al., 2003) to this task. The CNG classifier uses a dissimilarity measure based on the differences in the frequencies of the character n-grams that are most common in the considered documents. To train the classifier, a class is represented by one class document created by concatenating the training documents belonging to the class. A sample document is labelled by the class with the minimum dissimilarity. For the six class classification (combinations of two possible gender labels and three possible age labels) <b>we</b> <b>achieved</b> <b>the</b> <b>accuracy</b> of 0. 2814 on the English test dataset and 0. 2592 on the Spanish test dataset. Our results are below the medians {{of the results of the}} competition participants. ...|$|R
40|$|We {{develop a}} {{technique}} for probing harmonic {{measure of the}} diffusion limited aggregation (DLA) cluster surface with the variable size particle and generate one thousand clusters with 50 million particles using original off-lattice killing-free algorithm. Taking, in sequence, the limit of the vanishing size of the probing particles and then sending the growing cluster size to infinity, <b>we</b> <b>achieve</b> <b>the</b> unprecedented <b>accuracy</b> determining <b>the</b> fractal dimension, D= 1. 7100 (2) crucial to characterization of geometric properties of the DLA clusters. Comment: 4 pages, 5 figure...|$|R
40|$|The {{goal of this}} {{bachelor}} {{thesis is}} to create an overview of modern tools and machines for drilling process. The detailed analysis of the economic aspects of the drilling process was created at particular problem. The technology of drilling methods was modernized. <b>We</b> can <b>achieve</b> <b>the</b> <b>accuracy</b> comparable with finishing method of machining. Another advance of this method is high productivity of machining which is connected with the need of more powerful machines and tools. In addition, this thesis contains descriptions of machines from wide spectrum of producers...|$|R
40|$|Part 2 : Authentication and AuthorizationInternational audienceIn this paper, {{we propose}} a novel gait {{authentication}} mechanism by mining sensor resources on mobile phone. Unlike previous works, both built-in accelerometer and magnetometer {{are used to}} handle mobile installation issues, including {{but not limited to}} disorientation, and misplacement errors. The authentication performance is improved by executing deep examination at pre-processing steps. A novel and effective segmentation algorithm is also provided to segment signal into separate gait cycles with perfect accuracy. Subsequently, features are then extracted on both time and frequency domains. We aim to construct a lightweight but high reliable model; hence feature subsets selection algorithms are applied to optimize the dimension of the feature vectors as well as the processing time of classification tasks. Afterward, the optimal feature vector is classified using SVM with RBF kernel. Since there is no public dataset in this field to evaluate fairly the effectiveness of our mechanism, a realistic dataset containing the influence of mobile installation errors and footgear is also constructed with the participation of 38 volunteers (28 males, 10 females). <b>We</b> <b>achieved</b> <b>the</b> <b>accuracy</b> approximately 94. 93 % under identification mode, the FMR, FNMR of 0 %, 3. 89 % and processing time of less than 4 seconds under authentication mode...|$|R
40|$|The {{popularity}} of content management software (CMS) is growing vastly {{to the web}} developers and the business people because of its capacity for easy accessibility, manageability and usability of the distributed website contents. As per the statistics of Built with, 32 % of the web applications are developed with WordPress(WP) among all other CMSs [1]. It is obvious that quite {{a good number of}} web applications were built with WP in version 4. 7. 0 and 4. 7. 1. A recent research reveals that content injection vulnerability was found available in the above two versions of WP [2]. Unauthorized content injection by an intruder in a CMS managed application is one of the serious problems for the business {{as well as for the}} web owner. Therefore, detection of the vulnerability becomes a critical issue for this time. In this paper, we have discussed about the root cause of WP content injection of the above versions and have also proposed a detection model for the given vulnerability. A tool, SAISAN has been implemented as per our anticipated model and conducted an examination on 176 WP developed web applications using SAISAN. <b>We</b> <b>achieved</b> <b>the</b> <b>accuracy</b> of 92 % of the result of SAISAN as compared to manual black box testing outcome...|$|R
40|$|We explore {{techniques}} to significantly improve the compute efficiency {{and performance of}} Deep Convolution Networks without impacting their <b>accuracy.</b> To improve <b>the</b> compute efficiency, <b>we</b> focus on <b>achieving</b> high accuracy with extremely low-precision (2 -bit) weight networks, and to accelerate the execution time, we aggressively skip operations on zero-values. <b>We</b> <b>achieve</b> <b>the</b> highest reported <b>accuracy</b> of 76. 6...|$|R
40|$|In many {{environment}} sensing applications, we {{can observe}} {{the field by}} multiple types of sensors in multiple levels. We propose that <b>we</b> can <b>achieve</b> <b>the</b> <b>accuracy</b> of exhaustive sensing at one level by sparse sensing at multiple levels. Our previous results proved that <b>we</b> do <b>achieve</b> <b>the</b> same reconstruction <b>accuracy</b> as exhaustive sensing with reduced number of sensors. But by fusing measurement from different sensors directly in our previous approach, the improvement is limited. In our current work, we established field models to incorporate more information. The distribution models help assign uncertainty to the measurements. By applying correlation models in reconstruction process, we can probe the structure in the data and further improve reconstruction accuracy...|$|R
40|$|Abstract: Cancer is {{the most}} {{important}} cause of death for both men and women. The early detection of cancer can be helpful in curing the disease completely. So the requirement of techniques to detect the occurrence of cancer nodule in early stage is increasing. A disease that is commonly misdiagnosed is lung cancer. Neural Networks (NNs) {{play a vital role in}} the medical field in solving various health problems like acute diseases and even other mild diseases. Earlier diagnosis of Lung Cancer saves enormous lives, failing which may lead to other severe problems causing sudden fatal end. Its cure rate and prognosis depends mainly on the early detection and diagnosis of the disease. This thesis provides a Neural Network and SVM model for early detection of lung cancer. The model consists of an input layer, a hidden layer and an output layer. The network is trained with one hidden layer and one output layer by giving twelve inputs. One of the most common forms of medical malpractices globally is an error in diagnosis. By using the fusion of SVM and BPNN <b>we</b> <b>achieved</b> <b>the</b> <b>accuracy</b> of 98 %. The performance simulation is taken place in MATLAB 7. 10 environment. The MATLAB has inbuilt Neural Network toolbox and SVM has been implemented using two steps training and testing phases...|$|R
30|$|Results for the {{different}} cities. Among all cities, <b>we</b> <b>achieve</b> <b>the</b> best <b>accuracy</b> for Seattle and San Diego – <b>the</b> best weighted <b>accuracy</b> was obtained for San Diego using the random forest classifier (weighed accuracy equal to 0.72). The cities that were most difficult to estimate are San Francisco and Manhattan, where the random forest classifier reaches a weighted value equal to 0.61 for San Francisco. This is perhaps to be expected, given that Manhattan and San Francisco are very diverse. We conjecture that, due to high population concentrations, their tracts encompass a multitude of diverse socio-economic characteristics that cancel out, to a certain extent, expected patterns.|$|R
40|$|Abstract. The rapid {{expansion}} of blog and electronic data in Web 2. 0 is abounding and thus it is becoming important to identify the author‟s profile also. The problems of automatic identification of author‟s gender and age based on linguistic and stylistic pattern have been a subject of increasingly research interest in the recent years. The research methodologies are also helpful for several other applications like criminal detection, security and author detection etc. We have used lexical, syntactic and structural features for identifying the gender and age group of the authors. We have employed the Decision tree classifier for classifying the author profile. <b>We</b> have <b>achieved</b> <b>the</b> <b>accuracies</b> of 56. 83 % and 28. 95 % for gender and age group classification, respectively...|$|R
40|$|Treballs Finals de Grau d'Enginyeria Informàtica, Facultat de Matemàtiques, Universitat de Barcelona, Any: 2016, Director: Petia RadevaNowadays, we {{can find}} several {{diseases}} related with the unhealthy diet habits of the population, such as diabetes, obesity, anemia, bulimia and anorexia. In many cases, it is related with the food consumption of the people. Mediterranean diet is scientifically known as a healthy diet that helps to prevent those and other food problems. In particular, our work focuses on the recognition of Mediterranean food and dishes. It {{is part of a}} wider project that analyses the daily habits of users with wearable cameras, within the topic of Lifelogging. It appears as an objective tool for the analysis of the patient’s behavior, allowing specialist to discover patterns and understand user’s lifestyle to find unhealthy food patterns. With the aim to automatic recognize a complete diet, we introduce a challenging multilabeled dataset related to Mediterranean diet called FoodCAT. The first kind of labels contains 115 food classes with an average of 400 images per dish, and the second one is composed by 12 food categories with an average of 3800 pictures per class. This dataset will serve as a basis for the development of automatic diet tracking problems. Deep learning and more specifically Convolutional Neural Networks (CNNs), are actually the technologies with the state-of-the-art recognizing food automatically. In our work, we adapt the best, so far, CNNs architectures for image classification, to our objective into the diet tracking. Recognizing food categories, <b>we</b> <b>achieved</b> <b>the</b> highest <b>accuracies</b> top- 1 with 72. 29 %, and top- 5 with 97. 07 %. In a complete diet tracking recognizing dishes from Mediterranean diet, enlarged with the Food- 101 dataset, <b>we</b> <b>achieve</b> <b>the</b> highest <b>accuracies</b> top- 1 with 68. 07 %, and top- 5 with 89. 53 %, for a total of 115 + 101 food classes...|$|R
40|$|We {{propose a}} novel {{semantic}} segmentation algorithm by learning a deconvolution network. We learn the network {{on top of}} the convolutional layers adopted from VGG 16 -layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixel-wise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction; our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and <b>we</b> <b>achieve</b> <b>the</b> best <b>accuracy</b> (72. 5 %) among the methods trained with no external data through ensemble with the fully convolutional network...|$|R
40|$|In this work, {{we make a}} {{contribution}} to natural speech dialogue act detection. We focus our attention on the dialogue act classification using a Bayesian approach. Our classifier is tested on two corpora, the Switchboard and the Basurde tasks. A combination of a naive Bayes classifier and n-grams is used. The impact of different smoothing methods (Laplace and Witten Bell) and n-grams in classification are studied. With respect to the Switchboard corpus, an accuracy of 66 % is achieved using a uniform naive Bayes classifier, 3 -grams and Laplace smoothing to avoid zero probabilities. For the Basurde corpus, our system achieves performances similar to other methodologies we have previously tested. Through a combination of a naive Bayes classifier with 2 -grams and Witten Bell smoothing <b>we</b> <b>achieve</b> <b>the</b> best <b>accuracy</b> of 89 %. These results show that a Bayesian approach is well suited for these tasks. ...|$|R
40|$|When {{directly}} manipulating 3 D {{objects in}} an immersive environment <b>we</b> cannot normally <b>achieve</b> <b>the</b> <b>accuracy</b> and control {{that we have}} in the real world. This reduced accuracy stems from hand instability. We present PRISM, which dynamically adjusts the C/D ratio between the hand and the controlled object to provide increased control when moving slowly and direct, unconstrained interaction when moving rapidly. We describe PRISM object translation and rotation and present user studies demonstrating their effectiveness. In addition, we describe a PRISM-enhanced version of ray casting which is shown to increase <b>the</b> speed and <b>accuracy</b> of object selection...|$|R
40|$|Abstract. This paper {{presents}} {{a new approach}} to learn a rule based system for the task of part of speech tagging. Our approach is based on an incremental knowledge acquisition methodology where rules are stored in an exception-structure and new rules are only added to correct errors of existing rules; thus allowing systematic control of interaction between rules. Experimental results of our approach on English show that <b>we</b> <b>achieve</b> in <b>the</b> best <b>accuracy</b> published to date: 97. 095 % on the Penn Treebank corpus. We also obtain the best performance for Vietnamese VietTreeBank corpus. ...|$|R
40|$|For emotion recognition, we {{selected}} pitch, log energy, formant, mel-band energies, and mel frequency cepstral co-efficients (MFCCs) {{as the base}} features, and added veloc-ity/acceleration of pitch and MFCCs to form feature streams. We extracted statistics used for discriminative classifiers, as-suming that each stream is a one-dimensional signal. Extracted features were analyzed by using quadratic discriminant analy-sis (QDA) and support vector machine (SVM). Experimental results showed that pitch and energy {{were the most important}} factors. Using two different kinds of databases, we compared emotion recognition performance of various classifiers: SVM, linear discriminant analysis (LDA), QDA and hidden Markov model (HMM). With the text-independent SUSAS database, <b>we</b> <b>achieved</b> <b>the</b> best <b>accuracy</b> of 96. 3 % for stressed/neutral style classification and 70. 1 % for 4 -class speaking style clas-sification using Gaussian SVM, which is superior to the previ-ous results. With the speaker-independent AIBO database, <b>we</b> <b>achieved</b> 42. 3 % accuracy for 5 -class emotion recognition. 1...|$|R
40|$|Knowledge of supersecondary {{structures}} {{can provide}} important information about its spatial structure of protein. Some approaches {{have been developed}} for the prediction of protein supersecondary structure. However, the feature used by these approaches is primarily based on amino acid sequences. In this study, a novel model is presented to predict protein supersecondary structure by use of chemical shifts (CSs) information derived from nuclear magnetic resonance (NMR) spectroscopy. Using these CSs as inputs of the method of quadratic discriminant analysis (QD), <b>we</b> <b>achieve</b> <b>the</b> overall prediction <b>accuracy</b> of 77. 3 %, which is competitive with the same method for predicting supersecondary structures from amino acid compositions in threefold cross-validation. Moreover, our finding suggests that the combined use of different chemical shifts will influence <b>the</b> <b>accuracy</b> of prediction...|$|R
40|$|We present {{unsupervised}} speaker indexing {{combined with}} {{automatic speech recognition}} (ASR) for speech archives such as discussions. Our proposed indexing method is based on anchor models, by which we define a feature vector based on the similarity with speakers of a large scale speech database, and we incorporate several techniques to improve discriminant ability. ASR is performed using {{the results of this}} indexing. No discussion corpus is available to train acoustic and language models. So we applied the speaker adaptation technique to the baseline acoustic model based on the indexing. We also constructed a language model by merging two models that cover different linguistic features. <b>We</b> <b>achieved</b> <b>the</b> speaker indexing <b>accuracy</b> of 93 % and <b>the</b> word recognition <b>accuracy</b> of 57 % for real discussion data. 1...|$|R
40|$|Abstract. This paper {{describes}} an information extraction system de-veloped by team Hitachi for “Disease/Disorder Template filling ” task or-ganized by ShARe/CLEF eHealth Evaluation Lab 2014. We approached the task {{by building a}} baseline system using Apache cTAKES. We sub-mitted two separate runs; in our first run, rule based assertion module predict the norm slot value of assertion attributes excluding training data knowledge. However assertion module is changed to machine learning-based in second run. We trained models for Course modifiers, Severity modifier and Body Location relation extractor and applied a variety of rule based post processing including structural parsing. We performed two layer search on UMLS dictionary for refinement of body location. Eventually, we created rules for temporal expression extraction and also used them as features for model training of DocTime. We followed a dictionary matching technique for cue slot value detection in Task 2 b. Evaluation result of test data showed that our system performed very well in both subtasks. <b>We</b> <b>achieved</b> <b>the</b> highest <b>accuracy</b> 0. 868 in norm value detection, strict F 1 -score 0. 576 and relaxed F 1 -score 0. 724 in cue slot value identification, indicating promising enhancement on baseline system...|$|R
30|$|With {{increasing}} {{amount of}} acoustic and linguistic {{data from the}} judicial domain, using gender-dependent acoustic modeling and speaker adaptation based on maximum likelihood linear regression (MLLR) as well as much better text preprocessing and classification for robust domain-specific language modeling, <b>we</b> <b>achieved</b> <b>the</b> speech recognition <b>accuracy</b> nearly 95 % with {{a significant decrease in}} language model perplexity. Besides the better text processing and classification of training data, this result was achieved either by introducing classes of names, surnames, and other named entities into the recognition dictionary; representation of geographically named entities and technical terms by multiword expressions; by modeling of filled pauses in a language; or by effective adaptation of language models to the ted domain (see the Table 6).|$|R
40|$|Head pose {{estimation}} {{is critical}} in many applica-tions such as face recognition and human-computer in-teraction. Various classifiers such as LDA, SVM, or nearest neighbor are widely used for this purpose; how-ever, the recognition rates are limited due to the limited discriminative power of these classifiers for discretized pose estimation. In this paper, we propose a head pose estimation method using a Cluster-Classification Bayesian Network (CCBN), specifically designed for classification after clustering. A pose layout is defined where similar poses are assigned to the same block. This increases the discriminative power within the same block when similar yet different poses are present. <b>We</b> <b>achieve</b> <b>the</b> highest recognition <b>accuracy</b> on two public databases (CAS-PEAL and FEI) compared to the state-of-the-art methods. 1...|$|R
40|$|Current {{research}} conducted by the Institute for Photogrammetry at the Universitaet Stuttgart aims at the determination of a cylinder head’s pose by using a single monochromatic camera. The work {{is related to the}} industrial project RoboMAP, where the recognition’s result will be used as initiating information for a robot to position other sensors over the cylinder head. For this purpose a commercially available view-based algorithm is applied, which itself needs the object’s geometry as a-priori information. We describe the general functionality of the approach and present the results of our latest experiments. <b>The</b> results <b>we</b> <b>achieved</b> show that <b>the</b> <b>accuracy</b> as well as the processing time suite the project’s requirements very well, if the image acquisition is prepared properly...|$|R
40|$|Abstract. Hearing {{instruments}} (HIs) {{have emerged}} as true pervasive computers as they continuously adapt the hearing program to the user’s context. However, current HIs {{are not able to}} distinguish different hearing needs in the same acoustic environment. In this work, we explore how information derived from body and eye movements can be used to improve the recognition of such hearing needs. We conduct an experiment to provoke an acoustic environment in which different hearing needs arise: active conversation and working while colleagues are having a conversation in a noisy office environment. We record body movements on nine body locations, eye movements using electrooculography (EOG), and sound using commercial HIs for eleven participants. Using a support vector machine (SVM) classifier and person-independent training we improve <b>the</b> <b>accuracy</b> of 77 % based on sound to an accuracy of 92 % using body movements. With a view to a future implementation into a HI we then perform a detailed analysis of the sensors attached to <b>the</b> head. <b>We</b> <b>achieve</b> <b>the</b> best <b>accuracy</b> of 86 % using eye movements compared to 84 % for head movements. Our work demonstrates the potential of additional sensor modalities for future HIs and motivates to investigate the wider applicability of this approach on further hearing situations and needs...|$|R
3000|$|To {{answer the}} above {{questions}} we first performed a wide set of experiments {{to identify the}} best term representation, n-gram size, top-selection and feature selection method. After determining the optimal settings when using the OpCode representation, <b>we</b> compared <b>the</b> <b>achieved</b> <b>accuracy</b> to <b>the</b> byte n-gram representation used in [12]. In the second experiment we investigated the imbalance problem to determine the optimal settings of the training set for each classifier in varying [...] "real-life" [...] conditions. Finally, in the third experiment, we performed a chronological evaluation to determine how well a classifier, which was trained on past examples, can detect new malicious file and to investigate the importance and need in updating the training set frequently.|$|R
40|$|Integral {{images are}} {{commonly}} used in computer vision and computer graphics applications. Evaluation of box filters via integral images can be performed in constant time, regardless of the filter size. Although Heckbert [6] extended the integral image approach for more complex filters, its usage has been very limited, in practice. In this paper, we present an extension to integral images that allows for application of a wide class of non-uniform filters. Our approach is superior to Heckbert´s in terms of precision requirements and suitability for parallelization. We explain the theoretical basis of the approach and instantiate two concrete examples: filtering with bilinear interpolation, and filtering with approximated Gaussian weighting. Our experiments show <b>the</b> significant speedups <b>we</b> <b>achieve,</b> and <b>the</b> higher <b>accuracy</b> of our approach compared to Heckbert´s...|$|R
40|$|Abstract. Facial micro-expression {{recognition}} is an upcoming area in computer vision research. Up until the recent {{emergence of the}} extensive CASMEII spontaneous micro-expression database, there were numerous obstacles faced in the elicitation and labeling of data involving facial micro-expressions. In this paper, we propose the Local Binary Patterns with Six Intersection Points (LBP-SIP) volumetric descriptor based on the three intersecting lines crossing over the center point. The proposed LBP-SIP reduces the redundancy in LBP-TOP patterns, providing a more compact and lightweight representation; leading to more efficient computational complexity. Furthermore, we also incorporated a Gaus-sian multi-resolution pyramid to our proposed approach by concatenat-ing the patterns across all pyramid levels. Using an SVM classifier with leave-one-sample-out cross validation, <b>we</b> <b>achieve</b> <b>the</b> best recognition <b>accuracy</b> of 67. 21 %, surpassing the baseline performance with further computational efficiency. ...|$|R
40|$|We {{report on}} tests with a mind typing {{paradigm}} {{based on a}} P 300 brain-computer interface (BCI) {{on a group of}} amyotrophic lateral sclerosis (ALS), middle cerebral artery (MCA) stroke, and subarachnoid hemorrhage (SAH) patients, suffering from motor and speech disabilities. <b>We</b> investigate <b>the</b> <b>achieved</b> typing <b>accuracy</b> given <b>the</b> individual patient's disorder, and how it correlates with the type of classifier used. We considered 7 types of classifiers, linear as well as nonlinear ones, and found that, overall, one type of linear classifier yielded a higher classification accuracy. In addition to the selection of the classifier, we also suggest and discuss a number of recommendations to be considered when building a P 300 -based typing system for disabled subjects...|$|R
40|$|A {{number of}} {{classifiers}} {{have been proposed}} by the researchers for activity recognition using binary and ubiquitous sensors. Many researchers {{have shown that the}} hidden Markov model (HMM) and the conditional random field (CRF) -based activity classifiers work well to classify activities in comparison with the widely used na&# 239;ve Bayes-based activity classifier. However, it would not be an exact verdict if a na&# 239;ve Bayes-based activity classifier is properly smoothed. Parameter estimation plays the central role in the performance of a na&# 239;ve Bayes activity classifier. Data sparsity puts substantial challenges in parameter estimation because the sizes of the real-life activity datasets are relatively small. The distribution of the sensors may not be even among the activity classes. Additionally, some of the sensors would appear during testing but would not appear while training. This is called zero-frequency problems which assign zero probability of a sensor for a given activity. To prevent such estimation problems, we propose two smoothing techniques for adjusting the maximum likelihood to produce more precise probability of a sensor given an activity. We performed three experiments using three real-life activity datasets. It is observed that our proposed mechanism yields significant improvement in <b>the</b> <b>accuracy</b> of activity classification in comparison with its existing counterparts. <b>We</b> <b>achieved</b> <b>the</b> class <b>accuracy</b> ranging between 63 &#x 0025; and 83 &#x 0025;...|$|R
40|$|DNA {{methylation}} {{is one of}} {{the most}} studied and important epigenetic modifications in cells, playing a role in DNA transcription, splicing, and imprinting. Recently, advanced genome-wide DNA methylation profiling technologies have been developed, making it possible to conduct methylome-wide association studies. One of the problems with large scale DNA methylation studies is that the current technologies are either targeting only a limited number of CpG sites in the genome or whole genome sequencing is expensive and time consuming for most laboratories. Computational prediction of CpG site-specific methylation levels is the cost-saving and time-saving alternative. In this work, we found striking patterns of DNA methylation across the genome. We show that correlation among CpG sites decays rapidly within several hundreds base pairs in contrast to the LD structure of genotypes which holds for up to several KB. Using genomic features including, neighbor CpG site methylation and genomic distance, genomic context such as CpG island regions, and genomic regulatory elements, we built random forest classifiers to predict CpG site methylation levels. Our approach achieves 92 % prediction accuracy at single CpG sites in different genome-wide methylation datasets. <b>We</b> <b>achieves</b> <b>the</b> highest <b>accuracy</b> as 98 % for prediction within CpG island regions. What's more, our method identifies genomic features that interact with DNA methylation, which improves our understanding of mechanisms involved in DNA methylation modification and regulation. Thesi...|$|R
