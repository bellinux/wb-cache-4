107|188|Public
5000|$|<b>Workload</b> <b>Analysis</b> Tool to help analyse {{workload}} data {{in general}} practices and other primary care organisations ...|$|E
40|$|The Timeline Analysis Program (TLA- 1) was described. This {{program is}} a crew <b>workload</b> <b>analysis</b> {{computer}} program that was developed and expanded from previous <b>workload</b> <b>analysis</b> programs, and {{is designed to be}} used on the NASA terminal controlled vehicle program. The following information is described: derivation of the input data, processing of the data, and form of the output data. Eight scenarios that were created, programmed, and analyzed as verification of this model were also described...|$|E
40|$|An {{introduction}} to computer <b>workload</b> <b>analysis</b> is given, showing its {{range of application}} in computer centre management, system and application programming. Cluster methods are discussed {{which can be used}} in conjunction with workload data and cluster algorithms are adapted to the specific set problem. Several samples of CDC 7600 - accounting-data-collected at CERN, the European Organization for Nuclear Research-underwent a cluster analysis to determine job groups. The conclusions from resource usage of typical job groups in relation to computer <b>workload</b> <b>analysis</b> are discussed. (17 refs) ...|$|E
40|$|This paper {{introduces}} {{a class of}} event history analysis used to examine how the operations of an air traffic controller change under light and heavy traffic <b>workload.</b> The <b>analysis</b> begins by assessing the hazard rate, h(t), of a transition (or spell) between the controllet's communication and flight progress activitie...|$|R
40|$|This paper {{describes}} the DIGITAL Continuous Profiling Infrastructure, a sampling-based profiling {{system designed to}} run continuously on production systems. The system supports multiprocessors, works on unmodified executables, and collects profiles for entire systems, including user programs, shared libraries, and the operating system kernel. Samples are collected at a high rate (over 5200 samples/sec per 333 -MHz processor), yet with low overhead (1 [...] 3 % slowdown for most <b>workloads).</b> <b>Analysis</b> tools supplied with the profiling system use the sample data to produce a precise and accurate accounting, down {{to the level of}} pipeline stalls incurred by individual instructions, of where time is being spent. When instructions incur stalls, the tools identify possible reasons, such as cache misses, branch mispredictions, and functional unit contention. The fine-grained instruction-level analysis guides users and automated optimizers to the causes of performance problems and provides important in [...] ...|$|R
40|$|To date, most work {{regarding}} the formal analysis of access control schemes {{has focused on}} quantifying and comparing the expressive power {{of a set of}} schemes. Although expressive power is important, it is a property that exists in an absolute sense, detached from the application context within which an access control scheme will ultimately be deployed. By contrast, we formalize the access control suitability analy-sis problem, which seeks to evaluate {{the degree to which a}} set of candidate access control schemes can meet the needs of an application-specific workload. This process involves both reductions to assess whether a scheme is capable of implementing a <b>workload</b> (qualitative <b>analysis),</b> as well as cost analysis using ordered measures to quantify the over-heads of using each candidate scheme to service the <b>workload</b> (quantitative <b>analysis).</b> We formalize the two-facet suitability analysis problem, which formally describes this task. We then develop a mathematical framework for this type of analysis, and evaluate this framework both formally, by quantifying its efficiency and accuracy properties, and practically, by exploring an academic program committee workload...|$|R
40|$|This Papers {{focus on}} Job {{analysis}} {{as the basis}} of human resource system. It is describe about the job and workload and also the obstacles that are perhaps to observe during the work, and to supply all of activities of human resource management in the organization. <b>Workload</b> <b>analysis</b> is a process to decide the sum of time required to finish a specific job. The result of job and <b>workload</b> <b>analysis</b> goals to determine the number of employees needed in correspond to some specific workload and responsibilities given to an employee (the right man on the right place) ...|$|E
40|$|Abstract—Understanding the {{characteristics}} of MapReduce workloads in a Hadoop cluster {{is the key to}} making optimal configuration decisions and improving the system efficiency and throughput. However, <b>workload</b> <b>analysis</b> on a Hadoop cluster, especially in a large-scale e-commerce production environment, has not been well studied yet. In this paper, we performed a comprehensive <b>workload</b> <b>analysis</b> using the trace collected from a 2, 000 -node Hadoop cluster at Taobao, which is the biggest online e-commerce enterprise in Asia, ranked 10 th in the world as reported by Alexa. The results of the <b>workload</b> <b>analysis</b> are representative and generally consistent with the data warehouses for e-commerce web sites, which can help researchers and engineers understand the workload characteristics of Hadoop in their production environments. Based on the observations and implications derived from the trace, we designed a workload generator Ankus, to expedite the performance evaluation and debugging of new mechanisms. Ankus supports synthesizing an e-commerce style MapReduce workload at a low cost. Furthermore, we proposed and implemented a job scheduling algorithm Fair 4 S, which is designed to be biased towards small jobs. Small jobs account {{for the majority of the}} workload and most of them require instant and interactive responses, which is an important phenomenon at production Hadoop systems. The inefficiency of Hadoop fair scheduler for handling small jobs motivates us to design the Fair 4 S, which introduces pool weights and extends job priorities to guarantee the rapid responses for small jobs. Experimental evaluation verified the Fair 4 S accelerates the average waiting times of small jobs by a factor of 7 compared with the fair scheduler. Index Terms—Hadoop, MapReduce, <b>workload</b> <b>analysis,</b> workload synthesis, job scheduler F...|$|E
40|$|The {{main issue}} we address {{in this paper}} is the <b>workload</b> <b>analysis</b> of today's {{enterprise}} media servers. This analysis aims to establish a set of properties specific for enterprise media server workloads and to compare them with well known related observations about web server workloads. We propose two new metrics to characterize the dynamics and evolution of the accesses, {{and the rate of}} change in the site access pattern, and illustrate them with the analysis of two different enterprise media server workloads collected over a significant period of time. Another goal of our <b>workload</b> <b>analysis</b> study is to develop a media server log analysis tool, called MediaMetrics, that produces a media server traffic access profile and its system resource usage in a way useful to service providers...|$|E
40|$|This paper investigates {{formally}} {{the problem}} of unfairness in SRPT scheduling as compared with PS scheduling. The analysis assumes an M/G/ 1 model, and emphasizes job size distributions with a heavy-tailed property, as are characteristic of empirical <b>workloads.</b> The <b>analysis</b> shows {{that the degree of}} unfairness under SRPT is surprisingly small. The M/G/ 1 /SRPT and M/G/ 1 /PS queues are also analyzed under overload and closed-form expressions for mean response time as a function of job size are proved in this setting...|$|R
40|$|We {{analyze the}} I/O {{behavior}} of iBench, a new collection of productivity and multimedia application <b>workloads.</b> Our <b>analysis</b> reveals {{a number of}} differences between iBench and typical file-system workload studies, including the complex organization of modern files, the lack of pure sequential access, the influence of underlying frameworks on I/O patterns, {{the widespread use of}} file synchronization and atomic operations, and the prevalence of threads. Our results have strong ramifications for the design of next generation local and cloud-based storage systems. 1...|$|R
30|$|Now, we {{evaluate}} our compile-time {{approach on}} {{the analysis of}} two real-world dynamic graphs: one that produces a constant workload (MD) and a second one that generates a non-constant <b>workload</b> (FB). Our <b>analysis</b> scripts for performing the evaluation are available as an open-source repository 6.|$|R
40|$|The {{needs for}} human {{resources}} (HR) {{has increased in}} terms of both quality and quantity. Measurement of the workload is needed to set a time for an employee who meets the requirements of (qualified) in carrying out certain work on the level of achievement that has been set. To overcome the problem of measuring the workload on Kopontren Al-Huda Surabaya which is built from the Surabaya City Government, this study used <b>workload</b> <b>analysis</b> method, that will provide information on the allocation of human resources employee to resolve the existing workload. Based on the results of measurements of the workload at the Al-Huda Kopontren Surabaya using <b>Workload</b> <b>analysis</b> can be concluded that the average workload of employees in the baguan Tabulator is 8650. 866 hours / year with the optimal number of employees is four. In the Adhoc department, average workload before the research is 12367. 372 hours / year with the optimal number of employees is six people. In the human resources department and Trainer average workload is 7452. 393 hours / year with the optimal number of employees is four. In the General section average workload is 15938. 237 hours / year with the optimal number of employees is eight people. Keywords: efficiency, human resources, <b>workload</b> <b>analysis,</b> the optimal number of employee...|$|E
40|$|Title of research: "The {{number of}} Human Resource Planning In Business Making Iron Fence Jetis At Hamlet Village District Mulyoagung Dau Malang". The {{aim of this}} study was to {{determine}} the workload of employees, to know the number of employees that should be available in business of making the fence and the projection of human resource needs in the future. This study uses analysis workload and workforce analysis. <b>Workload</b> <b>analysis</b> is used to set standards for employee expenses. The workforce analysis is used to determine the amount of human resource requirements. <b>Workload</b> <b>analysis</b> results produces standards for employee expenses fence-making business at 16. 07 hours per unit of fence-making business. While the workforce produces labor requirements were 58 fence-making business people. Compared with the existing workforce, the overall human resources force fence-making business excess employment for 19 people. In the Year 2011 were increased of 3 (people), in 2012 were increased of 14 (people), in 2013 were increased of 28 (people), in 2014, an were increased of 42 (people) and in 2015 were increased of 58 (people). Suggestions for fence-making business, for reforming the excess human resources particular section to be moved to other parts that still need, and probably find another unit that is still needed so that productivity can be maximized. Keywords: <b>workload</b> <b>analysis</b> and workforce analysi...|$|E
40|$|Abstract—Workflow model {{analysis}} is performed at logic, temporal, and performance levels. This paper mainly {{deals with the}} performance level issues. Workflow net (WF-net) is extended with time information to the timing workflow net (TWF-net). To provide a formal framework for modeling and analyzing workflow, this paper proposes a multidimension workflow net (MWF-net) that include multiple TWF-nets and the organization and resource information. The algorithm to decompose a free-choice and acyclic Petri nets (PN) into a set of-components is extended to a TWF-net containing iteration structures. Then, resource availability and <b>workload</b> <b>analysis</b> is performed. A method for computing the lower bound of average turnaround time of transaction instances processed in a MWF-net is proposed. Finally a case study is used {{to show that the}} proposed method can be effectively utilized in practice. Index Terms—Performance, Petri nets, turnaround time, workflow model, <b>workload</b> <b>analysis...</b>|$|E
40|$|Abstract—As {{the amount}} of data explodes rapidly, more and more {{corporations}} are using data centers to make effective de-cisions and gain a competitive edge. Data analysis applications {{play a significant role}} in data centers, and hence it has became increasingly important to understand their behaviors in order to further improve the performance of data center computer systems. In this paper, after investigating three most important application domains in terms of page views and daily visitors, we choose eleven representative data <b>analysis</b> <b>workloads</b> and characterize their micro-architectural characteristics by using hardware performance counters, in order to understand the impacts and implications of data <b>analysis</b> <b>workloads</b> on the systems equipped with modern superscalar out-of-order pro-cessors. Our study on the workloads reveals that data analysis applications share many inherent characteristics, which place them in a different class from desktop (SPEC CPU 2006), HPC (HPCC), and service workloads, including traditional server workloads (SPECweb 2005) and scale-out service workloads (four among six benchmarks in CloudSuite), and accordingly we give several recommendations for architecture and system optimizations. On the basis of our workload characterization work, we released a benchmark suite named DCBench for typical datacenter <b>workloads,</b> including data <b>analysis</b> and service <b>workloads,</b> with an open-source license on our project home page o...|$|R
40|$|As {{the amount}} of data explodes rapidly, more and more {{corporations}} are using data centers to make effective decisions and gain a competitive edge. Data analysis applications {{play a significant role}} in data centers, and hence it has became increasingly important to understand their behaviors in order to further improve the performance of data center computer systems. In this paper, after investigating three most important application domains in terms of page views and daily visitors, we choose eleven representative data <b>analysis</b> <b>workloads</b> and characterize their micro-architectural characteristics by using hardware performance counters, in order to understand the impacts and implications of data <b>analysis</b> <b>workloads</b> on the systems equipped with modern superscalar out-of-order processors. Our study on the workloads reveals that data analysis applications share many inherent characteristics, which place them in a different class from desktop (SPEC CPU 2006), HPC (HPCC), and service workloads, including traditional server workloads (SPECweb 2005) and scale-out service workloads (four among six benchmarks in CloudSuite), and accordingly we give several recommendations for architecture and system optimizations. On the basis of our workload characterization work, we released a benchmark suite named DCBench for typical datacenter <b>workloads,</b> including data <b>analysis</b> and service <b>workloads,</b> with an open-source license on our project home page on [URL] We hope that DCBench is helpful for performing architecture and small-to-medium scale system researches for datacenter computing. Comment: 11 pages, 12 figures, IISWC 201...|$|R
40|$|The Higher Education Funding Council funded report 'Promoting Positive Gender Outcomes in HE Through Active Workload Management' {{includes}} HEI {{case study}} interviews, surveys and <b>workload</b> data <b>analysis</b> {{to investigate the}} disparity between the genders in their careers. For example in 2010 although women made up 43 % of the academic workforce {{when looking at the}} Professorial role only 18. 7 % were women. The report, through the field work and data analysis, uncovers a range of quite subtle factors that appear to be working together to create this imbalance and includes recommendations for better practice in workload allocation...|$|R
30|$|Quite surprisingly, we {{have found}} {{a limited amount of}} work {{specific}} to <b>workload</b> <b>analysis</b> and inference techniques in the cloud. Most of the techniques used for traffic forecasting, resource consumption estimation, and anomaly detection have received little or no validation in a cloud environment. As such, it remains to establish the robustness of current techniques to noisy measurements typical of multi-tenant cloud environments.|$|E
40|$|Abstract — Advances in {{virtualization}} {{technology are}} enabling {{the creation of}} resource pools of servers that permit multiple application workloads to share each server in the pool. Understanding the nature of enterprise workloads is crucial to properly designing and provisioning current and future services in such pools. This paper considers issues of <b>workload</b> <b>analysis,</b> performance modeling, and capacity planning. Our goal is to automate the efficient use of resource pools when hosting large numbers of enterprise services. We use a trace based approach for capacity management that relies on i) the characterization of workload demand patterns, ii) the generation of synthetic workloads that predict future demands based on the patterns, and iii) a workload placement recommendation service. The accuracy of capacity planning predictions depends on our ability to characterize workload demand patterns, to recognize trends for expected changes in future demands, and to reflect business forecasts for otherwise unexpected changes in future demands. A <b>workload</b> <b>analysis</b> demonstrates the burstiness and repetitive nature of enterprise workloads. Workloads are automatically classified according to their periodic behavior. The similarity among repeated occurrences of patterns is evaluated. Synthetic workloads are generated from the patterns {{in a manner that}} maintains the periodic nature, burstiness, and trending behavior of the workloads. A case study involving six months of data for 139 enterprise applications is used to apply and evaluate the enterprise <b>workload</b> <b>analysis</b> and related capacity planning methods. The results show that when consolidating to 8 processor systems, we predicted future per-server required capacity to within one processor 95 % of the time. The accuracy of predictions for required capacity suggests that such resource savings can be achieved with little risk. I...|$|E
40|$|The Java Modelling Tools (JMT) is an {{open source}} suite for {{performance}} evaluation, capacity planning and modelling of computer and communication systems. The suite implements numerous state-of-the-art algorithms for the exact, asymptotic and simulative analysis of queueing network models, either with or without product-form solution. Models can be described either through wizard dialogs or with a graphical user-friendly interface. The suite includes also a <b>workload</b> <b>analysis</b> tool based on clustering techniques...|$|E
40|$|Within {{the broad}} topical {{designation}} of “smart grid,” research in demand response, or demand-side management, focuses on investigating possibilities for electrically powered devices to adapt their power consumption patterns to better match {{the availability of}} intermittent renewable energy sources, especially wind. Devices such as battery chargers, heating and cooling systems, and computers can be controlled to change the time, duration, and magnitude of their power consumption while still meeting workload constraints such as deadlines and rate of throughput. This thesis presents a system by which a computer server, or multiple servers in a data center, can estimate the power imbalance on the electrical grid and use that information to dynamically change the power consumption as a service to the grid. Implementation on a testbed demonstrates the system with a hypothetical but realistic usage case scenario of an online video streaming service {{in which there are}} workloads with deadlines (high-priority) and workloads without deadlines (low-priority). The testbed is implemented with real servers, estimates the power imbalance from the grid frequency with real-time measurements of the live outlet, and uses a distributed, real-time algorithm to dynamically adjust the power consumption of the servers based on the frequency estimate and the throughput of video transcoder <b>workloads.</b> <b>Analysis</b> of the system explains and justifies multiple design choices, compares the significance of the system in relation to similar publications in the literature, and explores the potential impact of the system...|$|R
40|$|We present HSAN - {{a hybrid}} storage area network, which uses both in-band (like NFS [13]) and {{out-of-band}} virtualization (like SAN FS [10]) access models. HSAN uses hybrid servers {{that can serve}} as both metadata and NAS servers to intelligently decide the access model per each request, based {{on the characteristics of}} requested data. This is in contrast to existing efforts that merely provide concurrent support for both models and do not exploit model appropriateness for requested data. The HSAN hybrid model is implemented using low overhead cacheadmission and cache-replacement schemes and aims to improve overall response times {{for a wide variety of}} <b>workloads.</b> Preliminary <b>analysis</b> of the hybrid model indicates performance improvements over both models...|$|R
40|$|Abstract. Performance {{prediction}} in Grid computing {{presents an}} important challenge due to Grid environments are volatiles, heterogeneous and not reliable. We {{suggest that the}} historical data related to applications, resources and users can provide an adequate amount of information for modelling and predicting Grid components behaviours. That information can be use to build a dynamic top-bottom Grid Service and Resources performance descriptions. In this paper we present the initial work {{that we have done}} in order to define prediction techniques. We show how the <b>workload</b> load <b>analysis</b> carried out in the previous stage of the presented work, has guided us in the design of prediction techniques, the mechanisms created for their evaluation and the performance predictors analysis. ...|$|R
40|$|Abstract — The {{proliferation}} of service oriented computing boosted by advances in cloud computing {{has led to}} design of new services that combine the power of both trends. The HP ePrint Service allows customers to print from anywhere to an HP ePrint-enabled printer accessible via the Internet. As ePrint is a hosted service, it must provide customers with a high quality of service while keeping the costs of supporting the load as low as possible. Understanding the load {{and the nature of}} this new service is crucial for properly designing the service’s support infrastructure under rapidly growing user demand. With the complexity of services increasing and application requirements for QoS growing, the research challenge is to design an integrated framework of <b>workload</b> <b>analysis</b> combined with system measurement and modeling techniques that support performance analysis for the service. In this work, we present a detailed <b>workload</b> <b>analysis</b> of ePrint and a new performance tool for the automatic evaluation of required capacity for processing its diverse workload in a production environment while satisfying QoS requirements. I...|$|E
40|$|Workload {{characterization}} is {{an integral}} part of performance analysis of high performance computing (HPC) systems. An understanding of workload properties sheds light on resource utilization and can be used to inform performance optimization both at the software and system configuration levels. It can provide information on how computational science usage modalities are changing that could potentially aid holistic capacity planning for the wider HPC ecosystem. Here, we report on the results of a detailed <b>workload</b> <b>analysis</b> of the portfolio of supercomputers comprising the NSF Innovative HPC program in order to characterize its past and current workload and look for trends to understand the nature of how the broad portfolio of computational science research is being supported and how it is changing over time. The <b>workload</b> <b>analysis</b> also sought to illustrate a wide variety of usage patterns and performance requirements for jobs running on these systems. File system performance, memory utilization and the types of parallelism employed by users (MPI, threads, etc) were also studied for all systems for which job level performance data was available. Comment: 93 pages, 82 figures, 19 table...|$|E
40|$|The present paper {{reviews the}} results of two {{experiments}} in which <b>workload</b> <b>analysis</b> was conducted based upon performance measures, brain evoked potentials and magnitude estimations of subjective load. The three types of measures were jointly applied to {{the description of the}} behavior of subjects in a wide battery of experimental tasks. Data analysis shows both instances of association and dissociation between types of measures. A general conceptual framework and methodological guidelines are proposed to account for these findings...|$|E
40|$|This paper {{provides}} a systematic comparison of various characteristics of computationally-intensive <b>workloads.</b> Our <b>analysis</b> focuses on standard HPC benchmarks and representative applications. For the selected workloads we provide {{a wide range}} of characterizations based on instruction tracing and hardware counter measurements. Each workload is analyzed at the instruction level by comparing the dynamic distribution of executed instructions. We also analyze memory access patterns including various aspects of cache utilization and locality properties of address distributions. Since prefetching {{plays an important role in}} the performance of computational workloads, we explore the prefetching potential and for parallel workloads we study the sharing properties of memory accesses. For the purpose of completeness, HPC workloads are compare...|$|R
40|$|The {{extent to}} which distracting {{information}} influences decisions can be informative {{about the nature of}} the underlying cognitive and perceptual processes. In a recent paper, a response time-based measure for quantifying the degree of interference (or facilitation) from distracting information termed resilience was introduced. Despite using a statistical measure, the analysis was limited to qualitative comparisons between different model predictions. In this paper, we demonstrate how statistical procedures from <b>workload</b> capacity <b>analysis</b> can be applied to the new resilience functions. In particular, we present an approach to null-hypothesis testing of resilience functions and a method based on functional principal components analysis for analyzing differences in the functional form of the resilience functions across participants and conditions...|$|R
40|$|Abstract:- Platform {{independent}} {{dynamic analysis}} {{has been shown}} to be an important technique for performance <b>analysis</b> and <b>workload</b> characterization of programs that run on the Java Virtual Machine. In this paper we explore how this methodology can me used to study method invocation. We identify differences in program behaviour and propose a metric to predict dynamic compilation efficiency. - Java Virtual Machine, method invocation, performance <b>analysis,</b> <b>workload</b> characteriza-Key-Words: tion...|$|R
40|$|The {{analysis}} of storage system workloads {{is important for}} a number of reasons. It is necessary to understand the usage patterns of secondary storage to enable architects to understand and build a new, or improve upon an existing storage system. It is also important for a storage administrator to understand the workload profile when configuring and tuning a system. Furthermore, the {{analysis of}} workloads is necessary to understand the storage requirements and behavior of application software. A different reason for <b>workload</b> <b>analysis,</b> is the need to come up with adequate models for performance evaluation. For the evaluation, based on simulation or otherwise, to be reliable one has to analyze, understand and adequately model the workloads. In this paper we describe a methodology to go about storage system <b>workload</b> <b>analysis</b> and illustrate it with a general tool we call the Enterprize Storage System Workload Analyzer or ESSWA. Although such analyzes are not new, we believe our proposals have a number of advantages over previous analyzers described in the literature. We experimented with ESSWA by analyzing workloads represented by three sets of publicly available workload traces...|$|E
40|$|The present paper {{describes}} a manning and crew <b>workload</b> <b>analysis</b> conducted {{to support the}} development of the mine warfare module for the Littoral Combat Ship. An initial analysis was conducted to develop mission scenarios, a functional breakout, a task list, and an estimate of manning requirements. Ways to reduce the required manning from 27 sailors to 15 sailors were identified, contingent upon design changes and hybrid training. A detailed <b>workload</b> <b>analysis</b> was conducted to assess the viability of the reduced manning concepts. Results indicate that although the reduced crew could handle peak operations requirements, they could not accomplish all the tasks required for the mission within a standard 70 -hour workweek. A crew of 19 would be expected to accomplish all the tasks. The present analysis illustrates the value of jointly performing human engineering and manning analyses. Insight gained can be shared simultaneously with system designers, personnel planners, and training developers. Human factors engineers often focus on the design of individual system components and how they are arranged in a workspace. Such issues as how many operators will be required, and how those operators will be trained, are often left to other technical disciplines. Even if the human factor...|$|E
40|$|Abstract—Since the {{performance}} of processors increase {{at a much higher}} rate than that of the mass storage devices, the I/O subsystem has created a bottleneck for many computer systems. In order to improve {{the performance}}, one has to fully understand both the workloads and the system to which the workload is presented. This paper discusses how these objectives can be met by using <b>workload</b> <b>analysis</b> and analytic modeling, and applying them in to an Enterprise Storage System (ESS) of an operationa...|$|E
30|$|We {{have studied}} {{standard}} <b>workload</b> questionnaires, <b>analysis</b> of communication patterns, and electro-dermal activity as measures of operator workload. Our {{findings indicate that}} each of these methods have unique advantages and disadvantages and, depending on circumstances, may be more or less suitable to measure workload. The ISA and SWAT questionnaires were generally able to measure workload levels. These scales are commonly used, which is beneficial in that it facilitates easy comparison with previous work in the domain. There also an established understanding in the research community regarding how {{they may or may not}} be used, and further, information regarding the tool’s reliability and validity metrics is well known. There are, however, several shortcomings of questionnaire use, including problems with recall, rationalisation, and, as discussed earlier, interruption of workflow.|$|R
40|$|At {{the present}} time, {{it can be}} a {{significant}} challenge to build a large-scale distributed file system that simul-taneously maintains both high availability and high per-formance. Although many fault tolerance technologies have been proposed and used in both commercial and academic distributed file systems to achieve high avail-ability, most of them typically sacrifice performance for higher system availability. Additionally, recent studies show that system availability and performance are re-lated to the system workload. In this paper, we analyze the correlations among availability, performance, and workloads based on a replication strategy, and we dis-cuss the trade off between availability and performance with different <b>workloads.</b> Our <b>analysis</b> leads to the de-sign of an online controller that can dynamically achieve optimal performance and availability by tuning the sys-tem replication policy. ...|$|R
40|$|In this paper, {{we present}} a {{characterization}} of disk drive workloads measured in systems representing the enterprise, desktop, and consumer electronics environments. We observe that the common characteristics across all traces are disk drive idleness and <b>workload</b> burstiness. Our <b>analysis</b> shows {{that the majority of}} characteristics, including request disk arrival rate, response time, service time, WRITE performance, and request size, are environment dependent. However, characteristics such as READ/WRITE ratio and access pattern are application dependent. ...|$|R
