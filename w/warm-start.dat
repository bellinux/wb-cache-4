91|32|Public
5000|$|Fluorescent lamps work {{by passing}} {{electricity}} through mercury vapor, {{which in turn}} emits ultraviolet light. The ultraviolet light is then absorbed by a phosphor coating inside the lamp, causing it to glow, or fluoresce. Conventional linear fluorescent lamps have life spans around 20,000 and 30,000 hours based on 3 hours per cycle according to lamps NLPIP reviewed in 2006. Induction fluorescent relies on electromagnetism rather than the cathodes used to start conventional linear fluorescent. The newer rare earth triphosphor blend linear fluorescent lamps made by Osram, Philips, Crompton and others have a life expectancy greater than 40,000 hours, if coupled with a <b>warm-start</b> electronic ballast. The life expectancy depends {{on the number of}} on/off cycles, and is lower if the light is cycled often. The ballast-lamp combined system efficacy for then current linear fluorescent systems in 1998 as tested by NLPIP ranged from 80 to 90 lm/W.|$|E
30|$|We tested <b>warm-start</b> and semi <b>warm-start</b> approaches, and {{the results}} of {{experiments}} show that they work well. However, until we have developed the solution to the non-native speech recognition problem in a fully unsupervised manner (without warm start), there is still room for improvement.|$|E
40|$|We {{implement}} several <b>warm-start</b> {{strategies in}} interior-point methods for linear programming (LP). We study {{the situation in}} which both the original LP instance and the perturbed one have exactly the same dimensions. We consider different types of perturbations of data components of the original instance and different sizes of each type of perturbation. We modify the state-of-the-art interior-point solver PCx in our implementation. We evaluate the effectiveness of each <b>warm-start</b> strategy {{based on the number of}} iterations and the computation time in comparison with "cold start" on the NETLIB test suite. Our experiments reveal that each of the <b>warm-start</b> strategies leads to a reduction in the number of interior-point iterations especially for smaller perturbations and for perturbations of fewer data components in comparison with cold start. On the other hand, only one of the <b>warm-start</b> strategies exhibits better performance than cold start in terms of computation time. Based on the insight gained from the computational results, we discuss several potential improvements to enhance the performances of such <b>warm-start</b> strategies. © 2007 Springer Science+Business Media, LLC...|$|E
40|$|An {{important}} {{advantage of}} infeasible interior-point methods compared to feasible interior-point methods is {{their ability to}} be <b>warm-started</b> from approximate solutions. It is therefore important for the convergence theory of these algorithms not to depend {{on being able to}} alter the starting point. In two recent papers, Yin Zhang and Stephen Wright prove convergence results for some infeasible interior-point methods. Unfortunately, their analysis places a restriction on the starting point. It is easy to meet the restriction by altering the starting point, but this may take the point farther away from the solution, removing the advantage of <b>warm-starting</b> the algorithms. In this paper we extend Zhang and Wright's results to apply to arbitrary strictly positive starting points. We then present an algorithm for solving the Box-Constrained Linear Complementarity problem and prove its convergence. 1 Introduction Quite often, in using an iterative method to solve a problem, it is possible to u [...] ...|$|R
40|$|Optimization {{approaches}} {{based on}} operator splitting are becoming popular for solving sparsity regularized statistical machine learning models. While many have proposed fast algorithms {{to solve these}} problems for a single regularization parameter, conspicuously less {{attention has been given}} to computing regularization paths, or solving the optimization problems over the full range of regularization parameters to obtain a sequence of sparse models. In this chapter, we aim to quickly approximate the sequence of sparse models associated with regularization paths for the purposes of statistical model selection by using the building blocks from a classical operator splitting method, the Alternating Direction Method of Multipliers (ADMM). We begin by proposing an ADMM algorithm that uses <b>warm-starts</b> to quickly compute the regularization path. Then, by employing approximations along this <b>warm-starting</b> ADMM algorithm, we propose a novel concept that we term the ADMM Algorithmic Regularization Path. Our method can quickly outline the sequence of sparse models associated with the regularization path in computational time that is often less than that of using the ADMM algorithm to solve the problem at a single regularization parameter. We demonstrate the applicability and substantial computational savings of our approach through three popular examples, sparse linear regression, reduced-rank multi-task learning, and convex clustering. Comment: 27 pages, 4 figure...|$|R
50|$|Logistic model {{trees are}} based on the earlier idea of a model tree: a {{decision}} tree that has linear regression models at its leaves to provide a piecewise linear regression model (where ordinary decision trees with constants at their leaves would produce a piecewise constant model). In the logistic variant, the LogitBoost algorithm is used to produce an LR model at every node in the tree; the node is then split using the C4.5 criterion. Each LogitBoost invocation is <b>warm-started</b> from its results in the parent node. Finally, the tree is pruned.|$|R
30|$|The <b>warm-start</b> {{approach}} chart clearly reflects {{the moment when}} we switch from (initial) pre-training to DSL (around the 130 th epoch). The convergence rate for the MSTT model declines from that point. That means more time is required to achieve comparable results. However, the final accuracy achieved by the <b>warm-start</b> DSL approach is higher.|$|E
30|$|During the <b>warm-start</b> and semi <b>warm-start</b> approach, for {{training}} MSTT and MTTS models, we used 10 % (around 7000 recordings) of the English Speech Database Read by Japanese Students (UME-ERJ) for Japanese people, and 10 % (a similar quantity) of recordings scraped from Youtube for Polish speakers, which we labeled ourselves. The {{rest of the}} data was used for verification.|$|E
30|$|In {{the second}} experiment, we checked a semi <b>warm-start</b> approach, {{training}} only MSTT {{model with a}} small amount of labeled data before switching to DSL.|$|E
40|$|Isotonic {{regression}} (IR) is a non-parametric {{calibration method}} used in supervised learning. For performing large-scale IR, we propose a primal-dual active-set (PDAS) algorithm which, {{in contrast to}} the state-of-the-art Pool Adjacent Violators (PAV) algorithm, can be parallized and is easily <b>warm-started</b> thus well-suited in the online settings. We prove that, like the PAV algorithm, our PDAS algorithm for IR is convergent and has a work complexity of O(n), though our numerical experiments suggest that our PDAS algorithm is often faster than PAV. In addition, we propose PDAS variants (with safeguarding to ensure convergence) for solving related trend filtering (TF) problems, providing the results of experiments to illustrate their effectiveness...|$|R
40|$|Many {{sequential}} mathematical optimization {{methods and}} simulation-based heuristics for optimal control {{and design of}} water distribution networks rely on {{a large number of}} hydraulic simulations. In this paper, we propose an efficient inexact subspace Newton method for hydraulic analysis of water distribution networks. By using sparse and well-conditioned fundamental null space bases, we solve the nonlinear system of hydraulic equations in a lower-dimensional kernel space of the network incidence matrix. In the inexact framework, the Newton steps are determined by solving the Newton equations only approximately using an iterative linear solver. Since large water network models are inherently badly scaled, a Jacobian regularization is employed to improve the condition number of these linear systems and guarantee positive definiteness. After presenting a convergence analysis of the regularised inexact Newton method, we use the conjugate gradient (CG) method to solve the sparse reduced Newton linear systems. Since CG is not effective without good preconditioners, we propose tailored constraint preconditioners that are computationally cheap because they are based only on invariant properties of the null space linear systems and do not change with flows and pressures. The preconditioners are shown to improve the distribution of eigenvalues of the linear systems and so enable a more efficient use of the CG solver. Since contiguous Newton iterates can have similar solutions, each CG call is <b>warm-started</b> with the solution for a previous Newton iterate to accelerate its convergence rate. Operational network models are used to show the efficacy of the proposed preconditioners and the <b>warm-starting</b> strategy in reducing computational effort...|$|R
40|$|We {{consider}} a general robust block-structured optimization problem, coming from applications in network and energy optimization. We propose and study an iterative cutting-plane algorithm, generic {{for a large}} class of uncertainty sets, able to tackle large-scale instances by leveraging on their specific structure. This algorithm combines known techniques (cutting-planes, proximal stabilizations, efficient heuristics, <b>warm-started</b> bundle methods) in an original way for better practical efficiency. We provide a theoretical analysis of the algorithm and connections to existing literature. We present numerical illustrations on real-life problems of electricity generation under uncertainty. These clearly show {{the advantage of the}} proposed regularized algorithm over classic cutting plane approaches. We therefore advocate that regularized cutting plane methods deserve more attention in robust optimization...|$|R
40|$|In {{this paper}} we propose a <b>warm-start</b> {{technique}} for interior point methods applicable to multi-stage stochastic linear programming problems. The main idea is to generate an initial point by decomposing the problem at the second stage and using an approximate solution of the subproblems {{as a starting point for}} the complete instance. We analyse this scheme and produce theoretical conditions under which the <b>warm-start</b> iterate is successful. We describe the implementation within the OOPS solver and the results of the numerical tests we performed. ...|$|E
40|$|Abstract — Limits on {{the storage}} space or the {{computation}} time restrict {{the applicability of}} model predictive controllers (MPC) in many real problems. Currently available methods either compute the optimal controller online or derive an explicit control law. In this paper we introduce a new approach combining the two main paradigms of explicit and online MPC to overcome their individual limitations. The algorithm com-putes a piecewise affine approximation of the optimal solution {{that is used to}} <b>warm-start</b> an active set linear programming procedure. A preprocessing method is introduced that provides hard real-time, stability and performance guarantees for the proposed controller. By choosing a combination {{of the quality of the}} approximation and the number of online active set iterations the presented procedure offers a tradeoff between the <b>warm-start</b> and online computational effort. We show how the problem of identifying the optimal tradeoff for a given set of requirements on online computation time, storage and performance can be solved. Finally, we demonstrate the potential of the proposed <b>warm-start</b> procedure on a numerical example. I...|$|E
40|$|Abstract. Interior point methods (IPM) {{have been}} {{recognised}} as an efficient approach for {{the solution of}} large scale stochastic programming problems due to their ability of exploiting the block-angular structure of the augmented system particular to this problem class. Stochastic programming problems, however, have exploitable structure beyond the simple matrix shape: namely the scenarios are typically a discrete sampling of an underlying (continuous) probability distribution. An appealing way of exploiting this would be to initially use a coarser discretisation, i. e. less scenarios, to obtain an approximate solution, which could then be used to <b>warm-start</b> the solver on the full problem. In this paper we present a multi-step <b>warm-start</b> scheme for stochastic programming problems, where a sequence of problems defined over scenario trees of differing sizes is given and an IPM <b>warm-start</b> point is constructed by successively finding approximations to the central path of the problems defined over the given trees. We analyse the resulting algorithm, argue that it yields improved complexity over either the coldstart or a naive two-step scheme, and give numerical results...|$|E
40|$|International audienceEveryday, {{electricity}} generation companies submit a generation schedule to the grid operator {{for the coming}} day; computing an optimal schedule is called the unit-commitment problem. Generation companies can also occasionally submit changes to the schedule, {{that can be seen}} as intra-daily incomplete recourse actions. In this paper, we propose a two-stage formulation of unit-commitment, wherein both the first and second stage problems are full unit-commitment problems. We present a primal-dual decomposition approach to tackle large-scale instances of these two-stage problems. The algorithm makes extensive use of <b>warm-started</b> bundle algorithms, and requires no specific knowledge of the underlying technical constraints. We provide an analysis of the theoretical properties of the algorithm, as well as computational experiments showing the interest of the approach for real-life large-scale unit-commitment instances...|$|R
40|$|The paper {{presents}} {{a simple and}} effective Lagrangian relaxation approach for {{the solution of the}} optimal short-term unit commitment problem in hydrothermal power-generation systems. The proposed approach, based on a disaggregated Bundle method for the solution of the dual problem, with a new <b>warm-starting</b> procedure, achieves accurate solutions in few iterations. The adoption of a disaggregated Bundle method not only improves the convergence of the proposed approach but also provides information that are suitably exploited for generating a feasible solution of the primal problem and for obtaining an optimal hydro scheduling. A comparison between the proposed Lagrangian approach and other ones, based on subgradient and Bundle methods, is presented for a simple yet reasonable formulation of the Hydrothermal Unit Commitment problem...|$|R
40|$|Abstract. We {{present an}} {{approach}} for nonlinear programming (NLP) {{based on the}} direct minimization of an exact differentiable penalty function using trust-region Newton techniques. As opposed to existing algorithmic approaches to NLP, the approach provides all the features required for scalability: it can efficiently detect and exploit directions of negative curvature, it is superlinearly convergent, and it enables the scalable computation of the Newton step through iterative linear algebra. Moreover, it presents features that are desirable for parametric optimization problems {{that have to be}} solved in a latency-limited environment, as is the case for model predictive control and mixed-integer nonlinear programming. These features are fast detection of activity, efficient <b>warm-starting,</b> and progress on a primal-dual merit function at every iteration. We derive general convergence results for our approach and demonstrate its behavior through numerical studies...|$|R
30|$|The setup of {{this method}} {{contains}} two more models. The first one is a speech recognition model (MSTT), which can recognize phonemes for a given sound sequence. The other one, complementary to the first, is a speech generation model (MTTS), with the functionality of generating a speech signal for a given textual sentence. In the process of training using the DSL approach (described later), these two latter models (MSTT and MTTS) {{will be the only}} trainable ones. They will be initialized by means of either a <b>warm-start</b> or a semi <b>warm-start</b> mode and will have their weights updated according to a gradient descent-based algorithm.|$|E
40|$|In {{predictive}} control, a quadratic program (QP) {{needs to}} be solved at each sampling instant. We present a new <b>warm-start</b> strategy to solve a QP with an interior-point method whose data is slightly perturbed from the previous QP. In this strategy, an initial guess of the unknown variables in the perturbed problem is determined from the computed solution of the previous problem. We demonstrate the effectiveness of our <b>warm-start</b> strategy {{to a number of}} online benchmark problems. Numerical results indicate that the proposed technique depends upon the size of perturbation and it leads to a reduction of 30 – 74 % in floating point operations compared to a cold-start interior-point method...|$|E
40|$|We {{present a}} new {{adaptive}} algorithm for convex quadratic multicriteria optimization. The algorithm {{is able to}} adaptively refine the approximation to the set of efficient points {{by way of a}} <b>warm-start</b> interior-point scalarization approach. Numerical results show that this technique is an order of magnitude faster than a standard method used for this problem...|$|E
40|$|An {{effective}} means for analyzing {{the impact of}} novel operating schemes on power systems is time domain simulation, for example for investigating optimization-based curtailment of renewables to alleviate voltage violations. Traditionally, interior-point methods are used for solving the non-convex AC optimal power flow (OPF) problems arising {{in this type of}} simulation. This paper presents an alternative algorithm that better suits the simulation framework, because it can more effectively be <b>warm-started,</b> has linear computational and memory complexity in the problem size per iteration and globally converges to Karush-Kuhn-Tucker (KKT) points with a linear rate if they exist. The algorithm exploits a difference-of-convex-functions reformulation of the OPF problem, which can be performed effectively. Numerical results are presented comparing the method to state-of-the-art OPF solver implementations in MATPOWER, leading to significant speedups compared to the latter...|$|R
40|$|International audienceWe {{consider}} optimization {{problems that}} consist in minimizing a quadratic function under an atomic norm regularization or constraint. In {{the line of}} work on conditional gradient algorithms, we show that the fully corrective Frank-Wolfe (FCFW) algorithm — which is most naturally reformulated as a column generation algorithm in the regularized case — can be made particularly efficient for difficult problems in this family by solving the simplicial or conical subproblems produced by FCFW using a special instance of a classical active set algorithm for quadratic programming (Nocedal and Wright, 2006) that generalizes the min-norm point algorithm (Wolfe, 1976). Our experiments show that the algorithm takes advantages of <b>warm-starts</b> and of the sparsity induced by the norm, displays fast linear convergence, and clearly outperforms the state-of-the-art, for both complex and classical norms, including the standard group Lasso...|$|R
40|$|We {{present a}} {{branch-and-bound}} algorithm for discretely-constrained mathematical programs with equilibrium constraints (DC-MPEC). This is {{a class of}} bilevel programs with an integer program in the upper-level and a complementarity problem in the lower-level. The algorithm builds on the work by Gabriel et al. (Journal of the Operational Research Society 61 (9) : 1404 – 1419, 2010) and uses Benders decomposition to form a master problem and a subproblem. The new dynamic partition scheme that we present ensures that the algorithm converges to the global optimum. Partitioning is done to overcome the non-convexity of the Benders subproblem. In addition Lagrangean relaxation provides bounds that enable fathoming in the branching tree and <b>warm-starting</b> the Benders algorithm. Numerical tests show significantly reduced solution times compared to the original algorithm. When the lower level problem is stochastic our algorithm can easily be further decomposed using scenario decomposition. This is demonstrated on a realistic case...|$|R
40|$|We {{present a}} new primal-dual {{algorithm}} for convex quadratic multicriteria optimization. The algorithm {{is able to}} adaptively refine the approximation to the set of efficient points {{by way of a}} <b>warm-start</b> interior-point scalarization approach. Results of this algorithm when applied ona three-criteria real-world power plant optimization problem are reported, thereby illustrating the feasibility of this approach when used in practice...|$|E
40|$|We {{present a}} fast {{solution}} for performing multi-scale detail decomposition. The proposed method {{is based on}} an accelerated iterative shrinkage algorithm, able to process high definition color images in real-time on modern GPUs. Our strategy to accelerate the smoothing process is based on the use of first order proximal operators. We use the approximation to both designing suitable shrinkage operators as well as deriving a proper <b>warm-start</b> solution. The method supports full color filtering and can be implemented efficiently and easily on both the CPU and the GPU. We demonstrate the performance of the proposed approach on fast multi-scale detail manipulation of low and high dynamic range images and show that we get good quality results with reduced processing time. by running very few gradient shrinkage-reconstruction iterations. Similar methods such as [Xu et al. 2011] unfortunately need a relatively high number of iterations to produce a suitable result. Our strategy to improve half-quadratic solvers is based on two points: (1) designing new shrinkage operators that can produce a suitable photographic look for multi-scale detail manipulation at low iterations, (2) deriving an efficient <b>warm-start</b> solution. We show that we get good quality results with reduced processing time as can b...|$|E
30|$|Current {{automatic}} {{speech recognition}} (ASR) systems achieve over 90 – 95 % accuracy, depending on the methodology applied and datasets used. However, the level of accuracy decreases significantly when the same ASR system is used by a non-native speaker of the language to be recognized. At the same time, the volume of labeled datasets of non-native speech samples is extremely limited both in size and {{in the number of}} existing languages. This problem makes it difficult to train or build sufficiently accurate ASR systems targeted at non-native speakers, which, consequently, calls for a different approach that would make use of vast amounts of large unlabeled datasets. In this paper, we address this issue by employing dual supervised learning (DSL) and reinforcement learning with policy gradient methodology. We tested DSL in a <b>warm-start</b> approach, with two models trained beforehand, and in a semi <b>warm-start</b> approach with only one of the two models pre-trained. The experiments were conducted on English language pronounced by Japanese and Polish speakers. The results of our experiments show that creating ASR systems with DSL can achieve an accuracy comparable to traditional methods, while simultaneously making use of unlabeled data, which obviously is much cheaper to obtain and comes in larger sizes.|$|E
40|$|In {{this work}} we {{establish}} {{the relation between}} optimal control and training deep Convolution Neural Networks (CNNs). We show that the forward propagation in CNNs {{can be interpreted as}} a time-dependent nonlinear differential equation and learning as controlling the parameters of the differential equation such that the network approximates the data-label relation for given training data. Using this continuous interpretation we derive two new methods to scale CNNs with respect to two different dimensions. The first class of multiscale methods connects low-resolution and high-resolution data through prolongation and restriction of CNN parameters. We demonstrate that this enables classifying high-resolution images using CNNs trained with low-resolution images and vice versa and <b>warm-starting</b> the learning process. The second class of multiscale methods connects shallow and deep networks and leads to new training strategies that gradually increase the depths of the CNN while re-using parameters for initializations...|$|R
40|$|Quantization {{methods have}} been {{introduced}} to perform large scale approximate nearest search tasks. Residual Vector Quantization (RVQ) {{is one of the}} effective quantization methods. RVQ uses a multi-stage codebook learning scheme to lower the quantization error stage by stage. However, there are two major limitations for RVQ when applied to on high-dimensional approximate nearest neighbor search: 1. The performance gain diminishes quickly with added stages. 2. Encoding a vector with RVQ is actually NP-hard. In this paper, we propose an improved residual vector quantization (IRVQ) method, our IRVQ learns codebook with a hybrid method of subspace clustering and <b>warm-started</b> k-means on each stage to prevent performance gain from dropping, and uses a multi-path encoding scheme to encode a vector with lower distortion. Experimental results on the benchmark datasets show that our method gives substantially improves RVQ and delivers better performance compared to the state-of-the-art...|$|R
40|$|Sparse inverse {{covariance}} {{selection is}} {{a powerful tool for}} estimating sparse graphs in statistical learning, whose mathematical model is to maximize the regularized log-likelihood function. In this thesis, we consider introducing some non-convex regularizers to this problem. In particular, we first propose several non-convex regularized maximum likelihood estimation models. Using the specific structure of those regularizers, we then develop a DC programming approach for solving these models. We show that each subproblem of this approach is a weighted graphical lasso problem, and we propose a <b>warm-started</b> alternating direction method of multipliers to solve each subproblem. In addition, we propose a decomposition scheme by extending the exact covariance thresholding technique for graphical lasso to our general non-convex model, which enables us to solve large-scale problems efficiently. Finally, we compare the performance of our approach with some existing approaches on both randomly generated and real-life instances, and report some promising computational results...|$|R
40|$|The {{problem of}} {{assessing}} {{the performance of}} algorithms used for the minimization of an ℓ_ 1 -penalized least-squares functional, {{for a range of}} penalty parameters, is investigated. A criterion that uses the idea of `approximation isochrones' is introduced. Five different iterative minimization algorithms are tested and compared, as well as two <b>warm-start</b> strategies. Both well-conditioned and ill-conditioned problems are used in the comparison, and the contrast between these two categories is highlighted. Comment: 18 pages, 10 figures; v 3 : expanded version with an additional synthetic test problem...|$|E
40|$|We present new {{results for}} the {{conditional}} gradient method (also known as the Frank-Wolfe method). We derive computational guarantees for arbitrary step-size sequences, which are then applied to various step-size rules, including simple av-eraging and constant step-sizes. We also develop step-size rules and complexity bounds that depend naturally on the <b>warm-start</b> quality of the initial (and sub-sequent) iterates. Our results include complexity bounds for optimality bound gap and the Wolfe gap. Lastly, we present complexity bounds {{in the presence of}} approximate computation of gradients and/or linear optimization subproblem solutions. The results herein are mostly a condensation of the paper [1]. ...|$|E
40|$|We present new {{results for}} the Frank-Wolfe method (also known as the {{conditional}} gradient method). We derive computational guarantees for arbitrary step-size sequences, which are then applied to various step-size rules, including simple averaging and constant step-sizes. We also develop step-size rules and computational guarantees that depend naturally on the <b>warm-start</b> quality of the initial (and subsequent) iterates. Our results include computational guarantees for both duality/bound gaps and the so-called FW gaps. Lastly, we present complexity bounds {{in the presence of}} approximate computation of gradients and/or linear optimization subproblem solutions. Comment: Changed the name of the method from "conditional gradient" to "Frank-Wolfe...|$|E
40|$|We {{present an}} {{algorithm}} for the minimization of f: R n → R, {{assumed to be}} locally Lipschitz and continuously differentiable in an open dense subset D of R n. The objective f may be nonsmooth and/or nonconvex. The method {{is based on the}} gradient sampling algorithm (GS) of Burke, Lewis, and Overton [SIAM J. Optim., 15 (2005), pp. 751 - 779]. It differs, however, from previously proposed versions of GS in that it is variable-metric and only O(1) gradient evaluations are required per iteration. Numerical experiments illustrate that the algorithm is much more efficient than GS in that it consistently requires significantly fewer gradient evaluations. In addition, the adaptive sampling procedure allows for <b>warm-starting</b> of the quadratic subproblem solver so that the number of subproblem iterations per nonlinear iteration is also reduced. Global convergence of the algorithm is proved assuming that the Hessian approximations are positive definite and bounded, an assumption shown to be true for the proposed Hessian approximation updating strategies...|$|R
40|$|International audienceWe {{present a}} {{trajectory}} optimization framework for legged locomotion on rough terrain. We jointly optimize {{the center of}} mass motion and the foothold locations, while considering terrain conditions. We use a terrain costmap to quantify the desirability of a foothold location. We increase the gait's adaptability to the terrain by optimizing the step phase duration and modulating the trunk attitude, resulting in motions with guaranteed stability. We show {{that the combination of}} parametric models, stochastic-based exploration and receding horizon planning allows us to handle the many local minima associated with different terrain conditions and walking patterns. This combination delivers robust motion plans without the need for <b>warm-starting.</b> Moreover, we use soft-constraints to allow for increased flexibility when searching in the cost landscape of our problem. We showcase the performance of our trajectory optimization framework on multiple terrain conditions and validate our method in realistic simulation scenarios and experimental trials on a hydraulic, torque controlled quadruped robot...|$|R
40|$|We {{develop a}} {{framework}} for <b>warm-starting</b> Bayesian optimization, that reduces the solution time required to solve an optimization problem that is one in a sequence of related problems. This is useful when optimizing the output of a stochastic simulator that fails to provide derivative information, for which Bayesian optimization methods are well-suited. Solving sequences of related optimization problems arises when making several business decisions using one optimization model and input data collected over different time periods or markets. While many gradient-based methods can be warm started by initiating optimization at {{the solution to the}} previous problem, this warm start approach does not apply to Bayesian optimization methods, which carry a full metamodel of the objective function from iteration to iteration. Our approach builds a joint statistical model of the entire collection of related objective functions, and uses a value of information calculation to recommend points to evaluate. Comment: To Appear in the Proc. of the 2016 Winter Simulation Conferenc...|$|R
