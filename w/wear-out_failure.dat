22|34|Public
25|$|In {{addition}} to the <b>wear-out</b> <b>failure</b> modes common to all fluorescent lamps, the electronic ballast may fail, since it {{has a number of}} component parts. Ballast failures are usually due to overheating and may be accompanied by discoloration or distortion of the ballast enclosure, odors, or smoke. The lamps are internally protected and are meant to fail safely {{at the end of their}} lives. Industry associations are working toward advising consumers of the different failure modes of CFLs compared to incandescent lamps, and to develop lamps with inoffensive failure modes. New North American technical standards aim to eliminate smoke or excess heat at the end of lamp life.|$|E
5000|$|... #Caption: The 'bathtub curve' hazard {{function}} (blue, upper solid line) is {{a combination}} of a decreasing hazard of early failure (red dotted line) and an increasing hazard of <b>wear-out</b> <b>failure</b> (yellow dotted line), plus some constant hazard of random failure (green, lower solid line).|$|E
50|$|In {{addition}} to the <b>wear-out</b> <b>failure</b> modes common to all fluorescent lamps, the electronic ballast may fail, since it {{has a number of}} component parts. Ballast failures are usually due to overheating and may be accompanied by discoloration or distortion of the ballast enclosure, odors, or smoke. The lamps are internally protected and are meant to fail safely {{at the end of their}} lives. Industry associations are working toward advising consumers of the different failure modes of CFLs compared to incandescent lamps, and to develop lamps with inoffensive failure modes. New North American technical standards aim to eliminate smoke or excess heat at the end of lamp life.|$|E
50|$|The {{third part}} is an {{increasing}} failure rate, known as <b>wear-out</b> <b>failures.</b>|$|R
25|$|Electrolytic {{capacitors}} {{with solid}} electrolyte {{do not have}} <b>wear-out</b> <b>failures</b> so {{they do not have}} a lifetime specification in the sense of non-solid aluminum electrolytic capacitors.|$|R
25|$|The {{published}} {{figures show}} that both capacitor types, tantalum and aluminum, are reliable components, comparable with other electronic components and achieving safe operation for decades under normal conditions. But a great difference exists {{in the case of}} <b>wear-out</b> <b>failures.</b> Tantalum capacitors with solid electrolyte have no wear-out mechanism so that the constant failure rate is least, up to the point when all capacitors fail. Electrolytic capacitors with non-solid electrolyte, however, have a limited time of constant random failures up to that point when the <b>wear-out</b> <b>failures</b> start. This time of the constant random failure rate corresponds with the lifetime or service life of “wet” aluminum electrolytic capacitors.|$|R
50|$|Solid {{aluminum}} {{electrolytic capacitors}} have no known inherent <b>wear-out</b> <b>failure</b> mechanism. In addition the solid electrolyte offers {{a very long}} time stability of the electrical and thermal characteristics. They remain constant throughout {{a very long time}} without time-depending changes. The dependence of the impedance and ESR at lower temperatures is very low compared with non-solid electrolytes. The capacitors are insensible against high inrush or switch-off currents and can be operated without a series resistor, whereby the SAL electrolytic capacitors at high current loads have a much higher reliability with respect to tantalum electrolytic capacitors. In addition, the dielectric aluminum oxide in combination with the electrolyte manganese dioxide has a relatively high voltage resistance against wrong polarity.|$|E
3000|$|... 0 / 18  =  920  h, {{while the}} {{critical}} point between random failure phase and <b>wear-out</b> <b>failure</b> phase was located at about t [...]...|$|E
30|$|The {{results of}} trend test are {{consistent}} with the estimation output of TTT diagram. The system failure time data follows the trend of bathtub curve, and undergoes three phases—early failure, random failure and <b>wear-out</b> <b>failure.</b>|$|E
2500|$|Tantalum {{electrolytic}} capacitors with solid manganese dioxide electrolyte {{do not have}} <b>wear-out</b> <b>failures</b> so {{they do not have}} a lifetime specification in the sense of non-solid aluminum {{electrolytic capacitors}}. Also, tantalum capacitors with non-solid electrolyte, the [...] "wet tantalums", do not have a lifetime specification because they are hermetically sealed and evaporation of electrolyte is minimized.|$|R
50|$|The life time, service life, load life or {{useful life}} of {{electrolytic}} capacitors {{is a special}} characteristic of non-solid electrolytic capacitors, whose liquid electrolyte can evaporate over the time leading to <b>wear-out</b> <b>failures.</b> Solid tantalum capacitors with MnO2 electrolyte have no wear-out mechanism so that the constant failure rate least {{up to the point}} all capacitors have failed. They don’t have a life time specification like non-solid Al-e-caps.|$|R
50|$|The life time, service life, load life or {{useful life}} of {{electrolytic}} capacitors {{is a special}} characteristic of non-solid electrolytic capacitors, especially non-solid aluminum electrolytic capacitors which liquid electrolyte can evaporate over the time leading to <b>wear-out</b> <b>failures.</b> Solid niobium capacitors with manganese dioxide electrolyte have no wear-out mechanism so the constant failure rate lasts {{up to the point}} all capacitors have failed. They don’t have a life time specification like non-solid aluminum electrolytic capacitors.|$|R
40|$|ABSTRACT. In this study, {{aiming for}} asset {{management}} of large-scaled information systems that support infrastructures, the failure generation process of <b>wear-out</b> <b>failure</b> component, whose failure rate changes with time, is formulated with a Weibull hazard model. Information systems {{are composed of}} many devices. In order to consider the heterogeneity of the hazard rate of each device, the random proportional Weibull hazard model, which expresses the heterogeneity of the hazard rate in random variables is proposed. Furthermore, the authors develop a methodology that expresses the heterogeneity of hazard rates in gamma distribution, as well as estimates unknown parameters and the heterogeneity of hazard rates contained in the hazard model. Finally, using the traffic control system of expressways, the failure rate for <b>wear-out</b> <b>failure</b> component is estimated from actual failure history data, and {{the validity of the}} methodology is investigated through a case study...|$|E
3000|$|..., i.e. k and l, {{shall be}} {{determined}} {{based on the}} system’s failure time data and trend estimation. Second, the first k time data at early failure phase and the last n–l time data at <b>wear-out</b> <b>failure</b> phase are adopted for interval estimation of Weibull process, and the extreme value of estimation interval {{is regarded as the}} optimization range of parameters a [...]...|$|E
30|$|Continuous bowel {{decompression}} management {{using an}} ITT was simple, less invasive, and lower cost compared with diverting stoma. Moreover, this management did not disturb the patients’ growth {{as shown by}} their body weight gain during the waiting period for curative surgery. In the case of malposition or <b>wear-out</b> <b>failure</b> of an ITT, some tubes had to be repositioned or replaced during the waiting period, which was easily carried out at our outpatient clinic.|$|E
40|$|Flexible {{manufacturing}} cells (FMCs) often {{operate with}} increasing failure rate due to extensive utilization and wear-outs of equipment. While maintenance plans can eliminate <b>wear-out</b> <b>failures,</b> random failures are still unavoidable. This paper discusses {{a procedure that}} combines simulation and analytical models to analyze the effects of corrective, preventive, and opportunistic maintenance policies on productivity of a flexible manufacturing cell. The production output rate of an FMC, which {{is a function of}} availability, is determined under different maintenance policies and mean time between failures. Maintenance Reliability FMC Simulation...|$|R
40|$|To {{meet the}} {{challenge}} of dramatically market, manufacturing process requires the high production efficiency and the stability of the machining accuracy. Thus availability is the one of crucial measurements of complex equipments. This study addresses the problem of evaluating the performance of equipment which subject to both <b>wear-out</b> <b>failures</b> and random failures. Moreover, the different degradation states of the equipment are analyzed and then, the model of the states is established using generalized stochastic Petri nets and the analytical method for assessing the availability is also built up. Finally, the availabilities under different maintenance strategies are compared and the effectiveness and efficiency of the method is verified...|$|R
25|$|The lifetime, service life, load life or {{useful life}} of {{electrolytic}} capacitors {{is a special}} characteristic of non-solid aluminum electrolytic capacitors, whose liquid electrolyte can evaporate over time. Lowering the electrolyte level influences the electrical parameters of the capacitors. The capacitance decreases and the impedance and ESR increase with decreasing amounts of electrolyte. This very slow electrolyte drying-out depends on the temperature, the applied ripple current load, and the applied voltage. The lower these parameters compared with their maximum values the longer the capacitor's “life”. The “end of life” point {{is defined by the}} appearance of <b>wear-out</b> <b>failures</b> or degradation failures when either capacitance, impedance, ESR or leakage current exceed their specified change limits.|$|R
40|$|Abstract: Four sliding wear {{tests were}} {{conducted}} using a ball-on-disc tester at room temperature to investigate sliding wear processes in which Fe powders {{were added to the}} lubricant. The ball sample was a standard E 52100 ball of 8 mm in diameter. The disc samples were made of 5140 steel and heat-treated to a hardness of 52 HRC. Wear debris analysis and surface analysis of the tested samples were carried out to investigate the wear particles generated from the wear process and to study the effects of the soft Fe powder. It was proved that Fe powders in the lubricant {{played an important role in}} the wear process and had a significant effect on the main wear mechanisms, resulting in <b>wear-out</b> <b>failure...</b>|$|E
40|$|Abstract. Plasma-sprayed HA {{coatings}} (HACs) on Ti- 6 Al- 4 V substrates with post-heat treatments {{were employed}} to improve the microstructural homogeneity, bonding strength and reliability of the HACs. A defect-healing effect can be recognized to diminish coating defects with the hydrothermal treatment, and 150 °C hydrothermally-treated HACs shows a significant improved bonding strength than 600 °C vacuum heating HACs because of its dense structure from the defect-healing effect. Low-temperature hydrothermal treatment demonstrates a more superior crystallization effect than vacuum heat treatment. Based on the statistical calculation by the Weibull distribution function, hydrothermally-treated HACs show a <b>wear-out</b> <b>failure</b> with a higher Weibull modulus than vacuum heating HACs. Hydrothermally-treated HACs possessed better reliability {{can be attributed to}} the suppression of defect content (about 2. 6 - 3. 2 volume %) and its smaller strength data fluctuation...|$|E
40|$|The {{main reason}} for the {{premature}} breakdown of today's electronic products (computers, cars, tools, appliances, etc.) is {{the failure of the}} components used to build these products. Today professionals are looking for effective ways to minimize the degradation of electronic components to help ensure longer-lasting, more technically sound products and systems. This practical book offers engineers specific guidance on how to design more reliable components and build more reliable electronic systems. Professionals learn how to optimize a virtual component prototype, accurately monitor product reliability during the entire production process, and add the burn-in and selection procedures that are the most appropriate for the intended applications. Moreover, the book helps system designers ensure that all components are correctly applied, margins are adequate, <b>wear-out</b> <b>failure</b> modes are prevented during the expected duration of life, and system interfaces cannot lead to failure...|$|E
40|$|Abstract: According to {{abrasion}} {{process of}} machine equipment parts of {{overhead line system}} for traction power supply and characters of <b>wear-out</b> <b>failures,</b> considering on factor changes {{of all kinds of}} production conditions in practical opera-tion, set permit of abrasion loss in coordinating with normal distribution, and abrasion loss of parts as random variables, and then establish the relation between maintenance costs and random variable of abrasion loss with the principle of low-est maintenance cost, thus concluding with the optimum maintenance abrasion value. Then the functional relation between abrasion loss and running time can be concluded with the gray model, thus the decision model in the optimum period with the lowest maintenance cost can be concluded...|$|R
50|$|The lifetime, service life, load life or {{useful life}} of {{electrolytic}} capacitors {{is a special}} characteristic of non-solid aluminum electrolytic capacitors, whose liquid electrolyte can evaporate over time. Lowering the electrolyte level influences the electrical parameters of the capacitors. The capacitance decreases and the impedance and ESR increase with decreasing amounts of electrolyte. This very slow electrolyte drying-out depends on the temperature, the applied ripple current load, and the applied voltage. The lower these parameters compared with their maximum values the longer the capacitor's “life”. The “end of life” point {{is defined by the}} appearance of <b>wear-out</b> <b>failures</b> or degradation failures when either capacitance, impedance, ESR or leakage current exceed their specified change limits.|$|R
40|$|Reliability of {{base metal}} {{electrode}} (BME) multilayer ceramic capacitors (MLCCs) that until recently were used mostly in commercial applications, have been improved substantially by using new materials and processes. Currently, the inception of intrinsic <b>wear-out</b> <b>failures</b> in high quality capacitors became {{much greater than the}} mission duration in most high-reliability applications. However, in capacitors with defects degradation processes might accelerate substantially and cause infant mortality failures. In this work, a physical model that relates the presence of defects to reduction of breakdown voltages and decreasing times to failure has been suggested. The effect of the defect size has been analyzed using a thermal runaway model of failures. Adequacy of highly accelerated life testing (HALT) to predict reliability at normal operating conditions and limitations of voltage acceleration are considered. The applicability of the model to BME capacitors with cracks is discussed and validated experimentally...|$|R
40|$|The Dawn ion {{thrusters}} {{meet all}} of the performance and life requirements for the Dawn mission. FT 001 has two magnets in sideways in the middle magnet ring, but this has only a minor impact on the thruster performance. Electron-backstreaming is the first <b>wear-out</b> <b>failure</b> mechanism for the Dawn ion thrusters - PFA Modeling Indicates: a) The mission cannot be completed with a single thruster; b) The probability for mission success based on accelerator grid wear is > 99 % for the worst case thruster usage for nominal thruster beamlet profiles; and c) FT 002 has the most peaked beam profile which impacts the thruster s throughput capability. PFA results indicate that this reduces the probability for mission success based on accelerator grid wear to 97 % based on the worst case thruster usage. The wear-out probability is zero if all three thrusters are used during the mission...|$|E
40|$|This paper {{describes}} the qualification of the 50 V, 0. 5 m GaN on SiC {{process that has}} been released at the III-V fab of UMS in Ulm in cooperation with IAF and NXP Semiconductors. The qualification at NXP is split into two parts: Part 1 : investigation of die related <b>wear-out</b> <b>failure</b> mechanisms using a power device with 7. 2 mm gate width and Part 2 : final process release using the first product of NXP, a 50 W wide band amplifier. The aim of the first part {{is to determine the}} acceleration factors for the major electric failure mechanism. These will be then used to define the qualification program of the wide band amplifier which will also include all package related tests. In this paper we will show how the tests are defined using the mission profile of the product. In addition, we will show the results. They are compared to results published in literature and are shown to be very promising...|$|E
40|$|We study {{distributed}} {{detection and}} fusion in sensor networks with bathtub-shaped failure (BSF) {{rate of the}} sensors which may or not send data to the Fusion Center (FC). The reliability of semiconductor devices is usually represented by the failure rate curve (called the “bathtub curve”), which {{can be divided into}} the three following regions: initial failure period, random failure period, and <b>wear-out</b> <b>failure</b> period. Considering the possibility of the failed sensors which still work but in a bad situation, it is unreasonable to trust the data from these sensors. Based on the above situation, we bring in new characteristics to failed sensors. Each sensor quantizes its local observation into one bit of information which is sent to the FC for overall fusion because of power, communication, and bandwidth constraints. Under this sensor failure model, the Extension Log-likelihood Ratio Test (ELRT) rule is derived. Finally, the ROC curve for this model is presented. The simulation results show that the ELRT rule improves the robust performance of the system, compared with the traditional fusion rule without considering sensor failures...|$|E
40|$|International audienceFuture chip multiprocessors (CMPs) will {{be capable}} of deconfiguring faulty units in order to permit {{continued}} operation {{in the presence of}} <b>wear-out</b> <b>failures.</b> However, the unforeseen downside is pipeline imbalance due to other portions of the pipeline now being overprovisioned with respect to the deconfigured functionality. We propose PowerTransfer, a novel CMP architecture that dynamically redistributes the chip power under pipeline imbalances that arise from deconfiguring faulty units. Through rebalancing – achieved by temporary, symbiotic deconfiguration of additional functionality within the degraded core – power is harnessed for use elsewhere on the chip. This additional power is dynamically transferred to portions of the multi-core chip that can realize a performance boost from turning on previously dormant microarchitectural features. We demonstrate that a realistic PowerTransfer manager achieves chip-wide performance improvements of up to 25 % compared to architectures that simply deconfigure faulty units without regard to the resulting inefficiency...|$|R
40|$|Memories today expose an all-or-nothing {{correctness}} {{model that}} incurs significant costs in performance, energy, area, and design complexity. But not all applications need high-precision storage {{for all of}} their data structures all of the time. This paper proposes mechanisms that enable applications to store data approximately and shows that doing so can improve the performance, lifetime, or density of solid-state memories. We propose two mechanisms. The first allows errors in multi-level cells by {{reducing the number of}} programming pulses used to write them. The second mechanism mit-igates <b>wear-out</b> <b>failures</b> and extends memory endurance by mapping approximate data onto blocks that have exhausted their hardware error correction resources. Simulations show that reduced-precision writes in multi-level phase-change memory cells can be 1. 7 × faster on average and using failed blocks can improve array lifetime by 23 % on average with quality loss under 10 %...|$|R
40|$|Integrated Circuits (ICs) are an {{essential}} part of nearly every electronic device. From toys to appliances, spacecraft to power plants, modern society truly depends on the reliable operation of billions of ICs around the world. The steady shrinking of IC transistors over past decades has enabled drastic improvements in IC performance while reducing area and power consumption. However, with continued scaling of semiconductor fabrication processes, failure sources of many types are becoming more pronounced and are increasingly affecting system operation. Additionally, increasing variation during fabrication also increases the difficulty of yielding chips in a cost-effective manner. Finally, phenomena such as early-life and <b>wear-out</b> <b>failures</b> pose new challenges to ensuring robustness. One approach for ensuring robustness centers on performing test during run-time, identifying the location of any defects, and repairing, replacing, or avoiding the affected portion of the system. Leveraging the existing design-for-testability (DFT) structures, thorough tests that target these delay defects are applied using the scan logic. Testing is performed periodically to minimize user-perceived performance loss, and if testing detects any failures, on-chip diagnosis is performed to localize the defect to the level of repair, replacement, or avoidance. In this dissertation, an on-chip diagnosis solution using a fault dictionary is described and validated through a large variety of experiments. Conventional fault dictionary approaches can be used to locate failures but are limited to simplistic fail behaviors due to the significant computational resources required for dictionary generation and memory storage. To capture the misbehaviors expected from scaled technologies, including early-life and <b>wear-out</b> <b>failures,</b> the Transition-X (TRAX) fault model is introduced. Similar to a transition fault, a TRAX fault is activated by a signal level transition or glitch, and produces the unknown value X when activated. Recognizing that the limited options for runtime recovery of defective hardware relax the conventional requirements for defect localization, a new fault dictionary is developed to provide diagnosis localization only to the required level of the design hierarchy. On-chip diagnosis using such a hierarchical dictionary is performed using a new scalable hardware architecture. To reduce the computation time required to generate the TRAX hierarchical dictionary for large designs, the incredible parallelism of graphics processing units (GPUs) is harnessed to provide an efficient fault simulation engine for dictionary construction. Finally, the on-chip diagnosis process is evaluated for suitability in providing accurate diagnosis results even when multiple concurrent defects are affecting a circuit...|$|R
30|$|However, the {{lifetime}} failure data are often non-monotonic, {{and have a}} bathtub-shaped failure rate, which {{can be divided into}} three phases—early failure, random failure, and <b>wear-out</b> <b>failure.</b> For this reason, many scholars studied the phenomenon. Chen (2000) put forward a new two-parameter Weibull distribution model, in which the failure rate function followed the trend of bathtub curve when the shape parameter satisfied a certain condition. Later, Xie et al. (2002) and Zhang (2004) expanded the two-parameter model above by introducing the third parameter, and proposed the three-parameter Weibull distribution models independently. The results indicated that those models can reach high accuracy in fitting the trend of bathtub curve more easily than traditional models. Lai et al. (2003), based on Weibull distribution and Extreme Value Type I distribution, proposed an improved Weibull distribution model, which also described the trend of bathtub curve very well. They also completed the model’s parameter estimation by means of Weibull probability plot. El-Gohary et al. (2013) gave a new distribution known as the generalized Gompertz distribution by dealing with a new generalization of exponential. Generalized Gompertz distribution, and generalized exponential distributions, which had bathtub curve failure rate depending upon the shape parameter. And the maximum likelihood estimators of the parameters were derived using a simulations study.|$|E
40|$|The ST 4 /Champollion {{mission is}} {{designed}} to rendezvous with and land on the comet Tempel 1 and return data from the first-ever sampling of a comet surface. Ion propulsion is an enabling technology for this mission. The ion propulsion system on ST 4 consists of three ion engines each essentially identical to the single engine that flew on the DS 1 spacecraft. The ST 4 propulsion system will operate at a maximum input power of 7. 5 kW (3. 4 times greater than that demonstrated on DS 1), will produce a maximum thrust of 276 mN, and will provide a total (Delta) V of 11. 4 km/s. To accomplish this the propulsion system will carry 385 kg of xenon. All three engines will be operated simultaneously for the first 168 days of the mission. The nominal mission requires that each engine be capable of processing 118 kg. If one engine fails after 168 days, the remaining two engines can perform the mission, but must be capable of processing 160 kg of xenon, or twice the original thruster design requirement. Detailed analyses of the thruster <b>wear-out</b> <b>failure</b> modes coupled with experience from long-duration engine tests indicate that the thrusters have a high probability of meeting the 160 -kg throughput requirement...|$|E
40|$|In this paper, {{the concept}} of {{terotechnology}} and the formulation of l$e cycle cost has been takenfiom {{the point of view}} ofthe user as against that of the manufacturer, and the stages of design, development of prototypes, manufacture and testing of the machine have not been considered. This is. felt appropriate since terotechnology has to date found greater application (and will continue to do so, except for military systems and installations wherein it has possibly found the greatest application) in the case of ’ large capital equipment and machines, for example for process plants, integrated iron and steelworks, power plants etc. The hazard curve provides the basis,for the estimation of the time-dependent maintenance cost incurred ouer the life cycle of an equipment. Accordinyly a system study of the hazard rate of power units of a thermal power plant was undertaken and has been presented. Using the mod$ied ‘bath-tub ’ curve obtained from the case study and the Weibull model, an equation of the total life cycle cost has been developed. Thus the model takes into account deterioration of components and system performance over time. The model brings out the efficacy of preventive main-tenance action in the form of condition monitoring and shows that the total life cycle can be increased i f appropriate preventive maintenance actions are taken in the random failure and <b>wear-out</b> <b>failure</b> regions. ...|$|E
40|$|Abstract—Technology scaling {{improvement}} is affecting {{the reliability of}} ICs due to increases in static and dynamic variations as well as <b>wear-out</b> <b>failures.</b> This is particularly true for caches that dominate the area of modern processors and are built with minimum-sized, but prone to failure, SRAM cells. Our attempt to address this cache reliability challenge is an analytical model for determining the implications on cache missrate of block-disabling due to random cell failure. The proposed model is distinct from previous work in that is an exact model rather than an approximation {{and yet it is}} simpler than previous work. Its simplicity stems from the lack of fault-maps in the analysis. The model capabilities are illustrated through a study of cache miss-rate trends in future technology nodes. The model is also used to determine the accuracy of a random fault map methodology. The analysis reveals, for the assumptions, programs and cache configuration used in this study, a surprising result: a relative small number of random fault maps, 100 - 1000, is sufficient to obtain accurate mean and standard-deviation values for the missrate. Additional investigation revealed that the cause of this behavior is a high correlation between the number of accesses and access distribution between cache sets. I...|$|R
40|$|Aggressive {{scaling of}} CMOS {{transistors}} has enabled extensive system integration and building {{faster and more}} efficient systems. On the flip side, this has resulted in {{an increasing number of}} devices that fail in shipped components in-the-field {{for a variety of reasons}} including soft errors, <b>wear-out</b> <b>failures,</b> and infant mortality. The pervasiveness of the problem across a broad market demands low cost and generic reliability solutions, precluding traditional solutions that employed excessive redundancy or piecemeal solutions that address only a few failure modes. This dissertation presents SWAT (SoftWare Anomaly Treatment), a low cost resiliency solution that effectively handles hardware faults while incurring low cost during the common mode of faultfree operations. SWAT is based on two key observations about the design of resilient systems. First, only those hardware faults that affect software need to be handled and second, since the common mode of operation is fault-free, fault-free execution should incur near-zero overheads. SWAT thus uses novel zero to low cost hardware and software monitors that watch for anomalous software behavior to detect hardware faults. SWAT then relies on hardware support for checkpointing and rollback recovery. When dealing with fault recovery in the presence of I/O, we identify that existing software-level mechanisms that handle output buffering fall short. This dissertation therefore propose...|$|R
40|$|Several {{operational}} {{strategies for}} offshore wind farms {{have been established}} and explored {{in order to improve}} understanding of operational costs with a focus on heavy lift vessel strategies. Additionally, an investigation into the uncertainty surrounding failure behaviour has been performed identifying the robustness of different strategies. Four operational strategies were considered: fix on fail, batch repair, annual charter and purchase. A range of failure rates have been explored identifying the key cost drivers and under which circumstances an operator would choose to adopt them. When failures are low, the fix on fail and batch strategies perform best and allow flexibility of operating strategy. When failures are high, purchase becomes optimal and is least sensitive to increasing failure rate. Late life failure distributions based on mechanical and electrical components behaviour have been explored. Increased operating costs because of <b>wear-out</b> <b>failures</b> have been quantified. An increase in minor failures principally increase lost revenue costs and can be mitigated by deploying increased maintenance resources. An increase in larger failures primarily increases vessel and repair costs. Adopting a purchase strategy can negate the vessel cost increase; however, significant cost increases are still observed. Maintenance actions requiring the use of heavy lift vessels, currently drive train components and blades are identified as critical for proactive maintenance to minimise overall maintenance costs...|$|R
