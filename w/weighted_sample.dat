222|589|Public
5000|$|For {{uncorrelated}} observations with variances , {{the variance}} of the <b>weighted</b> <b>sample</b> mean is ...|$|E
50|$|As a side note, other {{approaches}} {{have been described}} to compute the <b>weighted</b> <b>sample</b> variance.|$|E
5000|$|Consequently, {{if all the}} {{observations}} have equal variance, , the <b>weighted</b> <b>sample</b> mean will have variance ...|$|E
40|$|We {{study the}} classic problem of {{estimating}} {{the sum of}} n variables. The traditional uniform sampling approach requires a linear number of samples to provide any non-trivial guarantees on the estimated sum. In this paper we consider various sampling methods besides uniform sampling, in particular sampling a variable with probability proportional to its value, referred to as linear <b>weighted</b> <b>sampling.</b> If only linear <b>weighted</b> <b>sampling</b> is allowed, we show an algorithm for estimating sum with Õ(√ n) samples, and it is almost optimal {{in the sense that}} Ω (√ n) samples are necessary for any reasonable sum estimator. If both uniform <b>sampling</b> and linear <b>weighted</b> <b>sampling</b> are allowed, we show a sum estimator with Õ (3 √ n) samples. More generally, we may allow general <b>weighted</b> <b>sampling</b> where the probability of sampling a variable is proportional to any function of its value. We prove a lower bound of Ω (3 √ n) samples for any reasonable sum estimator using general <b>weighted</b> <b>sampling,</b> which implies that our algorithm combining uniform and linear <b>weighted</b> <b>sampling</b> is an almost optimal sum estimator...|$|R
40|$|We {{consider}} importance sampling as well {{as other}} properly <b>weighted</b> <b>samples</b> with respect to a target distribution π from a different point of view. By considering the associated weights as sojourn times until the next jump, we define appropriate jump processes. When the original sample sequence forms an ergodic Markov chain, the associated jump process is an ergodic semi [...] Markov process with stationary distribution π. Hence, the type of convergence of properly <b>weighted</b> <b>samples</b> may be stronger than that of weighted means. In particular, when the samples are independent and the mean weight is bounded above, we describe a slight modification in order to achieve exact (<b>weighted)</b> <b>samples</b> from the target distribution. Comment: 15 pagers, 1 figur...|$|R
40|$|We {{study the}} problem of {{approximately}} answering aggregation queries using sampling. We observe that uniform sampling performs poorly when {{the distribution of the}} aggregated attribute is skewed. To address this issue, we introduce a technique called outlier-indexing. Uniform sampling is also inefective for queries with low selectivity. We rely on <b>weighted</b> <b>sampling</b> based on workload information to overcome this shortcoming. We demonstrate that a combination of outlier-indexing with <b>weighted</b> <b>sampling</b> can be used to answer aggregation queries with significantly reduced approximation error compared to either uniform <b>sampling</b> or <b>weighted</b> <b>sampling</b> alone. We discuss the implementation of these techniques on Microsoft's SQL Server; and present experimental results that demonstrate the merits of our techniques...|$|R
5000|$|The biased <b>weighted</b> <b>sample</b> {{variance}} [...] {{is defined}} similarly {{to the normal}} biased sample variance : ...|$|E
5000|$|... 4. Verify that {{covariates}} are balanced across {{treatment and}} comparison {{groups in the}} matched or <b>weighted</b> <b>sample</b> ...|$|E
50|$|Similarly to <b>weighted</b> <b>sample</b> variance, {{there are}} two {{different}} unbiased estimators {{depending on the type of}} the weights.|$|E
40|$|AbstractIn this paper, {{we discuss}} {{sampling}} and reconstruction of signals in the weighted reproducing kernel space {{associated with an}} idempotent integral operator. We show that any signal in that space can be stably reconstructed from its <b>weighted</b> <b>samples</b> taken on a relatively-separated set with sufficiently small gap. We also develop an iterative reconstruction algorithm for the reconstruction of a signal from its <b>weighted</b> <b>samples</b> taken on a relatively-separated set with sufficiently small gap...|$|R
3000|$|... where (X̆^(n)_k+ 1, π̆^(n)_k+ 1) {{represents}} a target sample with its associated weight and N̆_k+ 1 {{is the number}} of <b>weighted</b> <b>samples.</b>|$|R
3000|$|... |yt− 1). They utilise either <b>weighted</b> <b>samples</b> of a plug-in {{probability}} assignment {{based on}} prior/likelihood or a mixture model based density [76].|$|R
5000|$|In particular, if {{the means}} are equal, , then the {{expectation}} of the <b>weighted</b> <b>sample</b> mean will be that value, ...|$|E
5000|$|In a <b>weighted</b> <b>sample,</b> each {{row vector}} [...] (each set of single {{observations}} {{on each of}} the K random variables) is assigned a weight [...]|$|E
5000|$|In a <b>weighted</b> <b>sample,</b> each vector [...] (each set {{of single}} {{observations}} {{on each of}} the K random variables) is assigned a weight [...] Without loss of generality, assume that the weights are normalized: ...|$|E
5000|$|Each machine {{does its}} own <b>weighted</b> <b>sampling</b> using key [...] as {{described}} in previous section and produces a sample of size <= k items.|$|R
40|$|We analyze a batched {{variant of}} Stochastic Gradient Descent (SGD) with <b>weighted</b> <b>sampling</b> {{distribution}} for smooth and non-smooth objective functions. We show that by distributing the batches computationally, a significant speedup in the convergence rate is provably possible compared to either batched <b>sampling</b> or <b>weighted</b> <b>sampling</b> alone. We propose several computationally efficient schemes to approximate the optimal weights, and compute proposed sampling distributions explicitly {{for the least}} squares and hinge loss problems. We show both analytically and experimentally that substantial gains can be obtaine...|$|R
40|$|We {{present a}} <b>weighted</b> <b>sampling</b> Moran {{particle}} system {{model for the}} numerical solving of a class of Feynman-Kac formulae which arise in different fields. Our major motivation was from nonlinear filtering, but our approach is context free. We will show that under certain regularity conditions the resulting interacting particle scheme converges to the considered nonlinear equations. In the setting of nonlinear filtering, the -convergence exponent resulting from our proof also improves recent results on other particle interpretations of these equations. <b>Weighted</b> <b>sampling</b> Moran processes Measure valued dynamical systems defined by Feynman-Kac formulae Quantitative weak propagation of chaos Nonlinear filtering...|$|R
5000|$|The <b>weighted</b> <b>sample</b> mean, , with {{normalized}} weights (weights summing to one) {{is itself}} a random variable. Its expected value and standard deviation {{are related to the}} expected values and standard deviations of the observations as follows, ...|$|E
5000|$|Typically when a mean is {{calculated}} {{it is important}} to know the variance and standard deviation about that mean. When a weighted mean [...] is used, the variance of the <b>weighted</b> <b>sample</b> is different from the variance of the unweighted sample.|$|E
5000|$|For example, if values [...] {{are drawn}} from the same distribution, then we can treat this set as an {{unweighted}} sample, or we can treat it as the <b>weighted</b> <b>sample</b> [...] with corresponding weights , and we get the same result either way.|$|E
40|$|<b>Weighted</b> <b>sampling</b> without {{replacement}} {{has proved}} to be a very important tool in designing new algorithms. Efraimidis and Spirakis (IPL 2006) presented an algorithm for <b>weighted</b> <b>sampling</b> without replacement from data streams. Their algorithm works under the assumption of precise computations over the interval [0, 1]. Cohen and Kaplan (VLDB 2008) used similar methods for their bottom-k sketches. Efraimidis and Spirakis ask as an open question whether using finite precision arithmetic impacts the accuracy of their algorithm. In this paper we show a method to avoid this problem by providing a precise reduction from k-sampling without replacement to k-sampling with replacement. We call the resulting method Cascade Sampling. Comment: 8 pages, 1 figur...|$|R
40|$|Adjustment by minimum {{discriminant}} information {{provides an}} approach to linking test forms {{in the case of}} a nonequivalent groups design with no satis-factory common items. This approach employs background information on individual examinees in each administration so that <b>weighted</b> <b>samples</b> of examinees form pseudo-equivalent groups in the sense that they resemble samples from equivalent groups. Linking methods for equivalent groups are then applied to the <b>weighted</b> <b>samples.</b> To illustrate the approach, 29 adminis-trations from a testing program are linked via the method of pseudo-equivalent groups. Because the forms used are currently linked by use of kernel equating, it is possible to compare the reasonableness of results from pseudo-equivalent groups to results from kernel equating...|$|R
40|$|AbstractThe bin packing {{problem is}} defined as follows: given a set of n items with sizes 0 0, we present an {{algorithm}} Aϵ that has sampling access to the input instance and outputs a value k such that Copt≤k≤(1 +ϵ) ⋅Copt+ 1, where Copt {{is the cost of}} an optimal solution. It is clear that uniform sampling by itself will not allow a sublinear-time algorithm in this setting; a small number of items might constitute most of the total weight and uniform samples will not hit them. In this work we use <b>weighted</b> <b>samples,</b> where item i is sampled with probability proportional to its weight: that is, with probability wi/∑iwi. In the presence of <b>weighted</b> <b>samples,</b> the approximation algorithm runs in Õ(n⋅poly(1 /ϵ)) +g(1 /ϵ) time, where g(x) is an exponential function of x. When both <b>weighted</b> <b>sampling</b> and uniform sampling are allowed, Õ(n 1 / 3 ⋅poly(1 /ϵ)) +g(1 /ϵ) time suffices. In addition to an approximate value to Copt, our algorithm can also output a constant-size “template” of a packing that can later be used to find a near-optimal packing in linear time...|$|R
50|$|After Gallup {{collects}} {{and processes}} survey data, each respondent is assigned a weight {{so that the}} {{demographic characteristics of the}} total <b>weighted</b> <b>sample</b> of respondents match the latest estimates of the demographic characteristics of the adult population available from the U.S. Census Bureau. Gallup weights data to census estimates for gender, race, age, educational attainment, and region.|$|E
5000|$|Where [...] is the (<b>weighted)</b> <b>sample</b> {{variance}} {{derived from}} the observed proportions of success in sets in [...] "Lexis trials" [...] and [...] is the variance computed from the expected Bernoulli distribution {{on the basis of}} the overall average proportion of success. Trials where L falls significantly above or below 1 are known as supernormal and subnormal, respectively.|$|E
50|$|Located on PostGlobal, washingtonpost.com and Newsweek.com’s panel blog on {{international}} issues, the Global Power Barometer (GPB) is a visual monitor updated each weekday {{by the research}} firm Denver Research Group, Inc. (DRGI) using a <b>weighted</b> <b>sample</b> of sources from the media, academia, governments, and NGOs from around the world. The GPB offers {{an indication of the}} most powerful nations and ideologies on any given day. PostGlobal is hosted by journalists Fareed Zakaria and David Ignatius.|$|E
40|$|The bin packing {{problem is}} defined as follows: given a set of n items with sizes 0 0, we present an {{algorithm}} Aǫ that has sampling access to the input instance and outputs a value k such that Copt ≤ k ≤ (1 +ǫ) ·Copt+ 1, where Copt {{is the cost of}} an optimal solution. It is clear that uniform sampling by itself will not allow a sublinear-time algorithm in this setting; a small number of items might constitute most of the total weight and uniform samples will not hit them. In this work we use <b>weighted</b> <b>samples,</b> where item i is sampled with probability proportional to its weight: that is, with probability wi/ iwi. In the presence of <b>weighted</b> <b>samples,</b> the approximation algorithm runs in Õ(n · poly(1 /ǫ)) + g(1 /ǫ) time, where g(x) is an exponential function of x. When both <b>weighted</b> <b>sampling</b> and uniform sampling are allowed, Õ(n 1 / 3 · poly(1 /ǫ)) + g(1 /ǫ) time suffices. In addition to an approximate value to Copt, our algorithm can also output a constant-size “template ” of a packing that can later be used to find a near-optimal packing in linear time...|$|R
40|$|Several {{methods for}} the {{reconstruction}} of white matter tracts from DT-MRI have been proposed in the literature. We present a novel approach for white matter tractography based on sequential importance sampling and resampling (SBR). By modelling all possible fibre paths originating from a starting point as a distribution, a probability can be assigned for each path based on its fitness to the measured DT data and its shape. We use SISR to sequentially build up a set of <b>weighted</b> <b>samples</b> representing this distribution. Connectivity can then be derived from these <b>weighted</b> <b>samples.</b> Method A fibre path x of length N originating from a point x 0 {{can be defined as}} x = {xo, _ [...] , XN}, where xi is represented in...|$|R
40|$|This note {{extends the}} {{standard}} kernel density estimator {{to the case}} of <b>weighted</b> <b>samples</b> in several ways. In the first place I consider the obvious extension by substituting the simple sum in the definition of the estimator by a weighted sum, but I also consider other alternatives of introducing weights, based on adaptive kernel density estimators, and consider the weights as indicators of the informational content of the observations and in this sense as signals of the local density of the data. All these ideas are shown using the Penn World Table {{in the context of the}} macroeconomic convergence issue. Copyright Springer-Verlag Berlin Heidelberg 2003 Key words: <b>Weighted</b> <b>samples,</b> survey data, regional data, kernel density estimates, convergence., JEL classification: C 00, C 8.,...|$|R
50|$|A study {{through the}} Mental Health Prevalence Project which used “three major {{indicators}} of mental illness: {{diagnosis of a}} serious mental illness, history of inpatient psychiatric care, and psychotropic medication use” found that female offenders have “on average, {{twice the rate of}} various indicators as males.” The study found (using a <b>weighted</b> <b>sample)</b> that 17.8% of male offenders and 35.1% of female offenders have a mental health problem upon being committed. This study did not treat substance abuse as a mental health disorder.|$|E
5000|$|The cost {{function}} is minimized {{by taking the}} partial derivatives for all entries [...] of the coefficient vector [...] and setting the results to zeroNext, replace [...] with {{the definition of the}} error signalRearranging the equation yieldsThis form can be expressed in terms of matriceswhere [...] is the <b>weighted</b> <b>sample</b> covariance matrix for , and [...] is the equivalent estimate for the cross-covariance between [...] and [...] Based on this expression we find the coefficients which minimize the {{cost function}} asThis is the main result of the discussion.|$|E
5000|$|In many applications, {{amount of}} data from which a small sample is needed is too large and it is {{desirable}} to distribute sampling tasks among many machines in parallel {{to speed up the}} process. A simple approach that is often used, although less performant, is to assign a random number as key to each item and then perform a distributed sort and finally obtain a sample of desired size from top k items. If <b>weighted</b> <b>sample</b> is desired then key is computed using [...] where [...] is the random number and [...] is the weight of an item. The inefficiency in this approach obviously arises from required distributed sort on very large {{amount of data}}.|$|E
30|$|We {{addressed}} {{the problem of}} reputation discovery and aggregation of sentiments by exploring the underlying entities that coexist in the data. We {{stressed the importance of}} information interpretation in explaining the reputation of an entity. We introduced a <b>weighted</b> <b>sampling</b> technique to improve the richness of the dataset.|$|R
40|$|We propose nested {{sequential}} Monte Carlo (NSMC), {{a methodology}} to sample from se-quences of probability distributions, even where the random variables are high-dimensional. NSMC generalises the SMC framework by re-quiring only approximate, properly <b>weighted,</b> <b>samples</b> from the SMC proposal distribution, while still {{resulting in a}} correct SMC algorithm. Furthermore, NSMC can in itself be used to pro-duce such properly <b>weighted</b> <b>samples.</b> Conse-quently, one NSMC sampler {{can be used to}} con-struct an efficient high-dimensional proposal dis-tribution for another NSMC sampler, and this nesting of the algorithm can be done to an arbi-trary degree. This allows us to consider complex and high-dimensional models using SMC. We show results that motivate the efficacy of our ap-proach on several filtering problems with dimen-sions in the order of 100 to 1 000. 1...|$|R
40|$|We {{propose a}} new {{probabilistic}} method for neighbor-hood selection in collaborative filtering (CF) mod-els {{according to which}} the neighborhood selection {{is based on an}} underlying probability distribution. ◦ We use an efficient method for <b>weighted</b> <b>sampling</b> [6] of k neigh-bors without replace-ment that takes into con-sideration the similarity levels between the users...|$|R
