2019|879|Public
5|$|The cervix was {{documented}} in anatomical literature {{in at least}} the time of Hippocrates; cervical cancer was first described more than 2,000 years ago, with descriptions provided by both Hippocrates and Aretaeus. However, there was some variation in <b>word</b> <b>sense</b> among early writers, who used the term to refer to both the cervix and the internal uterine orifice. The first attested {{use of the word}} to refer to the cervix of the uterus was in 1702.|$|E
25|$|The {{first three}} evaluations, Senseval-1 through Senseval-3, {{were focused on}} <b>word</b> <b>sense</b> disambiguation, each time growing {{in the number of}} {{languages}} offered in the tasks and in the number of participating teams. Beginning with the fourth workshop, SemEval-2007 (SemEval-1), the nature of the tasks evolved to include semantic analysis tasks outside of <b>word</b> <b>sense</b> disambiguation.|$|E
25|$|Hit counts {{were used}} for {{carefully}} constructed search engine queries to identify rank orders for <b>word</b> <b>sense</b> frequencies, as an input to a <b>word</b> <b>sense</b> disambiguation engine. This method was further explored {{with the introduction of}} the concept of a parallel corpora where the existing Web pages that exist in parallel in local and major languages be brought together. It was demonstrated {{that it is possible to}} build a language-specific corpus from a single document in that specific language.|$|E
40|$|We {{describe}} an unusual data set {{of thousands of}} annotated images with interesting sense phenomena. Natural language image sense annotation involves increased semantic complexities compared to disambiguating <b>word</b> <b>senses</b> when annotating text. These issues are discussed and illustrated, including the distinction between <b>word</b> <b>senses</b> and iconographic senses. ...|$|R
5000|$|Kilgarriff, A. 1997. I don't {{believe in}} <b>word</b> <b>senses.</b> Comput. Human. 31(2), pp. 91-113.|$|R
5000|$|ULTRA has vocabularies {{based on}} about 10,000 <b>word</b> <b>senses</b> {{in each of}} its five languages.|$|R
25|$|There {{has been}} much {{discussion}} about the possible developments {{in the arena of}} the Web as a corpus. The development of using the web as a data source for <b>word</b> <b>sense</b> disambiguation was brought forward in The EU MEANING project in 2002. It used the assumption that within a domain, words often have a single meaning, and that domains are identifiable on the Web. This was further explored by using Web technology to gather manual <b>word</b> <b>sense</b> annotations on the Word Expert Web site.|$|E
25|$|Senseval-3 {{took place}} in March–April 2004, {{followed}} by a workshop held in July 2004 in Barcelona, in conjunction with ACL 2004. Senseval-3 included 14 different tasks for core <b>word</b> <b>sense</b> disambiguation, as well as identification of semantic roles, multilingual annotations, logic forms, subcategorization acquisition.|$|E
25|$|SemEval (Semantic Evaluation) is {{an ongoing}} series of {{evaluations}} of computational semantic analysis systems; it evolved from the Senseval <b>word</b> <b>sense</b> evaluation series. The evaluations are intended to explore the nature of meaning in language. While meaning is intuitive to humans, transferring those intuitions to computational analysis has proved elusive.|$|E
50|$|The Second Edition added over 3,000 new <b>words,</b> <b>senses</b> {{and phrases}} {{drawn from the}} Oxford English Corpus.|$|R
40|$|Abstract: In {{spite of}} the growing of ontological {{engineering}} tools, ontology knowledge acquisition remains a highly manual, time-consuming and complex task. Automatic ontology learning is a well-established research field whose goal is to support the semi-automatic construction of ontologies starting from available digital resources (e. g., A corpus, web pages, dictionaries, semi-structured and structured sources) {{in order to reduce}} the time and effort in the ontology development process. This paper proposes an enhanced methodology for enriching Lexical Ontologies such as the popular open-domain vocabulary –WordNet. Ontologies like WordNet can be semantically enriched to obtain extensions and enhancements to its lexical database. The proliferation of senses in WordNet is considered as one of its main shortcomings for practical applications. Therefore, the presented methodology depends on the Coarse-Grained <b>word</b> <b>senses.</b> These senses are generated from applying WordNet Fine-Grained <b>word</b> <b>senses</b> to a Merging Sense algorithm. This algorithm merges only semantically similar <b>word</b> <b>senses</b> instead of applying traditional clustering techniques. A performance comparison is illustrated between two different data sources (Web, Corpus) used in the Enrichment process. The results obtained from using Coarse-Grained <b>word</b> <b>senses</b> in both cases yields better precision than Fine-Graine...|$|R
5000|$|McCarthy, D.; R. Koeling, J. Weeds, J. Carroll. 2007. Unsupervised {{acquisition}} of predominant <b>word</b> <b>senses.</b> Computational Linguistics 33(4): 553-590.|$|R
25|$|PageRank {{has been}} used to rank spaces or streets to predict how many people (pedestrians or vehicles) come to the {{individual}} spaces or streets. In lexical semantics it {{has been used}} to perform <b>Word</b> <b>Sense</b> Disambiguation, Semantic similarity, and also to automatically rank WordNet synsets according to how strongly they possess a given semantic property, such as positivity or negativity.|$|E
25|$|The Multilingual WSD {{task was}} {{introduced}} for the SemEval-2013 workshop. The task {{is aimed at}} evaluating <b>Word</b> <b>Sense</b> Disambiguation systems in a multilingual scenario using BabelNet as its sense inventory. Unlike similar task like crosslingual WSD or the multilingual lexical substitution task, where no fixed sense inventory is specified, Multilingual WSD uses the BabelNet as its sense inventory. Prior {{to the development of}} BabelNet, a bilingual lexical sample WSD evaluation task was carried out in SemEval-2007 on Chinese-English bitexts.|$|E
25|$|From the {{earliest}} days, assessing {{the quality of}} <b>word</b> <b>sense</b> disambiguation (WSD) algorithms had been primarily a matter of intrinsic evaluation, and “almost no attempts {{had been made to}} evaluate embedded WSD components”. Only very recently (2006) had extrinsic evaluations begun to provide some evidence for the value of WSD in end-user applications. Until 1990 or so, discussions of the sense disambiguation task focused mainly on illustrative examples rather than comprehensive evaluation. The early 1990s saw the beginnings of more systematic and rigorous intrinsic evaluations, including more formal experimentation on small sets of ambiguous words.|$|E
50|$|A {{distinction}} between archaic and obsolete <b>words</b> and <b>word</b> <b>senses</b> {{is widely used}} by dictionaries. An archaic <b>word</b> or <b>sense</b> is one that still has some current use but whose use has dwindled to a few specialized contexts, outside which it connotes old-fashioned language. In contrast, an obsolete <b>word</b> or <b>sense</b> {{is one that is}} no longer used at all. A reader encounters them when reading texts that are centuries old. For example, the works of Shakespeare are old enough that some obsolete <b>words</b> or <b>senses</b> are encountered therein, for which glosses (annotations) are often provided in the margins.|$|R
5000|$|Auto-antonymy: Change of a <b>word's</b> <b>sense</b> and {{concept to}} the {{complementary}} opposite, e.g., {{bad in the}} slang sense of [...] "good".|$|R
40|$|Abstract. A {{quantitative}} {{comparative study}} of homonymy in four well-known electronic Spanish dictionaries—EuroWordNet and three traditional dictionaries—is presented. It is shown that though structuring of <b>word</b> <b>senses</b> is quite different in all dictionaries under comparison, EuroWordNet differs from the traditional dictionaries much more than these differ from each other. It is also shown that the ordering of the <b>word</b> <b>senses</b> in Spanish EuroWordNet less agrees {{with the use of}} the senses in texts than the ordering in traditional dictionaries. ...|$|R
2500|$|... {{international}} organization {{devoted to the}} evaluation of <b>Word</b> <b>Sense</b> Disambiguation Systems (endorsed by SIGLEX) ...|$|E
2500|$|For example, in the <b>word</b> <b>sense</b> {{induction}} and disambiguation task, {{there are}} three separate phases: ...|$|E
2500|$|... {{evaluated}} <b>word</b> <b>sense</b> disambiguation {{systems on}} {{three types of}} tasks (the all-words, lexical-sample and the translation task) ...|$|E
40|$|The {{subject of}} this journal goes {{by a variety of}} names: numeracy, {{quantitative}} literacy, and quantitative reasoning. Some authors use the terms interchangeably. Others see distinctions between them. Study of psycholinguistic and ontological concepts laid out in the literature of WordNet and familiarity with the papers in this journal suggests a vocabulary matrix consisting of four rows (<b>word</b> <b>senses)</b> and three columns (word forms, namely numeracy, QL, and QR). The four <b>word</b> <b>senses</b> correspond to four sets of synonyms...|$|R
40|$|The present {{contribution}} {{focuses on}} the integration of <b>word</b> <b>senses</b> in a vector representation of texts, using a probabilistic model. The vector representation under consideration is the DSIR model, that extends the standard Vector Space (VS) model by taking both occurrences and co-occurrences of words into account. Integration of <b>word</b> <b>senses</b> into the co-occurrence model is done using a Markov Random Field model with hidden variables, using semantic information derived from synonymy relations extracted from a synonym dictionary...|$|R
5000|$|Precise - The {{definition}} {{should use}} words {{that have a}} precise meaning. Try to avoid words that have multiple meanings or multiple <b>word</b> <b>senses.</b>|$|R
2500|$|... {{included}} {{tasks for}} <b>word</b> <b>sense</b> disambiguation, {{as well as}} identification of semantic roles, multilingual annotations, logic forms, subcategorization acquisition.|$|E
2500|$|... {{the first}} {{evaluation}} exercise on <b>word</b> <b>sense</b> disambiguation systems; the lexical-sample task was evaluated on English, French and Italian ...|$|E
2500|$|... To {{facilitate}} {{the ease of}} integrating WSD systems into other Natural Language Processing (NLP) applications, such as Machine Translation and multilingual Information Retrieval, the cross-lingual WSD evaluation task was introduced a language-independent and knowledge-lean approach to WSD. The task is an unsupervised <b>Word</b> <b>Sense</b> Disambiguation task for English nouns by means of parallel corpora. It follows the lexical-sample variant of the Classic WSD task, restricted to only 20 polysemous nouns.|$|E
40|$|Summary. A lexicon is a {{linguistic}} object and hence {{is not the}} same thing as an ontology, which is non-linguistic. Nonetheless, <b>word</b> <b>senses</b> are in many ways similar to ontological concepts and the relationships found between <b>word</b> <b>senses</b> resemble the relationships found between concepts. Although the arbitrary and semi-arbitrary distinctions made by natural languages limit the degree to which these similarities can be exploited, a lexicon can nonetheless serve in the development of an ontology, especially in a technical domain...|$|R
40|$|We {{describe}} {{an approach to}} classifying <b>word</b> <b>senses</b> and semantic roles {{that focuses on the}} entailments that can be drawn from sentences. A key observation is that entailments arise from sentences, not <b>word</b> <b>senses,</b> and by taking a compositional approach to deriving entailments, we can significantly simplify the number of senses that are required. In addition, we argue that most uses of semantic roles can be replaced by taking prepositional meaning seriously, resulting in a strongly compositional approach to computing sentence meaning. ...|$|R
40|$|Abstract. In this paper, {{we present}} a method for {{detecting}} unknown <b>word</b> <b>senses</b> using a concept dictionary and newspaper articles. It {{is very important to}} detecting unknown <b>word</b> <b>senses</b> for document classification, information retrieval, information extraction, etc. Although for extracting similar word pairs the methods which use similarity of case structure between two words are used, comparison between similarity of two words suffers word sparseness problem. Especially, it is necessary to solve this problem for detecting <b>word</b> <b>senses</b> of proper nouns which are not listed in the dictionary. The proposed method used hierarchical semantic features of a concept dictionary {{in order to deal with}} this problem. We performed some experiments in order to confirm effectiveness of the method. 2 System Overview Figure 1 illustrates the overview of our system. Our system consists of 5 steps which are described in the following list. ...|$|R
2500|$|Wikipedia {{has seen}} been widely {{used as a}} corpus for {{linguistic}} research in computational linguistics, information retrieval and natural language processing. In particular, it commonly serves as a target knowledge base for the entity linking problem, which is then called [...] "wikification", and to the related problem of <b>word</b> <b>sense</b> disambiguation. Methods similar to wikification can in turn be used to find [...] "missing" [...] links in Wikipedia.|$|E
50|$|UBY applies a <b>word</b> <b>sense</b> {{alignment}} approach (subfield of <b>word</b> <b>sense</b> disambiguation) for combining {{information about}} nouns and verbs.Currently, UBY contains 12 integrated resources in English and German.|$|E
50|$|Besides the {{training}} of machine translation systems, other applications of word alignment include translation lexicon induction, <b>word</b> <b>sense</b> discovery, <b>word</b> <b>sense</b> disambiguation and the cross-lingual projection of linguistic information.|$|E
40|$|Representing the {{semantics}} {{of linguistic}} items in a machine-interpretable form {{has been a}} major goal of Natural Language Processing since its earliest days. Among the range of different linguistic items, words have attracted the most research attention. However, word representations have an important limitation: they conflate different meanings of a word into a single vector. Representations of <b>word</b> <b>senses</b> have the potential to overcome this inherent limitation. Indeed, the representation of individual <b>word</b> <b>senses</b> and concepts has recently gained in popularity with several experimental results showing that a considerable performance improvement can be achieved across different NLP applications upon moving from word level to the deeper sense and concept levels. Another interesting point regarding the representation of concepts and <b>word</b> <b>senses</b> is that these models can be seamlessly applied to other linguistic items, such as words, phrases and sentences...|$|R
40|$|Information {{retrieval}} using <b>word</b> <b>senses</b> {{is emerging}} as a good research challenge on semantic information retrieval. In this paper, we propose a new method using <b>word</b> <b>senses</b> in information retrieval: root sense tagging method. This method assigns coarse-grained <b>word</b> <b>senses</b> defined in WordNet to query terms and document terms by unsupervised way using co-occurrence information constructed automatically. Our sense tagger is crude, but performs consistent disambiguation by considering only the single most informative word as evidence to disambiguate the target word. We also allow multiple-sense assignment to alleviate the problem caused by incorrect disambiguation. Experimental results on a large-scale TREC collection show that our approach to improve retrieval effectiveness is successful, {{while most of the}} previous work failed to improve performances even on small text collection. Our method also shows promising results when is combined with pseudo relevance feedback and state-of-the-art retrieval function such as BM 25...|$|R
50|$|Each {{word has}} {{a set of}} {{patterns}} assigned to it which describe typical contexts {{in which they are}} used. Often these are separate for different <b>word</b> <b>senses.</b>|$|R
