140|587|Public
5000|$|The self-consistency {{principle}} {{says that}} both forms must be asymptotically equivalent for energies or masses sufficiently high (asymptotic limit). Also, {{the density of}} states and the mass spectrum must be asymptotically equivalent {{in the sense of}} the <b>weak</b> <b>constraint</b> proposed by Hagedorn as ...|$|E
30|$|Remark 3.3. If h is a {{differentiable}} function at x̅ and admits an upper regular convexifactor ∂*h(x̅) at x̅, then the above Slater-type <b>weak</b> <b>constraint</b> qualification reduces to Slater's <b>weak</b> <b>constraint</b> qualification given by Mangasarian [29].|$|E
40|$|We {{present a}} method to control {{unbalanced}} fast dynamics in an ensemble Kalman filter by introducing a <b>weak</b> <b>constraint</b> on the imbalance in a spatially sparse observational network. We show that the balance constraint produces significantly more balanced analyses than ensemble Kalman filters without balance constraints and than filters implementing incremental analysis updates (IAU). Furthermore, our filter with the <b>weak</b> <b>constraint</b> on imbalance produces good rms error statistics which outperform those of ensemble Kalman filters without balance constraints for the fast fields...|$|E
40|$|The {{formulation}} of four-dimensional variational data assimilation allows {{the incorporation of}} constraints into the cost function which need only be weakly satisfied. In this paper we investigate the value of imposing conservation properties as <b>weak</b> <b>constraints.</b> Using {{the example of the}} two-body problem of celestial mechanics we compare <b>weak</b> <b>constraints</b> based on conservation laws with a constraint on the background state. We show how the imposition of conservation-based <b>weak</b> <b>constraints</b> changes the nature of the gradient equation. Assimilation experiments demonstrate how this can add extra information to the assimilation process, even when the underlying numerical model is conserving...|$|R
40|$|This paper {{contributes}} {{to the area of}} inductive logic programming by presenting a new learning framework that allows the learning of <b>weak</b> <b>constraints</b> in Answer Set Programming (ASP). The framework, called Learning from Ordered Answer Sets, generalises our previous work on learning ASP programs without <b>weak</b> <b>constraints,</b> by considering a new notion of examples as ordered pairs of partial answer sets that exemplify which answer sets of a learned hypothesis (together with a given background knowledge) are preferred to others. In this new learning task inductive solutions are searched within a hypothesis space of normal rules, choice rules, and hard and <b>weak</b> <b>constraints.</b> We propose a new algorithm, ILASP 2, which is sound and complete with respect to our new learning framework. We investigate its applicability to learning preferences in an interview scheduling problem and also demonstrate that when restricted to the task of learning ASP programs without <b>weak</b> <b>constraints,</b> ILASP 2 can be much more efficient than our previously proposed system. Comment: To appear in Theory and Practice of Logic Programming (TPLP), Proceedings of ICLP 201...|$|R
40|$|This paper {{presents}} {{an extension of}} Disjunctive Datalog (DATALOG;:) by integrity constraints. These are of two types: strong, that is, classical integrity <b>constraints,</b> and <b>weak,</b> that is, <b>constraints</b> that are satisfied if possible. While strong constraints must be satisfied, <b>weak</b> <b>constraints</b> express desiderata, that is, they may be violated - actually, their semantics tends to minimize the number of violated instances of <b>weak</b> <b>constraints.</b> <b>Weak</b> <b>constraints</b> may be ordered according to their importance to express different priority levels. As a result, the proposed language (call it DATALOG;:;c) is well-suited to represent commonsense reasoning and knowledge-based problems arising in different areas of computer science such as planning, graph theory optimizations and abductive reasoning. The formal definition of the language is first given. The declarative semantics of DATALOG;:;c is defined {{in a general way}} that allows us to put constraints on top of any existing (model-theor [...] ...|$|R
30|$|We now deduce the Kuhn-Tucker-type {{necessary}} optimality {{conditions for}} (P) {{under the assumption}} of the Slater-type <b>weak</b> <b>constraint</b> qualification which {{is defined as follows}} on the lines in the study of Mangasarian [29].|$|E
30|$|Initially DTW {{method was}} {{developed}} for the one-dimensional signal processing (in speech recognition, e.g.). So, {{for this kind of}} the signal the Euclidean distance minimization with a <b>weak</b> <b>constraint</b> (the derivative of the synchronization path is constrained) works very well. In our case the dimensionality of the signal is up to 37 D and <b>weak</b> <b>constraint</b> does not yield satisfactory robustness due to the noise and the signal complexity. We propose to minimize a composite distance that consists of two terms: a distance itself and a smoothness term. Such kind of a distance has the same meaning of the energy in MRF optimization techniques.|$|E
40|$|International audienceEntropy functionals (i. e. convex {{integral}} functionals) and {{extensions of}} these functionals are minimized on convex sets. This paper {{is aimed at}} reducing {{as much as possible}} the assumptions on the constraint set. Dual equalities and characterizations of the minimizers are obtained with <b>weak</b> <b>constraint</b> qualifications...|$|E
3000|$|... is the {{interval}} with a <b>weakest</b> <b>constraint,</b> we can, obviously, expect better outage performance {{with a wide}} second section. Also, increasing [...]...|$|R
5000|$|... (and the converses are not true), {{although}} MFCQ is not {{equivalent to}} CRCQ.In practice <b>weaker</b> <b>constraint</b> qualifications are preferred since they provide stronger optimality conditions.|$|R
5000|$|It has {{not always}} been like this: in the old days of the [...] "gentleman scientist" [...] funding (and to a lesser extent publication) were far <b>weaker</b> <b>constraints.</b>|$|R
40|$|The bulk of {{this paper}} {{contains}} a concise mathematical overview of the subject of data assimilation, highlighting three primary ideas: (i) the standard optimization approaches of 3 DVAR, 4 DVAR and <b>weak</b> <b>constraint</b> 4 DVAR are described and their interrelations explained; (ii) statistical analogues of these approaches are then introduced, leading to filtering (generalizing 3 DVAR) and a form of smoothing (generalizing 4 DVAR and <b>weak</b> <b>constraint</b> 4 DVAR) and the optimization methods are shown to be maximum a posteriori estimators for the probability distributions implied by these statistical approaches; (iii) by taking a general dynamical systems perspective on the subject it is shown that the incorporation of Lagrangian data can be handled by a straightforward extension of the preceding concepts. We argue that the smoothing approach to data assimilation, based on statistical analogues of 4 DVAR and <b>weak</b> <b>constraint</b> 4 DVAR, provides the optimal solution to the assimilation of spacetime distributed data into a model. The optimal solution obtained is a probability distribution on the relevant class of functions (initial conditions, or time-dependent solutions). The approach is a useful one {{in the first instance}} because it clarifies the notion of what is the optimal solution, thereby providing a benchmark against which existing approaches can be evaluated. In the longer term it also provides the potential for new methods to create ensembles of solutions to the model, incorporating the available data in an optimal fashion. Two examples are given illustrating this approach to data assimilation, both in the context of Lagrangian data, one based on statistical 4 DVAR and the other on <b>weak</b> <b>constraint</b> statistical 4 DVAR. The former is compared with the ensemble Kalman filter which is thereby shown to be inaccurate in a variety of scenarios. Copyright c ○ 2008 John Wiley & Sons, Ltd...|$|E
40|$|The <b>weak</b> <b>constraint</b> inverse for {{nonlinear}} dynamical {{models is}} discussed and derived {{in terms of}} a probabilistic formulation. The well-known result that for Gaussian error statistics the minimum of the <b>weak</b> <b>constraint</b> inverse is equal to the maximum likelihood estimate is rederived. Then several methods based on ensemble statistics which can be used to find the smoother (as opposed to the filter) solution are introduced and compared to traditional methods. A strong point of the new methods is that they avoid the integration of adjoint equations, which is a complex task for real oceanographic or atmospheric applications. They also avoid iterative searches in an Hilbert space, and error estimates can be obtained without much additional computational effort. The feasibility of the new methods is illustrated in a two-layer quasi-geostrophic ocean model...|$|E
40|$|Abstract. Entropy (i. e. convex integral) functionals and {{extensions}} of these functionals are minimized on convex sets. This paper {{is aimed at}} reducing {{as much as possible}} the assumptions on the constraint set. Primal attainment, dual equalities, dual attainment and characterizations of the minimizers are obtained with <b>weak</b> <b>constraint</b> qualifications...|$|E
40|$|Traditional inter-domain routing {{protocols}} based on superdomains maintain either "strong" or "weak" ToS and policy constraints for each visible superdomain. With strong constraints, a valid path {{may not be}} found even though one exists. With <b>weak</b> <b>constraints,</b> an invalid domain-level path may {{be treated as a}} valid path. We present an inter-domain routing protocol based on superdomains, which always finds a valid path if one exists. Both strong and <b>weak</b> <b>constraints</b> are maintained for each visible superdomain. If the strong constraints of the superdomains on a path are satisfied, then the path is valid. If only the <b>weak</b> <b>constraints</b> are satisfied for some superdomains on the path, the source uses a query protocol to obtain a more detailed "internal" view of these superdomains, and searches again for a valid path. Our protocol handles topology changes, including node/link failures that partition superdomains. Evaluation results indicate our protocol scales well to large internetwo [...] ...|$|R
40|$|Abstract. We {{focus on}} the {{question}} of how the shape of a cost-function determines the features manifested by its local (and hence global) minimizers. Our goal is to check the possibility that the local minimizers of an unconstrained cost-function satisfy different subsets of affine constraints dependent on the data, hence the word “weak”. A typical example is the estimation of images and signals which are constant on some regions. We provide general conditions on cost-functions which ensure that their minimizers can satisfy <b>weak</b> <b>constraints</b> when noisy data range over an open subset. These cost-functions are non-smooth at all points satisfying the <b>weak</b> <b>constraints.</b> In contrast, the local minimizers of smooth cost-functions can almost never satisfy <b>weak</b> <b>constraints.</b> These results, obtained in a general setting, are applied to analyze the minimizers of cost-functions, composed of a data-fidelity term and a regularization term. We thus consider the effect produced by non-smooth regularization, in comparison with smooth regularization. In particular, these results explain the stair-casing effect, well known in total-variation methods. Theoretical results are illustrated using analytical examples and numerical experiments...|$|R
2500|$|Another {{approach}} to logic-based activity recognition {{is to use}} stream reasoning based on Answer Set Programming, and {{has been applied to}} recognising activities for health-related applications, [...] which uses <b>weak</b> <b>constraints</b> to model a degree of ambiguity/uncertainty.|$|R
3000|$|Definition 3.2. The {{function}} h is said {{to satisfy}} the Slater-type <b>weak</b> <b>constraint</b> qualification at x̅ ∈ E if h J [...] is ∂*-pseudoconvex at x̅, and there exists an x o [*]∈[*]Rn such that h J (x o [...]) < 0 where J = {j|h j [...] (x̅) = 0 }.|$|E
3000|$|Theorem 3.4. Let x̅ ∈ E be a weak {{efficient}} {{solution of}} (P). Suppose that the hypotheses of Theorem 3.1 hold. Then, there exist vectors α*[*]∈[*]R+ p and μ*[*]∈[*]R+ m [...] (not both zero) such that (1), (2) and (3) hold. If the Slater-type <b>weak</b> <b>constraint</b> qualification holds at x̅, then α* ≠ 0.|$|E
40|$|Advanced data {{assimilation}} becomes extremely {{complicated and}} challenging when used with strongly nonlinear models. Several previous works have reported various problems when applying existing popular data assimilation techniques with strongly nonlinear dynamics. Common for these techniques {{is that they}} can all be considered as extensions to methods which have proven to work well with linear dynamics. This paper shows that a <b>weak</b> <b>constraint</b> variational formulation for the Lorenz model, where the full model state in space and time is considered as control variables, can be minimized using a gradient descent method. It is further shown that the <b>weak</b> <b>constraint</b> formulation removes some of the previous reported problems associated to the predictability limit of nonlinear models when strong constraint formulations are used. Further, by using a gradient descent method, problems associated to the use of an approximate tangent linear model when solving the Euler-Lagrange equations or when the e [...] ...|$|E
40|$|We {{consider}} interpolation between keyframe hierarchies. We {{impose a}} set of <b>weak</b> <b>constraints</b> that allows smooth interpolation between two keyframe hierarchies in an animation or, more generally, allows the interpolation in an n family of hierarchies. We use hierarchical triangulations obtained by the Rivara element bisection algorithm and impose a <b>weak</b> compatibility <b>constraint</b> {{on the set of}} root elements of all keyframe hierarchies. We show that the introduced <b>constraints</b> are rather <b>weak.</b> The strength o...|$|R
50|$|In mathematics, {{the linear}} {{programming}} relaxation of a 0-1 integer {{program is the}} problem that arises by replacing the constraint that each variable must be 0 or 1 by a <b>weaker</b> <b>constraint,</b> that each variable belong to the interval 0,1.|$|R
30|$|The {{assumptions}} in condition C 3 and C 11 on mixing coefficient α (k) {{is sufficient}} conditions for mixing coefficient, [25] pointed out this assumptions on mixing coefficient are satisfied given some general conditions. Condition C 13 is also satisfied under some <b>weak</b> <b>constraints</b> on γ or τ.|$|R
40|$|AbstractEntropy (i. e. convex integral) functionals and {{extensions}} of these functionals are minimized on convex sets. This paper {{is aimed at}} reducing {{as much as possible}} the assumptions on the constraint set. Primal attainment, dual equalities, dual attainment and characterizations of the minimizers are obtained with <b>weak</b> <b>constraint</b> qualifications. These results improve several aspects of the literature on the subject...|$|E
40|$|The Chaplygin gas (CG) and the {{generalized}} Chaplygin gas (GCG) models, proposed as candidates of the unified dark matter-dark energy (UDME), are tested with the look-back time (LT) redshift data. We {{find that the}} LT data only give a very <b>weak</b> <b>constraint</b> on the model parameter. However, by combing the LT with the baryonic acoustic oscillation peak, we obtain, at the 95. 4...|$|E
40|$|A <b>weak</b> <b>constraint</b> variational {{formulation}} is {{used for}} inverse calculations and parameter estimation in a one-dimensional Ekman model. When parameters in the model are allowed to contain errors the inverse problem becomes nonlinear even if the model itself is linear. It is shown that a convergent iteration can be defined for the nonlinear system of Euler [...] Lagrange equations and that improved estimates of the poorly known parameters can be calculated by solving the inverse problem {{for each of the}} linear iterates using the representer method. The formulation of the variational problem and the solution methods are illustrated using a simple example. The use of a simple dynamical model makes it possible to give an instructive presentation of the representer method. The method is finally used in an example using real current meter data. It is shown that the <b>weak</b> <b>constraint</b> formulation results in smooth solutions in good agreement with the data all through the water column, and that it is sup [...] ...|$|E
40|$|Fast neutron-mirror neutron (n-n') {{oscillations}} were proposed {{recently as}} the explanation of the GZK puzzle. We discuss possible laboratory experiments to search for such oscillaions and to improve the present very <b>weak</b> <b>constraints</b> {{on the value of}} the n-n' oscillation probability. Comment: 11 pages, 0 figures, 1 tabl...|$|R
40|$|We {{report on}} a recent {{re-examination}} of dark matter constraints from halo wide stellar binaries, motivated by observations we made of four of the very wide binary candidates on which earlier constraints were based. Our new {{data suggest that the}} current stellar binary data place only <b>weak</b> <b>constraints</b> on the properties of dark matter in the halo of the Milky Way...|$|R
3000|$|The {{optimization}} problem in (5) is a binary integer programming problem, {{which can be}} solved efficiently by a linear programming (LP)-based branch-and-bound algorithm [39]. The algorithm searches for an optimal solution to the binary integer programming problem by solving a series of LP relaxation problems, in which the binary integer requirement on the variables is replaced by the <b>weaker</b> <b>constraint</b> 0 ≤b [...]...|$|R
40|$|This paper {{combines}} the elegant technique of Data Assimilation and a Monte Carlo procedure to analyze time series {{data for the}} North East Arctic Cod stock (NEACs). A simple nonlinear dynamic resource model is calibrated to time series data using the variational adjoint parameter estimation method and the Monte Carlo technique. By exploring the efficient features of the variational adjoint technique coupled with the Monte Carlo method, optimal or best parameter estimates with their error statistics are obtained. Thereafter, the <b>weak</b> <b>constraint</b> formulation resulting in a stochastic ordinary differential equation (SODE) is used to find an improved estimate of the dynamical variable, i. e. the stock. Empirical {{results show that the}} average fishing mortality imposed on the NEACs is about 16 % more than the intrinsic growth rate of the biological species. Copyright Springer 2005 dynamic resource model, Inverse methods, Monte Carlo, variational adjoint parameter estimation, <b>weak</b> <b>constraint,</b> C 15, Q 22,...|$|E
40|$|In this paper, a <b>weak</b> <b>constraint</b> {{formulation}} of the digital filter based on the Dolph–Chebyshev window is introduced in a preoperational version of the 4 DVAR analysis of Météo-France. The constraint is imposed only on the analysis increments to damp spurious fast oscillations associated with gravity–inertia waves. In the incremental {{formulation of}} 4 DVAR, the analysis increments are obtained from a global model at a uniform low resolution with a simplified set of physical parameterizations, while the high-resolution forecast is obtained with a model that uses a variable-resolution grid having a higher resolution over France and the complete set of physical parameterizations. Both models have the same vertical resolution. In a set of preliminary experiments using the same background field and {{the same set of}} observations, it is shown that the <b>weak</b> <b>constraint</b> imposed only on the low-resolution increments manages to control efficiently the emergence of fast oscillations in the resulting high-resolution forecast while maintaining a closer fit to the observations than is possible if the digital filter initialization is applied externally on the final analysis increments. It is also shown that this <b>weak</b> <b>constraint</b> does not add any significant computer cost to the 4 DVAR analysis. Finally, 4 DVAR has been cycled over a period of 2 weeks and the results show that, compared to 3 DVAR, the initial dynamical imbalances are significantly less in 4 DVAR even if no constraint is imposed at all. However, it has been noted that the innovation statistics show a positive impact when a constraint is applied...|$|E
40|$|The Bayesian {{formulation}} for inverse problems gives a way {{of making}} inferences about unknown quantities not directly observable. The application of Bayes' Theorem combines the prior information and the observation to give a posterior measure, which contains information about the quantity we are trying to estimate. In this thesis, we review a particular formulation, conventionally known as the strong constraint formulation of inverse problems. We describe methods to obtain summaries of information from the posterior measure. We also describe how prior measures are constructed using linear differential operators, to quantify as accurately as possible our knowledge of the parameters, independent of any observations. Then, we note that the strong constraint formulation of inverse problems makes it hard to obtain summaries of information of the posterior measure, typically obtained through an optimization of a misfit functional. Therefore, we introduce the <b>weak</b> <b>constraint</b> formulation in a Bayesian context for inverse problems, which eases the task of optimization. We use this formulation to perform sampling of the posterior measure. This method is tested on some simple test problems. We also compare the results between a strong and <b>weak</b> <b>constraint</b> formulation of inverse problems by studying a one-dimensional example. Finally, we apply the <b>weak</b> <b>constraint</b> formulation to the problem of full waveform inversion, which is a common problem in seismology. The forward problem we use here is the Laplace transform of the acoustic wave equation, and the inverse problem is solved in several frequencies. There are two approaches when observations at several frequencies are available. First is the sequential method, which processes the observation at different frequencies individually. The second method, which is the simultaneous method, processes the observations at all frequencies at once. We use the simultaneous method here, and used a non-trivial model problem, which yields promising results. </p...|$|E
40|$|International audienceIn this paper, we {{investigate}} image segmentation by region merging. Given any similarity measure between regions, satisfying some <b>weak</b> <b>constraints,</b> we give a general predicate for answering if two regions {{are to be}} merged or not during the segmentation process. Our predicate is generic and has six properties. The first one is its inde-pendance {{with respect to the}} similarity measure, that leads to a user-independant and adaptative predicate. Second, it is non-parametric, and does not rely on any assumption concerning the image. Third, due to its <b>weak</b> <b>constraints,</b> knowledge may be included in the predicate to fit better to the user's behaviour. Fourth, provided the similarity is well-chosen by the user, we are able to upperbound one type of error made during the image segmentation. Fifth, it does not rely on a particular segmentation algorithm and can be used with almost all region-merging algorithms in various application domains. Sixth, it is calculated quickly, and can lead with appropriated algorithms to very efficient segmentation...|$|R
40|$|Abstract: In this paper, the {{temperature}} stress of singer piece {{was calculated by}} FEM and the stress reaction of PP fiber and steel fiber was analyzed. It is knew that the fiber with lower elastic modulus and lagerer spacing can provide <b>weaker</b> <b>constraints</b> and the restriction stress of fiber concrete is smaller. The calculation result of fiber reason space between {{is consistent with the}} common value in engineering...|$|R
40|$|We {{consider}} bipartite quantum {{systems that}} are described completely by a state vector |Ψ(t) > and the fully deterministic Schrödinger equation. Under <b>weak</b> <b>constraints</b> and without any artificially introduced decoherence or irreversibility, the smaller of the two subsystems shows thermodynamic behaviour like relaxation into an equilibrium, maximization of entropy {{and the emergence of}} the Boltzmann energy distribution. This generic behaviour results from entanglement. Comment: 5 pages, 9 figure...|$|R
