270|701|Public
2500|$|Head-mounted {{displays}} (HMDs) {{have small}} displays that are mounted on headgear {{worn by the}} user. These systems are connected directly into the virtual simulation to provide the user with a more immersive experience. <b>Weight,</b> <b>update</b> rates and field of view {{are some of the}} key variables that differentiate HMDs. Naturally, heavier HMDs are undesirable as they cause fatigue over time. If the update rate is too slow, the system is unable to update the displays fast enough to correspond with a quick head turn by the user. [...] Slower update rates tend to cause simulation sickness and disrupt the sense of immersion. Field of view or the angular extent of the world that is seen at a given moment field of view can vary from system to system and has been found to affect the users sense of immersion.|$|E
50|$|The {{learning}} algorithm can {{be divided}} into two phases: propagation and <b>weight</b> <b>update.</b>|$|E
50|$|Next to the cascade {{correlation}} algorithm and the Levenberg - Marquardt algorithm, Rprop {{is one of}} {{the fastest}} <b>weight</b> <b>update</b> mechanisms.|$|E
50|$|Backpropagate {{the error}} through {{the net and}} perform <b>weight</b> <b>updates.</b>|$|R
5000|$|Learn {{incrementally}} through <b>weight</b> <b>updates</b> {{and eventually}} {{settle on the}} correct output ...|$|R
2500|$|The <b>weight</b> <b>updates</b> of {{backpropagation}} {{can be done}} via stochastic {{gradient descent}} using the following equation: ...|$|R
50|$|Based {{on current}} {{knowledge}} in algorithms, multiplicative <b>weight</b> <b>update</b> method {{was first used}} in Littlestone's Winnow Algorithm. It is utilized in machine learning to solve a linear program.|$|E
5000|$|... "Multiplicative weights" [...] {{implies the}} {{iterative}} rule used in algorithms {{derived from the}} Multiplicative <b>Weight</b> <b>Update</b> Method. It is given with different names in the different fields where it was discovered or rediscovered.|$|E
50|$|In Machine Learning, Littlestone and Warmuth {{generalized}} the Winnow algorithm to the Weighted Majority algorithm. Later, Freund and Schapire generalized it in {{the form}} of Hedge algorithm. AdaBoost Algorithm formulated by Yoav Freund and Robert Schapire also employed the Multiplicative <b>Weight</b> <b>Update</b> Method.|$|E
30|$|Indeed, the WRE {{approach}} (cf. Table 9) {{allows a}} relative gain of 10 %. The CER {{of the very}} compact model using FD <b>weight</b> <b>updating</b> rule without adaptation is 5.99 % and with adaptation it decreases to 5.36 %, which represents a relative decrease of 10.52 %. The gains obtained with compact models are similar (a relative decrease of 10.1 %, with FD <b>weight</b> <b>updating</b> rule).|$|R
40|$|Penelitian ini bertujuan untuk (1) mengidentifikasi karakteristik butir-butir tes pada perangkat soal ujian nasional mata pelajaran Matematika tingkat SMP tahun pelajaran 2009 / 2010 yang dikalibrasi dengan metode kalibrasi fixed {{parameter}}, dan (2) mengetahui metode kalibrasi fixed parameter yang paling akurat di antara metode NWU-OEM (no prior <b>weights</b> <b>updating</b> and one expectation-maximization cycle), NWU-MEM (no prior <b>weights</b> <b>updating</b> {{and multiple}} expectation-maximization cycles), OWU-OEM (one  prior <b>weights</b> <b>updating</b> and one expectation-maximization cycle), OWU-MEM (one prior <b>weights</b> <b>updating</b> and multiple expectation-maximization cycles), dan MWU-MEM (multiple <b>weights</b> <b>updating</b> and multiple expectation-maximization cycles). Penelitian ini menggunakan pendekatan kuantitatif deskriptif. Subjek penelitian adalah data respons ujian nasional mata pelajaran Matematika tingkat SMP tahun pelajaran 2009 / 2010 dari provinsi DI Yogyakarta. Kriteria akurasi metode adalah nilai fungsi informasi tes dan kesalahan pengukuran. Hasil penelitian adalah sebagai berikut. (1) Statistik parameter butir-butir tes pada perangkat ujian nasional mata pelajaran Matematika tingkat SMP tahun pelajaran 2009 / 2010 menunjukkan rerata indeks daya beda butir berada pada interval [1, 07 sampai   1, 14], rerata indeks kesukaran butir [- 0, 35 sampai  - 0, 20], dan rerata pseudo guessing < 0, 25. Nilai theta-nilai kemampuan-pada posisi  fungsi informasi butir menjadi maksimal menunjukkan grafik fungsi kelima metode kalibrasi fixed-parameter hampir berimpit. (2) Metode OWU-OEM merupakan metode yang paling akurat dalam mengestimasi parameter butir pada perangkat tes ujian nasional mata pelajaran Matematika tahun pelajaran 2009 / 2010. Kata kunci: akurasi, kalibrasi, fixed parameter, algoritma, Expectation-Maximization ______________________________________________________________ THE ACCURACY OF THE FIXED PARAMETER CALIBRATION METHOD:STUDY OF MATHEMATICS NATIONAL EXAMINATION TESTAbstract This study aimed to: (1) identify {{the characteristics of}} the test items on the mathematics test of the national examination which are calibrated with the fixed parameter calibration methods, and (2) reveal the most accurate fixed parameter calibration methods among NWU-OEM (no prior <b>weights</b> <b>updating</b> and one expectation-maximization cycle), NWU-MEM (no prior <b>weights</b> <b>updating</b> and multiple expectation-maximization cycles), OWU-OEM (one  prior <b>weights</b> <b>updating</b> and one expectation-maximization cycle), OWU-MEM (one prior <b>weights</b> <b>updating</b> and multiple expectation-maximization cycles), and MWU-MEM (multiple <b>weights</b> <b>updating</b> and multiple expectation-maximization cycles) methods. This study used descriptive quantitative approach. The subject is the testee’   responses to the mathematics national examination in junior high school in 2009 / 2010. The criteria of the accuracy methods are TIF and SEM. The research results are as follows. (1) Item of statistical parameter on Mathematics national examination test in 2009 / 2010 showed the average of item discrimination on the interval [1. 07, 1. 14], the average of item difficulty on the interval [- 0. 35, - 0. 20], and the average of pseudo guessing is c < 0. 25. Theta - ability - score where the  item information function maximalist showed the function of five fixed-parameter calibration methods almost coincides. (2) OEM-OWU method is the most accurate in estimating the parameters on mathematics national examination test in 2009 / 2010. Keywords: Accuracy, Calibration, Fixed Parameter, Algorithm, Expectation-Maximizatio...|$|R
30|$|In clean {{conditions}} (refering to Table 7), we {{can observe}} that the ULT step {{does not allow}} a DER decrease superior to the WRE alone approach. Nevertheless, {{there is a significant}} decrease of DER compared to the baseline. Indeed, the DER of the very compact model is reduced more than 38 % (to 3.04 % with MLE <b>weight</b> <b>updating</b> rule) and more than 48 % (to 2.26 % with the FD <b>weight</b> <b>updating</b> rule) for the compact model.|$|R
5000|$|One {{feature of}} the choice of {{exponential}} error function is that the error of the final additive model {{is the product of}} the error of each stage, that is, [...] Thus {{it can be seen that}} the <b>weight</b> <b>update</b> in the AdaBoost algorithm is equivalent to recalculating the error on [...] after each stage.|$|E
5000|$|... where η is the {{learning}} rate (typically 0.002 to 0.01), y is the predicted bit, and (y − P(1)) is the prediction error. The <b>weight</b> <b>update</b> algorithm differs from backpropagation {{in that the}} terms P(1)P(0) are dropped. This is because {{the goal of the}} neural network is to minimize coding cost, not root mean square error.|$|E
50|$|The {{algorithm}} {{most often}} used to train RBMs, that is, to optimize the weight vector , is the contrastive divergence (CD) algorithm due to Hinton, originally developed to train PoE (product of experts) models.The algorithm performs Gibbs sampling and is used inside a gradient descent procedure (similar to the way backpropagation is used inside such a procedure when training feedforward neural nets) to compute <b>weight</b> <b>update.</b>|$|E
30|$|Table 8 show {{results for}} the case of noisy condition. The ULT+WRE {{approach}} reduces the CER to 5.11 % (FD <b>weight</b> <b>updating</b> rule) for the very compact model. This represents a relative reduction of around 12 % compared to the baseline (CER at 5.80 %). With the upper memory size constraint, the CER decreases to 4.01 % (MLE <b>weight</b> <b>updating</b> rule). Compared to the 4.80 % of the baseline, it corresponds to a relative reduction of about 16 % while the memory footprint stays unchanged.|$|R
40|$|Recent {{works have}} {{highlighted}} scale invariance or symmetry that {{is present in}} the weight space of a typical deep network and the adverse effect that it has on the Euclidean gradient based stochastic gradient descent optimization. In this work, we show that these and other commonly used deep networks, such as those which use a max-pooling and sub-sampling layer, possess more complex forms of symmetry arising from scaling based reparameterization of the network weights. We then propose two symmetry-invariant gradient based <b>weight</b> <b>updates</b> for stochastic gradient descent based learning. Our empirical evidence based on the MNIST dataset shows that these updates improve the test performance without sacrificing the computational efficiency of the <b>weight</b> <b>updates.</b> We also show the results of training with one of the proposed <b>weight</b> <b>updates</b> on an image segmentation problem. Comment: Submitted to ICLR 2016. arXiv admin note: text overlap with arXiv: 1511. 0102...|$|R
3000|$|... where ε 0 {{is a small}} regular {{parameter}} {{for preventing}} from zero division. For the special case e(n)= 0 and ψ[e(n)]= 0, the <b>weight</b> <b>updating</b> is suspended.|$|R
5000|$|Given {{the same}} setup with N experts. Consider the special {{situation}} where the proportions of experts predicting positive and negative, counting the weights, are both close to 50%. Then, {{there might be a}} tie. Following the <b>weight</b> <b>update</b> rule in weighted majority algorithm, the predictions made by the algorithm would be randomized. The algorithm calculates the probabilities of experts predicting positive or negatives, and then makes a random decision based on the computed fraction: ...|$|E
5000|$|Multiplicative <b>weight</b> <b>update</b> {{method is}} a meta-algorithm. It is an {{algorithmic}} technique which [...] "maintains a distribution {{on a certain}} set of interest, and updates it iteratively by multiplying the probability mass of elements by suitably chosen factors based on feedback obtained by running another algorithm on the distribution". It was discovered repeatedly in very diverse fields such as machine learning (AdaBoost, Winnow, Hedge), optimization (solving LPs), theoretical computer science (devising fast algorithm for LPs and SDPs), and game theory.|$|E
50|$|The {{optimization}} algorithm repeats a two phase cycle, propagation and <b>weight</b> <b>update.</b> When an input vector {{is presented to}} the network, it is propagated forward through the network, layer by layer, until it reaches the output layer. The output of the network is then compared to the desired output, using a loss function. The resulting error value is calculated {{for each of the}} neurons in the output layer. The error values are then propagated from the output back through the network, until each neuron has an associated error value that reflects its contribution to the original output.|$|E
40|$|A new {{recurrent}} {{neural network}} based on B-spline function approximation is presented. The network {{can be easily}} trained and its training converges more quickly than that for other recurrent neural networks. Moreover, an adaptive <b>weight</b> <b>updating</b> algorithm for the recurrent network is proposed. It can speed up the training process of the network greatly and its learning speed is more quickly than existing algorithms, e. g., back-propagation algorithm. Examples are presented comparing the adaptive <b>weight</b> <b>updating</b> algorithm and the constant learning rate method, and illustrating its application to modelling of nonlinear dynamic system. published_or_final_versio...|$|R
40|$|Spike-timing-dependent {{plasticity}} (STDP) incurs both causal and acausal synaptic <b>weight</b> <b>updates,</b> {{for negative}} and positive time differences between pre-synaptic and post-synaptic spike events. For realizing such updates in neuromorphic hardware, current implementations either require forward and reverse lookup access to the synaptic connectivity table, or rely on memory-intensive architectures such as crossbar arrays. We present a novel method for realizing both causal and acausal <b>weight</b> <b>updates</b> using only forward lookup access of the synaptic connectivity table, permitting memory-efficient implementation. A simplified implementation in FPGA, using a single timer variable for each neuron, closely approximates exact STDP cumulative <b>weight</b> <b>updates</b> for neuron refractory periods greater than 10 ms, and reduces to exact STDP for refractory periods greater than the STDP time window. Compared to conventional crossbar implementation, the forward table-based implementation leads to substantial memory savings for sparsely connected networks supporting scalable neuromorphic systems with fully reconfigurable synaptic connectivity and plasticity. Comment: Submitted to BioCAS 201...|$|R
40|$|Abstract — We {{present a}} novel two {{transistor}} synapse (“ 2 TS”) that exhibits spike timing dependent plasticity (“STDP”). Temporal coincidence of synthetic pre- and post- synaptic action potentials across the 2 TS induces localized floating gate injection and tunneling {{that result in}} proportional Hebbian synaptic <b>weight</b> <b>updates.</b> In the absence of correlated pre- and post-synaptic activity, no significant <b>weight</b> <b>updates</b> occur. A compact implementation of the 2 TS has been designed, simulated, and fabricated in a commercial 0. 5 µm process. Suitable synthetic neural waveforms for symmetric STDP have been derived and circuit and network operation have been modeled and tested. Simulations agree with theory and preliminary experimental results. I...|$|R
5000|$|The {{basic idea}} behind LMS filter is to {{approach}} the optimum filter weights , by updating the filter weights in a manner to converge to the optimum filter weight. This {{is based on the}} gradient descent algorithm. The algorithm starts by assuming small weights(zero in most cases) and, at each step, by finding the gradient of the mean square error, the weights are updated.That is, if the MSE-gradient is positive, it implies, the error would keep increasing positively, if the same weight is used for further iterations, which means we need to reduce the weights. In the same way, if the gradient is negative, we need to increase the weights. So, thebasic <b>weight</b> <b>update</b> equation is : ...|$|E
5000|$|Head-mounted {{displays}} (HMDs) {{have small}} displays that are mounted on headgear {{worn by the}} user. These systems are connected directly into the virtual simulation to provide the user with a more immersive experience. <b>Weight,</b> <b>update</b> rates and field of view {{are some of the}} key variables that differentiate HMDs. Naturally, heavier HMDs are undesirable as they cause fatigue over time. If the update rate is too slow, the system is unable to update the displays fast enough to correspond with a quick head turn by the user. Slower update rates tend to cause simulation sickness and disrupt the sense of immersion. Field of view or the angular extent of the world that is seen at a given moment field of view can vary from system to system and has been found to affect the users sense of immersion.|$|E
30|$|In this paper, we {{consider}} the resilient backpropagation algorithm (RPROP) [32]. Basically, RPROP performs a direct adaptation of the <b>weight</b> <b>update</b> based on local gradient information. Only {{the sign of the}} partial derivative is used to perform both learning and adaptation. In doing so, the size of the partial derivative does not influence the <b>weight</b> <b>update.</b>|$|E
3000|$|The models {{based on}} the FDW <b>weight</b> <b>updating</b> rule seem not benefit from the {{adaptation}} phase; {{there is no significant}} decrease of the CER. It results certainly from the fact that FDW is {{based on the}} hypothesis that [...]...|$|R
30|$|We can {{notice that}} the WRE {{approach}} alone does not allow a decrease of the CER. The best CER reaches 5.99 % (WRE with FD <b>weight</b> <b>updating</b> rule) for the smallest model, whereas the CER of the baseline is 5.80 %.|$|R
40|$|This paper {{presents}} a fast Adaboost algorithm based on weight constraints, which can shorten the training time {{when dealing with}} a larger training data set. In the algorithm, the sample feature space is divided into several groups. Considering {{that the number of}} groups and sample dimension feature space are closely related, when the feature space dimension exceeds the number set group, the sample according to the set threshold array searches. Meanwhile, considering the sample <b>weights</b> <b>update</b> related to the number of misclassification samples, sample <b>weights</b> <b>update</b> adds to the number of misclassification, limiting its next iteration of the weights, thereby effectively preventing excessive weight adaptation phenomenon. Experiment shows that the algorithm achieves better results on UCI datasets...|$|R
40|$|This paper {{proposes a}} method for {{estimating}} the number of iterations required until convergence of a feed-forward neural network for any <b>weight</b> <b>update</b> frequency. The NETtalk neural application {{has been used to}} get data on convergence and the best learning rate. Our results show that less frequent <b>weight</b> <b>update</b> implies slower convergence. A formula is derived from the results, describing the relation between the error at start of training to the number of iterations at convergence. Fairly good estimation accuracy is obtained. The estimation formula is of major importance to a parallel BP implementation, which computes different training patterns simultaneously. 1 Introduction The <b>weight</b> <b>update</b> frequency of Back Propagation [1] has got a major impact on the error convergence during training. However, no theory exactly describes this relation. In this paper we will try to model the relation between the <b>weight</b> <b>update</b> interval (i. e. the number of training patterns that is presented between [...] ...|$|E
40|$|We {{discuss the}} <b>weight</b> <b>update</b> {{rule in the}} Cascade Correlation neural net {{learning}} algorithm. The <b>weight</b> <b>update</b> rule implements gradient descent optimization of the correlation between a new hidden unit's output and the previous network's error. We present a derivation of the gradient of the correlation function and show that our resulting <b>weight</b> <b>update</b> rule results in slightly faster training. We also show that the new rule is mathematically equivalent to the one presented in the original Cascade Correlation paper and discuss numerical issues underlying the difference in performance. Since a derivation of the Cascade Correlation <b>weight</b> <b>update</b> rule was not published, this paper should be useful to {{those who wish to}} understand the rule. In International Conference on Neural Networks, Perth, Western Australia, 1995, IEEE Press. 1. Introduction The Cascade Correlation algorithm of Fahlman & Lebiere (1990) is an elegant, simple, and powerful tool for classification and regression. It is a [...] ...|$|E
40|$|Discuss {{approaches}} to combine techniques used by ensemble learning methods. Randomness {{which is used}} by Bagging and Random Forests is introduced into Adaboost to get robust performance under noisy situation. Declare that when the randomness introduced into AdaBoost equals to 100, the proposed algorithm {{turns out to be}} a Random Forests with <b>weight</b> <b>update</b> technique. Approaches are discussed to improve the performance of Random Forests with <b>weight</b> <b>update</b> technique introduced...|$|E
40|$|This paper {{addresses}} {{the problem of}} text classification in high dimensionality spaces by applying linear <b>weight</b> <b>updating</b> classifiers that have been highly studied {{in the domain of}} machine learning. Our experimental results are based on the Winnow family of algorithms that are simple to implement and efficient in terms of computation time and storage requirements. We applied an exponential multiplication function to <b>weight</b> <b>updates</b> and we experimentally calculated the optimal values of the learning rate and the separating surface parameters. Our results are {{at the level of the}} best results that were reported on the family of linear algorithms and perform nearly as well as the top performing methodologies in the literature. © Springer-Verlag Berlin Heidelberg 2006...|$|R
40|$|Sequential Monte Carlo (SMC) {{methods have}} shown their {{effectiveness}} in data assimilation for wildfire simulation; however, when errors of wildfire simulation models are extremely large or rare events hap-pen, the current SMC methods have limited impacts on improving the simulation results. The major prob-lem {{lies in the}} proposal distribution that is commonly chosen as the system transition prior {{in order to avoid}} difficulties in importance <b>weight</b> <b>updating.</b> In this article, we propose a more effective proposal dis-tribution by taking advantage of information contained in sensor data, and also present a method to solve the problem in <b>weight</b> <b>updating.</b> Experimental results demonstrate that a SMC method with this proposal distribution significantly improves wildfire simulation results when the one with a system transition prior proposal fails. ...|$|R
40|$|We {{prove that}} QIP(2), {{the class of}} {{problems}} having two-message quantum interactive proof systems, is a subset of PSPACE. This relationship is obtained {{by means of an}} efficient parallel algorithm, based on the multiplicative <b>weights</b> <b>update</b> method, for approximately solving a certain class of semidefinite programs. ...|$|R
