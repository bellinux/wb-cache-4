8|38|Public
5000|$|In English {{and many}} other {{languages}} using some form of the Latin alphabet, the space is a good approximation of a word divider (<b>word</b> <b>delimiter),</b> although this concept has limits because of the variability with which languages emically regard collocations and compounds. Many English compound nouns are variably written (for example, ice box = ice-box = icebox; pig sty = pig-sty = pigsty) with a corresponding variation in whether speakers {{think of them as}} noun phrases or single nouns; there are trends in how norms are set, such as that open compounds often tend eventually to solidify by widespread convention, but variation remains systemic. In contrast, German compound nouns show less orthographic variation, with solidification being a stronger norm.|$|E
40|$|Treebank is an {{important}} resource for both research and application of natural language processing. For Vietnamese, we still lack of such kind of corpora. This paper presents up-to-date results of a project for Vietnamese treebank con-struction. Since Vietnamese is an isolated language and has no <b>word</b> <b>delimiter,</b> there are many ambiguities in sentence analysis. We systematically applied a lot of linguistic techniques to handle such ambiguities. Annotators are supported by automatic-labeling tools and a tree-editor tool. Raw texts are extracted from Youth, an online Vietnamese daily newspaper. The current annotation agreement is around 90 percent. ...|$|E
40|$|Held in {{conjunction}} with ACL-IJCNLP 2009 International audienceTreebank is an important resource for both research and application of natural language processing. For Vietnamese, we still lack such kind of corpora. This paper presents up-to-date results of a project for Vietnamese treebank construction. Since Vietnamese is an isolating language and has no <b>word</b> <b>delimiter,</b> there are many ambiguities in sentence analysis. We systematically applied a lot of linguistic techniques to handle such ambiguities. Annotators are supported by automatic labeling tools and a tree-editor tool. Raw texts are extracted from Tuoi Tre (Youth), an online Vietnamese daily newspaper. The current annotation agreement is around 90 percent...|$|E
40|$|We {{present a}} Chinese word {{segmentation}} model learned from punctuation marks which are perfect <b>word</b> <b>delimiters.</b> The learning is {{aided by a}} manually segmented corpus. Our method is considerably more effective than previous methods in unknown word recognition. This is a step toward addressing {{one of the toughest}} problems in Chinese word segmentation. 1...|$|R
40|$|There {{are several}} {{problems}} encountered for Chinese language processing as Chinese is written without <b>word</b> <b>delimiters.</b> The difficulty in defining a word {{makes it even}} harder. This paper explores the possibility of automatically segmenting Chinese character sequences into words and classifying these words through distributional analysis {{in contrast with the}} usual approaches that depends on dictionaries...|$|R
5000|$|The for-statement has {{the form}} for i:=base(increment)limit, {{directly}} resembling the loop of Rutishauser's programming language Superplan, replacing “=” with “:=”, and replacing its German keyword Für with the direct English translation for; ALGOL 60 replaced the parentheses with the <b>word</b> <b>delimiters</b> step and until, {{such that the}} previous statement instead would be i:=base step increment until limit.|$|R
40|$|Spelling {{check is}} an {{important}} preprocessing task when dealing with user generated texts such as tweets and product comments. Compared with some western languages such as English, Chinese spelling check is more complex {{because there is no}} <b>word</b> <b>delimiter</b> in Chinese written texts and misspelled characters can only be determined in word level. Our system works as follows. First, we use character-level n-gram language models to detect potential misspelled characters with low probabilities below some predefined threshold. Second, for each potential incorrect character, we generate a candidate set based on pronunciation and shape similarities. Third, we filter some candidate corrections if the candidate cannot form a legal word with its neighbors according to a word dictionary. Finally, we find the best candidate with highest language model probability. If the probability is higher than a predefined threshold, then we replace the original character; or we consider the original character as correct and take no action. Our preliminary experiments shows that our simple method can achieve relatively high precision but low recall. ...|$|E
40|$|Word-gesture {{keyboards}} enable fast {{text entry}} by letting users draw {{the shape of}} a word on the input surface. Such keyboards have been used extensively for touch devices, but not in mid-air, even though their fluent gestural input seems well suited for this modality. We present Vulture, a word-gesture keyboard for mid-air operation. Vulture adapts touch based word-gesture algorithms to work in mid-air, projects users ’ movement onto the display, and uses pinch as a <b>word</b> <b>delimiter.</b> A first 10 -session study suggests text-entry rates of 20. 6 Words Per Minute (WPM) and finds hand-movement speed to be the primary predictor of WPM. A second study shows that with training on a few phrases, participants do 28. 1 WPM, 59 % of the text-entry rate of direct touch input. Participants ’ recall of trained gestures in mid-air was low, suggesting that visual feedback is important but also limits performance. Based on data from the studies, we discuss improvements to Vulture and some alternative designs for mid-air text entry. Author Keywords Word-gesture keyboard; shape writing; text entry; mid-air interaction; in-air interaction; freehand interaction. ACM Classification Keywords I. 3. 6 Methodology and Techniques: Interaction technique...|$|E
40|$|International audienceRecently, it {{has been}} claimed that a linear {{relationship}} between a measure of information content and word length is expected from word length optimization and {{it has been}} shown that this linearity is supported by a strong correlation between information content and word length in many languages (Piantadosi et al 2011 Proc. Nat. Acad. Sci. 108 3825). Here, we study in detail some connections between this measure and standard information theory. The relationship between the measure and word length is studied for the popular random typing process where a text is constructed by pressing keys at random from a keyboard containing letters and a space behaving as a <b>word</b> <b>delimiter.</b> Although this random process does not optimize word lengths according to information content, it exhibits a linear relationship between information content and word length. The exact slope and intercept are presented for three major variants of the random typing process. A strong correlation between information content and word length can simply arise from the units making a word (e. g., letters) and not necessarily from the interplay between a word and its context as proposed by Piantadosi and co-workers. In itself, the linear relation does not entail the results of any optimization process...|$|E
40|$|We {{address the}} problem of {{informal}} word recognition in Chinese microblogs. A key problem is the lack of <b>word</b> <b>delimiters</b> in Chinese. We exploit this reliance as an opportunity: recognizing the relation be-tween informal word recognition and Chi-nese word segmentation, we propose to model the two tasks jointly. Our joint in-ference method significantly outperforms baseline systems that conduct the tasks in-dividually or sequentially. ...|$|R
40|$|The {{first step}} in Chinese NLP is to tokenize or segment {{character}} sequences into words, since the text contains no <b>word</b> <b>delimiters.</b> Recent heavy activity in this area has shown the biggest stumbling block to be words that are absent from the lexicon, since successful tokenizers to date {{have been based on}} dictionary lookup (e. g., Chang & Chen 1993...|$|R
40|$|Given {{the lack}} of <b>word</b> <b>delimiters</b> in written Japanese, word {{segmentation}} is generally considered a crucial first step in processing Japanese texts. Typical Japanese segmentation algorithms rely either on a lexicon and grammar or on pre-segmented data. In contrast, we introduce a novel statistical method utilizing unsegmented training data, with performance on kanji sequences comparable to and sometimes surpassing that of morphological analyzers over a variety of error metrics...|$|R
40|$|Abstract. The suffix tree {{of string}} w {{represents}} all suffixes of w, {{and thus it}} supports full indexing of w for exact pattern matching. On the other hand, a sparse suffix tree of w represents only {{a subset of the}} suffixes of w, and therefore it supports sparse indexing of w. There has been a wide range of applications of sparse suffix trees, e. g., natural language processing and biological sequence analysis. Word suffix trees are a variant of sparse suffix trees that are defined for strings that contain a special <b>word</b> <b>delimiter</b> #. Namely, the word suffix tree of string w = w 1 w 2 · · · wk, consisting of k words each ending with #, represents only the k suffixes of w of the form wi · · · wk. Recently, we presented an algorithm which builds word suffix trees in O(n) time with O(k) space, where n is the length of w. In addition, we proposed sparse directed acyclic word graphs (SDAWGs) and an on-line algorithm for constructing them, working in O(n) time and space. As a further achievement of this research direction, this paper introduces yet a new text indexing structure named sparse compact directed acyclic word graphs (SCDAWGs). We show that the size of SCDAWGs is smaller than that of word suffix trees and SDAWGs, and present an SCDAWG construction algorithm that works in O(n) time with O(k) space and in an on-line manner. ...|$|E
40|$|Khmer is the {{official}} language of Cambodia. It is a complex language. Similar to Chinese, Japanese and Thai, Khmer words are written without spaces or other <b>word</b> <b>delimiters.</b> This is a major challenge in spell checking Khmer {{since there is no}} simple way to determine word boundaries. However, it is feasible to spell check Khmer. The process of spell checking Khmer is different from the spell checking process in other languages that have <b>word</b> <b>delimiters</b> like English. In Khmer, words are constructed from root words that are made up of consonantal clusters, which can be misspelled. In order to do the spell checking, first we need to find the approximate clusters of each input clusters. We then give the possible sequences of the consonantal clusters to a hidden Markov model. The model will give the score of every sequence of consonantal clusters. Based the possible sequences and their scores, we know the word boundaries, whether or not a word is correctly spelled and some alternative words if it is misspelled. i...|$|R
40|$|The {{first step}} in Chinese NLP is to tokenize or segment {{character}} sequences into words, since the text contains no <b>word</b> <b>delimiters.</b> Recent heavy activity in this area has shown the biggest stumbling block to be words that are absent from the lexicon, since successful tokenizers to date {{have been based on}} dictionary lookup (e. g., Chang & Chen 1993; Chiang et al. 1992; Linet al. 1993; Wu & Tseng 1993; Sproat et al. 1994) ...|$|R
40|$|The {{processing}} of Japanese text {{is complicated by}} the fact that there are no <b>word</b> <b>delimiters.</b> To segment Japanese text, systems typically use knowledge-based methods and large lexicons. This paper presents a novel approach to Japanese word segmentation which avoids the need for Japanese word lexicons and explicit rule bases. The algorithm utilizes a hidden Markov model, a stochastic process, to determine word boundaries. This method has achieved 91 % accuracy in segmenting words in a test corpus. 1...|$|R
40|$|We {{present a}} novel OCR error {{correction}} method for languages without <b>word</b> <b>delimiters</b> {{that have a}} large character set, such as Japanese and Chinese. It consists of a statistical OCR model, an approxi-mate word matching method using character shape similarity, and a word segmentation algorithm us-ing a statistical language model. By using a sta-tistical OCR model and character shape similarity, the proposed error corrector outperforms the previ-ously published method. When the baseline char-acter recognition accuracy is 90 %, it achieves 97. 4 % character recognition accuracy. ...|$|R
40|$|Chinese {{is written}} without <b>word</b> <b>delimiters</b> so <b>word</b> {{segmentation}} {{is generally considered}} a key step in processing Chinese texts. This paper presents a new statistical approach to segment Chinese sequences into words based on contextual entropy {{on both sides of}} a bigram. It is used to capture the dependency with the left and right contexts in which a bigram occurs. Our approach tries to segment by finding the word boundaries instead of the words. Experimental results show that it is effective for Chinese word segmentation. ...|$|R
40|$|Fast re-training of word {{segmentation}} {{models is}} required for adapting to new resources or domains in NLP of many Asian languages without <b>word</b> <b>delimiters.</b> The traditional tokenization model is efficient but inaccurate. This paper proposes a phrase-based model that factors sentence tokenization into phrase tokenizations, the dependencies of which are also taken into account. The model has a good OOV recognition ability, which improves the overall performance significantly. The training is a linear time phrase extraction and MLE procedure, while the decoding is via dynamic programming based algorithms. ...|$|R
40|$|This paper {{studies the}} {{information}} {{content of the}} chromosomes of twenty-three species. Several statistics considering different number of bases for alphabet character encoding are derived. Based on the resulting histograms, <b>word</b> <b>delimiters</b> and character relative frequencies are identified. The knowledge of this data allows moving along each chromosome while evaluating the flow of characters and words. The resulting flux of information is captured by means of Shannon entropy. The results are explored in the perspective of power law relationships allowing a quantitative evaluation of the DNA of the species...|$|R
30|$|Tokenization {{deals with}} the {{splitting}} of text into units during data pre-processing. Text can be tokenized into paragraphs, sentences, phrases and single <b>words.</b> The <b>delimiters</b> used in this process vary with data sets.|$|R
40|$|Creative Commons Attribution License, which permits {{unrestricted}} use, distribution, {{and reproduction}} in any medium, provided the original work is properly cited. This paper studies the information {{content of the}} chromosomes of twenty-three species. Several statistics considering different number of bases for alphabet character encoding are derived. Based on the resulting histograms, <b>word</b> <b>delimiters</b> and character relative frequencies are identified. The knowledge of this data allows moving along each chromosome while evaluating the flow of characters and words. The resulting flux of information is captured by means of Shannon entropy. The results are explored in the perspective of power law relationships allowing a quantitative evaluation of the DNA of the species. 1...|$|R
40|$|In both Chinese and Dzongkha languages, the {{greatest}} challenge is to identify the word boundaries {{because there are no}} <b>word</b> <b>delimiters</b> as it is in English and other Western languages. Therefore, preprocessing and word segmentation {{is the first step in}} Dzongkha language processing, such as translation, spell-checking, and information retrieval. Research on Chinese word segmentation was conducted long time ago. Therefore, it is relatively mature, but the Dzongkha word segmentation has been less studied by researchers. In the paper, we have investigated this major problem in Dzongkha language processing using a probabilistic approach for selecting valid segments with probability being computed on the basis of the corpus...|$|R
40|$|Chinese {{is written}} without using spaces or other <b>word</b> <b>delimiters.</b> Although a text may {{be thought of}} as a {{corresponding}} sequence of words, there is considerable ambiguity in the placement of boundaries. Interpreting a text as a sequence of words is bene�cial for some information retrieval and storage tasks: for example, full-text search, word-based compression, and keyphrase extraction. We describe a scheme that infers appropriate positions for word boundaries using an adaptive language model that is standard in text compression. It is trained on a corpus of presegmented text, and when applied to new text, interpolates word boundaries so as to maximize the compression obtained. This simple and general method performs well with respect to specialized schemes for Chinese language segmentation. 1...|$|R
40|$|The Chinese {{language}} is written without using spaces or other <b>word</b> <b>delimiters.</b> Although a text may {{be thought of}} as a corresponding sequence of words, there is considerable ambiguity in the placement of boundaries. Interpreting a text as a sequence of words is beneficial for some information retrieval and storage tasks: for example, full-text search, word-based compression, and keyphrase extraction. We describe a scheme that infers appropriate positions for word boundaries using an adaptive language model that is standard in text compression. It is trained on a corpus of pre-segmented text, and when applied to new text, interpolates word boundaries so as to maximize the compression obtained. This simple and general method performs well with respect to specialized schemes for Chinese language segmentation. Keywords: Chinese segmentation, language models, text compression, statistical models, text mining...|$|R
40|$|Given {{the lack}} of <b>word</b> <b>delimiters</b> in written Japanese, word {{segmentation}} is generally considered a crucial first step in processing Japanese texts. Typical Japanese segmentation algorithms rely either on a lexicon and syntactic analysis or on pre-segmented data; but these are laborintensive, and the lexico-syntactic techniques are vulnerable to the unknown word problem. In contrast, we introduce a novel, more robust statistical method utilizing unsegmented training data. Despite its simplicity, the algorithm yields performance on long kanji sequences comparable to and sometimes surpassing that of state-of-the-art morphological analyzers over a variety of error metrics. The algorithm also outperforms another mostly-unsupervised statistical algorithm previously proposed for Chinese. Additionally, we present a two-level annotation scheme for Japanese to incorporate multiple segmentation granularities, and introduce two novel evaluation metrics, both based {{on the notion of}} a compatible bracket, that can account for multiple granularities simultaneously. ...|$|R
40|$|Neural machine {{translation}} (NMT) heavily relies on word-level modelling to learn semantic representations of input sentences. However, for languages without natural <b>word</b> <b>delimiters</b> (e. g., Chinese) where input sentences {{have to be}} tokenized first, conventional NMT is confronted with two issues: 1) {{it is difficult to}} find an optimal tokenization granularity for source sentence modelling, and 2) errors in 1 -best tokenizations may propagate to the encoder of NMT. To handle these issues, we propose word-lattice based Recurrent Neural Network (RNN) encoders for NMT, which generalize the standard RNN to word lattice topology. The proposed encoders take as input a word lattice that compactly encodes multiple tokenizations, and learn to generate new hidden states from arbitrarily many inputs and hidden states in preceding time steps. As such, the word-lattice based encoders not only alleviate the negative impact of tokenization errors but also are more expressive and flexible to embed input sentences. Experiment results on Chinese-English translation demonstrate the superiorities of the proposed encoders over the conventional encoder...|$|R
40|$|This paper {{studies the}} {{mechanisms}} behind the differential effects of inserting a space either {{before or after}} a two-character unit on information processing through the examination of eye movements in Chinese, a language {{where there is no}} <b>word</b> <b>delimiters.</b> A two-character unit in this study is either a word-preserving stimulus or a word-disrupting stimulus (i. e., nonword). The study aims for {{a better understanding of the}} cognitive mechanisms for lexical processing that may underlie observed facilitory or inhibitory effects of the spacing conditions in sentence context. Results show that inserting a space after a word facilitates lexical processing, but inserting a space before a word does not. Inserting a space before and after a nonword, however, does not show these effects. These results indicate that the effects of spaces before and after words are mainly influenced by word segmentation mechanisms rather than landing position effects or other factors. Department of Computing 2016 - 2017 > Academic research: refereed > Publication in refereed journalbcm...|$|R
40|$|The {{first step}} in Chinese NLP is to tokenize or segment {{character}} sequences into words, since the text contains no <b>word</b> <b>delimiters.</b> Recent heavy activity in this area has shown the biggest stumbling block to be words that are absent from the lexicon, since successful tokenizers to date {{have been based on}} dictionary lookup (e. g., Chang & Chen 1993; Chiang et al. 1992; Lin et al. 1993; Wu & Tseng 1993; Sproat et al. 1994). We present empirical evidence for four points concerning tokenization of Chinese text: (1) More rigorous "blind" evaluation methodology is needed to avoid inflated accuracy measurements; we introduce the nk-blind method. (2) The extent of the unknown-word problem is far more serious than generally thought, when tokenizing unrestricted texts in realistic domains. (3) Statistical lexical acquisition is a practical means to greatly improve tokenization accuracy with unknown words, reducing error rates as much as 32. 0 %. (4) When augmenting the lexicon, linguistic constrain [...] ...|$|R
40|$|Word {{segmentation}} is {{a problem}} in several Asian languages that have no explicit <b>word</b> boundary <b>delimiter,</b> e. g. Chinese, Japanese, Korean and Thai. We propose to use featurebased approaches for Thai word segmentation. A feature can be anything that tests for specific information in the context around the word in question, such as context words and collocations. To automatically extract such features from a training corpus, we employ two learning algorithms, namely RIPPER and Winnow. Experimental results show that both algorithms appear to outperform the existing Thai word segmentation methods, especially for context-dependent strings. 1 Introduction Word segmentation is a crucial problem in natural language processing for several Asian languages that have no explicit <b>word</b> boundary <b>delimiter,</b> e. g. Chinese, Japanese, Korean and Thai. The problem can be formally defined as finding arg max W i P (W i jC) = arg max W i P (W i) P (CjW i) =P (C) (1) where C = c 1 c 2 : : : c m is an input [...] ...|$|R
40|$|Human labeled corpus is {{indispensable}} {{for the training}} of supervised word segmenters. However, it is time-consuming and labor-intensive to label corpus manually. During the process of typing Chinese text by Pingyin, people usually need to type "space " or nu-meric keys to choose the words due to homo-phones, which {{can be viewed as}} a cue for segmentation. We argue that such a process can be used to build a labeled corpus in a more natural way. Thus, in this paper, we in-vestigate Natural Typing Annotations (NTAs) that are potential <b>word</b> <b>delimiters</b> produced by users while typing Chinese. A detailed analysis on over three hundred user-produced texts containing NTAs reveals that high-quality NTAs mostly agree with gold seg-mentation and, consequently, can be used for improving the performance of supervised word segmentation model in out-of-domain. Experiments show that a classification model combined with a voting mechanism can reli-ably identify the high-quality NTAs texts that are more readily available labeled corpus. Furthermore, the NTAs might be particularly useful to deal with out-of-vocabulary (OOV) words such as proper names and neo-logisms. ...|$|R
40|$|This study {{reports the}} {{development}} of a Myanmar word segmentation method using Unicode standard encoding. Word segmentation is an essential step prior to natural language processing in the Myanmar lan-guage, because a Myanmar text is a string of characters without explicit <b>word</b> boundary <b>delimiters.</b> The pro-posed method has two phases: syllable segmentation and syllable merging. A rule-based heuristic approach was adopted for syllable segmentation, and a dictionary-based statistical approach for syllable merging. Evaluation of test results showed that the method is very effective for the Myanmar language...|$|R
40|$|Zipf's law {{states that}} the {{relationship}} between the frequency of a word in a text and its rank (the most frequent word has rank, the 2 nd most frequent word has rank, [...] .) is approximately linear when plotted on a double logarithmic scale. It has been argued that the law is not a relevant or useful property of language because simple random texts - constructed by concatenating random characters including blanks behaving as <b>word</b> <b>delimiters</b> - exhibit a Zipf's law-like word rank distribution. In this article, we examine the flaws of such putative good fits of random texts. We demonstrate - by means of three different statistical tests - that ranks derived from random texts and ranks derived from real texts are statistically inconsistent with the parameters employed to argue for such a good fit, even when the parameters are inferred from the target real text. Our findings are valid for both the simplest random texts composed of equally likely characters as well as more elaborate and realistic versions where character probabilities are borrowed from a real text. The good fit of random texts to real Zipf's law-like rank distributions has not yet been established. Therefore, we suggest that Zipf's law might in fact be a fundamental law in natural languages...|$|R
40|$|The {{absence of}} {{explicit}} <b>word</b> boundary <b>delimiters,</b> such as spaces, in Japanese texts causes {{all kinds of}} troubles for Japanese morphological analysis systems. Particularly, out-of-vocabulary words represent {{a very serious problem}} for the systems which rely on dictionary data to establish word boundaries. In this paper we present a solution for decompounding of katakana sequences (one of the main sources of the out-of-vocabulary words) using a discriminative model based on Conditional Random Fields. One of the notable features of the proposed approach is its simplicity and memory efficiency...|$|R
3000|$|Additional {{information}} about the music is sparse in this work {{because of the large}} size of the music collection used (refer to Section 3.1): besides the year of release only the artist and title information is available for each song. While the date is directly used as a numeric attribute, the artist and title fields are processed in a similar way as the lyrics (cf. Section 2.1. 2 for a more detailed explanation of the methods): only the binary {{information about}} the occurrence of a word stem is obtained. The word stems are generated by string to word vector conversion applied to the artist and title attributes. Standard <b>word</b> <b>delimiters</b> are used to split multiple text strings to words and the Porter stemming algorithm [24] reduces words to common stems in order to map different forms of one word to their common stem. To limit the number of attributes that are left after conversion, a minimum word frequency is set, which determines how often a word stem must occur within one class. While the artist word list looks very specific to the collection of artists in the database, the title word list seems to have more general relevance with words like [...] "love", [...] "feel", or [...] "sweet". In total, the metadata attributes consist of one numeric date attribute and 152 binary numeric word occurrence attributes.|$|R
40|$|The {{application}} of text-to-speech (TTS) conversion has become {{widely used in}} recent years. Chinese TTS faces several unique difficulties. The most critical {{is caused by the}} lack of <b>word</b> <b>delimiters</b> in written Chinese. This means that Chinese word segmentation (CWS) must be the first step in Chinese TTS. Unfortunately, due to the ambiguous nature of word boundaries in Chinese, even the best CWS systems make serious segmentation errors. Incorrect sentence interpretation causes TTS errors, preventing TTS’s wider use in applications such as automatic customer services or computer reader systems for the visually impaired. In this paper, we propose a novel method that exploits unlabeled internal data to reduce word segmentation errors without using external dictionaries. To demonstrate the generality of our method, we verify our system on the most widely recognized CWS evaluation tool [...] the SIGHAN bakeoff, which includes datasets in both traditional and simplified Chinese. These datasets are provided by four representative academies or industrial research institutes in HK, Taiwan, Mainland China, and the U. S. Our experimental results show that with only internal data and unlabeled test data, our approach reduces segmentation errors by an average of 15 % compared to the traditional approach. Moreover, our approach achieves comparable performance to the best CWS systems that use external resources. Further analysis shows that our method has the potential to become more accurate as the amount of test data increases. Index Terms: text-to-speech, Chinese word segmentation, segmentation errors, internal unlabeled dat...|$|R
