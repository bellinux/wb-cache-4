0|37|Public
50|$|Packet writing (or {{incremental}} packet writing, IPW) is {{an optical}} disc recording technology used to allow <b>write-once</b> <b>and</b> rewritable CD and DVD media {{to be used}} in a similar manner to a floppy disk from within the operating system.|$|R
50|$|Various {{models and}} {{protocols}} {{have been devised}} for maintaining coherence, such as MSI, MESI (aka Illinois), MOSI, MOESI, MERSI, MESIF, <b>write-once,</b> <b>and</b> Synapse, Berkeley, Firefly and Dragon protocol. In 2011, ARM Ltd proposed the AMBA 4 ACE for handling coherency in SoCs.|$|R
50|$|API support can {{be added}} to Windows XP for burning DVDs and Blu-ray Discs (Mastered-style burning <b>and</b> UDF) on <b>write-once</b> <b>and</b> rewritable DVD and Blu-ray media by {{installing}} the Windows Feature Pack for Storage which upgrades IMAPI to version 2. Note that this does not add DVD or Blu-ray burning features to Windows Explorer but third party applications can use the APIs to support DVD and Blu-ray burning.|$|R
5000|$|DVD {{recordable}} and DVD rewritable {{refer to}} part of optical disc recording technologies. DVD optical disc formats {{that can be}} recorded by a DVD recorder, (written, [...] "burned"), either write once or rewritable (write multiple times) format written by laser, as compared to DVD-ROM, which is mass-produced by pressing, primarily for the distribution of home video. DVD recordable is a general term that refers to both <b>write-once</b> <b>and</b> rewritable formats, whereas DVD rewritable refers only to rewritable formats.|$|R
50|$|Professional Disc for DATA (PDD or ProDATA) was a general-use {{recording}} media variant of PFD, aimed primarily at {{small and medium-sized}} enterprise for data archival and backup. PDD drives and media became available in mid-2004. The BW-RS101 external SCSI-3 drive originally retailed in the UK at £2,344 (excl. VAT) directly from Sony, <b>and</b> 23 GB <b>write-once</b> <b>and</b> re-writeable media retailed for £30 each. Two other drives - the BW-F101/A internal SCSI drive and the BW-RU101 external USB 2.0 drive also became available around the same time.|$|R
40|$|Abstract. A {{flash memory}} has <b>write-once</b> <b>and</b> {{bulk-erase}} properties so that an intelligent allocation algorithm {{is essential to}} providing appli-cations efficient storage service. This paper first demonstrates that the online version of FLASH allocation problem is difficult, since we can find an adversary that makes every online algorithm to use as many number of blocks as a naive and inefficient algorithm. As a result we propose an offline allocation algorithm called Best Match (BestM) for allocating blocks in FLASH file systems. The experimental results indicate that BestM delivers better performance than a previously proposed First Re-arrival First Serve (FRFS) method. ...|$|R
40|$|Abstract:- In this paper, we will {{propose a}} {{flexible}} and scalable statistical environment {{based on the}} web service and offer a unified XML-based programming interface for statistical computing. We separate various statistical functionalities into groups of services and design its interface and architecture. All of services are defined as an agent and implemented using web service technology. And all the statistical operations, data, and results are formulated as a XML document. Finally, a prototype was designed and implemented to demonstrate the feasibility of the proposed framework. Statisticians can easily program their solution and the programs can be <b>Write-Once</b> <b>and</b> Run-Anywhere (WORA) ...|$|R
40|$|Flash memory {{technology}} is becoming critical in building embedded systems applications {{because of its}} shock-resistant, power economic, and non-volatile nature. Because flash memory is a <b>write-once</b> <b>and</b> bulkerase medium, a translation layer and a garbage collection mechanism is needed to provide applications a transparent storage service. In this paper, we propose a real-time garbage collection mechanism, which provides a deterministic performance, for hard real-time systems. A wear-levelling method, {{which serves as a}} non-real-time service, is presented to resolve the endurance issue of flash memory. The capability of the proposed mechanism is demonstrated by a series of experiments over our system prototype. ...|$|R
50|$|There {{are several}} {{standards}} that define how to structure data files on a CD-ROM. ISO 9660 defines the standard file {{system for a}} CD-ROM. ISO 13490 is an improvement on this standard which adds support for non-sequential <b>write-once</b> <b>and</b> re-writeable discs such as CD-R and CD-RW, as well as multiple sessions. The ISO 13346 standard was designed to address most of the shortcomings of ISO 9660, and a subset of it evolved into the UDF format, which was adopted for DVDs. The bootable CD specification was issued in January 1995, to make a CD emulate a hard disk or floppy disk, is called El Torito.|$|R
5000|$|Virtual Allocation Table a.k.a. VAT (Incremental Writing). Used {{specifically}} for writing to CD-R <b>and</b> (<b>write-once)</b> media ...|$|R
40|$|Flash memory {{technology}} {{is becoming more}} popular in designing and building embedded systems applications because of its shock-resistent, power economic, and nonvolatile nature. Because flash memory is a <b>write-once</b> <b>and</b> bulk-erase medium, the garbage collection mechanism on Flash Translation Layer is needed to provide applications a transparent and high bandwidth storage service. In this paper, we propose and implement a FAT-aware log-based Flash Translation Layer, which has two points of garbage collection time. We also propose two versions of victim selection policy which is to select a log block and invalidate it. The performance between the proposed victim selection policies is evaluated {{in terms of the}} effectiveness and the overhead by a series of experiments over our implemented system...|$|R
40|$|In {{this paper}} {{we report on}} recent phase-change {{research}} from the EU FP 6 project ProTeM (Probe-based Terabit per square inch Memory) that aims to develop scanning-probe based storage media and systems suited to archival and back-up storage applications, with storage densities in the range 1 to 10 Tbits/sq. in. and high write/read speeds. In particular we discuss the design and characterisation of new phase-change materials suited to <b>write-once</b> <b>and</b> re-writable probe-based storage, {{as well as new}} approaches to the self-assembly of phase-change materials for the production of patterned phase-change media. We also report on the important issue of tip wear when using electrical nanoprobes in contact with phase-change media, and describe new tip designs for the mitigation of tip wear effects. Key words: scanning probe storage, phase-change memories, ProTeM 1...|$|R
40|$|Embedded {{systems have}} been {{developing}} rapidly in re-cent years, and flash memory technology has become an es-sential building block because of its shock-resistance, low power consumption, and non-volatile nature. Since flash memory is a <b>write-once</b> <b>and</b> bulk-erase medium, an in-telligent allocation algorithm is essential to providing applications efficient storage service. In this paper, we pro-pose three allocation algorithms – a First Come First Serve (FCFS) method, a First Re-arrival First Serve (FRFS) method, and an Online First Re-arrival First Serve (OFRFS) method. Both FCFS and OFRFS are on-line allocation mechanisms which make allocation de-cision as the requests arrive. The FRFS method, which serves as an off-line mechanism, is developed as the stan-dard of performance comparison. The capability of the proposed mechanisms is demonstrated {{by a series of}} exper-iments and simulations. The experimental results indicate that FRFS provide superior performance when the data ac-cess pattern is analyzed in advance, and the on-line OFRFS method provides good performance by run-time estima-tion of access patterns. 1...|$|R
40|$|Flash memory {{technology}} is becoming critical in building embedded systems applications {{because of its}} shock-resistant, power economic, and non-volatile nature. With the recent technology breakthroughs in both capacity and reliability, flash-memory storage systems are now very popular in many types of embedded systems. However, because flash memory is a <b>write-once</b> <b>and</b> bulk-erase medium, we need a translation layer and a garbage collection mechanism to provide applications a transparent storage service. In the past work, various techniques were introduced to improve the garbage collection mechanism. These techniques aimed at both performance and endurance issues, but they all failed in providing applications a guaranteed performance. In this paper, we propose a real-time garbage collection mechanism, which provides a guaranteed performance, for hard real-time systems. On the other hand, the proposed mechanism supports non-real-time tasks so that the potential bandwidth of the storage system can be fully utilized. A wear-levelling method, which is executed as a non-real-time service, is presented to resolve the endurance problem of flash memory. The capability of the proposed mechanism is demonstrated {{by a series of}} experiments over our system prototype...|$|R
5000|$|February 2013: OpenEdge 11.2 release {{provided}} enhanced mobility capabilities through: visual {{designer and}} UI toolkit for phone and tablet applications, REST {{support for the}} OpenEdge application server, JavaScript Data Binding support libraries, <b>and</b> <b>write-once,</b> run anywhere support for iOS and Android; ...|$|R
50|$|The Compact Disc File System (CDFS) is a {{file system}} for {{read-only}} <b>and</b> <b>write-once</b> CD-ROMs developed by Simson Garfinkel and J. Spencer Love at the MIT Media Lab between 1985 and 1986. The file system {{provided for the}} creation, modification, renaming and deletion of files and directories on a write-once media. The file system was developed with a <b>write-once</b> CD-ROM simulator <b>and</b> was used to master {{one of the first}} CD-ROMs in 1986. CDFS was never sold, but its source code was published on the Internet and the CD-ROMs were distributed to Media Lab sponsors. The file system is the basis of WOFS (Write-once File System), sold by N/Hance systems in 1989.|$|R
40|$|Li, XiaomingIn {{the past}} years, {{processors}} have evolved into multi-core, and programs also turn into multi-threading. To {{meet the demand}} of parallel processing, we face new challenges to explore the implicit parallelism in programs and ease the programming on large scale parallel system. The MIT Fresh Breeze system applies principles of data flow computing, leverages the <b>write-once</b> <b>and</b> chunk based memory model to eliminate the cache coherence issue in modern multi-core processor. We develop a template based transform component in the Fresh Breeze compiler. It enables compiling funJava (subset of Java language) programs into codelets, automatically adapts the linear space memory model to the Fresh Breeze memory model. We test three common algebra function, demonstrate the Fresh Breeze system's massive concurrent execution ability and achieve the sub linear speedup performance. In addition, through evaluations by tuning the major features, we find optimal configurations for fresh breeze processor design. Comparison on various combinations of memory layouts proves that the conventional layout optimization techniques are able to effectively apply on the Fresh Breeze system. It brings future work on the global layout optimization {{for a group of}} functions in applications. University of Delaware, Department of Electrical and Computer EngineeringM. S...|$|R
50|$|The BDXL format allows 100 GB <b>and</b> 128 GB <b>write-once</b> discs, <b>and</b> 100 GB rewritable discs for {{commercial}} applications. It was defined in June 2010. BD-R 3.0 Format Specification (BDXL) defined a multi-layered disc recordable in BDAV format {{with the speed}} of 2× and 4×, capable of 100/128 GB and usage of UDF2.5/2.6. BD-RE 4.0 Format Specification (BDXL) defined a multi-layered disc rewritable in BDAV {{with the speed of}} 2× and 4×, capable of 100 GB and usage of UDF2.5 as file system.|$|R
25|$|Compact disc (CD) is {{a digital}} optical disc data storage format {{released}} in 1982 and co-developed by Philips and Sony. The format was originally developed to store and play only sound recordings but was later adapted for {{storage of data}} (CD-ROM). Several other formats were further derived from these, including <b>write-once</b> audio <b>and</b> data storage (CD-R), rewritable media (CD-RW), Video Compact Disc (VCD), Super Video Compact Disc (SVCD), Photo CD, PictureCD, CD-i, and Enhanced Music CD. The first commercially available Audio CD player, the Sony CDP-101, was released October 1982 in Japan.|$|R
40|$|DS is a {{dependable}} {{and secure}} data storage for mobile, wireless networks {{based on a}} peer-to-peer paradigm. DS share files under a <b>write-once</b> model, <b>and</b> ensures data confidentiality and dependability by encoding files in a Redundant Residue Number System. The paper analyzes the code efficiency of DS using a set of moduli allowing for efficient encoding and decoding procedures based on single precision arithmetic, with the Information Dispersal Algorithm approach (IDA) shows that DS features security features which are not provided by IDA, while the two approaches are comparable {{from the viewpoint of}} code efficiency and encoding/decoding complexity...|$|R
40|$|Abstract—Motivated by the {{emerging}} interests in non-volatile solid-state computer memories such as flash memories, this paper studies {{the problem of}} repeatedly storing information on memory cells with noise and state. The goal is to reliably convey t messages by writing Xnj on an n-cell noisy memory p(yj |xj, yj− 1), which stores Y nj at the j-th write. We model this problem as a channel with state and introduce the multiple-write noisy memory model, which includes the <b>write-once</b> memory <b>and</b> flash memory models as special cases. The t-write sum-capacity for the multiple-write noisy memory is established as Csum(t) = max I(X 1;Y 1) + t∑ j=...|$|R
40|$|We {{introduce}} the Game of Paxos {{to simplify the}} presentation of Paxos-style consensus protocols. We use this game to show how Lamport’s Paxos and Castro and Liskov’s PBFT are the same consensus protocol, but for different failure models. In this game, players try to store some value in a <b>write-once</b> register <b>and</b> quit only when they learn the register’s final value. The write-once register contains two novel abstractions: (i) a Paxos register that captures how processes in both Paxos and PBFT propose and decide values and (ii) tokens that capture how these protocols guarantee agreement despite partial failures. We encapsulate the differences between Paxos and PBFT in the implementation details of these abstractions. ...|$|R
50|$|Standard CDs have a {{diameter}} of 120 mm and can hold up to 80 minutes of audio (700 MB of data). The Mini CD has various diameters ranging from 60 to 80 mm; they are sometimes used for CD singles or device drivers, storing up to 24 minutes of audio. The technology was later adapted and expanded to include data storage CD-ROM, <b>write-once</b> audio <b>and</b> data storage CD-R, rewritable media CD-RW, Super Audio CD (SACD), Video Compact Discs (VCD), Super Video Compact Discs (SVCD), PhotoCD, PictureCD, CD-i, and Enhanced CD. CD-ROMs and CD-Rs remain widely used technologies in the computer industry. The CD and its extensions have been extremely successful: in 2004, worldwide sales of CD audio, CD-ROM, and CD-R reached about 30 billion discs. By 2007, 200 billion CDs had been sold worldwide.|$|R
40|$|There {{are many}} factors {{motivating}} {{the need to}} securely store private data {{for long periods of}} time. These range from requirements on business data demanded by recent legislation to the push towards digital creation of cultural and family heritage data. This information often needs to be stored securely; data such as medical and legal documents that could be important to future generations must be kept indefinitely but must not be publicly accessible. Unfortunately, existing storage solutions have yet to prove their ability to store data securely for long periods of time. To this end, the goal of secure, archival storage is to provide long-term data security and reliability. Archival storage has a unique usage model that is poorly served with techniques commonly found in conventional storage. The usage model of secure, long-term archival storage is <b>write-once,</b> read-maybe <b>and</b> thus stresses throughput ove...|$|R
40|$|A {{generalized}} rewriting {{model is}} defined for flash memory that represents stored data and permitted rewrite operations by a directed graph. This {{model is a}} generalization of previously introduced rewriting models of codes, including floating codes, <b>write-once</b> memory codes, <b>and</b> buffer codes. This model is used to design a new rewriting code for flash memories. The new code, referred to as trajectory code, allows stored data to be rewritten {{as many times as}} possible without block erasures. It is proved that the trajectory codes are asymptotically optimal {{for a wide range of}} scenarios. In addition, rewriting codes that use a randomized rewriting scheme are presented that obtain good performance with high probability for all possible rewrite sequences...|$|R
40|$|The Kodak Digital Science OD System 2000 E {{automated}} disk library (ADL) base module <b>and</b> <b>write-once</b> drive {{are being}} developed as the next generation commercial product to the currently available System 2000 ADL. Under government sponsorship with the Air Force's Rome Laboratory, Kodak is developing magneto-optic (M-O) subsystems compatible with the Kodak Digital Science ODW 25 drive architecture, which {{will result in a}} multifunction (MF) drive capable of reading and writing 25 gigabyte (GB) WORM media and 15 GB erasable media. In an OD system 2000 E ADL configuration with 4 MF drives and 100 total disks with a 50 % ration of WORM and M-O media, 2. 0 terabytes (TB) of versatile near line mass storage is available...|$|R
40|$|Abstract—A {{generalized}} rewriting {{model is}} defined for flash memory that represents stored data and permitted rewrite operations by a directed graph. This {{model is a}} generalization of previously introduced rewriting models of codes, including floating codes, <b>write-once</b> memory codes, <b>and</b> buffer codes. This model is used to design a new rewriting code for flash memories. The new code, referred to as trajectory code, allows stored data to be rewritten {{as many times as}} possible without block erasures. It is proved that the trajectory codes are asymptotically optimal {{for a wide range of}} scenarios. In addition, rewriting codes that use a randomized rewriting scheme are presented that obtain good performance with high probability for all possible rewrite sequences. Index Terms—Codes, flash memory, nonvolatile memory. I...|$|R
40|$|The {{storage density}} of shift-multiplexed {{holographic}} memory is calculated and compared with experimentally achieved densities {{by use of}} photorefractive <b>and</b> <b>write-once</b> materials. We consider holographic selectivity {{as well as the}} recording material s dynamic range (M /#) and required diffraction efficiencies in formulating the calculations of storage densities, thereby taking into account all major factors limiting the raw storage density achievable with shift-multiplexed holographic storage systems. We show that the M /# is the key factor in limiting storage densities rather than the recording material s thickness for organic materials in which the scatter is relatively high. A storage density of 100 bits m 2 is experimentally demonstrated by use of a 1 -mm-thick LiNbO 3 crystal as the recording medium...|$|R
40|$|This {{application}} note presents several mechanisms {{supported by the}} TMS 320 DSP Algorithm Standard (referred to as XDAIS) which can be employed by algorithm writers to assist application developers in optimizing their system’s performance and sharing of memory resources. The first mechanism is based on using statically allocated global data structures. The second mechanism is built on the concepts of scratch memory and memory overlaying techniques based on algorithm activation and deactivation. The third mechanism {{is referred to as}} the <b>write-once</b> buffers technique <b>and</b> support sharing of relocatable read-only data across instances of a common algorithm. The last mechanism {{is referred to as the}} parent instance technique, whereby the algorithm developer defines and associates an anonymous parent instance with each algorithm instance that gets created. Parent instance memory can b...|$|R
40|$|We {{investigate}} the duality of the binary erasure channel (BEC) and the binary defect channel (BDC). This duality holds for channel capacities, capacity achieving schemes, minimum distances, and upper bounds on {{the probability of}} failure to retrieve the original message. In addition, the relations between BEC, BDC, binary erasure quantization (BEQ), <b>and</b> <b>write-once</b> memory (WOM) are described. From these relations we claim that {{the capacity of the}} BDC can be achieved by Reed-Muller (RM) codes under maximum a posterior (MAP) decoding. Also, polar codes with a successive cancellation encoder achieve the capacity of the BDC. Inspired by the duality between the BEC and the BDC, we introduce locally rewritable codes (LWC) for resistive memories, which are the counterparts of locally repairable codes (LRC) for distributed storage systems. The proposed LWC can improve endurance limit and power efficiency of resistive memories. Comment: Presented at Information Theory and Applications (ITA) Workshop 2016. arXiv admin note: text overlap with arXiv: 1602. 0120...|$|R
40|$|Modern {{buildings}} are being integrated with myriad (often > 1000 s) networked sensors to improve convenience, occupant comfort accessibility and energy-efficient operations. These technological improvements hold {{the promise of}} significant advances in centralized operation and management, fault diagnosis, and integration to an emerging smart grid. As of 2012, 14 % of the buildings in the U. S. deployed Building Management Systems (BMS) to provide some kind of programmatic interface to the the sensors, actuators, and historical data management. Innovations in "Internet of Things" (IoT) devices have further led to connected lights, power meters, occupancy sensors and appliances {{that are capable of}} interfacing with the underlying BMS systems used in building automation. New {{buildings are}} installed with a BMS by design, and older buildings are being continuously retrofitted with networked systems for improved efficiency. However, whether provided by novel sensor networks or legacy instrumentation, extracting meaningful information from sensor data and taking actions based on that data depends fundamentally on the metadata available to interpret it. There are more than 5 million commercial buildings in the US, with the sensors in each building set up with customized and obscure metadata. One cannot achieve scalable deployments of software analytics and applications across buildings if deploying them requires vendors and domain experts spending spending 100 s of hours fixing each building. Today even well-established applications do not get deployed at scale because of this very reason. Thus, the major challenge is scalability, i. e a paradigm where an application can be written once and deployed on 100 s or 1000 s of buildings. This thesis evaluates the challenges with existing metadata of sensors in smart-buildings and proposes ways to normalize it to uniform standard that would allow scalable (<b>write-once</b> <b>and</b> deploy everywhere) application development. We develop three empirical criteria for successful metadata schemas [...] - (a) completeness, or the ability to capture all sensors, (b) ability to capture all relationships between sensors required by state-of-the-art applications, and (c) flexibility in incorporating novel sensors and applications, and usability. We empirically demonstrate that no existing smart-building sensor metadata schema satisfy these properties and develop a schema based on an underlying graphical data model, Brick, that does. We validate Brick across 6 large and diverse commercial buildings (comprising more than 17, 000 sensors) in two different continents and set up by different BMS vendors. We also develop a human-in-the-loop synthesis technique which uses syntactic and data-driven steps to parse legacy metadata into a common schema. This technique allows building-experts, who might not be conversant with sophisticated regular expression programs, to parse more than 70 % of the legacy metadata in a building to a common schema by providing example parses of only about 1 % of the sensors. We also show how to use active perturbations of subsystems in a building to construct functional relationships between subsystems that may have been missing or incorrectly captured in legacy metadata with an accuracy of ~ 80 %. Finally, we demonstrate the power of our normalized smart-building metadata schema paradigm by using the standardized sensor and relationship representations to implement both simple (e. g. finding errant zones, identifying inefficient air handling subsystems to save energy) and sophisticated applications (e. g finding rough occupancy estimates) that scale across real-world smart-buildings...|$|R
2500|$|Mixtures of decimal prefixes and binary sector sizes require care to {{properly}} calculate total capacity. Whereas semiconductor memory naturally favors powers of two (size doubles each time an address pin {{is added to}} the integrated circuit), the capacity of a disk drive is the product of sector size, sectors per track, tracks per side and sides (which in hard disk drives with multiple platters can be greater than 2). Although other sector sizes have been known in the past, formatted sector sizes are now almost always set to powers of two (256 bytes, 512 bytes, etc.), and, in some cases, disk capacity is calculated as multiples of the sector size rather than only in bytes, leading to a combination of decimal multiples of sectors and binary sector sizes. For example, 1.44MB 3½-inch HD disks have the [...] "M" [...] prefix peculiar to their context, coming from their capacity of 2,880 512-byte sectors (1,440 KiB), consistent with neither a decimal megabyte nor a binary mebibyte (MiB). Hence, these disks hold 1.47MB or 1.41MiB. Usable data capacity {{is a function of the}} disk format used, which in turn is determined by the FDD controller and its settings. Differences between such formats can result in capacities ranging from approximately 1300 to 1760 KiB (1.80 MB) on a standard 3½-inch high-density floppy (and up to nearly 2 MB with utilities such as 2M/2MGUI). The highest capacity techniques require much tighter matching of drive head geometry between drives, something not always possible and unreliable. For example, the LS-240 drive supports a 32MB capacity on standard 3½-inch HD disks, but it is, however, a <b>write-once</b> technique, <b>and</b> requires its own drive.|$|R
40|$|The error-handling {{capability}} of the Compact Disc Error Correction Code (ECC) {{is influenced by the}} statistical distribution of the errors that contaminate the recorded data. Error measurement hardware and software are developed to examine the recorded data on Compact Discs and report the locations of the erroneous data with single-byte resolution. A sample population of 8 read-only <b>and</b> 8 <b>write-once</b> Compact Discs are subjected to error measurement and the recovered error statistics are compared for the two types of media. A novel Markov state machine is developed to quantitatively characterize the measured errors and produce block-error probability rates at the input of the ECC decoder. These probability rates are used to obtain reliability estimates for the recovered data bytes at the output of the ECC decoder. The {{capability of}} the Compact Disc's ECC decoder to detect and subsequently correct the erroneous data bytes is greatly influenced by the particular decoding strategy used by the ECC decoder. The performance of seven different ECC decoding strategies that are applied to the recovered data from read-only <b>and</b> <b>write-once</b> Compact Discs are evaluated. In addition, the ECC decoder performance of a newly proposed Compact Disc recording format known as the Compact Disc-Direct Access Storage Disc (CD-DASD) is investigated. Reliability estimates for the data occurring at the output of the CD-DASD ECC decoder is compared to that of conventional CD-ROM's Cross-Interleaved Reed-Solomon Code decoder...|$|R
40|$|Storage {{systems are}} {{experiencing}} a historical paradigm shift from hard disk to nonvolatile memories {{due to its}} advantages such as higher density, smaller size and non-volatility. On the other hand, Solid Storage Disk (SSD) also poses critical challenges to application and system designers. The first challenge is called endurance. Endurance means flash memory can only experience {{a limited number of}} program/erase cycles, and after that the cell quality degradation can no longer be accommodated by the memory system fault tolerance capacity. The second challenge is called reliability, which means flash cells are sensitive to various noise and disturbs, i. e., data may change unintentionally after experiencing noise/disturbs. The third challenge is called security, which means it is impossible or costly to delete files from flash memory securely without leaking information to possible eavesdroppers. In this dissertation, we first study noise modeling and capacity analysis for NAND flash memories (which is the most popular flash memory in market), which gains us some insight on how flash memories are working and their unique noise. Second, based on the characteristics of content-replication codewords in flash memories, we propose a joint decoder to enhance the flash memory reliability. Third, we explore data representation schemes in flash memories and optimal rewriting code constructions in order to solve the endurance problem. Fourth, in order to make our rewriting code more practical, we study noisy write-efficient memories <b>and</b> <b>Write-Once</b> Memory (WOM) codes against inter-cell interference in NAND memories. Finally, motivated by the secure deletion problem in flash memories, we study coding schemes to solve both the endurance and the security issues in flash memories. This work presents a series of information theory and coding theory research studies on the aforesaid three critical issues, and shows that how coding theory can be utilized to address these challenges...|$|R
40|$|An {{international}} standard has emerged {{for the first}} true multimedia format. Digital Versatile Disk (by its official name), you may know it as Digital Video Disks. DVD has applications in movies, music, games, information CD-ROMS, and many other areas where massive amounts of digital information is needed. Did I say massive amounts of data? Would you believe over 17 gigabytes on {{a single piece of}} plastic the size of an audio-CD? That`s the promise, at least, by the group of nine electronics manufacturers who have agreed to the format specification, and who hope to make this goal a reality by 1998. In this major agreement, which didn`t come easily, the manufacturers will combine Sony and Phillip`s one side double-layer NMCD format with Toshiba and Matsushita`s double sided Super-Density disk. By Spring of this year, they plan to market the first 4. 7 gigabyte units. The question is: Will DVD take off? Some believe that read-only disks recorded with movies will be about as popular as video laser disks. They say that until the eraseable/writable DVD arrives, the consumer will most likely not buy it. Also, DVD has a good market for replacement of CD- Roms. Back in the early 80 `s, the international committee deciding the format of the audio compact disk decided its length would be 73 minutes. This, they declared, would allow Beethoven`s 9 th Symphony to be contained entirely on a single CD. Similarly, today it was agreed that playback length of a single sided, single layer DVD would be 133 minutes, long enough to hold 94 % of all feature-length movies. Further, audio can be in Dolby`s AC- 3 stereo or 5. 1 tracks of surround sound, better than CD-quality audio (16 -bits at 48 kHz). In addition, there are three to five language tracks, copy protection and parental ``locks`` for R rated movies. DVD will be backwards compatible with current CD-ROM and audio CD formats. Added versatility comes by way of multiple aspect rations: 4 : 3 pan-scan, 4 : 3 letterbox, and 16 : 9 widescreen. MPEG- 2 is the selected image compression format, with full ITU Rec. 601 video resolution (72 Ox 480). MPEG- 2 and AC- 3 are also part of the U. S. high definition Advance Television standard (ATV). DVD has an average video bit rate of 3. 5 Mbits/sec or 4. 69 Mbits/sec for image and sound. Unlike digital television transmission, which will use fixed length packets for audio and video, DVD will use variable length packets with a maximum throughput of more than 1 OMbits/sec. The higher bit rate allows for less compression of difficult to encode material. Even with all the compression, narrow-beam red light lasers are required to significantly increase the physical data density of a platter by decreasing the size of the pits. This allows 4. 7 gigabytes of data on a single sided, single layer DVD. The maximum 17 gigabyte capacity is achieved by employing two reflective layers {{on both sides of the}} disk. To read the imbedded layer of data, the laser`s focal length is altered so that the top layer pits are not picked up by the reader. It will be a couple of years before we have dual-layer, double-sided DVDS, and it will be achieved in four stages. The first format to appear will be the single sided, single layer disk (4. 7 gigabytes). That will allow Hollywood to begin releasing DVD movie titles. DVD-ROM will be the next phase, allowing 4. 7 gigabytes of CD-ROM-like content. The third stage will be <b>write-once</b> disks, <b>and</b> stage four will be rewritable disks. These last stages presents some issues which have yet to be resolved. For one, copyrighted materials may have some form of payment system, and there is the issue that erasable disks reflect less light than today`s DVDS. The problem here is that their data most likely will not be readable on earlier built players...|$|R

