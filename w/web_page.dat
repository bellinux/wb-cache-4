9723|10000|Public
5|$|Recently, he {{has become}} a less obscure figure with the {{publication}} of several book-length biographies on him, as well as anthologies of his writings. In 2007 a literary critic for New Yorker magazine observed that five such biographies and two such anthologies had been published since 2000. There has also been a <b>web</b> <b>page</b> created that is dedicated to Wallace scholarship.|$|E
5|$|The 1999 {{collapse}} of Bonfire was witnessed {{by thousands of}} people around the world. The Texas A Department of Computer Science set up a camera aimed at the Bonfire site that took a picture every 10minutes and posted it on the Internet. On the day of the collapse over 29,000 visitors visited the <b>web</b> <b>page,</b> at a time when only 20 million people worldwide had Internet access.|$|E
5|$|To {{convert the}} {{backlink}} data gathered by BackRub's web crawler into {{a measure of}} importance for a given <b>web</b> <b>page,</b> Brin and Page developed the PageRank algorithm, and realized {{that it could be}} used to build a search engine far superior to existing ones. The new algorithm relied on a new kind of technology that analyzed the relevance of the backlinks that connected one <b>Web</b> <b>page</b> to another, and allowed the number of links and their rank, to determine the rank of the page. Combining their ideas, the pair began utilizing Page's dormitory room as a machine laboratory, and extracted spare parts from inexpensive computers to create a device that they used to connect the nascent search engine with Stanford's broadband campus network. After filling Page's room with equipment, they then converted Brin's dorm room into an office and programming center, where they tested their new search engine designs on the Web. The rapid growth of their project caused Stanford's computing infrastructure to experience problems.|$|E
40|$|This bachelor`s thesis {{deals with}} <b>web</b> <b>pages</b> design and {{structure}} of new <b>web</b> <b>pages</b> after previous analysis of actual <b>web</b> <b>pages.</b> It includes at analysis of negatives and positives of the actual website and new <b>web</b> <b>pages</b> design, functions, design of code structure and content structure of new <b>web</b> <b>pages</b> with possibility of later application...|$|R
40|$|In this paper, we will {{introduce}} a new algorithm {{used to determine the}} sink <b>web</b> <b>pages</b> in a <b>web</b> application. The sink <b>web</b> <b>pages</b> are defined by using a partial order relation among the <b>web</b> <b>pages</b> of a <b>web</b> application. Using only these <b>web</b> <b>pages,</b> we will describe a method of determining all the <b>web</b> <b>pages</b> in a <b>web</b> application, containing errors...|$|R
40|$|As {{a part of}} {{this thesis}} {{existing}} <b>web</b> <b>pages</b> about planar geometrical transformations were evaluated and new <b>web</b> <b>pages</b> were created. Pages in Czech and in English language were evaluated shortly verbally. There is a table summarizing findings about existing <b>web</b> <b>pages</b> {{at the end of this}} part. The second part of this thesis contains newly created <b>web</b> <b>pages.</b> Scope of subject matter of created <b>web</b> <b>pages</b> overreaches common secondary school curriculum. There are Java applets used for illustrating findings about geometrical transformations as an integral part of <b>web</b> <b>pages.</b> Solving of exercises is accompanied by step-by-step operation. <b>Web</b> <b>pages</b> contain definitions, theorems with proofs and explanations of constructions...|$|R
5|$|Acid2 {{is a test}} page {{published}} and promoted by the Web Standards Project to expose <b>web</b> <b>page</b> rendering flaws in web browsers and other applications that render HTML. Named after the acid test for gold, it {{was developed in the}} spirit of Acid1, a relatively narrow test of compliance with the Cascading Style Sheets 1.0 (CSS1) standard, and was released on 13 April 2005. As with Acid1, an application passes the test if the way it displays the test page matches a reference image.|$|E
5|$|Data URIs: The actual {{images that}} form the eyes are encoded as data URIs, which allow {{multimedia}} to be embedded in web pages rather than stored as a separate file. Acid2 tests the most common case, where a binary image is base64-encoded into text and then that encoded text is included in a data URI in the <b>web</b> <b>page.</b> Although the IETF published the data URI specification in 1998, they never formally adopted it as a standard. Nonetheless, the HTML 4.01 specification references the data URI scheme, and data URI support has now been implemented in most browsers.|$|E
5|$|PHP code may be {{embedded}} into HTML or HTML5 markup, {{or it can}} be used {{in combination}} with various web template systems, web content management systems and web frameworks. PHP code is usually processed by a PHP interpreter implemented as a module in the web server or as a Common Gateway Interface (CGI) executable. The web server software combines the results of the interpreted and executed PHP code, which may be any type of data, including images, with the generated <b>web</b> <b>page.</b> PHP code may also be executed with a command-line interface (CLI) and can be used to implement standalone graphical applications.|$|E
40|$|Finding and obtaining {{information}} efficiently {{from the}} Web {{is one of}} the important ele-ments in realizing Smart Home environment. Users expect to find most relevant information within the shortest possible time. In this paper, we investigate the similarity of <b>Web</b> <b>pages</b> within Strongly Connected Components (SCCs). SCCs are overlapping groups of <b>Web</b> <b>pages</b> that may imply a relationship between the <b>Web</b> <b>pages</b> of the same component. Therefore, we seek to trace the similarity of these groups of <b>Web</b> <b>pages</b> using Cosine Similarity. Our experiment performed on Malaysian <b>Web</b> <b>pages</b> indicates that <b>Web</b> <b>pages</b> within same SCC carry a common topic or theme. This finding proves that we may locate <b>Web</b> <b>pages</b> with similar topic using the hyperlinks structure, without performing expensive analysis on the contents of the <b>Web</b> <b>pages.</b> ...|$|R
50|$|Google Web Accelerator sent {{requests}} for <b>web</b> <b>pages,</b> except for secure <b>web</b> <b>pages</b> (HTTPS), to Google, which logged these requests. Some <b>web</b> <b>pages</b> embedded personal information in these page requests.|$|R
40|$|This project {{explains}} {{the process of}} clustering <b>web</b> <b>pages.</b> With the immense {{increase in the number}} of <b>web</b> <b>pages</b> available on the internet, it has become difficult to search for <b>web</b> <b>pages.</b> The clustering of <b>web</b> <b>pages</b> will improve the presentation of <b>web</b> <b>pages</b> to the user and saves the time spent on searching <b>web</b> <b>pages.</b> Various clustering techniques have been proposed by various research scientists to cluster the <b>web</b> <b>pages,</b> but all the techniques suggested have some drawbacks. Since there is lot of scope for further improvement in the field of clustering, the system proposed in this report takes the clustering of <b>web</b> <b>pages</b> a step ahead. The proposed system use the queries from the user and get the results from search engine, then processes the results and provides the final result clusters to users...|$|R
5|$|Page and Brin {{used the}} former's basic HTML {{programming}} skills {{to set up}} a simple search page for users, as they did not have a <b>web</b> <b>page</b> developer to create anything visually elaborate. They also began using any computer part they could find to assemble the necessary computing power to handle searches by multiple users. As their search engine grew in popularity among Stanford users, it required additional servers to process the queries. In August 1996, the initial version of Google, still on the Stanford University website, was made available to Internet users.|$|E
5|$|In late-October 2013, HTC {{released}} Sense 5.5 for the HTC One, {{a software}} revision which adds RSS and Google+ support to BlinkFeed, {{allows users to}} disable BlinkFeed entirely, adds a tool for making animated GIFs, and additional Highlights themes. Shortly after its unveiling on 31 October 2013, HTC confirmed an upcoming release of Android 4.4.2 for all HTC One models, beginning with the Google Play version, and for Sense-equipped models in January 2014. For 4.4, HTC introduced a <b>web</b> <b>page</b> that allows users to track the development and release process of Android upgrade for its products, and aimed to release the 4.4 upgrade for the HTC One within 90 days of the release of its source code.|$|E
5|$|PHP is a {{general-purpose}} {{scripting language}} that is especially suited to server-side web development, in which case PHP generally runs on a web server. Any PHP code in a requested file is executed by the PHP runtime, usually to create dynamic <b>web</b> <b>page</b> content or dynamic images used on websites or elsewhere. It {{can also be used}} for command-line scripting and client-side graphical user interface (GUI) applications. PHP can be deployed on most web servers, many operating systems and platforms, and can be used with many relational database management systems (RDBMS). Most web hosting providers support PHP for use by their clients. It is available free of charge, and the PHP Group provides the complete source code for users to build, customize and extend for their own use.|$|E
40|$|The aim of {{this thesis}} is create {{methodology}} for evaluation of Czech cities <b>web</b> <b>pages.</b> The thesis includes aplication of this methodology into <b>Web</b> <b>pages</b> of Domazlice, Tachov and Rokycany. For this purpose I formulated criteria for evaluation of <b>Web</b> <b>pages.</b> In term of these criteria is accentuated informative value, usability, accessability and design of <b>Web</b> <b>pages.</b> Informative value was defined to reply for assessed pages sphere. In next part of this thesis I performed cricital analysis of <b>Web</b> <b>pages</b> of selected cities. On {{the basis of these}} results are formulated main drawbacks of <b>Web</b> <b>pages</b> and suggested solution...|$|R
40|$|In {{this paper}} we propose new methods for {{ordering}} the <b>Web</b> <b>pages</b> returned from search engines. Given a few search keywords, nowadays most search engines could retrieve {{more than a few}} thousand <b>Web</b> <b>pages.</b> The problem is how to order the retrieved <b>Web</b> <b>pages</b> and then to present the most relevant <b>Web</b> <b>pages</b> first. We propose new factors to allo...|$|R
40|$|Abstract — With {{the rapid}} growth of World Wide Web, finding useful and desired {{information}} in {{a short amount of time}} becomes an important issue for Web users. Search engines and focused crawlers help people to navigate the internet. A user expresses her information need in the form of a query and there is huge number of <b>Web</b> <b>pages</b> returning to this query. However, the majority of users view only a single page (the top 10 <b>Web</b> <b>pages</b> as ranked by the search engine) returned by a search engine. Even if the returned <b>Web</b> <b>pages</b> do not provide the exact information they need, the users also do not refine their query based on the returning results of their initial query. Thus, not only finding relevant <b>Web</b> <b>pages</b> but also ranking them plays an important role for the search engines. For this reason, determining the quality of <b>Web</b> <b>pages</b> is one of the main priorities of search engines, since low quality <b>Web</b> <b>pages</b> cause search engines results to be extremely vague and flooded with irrelevant <b>Web</b> <b>pages.</b> In this paper, we propose a novel method for determining the quality of <b>Web</b> <b>pages.</b> The proposed method first identifies the genre of <b>Web</b> <b>pages</b> and then it determines the quality of <b>Web</b> <b>pages</b> based on their genre. Our experimental results show that our proposed method is very effective and efficient. I...|$|R
5|$|Perl's text-handling {{capabilities}} can be {{used for}} generating SQL queries; arrays, hashes, and automatic memory management make it easy to collect and process the returned data. For example, in Tim Bunce's Perl DBI application programming interface (API), the arguments to the API can be the text of SQL queries; thus it is possible to program in multiple languages at the same time (e.g., for generating a <b>Web</b> <b>page</b> using HTML, JavaScript, and SQL in a here document). The use of Perl variable interpolation to programmatically customize each of the SQL queries, and the specification of Perl arrays or hashes as the structures to programmatically hold the resulting data sets from each SQL query, allows a high-level mechanism for handling large amounts of data for post-processing by a Perl subprogram.|$|E
5|$|Geologists and the {{official}} Ricketts Glen State Park <b>web</b> <b>page</b> classify the falls at Ricketts Glen State Park into two types. Wedding-cake falls descend {{in a series of}} small steps. Within the park, this type of falls usually flows over thin layers of Huntley Mountain Formation sandstone. In bridal-veil falls, the second type, water falls over a ledge and drops vertically into a plunge pool in the stream bed below. Within the park, this type of falls flows over Catskill Formation rocks or the red shale and sandstone of the Huntley Formation. In the park, the harder caprock which forms the ledge from which the bridal-veil falls drops is gray sandstone. The softer red shale below is eroded away by water, sand and gravel to form the plunge pool. Brown's book Pennsylvania waterfalls: a guide for hikers and photographers uses four types to classify waterfalls: falls, cascade, slide, and chute.|$|E
5|$|In September 2014, {{a wave of}} {{ransomware}} Trojans surfaced {{that first}} targeted users in Australia, under the names CryptoWall and CryptoLocker (which is, as with CryptoLocker 2.0, unrelated to the original CryptoLocker). The Trojans spread via fraudulent e-mails claiming to be failed parcel delivery notices from Australia Post; to evade detection by automatic e-mail scanners that follow all links on a page to scan for malware, this variant was designed to require users to visit a <b>web</b> <b>page</b> and enter a CAPTCHA code before the payload is actually downloaded, preventing such automated processes {{from being able to}} scan the payload. Symantec determined that these new variants, which it identified as CryptoLocker.F, were again, unrelated to the original CryptoLocker due to differences in their operation. A notable victim of the Trojans was the Australian Broadcasting Corporation; live programming on its television news channel ABC News 24 was disrupted for half an hour and shifted to Melbourne studios due to a CryptoWall infection on computers at its Sydney studio.|$|E
40|$|This paper {{presents}} a publishing system for efficiently creating dynamic Web content. Complex <b>Web</b> <b>pages</b> are constructed from simpler fragments. Fragments may recursively embed other fragments. Relationships between <b>Web</b> <b>pages</b> and fragments {{are represented by}} object dependence graphs. We present algorithms for efficiently detecting and updating <b>Web</b> <b>pages</b> affected after one or more fragments change. We also present algorithms for publishing sets of <b>Web</b> <b>pages</b> consistently; different algorithms are used depending upon the consistency requirements. Our publishing system provides an easy method for Web site designers to specify and modify inclusion relationships among <b>Web</b> <b>pages</b> and fragments. Users can update content on multiple <b>Web</b> <b>pages</b> by modifying a template. The system then automatically updates all <b>Web</b> <b>pages</b> affected by the change. Our system accommodatesboth content that must be proofread before publication and is typically from humans as well as content {{that has to be}} published immed [...] ...|$|R
40|$|Web spider {{is one of}} {{the most}} {{important}} components of the search engine, which has been used for accessing the <b>web</b> <b>pages</b> through the World Wide Web(WWW). When we crawl the <b>web</b> <b>pages</b> with different data sizes, then we can access limited <b>web</b> <b>pages</b> according to transfer rate of total data size in per unit time. In case of increasing the data size of <b>web</b> <b>pages,</b> number of <b>web</b> pagesdecrease. So it affects the performance of web spider. This paper describes the issue of data size of <b>web</b> <b>pages,</b> crawled by the web spider. This problem has been implemented here regarding to achieve high performance of web spider by using compression technique, which reduces the data size with big compression ratio of crawled <b>web</b> <b>pages.</b> </strong...|$|R
50|$|Photos of club {{activities}} {{can be seen on}} the EEMC <b>web</b> <b>pages</b> and through links on the <b>web</b> <b>pages.</b>|$|R
5|$|The Florida State University Libraries {{house one}} of the largest {{collections}} of documents in the state of Florida. The Libraries' collections include over 3.2 million volumes, with a website offering access to more than 1,064 databases, 119,385 e-journals, and over 1.1 million e-books. In total, Florida State has thirteen libraries and millions of books and journals to choose from. The collection covers virtually all disciplines and includes a wide array of formats – from books and journals to manuscripts, maps, and recorded music. Increasingly collections are digital and are accessible on the Internet via the library <b>web</b> <b>page</b> or the library catalog. The FSU Library System also maintains subscriptions to a vast number of online databases which can be accessed from any student account on or off campus. The current dean of the Library System is Julia Zimmerman, who oversees a staff of over 268 employees and a $17.5million annual budget recorded in 2013.|$|E
25|$|There is {{a common}} {{misconception}} that JavaScript was influenced by an earlier <b>Web</b> <b>page</b> scripting language developed by Nombas named C-- (not {{to be confused with}} the later C-- created in 1997). Brendan Eich, however, had never heard of C-- before he created LiveScript. Nombas did pitch their embedded <b>Web</b> <b>page</b> scripting to Netscape, though <b>Web</b> <b>page</b> scripting was not a new concept, as shown by the ViolaWWW Web browser. Nombas later switched to offering JavaScript instead of C-- in their ScriptEase product and was part of the TC39 group that standardized ECMAScript.|$|E
25|$|The {{consequences}} of the flaw are severe, it allows any <b>Web</b> <b>page</b> you visit to download, install, and run any code it likes on your computer. Any <b>Web</b> <b>page</b> can seize control of your computer; then it can do anything it likes. That's about as serious as a security flaw can get.|$|E
40|$|ABSTRACT In this paper, we {{are trying}} to {{introduce}} a method of selection of some <b>web</b> <b>pages</b> from a <b>web</b> application, which will be verified by using different validating mechanisms. The number of selected <b>web</b> <b>pages</b> cannot be higher than a previously established constant. The method of selection of these <b>web</b> <b>pages</b> must assure the highest possible quality of the verification of the entire application. The error detection of these <b>web</b> <b>pages</b> will automatically lead to the error detection in other pages. This fact will be realised by using an equivalence relation among the <b>web</b> <b>pages</b> of the <b>web</b> application...|$|R
40|$|Abstract. In {{this paper}} we {{describe}} the semantic partitioner algorithm, that uses the structural and presentation regularities of the <b>Web</b> <b>pages</b> to automatically transform them into hierarchical content structures. These content structures enable us to automatically annotate labels in the <b>Web</b> <b>pages</b> with their semantic roles, thus yielding meta-data and instance information for the <b>Web</b> <b>pages.</b> Experimental results with the TAP knowledge base and computer science department Web sites, comprising 16, 861 <b>Web</b> <b>pages</b> indicate that our algorithm is able gather meta-data accurately from various types of <b>Web</b> <b>pages.</b> The algorithm is able to achieve this performance without any domain specific engineering requirement. ...|$|R
40|$|In this paper, we {{are trying}} to {{introduce}} a method of selection of some <b>web</b> <b>pages</b> from a <b>web</b> application, which will be verified by using different validating mechanisms. The number of selected <b>web</b> <b>pages</b> cannot be higher than a previously established constant. The method of selection of these <b>web</b> <b>pages</b> must assure the highest possible quality of the verification of the entire application. The error detection of these <b>web</b> <b>pages</b> will automatically lead to the error detection in other pages. This fact will be realised by using an equivalence relation among the <b>web</b> <b>pages</b> of the <b>web</b> application...|$|R
25|$|For the web designer, {{when each}} <b>web</b> <b>page</b> {{comes from a}} web template, they can think about a modular <b>web</b> <b>page</b> {{structured}} with components that can be modified independently of each other. These components may include a header, footer, global navigation bar (GNB), local navigation bar and content such as articles, images, videos etc.|$|E
25|$|In this example, the meta element {{describes}} {{the contents of}} a <b>web</b> <b>page.</b>|$|E
25|$|In 1994, Craig Jackson {{created a}} <b>web</b> <b>page</b> for the Coalition to Ban DHMO.|$|E
40|$|Since very recently, users on {{the social}} {{bookmarking}} service Delicious can stack <b>web</b> <b>pages</b> in addition to tagging them. Stacking enables users to group <b>web</b> <b>pages</b> around specific themes {{with the aim of}} recommending to others. However, users still stack a small subset of what they tag, and thus many <b>web</b> <b>pages</b> remain unstacked. This paper presents early research towards automatically clustering <b>web</b> <b>pages</b> from tags to find stacks and extend recommendations. 1...|$|R
30|$|Except for {{presentation}} functions, HTML tags {{also provide}} valuable structural information for <b>web</b> <b>pages,</b> {{which are very}} helpful to improve extraction performance. The nested structure of HTML tags in <b>web</b> <b>pages</b> can be interpreted into a tree structure, which is named DOM model. We use Jsoup parser [30] to interpret <b>web</b> <b>pages</b> into DOM trees. DOM tree is the logical model of <b>web</b> <b>pages,</b> which allows <b>web</b> <b>pages</b> to be processed easily in memory. DOM model transforms each pair of HTML tags into a subtree, such as 〈DIV〉 and 〈 /DIV〉. A traversal operation can be exerted on DOM tree to access all branch and leaf nodes, and then output 〈path, content〉 pairs. In addition, we also use regular expressions to remove those unrelated tags and parts from <b>web</b> <b>pages,</b> such as subtrees covered by 〈script〉 and 〈 /script〉, which are not relevant to the required content. The pre-pruning will simplify the <b>web</b> <b>pages,</b> which is demonstrated in Fig.  2 b, c and d.|$|R
40|$|In this paper, a {{recommendation}} system for querying <b>web</b> <b>pages</b> is developed. When users query <b>web</b> <b>pages</b> through a search engine, the query keywords, browsed <b>web</b> <b>pages,</b> and feedback values are collected as user query transactions. A clustering algorithm based on bipartite graph analysis {{is designed to}} determine clusters of query keywords and the browsed <b>web</b> <b>pages,</b> called access preference clusters. Next, association rules of query keywords and <b>web</b> <b>pages</b> are mined for each access preference cluster. The feedback values of browsed <b>web</b> <b>pages</b> are incorporated into the calculation of the support and confidence for each association rule to reflect the subjective opinions. Based on the mined clusters and rules, the system applies the concept of collaborative filtering to recommend highly semantics relevant <b>web</b> <b>pages</b> in the access preference clusters partially covered by a user profile or a given query. The initial experiment result shows the system can improve the querying effect of searching engines...|$|R
