0|3463|Public
40|$|The {{future of}} {{communication}} and advertising is mobile. Some forecasts estimate that by 2015 more than 7. 1 billion mobile-connected devices {{will be ready to}} operate as an advertising medium; however there is little empirical evidence as to exactly how mobile advertising works in the currently prevailing mobile technology. This research examines mental imagery elicited by advertising in the mobile medium and its mediating role in responses to the message: ad trust and purchase intention. Using a factorial experimental design, we examine the influence of the type of message –SMS <b>vs.</b> <b>visual</b> ad– and <b>content</b> orientation strategy –informational vs. transformational– on mental imagery. Results show a greater impact of visual and transformational ads on vividness and elaboration, while SMSs exert greater effect on the quantity dimension. Vivid and elaborate mental imagery mediates the effect of advertising strategies on ad trust and exerts a positive influence on purchase intention...|$|R
40|$|Background: Normal reading {{requires}} eye {{guidance and}} activation of lexical representations so that words in text {{can be identified}} accurately. However, {{little is known about}} how the <b>visual</b> <b>content</b> of text supports eye guidance and lexical activation, and thereby enables normal reading to take place. Methods and Findings: To investigate this issue, we investigated eye movement performance when reading sentences displayed as normal and when the spatial frequency content of text was filtered to contain just one of 5 types of visual content: very coarse, coarse, medium, fine, and very fine. The effect of each type of <b>visual</b> <b>content</b> specifically on lexical activation was assessed using a target word of either high or low lexical frequency embedded in each sentence Results: No type of <b>visual</b> <b>content</b> produced normal eye movement performance but eye movement performance was closest to normal for medium and fine <b>visual</b> <b>content.</b> However, effects of lexical frequency emerged early in the eye movement record for coarse, medium, fine, and very fine <b>visual</b> <b>content,</b> and were observed in total reading times for target words for all types of <b>visual</b> <b>content.</b> Conclusion: These findings suggest that while the orchestration of multiple scales of <b>visual</b> <b>content</b> is required for normal eye-guidance during reading, a broad range of <b>visual</b> <b>content</b> can activate processes of word identification independently. Implications for understanding the role of <b>visual</b> <b>content</b> in reading are discussed. Peer-reviewedPublisher Version 11810...|$|R
5000|$|... @Large Films {{produces}} award-winning <b>visual</b> <b>content,</b> {{including traditional}} Broadcast, In-Store/POS, Tradeshow/Exhibition, Web, Educational, Internal Marketing, and many more. Anywhere that <b>visual</b> <b>content</b> is displayed, @Large has the expertise and production capabilities {{to fit the}} medium.|$|R
30|$|MARVEL dataset {{includes}} several labeled vessel attributes {{some of which}} relate to the <b>visual</b> <b>content.</b> Here, as interesting applications, by studying only the <b>visual</b> <b>content,</b> we targeted predicting and classifying four important attributes: draught, gross tonnage, length, and summer deadweight.|$|R
50|$|<b>Visual</b> <b>content</b> - Breathe Editing Inc.|$|R
40|$|Abstract — Understanding {{of images}} {{continues}} {{to be one of}} the most exciting and rapidly-growing research areas in various fields of technology. The recent advancements in hardware and telecommunication technologies like satellite communication in combination with the ongoing web proliferation have boosted growth of digital <b>visual</b> <b>content</b> on a large scale. However, this rate of growth has not been matched by the simultaneous improvement of technologies to support efficient image analysis and their retrieval. As a result, the overflow of available <b>visual</b> <b>content</b> resulted in large number of users facing hindrance in accessing information of the appropriate <b>visual</b> <b>content.</b> Moreover, with the immense number of diverse application areas that have emerged, which rely solely on image processing systems, has further revealed the tremendous potential for effective use of <b>visual</b> <b>content</b> through intelligent analysis. Bette...|$|R
40|$|In this paper, we {{theoretically}} {{analyze the}} efficiency of a multiscale non-linear video decomposition scheme, which is used for progressive retrieval and navigation of video sequences (hierarchical summarization). In particular, the efficiency is expressed as the difficulty for a user to locate a relevant video segments, while moving through different levels of hierarchy. It is proved that multiscale video organization is more efficient than the sequential one, even in the worst case where the organization is not performed on a <b>visual</b> <b>content</b> perception basis. However, much greater improvement is achieved in case that video organization is accomplished {{with respect to the}} human perception of the <b>visual</b> <b>content</b> based on <b>visual</b> <b>content</b> classification algorithms. Experimental evaluation of multiscale organization is presented with respect to sequential scanning for different <b>visual</b> <b>content</b> organization and video decomposition methods. 1...|$|R
5000|$|Guideline 1: Provide {{equivalent}} {{alternatives to}} auditory and <b>visual</b> <b>content</b> ...|$|R
5000|$|Informatics and <b>Visual</b> <b>Contents</b> (joint study {{programme}} with Dongseo University, South Korea) ...|$|R
5000|$|Tribute videos with <b>visual</b> <b>content</b> {{merged with}} popular rock and pop songs.|$|R
5000|$|<b>V</b> <b>Visual</b> Arts Award Emilia Ortiz of Nayarit State Government (May 2005).|$|R
5000|$|Next Generation Web Searches for <b>Visual</b> <b>Content,</b> Michael Lew, IEEE Computer, pp. 46-53, 2000.|$|R
40|$|<b>Visual</b> <b>Contents</b> such as {{images and}} video does not only contain objects, {{location}} and actions but also cues about affect, emotion and sentiment. Such information I very useful to understand <b>visual</b> <b>content</b> beyond semantic concept presence thus {{making it more}} explainable to the user. Images are the easiest medium through which people can express their emotions on social networking sites. Social media users are increasingly using images and videos to express their opinions and share their experiences. Sentiment analysis of such large scale <b>visual</b> <b>content</b> can help better extract user sentiments toward events or topics, {{such as those in}} image tweets, so that prediction of sentiment from <b>visual</b> <b>content</b> is complementary to textual sentiment analysis. Significant progress has been made with this technology, however, there is little research focus on the picture sentiments. This paper proposes a novel approach that exploits latent correlations among multiple views: visual and textual views, and a sentiment view constructed using SentiWordNet...|$|R
50|$|Edugraphics {{combines}} 3 basic elements: <b>visual</b> <b>content</b> (icons, images, colors), pedagogical content, and {{a learning}} scenario (story).|$|R
40|$|Image {{classification}} is {{an enthusiastic}} research field where {{large amount of}} image data is classified into various classes based on their <b>visual</b> <b>contents.</b> Researchers have presented various low-level features-based techniques for classifying images into different categories. However, efficient and effective classification and retrieval is still a challenging problem due to complex nature of <b>visual</b> <b>contents.</b> In addition, the traditional information retrieval techniques are vulnerable to security risks, making it easy for attackers to retrieve personal <b>visual</b> <b>contents</b> such as patients records and law enforcement agencies databases. Therefore, we propose a novel ontology-based framework using image steganography for secure image classification and information retrieval. The proposed framework uses domain-specific ontology for mapping the low-level image features to high-level concepts of ontologies which consequently results in efficient classification. Furthermore, the proposed method utilizes image steganography for hiding the image semantics as a secret message inside them, making the information retrieval process secure from third parties. The proposed framework minimizes the computational complexity of traditional techniques, increasing its suitability for secure and real-time <b>visual</b> <b>contents</b> retrieval from personalized image databases. Experimental results confirm the efficiency, effectiveness, and security of the proposed framework as compared with other state-of-the-art systems. Comment: A short paper of 11 pages for secure <b>visual</b> <b>contents</b> retrieval. The original version can be accessed at this link: [URL]...|$|R
50|$|Kinesthetic learning- The {{child is}} animated, {{possibly}} singing, while processing the audio and <b>visual</b> <b>content</b> from Grover’s song.|$|R
40|$|<b>Visual</b> <b>content</b> in {{learning}} material most {{commonly found in}} schools learning materials and less in higher education learning. Students in universities and colleges are dependent on wordy textbook and lecture notes to study. Use of <b>visual</b> <b>contents</b> depends on educator's interests, needs and willingness to provide the material to students. Nowadays, learning started to emerge at a rapid pace in producing learners with excellent academic achievements. The role of cloud computing hence increases the capability of delivering education from educator's perspectives. The {{purpose of this paper}} is to highlight important features of cloud computing in enhancing the use of interactive <b>visual</b> <b>content</b> in higher education learning and promotes interactive learning to students. Systematic Literature Review (SLR) method is used to obtain primary data from online databases Scopus and by using the coding procedure in Grounded Theory(Strauss & Corbin, 1990), research produces meta-model data of codes extractions from primary data. Findings shows there are four major abstractions of cloud features that lead to enhancing interactive <b>visual</b> <b>content</b> use in higher education...|$|R
40|$|Omnidirectional <b>visual</b> <b>content</b> {{is a form}} of {{representing}} graphical and cinematic media content which provides subjects with the ability to freely change the direction of view. Along with virtual reality, omnidirectional imaging is becoming a very important type of the modern media content. This brings new challenges to the omnidirectional <b>visual</b> <b>content</b> processing, especially in the field of compression and quality evaluation. More specifically, the ability to assess quality of omnidirectional images in reliable manner is a crucial step to provide high level quality of immersive experience. In this paper we introduce a testbed suitable for subjective evaluations of omnidirectional <b>visual</b> <b>contents.</b> We also show the results of a conducted pilot experiment to illustrate the applicability of the proposed testbed...|$|R
5000|$|Media {{processing}} technologies {{required to}} prepare <b>visual</b> <b>content</b> for analysis include transcoding, decoding, enhancement, restoration, edge detection, and segmentation.|$|R
2500|$|... the {{sub-field}} {{of computer}} science which studies methods for digitally synthesizing and manipulating <b>visual</b> <b>content,</b> see study of computer graphics ...|$|R
50|$|The podcast is {{presented}} in an enhanced format that allows listeners to pick a chapter and provides additional <b>visual</b> <b>content.</b>|$|R
5000|$|... the {{sub-field}} {{of computer}} science which studies methods for digitally synthesizing and manipulating <b>visual</b> <b>content,</b> see study of computer graphics ...|$|R
50|$|Changes {{in camera}} {{perspective}} {{are accompanied by}} changes in the <b>visual</b> <b>content</b> available to the observer. Using eye-tracking as a measure and monitor of visual attention, researchers deduced that visual attention mediates the camera perspective bias. That is, the correlation between camera perspective and the resulting bias {{is caused by the}} viewer's visual attention, which is decided by the angle of the camera. This provides evidence that differences in <b>visual</b> <b>content</b> may also mediate bias.|$|R
50|$|Guilford's {{original}} {{model was}} composed of 120 components (when the behavioral component is included) because he had not separated Figural Content into separate Auditory and <b>Visual</b> <b>contents,</b> nor had he separated Memory into Memory Recording and Memory Retention. When he separated Figural into Auditory and <b>Visual</b> <b>contents,</b> his model increased to 5 x 5 x 6 = 150 categories. When Guilford separated the memory functions, his model finally increased to 180 factors.|$|R
40|$|Abstract. Two common {{approaches}} to retrieving images from a collection are retrieval by text keywords, and retrieval by <b>visual</b> <b>content.</b> However, {{it is widely}} recognised {{that it is impossible}} for keywords alone to fully describe <b>visual</b> <b>content.</b> In this paper we present our approach of combining evidence from a contentoriented XML retrieval system and a content-based image retrieval system using a linear evidence combination approach as part of the INEX 2005 Multimedia track. ...|$|R
5000|$|... {{appropriate}} smiling or {{crying in}} response to the linguistic or <b>visual</b> <b>content</b> of emotional but not to neutral topics or stimuli.|$|R
40|$|Automatically {{describing}} video {{content with}} natural {{language is a}} fundamental challenge of multimedia. Recurrent Neural Networks (RNN), which models sequence dynamics, has attracted increasing attention on visual interpretation. However, most existing approaches generate a word locally with given previous words and the <b>visual</b> <b>content,</b> while the relationship between sentence semantics and <b>visual</b> <b>content</b> is not holistically exploited. As a result, the generated sentences may be contextually correct but the semantics (e. g., subjects, verbs or objects) are not true. This paper presents a novel unified framework, named Long Short-Term Memory with visual-semantic Embedding (LSTM-E), which can simultaneously explore the learning of LSTM and visual-semantic embedding. The former aims to locally maximize the probability of generating the next word given previous words and <b>visual</b> <b>content,</b> while the latter {{is to create a}} visual-semantic embedding space for enforcing the relationship between the semantics of the entire sentence and <b>visual</b> <b>content.</b> Our proposed LSTM-E consists of three components: a 2 -D and/or 3 -D deep convolutional neural networks for learning powerful video representation, a deep RNN for generating sentences, and a joint embedding model for exploring the relationships between <b>visual</b> <b>content</b> and sentence semantics. The experiments on YouTube 2 Text dataset show that our proposed LSTM-E achieves to-date the best reported performance in generating natural sentences: 45. 3 % and 31. 0 % in terms of BLEU@ 4 and METEOR, respectively. We also demonstrate that LSTM-E is superior in predicting Subject-Verb-Object (SVO) triplets to several state-of-the-art techniques...|$|R
40|$|Distributed visual {{analysis}} applications, such as mobile visual search or Visual Sensor Networks (VSNs) {{require the}} transmission of <b>visual</b> <b>content</b> on a bandwidth-limited network, from a peripheral node to a processing unit. Traditionally, a Compress-Then-Analyze approach has been pursued, in which sensing nodes acquire and encode the pixel-level representation of the <b>visual</b> <b>content,</b> that is subsequently transmitted to a sink node {{in order to be}} processed. This approach might not represent the most effective solution, since several analysis applications leverage a compact representation of the content, thus resulting in an inefficient usage of network resources. Furthermore, coding artifacts might significantly impact the accuracy of the visual task at hand. To tackle such limitations, an orthogonal approach named Analyze-Then-Compress has been proposed. According to such a paradigm, sensing nodes are responsible for the extraction of visual features, that are encoded and transmitted to a sink node for further processing. In spite of improved task efficiency, such paradigm implies the central processing node not being able to reconstruct a pixel-level representation of the <b>visual</b> <b>content.</b> In this paper we propose an effective compromise between the two paradigms, namely Hybrid-Analyze-Then-Compress (HATC) that aims at jointly encoding <b>visual</b> <b>content</b> and local image features. Furthermore, we show how a target tradeoff between image quality and task accuracy might be achieved by accurately allocating the bitrate to either <b>visual</b> <b>content</b> or local features. Comment: submitted to IEEE International Conference on Image Processin...|$|R
40|$|The <b>visual</b> <b>content</b> of out-of-body {{experiences}} (OBEs) {{has received}} little attention {{but a number}} of theories of OBEs include implicit predictions regarding the determinants of this phenomenological feature. Hypnagogic imagery and unusual sleep experiences, weak synaesthesia and preference for employing object and spatial visual imagic cognitive styles were psychometrically measured along with the incidence of self-reported OBEs and the absence or presence of <b>visual</b> <b>content</b> therein, in a sample of individuals drawn from the general population. Seventy percent of individuals who had experienced an OBE reported that the experience included some form of <b>visual</b> <b>content.</b> These individuals exhibited greater scores on the measures of preference for object visual imagic cognition and weak synaesthesia than those who reported an absence of <b>visual</b> <b>content</b> during their OBE. Subsequent analysis revealed that the measure of weak synaesthesia was the stronger discriminator of the two cohorts. The results are discussed {{within the context of the}} synaesthetic model of visual phenomenology during OBEs (Brugger, 2000; Irwin, 2000). This account proposes that <b>visual</b> <b>content</b> appears during these experiences through a process of cognitive dedifferentiation in which visual hallucinations are derived from available non-visual sensory cues and that such dedifferentiation is made possible through an underlying characteristic hyperconnectivity of cortical structures regulating vestibular and visual representations of the body and those responsible for the rotation of environmental objects. Predictions derived from this account and suggestions for future research are proffered...|$|R
30|$|Eventually, {{we include}} in the Appendix {{additional}} {{information that can be}} useful to perform a deeper analysis on the available <b>visual</b> <b>contents.</b>|$|R
5000|$|List of CBIR Engines - list of engines which {{search for}} images based image <b>visual</b> <b>content</b> such as color, texture, shape/object, etc.|$|R
40|$|The {{contrast}} {{statistics of}} natural {{images can be}} adequately characterized by a two-parameter Weibull distribution. Here we show how distinct regimes of this Weibull distribution lead to various classes of <b>visual</b> <b>content.</b> These regimes can be determined using model selection techniques from information theory. We experimentally explore the occurrence of the content classes, as related to the global statistics, local statistics, and to human attended regions. As such, we explicitly link local image statistics and <b>visual</b> <b>content...</b>|$|R
50|$|<b>Visual</b> <b>content</b> was {{provided}} by Tim Budgen of WarpTV. A live 45 minute mix of the visuals was included in their 2003 DVD.|$|R
5000|$|In August 2008 a DVD was {{released}} titled [...] "Alles voor jou" [...] with <b>visual</b> <b>content</b> of 40 {{songs from the}} period 1959-1989.|$|R
5000|$|Sandeep Varma has co - founded Filmboard Movies, {{which seeks}} to {{revolutionize}} the way audio - <b>visual</b> <b>content</b> is created in the industry.|$|R
5000|$|... a {{condition}} that requires increased spacing between aircraft, such as the Instrument Landing System (ILS) approaches <b>vs.</b> <b>visual</b> flight rules (VFR) approaches.|$|R
