73|433|Public
5000|$|Compositional models {{assume that}} idiom {{comprehension}} uses normal language processing. When an idiomatic expression is encountered, it is processed incrementally {{like a normal}} expression. Components of an idiomatic <b>word</b> <b>string</b> contribute to a figurative meaning in either a literal or metaphorical way. Compositional models include the configurational hypothesis and the conceptual metaphor hypothesis.|$|E
50|$|Conversely, {{because the}} FCG grammar is bi-directional, the {{production}} of an utterance from an initial set of meanings begins with the meanings as feature values of the sole Left Pole unit named 'top'. As successive grammar rules are applied, same-named units are added to both pole trees. At {{the end of a}} successful production, the <b>word</b> <b>string</b> is extracted from the 'form' features of Right Pole units.|$|E
5000|$|... "Complex {{cognitive}} processes {{account for the}} human ability to associate acoustic signals with meanings and intentions. For a computer, on the other hand, speech is essentially a series of digital values. However, despite these differences, the core problem of speech recognition {{is the same for}} both humans and machines: namely, of finding the best match between a given speech sound and its corresponding <b>word</b> <b>string.</b> Automatic speech recognition technology attempts to simulate and optimize this process computationally." ...|$|E
5000|$|Songs Without <b>Words,</b> <b>string</b> {{ensemble}} with piano (arr. by Louis Hintze), 1937 ...|$|R
3000|$|... are {{synchronized}} {{by using}} an alignment algorithm by minimization of the editing distance between the two <b>word</b> <b>strings</b> [...]...|$|R
40|$|Sentences {{are easier}} to {{memorize}} than ungrammatical <b>word</b> <b>strings,</b> a phenomenon known as the sentence superiority effect. Yet, {{it is unclear how}} higher-order linguistic information facilitates verbal working memory and how this is implemented in the neural system. The goal of the current fMRI study was to specify the brain mechanisms underlying the sentence superiority effect during encoding and during maintenance in working memory by manipulating syntactic structure and working memory load. The encoding of sentence material, as compared with the encoding of ungrammatical <b>word</b> <b>strings,</b> recruited not only inferior frontal (BA 47) and anterior temporal language-related areas but also the medial-temporal lobe, which is not classically reported for language tasks. During maintenance, it was sentence structure as contrasted to ungrammatical <b>word</b> <b>strings</b> that led to activation decrease in Broca's area, SMA, and parietal regions. Furthermore, in Broca's area, an interaction effect revealed a load effect for ungrammatical <b>word</b> <b>strings</b> but not for sentences. The sentence superiority effect, thus, is neurally reflected in a twofold pattern, consisting of increased activation in classical language as well as memory areas during the encoding phase and decreased maintenance-related activation. This pattern reflects how chunking, based on sentential syntactic and semantic information, alleviates rehearsal demands and thus leads to improved working memory performance...|$|R
50|$|Contrary to the {{previous}} model, the lexical representation hypothesis posits that idioms are stored along with normal words in memory. Idioms are processed both literally and figuratively simultaneously. However, the figurative meaning (lexical representation) of the idiom is accessed first since the <b>word</b> <b>string</b> is stored and processed as one entry. Literal processing consumes more time since it requires the understanding of each word individually. Research conducted to support this hypothesis found that people recognized grammatical idioms as meaningful expressions more quickly than non-idiomatic but grammatical phrases when they were presented out of context.|$|E
50|$|The {{prediction}} of the configurational hypothesis {{was not supported}} by research findings. Researchers found that even after a <b>word</b> <b>string</b> is recognized as an idiom, its literal meaning is still activated. Another criticism of the compositional models concerns the role of familiarity in idiom comprehension. As discovered by research on non-compositional models, idiomatic expressions are processed faster than non-idiomatic expressions. This is likely due to people’s familiarity of idioms. This suggests that when highly familiar idioms are encountered, people may not need to engage in literal processing or utilize conceptual metaphor to infer their meanings.|$|E
5000|$|A {{ciphertext}} of the ACE encryption scheme has the form,where {{the components}} are defined as: — integers from [...] (whose multiplicative order modulo [...] divides [...] ). — element [...] — element [...] {{we call the}} preamble, and [...] — the cryptogram. If a cleartext is a string consisting of [...] байт, then the length of [...] is equal to [...]We need to introduce the function , which maps a ciphertext to its byte-stringrepresentation, and the corresponding inverse function [...] For the integer , <b>word</b> <b>string</b> , integers , and byte string , [...]For integer , byte string , such that , [...]|$|E
40|$|Logologists {{have been}} aware of <b>word</b> <b>strings</b> [...] {{overlapping}} lists of words such as sat, ate, tea, ear, are [...] since at least the time of the great English puzzle-constructor Henry Ernest Dudeney. However, no one seems to have realized that <b>word</b> <b>strings</b> can be diagrammed in a network, much as has been done for word ladders (see, for example, the May and August 1973 issue of Word Ways, or Chapter 4 of my book 2 ̆ 2 Word Recreations, 2 ̆ 2 published by Dover in 1979) ...|$|R
40|$|This paper reports {{our work}} to improve a bigram {{language}} model for Japanese TV broadcast news speech recognition. First, frequent <b>word</b> <b>strings</b> were grouped into phrases {{in order that}} the phrases {{were added to the}} lexicon as new units of recognition. The test set perplexity was improved when frequent function <b>word</b> <b>strings</b> were used as additional recognition units. The speech recognition performance was improved both by grouping function <b>word</b> <b>strings</b> and by grouping compound nouns that were selected by word association ratio. Secondly, in order to alleviate the OOV problem related with nouns, we built and tested a language model that allows switching its noun lexicon according to the domain of the article to be recognized next. 1 INTRODUCTION The DARPA Hub- 4 launched in 1995 is evaluating LVCSR systems to transcribe audio recordings of broadcast news [1]. The evaluation results have been updated annually [2]. The Japanese counterpart of HUB- 4 broadcast news materials has been being deve [...] ...|$|R
50|$|Kalavati - as {{the name}} suggests, she speaks often in a fancy way, which only Sheetal understands. She confuses {{everyone}} with her arrows of sharp and complex Hindi <b>words</b> and <b>word</b> <b>strings.</b>|$|R
50|$|Suffix Tree Clustering, often {{abbreviated}} as STC is {{an approach}} for clustering that uses suffix trees. A suffix tree cluster {{keeps track of}} all n-grams of any given length to be inserted into a set <b>word</b> <b>string,</b> while simultaneously allowing differing strings to be inserted incrementally in a linear order. This {{has the advantage of}} ensuring {{that a large number of}} clusters can be handled sequentially. However, a potential disadvantage may be that it also increases the number of possible documents that need to be looked through when handling large sets of data. Suffix tree clusters can either be decompositional or agglomerative in nature, depending on the type of data being handled.|$|E
5000|$|Onomatopoeic {{effect can}} also be {{produced}} in a phrase or <b>word</b> <b>string</b> {{with the help of}} alliteration and consonance alone, without using any onomatopoeic words. The most famous example is the phrase [...] "furrow followed free" [...] in Samuel Taylor Coleridge's The Rime of the Ancient Mariner. The words [...] "followed" [...] and [...] "free" [...] are not onomatopoeic in themselves, but in conjunction with [...] "furrow" [...] they reproduce the sound of ripples following {{in the wake of a}} speeding ship. Similarly, alliteration has been used in the line [...] "as the surf surged up the sun swept shore...", to recreate the sound of breaking waves, in the poem [...] "I, She and the Sea".|$|E
50|$|The configurational {{hypothesis}} rose {{in response}} to the former hypotheses. It was based on a finding that demonstrated that people did not make an idiomatic interpretation once the first part of an idiom is encountered (against the prediction of the direct access hypothesis) or {{at the end of the}} idiom (against the prediction of the lexical representation hypothesis). Instead, the configurational hypothesis posits that a sufficient portion of an idiomatic <b>word</b> <b>string</b> must be processed literally before the idiom is identified. The point where most people recognize the string as an idiom is the idiom key. Once the key is encountered, the words in the rest of the string will not be processed literally. The idiom key is the most important part of the idiom. If it is replaced by other words, the idiomatic meaning cannot be activated.|$|E
5000|$|Conclusion: Therefore, {{human beings}} {{must have some}} form of innate {{linguistic}} capacity that provides additional knowledge to language learners. Essentially, stimulus is not an entirely adequate way to explain the process of learning. The poverty of stimulus argument attempts to explain how native speakers form a capacity to identify possible and impossible interpretations through ordinary experience. Thus, [...] "language acquisition is not merely a matter of acquiring a capacity to associate <b>word</b> <b>strings</b> with interpretations. Much less is it a mere process of acquiring a (weak generative) capacity to produce just the valid <b>word</b> <b>strings</b> of the language." ...|$|R
5000|$|In other <b>words,</b> each <b>string</b> in [...] is the prefix of {{a string}} [...] in , with the {{remainder}} of the <b>word</b> being a <b>string</b> in [...]|$|R
40|$|In this paper, {{we propose}} a new method for {{effective}} error analysis of machine translation (MT) systems. In previous work on error analysis of MT, error trends are often shown by frequency. However, if {{we attempt to}} perform a more detailed analysis based on frequently erroneous <b>word</b> <b>strings,</b> the <b>word</b> <b>strings</b> also often occur in correct translations, and analyzing these correct sen-tences decreases the overall efficiency of error analysis. In this paper, we propose the use of regularized discriminative language models (LMs) to allow for more focused MT error analysis. In experiments, we demonstrate that our method is more efficient than frequency-based analysis, and examine differences across systems, language pairs, and evaluation measures. 1...|$|R
50|$|The dual idiom {{representation}} model {{suggests that}} idiomatic expressions are simultaneously “long words” and compositional phrases. Thus, idiom comprehension involves both direct memory retrieval and normal language processing. Idioms behave noncompositionally {{to the degree}} they are familiar. When highly familiar idioms encountered, their idiomatic meanings can be directly activated. In addition, idioms can behave compositionally if they are decomposable. Decomposability of an idiom refers {{to the extent to}} which the literal meaning of the individual words in the <b>word</b> <b>string</b> participate in the overall figurative meaning of the idiom. As suggested earlier, the idiom play with fire is decomposable while kick the bucket is nondecomposable. Comprehension is facilitated in a decomposable idiom since there are commonality between the results of its literal analysis and its idiomatic meaning. Research shows that familiarity had the larger affect on the speed of idiom comprehension than decomposability. Decomposability nonetheless facilitates processing the meaningfulness of the idiomatic expression.|$|E
5000|$|In 2011, a {{new version}} of the Idol intro was created once again by Aerodrome Pictures in Los Angeles which first {{appeared}} on the Season 10 premiere of American Idol. The intro starts with a stage being spotted under a spotlight from the sky. Then, a stylized bridge leading towards a screen is shown. Winners of Idol, as well as notable alumni from past seasons are shown on the screen. The camera turns to a pyramid-like shape blooming and a CGI figure appears. Depending on the figure's gender in the intro, the figure would yield a different action in front of the American Idol logo and the huge [...] "Idol" [...] <b>word</b> <b>string</b> (in [...] "American Idol" [...] and [...] "Indonesian Idol" [...] but not in [...] "Arab Idol"): the male figure raises his hand in victory and strikes a pose while the female figure raises her arms in victory before bowing down. Sometimes, a female figure will appear.|$|E
50|$|There {{are several}} criticisms for the non-compositional models. First, studies {{demonstrated}} that idiom expressions are not processed {{more slowly than}} literal expression. In fact, it is often the opposite, which is in contrary {{to the prediction of}} literal first hypothesis. Second, idioms {{have been found to be}} more than just “frozen phrases” or long words. For example, some idioms can be transformed to some extent and still be recognized and understood. For example, spill the beans can be used as “the beans were spilt by Mary”. This is possible because spill the bean can be mapped on the meaning “reveal the secret”, i.e. spill (reveal) and beans (secret). Such idiom shows that the internal structure of the <b>word</b> <b>string</b> matters during comprehension. The meaning of some idioms, like play with fire, can be also inferred from literal interpretation of their components (i.e. to do something dangerous). These findings reveal that idioms are not a homogeneous, distinct group and thus may not involve different processing strategies from those for literal expressions.|$|E
40|$|In {{this paper}} I present ongoing {{work on the}} data-oriented parsing (DOP) model. In {{previous}} work, DOP was tested on a cleaned-up set of analyzed part-of-speech strings from the Penn Treebank, achieving excellent test results. This left, however, two important questions unanswered: (1) how does DOP perform if tested on unedited data, and (2) how can DOP be used for parsing <b>word</b> <b>strings</b> that contain unknown words? This paper addresses these questions. We show that parse results on unedited data are worse than on cleaned-up data, although still very competitive if compared to other models. As to the parsing of <b>word</b> <b>strings,</b> we show that the hardness of the problem does not so much depend on unknown words, but on previously unseen lexical categories of known words. We give a novel method for parsing these words by estimating the probabilities of unknown subtrees. The method is of general interest since it shows that good performance can be obtained {{without the use of}} a part-of- speech tagger. To the best of our knowledge, our method outperforms other statistical parsers tested on Penn Treebank <b>word</b> <b>strings...</b>|$|R
5000|$|Píseň beze slov (Song without <b>Words)</b> for <b>string</b> quartet (1942) ...|$|R
30|$|The actual <b>word</b> <b>strings</b> in {{the corpus}} {{are taken to}} be our Reference set, against which we intend to compare (therefore intrinsically) the output {{produced}} by 10 alternative generation strategies: the four FLMs (2 WL, 3 WL, 2 WLP and 3 WLP) discussed in the previous section, and six baseline systems as follows.|$|R
5000|$|Many {{languages}} {{borrow the}} English <b>word</b> <b>string</b> {{to refer to}} this type of underwear, usually without the G. Another common name is tanga (or sometimes string tanga), especially in the German Tanga. A frequent metaphor, especially in Brazil, is dental floss; in Brazil a thong is called fio dental (Portuguese for dental floss); in English, the term [...] "Butt floss" [...] is sometimes used. In Lithuanian it is [...] "siaurikės" [...] ("narrows"), Italian [...] "perizoma" [...] or [...] "tanga", in Turkish [...] "ipli külot" [...] ("stringed underpants"), and in Bulgarian as [...] "prashka" [...] (...) , which means a slingshot. In Israel the thong, mostly the G-string, is called Khutini (...) , from the word Khut, which means String. Similarly, in Iran, it is called [...] "Shortbandi" [...] (...) in which [...] "short" [...] (from English: Shorts) means [...] "briefs" [...] and [...] "bandi" [...] means [...] "with a string". A Puerto Rican Spanish slang term, used by Reggaeton artists, is gistro. Australians often colloquially refer to the G-string as a g-banger or simply banger.|$|E
5000|$|Today, St. Louis Children's Hospital in St. Louis, Missouri has a [...] "Center for Cerebral Palsy Spasticity" [...] that is {{the only}} internationally known clinic in the world to have {{conducted}} concentrated first-hand clinical research on SDR over an extended period. Its chief neurosurgeon in the field, Doctor T.S. Park (who was initially trained by Dr. Peacock), has performed thousands of SDR surgeries, some of them on adults, and is the originator of the L1-laminectomy modification to the SDR surgery in 1991, which sections the first dorsal root and enables the removal of significantly less spine-bone than in surgeries performed before 1991, as well as inherent release of the hip flexor muscles specifically {{as a result of that}} particular sectioning — prior to that, total hip flexor release was not necessarily possible. That L1-laminectomy modification has since become the standard method, and SLCH has become internationally known as a major provider of the SDR surgery to those in need of it; for example, it is one of the first Google search results when inputting the <b>word</b> <b>string</b> [...] "selective dorsal rhizotomy". It is this clinic's opinion that patients with spastic diplegia or quadriplegia should have spasticity reduced first through SDR before undergoing muscle release or tendon release procedures, and other surgeons today share this view. A major qualifier in the cases taken on at SLCH, however, is that all of its adults have had only mild cases of spastic diplegia.|$|E
40|$|Two word strings {{are aligned}} when zero or more words in one are paired with zero or more {{words in the}} other. More precisely: A word {{alignment}} is a pairing of a word in one <b>word</b> <b>string</b> with its counterpart in another <b>word</b> <b>string.</b> For two word strings S 1 and S 2, a sentence alignment of S 1 to S 2 {{is a set of}} word alignments where the first word is in S...|$|E
5000|$|The word {{problem for}} semi-Thue {{systems can be}} stated as follows: Given a semi-Thue system [...] and two <b>words</b> (<b>strings)</b> , can [...] be {{transformed}} into [...] by applying rules from ? This problem is undecidable, i.e. there is no general algorithm for solving this problem. This even holds if we limit the input to finite systems.|$|R
30|$|When formulaic {{sequences}} {{are well}} entrenched in memory, co-text is imbued with predictability (e.g., Last but not …). This may also work in reverse {{in a way}} that enables something partly heard to be reconstructed post facto (e.g., Did Michael Jackson really […] suicide?). All this eases processing so that attention can be allocated to parts of the discourse that are less formulaic and therefore less predictable. In fact, only when a sequence is deeply entrenched in a language user’s long-term memory, it qualifies as truly formulaic for that user. <b>Word</b> <b>strings</b> that bring the aforesaid processing advantages to a native speaker may not (yet) do so for language learners. Although high proficient language learners recognize and use many standardized <b>word</b> <b>strings</b> that are formulaic for native speakers, they may not process these strings the same way as native speakers do (Boers & Lindstromberg, 2012).|$|R
40|$|This paper {{stresses}} the importance of converting a <b>string</b> of lexical <b>words</b> to that of prosodic words in TTS systems by presenting the surface differences and perceptual differences between them. A statistical rule based method and a CART based method are proposed as solutions. Though ComplicatedSet based CART method performs the best, the achievement is obtained at the cost of heavy computation workloads needed by a parser. Statistical rule based method results higher recall but lower precision, comparing to SimpleSet CART method. It is very difficult to tell which is better, since we don't know which affects naturalness more, precision or recall. Both of them require only lexicon word segmentation and POS tagging in the preprocessing stage, and are easily realized in TTS systems. Results of the preference test discloses that significant improvements on naturalness are perceived when lexical <b>word</b> <b>strings</b> are converted into prosodic <b>word</b> <b>strings</b> by our approach...|$|R
40|$|This paper {{describes}} a tabular method {{of finding the}} <b>word</b> <b>string,</b> together with its dependency structure, that has the maximum preference score in a given series of word sets. The technique, which originated in the problem of constraining the process of Japanese speech recognition by linguistic knowledge, is potentially applicable to parsing and disambiguation of various natural languages. The preference score of a <b>word</b> <b>string</b> is defined by taking syntax, semantics, and reliability of each word into consideration. A set of recurrence equations is derived {{based on the principle}} of Dynamic Programming, which leads to an algorithm to solve the combinatorial optimization problem. In the algorithm, searching for the optimal <b>word</b> <b>string</b> proceeds concurrently with parsing of the string, looking up and filling in two triangular tables. 1 INTRODUCTION Let us consider, for example, a speech recognizer that accepts spoken sentences uttered with clear pauses between consecutive words. It outputs [...] ...|$|E
40|$|In this paper, {{we address}} the problem of {{combining}} several lan-guage models (LMs). We find that simple interpolation methods, like log-linear and linear interpolation, improve the performance but fall short of the performance of an oracle. The oracle knows the reference <b>word</b> <b>string</b> and selects the <b>word</b> <b>string</b> with the best per-formance (typically, word or semantic error rate) from a list of word strings, where each <b>word</b> <b>string</b> has been obtained by using a dif-ferent LM. Actually, the oracle acts like a dynamic combiner with hard decisions using the reference. We provide experimental results that clearly show the need for a dynamic language model combina-tion to improve the performance further. We suggest a method that mimics the behavior of the oracle using a neural network or a de-cision tree. The method amounts to tagging LMs with confidence measures and picking the best hypothesis corresponding to the LM with the best confidence. 1...|$|E
30|$|We {{performed}} speaker-independent phone recognition on {{the task}} of TIMIT [27] and speaker-dependent large vocabulary speech recognition {{on the task}} of telehealth captioning [28]. The experimental outcomes were measured in phone accuracy and word accuracy, respectively, for TIMIT and telehealth through aligning each phone or <b>word</b> <b>string</b> hypothesis against its reference string by using the Levenshtein distance [31].|$|E
40|$|The {{complement}} to decomposition in scientific research is composition. In human language computing, composition {{is achieved by}} way of semantic association and the generation of strings of entities. That generation of strings takes place progressively: e. g., <b>strings</b> of symbols (<b>words),</b> <b>strings</b> of strings (sentences), strings of strings of strings (paragraphs), etc. The mathematical (topological, graph-theoretic) analysis of Roget 2 ̆ 7 s Thesaurus (1962) has opened a door onto a broad vista of potential achievements {{in such areas as}} artificial intelligence and expert systems, through the analysis of concept association, or concept composition...|$|R
30|$|In {{the texts}} of the {{training}} dataset, the Treebank speech corpus, no <b>word</b> <b>strings</b> were quoted in Chinese brackets. Thus, we could not directly analyze the relationship between Chinese brackets and labeled break types. In this study, we directly analyzed {{the characteristics of the}} brackets and their associated quoted phrases from the ASBC text corpus, as presented in Section 2.4.|$|R
40|$|AbstractPrimitive <b>words,</b> or <b>strings</b> over {{a finite}} {{alphabet}} that cannot be written {{as a power}} of another string, {{play an important role}} in formal language theory, coding theory, and combinatorics on words to name a few. In this paper, we extend some fundamental results about primitive words to primitive partial words. Partial <b>words</b> are <b>strings</b> that may have a number of “do not know” symbols...|$|R
