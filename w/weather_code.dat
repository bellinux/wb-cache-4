13|51|Public
2500|$|The Japanese {{submarine}} I-23 {{was supposed}} to station itself just south of Oahu as a [...] "lifeguard" [...] and weather spotter for the flying boats, but was lost sometime after 14 February. Japanese cryptanalysts had broken the United States Navy <b>weather</b> <b>code,</b> but a code change on 1 March eliminated that alternative source of weather information over Pearl Harbor. [...] The mission proceeded on the assumption of clear skies over Pearl Harbor from knowledge of conditions at French Frigate Shoals.|$|E
2500|$|In the US <b>weather</b> <b>code</b> remarks, three digits are {{all that}} are transmitted; decimal points and {{the one or two}} most {{significant}} digits are omitted: [...] is transmitted as 132; [...] is transmitted as 000; 998.7 mbar is transmitted as 987; etc. The highest sea-level pressure on Earth occurs in Siberia, where the Siberian High often attains a sea-level pressure above , with record highs close to [...] The lowest measurable sea-level pressure is found at the centers of tropical cyclones and tornadoes, with a record low of [...] (see Atmospheric pressure records).|$|E
50|$|The Navy's {{weather and}} ocean {{programs}} contributed greatly to Allied victory in World War II. In the Pacific, Navy forecasters cracked the Japanese <b>weather</b> <b>code.</b> Hydrographic survey ships, often under enemy fire, collected data along foreign coastlines {{for the creation}} of critical navigation charts.|$|E
50|$|The short <b>weather</b> cipher <b>coded</b> <b>weather</b> reports using a {{polyphonic}} single-letter code with X missing.|$|R
5000|$|A similar {{coding system}} {{was used for}} weather reports from U-boats, the Wetterkurzschlüssel (Short <b>Weather</b> Cipher). <b>Code</b> books were {{captured}} from [...] on 30 October 1942.|$|R
5000|$|Beaver Island is {{an island}} {{community}} of the Halifax Regional Municipality in the Canadian province of Nova Scotia. The <b>weather</b> station <b>code</b> is CWBV. Since 1846 {{there has been a}} lighthouse on the island.|$|R
50|$|Lloyd Shapley {{was born}} on June 2, 1923, in Cambridge, Massachusetts, one of the sons of Martha (Betz) and the {{distinguished}} astronomer Harlow Shapley, both from Missouri. He attended Phillips Exeter Academy and {{was a student at}} Harvard when he was drafted in 1943. He served in the United States Army Air Corps in Chengdu, China and received the Bronze Star decoration for breaking the Soviet <b>weather</b> <b>code.</b>|$|E
5000|$|The Japanese {{submarine}} I-23 {{was supposed}} to station itself just south of Oahu as a [...] "lifeguard" [...] and weather spotter for the flying boats, but was lost sometime after 14 February. Japanese cryptanalysts had broken the United States Navy <b>weather</b> <b>code,</b> but a code change on 1 March eliminated that alternative source of weather information over Pearl Harbor. The mission proceeded on the assumption of clear skies over Pearl Harbor from knowledge of conditions at French Frigate Shoals.|$|E
50|$|Voegele {{was known}} to have worked {{on a number of}} cyphers {{during the course of the}} war. These {{included}} the Government Telegraph Code, India Code, Syko, the RAF 4-digit code, Aircraft Movement Code in Autumn 1942, Bomber Code in Winter 1942, January 1943, Slidex in May 1943, Aircraft Reporting Code from July 1943, UCO <b>Weather</b> <b>Code</b> from May 1944 and Weather Codes throughout the war. Typex was attempted by Voegele in the early 1940s. Little was known about Ferdinand Voegele after the end of the war.|$|E
5|$|Beaufort {{developed}} the Wind Force Scale and <b>Weather</b> Notation <b>coding,</b> {{which he was}} to use in his journals {{for the remainder of}} his life. He also promoted the development of reliable tide tables around British shores, and with his friend William Whewell, expanded weather record-keeping at 200 British Coast guard stations.|$|R
40|$|Over {{the last}} two decades, {{processor}} speeds have been improving much faster than memory speeds. As a result, memory access delay is a major performance bottleneck in today’s systems. Because compilers often fail to automatically choreograph data and computation to avoid memory access delay, we have developed a source-to-source transformation tool for this purpose. To use our tool, developers annotate their code with directives that specify how our tool should apply loop transformations to improve performance. In this paper, we describe a set of storage reduction optimizations that are automatically applied by our tool. These optimizations improve code performance by reducing the memory hierarchy footprint of temporary arrays. Our experiments with a numerical kernel and two <b>weather</b> <b>codes</b> show that our storage reduction optimizations amplify the benefits of loop transformations and doubles performance achievable with loop transformations alone. ...|$|R
40|$|Over {{the last}} two decades, {{processor}} speeds have improved much faster than memory speeds. As a result, memory access delay is a major performance bottleneck in today’s systems. Compilers often fail to choreograph data and computation automatically to avoid memory access delay; we have developed an annotation-driven source-to-source transformation tool for this purpose. This tool uses a set of compiler transformations that improve temporal reuse in scientific applications (1) by reducing the size of tempo-rary arrays and (2) by overlaying storage for multiple tem-porary arrays that are not live at the same time. We also describe two supporting transformations, statement motion and loop alignment, that improve the effectiveness of stor-age reduction. Our experiments with a numerical kernel and two <b>weather</b> <b>codes</b> show that our storage reduction optimizations amplify the benefits of loop transformations and double performance achievable with loop transforma-tions alone...|$|R
5000|$|In the US <b>weather</b> <b>code</b> remarks, three digits are {{all that}} are transmitted; decimal points and {{the one or two}} most {{significant}} digits are omitted: [...] is transmitted as 132; [...] is transmitted as 000; 998.7 mbar is transmitted as 987; etc. The highest sea-level pressure on Earth occurs in Siberia, where the Siberian High often attains a sea-level pressure above , with record highs close to [...] The lowest measurable sea-level pressure is found at the centers of tropical cyclones and tornadoes, with a record low of [...] (see Atmospheric pressure records).|$|E
40|$|Quantitative SYNOP Code weather {{variables}} such as rainfall amount, although of high societal and environmental importance, are frequently subject to recording errors and inhomogeneities resulting in uncertain conclusions. Here we assess {{the viability of the}} more qualitative Past <b>Weather</b> <b>Code</b> (PWC) for its use in robust climate analysis in the belief that it is less prone to both random and systematic errors. The Past <b>Weather</b> <b>Code</b> data, from a selection of the National Oceanographic and Atmospheric Administration’s Integrated Surface Database (ISD) (4731 sufficiently long stations), is quality assessed by searching for inhomogeneities in station PWC time series, removing the offending stations and averaging the remaining stations into a global gridded dataset. PWCs 6 (Rainfall), 7 (Snowfall) and 9 (Thunderstorms) are found to robustly exhibit seasonal features, e. g. the Indian monsoon and peak Northern Hemispheric winter snowfall. Precipitation responses to the North Atlantic Oscillation are also detected in winter PWC 6 data over Europe...|$|E
40|$|Surface weather {{observations}} of clouds were analyzed {{to obtain a}} global cloud climatology (Warren et al, 1986; 1988). The form of the synoptic <b>weather</b> <b>code</b> limits the types of cloud information which are available from these reports. Comparison of surface weather reports with instrumental observations during the FIRE field experiments can help to clarify the operational definitions which {{were made in the}} climatology {{because of the nature of}} the synoptic code. The long-term climatology from surface weather observations is also useful background for planning the location and timing of intensive field experiments...|$|E
30|$|In this study, a web data {{crawling}} {{strategy was}} conducted with a python script based on the PyCharm software. Web-crawling method can retrieve data faster and in greater depth. Many historical weather data pages hide in the deep or invisible web. These pages are typically only accessible by submitting dynamic queries of certain regions and a certain time to a background database. The data were extracted with regular expressions by parsing the structured HTML pages. The extracted data columns include time, region, temperature and weather. As the obtained weather data record the text information such as sunny, cloudy, rainy and snowy with different degrees, effect coding was applied to allow for nonlinear effects in the levels of attributes. For instance, sunny <b>weather</b> was <b>coded</b> as 0 and cloudy <b>weather</b> was <b>coded</b> as 1. Moreover, the weather data extracted by the web crawlers were stored into the Mysql database as a separate dataset for the following data emerging procedure.|$|R
40|$|The {{prediction}} of the plasma environment in time, the plasma weather, is discussed. It {{is important to}} be able to predict when large magnetic storms will produce auroras, which will affect the space station operating in low orbit, and what precautions to take both for personnel and sensitive control (computer) equipment onboard. It is also important to start to establish a set of plasma weather records and a record of the ability to predict this weather. A successful forecasting system requires a set of satellite weather stations to provide data from which predictions can be made and a set of plasma <b>weather</b> <b>codes</b> capable of accurately forecasting the status of the Earth's magnetosphere. A numerical magnetohydrodynamic fluid model which is used to model the flow in the magnetosphere, the currents flowing {{into and out of the}} auroral regions, the magnetopause, the bow shock location and the magnetotail of the Earth is discussed...|$|R
40|$|ABSTRACT The aim of {{this study}} is to {{introduce}} and apply a statistical method called Multiple Logistic Regression (MLR) for analyzing the differences in the information output of automated weather stations versus a professional observer. Difficult weather periods have been chosen as a base for the analysis, e. g., periods when the temperature is close to zero, the precipitation intensity is very low, or periods with mixed precipitation. This kind of weather events are recognized as an important challenge for the analysis of difficulties and differences in the information output of automated weather stations. The Present <b>Weather</b> <b>codes</b> are variables of nominal type. The observation error definitions based on differences in the <b>weather</b> <b>codes</b> have been created. The use of Multiple Logistic regression model has been studied when the response variable is an observation error term with two possible outcomes, the determination is true or false. Further, in the last chapter a period with mixed precipitation and with more than two error categories has been studied. All the official weather variables produced by the Finnish Meteorological Institute and also the data from the test sensors were considered as predictor candidates. The difficulties encountered in the automated Present Weather determination have in this study been explained from the point of view of the meteorological and physical sciences. For example, very low intensity of drizzle or solid precipitation combined with a certain wind direction was found to be the explanation for some ‘determination errors’. The dew point temperature was found to be an important predictive variable generally. The method of MLR was proved to be appropriate and accurate for this study and usable for corresponding analysis in the future. Endast sammandrag. Inbundna avhandlingar kan sökas i Helka-databasen ([URL] Elektroniska kopior av avhandlingar finns antingen öppet på nätet eller endast tillgängliga i bibliotekets avhandlingsterminaler. Only abstract. Paper copies of master’s theses are listed in the Helka database ([URL] Electronic copies of master’s theses are either available as open access or only on thesis terminals in the Helsinki University Library. Vain tiivistelmä. Sidottujen gradujen saatavuuden voit tarkistaa Helka-tietokannasta ([URL] Digitaaliset gradut voivat olla luettavissa avoimesti verkossa tai rajoitetusti kirjaston opinnäytekioskeilla...|$|R
40|$|A new {{simulation}} chain {{for early}} prediction of rainfall-induced landslides in unsaturated soils is presented. It includes a special computational <b>weather</b> <b>code</b> for forecasting {{the evolution of}} the synoptic weather and its changes due to interaction with the Earth’s surface (rainfall pattern), and a hydro-mechanical code to analyse rainfall effects on slope stability by computing degree of saturation and pore pressure changes due to rainwater infiltration. The linkage between these two numerical codes is ensured by an interface with the aim of bringing the data provided by the first code, which operates at basin or slope scale. The simulation chain can work in computational times that may be considered suitable for civil protection operations...|$|E
40|$|International audienceWith the {{computing}} power it brings, {{the new generation}} of massively parallel computers allows one to perform a "seamless" modeling of weather on large grids, but it requires a large parallel computing capability for the models. Due to improvements in this direction in the meteorological model of the French research community MESO-NH, a computing performance of over 4 Teraflop/s was obtained with 130000 cores on the machine of the European computer center PRACE. This is a first in France for a <b>weather</b> <b>code.</b> A high resolution simulation, covering the Atlantic and Europe, of the tropical cyclone Helene and its interaction with a planetary wave was also conducted, for which the excellent result gives hope to an improved weather forecasting in Europe...|$|E
40|$|Received on **** * / {{accepted}} on ***** This paper proposes {{and describes}} a methodology developed to port complex scientific applications originally written in FORTRAN to nVidia CUDA. The {{significance of this}} {{lies in the fact}} that, despite the performance improvement and programmer-friendliness provided by CUDA, it presently lacks support for FORTRAN. The methodology described in this paper addresses this problem using a multiple step process that includes identification of software modules that benefit from being ported, familiarization with the code, porting, optimizing, and verifying the ported code. It was developed and carried out by porting an existing module of a weather forecasting application written in FORTRAN. Using this approach, we obtained a functional prototype of the ported module in approximately 3 months, despite our lack of knowledge of the theory of the <b>weather</b> <b>code.</b> Considering the relevance of this application to other scientific applications also written in FORTRAN, we believe that the proposed porting methodology described can be successfully utilized in several other existing scientific applications...|$|E
40|$|This report {{describes}} the parallelization of a <b>weather</b> prediction <b>code</b> using the Technical Report: SCCS 533 CHAPTER 1. INTRODUCTION 2 dataparallel programming scheme. In addition, it is shown how {{to obtain a}} version for message passing computers while using High Performance Fortran. Benchmarking for the dataparallel program is done on a CM 5 with 32 nodes and vector units. 1. 1 The Advanced Regional Prediction Syste...|$|R
50|$|Stage 1: Taking {{advantage}} of public information: recognise and adopt well-structured external schemes of reference data, such as post <b>codes,</b> <b>weather</b> data, GPS positioning data and travel timetables, exemplified {{in the personal}} computing press.|$|R
40|$|This {{document}} is a technical {{account of the}} history of Flosolver development at NAL - starting with the early development using Intel's 8086 / 8087 and ending with a narrative on the successful parallelization of the T- 80 <b>weather</b> prediction <b>code</b> on Flosolver Mk 3 based on the i 860 processor. The document also discusses system software, future plans and the financial details related to India's first parallel computer. 13...|$|R
40|$|Surface {{synoptic}} weather {{reports for}} the entire globe for the 10 -year period from December 1981 through November 1991 have been processed, edited, and rewritten to provide a data set designed for use in cloud analyses. The information in these reports relating to clouds, including the present weather information, was extracted and put {{through a series of}} quality control checks. Correctable inconsistencies within reports were edited for consistency, so that the ``edited cloud report`` can be used for cloud analysis. Cases of ``sky obscured`` were interpreted by reference to the present <b>weather</b> <b>code</b> as to whether they indicated fog, rain or snow and were given appropriate cloud type designations. Nimbostratus clouds were also given a special designation. Changes made to an original report are indicated in the edited report so that the original report can be reconstructed if desired. While low cloud amount is normally given directly in the synoptic report, the edited cloud report also includes the amounts, either directly reported or inferred, of middle and high clouds, both the non-overlapped amounts and the ``actual`` amounts. Since illumination from the moon is important for the adequate detection of clouds at night, both the relative lunar illuminance and the solar altitude are given; well as a parameter that indicates whether our recommended illuminance criterion was satisfied. This data set contains 124 million reports from land stations and 15 million reports from ships. Each report is 56 characters in length. The archive consists of 240 files, one file for each month of data for land and ocean separately. With this data set a user can develop a climatology for any particular cloud type or group of types, for any geographical region and any spatial and temporal resolution desired...|$|E
50|$|The {{first attempt}} to create a weather beacon as a form of {{advertising}} was from Douglas Leigh, who, in 1941, arranged a lighting scheme for the Empire State Building to display a <b>weather</b> forecast <b>code</b> with a decoder to be packaged with Coca-Cola bottles. The plan was never implemented because of the attack on Pearl Harbor later that year. Leigh resurrected his idea in Minneapolis in October 1949 with the Northwestern National Bank Weatherball.|$|R
5000|$|During {{these early}} years of command, Beaufort {{developed}} the first versions of his Wind Force Scale and <b>Weather</b> Notation <b>coding,</b> which he was to use in his journals {{for the remainder of}} his life. From the circle representing a weather station, a staff (rather like the stem of a note in musical notation) extends, with one or more half or whole barbs. For example, a stave with 3½ barbs represents Beaufort seven on the scale, decoded as 32-38 mph, or a [...] "moderate Gale".|$|R
50|$|The {{number of}} event {{types in the}} {{national}} system has grown to eighty. At first, all but three of the events (civil emergency message, immediate evacuation, and emergency action notification emergency) were weather-related (such as a tornado warning). Since then, several classes of non-weather emergencies have been added, including, in most states, the AMBER Alert System for child abduction emergencies. In 2016, three additional <b>weather</b> alert <b>codes</b> were authorized for use in relation to hurricane events, including Extreme Wind Warning (EWW), Coastal Flood Warning (CFW) and Coastal Flood Advisory (CFA).|$|R
40|$|Investigating the {{potential}} impact of climate on agro-ecosystems using simulation models is underpinned by the availability of climate data at the appropriate temporal scale (daily or higher resolution). The production of artificial series of weather data has traditionally adopted a variety of alternative methods. These range from empirical functions where simple relationships between weather variables are used to estimate missing data from available data, to sophisticated approaches where physically-based models are used. All such approaches illustrate from different perspectives that there is actually a wealth of well developed solutions to the basic problem of estimating or generating <b>weather</b> data, <b>coded</b> in...|$|R
40|$|Long-term (1984 – 2012) surface {{observations}} from 70 {{stations in the}} Sahara and Sahel are used to explore the diurnal, seasonal and geographical variations in dust emission events and thresholds. The frequency of dust emission (FDE) is calculated using the present <b>weather</b> <b>codes</b> of SYNOP reports. Thresholds are estimated as the wind speed {{for which there is}} a 50 % probability of dust emission and are then used to calculate strong wind frequency (SWF) and dust uplift potential (DUP), where the latter is an estimate of the dust-generating power of winds. Stations are grouped into six coherent geographical areas for more in-depth analysis. FDE is highest at stations in Sudan and overall peaks in spring north of 23 ° N. South of this, where stations are directly influenced by the summer monsoon, the annual cycle in FDE is more variable. Thresholds are highest in northern Algeria, lowest in the latitude band 16 – 21 ° N and have greatest seasonal variations in the Sahel. Spatial variability in thresholds partly explain spatial variability in frequency of dust emission events on an annual basis. However, seasonal variations in thresholds for the six grouped areas are not the main control on seasonal variations in FDE. This is demonstrated by highly correlated seasonal cycles of FDE and SWF which are not significantly changed by using a fixed, or seasonally varying, threshold. The likely meteorological mechanisms generating these patterns such as low-level jets and haboobs are discussed...|$|R
40|$|New grid {{adaptation}} {{approaches have}} been developed for the dynamic solution adapta-tion grid algorithm (DSAGA) to improve grid refinement for optical turbulence prediction. The grid adaptation algorithm and a four-equation hybrid LES/RANS turbulence model developed earlier have been incorporated into the numerical <b>weather</b> prediction <b>code</b> MM 5 (adaptive version) to provide a new approach for the calculation of C 2 n, a quantitative mea-sure of atmospheric optical turbulence. By comparing with observation, the numerical results suggest that combination of the new grid adaptation scheme and the turbulence model is capable of improving C 2 n prediction with dynamic grid refinement in the strato-sphere with little increase in computational effort...|$|R
40|$|In a {{previous}} report R. E. Newton proposed two {{improvements to the}} Staniforth-Mitchell parotropic numerical <b>weather</b> prediction <b>code.</b> Reported here are results of performince comparisons between the original code and an amended version which incorporates the two improvements: a new solution routine for the eigenproblems and a direct solution of the Helmholtz equation. It is found that the proposed improvements do reduce the computation time, but that the new eigenproblem algorithm results in excessive round-off error when single-precision arithmetic is used. This report was prepared in conjunction with research conducted for the Naval Environmental Prediction Research Facility and funded by the Naval Postgraduate School. [URL] Direct Fundin...|$|R
40|$|In {{this paper}} {{we present a}} {{methodology}} followed to parallelize a <b>weather</b> forecasting <b>code</b> running operationally on a small Network of Workstations with real data sets. The 1 D Eta/NMC model code was originally written in Fortran 77 and optimized for vector supercomputers. We describe the steps followed to parallelize the Eta/NMC-dust code. The examination of program workload gave important insights into bottlenecks of the Eta/NMC-dust code. The implementation of the model with MPI libraries on a MIMD platform, the data partitioning used and some implementation characteristics of the model are presented in the paper. We present {{the performance of the}} parallel implementation of the model in terms of the speedup gained on a number of processors. ...|$|R
40|$|Improvements to the Staniforth-Mitchell {{barotropic}} numerical <b>weather</b> prediction <b>code</b> are proposed. Three separate {{contributions are}} described: (1) a direct {{solution of the}} Helmholtz equation is offered {{as a substitute for}} the iterative scheme currently employed; (2) a new algorithm is developed to solve the generalized eigenvalue problem for symmetric tridiagonal matrices; (3) the latter algorithm is extended to deal with a periodic boundary condition. It is estimated that a direct solution of the Helmholtz equation, for a 13 x 13 grid, will effect a computation time saving per time step of at least 10 percent. FORTRAN 77 listings of the subroutines needed to implement the proposed improvements are included. supported by the Naval Environmental Prediction Research Facility[URL]...|$|R
40|$|Namias has {{suggested}} cross-sections of potential temperature, {{which can be}} drawn very quickly. Since unsaturated air maintains a constant potential temperature during vertical motions, places in the cross-section where the potential temperature lines dip downward would probably represent subsidence (or a transition from a colder to a warmer air mass), and those where the potential temperature lines slope upw-ard would represent zones of rising air (or a transition from a warm to a colder air mass). To return {{to the question of}} time, it has been the experience in TWA 4 that to construct the adiabatic chart and the Rossby diagram requires about 10 minutes with the data received in the present <b>Weather</b> Bureau <b>code.</b> W. H. Clover has developed a code which we use i...|$|R
40|$|This {{section is}} based on [11], with some new developments. Our current code {{implements}} the mathematical ideas from [3] in a simplified contemporary numerical framework in that the fire propagation, originally done by a custom ad hoc tracer code, is now done using a simpler level set method, and the fire model is now coupled with the Weather Research and Forecasting model (WRF, www. wrfmodel. org), a community supported numerical <b>weather</b> prediction <b>code.</b> This adds capabilities to the codes used in previous work such as theoretical experiments of [3] and [4], where it was used in reanalysis of a real fire. Specifically, it allows {{taking advantage of the}} WRF architecture, which provides a natural parallelization of both the fire and the atarXiv: 0801. 3875 v...|$|R
