11|16|Public
40|$|This paper {{reports the}} results on methods of {{comparing}} the memory retrieval capacity of the Hebbian neural network which implements the B-Matrix approach, by using the <b>Widrow-Hoff</b> <b>rule</b> of learning. We then, extend the recently proposed Active Sites model by developing a delta rule to increase memory capacity. Also, this paper extends the binary neural network to a multi-level (non-binary) neural network. Comment: 12 Pages, 7 figure...|$|E
30|$|Brains are neural systems, {{which allow}} quick {{adaption}} to changing situations during lifetime of an organism. Neural networks are complex systems (Mainzer 2008 a) of threshold elements with firing and nonfiring states, according to learning strategies (e.g., Hebbian learning). Beside deterministic homogeneous Hopfield networks, there are so-called Boltzmann machines with stochastic network architecture of nondeterministic processor elements and a distributed knowledge representation, which is described mathematically by an energy function. While Hopfield systems use a Hebbian learning strategy, Boltzmann machines favor a backpropagation strategy (<b>Widrow-Hoff</b> <b>rule)</b> with hidden neurons in a many-layered network.|$|E
40|$|International audienceDescribes a {{completely}} connected feedback network with 64 binary neurons, using digital CMOS technology. The architecture implements a linear systolic loop, {{in which each}} neuron stores locally its own synaptic coefficients, and the potential calculation needs N time steps, each performing N partial weighted sums, to realize the N^ 2 operations needed. It implements internal learning capabilities, using the <b>Widrow-Hoff</b> <b>rule,</b> which converges towards the pseudo-inverse rule by iteration, thus allowing partial correlation between prototypes, and a higher capacity, compared to the Hebb rule. Also, it implements an internal mechanism for detecting relaxations on spurious states. The average retrieval speed is about 20 mu s, whereas the learning time is approximately 15 to 30 ms for 15 moderately correlated prototypes...|$|E
30|$|For {{the purpose}} of {{automatically}} classifying and monitoring the population density of L. migratoria migratoria pest, we used the backpropagation neural network. Standard backpropagation is a gradient descent algorithm, as is the <b>Widrow-Hoff</b> learning <b>rule,</b> in which the network weights are moved along the negative of the gradient of the performance function. The term ‘backpropagation’ refers to {{the manner in which}} the gradient is computed for nonlinear multilayer networks. There are a number of variations on the basic algorithm that are based on other standard optimization techniques, such as conjugate gradient and Newton methods.|$|R
40|$|An {{extension}} of Kohonen's self-organizing mapping algorithm {{together with an}} error-correction scheme based on the <b>Widrow-Hoff</b> learning <b>rule</b> is applied to develop a learning algorithm for the visuomotor coordination of a simulated robot arm. Learning occurs by a sequence of trial movements without the need of an external teacher. Using input signals {{from a pair of}} cameras, the "closed" robot arm system is able to reduce its positioning error to about 0. 3 percent of the linear dimensions of its work space. This is achieved by choosing the connectivity of a 3 D-lattice between the units of the neural net...|$|R
40|$|Abstract. This paper {{addresses}} {{the problem of}} estimating head pose {{over a wide range}} of angles from low-resolution images. Faces are detected using chrominance-based features. Grey-level normalized face imagettes serve as input for linear auto-associative memory. One memory is computed for each pose using a <b>Widrow-Hoff</b> learning <b>rule.</b> Head pose is classified with a winner-takes-all process. We compare results from our method with abilities of human subjects to estimate head pose from the same data set. Our method achieves similar results in estimating orientation in tilt (head nodding) angle, and higher precision for estimating orientation in the pan (side-to-side) angle. 1...|$|R
40|$|In many cross-lingual {{applications}} we need {{to convert}} a transliterated word into its original word. In this paper, we present a similarity-based framework to model the task of backward transliteration, and provide a learning algorithm to automatically acquire phonetic similarities from a corpus. The learning algorithm is based on <b>Widrow-Hoff</b> <b>rule</b> with some modifications. The experiment {{results show that the}} learning algorithm converges quickly, and the method using acquired phonetic similarities remarkably outperforms previous methods using pre-defined phonetic similarities or graphic similarities in a corpus of 1574 pairs of English names and transliterated Chinese names. The learning algorithm does not assume any underlying phonological structures or rules, and can be extended to other language pairs once a training corpus and a pronouncing dictionary are available...|$|E
40|$|International audienceSummary form only given. An {{electronic}} {{implementation of}} a completely connected feedback network, containing 64 neurons, is considered. The technology is fully digital CMOS, with binary neurons and 9 -bit-wide signed synaptic coefficients. The architecture trades off connectivity versus speed by implementing a linear systolic loop, in which each neuron locally stores its own synaptic coefficients. The authors have first implemented internal learning capabilities. They used the <b>Widrow-Hoff</b> <b>rule,</b> which converges towards the projection rule by iteration, thus allowing partial correlation between prototypes and a higher capacity compared to the Hebb rule. They have also implemented an internal mechanism for detecting relaxations on spurious states. The combination of these two properties gives the network a rather high degree of autonomy, making unnecessary {{the use of an}} external computer for tasks other than just writing or reading data and asserting simple control signals...|$|E
40|$|Image {{and video}} {{filtering}} {{is a key}} image-processing task in computer vision especially in noisy environment. In {{most of the cases}} the noise source is unknown and hence possess a major difficulty in the filtering operation. In this paper we present an error-correction based learning approach for iterative filtering. A new FIR filter is designed in which the filter coefficients are updated based on <b>Widrow-Hoff</b> <b>rule.</b> Unlike the standard filter the proposed filter has the ability to remove noise without the a priori knowledge of the noise. Experimental result shows that the proposed filter efficiently removes the noise and preserves the edges in the image. We demonstrate the capability of the proposed algorithm by testing it on standard images infected by Gaussian noise and on a real time video containing inherent noise. Experimental result shows that the proposed filter is better than some of the existing standard filter...|$|E
40|$|Abstract — Generalized Adaptive Linear Element (GADALINE) Artificial Neural Network (ANN) as an Artificial Intelligence (AI) {{technique}} {{is used in}} this paper to online adaptive control of a Non-linear Inverted Pendulum (IP) system. The ANN controller is designed with specifications as: network type is three (Input, Hidden and Output) layered Feed-Forward Network (FFN), training is done by <b>Widrow-Hoffs</b> delta <b>rule</b> or Least Mean Square algorithm (LMS), that updates weight and bias states to minimize the error function. The research is focused on how to adapt the control actions {{to solve the problem}} of “parameter variations”. The method is applied to the Nonlinear IP model with the application of some uncertainties, and the experimental results show that the system responds very well to handle those uncertainties...|$|R
40|$|This paper {{proposes a}} novel linear {{recurrent}} neural network for multivariable system identification, namely a linerec neural network (LNN). Based on this network, {{the transfer function}} matrix model of a multivariable system can be identified directly according to its input and output data. In this way, LNNs differ from existing neural networks. An LNN is constructed based on the identification of prior knowledge in a system, and its weights have definite physical meaning. An LNN is equivalent to a linear equation set, and its training algorithm is based on <b>Widrow-Hoff</b> learning <b>rules.</b> In this paper, the theoretical foundation, structural algorithm and learning rules of LNNs are proposed and studied. To guarantee learning convergence, network training stability is analysed using discrete Lyapunov stability theory. Finally, simulation results show the feasibility of LNNs for multivariable system identification...|$|R
30|$|An {{artificial}} {{neural network}} can be demarcated as a mathematical exemplification of a regular phenomenon. The artificial neurons can be connected in a very structured and uniform way so that they accurately represent the attributes of a system. There have been some standardized designs of ANNs developed by scientists which {{has been used for}} many applications all around the world. ANNs have redefined science in a much simpler way. By taking the generalized form of <b>Widrow-Hoff</b> learning <b>rule</b> into consideration, the error back-propagation algorithm was used [23]. The back-propagation algorithm is actually based on the gradient descent algorithm, where initially the performance function is taken into consideration and then the network weights are moved accordingly (through the negative gradient). Lots of variations are possible in this algorithm. Well-trained error back-propagation algorithms give reasonably good answers, even when they have been presented with inputs which have not come under their peripherals before.|$|R
40|$|This report {{gives an}} {{overview}} of the JavaXCSF code and explains, where to get the code. Furthermore, the settings and features are described briefly. The document also explains how to use the code and how to extend it. JavaXCSF is an implementation of the XCSF Learning Classifier System that is used for function approximation. The code contains four types of conditions, namely rectangular, ellip-soidal, rotating rectangular, and rotating ellipsoidal conditions. In ad-dition to a simple constant predictor based on the <b>Widrow-Hoff</b> <b>rule,</b> implementations of a linear recursive least squares (RLS) prediction and a quadratic RLS prediction are included. Eleven test functions may serve as example problems. In order to adapt to current state of the art CPU’s, which usually have more than one core, the code also optionally supports parallelized matching to exploit the full power of multicore CPU’s by using multiple parallel threads...|$|E
40|$|We {{make a case}} that, {{even with}} severe {{efficiency}} constraints, taking the number of bits to code each motion vector into account when estimating motion for video compression results in significantly better performance at low bit rates, using simulation studies on established benchmark image sequences. In particular, we examine an algorithm that differs from a "vanilla" implementation of the H. 261 standard by choosing motion vectors to minimize a cost function of prediction error {{and the number of}} bits to code a particular motion vector, where the coefficients of the cost function are adapted on-line using the <b>Widrow-Hoff</b> <b>rule.</b> We show that this algorithm performs comparably to a variety of more idealized, computationally intensive methods we examined in earlier papers and substantially better than the original "vanilla" method, which ignores the number of bits to code the motion vector when choosing it. 1 Introduction Hybrid video coding that combines block-matching motion compensatio [...] ...|$|E
40|$|We {{consider}} {{the problem of}} maximizing {{the total number of}} successes while learning about a probability function determining the likelihood of a success. In particular, we {{consider the}} case in which the probability function is represented by a linear function of the attribute vector associated with each action/choice. In the scenario we consider, learning proceeds in trials and in each trial, the algorithm is given a number of alternatives to choose from, each having an attribute vector associated with it, and for the alternative it selects it gets either a success or a failure with probability determined by applying a fixed but unknown linear success probability function to the attribute vector. Our algorithms consist of a learning method like the <b>Widrow-Hoff</b> <b>rule</b> and a probabilistic selection strategy which work together to resolve the so-called exploration-exploitation tradeoff. We analyze the performance of these methods by proving bounds on the worst-case regret, or how many less [...] ...|$|E
40|$|A Neural Network is a {{powerful}} data modeling tool that is able to capture and represent complex input/output relationships. The motivation {{for the development of}} neural network technology stemmed from the desire to develop an artificial system that could perform "intelligent" tasks similar to those performed by the human brain. Back propagation was created by generalizing the <b>Widrow-Hoff</b> learning <b>rule</b> to multiple-layer networks and nonlinear differentiable transfer functions. The term back propagation refers to {{the manner in which the}} gradient is computed for nonlinear multilayer networks. There are a number of variations on the basic algorithm that are based on other standard optimization techniques, such as conjugate gradient and Newton methods.. Properly trained back propagation networks tend to give reasonable answers when presented with inputs that they have never seen. These variances of backpropagation are used for character recognition and their comparative study on the performance of different algorithm is made in the pape...|$|R
40|$|Abstract—A novel {{approach}} towards {{diagnosis of}} parameter deviations in linear system using {{artificial neural network}} based identification algorithm is proposed in this brief. The fault diagnosis is done by identifying {{the parameters of the}} system from the measurement of state variables of the system. The input excitation for the system is unit step and the artificial neural network which identifies the parameters of the system is trained using <b>widrow-hoff</b> training <b>rule.</b> The system is identified in discrete domain by using the state variables of the system at to consecutive sampling instants in a recursive procedure. The faults are introduced arbitrarily and the neural network identifies the parameter continuously from the measurement of state variables. The proposed method is feasible for on line fault diagnosis through digital implementation. The mathematical formulation of the technique and the simulation results are presented to validate the feasibility of the proposed approach. Keywords—Parameter deviation, ANN, System identification, Fault diagnosis, linear system...|$|R
40|$|In a {{previous}} work (Mohemmed et al., Method for training a spiking neuron to associate input–output spike trains) [1] we {{have proposed a}} supervised learning algorithm based on temporal coding to train a spiking neuron to associate input spatiotemporal spike patterns to desired output spike patterns. The algorithm {{is based on the}} conversion of spike trains into analogue signals and the application of the <b>Widrow–Hoff</b> learning <b>rule.</b> In this paper we present a mathematical formulation of the proposed learning rule. Furthermore, we extend the application of the algorithm to train a SNN consisting of multiple spiking neurons to perform spatiotemporal pattern classification and we show that the accuracy of classification is improved significantly over a single spiking neuron. We also investigate a number of possibilities to map the temporal output of the trained spiking neuron into a class label. Potential applications for motor control in neuro-rehabilitation and neuro-prosthetics are discussed as a future work...|$|R
40|$|Spiking Neural Networks (SNN) {{were shown}} to be {{suitable}} tools for the processing of spatio-temporal information. However, due to their inherent complexity, the formulation of efficient supervised learning algorithms for SNN is difficult and remains an important problem in the research area. This article presents SPAN – a spiking neuron that is able to learn associations of arbitrary spike trains in a supervised fashion allowing the processing of spatio-temporal information encoded in the precise timing of spikes. The idea of the proposed algorithm is to transform spike trains during the learning phase into analog signals so that common mathematical operations can be performed on them. Using this conversion, {{it is possible to}} apply the well-known <b>Widrow-Hoff</b> <b>rule</b> directly to the transformed spike trains in order to adjust the synaptic weights and to achieve a desired input/output spike behavior of the neuron. In the here presented experimental analysis, the proposed learning algorithm is evaluated regarding its learning capabilities, its memory capacity, its robustness to noisy stimuli and its classification performance. Differences and similarities of SPAN regarding two related algorithms, ReSuMe and Chronotron, are discussed...|$|E
40|$|A new {{learning}} rule (Precise-Spike-Driven (PSD) Synaptic Plasticity) is proposed for processing and memorizing spatiotemporal patterns. PSD is a supervised {{learning rule}} that is analytically {{derived from the}} traditional <b>Widrow-Hoff</b> <b>rule</b> {{and can be used}} to train neurons to associate an input spatiotemporal spike pattern with a desired spike train. Synaptic adaptation is driven by the error between the desired and the actual output spikes, with positive errors causing long-term potentiation and negative errors causing long-term depression. The amount of modification is proportional to an eligibility trace that is triggered by afferent spikes. The PSD rule is both computationally efficient and biologically plausible. The properties of this learning rule are investigated extensively through experimental simulations, including its learning performance, its generality to different neuron models, its robustness against noisy conditions, its memory capacity, and the effects of its learning parameters. Experimental results show that the PSD rule is capable of spatiotemporal pattern classification, and can even outperform a well studied benchmark algorithm with the proposed relative confidence criterion. The PSD rule is further validated on a practical example of an optical character recognition problem. The results again show that it can achieve a good recognition performance with a proper encoding. Finally, a detailed discussion is provided abou...|$|E
40|$|In this paper, a {{nonlinear}} dynamic {{neural network}} model is proposed for the identification of ship manoeuvring coefficients. A generalized <b>Widrow-Hoff</b> learning <b>rule</b> in feed-forward, back propagation networks give reasonable answers when presented with inputs not previously seen. With a proper optimization technique, the network is an excellent choice for nonlinear system identification, using reliable and inexpensive computing hardware. It is shown that these networks {{can be used to}} identify ship manoeuvres in every practical application, including an open-loop manner. An efficient, quick off-line training process allows the use of neural networks in predicting the ship's future position, speed and heading with input from rudder angle, engine rpm and the ship kinematics. Dealing with input noise from ship sensors is implicit in the proposed method and is very effective, in fact some level of noise is necessary for the training purposes. In the paper the combination of the MMG standard manoeuvring model with the neural network prediction simulates the real-time realization of ship manoeuvring. Peer reviewed: NoNRC publication: Ye...|$|R
40|$|The {{learning}} process for multi layered neural networks with many nodes makes heavy demands on computational resources. In some neural network models, the learning formulas, {{such as the}} Widrow-Hoff formula, do not change the eigenvectors of the weight matrix while flatting the eigenvalues. In infinity, this iterative formulas result in terms formed by the principal components of the weight matrix: i. e., the eigenvectors corresponding to the non-zero eigenvalues. In quantum computing, the phase estimation algorithm is known to provide speed-ups over the conventional algorithms when it {{is used for the}} eigenvalue related problems. Therefore, it is appealing to ask whether we can model such learning formulas in quantum computing and gain a computational speed-up. Combining the quantum amplitude amplification with the phase estimation algorithm, a quantum implementation model for artificial neural networks using the <b>Widrow-Hoff</b> learning <b>rule</b> is presented. In addition, the complexity of the model is found to be linear {{in the size of the}} weight matrix. This provides a quadratic improvement over the classical algorithms...|$|R
40|$|Hopfield networks, {{a type of}} Recurrent Neural Network, {{may be used as}} a {{tool for}} {{classification}} by storing exemplars as memories. This method of using the Hopfield network for classification has certain shortcomings, such as the limits on the number of class exemplars that can be stored {{and the size of the}} connection matrix required when used for classification of images. This paper describes a method of growing a Hopfield style network for use in classification of patterns. The complete network that is grown consists of three networks in sequence with the middle network being a Sully recurrent Hopfield style network. The first network is a one layer feedforward network while the last network is simply a selector network which selects components from the terminal state of the recurrent network. The Hopfield style network grows automatically during training as additional nodes are required. The resulting network can be trained for the purpose of classifying bi-polar vectors. Connection matrices are determined using a modified <b>Widrow-Hoff</b> learning <b>rule,</b> such that the exemplars are attracted to exemplars or prototypes within the same class. An unclassified element is then classified by the class of its attractor No pre-processing is required to determine prototypes and all the training elements are used directly. A reduction in the number of arithmetic operations from order of magnitude n(2) to n takes place by growing the network rather than initialising the network to size n. The method is successfully applied to the classification of cervical cells for cancer detection and to the classification of diabetes patients recorded in the ''Pima Indians Diabetes Data Base''. Results depict how the network grows as learning takes place. (C) 1997 Elsevier Science Ltd...|$|R
30|$|Another way to {{determine}} the relationship {{is the use of}} classification techniques on a large number of training data in order to learn an efficient separation between pose classes. SVM classifiers are quite used in literature to classify head poses [23 – 25] since they are adapted to real-time applications. In [23], the authors show that the multi-scale Gaussian derivatives, which are a particular case of steerable filters, combined to SVM give good results. In [26], normalized faces are used to train an auto-associative memory using the <b>Widrow-Hoff</b> correction <b>rule</b> in order to classify head poses. In one of the most recent works [27], the authors consider that object detection and continuous pose estimation are interdependent problems and they jointly formulate them as a structured prediction problem, by learning a single and continuously parameterized object appearance model over the entire pose space. After that, they design a cascaded discrete-continuous inference algorithm to effectively optimize a non-convex objective, by generating a diverse proposal to explore the complicated search space. Then, the model is learned using a structural SVM for joint object localization and continuous state estimation and a new training approach which reduces the processing time. Among the experiments, the authors perform the head pose estimation over the Pointing’ 04 database without considering the detection task, since they note that the images contain clean backgrounds. Before applying their method, the heads are cropped manually and the HOG descriptor is applied on three scales. Based on the relationship between the symmetry of the face image and the head pose, the authors in [28] propose a face representation method for head yaw estimation which is robust against rotations and illumination variations. First, they extract the multi-scale and multi-orientation Gabor representations of the face image, and then they use covariance descriptors to compute the symmetry between two regions in terms of Gabor representations under the same scale and orientation. Second, they apply a metric learning method named KISS MEtric learning (KISSME) to enhance the discriminative ability and reduce the dimension of the representation. Finally, the nearest centroid (NC) classifier is applied to obtain the final pose.|$|R
40|$|An {{important}} {{requirement of}} a rational policy for provision of outdoor recreation opportunities is {{some understanding of}} natural processes and public concern and /or preferences. Computerized land use suitability mapping is a technique which can help find the best location {{for a variety of}} developmental actions given a set of goals and other criteria. Over the past two decades, the methods and techniques of land use planning have been engaged in a revolution on at least two fronts as to shift the basic theories and attitudes of which land use decisions are based. The first of these fronts is the inclusion of environmental concerns, and the second is the application of more systematic methods or models. While these automated capabilities have shed new light on environmental issues, they, unfortunately, have failed to develop sufficient intelligence and adaptation to accurately model the dynamics of ecosystems. The work reported proceeds on the belief that neural network models can be used to assess and develop resource management strategies for Mounds State Park, Anderson, Indiana. The study combines a photographic survey technique with a geographic information system (GIS) and artificial neural networks (NN) to investigate the perceived impact of park management activities on recreation opportunities and experiences. It is unique in that it incorporates both survey data with spatial data and an optimizing technique to develop a model for predicting perceived management values for short and long term recreation management. According to Jeannette Stanley and Evan Bak (1988) a neural network is a massively parallel, dynamic systems of highly interconnected interacting parts based on neurobiological models. The behavior of the network depends heavily on the connection details. The state of the network evolves continually with time. Networks are considered clever and intuitive because they learn by example rather than following simple programming rules. They are defined by a set of rules or patterns based on expertise or perception for better decision making. With experience networks become sensitive to subtle relationships in the environment which are not obvious to humans. The model was developed as a counter-propagation network with a four layer learning network consisting of an input layer, a normalized layer, a kohonen layer, and an output layer. The counter-propagation network is a feed-forward network which combines Kohonen and <b>Widrow-Hoff</b> learning <b>rules</b> for a new type of mapping neural network. The network was trained with patterns derived by mapping five variables (slope, aspect, vegetation, soil, site features) and survey responses from three groups. The responses included, for each viewshed, the preference and management values, and three recreational activities each group associated with a given landscape. Overall the model behaves properly in learning the different rules and generalizing in cases where inputs had not been shown to the network apriori. Maps are provided to illustrate the different responses obtained from each group and simulated by the model. The study is not conclusive as to the capabilities of the combination of GIS techniques and neural networks, but it gives a good flavor of what can be achieved when accurate mapping information is used by an intelligent system for decision making. Department of Landscape ArchitectureThesis (M. L. A. ...|$|R
30|$|To {{make use}} of the full {{spectral}} resolution, we used a hybrid ANN architecture, the details of which are given in [24, 25]. Briefly, it consists of an input layer with as many nodes as the number of spectral bands plus one ‘bias’ neuron, a two-dimensional self-organizing map (SOM) [42] as the hidden layer, and the SOM layer is fully connected to a categorization learning output layer. Inputs to the output layer are the responses of all SOM neurons with the three largest responses normalized to sum to one, the rest set to zero, before passing them to the output layer. The output layer has one node for each class, i.e., the 1 -in-c encoding is used for class labels. (The c th output is expected to produce 1, the others to produce 0, for the c th class.) It learns with a <b>Widrow-Hoff</b> (delta) learning <b>rule</b> [3, 43] and uses a linear activation function. This hybrid network learns in two phases. First, in an unsupervised regime, it builds its own view of the manifold structure by forming a topology-preserving feature map of the data (clusters, if they exist) in the hidden SOM layer (while the output layer remains idle). In a subsequent supervised learning phase, the weights between the SOM hidden layer and the output layer are trained to recognize class labels. The pre-formed clusters - the model of the data manifold - in the SOM help prevent the learning of inconsistent labels and thus greatly support accurate learning of class labels in the supervised phase. This results in better generalization from a small number of samples and leading to higher classification accuracy, than without the SOM stage. Back propagation, in contrast, is powerful enough to simply ‘memorize’ inconsistent labeling if the number of training samples is small. For example, the network can learn to assign labels A and B to two individual training samples even if their characteristics are very similar (for example, B is a mislabeled sample from class A). In this case, no reasonable prediction can be expected because the network does not derive general class properties. This situation can be avoided with the hybrid ANN paradigm we described. It is also much faster to train the supervised output layer of this network than to train a BP network, since the output layer only learns the labeling of the classes (based on the cluster boundaries internally identified by the SOM). The delta learning rule with linear activation function is much simpler than back propagation, which helps relatively easy training even with very high dimensional data, in this SOM-hybrid architecture. The investment of training the SOM layer has the additional benefit that it can be reused in different supervised training sessions, for example, to train for various sets of classes, since the cluster structure of the manifold is the same regardless of how many classes are labeled. The good scale-up properties and high classification accuracies of this network have been demonstrated by previous hyperspectral analyses [24 – 26, 44] for up to 200 spectral channels and 20 to 30 classes.|$|R

