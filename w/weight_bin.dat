3|29|Public
40|$|Increasing {{rates of}} obesity {{and heart disease}} are compromising {{quality of life for}} a growing number of people. There is much {{research}} linking adult disease with the growth and development both in utero and {{during the first year of}} life. The pig is an ideal model for studying the origins of developmental programming. The objective of this paper was to construct percentile growth curves for the pig for use in biomedical studies. The body <b>weight</b> (<b>BIN)</b> of pigs was recorded from birth to 150 days of age and their crown-to-rump length was measured over the neonatal period to enable the ponderal index (Pl; kg/m(3)) to be calculated. Data were normalised and percentile curves were constructed using Cole's lambda-mu-sigma (LMS) method for BW and PI. The construction of these percentile charts for use in biomedical research will allow a more detailed and precise tracking of growth and development of individual pigs under experimental conditions...|$|E
30|$|This {{subset of}} cutting and {{packaging}} problem is NP-hard in nature (Garey and Johnson 1979). Dyckhoff (1990) gave the first clear classification for cutting and packing problem. The characterization {{is based on}} the fact that many are similar in their logical structure, but different in application areas. There are various categories in packing which include cutting stock problem, knapsack, bin packing and loading problem. Cutting stock involves cutting off available raw stock to meet customer demand such that trim loss is minimized. Knapsack is mostly considered as a sub-problem in many cases where certain weight is associated with the object. The objective is to pack these objects in a fixed size larger container to maximize the overall <b>weight.</b> <b>Bin</b> packing aims at packing items into bins. The dimensions are bounded, such that the remaining space in the used bin and the overall bin required to pack all items are minimized. All these problems range from single to multidimensions. Strip packing is considered in higher dimensions like two (2 D) and three (3 D).|$|E
40|$|Shelled yellow corn is {{commonly}} stored in concrete or corrugated steel bins. Granular materials compact {{under their own}} weight, primarily due to particle rearrangement, leading {{to an increase in}} bulk density and a change in volume when stored. Reliable grain pack factors are needed to estimate storage capacities and to accurately monitor grain inventories. A science-based model (WPACKING) of pack factors is available that uses the differential form of Janssen’s equation and takes into account the variation in density caused by pressure variation with height and moisture content of the grain and accounts for the effects of grain type, test <b>weight,</b> <b>bin</b> geometry, and bin material. However, this model needs to be compared to field data over a wide range of conditions to ensure robust prediction accuracy. The objective of this research was to determine the field pack factors and bin capacities for on-farm and commercial bins used to store corn in the U. S. and compare them to predictions of the WPACKING program. Bin inventory measurements were conducted in concrete bins with depths up to 31. 4 m (114. 8 ft) and corrugated steel bins with diameters up to 32. 8 m (156 Â ft). These values were also compared to the techniques used by the USDA Risk Management Agency (RMA) and the USDA Farm Service Agency, Warehouse Branch (FSA-W). The differences between predicted and reported mass were - 4. 54 % (maximum underprediction) to + 4. 53 % (maximum overprediction) for WPACKING, - 2. 69 % to 4. 97 % for the RMA method, and - 3. 33 % to + 5. 67 % for the FSA-W method. The absolute average difference was lowest for the WPACKING model (0. 90 %) compared to the RMA and FSA-W methods (1. 61 % and 1. 86 %, respectively). WPACKING had less than half as many prediction differences above 1 % (13 out of 51 bins) as did the RMA and FSA-W methods, which had 29 out of 51 and 33 out of 51, respectively. The RMA and FSA-W methods do {{not take into account the}} variations in pack factor due to bin type and moisture content of the stored grain...|$|E
40|$|This paper {{describes}} a modelling approach designed {{to investigate the}} variability in nett amenity <b>bin</b> <b>weights</b> produced by nine household waste recycling centres (HWRCs) in West Sussex, UK over a 12 -month period. Compaction technique, vehicle type, site design and month were identified as key factors explaining 76 % of the variability in the data. For each significant factor, a weighting coefficient was calculated to generate a predicted nett <b>weight</b> for every <b>bin</b> transaction. Analysis of predicted and observed mean <b>bin</b> <b>weights</b> suggested that three sites had similar characteristics but returned significantly different mean nett <b>bin</b> <b>weights.</b> Subsequent waste and site audits determined the possible sources of the remaining variability. Significant differences were identified in the proportions of bagged waste and dry recyclables deposited in the amenity waste stream at the sites, with significantly less observed at one site. Operational and managerial techniques (e. g. material separation, compaction frequency and site management ethos) were also identified as factors impacting on mean <b>bin</b> <b>weights</b> and general site performance. The model {{can be used to}} identify sites producing significantly different <b>bin</b> <b>weights,</b> enabling detailed ‘back-end’ waste analyses to be efficiently targeted and best practice in HWRC operation identifie...|$|R
40|$|In {{this note}} we review {{different}} approaches to non parametric regression. Kernel estimators are motivated from local averaging, solving ill-posed problems and <b>weighting</b> of <b>binned</b> data. Kernel estimators are compared to k-NN estimators and splines. The choice of smoothing paramet. er is discussed and finally the method is applied for nonparametric prediction of time series. ...|$|R
40|$|In {{order to}} develop a grain flow sensor, a test rig was built. Three {{weighting}} sensors were mounted on the <b>weighting</b> <b>bin</b> in the test rig to calibrate grain flow sensor and verify the accuracy of grain flow sensor. A valve plate was inserted {{in the bottom of}} the feed bin. The feed flow could be controlled by adjusting opening of the valve plate. A weighting measurement model and a feed flow model were established respectively according to user manual and calibration experiments. Weighting accuracy and feed rate for the test rig was analyzed through experiments. Results showed that the maximum relative error of weighting was 1. 12 %, the minimum relative error was 0. 36 %, and the average relative error was 0. 61 %. The average relative error of feed flow ranging from 0. 5 to 2. 4 kg/s was less than 4 %. The test rig developed is stable and reliable, and meets the requirements for development and testing of the grain flow sensor...|$|R
40|$|This paper {{describes}} a modelling approach used {{to investigate the}} significance of key factors (vehicle type, compaction type, site design, temporal effects) in influencing the variability in observed nett amenity <b>bin</b> <b>weights</b> produced by Household Waste Recycling Centres (HWRC). Understanding such variability is a prerequisite to achieving best operational practices, to minimise the number of vehicle movements between each HWRC and disposal sites, and achieve consequential environmental and traffic benefits. The method described can help to quickly identify sites that are producing significantly lighter bins, enabling detailed back-end analyses to be efficiently targeted and best practice in HWRC operation identified. Tested on weigh ticket data obtained from nine HWRCs across West Sussex, the model suggested that compaction technique, vehicle type, month and site design explained 76 % of the variability in the observed nett amenity weights. For each factor, a weighting coefficient was calculated to generate a predicted nett <b>weight</b> for each <b>bin</b> transaction and Bognor Regis, Crawley and East Grinstead were identified as having similar characteristics but returning significantly different mean nett <b>bin</b> <b>weights.</b> Waste and site audits were then conducted at the three sites to try and determine the possible sources of the remaining variability. Significant differences were identified in the proportions of contained waste (bagged), wood, and dry recyclables entering the amenity waste stream with significantly less contained waste and dry recyclables observed in the amenity waste bins at Bognor Regis...|$|R
30|$|The {{pseudorandom}} codes used, e.g., in the GPS system, are repetitive. Therefore, cyclic correlation is {{used with}} the FFT length equal {{to the length of}} the code epoch. Regarding FFT domain filtering, it is also enough to do the <b>weighting</b> for the <b>bins</b> of each FFT block, which implements cyclic convolution. There is no need to consider overlap-save of overlap-add type of processing [16] in this application.|$|R
40|$|We {{study the}} 2 -dimensional vector packing problem, {{which is a}} {{generalization}} of the classical bin packing problem where each item has 2 distinct <b>weights</b> and each <b>bin</b> has 2 corresponding capacities. The goal is to group items into minimum number of bins, without violating the bin capacity constraints. We propose a Θ(n) -time approximation algorithm that is inspired by the O(n^ 2) algorithm proposed by Chang, Hwang, and Park...|$|R
40|$|A global push towards {{renewable}} energy {{has seen the}} birth of cogeneration in Australian sugar mills. To maximise {{the amount of energy}} extracted from the material, whole crop harvesting has been introduced, where a higher proportion of total biomass is sent to the mill. The adverse affect of this is a significant reduction in bulk density of the harvested material. Transport costs to harvester owners and millers significantly increase as <b>bin</b> <b>weights</b> are reduced, and therefore there is a case for developing a harvester chopper system which maintains <b>bin</b> <b>weight</b> as the amount of trash sent to the mill increases. A single drum chopper system was developed in this study and a prototype was constructed for experimental assessment of the performance of the system. Cane and juice losses and billet quality were measured for a range of operational conditions which included varying the chopper drum speed and material pour rate. The cutting process was captured by high speed photography for analysis into the causes of damage and losses. Speeding up the chopper drum and therefore shortening the billet length proved to have the most detrimental effect on system performance, with reducing the target billet length from 200 mm to 100 mm resulting in over three times the overall losses. An increase in pour rate did not {{have a significant effect on}} losses or billet quality. For the set of trial conditions most closely representing those previously done with differential choppers, the single drum system produced similar efficiency results. However, the advantages of this system are most prominent in whole crop harvesting where shorter billets are required to maintain <b>bin</b> <b>weights.</b> A number of modifications to the concept are suggested which are expected to significantly enhance the performance of the concept in future trials...|$|R
40|$|AbstractLet τ be {{a list of}} n {{items with}} nonnegative weights {{assigned}} to them. We want to assign these items to m bins (n ≤ 3 m) with the object of minimizing the maximum <b>weight</b> of the <b>bins</b> such that no bin contains more than three items. As approximation algorithm for this NP-complete problem we use {{a modified version of}} the famous LPT-algorithm for multiprocessor scheduling. The main subject is to prove a worst-case bound of 43 – 13 m...|$|R
40|$|In an {{environmental}} context, {{the use of}} RFID (radio frequency identification) and load cell sensor technology can be employed for not only bringing down waste management costs, but also to facilitate automating and streamlining waste (e. g., garbage, recycling, and green) identification and weight measurement processes for designing smart waste management systems. In this paper, we outline a RFID and sensor model for designing a system in real-time waste management. An application of the architecture is described {{in the area of}} RFID and sensor based automatic waste identity, <b>weight,</b> and stolen <b>bins</b> identification system (WIWSBIS). <br /...|$|R
50|$|The plan {{covered a}} wide range of issues and was {{somewhat}} confused, but most SGGA members were enthusiastic about it.The plan covered grain handling, the grain blockade, farm credit and market speculation. It identified and proposed remedies for practices by the elevator companies that included excessive dockage fees, light <b>weights,</b> refusing to <b>bin</b> special grain, replacing special binned grain with lower-quality grain and preventing farmers who had bought storage space in an elevator from dealing with non-company buyers. All these abuses derived from the effective monopoly of the large grain handlers, and could be eliminated by the government taking over the local elevators.|$|R
5000|$|On 1 March a 26-year-old man {{was arrested}} on {{suspicion}} of attempting to pervert the course of justice. He was not {{the driver of the}} bin lorry nor a relative of McKeague. On 7 March the suspect was released and the police stated that they believed he had genuinely made a mistake and that the charge had been dropped. In conjunction with this, police revealed that an error had been made in the calculations of the <b>weight</b> that the <b>bin</b> lorry and that it was close to 100 kg. Urquhart stated on Facebook that [...] "This can really, devastatingly, only mean one thing".|$|R
30|$|This paper {{proposes a}} {{promising}} method to enhance NTF performance, {{taking advantage of}} a spatial cue given by users or from accompanying images. The enhancement is mainly achieved by introducing weights on bin-wise NTF cost functions, which differentiates a target component from other components. Since a spatial cue indicates which bins of the tensor spectrogram are important, it is possible {{to improve the quality of}} an approximation to the specific bins of the tensor by giving more <b>weights</b> to <b>bins</b> where the target is likely to exist and less weights to the others. Virtanen et al. proposed perceptually weighted NMF that provides perceptually motivated weights for each critical band in each frame in accordance with the loudness perception of the human auditory system [14]. Nevertheless, to our knowledge, no research regarding NMF that incorporates the weighting function for spatially focusing on target component estimation has been proposed. The evaluation results show that this method is advantageous in terms of separation quality over conventional PARAFAC-NTF and other source separation techniques such as the Degenerate Unmixing Estimation Technique (DUET) [15 – 17].|$|R
40|$|Abstract—Data {{integration}} {{is the problem}} of combining data residing at different sources, and providing the user with a unified view of these data. One of the critical issues of data {{integration is}} the detection of similar entities based on the content. This complexity is due to three factors: the data type of the databases are heterogenous, the schema of databases are unfamiliar and heterogenous as well, and the amount of records is voluminous and time consuming to analyze. As solution to these problems we extend our work in another of our papers by introducing a new measure to handle heterogenous textual and numerical data type for co-incident meaning extraction. Firstly, to in order accommo-date the heterogeneous data types we propose a new <b>weight</b> called <b>Bin</b> Frequency- Inverse Document Bin Frequency (BF-IDBF) for effective heterogeneous data pre-processing and classification by unified vectorization. Secondly in order to handle the unfamiliar data structure, we use the unsu-pervised algorithm Self-Organizing Map. Finally to help the user to explore and browse the semantically similar entities among the copious amount of data, we use a SOM based visualization tool to map the database tables based on their semantical content...|$|R
40|$|This paper {{contains}} the fully reduced observational {{data on the}} Beta Cep star BW Vul, which were obtained {{in the summer of}} 1982 during an international campaign at thirteen observatories in the northern hemisphere. Useful measurements were made during a total of 486 hours. All observations have been assigned proper <b>weights</b> and were <b>binned</b> in time intervals of 90 seconds. In this manner the observations were combined into single data points which resulted in about 6000 mean differential measurements. A preliminary ephemeris for 1982 is derived, and a mean lightcurve is given. The results indicate a fair degree of stability in the shape and in the amplitude of the lightcurve. Possible variations exist in the morphology of the stillstand phenomenon. Peer reviewe...|$|R
40|$|The toxin of Vibrio cholerae dissociates into subunit A and an {{aggregate}} of sub-unit B (choleragenoid); the dissociation is rapid under denaturing conditions and slow at neutral pH. Subunit A has a molecular weight of 27, 000 daItons (measured by sedimentation equilibrium or gel chromatography) {{and has two}} polypeptide chains (mol wt, approximately 22, 000 and 5, 000 daItons) joined by disulfide bonds. The molecular <b>weight</b> of subunit <b>Bin</b> 6 M guanidine hydrochloride is 14, 000 daItons when determined by sedimentation equilibrium or gel chromatography, al-though dodecylsuIfate gel electrophoresis suggests a lower value. These results suggest a structure of AB 4 for the toxin; studies of cross-linking with methyl-s-mercaptobutyrimidate confirm this structure. The properties of antibodies both to cholera toxin and to choleragenoid are compatible with this structure, but subunit A has very low immunogenicity. Subunit A by itself is active, and this activity is abolished by a large excess of antitoxin but not by choleragenoid, anticholera-genoid, or ganglioside G~n (galactosyl-N-acetylgalactosaminyl [sialosyl] lactosyl ceramide; GGnSLC). It is suggested that the function of subunit B is not to inter...|$|R
40|$|The {{subject of}} this work is {{to solve the problem}} of packing a set of {{shipments}} (various cuboid boxes) into containers of various shapes without wasting loading space. All the boxes have to be loaded and few are identical. As it is the case for all the packing problems, the packing has to satisfy geometry constraints: the items cannot overlap and have to lie entirely inside the bins. The richness of our application is to manage additional and common constraints: the <b>bin</b> <b>weight</b> capacity, the rotations of the boxes, the stability and the fragility of the boxes and the uniformity of the weight distribution inside the ULDs. The last constraint is crucial in air transportation: when ULDs are packed inside the airplane, the centre of gravity is computed assuming each ULD has a centre of gravity close to the geometrical centre of its basis. This type of constraints can be adapted to road transportation for the axle weight limits, which plays a key role since the weigh-in-motion systems become more common...|$|R
40|$|In {{this paper}} {{we deal with}} the d-dimensional vector packing problem, which is a {{generalization}} of the classical bin packing problem in which each item has d distinct <b>weights</b> and each <b>bin</b> has d corresponding capacities. We address the case in which the vectors of weights associated with the items are totally ordered, i. e., given any two weight vectors a i; a j, either a i is componentwise not smaller than a j or a j is componentwise not smaller than a i, and construct an asymptotic polynomial-time approximation scheme for this case. As a corollary, we also obtain such a scheme for the bin packing problem with cardinality constraint, whose existence was an open question {{to the best of our}} knowledge. We also extend the result to instances with constant Dilworth number, i. e. instances where the set of items can be partitioned into a constant number of totally ordered subsets. We use ideas from classical and recent approximation schemes for related problems, as well as a nontrivial procedure to round an LP solution associated with the packing of the small items. ...|$|R
40|$|The Multiple Subset Sum Problem (MSSP) is the {{selection}} of items from a given ground set and their packing into a given number of identical bins such that {{the sum of the}} item <b>weights</b> in every <b>bin</b> does not exceed the bin capacity and the total sum of the weights of the items packed is as large as possible. This problem is a relevant special case of the multiple knapsack problem, for which the existence of a Polynomial-Time Approximation Scheme (PTAS) is an important open question in the eld of knapsack problems. One main result of the present paper is the construction of a PTAS for MSSP. For the bottleneck case of the problem, where the minimum total weight contained in any bin is to be maximized, we describe a 2 = 3 -approximation algorithm and show that this is the best possible approximation ratio. Moreover, PTASs are derived for the special cases in which either the number of bins or the number of dierent item weights is constant. We nally show that, even for the case of only two bins, no fully polynomial-time approximation scheme exists for both versions of the problem...|$|R
40|$|The bin packing {{structure}} {{arises in}} a wide range of service operational applications, where a set of items are assigned to multiple bins with fixed capacities. With random item <b>weights,</b> a chance-constrained <b>bin</b> packing problem bounds, for each bin, the probability that the total weight of packed items exceeds the bin's capacity. Different from the stochastic programming approaches relying on full distributional information of the random item weights, we assume that only the information of the mean and covariance matrix is available, and consider distributionally robust chance-constrained bin packing (DCBP) models in this paper. Using two types of ambiguity sets, we equivalently reformulate the DCBP models as 0 - 1 second-order cone (SOC) programs. We further exploit the submodularity of the 0 - 1 SOC constraints under special and general covariance matrices, and utilize the submodularity as well as lifting and bin-packing structure to derive extended polymatroid inequalities to strengthen the 0 - 1 SOC formulations. We incorporate the valid inequalities in a branch-and-cut algorithm for efficiently solving the DCBP models. Finally, we demonstrate the computational efficacy of our approaches and performance of DCBP solutions on diverse test instances...|$|R
30|$|This paper {{seeks to}} improve CSP {{analysis}} in noisy environments {{with a special}} weighting algorithm. We assume the target sound source is a human speaker and the noise is broadband noise such as a fan, wind, or road noise in an automobile. Denda et al. proposed weighted CSP analysis using average speech spectrums as weights [7]. The assumption is that a subband with more speech power conveys more reliable information for localization. However, it {{did not use the}} harmonic structures of human speech. Because the harmonic bins must contain more speech power than the other bins, they should give us more reliable information in noisy environments. The use of harmonic structures for localization has been investigated in prior art [8, 9], but not for CSP analysis. This work estimated the pitches (F 0) of the target sound and extracted localization cues from the harmonic structures based on those pitches. However, the pitch estimation and the associated voiced-unvoiced classification may be insufficiently accurate in noisy environments. Also, {{it should be noted that}} not all harmonic bins have distinct harmonic structures. Some bins may not be in the speech formants and be dominated by noise. Therefore, we want a special weighting algorithm that puts larger <b>weights</b> on the <b>bins</b> where the harmonic structures are distinct, without requiring explicit pitch detection and voiced-unvoiced classification.|$|R
50|$|On 3 November 1937 Hotchkiss {{presented}} {{the prototype of}} a tractor; on 10 December of a trailer. On both elements however the manufacturer had not done any testing, the vehicles having been transported to Vincennes {{as soon as they}} were finished. Therefore, the commission delayed its trials until 27 December to allow Hotchkiss to make final adjustments on the base area. The tractor was tested until 10 February 1938. The type closely resembled the general outline of the Renault UE. The main difference was the presence of two bins instead of one, able to tilt sideways, positioned over the back of the mudguards. This doubled the carrying capacity. Instead of the small hoods, two very large armoured covers, retractable to the back, served as both entrance hatch and visor. The engine, differential and steering system were judged to be acceptable. The suspension system however was considered to be too weak, not having been reinforced to match the larger cargo mass to avoid surpassing the specified total weight. It consisted of two bogies, each with two small road wheels, sprung by narrow horizontal coil springs. A large tension wheel trailed on the ground, which lowered ground pressure to compensate for the larger <b>weight</b> of the <b>bins</b> but also increased track resistance and vibration. The cross-country speed was just 15 km/h. As even during testing when fully loaded entire bogies collapsed, the prototype was rejected.|$|R
40|$|We study a {{specific}} bin packing problem which {{arises from the}} channel assignment problems in cellular networks. In cellular communications, frequency channels are some limited resource which may need to share by various users. However, {{in order to avoid}} signal interference among users, a user needs to specify to share the channel with at most how many other users, depending on the user's application. Under this setting, the problem of minimizing the total channels used to support all users can be modeled as {{a specific}} bin packing problem as follows: Given a set of items, each with two attributes, weight and fragility. We need to pack the items into bins such that, for each bin, the sum of <b>weight</b> in the <b>bin</b> must be at most the smallest fragility of all the items packed into the bin. The goal is to minimize the total number of bins (i. e., the channels in the cellular network) used. We consider the on-line version of this problem, where items arrive one by one. The next item arrives only after the current item has been packed, and the decision cannot be changed. We show that the asymptotic competitive ratio is at least 2. We also consider the case where the ratio of maximum fragility and minimum fragility is bounded by a constant. In this case, we present a class of online algorithms with asymptotic competitive ratio at most of 1 / 4 + 3 r/ 2, for any r> 1. © 2007 Springer Science+Business Media, LLC. link_to_subscribed_fulltex...|$|R
40|$|According to the International Air Transport Association and Air Transport Action Group, 51. 3 {{million metric}} tons of goods were transported by {{airlines}} in 2014. To transport luggage, freight and mail, special containers, called Unit Load Devices (ULD), are used. The method of loading packages into ULDs represents a key element for cargo safety and aircraft weight and balance, {{as well as for}} the economy of airline companies. This thesis aims to solve the problem of packing a set of boxes into containers of various shapes without wasting loading space. The goal is to select the best set of ULDs to pack all the boxes achieving a minimum unused volume. As for all the packing problems, geometric constraints have to be satis ed: items cannot overlap and have to lie entirely within the bins. The richness of this application is to manage additional and common constraints: the <b>bin</b> <b>weight</b> limit, rotations, stability and fragility of the boxes, and weight distribution within a ULD. In practice, this problem is manually solved with no strict guarantee that the constraints are met. First, the problem is formulated as a mixed integer linear program. As this problem is NP-hard, it opens the way to heuristics. A second approach makes use of the formulation to apply three matheuristic methods, combining exact approaches and heuristics. Third, a tailored two-phase constructive heuristic is developed for this speci c problem; it aims to nd good initial solutions in short computational times. These approaches contain parameters that have been tuned using the irace parametrisation technique. For the experiments, several instances have been created on the basis of a box data set which stems from a real world case...|$|R
40|$|Learning a {{predictive}} {{model for a}} large scale real-world problem presents several challenges: {{the choice of a}} good feature set and a scalable machine learning algorithm with small generalization error. A support vector machine (SVM), based on statistical learning theory, obtains good generalization by restricting the capacity of its hypothesis space. A SVM outperforms classical learning algorithms on many benchmark data sets. Its excellent performance makes it the ideal choice for pattern recognition problems. However, training a SVM involves constrained quadratic programming, which leads to poor scalability. In this dissertation, we propose several methods to improve a SVM 2 ̆ 7 s scalability. The evaluation is done mainly {{in the context of a}} plankton recognition problem. One approach is called active learning, which selectively asks a domain expert to label a subset of examples from a lot of unlabeled data. Active learning minimizes the number of labeled examples needed to build an accurate model and reduces the human effort in manually labeling the data. We propose a new active learning method 2 ̆ 2 Breaking Ties 2 ̆ 2 (BT) for multi-class SVMs. After developing a probability model for multiple class SVMs, 2 ̆ 2 BT 2 ̆ 2 selectively labels examples for which the difference in probabilities between the predicted most likely class and second most likely class is smallest. This simple strategy required several times less labeled plankton images to reach a given recognition accuracy when compared to random sampling in our plankton recognition system. To speed up a SVM 2 ̆ 7 s training and prediction, we show how to apply bit reduction to compress the examples into several <b>bins.</b> <b>Weights</b> are assigned to different bins based on the number of examples in the bin. Treating each bin as a weighted example, a SVM builds a model using the reduced-set of weighted examples...|$|R
40|$|We {{consider}} the bin-packing {{problem with the}} constraint that the elements are in the plane, and only elements within an oriented unit square can be placed within a single bin. The elements are of given <b>weights,</b> and the <b>bins</b> have unit capacities. The problem is to minimize the number of bins used. Since the problem is obviously NP-hard, no algorithm is likely {{to solve the problem}} optimally in better than exponential time. We consider an obvious suboptimal algorithm and analyze its worst-case behavior. It is shown that the algorithm guarantees a solution requiring no more than 3. 8 times the minimal number of bins. We can show, however, a lower bound of 3. 75 in the worst case. We then generalize the problem to arbitrary convex figures and analyze a class of algorithms in this case. We also consider a generalization to multidimensional "bins, " i. e., the weights of points in the plane are vectors, and the capacities of bins are unit vectors. THE classical bin-packing problem can be stated as follows: Given n numbers between 0 and 1, pack them into "bins " such that the sum of numbers in a bin does not exceed 1 and the number of bins used is minimized. This problem has been studied thoroughly (see, e. g., [7 - 9, 11 - 14, 16]) and has applications in operations research [2, 4, 6, 10], computer operating system design and memory allocation [7 - 9, 11, 16]. More recently, the multi-weight bin-packing problem has also been studied by various authors [8, 16]. Now all these problems find yet another application in the area of computer network design [3]. In the design of a distributed computer system, three design problems are of major importance...|$|R
40|$|A {{number of}} {{attempts}} have been made to monitor cane yield variation across a block in Australia. These have ranged from the early yield monitoring systems based on discrete mass measurement, to the current focus of predicting yield via surrogate measurements based on chopper pressure, feed train roller displacement and elevator power. Recent work aimed at assessing commercially available sensors (Jensen et al. 2010) suggested that there were several areas in which there was room for marked improvement. Rather than testing commercially available sensors, this paper details the testing that was conducted on evaluating the measurement concept. These concepts included; the pressure drop across the elevator and chopper motors, a load cell in the elevator floor and the angle of opening of the top feed roller. These concepts cover those being employed in the commercial units, both past and present. Trials were conducted during the 2010 season in the Bundaberg region and in both the Bundaberg and Herbert regions in 2011. Campbell Scientific CR 3000 dataloggers were used to read each of the sensors at 40 Hz and record the averaged value every second, along with the GPS information. In addition to this sensor data, sugarcane yield was also measured to determine the accuracy and resolution of the respective yield monitoring concepts. Yield was determined using two methods and included mill (<b>bin)</b> <b>weight</b> data for individually consigned sub-blocks and weighed 50 m row samples into a weigh truck. The approach used was consistent with the methodology previously developed to assess the accuracy of commercial yield monitoring equipment during 2008 / 09. Preliminary analysis indicates that there are considerable similarities between the yield monitoring concepts in terms of their ability to measure yield, and that how the sensor data is recorded and managed is critical to the accuracy and overall performance of these concepts as yield monitors. This paper reports on the findings of this work and makes recommendations for further refining the devices and for additional work. ...|$|R
3000|$|As already {{mentioned}} in Section 4.2, the resolution range for the MS-VQ quantizers used in LT coding is within 20 to 36 [*]bits/vector. The ASD target was being varied from 2.6 [*]dB {{to a minimum}} value with a 0.2 [*]dB step. The minimum value is 1.0 [*]dB for r = 36, 34, 32 and 30 [*]bits/vector, and then it is increased by 0.2 [*]dB each time the resolution is decreased by 2 [*]bits/vector (it is thus 1.2 [*]dB for r = 28 [*]bits/vector, 1.4 [*]dB for r = 26 [*]bits/vector, and so on). In parallel, the distortion-rate values were also calculated for usual frame-by-frame quantization using the same quantizers than in the LT coding process, and using the same test corpus. In this case, the resolution range was extended to lower values for a better comparison. For the 2 D-transform coding methods, the temporal size was varied from 1 to 20 for voiced LSFs, and from 1 to 10 for unvoiced LSFs. This choice was made after the histograms of Figure 2 and after considerations on computational limitations. 11 It is coherent with the values considered in [19]. We calculated the corresponding ASD for the complete test corpus, and for seven values of the optimal scalar quantizers resolution: 0.75, 1, 1.25, 1.5, 1.75, 2.0 and 2.25 [*]bits/parameter. This corresponds to 375, 500, 625, 750, 875, 1, 000 and 1, 125 [*]bits/s, respectively, (since the hop size is 20 [*]ms). We also calculated {{for each of these}} resolutions a weighted average value of the spectral distortion (ASD), the <b>weights</b> being the <b>bins</b> of the histogram of Figure 2 (for the test corpus) normalized by the total size of the corpus. This enables one {{to take into account the}} distribution of the temporal size of the LSF sequences in the rate-distortion relationship, for a fair comparison with the proposed LT coding technique. This way, we assume that both the proposed method and 2 D-transform coding methods work with the same [...] "adaptive" [...] temporal-block configuration.|$|R
40|$|Thesis (Master's) [...] University of Washington, 2016 - 06 Background: Heat {{health effects}} are an {{important}} public health problem in outdoor workers, including agricultural workers. Outdoor agricultural workers who perform heavy physical labor in hot conditions {{are at increased risk}} for developing occupational heat-related illness. Heat stress, under certain environmental conditions, has been reported to reduce worker productivity. Climate models project future increases in the frequency, severity, and duration of heat waves. Objectives: This study aimed to characterize heat stress and physiological effects of heat stress (heat strain) in outdoor tree fruit workers performing harvest activities in Yakima Valley, Washington, and to assess the relationship between heat exposure and productivity in these workers. Methods: During the summer of 2015, 46 pear and apple harvesters from six orchards participated in a cross-sectional study in Yakima Valley, Washington for one work shift each during warmer periods in August (n= 34 pear harvesters) and cooler periods in September (n= 12 apple harvesters). All participants were paid by the amount of fruit harvested (piece-rate). Heat stress and strain were characterized using American Conference of Governmental Hygienist (ACGIH) guidelines, which recommend thermal Action Limits and Threshold Limit Values based on several factors, including environmental conditions, metabolic rate of task, and clothing ensembles. Heat exposure was measured near individual workers using hand-held Wet Bulb Globe Temperature (WBGT) monitors, metabolic rate was estimated using field observations and personal hip-mounted accelerometers, and research staff observed workers’ clothing. Heart rate and core body temperature were monitored {{over the course of the}} work shift using heart rate monitors and wireless ingestible core body temperature sensors. A computer-assisted self interview survey instrument captured other relevant demographic, individual, and work factors. The total <b>weight</b> of fruit <b>bins</b> collected per time worked was used to assess productivity. Effect estimates of the association between maximum work shift WBGT and productivity were estimated using linear mixed effects models with a random intercept for orchards, using Kenward-Roger methods for small sample sizes, adjusted for relevant confounders. Results: Surveys of workers indicated that 24 (52 %) had experienced symptoms of heat strain and heat-related illness, and only 13 (28 %) received training on working in the heat. Of the 34 participants who worked in pear harvest in August, 25 (74 %) exceeded the ACGIH Action Limit (WBGTeffective 25 ⁰ C), and 21 (62 %) exceeded the Threshold Limit Value (WBGTeffective 28 ⁰ C) for the moderate work task (300 Watts) of harvesting. Using personal accelerometer data to estimate metabolic rate (n= 39), 12 (31 %) participants exceeded the Action Limit and four (10 %) exceeded the Threshold Limit Value. Of the 12 participants exceeding the Action Limit, based on accelerometer data, nine (75 %) exceeded the maximum heart rate (180 -age beats per minute), and five (42 %) exceeded the maximum internal core body temperature of 38. 5 °C recommended by ACGIH. There was a trend of a decrease in productivity with increasing maximum daily WBGT, but this association was not statistically significant. Conclusions: Current summer tree-fruit harvesters in Yakima Valley, Washington are laboring in thermal environments hazardous to health. Payment schemes may provide incentives for workers to not slow down, and increase the risk of HRI. Acclimatization practices, HRI training, and orchard management practices could be improved to increase biological adaptation to heat stress and prevent HRI. The relationship between heat exposure and productivity in tree fruit harvesters is complex and likely affected by monetary factors and orchard and harvest characteristics. The effects of heat stress on heat strain and productivity in outdoor workers should be considered in future planning, given the projected increase in frequency, severity, and duration of heat waves...|$|R

