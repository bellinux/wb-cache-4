11|252|Public
50|$|Spoon Server is an {{application}} deployment platform that allows software packaged with Spoon Studio to be deployed, patched, and managed via the <b>web.</b> <b>End</b> users log into a website from any browser {{and are able}} to launch and use desktop-level applications like Microsoft Word and Photoshop without having to install them. It is a private version of the Spoon.net service.|$|E
40|$|Abstract—In this paper, {{a method}} of {{displaying}} 3 D model on the <b>web</b> <b>end</b> based on Three. js library which can simulate the wrinkle and distortion of fabric and support the replacement and modification of texture is realized. The visual effect of a specific fabric texture can be evaluated after texture mapping from the custom area in the model. At first, extract the texture image of 3 D model and model it by using the finite element mesh algorithm. Using cubic convolution interpolation algorithm to solve the texture mapping anti aliasing problem can effectively smooth the jags after texture mapping so that the result of fabric simulation is more accurate and realistic. The luminance information of the scene is preserved based on fusion algorithm to make texture mapping more stereo. Introduce the processed texture and 3 D model into the <b>Web</b> <b>end</b> for 3 D display, which can help to evaluate the effect of different fabrics in different scenes. Keywords-three. js; three-dimensional display; texture mapping; luminance fusion; cubic convolution interpolation I...|$|E
40|$|Zoomify Image is {{a mature}} product for easily {{publishing}} large, high resolution {{images on the}} <b>Web.</b> <b>End</b> users view these images with existing Web browser software {{as quickly as they}} do normal, downsampled images. A Flash based Zoomifyer client asynchronously streams image data to the Web browser as needed, resulting in response times approaching those of desktop applications using minimal bandwidth. The author, a librarian at Cornell University and the principle architect of a small, open source company, worked closely with Zoomify to produce a cross-platform, open source implementation of that company's image processing software and discusses how to easily deploy the product into a widely used Web publishing environment. Limitations are also discussed as are areas of improvement and alternatives...|$|E
5000|$|The {{following}} are <b>web</b> front <b>ends</b> (requires e.g. IRC chat server): ...|$|R
50|$|CARTO is an {{open source}} {{software}} built on PostGIS and PostgreSQL. The tool uses JavaScript extensively in the front <b>end</b> <b>web</b> application, back <b>end</b> Node.js based APIs, and for client libraries. CARTO consists of two primary offerings.|$|R
30|$|The {{application}} server in this sub-system hosting the <b>web</b> front <b>end</b> is explained {{in more detail}} in the following section.|$|R
40|$|Web sites, web browsers, {{web site}} authors, web {{component}} authors, and end users interact {{in a complicated}} environment with many recognized and unrecognized trust relationships. The web browser is the arena in which many important trust relationships interact, thus it bears a considerable burden in protecting the interests and security of <b>web</b> <b>end</b> users as well as web site authors. Existing proposals, draft standards, implemented features, and web application techniques {{go a long way}} towards allowing rich and compelling content interactions, but they do not provide for rich, mutually-distrusting content to be safely embedded in a single page. This proposal suggests a declarative policy mechanism that permits untrusted content to be safely embedded in a web site while still retaining some richness. It also suggests a policy integration approach to allow multiple cooperative (but not necessarily trusting) parties to provide components of a policy that combine together in a safe manner. It incorporates techniques including fine-grained and coarse-grained permission dropping and white-listing protections for retained capabilities. Finally, the proposed concepts are applied to a number of real-world CVE vulnerabilities, and it is explained how the proposal does or does not prevent or mitigate the attack. The solution is shown to be effective against cross-style-scripting style attacks, and to not be effective at preventing incoming cross-site request forgery attacks...|$|E
40|$|Research in {{the domain}} of {{landscape}} virtual reconstructions has been mainly focused on digitization and recording inside GIS systems, or real time visualization, paying a minor attention {{to the development of a}} methodological approach for the landscape narration, combing different registers, conceptual, emotional incitements and, thus, able to arouse in the public a feeling of emotional “sensing” and self- identification. The landscape reflects also the human activities in the territory and the communities’ cultural patterns, their sense of “belonging”. In a virtual museum of landscapes, the multidisciplinary approach, the multiplication of perspectives and voices, storytelling, acquire primary importance. A Virtual Museum of landscapes should integrate both holistic and delimited visions. The holistic vision requires a diachronic approach, including both present and past phases of life. On the other side, delimited, or “monographic”, representations are useful to go deeper into specific and exemplar stories, regarding specific groups of people. Beside, the emergence of new social media enhancing cultural interactions among people induce the creation of specific social platforms for Cultural Heritage for the active participation of a large number of stakeholders. Co-creation scenarios and tools can be particularly promising. Aton is an example of front-end VR social platform in the <b>web</b> <b>end,</b> for the efficient streaming of medium/large landscape, their exploration and characterization. The Tiber Valley Virtual Museum is an example of sensorial cultural landscape. Starting from the acquisition of topographical data through integrated technologies, several multi-sensory scenarios have been created, inside which visitors can feel embodied and involved...|$|E
40|$|Currently, {{with the}} {{exponential}} rising quantity of automated textual data {{available on the}} <b>Web,</b> <b>end</b> users require {{the ability to get}} information in summary form, while keeping the most vital information in the document. As a result of this, the necessity for the creation of Summarization systems became vital. Summarization systems, collect and focus on the most important ideas of the papers and help the users to find and understand the main ideas of the text faster and in a simpler way from the dispensation of information. Compelling set of such systems are those that create summaries of extracts. This type of summary, which is called Extractive Summarization, extracts the most applicable sentences from the main document. The used methods, usually assign a score for every sentence in the text, based on specific features. Then choose the most important sentences, according to the degree of score for each sentence. These features include but not limited to, the sentence length, its similarity with the title, the position of the sentence in the main document, and the frequency of the words in the sentence. Nevertheless, not have been achieved quality summaries corresponds with the ones made by humans, and therefore proposing the techniques continue to be raised, for the aims of improving the outcomes. One methodology for creating extractive summary is using the graph theory. One field in graph theory called graph pruning / reduction, which aims to find the greatest illustration of the original graph with less number of nodes and edges. This paper proposes a method of extractive summarization based on a graph reduction technique called the triangle counting approach. This method has three main phases. The first phase is graph representation, where nodes are the sentences and edges are the similarity between the sentences. The second phase is triangles construction, and the third phase is bit vector representation for the triangles nodes and finally create the summary based on the values of bit vector. The proposed method was evaluated, using ROUGE measures on the dataset DUC 2002. The results showed that by using triangle counting as a reduction technique, it performs better than {{the state of the art}} methods...|$|E
50|$|The Bible Tool is a <b>web</b> front <b>end</b> to SWORD. One {{instance}} of the tool is hosted at CrossWire's own site.|$|R
5000|$|A <b>web</b> service <b>end</b> {{point that}} no one uses, but which can be rationally {{understood}} to represent one or more useful steps in a business process.|$|R
5000|$|Carrot2 Web Application: exposes Carrot² {{clustering}} as a <b>web</b> {{application for}} <b>end</b> users.|$|R
40|$|Stable isotope mixing {{models in}} aquatic ecology require delta C- 13 values for food <b>web</b> <b>end</b> members such as {{phytoplankton}} and bacteria, {{however it is}} rarely possible to measure these directly. Hence there is a critical need for improved methods for estimating the delta C- 13 ratios of phytoplankton, bacteria and terrestrial detritus from within mixed seston. We determined the delta C- 13 values of lipids, phospholipids and biomarker fatty acids and used these to calculate isotopic differences compared to the whole-cell delta C- 13 values for eight phytoplankton classes, five bacterial taxa, and three types of terrestrial organic matter (two trees and one grass). The lipid content was higher amongst the phytoplankton (9. 5 +/- 4. 0 %) than bacteria (7. 3 +/- 0. 8 %) or terrestrial matter (3. 9 +/- 1. 7 %). Our measurements revealed that the delta C- 13 values of lipids followed phylogenetic classification among phytoplankton (78. 2 % of variance was explained by class), bacteria and terrestrial matter, {{and there was a}} strong correlation between the delta C- 13 values of total lipids, phospholipids and individual fatty acids. Amongst the phytoplankton, the isotopic difference between biomarker fatty acids and bulk biomass averaged - 10. 7 +/- 1. 1 parts per thousand for Chlorophyceae and Cyanophyceae, and - 6. 1 +/- 1. 7 parts per thousand for Cryptophyceae, Chrysophyceae and Diatomophyceae. For heterotrophic bacteria and for type I and type II methane-oxidizing bacteria our results showed a - 1. 3 +/- 1. 3 parts per thousand, - 8. 0 +/- 4. 4 parts per thousand, and - 3. 4 +/- 1. 4 parts per thousand delta C- 13 difference, respectively, between biomarker fatty acids and bulk biomass. For terrestrial matter the isotopic difference averaged - 6. 6 +/- 1. 2 parts per thousand. Based on these results, the delta C- 13 values of total lipids and biomarker fatty acids can be used to determine the delta C- 13 values of bulk phytoplankton, bacteria or terrestrial matter with +/- 1. 4 parts per thousand uncertainty (i. e., the pooled SD of the isotopic difference for all samples). We conclude that when compound-specific stable isotope analyses become more widely available, the determination of delta C- 13 values for selected biomarker fatty acids coupled with established isotopic differences, offers a promising way to determine taxa-specific bulk delta C- 13 values for the phytoplankton, bacteria, and terrestrial detritus embedded within mixed seston...|$|E
40|$|We {{implemented}} {{several new}} web-based tools {{to improve the}} efficiency and versatility {{of access to the}} APS Catalog of the POSS I (Palomar Observatory-National Geographic Sky Survey) and its associated image database. The most important addition was a federated database system to link the APS Catalog and image database into one Internet-accessible database. With the FDBS, the queries and transactions on the integrated database are performed {{as if it were a}} single database. We installed Myriad the FDBS developed by Professor Jaideep Srivastava and members of his group in the University of Minnesota Computer Science Department. It is the first system to provide schema integration, query processing and optimization, and transaction management capabilities in a single framework. The attached figure illustrates the Myriad architecture. The FDBS permits horizontal access to the data, not just vertical. For example, for the APS, queries can be made not only by sky position, but also by any parameter present in either of the databases. APS users will be able to produce an image of all the blue galaxies and stellar sources for comparison with x-ray source error ellipses from AXAF (X Ray Astrophysics Facility) (Chandra) for example. The FDBS is now available as a beta release with the appropriate query forms at our web site. While much of our time was occupied with adapting Myriad to the APS environment, we also made major changes in Star Base, our DBMS for the Catalog, at the web interface to improve its efficiency for issuing and processing queries. Star Base is now three times faster for large queries. Improvements were also made at the <b>web</b> <b>end</b> of the image database for faster access; although work still needs to be done to the image database itself for more efficient return with the FDBS. During the past few years, we made several improvements to the database pipeline that creates the individual plate databases queries by StarBase. The changes include improved positions especially for galaxies, using a new median centroider and integrated magnitudes for galaxies with an improved density-to-intensity calibration with a "sky" background subtraction. In the original version of StarBase the object classification fainter than 19. 5 - 20. 0 mag., was an extrapolation of the networks trained on brighter objects. We have used a new catalog of galaxies at the NGP to train a neural network on objects fainter than 20 th mag. This improved classification is used in the new version of StarBase. We have also added a FITS table option for the returned data from queries on the object catalog. The APS image database includes images in both colors so we have added a tool for querying the image database in both colors simultaneously. The images can be displayed in parallel or blinked for comparison...|$|E
40|$|Demand {{continues}} to increase for bridges with long spans and shallow depths. Due to safety concerns, four-span overpasses are being replaced with two span overpasses to avoid placement of piers near the highway shoulders. In the meantime, the bridge profile is restricted due to existing businesses nearby. Thus, nearly the same superstructure depth must be used for double the span length. This dissertation focuses on topics aiming at providing precast prestressed concrete girders with the shallowest possible depth for a given span. It forms parts of larger projects conducted by the University of Nebraska for the Nebraska Department of Roads and for the Wire Reinforcement Institute. Specifically, the following issues were researched: (1) Use of 0. 7 in. diameter Grade 270 ksi strands for pretensioning of precast concrete girders at a strand spacing of 2 inches by 2 inches. This arrangement gives nearly 190 percent of the prestressing with 0. 5 in. diameter strands and nearly 135 percent with 0. 6 in. strands. The research focuses on the required confinement steel to allow determination of transfer and development lengths according to current procedures in the AASHTO LRFD Bridge Design Specifications for smaller strands. (2) Develop a self consolidating concrete (SCC) mix, using Nebraska aggregates that will allow for a specified design strength at service of 15 ksi and a minimum strength at one day of 10 ksi, representing the demand {{at the time of}} release of the prestress to the concrete member. Prior to this study, standard concrete strength prevailing in Nebraska has been 8 ksi at service and 6. 5 ksi at release. It was the goal of the research to keep the cost of materials as low as possible but not exceeding $ 250 per cubic yard, compared to the proprietary mixes that cost approximately four times this amount. (3) Use of 80 ksi welded wire reinforcement (WWR) as the auxiliary reinforcement for shear, <b>web</b> <b>end</b> splitting and flange confinement. This would result in higher quality product, less reinforcement congestion, about 25 percent savings in the steel materials, and considerable savings in girder fabrication costs. ^ A combination of theoretical and experimental work has resulted in the following findings: (1) A shear friction model can be used to estimate the required amount of confinement of the bottom flange. (2) A reasonable reinforcement detail is needed, even with very heavily prestressed NU I girder bottom flange, to allow use of the current methods of estimating strands transfer and development lengths. (3) Two SCC mixes with materials costs less that $ 200 dollars per cubic yard and with the required strengths were able to be developed. The mixes exhibited excellent flowability and predictable engineering properties. (4) Grade 80 WWR was successfully used. Its shear resistance was theoretically predictable. It produced higher capacity than the Ultra High Performance steel fiber concrete demonstrated by the Federal Highway Administration, with much lower costs and conventionally predicable design strength. ...|$|E
25|$|With {{the rise}} of the World Wide Web (WWW), web {{front-ends}} (web2news) have become more common. <b>Web</b> front <b>ends</b> have lowered the technical entry barrier requirements to that of one application and no Usenet NNTP server account. There are numerous websites now offering web based gateways to Usenet groups, although some people have begun filtering messages made by some of the web interfaces for one reason or another. Google Groups is one such <b>web</b> based front <b>end</b> and some <b>web</b> browsers can access Google Groups via news: protocol links directly.|$|R
5000|$|In 2009, Coradiant {{acquired}} Symphoniq Corporation, a {{maker of}} tools to track <b>web</b> transactions from <b>end</b> users.|$|R
5000|$|... #Caption: Semi-tunnel {{crankshaft}} from a Tatra T27 This is a built-up crankshaft with bolted webs. The {{crank bearings}} (excluding the bearing races) {{are smaller than}} the <b>webs.</b> The <b>end</b> bearings of this semi-tunnel crankshaft (not shown) are also of conventional small size.|$|R
40|$|Thesis (Ph. D.) University of Alaska Fairbanks, 2016 Arctic {{waters off}} the coast of Alaska have become {{increasingly}} open to human activities via dramatic climatic changes, such as reduced sea ice thickness and extent, warming ocean temperatures, and increased freshwater input. This research advances knowledge of snow crab trophic dynamics and stock characteristics in Arctic waters off the Alaska coast. Here, I provided baseline information regarding snow crab position in Beaufort Sea benthic food webs, its specific dietary habits in the Chukchi and Beaufort seas, and expanded upon previously limited life-history and population dynamic data in the Chukchi and Beaufort seas. I first detailed benthic food webs on the Alaskan Beaufort Sea shelf and snow crab trophic positions within these food webs using stable δ¹³C and δ¹⁵N isotope analysis. Water column and sediment particulate organic matter (POM) were used as primary food <b>web</b> <b>end</b> members. Isotopic niche space (δ¹³C – δ¹⁵N) occupied by benthic taxa provided measures of community-wide trophic redundancy and separation. Water column and sediment POM δ¹³C values generally reflected terrestrial POM inputs in the eastern and central shallow (14 - 90 m) Beaufort regions, but were indicative of persistent marine influence in the western and central deep (100 - 220 m) regions. Food web structure, as reflected by consumer trophic levels (TLs), trophic redundancy, and trophic niche space, supported the POM findings. In the eastern and central shallow regions, consumers occupied mainly lower TL (TL= 1 - 3), whereas consumers in the western regions occupied intermediate and higher TL (TL= 3 - 4). Overall trophic redundancy and niche space occupied by food webs in these four regions, however, was similar. The central deep Beaufort food web was unique in all metrics evaluated, and the comparatively largest isotopic niche space, with high trophic niche separation but low trophic redundancy, suggests that this region may be most vulnerable to perturbations. Snow crabs occupied food webs in the central deep and western shallow and deep Beaufort regions, where they maintained a consistent TL of 4. 0 across regions. I then investigated snow crab dietary habits across the Chukchi, and the Alaskan and Canadian Beaufort seas in the size range of 40 to 130 mm CW using stomach contents and stable isotope analyses. Snow crabs consumed four main prey taxa: polychaetes, decapod crustaceans (crabs, amphipods), echinoderms (mainly ophiuroids), and mollusks (bivalves, gastropods). Crab diets in the southern and northern Chukchi Sea regions were similar to those in the western Beaufort Sea in that bivalve, amphipod, and crustacean consumption was highest. The Canadian Beaufort region was most unique in prey composition and in stable isotope values. Cannibalism on snow crabs was higher in the Chukchi Sea regions relative to the Beaufort Sea regions, suggesting that cannibalism may have an impact on recruitment in the Chukchi Sea via reduction of cohort strength after settlement to the benthos, as known from the Canadian Atlantic. Based on a comparison with southern Chukchi Sea macrofauna data, these results document the non-selective, omnivorous role of snow crabs across the entire Pacific Arctic, as well as providing first evidence for cannibalism in the Chukchi Sea. Finally, I generated new estimates of stock biomass, abundance, and maximum sustainable yield, length-weight relationships, size-at-maturity, and fecundity of snow crab in the Alaskan Arctic. Although snow crabs were more abundant in the Chukchi Sea, no crabs larger than the minimum marketable size (> 100 mm carapace width, based on Bering Sea metric) occurred in this region. Harvestable biomass of snow crabs only occurred in the Beaufort Sea, but was considerably lower than previous estimates in the Arctic FMP. Length-weight relationships were generally similar for male and female snow crabs between the Chukchi and Beaufort seas. Size-at-maturity and female fecundity in the Chukchi Sea were similar to snow crabs occurring in other geographic regions; low sample sizes in the Beaufort prevented size-at-maturity and fecundity analysis. Together these results contribute new understanding of Arctic snow crab population dynamics by utilizing a rich dataset obtained recently from the Chukchi and Beaufort regions...|$|E
40|$|Many filed {{inspections}} and steel bridge surveys report that steel plate girder bridges {{are more susceptible}} to corrode at the end due to the accumulation of the water at bearing region. This water pools at the bearing is the result of rain shower and water leakage from the construction joints, which cause the corrosion of bearing stiffener and nearby web. The statistical data shows that only in Japan in more than 70 % corroded steel bridges, corrosion was found near the girder ends, which is primarily due to very aggressive environmental and severe topographical conditions of Japan (Chapter 2). Corrosion at plate girder ends damages the bearing stiffener and nearby web locally and in most of the cases rest of the girder part remains intact. Bearing stiffeners have a very important role in resisting the compressive load. Moreover, besides resisting the compressive load bearing stiffeners are also designed as anchor to balance the horizontal component of tension field action developed in the exterior web and resist the moment transferred by the in plane bending. Thus, any significant damage on bearing stiffener and nearby web may cause the buckling/crushing near the bearing and may also cause a significant loss in the bearing and shear capacity. Further, some field inspection data regarding the steel bridge damages was also collected from the Pakistan (Chapter 3) to assess the feasibility of required action to deal with these steel bridges. The inspection data also depicts the similar plate girder ends damages as reported in Japan. As number old of steel brides are increasing day by day in Japan and all around the world and {{it is very difficult to}} replace the all old brides at the same time along with the construction of the new bridges due to many constraints, i. e. economy of the country, traffic switching problem, lack of expertise and resources, manpower and machinery etc. Moreover, many bridges may have the adequate capacity to be used (Chapter 1). Therefore, all the corroded and old bridges demand the proper evaluation to assess their current performance level e. g. residual bearing and shear capacity so that suitable measure regarding the repair, retrofitting or replacement of these steel bridges can be adopted. The present study focusesiion the experimental and analytical evaluation of bearing and shear capacity of corroded bridges affected by the end panel corrosion. The chapter 4 discusses the experimental program conducted on the large scale plate girder ends specimens, simulated with local corrosion damage on the bearing stiffener. In all, total five plate girder end specimens were used. On one specimen no damage is considered and regarded as the healthy specimen and on the rest of two pair, 50 % thickness of the bearing stiffener is reduced within the damage height “Dh” of 20 mm and 60 mm uniformly and non-uniformly starting from the top face of the bottom flange near the weld seam. The study reported that 20 mm damage height does not change either the ultimate capacity or buckling failure mode. However, a relatively large damage of 60 mm shift the buckling of the stiffener within the damage zones along with the normal buckling within the normal web buckling. The chapter 5 deals with the extension of study to evaluate the bearing capacity of the plate girder ends analytically. For this purpose the test results obtained through the experimental program (conducted and mention in chapter 4) are modeled in a powerful finite element (FE) software ABAQUS. The comparison of the experimental and analytical results are made in term of the load-displacement relationships and deformed buckling modes and analytical results show the good agreement. Four various damage groups, Stiffener damage only, stiffener damage plus the free <b>web</b> <b>end</b> damage, stiffener damage plus interior web damage and stiffener damage plus the web damage at both side of the bearing stiffener were considered in the analytical program. Various residual thicknesses i. e. 75 %, 50 % and 25 etc. were assumed within the different damage heights zones. As the study is targeted to study the effect of the local damage, the maximum damage height is considered as 100 mm (10 % of girder height). True stress-strain relationships are determined by performing the tension test on the coupon cut from the used specimens with isotropic strain hardening rule are utilized in the analytical study. Initial deflections and welding residual stresses are also considered in the finite element (FE) study. Many trial analyses with different damage shapes and forms suggest that a simple uniform rectangular type of the damage with minimum thickness can be utilized in the FE simulation. Further, analytical results conclude that bearing stiffener has very important role in resisting the bearing load and any severe damage may reduce its radiusiiiof gyration along the week axis which may give rise to the stability problem by causing the buckling/ crippling failure at bearing. The results also revealed that the web damage alone does not affect the load carrying capacity significantly however; it reduces the load carrying capacity substantially when web damage is combined with the stiffener damage. The Chapter 6 describes the effect of local corrosion damages at the girder end on its shear capacity. An extensive analytical study is conducted on a four panel plate girder model and similar damage cases are assumed as considered (in Chapter 5) for the bearing capacity analysis. A mild steel with yield stresses equal to 345 Mpa with elastics perfectly plastics (EPP) characteristics under kinematics plasticity rule is used. A small initial out-of-plane deflection is also incorporated while; no residual stresses are considered in the study as they have no effect on the shear capacity. Analytical results indicate that a loss of the bearing stiffener more than 50 % can be fatal for the plate girder and it reduces the anchorage for the tension field action and thus refrains the girder to achieve its full post buckling strength. Also, complete loss of stiffener thickness shifts the failure mode of the girder from shear to buckling. The Chapter 7 comprises of some proposed empirical relationships to estimate ultimate capacity under the shear and compressive loads application. This section also discusses a proposed damage parameter named the “Reduced Thickness Ratio” to assess the buckling failure mode of the plate girder specimen under compressive loadFinally, Chapter 8 consists of a summary of conclusions and recommendations that are drawn from the above mentioned work. Subsequently some recommendations are also made for future research...|$|E
50|$|The Paris Opera Awards (POA) is an {{international}} contest for opera singers held in Paris, France. It is held under the patronage of the French representation at the European Commission and it starts on the <b>web</b> and <b>ends</b> with a final gala evening in Paris.|$|R
50|$|KnowledgeBench is {{compatible}} with industry standard Java application server platforms, including Apache Tomcat, IBM WebSphere, Oracle WebLogic Server, Jetty and other Java EE platforms. The <b>web</b> front <b>end</b> can be hosted by the application server or fronted by an Apache web server or Microsoft's IIS web server.|$|R
40|$|This paper {{discusses}} how the Weather Forecast Office Philadelphia/Mount Holly NJ deploys {{and presents}} selected audio {{products from the}} NOAA Weather Radio broadcast suite to <b>web</b> site <b>end</b> users. It provides process definition and discusses procedures {{that can be used}} by other forecast offices to deploy a similar site...|$|R
5000|$|After {{a decade}} of {{publishing}} art comics on the <b>web,</b> Serializer <b>ended</b> in 2012, saying [...] "Thank you to all the editors, creators, subscribers, and free readers who made serializer.net such a fantastic experiment over the years. Our time has come and gone, but we carry on ... elsewhere!" ...|$|R
5000|$|Virtual Office - an [...] "out of the box" [...] <b>web</b> portal for <b>end</b> users {{providing}} {{access to}} e-mail, personal file storage, company address book, etc.|$|R
50|$|Following the {{announcement}} of the nominees on October 28, online voting opened on the official MAMA website via PC and mobile <b>web.</b> The voting <b>ended</b> on December 1, 2016.|$|R
50|$|Cow and Boy is a webcomic {{created by}} Mark Leiknes. It was {{formerly}} distributed as a print comic by United Features Syndicate, running from January 2, 2006 to December 31, 2012; it {{then moved to}} http://www.cowandboy.com as a self-published webcomic supported by voluntary $12/year contributions from readers. Following an announcement from Leiknes, the <b>web</b> version <b>ended</b> on December 31, 2013.|$|R
40|$|Flux compactifications {{of string}} theory exhibiting the {{possibility}} of discretely tuning the cosmological constant to small values have been constructed. The highly tuned vacua in this discretuum have curvature radii which scale as large powers of the flux quantum numbers, exponential {{in the number of}} cycles in the compactification. By the arguments of Susskind/Witten (in the AdS case) and Gibbons/Hawking (in the dS case), we expect correspondingly large entropies associated with these vacua. If they are to provide a dual description of these vacua on their Coulomb branch, branes traded for the flux need to account for this entropy at the appropriate energy scale. In this note, we argue that simple string junctions and <b>webs</b> <b>ending</b> on the branes can account for this large entropy, obtaining a rough estimate for junction entropy that agrees with the existing rough estimates for the spacing of the discretuum. In particular, the brane entropy can account for the (A) dS entropy far away from string scale correspondence limits. ...|$|R
50|$|June, 1997: Quepasa is {{incorporated}} as Internet Century Inc. The business is initially operated {{as a high}} <b>end</b> <b>web</b> applications developer, with offices in Las Vegas, Nevada and Los Angeles, California.|$|R
5000|$|Microsoft Vizact 2000: A {{program that}} [...] "activated" [...] {{documents}} using HTML, adding effects such as animation. It {{allows users to}} create dynamic documents for the <b>Web.</b> Development has <b>ended</b> due to unpopularity.|$|R
40|$|Academic {{libraries}} {{function as}} the resource for retrieving information related to the end users requirements. Since academic libraries are now competing for the end users attention and interest, the current dispute faced is to deliver prompt, instant, seamless right of entry to resources and information to stay on relevant in the fast growing information technology era. Web OPAC demonstrate as a remote retrieval and function as question-answering, richly interactive information discovery and retrieval system to support decision making, that has no fundamental boundaries on the type and formats of data and information it can find, access, recover, exhibit, and distribute. There are numerous studies conducted to measure <b>Web</b> OPAC <b>end</b> user satisfaction, however the measurement criteria is unstructured and varies depending on the scholars interest. The objective {{of this study is}} to develop <b>Web</b> OPAC <b>end</b> user satisfaction criteria. In order to achieve the objective, this study has explored the criteria used in Library Science and Information System end user satisfaction as recommended by previous researchers. <b>Web</b> OPAC <b>end</b> user satisfaction criteria has been refined which integrated End User Computing Satisfaction (EUCS) criteria with Library Science end user satisfaction criteria. The criteria are evaluated by using survey and instrument verification which involved experience end users, senior management and system owner of UTM Library. This study could assist the Information Technology managers, system owners and the library management in determining the satisfactory level of an information system in an academic library. An academic library will be able to identify which criteria to address in order to have an ideal system that could satisfy the end user...|$|R
50|$|Ampache's goal is {{to allow}} access to a person's music from {{anywhere}} in the world. It is written specifically for private/small group implementations, but does allow an admin to enable public registration. Ampache's primary objective is to maintain a simple, secure and fast <b>web</b> front <b>end</b> that will run on almost any hardware and any platform that supports PHP. It is also written to accommodate large music collections.|$|R
40|$|Anchor text has {{a history}} of {{enriching}} documents for a variety of tasks within the World Wide Web. Anchor texts are useful because they are similar to typical Web queries, and because they express the document’s context. Therefore, it is a common practice for Web search engines to incorporate incoming anchor text into the document’s standard textual representation. However, this approach will not suffice for documents with very few inlinks, and it does not incorporate the document’s full context. To mediate these problems, we employ link paths, which contain anchor texts from paths through the <b>Web</b> <b>ending</b> at the document in question. We propose and study several different ways to aggregate anchor text from link paths, and we show that the information from link paths can be used to (1) improve known item search in site-specific search, and (2) map Web pages to database records. We rigorously evaluate our proposed approach on several real world test collections. We find that our approach significantly improves performance over baseline and existing techniques in both tasks...|$|R
5000|$|Using the Phalanger compiler, Jadu CMS (or {{any other}} PHP based applications) {{and the front}} <b>end</b> <b>web</b> {{templates}} are compiled down to two DLLs. - bringing together two fiercely competitive programming disciplines together - making PHP interoperable with the [...]NET framework.|$|R
40|$|This paper {{presents}} the provenance storing system prOOst {{which uses a}} semi-structured approach to store the provenance data based on the Open Provenance Model (OPM). It uses the graph database “Neo 4 j” for storage and the graph traversal language “Gremlin” for querying. Furthermore, it provides a REST interface to record data into the store, and a <b>web</b> front <b>end</b> to query the database. The prOOst provenance system was published as Open Source software and is available on SourceForge...|$|R
5000|$|According to The Domains, [...] "Those {{now using}} a <b>web</b> address <b>ending</b> in [...]club include brands, celebrities, sports figures, {{innovative}} entrepreneurs and startups, associations, and clubs around the globe... {{tens of thousands}} of clubs, business and individuals are actively using a [...]club address for their web presence, from Rotary Clubs, to school clubs, to passionate bloggers." [...] Prominent individuals using the extension include rapper 50 Cent, professional basketball player Tyler Johnson, and Indian cricket star Virat Kohli.|$|R
