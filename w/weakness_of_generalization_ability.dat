0|10000|Public
40|$|This work {{concentrate}} {{on a novel}} method for empirical estimation <b>of</b> <b>generalization</b> <b>ability</b> <b>of</b> neural networks. Given a set of training (and testing) data, one can choose a network architecture (number of layers, number of neurons in each layer etc.), an initialization method, and a learning algorithm to obtain a network. One measure of performance of the trained network is how closely its actual output approximates the desired output for an input that it has never seen before. Current methods provide a "number" that indicates the estimation <b>of</b> <b>generalization</b> <b>ability</b> <b>of</b> the network. However, this number provides no further information to understand the contributing factors when <b>generalization</b> <b>ability</b> is not very good. The proposed method uses a number of parameters to define <b>generalization</b> <b>ability.</b> A set <b>of</b> values of these parameters provides an estimate <b>of</b> <b>generalization</b> <b>ability.</b> In addition, a value of each parameter indicates the contribution of such factors as network architecture, initialization method, and training data set etc. Furthermore, a method has been developed to verify the validity of estimated values of the parameters...|$|R
30|$|The {{validation}} results {{listed in}} Table  9 {{indicate that the}} parallelized BPNNs based on the presented technique have a well-performance <b>of</b> <b>generalization</b> <b>ability</b> for the new data cases which {{are not included in}} the training data set.|$|R
40|$|In {{this paper}} we review the eld of {{reinforcement}} learning under the aspect <b>of</b> <b>generalization</b> <b>abilities.</b> We develop a framework describing the facets <b>of</b> <b>generalization</b> in reinforcement learning and formulate some consequences like {{the introduction of}} observations, among others. We show the principles of learning in this framework and give an exemplary observation-based learning algorithm...|$|R
5000|$|Theory <b>of</b> {{controlling}} the <b>generalization</b> <b>ability</b> <b>of</b> learning processes ...|$|R
40|$|In {{this paper}} {{we use a}} {{multilayer}} neural network (MNN) constructed by a Monte Carlo based algorithm to forecast time series events. Experiments are carried out on two benchmark problems in time series forecasting literature. Our result shows a comparative performance with other prediction methods. In our approach, a multilayer neural network with high level <b>of</b> <b>generalization</b> <b>ability</b> is obtained without sensible choice of external parameters...|$|R
5000|$|How can one {{control the}} rate <b>of</b> {{convergence}} (the <b>generalization</b> <b>ability)</b> <b>of</b> the learning process? ...|$|R
40|$|An {{improved}} kernel parameter optimization method {{based on}} Structural Risk Minimization (SRM) principle is proposed {{to enhance the}} <b>generalization</b> <b>ability</b> <b>of</b> traditional Kriging surrogate model. This article first analyses the importance <b>of</b> the <b>generalization</b> <b>ability</b> as an assessment criteria of surrogate model {{from the perspective of}} statistics and proves the applicability to Kriging. Kernel parameter optimization method is used to improve the fitting precision of Kriging model. With the smoothness measure <b>of</b> the <b>generalization</b> <b>ability</b> and the anisotropy kernel function, the modified Kriging surrogate model and its analysis process are established. Several benchmarks are tested to verify the effectiveness of the modified method under two different sampling states: uniform distribution and nonuniform distribution. The results show that the proposed Kriging has better <b>generalization</b> <b>ability</b> and adaptability, especially for nonuniform distribution sampling...|$|R
40|$|Abstract. It {{is widely}} {{believed}} in the pattern recognition field {{that the number of}} examples needed to achieve an acceptable level <b>of</b> <b>generalization</b> <b>ability</b> depends on the number of independent parameters needed to specify the network configuration. The paper presents a neural network for classification of high-dimensional patterns. The network architecture proposed here uses a layer which extracts the global features of patterns. The layer contains neurons whose weights are induced by a neural subnetwork. The method reduces the number of independent parameters describing the layer to the parameters describing the inducing subnetwork...|$|R
40|$|Abstract—There {{are many}} {{approaches}} to the face recognition. This paper presents an approach that combines advantage <b>of</b> <b>generalization</b> <b>ability</b> <b>of</b> Principal Component Analysis (PCA) and specialization ability of Case- based reasoning (CBR). CBR is expected to improve the <b>generalization</b> <b>ability</b> <b>of</b> PCA in the recognition process. By using PCA the new image is projected into its eigenface components. The projected vector represents a description component of a new face case. The CBR module compares the similarity of the new face case and previously stored face cases in the face casebase and retrieves the most similar case. The solution component of the retrieved case represents {{the results of the}} recognition process. Index Terms — case based reasoning, face recognition, eigenvalues, eigenvectors, principal component analysis. I...|$|R
40|$|Abstract. Typical {{learning}} classifier systems employ conjunctive logic {{rules for}} representing domain knowledge. The classifier XCS {{is an extension}} of LCS with the ability to learn boolean logic functions for data mining. However, most data mining problems cannot be expressed simply with boolean logic. Neural Logic Network (Neulonet) learning is a technique that emulates the complex human reasoning processes through the use of net rules. Each neulonet is analogous to a learning classifier that is rewarded using support and confidence measures which are often used in association-based classification. Empirical results shows promise in terms <b>of</b> <b>generalization</b> <b>ability</b> and the comprehensibility of rules...|$|R
40|$|To achieve {{compactness}} in {{the final}} weights is to be of high interest in terms <b>of</b> <b>generalization</b> <b>ability</b> <b>of</b> neural networks. In literature, techniques such as weight elimination, weight decay have been proposed and a tendency to decay to zero for the connections between neurons is subsequently employed {{in order to obtain}} a smaller network which improves the generalization capability. Within this paper a new method to achieve compactness {{in the final}} weights is proposed. The idea is based on discovering the link between the statistical characteristics of the input pattern vectors and that of the final weights...|$|R
40|$|The noise {{injection}} {{into the}} training samples {{has been shown}} to lead to improvement <b>of</b> the <b>generalization</b> <b>ability</b> <b>of</b> artificial neural network(ANN) classifiers. In this paper, we investigate the positive effect of the noise injection on the <b>generalization</b> <b>ability</b> <b>of</b> ANN classifiers in high dimensions. We further show that the noise injection technique is very useful in situations where the true Bayes error is small. ...|$|R
50|$|Fitting the {{training}} set too closely {{can lead to}} degradation <b>of</b> the model's <b>generalization</b> <b>ability.</b> Several so-called regularization techniques reduce this overfitting effect by constraining the fitting procedure.|$|R
30|$|The SCF {{credit risk}} {{assessment}} model based on SVM is <b>of</b> good <b>generalization</b> <b>ability</b> and robustness, {{which is more}} effective than BP neural network assessment model. Hence, Banks can raise the accuracy of credit risk assessment on SMEs by applying the SVM model, which can alleviate credit rationing on SMEs.|$|R
40|$|Though phrase-based SMT has {{achieved}} high translation quality, it still lacks <b>of</b> <b>generalization</b> <b>ability</b> to capture word order differences between languages. In this paper we describe a general method for tree-to-string phrasebased SMT. We study how syntactic transformation is incorporated into phrase-based SMT and its effectiveness. We design syntactic transformation models using unlexicalized form of synchronous context-free grammars. These models {{can be learned}} from sourceparsed bitext. Our system can naturally make use of both constituent and non-constituent phrasal translations in the decoding phase. We considered various levels of syntactic analysis ranging from chunking to full parsing. Our experimental results of English-Japanese and English-Vietnamese translation showed a significant improvement over two baseline phrase-based SMT systems. ...|$|R
30|$|One {{demonstration}} {{is sufficient}} to reproduce the task {{as far as the}} task condition is constant. However, our method has no advantage over conventional teaching/playback in such a constant condition. Thus we should perform two or more demonstrations in different conditions to adapt to changes of task conditions with the help <b>of</b> the <b>generalization</b> <b>ability</b> <b>of</b> neural networks.|$|R
30|$|The {{experimental}} {{validation of}} the formulated model is demonstrated in this section. Firstly, a short description of the datasets used for evaluation is provided. Then, the quantitative evaluation of the proposed HDLA model in terms <b>of</b> its <b>generalization</b> <b>ability</b> is carried out. Finally, the retrieval efficiency of the latent topic-based image representation obtained with the proposed HDLA model is compared with state-of-the-art approaches.|$|R
40|$|By making {{assumptions}} on {{the probability}} {{distribution of the}} potentials in a feed-forward neural network we have derived lower bounds for the <b>generalization</b> <b>ability</b> <b>of</b> the network {{in terms of the}} number of training patterns. The results are consistent with simulations carried out on a simple geometrical function. I INTRODUCTION An important property of artificial neural networks is their ability to generalize. If a network has been trained to implement a function, to within a given tolerance, on a set of examples, it is useful to have a quantitative measure of the degree to which the network will implement the function on a previously unseen example. For networks with binary weights a sound theoretical framework for generalization has been developed by Schwartz et. al. [1] and tested on the contiguity problem. These networks have also been studied by Mato and Parga [2] with particular reference to the dependance <b>of</b> <b>generalization</b> <b>ability</b> on the number of hidden layers. The question [...] ...|$|R
40|$|Abstract — Statistical shape {{models have}} wide {{applications}} in medical image analysis both for image segmentation and morphometry. In this paper, inspired by Minimum Description Length (MDL), {{we developed a}} novel algorithm for automatic landmark building using an Entropy-based cost-function. The results are tested on four different datasets (metacarpal bones, heads of femur, silhouettes of facial profiles, and hand outlines), and compared with the original MDL constructed using the “one fixed master shape ” model approach. It {{can be seen from}} our preliminary results that, the new Minimum Entropy Model (MEM) conveys better than the MDL technique on the measures <b>of</b> <b>Generalization</b> <b>Ability,</b> Specificity and Similar Compactness. It also shows good potential in solving the so-called “run away” problem in MDL...|$|R
40|$|The parity {{function}} {{is one of}} the most used Boolean function for testing learning algorithms because both of its simple definition and its great complexity. Being one of the hardest problems, many different architectures have been constructed to compute parity, essentially by adding neurons in the hidden layer in order to reduce the number of local minima where gradient-descent learning algorithms could get stuck. We construct a family of modular architectures that implement the parity function in which, every member of the family can be characterized by the fan-in max of the network, i. e., the maximum number of connections that a neuron can receive. We analyze the <b>generalization</b> <b>ability</b> <b>of</b> the modular networks first by computing analytically the minimum number of examples needed for perfect generalization and second by numerical simulations. Both results show that the <b>generalization</b> <b>ability</b> <b>of</b> these networks is systematically improved by the degree of modularity of the network. We also analyze the influence of the selection of examples in the emergence <b>of</b> <b>generalization</b> <b>ability,</b> by comparing the learning curves obtained through a random selection of examples to those obtained through examples selected accordingly to a general algorithm we recently proposed...|$|R
40|$|In Neural Network (NN) training, local minimum is an {{integrated}} problem. In this paper, {{a modification of}} standard backpropagation (BP) algorithm, called backpropagation with vector chaotic learning rate (BPVL) is proposed to improve the performance of NNs. BPVL method generates a chaotic time series as Vector form of Mackey Glass and logistic map. A rescaled version of these series is used as learning rate (LR). In BP training the weights of NN become inactive, after arrival of local minima in the training session. Using integrated chaotic learning rate, the weight update accelerated in the local minimum region. BPVL is tested on six real world benchmark classification problems such as breast cancer, diabetes, heart disease, australian credit card, horse and glass. The proposed BPVL outperforms the existing BP and BPCL in terms <b>of</b> <b>generalization</b> <b>ability</b> and also convergence rate...|$|R
40|$|Representation {{learning}} {{has become an}} invaluable approach for learning from symbolic data such as text and graphs. However, while complex symbolic datasets often exhibit a latent hierarchical structure, state-of-the-art methods typically learn embeddings in Euclidean vector spaces, which do not account for this property. For this purpose, we introduce a new approach for learning hierarchical representations of symbolic data by embedding them into hyperbolic space [...] or more precisely into an n-dimensional Poincaré ball. Due to the underlying hyperbolic geometry, this allows us to learn parsimonious representations of symbolic data by simultaneously capturing hierarchy and similarity. We introduce an efficient algorithm to learn the embeddings based on Riemannian optimization and show experimentally that Poincaré embeddings outperform Euclidean embeddings significantly on data with latent hierarchies, {{both in terms of}} representation capacity and in terms <b>of</b> <b>generalization</b> <b>ability...</b>|$|R
40|$|In this paper, {{we study}} the {{performance}} of extremum estimators from the perspective <b>of</b> <b>generalization</b> <b>ability</b> (GA) : {{the ability of a}} model to predict outcomes in new samples from the same population. By adapting the classical concentration inequalities, we derive upper bounds on the empirical out-of-sample prediction errors {{as a function of the}} in-sample errors, in-sample data size, heaviness in the tails of the error distribution, and model complexity. We show that the error bounds may be used for tuning key estimation hyper-parameters, such as the number of folds K in cross-validation. We also show how K affects the bias-variance trade-off for cross-validation. We demonstrate that the L_ 2 -norm difference between penalized and the corresponding un-penalized regression estimates is directly explained by the GA of the estimates and the GA of empirical moment conditions. Lastly, we prove that all penalized regression estimates are L_ 2 -consistent for both the n ≥ p and the n < p cases. Simulations are used to demonstrate key results. Keywords: <b>generalization</b> <b>ability,</b> upper bound <b>of</b> <b>generalization</b> error, penalized regression, cross-validation, bias-variance trade-off, L_ 2 difference between penalized and unpenalized regression, lasso, high-dimensional data. Comment: The theoretical <b>generalization</b> and extension <b>of</b> arXiv: 1606. 0014...|$|R
40|$|Model {{selection}} {{is difficult to}} analyse yet theoretically and empirically important, especially for high-dimensional data analysis. Recently the least absolute shrinkage and selection operator (Lasso) has been applied in the statistical and econometric literature. Consis- tency of Lasso has been established under various conditions, {{some of which are}} difficult to verify in practice. In this paper, we study model selection from the perspective <b>of</b> <b>generalization</b> <b>ability,</b> under the framework of structural risk minimization (SRM) and Vapnik-Chervonenkis (VC) theory. The approach emphasizes the balance between the in-sample and out-of-sample fit, which can be achieved by using cross-validation to select a penalty on model complexity. We show that an exact relationship exists between the <b>generalization</b> <b>ability</b> <b>of</b> a model and model selection consistency. By implementing SRM and the VC inequality, we show that Lasso is L 2 -consistent for model selection under assumptions similar to those imposed on OLS. Furthermore, we derive a probabilistic bound for the distance between the penalized extremum estimator and the extremum estimator without penalty, which is dominated by overfitting. We also propose a new measurement of overfitting, GR 2, based on <b>generalization</b> <b>ability,</b> that converges to zero if model {{selection is}} consistent. Using simulations, we demonstrate that the proposed CV-Lasso algorithm performs well in terms of model selection and overfitting control...|$|R
40|$|Abstract—The parity {{function}} {{is one of}} the most used Boolean function for testing learning algorithms because both of its simple definition and its great complexity. Being one of the hardest problems, many different architectures have been constructed to compute parity, essentially by adding neurons in the hidden layer in order to reduce the number of local minima where gradient-descent learning algorithms could get stuck. We construct a family of modular architectures that implement the parity function in which, every member of the family can be characterized by the fan-in max of the network, i. e., the maximum number of connections that a neuron can receive. We analyze the <b>generalization</b> <b>ability</b> <b>of</b> the modular networks first by computing analytically the minimum number of examples needed for perfect generalization and second by numerical simulations. Both results show that the <b>generalization</b> <b>ability</b> <b>of</b> these networks is systematically improved by the degree of modularity of the network. We also analyze the influence of the selection of examples in the emergence <b>of</b> <b>generalization</b> <b>ability,</b> by comparing the learning curves obtained through a random selection of examples to those obtained through examples selected accordingly to a general algorithm we recently proposed. Index Terms—Circuit complexity, generalization, learning from examples, modular neural networks, parity function. I...|$|R
40|$|The {{well-known}} bounds on the generalizationability {{of learning}} machines, {{based on the}} Vapnik–Chernovenkis (VC) dimension,are very loose when applied to Support Vector Machines (SVMs). In this work we evaluate {{the validity of the}} assumption that these bounds are,nevertheless, good indicators <b>of</b> the <b>generalization</b> <b>ability</b> <b>of</b> SVMs. We show that this assumption is, in general, true and assess its correctness, in a statistical sense, on several pattern recognition benchmarks through the use of the bootstrap technique...|$|R
40|$|Ensemble {{learning}} aims at combining several {{slightly different}} learners to construct stronger learner. Ensemble of a well selected subset of learners would outperform than ensemble of all. However, the well studied accuracy / diversity ensemble pruning framework {{would lead to}} over fit of training data, which results a target learner <b>of</b> relatively low <b>generalization</b> <b>ability.</b> We propose to ensemble with base learners trained by both labeled and unlabeled data, by adopting data dependant kernel mapping, which has been proved successful in semi-supervised learning, to get more generalized base learners. We bootstrap both training data and unlabeled data, namely point cloud, to build slight different data set, then construct data dependant kernel. With such kernels data point can be mapped to different feature space which results effective ensemble. We also proof that ensemble of learners trained by both labeled and unlabeled data is <b>of</b> better <b>generalization</b> <b>ability</b> in the meaning of graph Laplacian. Experiments on UCI data repository show {{the effectiveness of the}} proposed method...|$|R
40|$|Abstract—In eural etwork () training, local minimum is an {{integrated}} problem. In this paper, {{a modification of}} standard backpropagation (BP) algorithm, called backpropagation with vector chaotic learning rate (BPVL) is proposed to improve the performance of s. BPVL method generates a chaotic time series as Vector form of Mackey Glass and logistic map. A rescaled version of these series is used as learning rate (LR). In BP training the weights of become inactive, after arrival of local minima in the training session. Using integrated chaotic learning rate, the weight update accelerated in the local minimum region. BPVL is tested on six real world benchmark classification problems such as breast cancer, diabetes, heart disease, australian credit card, horse and glass. The proposed BPVL outperforms the existing BP and BPCL in terms <b>of</b> <b>generalization</b> <b>ability</b> and also convergence rate. Keywords—eural network; backpropagation; BPCL; BPVL chaos; generalization ability; convergence rate. I...|$|R
30|$|Convolution {{neural network}} (CNN) is an {{artificial}} neural network developed {{on the basis of}} human learning ability for knowledge and computer network. The convolution neural network has more applicability for deep learning. The feature extraction and feature classification of image characters can be carried out simultaneously, and it also has the advantages <b>of</b> strong <b>generalization</b> <b>ability</b> and less training parameters for global optimization. It is one of the pioneering research achievements in the field of machine autonomous learning.|$|R
40|$|In {{the last}} decades {{ensemble}} learning has established itself as a valuable strategy within the computational intelligence modeling and machine learning community. Ensemble learning is a paradigm where multiple models combine in some way their decisions, or their learning algorithms, or different data to improve the prediction performance. Ensemble learning aims at improving the <b>generalization</b> <b>ability</b> and {{the reliability of the}} system. Key factors of ensemble systems are diversity, training and combining ensemble members to improve the ensemble system performance. Since there is no unified procedure to address all these issues, this work proposes and compares Genetic Algorithm and Simulated Annealing based approaches for the automatic development of Neural Network Ensembles for regression problems. The main contribution of this work is the development of optimization techniques that select the best subset of models to be aggregated taking into account all the key factors of ensemble systems (e. g., diversity, training ensemble members and combination strategy). Experiments on two well-known data sets are reported {{to evaluate the effectiveness of}} the proposed methodologies. Results show that these outperform other approaches including Simple Bagging, Negative Correlation Learning (NCL), AdaBoost and GASEN in terms <b>of</b> <b>generalization</b> <b>ability...</b>|$|R
40|$|We {{consider}} {{the problem of}} detecting mitotic figures in breast cancer histology slides. We investigate whether the performance of state-of-the-art detection algorithms {{is comparable to the}} performance of humans, when they are compared under fair conditions: our test sub-jects were not previously exposed to the task, and were required to learn their own classification criteria solely by studying the same training set available to algorithms. We designed and implemented a standardized web-based test based on the publicly-available MITOS dataset, and compared results with the performance of the 6 top-scoring algorithms in the ICPR 2012 Mitosis Detection Contest. The problem is presented as a classification task on a balanced dataset. 45 different test subjects produced a total of 3009 classifications. The best individual (accuracy = 0. 859 ± 0. 012), is outperformed by the most accurate algorithm (accuracy = 0. 873 ± 0. 004). This suggests that state-of-the-art detection algorithms are likely limited {{by the size of the}} training set, rather than by lack <b>of</b> <b>generalization</b> <b>ability...</b>|$|R
40|$|The paper {{presents}} {{the design of}} municipal creditworthiness parameters. Further, the design of model for municipal creditworthiness classification is presented. The realized data pre-processing makes the suitable economic interpretation of results possible. Municipalities are assigned to clusters by unsupervised methods. The combination of Kohonen’s self-organizing feature maps and K-means algorithm is a suitable method for municipal creditworthiness modelling. The number of classes in this model is determined by indexes {{evaluating the quality of}} clustering. The model is composed of Kohonen’s self-organizing feature maps and fuzzy logic neural networks, where the output of Kohonen’s self-organizing feature maps re{{presents the}} input of fuzzy logic neural networks. The suitability of the designed model is compared with other fuzzy logic based classifiers. The highest classification accuracy is achieved by the fuzzy logic neural network. The fuzzy logic neural network’s model is the best in terms <b>of</b> <b>generalization</b> <b>ability</b> due to this fact. By the designed model high classification accuracy, low average classification error and suitable interpretation of results are achieved. ...|$|R
30|$|The {{proposed}} {{approach has}} merits for online application as well. The deficiency <b>of</b> weak <b>generalization</b> <b>ability</b> for many kernel-based intelligent algorithms such as support vector machines requires large datasets and long training times {{to ensure the}} modeling performance, which is computationally expensive and ineffective for online rolling situations. Such a problem can be significantly mitigated by the recursive and sparse features of KRLS. Simulation {{results show that the}} elapsed time is distinctly saved on the premise of modification effect by choosing a proper sparse parameter.|$|R
40|$|MIT’s Lincoln Labs {{to study}} {{intrusion}} detection systems, {{the performance of}} robust support vector machines (RVSMs) was {{compared with that of}} conventional support vector machines and nearest neighbor classifiers in separating normal usage profiles from intrusive profiles of computer programs. The results indicate the superiority of RSVMs {{not only in terms of}} high intrusion detection accuracy and low false positives but also in terms <b>of</b> their <b>generalization</b> <b>ability</b> in the presence of noise and running time. Keywords—Intrusion detection, computer security, robust support vector machines, noisy data. I...|$|R
40|$|This paper {{introduces}} a new classifier design method {{that is based}} on a modification of the classical Ho-Kashyap procedure. The proposed method uses the absolute error, rather than the squared error, to design a linear classifier. Additionally, easy control <b>of</b> the <b>generalization</b> <b>ability</b> and robustness to outliers are obtained. Next, an extension to a nonlinear classifier by the mixture-of-experts technique is presented. Each expert is represented by a fuzzy if-then rule in the Takagi-Sugeno-Kang form. Finally, examples are given to demonstrate the validity of the introduced method...|$|R
40|$|Using the 1998 DARPA BSM {{data set}} {{collected}} at MIT's Lincoln Labs to study intrusion detection systems, {{the performance of}} robust support vector machines (RSVMs) was {{compared with that of}} conventional support vector machines and nearest neighbor classifiers in separating normal usage profiles from intrusive profiles of computer programs. The results indicate the superiority of RSVMs {{not only in terms of}} high intrusion detection accuracy and low false positives but also in terms <b>of</b> their <b>generalization</b> <b>ability</b> in the presence of noise and running time...|$|R
