15|53|Public
40|$|This paper {{describes}} a software tool {{that allows us}} to teach students the principles and concepts of Fuzzy Information Retrieval Systems based on weighted queries. This tool is used in the course Information Retrieval Systems Based on Artificial Intelligence at the Faculty of Library and Information Science at the University of Granada. With this teaching tool students learn the management of the fuzzy <b>weighted</b> <b>query</b> languages which could be used in any conventional Web search engine to improve the representation of user information needs...|$|E
40|$|This {{document}} {{contains a}} description of experiments for the 2008 Relevance Feedback track. We experiment with different amounts of feedback, including negative relevance feedback. Feedback is implemented using massive <b>weighted</b> <b>query</b> expansion. Parsimonious query expansion using Dirichlet smoothing performs best on this relevance feedback track dataset. Additional blind feedback gives better results, except when the blind feedback set is {{of the same size}} as the explicit feedback set. On a number of topics topical feedback is applied, which turns out to be mainly beneficial for early precision...|$|E
40|$|Abstract. This paper {{analyzes}} {{the effect of}} various similarity measures namely inner product for un-weighted query terms, inner product for <b>weighted</b> <b>query</b> terms, cosine of the angle between query and document vectors and Euclidean distance. The study was motivated {{by the fact that}} many researchers especially in Malay document retrievals tend to use simple method of calculating similarity measure that is based on inner product for un-weighted query terms. This paper shows that Euclidean distance outperforms other similarity measures significantly. The results suggest that Euclidean distance should be used to improve performance of document retrieval systems...|$|E
40|$|This paper {{describes}} a computer-supported learning system {{to teach students}} the principles and concepts of Fuzzy Information Retrieval Systems based on <b>weighted</b> <b>queries.</b> This tool is used to support the teacher’s activity in the degree course Information Retrieval Systems Based on Artificial Intelligence at the Faculty of Library and Information Sciences at the University of Granada. Learning of languages of <b>weighted</b> <b>queries</b> in Fuzzy Information Retrieval Systems is complex because {{it is very difficult}} to understand the different semantics that could be associated to the weights of queries together with their respective strategies of query evaluation. We have developed and implemented this computer-supported education system because it allows to support the teacher’s activity in the classroom to teach the use of <b>weighted</b> <b>queries</b> in FIRSs and it helps students to develo...|$|R
40|$|AbstractPath queries {{have been}} {{extensively}} used to query semistructured data, {{such as the}} Web and XML documents. In this paper we introduce <b>weighted</b> path <b>queries,</b> an extension of path queries enabling several classes of optimization problems (such as the computation of shortest paths) to be easily expressed. <b>Weighted</b> path <b>queries</b> {{are based on the}} notion of weighted regular expression, i. e., a regular expression whose symbols are associated to a weight. We characterize the problem of answering <b>weighted</b> path <b>queries</b> and provide an algorithm for computing their answer. We also show how <b>weighted</b> path <b>queries</b> can be effectively embedded into query languages for XML data to express in a simple and compact form several meaningful research problems...|$|R
40|$|In {{this paper}} a new {{modelling}} for a weighted Information Retrieval System (IRS) in a linguistic context is proposed. This linguistic IRS (LIRS) achieves more precise and consistent relevance degrees that early weighted IRSs proposed [8, 9]. To do this, a new redefinition of matching function defined in [11] is used. Keywords: Fuzzy Information Retrieval, Linguistic Modelling, <b>Weighted</b> <b>Queries...</b>|$|R
40|$|The {{goal of the}} TREC conference Video Retrieval Track is the {{investigation}} of content-based retrieval from digital video. We participated at the conference with a query and browsing system. For search we developed a <b>weighted</b> <b>query</b> mechanism by integrating 1) text (OCR and speech recognition) content using full text and n-grams through the MG system, 2) color correlogram indexing of shots and images reported last year in TREC, and 3) ranked versions of the extracted binary features. The command line version of the interface allows users to make various queries, store them and use weighted combinations to generate a compound query...|$|E
40|$|We {{hypothesized}} that language modeling retrieval would improve if we reduced {{the need for}} document smoothing to provide an inverse document frequency (IDF) like effect. We created inverse collection frequency (ICF) <b>weighted</b> <b>query</b> models {{as a tool to}} partially separate the IDF-like role from document smoothing. Compared to maximum likelihood estimated (MLE) queries, the ICF weighted queries achieved a 6. 4 % improvement in mean average precision on description queries. The ICF weighted queries performed better with less document smoothing than that required by MLE queries. Language modeling retrieval may benefit from a means to separately incorporate an IDF-like behavior outside of document smoothing...|$|E
40|$|Mika Rautiainen MediaTeam Oulu University of 011111 P. O. BOX 4500, Finland mika. rautiainen(ee. ouhl. fi Our {{team from}} the University of Maryland and INSA de Lyon participated in the feature {{extraction}} evaluation with overlay text features and in the search evaluation with a query retrieval and browsing system. For search we developed a <b>weighted</b> <b>query</b> mechanism by integrating 1) text (OCR and speech recognition) content using full text and n-gl'ams through the MG system, 2) color correlograln indexing of ilnage and video shots reported last year in TREC, and 3) ranked versions of the extracted binary features. A command line version of the interface allows users to formulate simple qneries, store them and use weighted Colnbinations of the simple queries to generate coinpound queries...|$|E
40|$|The common way {{to search}} large indexed {{databases}} is through conjunctive queries on binary relations, {{such as those}} associating keywords to web page references. We extend this model by adding positive weights {{to the terms of}} the query. In this context we give and analyze two algorithms to answer such queries in a pertinent way. We extend this approach to solve <b>weighted</b> <b>queries</b> on tree-structured objects such as file-systems or XML documents...|$|R
40|$|International audienceIn {{this paper}} we propose a way to cope with {{questions}} typed by dyslexic users as they are usually a deformation of the intended query that cannot be corrected with classical spellcheckers. We first propose a new model for statistic question answering systems based on a probabilistic information retrieval model and a combination of results. This model allows a multiple <b>weighted</b> terms <b>query</b> as an input. We also introduce a phonology based approach at the sentence level to derive possible intended terms from typed questions. This approach uses the finite state machine framework to go from phonetic hypothesis and spellchecker proposals to hypothesized sentences thanks to a language model. The final <b>weighted</b> <b>queries</b> are obtained thanks to posterior probabilities computation. They are evaluated according to new density and appearance rating measures which adapt recall and precision to non binary data...|$|R
40|$|PFSQL is {{the query}} {{language}} used for querying fuzzy relational databases. Oneof {{the most distinguished}} features of PFSQL is the possibility to prioritize conditions. Priorities are most often confused with weights. In this paper we compare queries withprioritized conditions with queries with weighed conditions. Since PFCSP systems are thetheoretical background for PFSQL and similarly WFCSP are the theoretical backgroundfor <b>weighted</b> <b>queries</b> we elaborate these two systems. Queries with thresholds are anotherfeature of PFSQL. When a threshold {{is attached to a}} condition only the tuples that satisfythe condition with a higher value then the threshold are displayed in the result. Throughexamples we compare these features of PFSQL...|$|R
40|$|Most image {{retrieval}} systems only allow {{a fragment}} of text or an example image as a query. Most users have more complex information needs that are not easily expressed in either of these forms. This paper proposes a model based on the Inference Network framework from information retrieval that employs a powerful query language that allows structured query operators, term weighting, and the combination of text and images within a query. The model uses non-parametric methods to estimate probabilities within the inference network. Image annotation and retrieval results are reported and compared against other published systems and illustrative structured and <b>weighted</b> <b>query</b> results are given to show {{the power of the}} query language. The resulting system both performs well and is robust compared to existing approaches...|$|E
40|$|We compare a user-de®ned passage {{feedback}} (pf) {{system to}} a document feedback (df) system. Df employed the adaptive linear model for retrieval, while pf used <b>weighted</b> <b>query</b> expansion based on positive and negative feedback. Twenty-four searchers performed the same six tasks in varying search and systemorder per TREC- 8 guidelines. We hypothesized that pf, which featured interactive query expansion, would outperform df, which relied on automatic query expansion. Initial analysis appeared to reject this hypothesis, as df showed slightly higher overall performance than pf. However, analysis by system-order groups indicates only the ®rst pf use had lower performance. These data suggest that pf was more di cult to learn than df, though the second pf use yielded competitive performance. If performance of pf is indeed a€ected by learning, an improved pf system with usability enhancements {{may prove to be}} an e€ectiv...|$|E
40|$|In our TREC- 8 {{interactive}} experiment, {{we compared}} {{the effectiveness of a}} user-defined passage feedback system with a conventional document feedback system. The passage feedback system (pf) employed user-defined passages to formulate a feedback query, while the document feedback (df) system utilized document-level relevance judgments. The df system employed the adaptive linear model for retrieval, and pf used <b>weighted</b> <b>query</b> expansion based on positive and negative feedback. Twenty-four searchers performed the same 6 search tasks (3 per system) in varying search and system order in accordance with TREC- 8 guidelines. We hypothesized that the pf system, which featured interactive query expansion by user-selected passages, would have an advantage over the df system, which relied on automatic query expansion based on relevance judgments of whole documents. Our initial analysis of experiment results appeared to reject this hypothesis, as the document feedback system showed slightly higher ove [...] ...|$|E
40|$|An Information Retrieval (IR) model defined {{using an}} ordinal fuzzy {{linguistic}} approach is proposed. It accepts ordinal linguistic <b>weighted</b> <b>queries</b> {{based on two}} weighting elements: the query terms and the query sub-expressions. In such a way, users may easily express simultaneously several semantic restrictions in a query. A symmetrical threshold semantic is associated to the weights of the query terms and an importance semantic is associated to the weights of the query sub-expressions. The advantage of this IR model with respect to others is the facility for expressing different semantic restrictions on the desired documents simultaneously, incorporating more flexibility in the user-IR system interaction a...|$|R
40|$|Although {{the fuzzy}} {{retrieval}} model constitutes a powerful {{extension of the}} boolean one, being {{able to deal with}} the imprecision and subjectivity existing in the Information Retrieval process, users are not usually able to express their query requirements {{in the form of an}} extended boolean query including weights. To solve this problem, different tools to assist the user in the query formulation have been proposed. In this paper, the genetic algorithm-programming technique is considered to build an algorithm of this kind that will be able to automatically learn <b>weighted</b> <b>queries</b> [...] modeling the user's needs [...] for a fuzzy information retrieval system by applying an off-line adaptive process starting from a set of relevant documents...|$|R
40|$|Information {{retrieval}} is {{an activity}} that implies to achieve documents that better fulfil the user information needs. For achieving this activity an Information Retrieval System uses matching functions which specify the degree of relevance of a document {{with respect to a}} user <b>query.</b> Assuming linguistic <b>weighted</b> <b>queries</b> we present a new linguistic matching function for a threshold weighting semantics which is defined using a 2 -tuple fuzzy linguistic approach [1]. This new 2 -tuple linguistic matching function can be interpreted as a tuning of that defined in [2] using an ordinal linguistic approach. We show that it simplifies the processes of computing in the retrieval activity, avoids the loss of precision in final results, and consequently, can help to improve the users’ satisfaction...|$|R
40|$|Most {{information}} retrieval systems based on linguistic approaches use symmetrically and uniformly distributed linguistic term sets {{to express the}} weights of queries and the relevance degrees of documents. However, to improve the system-user interaction it seems more adequate to express these linguistic weights and degrees by means of unbalanced linguistic scales, i. e., linguistic term sets with different discrimination levels {{on both sides of}} mid linguistic term. In this contribution we present an {{information retrieval}} system which accepts weighted queries whose weights are expressed using unbalanced linguistic term sets. Then, system provides the retrieved documents classified in linguistic relevance classes assessed on unbalanced linguistic term sets. To do so, we propose a methodology to manage unbalanced linguistic information and we use the linguistic 2 -tuple model as representation base of the unbalanced linguistic information. Additionally, the linguistic 2 -tuple model allows us {{to increase the number of}} relevance classes in the output and also to improve the performance of information retrieval system. Index Terms Information retrieval systems, <b>weighted</b> <b>query,</b> unbalanced linguistic term set, computing with words. I...|$|E
40|$|Our {{team from}} the University of Maryland and INSA de Lyon participated in the feature {{extraction}} evaluation with overlay text features and in the search evaluation with a query retrieval and browsing system. For search we developed a <b>weighted</b> <b>query</b> mechanism by integrating 1) text (OCR and speech recognition) content using full text and n-grams through the MG system, 2) color correlogram indexing of image and video shots reported last year in TREC, and 3) ranked versions of the extracted binary features. A command line version of the interface allows users to formulate simple queries, store them and use weighted combinations of the simple queries to generate compound queries. One novel component of our interactive approach is the ability for the users to formulate dynamic queries previously developed for database applications at Maryland. The interactive interface treats each video clip as visual object in a multi-dimensional space, and each ”feature ” of that clip is mapped to one dimension. The user can visualize any two dimensions by placing any two features on the horizontal and vertical axis with additional dimensions visualized by adding attributes to each object. ...|$|E
40|$|Abstract—In many {{advanced}} applications, {{data are}} described by multiple high-dimensional features. Moreover, different queries may weight these features differently; some {{may not even}} specify all the features. In this paper, we propose our solution to support efficient query processing in these applications. We devise a novel representation that compactly captures f features into two components: The first component is a 2 D vector that reflects a distance range (minimum and maximum values) of the f features {{with respect to a}} reference point (the center of the space) in a metric space and the second component is a bit signature, with two bits per dimension, obtained by analyzing each feature’s descending energy histogram. This representation enables two levels of filtering: The first component prunes away points that do not share similar distance ranges, while the bit signature filters away points based on the dimensions of the relevant features. Moreover, the representation facilitates the use of a single index structure to further speed up processing. We employ the classical B þ-tree for this purpose. We also propose a KNN search algorithm that exploits the access orders of critical dimensions of highly selective features and partial distances to prune the search space more effectively. Our extensive experiments on both real-life and synthetic data sets show that the proposed solution offers significant performance advantages over sequential scan and retrieval methods using single and multiple VA-files. Index Terms—Multifeature, indexing, query processing, high-dimensional, <b>weighted</b> <b>query.</b> ...|$|E
40|$|Abstract: An {{information}} retrieval system (IRS) based on fuzzy multi-granular linguistic information is proposed. The system has an evaluation method to process multi-granular linguistic information, {{in such a}} way that the inputs to the IRS are represented in a different linguistic domain than the outputs. The system accepts Boolean queries whose terms are weighted by means of the ordinal linguistic values represented by the linguistic variable "Importance " assessed on a label set S. The system evaluates the <b>weighted</b> <b>queries</b> according to a threshold semantic and obtains the linguistic retrieval status values (RSV) of documents represented by a linguistic variable "Relevance " expressed in a different label set S'. The advantage of this linguistic IRS with respect to others is that the use of the multi-granular linguistic information facilitates and improves the IRS-user interaction 1...|$|R
40|$|In digital {{libraries}} image retrieval queries can {{be based on}} the similarity of objects, using several feature attributes like shape, texture, color or text. Such multi-feature queries return a ranked result set instead of exact matches. Besides, the user wants to see only the k top-ranked objects. We present a new algorithm called Quick-Combine (European patent pending, nr. EP 00102651. 7) for combining multi-feature result lists, guaranteeing the correct retrieval of the k top-ranked results. For score aggregation virtually any combining function can be used, including <b>weighted</b> <b>queries.</b> Compared to Fagin's algorithm we have developed an improved termination condition in tuned combination with a heuristic control flow adopting itself narrowly to the particular score distribution. Top-ranked results can be computed and output incrementally. We show that we can dramatically improve performance, in particular for non-uniform score distributions. Benchmarks on practical [...] ...|$|R
40|$|In now day’s data {{retrieval}} {{is the main}} focusing term in web data extraction. The process of XML data extraction in real time using search engines like Google, Ask, Bing and Yahoo etc. The RDBMS has some central methods to perform searching mechanism in real time data sets, but RDBMS is not suitable for XML data extraction. XQuery path language is the main methodology for Lowest Common Ancestors for implementing fuzzy type operations with XML data extraction. Fuzzy relational data extraction is very expensive of minimal cost. In fuzzy relational data extraction retrieving data becomes very complex while multiple number of attributes queried processing on XML data. In this paper, we propose to suggest FMADM (Fuzzy Multi Attribute Decision Making) for extracting multi attribute XML data. We plan for results with multi attribute features in central method issues with <b>weighted</b> <b>queries.</b> Our experimental results show efficient XML data extraction in real time data sets...|$|R
40|$|Semantic {{search is}} a {{research}} {{area in which the}} goal is to understand the users intended meaning of the query. This requires disambiguation of the user query and interpreting the semantics of the query. Semantic search would thus improve the users search experience through more precise result sets. Moreover, ontologies are explicit conceptualizations of domains, defining concepts, their properties, and the relations among them. This makes ontologies semantic representations of specific domains, suitable to use as a basis for semantic search applications. In this thesis we explore how such a semantic search system based on ontologies may be constructed. The system is built as a query reformulation module that uses an underlying search engine based on Lucene. We employ text mining techniques to semantically enrich an ontology by building feature vectors for the concepts of the ontology. The feature vectors are tailored to a specific document collection and domain, reflecting the vocabulary in the document collection and the domain. We propose four query reformulation strategies for evaluation. The interpretation and expansion of the user query is based on the ontology and the feature vectors. Finally the reformulated query is fired as a <b>weighted</b> <b>query</b> into the Lucene search engine. The evaluation of the implemented prototype reveals that search is in general improved by our reformulation approaches. It is however difficult to give any definite conclusion to which query types benefit the most from our approach, and which reformulation strategy improves the search result the most. All four of the reformulation strategies seem to on average perform quite equally. </p...|$|E
40|$|A {{desirable}} {{feature of}} a database system {{is its ability}} to reason with probabilistic information. This dissertation proposes a model for the process of reasoning with probabilistic information in databases. A probabilistic data model has been chosen as the framework for this study and the information theoretical aspects of the Maximum Entropy Formalism as the inference engine. This formalism, although semantically interesting, offers major complexity problems. Probabilistic data models generally assume some knowledge of the uncertainty space, and the Maximum Entropy Formalism finds the least commitment probability distribution within the uncertainty space. This dissertation is an investigation of how successfully the entropy principle could be applied to probabilistic data. The Boolean logic and <b>weighted</b> <b>queries</b> when applied to probabilistic databases have certain pros and cons. A query logic based on the combined advantages of both the Boolean logic and <b>weighted</b> <b>queries</b> would be a desirable alternative. The proposed model based on the Maximum Entropy Formalism meets the foregoing desiderata of making the query language more expressive. Probabilistic data models generally deal with tuple-level probabilities whereas the proposed model has the ability to handle attribute-level probabilities. This model also has the ability to guess the unknown joint probability distributions. Three techniques to compute the probability distributions were studied in this dissertation: (1) a brute-force, iterative algorithm, (2) a heuristic algorithm, and (3) a simulation approach. The performance characteristics of these algorithms were examined and conclusions were drawn about the potentials and limitations of the proposed model in probabilistic database applications. Traditionally, the probabilistic solution proposed by the Maximum Entropy Formalism is arrived at by solving nonlinear simultaneous equations for the aggregate factors of the nonlinear terms. The proposed heuristic approach and simulation technique were shown to have the highly desirable property of tractability. Further research is needed to improve the accuracy of the heuristic and make it more attractive. Although the proposed model and algorithms are applicable to tables with a few probabilistic attributes, say two or three, the complexity of extending the model to a large number of probabilistic attributes still remains unsolved as it falls in the realm of NP-hard problems...|$|R
40|$|Consider a {{universe}} of tokens, {{each of which is}} associated with a weight, and a database consisting of strings that can be represented as subsets of these tokens. Given a query string, also represented as a set of tokens, a <b>weighted</b> string similarity <b>query</b> identifies all strings in the database whose similarity to the query is larger than a user specified threshold. <b>Weighted</b> string similarity <b>queries</b> are useful in applications like data cleaning and integration for finding approximate matches in the presence of typographical mistakes, multiple formatting conventions, data transformation errors, etc. We show that this problem has semantic properties that can be exploited to design index structures that support very efficient algorithms for query answering. ...|$|R
40|$|Keyword {{queries are}} {{extremely}} {{easy for a}} user to write. They have become a standard way to query for information in web search engines and most other information retrieval systems whose users are usually laypersons and might not have knowledge about the database schema or contained data. As keyword queries do not impose any structural constraints on the retrieved information, {{the quality of the}} obtained results is far from perfect. However, one can hardly improve it without changing the ways the queries are asked and the methods the information is stored in the database. The purpose of this thesis is to propose a method {{to improve the quality of}} the information retrieving by adding weights to the existing ways of keyword queries asking and information storing in the database. We consider <b>weighted</b> <b>queries</b> on two different data structures: weighted binary relations and weighted multi-labeled trees. We propose adaptive algorithms to solve these queries and prove the measures of the complexity of these algorithms in terms of the high-level operations. We describe how these algorithms can be implemented and derive the upper bounds on their complexity in two specific models of computations: the comparison model and the word-RAM model...|$|R
40|$|We {{implement}} a recent theoretical proposal to represent inverted lists in memory, {{in a way}} that docid-sorted and weight-sorted lists are simultaneously represented in a single wavelet tree data structure. We compare our implementation with classical representations, where the ordering favors either bag-of-word queries or Boolean and <b>weighted</b> conjunctive <b>queries,</b> and demonstrate that the new data structure is faster than {{the state of the art}} for conjunctive queries, while it offers an attractive space/time tradeoff when both kinds of queries are of interest...|$|R
40|$|AbstractAn {{important}} question in IRSs {{is how to}} facilitate the IRS-user interaction, even more when {{the complexity of the}} fuzzy query language makes difficult to formulate user queries. The use of linguistic variables to represent the input and output information in the retrieval process of IRSs significantly improves the IRS-user interaction. In the activity of an IRS, there are aspects of different nature to be assessed, e. g., the relevance of documents, the importance of query terms, etc. Therefore, these aspects should be assessed with different uncertainty degrees, i. e., using several label sets with different granularity of uncertainty. In this contribution, an IRS based on fuzzy multi-granular linguistic information and a method to process the multi-granular linguistic information are proposed. The system accepts Boolean queries whose terms can be simultaneously weighted by means of ordinal linguistic values according to three semantics: a symmetrical threshold semantics, a relative importance semantics and a quantitative semantics. In the three semantics, the linguistic weights are represented by the linguistic variable “Importance”, but assessed on different label sets S 1, S 2 and S 3, respectively. The IRS evaluates <b>weighted</b> <b>queries</b> and obtains the linguistic retrieval status values of documents represented by the linguistic variable “Relevance” which is expressed on a different label set S′...|$|R
40|$|Given two spatial {{datasets}} P (e. g., facilities) and Q (queries), {{an aggregate}} nearest neighbor (ANN) query retrieves the point(s) of P {{with the smallest}} aggregate distance(s) to points in Q. Assuming, for example, n users at locations q 1, [...] . qn, an ANN query outputs the facility p belongs to P that minimizes the sum of distances |pqi| for 1 is {{less than or equal}} to i is {{less than or equal to}} n that the users have to travel in order to meet there. Similarly, another ANN query may report the point p belongs to P that minimizes the maximum distance that any user has to travel, or the minimum distance from some user to his/her closest facility. If Q fits in memory and P is indexed by an R-tree, we develop algorithms for aggregate nearest neighbors that capture several versions of the problem, including <b>weighted</b> <b>queries</b> and incremental reporting of results. Then, we analyze their performance and propose cost models for query optimization. Finally, we extend our techniques for disk-resident queries and approximate ANN retrieval. The efficiency of the algorithms and the accuracy of the cost models are evaluated through extensive experiments with real and synthetic datasets...|$|R
40|$|Abstract. We present {{two methods}} for {{estimating}} replacement probabilities without using parallel corpora. The first method proposed exploits the possi-ble translation probabilities latent in Machine Readable Dictionaries (MRD). The second method is more robust, and exploits context similarity-based techniques {{in order to}} estimate word translation probabilities using the Inter-net as a bilingual comparable corpus. The experiments show a statistically significant improvement over non <b>weighted</b> structured <b>queries</b> in terms of MAP by using the replacement probabilities obtained with the proposed methods. The context similarity-based method {{is the one that}} yields the most significant improvement...|$|R
40|$|Web-based tagging systems, {{which include}} social {{bookmarking}} {{systems such as}} Delicious, have become increasingly popular. These systems allow participants to annotate or tag web resources. This research examined the use of social annotations {{to improve the quality}} of web searches. The research involved three components. First, social annotations were used to index resources. Two annotation-based indexing methods were proposed: annotation based indexing and full text with annotation indexing. Second, social annotations were used to improve search result ranking. Six annotation based ranking methods were proposed: Popularity Count, Propagate Popularity Count, <b>Query</b> <b>Weighted</b> Popularity Count, <b>Query</b> <b>Weighted</b> Propagate Popularity Count, Match Tag Count and Normalized Match Tag Count. Third, social annotations were used to both index and rank resources. The result from the first experiment suggested that both static feature and similarity feature should be considered when using social annotations to re-rank search result. The result of the second experiment showed that using only annotation as an index of resources may not be a good idea. Since social Annotations could be viewed as a high level concept of the content, combining them to the content of resource could add some more important concepts to the resources. Last but not least, the result from the third experiment confirmed that the combination of using social annotations to rank the search result and using social annotations as resource index augmentation provided a promising rank of search results. It showed that social annotations could benefit web search...|$|R
40|$|Abstract—For {{securing}} databases outsourced to the cloud, it {{is important}} to allow cloud users to verify that their queries to the cloud-hosted databases are correctly executed by the cloud. Existing solutions on this issue suffer from a high communication cost, a heavy storage overhead or an overwhelming computational cost on clients. Besides, only simple SQL queries (e. g., selection <b>query,</b> projection <b>query,</b> <b>weighted</b> sum <b>query,</b> etc) are supported in existing solutions. For practical considerations, it is desirable to design a client-verifiable (or publicly verifiable) aggregation query scheme that supports more flexible queries with affordable storage overhead, communication and computational cost for users. This paper investigates this challenging problem and proposes an efficient publicly verifiable aggregation query scheme for databases outsourced to the cloud. By designing a renewable polynomial-based authentication tag, our scheme supports a wide range of practical SQL queries including polynomial queries of any degrees, variance query and many other linear queries. Remarkably, our proposed scheme only introduces constant communication and computational cost to cloud users. Our scheme is provably secure under the Static Diffie-Hellman problem, the t-Strong Diffie-Hellman problem and the Computational Diffie-Hellman problem. We show the efficiency and scalability of our scheme through extensive numerical analysis. I...|$|R
40|$|It is {{well known}} that the problem of {{computing}} relative approximations of <b>weighted</b> counting <b>queries</b> such as the probability of evidence in a Bayesian network, the partition function of a Markov network, and the number of solutions of a constraint satisfaction problem is NP-hard. In this paper, we settle therefore on an easier problem of computing high-confidence lower bounds. We propose to use importance sampling and Markov inequality for solving it. However, a straight-forward application of the Markov inequality often yields poor lower bounds. We therefore propose several new schemes for improving its performance in practice. Empirically, we show that our new schemes are quite powerful, often yielding substantially higher (better) lower bounds than all state-of-the-art schemes. ...|$|R
40|$|This paper {{presents}} {{the application of}} context vector similarity {{for the purpose of}} word sense discrimination during query translation. The random indexing vector space method is used to accumulate the context vectors. Pair wise similarity of the context vectors of ambiguous terms with that of anchor terms indicated the possible correct translation of a query term. Two retrieval experiments were conducted using the discriminated <b>queries</b> and <b>weighted</b> maximally expanded <b>queries.</b> The discriminated queries show a substantial increase in retrieval performance...|$|R
