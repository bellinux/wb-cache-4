4|2|Public
50|$|In a CPU cache, a {{write buffer}} {{can be used}} to hold data being written from the cache to main memory or to the next cache in the memory hierarchy. This is a {{variation}} of <b>write-through</b> <b>caching</b> called buffered write-through.|$|E
5000|$|The MicroVAX 3500 and MicroVAX 3600, code named [...] "Mayfair", were {{introduced}} in September 1987 and {{were meant to be}} the higher end complement of the MicroVAX family. These new machines featured {{more than three times the}} performance of the MicroVAX II and supported 32 MB of ECC main memory (twice that of the MicroVAX II). The performance improvements over the MicroVAX II resulted from the increased clock rate of the CVAX chip set, which operated at 11.11 MHz (90 ns cycle time) along with a two-level, <b>write-through</b> <b>caching</b> architecture. It used the KA650 CPU module.|$|E
40|$|As {{the size}} of cloud systems {{and the number of}} hosted VMs rapidly grow, the {{scalability}} of shared VM storage systems becomes a serious issue. Client-side flash-based caching has the potential to improve the performance of cloud VM stor-age by employing flash storage available on the client-side of the storage system to exploit the locality inherent in VM IOs. However, because of the limited capacity and durabil-ity of flash storage, it is important to determine the proper size and configuration of the flash caches used in cloud sys-tems. This paper provides answers to the key design ques-tions of cloud flash caching based on dm-cache, a block-level caching solution customized for cloud environments, and a large amount of long-term traces collected from real-world public and private clouds. The study first validates that cloud workloads have good cacheability and dm-cache-based flash caching incurs low overhead with respect to commod-ity flash devices. It further reveals that write-back caching substantially outperforms <b>write-through</b> <b>caching</b> in typical cloud environments due to the reduction of server IO load. It also shows that there is a tradeoff on making a flash cache persistent across client restarts which saves hours of cache warm-up time but incurs considerable overhead from com-mitting every metadata update persistently. Finally, to re-duce the data loss risk from using write-back caching, the pa-per proposes a new cache-optimized RAID technique, which minimizes the RAID overhead by introducing redundancy of cache dirty data only, and shows to be significantly faster than traditional RAID and <b>write-through</b> <b>caching.</b> 1...|$|E
50|$|The current {{version of}} CAS for Linux {{supports}} write-through, write-back, and write-around caching. The Windows versions of CAS support <b>write-through</b> and write-back <b>caching.</b>|$|R
40|$|This paper investigates issues {{involving}} writes and caches. First, tmdeoffs on writes that miss in the cache are inves-tigated. In particular, whether the missed cache block is fetched on a write miss, whether the missed cache block is allocated in the cache, {{and whether the}} cache line is written before hit or miss is known are considered. Depending on {{the combination of these}} polices chosen, the entire cache miss rate can vary by a factor of two on some applications. The combination of no-fetch-on-write and write-allocate can provide better performance than cache line allocation instructions. Second, tradeoffs between <b>write-through</b> and write-back <b>caching</b> when writes hit in a cache are con-sidered. A mixture of these two altematives, called write caching is proposed. Write caching places a small fully-associative cache behind a write-through cache. A write cache can eliminate almost as much write traffic as a write-back cache. 1...|$|R
40|$|Modern {{data centers}} are {{increasingly}} using shared storage solutions {{for ease of}} management. Data is cached on the client side on inexpensive and high-capacity flash devices, helping improve performance and reduce contention on the storage side. Currently, <b>write-through</b> <b>caching</b> is used because it ensures consistency and durability under client failures, but it offers poor performance for write-heavy workloads. In this work, we propose two write-back based caching policies, called write-back flush and write-back persist, that provide strong reliability guarantees, under two different client failure models. These policies rely on storage applications such as file systems and databases issuing write barriers to persist their data, because these barriers are the only reliable method for storing data durably on storage media. Our evaluation shows that these policies achieve performance close to write-back caching, while providing stronger guarantees than vanilla write-though caching. i...|$|E

