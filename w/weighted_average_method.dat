163|10000|Public
50|$|The GII is {{computed}} {{by taking}} a simple average of the scores in two sub-indices, the Innovation Input Index and Innovation Output Index, which are composed of five and two pillars respectively. Each of these pillars describe an attribute of innovation, and comprise up to five indicators, and their score is calculated by the <b>weighted</b> <b>average</b> <b>method.</b>|$|E
50|$|Each species {{trend is}} {{aggregated}} {{to produce an}} index for the terrestrial, marine and freshwater systems. This process uses a <b>weighted</b> <b>average</b> <b>method</b> which places most weight on the largest (most species-rich) groups within a biogeographic realm. This is done to counteract the uneven spatial and taxonomic distribution of data in the LPD. The three system indices are then averaged to produce the global LPI.|$|E
5000|$|Cumulative {{effect of}} changes in {{accounting}} policies (principles) {{is the difference between}} the book value of the affected assets (or liabilities) under the old policy (principle) and what the book value would have been if the new principle had been applied in the prior periods. For example, valuation of inventories using LIFO instead of <b>weighted</b> <b>average</b> <b>method.</b> The changes should be applied retrospectively and shown as adjustments to the beginning balance of affected components in Equity. All comparative financial statements should be restated. (IAS 8) ...|$|E
30|$|As can be seen, all the adopted <b>weighted</b> <b>averaging</b> <b>methods</b> give {{better results}} {{compared}} to the simple averaging (Sum) rule. Also, among the <b>weighted</b> <b>averaging</b> <b>methods,</b> a better performance is achieved using the LDA method.|$|R
50|$|Physics has {{for long}} {{employed}} a <b>weighted</b> <b>averages</b> <b>method</b> {{that is similar}} to meta-analysis.|$|R
3000|$|... ◦ Contour smoothing: {{in order}} to reduce the {{possible}} errors that appear due to unwanted movement of writer’s hand during writing we use an optimal local <b>weighted</b> <b>averaging</b> <b>method</b> [21] ensuring that these glitches are filtered out and only the strokes relevant for our analysis are kept. We opted for this algorithm as opposed to other less complex local <b>weighted</b> <b>averaging</b> <b>methods</b> because this method is known to provide more accurate estimations of contour point positions, tangent slopes, or deviation angles which are essential for our handwriting analysis task.|$|R
30|$|The above formula can be {{regarded}} as a general <b>weighted</b> <b>average</b> <b>method</b> to solve the deblurring problem.|$|E
30|$|To {{convert a}} fuzzified output {{into a single}} crisp value {{with respect to a}} fuzzy set the <b>weighted</b> <b>average</b> <b>method</b> [32] is adopted.|$|E
3000|$|... where μ(Zi) is the {{membership}} degree of the output fuzzy set, {{and the area}} center of gravity method is actually the <b>weighted</b> <b>average</b> <b>method.</b>|$|E
40|$|This paper {{compares the}} {{ordinary}} unweighted <b>average,</b> <b>weighted</b> <b>average,</b> and maximum likelihood methods for estimating a common bioactivity from multiple parallel line bioassays. Some of these or similar methods {{are also used}} in meta-analysis. Based on a simulation study, these methods are assessed by comparing coverage probabilities of the true relative bioactivity {{and the length of}} the confidence intervals computed for these methods. The ordinary unweighted <b>average</b> <b>method</b> outperforms all statistical methods by consistently giving the best coverage probability but with somewhat wider confidence intervals. The <b>weighted</b> <b>average</b> <b>methods</b> give good coverage and smaller confidence intervals when combining homogeneous bioactivities. For heterogeneous bioactivities, these methods work well when a liberal significance level for testing homogeneity of bioactivities is used. The maximum likelihood methods gave good coverage when homogeneous bioactivities were considered. Overall, the preferred methods are the ordinary unweighted <b>average</b> and two <b>weighted</b> <b>average</b> <b>methods</b> that were specifically developed for bioassays. Copyright (c) 2013 John Wiley & Sons, Ltd...|$|R
40|$|In several {{applications}} where binary contours {{are used}} to represent and classify patterns, smoothing must be performed to attenuate noise and quantization error. This is often implemented with local <b>weighted</b> <b>averaging</b> of contour point coordinates, because of the simplicity, low-cost and effectiveness of such methods. Invoking the `optimality' of the Gaussian filter, many authors will use Gaussian-derived weights. But generally these filters are not optimal, {{and there has been}} little theoretical investigation of local <b>weighted</b> <b>averaging</b> <b>methods</b> per se. This paper focusses on the direct derivation of optimal local <b>weighted</b> <b>averaging</b> <b>methods</b> tailored towards specific computational goals such as the accurate estimation of contour point positions, tangent slopes, or deviation angles. A new and simple digitization noise model is proposed to derive the best set of weights for different window sizes, for each computational task. Estimates of the fraction of the noise actually removed by these [...] ...|$|R
30|$|In this paper, a new <b>weighted</b> <b>averaging</b> {{combination}} <b>method</b> on {{basis of}} evidence distance and Deng entropy is {{brought up to}} manage conflict in sensor data fusion. The proposed method has three advantages. First, it adopts Deng entropy to measure the information volume and applies evidence distance in measuring conflict degree. The new method takes into consideration of not only evidences’ relationships but also evidences’ inner properties which is more reasonable. Besides, the proposed method preserves the desirable properties of the <b>weighted</b> <b>averaging</b> <b>method.</b> What’s more, the new method requires less information and is much more simple to make decision compared with other methods. Generally speaking, it is an efficient method to deal with conflict in sensor data fusion and helps a lot with proper identification in WSN.|$|R
40|$|Abstract. The {{method to}} {{evaluate}} spacial schemes for public facilities is <b>weighted</b> <b>average</b> <b>method,</b> which was deduced {{to evaluate the}} spacial schemes of primary schools in Yuter community, and select the best one. For this purpose, investment coefficient, distance coefficient and enrolment proportion were introduced as the variables. According to the addition rule and the multiplication rule, the optimal scheme is the layout with one school. This <b>weighted</b> <b>average</b> <b>method</b> is effective and easy to be implemented in evaluation of spacial layout...|$|E
40|$|This thesis {{defines the}} methods of {{determining}} the normal price of a house. Among the methods {{that lead to the}} detection of the normal price, the method includes finding material values, comparative method, the method of determination of the yield value method of averaging, which is connected to Naegeliho <b>weighted</b> <b>average</b> <b>method,</b> Bradáčova modification <b>weighted</b> <b>average</b> <b>method</b> and the rest. Further analyzes of the established method in practice and as a result these methods will be compared with each other...|$|E
30|$|To {{illustrate}} {{the validity and}} feasibility of the proposed method, a comparison between the new information fusion process using HFSs and the aggregation process using <b>weighted</b> <b>average</b> <b>method</b> is performed.|$|E
40|$|AbstractWe {{introduce}} a <b>weighted</b> <b>averaging</b> <b>method</b> {{for improving the}} inevitable error induced by the Gibbs phenomenon appearing in a spectral approximation for a discontinuous function. In the result we have a family of filters generalizing the well known Fejer filter. In addition, for high resolution recovery, we propose an adaptive filter which is competitive with the existing exponentially convergent adaptive filter. Several numerical examples are included to show {{the applicability of the}} method presented...|$|R
40|$|In this paper, {{we develop}} {{a method of}} multi-attribute {{decision-making}} with both weights and attribute ratings expressed by single valued neutrosophic sets(SVN-sets). The method is called linear <b>weighted</b> <b>averaging</b> <b>method</b> of SVN-sets. Then, we present a sensitivity analysis of attribute weights which give changing intervals of attribute weights in which the ranking order of the alternatives is required to remain unchanging. Finally, validity and applicability of the proposed method are illustrated with a real application...|$|R
3000|$|Step 4   Trapezoidal fuzzy {{membership}} {{functions are}} transformed within the [- 1, 1] interval by <b>weighted</b> <b>average</b> defuzzification <b>method</b> and adjacency matrix {{of the concepts}} is accepted as a weight matrix (W) of the HFCM.|$|R
30|$|The value {{obtained}} for the best alternative based on our proposal {{is greater than the}} {{value obtained}} based on the <b>weighted</b> <b>average</b> <b>method</b> because the proposal considers all the information provided by experts avoiding the loss of information.|$|E
30|$|It is {{reasonable}} to employ the Choquet integral {{in terms of the}} λ-fuzzy measure to aggregate the performance values instead of the <b>weighted</b> <b>average</b> <b>method,</b> since the Choquet integral does not assume the independence of one element from another.Choquet integral is defined as follows.|$|E
3000|$|The <b>weighted</b> <b>average</b> <b>method</b> {{is widely}} used to {{aggregate}} the experts’ opinion in the aggregation process of the group decision making problem. In the <b>weighted</b> <b>average</b> <b>method,</b> the weight is assigned to each expert. In this paper, we assume that three experts’ opinions are equally important, i.e., (1  /  3,  1  /  3,  1  /  3). Let Ẽ_ij be the aggregated information of the effective control scope. In {{order to make a}} validity comparison between the results of the two different methods, the c̅_ 1 ^h (a_i [...]) will be utilized to generate the effective control scope Ẽ_ij, where Ẽ_ij = 1 / 3 ∑ _h= 1 ^ 3 c̅_j ^h (a_i [...]). The results are shown in Table 9.|$|E
30|$|In this section, {{we explain}} our {{proposed}} method named weighted Squared TOPS (WS-TOPS). WS-TOPS applies {{the following two}} approaches to Squared TOPS to improve DOA estimation performance. One is the modified squared matrix method, which is the algorithm to suppress the false peaks in the spatial spectrum of Squared TOPS. The other is the selective <b>weighted</b> <b>averaging</b> <b>method,</b> which is the algorithm to improve the DOA estimation accuracy by using the signal subspaces of multiple frequency bands. The details of these algorithms are shown in the following subsections.|$|R
30|$|The {{evidence}} {{distance is}} first proposed by Jousselme et al. (2001) and then applied to <b>weighted</b> <b>averaging</b> combination <b>method</b> by Yong et al. (2004). It can measure the conflict degree among evidences effectually. The concept of evidence distance is given below.|$|R
30|$|When the {{geometric}} relationship between images is determined, {{in order to}} keep the visual color consistent and maintain a smooth visual transition, this paper uses a <b>weighted</b> <b>average</b> fusion <b>method</b> based on Gaussian model to stack multiple images into a panoramic image.|$|R
30|$|According to Eq. (9), {{we can get}} the {{weighted}} averaging evidence by the <b>weighted</b> <b>average</b> <b>method</b> of multi-source evidence after obtaining the weight of each evidence. Finally, the new evidence is combined for (n- 1) times by Dempster’s combination rule and the fusion result can be obtained.|$|E
40|$|Abstract: According to the {{characteristics}} of the Chinese e-government development, the paper presents the customer satisfaction indicators about e-government measurement. It takes Yongchuan District in Chongqing for example and designs the questionnaire to collect sample data. It goes on an empirical analysis of customer satisfaction using <b>weighted</b> <b>average</b> <b>method.</b> The weights ar...|$|E
30|$|Trust {{calculation}} (inference) method {{refers to}} the method of calculating (speculating) the trust degree of the subject based on the collected evaluation data. The calculation (speculation) method {{does not depend on}} the trust value. The common calculation methods in the literature are the <b>weighted</b> <b>average</b> <b>method,</b> Bayesian method, fuzzy reasoning method, and gray reasoning method.|$|E
30|$|The {{fractional}} reaction–subdiffusion {{equation is}} one of the most famous subdiffusion equations. These equations are widely used in recent years to simulate many physical phenomena. In this paper, we consider a new version of such equations, namely the variable order linear and nonlinear reaction–subdiffusion equation. A numerical study is introduced using the <b>weighted</b> <b>average</b> <b>methods</b> for the variable order linear and nonlinear reaction–subdiffusion equations. A stability analysis of the proposed method is given by a recently proposed procedure similar to the standard John von Neumann stability analysis. The paper is ended with the results of numerical examples that support the theoretical analysis.|$|R
40|$|Zusammenfassung (dt.) Abstract: This thesis {{deals with}} the problem of {{modelling}} memristor and memristive networks. Memristors offer a variety of possible applications, starting at storage devices up to neuromorphic applications. A memristor based sorting circuit is proposed in this thesis. The simulation of the network starts from the basic elements, namely the comparators. The functionality and behaviour is modelled using a non-linear dynamic memristor model. Sorting {{is one of the key}} subroutines required by Induced Ordered <b>Weighted</b> <b>Averages</b> <b>method</b> (IOWA). IOWA can be used as a speech recognition method, which is an ongoing research area at our department...|$|R
40|$|We {{introduce}} a generalized sigmoidal transformation wm(r;x) {{on a given}} interval [a,b] with a threshold at x=r∈(a,b). Using wm(r;x), we develop a <b>weighted</b> <b>averaging</b> <b>method</b> {{in order to improve}} Fourier partial sum approximation for a function having a jump-discontinuity. The method is based on the decomposition of the target function into the left-hand and the right-hand part extensions. The resultant approximate function is composed of the Fourier partial sums of each part extension. The pointwise convergence of the presented method and its availability for resolving Gibbs phenomenon are proved. The efficiency of the method is shown by some numerical examples...|$|R
30|$|Shom’s {{datasets}} were resampled every 200  m along profiles by a <b>weighted</b> <b>average</b> <b>method.</b> To compare {{marine data}} with models defined up to SH degree 720 or subsampled to 0.5 °, the highest frequencies (<[*] 55  km) are filtered with an order 8 Butterworth filter after removing the average tendency and padding the data.|$|E
40|$|Thermal {{boundary}} conditions has played {{an increasingly important}} role in revealing the nature of short-range spin glasses {{and is likely to}} be relevant also for other disordered systems. Diffusion method initializing each replica with a random boundary condition at the infinite temperature using population annealing has been used in recent large-scale simulations. However, the efficiency of this method can be greatly suppressed because of temperature chaos. For example, most samples have some {{boundary conditions}} that are completely eliminated from the population in the process of annealing at low temperatures. In this work, I study a <b>weighted</b> <b>average</b> <b>method</b> to solve this problem by simulating each boundary conditions separately and collect data using weighted averages. The efficiency of the two methods are studied using both population annealing and parallel tempering, showing that the <b>weighted</b> <b>average</b> <b>method</b> is more efficient and accurate. Comment: 7 pages, 8 figure...|$|E
30|$|It can be {{seen that}} among the three gray-scale algorithms, the image {{obtained}} by the maximum value algorithm has the lowest brightness, the image of the average value algorithm has low brightness, and the image resolution is poor. However, compared with the maximum method, the <b>weighted</b> <b>average</b> <b>method</b> has no significant difference in image resolution, but the image brightness is relatively moderate.|$|E
40|$|Decision-making in {{emergency}} response situations can {{be supported by}} location-based information. However, current location-based services do not usually provide mechanisms to deal with conflicting information. To address this limitation, multi-criteria evaluation methods recently have been implemented in mobile geographic information systems. This paper describes an emergency response scenario that requires trade-offs between multiple decision criteria. The role of an emergency operations centre providing first responders with decision options is also discussed. An architecture and user interface for a mobile decision support system are then developed, in which multi-criteria decision strategies are implemented through the ordered <b>weighted</b> <b>averaging</b> <b>method...</b>|$|R
40|$|This article {{introduces}} a <b>method</b> for computing <b>weighted</b> <b>averages</b> on spheres based on least squares minimization that respects spherical distance. We prove existence and uniqueness {{properties of the}} <b>weighted</b> <b>averages,</b> and give fast iterative algorithms with linear and quadratic convergence rates. Our methods are appropriate to problems involving averages of spherical data in meteorological, geophysical, and astronomical applications. One simple application is a <b>method</b> for smooth <b>averaging</b> of quaternions, which generalizes Shoemake’s spherical linear interpolation. The <b>weighted</b> <b>averages</b> <b>methods</b> allow a novel method of defining Bézier and spline curves on spheres, which provides direct generalization of Bézier and B-spline curves to spherical spline curves. We present a fast algorithm for spline interpolation on spheres. Our spherical splines allow the use of arbitrary knot positions; potential applications of spherical splines include smooth quaternion curves for applications in graphics, animation, robotics, and motion planning...|$|R
30|$|To {{address the}} issue, Murphy (2000) {{proposed}} a different idea that averaging the belief function first and fusing the evidences next. Deng introduced the evidence distance and proposed {{to adopt the}} <b>weighted</b> <b>averaging</b> <b>method</b> to improve the Dempster combination rule (Yong et al. 2004; Han et al. 2007). Zhang introduced the vector space to measure the conflict degree by {{the distance of the}} space vectors (Zhang et al. 2014). This paper proposes a new method which introduces Deng entropy (Deng 2015 a) to measure the information volume of the evidence. The new method considers both information volume and conflict degree, which is more reasonable in conflict management.|$|R
