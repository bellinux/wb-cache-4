16|10000|Public
5000|$|PopMatters said [...] "The {{animated}} Jay Sherman...is {{perfect for}} Lovitz. He may never find anything in live action that serves him quite so well. I don’t mean {{that as a}} dig at his comic abilities (or his looks); it’s just that Sherman is an ideal outlet for the actor’s ham." [...] It said the cartoon format allows for [...] "his two biggest strengths as a performer: sarcasm and ironic overacting." [...] It added [...] "A marriage of voice-acting, writing, and animation that rivals {{some of the best}} Pixar work, Sherman is nonetheless hampered occasionally by the writers’ over-sampling from the Homer Simpson playbook, mainly gags concerning Jay’s girth and accompanying appetites. More effectively." [...] PopMatters said [...] "when it originally aired...the series was (for better or worse) slightly ahead of its time, outlandish in a way that The Simpsons would not adopt until later. Rewatching it now, The Critic seems most similar stylistically to the more recent series Family Guy, with its frequent cutaway gags...and blurring of fantasy and reality. The Simpsons introduced these qualities in moderation; The Critic and Family Guy are addicted to them, sometimes to a crippling degree." [...] It explained that [...] "The reference-heavy, media-saturated, sketch-like structure <b>works</b> <b>better</b> <b>for</b> <b>The</b> Critic than Family Guy, though, because the former is less in love with itself and its desire to shock or offend. Indeed, it’s more strange than twisted, Unlike Family Guy, it has a frame of reference beyond television, beyond even its self-created film niche." [...] It adds that [...] "The satire isn’t always as biting as it could be; many of the movie parodies eschew real critique in favor of non sequiturs or homage. Too often the writers rely on audience familiarity with popular movies. It’s amusing to be sure, but rarely as deadpan hilarious as The Simpsons parade of fabricated Troy McClure B-movies." ...|$|E
30|$|The SPM also {{performs}} consistently {{better to}} serve non-utilitarian perspectives, particularly for infrastructure characterized with a linear type of network (see system characteristics in Table  1). For example, in reducing poverty incidence (Table  7), SPM <b>works</b> <b>better</b> <b>for</b> <b>the</b> road and electricity infrastructures. These infrastructures that are formed by networks of a linear typology {{tend to be}} more sensitive to SPM than infrastructure networks of plane or space typologies. These space typologies occur when the network creates similar level of service between locations in close proximity (e.g., wireless telecommunication and groundwater) and have no issue with local spatial inequity.|$|E
3000|$|Then for ν (ε [...])= O(ε |ε |^- 1), 36 we {{can recover}} the {{remainder}} estimate O(h^ 1 -d); under even weaker regularity conditions by rescaling, we can recover weaker remainder estimates. On the other hand, if ν (ε [...])=o(ε |ε |^- 1), we can recover the remainder estimate o(h^ 1 -d) under the standard non-periodicity condition. 37 For proofs and details, see Section  4.6 of [14]. There {{is an alternative}} to the bracketing construction based on perturbation theory, which <b>works</b> <b>better</b> <b>for</b> <b>the</b> trace asymptotics and also covers the pointwise asymptotics. For an exposition, see Section  4.6 of [14].|$|E
50|$|Treatment {{of minor}} {{depressive}} disorder {{has not been}} studied as extensively as major depressive disorder. Although there are often similarities in the treatments used, there are also differences in what may <b>work</b> <b>better</b> <b>for</b> <b>the</b> treatment of minor depressive disorder. Some third-party payers do not pay to cover treatment for minor depressive disorder.|$|R
5000|$|Due to {{the large}} areas of minable granite in the area the palace was {{transformed}} in 1891 into a prison. Prisoners mined the granite until 1910, when a successful trial with farming and logging proved to <b>work</b> <b>better</b> <b>for</b> <b>the</b> prison. As more violent criminals were admitted to Svartsjö Palace, a special closed section was created with 337 cells built with steel walls.|$|R
5000|$|The first liquid that is {{obtained}} is called head or pringote, {{with a high}} alcohol degree it's destined only to the topical use, with therapeutic intentions. The following fluid is called body or heart, owns between 35 and 70 alcoholic degrees. The third party is the tail, that contains between 10 and 35 alcohol degrees and <b>work</b> <b>better</b> <b>for</b> <b>the</b> mixes.|$|R
40|$|Abstract: This paper {{introduces}} {{the application of}} the Flat Functional-link Neural Network (FFNN) to predict handwritten Chinese moving trajectories. To solve the prediction problem of a non-stationary time series, conventional neural networks need {{a lot of time and}} samples to train, but FFNN can solve this problem very well. Considering the structure of Chinese characters, the paper makes some improvement for FFNN and chooses the appropriate training samples, and promising experimental results have been obtained. Furthermore a comparison is performed between the predictions of the Flat NN and a Kalman filter. Experiments suggest that the improved FFNN predictor <b>works</b> <b>better</b> <b>for</b> <b>the</b> prediction of trajectories of handwritten Chinese characters...|$|E
40|$|The theory {{shown here}} can provide thermal {{stability}} criteria based on physics and a goal {{steady state error}} rather than on an arbitrary "X% Q/mC(sub P) " method. The ability to accurately predict steady-state temperatures well before thermal balance is reached could be very useful during testing. This holds true for systems where components are changing temperature at different rates, although it <b>works</b> <b>better</b> <b>for</b> <b>the</b> components closest to the sink. However, the application to these test cases shows some significant limitations: This theory quickly falls apart if the thermal control system in question is tightly coupled to a large mass not accounted for in the calculations, so it is more useful in subsystem-level testing than full orbiter tests. Tight couplings to a fluctuating sink causes noise in the steady state temperature predictions...|$|E
40|$|The time {{aggregation}} {{properties of}} the Hodrick-Prescott (HP) filter to decompose a time series into trend and cycle are analized for the case of annual, quarterly, and monthly data. It is seen that aggregation of the disagreggate component estimators cannot be obtained as the exact result from applying an HP filter to the aggregate series (and viceversa). Nevertheless, using several criteria, one can find HP decompositions for different levels of aggregation that provide similar results. The approximation <b>works</b> <b>better</b> <b>for</b> <b>the</b> case of temporal aggregation than for systematic sampling. The criterion finally proposed to find “close to equivalent ” HP filters for different frequencies of observation is trivial to apply, and {{does not depend on}} the particular series at hand, nor on the series model. BANCO DE ESPAÑA / DOCUMENTO DE TRABAJO N. 0108 3 1...|$|E
50|$|Daniels and DuMoulin have {{clarified}} {{in interviews}} that neither {{is really a}} strict partisan for one lifestyle over the other, recognizing that both lifestyles offer distinct advantages and disadvantages and that both lifestyles will be <b>the</b> <b>better</b> choice <b>for</b> different buyers, but that it simply <b>worked</b> <b>better</b> <b>for</b> <b>the</b> show's concept to have the two hosts each act as a consistent advocate. In fact, both Dumoulin and Daniels currently live in suburban Surrey after {{having grown up in}} urban central Vancouver.|$|R
50|$|The University of Michigan Executive System, or UMES, a batch {{operating}} system {{developed at the}} University of Michigan in 1958, was widely used at many universities. Based on the General Motors Executive System <b>for</b> <b>the</b> IBM 701, UMES was revised {{to work on the}} mainframe computers in use at the University of Michigan during this time (IBM 704, 709, and 7090) and to <b>work</b> <b>better</b> <b>for</b> <b>the</b> small student jobs that were expected to be the primary work load at the University.|$|R
40|$|In {{this paper}} we {{describe}} the amalgamated free product of finite and semifinite hyperfinite von Neumann algebras over atomic type I subalgebras. To do this we extend the notions of free dimension and standard embeddings used in <b>the</b> related results <b>for</b> finite von Neumann algebras to ones which <b>work</b> <b>better</b> <b>for</b> <b>the</b> semifinite case. We also define classes R 3 (of finite von Neumann algebras) and R 4 (of semifinite von Neumann algebras) which are closed under such amalgamated free products...|$|R
40|$|Proyecto de Graduación (Licenciatura en Ingeniería en Construcción). Instituto Tecnológico de Costa Rica. Escuela de Ingeniería en Construcción, 2007. This Project is in {{the area}} of river Hydraulics, giving a {{solution}} to a particular problem of erosion at the bridge over the Barranca River. It also attempts to raise conscience of the bridge maintenances necessity. The study of the fluvial principles is developed in ten classic examples for Costa Rica bridges, and it is model in HECRAS. Whit the study of the ten particular cases it is established the basic design parameters: design discharge, flow type, geometric section, the roughness, morphologic and hydraulic, determine the length and the height to protect, the revetment type and the granulometry. With these parameters the solution that <b>works</b> <b>better</b> <b>for</b> <b>the</b> bridge at Barranca River is the articulating concrete blocksInstituto Tecnológico de Costa Rica. Escuela de Ingeniería en Construcción...|$|E
40|$|Abstract—Relocalisation in 6 D is {{relevant}} {{to a variety of}} Robotics applications and in particular to agile cameras explor-ing a 3 D environment. While the use of geometry has commonly helped to validate appearance as a back-end process in several relocalisation systems before, we are interested in using 3 D information to assist fast pose relocalisation computation as part of a front-end task. Our approach rapidly searches for a reduced number of visual descriptors, previously observed and stored in a database, {{that can be used to}} effectively compute the camera pose corresponding to the current view. We guide the search by means of constructing validated candidate sets using a 3 D test involving the depth information obtained with an RGB-D camera (e. g. stereo of with structured light). Our experiments demonstrate that this process returns a compact quality set that <b>works</b> <b>better</b> <b>for</b> <b>the</b> pose estimation stage tha...|$|E
40|$|Many vision-based {{approaches}} for obstacle detection often state that vertical thin structure is of importance, e. g. poles and trees. However, {{there are also}} problem in detecting thin horizontal structures. In an industrial case there are horizontal objects, e. g. cables and fork lifts, and slanting objects, e. g. ladders, that also has to be detected. This paper focuses on the problem to detect thin horizontal structures. The system uses three cameras, situated as a horizontal pair and a vertical pair, which {{makes it possible to}} also detect thin horizontal structures. A comparison between a sparse disparity map based on edges and a dense disparity map with a column and row filter is made. Both methods use the Sum of Absolute Difference to compute the disparity maps. Special interest has been in scenes with thin horizontal objects. Tests show that the sparse dense method based on the Canny edge detector <b>works</b> <b>better</b> <b>for</b> <b>the</b> environments we have tested...|$|E
40|$|SYNOPSIS: Basic {{changes in}} market {{regulation}} have {{provided an opportunity}} for low-income consumers in Zimbabwe to improve their diets and real incomes, even while macro-economic adjustments to remove unsustainable food subsidies were being implemented. This synthesis also shows how information on consumer preferences among income groups {{can be used to}} make markets <b>work</b> <b>better</b> <b>for</b> <b>the</b> poor, through active promotion of (self-targeted) commodities. INTRODUCTION: Over the past 10 years, governments and international donor organizations have increasingly recognized the social costs associated with structural adjustment. Almost a decade ago, an influential United Nations report title...|$|R
40|$|Events in Europe are impelling Americans to give European {{civil law}} systems more attention. While {{commercial}} considerations are providing the catalyst, better U. S. law {{could be a}} by-product. Americans familiar with European systems will recognize, as Pound did, {{the extent to which}} <b>the</b> causes <b>for</b> dissatisfaction with <b>the</b> administration of justice in the United States lie in our peculiar legal system. With knowledge of civil law systems, we could <b>work</b> <b>better</b> <b>for</b> <b>the</b> future that Pound sought, one where our courts will be 2 ̆ 2 swift and certain agents of justice 2 ̆ 2 and the 2 ̆ 2 sporting theory of justice 2 ̆ 2 will be just a memory...|$|R
500|$|John Lasseter, {{executive}} producer of Bolt, decided to make [...] "I Thought I Lost You" [...] <b>the</b> theme <b>for</b> Bolt, since it lyrically summarized the film's plot. He said, [...] " [...] so sums up {{the theme of this}} film. You know, a dog and its owner and they both were separated, but they love each other so much — there's such an emotional payoff when these characters get reunited, and I think that's what this song's about." [...] Lasseter thought that solely the song worked, but it <b>worked</b> <b>better</b> <b>for</b> <b>the</b> film. [...] "I Thought I Lost You" [...] is one of two songs on the Bolt soundtrack and was released to Radio Disney to promote Bolt and its accompanying soundtrack.|$|R
40|$|Self-, N 2 - and Ar-broadening {{coefficients}} were measured for the stretch-bend infrared combination bands nu- 1 + nu- 1 / 2 (4004 /cm) of HCN and nu- 1 + nu- 1 / 5 (4091 /cm) of C 2 H 2, using a tunable difference-frequency laser. At atmospheric pressures, the Q branches of these bands exhibit significant rotational narrowing or line mixing. The broadening coefficients are fit with empirical rotationally inelastic collision rate laws, {{which are then}} used to model the line mixing in the overlapped Q-branch profiles. Simple energy gap fitting laws appear to be suitable for the shorter-range intermolecular quadrupole-quadrupole and induction forces, whereas an energy-corrected-sudden scaling law <b>works</b> <b>better</b> <b>for</b> <b>the</b> longer-range dipole-dipole and dipole-quadrupole collision partners. In all cases, the line-coupling coefficients are substantially reduced from the rotationally inelastic rates fit to the broadening coefficients, indicating that 35 - 70 percent of the broadening {{may be due to}} other collisional mechanisms such as cross-relaxation to the degenerate H state vibrational level...|$|E
40|$|Abstract — Many vision-based {{approaches}} for obstacle detection often state that vertical thin structure is of importance, e. g. poles and trees. However, {{there are also}} problem in detecting thin horizontal structures. In an industrial case there are horizontal objects, e. g. cables and fork lifts, and slanting objects, e. g. ladders, that also has to be detected. This paper focuses on the problem to detect thin horizontal structures. The system uses three cameras, situated as a horizontal pair and a vertical pair, which {{makes it possible to}} also detect thin horizontal structures. A comparison between a sparse disparity map based on edges and a dense disparity map with a column and row filter is made. Both methods use the Sum of Absolute Difference to compute the disparity maps. Special interest has been in scenes with thin horizontal objects. Tests show that the sparse dense method based on the Canny edge detector <b>works</b> <b>better</b> <b>for</b> <b>the</b> environments we have tested. Index Terms — Computer vision, Obstacle detection, Stereo vision, Thin structures...|$|E
40|$|Previous {{research}} provides {{evidence for}} the negative relation between earnings volatility and earnings forecasting. This paper examines if earnings forecast models can adjust for firms’ earnings volatility and improves the forecasts by choosing a specific estimation method and a specific forecast model. The sample is divided into quartiles based on the firms’ earnings volatility, to examine if the choice of estimation method, the full sample (FS) or the first quartile (Q 1) method, and the choice of forecast model, the ones by Hou et al. (2012) and Clubb and Wu (2014), matter depending on the firms’ degree of earnings volatility. The forecasts on US firms are compared based on bias and accuracy over the period 2000 - 2010. The results confirm the negative relation between earnings volatility and earnings forecasting. Furthermore, the choice of estimation method proves {{to be a way}} to account for earnings volatility, where the FS method shows to give better forecasts for the highest volatility firms while the Q 1 method is to prefer for the lower volatility firms. The choice of model appears to not depend on earnings volatility except for the model by Clubb and Wu (2014) that <b>works</b> <b>better</b> <b>for</b> <b>the</b> lower volatility firms when using the Q 1 method...|$|E
6000|$|... "Take it," [...] said Lingard. [...] "It {{is still}} mine. How can I forget that, when facing death, {{you thought of}} my safety? There are many dangers before us. We shall be often separated--to <b>work</b> <b>better</b> <b>for</b> <b>the</b> same end. If ever you and Immada need help at once and I am within reach, send me a message with this ring and if I am alive I will not fail you." [...] He {{looked around at the}} pale daybreak. [...] "I shall talk to Belarab straight--like we whites do. I have never seen him, but I am a strong man. Belarab must help us to reconquer your country and when our end is {{attained}} I won't let him eat you up." ...|$|R
40|$|Objective:. This paper proposes and {{demonstrates}} {{a method for}} segmenting populations {{as part of a}} Total Market Approach to designing, managing, and evaluating reproductive and sexual health interventions in developing countries. It applies the “Making Market Systems <b>Work</b> <b>Better</b> <b>for</b> <b>the</b> Poor”, or “M 4 P”, concept to address the general problem of situations where one or more merit goods or services are perceived to be sub-optimally supplied and consumed under existing marketing conditions. Five bases of segmentation are proposed to increase health system performance: 1) vulnerability, 2) consumption, 3) access and psycho-social determinants of consumption, including willingness to pay, 4) equity-based measures, and 5) source of supply preference. Data and Methods: Data are representative of males aged 16 - 35 years old in 2004 in six Sout...|$|R
40|$|In {{this paper}} {{we seek to}} explore the {{interaction}} between the style of a broadcast news story and its summarization technique. We report the performance of three different summarization techniques on broadcast news stories, which are split into planned speech and spontaneous speech. The initial results indicate that some summarization techniques <b>work</b> <b>better</b> <b>for</b> <b>the</b> documents with spontaneous speech than for those with planned speech. Even for human beings some documents are inherently difficult to summarize. We observe this correlation between degree of dif culty in summarizing and performance of the three automatic summarizers. Given the high frequency of named entities in broadcast news and even greater number of references to these named entities, we also gauge the effect of named entity and coreference resolution in a news story, on the performance of these summarizers...|$|R
30|$|As a {{baseline}} {{for the performance}} comparison, CDAC-OFDM with the iterative ICI cancelation using the entire detection symbols in the OFDM frame is included. Note that the iterative ICI cancelation results in almost no improvement to CDAC-OFDM. This is because the initial detection performance of CDAC-OFDM under frequency-asynchronous environment is poor, and thus, the ICI cancelation based on unreliable initial detection does not work properly. On the other hand, the iterative ICI cancelation <b>works</b> <b>better</b> <b>for</b> <b>the</b> case when it is applied to FADAC-OFDM which has a superb initial detection performance. However, the performance gain of ICI cancelation scheme in [10] is still not so significant. This scheme uses all the detected symbols in the interference reconstruction step without any consideration of {{the reliability of the}} detection symbols. Even in FADAC-OFDM, some of the detection symbols may eventually have relatively high possibility of errors due to severe ICI terms such as inter-block ICI as mentioned before. It is shown that after the first iteration, the performance fairly improves but from the second iteration, the performance is stuck in the same value. This is because the reconstructed interference term for cancelation is not updated anymore due to the erroneous portion of the constructed interference. Consequently, the performance gap between the scheme in [10] and the case of no ICI is still significant.|$|E
40|$|This paper {{examines}} {{the effectiveness of}} corrective feedback on learners of the Japanese language. The current study had a total of 25 students who agreed to participate, consisting of both advanced and intermediate levels. There are six main types of corrective feedback established and defined by Lyster and Ranta (1997), this study focused on two particular types, recast (a category of implicit) and elicitation (a category of prompt). Comparing a particular feedback or a category of feedback with another {{has been one of}} the ongoing topics in the field of second language (L 2) learning. The present study is intended to examine which feedback <b>works</b> <b>better</b> <b>for</b> <b>the</b> learners in terms of repairing their mistakes and to investigate which learner group shows a better effect on each feedback. The results suggest that elicitation is more beneficial to L 2 learners than recast in reformulating their utterance. The reason for this is likely that elicitation is not as implicit as recast; thus, the learners had a better opportunity to notice elicitation that was given when they made a mistake. Interestingly, this outcome also provided a comparison between the advanced and intermediate groups. Both repaired their mistakes more after elicitation was given, but the advanced group did better. Since each group displayed almost the same moderate rate of repairing for recast, what truly differentiated one group from the other was elicitation. This result suggests that learners who have more knowledge of the target language will benefit from elicitation...|$|E
40|$|The {{development}} of a conceptual rainfall-runoff model generally requires calibration to obtain non-observable parameters. This becomes especially difficult in river basins where data are absent, frequently intermittent or inaccurate. Rainfall-runoff models are usually calibrated on discharge data that are derived from stage observations {{by means of a}} rating curve. However, this method is error-prone, as it is not able to get around problems such as a changing geometry of the river channel, flows over floodplain and a variable roughness coefficient due to vegetation. Moreover, uncertain stage observations can be a severe source of error as well, because of incorrect placement of the staff gauges or incorrect and infrequent stage readings. Calibration on uncertain discharge data can affect modeled parameters and consequently be cause for erroneous model predictions. This works uses water levels of reservoirs to calibrate a conceptual rainfall-runoff model in the Umbeluzi River Basin. The conceptual model provides the inflow to the reservoir, subsequently reservoir storage is modeled and compared to the observed storage. The proposed method has several advantages over the conventional method. First of all, stage observations are accurately observed because of the social and economic importance of the reservoir. Moreover, the method is well suited to observe floods, as the shape of the reservoir is well confined and its capacity generally more than adequate. It also captures the entire volume of a flood, in contrary to the conventional method where effects of hysteresis can be cause for misconception of the river discharge. An additional advantage is that reservoirs can integrate multiple tributaries, allowing to model tributaries that are not or scarcely gauged. Lastly, this method may have some economic benefits as the water levels and the surface area of the reservoir can in principal be derived by radar altimetry through satellites. This allows for the system to be remotely calibrated. By incorporating irrigation into the hydrological model, the effects of artificial reservoirs on the basin hydrology are assessed. The conceptual model is calibrated with inclusion and exclusion of irrigation and subsequently best parameter sets are compared. Results show that the excess irrigation water that percolates to the ground water has a significant influence on the resulting base flow, such that model parameters have to be compensated when irrigation is excluded. Moreover, a changing crop need due to the varying lengths of dry spells, is cause for variability in irrigation over the years. This makes modeling base flow challenging. It appears that inclusion of irrigation into the hydrological model {{is the only way to}} obtain a consistent ground water parameter. This thesis shows a validation approach whereby multiple system components are validated on various independent data-sources. The modeled evaporation is compared to evaporation that is developed with use of satellite data. Both time-series correspond well, which implies that the lumped modeled water storage of the catchment is plausible. In addition, it implicitly validates the amount of transpiration from irrigation, which is included in the actual evaporation. To validate on smaller system components, the water levels of the conceptualized floodplain storage are compared to observed river stage. Although absolute values are not comparable, the emptying and filling of the flood plains harmonizes with the rate of change of observed stage levels. Lastly, the model is tested on a tributary of the catchment, generating reasonable model prediction, despite the poor data quality. The author argues that this way of validation is more valuable than validating on time-series outside the calibration period, as it takes multiple aspects of the system into account. For this study both a lumped and a topography driven model is tested. During the calibration, both models give a similar model performance. However the optimum parameter set of the topography driven mode was better applicable in other catchments. Moreover, the parameters derived where also more realistic. It is therefore that the topography driven model <b>works</b> <b>better</b> <b>for</b> <b>the</b> Umbeluzi catchment. HydrologyWater ManagementCivil Engineering and Geoscience...|$|E
40|$|In {{this article}} I argue that coalitions {{will tend to}} employ control {{mechanisms}} to facilitate the adoption of compromise policies only when the expected benefit of their use is high enough. When partners are already satisfied with log-rolling policies (compartmentalized by jurisdiction), or when compromise is already attainable self-enforcingly, there are few incentives to use them. Conversely, when partners are interested in compromise policies but are unable to reach that outcome in equilibrium, then control mechanisms {{are likely to be}} implemented. The empirical evidence offered tends to support the two main hypotheses of this work: control mechanisms are less necessary when the tangentiality of partners’ preferences is high and when they foresee frequent mutual interactions. However, that seems to <b>work</b> <b>better</b> <b>for</b> <b>the</b> allocation of watchdog junior minister...|$|R
5000|$|When {{the naval}} Flanker trainer was being {{conceived}} the Soviet Air Force was evaluating a replacement <b>for</b> <b>the</b> Su-24 [...] "Fencer" [...] strike aircraft, {{and it became}} evident to Soviet planners {{at the time that}} a replacement <b>for</b> <b>the</b> Su-24 would need to be capable of surviving engagements with the new American F-15 and F-16. The Sukhoi bureau concentrated on adaptations of the standard Su-27UB tandem seat trainer. However the Soviet Air Force favoured the crew station (side-by-side seating) approach used in the Su-24 as it <b>worked</b> <b>better</b> <b>for</b> <b>the</b> high workload and potentially long endurance strike roles. Therefore, the conceptual naval side-by-side seated trainer was used as <b>the</b> basis <b>for</b> development of <b>the</b> Su-27IB (Russian <b>for</b> [...] "Istrebityel Bombardirovshchik" [...] - [...] "Fighter Bomber") as an Su-24 replacement in 1983. The first production airframe was flown in early 1994 and renamed the Su-34 (NATO reporting name 'Fullback').|$|R
5000|$|He {{has been}} criticized by Kenneth Arrow for a {{somewhat}} poor reading of some sociological classics (though he never {{claimed to be a}} sociologist), such as the works of Émile Durkheim; while this may partly reflect his somewhat inadequate academic training, it also derives from his impatience with non-participatory sociology and his preference (this became a defining characteristic of [...] "his" [...] discipline of 'social administration') for engagement with contemporary social policy issues and even some of its more fallible institutions. For example, he was much criticised for his role as a vice-chairman of the government's Supplementary Benefits Commission which some critics felt did not allow him enough distance. He, by contrast argued in favour of trying to make inadequate institutions <b>work</b> <b>better</b> <b>for</b> <b>the</b> benefit of the poor even if his involvement with them had the potential to sully the purity of his reputation.|$|R
40|$|The {{aim of this}} Master {{thesis is}} the study of the {{superfluid}} properties in nuclear matter in general, especially in what concerns very neutron rich systems such as ``exotic" nuclei, infinite nuclear matter and neutron stars which, in a certain sense, can be thought as ``giant nuclei". Superfluidity in nuclear matter is due to the pairing of nucleons which is similar to what happens in metals when superconductivity occurs thanks to the formation of the Cooper pairs of electrons. An experimental evidence of the pairing in nuclei is, for example, the odd-even staggering effect, namely the nuclei having an even number of nucleons are more stable than those having an odd one. In neutron stars, the pairing interaction influences the cooling time of the inner crust and the ``glitches", that is the observed sudden change in the rotation period of these stars. So, our aim is to investigate the properties of the pairing interaction which accounts for superfluidity and to find an interaction which is as realistic as possible and better fits to the already existing experimental data. The thesis is structured in four Chapters, each one being dedicated to a particular subject, as we will better specify in what follows. In Chapter 1, we give a brief description of neutron stars which represent the final stage evolution of massive stars and, as the name says, they are mainly made of neutrons. The masses of these stars are of the order of the solar one, but their radii are of the order of 10 km and thus their densities are comparable and greater than the nuclei saturation density. They are compact objects and we need to use the theory of general relativity to describe their structure. This was done by Tolman, Oppenheimer and Volkoff (TOV) who solved the Einstein equation (for non rotating spherical symmetric mass distribution) and obtained the relativistic hydrostatic equilibrium equations. Once we have the equation of state (EoS), namely the behavior of the pressure with density, we can solve the structure of the star and so obtain the profile of the mass (M), density (d) and pressure (P) all along the star. We use two different equations of state: the Friedman-Panharipande-Skyrme (FPS) and Skyrme-Lyon (SLy) EoS. These EoS describe the nucleon-nucleon interaction and belong to the so called ``effective" interactions, whose parameters are chosen in order to reproduce some properties of the infinite nuclear matter and finite nuclei. We solve these equations using the Runge-Kutta method and thus obtain the M, P and d profiles and see {{the differences between the two}} EoS. We also calculate the maximum mass that the neutron star can have for each EoS and we find that the one given by the SLy 4 EoS is grater than the one given by the FPS EoS. The knowledge of the maximum allowed mass Mmax of neutron stars is very important, since it represents the boundary between two different classes of compact stars: neutron stars (M Mmax). Anyway, in this work, we especially focus on the study of the superfluid properties in the inner crust of neutron stars due to the pairing of the unbound neutrons whose distribution in the crust is given by the Negele and Vautherin model, but first we see (in the second Chapter) how is the pairing interaction studied in nuclei and infinite nuclear matter. In Chapter 2, we study some nuclei and the infinite nuclear matter in the mean field approximation of the non relativistic quantum many-body problem. In these approximations, nucleons in nuclei are supposed to move in a mean field they themselves create. We use the Hartree-Fock (HF) and the Hartree-Fock-Bogoliubov (HFB) approximations to describe finite nuclei and the Bardeen-Cooper-Schrieffer (BCS) approximation, which is a particular case of the HFB theory, to describe infinite nuclear matter and the inner crust of the neutron stars where there is also a gas of unbound superfluid neutrons which can be approximated with infinite nuclear matter made of neutrons. The HF, HFB and BCS work well in that concerns the reproduction of ground state or static properties of the mean and heavy mass nuclei, but they fail to reproduce the dynamic properties concerning the excited states. The description of the dynamic properties of nuclei is done using, for example, the Quasi-particle-Random-Phase-Approximation (QRPA) theory. The HF approximation describes well the closed-shell nuclei, such as the 16 O, but it fails to describe nuclei where the pairing correlations among nucleons become important and these correlations are taken into account in the HFB approximation. We use the Skyrme-Lyon 4 (SLy 4) ``phenomenological" and ``effective" zero-range interaction to describe the nucleon-nucleon interaction inside nuclei. The term ``effective" refers to the difficulties that are encountered while describing the nucleon-nucleon interaction inside nuclei, since it becomes repulsive at short distance, and the term ``phenomenological" means that the parameters of the interaction are adjusted to reproduce some properties of both the infinite nuclear matter and finite nuclei. At first, we perform some HF calculation using the SLy 4 interaction for the 16 O and the 68 Ni. We reproduce well some known properties, such as the radii and the binding energies. Then, in order to better understand the pairing interaction, we perform HFB calculations concerning the tin isotopes 124 Sn and 136 Sn. We focus on these nuclei since the 124 Sn is stable and has already been experimentally studied, while the 136 Sn is very neutron rich and thus pairing correlations are supposed to have a great importance (this nucleus has not yet been experimentally studied). So, apart from the SLy 4 interaction, we put in the hamiltonian describing these nuclei a paring interaction. We use different paring interactions which belong to the so called ``density-dependent-delta interactions" (DDDI), which, as the name says, depend on the nucleonic density, are local interactions and, according to their dependence on the density, they can be ``isoscalar" or ``isovector". The parameters of the DDDI are fitted once again in order to reproduce some properties of infinite nuclear matter and nuclei. For example, we use two DDDI pairing interactions, the ``surface" (which is peaked at the surface of the nucleus) and the ``mixed" (which is located in a broad region inside the nucleus) ones, whose parameters are chosen so that we can reproduce the two-neutron separation energies in the tin isotopes. In particular, we focus on the pairing gap among neutrons and thus, we calculate it for each kind of interaction and for each nucleus to see the differences among the different pairing interactions. In the case of the stable 124 Sn, all the DDDI give a mean neutron pairing gap which reproduces well the experimental one, while in the case of the exotic 136 Sn, there are important differences among the mean neutron pairing gap values given by the different DDDI. In particular, the ``surface" interaction gives a quite high pairing gap compared to the other DDDI. We also calculate the pairing gap in infinite symmetric (same fraction of protons and neutrons), asymmetric and pure neutron infinite nuclear matter in the BCS theory and for each of the considered DDDI. In this case too, we see that the ``surface" interaction gives a very high mean pairing gap compared to the other interactions. As we have already said, we use the information obtained in the case of the pure neutron infinite matter to calculate the pairing gap also in the inner crust of neutron stars where the gas of unbound neutrons in the crust is described by the Negele and Vautherin model and we see, once again, how does the pairing gap given by the different DDDI vary along the crust. As expected, the pairing gap due to the ``surface" interaction is peaked in a more narrow region compared to the other interactions, while the ``mixed" one is located in a more broad region inside the crust of the star. After having seen the behavior of the pairing gap in nuclei and infinite matter, we perform Local Density Approximation (LDA) calculations over the isotopic chain of the tin nuclei where the number of neutrons is varied from 44 to 110. The LDA, as the name says, consists in locally approximating the nuclei with infinite nuclear matter and thus it tells us how accurate are we when we use the information obtained from finite nuclei to describe infinite nuclear matter and vice versa. The better or worse agreement between LDA and HFB calculations depends, as expected, both on the considered nucleus and pairing interaction. In particular, we see that the LDA <b>works</b> <b>better</b> <b>for</b> <b>the</b> mid-shell nuclei (far from nuclei having a magic number of neutrons) and that in general the worse agreement between HFB and LDA is obtained in the case of the ``surface" interaction. Until now, we have seen, from a theoretical point of view, how does the pairing gap varies in different nuclear systems in connection with which kind of pairing interaction we consider, but, as we know, the final tests that physical theories have to pass are observations and experiments: to this is dedicated the third Chapter. In Chapter 3, we give a brief description of the experimental methodologies and setups which are used to investigate the pairing properties and in particular we refer to an experiment which took place at the Grand Accélérateur National d'Ions Lourds (GANIL) located in Caen (France). In this experiment the 69 Ni is studied and we give just a very brief summary of it, since it goes beyond the aim of this work. Anyway, the methodology used in this experiment is the same as the one used to explore the pairing interaction with exotic beams. In particular, we give a brief description of the heavy and light particle detectors, such as the MUST 2 detector. The pairing interaction properties can be experimentally explored performing direct transfer reactions using exotic beams in inverse kinematics. The term ``direct reaction" means that the passage from the initial to the final state of the reaction is done in one or few steps and proceeds in a short time, while the term ``inverse kinematics" means that the exotic beam (e. g. tin isotopes) is used as projectile and stable nuclei (e. g. protons) as targets. Therefore, we can study the pairing interaction performing, for example, two-neutron transfer reactions, where a couple of neutrons is transferred from the projectile to the target (or vice versa), but, before performing nuclear transfer reaction experiments, we need theoretical arguments concerning the reactions of interest and this is the subject of Chapter 4. In Chapter 4, we study from a theoretical point of view the following two neutron transfer reactions: 124 Sn(p,t) 122 Sn and 136 Sn(p,t) 134 Sn. We choose these reactions since the first one has already been experimentally studied and thus it allows us to compare our theoretical results with the already existing data; while, the second one is studied since the pairing correlations are supposed to have a very important role in the very neutron rich 136 Sn nucleus. We notice that the 136 Sn(p,t) 134 Sn reaction has not yet been experimentally investigated. Cross section measurements of transfer reactions allow us to compare theoretical and experimental results concerning the pairing interaction. We therefore calculate theoretical cross sections for each of the considered pairing interactions. The theoretical cross-section of the nuclear reaction is described in the one-step zero-range Distored-Wave-Born-Approximation (DWBA) theory and we use global optical phenomenological potentials both in the entrance and in the exit channels of the reaction. The form factors of the reactions are provided by the QRPA theory and we use them us inputs for our calculations. Note that the information concerning the pairing interaction is contained in the form factors, since there is a different form factor for each of the different pairing interactions. In particular, we focus only on the ``surface" and the ``mixed" pairing interactions and consider, for each kind of the (p,t) reactions, both the transition from the ground state (gs) of the initial nucleus (for example, 124 Sn) to the gs of the final nucleus (for example, 122 Sn) and the transition from the gs to the first excited (1 st 0 +) state (gs- 1 st 0 +). We vary the energy of the incident proton (Ep) from 10 MeV to 35 MeV, to see which should be the energy of the eventual beam to use in the experiments to constrain the pairing interaction. So, varying Ep from 10 MeV to 35 MeV, we calculate the differential cross sections for the two reactions of interest, for each pairing interaction and for the two transitions (gs-gs and gs- 1 st 0 +). The code we use (DWUCK 4, that is Distorted Waves University of Colorado Kunz 4) does not provide normalized cross sections (CS) and thus we can not provide absolute cross sections. For this reason we calculate the ratios of the absolute CS of the considered transitions, namely CS(gs-gs) /CS(gs- 1 st 0 +). Experimental data for the reaction 124 Sn(p,t) 122 Sn already exist for the transition gs-gs (at Ep= 20 MeV) and for the transition gs- 1 st 0 + (at Ep= 34. 9 MeV) and we compare our theoretical results to them. Our theoretical cross sections well reproduce the shapes and the diffraction minima of the differential cross sections in both cases. On the contrary, our cross section ratios are not in good agreement with the experimental ones. This is due to the fact that, at Ep= 20 MeV, our calculations are very dependent on the Q of the reaction, since it is very negative and, in this case, the energy of the beam is not high enough so that we can reliably compare our theoretical cross section with the experimental ones. In what concerns the 136 Sn(p,t) 134 Sn, we do not have experimental data to compare our theoretical results with, but we find that we can disentangle among the different pairing interactions just considering the shape of our cross sections, as it happens, for example, in the case of the transition gs- 1 st 0 + and at Ep= 15 MeV. In this case, at Ep= 10 MeV, analogously to what happens for the 124 Sn(p,t) 122 Sn at Ep= 20 MeV, our theoretical cross sections are not reliable once again because of the Q of the reaction which is not high enough. Anyway, at higher energies we do not have this problem and we show that we can experimentally disentangle between the ``surface" and the ``mixed" interactions only for Ep values at around 15 MeV. This result is very important, since nowadays facilities produce exotic beams whose energies are not higher than 15 MeV and the very next future facilities, such as the SPIRAL 2, will study transfer reactions with the tin isotopes and thus will help us to shed more light on the pairing interaction and all the properties which depend on it, such as the cooling time of neutron stars, nuclear structure [...] ...|$|E
30|$|The {{trade-off}} between {{privacy and}} utility {{is shown in}} Table  1. Both CountingLab’s and Lloyds’s model <b>work</b> <b>better</b> <b>for</b> <b>the</b> direct than <b>for</b> <b>the</b> hierarchical setting. In contrast, the hierarchical benchmark forecast outperforms its direct counterpart. Thus, only the benchmark model is a suitable candidate for Differential Privacy. This is an interesting and unexpected result (note that although the performance of Lloyd’s forecast improves with limited amount of noise it never has a positive utility). The desired membership inference confidence region ρ≤ 0.6 is achieved <b>for</b> <b>the</b> benchmark model <b>for</b> λ= 56, 234 with Δf= 15.35 and offers a positive utility of 5.94 % {{with respect to the}} direct forecast. Thus, a setting has been found where both, privacy and utility, have been reached. The authors want to highlight that they assume communication of individual, understandable membership inference risk ρ based on individual Δf as crucial to foster consumer acceptance of privacy-preserving techniques.|$|R
40|$|Abstract. The {{electron}} localization measure {{proposed by}} Becke and Edgecombe {{is shown to}} be related to the covariance of the electron pair distribution. Just as with the electron localization function, the local covariance does not seem to be, in and of itself, a useful quantity for elucidating shell structure. A func-tion of the local covariance, however, is useful for this purpose. A different function, based on the hyper-bolic tangent, is proposed to elucidate the shell structure encapsulated by the local covariance; this function also seems to <b>work</b> <b>better</b> <b>for</b> <b>the</b> electron localization measure of Becke and Edgecombe. In ad-dition, we propose a different measure <b>for</b> <b>the</b> electron localization that incorporates both the electron lo-calization measure of Becke and Edgecombe and the Laplacian of the electron density; preliminary indications are that this measure is especially good at elucidating the shell structure in valence regions. Methods for evaluating electron localization functions directly from the electron density, without re-course to the Kohn-Sham orbitals, are discussed...|$|R
40|$|In {{this paper}} {{we deal with}} the problem of obtaining a random {{procedure}} for generating fuzzy measures. We use the fact that the polytope of fuzzy measures is an order polytope, so that it has special properties that allow to build a uniform algorithm. First, we derive an exact procedure based on an existing procedure to generate random linear extensions; then, we study the applicability of this algorithm to the polytope of fuzzy measures, showing that the complexity grows dramatically with the cardinality of the referential set. Next, we study other heuristics appearing in <b>the</b> literature <b>for</b> <b>the</b> polytope of fuzzy measures; our results seem to mean that these procedures cannot be applied to this case either. Finally, we propose another heuristic that reduces the complexity and could be used instead of the other procedures. We finish comparing the performance of this heuristic with the other possibilities, showing that our alternative seems to <b>work</b> <b>better</b> <b>for</b> <b>the</b> polytope of fuzzy measures...|$|R
40|$|The paper {{describes}} an interesting analysis aimed at testing {{a new technique}} for cali-brating hydrological models in ungauged catchments. The topic is extremely interest-ing, as the numerous contributions recently proposed by the scientific literature demon-strate. The work presented here conveys two original ideas that in my opinion are interesting: 1) the full parameter vector of the hydrological model, and not the individual parameter values, is transferred from catchment to catchment. 2) The parameter transferring technique is coupled with a regional study for performing model calibration. I think this method is valuable as I believe that ungauged basins should be treated by using all the available information, including regional databases. S 330 I believe this study is very interesting for readers of HESS. I have only minor sugges-tions that are listed here below. 1) The catchments description is not detailed. It {{would be interesting to}} know whether some catchments are nested. The proposed method should <b>work</b> <b>better</b> <b>for</b> <b>the</b> case o...|$|R
50|$|The {{positive}} Award {{was given}} to {{two members of the}} union Sintracarbon in Colombia for their fight <b>for</b> <b>better</b> <b>work</b> conditions <b>for</b> <b>the</b> workers in the country's biggest coal mine (El Cerrejón).|$|R
