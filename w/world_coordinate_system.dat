222|10000|Public
5000|$|The <b>world</b> <b>{{coordinate}}</b> <b>system</b> is the {{coordinate system}} in which the virtual world is created.This should meet a few conditions for the following mathematics to be easily applicable: ...|$|E
5000|$|<b>World</b> <b>coordinate</b> <b>system</b> (WCS) support, {{implementing}} PyWCS, the Python wrapper to WCSLIB. WCSLIB is a C library which implements the WCS {{standard in}} the Flexible Image Transport System (FITS) standard.|$|E
5000|$|The {{kinematic}} skeleton {{is constructed}} by a tree-structured chain, {{as illustrated in}} the Figure. Each rigid body segment has its local coordinate system that can be transformed to the <b>world</b> <b>coordinate</b> <b>system</b> via a 4×4 transformation matrix , ...|$|E
40|$|The <b>world</b> <b>coordinate</b> <b>systems</b> for {{dispersion}} calibrated spectra {{used in the}} IRAF/NOAO spectroscopy {{packages are}} presented. In particular, the image header keywords which define the coordinates. These keywords appear both {{as part of the}} IRAF image structure and map directly to FITS format. The types of spectra include multidimensional images with one or more spatial axes and a linear or log-linear dispersion axis and special equispec and multispec formats having multiple independent one dimensional spectra in a single multidimensional image. The multispec format also includes general nonlinear dispersion <b>coordinate</b> <b>systems</b> using polynomial, spline, sampled table, and look-up table functions. 1. Introduction IRAF 2 version 2. 10 is the first release to provide general support for user <b>world</b> <b>coordinate</b> <b>systems</b> (WCS) in image and photon list data. For spectra the <b>world</b> <b>coordinate</b> <b>systems</b> include at least one dispersion coordinate; i. e wavelength, frequency, etc. The NOAO spectroscopic [...] ...|$|R
40|$|The {{calibration}} method for a multiple rangefinders system is proposed in this paper. Generally, multiple rangefinders {{are required to}} obtain the whole surface shape of a large object such as a human body. The proposed method solves range data registration by a prior calibration using a reference plane with rectangular markers. The <b>world</b> <b>coordinates</b> <b>system</b> is defined on the reference plane. Because range data have about two hundred thousands of 3 -D points, the normal vector of the reference plane is accurately estimated by fitting the regression plane to the 3 -D points. If the Z-axis of the <b>world</b> <b>coordinates</b> <b>system</b> for our {{calibration method}} {{is defined as the}} axis which cross meets the reference plane, it is determined by the normal vector. On the other hand the X and Y axes are defined as the horizontal line and vertical line of rectangular markers. They are determined by detecting and extracting the rectangular markers from the intensity image. Therefore, the orientation of each rangefinder is estimated based on the <b>world</b> <b>coordinates</b> <b>system.</b> In the experiments, the multiple rangefinders system which consists of twelve rangefinders is used. Experimental results indicate that the RMSE is 2. 3 mm in the case of measuring a cylinder object...|$|R
5000|$|NDF : NDF is the Project's {{principal}} data format. Built upon HDS the N-dimensional Data Format—is {{for storing}} bulk {{data in the}} form of n-dimensional arrays of numbers: mostly spectra, images, and cubes. It supports concepts such as quality, data errors, <b>world</b> <b>coordinate</b> <b>systems,</b> and Metadata. It is also extensible to handle user-defined information.|$|R
50|$|Given a 2D {{image of}} an object, and the camera that is {{calibrated}} {{with respect to a}} <b>world</b> <b>coordinate</b> <b>system,</b> it is also possible to find the pose which gives the 3D object in its object coordinate system. This works as follows.|$|E
50|$|The {{metadata}} include {{information about}} {{the creator of the}} image, the content (including description and subject category), the method of observation (including facility, instrument and spectral information), the <b>World</b> <b>Coordinate</b> <b>System</b> (WCS) position in the sky, and the publisher of the image.|$|E
50|$|To enable {{rapid access}} of {{specific}} {{stars in the}} catalogue, WCSTools software numbers each star using its Guide Star region number (0001-9537) and a five-digit star number within each region, separated by a decimal point. sty2 lists Tycho-2 stars by number or sky region. imty2 lists the Tycho-2 stars within an IRAF or FITS image using the <b>world</b> <b>coordinate</b> <b>system</b> defined in its header.|$|E
3000|$|... ′-axis is {{extending}} {{towards the}} persons within the classroom. The 3 D coordinates {{of a point}} within <b>world</b> (classroom) <b>coordinate</b> <b>system</b> are given by vector p [...]...|$|R
5000|$|AST : A {{flexible}} and powerful library for handling <b>World</b> <b>Coordinate</b> <b>Systems,</b> partly {{based on the}} SLALIB library. If you are writing software for astronomy and need to use celestial coordinates (e.g. RA and Dec), spectral coordinates (e.g. wavelength, frequency, etc.), or other <b>coordinate</b> <b>system</b> information, then this library should be of interest. It provides solutions {{for most of the}} problems you will meet and allows you to write robust and flexible software. It is able to read and write WCS information in a variety of formats, including FITS-WCS. It has Fortran, C and Python bindings.|$|R
40|$|Abstract. The NCSA Horizon Image Data Browser is a Java package which {{includes}} ready-to-use applets and applications {{as well as}} reusable classes for building new applications for browsing scientic images locally or over the network. We present {{the current status of}} the development of this package focusing on three general features: 1) flexible support for metadata through a schema-independent design, 2) the implementation of our metadata model to support <b>world</b> <b>coordinate</b> <b>systems,</b> and 3) support for use of Horizon applications within the NCSA Habanero Collaborative environment. 1...|$|R
5000|$|... are the {{extrinsic}} parameters which {{denote the}} coordinate system transformations from 3D world coordinates to 3D camera coordinates. Equivalently, the extrinsic parameters define {{the position of}} the camera center and the camera's heading in world coordinates. [...] is {{the position of the}} origin of the <b>world</b> <b>coordinate</b> <b>system</b> expressed in coordinates of the camera-centered coordinate system. [...] is often mistakenly considered the position of the camera. The position, , of the camera expressed in world coordinates is [...] (since [...] is a rotation matrix).|$|E
50|$|The objects {{contained}} within the scene (houses, trees, cars) are often designed in their own object coordinate system (also called model coordinate system or local coordinate system) for reasons of simpler modeling. To assign these objects to coordinates in the <b>world</b> <b>coordinate</b> <b>system</b> or global coordinate system of the entire scene, the object coordinates are transformed by means of translation, rotation or scaling. This is done by multiplying the corresponding transformation matrices. In addition, several differently transformed copies can be formed from one object, for example a forest from a tree; This technique is called instancing.|$|E
5000|$|Example: If we are {{to develop}} a flight simulator, we can choose the <b>world</b> <b>coordinate</b> <b>system</b> so that the origin {{is in the middle}} of the earth and the unit is set to one meter. In addition, in order to make the {{reference}} to reality easier, we define that the X axis should intersect the equator on the zero meridian, and the Z axis passes through the poles. In a Right-handed system, the Y-axis runs through the 90°-East meridian (somewhere in the Indian Ocean). Now we have a coordinate system that describes every point on Earth in three-dimensional Cartesian coordinates. In this coordinate system, we are now modeling the principles of our world, mountains, valleys and oceans.|$|E
40|$|In {{our current}} {{research}} {{we examine the}} integration of haptic interfaces into augmented reality setups. The ultimate target of these endeavours is {{the application of the}} framework to training of manipulative skills in surgical environments. To this end, highly accurate calibration, system stability, and low latency are indispensable prerequisites. Therefore, we developed a new calibration method to exactly align the haptic and <b>world</b> <b>coordinate</b> <b>systems.</b> Moreover, a distributed framework was created, which ensures low latency and component synchronization. Finally, to demonstrate our results, we integrated all elements into an augmented reality haptics ping-pong game...|$|R
30|$|This section {{describes}} the nonlinear state estimation problem solved here with EKF-SLAM, how the vessel dynamics, and sensors are modeled and how different performance measures are computed. All vectors {{are expressed in}} a <b>world</b> fixed <b>coordinate</b> <b>system</b> unless otherwise stated.|$|R
30|$|Driven by {{the burden}} of large file sizes when dealing with the IFC file format, the paper investigates methods {{to reduce the size}} of IFC models. In literature, the {{duplication}} of elements is pointed out as the main reason. Especially, geometric duplicates can lead to unmanageable file sizes. In many cases, existing optimizers (Solibri IFC Optimizer and IFCCompressor) already can significantly reduce file size. However, when considering geometries, which have been placed directly into the <b>world</b> <b>coordinates</b> <b>system,</b> the optimizers do not eliminate such redundancy. Such models require the solution to the absolute orientation problem to identify duplicates.|$|R
50|$|FITS image headers {{can contain}} {{information}} about {{one or more}} scientific coordinate systems that are overlaid on the image itself. Images contain an implicit Cartesian coordinate system that describes the location of each pixel in the image, but scientific uses usually require working in 'world' coordinates, for example the celestial coordinate system. As FITS has been generalized from its original form, the <b>world</b> <b>coordinate</b> <b>system</b> (WCS) specifications have {{become more and more}} sophisticated: early FITS images allowed a simple scaling factor to represent the size of the pixels; but recent versions of the standard permit multiple nonlinear coordinate systems, representing arbitrary distortions of the image. The WCS standard includes many different spherical projections, including, for example, the HEALPix spherical projection widely used in observing the cosmic microwave background radiation.|$|E
50|$|Science {{data from}} HST {{arrive at the}} STScI {{a few hours after}} being downlinked from TDRSS and {{subsequently}} passing through a data capture facility at NASA's Goddard Space Flight Center. Once at STScI, the data are processed by a series of computer algorithms that convert its format into an internationally accepted standard (known as FITS: Flexible Image Transport System), correct for missing data, and perform final calibration of the data by removing instrumental artifacts. The calibration steps are different for each HST instrument, but as a general rule they include cosmic ray removal, correction for instrument/detector non-uniformities, flux calibration, and application of <b>world</b> <b>coordinate</b> <b>system</b> information (which tells the user precisely where on the sky the detector was pointed). The calibrations applied are the best available at the time the data pass through the pipeline. The STScI is working with instrument developers to define similar processes for Kepler and JWST data.|$|E
50|$|Usually {{those methods}} consist of two parts. The first stage is to detect {{interest}} points, fiducial markers or optical {{flow in the}} camera images. This step can use feature detection methods like corner detection, blob detection, edge detection or thresholding and/or other image processing methods. The second stage restores a real <b>world</b> <b>coordinate</b> <b>system</b> from the data obtained in the first stage. Some methods assume objects with known geometry (or fiducial markers) {{are present in the}} scene. In some of those cases the scene 3D structure should be precalculated beforehand. If part of the scene is unknown simultaneous localization and mapping (SLAM) can map relative positions. If no information about scene geometry is available, structure from motion methods like bundle adjustment are used. Mathematical methods used in the second stage include projective (epipolar) geometry, geometric algebra, rotation representation with exponential map, kalman and particle filters, nonlinear optimization, robust statistics.|$|E
40|$|AbstractThis paper {{introduced}} WGS- 84 and BJ- 54 coordinate transformation {{model and}} the influence comparison from geodetic elevation between seven-parameter model and three-parameter model, and finished transformation parameters calculation and accuracy analysis of BJ- 54 <b>coordinate</b> <b>system</b> at home and the <b>world</b> 84 <b>coordinate</b> <b>system</b> by an example, which had some reference to other GPS measurement projects...|$|R
40|$|Table 0. 1 : list {{of symbols}} A <b>world</b> {{inertial}} <b>coordinate</b> <b>system,</b> fixed system accx,y,z accelerometer measurements Ad air density B body fixed <b>coordinate</b> <b>system</b> b thrust factor bi gyro bias c propeller blade chord Cl lift coefficient d drag coefficient dl velocity momentum coupling coefficient g gravity acceleration I 3 3 by 3 unity matrix Ix,y,z moment of inertia J propeller moment of inerti...|$|R
40|$|Abstract. DEIMOS (the DEep Imaging Multi-Object Spectrograph) began {{producing}} {{scientific data}} from the Keck II telescope in 2002 June. The instrument is extremely configurable, and {{the form of the}} output data is highly variable. Filters and gratings may be swapped, gratings and mirrors tilt, readout modes and active amplifiers of the 8 -CCD mosaic change, and numerous field-specific astrometric slitmasks may be inserted. For archival purposes and to enable fully-automated data reduction, FITS files from DEIMOS document the instrument state, all aspects of the slitmask design, and multiple <b>world</b> <b>coordinate</b> <b>systems</b> for the mosaic images. The FITS files are compatible with existing local conventions for mosaic image display systems and also with incipient FITS WCS standards...|$|R
50|$|The HEALPix {{projection}} is {{a general}} class of spherical projections, sharing several key properties, which map the 2-sphere to the Euclidean plane. Any of these can be followed by partitioning (pixelising) the resulting region of the 2-plane. In particular, when one of these projections (the H=4, K=3 HEALPix projection) {{is followed by a}} pixelisation of the 2-plane, the result is generally known as the HEALPix pixelisation, which is widely used in physical cosmology for maps of the cosmic microwave background. This pixelisation {{can be thought of as}} mapping the sphere to twelve square facets (diamonds) on the plane followed by the binary division of these facets into pixels, though it can be derived without using the projection. The associated software package HEALPix implements the algorithm. The HEALPix projection (as a general class of spherical projections) is represented by the keyword HPX in the FITS standard for writing astronomical data files. It was approved as part of the official FITS <b>World</b> <b>Coordinate</b> <b>System</b> (WCS) by the IAU FITS Working Group on April 26, 2006.|$|E
50|$|In this method, {{proposed}} by Philipp Lindner and Gerd Wanielik, laser data is processed using a multidimensional occupancy grid. Data from a 4 layer laser is pre-processed at the signal level and then processed {{at a higher}} level to extract the features of the obstacles. A combination 2- and 3-dimensional grid structure is utilized and the space in these structures is tessellated into several discrete cells. This method allows a huge amount of raw measurement data to be effectively handled by collecting it in spatial containers, the cells of the evidence grid. Each cell is associated with a probability measure that identifies the cell occupation. This probability is calculated by using the range measurement of the lidar sensor obtained over time and a new range measurement, which are related using Bayes' theorem. A two dimensional grid can observe an obstacle in front of it, but cannot observe the space behind the obstacle. TO address this, the unknown state behind the obstacle is assigned a probability of 0.5. By introducing the third dimension or in other terms using a multi-layer laser, the spatial configuration of an object could be mapped into the grid structure to a degree of complexity. This is achieved by transferring the measurement points into a 3 dimensional grid. The grid cells which are occupied will possess a probability greater than 0.5 and the mapping would be color coded based on the probability. The cells which are not occupied will possess a probability less than 0.5 and this area will usually be white space. This measurement is then transformed to a grid coordinate system by using the sensor position on the vehicle and the vehicle position in the <b>world</b> <b>coordinate</b> <b>system.</b> The coordinates of the sensor depends upon its location on the vehicle and the coordinates of the vehicle is computed using egomotion estimation, which is estimating the vehicle motion relative to a rigid scene. For this method, the grid profile must be defined. The grid cells touched by the transmitted laser beam are calculated by applying Bresenham's line algorithm. To obtain the spatial extended structure, a connected component analysis of these cells is performed. This information is then passed on to a rotating caliper algorithm to obtain the spatial characteristics of the object. In addition to the lidar detection, RADAR data obtained by using two short range radars is integrated to get additional dynamic properties of the object, such as its velocity. The measurements are assigned to the object using a potential distance function.|$|E
3000|$|We {{define a}} <b>world</b> <b>coordinate</b> <b>system</b> to {{estimate}} gaze point location of individual test persons {{located in the}} classroom (Fig.  1). The origin of a <b>world</b> <b>coordinate</b> <b>system</b> is set at the floor level and at the left corner of the classroom, with the x [...]...|$|E
40|$|An {{imaging system}} with a single {{effective}} viewpoint is called a central projection system. The conventional perspective camera {{is an example of}} central projection system. A catadioptric realization of omnidirectional vision combines reflective surfaces with lenses. Catadioptric systems with an unique projection center are also examples of central projection systems. Whenever an image is acquired, points in 3 D space are mapped into points in the 2 D image plane. The image formation process represents a transformation from R 3 to R 2, and mathematical models can be used to describe it. This paper discusses the definition of <b>world</b> <b>coordinate</b> <b>systems</b> that simplify the modeling of general central projection imaging. We show that an adequate choice of the <b>world</b> <b>coordinate</b> reference <b>system</b> can be highly advantageous. Such a choice does not imply that new information will be available in the images. Instead the geometric transformations will be represented in a common and more compact framework, while simultaneously enabling newer insights. The first part of the paper focuses on static imaging systems that include both perspective cameras and catadioptric systems. A systematic approach to select the world reference frame is presented. In particular we derive <b>coordinate</b> <b>systems</b> that satisfy two differential constraints (the “compactness” and the “decoupling” constraints). These <b>coordinate</b> <b>systems</b> have several advantages for the representation of the transformations between the 3 D world and the image plane. The second part of the paper applies the derived mathematical framework to active tracking of moving targets. In applications of visual control of motion the relationship between motion in the scene and image motion must be established. In the case of active tracking of moving targets these relationships become more complex due to camera motion. Suitable <b>world</b> <b>coordinate</b> reference <b>systems</b> are defined for three distinct situations: perspective camera with planar translation motion, perspective camera with pan and tilt rotation motion, and catadioptric imaging system rotating around an axis going through the effective viewpoint and the camera center. Position and velocity equations relating image motion, camera motion and target 3 D motion are derived and discussed. Control laws to perform active tracking of moving targets using visual information are established. [URL]...|$|R
5000|$|... where F, G and H are {{arbitrary}} {{functions of}} position. It {{is not hard}} to imagine such a world; we live on one. The surface of the world is curved, which is why it's impossible to make a perfectly accurate flat map of the <b>world.</b> Non-Cartesian <b>coordinate</b> <b>systems</b> illustrate this well; for example, in the spherical coordinates (r, θ, φ), the Euclidean distance can be written ...|$|R
40|$|S 2 PLOT is a user-oriented {{programming}} {{library for}} generating and exploring 3 -dimensional (3 -d) scientific plots and diagrams. It provides a lightweight interface [...] -inspired {{by the simple}} yet widely-used PGPLOT [...] -to produce hardware-accelerated visualisations of point, line, image and volumetric data. S 2 PLOT provides C and FORTRAN interfaces, and supports monoscopic, stereoscopic and curved (eg. dome) display devices. PGPLOT-savvy astronomers can usually write their first S 2 PLOT program in less than ten minutes. In this paper, we introduce the latest S 2 PLOT version and highlight major new additions to the library, including volume rendering and isosurfacing of astronomical data. We describe a simple extension that enables the embedding of large-area FITS images directly into S 2 PLOT programs using standard <b>World</b> <b>Coordinate</b> <b>Systems,</b> and we introduce the Python interface to S 2 PLOT...|$|R
30|$|The {{extrinsic}} {{parameters of}} the left pair are calibrated using a unique <b>world</b> <b>coordinate</b> <b>system</b> for the two cameras and the projector. The intrinsic {{parameters of the}} projector and left camera have already been calibrated using a calibration plate with discrete markers on the surface, as described in Section 2 B. The extrinsic parameters are computed using the same coordinate position of the calibration plate to guarantee {{they are in the}} same <b>world</b> <b>coordinate</b> <b>system.</b>|$|E
30|$|Computing the {{relationship}} between the two cameras using the intrinsic parameters of the two cameras, center of each marker coordinates in two camera images, and world coordinates of the center of each marker. The transformation between the left camera and the projector is computed by establishing {{the relationship}}s between the <b>world</b> <b>coordinate</b> <b>system</b> and the projector coordinate system as well as between the same <b>world</b> <b>coordinate</b> <b>system</b> and the left camera coordinate system.|$|E
3000|$|... is {{identified}} by the transformation needed to align the <b>world</b> <b>coordinate</b> <b>system</b> to the camera coordinate system. This means that a translation vector, [...]...|$|E
40|$|We {{present a}} {{promising}} approach for combined visual model acquisition and visual servo control. The approach differs from previous {{work in that}} a full coupled Jacobian is estimated on-line without any prior models, introduction of special calibration movements, {{or the use of}} absolute <b>world</b> <b>coordinate</b> <b>systems.</b> A trust region controller allow stable and convergent control when the underlying visual-motor model is highly non-linear. On the high level actions and tasks are coded in terms of desired general perceptions rather than motor sequences. We argue that our vision space approach is particularly suited for easy teaching/programming of a robot. For instance a task can be taught by supplying an image sequence illustrating it. The resulting robot behavior is robust to changes in the environment, dynamically adjusting the motor control rules in response to environmental variation. We provide an extensive experimental evaluation of the positioning precision and convergence of the visual s [...] ...|$|R
40|$|Human limb segment angle {{tracking}} requires {{a system which}} can track through all orientations. The major problem addressed by this research {{was to develop a}} real time inertial motion tracking system based on quaternions to overcome the singularities of Euler angle filters. This work involves clarification of the theory behind quaternion attitude estimation and development of a system capable of determining the orientation of an object in <b>world</b> <b>coordinates.</b> <b>System</b> sensors were built using miniature accelerometers, rate sensors, and magnetometers. The software system was designed by using Unified Modeling Language (UML) with object oriented design techniques. The actual implementation created a real time orientation tracking system. The system was tested with dynamic tilt table experiments. Test results showed that the quaternion attitude estimation filter system can track human limb segments in real time within 1 degree of accuracy in an orientation and with a 55 Hz update rate. Turkish Navy author...|$|R
40|$|Robot arm manipulators, {{some thirty}} years after their {{commercial}} introduction, have found widespread application in structured industrial environments, performing, for instance, repetitive tasks in a mass production line. Successful application in unstructured environments however has proven much harder. Yet there are many such tasks where robots would be useful. We present a promising approach to visual (and more general sensing) robot control, that does not require modeling of robot transfer functions {{or the use of}} absolute <b>world</b> <b>coordinate</b> <b>systems,</b> and thus is suitable for use in unstructured environments. Our approach codes actions and tasks in terms of desired general perceptions rather than motor sequences. We argue that our vision space approach is particularly suited for easy teaching/programming of a robot. For instance a task can be taught by supplying an image sequence illustrating it. The resulting robot behavior is robust to changes in the environment, dynamically adjustin [...] ...|$|R
