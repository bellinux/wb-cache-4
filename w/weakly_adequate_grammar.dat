0|37|Public
5000|$|... "A {{linguistic}} {{theory that}} aims for explanatory adequacy {{is concerned with}} the internal structure of the device grammar; that is, it aims to provide a principled basis, independent of any particular language, for the selection of the descriptively <b>adequate</b> <b>grammar</b> of each language." ...|$|R
40|$|Mini-conference on Tuesday (10 - 1, but {{we should}} be done around 12 : 40) in conference room. Prepare handout for 15 -minute presentation, 5 minutes of questions. • Papers due Friday, March 23 (PDF by e-mail is fine). Overview: How can we find out what generalizations are real to the speaker? How can we find out whether some generalizations are better than others? 1 Back to the Chomskyan basics 1 Let a grammar consist of (at least) 2 • a {{function}} that labels any utterance as grammatical or ungrammatical. • a function that assigns truth conditions to any utterance The grammar might be implemented as a lexicon {{and a list of}} rules, or a set of constraints, or something else. Let a linguistic theory be a function that, given a (finite) set of utterances (the learning data), produces a grammar. 3 These functions should be accompanied by algorithms for calculating them. So [...] . • an observationally <b>adequate</b> <b>grammar</b> labels the utterances that a typical learner would encounter as grammatical (perhaps trivially, e. g. by listing them), and assigns the right truth conditions to them. • a descriptively <b>adequate</b> <b>grammar</b> captures the psychologically real generalizations • the real prize, an explanatorily adequate theory, will, given typical learning data, return an descriptively <b>adequate</b> <b>grammar</b> But how do we figure out what the psychologically real generalizations are????? 2 Example: English noun plurals cat k�æt k�æts pea p�i p�iz sack sæk sæks cow k�a � k�a�z dog d� � d��z man mæn m�n grub ���b ���bz foot f�t fit dish d� � d���z wife wa�f wa�vz fudge f�d� � f�d���z whiff w�f w�fs 1 Mostly Chomsky 1965 pp. 25 - 27 but an amalgam of various Chomsky works, a simplified and colored by my own views...|$|R
40|$|Automating the {{construction}} of semantic grammars is a difficult and interesting problem for machine learning. This paper shows how the semantic-grammar acquisition problem {{can be viewed as}} the learning of search-control heuristics in a logic program. Appropriate control rules are learned using a new first-order induction algorithm that automatically invents useful syntactic and semantic categories. Empirical results show that the learned parsers generalize well to novel sentences and out-perform previous approaches based on connectionist techniques. Introduction Designing computer systems to "understand" natural language input is a difficult task. The laboriously hand-crafted computational grammars supporting natural language applications are often inefficient, incomplete and ambiguous. The difficulty in constructing <b>adequate</b> <b>grammars</b> {{is an example of the}} "knowledge acquisition bottleneck" which has motivated much research in machine learning. While numerous researchers have studied [...] ...|$|R
40|$|This article {{summarizes}} {{the findings of}} some of our studies of the data base of syntactic theory, contrasting the characteristics of frequency data and judgement data. Examination of frequency data reveals that the factors affecting its production interact competitively and probabilistically. This contrasts strongly with the patterns observed in judgement data, which point to a system in which violations of constraints produce negative weightings on form/meaning pairs. Since both data types are the result of human linguistic processing, we present a model of the architecture that such a system might have in order to produce such contrasting data. This Decathlon Model has two modules: Constraint Application and Output Selection. The first is blind, exceptionless and applies violation costs cumulatively (Keller 2000), the second is competitive and probabilistic. This constrains frameworks of syntactic explanation: an empirically <b>adequate</b> <b>grammar</b> must include gradien...|$|R
2500|$|In the 1960s, Chomsky {{introduced}} two central ideas {{relevant to}} the construction and evaluation of grammatical theories. The first was the distinction between competence and performance. Chomsky noted the obvious fact that people, when speaking in the real world, often make linguistic errors (e.g., starting a sentence and then abandoning it midway through). He argued that these errors in linguistic performance were ir{{relevant to the}} study of linguistic competence (the knowledge that allows people to construct and understand grammatical sentences). Consequently, the linguist can study an idealised version of language, greatly simplifying linguistic analysis (see the [...] "Grammaticality" [...] section below). The second idea related directly to the evaluation of theories of grammar. Chomsky distinguished between grammars that achieve descriptive adequacy and those that go further and achieved explanatory adequacy. A descriptively <b>adequate</b> <b>grammar</b> for a particular language defines the (infinite) set of grammatical sentences in that language; that is, it describes the language in its entirety. A grammar that achieves explanatory adequacy has the additional property that it gives an insight into the underlying linguistic structures in the human mind; that is, it does not merely describe the grammar of a language, but makes predictions about how linguistic knowledge is mentally represented. For Chomsky, the nature of such mental representations is largely innate, so if a grammatical theory has explanatory adequacy it must be able to explain the various grammatical nuances of the languages of the world as relatively minor variations in the universal pattern of human language. Chomsky argued that, even though linguists were still a long way from constructing descriptively <b>adequate</b> <b>grammars,</b> progress in terms of descriptive adequacy will only come if linguists hold explanatory adequacy as their goal. In other words, real insight into the structure of individual languages can only be gained through comparative study {{of a wide range of}} languages, on the assumption that they are all cut from the same cloth.|$|R
40|$|One of {{the major}} {{arguments}} which {{have led to the}} attempted con-struction of so-called text grammars {{is based on the assumption}} that the notion of grammaticalness is not absolute but relative. 1 That is, a sentence can be characterized to be grammatical only with respect to an ordered set (possibly empty) of other sentences, preceding it in a discourse. An <b>adequate</b> <b>grammar</b> may hence be required to explicate this type of grammatical relativity by formulating rules and constraints determining how the structure of sentences depends on the structure of the preceding (or perhaps following) sentences. 2 It has been shown that this sort of dependency is primarily semantic, whereas morphophonological and syntactical discourse constraints derive from the underlying semantic ones. The intuitive notion thus reconstructed is that of coherence. The semantic discourse constraints at issue pertain both to meaning and reference, and can be made explicit in an appropriate model-theoreti...|$|R
40|$|Syntactic models {{should be}} descriptively {{adequate}} and parsable. A syntactic description is autonomous {{in the sense}} that it has certain explicit formal properties. Such a description relates to the semantic interpretation of the sentences, and to the surface text. As the formalism is implemented in a broad-coverage syntactic parser, we concentrate on issues that must be resolved by any practical system that uses such models. The correspondence between the structure and linear order is discussed. 1 Introduction The aim of this paper is to define a dependency grammar framework which is both linguistically motivated and computationally parsable. A linguistically <b>adequate</b> <b>grammar</b> is the primary target because if we fail to define a descriptive grammar, its application is less useful for any linguistically motivated purposes. In fact, our understanding of the potential benefits of the linguistic means can increase only if our practical solutions stand on an adequate descriptive basis. Trad [...] ...|$|R
40|$|An <b>adequate</b> <b>grammar</b> of a {{language}} should show the relationship between sensesor concepts and the symbols (vocal or visual) used to convey them. This presentation attempts to identify the various conceptual elements contained in simple sentences in the standard written Bahasa Indonesia and show how these elements contribute towards determining the conventional representation of these sentences. On the basis of different configurations of various conceptional elements involved simple sentence in Bahasa Indonesia may signify either {{one of the following}} types of predications: 1) a process, with the principal, 2) an action with the actor, 3) a qualification with the principal and its attribute, 4) an existence with the principal and the place, 5) an identification with the principal and its identity, 6) a possession with the possessor and the possessed, 7) an attitude with the principal the actor, the patient and the beneficiary, 10) an action with the actor, and patient, and the starting and the terminal place...|$|R
40|$|Results of {{computational}} complexity {{exist for}} a wide range of phrase structure-based grammar formalisms, while there is an apparent lack of such results for dependencybased formalisms. We here adapt a result on the complexity of ID/LP-grammars to the dependency framework. Contrary to previous studies on heavily restricted dependency grammars, we prove that recognition (and thus, parsing) of linguistically <b>adequate</b> dependency <b>grammars</b> is N P-complete. ...|$|R
25|$|The Nauruan {{language}} {{is the official}} language of Nauru. English is widely understood and is used for most government and commercial purposes. According to the 2011 census, 95.3% of the population speaks Nauruan, 66.0% speak English, and 11.9% speak another language. Nauruan is an Austronesian language, however, no <b>adequate</b> written <b>grammar</b> of the language has been compiled, and its relationships to other Micronesian languages are not well understood.|$|R
5000|$|Markedness entered {{generative}} linguistic theory through Chomsky and Halle's The Sound Pattern of English. For Chomsky and Halle, phonological features {{went beyond}} a universal phonetic vocabulary to encompass an 'evaluation metric', {{a means of}} selecting the most highly-valued <b>adequate</b> <b>grammar.</b> In The Sound Pattern of English, {{the value of a}} grammar was the inverse of the number of features required in that grammar. However, Chomsky and Halle realized that their initial approach to phonological features made implausible rules and segment inventories as highly valued as natural ones. The unmarked value of a feature was cost-free with respect to the evaluation metric, while the marked feature values were counted by the metric. Segment inventories could also be evaluated according to the number of marked features. However, the use of phonological markedness as part of the evaluation metric was never able to fully account for the fact that some features are more likely than others or for the fact that phonological systems must have a certain minimal complexity and symmetry ...|$|R
40|$|Results of {{computational}} complexity {{exist for}} a wide range of phrase structure-based grammar formalisms, while there is an apparent lack of such results for dependency-based formalisms. We here adapt a result on the complexity of ID/LP-grammars to the dependency framework. Contrary to previous studies on heavily restricted dependency grammars, we prove that recognition (and thus, parsing) of linguistically <b>adequate</b> dependency <b>grammars</b> is NP-complete. Comment: 8 pages, requires LaTeX 2 e, epsfig, latexsym, amsmat...|$|R
40|$|What someone didn’t say can be revealing. Consider the {{following}} familiar passage. Although even descriptive adequacy {{on a large}} scale is by no means easy to approach, it is crucial for the productive development of linguistic theory that much higher goals than this be pursued. To facilitate the clear formulation of deeper questions, it is useful to consider the abstract problem of constructing an “acquisition model ” for language, that is, a theory of language learning or grammar construction. (pp. 24 - 25) 1 As possible contrasts, imagine that Chomsky had instead written (1) or (2). (1) It’s hard to formulate descriptively <b>adequate</b> <b>grammars.</b> But sometimes, it helps to ask how children could acquire the languages they do acquire. (2) Approximating descriptive adequacy is already hard. But in linguistics, the real goal is to explain how children acquire the languages they do acquire. Alternative (1) would have suggested that the linguist’s job is to describe languages, but that as with many difficult tasks—e. g., achieving wisdom—indirect methods can be useful. And as discussed below, Chomsky’s conception of descriptive adequacy was already quite demanding. Yet he urged linguists to strive for more...|$|R
50|$|Functional grammar (FG) and {{functional}} discourse grammar (FDG) are grammar models and theories motivated by functional theories of grammar. These theories explain how linguistic utterances are shaped, {{based on the}} goals and knowledge of natural language users. In doing so, it contrasts with Chomskyan transformational grammar. Functional discourse grammar has been developed as a successor to functional grammar, attempting to be more psychologically and pragmatically <b>adequate</b> than functional <b>grammar.</b>|$|R
40|$|Free {{idempotent}} generated semigroups IG(E), where E is a biordered set, {{have provided}} a focus of recent research, {{the majority of the}} efforts concentrating on the behaviour of the maximal subgroups. Inspired by an example of Brittenham, Margolis and Meakin, several proofs have been offered that any group occurs as a maximal subgroup of some IG(E), the most recent being that of Dolinka and Ruškuc, who show that E can be taken to be a band. From a result of Easdown, Sapir and Volkov, periodic elements of any IG(E) lie in subgroups. However, little else is known of the `global' properties of IG(E), other than that it need not be regular, even where E is a semilattice. Since its introduction by Fountain in the late 1970 s, the study of abundant and related semigroups has given rise to a deep and fruitful research area. The classes of abundant and adequate semigroups extend those of regular and inverse semigroups, respectively, and themselves are contained in the classes of weakly abundant and <b>weakly</b> <b>adequate</b> semigroups. Recent significant developments include the description by Kambites, using birooted labelled trees, of the free semigroups in the quasi-variety of adequate semigroups. Our main result shows that for any band B, the semigroup IG(B) is a weakly abundant semigroup and moreover satisfies a natural condition called the congruence condition. We show that if B is a band for which uv=vu=v for all u,v∈ B with BvB⊂ BuB (a condition certainly satisfied for semilattices), then IG(B) is abundant with solvable word problem. Further, IG(B) is also abundant for a normal band B for which IG(B) satisfies a given technical condition, and we give examples of such B. On the other hand, we give an example of a normal band B such that IG(B) is not abundant...|$|R
40|$|Deciding {{whether a}} {{synchronous}} grammar formalism generates a given word alignment (the alignment coverage problem) depends on finding an <b>adequate</b> instance <b>grammar</b> and then {{using it to}} parse the word alignment. But {{what does it mean}} to parse a word alignment by a synchronous grammar? This is formally undefined until we define an unambiguous mapping between grammatical derivations and word-level alignments. This paper proposes an initial, formal characterization of alignment coverage as intersecting two partially ordered sets (graphs) of translation equivalence units, one derived by a grammar instance and another defined by the word alignment. As a first sanity check, we report extensive coverage results for ITG on automatic and manual alignments. Even for the ITG formalism, our formal characterization makes explicit many algorithmic choices often left underspecified in earlier work. ...|$|R
40|$|Results of {{computational}} complexity {{exist for}} a wide range of phrase structure-based grammar formalisms, while there is an apparent lack of such results for dependency-based formalisms. We here adapt a result on the complexity of ID/LP-grammars to the dependency framework. Contrary to previous studies on heavily restricted dependency grammars, we prove that recognition (and thus, parsing) of linguistically <b>adequate</b> dependency <b>grammars</b> is NP-complete. 1 Introduction The introduction of dependency grammar (DG) into modern linguistics is marked by Tesnière (1959). His conception addressed didactic goals and, thus, did not aim at formal precision, but rather at an intuitive understanding of semantically motivated dependency relations. An early formalization was given by Gaifman (1965), who showed the generative capacity of DG to be (weakly) equivalent to standard context-free grammars. Given this equivalence, interest in DG as a linguistic framework diminished considerably, although many [...] ...|$|R
40|$|The {{purpose of}} this paper is to briefly examine two {{proposed}} extensions of statistical/probabilistic methodology, long familiar to the sciences, to linguistics. On the one hand it will be argued that the invocation of probabilistic measures is indispensable to any sensible criteria of grammatical adequacy, and on the other hand it will be suggested that probabilistic automata can be relevant to studies of language behavior. 1. The fully <b>adequate</b> (categorial/generative) <b>grammar</b> is one with which there corresponds an algorithm by means of which we can (recognize/ generate) all and only those syntactically correct sequences in the corresponding language. At this writing, there does not exist any such 2 ̆ 7 ideal 2 ̆ 7 grammar for any natural language; and as long as this situation remains, it will be necessary for the linguist to 2 ̆ 7 rank 2 ̆ 7 competing grammars for both reasons of suitability for corpora, and assessment in terms of potential adequacy. Because of the prima facie potential of the transformational grammars introduced since the mid- 19502 ̆ 7 s, linguists have not made any rigorous attempt at providing a measure of descriptive adequacy of grammars. Lately, such intuitive criteria as simplicity, intuitivity, economy, etc. have been levied against competing grammars, in adjudication of adequacy. But these are certainly not the kinds of objective criteria necessary to any independently valuable method of resolving disputes over relative adequacy. This is not to say that these quasi-criteria are without import to the linguist. Surely, in a ceteris paribus situation it is reasonable to prefer the simpler model to the more complex. But up to now there is no method of 2 ̆ 7 ranking 2 ̆ 7 available by which we can determine when a ceteris paribus situation obtains. In linguistics, just as in the sciences, only when the adequacies of competing models are established are issues of simplicity, economy and the like, germane. Certainly, the application of statistical/probabilistic procedures to the field of linguistics is not new. Precedents have been established in taxonomic studies, analyses of distributions of word types in corpora (viz. Zipf 2 ̆ 7 s Law), etc. But the notion of using an interjacent probabilistic grammar in determining descriptive adequacy is quite innovative. Of the recent developments in this area, perhaps the most notable is that of Suppes (1970). Suppes 2 ̆ 7 motivation for this paper was the disregard of conventional grammatical models to such fundamental and universal characteristics of natural languages as relatively short utterance length, predominance of grammatically simple utterances, etc. It seems irrational to Suppes to be tolerant of grammars which pay an inordinate amount of attention to those syntactic structures which are 2 ̆ 7 deviant, 2 ̆ 7 or at least atypical of general usage, and whose relative frequency of occurrence in the corpus is low. To put the matter differently, if any putatively <b>adequate</b> <b>grammar</b> is to be of value, it must be able to account for a sizeable portion of the corpus, thereby identifying those grammatical types which demand further scrutiny. In order to establish the relative values for alternative grammars, Suppes suggests we consult a probabilistic grammar...|$|R
40|$|Attribute Grammars (AGs) are a {{generalization}} {{of the concept}} of Context-Free Grammars (CFGs). The formalism of AGs has been widely used for the specification and implementation of programming languages. On the other hand there is an intimate relationship between AGs and Logic Programming. The paper presents a parallel method for learning semantic functions of Attribute Grammars (AGs) based on AGLEARN (2) using PAGE system (12). The method is more efficient in both execution time and interaction needed than the sequential one. The method presented is <b>adequate</b> for S-attributed <b>grammars</b> and for L-attributed grammars as well...|$|R
40|$|This {{dissertation}} {{provides an}} explicit syntactic and semantic {{account for a}} reasonably large sample of question constructions in Swedish. Within generative grammar, the existence of non-local dependencies as in constituent questions has been taken as evidence for the need to postulate transformational rules in the grammar of natural languages. Recently a number of linguists have proposed ways of handling such dependencies without transformations. Until now, these proposals {{have been based on}} English. In this study, we investigate the possibility of extending non-transformational approaches to languages like Swedish where question formation differs from English in a significant way. In Swedish, more than one constituent can be extracted from a clause. We discuss the consequences of this fact for transformational and non-transformational approaches to Swedish. It is shown that the non-transformational approaches need to be substantially modified in order to provide a syntactically and semantically <b>adequate</b> <b>grammar</b> for Swedish. The implications of these modifications are assessed {{from the point of view}} of choosing between grammars. The main part of the dissertation consists of an analysis of the semantics of constituent questions. We propose an extension to the semantics for questions in the framework of Montague grammar given by Hamblin and Karttunen. Most current approaches to questions take the entire question phrase to be the interrogative quantifier. We point out that these approaches are not adequate for questions where the interrogative phrase contains an anaphor bound from inside the sentence. In addition, these approaches cannot account for all readings of temporally ambiguous sentences. To allow the semantic rules to handle such cases as well, a more general approach to questions is proposed. On this approach, only the 2 ̆ 7 which 2 ̆ 7 part of the question phrase constitutes the interrogative quantifier. This quantifier ranges not over individuals directly, as in the previous theories, but over functions that pick out sets of individuals. In simple questions, the result of the proposed analysis is tantamount to the results on earlier approaches. However, it is shown that only the proposed approach can generalize to more complex questions. The analysis proposed here is compared to current approaches to questions within transformational grammar. Finally, we discuss the relative merits of a structurally based and a semantically based approach to anaphoric relations...|$|R
40|$|International audienceWe {{design and}} {{investigate}} a sequential discontinuous Galerkin method to approximate two-phase immiscible incompressible flows in heterogeneous porous media with discontinuous capillary pressures. The nonlinear interface conditions are enforced <b>weakly</b> through an <b>adequate</b> {{design of the}} penalties on interelement jumps of the pressure and the saturation. An accurate reconstruction of the total velocity is considered in the Raviart-Thomas(-Nedelec) finite element spaces, together with diffusivity-dependent weighted averages to cope with degeneracies in the saturation equation and with media heterogeneities. The proposed method is assessed on one-dimensional test cases exhibiting rough solutions, degeneracies, and capillary barriers. Stable and accurate solutions are obtained without limiters...|$|R
40|$|Until fairly recently, {{word order}} {{variation}} {{was assigned to}} factors such as, 2 ̆ 7 taste 2 ̆ 7, 2 ̆ 7 idiolect 2 ̆ 7, and 2 ̆ 7 style 2 ̆ 7 (Ross 1967 : 44). The present study {{is an attempt to}} show that the use of word order in Hindi, a flexible word order language, is more than this. Word order variation can essentially perform the same functions as movement transformations. The two are similar in their function and operation in that they allow rearrangement of elements or constituents, and indicate semantic and pragmatic differences. ^ So far, very little has been done in linguistics on the subject of word order in Hindi. I have shown that this is a much larger subject than might be supposed. An <b>adequate</b> <b>grammar</b> of Hindi ought to take this subject into account. In this study, I have touched upon the following areas of word order: basic word order, syntactic restrictions, and discourse functions of varying orders. ^ Chapter 1, an introduction to the present study, argues that word order variation in flexible word order languages like Hindi is similar to movement transformations in languages with less flexible word orders. In chapter 2, I deal with the issue of basic word order in general theory and in Hindi. With the help of a few tests I support the general hypothesis that the basic word order in Hindi is SOV. In chapter 3, I look at the syntactic restrictions on the movement of elements or constituents in simple and complex sentences. I show that there are very few constraints on the movement of major constituents but that there are severe constraints on the movement of elements or constituents when they move into other constituents or phrases. In chapter 4, I focus on one particular movement, Topicalization. I show that in Hindi, a formal constraint cannot explain all the data concerning topicalization but that these can be explained with the functional notion 2 ̆ 7 topicality 2 ̆ 7. In chapter 5, I demonstrate that difference in Hindi word order mainly serve to achieve different discourse functions, including suspense, emphasis, de-emphasis, and announcement of topic. The use of word order variation for expressing grammatical relations and semantic reasons is limited. The sixth and last chapter contains a summary and conclusions of the present study. ...|$|R
40|$|We {{design and}} {{investigate}} a sequential discontinuous Galerkin method to approximate two-phase immiscible incompressible flows in heterogeneous porous media with discontinuous capillary pressures. The nonlinear interface conditions are enforced <b>weakly</b> through an <b>adequate</b> {{design of the}} penalties on interelement jumps of the pressure and the saturation. An accurate reconstruction of the total velocity is considered in the Raviart-Thomas(-Nedelec) finite element spaces, together with diffusivity-dependent weighted averages to cope with degeneracies in the saturation equation and with media heterogeneities. The proposed method is assessed on one-dimensional test cases exhibiting rough solutions, degeneracies, and capillary barriers. Stable and accurate solutions are obtained without limiters. (C) 2010 Elsevier B. V. All rights reserved. Groupement MoMaS (PACEN/CNRS, ANDRA, BRGM, CEA, EdF, IRSN), FranceGroupement MoMaS (PACEN/CNRS, ANDRA, BRGM, CEA, EdF, IRSN), Franc...|$|R
40|$|Abstract- The {{mastery of}} grammar forms {{a basis for}} {{achieving}} proficiency in the four language skills. Teachers are, therefore, supposed to ensure that learners acquire <b>adequate</b> competence in <b>grammar.</b> In order to achieve this, the areas taught in grammar include: parts of speech, phrases, clauses and sentences. Proficiency in oral skills in the English language is tested in two ways, namely through written and oral methods. Based on a study conducted in secondary schools within Eldoret Municipality, this paper examines the relationship between scores attained in the two tests. The study was formulated and interpreted with reference to Communicative Language Teaching (CLT) approach. The study used correlation research design, which enabled the researcher to assess the degree of relationship between the scores attained from a written and an oral test of five sub-skills of the speaking skill. This assisted in establishing th...|$|R
40|$|This paper {{presents}} a new connectionist approach to grammatical inference. Using only positive examples, the algorithm learns regular graph grammars, representing two-dimensional iterative structures drawn on a discrete Cartesian grid. This work is {{intended as a}} case study in connectionist symbol processing and geometric conceptformation. A grammar is represented by a self-configuring connectionist network that is analogous to a transition diagram except that it can deal with graph grammars as easily as string grammars. Learning starts with a trivial grammar, expressing no grammatical knowledge, which is then refined, by a process of successive node splitting and merging, into a <b>grammar</b> <b>adequate</b> to describe the population of input patterns. In conclusion, I argue that the connectionist style of computation is, in some ways, better suited than sequential computation to the task of representing and manipulating recursive structures. 1. Introduction Connectionism is conventionally seen [...] ...|$|R
40|$|This paper {{gives an}} {{introduction}} into {{the principles of}} interactivity in music video games. Music video games are an old but small genre of games. The earliest direct ancestors emerged in the 1970 ies. Some recent music video games were hugely successful. Until today, {{there are only a}} few different approaches to their design. The purpose of this article is to shed light on what these design principles are, and how the player is immersed. By analysing several games qualitatively, we extracted certain typical features of games of this genre: active scores, rhythm action, quantisation, synaesthesia, play as performance, free-form play, and sound agents. All these aspects of music video games are discussed in this paper with the aim of describing how they affect the interactivity of the games. The result is a grammar of the language of music video games. Linked to <b>adequate</b> metaphors, this <b>grammar</b> can build a veritable repository for rhythm based, melodically interactive games and digital electronic instruments...|$|R
40|$|This paper {{presents}} Infinite RAAM (IRAAM), a new {{fusion of}} recurrent neural networks with fractal geometry, {{allowing us to}} understand the behavior of these networks as dynamical systems. Our recent work with IRAAMs has shown that {{they are capable of}} generating the context-free (non-regular) language � Ò � Ò for arbitrary values of Ò. This paper expands upon that work, showing that IRAAMs are capable of generating syntactically ambiguous languages but seem less capable of generating certain context-free constructions that are absent or disfavored in natural languages. Together, these demonstrations support our belief that IRAAMs can provide an explanatorily adequate connectionist model of grammatical competence in natural language. Natural Language Issues In an early and extremely influential paper, Noam Chomsky (1956) showed that natural languages (NL’s) cannot be modeled by a finite-state automaton, because of the existence of center-embedded constructions. A second and equally important observation from this work was that a minimally <b>adequate</b> NL <b>grammar</b> must be ambiguous, assigning more than one structure (interpretation) to some sentences, for example, They are flying planes. The first observation {{led to the development of}} Chomsky’s formal hierarchy of languages, based on the computational resources of the machines needed to recognize them. In this hierarchy, Chomsky’s observation about center-embedding is expressed by saying that NL’s are non-regular; i. e., they cannot be generated by a grammar having only rules of the form � � ��, where � and � are non-terminal symbols and � is a terminal symbol. Whether NL’s are merely non-regular, belonging in the next, context-free (CF) level of the Chomsky hierarchy, or are more powerful, belonging further up in the hierarchy, became the subject of heated debate (Higginbotham 1984; Postal and Langendoen 1984; Shieber 1985). Non-CF phenomena such as reduplication/copying (Culy 1985) and crossed serial dependencie...|$|R
40|$|It has {{not been}} my aim to provide {{conclusive}} evidence for or against anyone hypothesis regarding Time and Tense. I have simply attempted to collect together and collate much {{of what has been}} written on the topic of tense in English, in order to show what the current trends of thought are. In Chapter One I presented a brief survey of some of the more basic notions associated with time and tense, in order to provide a background for the more linguistic approach to follow. I therefore examined such issues as the difference between time and tense, the problem of the passage and directionality of time, of the present moment, time and space, tense as a universal, "and various features of tense systems. I sketched Bull's system of scalars, vectors and axes as representative of our English tense system. Chapter Two dealt with time and logic, but as I am a mere layman in matters logical, I refrained from discussing any individual logical system in depth, and rather discussed various problems which appear to confront the logician in formulating a tensed or tenseless logic. This chapter aimed at providing {{a better understanding of the}} linguistic issues to follow, for time and logic are intimately connected with language. Chapter Three was more linguistically oriented, and in it I attempted to provide a broad outline of the development of thoughts about tense before the Transformationalist period (pre 1960). Because of the vast scope involved, I had, perforce, to be brief at times. I gave attention to tense in classical grammatical studies, and summarized how it was seen from about 1500 to 1800. I gave more detailed treatment to the twentieth century, focussing specifically on grammarians like Jespersen (1933), Twaddell (1960), Ota (1963), Palmer (1965) and others - all, writers typical of the structuralist era. At the end of Chapter Three I provided an overall summary of ideas on the main tenses by the end of the structuralist period - ideas which were to change radically within the next few years. In Chapter Four I discussed the ideas of tense of some of the main transformationalist/generativists - Diver (1964), Crystal (1966), Huddlestone (1968), Gallagher (1970), McCawley (1971) and Seuren (1974), in an attempt to show how theories on tense were becoming increasingly abstract, and how most data indicated that it is highly probable that tense is an abstract higher predicate of the sentence in which it appears in surface structure, closely related to temporal adverbs. Chapter Five continued in the same vein. I tried to show, using syntactic tests, that tense is a higher predicate, and used arguments involving Conjunction Reduction (based on Kiparsky (1968)), VP Constituency, Sequence of Tense, Pronominalization, and Quantification. In Chapter Six I focussed more closely on tense-time adverbials, in order to show that they have the same syntactic properties as tense, are also probably deep superordinate predicates, and are closely related to tense. My suggestion was that either tense is derived from temporal adverbs or vice versa, as this would simplify the grammar. The derivation procedures at the end of the chapter (6. 8) were largely based on Hausmann (1971). I made no detailed reference to extralinguistic matters which affect tenses, in this study - such factors as are diScussed by G. Lakoff (1971) (presuppositions and relative well-formedness) and by R. Lakoff (1975). Tense is not a matter of pure Structuralism, just as language is not - extralinguistic factors ought to be accounted for before any study can claim to be conclusive. For this reason I do not in any way claim to have made an exhaustive study of time and tense - I have simply attempted to summarize and coordinate thoughts on the subject, and to suggest tentatively that the most <b>adequate</b> <b>grammar</b> of English would probably derive tense from underlying temporal adverbs. ...|$|R
40|$|Abstract Objective: to psychometrically {{test the}} Brazilian {{version of the}} Treatment Satisfaction Questionnaire for Medication - TSQM (version 1. 4), {{regarding}} ceiling and floor effect, practicability, acceptability, reliability and validity. Methods: participants with coronary heart disease (n= 190) were recruited from an outpatient cardiology clinic at a university hospital in Southeastern Brazil and interviewed to evaluate their satisfaction with medication using the TSQM (version 1. 4) and adherence using the Morisky Self-Reported Measure of Medication Adherence Scale and proportion of adherence. The Ceiling and Floor effect were analyzed considering the 15 % worst and best possible TSQM scores; Practicability was assessed by time spent during TSQM interviews; Acceptability by proportion of unanswered items and participants who answered all items; Reliability through the Cronbach's alpha coefficient and Validity through the convergent construct validity between the TSQM and the adherence measures. Results: TSQM was easily applied. Ceiling effect {{was found in the}} side effects domain and floor effect in the side effects and global satisfaction domains. Evidence of reliability was close to satisfied in all domains. The convergent construct validity was partially supported. Conclusions: the Brazilian TSQM presents evidence of acceptability and practicability, although its validity was <b>weakly</b> supported and <b>adequate</b> internal consistency was observed for one domain...|$|R
50|$|From its {{inception}} {{as in the}} 1950s, the Chomskyan brand of linguistics has been concerned with a person's knowledge of language, the unconscious mental knowledge that makes him a native speaker of some language. This innate, unobservable knowledge of language is called the speaker's linguistic competence, whereas his outward linguistic behavior which provides the linguist with observable data is called his linguistic performance. Generative grammar tries to provide an adequate model of linguistic competence. When such a grammar can generate (i.e. provide an explicit structural description for) all the grammatical (i.e. syntactically well-formed) sentences of a language and successfully detect all the ungrammatical sentences, it is called observationally adequate. But this kind of adequacy {{does not provide a}} sufficiently adequate model of linguistic competence. If the grammar can systematically and accurately describe the adult speaker's linguistic competence, it is called descriptively <b>adequate.</b> If a <b>grammar</b> can describe the acquisition of this linguistic competence in a human child from its initial state to the final state, then it is said to have explanatory adequacy. In Government and Binding theory, Chomsky is interested in the initial state of linguistic competence in a child, which is called a human's biological endowment for language. Since every normal human child is capable of acquiring any language, this endowment is also called by the name of universal grammar.|$|R
40|$|A {{procedure}} is described which induces type-logical grammar lexicons from sentences annotated with skeletal {{terms of the}} simply typed lambda calculus. A generalized formulae-as-types correspondence is exploited to obtain all the typelogical proofs of the sample sentences from their lambda terms, and the resulting lexicons are then optimally unified, which e#ectively unifies the syntactic categories of words which have the same syntactic behavior evident in the induced structures. This e#ort extends the earlier induction of such lexicons for classical categorial grammar (Buszkowski and Penn, 1990) to at first the non-associative Lambek calculus, {{and then to a}} large class of type logics enriched by modal operators and structural rules. The motivation for this approach is linguistic [...] -we have implemented a theoretically operational procedure for semantic bootstrapping of natural language syntax, which is the first one in any setting with su#cient scope {{to meet the demands of}} descriptively <b>adequate</b> natural language <b>grammars.</b> One of the main points of the enterprise is that the syntactic and semantic categories operating in the language are learned, in direct opposition to more familiar grammar induction procedures which begin with a fixed set of categories and frequently part-of-speech tagged data as well. This general approach could be used by linguists to learn something about lexical categories and to develop linguistically insightful complete grammars for large fragments...|$|R
40|$|We {{have been}} {{developing}} a spoken language system {{to recognize and}} understand spontaneous speech. It is difficult for such systems to achieve good coverage of the lexicon and grammar that subjects might use because spontaneous speech often contains disfluencies and ungrammatical constructions. Our goal is to respond appropri-ately to input, even though coverage is not complete. The natural language component of our system is oriented toward the extraction of information relevant to a task, and seeks to directly optimize the correctness of the extracted information (and therefore the system response). We use a flexible frame-based parser, which parses {{as much of the}} input as possible. This approach leads both to high accuracy and robustness. We have implemented a version of this system for the Air Travel Information Service (ATIS) task, which is being used by several ARPA-funded sites to develop and evaluate speech understanding systems. Users are asked to perform a task that requires getting information from an Air Travel database. In this paper, we describe recent improvements in our system resulting from our efforts to improve the coverage given a limited amount of training data. These improvements address a number of problems including generating an <b>adequate</b> lexicon and <b>grammar</b> for the rec-ognizer, generating and generalizing an appropriate grammar for the parser, and dealing with ambiguous parses. 1...|$|R
40|$|The Saami {{language}}s, {{a branch}} of the Uralic language family, are a group of nine rather closely related but mutually mostly unintelligible languages spoken by the indigenous Saami people of Central and Northern Scandinavia, Northern Finland and extreme Northwestern Russia. All of the languages are either endangered, mostly seriously so, or moribund. The study of Saami languages has a very long tradition in the field of Uralic linguistics. Even though the research tradition has been heavily oriented towards historical linguistics, previous generations of scholars have also compiled vast amounts of primary language materials such as audio recordings and texts. In this sense, thus, most Saami languages can be characterized as very well documented. From a descriptive point of view, however, the situation is somewhat different: <b>adequate</b> reference <b>grammars</b> are few, and dictionaries of many languages are in an outdated format, employing a narrow phonetic transcription. Himmelmann (1998) has drawn attention to the distinction between ‘documentary’ and ‘descriptive’ linguistics, emphasizing the importance of extensive primary documentation that can serve a variety of linguistic and other purposes. In contrast, work aiming at the description of less studied languages has often been oriented towards the publication of reference grammars and dictionaries, sometimes altogether skipping the step of archiving and publication of primary linguistic data. In the case of Saami languages the situation is almost a reverse one: there is a huge amount of archived or published primary linguistic material, whereas descriptive work is more limited, {{or in the case of}} some languages almost entirely lacking. In my presentation I will discuss the benefits and drawbacks of this data-oriented or ‘documentary’ tradition. In particular, I will examine how the extant materials facilitate grammatical description (e. g., writing of reference grammars) as well as practical efforts at language revitalization. It will be seen that while the existence of extensive linguistic corpora is in many ways beneficial to both endeavors, the situation is nevertheless not as ideal as proponents of a broad ‘documentary’ approach have envisioned. Himmelmann, Nikolaus P. (1998). Documentary and Descriptive Linguistics. [...] Linguistics 36 : 161 - 195...|$|R
40|$|This study aims to {{formulate}} a theory of L 2 <b>grammar</b> <b>adequate</b> enough {{to account for the}} final L 2 state. We argued that L 2 I-language was free of L 1 properties, the basis of the CHL 2 Uniformity Hypothesis (CUH), and that L 1 -related performance data were effects of the Relativized Transfer Condition (RTC), constituting the L 2 performance systems. The English resultatives (Mary painted the house red), available in Mandarin and depictives (John ate the meat raw), unavailable in Mandarin, were used to examine the hypotheses. Nineteen Mandarin speakers of English and nineteen native speakers of English participated in the study. The L 2 subjects had lived in the United States for an average of ten years and 5 months {{at the time of the}} experiments. The subjects were tested in four experiments: the Guided Production (GP) test, the Clause-combining (CC) test, the Grammaticality Judgment (GJ) test, and the Interpretation (IT) test. Results were processed through t-tests, one-way ANOVA, and factorial ANOVA procedures. Important findings emerged. First, L 2 subjects showed knowledge of both English resultatives and depictives, indistinct from that of the controls in some, but not all, tests. Second, while their knowledge of the canonical constructions resembled that of the controls, L 2 subjects were more reluctant to construct resultatives and depictives than the native counterparts in some tests. We attribute such irregularities to the modality of measurements, which affected the L 2 subjects' performance, but not their grammatical knowledge. This speculation was confirmed in experiments (i. e., the CC, GJ, and IT tests), where L 2 subjects, when specifically directed to produce resultatives and depictives, performed just like the controls. We therefore conclude that the final L 2 state coincides with the final state attained by the native speakers. We further claim that it is logical to speculate that the linguistic and acquisitional mechanisms that led to the final L 2 state must constitute exactly the same set as the one employed by the native speakers. Therefore, we conclude that the CUH (CHL 2 Uniformity Hypothesis) is true of late L 2 speakers. By the same token, we also conclude that the RTC (Relativized Transfer Condition) consists of adult L 2 development...|$|R

