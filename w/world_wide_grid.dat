9|10000|Public
40|$|Abstract. The World Wide Web {{has become}} a {{phenomenon}} that now influences our everyday life in any possible areas and disciplines. This paper investigates how a grid equivalent of the WWW, the <b>World</b> <b>Wide</b> <b>Grid</b> can be created. We define requirements towards a workflow-oriented computational <b>World</b> <b>Wide</b> <b>Grid</b> and propose a solution how current production Grids can be connected in order to form the technical basis of this infrastructure. A meta-broker concept and its utilization to achieve {{the highest level of}} parallelism by the created architecture in a user transparent way are explained...|$|E
40|$|International audienceThe grid {{environment}} presents numerous {{opportunities for}} business applications {{as well as}} for scientific ones. Nevertheless the current trends seem to lead to several independent specialized grids in opposition to the early visions of one generic <b>world</b> <b>wide</b> <b>grid.</b> In such a cross-grid context, the environment might be harder to manipulate whereas more decisions must be handled from user-side. Our proposal is a distance-based decision-making support designed to be usable, adaptable and accurate. Our main contribution is to ensure the profitability of classical monitoring solutions by improving their usability. Our approach is illustrated and validated with experiments in a real grid environment...|$|E
40|$|The World Wide Web {{has become}} a {{phenomenon}} that now influences our everyday life in any possible areas and disciplines. This chapter describes a potential roadmap establishing its grid equivalent, a scientific, workflow-oriented, computational <b>World</b> <b>Wide</b> <b>Grid</b> (WWG). In order to achieve such a WWG, this chapter suggests three major steps. First, create uniform meta-brokers and connect existing production grids by {{them in order to}} form the WWG infrastructure where existing production grids become interoperable at the level of job submission. Second, create workflow-oriented, advanced grid portals and connect them to the meta-brokers in order to exploit workflow level grid interoperability. Finally, create workflow grid services and their registry and repository {{in order to make the}} workflows developed by different workflow communities interoperable and shareable...|$|E
40|$|Scale-free {{networks}} are abundant both in nature (e. g. protein interactions and social networks) and man-made systems (e. g. <b>World</b> <b>Wide</b> Web, power <b>grids,</b> and citation networks) (Barabási and Albert 1999; Newman 2003). In a scale-free network {{the number of}} edges per vertex follows a power-law. A characteristic feature of scale-free networks {{is that they have}} small average path-lengths, ℓ, i. e. th...|$|R
30|$|The BA model {{represents}} a ‘rich get richer’ {{process and the}} resulting scale-free network topology {{can be used to}} model many real world networks, such as the <b>World</b> <b>Wide</b> Web, power <b>grids,</b> metabolic networks and social networks (Surana et al., 2005). This concept explains the existence of ‘hubs’ (a few nodes with a large number of connections), which is a defining feature of scale-free networks.|$|R
40|$|W eb {{intelligence}} {{offers a}} new directionfor scientific research and develop-ment, pushing technology towardmanipulating the meaning of dataand creating a distributed intelligence that can actually get things done. WI explores the fundamental and practical impact that artificial intelligence and advanced information technology {{will have on the}} next generation of Web-empow-ered systems, services, and environments. In an era dominated by the <b>World</b> <b>Wide</b> Web, <b>Grid</b> computing, intelligent-agent technology, and ubiquitous social computing, WI represents infor-mation technology’s next challenge. THE INTELLIGENT WEB The Web significantly affects both academic research and everyday life, revolutionizing how we gather, store, process, present, share, and use infor-mation. It offers great opportunities and challenge...|$|R
40|$|Computational grids that couple {{geographically}} distributed {{resources such}} as PCs, workstations, clusters, and scientific instruments, have emerged as a next generation computing platform for solving large-scale problems in science, engineering, and commerce. However, application development, resource management, and scheduling in these environments {{continue to be a}} complex undertaking. In this article, we discuss our efforts in developing a resource management system for scheduling computations on resources distributed across the world with varying quality of service (QoS). Our service-oriented grid computing system called Nimrod-G manages all operations associated with remote execution including resource discovery, trading, scheduling based on economic principles and a user-defined QoS requirement. The Nimrod-G resource broker is implemented by leveraging existing technologies such as Globus, and provides new services that are essential for constructing industrial-strength grids. We present the results of experiments using the Nimrod-G resource broker for scheduling parametric computations on the <b>World</b> <b>Wide</b> <b>Grid</b> (WWG) resources that span five continents...|$|E
40|$|International audienceIn {{the last}} years, several {{distributed}} system paradigms have emerged, aiming at {{the share of}} {{a large amount of}} resources among a large number of users. The Virtual Organizations concept in a multi-institutional context presents obviously numerous opportunities for business applications as well as for scientific ones. Nevertheless, the current trends seem to lead to several independent specialized grids in opposition to the early vision of one generic <b>world</b> <b>wide</b> <b>grid.</b> In such a context, large scale heterogeneous databases must be seen as several specialized databases homed in different grids. The challenge is to allow users of these grids to access all databases with the same efficiency that if they are supported by the same grid. Our proposal is an overlay software architecture built on top of grid middlewares which acts on behalf of the user to facilitate the use of inter-grid databases. The proposed mediation solution deals with database heterogeneity and access rights management and is capable of making decisions regarding the selection of resources according to their homing grid and infrastructure performances...|$|E
40|$|Grid {{resource}} management {{has been a}} highly studied research field since Grids were born. Though well-designed, evaluated and widely used resource brokers and meta-schedulers have been developed, new capabilities are still required, while the major demand is for interoperability support. Most of the existing brokering solutions can hardly cross the borders of current middleware systems that are lacking the support of these requirements. In this paper we (i) investigate the current {{resource management}} solutions from the lowest {{to the highest level}} of the Grid middleware, (ii) examine and compare their connections by presenting an anatomy that helps users to grasp the basics of their operation and the researchers to identify common components and open issues. Then we (iii) introduce meta-brokering, which enables higher level resource management by utilizing existing Grid brokers, and provide an implementation of this solution, the Grid Meta-Broker Service, which is a new interoperable grid middleware service for interconnecting Grid islands to compose a <b>World</b> <b>Wide</b> <b>Grid,</b> where users and portals can transparently utilize a growing number of Grids in the future. Finally we (iv) evaluate the presented solution in a simulated Grid environment using real workloads...|$|E
40|$|International audienceThe <b>World</b> <b>Wide</b> Web and <b>grid</b> {{computing}} {{provide new}} {{opportunities and challenges}} for artificial intelligence. This talk examines how case-based reasoning can respond to these challenges by leveraging large-scale information sources. It highlights opportunities for exploiting naturally arising cases and augmenting them with additional open sources, to enable robust support for human reasoning. It illustrates with examples from current research, focusing especially on how CBR can leverage frameworks being developed in burgeoning research activity in provenance capture and storage...|$|R
40|$|Grid {{computing}} provides high computing power, enormous data storage, {{and collaboration}} possibilities to its users. A Computational Grid {{is a collection}} of heterogeneous computers and resources spread across multiple administrative domains with the intent of providing users uniform access to these resources. There are many ways to access the resources of a Computational Grid, each with unique security requirements and implications for both the resource user and the resource provider. A comprehensive set of Grid usage scenarios are presented and analyzed with regard to security requirements such as authentic cation, authorization, integrity, and confidentiality. The main value of these scenarios and the associated security discussions are to provide a library of situations against which an application designer can match, thereby facilitating security-aware application use and development from the initial stages of the application design and invocation. A broader goal of these scenarios are to increase the awareness of security issues in Grid Computing. In the networked access to computation with a single-sign-on system as the portal to the possibilities of <b>world</b> <b>wide</b> computing <b>grids</b> security plays an important role...|$|R
40|$|Computational grid is a {{potential}} technology mainly used for distributed environment. The major issues related with Grid are resource discovery, heterogeneity, fault tolerance and task scheduling. Grid task scheduling is an integrated component of computing which effectively utilizes the idle time of resources. Efficient scheduling algorithm is needed to utilize the resources effectively and reduce the overall completion time. This paper analyzes the performance of scheduling algorithms from {{different point of view}} such as makespan, execution time, completion time and load balancing. First, the general view of <b>World</b> <b>Wide</b> Web <b>grid</b> computing environment and its functions are discussed. Then this paper examines the performance of four scheduling algorithms such as Min-Min, Max-Min, Minimum Completion Time and Minimum Execution Time. Based on the comparative study of various algorithms, some common issues are proposed. The conventional Max-Min grid task scheduling algorithm effectively utilizes the resources and minimizes the makespan than other scheduling algorithms. This survey shows that Max-Min grid task scheduling algorithm outperforms the other algorithms in both task and resource heterogeneous environment...|$|R
40|$|The {{data from}} the <b>world</b> <b>wide</b> <b>grid</b> of neutron {{monitoring}} stations was analyzed for a comparative study of the phase of diurnal anisotropy on quiet and disturbed days on a long term basis up to recent period. It has been observed that the phase of the diurnal anisotropy on disturbed days where {{the value of the}} Ap-index is higher, is found to shift towards earlier hours in comparison to the phase of the diurnal anisotropy on quiet days where the value of Ap-index is lower on all the stations from 1965 to 71. Such a trend is not observable for the later period. This affect is found to be more pronounced on equatorial stations, in particular, in comparison to high latitude stations. It was derived from these observational facts that the relationship between Ap-index and the phase of the diurnal anisotropy is not invariant throughout the period of consideration. Furthermore, the exact cause of such a drastic change is not known, but it demonstrates very clearly that the interplanetary conditions which are responsible for both, diurnal anisotropy of cosmic ray intensity and the geomagnetic Ap-index variation, have drastically changed during the period 1971 and onwards...|$|E
40|$|The {{accelerated}} {{development in}} Grid and peer-to-peer computing has positioned them as promising {{next generation computing}} platforms. They enable the creation of Virtual Enterprises (VE) for sharing resources distributed across the world. However, resource management, application development and usage models in these environments is a complex undertaking. This {{is due to the}} geographic distribution of resources that are owned by different organizations. The resource owners of each of these resources have different usage or access policies and cost models, and varying loads and availability. In order to address complex resource management issues, we have proposed a computational economy framework for resource allocation and for regulating supply and demand in Grid computing environments. The framework provides mechanisms for optimizing resource provider and consumer objective functions through trading and brokering services. In a real world market, there exist various economic models for setting the price for goods based on supply-and-demand and their value to the user. They include commodity market, posted price, tenders and auctions. In this paper, we discuss the use of these models for interaction between Grid components in deciding resource value and the necessary infrastructure to realize them. In addition to normal services offered by Grid computing systems, we need an infrastructure to support interaction protocols, allocation mechanisms, currency, secure banking, and enforcement services. Furthermore, we demonstrate the usage of some of these economic models in resource brokering through Nimrod/G deadline and cost-based scheduling for two different optimization strategies on the <b>World</b> <b>Wide</b> <b>Grid</b> (WWG) testbed that contains resources located on five c [...] ...|$|E
40|$|The {{challenge}} of CERN experiments at the Large Hadron Collider (LHC), which will collect data at {{rates in the}} range of PBs/year, requires the development of GRID technologies to optimize the exploitation of distributed computing power and the automatic access to distributed data storage. Several projects are addressing the problem of setting up the hardware infrastructure of a GRID, as well as the development of the middleware required to manage it: a working GRID should look like a set of services, accessible to registered applications, which will help cooperate the different computing and storage resources. As it happened for the <b>World</b> <b>Wide</b> Web, <b>GRID</b> concepts are in principle important not only for High Energy Physics (HEP) : for this reason, GRID developers, while keeping in mind the needs of HEP experiments, are trying to design GRID services in the most general way. As examples, two applications are described: the CERN/ALICE experiment at the LHC and a recently approved INFN project (GPCALMA) which will set up a GRID prototype between several mammographic centres in Italy...|$|R
40|$|The Open Science Grid usage has {{ramped up}} more than 25 % in the past twelve months due to both the {{increase}} in throughput of the core stakeholders - US LHC, LIGO and Run II - and increase in usage by nonphysics communities. It {{is important to understand}} the value collaborative projects, such as the OSG, contribute to the scientific community. This needs to be cognizant of the environment of commercial cloud offerings, the evolving and maturing middleware for grid based distributed computing, and the evolution in science and research dependence on computation. We present a first categorization of OSG value and analysis across several different aspects of the Consortium's goals and activities. And lastly, we presents some of the upcoming challenges of LHC data analysis ramp up and our ongoing contributions to the <b>World</b> <b>Wide</b> LHC Computing <b>Grid...</b>|$|R
40|$|Built on Internet and <b>World</b> <b>Wide</b> Web, the <b>Grid</b> is a {{new class}} of {{infrastructure}} which supports coordinated resource sharing and problem solving in dynamic, multi-institutional virtual organizations. In a grid architecture, a grid workflow management system is a type of user-level grid middleware. It aims to support large-scale sophisticated scientific and business processes in a variety of complex e-science and e-business applications. Such sophisticated processes are modeled or redesigned as grid workflow specifications at build-time stage by some modeling languages such as Grid Workflow Execution Language (GWEL), Abstract Grid Workflow Language (AGWL), and Martlet. The specifications normally contain a large number of computation, data and/or transaction intensive activities. Then, at run-time instantiation stage, grid workflow instances are created. Finally, at run-time execution stage, grid workflow instances are executed by facilitating the super computing and data sharing capability of underlying grid infrastructure to complete the computation, data and/or transaction intensive activities...|$|R
40|$|The Open Science Grid (OSG) {{provides}} a distributed facility where the Consortium members provide guaranteed and opportunistic access to shared computing and storage resources. The OSG project[1] {{is funded by}} the National Science Foundation and the Department of Energy Scientific Discovery through Advanced Computing program. The OSG project provides specific activities for the operation and evolution of the common infrastructure. The US ATLAS and US CMS collaborations contribute to and depend on OSG as the US infrastructure contributing to the <b>World</b> <b>Wide</b> LHC Computing <b>Grid</b> on which the LHC experiments distribute and analyze their data. Other stakeholders include the STAR RHIC experiment, the Laser Interferometer Gravitational-Wave Observatory (LIGO), the Dark Energy Survey (DES) and several Fermilab Tevatron experiments- CDF, D 0, MiniBoone etc. The OSG implementation architecture brings a pragmatic approach to enabling vertically integrated community specific distributed systems over a common horizontal set of shared resources and services. More information {{can be found at}} the OSG web site: www. opensciencegrid. org...|$|R
40|$|On {{behalf of}} the ATLAS Collaboration The ATLAS Distributed Data Management (DDM) is the system built {{on top of the}} <b>World</b> <b>Wide</b> LHC Computing <b>Grid</b> (WLCG) {{middleware}} and is responsible for the organization of the multi-Petabyte ATLAS data across more than 100 distributed grid sites. One particular component of the system- the DDM Site Services – is the set of agents responsible for the discovery and placement of ATLAS data between sites. DDM Site Services manage aggregated throughputs of over 6 GB/s or one million file-transfers a day and have to work with extremely high reliability and availability. This contribution reports on the production experience acquired during the last 2 years of LHC data taking and show the changes, adaptations and improvements that we implemented on the system to guarantee a flawless service. In addition we will give an update on the service and activity monitoring frameworks that publish the information needed by shifters and experts. Since the implementation is based on common grid middleware, these proceedings can be interesting for any community that is planning their move to the grid or would like to benefit from the approach of one of the largest heavy user communities...|$|R
50|$|CQ Amateur Radio organizes, adjudicates, and {{publishes the}} results of several annual radio competitions, {{including}} the CQ <b>World</b> <b>Wide</b> 160-meter Contest, the CQ <b>World</b> <b>Wide</b> WPX Contest, the CQ <b>World</b> <b>Wide</b> RTTY Contest, the CQ <b>World</b> <b>Wide</b> RTTY WPX Contest, the CQ <b>World</b> <b>Wide</b> DX Contest, and the CQ <b>World</b> <b>Wide</b> VHF Contest. All of these contests allow participation by amateur radio operators in any country of the world.|$|R
40|$|Cloud Computing is a newly {{evolving}} {{platform that}} can be accessed as a service by the users. It is used as storage for files, applications and infrastructure through the Internet. User can access everything as a service in on-demand basis named as pay-as-you-go model. Service-oriented Architecture (SOA) has been adopted in diverse circulated systems such as <b>World</b> <b>Wide</b> Web services, <b>grid</b> computing schemes, utility computing systems and cloud computing schemes. These schemes are called as Service Oriented Systems. One of the open issues is to prioritize service requests in dynamically altering environments where concurrent instances of processes may compete for assets. If we want to prioritize the request, we need to monitor the assets that the cloud services have and founded on the available assets the demanded assets can be assigned to the user. Hence, we propose an approach to find present status of the system by utilizing Dynamic Adaptation Approach. The major target of the research work is to prioritize the service demand, which maximizes the asset utilization in an effective kind that decreases the penalty function for the delayed service. The main concerns should be allotted to requests founded on promise violations of SLA objectives. While most existing work {{in the area of}} quality of service supervising and SLA modeling focuses normally on purely mechanical schemes, we consider service-oriented systems spanning both programs founded services and human actors. Our approach deals with these challenges and assigns priority to the requested service to avoid service delay using Prioritization Algorithm...|$|R
40|$|This unit {{explains}} the characteristics {{and the working}} principles of the <b>World</b> <b>Wide</b> Web {{as the most important}} protocol of the Internet. Topics covered in this unit include characteristics of the <b>World</b> <b>Wide</b> Web; using the <b>World</b> <b>Wide</b> Web for the dissemination of information on the Internet; and using the <b>World</b> <b>Wide</b> Web for the retrieval of information from the Internet...|$|R
5000|$|Year of issuance, popular {{name and}} {{designation}} from The American Card Catalog:1933 Goudey R3191933 American R3381933 <b>World</b> <b>Wide</b> Gum V3531934 Goudey R3201934 Goudey Premiums R390-11934 <b>World</b> <b>Wide</b> Gum V3541935 Goudey 4-in-1 R3211935 Goudey Premiums R390-21936 Goudey Wide Pens R3141936 Goudey R322 1936 <b>World</b> <b>Wide</b> Gum V3551938 Goudey [...] "Heads-Up" [...] R3231939 Goudey Premiums R3031939 <b>World</b> <b>Wide</b> Gum V3511941 Goudey R324 ...|$|R
5000|$|<b>World</b> <b>Wide</b> Web wa Shinderu (ワールドワイドウエブは死んでる; The <b>World</b> <b>Wide</b> Web is Dead) (December 23, 2015) ...|$|R
5000|$|Tim Berners-Lee, {{inventor}} of the <b>World</b> <b>Wide</b> Web {{and director of the}} <b>World</b> <b>Wide</b> Web Consortium ...|$|R
5000|$|<b>World</b> <b>Wide</b> Web Consortium (w3.org), {{international}} standards {{organization for the}} <b>World</b> <b>Wide</b> Web (abbreviated WWW or W3) ...|$|R
50|$|One {{measure of}} {{complexity}} of the Connect Four game {{is the number of}} possible games board positions.For classic Connect Four played on 6 high, 7 <b>wide</b> <b>grid,</b> there are 4,531,985,219,092 positionsfor all game boards populated with 0 to 42 pieces.|$|R
50|$|The <b>World</b> <b>Wide</b> Web Consortium (W3C) is {{the main}} {{international}} standards organization for the <b>World</b> <b>Wide</b> Web (abbreviated WWW or W3).|$|R
500|$|<b>World</b> <b>Wide</b> Web Consortium – {{founded in}} 1994 by Tim Berners-Lee, (W3C) {{is the main}} {{international}} standards organization for the <b>World</b> <b>Wide</b> Web ...|$|R
50|$|DSSSL {{was thought}} to be too complex for the <b>World</b> <b>Wide</b> Web, and the <b>World</b> <b>Wide</b> Web Consortium thought about {{creating}} a DSSSL-Lite.|$|R
5000|$|<b>World</b> <b>Wide</b> Web Consortium - Founded in 1994 by Tim Berners-Lee, (W3C) is {{the main}} {{international}} standards organization for the <b>World</b> <b>Wide</b> Web ...|$|R
50|$|The International <b>World</b> <b>Wide</b> Web Conference Committee (abbreviated as IW3C2 {{also written}} as IW3C2) is a {{professional}} non-profit organization registered in Switzerland (Article 60ff of the Swiss Civil Code) that promotes <b>World</b> <b>Wide</b> Web research and development. The IW3C2 organizes and hosts the annual <b>World</b> <b>Wide</b> Web Conference {{in conjunction with the}} W3C.|$|R
50|$|The {{series was}} {{announced}} on 10 July 2009, under the working title of Digital Revolution, {{to examine the}} impact the <b>World</b> <b>Wide</b> Web has had on society over its first 20 years. Technology journalist and academic Aleks Krotoski would present. The series was launched with an event at the BBC to mark the twentieth anniversary of the <b>World</b> <b>Wide</b> Web, which saw Tim Berners-Lee (credited with inventing the <b>World</b> <b>Wide</b> Web), Susan Greenfield, Bill Thompson and Chris Anderson discuss the <b>World</b> <b>Wide</b> Web.|$|R
50|$|Kingsnorth is {{bisected}} by the A2070 Ashford to Hamstreet road. And briefly has a {{dual carriageway}} within it, {{as well as}} along its nearest border with Ashford, which serves as its main approach road. The road network is otherwise almost a <b>wide</b> <b>grid</b> with slight curves.|$|R
5000|$|He {{had been}} President of American Express Merchant-Related Services <b>World</b> <b>Wide</b> and {{had joined the}} company as President of American Express Publishing <b>World</b> <b>Wide</b> in May 1984.|$|R
50|$|He {{is one of}} {{only six}} inductees in the <b>World</b> <b>Wide</b> Web Hall of Fame {{announced}} at the first international conference on the <b>World</b> <b>Wide</b> Web in 1994.|$|R
