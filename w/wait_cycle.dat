4|63|Public
50|$|Some system {{programs}} that never terminate execute an event loop, {{going to sleep}} {{at the start of}} each cycle and waiting for some event to awaken them. Once an event is received, the program services the event, then returns {{to the beginning of the}} next <b>wait</b> <b>cycle.</b>|$|E
50|$|The {{instruction}} set consists of 45% one-byte, 41% two-byte and 14% three-byte instructions. Each instruction takes 1, 2 or 4 machine cycles to execute. In case {{of access to}} slower memory, the access time may be extended by wait cycles (one <b>wait</b> <b>cycle</b> lasts one machine cycle, which is equivalent to two wait states). The XC800 Core supports a range of debugging features including basic stop/start, Single-step execution, breakpoint support and read/write access to the data memory, program memory and special function registers. A 16-bit co-processor provides additional computing performance and is optimized for the processing of multiply / divide operations and for the execution of CORDIC algorithm for trigonometric operations.|$|E
40|$|In the SoC development, the {{compatibility}} of IP cores {{is one of}} {{the challenges}} {{that need to be addressed}} carefully. Most of the time, IP cores is having different input output specifications with new platform. The Wishbone SoC interconnection Architecture is aim to provide a good solution for SoC integration issues by having common interface specifications. In this project, the Wishbone on-chip computer bus for 32 -bit cores is implemented in system verilog along with three different arbitration schemes which are fixed priority, round robin, and priority control. On top of that, the optimum transfer size for Wishbone bus in terms of bus throughput and average <b>wait</b> <b>cycle</b> is presented as well. It is found that the optimum transfer size for Wishbone bus is 64 bytes. Finally, the Wishbone bus is used to examine the bus performance of different arbitration schemes in Modelsim simulation. Round robin arbitration scheme is the best among three arbitration schemes in terms of bus throughput, logic complexity, and maximum <b>wait</b> <b>cycle...</b>|$|E
40|$|Enzyme {{kinetics}} are cyclic. A {{more realistic}} reversible three-step {{mechanism of the}} Michaelis-Menten kinetics is investigated in detail, and three kinds of <b>waiting</b> <b>cycle</b> times T, T+, T − are defined. It is shown that the mean <b>waiting</b> <b>cycle</b> times 〈T 〉, 〈T+〉, and 〈T− 〉 are the reciprocal of the steady-state cycle flux J ss, the forward steady-state cycle flux Jss + and the backward steady-state cycle flux Jss − respectively. We also show that the distribution of T+ conditioned on T+ < T − {{is identical to the}} distribution of T − conditioned on T − < T+, which is referred as generalized Haldane equality. Consequently, the mean <b>waiting</b> <b>cycle</b> time of T+ conditioned on T+ < T− (〈T+|T+ < T−〉) and the one of T − conditioned on T − < T+ (〈T−|T − < T+〉) are both just the same as 〈T 〉. In addition, the forward and backward stepping probabilities p +, p − are also defined and discussed, especially their relationship with the <b>cycle</b> fluxes and <b>waiting</b> <b>cycle</b> times. Furthermore, we extend the same results to the n-step cycle, and finally, experimental and theoretically based evidences are also included. KEY WORDS: <b>waiting</b> <b>cycle</b> times; generalized Haldane equality; single-molecule experiment; nonequilibrium steady states; cycle flux; stepping probability...|$|R
40|$|Enzyme {{kinetics}} are cyclic. A {{more realistic}} reversible three-step {{mechanism of the}} Michaelis-Menten kinetics is investigated in detail, and three kinds of <b>waiting</b> <b>cycle</b> times $T$, $T_{+}$, $T_{-}$ are defined. It is shown that the mean <b>waiting</b> <b>cycle</b> times $ $, $ $, and $ $ are the reciprocal of the steady-state cycle flux $J^{ss}$, the forward steady-state cycle flux $J^{ss}_{+}$ and the backward steady-state cycle flux $J^{ss}_{-}$ respectively. We also show that the distribution of $T_{+}$ conditioned on $T_{+}<T_{-}$ {{is identical to the}} distribution of $T_{-}$ conditioned on $T_{-}<T_{+}$, which is referred as generalized Haldane equality. Consequently, the mean <b>waiting</b> <b>cycle</b> time of $T_{+}$ conditioned on $T_{+}<T_{-}$ ($<T_{+}| T_{+} $) and the one of $T_{-}$ conditioned on $T_{-}<T_{+}$ ($<T_{-}| T_{-} $) are both just the same as $ $. In addition, the forward and backward stepping probabilities $p^{+},p^{-}$ are also defined and discussed, especially their relationship with the <b>cycle</b> fluxes and <b>waiting</b> <b>cycle</b> times. Furthermore, we extend the same results to the $n$-step cycle, and finally, experimental and theoretically based evidences are also included. Comment: 24 pages, 4 figures; in Journal of Physical Chemistry 200...|$|R
5000|$|Once the row {{has been}} {{activated}} or [...] "opened", {{read and write}} commands are possible to that row. Activation requires a minimum amount of time, called the row-to-column delay, or tRCD before reads or writes to it may occur. This time, rounded up to the next multiple of the clock period, specifies the minimum number of <b>wait</b> <b>cycles</b> between an active command, and a read or write command. During these <b>wait</b> <b>cycles,</b> additional commands may be sent to other banks; because each bank operates completely independently.|$|R
50|$|The {{shortest}} instructions require eight clock cycles or 2.7 μs {{to complete}} (assuming 0 external <b>wait</b> <b>cycles),</b> many others run between 10 and 14 cycles (3.3-4.7 μs); the longest-running instruction (DIV) {{can take up}} to 124 cycles (41.3 μs).|$|R
30|$|Given these results, {{it seems}} that adding {{processors}} to the system decreases execution time, which improves system performance. This variation is not linear because the processors share resources, and, sometimes, they cannot reach the same target simultaneously, which necessitates <b>waiting</b> <b>cycles</b> and diminishes system performance. In terms of energy consumption, {{up to a certain}} number of processors, the total system energy consumption decreases as the number of execution cycles is reduced, and then it tends to stabilize as the system performance improves. But increasing the number of processors over a certain limit tends to be ineffective, as it just adds new conflicts at the interconnect, leading to more <b>waiting</b> <b>cycles,</b> which alters overall performances, especially in terms of power consumption.|$|R
40|$|This paper {{proposes a}} cycle {{accounting}} architecture for Simultaneous Multithreading (SMT) processors that estimates the execution times {{for each of}} the threads had they been executed alone, while they are running simultaneously on the SMT processor. This is done by accounting each cycle to either a base, miss event or <b>waiting</b> <b>cycle</b> component during multi-threaded execution. Single-threaded alone execution time is then estimated as the sum of the base and miss event components; the <b>waiting</b> <b>cycle</b> component represents the lost cycle count due to SMT execution. The cycle accounting architecture incurs reasonable hardware cost (around 1 KB of storage) and estimates single-threaded performance with average prediction errors around 7. 2 % for two-program workloads and 11. 7 % for four-program workloads. The cycle accounting architecture has several important applications to system software and its interaction with SMT hardware. For one, the estimated single-thread alone execution time provides an accurate picture to system software of the actually consumed processor cycles per thread. The alone execution time instead of the total execution time (timeslice) may make system software scheduling policies more effective. Second, a new class of thread-progress aware SMT fetch policies based on per-thread progress indicators enable system software level priorities to be enforced at the hardware level. C. 1. 4 [Processor ar...|$|R
2500|$|The {{equivalent}} read burst {{takes one}} more cycle, because the target must <b>wait</b> 1 <b>cycle</b> for the AD bus {{to turn around}} before it may assert TRDY#: ...|$|R
50|$|Barriers {{are simple}} to {{implement}} and provide good responsiveness. They {{are based on}} the concept of implementing <b>wait</b> <b>cycles</b> to provide synchronization.Consider three threads running simultaneously, starting from barrier 1. After time t, thread1 reaches barrier 2 but it still has to wait for thread 2 and 3 to reach barrier2 as it does not have the correct data. Once, all the threads reach barrier 2 they all start again. After time t, thread 1 reaches barrier3 but it will have to wait for threads 2 and 3 and the correct data again.|$|R
5000|$|... #Caption: The <b>waiting</b> {{room and}} <b>cycle</b> storage on the Liverpool-bound platform.|$|R
40|$|Abstract. We {{propose a}} modular {{verification}} technique that guarantees {{the absence of}} deadlocks {{in a system of}} communicating processes. The network model is very general. It supports dynamic process and channel creation and the ability to send channel endpoints over channels, thereby allowing arbitrary dynamically configured networks. The technique is based on statically approximating all potential <b>wait</b> <b>cycles</b> that could arise at runtime. The potential cycle information is computed from the local perspective of each process, leading to a modular analysis. We prove the soundness of the analysis (namely dead-lock freedom) on a simple but realistic language that captures the essence of such communication networks. We also describe how the technique can be applied to a substantial example. ...|$|R
40|$|This paper {{presents}} {{the architecture of}} a DSP dedicated to discrete wavelet transform. The architecture consists in 2 microprogrammable processors whose complementarity enables to avoid any <b>wait</b> <b>cycles</b> during the algorithm execution so that the available computation power is continuously used. Thanks to this bi-processor organization, a 160000 -transistor ASIC coupled to a small external SRAM implements in real time a 3 -stage multiresolution transform on a CCIR 601 video signal. This chip has been realized in a 0. 7 mu m double metal CMOS technology. Moreover, the DSP has a full programmability {{with respect to the}} used filters and the picture format; it also has the possibility to take into account edge effects and therefore improve image quality. The circuit can be used in the coding {{as well as in the}} decoding...|$|R
40|$|High {{integration}} in integrated circuits {{often leads to}} the problem of running out of pins. Narrow data buses can be used to alleviate this problem at the cost of performance degradation due to <b>wait</b> <b>cycles.</b> In this paper, we address bus coding methods for low power core-based systems incorporating narrow buses. Although the conventional Bus-Invert code performs well for completely random patterns, we show that transition signaling combined with Bus-Invert, which we call BITS coding, can achieve much more power saving for data patterns of typical DSP applications. The application of BITS coding to a real circuit design is limited by the overhead of the encoder and decoder circuits and the extra bus line introduced. We propose an approximate version of BITS coding, which do not require the extra bus line while retaining the advantage of BITS coding. ...|$|R
40|$|Adaptive {{subdivision}} of triangular meshes is {{the next}} step for surface generation algorithms like those used for displacement mapping, where a highly detailed model can be constructed from a coarse triangle mesh and a displacement map. In this case the communication requirements between the CPU and the graphics pipeline can be reduced if the displacement mapping projection is performed in an adaptive tessellation unit attached to the graphics pipeline. This paper presents an architecture for the implementation of the adaptive subdivision of triangular meshes in which the coarse triangle mesh is tessellated according to the surface complexity described by the displacement map. The mesh connectivity description we employ permits the reduction of the memory requirements while simultaneous allowing quick access to all the data required for each subdivision step. As a result, we obtain a regular architecture, characterized by an optimum data management that minimizes the data storage and avoids the <b>waiting</b> <b>cycles</b> associated with the multiple data accesses required for each subdivision step...|$|R
25|$|These early {{architectures}} introduced {{parallel processing}} at the processor level, with innovations such as vector processing, {{in which the}} processor can perform several operations during one clock cycle, {{rather than having to}} <b>wait</b> for successive <b>cycles.</b>|$|R
50|$|It is {{necessary}} to inactivate the cyclin B-Cdk1 complex in order to exit the mitotic stage of the cell cycle. The cells can {{then return to the}} first gap phase G1 and <b>wait</b> until the <b>cycle</b> proceeds yet again.|$|R
30|$|The {{parallel}} algorithm of encoder {{is designed}} {{in such a}} way that each thread is independent and all the threads continue their execution by checking the status of the reference flag. All the threads can be independently executed without sharing of the cores by the threads. So, no data race condition occurs, and this will not incur any extra latency and eventually upgrade the overall encoding performance. Another benefit with this configuration is that for any thread, no extra <b>waiting</b> <b>cycle</b> is required to acquire the Mutual exclusion (Mutex) lock. Whenever there is no macroblock available for encoding, each thread will enter the shared region. In the shared region, only one thread is allowed to access at one time. Before entering the shared region, each thread will try to acquire a Mutex lock to gain an access and execute the code inside the shared region. In this way, no thread will fall into a waiting state, since there will be no repeated failures in acquiring the Mutex lock. This will not incur extra latency and eventually upgrade the overall encoding performance. Therefore, this solves the thread synchronization problem without the data race condition and thread locking overhead.|$|R
50|$|During {{the busy}} tourist season, traffic at the ferry often backs up, even at night, causing {{people to have}} to <b>wait</b> several ferry <b>cycles</b> {{to get to the}} other side. This causes {{frustration}} for commuters who use the ferry to get to and from work.|$|R
40|$|A routing {{algorithm}} able {{to tolerate}} two faulty blocks with {{a distance of}} two, no less, {{in at least one}} dimension by use of only two virtual interconnection networks is proposed in [2]. Their method first encloses faulty nodes with faulty blocks with a distance of no less than two in at least one dimension by deactivating method, then arranges the detours around faulty blocks by Algorithm 3. 1, and finally route messages on two virtual interconnection networks by Algorithm 2. 1. This paper first proves Algorithm 2. 1 is deadlock-free if the detours around faulty blocks are properly arranged. It then goes on to prove Algorithm 3. 1 properly arranges the detours around faulty blocks. 1 The correctness of Algorithm 2. 1 A <b>waiting</b> <b>cycle</b> of p messages M 1, M 2, ···,andMp is a directed graph G(V,E); message Mi (1 ≤ i ≤ p) {{is a member of the}} set E of directed edges and is labeled Mi in G; channel a is a member of the set V of vertices if it is requested by message Mi and held by message M(i mod p) + 1. Message Mj directly waits for message Mk if edge Mj i...|$|R
40|$|We {{propose a}} {{technique}} to avoid deadlocks {{in a system of}} communicating processes. Our network model is very general. It supports dynamic process and channel creation and the ability to send channel endpoints over channels, thereby allowing arbitrary dynamically configured networks. Deadlocks happen in such networks if there is a cycle created by a set of channels, and processes along the <b>cycle</b> circularly <b>wait</b> for messages from each other. Our approach allows cycles of channels to be created, but avoids circular waiting by ensuring that for every cycle C, some process P breaks circular waits by selecting to communicate on both endpoints involved in the cycle C at P. We formalize this strategy as a calculus with a type system. Our type system keeps track of markers called obstructions where <b>wait</b> <b>cycles</b> are intended to be broken. Programmers annotate message types with design decisions on how obstructions are managed. Using these annotations, our type checker works modularly and independently on each process, without suffering any state space explosion. We prove the soundness of the analysis (namely deadlock freedom) on a simple but realistic language that captures the essence of such communication networks. We also describe how the technique can be applied to a substantial example...|$|R
40|$|Adaptive {{subdivision}} of triangular meshes {{is highly}} desirable for surface generation algorithms including adaptive displacement mapping {{in which a}} highly detailed model can be constructed from a coarse triangle mesh and a displacement map. The communication requirements between the CPU and the graphics pipeline can be reduced if more detailed and complex surfaces are generated, as in displacement mapping, by an adaptive tessellation unit {{which is part of}} the graphics pipeline. Generating subdivision surfaces requires a large amount of memory in which multiple arbitrary accesses are required to neighbouring vertices to calculate the new vertices. In this paper we present a meshing scheme and new architecture for the implementation of adaptive subdivision of triangular meshes that allows for quick access using a small memory making it feasible in hardware, while at the same time allowing for new vertices to be adaptively inserted. The architecutre is regular and characterized by an efficient data management that minimizes the data storage and avoids the <b>wait</b> <b>cycles</b> that would be associated with the multiple data accesses required for traditional subdivision. This architecture is presented as an improvement for adaptive displacement mapping algorithms, but could also be used for adaptive subdivision surface generation in hardware...|$|R
5000|$|In {{a recent}} interview, Chairman Michel Demaré {{was asked whether}} Syngenta could remain independent. He responded, [...] "If you have the {{patience}} to <b>wait</b> for <b>cycles</b> to materialize, {{then it would be}} possible. But in these circumstances, where our shareholders have a kind of a benchmark share price, what they think this company is worth, {{it is very difficult to}} say that we can deliver this in the next twelve months." [...] He thereby acknowledged that the company needs to be sold, especially given industry consolidation, which is creating larger competitors.|$|R
40|$|The memory {{hierarchy}} subsystem {{has a significant}} impact on performance and energy consumption of an embedded system. Methods which increase the hit ratio of the cache hierarchy will typically enhance the performance and reduce the embedded system’s total energy consumption. This is mainly due to reduced cache-to-memory bus transactions, fewer main memory accesses and fewer processor <b>waiting</b> <b>cycles.</b> A heuristic approach is presented to reduce the total number of cache misses by carefully relocating selected sections of the application’s software code within the main memory, thus reducing conflict misses resulting from the cache hierarchy. The method requires no hardware modifications i. e. it is a software-only approach. For the first time such a method is applied to large program traces, and the miss rates and corresponding energy savings are observed while varying cache size, line size and associativity. Relocating the code consistently produces superior performance on direct-mapped cache. Since direct-mapped caches, being smaller in silicon area than caches with higher associativity (for the same size), cost less in terms of energy/access, and access faster, using direct-mapped instruction cache with code relocation for performance-oriented embedded systems is recommended. A maximum cache miss rate reduction from 71 % down to less than 1 % is achieved, with energy reductions of up to 63 % with only a small increase in main memory size...|$|R
5000|$|Following the {{bus station}} redevelopment, {{straightening}} tram tracks will allow for the usage of three car trams. Redevelopment of the tram stop will allow for the introduction of an [...] "Interchange Spine"; this will entail retail opportunities as well as <b>waiting</b> areas and <b>cycle</b> storage for passengers on all the surrounding transport services.|$|R
50|$|The {{previous}} {{forms of}} shifting testing left all concentrated on beginning {{the testing of}} software earlier in the development <b>cycle.</b> <b>Waiting</b> until software exists to begin testing, however, largely and unnecessarily limits the use of testing to uncovering coding defects. This delay is particularly disturbing because from 45 percent to 65 percent of defects are introduced in the requirements, architecture, and design activities.|$|R
50|$|The {{station has}} two side platforms, linked by a {{pedestrian}} subway {{to the south}} and a pedestrian overbridge to the north, which also links west over State Highway 2 to Pito-one Road. A small bus interchange connects buses from Maungaraki and Korokoro to trains to and from Wellington. Other facilities include two park and ride carparks, a ticket office, <b>waiting</b> room, and <b>cycle</b> storage.|$|R
40|$|This paper {{studies the}} effect of {{absenteeism}} in production line. The analysis shows {{that the number of}} absentees at production line increases the workload to the available worker and gives effect to the production. Thus, it gives a major potential for generating bottlenecks. To mitigate this problem and ensure that the workload of workers is at optimum level, an appropriate number of production plans need to be investigated. This paper demonstrates the integration of Discrete Event Simulation (DES) and System Dynamics (SD) in simulating {{the effect of}} absenteeism in production flow at an aircraft composite manufacturing factory. Based on the preliminary result, the effect of the absenteeism is found to influence the amount of throughput, <b>waiting</b> and <b>cycle</b> time...|$|R
40|$|AbstractStore-and-forward {{deadlock}} (SFD) {{occurs in}} packet switched computer networks when, among some cycle of packets buffered by the communication system, each packet in the <b>cycle</b> <b>waits</b> {{for the use}} of the buffer currently occupied by the next packet in the cycle. In this paper, a deadlock-free algorithm as well as a livelock-free algorithm for packet switching is obtained using the strategy of the banker's algorithm. Furthermore, the solution obtained is interpreted for the hyper-fast banker's problem...|$|R
6000|$|... 27. 'O gentle child, {{beautiful}} as thou wert, [...] 235 Why didst thou leave the trodden paths of men Too soon, and with weak hands though mighty heart Dare the unpastured dragon in his den? Defenceless as thou wert, oh, where was then Wisdom the mirrored shield, or scorn the spear? [...] 240 Or hadst thou <b>waited</b> the full <b>cycle,</b> when Thy spirit should have filled its crescent sphere, The monsters of life's waste had fled from thee like deer.|$|R
50|$|While <b>waiting</b> for The <b>Cycle</b> to be released, Mehrjui {{worked on}} several documentaries. Alamut, a {{documentary}} on the Isamailis, was commissioned by Iranian National Television in 1974. He was also commissioned by the Iranian Blood Transfusion Center to create three short documentaries about safe and healthy blood donations. The films were used by the World Health Organization in several countries for years. In 1978, the Iranian Ministry of Health commissioned Mehrjui to make the documentary Peyvast kolieh, about kidney transplants.|$|R
40|$|Although {{work-in-process}} (WIP) balance approaches {{can achieve}} average cycle time reduction, {{due to the}} characteristics of wafer fabrication facilities (wafer fabs), e. g., re-entrant flow, setup time and batch pro-cessing, a lack of effective mechanism to ensure lot movement at the right pace results in degraded cycle time variance, which might be a potential problem when due date is concerned. This paper attempts to solve this problem. Firstly four cycle time variance minimization rules which utilize <b>waiting</b> time, <b>cycle</b> time and due date information of lot are investigated. Then they are incorporated into two WIP balance approaches in literature {{to figure out whether}} they can overcome the drawback arising from WIP balance. In the end the benefit of cycle time variance minimization is illustrated by one example to address an im-proved ability to meet due date reliably. ...|$|R
6000|$|... 'O gentle child, {{beautiful}} as thou wert, [...] Why didst thou leave the trodden paths of men [...] Too soon, and with weak hands though mighty heart [...] Dare the unpastured dragon in his den? [...] Defenceless as thou wert, oh where was then [...] 5 [...] Wisdom the mirrored shield, or scorn the spear?-- [...] Or, hadst thou <b>waited</b> the full <b>cycle</b> when [...] Thy spirit should have filled its crescent sphere, The monsters of life's waste had fled from thee like deer.|$|R
40|$|Waiting {{for medical}} care is the {{by-product}} of system rationing, where demand exceeds supply. In this short report we expand on the conventional concept of the queue, {{by focusing on the}} regulation of demand and by incorporating a funnel and spout analogy. Real-world examples are used to illustrate the infancy of funnel or demand-side reform initiatives targeting the queue, and the suggestion is made that policy needs to address the concept of 'waiting' much earlier in the treatment <b>cycle.</b> <b>Waiting</b> lists Supply Demand Health systems System rationing...|$|R
50|$|Another {{effective}} way of implementing synchronization is by using spinlocks. Before accessing any shared resource or piece of code, every processor checks a flag. If the flag is reset, then the processor sets the flag and continues executing the thread. But, if the flag is set (locked), the threads would keep spinning in a loop and keep checking if the flag is set or not. But, spinlocks are effective only if the flag is reset for lower cycles otherwise {{it can lead to}} performance issues as it wastes many processor <b>cycles</b> <b>waiting.</b>|$|R
