1555|1460|Public
25|$|The {{first three}} evaluations, Senseval-1 through Senseval-3, {{were focused on}} <b>word</b> <b>sense</b> <b>disambiguation,</b> each time growing {{in the number of}} {{languages}} offered in the tasks and in the number of participating teams. Beginning with the fourth workshop, SemEval-2007 (SemEval-1), the nature of the tasks evolved to include semantic analysis tasks outside of <b>word</b> <b>sense</b> <b>disambiguation.</b>|$|E
25|$|Senseval-3 {{took place}} in March–April 2004, {{followed}} by a workshop held in July 2004 in Barcelona, in conjunction with ACL 2004. Senseval-3 included 14 different tasks for core <b>word</b> <b>sense</b> <b>disambiguation,</b> as well as identification of semantic roles, multilingual annotations, logic forms, subcategorization acquisition.|$|E
25|$|PageRank {{has been}} used to rank spaces or streets to predict how many people (pedestrians or vehicles) come to the {{individual}} spaces or streets. In lexical semantics it {{has been used}} to perform <b>Word</b> <b>Sense</b> <b>Disambiguation,</b> Semantic similarity, and also to automatically rank WordNet synsets according to how strongly they possess a given semantic property, such as positivity or negativity.|$|E
40|$|Statistical {{models of}} <b>word</b> [...] <b>sense</b> <b>disambiguation</b> are {{often based on}} {{a small number of}} contextual {{features}} or on a model that is assumed to characterize the interactions among a set of features. Model selection is presented as an alternative to these approaches, where a sequential search of possible models is conducted in order to find the model that best characterizes the interactions among features. This paper expands existing model selection methodology and presents the first comparative study of model selection search strategies and evaluation criteria when applied to the problem of building probabilistic classifiers for <b>word</b> [...] <b>sense</b> <b>disambiguation.</b> 1 Introduction In this paper <b>word</b> [...] <b>sense</b> <b>disambiguation</b> is cast as a problem in supervised learning, where a classifier is induced from a corpus of sense [...] tagged text. Suppose there is a training sample where each sense [...] tagged sentence is represented by the feature variables (F 1; : : :; FnΓ 1; S). Selected cont [...] ...|$|R
2500|$|For example, in the <b>word</b> <b>sense</b> {{induction}} and <b>disambiguation</b> task, {{there are}} three separate phases: ...|$|R
40|$|This paper {{presents}} an unsupervised graph-based method for automatic <b>word</b> <b>sense</b> induction and <b>disambiguation.</b> The innovative {{part of our}} method is the assignment of either a word or a word pair to each vertex of the constructed graph. <b>Word</b> <b>senses</b> are induced by clustering the constructed graph. In the disambiguation stage, each induced cluster is scored according {{to the number of}} its vertices found {{in the context of the}} target word. Our system participated in SemEval- 2010 <b>word</b> <b>sense</b> induction and <b>disambiguation</b> task. ...|$|R
25|$|The Multilingual WSD {{task was}} {{introduced}} for the SemEval-2013 workshop. The task {{is aimed at}} evaluating <b>Word</b> <b>Sense</b> <b>Disambiguation</b> systems in a multilingual scenario using BabelNet as its sense inventory. Unlike similar task like crosslingual WSD or the multilingual lexical substitution task, where no fixed sense inventory is specified, Multilingual WSD uses the BabelNet as its sense inventory. Prior {{to the development of}} BabelNet, a bilingual lexical sample WSD evaluation task was carried out in SemEval-2007 on Chinese-English bitexts.|$|E
25|$|Hit counts {{were used}} for {{carefully}} constructed search engine queries to identify rank orders for word sense frequencies, as an input to a <b>word</b> <b>sense</b> <b>disambiguation</b> engine. This method was further explored {{with the introduction of}} the concept of a parallel corpora where the existing Web pages that exist in parallel in local and major languages be brought together. It was demonstrated {{that it is possible to}} build a language-specific corpus from a single document in that specific language.|$|E
25|$|There {{has been}} much {{discussion}} about the possible developments {{in the arena of}} the Web as a corpus. The development of using the web as a data source for <b>word</b> <b>sense</b> <b>disambiguation</b> was brought forward in The EU MEANING project in 2002. It used the assumption that within a domain, words often have a single meaning, and that domains are identifiable on the Web. This was further explored by using Web technology to gather manual word sense annotations on the Word Expert Web site.|$|E
40|$|Digital {{libraries}} {{are becoming}} more ubiquitous and their contents can be of interest {{not only to the}} communities that create them but also to individuals and organizations around the world. The content of most digital libraries is not visible to web search engines and the internal search engines of many digital libraries do not support cross language access. In this paper, we present some ideas for a general framework to support cross-language information access for digital libraries. The Integrative Cross Language Information Access (iCLIA) framework is designed to facilitate cross language access to digital libraries by capitalizing on the integration of multiple sources for translational knowledge, integration of computational power and human intelligence, and integration of several methods for <b>words</b> <b>sense</b> <b>disambiguation.</b> Collaboration among digital library software developers, CLIR researchers and users is extremely important for the application of CLIR to existing and future digital library projects...|$|R
40|$|We {{present an}} {{approach}} to detecting hate speech in online text, where hate speech is defined as abusive speech targeting specific group characteristics, such as ethnic origin, religion, gender, or sexual orientation. While hate speech against any group may exhibit some common characteristics, we have observed that hatred against each different group is typically characterized {{by the use of}} a small set of high frequency stereotypical words; however, such words may be used in either a positive or a negative sense, making our task similar to that of <b>words</b> <b>sense</b> <b>disambiguation.</b> In this paper we describe our definition of hate speech, the collection and annotation of our hate speech corpus, and a mechanism for detecting some commonly used methods of evading common “dirty word ” filters. We describe pilot classification experiments in which we classify anti-semitic speech reaching an accuracy 94 %, precision of 68 % and recall at 60 %, for an F 1 measure of. 6375. ...|$|R
40|$|Abstract—Word <b>sense</b> <b>disambiguation</b> is an {{important}} intermediate stage for many natural language processing applications. The senses of an ambiguous word are the classification of usages for that specific word. This paper deals with the methodologies of determining the senses for a given word {{if they can not}} be obtained from an already available resource like WordNet. We offer a method that helps us to determine the sense boundaries gradually. In this method, first we decide on some features that are thought to be effective on the senses and divide the instances first into two, then according to the results of evaluations we continue dividing instances gradually. In a second method we use the pseudo words. We devise artificial words depending on some criteria and evaluate classification algorithms on these previously classified <b>words.</b> Keywords—Word <b>sense</b> <b>disambiguation,</b> <b>sense</b> determination, pseudo <b>words,</b> <b>sense</b> granularity. I...|$|R
25|$|From the {{earliest}} days, assessing {{the quality of}} <b>word</b> <b>sense</b> <b>disambiguation</b> (WSD) algorithms had been primarily a matter of intrinsic evaluation, and “almost no attempts {{had been made to}} evaluate embedded WSD components”. Only very recently (2006) had extrinsic evaluations begun to provide some evidence for the value of WSD in end-user applications. Until 1990 or so, discussions of the sense disambiguation task focused mainly on illustrative examples rather than comprehensive evaluation. The early 1990s saw the beginnings of more systematic and rigorous intrinsic evaluations, including more formal experimentation on small sets of ambiguous words.|$|E
2500|$|... {{international}} organization {{devoted to the}} evaluation of <b>Word</b> <b>Sense</b> <b>Disambiguation</b> Systems (endorsed by SIGLEX) ...|$|E
2500|$|... {{evaluated}} <b>word</b> <b>sense</b> <b>disambiguation</b> {{systems on}} {{three types of}} tasks (the all-words, lexical-sample and the translation task) ...|$|E
5000|$|<b>Word</b> <b>Sense</b> Induction and <b>Disambiguation</b> task is a {{combined}} task evaluation where the sense inventory is first induced from a fixed training set data, consisting of polysemous {{words and the}} sentence that they occurred in, then WSD is performed on a different testing data set.|$|R
40|$|We {{present a}} corpus [...] based {{approach}} to <b>word</b> [...] <b>sense</b> <b>disambiguation</b> that only requires {{information that can}} be automatically extracted from untagged text. We use unsupervised techniques to estimate the parameters of a model describing the conditional distribution of the sense group given the known contextual features. Both the EM algorithm and Gibbs Sampling are evaluated to determine which is most appropriate for our data. We compare their disambiguation accuracy in an experiment with thirteen different words and three feature sets. Gibbs Sampling results in small but consistent improvement in disambiguation accuracy over the EM algorithm...|$|R
40|$|In {{this paper}} we {{describe}} our Semeval- 2013 task on <b>Word</b> <b>Sense</b> Induction and <b>Disambiguation</b> within an end-user application, namely Web search result clustering and diversification. Given a target query, induction and disambiguation systems are requested to cluster and diversify the search results returned by {{a search engine}} for that query. The task enables the end-to-end evaluation and comparison of systems. ...|$|R
2500|$|... {{included}} {{tasks for}} <b>word</b> <b>sense</b> <b>disambiguation,</b> {{as well as}} identification of semantic roles, multilingual annotations, logic forms, subcategorization acquisition.|$|E
2500|$|... {{the first}} {{evaluation}} exercise on <b>word</b> <b>sense</b> <b>disambiguation</b> systems; the lexical-sample task was evaluated on English, French and Italian ...|$|E
2500|$|... To {{facilitate}} {{the ease of}} integrating WSD systems into other Natural Language Processing (NLP) applications, such as Machine Translation and multilingual Information Retrieval, the cross-lingual WSD evaluation task was introduced a language-independent and knowledge-lean approach to WSD. The task is an unsupervised <b>Word</b> <b>Sense</b> <b>Disambiguation</b> task for English nouns by means of parallel corpora. It follows the lexical-sample variant of the Classic WSD task, restricted to only 20 polysemous nouns.|$|E
40|$|We {{describe}} our language-independent unsupervised <b>word</b> <b>sense</b> induction system. This system only uses topic {{features to}} cluster different <b>word</b> <b>senses</b> in their global context topic space. Using unlabeled data, this system trains a latent Dirichlet allocation (LDA) topic model then {{uses it to}} infer the topics distribution of the test instances. By clustering these topics distributions in their topic space we cluster them into different senses. Our hypothesis is that closeness in topic space reflects similarity between different <b>word</b> <b>senses.</b> This system participated in SemEval- 2 <b>word</b> <b>sense</b> induction and <b>disambiguation</b> task and achieved the second highest V-measure score among all other systems. ...|$|R
40|$|This paper {{presents}} the description and evaluation framework of SemEval- 2010 <b>Word</b> <b>Sense</b> Induction & <b>Disambiguation</b> task, {{as well as}} the evaluation results of 26 participating systems. In this task, participants were required to induce the senses of 100 target words using a training set, and then disambiguate unseen instances of the same words using the induced senses. Systems’ answers were evaluated in: (1) an unsupervised manner by using two clustering evaluation measures, and (2) a supervised manner in a WSD task. ...|$|R
40|$|Recent {{advances}} in <b>word</b> <b>sense</b> induction rely on clustering related words. In this paper, {{instead of using}} a clustering algorithm, we suggest to perform a Singular Value Decomposition (SVD) which can be guaranteed to always find a global optimum. However, in order to apply this method {{to the problem of}} <b>word</b> <b>sense</b> induction, a semantic interpretation of the dimensions computed by the SVD is required. Our finding is that in our specific setting the first dimension relates to semantic similarities between words, and the second dimension distinguishes between the two main senses of an ambiguous word. Based on this result we present an algorithm for fully unsupervised <b>word</b> <b>sense</b> induction and <b>disambiguation...</b>|$|R
2500|$|Wikipedia {{has seen}} been widely {{used as a}} corpus for {{linguistic}} research in computational linguistics, information retrieval and natural language processing. In particular, it commonly serves as a target knowledge base for the entity linking problem, which is then called [...] "wikification", and to the related problem of <b>word</b> <b>sense</b> <b>disambiguation.</b> Methods similar to wikification can in turn be used to find [...] "missing" [...] links in Wikipedia.|$|E
5000|$|... pyWSD, python {{implementations}} of <b>Word</b> <b>Sense</b> <b>Disambiguation</b> (WSD) technologies ...|$|E
5000|$|... {{multilingual}} <b>Word</b> <b>Sense</b> <b>Disambiguation</b> and Entity Linking {{with the}} Babelfy system ...|$|E
40|$|The Naive Mix {{is a new}} {{supervised}} learning algorithm {{that is based on}} a sequential method for selecting probabilistic models. The usual objective of model selection is to find a single model that adequately characterizes the data in a training sample. However, during model selection a sequence of models is generated that consists of the best [...] fitting model at each level of model complexity. The Naive Mix utilizes this sequence of models to define a probabilistic model which is then used as a probabilistic classifier to perform <b>word</b> [...] <b>sense</b> <b>disambiguation.</b> The models in this sequence are restricted to the class of decomposable log [...] linear models. This class of models offers a number of computational advantages. Experiments disambiguating twelve different words show that a Naive Mix formulated with a forward sequential search and Akaike's Information Criteria rivals established {{supervised learning}} algorithms such as decision trees (C 4. 5), rule induction (CN 2) and nearest [...] neighbor classif [...] ...|$|R
40|$|International audienceIn this paper, {{we present}} a unified model for the {{automatic}} induction of <b>word</b> <b>senses</b> from text, and the subsequent disambiguation of particular word instances using the automatically extracted sense inventory. The induction step and the disambiguation step {{are based on the}} same principle: words and contexts are mapped to a limited number of topical dimensions in a latent semantic word space. The intuition is that a particular sense is associated with a particular topic, so that different senses can be discriminated through their association with particular topical dimensions; in a similar vein, a particular instance of a word can be disambiguated by determining its most important topical dimensions. The model is evaluated on the SemEval- 2010 <b>word</b> <b>sense</b> induction and <b>disambiguation</b> task, on which it reaches state-of-the-art results...|$|R
40|$|This {{research}} {{is located in}} the natural language processing (NLP) domain, at the intersection of computer science and linguistics, more specifically on multilingual lexicography and lexicology. In a first long stay in Japan from November 2001 to March 2004, we made the observation that the French-Japanese lexical resources available on the Web were almost nonexistent. Which gave birth to the Papillon project of building a multilingual lexical database with a pivot structure (Sérasset et al., 2001). Since then, {{progress has been made in}} several areas (technical, academic, social) (Mangeot, 2006), but the production of real data has made very little progress. On the other hand, reuse of lexical resources is trendy (WSD, use of open-source resources like Wiktionary or Dbpedia, fusion with ontologies, etc.). Even if they can consolidate and expand the coverage of existing resources, these experiences always start from data created manually by lexicographers. Based on this observation, we defined the following project which involves building a multilingual lexical system with focus on the French-Japanese language pair. Construction will be based on the reuse of existing resources (Franco-Japanese lexicons, Wiktionary) and automatic operations (reification of translation links, <b>word</b> <b>senses</b> <b>disambiguation)</b> and also on a community of volunteer contributors working on the Web. They will be asked to contribute either via serious lexical games, or directly on dictionary entries according to their level of expertise and knowledge in the field of lexicography or bilingual translation. Resources generated will be royalty free and designed to be used both by humans via bilingual dictionaries and tools for automatic language processing (analysis, machine translation, etc.). We will begin with a brief inventory of bilingual lexicography in general and focus on French- Japanese in particular. We then present recent advances in the field of construction of lexical resources online. Then, we describe in more detail the lexical system that we plan to build. We conclude with a description of the steps involved in this construction...|$|R
5000|$|... #Subtitle level 3: Early {{evaluation}} of algorithms for <b>word</b> <b>sense</b> <b>disambiguation</b> ...|$|E
50|$|The {{first three}} evaluations, Senseval-1 through Senseval-3, {{were focused on}} <b>word</b> <b>sense</b> <b>disambiguation,</b> each time growing {{in the number of}} {{languages}} offered in the tasks and in the number of participating teams. Beginning with the fourth workshop, SemEval-2007 (SemEval-1), the nature of the tasks evolved to include semantic analysis tasks outside of <b>word</b> <b>sense</b> <b>disambiguation.</b>|$|E
5000|$|In {{computational}} linguistics the Yarowsky algorithm is an unsupervised learning algorithm for <b>word</b> <b>sense</b> <b>disambiguation</b> {{that uses the}} [...] "one sense per collocation" [...] and the [...] "one sense per discourse" [...] properties of human languages for <b>word</b> <b>sense</b> <b>disambiguation.</b> From observation, words tend to exhibit only one sense in most given discourse and in a given collocation.|$|E
40|$|This paper {{presents}} an original methodology to consider question answering. We noticed that query expansion is often incorrect {{because of a}} bad understanding of the question. But the automatic good understanding of an utterance {{is linked to the}} context length, and the question are often short. This methodology proposes to analyse the documents and to construct an informative structure from the results of the analysis and from a semantic text expansion. The linguistic analysis identifies words (tokenization and morphological analysis), links between words (syntactic analysis) and <b>word</b> <b>sense</b> (semantic <b>disambiguation).</b> The text expansion adds to each word the synonyms matching its sense and replaces the words in the utterances by derivatives, modifying the syntactic schema if necessary. In this way, whatever enrichment may be, the text keeps the same meaning, but each piece of information matches many realisations. The questioning method consists in constructing a local informative structure without enrichment, and matches it with the documentary structure. If a sentence in the informative structure matches the question structure, this sentence is the answer to the question. Comment: 11 p...|$|R
40|$|WordNet is a lexicon {{widely known}} {{and used as}} an ontological {{resource}} hosting comparatively large collection of semantically interconnected words. Use of such resources produces meaningful results and improves users’ search experience through the increased precision and recall. This paper presents our facet-enabled WordNet powered semantic search work done {{in the context of}} the bioenergy domain. The main hurdle to achieving the expected result was <b>sense</b> <b>disambiguation</b> further complicated by the occasional fine-grained distinction of meanings of the terms in WordNet. To overcome this issue, a novel <b>sense</b> <b>disambiguation</b> approach based on automatically built domain specific ontologies, WordNet synset hierarchy and term (or <b>word)</b> <b>sense</b> ranks is proposed...|$|R
40|$|The {{automatic}} <b>disambiguation</b> of <b>word</b> <b>senses</b> {{has been}} an interest and concern since {{the earliest days of}} computer treatment of language in the 1950 's. <b>Sense</b> <b>disambiguation</b> is an “intermediate task ” (Wilks and Stevenson, 1996) which is not an end in itself, but rather is necessary at one level or another to accomplish most natural language processing tasks. It i...|$|R
