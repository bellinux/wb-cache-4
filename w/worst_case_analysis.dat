266|10000|Public
500|$|The {{classical}} integer sorting algorithms of pigeonhole sort, counting sort, and radix sort {{are widely}} used and practical. Much of the subsequent research on integer sorting algorithms has focused less on practicality and more on theoretical improvements in their <b>worst</b> <b>case</b> <b>analysis,</b> and the algorithms that come from this line of research are not believed to be practical for current 64-bit computer architectures, although ...|$|E
2500|$|However, {{the result}} is always a <b>worst</b> <b>case</b> <b>analysis</b> for the {{distribution}} of [...] error, as other probability-based distributions are not considered.|$|E
2500|$|In many instances, {{however, the}} time {{taken by the}} {{algorithm}} may be even faster than this <b>worst</b> <b>case</b> <b>analysis</b> indicates. For instance, in the average case for sparse bipartite random graphs, [...] (improving a previous result of [...] ) showed that with high probability all non-optimal matchings have augmenting paths of logarithmic length. As a consequence, for these graphs, the Hopcroft–Karp algorithm takes [...] phases and [...] total time.|$|E
5000|$|Since 2002, AEi Systems has {{continued}} its growth in <b>Worst</b> <b>Case</b> Circuit <b>Analysis</b> ("WCCA") (also called [...] "Worst Case Analys or [...] "WCA") and failure and reliability analysis for space-bound DC-DC converters. In addition, {{the company has}} expanded its digital analysis capabilities, including signal integrity and <b>worst</b> <b>case</b> digital <b>analysis.</b>|$|R
5000|$|Analysis: <b>Worst</b> <b>Case</b> Circuit <b>Analysis</b> (WCCA) {{and failure}} and {{reliability}} analyses; signal integrity and other digital analyses.|$|R
50|$|The company's predecessor, Analytical Engineering, Inc., {{was founded}} in 1995. The company met with {{significant}} early success as the developer of simulation models and performed <b>worst</b> <b>case</b> circuit <b>analysis</b> on International Space Station's power buss electronics. A series of satellite projects continued to keep founder Steve Sandler and Analytical Engineering busy, including Tempo, Globalstar, GOES, GPS, MCI, Omegasat and P-81. Analytical Engineering provided computer simulation models and <b>worst</b> <b>case</b> circuit <b>analysis</b> on most projects.|$|R
5000|$|Motorola University's [...] "six sigma class" [...] on <b>worst</b> <b>case</b> <b>analysis</b> {{was created}} by AEi Systems ...|$|E
50|$|A further tacit {{assumption}} is that the <b>worst</b> <b>case</b> <b>analysis</b> of computational complexity is in question unless stated otherwise. An alternative approach is probabilistic analysis of algorithms.|$|E
50|$|Although this {{algorithm}} {{will always}} find optimal solutions, {{there is no}} <b>worst</b> <b>case</b> <b>analysis.</b> It is not known how many moves this algorithm might need. An implementation of this algorithm can be found here.|$|E
3000|$|..., {{making the}} {{approximate}} analysis more accurate. To study the <b>worst</b> <b>case,</b> in our <b>analysis,</b> we assume the ring width [...]...|$|R
5000|$|AEi Systems is {{the world}} leader in <b>Worst</b> <b>Case</b> Circuit <b>Analysis</b> (WCCA) and failure and {{reliability}} analysis for space-bound DC-DC converters, and AEi Systems engineers are recognized as Subject Matter Experts by Aerospace Corporation in connection with WCCA and the setting of WCCA standards.|$|R
40|$|The idea of {{predictable}} architectures {{has recently}} obtained more attention {{with the concept}} of a Precision Timed Machine (PRET) [5]. The two goals are to guarantee precise timing without sacrificing throughput, and to simplify the <b>worst</b> <b>case</b> timing <b>analysis</b> of code executing on PRET machines. Such a PRE...|$|R
5000|$|Sandler founded Analytical Engineering, Inc in 1995. He was the CEO until 2000, when {{it became}} AEi Systems LLC. In his current {{position}} at AEi, Sandler {{is responsible for the}} design, <b>worst</b> <b>case</b> <b>analysis,</b> reliability analysis, and FMECA analysis of satellite and high-reliability power electronic systems.|$|E
50|$|Ben-Haim {{argues that}} info-gap's {{robustness}} model is not min-max/maximin analysis {{because it is}} not <b>worst</b> <b>case</b> <b>analysis</b> of outcomes; it is a satisficing model, not an optimization model - a (straightforward) maximin analysis would consider worst-case outcomes over the entire space which, since uncertainty is often potentially unbounded, would yield an unbounded bad worst case.|$|E
50|$|Because of {{that the}} results of the {{interval}} finite element (or in general <b>worst</b> <b>case</b> <b>analysis)</b> may be overestimated in comparison to the stochastic fem analysis (see also propagation of uncertainty).However, in the case of nonprobabilistic uncertainty {{it is not possible to}} apply pure probabilistic methods.Because probabilistic characteristic in that case are not known exactly 2000.|$|E
40|$|Recent {{progress}} in <b>worst</b> <b>case</b> timing <b>analysis</b> of programs {{has made it}} possible to perform accurate timing analysis of pipelined execution and instruction caching, which is necessary when a RISC processor is used as the target processor of a real-time system. However, there has not been much {{progress in}} <b>worst</b> <b>case</b> timing <b>analysis</b> of data caching. This is mainly due to load/store instructions that reference multiple memory locations such as those used to implement array and pointer-based references. These load/store instructions are called dynamic load/store instructions and most current analysis techniques take a very conservative approach to their timing <b>analysis.</b> In many <b>cases,</b> it is assumed that each of the references from a dynamic load/store instruction will miss in the cache and replace a cache block that would otherwise lead to a cache hit. This conservative approach results in severe overestimation of the <b>worst</b> <b>case</b> execution time (WCET). This paper proposes two techniques to mi [...] ...|$|R
40|$|In the {{framework}} of the MURST Project “Tecniche per la garanzia di qualità in reti di telecomunicazioni multiservizi”, this paper deals with the deterministic <b>worst</b> <b>case</b> (DWC) <b>analysis</b> for multistage networks. Two analytical methods are presented for different source traffic models. Numerical results are discussed and compared with simulations, in order to assess the suitability of DWC-based QoS provisioning schemes for practical implementation. Table of content...|$|R
40|$|We {{present a}} guardian-based {{approach}} to detecting 'babbling idiots', faulty nodes which erroneously consume extra resource in an event triggered system. In general, one cannot detect all babbling idiots, but the maximum effect of undetected faults is bounded and small, {{and therefore can}} {{be taken into account}} in <b>worst</b> <b>case</b> response time <b>analysis</b> to guarantee that a babbling idiot cannot cause a timing failure elsewhere in the system. The approach is applied specifically to the CAN protocol to protect against faulty nodes transmitting message frames too often. We show that the overhead of including the effect of undetected frames into the <b>worst</b> <b>case</b> response time <b>analysis</b> is small enough to be of practical value...|$|R
5000|$|... can {{be found}} by {{interval}} methods. This provides an alternative to traditional propagation of error analysis.Unlike point methods, such as Monte Carlo simulation, interval arithmetic methodology ensures that {{no part of the}} solution area can be overlooked.However, the result is always a <b>worst</b> <b>case</b> <b>analysis</b> for the distribution of error, as other probability-based distributions are not considered.|$|E
50|$|For many algorithms, {{performance}} is dependent {{not only on}} the size of the inputs, but also on their values. One such example is the quicksort algorithm, which sorts an array of elements. Such data-dependent algorithms are analysed for average-case and worst-case data. Competitive analysis is a way of doing <b>worst</b> <b>case</b> <b>analysis</b> for on-line and randomized algorithms, which are typically data dependent.|$|E
5000|$|In many instances, {{however, the}} time {{taken by the}} {{algorithm}} may be even faster than this <b>worst</b> <b>case</b> <b>analysis</b> indicates. For instance, in the average case for sparse bipartite random graphs, [...] (improving a previous result of [...] ) showed that with high probability all non-optimal matchings have augmenting paths of logarithmic length. As a consequence, for these graphs, the Hopcroft-Karp algorithm takes [...] phases and [...] total time.|$|E
50|$|Traditional {{real-time}} systems {{make use}} of WCET (<b>Worst</b> <b>Case</b> Execution Time) <b>analysis</b> techniques in order to compute what is the maximum time an instance of, for example, a periodic task may execute on the CPU before blocking {{waiting for the next}} instance.|$|R
5000|$|AEi Systems {{has been}} {{involved}} in the development and analysis {{of some of the most}} sophisticated and technically challenging power systems in the world, both in the space and defense arena and also with respect to commercial and industrial applications. The company has provided computer simulation models and <b>worst</b> <b>case</b> circuit <b>analysis</b> on most projects. While many of the company's projects are for classified programs, among the company's publicly known projects are: ...|$|R
3000|$|... [7], we will {{consider}} this <b>worst</b> <b>case</b> in the <b>analysis.</b> In this <b>case,</b> the per‐cell codebooks for quantizing the per‐cell CDI vectors should have an identical size, say bCDI bits. Then, the total bits for each user to quantize all the per‐cell CDI vectors are BCDI=N [...]...|$|R
5000|$|... 1993 Naylor {{answered}} {{the question of what}} characterizes a good BSP tree. He used expected case models (rather than <b>worst</b> <b>case</b> <b>analysis)</b> to mathematically measure the expected cost of searching a tree and used this measure to build good BSP trees. Intuitively, the tree represents an object in a multi-resolution fashion (more exactly, as a tree of approximations). Parallels with Huffman codes and probabilistic binary search trees are drawn.|$|E
50|$|Algorithmic {{mechanism}} design {{differs from}} classical economic mechanism design in several respects. It typically employs the analytic tools of theoretical computer science, such as <b>worst</b> <b>case</b> <b>analysis</b> and approximation ratios, {{in contrast to}} classical mechanism design in economics which often makes distributional assumptions about the agents. It also considers computational constraints to be of central importance: mechanisms that cannot be efficiently implemented in polynomial time are {{not considered to be}} viable solutions to a mechanism design problem. This often, for example, rules out the classic economic mechanism, the Vickrey-Clarke-Groves auction.|$|E
5000|$|The {{origins of}} robust {{optimization}} {{date back to}} the establishment of modern decision theory in the 1950s and the use of <b>worst</b> <b>case</b> <b>analysis</b> and Wald's maximin model as a tool for the treatment of severe uncertainty. It became a discipline of its own in the 1970s with parallel developments in several scientific and technological fields. Over the years, it has been applied in statistics, but also in operations research,electrical engineering, control theory, finance, portfolio management logistics, manufacturing engineering, chemical engineering, [...] medicine, and computer science. In engineering problems, these formulations often take the name of [...] "Robust Design Optimization", RDO or [...] "Reliability Based Design Optimization", RBDO.|$|E
40|$|To our knowledge, at {{the time}} of writing no {{methodology}} exists to dimension a sensor network so that a <b>worst</b> <b>case</b> traffic scenario can be definitely supported. In this paper, the well known network calculus is tailored {{so that it can be}} used as a tool for <b>worst</b> <b>case</b> traffic <b>analysis</b> in sensor networks. To illustrate the usage of the resulting sensor network calculus, typical example scenarios are analyzed by this new methodology. Sensor network calculus provides the ability to derive deterministic statements about supportable operation modes of sensor networks and the design of sensor nodes...|$|R
40|$|We {{present a}} new GCD {{algorithm}} for two integers that combines both the Euclidean and the binary gcd approaches. We give its <b>worst</b> <b>case</b> time <b>analysis</b> and prove that its bit-time complexity is still O(n 2) for two n-bit integers. However, our preliminar experiments {{show that it}} is very fast for small integers. A parallel version of this algorithm matches the best presently known time complexity, namely O (n log n) time with n 1 +ɛ, for any constant ɛ> 0...|$|R
40|$|We {{report a}} {{comparative}} laboratory study between 2 peripheral intravenous catheters {{equipped with a}} passive fully automatic safety mechanism to assess generation of blood droplets during withdrawal. One presented no fluid droplets, whereas the other presented droplets in 48 % and 60 % for the best and <b>worst</b> <b>case,</b> with <b>analysis</b> of variance showing positive effects {{on the number of}} droplets generated (P <. 001). Safety devices can introduce hazards if health care workers are at risk from blood splatter...|$|R
5000|$|The {{calculation}} of time complexity of 'most frequent k char string similarity' is quite simple. In {{order to get}} the maximum frequent K characters from a string, the first step is sorting the string in a lexiconical manner. After this sort, the input with highest occurrence can be achieved with a simple pass in linear time complexity. Since major classical sorting algorithms are working in O(nlogn) complexity like merge sort or quick sort, we can sort the first string in O(nlogn) and second string on O(mlogm) times. The total complexity would be O(nlog n [...] ) + O (m log m) which is O(n log n) as the upper bound <b>worst</b> <b>case</b> <b>analysis.</b>|$|E
50|$|The {{classical}} integer sorting algorithms of pigeonhole sort, counting sort, and radix sort {{are widely}} used and practical. Much of the subsequent research on integer sorting algorithms has focused less on practicality and more on theoretical improvements in their <b>worst</b> <b>case</b> <b>analysis,</b> and the algorithms that come from this line of research are not believed to be practical for current 64-bit computer architectures, althoughexperiments have shown {{that some of these}} methods may be an improvement on radix sorting for data with 128 or more bits per key. Additionally, for large data sets, the near-random memory access patterns of many integer sorting algorithms can handicap them compared to comparison sorting algorithms that have been designed with the memory hierarchy in mind.|$|E
50|$|In the {{aftermath}} of the collapse of the Soviet Union, the United States and Russia have developed a deep cooperation designed to assure the security of Russia’s nuclear arsenal. While a number of steps have been taken to consolidate and improve the security of Russia’s strategic nuclear arms, particularly under the Cooperative Threat Reduction program, concern remains over the security of the Russian tactical nuclear weapons arsenal. In particular, a serious debate has arisen over the status of what are known as “suitcase nuclear weapons,” very small Soviet-era nuclear devices. This debate is all the more relevant when one considers the potential destruction that could occur should such a device {{fall into the hands of}} an organization like al-Qaeda. The term suitcase nuke is generally used to describe any type of small, man-portable nuclear device although there is serious debate as to the validity of the term itself. In a <b>worst</b> <b>case</b> <b>analysis,</b> a suitcase nuke would be small enough to be hand-carried into a major population or leadership center (downtown Manhattan or Capitol Hill, for example) undetected and then detonated. Although, by most accounts, the yield of such a device is likely far less than ten kilotons, its combined effects may have the potential to kill tens of thousands, if not more. There is a great deal of confusion over just how many of these suitcase devices exist or if they even exist at all. By some accounts, the Soviet Union built hundreds of these devices, of which several dozen are missing. Based on other reports, suitcase nukes were never built in large numbers or were never deployed. There is no definitive open source information on the number, location, security, or status of these suitcase nuclear bombs. Barring full disclosure from official government sources, one can only look back on the statements made up to this point and attempt to understand the range of possible truths about suitcase nukes. In any event, the uncertainty of the situation should remain a source of concern, particularly in light of the interest of terrorist networks in weapons of mass destruction.|$|E
40|$|In {{this paper}} we analyze two risk {{measures}} using the Binomial Model. In one case {{we show that}} the distance-to-default measure is indeed a Z-statistic. In an empirical application we estimate the probability of default for Chilean banks. Our second measure is a pseudo implied volatility which is obtained from a question. From a small survey we find that {{results are consistent with}} market values. Finally, we consider the <b>worst</b> <b>case</b> scenario <b>analysis</b> applied to Value at Risk and to callable bonds. ...|$|R
50|$|Atom is a {{concurrent}} programming language intended for embedded applications. Atom features compile-time task scheduling and generates code with deterministic execution time and memory consumption, simplifying <b>worst</b> <b>case</b> execution time <b>analysis</b> for applications that require hard realtime performance. Atom's concurrency model {{is that of}} guarded atomic actions, which eliminates the need for, and the problems of using, mutex locks.|$|R
40|$|International audienceThis paper {{considers}} a two-stage hybrid flowshop problem {{in which the}} first stage contains several identical discrete machines, and the second stage contains several identical batching machines. Each discrete machine can process {{no more than one}} task at time, and each batching machine can process several tasks simultaneously in a batch with the additional feature that the tasks of the same batch have to be compatible. A compatibility relation is defined between each pair of tasks, so that an undirected compatibility graph is obtained which turns out to be an interval graph. The batch processing time is equal to the maximal processing time of the tasks in this batch, and all tasks of the same batch start and finish together. The goal is to make batching and sequencing decisions in order to minimize the makespan. Since the problem is NP-hard, we develop several heuristics along with their <b>worst</b> <b>cases</b> <b>analysis.</b> We also consider the case in which tasks have the same processing time on the first stage, for which a polynomial time approximation scheme (PTAS) algorithm is presented...|$|R
