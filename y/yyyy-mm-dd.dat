34|0|Public
2500|$|The {{standard}} {{also allows}} for calendar dates to be written with reduced accuracy. For example, one may write [...] to mean [...] "1981 April". The 2000 version allowed writing [...] to mean [...] "April 5" [...] but the 2004 version does not allow omitting the year when a month is present. One may simply write [...] "1981" [...] to refer to that year or [...] "19" [...] {{to refer to the}} century from 1900 to 1999 inclusive. [...] Although the standard allows both the <b>YYYY-MM-DD</b> and YYYYMMDD formats for complete calendar date representations, if the day [...] is omitted then only the [...] format is allowed. By disallowing dates of the form YYYYMM, the standard avoids confusion with the truncated representation YYMMDD (still often used).|$|E
50|$|Date of Birth in {{the form}} <b>YYYY-MM-DD.</b>|$|E
5000|$|<b>YYYY-MM-DD</b> (the ISO 8601-standard) {{is used to}} {{some extent}} in {{official}} documents and in computer related materials.|$|E
50|$|In most Post-Soviet states DD.MM.YYYY {{format is}} used with dots as separators. Exceptionally Lithuania uses hyphens, thus: <b>YYYY-MM-DD</b> http://de2.lt/naudinga-informacija/lentel%C4%97s/2344-datos-ir-laiko-formatas-%C5%A1alyse.|$|E
50|$|Many {{countries}} use the ISO <b>YYYY-MM-DD</b> {{date format}} generally, {{and they have}} the advantage of being unambiguous. (YYYY means four-digit year, MM means two-digit month, and DD means two-digit day.) Local conventions can vary, and for the commonly used Gregorian calendar & Julian calendar sometimes include formats like DD-MM-YYYY, MM-DD-YYYY, <b>YYYY-MM-DD,</b> or even with the month written in Roman numerals. Dates can also be written out partly or completely in words in the local language.|$|E
5000|$|The federal {{government}} tends {{to use the}} big-endian format, but some federal forms, such as a commercial cargo manifest, offer a blank line with no guidance. Passport applications and tax returns use <b>YYYY-MM-DD.</b> English language newspapers generally use MDY (MMMM D, YYYY). In Quebec and New Brunswick the variation of DDMMYYYY is used when written in French. The Newfoundland Provincial Standard for Date/Time Documentation Format in the medical field is specified as <b>YYYY-MM-DD</b> ...|$|E
5000|$|RFC 3339 Date and Time on the Internet: Timestamps {{specifies}} <b>YYYY-MM-DD,</b> i.e. {{a particular}} {{subset of the}} options allowed by ISO 8601.|$|E
50|$|The {{national}} standard (KSXISO8601, formerly KSX1511) also recognizes the ISO-8601-compliant date/time format of <b>YYYY-MM-DD</b> HH:MM:SS, which {{is widely used}} in computing and on the Korean internet.|$|E
50|$|When using {{dates in}} versioning, for instance, file names, {{it is common}} to use the ISO 8601 scheme: <b>YYYY-MM-DD,</b> as this is easily string sorted to increasing/decreasing order. The hyphens are {{sometimes}} omitted.|$|E
50|$|ISO 2014, though superseded, is the {{standard}} that originally introduced the all-numeric date notation in most-to-least-significant order <b>YYYY-MM-DD.</b> The ISO week numbering system was introduced in ISO 2015, and the identification of days by ordinal dates was originally defined in ISO 2711.|$|E
50|$|The <b>YYYY-MM-DD</b> layout is {{the only}} common format that can provide this. Sorting other date {{representations}} involves some parsing of the date strings. This also works when a time in 24-hour format is included after the date, as long as all times are understood {{to be in the}} same time zone.|$|E
5000|$|ISO 8601 Data {{elements}} and interchange formats - Information interchange - Representation of dates and times specifies <b>YYYY-MM-DD</b> (the separators are optional, but only hyphens {{are allowed to}} be used), where all values are fixed length numeric, but also allows YYYY-DDD, where DDD is the ordinal number of the day within the year, e.g. 2001-365.|$|E
50|$|ISO 2014 is an {{international}} standard that was issued in April 1976, and superseded by ISO 8601 in June 1988. ISO 2014 was the standard that originally introduced the all-numeric date notation <b>YYYY-MM-DD</b> with the digits in order starting with the most significant digit first (similar to big-endian). It was technically identical to ISO Recommendation R 2014 from 1971.|$|E
50|$|The written {{date format}} in Australia, {{as in the}} UK and New Zealand, is (d)d/mm/yyyy (e.g., 30/09/2001), however the ISO 8601 date format of <b>YYYY-MM-DD</b> is also officially accepted. This is the {{recommended}} short date format for government publications. The first two digits of the year are often omitted in everyday use and on forms (e.g., 30/09/01).|$|E
5000|$|When {{numbers are}} used to {{represent}} months, {{a significant amount of}} confusion can arise from the ambiguity of a date order; especially when the numbers representing the day, month or year are low, it can be impossible to tell which order is being used. This can be clarified by using four digits to represent years, and naming the month; for example, [...] "Feb" [...] instead of [...] "02". The ISO 8601 date order with four-digit years: <b>YYYY-MM-DD</b> (introduced in ISO 2014), is specifically chosen to be unambiguous. The ISO 8601 standard also has the advantage of being language independent and is therefore useful when there may be no language context and a universal application is desired (expiration dating on export products, for example). Many Internet sites use <b>YYYY-MM-DD,</b> and those using other conventions often use -MMM- for the month to further clarify and avoid ambiguity (2001-MAY-09, 9-MAY-2001, MAY 09 2001, etc.).|$|E
5000|$|<b>YYYY-MM-DD</b> is {{official}} Nepali Vikram Samvat. An {{example of}} Vikram Samvat YYYY-DD-MM usage used is the online news portal Onlinekhabar. But for Gregorian calendar DD-MM-YY [...] The DD-MM-YY is the predominant short {{form of the}} numeric date usage. Almost all government documents need to be filled up in the YYYY-DD-MM format. An example of YYYY-DD-MM usage is the passport application form.|$|E
50|$|Another {{example of}} a non-dictionary use of {{lexicographical}} ordering appears in the ISO 8601 standard for dates, which expresses a date as <b>YYYY-MM-DD.</b> This formatting scheme has the advantage that the lexicographical order on sequences of characters that represent dates coincides with the chronological order: an earlier date is smaller in the lexicographical order than a later date. This date ordering makes computerized sorting of dates easier by avoiding {{the need for a}} separate sorting algorithm.|$|E
5000|$|In this syntax, the [...] {{is either}} a domain name or an email address,and the date is in the <b>YYYY-MM-DD</b> format, such as [...] Thus, a {{specific}} tag is tied to a specific domain name or email address at a specific point of time. It is required that the [...] "tagging entity" [...] creating the tag {{be in control of}} the specified domain or email address as of 00:00 UTC on the specified date. This requirement makes each tag globally and persistently unique. The authority name alone would not suffice for global uniqueness, since the ownership of domains and email addresses is subject to change.|$|E
5000|$|In Sweden, the ISO 8601 {{standard}} is followed in most written Swedish, but older forms remain. Dates are generally and officially {{written on the}} form <b>YYYY-MM-DD,</b> for instance 2001-08-31 for 31 August 2001, or using the full format (...) [...] Dates can also be shortened, allowing for two-digit years, so the dates are usually written on the form YY-MM-DD, which means that 31 August 2001 also can be written as 01-08-31. One can also omit the hyphens, leaving the notation to 010831.Older forms for 31 August 2001 are 31/8 2001, or with the two-digit notation 31/8 -01. The common trait for all Swedish date notations is that the month (August or 8) always is between the year (01) and the day (31st). Months are not capitalised when written using letters (i.e. , not [...] ).|$|E
5000|$|The {{standard}} {{also allows}} for calendar dates to be written with reduced accuracy. For example, one may write [...] "1981-04" [...] to mean [...] "1981 April". The 2000 version allowed writing [...] "--04-05" [...] to mean [...] "April 5" [...] but the 2004 version does not allow omitting the year when a month is present. One may simply write [...] "1981" [...] to refer to that year or [...] "19" [...] {{to refer to the}} century from 1900 to 1999 inclusive. Although the standard allows both the <b>YYYY-MM-DD</b> and YYYYMMDD formats for complete calendar date representations, if the day DD is omitted then only the YYYY-MM format is allowed. By disallowing dates of the form YYYYMM, the standard avoids confusion with the truncated representation YYMMDD (still often used).|$|E
50|$|Besides that, in Hungary the big-endian year-month-day order, {{has been}} {{traditionally}} used. In 1995, also in Germany, the traditional notation was replaced in the DIN 5008 standard, which defines common typographic conventions, with the ISO 8601 notation (e.g., “1991-12-31”), and becoming the prescribed date format in Germany since 1996-05-01. The latter {{is beginning to}} become more popular, especially in IT-related work and international projects. Since portions of the population continued to use the old format, the traditional format was re-introduced as alternative to the standard <b>yyyy-mm-dd</b> format to DIN 5008 in 2001 and DIN ISO 8601 in September 2006 but its usage is restricted to contexts where misinterpretation cannot occur. The expanded form of the date (e.g., “31. Dezember 1991”) continues to use the little-endian order and the ordinal-number dot for {{the day of the}} month.|$|E
50|$|In addition, the International Organization for Standardization {{considers}} its ISO 8601 {{standard to}} make sense from a logical perspective. Mixed units, for example feet and inches, or pounds and ounces, are normally written with the largest unit first, in decreasing order. Numbers are also written in that order, so the digits of 2006 indicate, in order, the millennium, the century within the millennium, the decade within the century, and the year within the decade. The only date order {{that is consistent with}} these well-established conventions is year-month-day. A plain text list of dates with this format can be easily sorted by file managers, word processors, spreadsheets and other software tools with built-in sorting functions. Some database systems use an eight-digit YYYYMMDD representation to handle date values. Naming folders with <b>YYYY-MM-DD</b> at the beginning allows them to be listed in date order when sorting by name - especially useful for organising document libraries.|$|E
3000|$|Timestamp: {{timestamp}} {{relative to}} the instant when the measurement of the current passing through the power line is done. Date in the format <b>YYYY-MM-DD</b> HH 24 :MI; [...]...|$|E
40|$|This dataset {{contains}} {{samples of}} solar flares measurements of classes X, M, C and B. For clarification, the flares {{are classified as}} follows: 	Class X: flares > 10 ^- 4 watts/m^ 2 	Class M: 10 ^- 5 watts/m^ 2 <flares < 10 ^- 4 watts/m^ 2 	Class C: 10 ^- 6 watts/m^ 2 <flares< 10 ^- 5 watts/m^ 2 	Class B: 10 ^- 7 watts/m^ 2 <flares< 10 ^- 6 watts/m^ 2 This dataset was assembled with data from [URL] The date (<b>yyyy-mm-dd</b> hh:mm:ss) the authors assembled the data is 2017 - 11 - 14 13 : 48 : 37 The original data source is the National Oceanic & Atmospheric Administration (NOAA), U. S. Departement of Commerce. Data description: 	Class: Class of the flare: X, M, C or B 	Date: Date of occurence in <b>yyyy-mm-dd</b> format. 	AR: Active Region ID attributed by NOAA. 	Begin: time the flare begins in hh:mm:ss format. 	Max: time the flare reaches its max value in hh:mm:ss format. 	End: time the flare vanishes in hh:mm:ss format. The dataset has 2, 256 tuples divided as follows: 	 171 tuples with X class flares data (7. 58...|$|E
40|$|This dataset {{contains}} X-ray flux {{data from}} March 1, 2001 through October 18, 2017. The {{source of this}} data is the National Oceanic and Atmospheric Administration (NOAA - [URL] or, more specifically, at [URL] The samples are generated every 2 or 3 seconds. Each tuple has the following information separated by a comma: 	time_tag: Date and time for each observation in the format <b>YYYY-mm-dd</b> hh:mm:ss. SSS UTC" 	xs: X-ray short wavelength channel irradiance (0. 5 - 0. 3 nm); 	xl: X-ray long wavelength channel irradiance (0. 1 - 0. 8 nm); 	"- 99999 " indicates missing values. The scripts used to obtain this data from the NOAA website {{are available in the}} repository [URL] The file is compressed with tar and gunzip...|$|E
40|$|Simulated river {{discharge}} at each subbasin from E-HYPEv 2. 5 in m 3 /s. Daily {{data for}} 1961 to 2001 as time-series. Original data source: See {{information about the}} E-HYPE setup. Tools for repurposing: HYPE. Temporal resolution: Daily. Spatial resolution: 215 km 2. Unit: m 3 /s. Data format: Zipped text file. On the first row there is a brief description {{and the rest of}} the file contains one row per time step, with the sub basin id as each column header on the second row. The first item on each line is the time step using the format <b>YYYY-MM-DD,</b> so 1961 - 01 - 01 for the first of January 1961. The dataset, Subbasin(EHYPE 2 pt 5). zip (shapefile with subbasin polygons) can be linked with the data...|$|E
40|$|Time {{series for}} 1961 to 2001 {{describing}} corrected air temperature for each E-HYPE subbasin. The data describes daily values for each subbasin. Unit of measurement: degrees Celsius. The temperature {{is derived from}} the WFD gridded data set which consists of the ERA- 40 reanalysis data set daily temperature (Weedon et al. 2011). For each subbasin (mean area 215 km 2), the grid point nearest the subbasin centroid is used. Temperature at the grid elevation is corrected in the HYPE model to temperature at the subbasin elevation using a factor in the parameter file. Original data source was WFD Forcing data and HYDE was used for repurposing. The data formate is a zipped text file. On the first row there is a brief description {{and the rest of the}} file contains one row per time step, with the sub basin id as each column header on the second row. The first item on each line is the time step using the format <b>YYYY-MM-DD,</b> so 1961 - 01 - 01 for the first of January 1961...|$|E
40|$|Time-series for 1961 to 2001 {{describing}} the input forcing precipitation for each E-HYPE subbasin. The data describes daily values for each subbasin (mean area 215 km 2). Unit of measurement: mm. The precipitation {{is derived from}} the WFD gridded data set which consists of the ERA- 40 reanalysis data set daily precipitation output (Weedon et al. 2011) corrected for wet days mean monthly precipitation and undercatch (for example to CRU and GPCC). For each subbasin, the gridpoint nearest the subbasin centroid is used. Within E-HYPE a correction is made to increase precipitation above 700 m elevation. Original data source was WFD Forcing data and HYDE was used for repurposing. The data format is a zipped text file. On the first row there is a brief description {{and the rest of the}} file contains one row per time step, with the sub basin id as each column header on the second row. The first item on each line is the time step using the format <b>YYYY-MM-DD,</b> so 1961 - 01 - 01 for the first of January 1961...|$|E
40|$|The bullet (•) {{indicates}} a category key. The arrow (→) is {{a reference to}} a parent data item. Data items in the DIFFRN_SCAN category describe the param-eters of one or more scans, relating axis positions to frames. Each scan is uniquely identified by the value of _diffrn_scan. id. The data items in this category give overall information for the scan. The detailed frame-by-frame data are given in DIFFRN_SCAN_FRAME and DIFFRN_SCAN_FRAME_AXIS. The val-ues of _diffrn_scan. date_start and *. date_end give the start-ing and ending time for a scan. The original definition of the <b>yyyy-mm-dd</b> data type, which includes date and time, has been extended in the CBF/imgCIF dictionary. This allows the sec-onds part of the time to include an optional decimal fraction. The approximate average integration time for each step of the scan is given by the value of _diffrn_scan. integration_time. The scan is tied to individual frame IDs by the values of _diffrn_scan. frame_id_start and *. frame_id_end. The num-ber of frames in the scan is given by the value of _diffrn_ scan. frames...|$|E
40|$|Time {{series of}} epileptic {{seizures}} in a 8 yo male with idiopathic catastrophic epilepsy (probably of mitochondrial origin) in a cannabidiol (CBD) trial. The data are of observed seizures during each {{day by the}} patient's parents and caregivers and thus are not exact values for total daily seizure activity; the values in this dataset are best considered a lower bound. However, observation effort was essentially constant over the time series, so the data likely reflect overall trends even though precise daily seizure counts are not available. The time series includes a 21 -day baseline period and a 36 -day trial period. The CBD was administered via g-tube in an olive oil base. Dosage was approximately 7. 5 mg/kg/day for two weeks, and approximately 8 mg/kg/day for three weeks. Each 1 ml of oil contained 10. 00 mg of total CBD and 0. 42 mg of total THC, for a CBD:THC ratio of approximately 20 : 1 (analysis by HPLC). Data Dictionary 	Date: <b>YYYY-MM-DD</b> 	Tonic: tonic seizure count ([URL] 	Tonic-Clonic: tonic-clonic (aka "grand mal") seizure count ([URL] 	Spasms: myoclonic seizure count ([URL] 	Total: total number of seizures (sum of above) 	Notes: notes by parent...|$|E
40|$|Livestock {{production}} is changing worldwide. It {{is also the}} case that the ban on antibiotic growth promoters in Europe, the shift in animal production centres to Brazil or Eastern Europe, increase in demand for traceability and natural production, and the emergence of new diseases, are all forcing livestock producers to adapt new husbandry, management, nutrition and healthcare techniques. Food safety is an explosive political issue - the expectations and demands of the informed consumer have altered perceptions of risk and brought food safety to the very front and centre of politics. The changes in legislation on the use of feed additives will impact livestock production, location of production and feed formulation. Veterinarians and producers look for alternatives to maintain intestinal health and maximise animal performance, whilst still complying with stringent EU legislation. This book reviews the changes in livestock production and some of the clinical and sub-clinical disease challenges faced in pig, poultry or ruminant production. The book discusses bacterial challenges such as E. coli, Salmonella. Clostridia and Campylobacter, as well as viral diseases such as circovirus or avian influenza. One of the key elements in determining intestinal health is the use of molecular technologies in determining the composition of the intestinal microflora in poultry, pigs and ruminants. The interaction between innate, cellular and humoral immunity and the effects on antigen recognition and immune response will impact overall performance and ultimately the profit for producers. The understanding of Glycomics and the role of carbohydrates in cell-to-cell communication is crucial in overcoming sub-clinical challenges in modern livestock production. ISBN- 13 : 978 - 90 - 76998 - 91 - 6 Price (¿) : 85. 00 Price ($) : 113. 00 Publication date: 2007 - 12 - 01 (<b>yyyy-mm-dd...</b>|$|E
30|$|The {{architecture}} {{consists of}} 5 layers including the (1) Interaction, Communication and sensing layer (ICS) were sensing, control of actuators and devices and interactions occur, (2) the data layer were sensor data is captured from the sensors, agent action data is recorded and contextual data is stored, (iii) {{the decision and}} logging layer were the people profile is processed and the actions {{carried out by the}} agents is logged, (iv) information layer were relationships between the agents, environment and person are managed and appropriate interventions decided, (v) context layer were contextual changes are detected and managed. Information is exchanged by the agents between layers (6 – 9). The sensor and sensor data agents work together to process sensor event data (A – B) and this results in a sensor event message being formed that contains the sensor type (PIR, bed-chair or door contact), time (in the format: <b>yyyy-mm-dd</b> hh:mm:ss) it was triggered and the event types (opening, closing and room visited) and the location of the sensor event (including the: bedroom, kitchen, main hall, Livingroom, Foodcupboard door, Fridgedoor…). The sensor event message (C) is sent to the context agent and this agent determines what has changed in the environment, for example if the back door has been opened and for how long it has been opened. The context agent sends a contextual event message is sent to the Intervention agent (D). Once the intervention agent receives this message, it determines the correct intervention to make to the person. This includes issuing a reminder to have meals at certain times of the day, alerting the person that the back door has been left open and during the night. The intervention message (F) is sent to the GUI agent, the appropriate method of putting forward the intervention is selected by the profile agent (F). Once the appropriate intervention has been selected, a record is stored (G) in the agent action data store. When feedback is requested by the person, this agent action data is analysed, patterns detected and appropriate feedback is selected (H). Interventions and feedback are presented to the person in the environment either with visual or auditory interactions (I) and (J). The MAS system is able to adapt the interventions that are provided to the person by tracking what the current contextual change, activity and what intervention has previously been issued. This is achieved by comparing the current event to the previous event throughout the current sensor event processing cycle. If the same event has previously occurred then how many times it has occurred and the time difference between the events is calculated. Events that logically follow each other (for example, door opening and door closing events) are recognised so that the context behind the event may be determined. For example, if the person is alerted that the back door has been opened for 10  minutes and did not close the door, the intervention issued for the door being open for 20  minutes would be different. This information is fed between layers (K) so that the most appropriate intervention is selected. The interface agent has been replaced with a GUI agent that offers more functionally and drives the interface during the multimodal interactions. The MAS consumes data from sensors that are located in the environment and these include bed-chair, door contact and PIR sensors as well as microphones for auditory interaction. The next section outlines the Multi-Agent system (MAS), what interventions and feedback are, and the underlying agent processes involved in forming the intervention and feedback messages.|$|E
40|$|This {{document}} {{explain how}} data were generated {{and how to}} interpret them. LICENSE: CC 0 But {{if you want to}} combine data with other datasets, feel free to use {{them as if they were}} published under CC 0 license. Data were published in February 2017. At that time, Zenodo only provided CC BY, CC BY-SA, CC BY-NC, CC BY-ND and CC BY-NC-ND. No CC 0 option was available. HOW DATA WERE COLLECTED Data provided in this dataset were collected in Fall 2016 as part of the Bachelor course 'Formation des usagers en bibliothèques' taught at the University of applied sciences Geneva (Information Sciences Department). Data were generated by the interaction of the students with DAD, the Dynamic assessment dashboard, software to be released in 2017 ([URL] Only data related to individual activities are published in this set. They were then cleaned and anonymised. The activities. csv file is provided for information purpose. Data are provided in 2 formats: * Comma-spearated values (submissions. csv) * SQLite (submissions. sqlite) CSV is not correctly read by Excel, it's recommended to convert it into an. xslx file before using it. SQLite is provided in order to apply different sorting and filters to the data. It can be read using SQLite manager for Firefox ([URL] CODEBOOK Here is the name, the meaning and the possible values of the columns (name - description [possible values]). ID - database row ID [(database unique row ID) ] ActivityID - unique activity ID [1, 2, 3, 4, 10, 14, 16, 21, 22, 23, 41] StudentID - unique anonymous student ID [# 50 to # 72] date - recorded by the database [timestamp] year - year extracted from date [2016, 2017] month - month extracted from date [01, 11, 12] day - day extracted from date [1 to 31] weekday - weekday calculated from date [1, 2, 3, 4, 5, 6, 7] hour - hour extracted from date - 00 to 23] minute - minute extracted from date [00 to 59] second - second extracted from date [00 to 59] gr - group activity or not [0] validated - was the submission validated or not [- 1 or 1] week - week number [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, H 1, H 2] ActivityID meaning Activities are described in activities. csv. This file conatins only individual activities description. The 2 activities iwth the bonus particularity are not present in the data because students didn't need to apply for them. A bonus was automatically added when activity 1 AND 2 AND 3 AND 4 (bonus = activity 5) as well as when activities 21 AND 22 AND 23 (activity 24) were completed. Date format The date is formatted like <b>YYYY-MM-DD</b> hh:mm:ss. Weekday meaning 1 stands for Monday, 2 for Tuesday and so on until 7 for Sunday. Group meaning 0 stands for 'individual activity' and 1 would stand for group activity'. Only entries with gr=" 0 " appears in this dataset. Validated meaning - 1 stands for 'rejected' and 1 for 'validated'. Week meaning The course lasts 10 weeks. 1 stands for the 1 st week and so on until 10 that stands for the 10 th week. H 1 and H 2 stand for the Christmas holiday weeks that took place between week 8 and 9. No submission were sent in weeks 1, 2 and 10...|$|E
40|$|Recognition {{of coastal}} {{precipitation}} ++++++++++++++++++++++++++++++++++++ This repository contains source-code for {{the recognition of}} coastal rainfall project. For more details, see the scientific publication `Global recognition and analysis of coastline associated rainfall using objective pattern recognition `_. It includes these modules and configuration files: * Pattern - python module to run apply the pattern recognition * Submit - python module to submit the recognition jobs to a linux cluster * config - configuration file to run the pattern recognition * setup - text file that defines the threshold combinations to create a threshold ensemble Prerequisites ============= `Python 2. 7 Release `_ is needed to run the code. Python packages [...] - Additionally the following python packages must be installed on the system to run the pattern recognition: * `Numpy `_ * `SciPy `_ * `OpenCV `_ * `Netcdf 4 -Python `_ All packages are open source and should be available via the package manager of your OS. To install the packages for Ubuntu 12. 04 and later, and Debian wheezy and later:: apt-get install python-numpy python-scipy python-opencv python-netcdf On Arch 2012. 07. 15 and later:: pacman -S python 2 -numpy python 2 -scipy opencv python 2 -netcdf 4 (via AUR) On Mac OS X 10. 6 and later (via port reposetory system) :: port install py 27 -numpy py 27 -scipy py 27 -netcdf 4 opencv +python 27 Data and format [...] - The code expects ``daily`` rainfall input files in netcdf format. As {{long as it is}} sub-daily the temporal resolution doesn't matter. Also any spatial resolution can be used to run the code. You will also need to supply a land-sea mask in netcdf format. This mask has to have the same spatial resolution as the rainfall estimates and {{is supposed to be a}} 2 D-array (latlon). If you wish to create an ensemble of different threshold setups you might want to take a look into the ``setup`` file. Here all possible threshold combinations are defined. You can change, delete or add definitions. But note that the names of each definition should start with ``config`` followed by a 2 digit integer such as config 01. For the application of the rainfall threshold a climatology of monthly mean rain data is needed. Therefore a multi year monthly mean in a netcdf file should be created form the rainfall data. Note that unit MUST be the same as for the rainfall estimates. For instance we are looking for a monthly average of 3 hly rainfall data and not for a monthly average of daily rainfall data. Building ======== Additional building should not be necessary Usage ===== The pattern recognition can be used in two ways:: * in single threading mode * in multithreading mode via parallel batch system (pbs) Regardless of the mode you are running the code you will have to edit the file config. This file provides all necessary parameters to run the code. The following variables are set: * folder = the directory of the rainfall estimates. The data must have the following structure: ``folder/YYYY/product-YYYY_MM_DD. nc``. If folder is set to ``/srv/data/rain/`` and the rainfall product is called ``Cmoprh`` the data is expected to have the following structure: ``/mnt/data/rain/ 1998 /Cmorph- 1998 _ 01 _ 01. nc`` for the 01. Jan 1998. * head = the leading string of all filenames, usually the rainfall product name * targetdir = the directory where the pattern recognition data is stored * slmfile = the filename of the land-sea mask in netcdf-format * ensemble = should be a threshold setup ensemble be created (boolean) * reso = the spatial resolution of the data in km * area = the threshold that is supposed to be meso-scale usually 500 km but you can change it. * ecce = the threshold for eccentricity (see publication for details) * beam = the threshold for the straight length variation * perc = the threshold for the rain intensity in percentiles * NOTE: If you choose to create a threshold ensemble the variables ecce, beam, prec have no effect. * monmean = netcdf file where a climatology for monthly mean rainfall data is stored. * erase = The pattern detection can operate more successfully when small islands are erased from the slm-data. Should this be done (boolean). Note: tha this feature is available but not very well tested * size = If erase is set to true what is the maximum size of islands that should be deleted (km^ 2) * scale = how many boxes representing the coastal area (boxcounting) * varname = the name of the rainfall variable in the netcdf-file * lon = the name of the longitude variable in the netdf-file * lat = the name of the latitude variable in the netcdf-file * time = the name of the time variable in the netcdf-file * units = the unit of the rainfall estimates * name = your name * institution = the name of your institution The module that reads the configuration is written in a way that you can add any variable you want to the config file. It simply has to have the following structure:: key = value Running in the single mode [...] If you want to run the pattern recognition in single processing mode simply run the Pattern script with the following command line options:: Pattern FirstDate LastDate configXX Where FirstDate is the starting data and LastData the last date. Note that the dates have to be in <b>YYYY-MM-DD</b> format. The config parameter is optional and only used if you want to create a threshold ensemble. the configXX value must be defined in the ``setup`` file. Note: You you are using a config parameter the ``targetdir`` variable is changed accordingly. So if you set ``config 01 `` as config parameter and set ``/srv/data/PatternDetect/`` as targetdir variable ``Config 01 `` will be added to the targetdir string. Examples:: 1) Pattern 1998 - 01 - 01 2012 - 12 - 31 config 13 2) Pattern 1998 - 01 - 01 2012 - 12 - 31 1) Run the pattern recognition between 01. Jan 1998 and 31. Dec 2012 for threshold setup ``config 13 `` as defined in the ``setup`` file. 2) Same as 1) but without a threshold ensemble. For more information run:: Pattern [...] help Running in paralell mode (PBS) [...] It is also possible to send several pattern recognition jobs to a Linux cluster to speed up the process of the recognition. In general there are two scenarios: * distribution of jobs to create a threshold ensemble * distribution of jobs between dates If you choose to create a threshold ensemble and want to distribute the jobs simply run:: submit [...] config=config 01,config 02, [...] .,configNN FirstDate LastDate If you don't want to create a threshold ensemble but yet want to send the job to a Linux cluster simply run:: submit FirstDate LastDate Note the FirstDate and LastDate have the same meaning and format as in running in single mode Please type:: submit [...] help to get more info like the maximum number of jobs that are submitted to the linux cluster. If you want to change the header string of the PBS script or change the submit command (e. g. to llsubmit) please edit the source-code of submit. Header and submit command are defined in the very beginning of the script. Testing ======= Some fake rain data with a land-sea mask is comes with the code. The data is stored in. test in this directory. If you want to test the pattern recognition simply run:: Pattern [...] test Contributing ============ We welcome all types of contributions, from blueprint designs to documentation to testing to deployment scripts. Bugs ==== Bugs should be filed to **martin. bergemann@monash. edu*...|$|E

