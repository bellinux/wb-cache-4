1465|1060|Public
25|$|In its {{simplest}} form, the bound {{states that}} the variance of any <b>unbiased</b> <b>estimator</b> {{is at least as}} high as the inverse of the Fisher information. An <b>unbiased</b> <b>estimator</b> which achieves this lower bound is said to be (fully) efficient. Such a solution achieves the lowest possible mean squared error among all unbiased methods, and is therefore the minimum variance unbiased (MVU) estimator. However, in some cases, no unbiased technique exists which achieves the bound. This may occur even when an MVU estimator exists.|$|E
25|$|The Rao–Blackwell theorem, {{a result}} which yields {{a process for}} finding the best {{possible}} <b>unbiased</b> <b>estimator</b> (in the sense of having minimal mean squared error). The MLE is often a good starting place for the process.|$|E
25|$|If the {{assumption}} of normality is replaced by assumptions of homoscedasticity and uncorrelatedness of errors, and if one still assumes zero mean, then the Gauss–Markov theorem entails that the solution is the minimal <b>unbiased</b> <b>estimator.</b>|$|E
40|$|The {{canonical}} {{form for the}} comparison of certain linear estimators using Pitman's Measure of Closeness is generalized to the class of all linear estimators. Under the assumption of normality, the equivalence of Pitman-closest linear <b>unbiased</b> <b>estimators</b> and best linear <b>unbiased</b> <b>estimators</b> is shown. A sufficient condition is given for which the BLUE will be Pitman-closer than the best linear equivalent estimator (BLEE). Pitman's measure of closeness order statistics best linear <b>unbiased</b> <b>estimators</b> best linear equivariant estimators...|$|R
40|$|Model {{selection}} criteria often arise by constructing unbiased or approximately <b>unbiased</b> <b>estimators</b> of measures known as expected overall discrepancies (Linhart & Zucchini, 1986, p. 19). Such measures quantify {{the disparity between}} the true model (i. e., the model which generated the observed data) and a fitted candidate model. For linear regression with nor-mally distributed error terms, the “corrected ” Akaike information criterion and the “mod-ified ” conceptual predictive statistic have been proposed as exactly <b>unbiased</b> <b>estimators</b> of their respective target discrepancies. We expand on previous work to additionally show that these criteria achieve minimum variance within the class of <b>unbiased</b> <b>estimators...</b>|$|R
40|$|This paper {{treats the}} problem of {{simultaneously}} estimating the precision matrices in multivariate normal distributions. A condition for improvement on the <b>unbiased</b> <b>estimators</b> of the precision matrices is derived under a quadratic loss function. The improvement condition {{is similar to the}} superharmonic condition established by Stein (1981). The condition allows us not only to provide various alternative estimators such as shrinkage type and enlargement type <b>estimators</b> for the <b>unbiased</b> <b>estimators,</b> but also to present a condition on a prior density under which the resulting generalized Bayes <b>estimators</b> dominate the <b>unbiased</b> <b>estimators.</b> Also, a uni?ed method improving upon both the shrinkage and the enlargement type estimators is discussed...|$|R
25|$|Since each {{observation}} has expectation λ so {{does this}} sample mean. Therefore, the maximum likelihood estimate is an <b>unbiased</b> <b>estimator</b> of λ. It {{is also an}} efficient estimator, i.e. its estimation variance achieves the Cramér–Rao lower bound (CRLB). Hence it is minimum-variance unbiased. Also it can be proved that the sum (and hence the sample mean {{as it is a}} one-to-one function of the sum) is a complete and sufficient statistic for λ.|$|E
25|$|The use of {{the term}} n−1 is called Bessel's correction, and it is also used in sample {{covariance}} and the sample standard deviation (the square root of variance). The square root is a concave function and thus introduces negative bias (by Jensen's inequality), which depends on the distribution, and thus the corrected sample standard deviation (using Bessel's correction) is biased. The unbiased estimation of standard deviation is a technically involved problem, though for the normal distribution using the term n−1.5 yields an almost <b>unbiased</b> <b>estimator.</b>|$|E
25|$|In 1810, {{after reading}} Gauss's work, Laplace, after proving the central limit theorem, {{used it to}} give a large sample {{justification}} for the method of least square and the normal distribution. In 1822, Gauss was able to state that the least-squares approach to regression analysis is optimal {{in the sense that}} in a linear model where the errors have a mean of zero, are uncorrelated, and have equal variances, the best linear <b>unbiased</b> <b>estimator</b> of the coefficients is the least-squares estimator. This result is known as the Gauss–Markov theorem.|$|E
5000|$|... the MVUE {{minimizes}} MSE among <b>unbiased</b> <b>estimators.</b> In {{some cases}} biased estimators have lower MSE {{because they have}} a smaller variance than does any <b>unbiased</b> estimator; see <b>estimator</b> bias.|$|R
40|$|We {{study the}} {{existence}} of algorithms generating almost surely nonnegative <b>unbiased</b> <b>estimators.</b> We show that given a nonconstant real-valued function f and a sequence of <b>unbiased</b> <b>estimators</b> of λ∈R, there is no algorithm yielding almost surely nonnegative <b>unbiased</b> <b>estimators</b> of f(λ) ∈R^+. The study is motivated by pseudo-marginal Monte Carlo algorithms that rely on such nonnegative <b>unbiased</b> <b>estimators.</b> These methods allow "exact inference" in intractable models, {{in the sense that}} integrals with respect to a target distribution can be estimated without any systematic error, even though the associated probability density function cannot be evaluated pointwise. We discuss the consequences of our results on the applicability of pseudo-marginal algorithms and thus on the possibility of exact inference in intractable models. We illustrate our study with particular choices of functions f corresponding to known challenges in statistics, such as exact simulation of diffusions, inference in large datasets and doubly intractable distributions. Comment: Published at [URL] in the Annals of Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|R
50|$|Similarly to {{weighted}} sample variance, {{there are two}} different <b>unbiased</b> <b>estimators</b> {{depending on the type of}} the weights.|$|R
2500|$|In fact, the minimum-variance <b>unbiased</b> <b>estimator</b> (MVUE) for θ is ...|$|E
2500|$|... minimum-variance <b>unbiased</b> <b>{{estimator}}</b> (UMVU) estimator for {{the maximum}} {{is given by}} ...|$|E
2500|$|However, the {{following}} <b>unbiased</b> <b>estimator</b> {{can be shown}} to have lower variance: ...|$|E
40|$|A {{sufficient}} {{condition for the}} uniqueness of multinomial sequential <b>unbiased</b> <b>estimators</b> is provided generalizing a classical result for binomial samples. <b>Unbiased</b> <b>estimators</b> are applied to infer the parameters of multidimensional or multinomial random walks that are observed until they reach a boundary. Clinical trials are shown to be representable within this scheme and an application to the estimation of the multinomial probabilities following multinomial clinical trials is presente...|$|R
40|$|It is {{proved that}} in a non-Bayesian {{parametric}} estimation problem, if the Fisher information matrix (FIM) is singular, <b>unbiased</b> <b>estimators</b> for the unknown parameter will not exist. Cramer-Rao bound (CRB), a popular tool to lower bound the variances of <b>unbiased</b> <b>estimators,</b> seems inapplicable in such situations. In this paper, we show that the Moore-Penrose generalized inverse of a singular FIM {{can be interpreted as}} the CRB corresponding to the minimum variance among all choices of minimum constraint functions. This result ensures the logical validity of applying the Moore-Penrose generalized inverse of an FIM as the covariance lower bound when the FIM is singular. Furthermore, the result can be applied as a performance bound on the joint design of constraint functions and <b>unbiased</b> <b>estimators.</b> Comment: 10 pages, accepted for publication in IEEE Transactions on Signal Processin...|$|R
5000|$|Topic 3 - Statistics and {{probability}} - {{the geometric}} and negative binomial distributions, <b>unbiased</b> <b>estimators,</b> statistical hypothesis testing and {{an introduction to}} bivariate distributions ...|$|R
2500|$|... or {{the minimum}} {{possible}} variance for an <b>unbiased</b> <b>estimator</b> divided by its actual variance.|$|E
2500|$|If [...] is an <b>unbiased</b> <b>estimator</b> of [...] (i.e., [...] ), {{then the}} Cramér–Rao bound reduces to ...|$|E
2500|$|The {{efficiency}} of an <b>unbiased</b> <b>estimator</b> [...] measures how close this estimator's variance {{comes to this}} lower bound; estimator efficiency is defined as ...|$|E
40|$|A Cramér-Rao type {{lower bound}} for minimum loss <b>unbiased</b> <b>estimators</b> with values in a {{manifold}} is derived, {{and the corresponding}} notion of efficiency is investigated. A by-product is a generalisation {{of the concept of}} covariance of a multivariate statistic to one of a statistic with values in a manifold. Cramer-Rao inequality minimum variance <b>unbiased</b> estimation <b>unbiased</b> <b>estimators</b> with values in a manifold Hessian Fisher information covariance efficiency Weingarten map exponential family of probability distributions mean location Fisher-von Mises distributions integral manifold...|$|R
40|$|We {{present a}} general result giving us {{families}} of incomplete and boundedly complete families of discrete distributions. For such families, the classes of <b>unbiased</b> <b>estimators</b> of zero with finite variance and of parametric functions {{which will have}} uniformly minimum variance <b>unbiased</b> <b>estimators</b> with finite variance are explicitly characterized. The general result allows us to construct {{a large number of}} families of incomplete and boundedly complete families of discrete distributions. Several new examples of such families are described. Comment: 7 page...|$|R
40|$|Maximum {{likelihood}} {{and uniform}} minimum variance <b>unbiased</b> <b>estimators</b> of steady-state probability distribution of system size, probability {{of at least}} ℓ customers in the system in steady state, and certain steady-state measures of effectiveness in the M/M/ 1 queue are obtained/derived based on observations on X, the number of customer arrivals during a service time. The estimators are compared using Asympotic Expected Deficiency (AED) criterion leading to recommendation of uniform minimum variance <b>unbiased</b> <b>estimators</b> over maximum likelihood estimators for some measures...|$|R
2500|$|In {{linguistics}} and cryptanalysis this sum {{is known}} as the repeat rate. The incidence of coincidence (IC) is an <b>unbiased</b> <b>estimator</b> of this statistic ...|$|E
2500|$|An <b>unbiased</b> <b>estimator</b> for the {{variance}} {{is given by}} applying Bessel's correction, using N−1 instead of N to yield the unbiased sample variance, denoted s2: ...|$|E
2500|$|In other words, [...] is an <b>unbiased</b> <b>estimator</b> of {{the first}} moment. If {{we assume that the}} mean [...] lies in the {{interval}} , then Arg will be a (biased) estimator of the mean [...]|$|E
40|$|AbstractThe {{problem of}} {{estimating}} the common regression coefficients is {{addressed in this}} paper for two regression equations with possibly different error variances. The feasible generalized least squares (FGLS) estimators have been believed to be admissible within the class of <b>unbiased</b> <b>estimators.</b> It is, nevertheless, established that the FGLS estimators are inadmissible in light of minimizing the covariance matrices if the dimension of the common regression coefficients is {{greater than or equal}} to three. Double shrinkage <b>unbiased</b> <b>estimators</b> are proposed as possible candidates of improved procedures...|$|R
40|$|AbstractFor many typical {{instances}} where Monte Carlo methods are applied {{attempts were made}} to find <b>unbiased</b> <b>estimators,</b> since for them the Monte Carlo error reduces to the statistical error. These problems usually take values in the scalar field. If we study vector valued Monte Carlo methods, then we are confronted with the question of whether there can exist <b>unbiased</b> <b>estimators.</b> This problem is apparently new. Below it is settled precisely. Partial answers are given, indicating relations to several classes of linear operators in Banach spaces...|$|R
40|$|Sugden and Smith (2002. J. Statist. Plann. Inference 102, 25 - 38) {{investigated}} {{conditions under}} which exact linear <b>unbiased</b> <b>estimators</b> of linear estimands, and also exact quadratic <b>unbiased</b> <b>estimators</b> of quadratic estimands, could be constructed under the randomisation approach. In this paper the method is applied to domains of study and extended to poststratified estimators of finite population totals. The resulting estimators generalise some of those in Doss et al. (1979. J. Statist. Plann. Inference 3, 235 - 247). Some further properties of these estimators are explored...|$|R
2500|$|A {{more general}} {{form of the}} bound can be {{obtained}} by considering an <b>unbiased</b> <b>estimator</b> [...] of the parameter [...] Here, unbiasedness is understood as stating that [...] In this case, the bound is given by ...|$|E
2500|$|So δ1 {{is clearly}} a very much {{improved}} estimator of that last quantity. [...] In fact, since S'n is complete and δ0 is unbiased, δ1 is the unique minimum variance <b>unbiased</b> <b>estimator</b> by the Lehmann–Scheffé theorem.|$|E
2500|$|Suppose [...] is {{an unknown}} {{deterministic}} parameter {{which is to}} be estimated from measurements , distributed according to some probability density function [...] The variance of any <b>unbiased</b> <b>estimator</b> [...] of [...] is then bounded by the reciprocal of the Fisher information : ...|$|E
3000|$|... over {{multiple}} iterations {{and random}} sensor positions. The lower bound {{of the position}} RMSE for <b>unbiased</b> <b>estimators</b> is given by the square root of the average of [...]...|$|R
40|$|We {{consider}} a superposition {{of an unknown}} number of independent homogeneous Poisson processes in which the source of each event can be identified. After observing the system for a fixed time t, the total rate, U(t), of the unobserved processes is to be estimated. We prove that a uniformly minimum mean squared error estimate of U(t) does not exist and all <b>unbiased</b> <b>estimators</b> of U(t) are negatively correlated with U(t) and derive the minimum mean squared error <b>estimator</b> among all <b>unbiased</b> <b>estimators.</b> Mean squared error Poisson process software reliability unbiasedness...|$|R
40|$|We {{consider}} {{the estimation of}} a sparse parameter vector from measurements corrupted by white Gaussian noise. Our focus is on unbiased estimation as a setting under which {{the difficulty of the}} problem can be quantified analytically. We show that there are infinitely many <b>unbiased</b> <b>estimators</b> but none of them has uniformly minimum mean-squared error. We then provide lower and upper bounds on the Barankin bound, which describes the performance achievable by <b>unbiased</b> <b>estimators.</b> These bounds are used to predict the threshold region of practical estimators. Comment: 4 pages, 2 figures. To appear in ICASSP 201...|$|R
