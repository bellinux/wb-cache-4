405|586|Public
25|$|The {{median of}} a {{symmetric}} <b>unimodal</b> <b>distribution</b> {{coincides with the}} mode.|$|E
25|$|The middle bound {{limits the}} nonparametric skew of a <b>unimodal</b> <b>distribution</b> to {{approximately}} ±0.775.|$|E
25|$|In {{simulations}} with a variates {{drawn from}} a uniform distribution the PCI2 has a symmetric <b>unimodal</b> <b>distribution.</b> The tails of its distribution are larger than those of a normal distribution.|$|E
2500|$|For a large {{class of}} <b>unimodal</b> <b>distributions</b> that are {{positively}} skewed the mode, median and mean fall in that order. Conversely {{for a large}} class of <b>unimodal</b> <b>distributions</b> that are negatively skewed the mean {{is less than the}} median which in turn is less than the mode. In symbols for these positively skewed <b>unimodal</b> <b>distributions</b> ...|$|R
50|$|For the {{extension}} of the Theorem to all symmetric <b>unimodal</b> <b>distributions</b> one can start with a classical result of Aleksandr Khinchin: namely that all symmetric <b>unimodal</b> <b>distributions</b> are scale mixtures of symmetric uniformdistributions.|$|R
40|$|In {{this paper}} we {{introduce}} {{a new class of}} multivariate <b>unimodal</b> <b>distributions,</b> motivated by Khintchine's representation. We start by proposing a univariate model, whose support covers all the <b>unimodal</b> <b>distributions</b> on the real line. The proposed class of <b>unimodal</b> <b>distributions</b> can be naturally extended to higher dimensions, by using the multivariate Gaussian copula. Under both univariate and multivariate settings, we provide MCMC algorithms to perform inference about the model parameters and predictive densities. The methodology is illustrated with univariate and bivariate examples, and with variables taken from a real data-set. Comment: 33 pages, 15 figure...|$|R
25|$|Infants {{are able}} to extract {{meaningful}} distinctions in the language {{they are exposed to}} from statistical properties of that language. For example, if English-learning infants are exposed to a prevoiced /d/ to voiceless unaspirated /t/ continuum (similar to the /d/ - /t/ distinction in Spanish) with the majority of the tokens occurring near the endpoints of the continuum, i.e., showing extreme prevoicing versus long voice onset times (bimodal distribution) they are better at discriminating these sounds than infants who are exposed primarily to tokens {{from the center of the}} continuum (<b>unimodal</b> <b>distribution).</b>|$|E
25|$|Distributional {{learning}} {{has also been}} found to help infants contrast two phonemes that they initially have difficulty in discriminating between. Maye, Weiss, and Aslin found that infants {{who were exposed to}} a bimodal distribution of a non-native contrast that was initially difficult to discriminate were better able to discriminate the contrast than infants exposed to a <b>unimodal</b> <b>distribution</b> of the same contrast. Maye et al. also found that infants were able to abstract features of a contrast (i.e., voicing onset time) and generalize that feature to the same type of contrast at a different place of articulation, a finding that has not been found in adults.|$|E
2500|$|For a <b>unimodal</b> <b>distribution</b> the {{following}} bounds are known and are sharp: ...|$|E
5000|$|For <b>unimodal</b> <b>distributions</b> the {{following}} bounds are known and are sharp: ...|$|R
40|$|Denote by [alpha] and [beta] the {{skewness}} and kurtosis respectively of {{a distribution}} with finite fourth moment. We show that [alpha] 2 [less-than-or-equals, slant] [beta] + 2. For <b>unimodal</b> <b>distributions</b> we prove that [alpha] 2 [less-than-or-equals, slant] [beta] +, and for infinitely divisible distributions [alpha] 2 [less-than-or-equals, slant] [beta]. skewness kurtosis mixtures of <b>distributions</b> <b>unimodal</b> <b>distributions</b> infinitely divisible distributions...|$|R
5000|$|Ziggurat algorithm, for monotonously {{decreasing}} density {{functions as}} well as symmetric <b>unimodal</b> <b>distributions</b> ...|$|R
2500|$|It can {{be shown}} for a <b>unimodal</b> <b>distribution</b> that the median [...] and the [...] mean [...] lie within (3/5)1/2 ≈ 0.7746 {{standard}} deviations of each other. In symbols, ...|$|E
2500|$|Since the {{discovery}} of infants’ statistical learning abilities in word learning, the same general mechanism has also been studied in other facets of language learning. For example, it is well-established that infants can discriminate between phonemes of many different languages but eventually become unable to discriminate between phonemes that do not appear in their native language; however, {{it was not clear}} how this decrease in discriminatory ability came about. Maye et al. suggested that the mechanism responsible might be a statistical learning mechanism in which infants track the distributional regularities of the sounds in their native language. To test this idea, Maye et al. exposed 6- and 8-month-old infants to a continuum of speech sounds that varied on {{the degree to which they}} were voiced. The distribution that the infants heard was either bimodal, with sounds from both ends of the voicing continuum heard most often, or unimodal, with sounds from the middle of the distribution heard most often. The results indicated that infants from both age groups were sensitive to the distribution of phonemes. At test, infants heard either non-alternating (repeated exemplars of tokens 3 or 6 from an 8-token continuum) or alternating (exemplars of tokens 1 and 8) exposures to specific phonemes on the continuum. Infants exposed to the bimodal distribution listened longer to the alternating trials than the non-alternating trials while there was no difference in listening times for infants exposed to the <b>unimodal</b> <b>distribution.</b> This finding indicates that infants exposed the bimodal distribution were better able to discriminate sounds from the two ends of the distribution than were infants in the unimodal condition, regardless of age. This type of statistical learning differs from that used in lexical acquisition, as it requires infants to track frequencies rather than transitional probabilities, and has been named “distributional learning.” ...|$|E
5000|$|The {{median of}} a {{symmetric}} <b>unimodal</b> <b>distribution</b> {{coincides with the}} mode.|$|E
40|$|In a large {{electorate}} it {{is natural}} to consider voters’ preference profiles as frequency distributions over the set of all possible preferences. We assume coherence in voters’ preferences resulting in accumulation of voters preferences. We show that such distributions can be studied via superpositions of simpler so called <b>unimodal</b> <b>distributions.</b> At these, it is shown that all well-known rules choose the mode as the outcome. We provide a set of sufficient conditions for a rule to have this trait of choosing the mode under <b>unimodal</b> <b>distributions.</b> Further we show that Condorcet consistent rules, Borda rule, plurality rule are robust under tail-perturbations of <b>unimodal</b> <b>distributions...</b>|$|R
40|$|We show {{a direct}} {{relationship}} between the variance and the differential entropy for subclasses of symmetric and asymmetric <b>unimodal</b> <b>distributions</b> by providing an upper bound on variance in terms of entropy power. Combining this bound with the well-known entropy power lower bound on variance, we prove that the variance of the appropriate subclasses of <b>unimodal</b> <b>distributions</b> can be bounded below and above by the scaled entropy power. As differential entropy decreases, the variance is sandwiched between two exponentially decreasing functions in the differential entropy. This establishes that for the subclasses of <b>unimodal</b> <b>distributions,</b> the differential entropy {{can be used as}} a surrogate for concentration of the distribution. Comment: 19 pages, 3 figures, To appear in IEEE Transactions on Information Theor...|$|R
50|$|Histogram shape-based {{methods in}} particular, but also many other {{thresholding}} algorithms, make certain {{assumptions about the}} image intensity probability distribution. The most common thresholding methods work on bimodal distributions, but algorithms have also been developed for <b>unimodal</b> <b>distributions,</b> multimodal distributions, and circular distributions.|$|R
50|$|Baker {{proposed}} a transformation to convert a bimodal to a <b>unimodal</b> <b>distribution.</b>|$|E
5000|$|For a <b>unimodal</b> <b>distribution</b> the {{following}} bounds are known and are sharp: ...|$|E
50|$|The middle bound {{limits the}} nonparametric skew of a <b>unimodal</b> <b>distribution</b> to {{approximately}} ±0.775.|$|E
40|$|We give simple proofs of two results about {{convolutions}} of <b>unimodal</b> <b>distributions.</b> The {{first of}} these results states that the convolution of two symmetric <b>unimodal</b> <b>distributions</b> on is <b>unimodal.</b> The other result states that symmetrization of a unimodal random variable gives a symmetric unimodal random variable. Both our proofs avoid Khintchine's representation of a random variable that is unimodal about zero, and use the integral representation of the expectation of a non-negative random variable with its tail probability as the integrand. Convolution Khintchine's representation Symmetry Unimodality...|$|R
30|$|The verify {{level control}} {{criteria}} {{and the proposed}} formulation of optimization problems can be extended to other emerging memories such as PCM which are modeled by <b>unimodal</b> <b>distributions.</b>|$|R
2500|$|For <b>unimodal</b> <b>distributions,</b> {{the mode}} is within [...] {{standard}} deviations of the mean, and the {{root mean square deviation}} about the mode is between the standard deviation and twice the standard deviation.|$|R
5000|$|... #Caption: Figure 1. {{probability}} density function of normal distributions, {{an example of}} <b>unimodal</b> <b>distribution.</b>|$|E
5000|$|In 1823 Gauss {{showed that}} for a <b>unimodal</b> <b>distribution</b> with a mode of zero ...|$|E
5000|$|Rohatgi and Szekely {{have shown}} that the {{skewness}} and kurtosis of a <b>unimodal</b> <b>distribution</b> are related by the inequality: ...|$|E
5000|$|A bimodal {{distribution}} most commonly arises as {{a mixture of}} two different <b>unimodal</b> <b>distributions</b> (i.e. distributions having only one mode). In other words, the bimodally distributed random variable X is defined as [...] with probability [...] or [...] with probability [...] where Y and Z are unimodal random variables and [...] is a mixture coefficient.|$|R
3000|$|...]). The uniform {{distribution}} is, among the <b>unimodal</b> <b>distributions</b> on a bounded support, {{the one that}} maximizes the variance (Gray and Odell 1967). Hence, uniform delays are a very challenging setting for source localization.|$|R
25|$|In {{symmetric}} <b>unimodal</b> <b>distributions,</b> such as {{the normal}} distribution, the mean (if defined), median and mode all coincide. For samples, if {{it is known that}} they are drawn from a symmetric distribution, the sample mean can be used as an estimate of the population mode.|$|R
5000|$|Let X be {{a random}} {{variable}} with <b>unimodal</b> <b>distribution,</b> mean &mu; and finite, non-zero variance &sigma;2. Then, for any &lambda; > &radic;(8/3) = 1.63299…, ...|$|E
5000|$|It can {{be shown}} for a <b>unimodal</b> <b>distribution</b> that the median [...] and the mean [...] lie within (3/5)1/2 ≈ 0.7746 {{standard}} deviations of each other. In symbols, ...|$|E
50|$|Choosing {{the narrowest}} interval, which for a <b>unimodal</b> <b>distribution</b> will involve {{choosing}} those values of highest probability density including the mode. This {{is sometimes called}} the highest posterior density interval.|$|E
40|$|The program generates {{random numbers}} from a bimodal <b>distribution.</b> The two <b>unimodal</b> <b>distributions</b> {{that make up}} the bimodal can be normal or skewed-normal (see sknor for more details). Different {{arguments}} can be inputted to the function, as specified by 'option'. bimodal distribution, normal distribution, skewed, skew-normal...|$|R
40|$|Spatial {{distribution}} {{is an important}} factor determining the intensity and outcome of plant competition. The commonly used measure of angular distributions of plants around a target plant (1 - r) is shown to be limited to <b>unimodal</b> <b>distributions.</b> We present a new index which is based on the variance {{of the differences between the}} azimuth of neighboring plants. The new index is an improvement as it characterizes the angular dispersion in both <b>unimodal</b> and multimodal <b>distributions...</b>|$|R
25|$|For many distributions, the {{standard}} deviation is not a particularly natural way of quantifying the structure. For example, uncertainty relations in {{which one of the}} observables is an angle has little physical meaning for fluctuations larger than one period. Other examples include highly bimodal <b>distributions,</b> or <b>unimodal</b> <b>distributions</b> with divergent variance.|$|R
