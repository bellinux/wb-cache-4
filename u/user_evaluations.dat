352|2243|Public
2500|$|On 6 June 2013, the House Armed Services Committee {{passed an}} {{amendment}} to the 2014 budget that would prevent the Army from cancelling the IC program before <b>user</b> <b>evaluations.</b> [...] Committee members voted unanimously for the amendment that would require <b>user</b> <b>evaluations,</b> a business case analysis, and reports back to congressional defense committees before a final decision is made. [...] If passed into law, it would not take effect until 1 October 2013, which gave the Army four months to decide the fate of the program without violating a congressional directive.|$|E
5000|$|Goodhue, Dale L. [...] "Understanding <b>user</b> <b>evaluations</b> of {{information}} systems." [...] Management science 41.12 (1995): 1827-1844.|$|E
5000|$|Goodhue {{research}} interests {{are in the}} field of [...] "Data management in large organizations, Task-Technology Fit, <b>User</b> <b>evaluations,</b> and IS success, Management of IS" ...|$|E
40|$|<b>User</b> <b>evaluation</b> is {{generally}} performed {{early in the}} development process to reveal usability problems, design flaws, and errors and correct them before deploying. Due to the short iterations of agile development, implementing <b>user</b> <b>evaluation</b> {{as part of the}} development process is a challenge that is often neglected. In a previous work, we proposed an approach that would enable the integration of <b>user</b> <b>evaluation</b> throughout the development process, by managing and automating <b>user</b> <b>evaluation</b> activities from with the integrated development environment (IDE). In this work, we focus on a case study in which small-sized agile software teams, made up of students in an annual software engineering project course, applied our integrated <b>user</b> <b>evaluation</b> approach for developing their software projects. The feedbacks from these agile teams show the intuitiveness and effectiveness of our integration approach...|$|R
40|$|A {{software}} system {{needs to be}} maintained through continuous enhancements and improvements. To understand the types of enhancements to be made, a <b>user</b> <b>evaluation</b> exercise is often conducted to determine the system’s weaknesses and limitations. This paper reports on the outcomes of a <b>user</b> <b>evaluation</b> survey carried out on an electronic Malaysian Sign Language Dictionary, e-Sign Dictionary. The evaluation was conducted using a questionnaire survey focusing on the functions and features of e-Sign Dictionary. A total of 45 respondents comprising deaf school teacher, deaf students, {{and the general public}} participated in the <b>user</b> <b>evaluation.</b> The results show that out of 45 respondents, 36 (81 %) respondents rated e-Sign Dictionary as a good or very good system. The results of the <b>user</b> <b>evaluation</b> would be used as guidelines to enhance and improve the functions and features of e-Sign Dictionary. </p...|$|R
40|$|We {{report on}} two new portals for searching MEDLINE/PubMed with {{handheld}} devices, PICO (Patient, Intervention, Comparison, Outcome) and a WAP (Wireless Application Protocol) browser interface. Early <b>user</b> <b>evaluation</b> and <b>user</b> feedback will be discussed. We also include an updated report of <b>user</b> <b>evaluation</b> of established search tools for handheld devices {{included in the}} first release...|$|R
50|$|Its fielding will {{be delayed}} until summer after the {{completion}} of another round of <b>user</b> <b>evaluations</b> ordered by MARCORSYSCOM. The Marine Corps plans on fielding 108,000.|$|E
50|$|On 6 June 2013, the House Armed Services Committee {{passed an}} {{amendment}} to the 2014 budget that would prevent the Army from cancelling the IC program before <b>user</b> <b>evaluations.</b> Committee members voted unanimously for the amendment that would require <b>user</b> <b>evaluations,</b> a business case analysis, and reports back to congressional defense committees before a final decision is made. If passed into law, it would not take effect until 1 October 2013, which gave the Army four months to decide the fate of the program without violating a congressional directive.|$|E
50|$|The new and {{improved}} Clippard Park has drawn increased patrons to enjoy the many diverse facilities at the park. <b>User</b> <b>evaluations</b> have all rated the park and its amenities as either highly satisfied or extremely satisfied. Instrument Skate Board, a professional skate team, has indicated that the skate park is the best skate park for both vertical and street skaters in the region.|$|E
40|$|This paper {{presents}} an evaluation methodology {{to reveal the}} relationships between the attributes of software products, practices applied during the development phase and the <b>user</b> <b>evaluation</b> of the products. For the case study, the games sector has been chosen due to easy access to the <b>user</b> <b>evaluation</b> of this type of software products. Product attributes and practices applied during the development phase have been collected from the developers via questionnaires. <b>User</b> <b>evaluation</b> results were collected from a group of independent evaluators. Two bipartite networks were created using the gathered data. The first network maps software products to the practices applied during the development phase and the second network maps the products to the product attributes. According to the links, similarities were determined and subgroups of products were obtained according to selected development phase practices. By this way, the effect of development phase on the <b>user</b> <b>evaluation</b> has been investigated...|$|R
40|$|This paper {{describes}} the findings {{for an international}} user study investigating cultural applicability of <b>user</b> <b>evaluation</b> methods. The case study evaluates cultural differences in understanding of a virtual campus website across four culturally different user groups by using the same methods for each group. Findings suggest that some <b>user</b> <b>evaluation</b> methods are less applicable than others are for a culturally diverse user base...|$|R
40|$|There {{are many}} <b>user</b> {{experience}} <b>evaluation</b> methods {{available in the}} literature. Four different <b>user</b> experience <b>evaluation</b> methods were reviewed. However, most <b>user</b> experience <b>evaluation</b> methods measured different dimensions of user experience. Therefore, Norman's emotional design is proposed as a common conceptual framework for user experience. <b>User</b> experience <b>evaluation</b> method should measure emotional response related to visceral, behavioural and reflective. Thus, a user experience testing {{is designed to provide}} holistic view of a product. Other challenges faced in <b>user</b> experience <b>evaluation</b> methods are also discussed in detail...|$|R
50|$|The {{interest}} in user experience also grew in {{importance in the}} late 1990s because of the overload of products and services in the information society that were difficult to understand and hard to use. A strong call emerged to design things from a user's point of view. Ambient intelligence is influenced by user-centered design where the user {{is placed in the}} center of the design activity and asked to give feedback through specific <b>user</b> <b>evaluations</b> and tests to improve the design or even co-create the design together with the designer (participatory design) or with other users (end-user development).|$|E
50|$|In 2010 Xavier Serra {{was awarded}} an Advanced Grant of the European Research Council {{to carry out}} the project CompMusic (Computational models for the {{discovery}} of the world's music). The main goal of CompMusic is to advance in the field of Music Computing by approaching a number of the current research challenges from a multicultural perspective. It aims to advance in the description and formalization of music, making it more accessible to computational approaches and reducing the gap between audio signal descriptions and semantically meaningful music concepts. It intends to develop information modelling techniques applicable to non-western music repertories and formulate computational models to represent culture specific music contexts. CompMusic approaches these research challenges by (1) combining methodologies from disciplines such as Information Processing, Computational Musicology, Music Cognition and Human-Computer Interaction, and (2) analyzing a variety of music information sources such as audio features, symbolic scores, text commentaries, <b>user</b> <b>evaluations,</b> etc. from some of the major non-western art-music traditions in North-India (Hindustani) South-India (Carnatic), Turkey (Ottoman), Maghreb (Andalusian) and China (Beijing Opera). CompMusic wants to challenge the current western centered information paradigms, advance our Information Technologies research, and contribute to our rich multicultural society.|$|E
5000|$|CompMusic (Computational {{models for}} the {{discovery}} of the world's music): CompMusic is a research project funded by the European Research Council. The main goal of CompMusic is to advance in the field of Music Computing by approaching a number of the current research challenges from a multicultural perspective. It aims to advance in the description and formalization of music, making it more accessible to computational approaches and reducing the gap between audio signal descriptions and semantically meaningful music concepts. It intends to develop information modelling techniques applicable to non-western music repertories and formulate computational models to represent culture specific music contexts. CompMusic approaches these research challenges by (1) combining methodologies from disciplines such as Information Processing, Computational Musicology, Music Cognition and Human-Computer Interaction, and (2) analyzing a variety of music information sources such as audio features, symbolic scores, text commentaries, <b>user</b> <b>evaluations,</b> etc.… from some of the major non-western art-music traditions in India (Hindustani, Carnatic), Turkey (Ottoman), Arab countries (Andalusian) and China (Beijing Opera). CompMusic wants to challenge the current western centered information paradigms, advance our Information Technologies research, and contribute to our rich multicultural society.|$|E
40|$|Abstract—We {{formulate}} a novel problem of summarising entities with limited presentation budget on entity-relationship knowledge graphs and propose an efficient algorithm for solving this problem. The algorithm has been implemented {{together with a}} visualising tool. Experimental <b>user</b> <b>evaluation</b> of the algorithm wasconductedonreallarge semanticknowledgegraphsextracted from the web. The reported results of experimental <b>user</b> <b>evaluation</b> are promising and encourage to continue the work on improving the algorithm. I...|$|R
40|$|Abstract- This paper {{describes}} a touch-based PDA interface for mobile robot teleoperation and the objective <b>user</b> <b>evaluation</b> results. The interface {{is composed of}} three screens; the Vision-only screen, the Sensor-only screen, and the Vision with sensory overlay screen. The Vision-only screen provides the robot’s camera image. The Sensor-only screen provides the ultrasonic and laser range finder sensory information. The Vision with sensory overlay screen provides the image and the sensory information in concert. A <b>user</b> <b>evaluation</b> was conducted. Thirty-novice users drove a mobile robot using the interface. Participants completed three tasks, one with each screen. The {{purpose of this paper}} is to present the <b>user</b> <b>evaluation</b> results related to the collected objective data...|$|R
3000|$|Supplementary {{material}} {{consisting of}} the subjective <b>user</b> <b>evaluation</b> results can be downloaded from the following link: [URL] [...]...|$|R
40|$|In this paper, we {{identify}} trends about, benefits from, and barriers to performing <b>user</b> <b>evaluations</b> in software engineering research. From a corpus of over 3, 000 papers spanning ten years, {{we report on}} various subtypes of <b>user</b> <b>evaluations</b> (e. g., coding tasks vs. questionnaires) and relate <b>user</b> <b>evaluations</b> to paper topics (e. g., debugging vs. technology transfer). We identify the external measures of impact, such as best paper awards and citation counts, that are correlated {{with the presence of}} <b>user</b> <b>evaluations.</b> We complement this with a survey of over 100 researchers from over 40 different universities and labs in which {{we identify}} a set of perceived barriers to performing <b>user</b> <b>evaluations.</b> Categories and Subject Descriptors H. 1. 2 [Informatio...|$|E
40|$|<b>User</b> <b>evaluations</b> of {{information}} systems are frequently used as measures of MIS success, since {{it is extremely difficult}} to get objective measures of system performance. However, <b>user</b> <b>evaluations</b> have been appropriately criticized as lacking a clearly articulated theoretical basis for linking them to systems effectiveness, and almost no research has been found that explicitly tests the link between <b>user</b> <b>evaluations</b> of systems and objectively measured performance. In this paper, we focus on <b>user</b> <b>evaluations</b> of task-technology fit for mandatory use systems and develop theoretical arguments for the link to individual performance. This is then empirically tested in a controlled experiment with objective performance measures and carefully validated <b>user</b> <b>evaluations.</b> Statistically significant support for the link is found for one measure of performance but not for a second. These findings are consistent with others which found that users are not necessarily accurate reporters of key constructs related to use of IS, specifically that self reporting is a poor measure of actual utilization. The possibility that <b>user</b> <b>evaluations</b> have a stronger link to performance when users receive feedback on their performance is proposed. Implication...|$|E
40|$|Active {{interactive}} {{genetic algorithms}} (aiGAs) rely on actively optimizing synthetic fitness functions. In interactive genetic algorithms (iGAs) framework, <b>user</b> <b>evaluations</b> {{provide the necessary}} input for synthesizing a reasonably accurate surrogate fitness function that models <b>user</b> <b>evaluations</b> or, in other words, his/her decision preferences. <b>User</b> <b>evaluations</b> collected via tournament selection only provide partial-ordering relations between solutions. Active iGAs assemble a partial-ordering graph of <b>user</b> <b>evaluations.</b> In such a directed graph, any contradictory evaluation provided by the user introduces a cycle in the graph. This property is explored in this paper to measure {{the consistency of the}} evaluations provided by the user along the evolutionary process. The consistency measures are applied to a real-world problem, the weight tuning of the cost function involved in corpus-based text-to-speech synthesis. Results show the usefulness of such measures to identify inconsistent users during the evolutionary tuning process, and successfully the number of evaluations required by more than half. ...|$|E
30|$|Three <b>user</b> <b>evaluation</b> {{studies and}} a {{demonstration}} of offering users with cost-benefit analyses of application deployments on multi-cloud setups.|$|R
50|$|Two {{production}} grade MAAREECH {{systems have}} been developed and <b>user</b> <b>evaluation</b> trials completed on board two Indian Navy ships.|$|R
30|$|A <b>user</b> <b>evaluation</b> was {{designed}} and executed to measure the effectiveness and performance of the AzAA portal in assisting with authorship analysis. Specifically, <b>user</b> <b>evaluation</b> {{participants were asked to}} complete a series of tasks involving use of portal text visualizations. Evaluation results demonstrate that the visualizations support greater task efficiency and accuracy for task performance. The AzAA portal demonstrates potential to possibly save analysts’ time while also enhancing understanding of online messages related to potential terrorist activities.|$|R
30|$|User {{responses}} directory {{includes the}} <b>user</b> <b>evaluations</b> of twelve subjects {{and the mean}} subjective responses.|$|E
40|$|Organizations spend {{millions}} of dollars on information systems to improve organizational or individual performance, but objective measures of system success are extremely difficult to achieve. For this reason, many MIS researchers (and potentially MIS practitioners) rely on <b>user</b> <b>evaluations</b> of systems as a surrogate for MIS success. However, these measures have been strongly criticized as lacking strong theoretical underpinnings. Furthermore, empirical evidence of their efficacy is surprisingly weak. Part of the explanation for the theoretical and empirical problems with <b>user</b> <b>evaluations</b> is that they are really a measurement technique rather than a single theoretical construct. <b>User</b> <b>evaluations</b> are elicited beliefs or attitudes about something, and they have been used to measure a variety of different "somethings. " What is needed for <b>user</b> <b>evaluations</b> to be an effective measure of IS success is the identification of some specific user evaluation construct, defined within a theoretical perspective that can usefully link underlying systems to their relevant impacts. We propose task-technology fit (TTF) as such a user evaluation construct. The TTF perspective views technology as a means by which a goal-directed individual performs tasks. TTF focuses on the degree to which systems characteristics match user task needs. We posit that higher task-technology fit will result in better performance. Further, we posit that users can successfully evaluate task-technology fit. This latter proposition is strongly supported in a survey of 259 users in 9 companies. <b>user</b> <b>evaluations</b> of IS, task-technology fit, measurement of MIS success...|$|E
40|$|<b>User</b> <b>evaluations</b> {{have gained}} {{increasing}} importance in visualization research {{over the past}} years, as in many cases these evaluations are {{the only way to}} support the claims made by visualization researchers. Unfortunately, recent literature reviews show that in comparison to algorithmic performance evaluations, the number of <b>user</b> <b>evaluations</b> is still very low. Reasons for this are the required amount of time to conduct such studies together with the difficulties involved in participant recruitment and result reporting. While it could be shown that the quality of evaluation results and the simplified participant recruitment of crowdsourcing platforms makes this technology a viable alternative to lab experiments when evaluating visualizations, the time for conducting and reporting such evaluations is still very high. In this paper, we propose a software system, which integrates the conduction, the analysis and the reporting of crowdsourced <b>user</b> <b>evaluations</b> directly into the scientific visualization development process. With the proposed system, researchers can conduct and analyze quantitative evaluations on a large scale through an evaluation-centric user interface with only a few mouse clicks. Thus, it becomes possible to perform iterative evaluations during algorithm design, which potentially leads to better results, as compared to the time consuming <b>user</b> <b>evaluations</b> traditionally conducted {{at the end of the}} design process. Furthermore, the system is built around a centralized database, which supports an easy reuse of old evaluation designs and the reproduction of old evaluations with new or additional stimuli, which are both driving challenges in scientific visualization research. We will describe the system's design and the considerations made during the design process, and demonstrate the system by conducting three <b>user</b> <b>evaluations,</b> all of which have been published before in the visualization literature...|$|E
40|$|The paper {{concerns}} a novel problem of summarising entities with limited presentation budget on entity-relationship knowledge graphs and propose an efficient algorithm for solving this problem. The algorithm has been implemented in two variants: undirected and directed, {{together with a}} visualisation tool. Experimental <b>user</b> <b>evaluation</b> of the algorithm was conducted on real large semantic knowledge graphs extracted from the web. The reported results of experimental <b>user</b> <b>evaluation</b> are promising and encourage to continue the work on improving the algorithm. ...|$|R
40|$|We {{present an}} {{analysis}} of four orientation tracking systems used for construction of environment maps. We discuss the analysis necessary to determine the robustness of tracking systems in general. Due to the difficulty inherent in collecting <b>user</b> <b>evaluation</b> data, we then propose a metric {{which can be used}} to obtain a relative estimate of these values. The proposed metric will still require a set of input videos with an associated distance to ground truth, but not an additional <b>user</b> <b>evaluation...</b>|$|R
40|$|Abstract. The paper {{concerns}} a novel problem of summarising entities with lim-ited presentation budget on entity-relationship knowledge graphs and propose an efficient algorithm for solving this problem. The algorithm has been implemented in two variants: undirected and directed, {{together with a}} visualisation tool. Ex-perimental <b>user</b> <b>evaluation</b> of the algorithm was conducted on real large semantic knowledge graphs extracted from the web. The reported results of experimental <b>user</b> <b>evaluation</b> are promising and encourage to continue the work on improving the algorithm. ...|$|R
40|$|We use task-technology-fit {{model to}} develop an {{instrument}} for obtaining <b>user</b> <b>evaluations</b> of IT in health care. Our results indicate that task-technology fit along with individual characteristics {{have an impact on}} user evaluation of the IT in health care. We modify the original task-technology fit model for adequate domain coverage in health care industry. The contribution of our study is towards development and testing of an instrument for measuring <b>user</b> <b>evaluations</b> of IT in health care. The proposed model is successfully tested using a survey of 156 respondents from the eastern United States. Copyright Idea Group Inc...|$|E
30|$|Although AR {{has been}} studied for over 40 years, {{only in the last}} decade it began to be {{formally}} evaluated [23, 24, 68]. One of the reasons why it took so long to have <b>user</b> <b>evaluations</b> may be a lack of knowledge on how to properly evaluate AR experiences and design experiments [24]. Dünser et al. [24] claim that {{there seems to be a}} lack of understanding regarding the need of doing studies and the right motivation for carrying them. If <b>user</b> <b>evaluations</b> are conducted out of incorrect motivation or if empirical methods are not properly applied, the findings are of limited value or can even be misleading.|$|E
40|$|This chapter {{gives an}} {{overview}} of research that describes user experiences with different types of energy-efficient buildings, focusing on indoor climate, technical operation, user attitudes and general satisfaction. Energy-efficient buildings are often rated better than conventional buildings on indoor climate, but on digging deeper, users have different concerns. The varying results from the <b>user</b> <b>evaluations</b> reflect {{that the quality of}} buildings differs. However, the complaints may also be a result of inappropriate use. The main aim of this chapter is to give guidelines for further research, based on existing <b>user</b> <b>evaluations</b> of energy-efficient buildings. Three important areas for further research on <b>user</b> <b>evaluations</b> could be identified. First, {{there is a shortage of}} research that takes into account the social context for evaluation; the social environment, the process of moving into an energy-efficient building and prior knowledge of environmental issues influence evaluation of the buildings. Energy-efficient buildings may also require specific architectural solutions and further research should consider architectural and aesthetic aspects in the evaluation. Research on the use and operation of energy-efficient buildings is increasing, but there is still a need to give more detailed attention to different ways of providing information and training in operation and use...|$|E
40|$|The {{objective}} of this thesis is to obtain knowledge regarding how effective <b>user</b> centred <b>evaluation</b> methods are and how <b>user</b> centred <b>evaluations</b> are conducted by IT professionals. This will be achieved by exploring <b>user</b> centred <b>evaluation</b> in experimental and practical settings. The knowledge gained in these studies should inspire suggestions for further research and suggestions for improvements on the <b>user</b> centred <b>evaluation</b> activity. Two experimental studies were conducted. One compares the results from using three <b>user</b> centred <b>evaluation</b> methods, and the other examines two factors while conducting heuristic evaluation. The {{results show that the}} think-aloud evaluation method was the most effective method in finding realistic usability problems of the three methods. The number of critical problems found during think-aloud evaluation increases, if heuristic evaluation is conducted prior to the think-aloud evaluations. Further, two studies of <b>user</b> centred <b>evaluation</b> in practical setting...|$|R
3000|$|... refine {{and improve}} the {{recommendation}} approach considering people with common interests; <b>users</b> <b>evaluation</b> of BROAD-RSI recommendations; evaluation of users’ interaction in the BROAD-RSI through their logs.|$|R
40|$|Parallel Programs is Hard Parallel {{programming}} is very difficult. For example, {{in one study}} comparing different parallel programming models, a large subset (more than 1 / 3) {{of the participants in}} each of the three groups under comparison did not successfully implement a correct solution that exhibited any speedup [1]. Other studies have shown that subjective impressions of the difficulty of a particular implementation strategy are not necessarily correlated with code correctness [6], that there is substantial variation between programmers [5], and that it is difficult to reconciling comparisons made along different criteria [7]. <b>User</b> <b>Evaluation</b> is Valuable Usability is a key factor in the effectiveness of a parallel programming system [2, 8, 11, 12]. Theoretical performance gains can only be realized if programmers are able to successfully reason about their parallel code. Proposed Contributions Lower barrier to conducting <b>user</b> <b>evaluation</b> Diversifed methods of <b>user</b> <b>evaluation</b> Refined theories of parallel program maintenance Improved representations for concurrency testin...|$|R
