1150|432|Public
25|$|The {{method of}} Lagrange {{multipliers}} {{can be used}} to reduce optimization problems with constraints to <b>unconstrained</b> <b>optimization</b> problems.|$|E
25|$|In {{order to}} solve this problem with a {{numerical}} optimization technique, we must first transform this problem such that the critical points occur at local minima. This is done by computing the magnitude of the gradient of the <b>unconstrained</b> <b>optimization</b> problem.|$|E
2500|$|Using Lagrange multipliers, {{this problem}} can be {{converted}} into an <b>unconstrained</b> <b>optimization</b> problem: ...|$|E
40|$|In {{a recent}} Letter, Holas {{introduced}} nth-order generalizations of the McWeeny canonical purification scheme, hoping that these {{might be useful}} in linear-scaling electronic structure theory methods that rely on purification of density matrices. In this comment we show that for theories that use purification to guide <b>unconstrained</b> <b>optimizations,</b> the generalizations offer no advantage. ...|$|R
30|$|Global {{optimization}} {{has been}} extensively applied in various science and engineering fields. <b>Unconstrained</b> global <b>optimization</b> is important in optimization. Thus, numerous studies on global optimization have been conducted using various strategies to achieve <b>unconstrained</b> global <b>optimization</b> (Deep et al. 2009; Fan and Yan 2015; Gwizdałła 2012). However, serious challenges in global optimization remain, such as non-linear, non-convex, and non-differential problems.|$|R
50|$|In {{numerical}} optimization, the Broyden-Fletcher-Goldfarb-Shanno (BFGS) {{algorithm is}} an iterative method for solving <b>unconstrained</b> nonlinear <b>optimization</b> problems.|$|R
2500|$|J. E. Dennis, Jr. and Robert B. Schnabel, A view of <b>unconstrained</b> <b>optimization</b> (pp.1–72); ...|$|E
5000|$|UOBYQA (<b>Unconstrained</b> <b>Optimization</b> BY Quadratic Approximation) ...|$|E
5000|$|JOVE - a {{sequential}} <b>unconstrained</b> <b>optimization</b> technique applying Newton's second-order gradient search; ...|$|E
40|$|The present book {{explains}} the applied nonlinear programming, which has wide spread scientific and industrial applications. Salient features One variable <b>optimization</b> <b>Unconstrained</b> and constrained <b>optimization</b> Geometric programming Multi-variable optimization The {{book is a}} useful reference/text for practitioners/students...|$|R
50|$|Quadratic <b>unconstrained</b> binary <b>optimization</b> (QUBO) is {{a pattern}} {{matching}} technique, common in machine learning applications. QUBO is an NP hard problem.|$|R
5000|$|... 1QBit's {{software}} reformulates optimization problems {{into the}} quadratic <b>unconstrained</b> binary <b>optimization</b> (QUBO) format necessary to compute with D-Wave's quantum annealing processors.|$|R
5000|$|NEWUOA (A highly {{efficient}} algorithm that solves <b>unconstrained</b> <b>optimization</b> problems without using derivatives) ...|$|E
5000|$|Using Lagrange multipliers, {{this problem}} can be {{converted}} into an <b>unconstrained</b> <b>optimization</b> problem: ...|$|E
5000|$|ZEUS - a {{sequential}} <b>unconstrained</b> <b>optimization</b> technique {{applying a}} Davidon-Fletcher-Powell (DFP) variable-metric search.|$|E
25|$|Black-Litterman model {{optimization}} is {{an extension}} of <b>unconstrained</b> Markowitz <b>optimization</b> that incorporates relative and absolute 'views' on inputs of risk and returns.|$|R
40|$|Our {{interest}} lies {{in solving}} large-scale <b>unconstrained</b> polynomial <b>optimization</b> problems. Because interior-point methods for solving {{these problems are}} severely limited by the large-scale, we are motivated to explore efficient implementations of an accelerated first-order method to solve this class of problems. By exploiting special structural properties of this problem class, we greatly reduce the computational cost of the first-order method at each iteration. We report promising computational results {{as well as a}} curious observationabout the behavior of the first-order method for <b>unconstrained</b> polynomial <b>optimization...</b>|$|R
5000|$|... {{solution}} of continuous <b>unconstrained</b> or constrained <b>optimization.</b>|$|R
50|$|The {{method of}} Lagrange {{multipliers}} {{can be used}} to reduce optimization problems with constraints to <b>unconstrained</b> <b>optimization</b> problems.|$|E
5000|$|... 2000 - Anthony Chen {{formulated}} {{the traffic}} equilibrium problem as an <b>unconstrained</b> <b>optimization</b> {{problem that is}} equivalent to the nonlinear complementarity problem.|$|E
50|$|The {{conjugate}} gradient method {{can also be}} used to solve <b>unconstrained</b> <b>optimization</b> problems such as energy minimization. It was mainly developed by Magnus Hestenes and Eduard Stiefel.|$|E
40|$|AbstractEffective <b>optimization</b> of <b>unconstrained</b> {{building}} <b>optimization</b> problem involves coupling {{a building}} energy simulation program with an optimization evolutionary algorithm {{such as the}} genetic algorithm (GA). The aim {{of this paper is}} to find the most appropriate GA set that obtains the optimum, or near optimum, solutions in a reasonable computational time (less numbers of simulations). Twelve control parameter sets of binary encoded GA are tested to solve <b>unconstrained</b> building <b>optimization</b> problems that are coupled with EnergyPlus simulation program. The results show that population size is the most significant control parameter and that the crossover probability and mutation rate have insignificant effects on the GA performance. In general, a binary encoded GA with small population sizes can be used to solve <b>unconstrained</b> building <b>optimization</b> problems by around 250 building simulation calls. In particular, the smaller population size of about 5 individuals helps reach the optimum solution faster than larger population sizes...|$|R
40|$|Abstract. While the <b>unconstrained</b> {{portfolio}} <b>optimization</b> {{problem can}} be solved efficiently by standard algorithms, {{this is not the}} case for the portfolio optimization problem with additional real world constraints like cardinality constraints, buy-in thresholds, roundlots etc. In this paper we investigate two extensions to Evolutionary Algorithms (EA) applied to the portfolio optimization problem. First, we introduce a problem specific EA representation and then we add a local search for feasible solutions to improve the performance of the EA. All algorithms are compared on the constrained and <b>unconstrained</b> portfolio <b>optimization</b> problem...|$|R
40|$|In {{this paper}} we {{address the problem of}} using quaternions in <b>unconstrained</b> {{nonlinear}} <b>optimization</b> of 3 -D rotations. Quaternions representing rotations have four elements but only three degrees of freedom, since they must be of norm one. This constraint has {{to be taken into account}} when applying e. g. the Levenberg-Marquardt algorithm, a method for <b>unconstrained</b> nonlinear <b>optimization</b> widely used in computer vision. We propose an easy to use method for achieving this. Experiments using our parametrization in photogrammetric bundle-adjustment are presented at the end of the paper. ...|$|R
50|$|UOBYQA (<b>Unconstrained</b> <b>Optimization</b> BY Quadratic Approximation) is a {{numerical}} optimization algorithm by Michael J. D. Powell. It {{is also the}} name of Powell's Fortran 77 implementation of the algorithm.|$|E
5000|$|NEWUOA {{algorithm}} {{was developed}} from UOBYQA (<b>Unconstrained</b> <b>Optimization</b> BY Quadratic Approximation). A {{major difference between}} them is that UOBYQA constructs quadratic models by interpolating the objective function at [...] points.|$|E
5000|$|NEWUOA {{software}} {{was released on}} December 16, 2004. It can solve <b>unconstrained</b> <b>optimization</b> problems of a few hundreds variables to high precision without using derivatives. In the software, [...] is set to [...] by default.|$|E
40|$|While the <b>unconstrained</b> {{portfolio}} <b>optimization</b> {{problem can}} be solved e#ciently by standard algorithms, {{this is not the}} case for the portfolio optimization problem with additional real world constraints like cardinality constraints, buy-in thresholds, roundlots etc. In this paper we investigate two extensions to Evolutionary Algorithms (EA) applied to the portfolio optimization problem. First, we introduce a problem specific EA representation and then we add a local search for feasible solutions to improve the performance of the EA. All algorithms are compared on the constrained and <b>unconstrained</b> portfolio <b>optimization</b> problem...|$|R
40|$|In {{this paper}} a new derivative-free method is {{developed}} for solving <b>unconstrained</b> nonsmooth <b>optimization</b> problems. This method {{is based on}} the notion of a discrete gradient. It is demonstrated that the discrete gradients can be used to approximate subgradients of a broad class of nonsmooth functions. It is also shown that the discrete gradients can be applied to find descent directions of nonsmooth functions. The preliminary results of numerical experiments with <b>unconstrained</b> nonsmooth <b>optimization</b> problems as well as the comparison of the proposed method with nonsmooth optimization solver DNLP from CONOPT-GAMS and derivative-free optimization solver CONDOR are presented...|$|R
40|$|In this paper, {{we find a}} non-dominated {{solution}} of a fuzzy maximum-return problem (<b>unconstrained</b> single-variable fuzzy <b>optimization</b> problem). We establish Newton method to find the {{solution of}} the <b>unconstrained</b> single-variable fuzzy <b>optimization</b> problem using the differentiability of α-level functions of a fuzzy-valued function and partial order relation on a set of fuzzy numbers. Comment: Communicated to Journa...|$|R
50|$|In {{order to}} solve this problem with a {{numerical}} optimization technique, we must first transform this problem such that the critical points occur at local minima. This is done by computing the magnitude of the gradient of the <b>unconstrained</b> <b>optimization</b> problem.|$|E
5000|$|NEWUOA solves <b>unconstrained</b> <b>optimization</b> {{problems}} {{without using}} derivatives, {{which makes it}} a derivative-free algorithm. The algorithm is iterative and exploits trust-region technique. On each iteration, the algorithm establishes a model function [...] by quadratic interpolation and then minimizes [...] within a trust region.|$|E
5000|$|Powell did {{not explain}} how he coined the name [...] "NEWUOA" [...] {{either in the}} {{introducing}} report nor in the software, although COBYLA, UOBYQA, BOBYQA and LINCOA are all named by acronyms. Probably [...] "NEWUOA" [...] means [...] "NEW <b>Unconstrained</b> <b>Optimization</b> Algorithm".|$|E
40|$|Two new {{complexes}} of Ti and Zr were synthesized with 3 -hydroxyflavone bidentate ligand and investi-gated in homogeneous ethylene polymerization. Both complexes {{were characterized}} by UV–VIS, 1 H and 13 C NMR, and electrochemical studies. The geometries and energies of all possible isomeric species werestudied by full <b>unconstrained</b> <b>optimizations</b> performed at Density Functional Theory (DFT) level. Thepolymerization reactions were performed at different experimental conditions with methylaluminox-ane (MAO), as the cocatalyst. Both complexes were active in ethylene polymerization in all the conditionsused. However, zirconium complex showed the best activity comparing to the titanium complex at 2500 Al/M ratio. The polymers obtained were high density polyethylene with ultra high molecular weights...|$|R
30|$|In this section, we {{establish}} two Lagrange multiplier theorems {{which show}} that tightly properly efficient {{solution of the}} constrained vector optimization problem (VP), is equivalent to tightly properly efficient solution of an appropriate <b>unconstrained</b> vector <b>optimization</b> problem.|$|R
40|$|Synthesis of antenna arrays {{subject to}} spatial and {{excitation}} constraints to yield arbitrarily prescribed patterns {{in both the}} mean-squared and minimax sense are discussed. The spatial constraints may require that the interelement spacings be greater than a prescribed value or that the element locations lie within a specified region. The excitation constraints are of the form where the current-taper ratio is constrained {{to be less than}} or equal to a prescribed value. The technique employed consists of reducing the constrained optimization problem into an unconstrained one by the use of simple transformations of the independent variables. In such cases where explicit transformations are not available, a created response surface technique (CRST) has been used to convert the constrained optimization problem into a series of <b>unconstrained</b> <b>optimizations.</b> The optimization has been carried out using a nonlinear simplex algorithm. Numerical examples are given wherein both the linear and circular arrays are synthesized subject to constraints...|$|R
