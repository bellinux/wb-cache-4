53|0|Public
5000|$|A {{disadvantage}} is {{the possible}} <b>under-coverage</b> {{that can be}} the case if the incentive or the cultural tradition of registering events and changes are weak, if the classification principles of the register are not clearly defined or if the classifications do not correspond {{to the needs of}} statistical production to be derived from them.|$|E
40|$|At first sight, web surveys {{seem to be}} an {{interesting}} and attractive means of data collection. They provide simple, cheap and fast access to a large group of people. However, web surveys also suffer from methodological problems. Outcomes of web surveys may be severally biased, particularly if self-selection of respondents is applied instead of proper probability sampling. <b>Under-coverage</b> is also a serious problem. This raises the question whether web surveys can be used for data collection in official statistics. This paper addresses the problems <b>under-coverage</b> and self-selection in web surveys, and attempts to describe how Internet data collection can be incorporated in normal data collection practices of official statistics...|$|E
30|$|The {{non-parametric}} Kruskal-Wallis and Mann-Withney {{tests have}} confirmed that the weight system guarantees the representativeness {{of the total population}} by augmenting the representativeness for the most elusive segment of the population (irregular migrants, irregular workers and recently arrived migrants) and solving potential problems of <b>under-coverage</b> for these categories.|$|E
40|$|To {{survey the}} general population, survey {{agencies}} often use sampling frames of landline numbers. However, these frames may exclude a relevant {{share of the}} target population. In 2012, 50 % of Italian adults are excluded from the sampling frame, as they are unlisted (UN) or do not own a landline telephone (NT). Using a unique national survey with full-coverage of the Italian population, our study describes {{the changes in the}} composition of landline telephone coverage over time and investigates differences in demographic and socio-economic characteristics between (i) respondents included in and excluded from the sampling frame and (ii) NT and UN respondents. It also explores whether these differences lead to <b>under-coverage</b> bias and evaluates the impact of current post-adjustment strategies on the total <b>under-coverage</b> bias as well as on its two components separately...|$|E
40|$|The aim of {{this study}} was to {{determine}} the extent to which selective <b>under-coverage</b> of births to mothers more likely to be at risk of HIV- 1 infection will result in a significant under-estimation of the true neonatal seroprevalence. Census data, local birth statistics, maternity data and data from the prevalence monitoring programme were used to produce a model to predict the effects of <b>under-coverage</b> in the uptake of neonatal metabolic screening which has been observed in babies with a mother of ethnic group black African. The adjustment factor which allows for <b>under-coverage</b> is the relative inclusion ratio (RIR); the probability that samples from a group at different risk of HIV infection were included in the survey divided by the probability of inclusion for samples from all other babies. The RIR was found to be close to unity (0. 97), indicating a minimal bias. Under usual conditions only if the relative inclusion ratio (RIR) declined to values of 0. 87 or below would there be a substantial bias. Despite some selective under representation, the results obtained from the Unlinked Anonymous HIV Monitoring Programme Dried Blood Spot Survey would seem to identify levels of prevalence in the population of child-bearing women with a good degree of accuracy and remains a useful tool for resource allocation, planning of services, provision of care and counselling...|$|E
40|$|At first sight, web surveys {{seem to be}} an {{interesting}} and attractive means of data collection. They provide simple, cheap, and fast access to a large group of potential respondents. However, web surveys are not without methodological problems. Specific groups in the populations are under-represented because they have less access to Internet. Furthermore, recruitment of respondents is often based on self-selection. Both <b>under-coverage</b> and self-selection may lead to biased estimates. This paper describes these methodological problems. It also explores the effect of various correction techniques (adjustment weighting and use of reference surveys). This all leads to the question whether properly design web surveys can be used for data collection. The paper attempts to answer this question. It concludes that <b>under-coverage</b> problems may solve itself in the future, but that self-selection leads to unreliable survey outcomes. Copyright (c) 2010 The Author. Journal compilation (c) 2010 International Statistical Institute. ...|$|E
40|$|Accelerated failure time (AFT) {{models are}} useful {{regression}} tools {{for studying the}} association between a survival time and covariates. Semiparametric inference procedures have been proposed in an extensive literature. Among these, use of an estimating equation which is monotone in the regression parameter and has some excellent properties was proposed by Fygenson and Ritov (1994). However, {{there is a serious}} <b>under-coverage</b> problem for small sample sizes. In this paper, we derive the limiting distribution of the empirical log-likelihood ratio for the regression parameter {{on the basis of the}} monotone estimating equations. Furthermore, the empirical likelihood (EL) confidence intervals/regions for the regression parameter are obtained. We conduct a simulation study in order to compare the proposed EL method with the normal approximation method. The simulation results suggest that the empirical likelihood based method outperforms the normal approximation based method in terms of coverage probability. Thus, the proposed EL method overcomes the <b>under-coverage</b> problem of the normal approximation method. Average length Confidence interval/region Coverage probability Monotone estimating equation Right censoring...|$|E
30|$|European countries, and {{especially}} Germany, are currently very much affected by human migration flows, {{with the result}} that the task of integration has become a challenge. Only very little empirical evidence on topics such as labor market participation and processes of social integration of migrant subpopulations is available to date from large-scale population surveys. The present paper provides an overview of the representation of the migrant population in the German Programme for the International Assessment of Adult Competencies (PIAAC) sample and evaluates reasons for the <b>under-coverage</b> of this population.|$|E
40|$|The {{sensitivity}} of multiple imputation methods to deviations from their distributional assumptions is investigated using simulations, where {{the parameters of}} scientific interest are the coefficients of a linear regression model, and values in predictor variables are missing at random. The performance of a newly proposed imputation method based on generalized additive models for location, scale, and shape (GAMLSS) is investigated. Although imputation methods based on predictive mean matching are virtually unbiased, they suffer from mild to moderate <b>under-coverage,</b> even in the experiment where all variables are jointly normal distributed. The GAMLSS method features better coverage than currently available methods...|$|E
40|$|This paper applies {{small area}} {{estimation}} techniques to Mozambican data to develop high resolution (subdistrictlevel) poverty and inequality maps [...] . The picture that emerges {{is one of}} considerable local-level economic heterogeneity, with the poor living alongside the nonpoor. Rather than finding stark pockets of intense poverty traps {{in one part of}} the country and a relative absence of poverty in other parts, the situation is much more nuanced. This suggests that targeting antipoverty efforts on purely geographic criteria is almost certain to be inefficient, with leakages to the nonpoor and <b>under-coverage</b> of the significant numbers of poor households in areas that are “less poor. ”" From TextInequality,Geographic targeting,Small area estimation,Poverty mapping,...|$|E
40|$|Background: Despite {{the fact}} that equity is the {{underlying}} principle of all major global health policies, difficulties have emerged in providing proper {{care for the poor}} with the introduction of user fees for health services. However, the criteria used to determine eligibility for free health services at public health facilities are either unclear or nonexistent in most sub-Saharan African countries. Objective: To assess the free health care delivery system {{and the extent to which}} strict criteria are followed in granting free health care services in Jimma town, southwest Ethiopia. Methods: A cross-sectional, exploratory study, employing both quantitative and qualitative study designs, was conducted from December 22 – 27, 2003. Results: Fifty-eight percent of the respondents were found to be patients exempted from fees on the day of interview. There exist no clearly stated criteria in the free health care provision system of Jimma town. The presence of leakage and <b>under-coverage</b> were 36. 9 % and 43. 6 % respectively. The occupation and income category of the respondents showed a statistically significant association with their service category at the public health facilities (p= 0. 000). Conclusion: The absence of clearly defined criteria for waiving user fees at public health facilities has made the free health care provision system difficult for both the providers and users. The system is also prone to the possibility of leakage and <b>under-coverage.</b> These findings imply the importance of a strict reconsideration of the exemption policy of the locality and the country with focus on efforts to produce clear criteria and guidelines in granting free health care...|$|E
40|$|We {{propose a}} general {{approach}} that jointly integrates {{horizontal and vertical}} equity criteria {{in the assessment of}} poverty alleviation programs, with the strength of each criterion being captured through its own inequity-aversion parameter. This contrasts with the assessment of poverty allevia-tion programs done with simple <b>under-coverage</b> and leakage ratios or with other methods that do {{not take into account the}} heterogeneity of the poor and that do not address directly the social benefits of achieving normative criteria. Our methodology is illustrated using Tunisian data and two alter-native poverty alleviation policies. We find inter alia that the social ranking of commodity and socio-demographic targeting in Tunisia depends on the policymaker’s comparative preference for vertical and horizontal equity...|$|E
40|$|We {{propose a}} general cost-of-inequality {{approach}} that jointly integrates {{horizontal and vertical}} equity criteria {{in the assessment of}} poverty alleviation programs, with the strength of each criterion being captured through its own inequity-aversion parameter. This contrasts with the assessment of poverty alleviation programs done with simple <b>under-coverage</b> and leakage ratios or with other methods that do {{not take into account the}} heterogeneity of the poor and that do not address directly the social benefits of achieving normative criteria. Our methodology is illustrated using Tunisian data and two alternative poverty alleviation policies. We find inter alia that the social ranking of commodity and socio-demographic targeting in Tunisia depends on the policymaker's comparative preference for vertical and horizontal equity. Poverty, Vertical Equity, Horizontal Inequity, Transfers, Targeting, Tunisia...|$|E
40|$|This paper {{introduces}} a new methodology to target direct transfers against poverty. Our method {{is based on}} estimation methods {{that focus on the}} poor. Using data from Tunisia, we estimate ‘focused ’ transfer schemes that highly improve anti-poverty targeting performances. Post-transfer poverty can be substantially reduced with the new estimation method. In terms of P 2, the most popular axiomatically valid poverty indicator, a 30 percent reduction in poverty from transfer schemes based on OLS method to focused transfer schemes, requires only a few hours of computer work based on methods available on popular statistical packages. Finally, the obtained levels of <b>under-coverage</b> of the poor is so low that reforms based on ‘proxy-means ’ focused transfer schemes are likely to avoid social unrest...|$|E
40|$|The U. S. Department of Agriculture’s National Agricultural Statistics Service (NASS) conducts the quinquennial U. S. Census of Agriculture, {{in years}} ending in 2 and 7. Beginning in 2009, NASS {{conducted}} {{a series of}} research projects that led {{to the conclusion that the}} assumptions underpinning the analysis of the 2007 Census were no longer valid. Consequently, NASS has adopted a unified approach to accounting for non-response, <b>under-coverage,</b> and misclassification using capture-recapture methodology. The two surveys used for capture-recapture are the Census and the June Area Survey (JAS). Challenges, such as resolving farm status when an operation is classified as a farm (non-farm) by the JAS and a non-farm (farm) by the Census, are discussed. Accounting for uncertainty using jackknife methods is presented...|$|E
30|$|Another {{important}} aim {{of large-scale}} assessments is to derive policy advice to better meet {{social and economic}} challenges. One such challenge is the heavy influx of refugees that the European Union faced in 2015 (OECD 2016). A solid database on immigrants, their social-demographic background, and the competencies that they bring with them is crucial for deriving strategies for their successful social and labor market integration. However, compiling such a database requires representatively surveying this population group in a large-scale assessment. Maehler et al. (2017) give {{an overview of the}} representation of the migrant population in the German PIAAC sample as a case study of a country that currently has a high migration rate. They evaluate reasons for the <b>under-coverage</b> of this population in terms of sampling frames, patterns in response behavior, and contact times.|$|E
40|$|This paper {{presents}} {{the findings of}} an evaluation of Chile's Progressive Housing Program (PHP), which is a public housing program that finances {{the purchase of a}} new house for lower income households. The evaluation finds that the programs' package (savings requirement, voucher and mortgage) design is inappropriate if the program is targeted to the poor. In fact the pro-poor targeting of the program was poor with high <b>under-coverage</b> and high leakage. Furthermore, the benefit of a minimum quality new house was not sustainable as many households slipped back into the housing shortage category overtime. This impact evaluation reveals that although the program had significant positive effects on materiality conditions (access to water, sewerage, and electricity), it had a negative effect on overcrowding, and had no discernible effects on welfare indicators (poverty, school attendance, occupation ratio) ...|$|E
40|$|AbstractRecent {{advances}} in the transformation model {{have made it possible}} to use this model for analyzing a variety of censored survival data. For inference on the regression parameters, there are semiparametric procedures based on the normal approximation. However, the accuracy of such procedures can be quite low when the censoring rate is heavy. In this paper, we apply an empirical likelihood ratio method and derive its limiting distribution via U-statistics. We obtain confidence regions for the regression parameters and compare the proposed method with the normal approximation based method in terms of coverage probability. The simulation results demonstrate that the proposed empirical likelihood method overcomes the <b>under-coverage</b> problem substantially and outperforms the normal approximation based method. The proposed method is illustrated with a real data example. Finally, our method can be applied to general U-statistic type estimating equations...|$|E
40|$|Indirect {{comparisons}} {{are becoming increasingly}} popular for evaluating medical treatments {{that have not been}} compared head-to-head in randomized clinical trials (RCTs). While indirect methods have grown in popularity and acceptance, {{little is known about the}} fragility of confidence interval estimations and hypothesis testing relying on this method. We present the findings of a simulation study that examined the fragility of indirect confidence interval estimation and hypothesis testing relying on the adjusted indirect method. Our results suggest that, for the settings considered in this study, indirect confidence interval estimation suffers from <b>under-coverage</b> while indirect hypothesis testing suffers from low power in the presence of moderate to large between-study heterogeneity. In addition, the risk of overestimation is large when the indirect comparison of interest relies on just one trial for one of the two direct comparisons. Indirect comparisons typically suffer from low power. The risk of imprecision is increased when {{comparisons are}} unbalanced...|$|E
40|$|Kendall and Gehan {{estimating}} {{functions are}} used to estimate the regression parameter in accelerated failure time (AFT) model with censored observations. The accelerated failure time model is the preferred survival analysis method because it maintains a consistent association between the covariate and the survival time. The jackknife empirical likelihood method is used because it overcomes computation difficulty by circumventing {{the construction of the}} nonlinear constraint. Jackknife empirical likelihood turns the statistic of interest into a sample mean based on jackknife pseudo-values. U-statistic approach is used to construct the confidence intervals for the regression parameter. We conduct a simulation study to compare the Wald-type procedure, the empirical likelihood, and the jackknife empirical likelihood in terms of coverage probability and average length of confidence intervals. Jackknife empirical likelihood method has a better performance and overcomes the <b>under-coverage</b> problem of the Wald-type method. A real data is also used to illustrate the proposed methods...|$|E
40|$|Recent {{advances}} in the transformation model {{have made it possible}} to use this model for analyzing a variety of censored survival data. For inference on the regression parameters, there are semiparametric procedures based on the normal approximation. However, the accuracy of such procedures can be quite low when the censoring rate is heavy. In this paper, we apply an empirical likelihood ratio method and derive its limiting distribution via U-statistics. We obtain confidence regions for the regression parameters and compare the proposed method with the normal approximation based method in terms of coverage probability. The simulation results demonstrate that the proposed empirical likelihood method overcomes the <b>under-coverage</b> problem substantially and outperforms the normal approximation based method. The proposed method is illustrated with a real data example. Finally, our method can be applied to general U-statistic type estimating equations. Kaplan-Meier estimator Martingale Proportional hazards model Proportional odds model Right censoring U-statistic...|$|E
40|$|Abstract: 	Multiple-frame {{surveys are}} {{commonly}} used to deal with <b>under-coverage</b> bias. The use {{of more than one}} frame introduces the possibility that frames overlap leading to increased inclusion probabilities of units that appear in multiple lists. Following the guide example of a dual frame set-up (DF) in telephone surveys, this contribution presents an extensive simulation study where different types of screenings to deal with the overlap issue are compared with the proper DF approach. We empirically show that the efforts of screening do not guarantee estimators more efficient than the DF estimators that do not need screening. Moreover simulation results show that screening at sample level does not correct for the increased inclusion probability of units in both frames produced by the DF set-up nor improve efficiency. The different estimation options will be compared with regards to survey costs, amount of information required and statistical properties of the final estimate...|$|E
40|$|Survey <b>under-coverage</b> of top incomes {{leads to}} bias in survey-based {{estimates}} of overall income inequality. Using income tax record data {{in combination with}} survey data is a potential approach to address the problem; we consider here the UK’s pioneering ‘SPI adjustment’ method that implements this idea. Since 1992, the principal income distribution series (reported annually in Households Below Average Income) {{has been based on}} household survey data in which the incomes of a small number of ‘very rich’ individuals are adjusted using information from ‘very rich’ individuals in personal income tax return data. We explain what the procedure involves, reveal {{the extent to which it}} addresses survey under- coverage of top incomes, and show how it affects estimates of overall income inequality. More generally, we assess whether the SPI adjustment is fit for purpose and consider whether variants of it could be employed by other countries. embarg...|$|E
40|$|Background: Indirect {{comparisons}} {{are becoming increasingly}} popular for evaluating medical treatments {{that have not been}} compared head-to-head in randomized clinical trials (RCTs). While indirect methods have grown in popularity and acceptance, {{little is known about the}} fragility of confidence interval estimations and hypothesis testing relying on this method. Methods: We present the findings of a simulation study that examined the fragility of indirect confidence interval estimation and hypothesis testing relying on the adjusted indirect method. Findings: Our results suggest that, for the settings considered in this study, indirect confidence interval estimation suffers from <b>under-coverage</b> while indirect hypothesis testing suffers from low power in the presence of moderate to large between-study heterogeneity. In addition, the risk of overestimation is large when the indirect comparison of interest relies on just one trial for one of the two direct comparisons. Interpretation: Indirect comparisons typically suffer from low power. The risk of imprecision is increased when {{comparisons are}} unbalanced...|$|E
40|$|The methods {{underpinning}} the UK’s annual structural earnings survey – the New Earnings Survey (NES) – {{have remained}} largely unchanged since the survey’s inception in 1970. A recent major review of earnings statistics in the UK, along with gradual {{changes in the}} labour market {{in recent years have}} led to a re-design of the NES. This paper describes some of the shortcomings of the NES and then presents the methodological work undertaken to redesign the survey. The methodological work covered several topics: better use of administrative data; development of the sampling design (which exploits a dual frame approach to reduce <b>under-coverage</b> of the survey); improved data collection procedures, including an intensive follow-up pilot; better procedures to analyse the characteristics of people who change jobs; and a weighting strategy to account for both non-selection and non-response. Experiences from pilot work are described along with plans for full implementation in 2004. Key words: distribution of earnings, dual frame survey, sampling design, weightin...|$|E
40|$|Researchers often utilise {{datasets}} {{that link}} information from multiple sources, but non-linkage biases caused by linked and non-linked subject differences are little understood, especially in business datasets. We address these knowledge gaps by studying biases in linkable 2010 UK Small Business Survey datasets. We identify correlates of business linkage propensity, {{and also for}} the first time its components: consent to linkage and register identifier appendability. As well, we take a novel approach to evaluating non-linkage bias risks, by computing dataset representativeness indicators (comparable, decomposable sample-subset similarity measures). We find that the main impacts on linkage propensities and bias risks are due to consenter / non-consenter differences explicable given business survey response processes, and differences between subjects with and without identifiers caused by register <b>under-coverage</b> of very small businesses. We then discuss consequences for the analysis of linked business datasets, and implications of the evaluation methods we introduce for linked dataset producers and users...|$|E
40|$|Empirical best linear {{unbiased}} prediction (EBLUP) method uses {{a linear}} mixed model in combining information from different sources of information. This method is particularly useful in small area problems. The variability of an EBLUP is traditionally {{measured by the}} mean squared prediction error (MSPE), and interval estimates are generally constructed using estimates of the MSPE. Such methods have shortcomings like <b>under-coverage</b> or over-coverage, excessive length and lack of interpretability. We propose a parametric bootstrap approach to estimate the entire distribution of a suitably centered and scaled EBLUP. The bootstrap histogram is highly accurate, and differs from the true EBLUP distribution by only $O(d^ 3 n^{- 3 / 2 }) $, where $d$ {{is the number of}} parameters and $n$ the number of observations. This result is used to obtain highly accurate prediction intervals. Simulation results demonstrate the superiority of this method over existing techniques of constructing prediction intervals in linear mixed models. Comment: Published in at [URL] the Annals of Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|E
40|$|In {{this note}} we {{consider}} coverage of confidence intervals calculated {{with and without}} systematic uncertainties. These calculations follow the prescription originally proposed by Cousins & Highland but here extended to account for different shapes, size and types of systematic uncertainties. Also two different ordering schemes are considered: the Feldman & Cousins ordering and its variant where conditioning on the background expectation is applied as proposed by Roe & Woodroofe. Without uncertainties Feldman & Cousins method over-covers as expected because of the discreteness of the Poisson distribution. For Roe & Woodroofe's method we find <b>under-coverage</b> for low signal expectations. When including uncertainties it becomes important to define the ensemble for which the coverage is determined. We consider two different ensembles, where in ensemble A all nuisance parameters are fixed and in ensemble B the nuisance parameters are varied according to their uncertainties. We also discuss the subtleties of {{the realization of the}} ensemble with varying systematic uncertainties. Comment: 6 pages, 3 figures. To appear in: Proceedings of Conference on Advanced Statisical Techniques in Particle Physics, Durham, 200...|$|E
40|$|We explore {{probability}} modelling of discretization uncertainty for {{system states}} defined implicitly by ordinary or partial differential equations. Accounting for this uncertainty can avoid posterior <b>under-coverage</b> when likelihoods are constructed from a coarsely discretized approximation to system equations. A formalism is proposed for inferring a fixed but a priori unknown model trajectory through Bayesian updating of a prior process conditional on model information. A one-step-ahead sampling scheme for interrogating {{the model is}} described, its consistency and first order convergence properties are proved, and its computational complexity is shown to be proportional to that of numerical explicit one-step solvers. Examples illustrate the flexibility of this framework {{to deal with a}} wide variety of complex and large-scale systems. Within the calibration problem, discretization uncertainty defines a layer in the Bayesian hierarchy, and a Markov chain Monte Carlo algorithm that targets this posterior distribution is presented. This formalism is used for inference on the JAK-STAT delay differential equation model of protein dynamics from indirectly observed measurements. The discussion outlines implications for the new field of probabilistic numerics...|$|E
40|$|Abstract Background European countries, and {{especially}} Germany, are currently very much affected by human migration flows, {{with the result}} that the task of integration has become a challenge. Only very little empirical evidence on topics such as labor market participation and processes of social integration of migrant subpopulations is available to date from large-scale population surveys. The present paper provides an overview of the representation of the migrant population in the German Programme for the International Assessment of Adult Competencies (PIAAC) sample and evaluates reasons for the <b>under-coverage</b> of this population. Methods We examine outcome rates and reasons for nonresponse among the migrant population based on sampling frame data, and we also examine para data from the interviewers’ contact protocols to evaluate time patterns for the successful contacting of migrants. Results and Conclusions This {{is the first time that}} results of this kind have been presented for a large-scale assessment in educational research. These results are also discussed in the context of future PIAAC cycles. Overall, they confirm the expectations in the literature that factors such as language problems result in lower contact and response rates among migrants...|$|E
40|$|This paper evaluates Progressive Housing Program; {{a public}} housing program that {{facilitates}} {{the purchase of}} a new home. The evaluation finds that the program’s package (savings requirement, voucher and mortgage) design is inappropriate if the program is targeted to the poor. In fact the pro-poor targeting of the program was poor with high <b>under-coverage</b> and high leakage. Further, the benefit, a minimum quality new house, was not sustainable as many households slipped back into the housing shortage category overtime. An impact evaluation reveals that although the program had significant positive effects on materiality conditions (access to water, sewerage, and electricity), it had a negative effect on overcrowding, and had no discernable effects on welfare indicators (poverty, school attendance, occupation ratio, etc.). This could be due to high residential segregation that resulted from attempting to maximize the number of housing solutions on the cheap. The study also cautions against the mechanical use of cost benefit calculations for policy decisions: the program’s internal rate of return was higher than the official cut off rate of 12 %. Housing Program, Poverty, Segregation, Impact Evaluation, Housing Shortage, Cost-Benefits Analysis...|$|E
40|$|Cell-free DNA (cfDNA) is short, extracellular, {{fragmented}} double-stranded DNA {{found in}} plasma. Plasma {{of patients with}} solid tumor {{has been found to}} show significantly increased quantities of cfDNA. Although currently poorly understood, the mechanism of cfDNA generation is speculated to be a product of genomic DNA fragmentation during cellular apoptosis and necrosis. Sequencing of cfDNA with tumor origin has identified tumor biomarkers, elucidating molecular pathology and assisting in accurate diagnosis. In this study, we performed whole-genome sequencing ofcfDNA samples with matching tumor and whole blood samples from five patients diagnosed with stage IV gastric or lung cancer. We analyzed the coverage spectrum of the human genome in our cfDNA samples. cfDNA exhibited no large regions with significant <b>under-coverage,</b> although we observed unbalanced coverage depth in cfDNA at transcription start sites and exon boundaries as a consequence of biased fragmentation due to ordered nucleosome positioning. We also analyzed the copy number variant status based on the whole-genome sequencing results and found high similarity between copy number profile constructed from tumor samples and cfDNA samples. Overall, we conclude that cfDNA comprises a good representation of the tumor genome in late stage gastric and lung cancer...|$|E
40|$|Cross-validation based point {{estimates}} of prediction accuracy are frequently reported in microarray class prediction problems. However these point estimates {{can be highly}} variable, particularly for small sample numbers, {{and it would be}} useful to provide confidence intervals of prediction accuracy. We performed an extensive study of existing confidence interval methods and compared their performance in terms of empirical coverage and width. We developed a bootstrap case cross-validation (BCCV) resampling scheme and defined several confidence interval methods using BCCV with and without bias-correction. The widely used approach of basing confidence intervals on an independent binomial assumption of the leave-one-out cross-validation errors results in serious <b>under-coverage</b> of the true prediction error. Two split-sample based methods previously proposed in the literature tend to give overly conservative confidence intervals. Using BCCV resampling, the percentile confidence interval method was also found to be overly conservative without bias-correction, while the bias corrected accelerated (BCa) interval method of Efron returns substantially anti-conservative confidence intervals. We propose a simple bias reduction on the BCCV percentile interval. The method provides mildly conservative inference under all circumstances studied and outperforms the other methods in microarray applications with small to moderate sample sizes. ...|$|E
40|$|In {{this paper}} we {{evaluate}} the targeting efficiency of Latin American housing {{programs of the}} demand-side- ABC type, acronyms based on the Spanish words for savings-voucher-mortgage. These are the typical; housing programs in Latin American countries. However, there is no systematic evaluation of the targeting efficiency of these programs. We find that most programs are not progressive and have inefficient targeting (high slippage and low coverage rates). Even reducing slippage to zero does not reduce substantially <b>under-coverage</b> given {{the small size of}} the programs relative to the size of housing shortage. Further, with their existing values of savings-voucher-mortgage-program house price they cannot target the poor without incurring an increased probability of mortgage delinquency. To avoid the mortgage delinquency problem requires substantial increase {{in the value of the}} housing voucher, of an average 47 %, that implies a sharp reduction in annual number of housing solutions, on average of 35 %, or a significant increase in the public budget allocated to the programs, on average of 185 %. Thus to obtain a perfectly poor household targeted housing program requires a substantial change in the programs design and has ramifications for the size of the program and/or budgetary requirements. Targeting, Housing...|$|E
40|$|The paper {{describes}} the editing and imputation procedures which {{were used for}} the demographic variables in the integrated 2008 census in Israel. I. BACKGROUND 1. The Israeli census is an "Integrated Census". It combines administrative data for 100 % of the population with data obtained from a large sample survey (approximately 17 % of the households in the country). 2. The main administrative data source for the Improved Administrative File (IAF) is the National Population Register (NPR). The NPR contains personal records for all citizens and permanent residents of Israel and includes demographic and residential information, as well as links to records of parents, spouses and children. All register records are identified by a unique “personal identity number ” (PIN), which can be used for matching records. Administrative families were constructed by applying a specially developed algorithm, based on family relations and addresses. The IAF is improved by various administrative data sources to depict more accurately the "de facto " population residing in the country. 3. The survey component has a dual design: an area sample, to estimate “under-coverage ” (“U”); and a sample drawn from the PR, to estimate “over-coverage ” (“O”). Over- and <b>under-coverage</b> ar...|$|E
40|$|<b>Under-coverage</b> {{is one of}} {{the most}} common {{problems}} of sampling frames. To reduce the impact of coverage error on survey estimates several frames can be combined in order to achieve a complete (or nearly complete) coverage of the target population. Multiple frame estimators have been developed to be used in the context of multiple frame surveys. Sampling frames may overlap which is the case when a single unit of the sampling frame is related with more than one element of the target population. Indirect sampling (Lavallée, 1995) is an alternative approach to classical sampling theory in dealing with the overlapping problem of sampling frames on survey estimates. Not infrequently a survey may need more than one sampling frame in order to improve coverage and simultaneously the sampling frame overlap. In this paper a new class of estimators is presented which is the result from merging multiple frames estimators (only the particular case of dual frames will be presented) with indirect sampling estimators in order to bring together in a single estimator the effect of several frames on survey estimates. Indirect Sampling, Generalized Weight Share Method, Dual Frame Surveys...|$|E
