2|29|Public
5000|$|Along with {{furnishing}} {{areas of}} the White House, the Kittinger Company has also produced furniture for the inaugurations of former President George W. Bush and President Barack Obama. In 2005, the White House commissioned six chairs from Kittinger for the second inauguration of former President George W. Bush. In 2009, the White House commissioned an additional two chairs and <b>utility</b> <b>table</b> for the inauguration of President Barack Obama. [...] The White House reused the pieces crafted by Kittinger again for the second inauguration of President Barack Obama in 2013.|$|E
40|$|In {{this paper}} we present our {{approach}} {{of improving the}} tradi-tional alpha-beta search process for strategic board games by modifying the method in two ways: 1) forgoing the evaluation of leaf nodes that are not terminal states and 2) employing a <b>utility</b> <b>table</b> that stores the utility for subsets of board con-figurations. In this paper we concentrate our efforts on the game of Connect Four. Our results have shown significant speed-up, {{as well as a}} framework that relaxes common agent assumptions in game search. In addition, it allows game de-signers to easily modify the agent’s strategy by changing the goal from dominance to interaction...|$|E
5000|$|... #Caption: A <b>Utility</b> Furniture {{dressing}} <b>table</b> made by Heal's, 1947. Oak.|$|R
40|$|This paper {{describes}} {{the first phase}} of a study of the impact of key events on long-term transport mode choice decisions. The suggested complexity of transport mode choice is modeled using a Bayesian Decision Network (BDN). An Internet-based questionnaire was designed to measure the various Conditional Probability Tables and the Conditional <b>Utility</b> <b>Tables</b> of the BDN. In total seven different key events were implemented in the questionnaire: Change in residential location, Change in household composition, Change in work location, Change in study location, Change in car availability, Change in availability of public transport pass, and Change in household income. The data of 554 respondents was used to illustrate how the tables can be constructed based on event history data...|$|R
40|$|This {{thesis is}} about the {{creation}} of a decider, which is able to evaluate decision trees and <b>utility</b> <b>tables.</b> The decider enables the robot to decide by its own, which action should be performed in the actual situation. Afterwards the decider is used for the calibration process of time of ﬂight cameras with non-overlapping ﬁeld of views on a mobile platform. A framework is the main objective of this thesis, which can be utilized for known problems as Localization and Exploration in mobile robotics. Therefore, clear interfaces will be designed, to ensure that the framework will be used in future works. After the implementation of the decider a decision tree is designed containing the calibration’s process. This tree must have clearly speciﬁed actions and states, which can be observed by the robot itself. At the end the combination of the decision process with the calibration will be evaluated...|$|R
40|$|We {{study how}} the {{structure}} of the interaction graph of a game affects the existence of pure Nash equilibria. In particular, for a fixed interaction graph, we are interested in whether there are pure Nash equilibria arising when random <b>utility</b> <b>tables</b> are assigned to the players. We provide conditions for {{the structure of}} the graph under which equilibria are likely to exist and complementary conditions which make the existence of equilibria highly unlikely. Our results have immediate implications for many deterministic graphs and generalize known results for random games on the complete graph. In particular, our results imply that the probability that bounded degree graphs have pure Nash equilibria is exponentially small {{in the size of the}} graph and yield a simple algorithm that finds small nonexistence certificates for a large family of graphs. Then we show that in any strongly connected graph of n vertices with expansion (1 +Ω(1)) _ 2 (n) the distribution of the number of equilibria approaches the Poisson distribution with parameter 1, asymptotically as n → +∞. Comment: Published in at [URL] the Annals of Applied Probability ([URL] by the Institute of Mathematical Statistics ([URL]...|$|R
40|$|Data {{maintenance}} {{is an important}} administrative task in database management systems (DBMSs). Data maintenance utilities ensure that the data in tables are stored as efficiently as possible so executing the utilities is crucial to preventing applications from suffering major performance degradation. Executing a <b>utility</b> on a <b>table,</b> however, directly interferes with applications'use of the <b>table</b> so <b>utilities</b> must be scheduled at times of low demand...|$|R
40|$|We {{study how}} the {{structure}} of the interaction graph affects the Nash equilibria of the resulting game. In particular, for a fixed interaction graph, we are interested if there exist Nash equilibria which arise when random <b>utility</b> <b>tables</b> are assigned to the players. We provide conditions for {{the structure of}} the graph under which equilibria are likely to exist and complementary conditions which make the existence of equilibria highly unlikely. Our results have immediate implications for many deterministic graphs and generalize known results for games on the complete graph. In particular, our results imply that the probability that bounded degree graphs have Nash equilibria is exponentially small {{in the size of the}} graph and yield a simple algorithm that finds small non-existence certificates for a large family of graphs. In order to obtained a refined characterization of the degree of connectivity associated with the existence of equilibria, we study the model in the random graph setting. In particular, we look at the case where the interaction graph is drawn from the Erdős-Rényi, G(n, p), where each edge is present independently with probability p. For this model we establish a double phase transition for the existence of pure Nash equilibria as a function of the average degree pn consistent with the non-monotone behavior of the model. We show that when the average degree satisfies np> (2 +Ω(1)) log n, the number of pure Nash equilibria follows a Poisson distribution with parameter 1. When 1 /n << np < (0. 5 −Ω(1)) logn pure Nash equilibria fail to exist with high probability. Finally, when np << 1 /n a pure Nash equilibrium exists with high probability. ...|$|R
40|$|We {{study how}} the {{structure}} of the interaction graph of a game affects the existence of pure Nash equilibria. In particular, for a fixed interaction graph, we are interested in whether there are pure Nash equilibria arising when random <b>utility</b> <b>tables</b> are assigned to the players. We provide conditions for {{the structure of}} the graph under which equilibria are likely to exist and complementary conditions which make the existence of equilibria highly unlikely. Our results have immediate implications for many deterministic graphs and generalize known results for random games on the complete graph. In particular, our results imply that the probability that bounded degree graphs have pure Nash equilibria is exponentially small {{in the size of the}} graph and yield a simple algorithm that finds small nonexistence certificates for a large family of graphs. Then we show that in any strongly connected graph of n vertices with expansion (1 +Ω(1)) log 2 (n) the distribution of the number of equilibria approaches the Poisson distribution with parameter 1, asymptotically as n→+∞. In order to obtain a refined characterization of the degree of connectivity associated with the existence of equilibria, we also study the model in the random graph setting. In particular, we look at the case where the interaction graph is drawn from the Erdős–Rényi, G(n, p), model where each edge is present independently with probability p. For this model we establish a double phase transition for the existence of pure Nash equilibria as a function of the average degree pn, consistent with the nonmonotone behavior of the model. We show that when the average degree satisfies np 3 ̆e(2 +Ω(1)) loge(n), the number of pure Nash equilibria follows a Poisson distribution with parameter 1, asymptotically as n→∞. When 1 /n≪np 3 ̆c(0. 5 −Ω(1)) loge(n), pure Nash equilibria fail to exist with high probability. Finally, when np=O(1 /n) a pure Nash equilibrium exists with constant probability...|$|R
40|$|Includes index. I. The {{relevant}} {{sections of}} the material public and general statutes considered, and of the special acts of the Metropolitan water board. [...] II. The regulations affecting waste, &c. and made under the Metropolis water act, 1871. [...] III. A selection containing the more relevant {{sections of the}} various special acts of the metropolitan water companies (now defunct), comprising those which are considered to have any present <b>utility.</b> [...] IV. <b>Table</b> showing the area served by the Metropolitan water board. Mode of access: Internet...|$|R
40|$|Procurement {{selection}} {{continues to}} be topical, and several advanced studies have been reported. One of the key concerns of procurement selection is how to enhance objectivity. A selection method developed using multi-attribute utility technology (MAUT) and the analytical hierarchy process (AHP) is proposed. With the input of industrial experts, critical procurement selection criteria and procurement strategies commonly used in Hong Kong were identified. Against these criteria, utility factors corresponding to various procurement strategies were then assigned by the experts to create a <b>utility</b> factor <b>table.</b> To cater for individual project characteristics, the relative importance weightings of the selection criteria were assessed using the analytical hierarchy process. Final selection was then based on the highest utility value derived from the procurement strategies, {{taking into account the}} relative importance of the selection criteria. The use of the model is illustrated with actual data. Department of Building and Real Estat...|$|R
40|$|Support {{data for}} UMMZ Miscellaneous Publication No. 203. Support data - Excel {{spreadsheet}} {{of all the}} identification tables (excluding the figures) used in the printed version. Support data - Excel spreadsheet of all the identification tables (excluding the figures) used in the printed version. This enables the reader to modify morphological data and will provide greater utility and flexibility, if new species and characters are added or subtracted. In particular, these tables {{also can be used}} in place of dichotomous keys by sorting rows to better compare particular desired characters (see <b>Utility</b> of Tabulation: <b>Tables,</b> Paragraphic Descriptions, and Keys above) ...|$|R
40|$|The world {{industrial}} energy consumptions {{have gained}} the focus from the sustainability experts. Industrial Energy Efficiency {{is one of}} the important aspects that contributed to the energy consumptions. Process Integration using Pinch Analysis is a very well established tool for energy conservation through systematic design methodology of Heat Exchanger Network - HEN. The HEN for heat recovery within a single process is design using the Grid Diagram. The streams temperature and heat duty are the main aspect in design a HEN through Grid Diagram. The hot utility at above Pinch region is typically placed at the highest temperature, while cold utility at below Pinch is located at lowest temperature. Heat Integration is extended for covering multiple processes heat recovery via utility system, known as Total Site Heat Integration. There is a lot of research done on graphical and numerical targeting of the minimum utility requirement for a Total Site (TS) system. Some mathematical models of the TS utility system have been introduced to optimise the TS utility consumption. However, these methodologies have not discussed the heat exchanger arrangements in the TS HEN. The heat exchangers arrangement in a TS system should be able to produce steam at the high temperature, while the hot utility consumption should be placed as low temperature as possible. This design requirement {{is not the same as}} the design terminology for Grid Diagram in single process heat integration. In this paper, the Utility-HEN Grid Diagram is introduced for assisting the HEN design for a TS system to maximise the heat recovery using indirect heat transfer. This tool is able to visualise the heat transfer between processes in the TS system. The Utility-HEN Grid Diagram allows the heat exchangers to be designed according to utility temperature. The design process would be very much beneficiated by the Multiple <b>Utility</b> Problem <b>Table</b> Algorithm (MU-PTA) and the Total Site <b>Utility</b> Distribution (TSUD) <b>Table...</b>|$|R
40|$|Basic {{words have}} {{been defined as}} those from which other words can be derived semantically. The {{assumption}} underlying the instructional utility of a list of basic words is that a knowledge of them would provide {{a key to the}} understanding of a much larger set of related words. Although {{it has been shown that}} there are far too many basic words in the English language to make a comprehensive list useful instructionally, it has been suggested that a small set of high frequency basic words that students encounter in academic situations would be very useful as a tool for literacy development. Consequently, a study sought to identify a list of K- 6 basic words students commonly encounter in content-area textbooks and standardized tests. From an initial corpus of 30, 371 words, 6, 768 basic words were identified and their grade levels were estimated. This provides for the first time a corpus of basic words that appear to be academically sound and are also few enough to have instructional <b>utility.</b> (One <b>table</b> of data is included; 14 references and an appendix [...] containing the list of basic words organized by grade level [...] are attached.) (Author/SR) Reproductions supplied by EDRS are the best that can be made from the original document...|$|R
40|$|ABSTRACT Coercion {{combines}} optimization {{and expert}} guided manual code modification for adapting simulations {{to meet new}} requirements. How to make right strategic decisions during this semi-automated process is an essential yet difficult problem. This paper demonstrates the importance of gaining insight for agile strategic decisions during the optimization process. It also proposes to chain various optimization techniques to gain and exploit insight. We investigate and summarize important characteristics of several common optimization techniques and possible insight they can provide. The result is presented in an optimization technique table {{that can be used}} as a user guide for gaining insight and making corresponding strategic decisions during the optimization process. To demonstrate the <b>utility</b> of the <b>table,</b> we also present several approaches to gaining insight and possible scenarios of users making suitable strategy decisions based on the insight. 1...|$|R
40|$|Abstract — It {{is often}} {{necessary}} to publish personal information for research purposes. Re-identification {{is a major}} privacy threat to data sets which contain personal sensitive information such as income, a numerical data type and disease, a categorical type. Algorithms such as K-anonymity, L-diversity leave all the sensitive attributes and apply generalization and suppression to the quasi identifiers. So the available truthful data provide good utility for data mining tasks but, here perfect privacy can not be claimed. Privacy can be achieved only by separating Quasi Identifier attributes from sensitive attributes. Then <b>utility</b> of the <b>table</b> gets reduced. Our aim is to design an algorithm that improves both privacy and utility. The numerical sensitive attribute values are protected from proximity attack and the categorical sensitive attribute values are protected from divergence attack by our algorithm. Our experiments which are conducted on Adult data set, proved the improved utility and privacy than the previous methods...|$|R
40|$|Abstract. The {{ostensible}} {{reason for}} a preparation of regression tables is to have them submitted to journals for publication purposes. Contrary to this professed view, regression tables are used during research and not after. Regression tables are information management tools that concentrate information from various sources for immediate consumption by researchers. A next logical step {{in the development of}} statistical packages {{is to be able to}} produce regression tables as fast and as naturally as performing regressions themselves. Regression tables ought to be produced easily, rapidly, and sequentially; they need to be displayed immediately on the computer screen. The <b>utility</b> of regression <b>tables</b> is much reduced if waited until the end of research. outreg, a program by John Gallup, has been modified and augmented extensively for this purpose. outreg 2 will immediately produce formatted regression tables. shellout opens them directly in programs associated with LaTex, Word, or Excel files. seeout will immediately display a regression table in the browser view...|$|R
40|$|Barycentric spherical robots (BSRs) {{rely on a}} noncollocated {{center of}} mass and center of {{rotation}} for propulsion. Unique challenges inherent to BSRs include a nontrivial correlation between internal actuation, momentum, and net vehicle motion. A new method is presented for deriving reduced dynamical equations of motion (EOM) for a general class of BSRs which extends and synthesizes prior efforts in geometric mechanics. Our method {{is an extension of}} the BKMM approach [1], allowing Lagrangian reduction and reconstruction to be applied to dynamical systems with symmetry-breaking potential energies, such as those encountered by BSRs rolling on a surface. The resulting dynamical equations are of minimal dimension and vehicle motion due to actuation and momenta appear linearly in a simple first-order differential equation. The EOM of a BSR named Moball [2] [3] are derived to illustrate the approach's <b>utility.</b> A simple <b>table</b> summarizes our algorithm's application to popular BSRs in the literature, and the approach is extended to sloped terrains...|$|R
40|$|Migrating large Oracle {{databases}} such as data {{warehouses and}} decision support systems, which can easily consume many terabytes, presents {{one of the}} biggest technical challenges for any organization. Prior to Oracle Database 10 g, there were no high-speed tools available to migrate in a short time, such as over a weekend. Such migrations would require extensive planning, would have to be executed over the course of many days, and would require a mix of data movement technologies such as the export/import <b>utility,</b> &quot;create <b>table</b> as select &quot; (CTAS), or third party migration utilities that are not supported by Oracle. Oracle Database 10 g Cross Platform Transportable Tablespace Technology – XTTS (1) makes it possible to move large amounts of data across platforms in a comparatively short time with fairly straightforward planning. And Oracle Recovery Manager – RMAN (2) easily and quickly takes care of endian conversion, if required. Assume your challenge is to move 7 TB across disparate hardware and versions of UNIX in less than 24 hours (a situation very {{similar to that of the}} authors'). How can this be done? The answer is provided below. TECHNOLOGIES FOR MIGRATION The process for selecting the technology set to facilitate large migrations requires considering the tools available and then choosing a set of technologies most appropriate for the various parts of the migration process. The technologies considered for the migration were: Technology Comments Export/import Classic data mover, available for years Various third-party tools Most originated prior to Oracle developed technology. XTTS and Data Pump make third-party tools far less valuabl...|$|R
40|$|This thesis {{looks at}} the problem of {{protecting}} large published statistical tables using cell suppression. Optimal cell suppression {{has been shown to}} be NP-Hard and can therefore only be applied to small tables. Using heuristic techniques to protect large tables tends to suppress far too many table cells lessening the <b>utility</b> of the <b>table.</b> Current state-of-the- art cell suppression algorithms can protect statistical tables with up to forty thousand cells. In this thesis a new model is derived that can fully protect statistical tables with up to one million cells without excessive over-suppression. This has been achieved by creating a new mathematical model that can protect cells in groups rather than individually. A pre-processing step was also introduced to reduce the number of cells that actually need to be protected. Further improvements have been gained through the employment of a self-adaptive Genetic Algorithm to optimise the order in which the groups of cells are protected and the employment of a surrogate fitness function to reduce execution time. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|The Energy Policy Act of 1992 embraces and {{implements}} many of {{the actions}} recommended by the President in the National Energy Strategy. Independent geothermal power producers may be direct beneficiaries of 1) further deregulation of IPPs through their exemption from {{the provisions of the}} Public Utility Holding Company Act and 2) potentially freer access to utility-owned transmission facilities. However, these doors will not be fully opened to geothermal energy until this resource can compete with other fuels in cost considerations. While changes in public policy, such as inclusion of externalities in the price of power or financial penalties on carbon dioxide emissions, will level the playing field somewhat, reductions in cost will be the ultimate marketing tool. This is particularly critical in the economics of power derived from "new," as yet undiscovered reservoirs which will reflect the high costs of today's exploration methods. The Department of Energy's geothermal R&D program, in cooperation with industry, is undertaking, as described in this paper, to achieve the technology cost reductions needed to permit this resource to enjoy a status equal to or better than that of competing fuels at the <b>utility</b> least-cost bargaining <b>table...</b>|$|R
40|$|The 1995 Edition of the Financial Statistics of Major U. S. Publicly Owned Electric Utilities {{publication}} presents 5 years (1991 through 1995) of summary {{financial data}} and current year detailed financial {{data on the}} major publicly owned electric utilities. The objective of the publication is to provide Federal and State governments, industry, {{and the general public}} with current and historical data {{that can be used for}} policymaking and decisionmaking purposes related to publicly owned electric <b>utility</b> issues. Generator (<b>Tables</b> 3 through 11) and nongenerator (Tables 12 through 20) summaries are presented in this publication. Five years of summary financial data are provided (Tables 5 through 11 and 14 through 20). Summaries of generators for fiscal years ending June 30 and December 31, nongenerators for fiscal years ending June 30 and December 31, and summaries of all respondents are provided in Appendix C. The composite tables present aggregates of income statement and balance sheet data, as well as financial indicators. Composite tables also display electric operation and maintenance expenses, electric utility plant, number of consumers, sales of electricity, and operating revenue, and electric energy account data. 9 figs., 87 tabs...|$|R
40|$|The {{objective}} of the water quality reuse classification tool described in this document is to facilitate decision making regarding human water uses. Because not all water uses require {{the same level of}} quality, more efficient water use can be achieved by coupling processes that produce lower quality effluent to processes with minimum quality influent requirements. If effluent water can be used directly in another process, the result is significant treatment energy and cost savings. Moreover, even if minimal treatment is needed to make a process effluent usable in another sector, this coupling will decrease costs in comparison to full-scale wastewater treatment. Water reuse is clearly beneficial, but a water quality classification tool is needed to help utilities, industries, and governments establish combinations that will yield more efficient water use. Such a water quality classification tool, shown in Table 1, has been designed. The water use categories have been arranged in order with applications requiring the highest water quality at the top and processes requiring the lowest quality water at the bottom. Using this <b>table,</b> <b>utility</b> managers, industrial operators, and government officials can determine whether a process effluent is acceptable as an influent for another water use. With this tool as a guide, decisio...|$|R
40|$|Total Site Heat Integration (TSHI) is a {{methodology}} for {{the integration of}} heat recovery among multiple processes and/or plants interconnected by common utilities on a site. Until now, {{it has not been}} used to analyze a site’s overall sensitivity to plant maintenance shutdown and production changes. This feature is vital for allowing engineers to assess the sensitivity of a whole site with respect to operational changes, to determine the optimum utility generation system size, to assess the need for backup piping, to estimate the amount of external utilities that must be bought and stored, and {{to assess the impact of}} sensitivity changes on a cogeneration system. This study presents four new contributions: (1) Total Site Sensitivity Table (TSST), a tool for exploring the effects of plant shutdown or production changes on heat distribution and utility generation systems over a Total Site; (2) a new numerical tool for TSHI, the Total Site Problem Table Algorithm (TS-PTA), which extends the well-established Problem Table Algorithm (PTA) to Total Site analysis; (3) a simple new method for calculating multiple utility levels in both the PTA and TS-PTA; and (4) the Total Site <b>Utility</b> Distribution (TSUD) <b>table,</b> which can be used to design a Total Site utility distribution network. These key contributions are clearly highlighted via the application of the numerical technique to two Case studies...|$|R
40|$|This paper {{provides}} {{a review of}} common statistical disclosure control (SDC) methods implemented at statistical agencies for standard tabular outputs containing whole population counts from a census (either enumerated or based on a register). These methods include record swapping on the microdata prior to its tabulation and rounding of entries in the tables after they are produced. The approach for assessing SDC methods {{is based on a}} disclosure risk-data utility framework and the need to find a balance between managing disclosure risk while maximizing the amount of information that can be released to users and ensuring high quality outputs. To carry out the analysis, quantitative measures of disclosure risk and data utility are defined and methods compared. Conclusions from the analysis show that record swapping as a sole SDC method leaves high probabilities of disclosure risk. Targeted record swapping lowers the disclosure risk, but there is more distortion of distributions. Small cell adjustments (rounding) give protection to census tables by eliminating small cells but only one set of variables and geographies can be disseminated in order to avoid disclosure by differencing nested tables. Full random rounding offers more protection against disclosure by differencing, but margins are typically rounded separately from the internal cells and tables are not additive. Rounding procedures protect against the perception of disclosure risk compared to record swapping since no small cells appear in the tables. Combining rounding with record swapping raises the level of protection but increases the loss of utility to census tabular outputs. For some statistical analysis, the combination of record swapping and rounding balances to some degree opposing effects that the methods have on the <b>utility</b> of the <b>tables.</b> Copyright 2007 The Author. Journal compilation (c) 2007 International Statistical Institute. ...|$|R
40|$|The 1997 {{edition of}} the ``Financial Statistics of Major U. S. Publicly Owned Electric Utilities`` {{publication}} presents 5 years (1993 through 1997) of summary financial data and current year detailed financial data on the major publicly owned electric utilities. The objective of the publication is to provide Federal and State governments, industry, {{and the general public}} with current and historical data {{that can be used for}} policymaking and decisionmaking purposes related to publicly owned electric <b>utility</b> issues. Generator (<b>Tables</b> 3 through 11) and nongenerator (Tables 12 through 20) summaries are presented in this publication. Five years of summary financial data are provided (Tables 5 through 11 and 14 through 20). Summaries of generators for fiscal years ending June 30 and December 31, nongenerators for fiscal years ending June 30 and December 31, and summaries of all respondents are provided in Appendix C. The composite tables present aggregates of income statement and balance sheet data, as well as financial indicators. Composite tables also display electric operation and maintenance expenses, electric utility plant, number of consumers, sales of electricity, operating revenue, and electric energy account data. The primary source of publicly owned financial data is the Form EIA- 412, ``Annual Report of Public Electric Utilities. `` Public electric utilities file this survey on a fiscal year basis, in conformance with their recordkeeping practices. The EIA undertook a review of the Form EIA- 412 submissions to determine if alternative classifications of publicly owned electric utilities would permit the inclusion of all respondents. The review indicated that financial indicators differ most according to whether or not a publicly owned electric utility generates electricity. Therefore, the main body of the report provides summary information in generator/nongenerator classifications. 2 figs., 101 tabs...|$|R
40|$|Dupont's Marshall Laboratory is an {{automotive}} paint {{research and}} development facility in Philadelphia, Pennsylvania. The campus is comprised of several buildings that are served by Trigen-Philadelphia Energy Corporation's district steam loop. In 1996 Dupont management {{announced that it was}} considering moving the facility out of Philadelphia primarily due to the high operating cost compared to where they were considering relocating. The city officials responded by bringing the local electric and gas <b>utilities</b> to the <b>table</b> to negotiate better rates for Dupont. Trigen also requested the opportunity to propose energy savings opportunities, and dedicated a team of engineers to review Dupont's steam system to determine if energy savings could be realized within the steam system infrastructure. As part of a proposal to help Dupont reduce energy costs while continuing to use Trigen's steam, Trigen recommended modifications to increase energy efficiency, reduce steam system maintenance costs and implement small scale cogeneration. These recommendations included reducing the medium pressure steam distribution to low pressure, eliminating the medium pressure to low pressure reducing stations, installing a back pressure steam turbine generator, and preheating the domestic hot water with the condensate. Dupont engineers evaluated these recommended modifications and chose to implement most of them. An analysis of Dupont's past steam consumption revealed that the steam distribution system sizing was acceptable if the steam pressure was reduced from medium to low. After a test of the system and a few modifications, Dupont reduced the steam distribution system to low pressure. Energy efficiency is improved since the heat transfer losses at the low pressure are less than at the medium pressure distribution. Additionally, steam system maintenance will be significantly reduced since 12 pressure reducing stations are eliminated. With the steam pressure reduction now occurring at one location, the opportunity existed to install a backpressure turbine generator adjacent to the primary pressure reducing station. The analysis of Dupont's steam and electric load profiles demonstrated that cost savings could be realized with the installation of 150 kW of self-generation. There were a few obstacles, including meeting the utility's parallel operation requirements, that made this installation challenging. Over two years have passed since the modifications were implemented, and although cost savings are difficult to quantify since process steam use has increased, the comparison of steam consumption to heating degree days shows a reducing trend. Dupont's willingness to tackle energy conservation projects without adversely affecting their process conditions can be an example to other industrial steam users...|$|R
40|$|In recent years, the 3 D {{printing}} technology {{has become increasingly}} popular, with wide-spread uses in rapid prototyping, design, art, education, medical applications, food and fashion industries. It enables distributed manufacturing, allowing users to easily produce customized 3 D objects in office or at home. The investment in 3 D {{printing technology}} continues to drive down the cost of 3 D printers, making them more affordable to consumers. As 3 D printing becomes more available, it also demands better computer algorithms to assist users in quickly and easily generating 3 D content for printing. Creating 3 D content often requires considerably more efforts and skills than creating 2 D content. In this work, I will study several aspects of 3 D shape design and optimization for 3 D printing. I start by discussing my work in geometric puzzle design, which is a popular application of 3 D printing in recreational math and art. Given user-provided input figures, {{the goal is to}} compute the minimum (or best) set of geometric shapes that can satisfy the given constraints (such as dissection constraints). The puzzle design also has to consider feasibility, such as avoiding interlocking pieces. I present two optimization-based algorithms to automatically generate customized 3 D geometric puzzles, which can be directly printed for users to enjoy. They are also great tools for geometry education. Next, I discuss shape optimization for printing functional tools and parts. Although current 3 D modeling software allows a novice user to easily design 3 D shapes, the resulting shapes are not guaranteed to meet required physical strength. For example, a poorly designed stool may easily collapse when a person sits on the stool; a poorly designed wrench may easily break under force. I study new algorithms to help users strengthen functional shapes in order to meet specific physical properties. The algorithm uses an optimization-based framework — it performs geometric shape deformation and structural optimization iteratively to minimize mechanical stresses in the presence of forces assuming typical use scenarios. Physically-based simulation is performed at run-time to evaluate the functional properties of the shape (e. g., mechanical stresses based on finite element methods), and the optimizer makes use of this information to improve the shape. Experimental results show that my algorithm can successfully optimize various 3 D shapes, such as chairs, <b>tables,</b> <b>utility</b> tools, to withstand higher forces, while preserving the original shape as much as possible. To improve the efficiency of physics simulation for general shapes, I also introduce a novel, SPH-based sampling algorithm, which can provide better tetrahedralization for use in the physics simulator. My new modeling algorithm can greatly reduce the design time, allowing users to quickly generate functional shapes that meet required physical standards...|$|R
40|$|Electromagnetic (EM) wave {{methods are}} used in many civil {{engineering}} applications for non-destructive investigations such as locating reinforcing steel and voids in concrete, sinkholes, buried <b>utilities,</b> the groundwater <b>table,</b> seepage through dams and determining the thickness of pavements. All of the applications depend on knowing the velocity of the EM wave through the material of interest. The velocity of EM waves through a vacuum {{is equivalent to the}} speed of light; however, as a material's ability to hold a charge increases (ability to be a capacitor), the velocity of the EM wave is slowed through that material. The relative permittivity (dielectric constant) of a material is a measure of a material's ability to hold a charge. Knowing the relative permittivity enables one to calculate the velocity of an EM wave through the material. The goal of a larger research program is to predict the relative permittivity of soils. A principal factor determining the relative permittivity of soil is the water content of the soil. In order to predict accurate relative permittivity for a soil, the water content of the soil must be known. The objective of the research project reported in this thesis was to choose a method to predict the water content of a soil and to quantify the accuracy of the method. The focus of the larger project is on the earth's near subsurface, 0 to about 1 m below ground surface. This depth is often characterized by alternately wet (saturated) to dry pores within the soil and {{is referred to as the}} vadose zone. Knowing the soil water condition at any time is paramount to being able to predict the relative permittivity and subsequently the velocity of EM waves through the soil. A computer code was selected to be used to predict the soil water along a vertical profile in the vadose zone. WinUNSAT-H, a Windows(R) -based version of the code UNSAT-H was used to model the subsurface and predict vertical soil water profiles as a function of time. Inputs into WinUNSAT-H include soil properties and atmospheric conditions. The code was used to predict soil water profiles for a column of sand which was constructed in the laboratory where physical experiments were performed to alternately saturate and then drain the water from the sand column and measure the actual soil water profile. The results of the predicted soil water were then compared to the measured water profile for the sand column. The code was used to predict the soil water profile during draining (drying or dewatering) of the sand column. The sand had a saturated hydraulic conductivity of 0. 04 cm/second which is a high hydraulic conductivity and leads to rapid drainage of water from the column of sand. The saturated volumetric water content of the sand was 0. 35 (all pores in the sand are filled with water) and the residual volumetric water content was 0. 05 (volumetric water content at which no further water will drain from the sand under gravity, i. e., a pressure must be applied to the sand to force any more water from the sand). After the sand was saturated with water, the water source was removed and the water in the pores of the sand was allowed to drain. The 160 cm column of sand drained to nearly the residual volumetric water content within 15 to 30 minutes. After two hours of drainage, the column of sand was essentially at the residual volumetric water content throughout the entire column...|$|R
40|$|The National Institute for Occupational Safety and Health {{collects}} and automates death certificates {{from the}} 52 vital statistics reporting {{units in the}} 50 States, New York City, and the District of Columbia for workers 16 {{years of age or}} older who die {{as a result of a}} work-related injury. Analysis of occupational injury deaths, such as those gathered through the National Traumatic Occupational Fatalities (NTOF) surveillance system, facilitates identification of high risk worker groups and potential injury risk factors by demographic, employment, and injury characteristics. This promotes the effective use of resources aimed at preventing injuries in the workplace. In reviewing these data, it is important to note the distinction between the number of deaths and fatality rates. The number of deaths indicates the magnitude of a problem and fatality rates depict the risk faced by workers. Fatal occupational injury data for 1980 through 1995 are provided for the U. S. and for each State. Major findings from this study: There were 93, 338 civilian workers who died from injuries sustained while working in the U. S., 1980 through 1995 (Table US- 1). The average annual occupational fatality rate for the U. S. civilian workforce was 5. 2 per 100, 000 workers for 1980 through 1995 (Table US- 1). Civilian fatal occupational injuries decreased 28 %, from 7, 343 fatalities in 1980 to 5, 314 in 1995 (Table US- 1). The average annual fatality rate per 100, 000 civilian workers decreased, from 7. 4 in 1980 to 4. 3 in 1995 - a 42 % decrease (Table US- 1). The greatest number of fatal occupational injuries occurred in California (9, 670), Texas (9, 423), Florida (5, 596), Illinois (4, 169), and Pennsylvania (3, 926) (Table US- 2). The States with the highest occupational injury fatality rates per 100, 000 workers were Alaska (24. 3), Wyoming (16. 7), Montana (12. 4), Idaho (10. 7), West Virginia (10. 4), and Mississippi (10. 1) (Table US- 2). The fatality rate for males (8. 8 per 100, 000 workers) was 11 times higher than the rate for females (0. 8 per 100, 000 workers) (Table US- 3). Eighty-five percent of civilian workers who died were white and 11 % were black (Table US- 3). Black workers had the highest fatality rate per 100, 000 workers (5. 8), followed by whites (5. 1) (Table US- 3). The age group with the largest number of occupational injury fatalities was the 25 - 34 year old age group (26 %) followed closely by the 35 - 44 year old age group (22 %) (Table US- 3). Workers 65 years and older had the highest fatality rate of all age groups (13. 6 deaths per 100, 000 workers) in every industry and occupation division (Tables US- 3, US- 17, US- 27). The leading causes of occupational injury death in the U. S. were motor vehicle crashes (23 %), homicides (14 %), machine-related incidents (13 %), falls (10 %), electrocutions (7 %), and being struck by falling objects (6 %) (Table US- 7). The highest rates by cause of death varied by gender: the highest rate for females was homicide (0. 3 per 100, 000 workers), while motor vehicle crashes (2. 0 per 100, 000 workers) were the cause of death with the highest rate among males (Table US- 7). While the rate of motor vehicle-related fatalities decreased 36 % between 1980 and 1995 (from 1. 7 per 100, 000 workers to 1. 1), motor vehicles continued to have the highest rate through 1995. Machines had the second highest rate per 100, 000 workers until 1990, when they were surpassed by homicides (Table US- 9). The highest rates by cause of death varied by race: the highest rate for whites was motor vehicle crashes (1. 2 per 100, 000 workers), while the highest rate by cause of death for blacks was homicide (1. 4 per 100, 000 workers) (Table US- 10). The industry divisions with the greatest proportion of fatalities were construction (18 %), transportation/communication/public utilities (17 %), manufacturing (15 %), and agriculture/forestry/fishing (12 %) (Table US- 13). The mining industry had the highest average annual fatality rate per 100, 000 workers (30. 4), followed by agriculture/forestry/fishing (19. 6), construction (15. 3), and transportation/communication/public <b>utilities</b> (12. 6) (<b>Table</b> US- 13). The highest rates by cause of death varied by industry: the highest rate in the agriculture/forestry/fishing industry was for machinery-related incidents (6. 6 per 100, 000 workers), while the highest rate by cause of death in the retail trade industry was for homicides (1. 7) (Table US- 16). The occupation divisions with the greatest proportion of fatalities were precision production/craft/repairers (21 %), transportation/material movers (17 %), farmers/foresters/fishers (13 %), and laborers (11 %) (Table US- 23). The occupation division of farmers/foresters/fishers had the highest average annual fatality rate per 100, 000 workers (21. 9), followed by transportation/material movers (21. 6), laborers (13. 7), and precision production/craft/repairers (9. 2) (Table US- 23). The highest rates by cause of death varied by occupation: the highest rate among executives/administrators/managers was for homicides (0. 8 per 100, 000 workers), while machinery-related incident rates were highest among farmers/foresters/fishers (7. 0) (Table US- 26). " - NIOSHTIC- 2 "July 2001. "Also available via the World Wide Web. Includes bibliographical references...|$|R

