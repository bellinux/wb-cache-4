0|1445|Public
40|$|This paper {{presents}} a brief survey on Automatic <b>Voice</b> <b>Recognition</b> {{so as to}} provide a technological perspective and {{an appreciation of the}} fundamental progress that has been accomplished in area of voice communication. The voice is a signal of infinite information. After years of research and development the accuracy of automatic <b>voice</b> <b>recognition</b> remains one of the important research challenges in respect to variations of the speakers, text and surroundings. Digital processing of <b>voice</b> <b>recognition</b> and speech signal is very important for fast and accurate automatic <b>voice</b> <b>recognition</b> technology. This review paper summarizes and compares some of the well known methods used in various stages of <b>voice</b> <b>recognition</b> system. It further helps to identify research topic and applications which are at the forefront of this exciting and challenging field of <b>voice</b> <b>recognition...</b>|$|R
40|$|This paper {{presents}} ongoing {{research on}} human-computer interaction in virtual environments using <b>voice</b> <b>recognition</b> systems. The paper starts describing <b>voice</b> <b>recognition</b> technologies (software and hardware based). The Virtual Reality Laboratory of the University of Colima has begun an initial {{development of a}} desktop virtual environment where a virtual molecule was manipulated and analyzed through voice commands. A hardware-based <b>voice</b> <b>recognition</b> system was used. Initial tests showed that the neural net, integrated in the <b>voice</b> <b>recognition</b> system, was fast and accurate enough {{to interact with the}} virtual environment...|$|R
40|$|In the {{advanced}} information society, the various interfaces are required. Especially, {{the accuracy of}} <b>voice</b> <b>recognition</b> becomes good. The current interface in Smartphone in equipped with <b>voice</b> <b>recognition.</b> However, such an interface can not understand the user’s mental state. We develop the Emotion Orientated Interface on Android Smartphone by using <b>voice</b> <b>recognition.</b> 開催日：平成 24 年 7 月 14 日　会場：広島市立大...|$|R
40|$|Recent {{performance}} {{breakthroughs in}} affordable, large vocabulary, speaker independent <b>voice</b> <b>recognition</b> systems have rekindled widespread interest in using <b>voice</b> <b>recognition</b> technology {{to enhance the}} palatability and effectiveness of clinician-mediated computing. However, even if industry fully addresses the formidable hardware requirements, less than perfect <b>recognition</b> accuracies, discrete <b>voice</b> <b>recognition</b> requirements, and throughput limitations, there are significant cognitive and implementation issues that must be adequately resolved before voice can become a ubiquitous input modality. Cognitive issues include making allowances for individual differences in verbal communication style and skill levels, the relative cognitive load of using a voice enabled interface compared to alternative modalities, and the user's cognitive style. Implementation issues include a significant training requirement, limited portability, lengthy user switching time, questionable privacy, satisfying hardware requirements and the suitability of <b>voice</b> <b>recognition</b> in specific work environments. The inevitable resolution of these issues coupled with continuously improving <b>voice</b> <b>recognition</b> performance, promises a new era for <b>voice</b> <b>recognition</b> in medicine...|$|R
40|$|Continuous <b>voice</b> <b>recognition</b> {{dictation}} {{systems for}} radiology reporting provide {{a viable alternative}} to conventional transcription services with the promise of shorter report turnaround times and increased cost savings. While these benefits may be realized in academic institutions, it is unclear how <b>voice</b> <b>recognition</b> dictation impacts the private practice radiologist who is now faced with the additional task of transcription. In this article, we compare conventional transcription services with a commercially available <b>voice</b> <b>recognition</b> system with the following results: 1) Reports dictated with <b>voice</b> <b>recognition</b> took 50 % longer to dictate despite being 24 % shorter than those conventionally transcribed, 2) There were 5. 1 errors per case, and 90 % of all <b>voice</b> <b>recognition</b> dictations contained errors prior to report signoff while 10 % of transcribed reports contained errors. 3). After signoff, 35 % of VR reports still had errors. Additionally, cost savings using <b>voice</b> <b>recognition</b> systems in non-academic settings may not be realized. Based on average radiologist and transcription salaries, the additional time spent dictating with <b>voice</b> <b>recognition</b> costs an additional $ 6. 10 per case or $ 76, 250. 00 yearly. The opportunity costs may be higher. Informally surveyed, all radiologists expressed dissatisfaction with <b>voice</b> <b>recognition</b> with feelings of frustration, and increased fatigue. In summary, in non-academic settings, utilizing radiologists as transcriptionists results in more error ridden radiology reports and increased costs compared with conventional transcription services...|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimited. Technological advances have had profound {{effects on the}} conduct of military operations in both peacetime and in war. One advance that has had a great impact outside the military by reducing human intervention is <b>Voice</b> <b>Recognition</b> (VR) technology. This thesis will examine {{the implementation of a}} <b>Voice</b> <b>Recognition</b> System as a shipdriving device and as a means of decreasing the occurrence of mishaps while reducing the level of fatigue of watchstanders on the bridge. Chapter I will discuss the need for the United States Navy to investigate the implementation of a <b>Voice</b> <b>Recognition</b> System to help reduce the probability of mishaps occurring. Chapter II will explain <b>voice</b> <b>recognition</b> technology, how it works, and how the proposed system can be fielded aboard U. S. Navy ships. Chapter III will examine the opinions (on the implementation of a <b>Voice</b> <b>Recognition</b> System) of officers charged with the safe navigation of naval ships. Chapter IV will review the concerns of officers, and will justify the implementation by answering these concerns. The conclusion will iterate the advances in <b>voice</b> <b>recognition,</b> and why a <b>Voice</b> <b>Recognition</b> system should be implemented on the bridges of U. S. Navy ships. Lieutenant, United States Nav...|$|R
40|$|Although smart {{devices have}} become a widely-adopted tool for {{communication}} in modern society, it still requires a steep learning curve among the elderly. By introducing a voice-based interface for smart devices using <b>voice</b> <b>recognition</b> technology, smart devices can become more user-friendly and useful to the elderly. However, the <b>voice</b> <b>recognition</b> technology used in current devices is attuned to the voice patterns of the young. Therefore, speech recognition falters when an elderly user speaks into the device. This paper has identified that the elderly's improper speech rate by each syllable contributes to the failure in the <b>voice</b> <b>recognition</b> system. Thus, upon modifying the speech rate by each syllable, the <b>voice</b> <b>recognition</b> rate saw an increase of 12. 3 %. This paper demonstrates that by simply modifying the speech rate by each syllable, {{which is one of}} the factors that causes errors in <b>voice</b> <b>recognition,</b> the recognition rate can be substantially increased. Such improvements in <b>voice</b> <b>recognition</b> technology can make it easier for the elderly to operate smart devices that will allow them to be more socially connected in a mobile world and access information at their fingertips. It may also be helpful in bridging the communication divide between generations...|$|R
25|$|In July 2017, Mozilla {{launched}} the project Common Voice {{to help make}} <b>voice</b> <b>recognition</b> open to everyone. Visitors to the website can donate their voice to help build an open-source <b>voice</b> <b>recognition</b> engine that anyone can use to make apps for devices and the web that make use of <b>voice</b> <b>recognition.</b> The website allows visitors to read a sentence to help the machine system learn how real people speak, as well as validate the read sentences of other people.|$|R
50|$|Perio Charting: <b>voice</b> <b>recognition</b> {{software}} compatible.|$|R
40|$|The rapidly {{changing}} world {{of computer technology}} has increased the possibilities for new methods of assessment. <b>Voice</b> <b>recognition</b> systems, natural language processing, computer simulations, and the Internet all present opportunities and challenges in assessment. <b>Voice</b> <b>recognition</b> systems offer a new approach in gathering data, natura...|$|R
40|$|This paper {{presents}} {{a method of}} implementing the <b>voice</b> <b>recognition</b> for the control of software applications. The solutions proposed are based on transforming {{a subset of the}} natural language in commands recognized by the application using a formal language defined by the means of a context free grammar. At the end of the paper is presented the modality of integration of <b>voice</b> <b>recognition</b> and of <b>voice</b> synthesis for the Romanian language in Windows applications. <b>voice</b> <b>recognition,</b> formal languages, context free grammars, text to speech...|$|R
40|$|Background: Medication errors have {{potential}} to cause harm and death; especially {{children who are}} three times more vulnerable than adults. Risk of medication errors is higher in out- patient settings due to a stressful work environment with less familiarity of individual patients. This problem in sub-Saharan Africa is however largely undetermined. A <b>Voice</b> <b>Recognition</b> System that converts verbal messages into text and stores it in a database in a retrievable format could impact on reduction of medication errors. Objectives: The primary objective was to compare medication prescription and dispensing errors in written prescriptions with those from a <b>Voice</b> <b>Recognition</b> System. Secondary objectives were to determine the types and frequency of medication errors, determinants of medication errors and acceptability of routine use of a <b>Voice</b> <b>Recognition</b> System to make medication prescriptions. Study design: A before -after Intervention study to determine the impact of introduction of a <b>Voice</b> <b>Recognition</b> System on the occurrence of medication errors. Methods: Prescriptions issued from the Paediatric Accident and Emergency Department at Aga Khan University Hospital Nairobi over a six month period were randomly selected and analyzed for errors. Patient‟s bio-data, diagnosis, prescriber‟s specialization and time of prescription were retrieved from outpatient medical records and documented in a standard study tool. A <b>Voice</b> <b>Recognition</b> System was installed and doctors and pharmacists consenting to use <b>Voice</b> <b>Recognition</b> were trained to enhance proficiency in its use. During consultations, doctors enrolled patients who provided written informed consent to have their prescriptions made using <b>Voice</b> <b>Recognition.</b> Prescription and dispensing records were analysed to determine the occurrence of medication errors. Questionnaires were issued to pharmacists and doctors to rate the use of <b>Voice</b> <b>Recognition</b> in the medication process. Results: During the VRS phase the proportion of female patients reviewed were 56. 9...|$|R
40|$|<b>Voice</b> <b>recognition</b> {{software}} allows {{computer users}} to bypass their keyboards {{and use their}} voices to enter text. While the library literature is somewhat silent about <b>voice</b> <b>recognition</b> technology, the medical and legal communities have reported some success using it. <b>Voice</b> <b>recognition</b> software was tested for dictation accuracy and usability within an agriculture library at the University of Illinois. Dragon NaturallySpeaking 8. 0 {{was found to be}} more accurate than speech recognition within Microsoft Office 2003. Helpful Web sites and a short history regarding this breakthrough technology are included...|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedThe {{goal of this}} study is to provide a single source of data that enables the selection of an appropriate <b>voice</b> <b>recognition</b> (VR) application for a decision support system (DSS) as well as for other computer applications. A brief background of both <b>voice</b> <b>recognition</b> systems and decision supports systems is provided with special emphasis given to the dialog component of DSS. The categories of <b>voice</b> <b>recognition</b> discussed are human factors, environmental factors, situational factors, quantitative factors, training factors, host computer factors, and experiments and research. Each of these areas of <b>voice</b> <b>recognition</b> is individually analyzed, and specific references to applicable literature are included. This study also includes appendices that contain: a glossary (including definitions) of phrases specific to both decision support system and <b>voice</b> <b>recognition</b> systems, keywords applicable to this study, an annotated bibliography (alphabetically and by specific topics) of current VR systems literature containing over 200 references, an index of publishers, a complete listing of current commercially available VR systems. [URL] United States Nav...|$|R
5000|$|... {{software}} for Contact Center, IVR applications, <b>voice</b> <b>recognition</b> systems ...|$|R
5000|$|Transcription {{to digital}} text from {{handwriting}} and <b>voice</b> <b>recognition.</b>|$|R
5000|$|... (Local) {{process the}} <b>voice</b> <b>recognition</b> on his local machine or ...|$|R
5000|$|Acquisition of Talk Technology, a {{producer}} of medical <b>voice</b> <b>recognition</b> systems.|$|R
5000|$|<b>Voice</b> <b>recognition,</b> {{with the}} option to revert to touchtone keypad input ...|$|R
40|$|This report {{describes}} {{an experiment in}} which military officers used <b>voice</b> <b>recognition</b> equipment under various mental loading conditions. Results showed <b>voice</b> <b>recognition</b> performance degraded as mental loading was imposed on the subjects. sponsored by Mr. Frank Deckelman, NAVELEX, Code 330. The work was performed by the authors at the Naval Postgraduate School, Monterey, California. [URL]...|$|R
40|$|Medical {{records have}} been {{evolving}} {{from the traditional}} paper-based to digital, and from the dictating of reports to <b>voice</b> <b>recognition</b> systems. The transition to digital operations will not be complete until {{we have the ability}} to combine <b>voice</b> <b>recognition</b> with automated indexing of text. This paper introduces the methods we used to evaluate the combination of existing <b>voice</b> <b>recognition</b> software programs and NOMINDEX, a system that maps a medical text to MeSH codes using the French ADM lexical database. These systems were applied to 28 patient discharge summaries in French, produced after a coronarography, and extracted from the MENELAS corpus of text. Using the best configuration for <b>voice</b> <b>recognition</b> the rate of accurate recognition exceeds 98 percent. Among the indexing concepts assigned by NOMINDEX, 25 percent were not pertinent and 12 percent of the relevant concepts were missing. Most errors were related to confusion between common language and medical language, and to the coverage of the ADM lexical database. Best results would be expected with a more comprehensive lexical resource. In addition only 3 percent of the errors generated by inadequate <b>voice</b> <b>recognition</b> impacted on automatic indexing by NOMINDEX...|$|R
5000|$|Speech Server - Speech {{applications}} for automated telephone systems, including <b>voice</b> <b>recognition</b> ...|$|R
50|$|Captions {{are created}} by a {{communications}} assistant using a computer with <b>voice</b> <b>recognition</b> software. The communications assistant listens to and revoices the hearing party's side of the conversation into the microphone of a headset. A <b>voice</b> <b>recognition</b> program creates the captions and they are sent out to the captioned telephone where they are read by the user.|$|R
40|$|Research Objectives: {{within an}} IAC framework, <b>voice</b> <b>{{recognition}}</b> and face recognition may usefully {{be viewed as}} parallel pathways. The demonstration of cross-modality priming of voices by faces and vice versa would add weight to such a view. Design: repetition priming of celebrity individuals was examined under within-modality and cross-modality conditions. Method: sixty participants completed a 2 -stage method involving semantic decisions to celebrities at study, and speeded familiarity decisions to primed and unprimed celebrities at test. Results: expected within-modality priming effects emerged: Faces primed subsequent face <b>recognition,</b> and <b>voices</b> primed subsequent <b>voice</b> <b>recognition.</b> Additionally, <b>voices</b> successfully primed face recognition, however, faces did not prime subsequent <b>voice</b> <b>recognition.</b> Conclusions: the emergence of cross-modality priming adds support to the integration of <b>voice</b> <b>recognition</b> within a person perception model such as IAC. Results are discussed {{with respect to the}} relative strength of face and voice inputs in these two parallel pathways<br/...|$|R
25|$|Voice input in English and Vietnamese {{using the}} <b>voice</b> <b>recognition</b> {{technology}} by Google.|$|R
50|$|In 2001 Cognitive Technologies {{sold its}} Russian {{language}} <b>voice</b> <b>recognition</b> library to Intel.|$|R
5000|$|Voice input in English and Vietnamese {{using the}} <b>voice</b> <b>recognition</b> {{technology}} by Google.|$|R
40|$|Abstract—This paper {{presents}} the effective and robust method for the feature extraction {{of the speaker}} dependent <b>voice</b> <b>recognition.</b> The authors developed a simple Matlab program for this purpose where the article discrete wavelet transform theory had been used. The voice of set of speakers had been inputted on the database and the discrete wavelet transform calculates the properties and variables {{needed in order to}} verify correctly the speaker. Experimental results show that our method is very effective and the results are satisfactory and finally, the wavelet-based <b>voice</b> <b>recognition</b> system and its performance are discussed and highlighted. Keywords—Speaker dependent, <b>Voice</b> <b>recognition,</b> Discrete wavelet transform I...|$|R
40|$|<b>Voice</b> or speaker <b>recognition</b> is {{critical}} {{in a wide variety}} of social contexts. In this study, we investigated the contributions of acoustic, phonological, lexical, and semantic information toward <b>voice</b> <b>recognition.</b> Native English speaking participants were trained to recognize five speakers in five conditions: non-speech, Mandarin, German, pseudo-English, and English. We showed that <b>voice</b> <b>recognition</b> significantly improved as more information became available, from purely acoustic features in non-speech to additional phonological information varying in familiarity. Moreover, we found that the recognition performance is transferable between training and testing in phonologically familiar conditions (German, pseudo-English, and English), but not in unfamiliar (Mandarin) or non-speech conditions. These results provide evidence suggesting that bottom-up acoustic analysis and top-down influence from phonological processing collaboratively govern <b>voice</b> <b>recognition...</b>|$|R
50|$|Microphones (4)—three {{microphones}} {{are used}} to hear sharp sounds and one is used to discriminate voice. The robot can localize {{the direction of a}} sharp sound and move towards it. I-Cybies can recognize spoken commands and respond in a specific manner. <b>Voice</b> <b>recognition</b> features biometric authentication. <b>Voice</b> <b>recognition</b> is activated by the head contact sensor or by the remote control unit.|$|R
50|$|The <b>voice</b> <b>recognition</b> {{systems are}} in limited use, due to {{problems}} with the technology. A new development called the captioned telephone now uses <b>voice</b> <b>recognition</b> to assist the human operators. Newer text-based communication methods, such as short message service (SMS), Internet Relay Chat (IRC), and instant messaging have also been adopted by the deaf as an alternative or adjunct to TTY.|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedA {{great deal of}} {{research has been conducted}} in the past 23 years concerning the use of <b>voice</b> <b>recognition</b> equipment with computers. The goal of this research has been to improve the nan- machine interface. With the breakthrough from discrete to continuous <b>voice</b> <b>recognition</b> technology in the 1973 's, a large step toward that goal was taken. This thesis attempts to show that continuous <b>voice</b> <b>recognition</b> technology can be effectively applied in a highly interactive, computer-aided wargaming environment. Through analysis of the strictly-formatted command syntax of the Naval Warfare Interactive Simulation System (NWISS) and use of commercially available, innovative, continuous speech hardware and software, a new input medium was created for the users of that wargame. The true effectiveness of this application of <b>voice</b> <b>recognition</b> technology must still be tested. Plans for such testing are being made and, to that extent, the thesis objectives are partly met. [URL]...|$|R
40|$|The current {{digital signal}} {{analysis}} algorithms are investigated that are implemented in automatic <b>voice</b> <b>recognition</b> algorithms. Automatic <b>voice</b> <b>recognition</b> means, {{the capability of}} a computer to recognize and interact with verbal commands. The digital signal is focused on, rather than the linguistic, analysis of speech signal. Several digital signal processing algorithms are available for <b>voice</b> <b>recognition.</b> Some of these algorithms are: Linear Predictive Coding (LPC), Short-time Fourier Analysis, and Cepstrum Analysis. Among these algorithms, the LPC is the most widely used. This algorithm has short execution time and do not require large memory storage. However, it has several limitations due to the assumptions used to develop it. The other 2 algorithms are frequency domain algorithms with not many assumptions, {{but they are not}} widely implemented or investigated. However, with the recent advances in the digital technology, namely signal processors, these 2 frequency domain algorithms may be investigated in order to implement them in <b>voice</b> <b>recognition.</b> This research is concerned with real time, microprocessor based recognition algorithms...|$|R
40|$|A {{research}} {{experiment was}} conducted to investigate how accurate a <b>voice</b> <b>recognition</b> system must be for daily production use. Specifically, {{the purpose of the}} research was to establish the percentage accuracy level at which a user becomes frustrated and decides not to use a <b>voice</b> <b>recognition</b> device. The experiment consisted of controlling the perceived recognition accuracy of a <b>voice</b> <b>recognition</b> system and then collecting data {{through the use of a}} questionnaire from the experimental users on the acceptability of the equipment. The experiment was not totally successful for a variety of reasons. This paper will discuss the research methodology, review the data collected, and suggest possible alternatives to the experimental design to overcome the problem areas encountered. (AuthorNA[URL]...|$|R
30|$|Characteristics Based Authentication (CBA) or Biometrics e.g. fingerprint, audio or <b>voice</b> <b>recognition,</b> {{signature}} {{recognition and}} face recognition.|$|R
40|$|Introduction: Use of {{electronic}} health record (EHR) systems can place a considerable data entry burden upon {{the emergency department}} (ED) physician. <b>Voice</b> <b>recognition</b> data entry has been proposed as one mechanism to mitigate some of this burden; however, no reports are available specifically comparing emergency physician (EP) time use or number of interruptions between typed and <b>voice</b> <b>recognition</b> data entry-based EHRs. We designed this study to compare physician time use and interruptions between an EHR system using typed data entry versus an EHR with <b>voice</b> <b>recognition.</b> Methods: We collected prospective observational data at 2 academic teaching hospital EDs, one using an EHR with typed data entry {{and the other with}} <b>voice</b> <b>recognition</b> capabilities. Independent raters observed EP activities during regular shifts. Tasks each physician performed were noted and logged in 30 second intervals. We compared time allocated to charting, direct patient care, and change in tasks leading to interruptions between sites. Results: We logged 4, 140 minutes of observation for this study. We detected no statistically significant differences in the time spent by EPs charting (29. 4 % typed; 27. 5 % voice) or the time allocated to direct patient care (30. 7 %; 30. 8 %). Significantly more interruptions per hour were seen with typed data entry versus <b>voice</b> <b>recognition</b> data entry (5. 33 vs. 3. 47; p= 0. 0165). Conclusion: The use of a <b>voice</b> <b>recognition</b> data entry system versus typed data entry did not appear to alter the amount of time physicians spend charting or performing direct patient care in an ED setting. However, we did observe a lower number of workflow interruptions with the <b>voice</b> <b>recognition</b> data entry EHR. Additional research is needed to further evaluate the data entry burden in the ED and examine alternative mechanisms for chart entry as EHR systems continue to evolve. [West J Emerg Med. 2014; 15 (4) : 541 - 547. ]...|$|R
