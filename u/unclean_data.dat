12|8|Public
50|$|In {{reference}} to databases, this is data that contain errors. <b>Unclean</b> <b>data</b> can contain such mistakes as spelling or punctuation errors, incorrect data {{associated with a}} field, incomplete or outdated data, or even data that has been duplicated in the database. It can be cleaned through {{a process known as}} data cleansing.|$|E
40|$|I {{propose a}} {{modeling}} approach to classifying trades as buys or sells. Modeled classifications consider information strengths, microstructure effects, and classification correlations. I also propose estimators for quotes prevailing at trade time. Comparisons using 2, 800 US stocks show modeled classifications are 1 - 2 % {{more accurate than}} current methods across dates, sectors, and the spread. For Nasdaq and NYSE stocks, 1 % and 1. 3 % of improvement comes from using information strengths; 0. 9 % and 0. 7 % of improvement comes from estimating quotes. I find evidence past studies used <b>unclean</b> <b>data</b> and indications of short-term price predictability. The method may help detect destabilizing order flow. ...|$|E
40|$|Network anomaly {{detection}} {{has been a}} hot topic in the past years. However, high false alarm rate, difficulties in obtaining exact clean data for the modeling of normal patterns and the deterioration of detection rate because of “unclean ” training set always make it {{not as good as}} we expect. Therefore, we propose a novel data mining method for network {{anomaly detection}} in this paper. Experimental results on the well-known KDD Cup 1999 dataset demonstrate it can effectively detect anomalies with high true positives, low false positives as well as with high confidence than the state-of-the-art anomaly detection methods. Furthermore, even provided with not purely “clean ” data (<b>unclean</b> <b>data),</b> the proposed method is still robust and effective...|$|E
40|$|Abstract: Knowledge mining in Open-Source Software (OSS) {{brings a}} great benefit for {{software}} engineering (SE). The researchers discover, investigate, and even sim-ulate {{the organization of}} development processes within open-source communities {{in order to understand}} the community-oriented organization and to transform its advan-tages into conventional SE projects. Despite a great number of different studies on OSS data, not much attention has been paid to the data filtering step so far. The noise within <b>uncleaned</b> <b>data</b> can lead to inaccurate conclusions for SE. A special challenge for data cleaning presents the variety of communicational and development infrastruc-tures used by OSS projects. This paper presents an adaptive filter-framework support-ing data cleaning and other preprocessing steps. The framework allows to combine filters in arbitrary order, defining which preprocessing steps should be performed. The filter-portfolio can by extended easily. A schema matching in case of cross-project analysis is available. Three filters- spam detection, quotation elimination and core-periphery distinction- were implemented within the filter-framework. In the analysis of three large-scale OSS projects (BioJava, Biopython, BioPerl), the filtering led to a significant data modification and reduction. The results of text mining (sentiment analysis) and social network analysis on <b>uncleaned</b> and cleaned <b>data</b> differ signifi-cantly, confirming the importance of the data preprocessing step within OSS empirical studies...|$|R
40|$|Traffic {{classification}} {{plays the}} {{significant role in}} the network security and management. However, accurate classification is challenging if the training data is contaminated with unclean traffic. Recent researches often assume clean training data, and hence performance reduced on real-time network traffic. To meet this challenge, in this paper, we propose a robust method, Unclean Traffic Classification (UTC), which incorporates noise elimination and suspected noise reweighting. Firstly, UTC eliminates strong noisy training data identified by a consensus filtering with multiple classifiers. Furthermore, UTC estimates the relevance of remaining training data and learns a robust traffic classifier. Through a number of experiments on a real-world traffic dataset, we show that the new method outperforms existing state-of-the-art traffic classification methods, under the extremely difficult circumstance with <b>unclean</b> training <b>data...</b>|$|R
40|$|The era of {{big data}} brings new {{challenges}} to the network traffic technique that is an essential tool for network management and security. To {{deal with the problems}} of dynamic ports and encrypted payload in traditional port-based and payload-basedmethods, the state-of-the-art method employs flow statistical features and machine learning techniques to identify network traffic. This chapter reviews the statistical-feature based traffic classification methods, that have been proposed in the last decade. We also examine a new problem: unclean traffic in the training stage of machine learning due to the labeling mistake and complex composition of big Internet data. This chapter further evaluates the performance of typical machine learning algorithms with <b>unclean</b> training <b>data.</b> The review and the empirical study can provide a guide for academia and practitioners in choosing proper traffic classification methods in real-world scenarios...|$|R
40|$|Data {{cleaning}} is a pre-processing {{technique used}} in most data mining problems. The purpose of data cleaning {{is to remove}} noise, inconsistent data and errors {{in order to obtain}} a better and representative data set to develop a reliable prediction model. In most prediction model, <b>unclean</b> <b>data</b> could sometime affect the prediction accuracies of a model. In this paper, we investigate classification problem, which make use of misclassification analysis technique for data cleaning. To demonstrate our concept, we have used artificial neural network (ANN) as the core computational intelligence technique. We use three benchmark data sets obtained from the University of California Irvine (UCI) machine learning repository to investigate the results from our proposed data cleaning technique. The experimental data sets used in our experiment are binary classification problems, which are German credit data, BUPA liver disorders, and Johns Hopkins Ionosphere. The results from our experiments show that the proposed cleaning technique could be a good alternative to provide some confidence when constructing a classification model...|$|E
40|$|Mobile ad-hoc network {{security}} problems are the sub ject of in depth analysis. A group of mobile nodes area unit {{connected to a}} set wired backbone. In MA NET, the node themselves implement the network management in a very cooperative fashion. All the n odes area unit accountable to create a constellatio n that is dynamically, modification it and conjointly {{the absence of any}} clear network boundaries. We t end to project a completely unique intrusion detection mod el for mobile ad-hoc network victimization. CP-KNN (Conformal Prediction K-Nearest Neighbor) algorithm ic rule is to classify the audit knowledge for anom aly detection. The non-conformity score worth is emplo yed to cut back the classification period of time f or multi level iteration. It is effectively notice an omalies with high true positive rate, low false pos itive rate and high confidence that the progressive of assorte d anomaly detection ways. Additionally it is inter fered by “noisy” knowledge (<b>unclean</b> <b>data),</b> the projected technique is strong, effective and conjointly it re tains its smart detection performance and to avoid the ab normal activity...|$|E
40|$|In most {{classification}} problems, {{sometimes in}} order to achieve better results, data cleaning is used as a preprocessing technique. The purpose of data cleaning is to remove noise, inconsistent data and errors in the training data. This should enable the use of a better and representative data set to develop a reliable classification model. In most classification models, <b>unclean</b> <b>data</b> could sometime affect the classification accuracies of a model. In this paper, we investigate the use of misclassification analysis for data cleaning. In order to demonstrate our concept, we have used Artificial Neural Network (ANN) as the core computational intelligence technique. We use four benchmark data sets obtained from the University of California Irvine (UCI) machine learning repository to investigate the results from our proposed data cleaning technique. The experimental data sets used in our experiment are binary classification problems, which are German credit data, BUPA liver disorders, Johns Hopkins Ionosphere and Pima Indians Diabetes. The results show that the proposed cleaning technique could be a good alternative to provide some confidence when constructing a classification model...|$|E
40|$|This Thesis {{focuses on}} the {{evaluation}} of the CUBE-algorithm. This was mainly developed for the application on measured data from multibeam echosounders. The measured values are generally unevenly distributed and partly inaccurate. The aim of the algorithm is to calculate a regular grid with estimates for the water depth in fixed positions out of these values. The special thing about that is that, in addition to the actual measured values, there is extra information about the data included the calculation. These are in particular information about the precision and measuring properties of the used measuring instruments. With the help of this knowledge it is tried to determine the true water depth out of the measured values to exclude inaccurate data from the regular grid. In this thesis the algorithm was applied to deep sea multibeam echosounder data, which has been measured by research vessel polarstern and assessed regarding the reliability with which incorrect values are excluded from the grid. The polarstern is operated by the Alfred Wegener Institute and used to explore the polar seas. At the institute the programs HIPS and SIPS of the company CARIS and Fledermaus by QPS are used to clean the data manually. The programs each contain an implementation of the CUBE-algorithm. After the application of the algorithm to the measured values with both programs, the visible differences in the results were analyzed. The analysis was carried out using the program ArcMap by Esri. Using a color-coded visualization of the grid and through difference grids the optical and metric deviations {{of the results of the}} algorithm to the results of a manual data cleanup were examined and compared with each other. The analysis of the comparisons has shown, that the deviations from the grid created by the CUBE-algorithm to manually cleaned data are dropped compared to <b>uncleaned</b> <b>data.</b> That means, referred to the used deep sea multibeam echosounder data, that the algorithm succeeds in excluding a part of the inaccurate data. The reliability with which this is achieved increases with the density of accurate values. In relation to Fledermaus the resuls from HIPS and SIPS showed the lover deviations compared to the manually cleaned data...|$|R
40|$|Background: Accurate genetic {{maps are}} {{required}} for successful and efficient linkage mapping of disease genes. However, most available genome-wide genetic maps were built using only small collections of pedigrees, and therefore have large sampling errors. A large set of genetic studies genotyped by the NHLBI Mammalian Genotyping Service (MGS) provide appropriate data for generating more accurate maps. Results: We collected a large sample of <b>uncleaned</b> genotype <b>data</b> for 461 markers generated by the MGS using the Weber screening sets 9 and 10. This collection includes genotypes for over 4, 400 pedigrees containing over 17, 000 genotyped individuals from different populations. We identified and cleaned numerous relationship and genotyping errors, as well as verified the marker orders. We used this dataset to test for population-specific genetic maps, and to re-estimate the genetic map distances with greater precision; standard errors for all intervals are provided. The map-interval sizes from the European (or European descent), Chinese, and Hispanic samples are in quite good agreement with each other. We found one map interval on chromosome 8 p with a statistically significant size difference between the European and Chinese samples, and several map intervals with significant size differences between the African American and Chinese samples. When comparing Palauan with European samples, {{a statistically significant difference}} was detected at the telomeric region of chromosome 11 p. Severa...|$|R
40|$|Abstract Background Accurate genetic {{maps are}} {{required}} for successful and efficient linkage mapping of disease genes. However, most available genome-wide genetic maps were built using only small collections of pedigrees, and therefore have large sampling errors. A large set of genetic studies genotyped by the NHLBI Mammalian Genotyping Service (MGS) provide appropriate data for generating more accurate maps. Results We collected a large sample of <b>uncleaned</b> genotype <b>data</b> for 461 markers generated by the MGS using the Weber screening sets 9 and 10. This collection includes genotypes for over 4, 400 pedigrees containing over 17, 000 genotyped individuals from different populations. We identified and cleaned numerous relationship and genotyping errors, as well as verified the marker orders. We used this dataset to test for population-specific genetic maps, and to re-estimate the genetic map distances with greater precision; standard errors for all intervals are provided. The map-interval sizes from the European (or European descent), Chinese, and Hispanic samples are in quite good agreement with each other. We found one map interval on chromosome 8 p with a statistically significant size difference between the European and Chinese samples, and several map intervals with significant size differences between the African American and Chinese samples. When comparing Palauan with European samples, {{a statistically significant difference}} was detected at the telomeric region of chromosome 11 p. Several significant differences were also identified between populations in chromosomal and genome lengths. Conclusions Our new population-specific screening set maps can be used to improve the accuracy of disease-mapping studies. As a result of the large sample size, the average length of the 95 % confidence interval (CI) for a 10 cM map interval is only 2. 4 cM, which is considerably smaller than on previously published maps. </p...|$|R
40|$|The {{quality of}} data can only be {{improved}} by cleaning data prior to loading into the data warehouse as correctness of data is essential for well-informed and reliable decision making. Data warehouse is the only viable solution that can bring that dream into a reality. The quality of the data can only be produced by cleaning data prior to loading into data warehouse. Data Cleaning {{is a very important}} process of the data warehouse. It is not a very easy process as many different types of <b>unclean</b> <b>data</b> can be present. So correctness of data is essential for well-informed and reliable decision making. Also, whether a data is clean or dirty is highly dependent on the nature and source of the raw data. Many attempts have been made till now to clean the data using different types of algorithms. In this paper an attempt has been made to provide a hybrid approach for cleaning data which combines modified versions of PNRS, Transitive closure algorithms and Semantic Data Matching algorithm {{can be applied to the}} data to get better results in data corrections...|$|E
40|$|Crime {{prevention}} {{is a primary}} concern of police as they perform their central role of protecting the lives and property of citizens. But the police force is usually relatively very small compared to the crime prone population they have to protect making them more of a reactive rather than preventive force. Police often have at their disposal vast amounts of least utilised crime data (such as crime incident reports) which if analysed could reveal some hidden information such as crime committing trends useful in crime prevention. Use of Information Systems techniques such as data mining and Geographic Information Systems for analysing these data is promising in boosting the police efforts. This paper reviews the applicability of various data mining methods and Geographic Information Systems in crime analysis and visualization in mainly poor planned settings characterised by missing electronic data a common phenomena in the developing countries like Uganda. The focus is on criminality of places rather than the tracing of individual criminals. The review tends to reveal {{that a combination of}} Geographic Information Systems and data mining techniques that can work under <b>unclean</b> <b>data</b> are best suited for use in the poorly planned settings. 1...|$|E
40|$|In {{almost all}} {{successful}} time-series clustering, feature selection methods are essential. Though lots of feature selection algorithms have being developed {{from time to}} time, issues concerning partially <b>unclean</b> <b>data</b> lead to controversies. In this paper, we introduce the idea of selecting interquartile range value (IQR) to set the minimum absolute expression change filtering boundary. The Short Time-Series Expression Miner (STEM) tool is employed to ensure that reliable data which pass the boundary support the STEM clustering method analyses. The main {{aim of this study}} is to access as to whether the idea of IQR value setting could ideally compress data and at the same time retrieve the most optimum information. Besides, analysis of clustering profiles generated enable good visualization effect. This is particularly needed in conditional studies of the effect and causality relations as determinant factors to judge the level of correlations among clusters. Our analysis is implemented on a fruit-fly species (Drosophila melanogaster) expression data available from GEO Datasets. The study data consist of 14010 gene profiles recorded in four time points; which subdivided further into three major conditions: unperturbed, perturbed and active...|$|E
40|$|Large-scale, high-resolution, photometrically {{calibrated}} {{images are}} key for many astrophysical problems. The INT Photometric H?? Survey has imaged the entire northern Galactic Plane in r, i and H?? filters. However, these images {{suffer from a}} number of common imaging problems, including, most critically, large-scale gradients due to scattered moonlight. The objective of this work is to produce an automated method for cleaning this data {{so that it can be}} used to produce large-scale and reliable H?? mosaics for scientific use. We created dark-time templates to account for airglow, fringing, and other sources of dark-time counts in the images and then used a Markov Chain Monte Carlo method to fit a linear, 2 -dimensional model to the scattered moonlight. Bright stars in the images are censored from the fitted images so they do not influence the fit. Other types of model were explored, as well as a method that employed Fourier transforms to clean the data, but without fruition. The method to fit the model to the moonlight background was originally tested in the i-band, before moving onto the r-band, subtracting scaled H?? images to remove nebulosity. An empirical scaling factor was then used to translate the model fit from the r-band to the H?? band, necessary because of varying atmospheric conditions. Finally, the cleaned data were shifted onto a common zero point before mosaicking into large scale images. The result is a strong groundwork for cleaning astronomical images by accounting for the various components to sky background but preserving features of interest. The results of this process applied to images that cover supernova remnant Simeis 147 show a substantial improvement over <b>uncleaned</b> imaging <b>data.</b> We also illustrate the versatility of this process by applying it, unprepared, to other regions in the Galactic Plane...|$|R
40|$|Support Vector Machines are {{a widely}} used {{classification}} technique. They are computationally efficient and provide excellent predictions even for high-dimensional data. Moreover, Support Vector Machines are very flexible {{due to the}} incorporation of kernel functions. The latter allow to model nonlinearity, but also to deal with nonnumerical data such as protein strings. However, Support Vector Machines can suffer a lot from <b>unclean</b> <b>data</b> containing, for example, outliers or mislabeled observations. Although several outlier detection schemes have been proposed in the literature, the selection of outliers versus nonoutliers is often rather ad hoc and does not provide much insight in the data. In robust multivariate statistics outlier maps are quite popular tools to assess the quality of data under consideration. They provide a visual representation of the data depicting several types of outliers. This paper proposes an outlier map designed for Support Vector Machine classification. The Stahel [...] Donoho outlyingness measure from multivariate statistics is extended to an arbitrary kernel space. A trimmed version of Support Vector Machines is defined trimming part of the samples with largest outlyingness. Based on this classifier, an outlier map is constructed visualizing data in any type of high-dimensional kernel space. The outlier map is illustrated on 4 biological examples showing its use in exploratory data analysis. Comment: Published in at [URL] the Annals of Applied Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|E
40|$|Little doubt {{exists that}} {{technologies}} for precisely, and automatically measuring energy use are timely. Pressure to reduce greenhouse gas emissions, {{the potential for}} a generation shortfall, and rising energy prices, modern building complexities, mean that estimating building energy use patterns, profiling of building energy use, and energy failure mode identification can help to maintain energy efficiency. However, further uses exist beyond operational management; These include urgently required meta-analysis of building stock by sector, up to national stock levels, to inform policymakers, since a dearth of national stock data exists at government level in many countries. Many existing systems use radio telemetry, often producing <b>unclean</b> <b>data.</b> Analysis of energy data for large datasets becomes expensive due to incompatible formats, hampering use of old data on new systems, and data from different systems should we acquire new data, or as we acquire physical buildings. We argue that a standard should include basic specifications for fundamentals such as date formats, but a secondary scalable layer will allow future-proofing of datasets for longitudinal study, and open the door to advanced analysis techniques such as complex event processing. Disaggregation to plant level, as well as building related activities, such as manufacturing activity, also becomes possible with a scalable data structure. This paper, proposes a framework for an energy data standard from a data analysis perspective built around four areas: Temporal, accuracy & precision, operational and energy documentation. 1...|$|E
40|$|Good data {{preparation}} {{is a key}} prerequisite to successful data mining [P 99]. Conventional wisdom suggests that {{data preparation}} takes about 60 to 80 % of the time involved in a data mining exercise [R 97]. There have been good reviews of {{the problems associated with}} data preparation [F 97, HS 98 and MS 97]. However the data cleaning aspect of data preparation is regarded as involving major human input and often has been neglected in practice. This paper reports work undertaken in support of a data mining programme at Rutherford Appleton Laboratory (RAL). It proposes a Clean Views Model for data cleaning. The action taken to clean data depends on the business purpose of the analysis. The cleaning action also depends on what you believe about the data expressed in the business rules that the data need to satisfy. These rules are expressed in a validator function. What you believe about the data can change during the data mining process and also business purposes change through time. This means that data cleaning should be described as an iterative exercise rather than a one-off procedure. We contend that the original, <b>unclean</b> <b>data</b> should be retained and that the cleaning produces a particular view, a Clean View, for a particular validation function. We have developed the Clean Views approach after looking at several datasets. We report some work on the flow of funds data associated with transitional economies [Ba 99]. These data were generated by the central banks of Eastern European countries and are used to assess their movement towards a market economy. We also report work done on data in an international mailing list concerned with a survey of alumni of business schools who have graduated as Masters of Business Administration...|$|E

