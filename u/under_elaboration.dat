22|22|Public
5000|$|The Rural Water Supply Master Plan, {{announced}} in 2008, is currently <b>under</b> <b>elaboration.</b>|$|E
5000|$|It {{required}} administrative {{documents that}} were not <b>under</b> <b>elaboration</b> to be communicated to [...] "those who asked for it".|$|E
40|$|AbstractThe present {{material}} {{proposes to}} bring into discussion both several European civil law instruments and legislative acts <b>under</b> <b>elaboration</b> {{at the level of}} the European Commission. These tend to eliminate the incompatibility obstacles met with different juridical systems, so that the European citizens can benefit from the justice access, freedom and security. The base of this cooperation consists in mutual agreement between the national juridical authorities of the orders and decisions taken in a member state and the execution and appliance of them in other member state...|$|E
40|$|Existing {{evidence}} for affect's influence on information processing and choice <b>under</b> high <b>elaboration</b> is mixed. In addition, affective choice is {{often viewed as}} erroneous {{in that it is}} assumed to lead to regret. We show that affect has a reliable impact on choice <b>under</b> high <b>elaboration,</b> which occurs through a combination of heuristic and systematic processing. Furthermore, consumers were able to correct for the impact irrelevant affect had on systematic processing but not for its impact on less conscious heuristic processing. Finally, affective purchases led to greater long-term satisfaction for important purchases, suggesting that affective choice can be functional. (c) 2006 by JOURNAL OF CONSUMER RESEARCH, Inc [...] ...|$|R
5000|$|<b>Under</b> high <b>elaboration,</b> a given {{variable}} (e.g., expertise) {{can serve}} as an argument (e.g., [...] "If Einstein agrees with the theory of relativity, then this is a strong {{reason for me to}} as well") or a biasing factor (e.g., [...] "If an expert agrees with this position it is probably good, so let me see who else agrees with this conclusion"), at the expense of contradicting information. Under low-elaboration conditions, a variable may act as a peripheral cue (e.g., the belief that [...] "experts are always right"). While this is similar to the Einstein example above, this is a shortcut which (unlike the Einstein example) does not require thought. <b>Under</b> moderate <b>elaboration,</b> a variable may direct the extent of information processing (e.g., [...] "If an expert agrees with this position, I should really listen to what (s)he has to say").|$|R
40|$|In this study, six educationally {{disadvantaged}} {{children were}} taught beginning letter sounds under two teaching conditions. After a baseline of no intervention, a single subject alternating treatments design {{was used to}} compare contingent elaborations and token reinforcement within children. Performance between treatments was analyzed in terms of cumulative number of letter sounds learned, total number of letter sounds learned, and maintenance of learning. Token probes were implemented to ascertain whether tokens remained functionally reinforcing {{over the course of the}} study. Five children responded to treatment over baseline. Three of these, characterized by above average Wepman auditory discrimination scores, performed better <b>under</b> <b>elaborations</b> until the final third of the study when differential performance between treatments was less pronounced. Remaining subjects, characterized by below average auditory discrimination, showed similar learning under both treatments or, as in the case of one child, no learning. No differences in maintenance were observed. Implications for the classroom and suggestions for further research were discussed...|$|R
40|$|Abstract — Because {{organizations}} create {{many different}} and irrelevant documents, which are <b>under</b> <b>elaboration</b> and intended for regulating the particular organization’s business processes (OBP) {{with the aim}} of efficient and effective management implementation over this organization, the issues of business processes construction are studied on the basis of intellectual analysis of texts of documents that describe the processes taking place within the organization such as job descriptions, agreements, specifications and standards, regulations and time limits of works, etc. Index Terms — Automatic categorization, business processes and intelligent analysis I...|$|E
40|$|The cam-follower pair is a {{very complex}} {{lubricated}} contact because its continuous variation of load, speed and radius of curvature. Experimental verification are necessary but very difficult to perform. After a literature review of the existing test rigs for cam-follower simulation, the main aspects of a new versatile apparatus are presented. Several design variations are presented, each one with its advantages and drawbacks. The final project will take into account some of the presented design solutions and the results of dynamic simulations of the rigs currently <b>under</b> <b>elaboration...</b>|$|E
40|$|International audienceSTEP is an ISO {{standard}} (ISO- 10303) for the computer-interpretable {{representation and}} exchange of product data. Parts of STEP standardize conceptual structures and usage {{of information in}} generic or specific domains. The standardization process of these constructs is an evolutionary approach, which uses generated prototypes at different phases of the process. This paper presents a method for the building of prototype generators, inspired by this standardization process, together with a tool used to support the method. Throughout stages of model integration, the embedded logic of prototype generators is defined. The successive stages fo-cuse on data model integration but this integration relies on a common agreement about construct functionalities <b>under</b> <b>elaboration...</b>|$|E
40|$|On the 17 th of November 2015, an {{earthquake}} of Mw 6. 4 hit the western Greek island of Lefkada, located in Ionian Sea, {{an area that}} is well known for its active tectonics. A second earthquake of Mw 5. 0 successively followed. These events induced rock falls and landslides having as consequences two life losses and extensive damages to roads and buildings. Shortly after the events, BEYOND acquired a set of Sentinel- 1 TOPSAR scenes, one before and one after the event. The images were combined to form an interferogram that depicts ground deformation due to the earthquake events. The fringes of the preliminary interferometric results, which are <b>under</b> further <b>elaboration,</b> reveal ground deformation of the order of ~ 20 cm along the Line of Sight at the western part of Lefkada Island. As smaller deformation field is also apparent at the northern part of Cephalonia Island...|$|R
40|$|Past {{research}} suggests that <b>under</b> high <b>elaboration</b> conditions, source cred-ibility can play more than one role in persuasion. In particular, source credibility can affect the valence of people’s thoughts generated in re-sponse to persuasive messages or it can affect the confidence with which people hold those thoughts. In the present research, two experiments ex-plore {{the conditions under which}} these conceptually distinct effects occur. It is demonstrated that the effect of source credibility on thought confi-dence is dominant when source information follows, rather than precedes, a persuasive message. When source information precedes a message, it affects the valence of issue–relevant thinking. Considerable research attention has been devoted to understanding the effects of source credibility (expertise and trustworthiness) on persuasion (Petty & Wegener, 1998; Pornpitakpan, 2004). Consistent with the predictions of multi–process theories such as the elabora-tion likelihood (Petty & Cacioppo, 1986) and heuristic–systematic (Chaiken, Liberman, & Eagly, 1989) models, it has been discovered that source credibility can affect persuasion through a variety of dis...|$|R
40|$|The {{construction}} {{of thousands of}} enterprises throughout the USSR is planned each year. The location {{of the region and}} site of each of them requires in-depth economic calculations that take into account a broad spectrum of natural, historical, national, and economic factors. They exert varying influence on the effectiveness of a variant <b>under</b> consideration. The <b>elaboration</b> of long-range plans for the location of the productive forces of the nation and of the various regions is based on scientific principles that reflect the action of the objective laws of development of socialist society. These principles improve with the growth of social production. ...|$|R
40|$|International audienceThere is a {{need for}} agent {{oriented}} software engineering methodologies that support the conceptual modeling of mobile-agents systems. For this reason, we have presented in a previous work, our meta-model to design multi-agents systems including mobile agents and we have discussed it versus some formalisms extending UML for mobile-agents modeling. The proposed meta-model serves as a platform independent meta-model in our model-driven engineering approach <b>under</b> <b>elaboration</b> as a methodology for the development of multi-agents systems including mobile-agents. This paper summarizes the different approaches for mobileagent modeling and situates our meta-model particularly versus three works supporting mobility by extending a multi-agents systems methodology (MaSE, GAIA, and AALAADIN). It aims to justify the choices that have guided our meta-model construction...|$|E
40|$|This paper {{describes}} the basic concepts for the Image Interchange Format (IIF) {{for the first}} International Image Processing and Interchange Standard (IPI), which is <b>under</b> <b>elaboration</b> by ISO/IEC JTC 1 /SC 24 (International Standards Organization/International Electronics Commission, Joint Technical Committee, Computer Graphics) - i. e., "information processing"/"computer graphics" - committee work. Starting {{with a discussion of}} existing image formats and surrent image interchange practices, this study outlines the need for a new approach to a general image interchange format. A requirements list and corresponding design goals for th IIF are presented. Finally, the relation to the other parts of the IPI standard are described. The authors are coworkers and contributors to the relevant committees within the ISO/IEC and the DIN German Institute for Standardization (DIN Deutsches Institut für Normung) ...|$|E
40|$|Growing {{awareness}} about requirements for healthy indoor air {{has resulted in}} a demand for products demonstrated to be safe for use in indoor environments. Emissions from construction products have been identified as a significant source of indoor air pollution {{since the beginning of the}} 1980 ¿s. Different approaches to evaluate construction products have emerged over time, and considerable practical experience has been gained during recent years. In some markets, emissions originating from indoor construction products have been noticeably reduced by developing quality criteria and labelling systems. A detailed review of the existing labelling schemes was compiled in 2005 in the European Collaborative Action, Report 24. This paper reports on ongoing activities concerningthe development of a harmonised evaluation concept that is <b>under</b> <b>elaboration</b> by the Danish ICL, the German AgBB, the Finnish M 1 and the JRC/IHCP/PCE. JRC. I. 2 -Chemical assessment and testin...|$|E
40|$|Of course it's true: I {{saw it on}} the Internet ” (Graham & Takis Metaxas, 2003) The {{question}} whether information is true or accurate has been central to societies throughout history and is especially so in our information-driven society. Theoretically, this question should be answered based on an individual’s personal knowledge; however, this is often impossible {{because of lack of}} knowledge. Therefore, in order to solve this impasse one has to pose a surrogate question: whether the information is credible, ("capable of being believed; trustworthy " Merriam-Webster Online Dictionary, 2005), a "trade off " formulated by Petty’s and Cacioppo (1986) <b>under</b> their <b>Elaboration</b> Likelihood Model which served as the main theoretical background for this study. Assigning quality to information is also dependent on its veracity therefore, other mechanisms that enable the user to define quality is brought into play. Indeed, in the Web context, such mechanism promoted by many libraries and librarians is the “checklist ” where certain predefined attributes are checked; sites and information are considered of quality...|$|R
40|$|The present {{research}} introduces a new {{mechanism by which}} emotion can affect evaluation. On {{the basis of the}} self-validation hypothesis (R. E. Petty, P. Briñol, & Z. L. Tormala, 2002), the authors predicted and found that emotion can influence evaluative judgments by affecting the confidence people have in their thoughts to a persuasive message. In each study, participants first read a strong or weak persuasive communication. After listing their thoughts about the message, participants were induced to feel happy or sad. Relative to sad participants, those put in a happy state reported more thought confidence. As a consequence, the effect of argument quality on attitudes was greater for happy than for sad participants. These self-validation effects generalized across different emotion inductions, different persuasion topics, and different measures of thought confidence. In one study, happy and sad conditions each differed from a neutral affect control. Most important, these metacognitive effects of emotion only occurred <b>under</b> high <b>elaboration</b> conditions. In contrast, individuals with relatively low motivation to think showed a main effect of emotion on attitudes, regardless of argument quality...|$|R
40|$|We report 2 {{studies that}} {{examined}} how {{the strength of}} humorous advertising executions and their relevance to the brand claims in the advertisement influence consumer memory for the claims. We infer the underlying memory processes by testing claims memory using recall, recognition, and indirect tests following incidental exposure to advertisements manipulating humor strength and claims relevance. Memory for the humor component was checked as corroborating evidence. We also validated these inferences by contrasting these effects on claims and humor memory with those <b>under</b> instructed <b>elaboration.</b> Study 1 shows that for humor of low claims relevance, brand claims memory is an inverted U-shaped function of humor strength. Compared to both nonhumor and high-strength humor, moderate humor facilitates both encoding and retrieval of the claims. The patterns of humor memory and instructed elaboration effects suggest that low-relevance humor is not spontaneously linked to the claims even when processing resources are available. Study 2 shows that when strong humor is made more relevant, brand claims memory improves even during incidental exposure. Corresponding humor memory and instructed elaboration effects imply that relevance encourages the formation of humor-claims links that facilitate encoding and retrieval of the claims. The results show that althoug...|$|R
40|$|The Image Interchange Format (IIF) {{is part of}} {{the first}} International Image Processing and Interchange Standard (IPI), which is <b>under</b> <b>elaboration</b> by ISO/IEC JTC 1 /SC 24. The IIF {{comprises}} both a data format definition and a functional gateway specification. This paper focuses on the design principles of the IIF data format intended to be used for image communication within and across application domains. It discusses the need for a new format and gives an introduction to the IIF. It concentrates, in particular, on technical issues concerning image data structures and encoding/compression methods provided by the IIF data format. The relation of the IIF to de-facto formats like TIFF and to the Open Document Architecture (ODA) is described. Furthermore, the relation to the reference model for open communication (OSI) and current application scenarios is pointed out...|$|E
40|$|Providing {{integrated}} access to multiple, distributed and heterogeneous data sources {{has become one}} of the major issue in database research and industry. Being a major European Esprit project started in 1993, IRO-DB is experimenting an on-demand solution to this problem. IRO-DB is an object-oriented federated database system to access multiple data sources from an ODMG compliant C++ interface. The system encompasses several components, including local database adapters to homogenize local data sources, a remote object access component to query and transfer collections of objects from site to site, and a mediator to define integrated views, decompose and optimize queries, and combine results. A first version is currently operational with a simplified query optimizer not using a cost model. This paper gives an overview of the IRO-DB architecture and describes in details the cost evaluator currently <b>under</b> <b>elaboration</b> for the next version of the distributed query optimizer. The cost model i [...] ...|$|E
40|$|We review {{some basic}} {{issues of the}} life-prescribed {{development}} of the Earth's system and the Earth's atmosphere and discourse the unity of Earth's type of life in physical and transcendental divisions. In physical division, we exemplify and substantiate the origin of atmospheric phenomena in the metabolic pathways acquired by the Earth's life forms. We are especially concerned with emergence of pro-life superficial environments <b>under</b> <b>elaboration</b> of the energy transformations. Analysis of the coupling phenomena of elaborated ozone-oxygen transformation and Arctic bromine explosion is provided. Sensing is a foundation {{of life and the}} Earth's life. We offer our explanation of human-like perception, reasoning and creativity. We suggest a number of propositions about association of transcendental and physical divisions and the purpose of existence. The study relates to the tradition of natural philosophy which it follows. The paper is suitable for the popular reading. Comment: 56 pages, incl. 6 excerpts from Plato, Leibniz, Goethe, Franz Marc and Lynn Marguli...|$|E
40|$|The Simplifying Conditions Method (SCM) {{is a set}} of {{guidelines}} for task analysis and sequencing of instructional content <b>under</b> the <b>Elaboration</b> Theory (ET). This article introduces the fundamentals of SCM and presents the findings from a formative research study on SCM. It was conducted in two distinct phases: design and instruction. In the first phase, the SCM process was used to design a course {{in order to determine the}} weaknesses of the SCM process. In the second phase, the course was taught using the SCM sequence in order to determine the weaknesses of SCM principles. Results suggest that the current SCM process is workable; however, three recomriendations were offered: (1) the need to stress the holistic rather than the step-by-step approach; (2) the need to add more detailed prescriptions; and (3) the need for formative evaluation. Second, the study yielded no critical weaknesses that might lead to possible improvements in the SCM principles, since most of the learners were very satisfied with the sequence of the instruction. Three figures present results. (Contains 34 references.) (AEF) Reproductions supplied by EDRS are the best that can be made from the original document...|$|R
5000|$|In {{designing}} {{a test for}} the aforementioned model, {{it is necessary to}} determine the quality of an argument, i.e., whether it is viewed as strong or weak. If the argument is not seen as strong, then the results of persuasion will be inconsistent. A strong argument is defined by Petty and Cacioppo as [...] "one containing arguments such that when subjects are instructed to think about the message, the thoughts they generate are fundamentally favorable." [...] An argument that is universally viewed as weak will elicit unfavorable results, especially if the subject considers it <b>under</b> high <b>elaboration,</b> thus being the central route. Test arguments must be rated by ease of understanding, complexity and familiarity. To study either route of the elaboration likelihood model, the arguments must be designed for consistent results. Also, when assessing persuasion of an argument, the influence of peripheral cues needs to be taken into consideration as cues can influence attitude {{even in the absence of}} argument processing. The extent or direction of message processing also needs to be taken into consideration when assessing persuasion, as variables can influence or bias thought by enabling or inhibiting the generation of a particular kind of thought in regard to the argument. [...] "While the ELM theory continues to be widely cited and taught as one of the major cornerstones of persuasion, questions are raised concerning its relevance and validity in 21st century communication contexts." ...|$|R
40|$|In Italy, Ecotoxicology {{has found}} a place with the Legislative Decree n. 152 (May 11, 1999), emanated in {{fulfilment}} of the Directives 91 / 271 /CEE "urban waste-water treatment" and 91 / 676 /CEE "protection of waters against pollution caused by nitrates from agricultural sources". This decree in reality goes beyond (actually anticipating {{the content of the}} Framework Directive on Water, still <b>under</b> way of <b>elaboration),</b> and charges the Regions with the duty to identify, for all and each water body, the class of quality {{on the basis of a}} chemical and biological monitoring and their classification according to the environmental quality objectives. To this aim, for all water bodies (lakes, rivers, groundwater, coastal waters) the ecological, chemical, and environmental status must be assessed by measuring specific parameters. This paper briefly summarises the role of biological monitoring in the classification of waters in five different categories, ranking from High to Foul Environmental status...|$|R
40|$|New {{theories}} on emotion emphasize the tight boundaries among emotion, motivation and cognition: {{according to them}} people are motivated to respond to events differently depending on how they have been appraised. The present study tries to examine what kind of emotional responses the subject is motivated to express while interacting with an artificial agent if he/she believes that this agent is able to understand his/her emotional states. For this purpose different kind of computer games were projected to elicit specific emotional appraisals and two different conditions were used: in one condition the avatar provided a simulated intelligent feedback, while in the second only guided the subject across the different tasks. Multimodal synchronized data were captured. All video tapes were codified frame by frame using The Observer 5. 0 and THEME Software. This is an in-progress study and data are still <b>under</b> <b>elaboration.</b> This paper includes initial results of the analysis of non-verbal communicative signals...|$|E
40|$|This report {{deals with}} the image data model and the Image Interchange Format (IIF) for the first International Image Processing and Interchange Standard (IPI), which is <b>under</b> <b>elaboration</b> by ISO/IEC JTC 1 /SC 24 - i. e. "information processing"/"computer graphics" - {{committee}} work. The authors are constributors and co-workers within the ISO/IEC committee and the DIN German Instituts for Standardization (DIN Deutsches Institut für Normung). The paper focuses on the image data model which describes image data structures and image attributes. Finally, the paper reports on the prospective design of an Image Interchange Format (IIF) which will be {{one part of the}} IPI standard and, thus, will be based on the common image data model. Parts of this report are included in the paper "Requirements and Basic Concepts for the first International Imaging Standsrd" by Blum, Hofmann, and Krömker, which has been submitted to the journal "IEEE Computer Graphics & Applications" for publication...|$|E
40|$|IRO-DB is an {{object-oriented}} federated database {{system to}} access multiple data sources from an ODMG compliant C++ inteqace. The system encompasses several components, including local database adapters to homogenize local data sources, a remote object access component to query and transfer collections of objects from site to site, ana ’ a mediator to de$ne integrated views, decompose ana’ optimize queries, and combine results. This paper gives {{an overview of}} the IRO-DB architecture and describes in detail the cost evaluator currently <b>under</b> <b>elaboration</b> for the next version of the distributed query optimizer. The cost model is composed of a set of mathematical formulas with coejficients to estimate the cost of the search operators. The coeficients are deduced from a calibrating objectoriented database composed of linked collections of objects. A tuning application is run on each local site to adjust the cost formulas and fix the coeficients. We report on the tuning of 02 and ObjectStore. We show that the estimation is quite accurate for path traversals with the 007 benchmark on top of ObjectStore. 1...|$|E
40|$|It {{has been}} argued that consumers' {{resistance}} to counter-attitudinal challenges increases with the amount of information possessed about the target brand and with the elaboration of this information. Five experiments exhibit why this may not necessarily be the case. Experiments 1 and 2 show that the favorability of post-challenge brand evaluations does not strictly depend on the amount of target information possessed, but on the part-worth of the target information that is accessed {{at the time of the}} challenge. Therefore, resistance to challenges may sometimes be higher when the individuals possess less information about the target. Experiments 3 and 4 show that reliance on prior target information is a function of the diagnosticity of the challenge. Therefore, the amount of target information previously learned may matter little when the challenge is in a diagnostic comparative format. Experiment 5 shows that the diagnosticity of the challenge also depends on its commensurability with previously learned target information. <b>Under</b> high commensurability, <b>elaboration</b> of the target information may lower resistance to noncomparative challenges from a superior competitor...|$|R
40|$|On September 8, 2017 the International Convention for the Control and Manage-ment of Ships’ Ballast Water and Sediments (BWMC) {{adopted in}} 2004 will enter into force. It imposes {{a lot of}} {{requirements}} on shipowners and port states. The aim {{of this article is}} to elaborate on the possible solutions that may be adopted in Polish ports as precau-tionary measures in the case of non-compliance with the provisions of BWMC. The article starts with a brief overview of BWMC and ballast water quality stand-ards. Further, it discusses the possible implications of not meeting the ballast water quality standards <b>under</b> BWMC. The <b>elaboration</b> of potential solutions and mitigation measures in the event of non-compliance with the BWMC constitutes the main part of the article. These are crucial to developing a port contingency plan and include, for example, shore-based reception facility for ballast water, mobile ballast water treatment systems, and using potable water. The article ends with a brief analysis of a possible fee systems for reception of ballast water. The research was based on a comprehensive analysis of the Convention and related legal documents, interviews with ports’ representatives as well as e-mail interviews with maritime authorities in the Baltic Sea countries. ...|$|R
40|$|Abstract Evolution of diet-derived sexual orna-ments—some of {{the most}} {{spectacular}} and diverse traits in the living world—highlights the gap between modern evo-lutionary theory and empirical data on the origin and inheritance of complex environment-dependent traits. Spe-cifically, current theory offers little insight into how strong environmental contingency of diet-dependent color bio-synthesis and environmental variability in precursor supply can be reconciled with extensive evolutionary elaboration, diversification, and convergence of diet-dependent displays among animal taxa. Moreover, biosynthetic pathways of diet-derived displays combine seemingly irreconcilable robustness, lability, and modularity to facilitate <b>elaboration</b> <b>under</b> variable environmental conditions. Here I show that an ontogenetic decrease in the predictability of an associa-tion between organismal and environmental components of color biosynthesis and the corresponding evolutionary transition from short-term epigenetic inheritance of peripheral biosynthetic components to genetic inheritance {{of the most}} reliable upstream components link the causes of developmental variation with the causes of inheritance in diet-derived displays. Using carotenoid-based colors as an empirical model, I outline general principles of a testable evolutionary framework of diversification and functional robustness of diet-derived displays, and suggest that such a framework provides insight into the foundational question of evolutionary biology—how to connect causes of within-generation developmental variation with causes of among-generation and among-taxa variation and thus with causes of evolution...|$|R
40|$|By linking {{nature and}} culture in a {{relationship}} of mutual presupposition or basic indistinctiveness, the framework of buen vivir seems to offer a “sustainable” alternative to the exploitation of the biophysical world and human communities that neoliberal globalization is taking to the extreme. Buen vivir, however, is an “invented tradition”, still <b>under</b> <b>elaboration.</b> Indigenous cultures are read {{in the light of}} most recent developments in social theory, which connect emancipatory practices and deconstruction of classic ontologies. On their side, new post-constructivist ontologies (this contribution focuses on feminist “new materialism”) build to a remarkable extent on the conceptualizations of nature provided by technoscience, which in their turn are attuned to neoliberal rationality. One may wonder, therefore, if the post-constructivist approach is pointing to a target of decreasing significance, while failing to acknowledge its own alignment with the vision of nature that underlies neoliberal policies. In any case, buen vivir remains a promising framework, especially in regard to its grafting onto a worldview that differs profoundly from the one which underpins both the greedy individualism of neoliberalism and the decentred post-humanism of contemporary social theory...|$|E
40|$|This paper gives a {{technical}} {{description of the}} new Imnational Image Processing and Interchange Standard (IPirst International Image Processing and Interchange Standard (IPI), which is <b>under</b> <b>elaboration</b> by ISO/IEC JTC 1 /SC 24. Considering the lacks and drawbacks of existing formats and current practice in exchanging digital images, {{the need for a}} new and more general approach to an image interchange format is illustrated. The IIF's data format is based on the IPI's image data model which describes data structures for images, image-related non-image data such as histograms, and image attributes for the definition of colourimetric and geometric properties. It addresses all raster graphics and prepress formats, telecommunication formats and the interchange of images among different application domains. It represents a superset of existing domain-specific image formats {{in such a way that}} the conversion of any popular de-facto standard image format into the IIF requires a minimum of computati on and works information-preserving. Besides the data format specification, the IIF also encompasses functionality for generating and parsing image data, for compressing and decompressing image data, and for the import/export of image data among the IIF, the Programmer's Imaging Kernel System (PIKS) and the application program. For the compression of image data, IIF generators provide mechanisms to apply appropriate compression schemes like JPEG or Group 3 facsimile to images or image parts...|$|E
40|$|The Cretaceous/Tertiary Boundary (KTB) bolide {{impactor}} {{that allegedly}} created the Chixulub crater in Yucatan, should {{have produced a}} series of events that must be recorded in the sedimentary sections of the surrounding areas. Cuba is an excellent spot to study these sediments and events, as in the territory of the island, there are abundant deposits of the Latest Cretaceous and Paleocene. Especially some of these rocks have been discussed recently as probably associated with the impact and related events (Pszczolkowski, 1986; Iturralde-Vinent, 1992). A joint Cuban-Japanese international project started in 1997, with the aim of identifying the KTB rocks, improve its dating, characterize its composition and origin, and model the sediment-producer events. Some of the initial results of this project have been published in part (Tada et al., 1998; Matsui et al., Kiyokawa et al., 1998), but many new data are still <b>under</b> <b>elaboration.</b> In this paper we would like to present some of the preliminary results, as an update report, until the analysis of the large number of samples collected and extensive field observations is completed. Suspected KTB sediments have been located in different geological context of Cuba, as in the Guaniguanico terrain detached from the Yucatan borderland (Cacarajícara Formation and Moncada calcarenite), in the southern paleomargin of the Bahamas platform (Lutgarda and Amaro Formations), and above the extinct Cretaceous volcanic arc (Peñalver and Mícar...|$|E
40|$|The National Museum of Australia (NMA) is garrulous, {{colorful}} and melodramatic, while {{being one of}} the few occasions of serious civic discourse in the medium of building in this country. The buildings which make up the museum mix populism with architectural erudition, and yet somehow remain unfamiliar, surprising and often humorous. While appearing to be assembled from a salvage yard of cultural allusions, architectural motifs and visual puns, as buildings they remain unprecendented, conscious architectural inventions of the highest order. Those who think of architecture as a kind of elegant ordering of conditions and requirements will find the NMA willfully perverse, because {{the fact that it is}} a simple, efficient and well considered building is largely invisible <b>under</b> a delirious <b>elaboration</b> of the building 2 ̆ 7 s own conceptual system. In this uninterest in mundane tasks, and in its contrived spatial effects and conceptual elaboration, the NMA is quite like Baroque architecture. I don 2 ̆ 7 t mean this comparison in a stylistic sense, what interests me at the NMA is a certain Baroqueness in the comportment of the architect in relation to the work. This is especially apparent in the status given to geometry and a pessimistic but unceasing attempt to make meaning...|$|R
40|$|International audienceTechnologies such as aerial {{photogrammetry}} allow production of 3 D topographic data including complex environments such as urban areas. Therefore, {{it is possible}} to create High Resolution (HR) Digital Elevation Models (DEM) incorporating thin above ground elements influencing overland flow paths. Even though this category of big data has a high level of accuracy, there are still errors in measurements and hypothesis <b>under</b> DEM <b>elaboration.</b> Moreover, operators look for optimizing spatial discretization resolution in order to improve flood models computation time. Errors in measurement, errors in DEM generation, and operator choices for inclusion of this data within 2 D hydraulic model, might influence results of flood models simulations. These errors and hypothesis may influence significantly flood modelling results variability. The {{purpose of this study is}} to investigate uncertainties related to (i) the own error of high resolution topographic data, and (ii) the modeller choices when including topographic data in hydraulic codes. The aim is to perform a Global Sensitivity Analysis (GSA) which goes through a Monte-Carlo uncertainty propagation, to quantify impact of uncertainties, followed by a Sobol' indices computation, to rank influence of identified parameters on result variability. A process using a coupling of an environment for parametric computation (Prométhée) and a code relying on 2 D shallow water equations (FullSWOF 2 D) has been developed (P-FS tool). The study has been performed over the lower part of the Var river valley using the estimated hydrograph of 1994 flood event. HR topographic data has been made available for the study area, which is 17. 5 km 2, by Nice municipality. Three uncertain parameters were studied: the measurement error (var. E), the level of details of above-ground element representation in DEM (buildings, sidewalks, etc.) (var. S), and the spatial discretization resolution (grid cell size for regular mesh) (var. R). Parameter var. E follows a probability density function, whereas parameters var. S and var. R. are discrete operator choices. Combining these parameters, a database of 2, 000 simulations has been produced using P-FS tool implemented on a high performance computing structure. In our study case, the output of interest is the maxima...|$|R
40|$|UMR AGAP - équipe DAAV (Diversité, {{adaptation}} et amélioration de la vigne) Grapevine yield sustainability under {{global warming}} is a major issue. A hypothesis is that long-term elevated temperatures may cause the failure of key phases of reproductive development, through their negative impact on carbon balance. However, testing the specific role of plant carbon status on yield <b>elaboration</b> <b>under</b> elevated temperatures is difficult on perennial crops, such as grapevine, and when environment fluctuates. To overcome these difficulties, the present work was conducted under fully controlled environment using Microvine, a new model for grapevine genetics and physiological studies. Five experiments were performed ingrowth chambers, applying contrasted day/night temperature over a one- to two-month period. Plant phenology, vegetative development and yield components were assessed before and after temperature treatments. In addition, the biomass growth and sugar contents of all individual above-ground organs and of roots were determined at harvest. High temperatures caused inflorescence abscission, but did not change flower and berry numbers on the remaining bunches. Flowering and véraison were also delayed by warm temperatures, on a thermal time basis. In contrast, plant phyllochron and biomass accumulation were unchanged. Carbon status in roots was altered under high temperatures, thus indicating carbon gain was impaired with sink demand. These results suggest that dynamics of carbohydrate pool within perennial organs act as an adjusting variable to buffer changes in carbon balance. This study provides the first basis for a modelling approach of yield responses to carbon balance under climate change...|$|R
