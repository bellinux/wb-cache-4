4521|1431|Public
25|$|The {{three major}} {{learning}} paradigms each correspond {{to a particular}} learning task. These are supervised learning, <b>unsupervised</b> <b>learning</b> and reinforcement learning.|$|E
25|$|Tasks {{that fall}} within the {{paradigm}} of <b>unsupervised</b> <b>learning</b> are in general estimation problems. The applications include clustering, the estimation of statistical distributions, compression and filtering.|$|E
25|$|SVC is {{a similar}} method that also builds on kernel {{functions}} but is appropriate for <b>unsupervised</b> <b>learning</b> and data-mining. It is considered a fundamental method in data science.|$|E
40|$|<b>Unsupervised</b> {{invariance}} <b>learning</b> {{of transformation}} sequences {{in a model}} of object recognition yields selectivity for non-accidental properties. Front. Comput. Neurosci. 9 : 115. doi: 10. 3389 /fncom. 2015. 00115 <b>Unsupervised</b> invariance <b>learning</b> of transformation sequences in a model of object recognition yields selectivity for non-accidental propertie...|$|R
40|$|This letter {{presents}} a novel <b>unsupervised</b> competitive <b>learning</b> rule called the boundary adaptation rule (BAR), for scalar quantization. It is shown both mathematically and by simulations that BAR converges to equiprobable quantizations of univariate probability density functions and that, in this way, it outperforms other <b>unsupervised</b> competitive <b>learning</b> rules. status: publishe...|$|R
40|$|A new <b>unsupervised</b> {{competitive}} <b>learning</b> rule is introduced, {{called the}} kernel-based Maximum Entropy learning Rule (kMER), for equiprobabilistic topographic map formation. The application envisaged is density-based clustering. An empirical study is conducted {{to compare the}} clustering performance of kMER {{with that of a}} number of other <b>unsupervised</b> competitive <b>learning</b> rules. status: publishe...|$|R
25|$|Tu, K. and Honavar, V. (2011). On the Utility of Curricula in <b>Unsupervised</b> <b>Learning</b> of Grammars. In: Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence (IJCAI 2011) pp.1523–1528.|$|E
25|$|Tu, K. and Honavar, V. (2012). Unambiguity Regularization for <b>Unsupervised</b> <b>Learning</b> of Probabilistic Grammars. In: Proceedings of EMNLP-CoNLL 2012 : Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. pp.1324–1334.|$|E
25|$|To {{overcome}} this problem, Schmidhuber's multi-level hierarchy of networks (1992) pre-trained one level {{at a time}} by <b>unsupervised</b> <b>learning,</b> fine-tuned by backpropagation. Behnke (2003) relied only on {{the sign of the}} gradient (Rprop) on problems such as image reconstruction and face localization.|$|E
30|$|Applying <b>unsupervised</b> feature <b>learning</b> {{to input}} data using either RICA or SFT, {{improves}} clustering performance.|$|R
5000|$|Supervised and <b>unsupervised</b> machine <b>learning</b> {{tools for}} data, {{images and sounds}} {{including}} artificial neural networks ...|$|R
5000|$|Best Paper: Object Class Recognition by <b>Unsupervised</b> Scale-Invariant <b>Learning,</b> Rob Fergus, Pietro Perona, and Andrew Zisserman ...|$|R
25|$|As an example, in an {{extension}} of a theory for <b>unsupervised</b> <b>learning</b> of invariant visual representations to the auditory domain and empirically evaluated its validity for voiced speech sound classification was proposed. Authors empirically demonstrated that a single-layer, phone-level representation, extracted from base speech features, improves segment classification accuracy and decreases the number of training examples in comparison with standard spectral and cepstral features for an acoustic classification task on TIMIT dataset.|$|E
25|$|When {{data are}} not labeled, {{supervised}} learning is not possible, and an <b>unsupervised</b> <b>learning</b> approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The clustering algorithm which provides an improvement to the support vector machines is called support vector clustering and is often used in industrial applications either when {{data are not}} labeled or when only some data are labeled as a preprocessing for a classification pass.|$|E
25|$|To {{implement}} the ideas described in previous sections, one {{need to know}} how to derive a computationally efficient invariant representation of an image. Such unique representation for each image can be characterized as it appears by a set of one-dimensional probability distributions (empirical distributions of the dot-products between image and a set of templates stored during <b>unsupervised</b> <b>learning).</b> These probability distributions in their turn can be described by either histograms or a set of statistical moments of it, as it will be shown below.|$|E
30|$|Apply {{repeatable}} <b>unsupervised</b> machine <b>learning</b> {{techniques to}} these data as evidence-based characterisation {{of the total}} cloud computing landscape.|$|R
30|$|As {{shown in}} Table 2, the {{performance}} of the <b>unsupervised</b> deep <b>learning</b> hashing methods is significantly worse than the supervised ones. However, in most of the case, it is hard to label all the images/videos for large scale visual search, which cannot use supervised hashing. Therefore, how to design the <b>unsupervised</b> deep <b>learning</b> hashing models to further improve the accuracy of unsupervised hashing is another important research topic in the future.|$|R
40|$|Synergetic {{computers}} form a {{class of}} self-organized algorithms. Due to their close similarity to nonlinear self-organized systems In physics and chemistry they are potential candidates for a new sort of image processing hardware. We will study the performance of an <b>unsupervised</b> synergetic <b>learning</b> algorithm with classification problems on both artificial and real texture data and will show that <b>unsupervised</b> synergetic <b>learning</b> can be successfully used for unsupervised pattern classification...|$|R
25|$|A deep {{predictive}} coding network (DPCN) is a {{predictive coding}} scheme that uses top-down information to empirically adjust the priors {{needed for a}} bottom-up inference procedure {{by means of a}} deep, locally connected, generative model. This works by extracting sparse features from time-varying observations using a linear dynamical model. Then, a pooling strategy is used to learn invariant feature representations. These units compose to form a deep architecture and are trained by greedy layer-wise <b>unsupervised</b> <b>learning.</b> The layers constitute a kind of Markov chain such that the states at any layer depend only on the preceding and succeeding layers.|$|E
25|$|Reinforcement {{learning}} is a third branch of machine learning, distinct from supervised and <b>unsupervised</b> <b>learning,</b> which also admits quantum enhancements. In quantum-enhanced reinforcement learning, a quantum agent interacts with a classical environment and occasionally receives rewards for its actions, which allows the agent to adapt its behaviour—in other words, to learn {{what to do in}} order to gain more rewards. In some situations, either because of the quantum processing capability of the agent, or due to the possibility to probe the environment in superpositions, a quantum speedup may be achieved. Implementations of these kinds of protocols in superconducting circuits and in systems of trapped ions have been proposed.|$|E
25|$|<b>Unsupervised</b> <b>learning</b> is {{the ability}} to find {{patterns}} in a stream of input. Supervised learning includes both classification and numerical regression. Classification is used to determine what category something belongs in, after seeing a number of examples of things from several categories. Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change. In reinforcement learning the agent is rewarded for good responses and punished for bad ones. The agent uses this sequence of rewards and punishments to form a strategy for operating in its problem space. These three types of learning can be analyzed in terms of decision theory, using concepts like utility. The mathematical analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory.|$|E
50|$|<b>Unsupervised</b> feature <b>{{learning}}</b> {{is learning}} features from unlabeled data. The goal of <b>unsupervised</b> feature <b>learning</b> is often to discover low-dimensional features that captures some structure underlying the high-dimensional input data. When the feature learning is performed in an unsupervised way, it enables {{a form of}} semisupervised learning where features learned from an unlabeled dataset are then employed to improve performance in a supervised setting with labeled data. Several approaches are introduced in the following.|$|R
40|$|ABSTRACT: In {{this paper}} we {{describe}} {{a series of}} algorithms that generate real-valued attributes used to classify tactical situations using an <b>unsupervised</b> machine <b>learning</b> system. Attributes for the classification of tactical situations include anchored and unanchored flanks, choke points, restricted avenues of attack and retreat, and interior line of support. 1. Introduction. Our research in Computational Military Tactical Planning (as introduced by Kewley and Embrechts[1]) suggests that, an <b>unsupervised</b> machine <b>learning</b> system (like Gennari and Langley’s ClassIT [2]) can make reasoned inferences abou...|$|R
50|$|<b>Unsupervised</b> {{dictionary}} <b>learning</b> {{does not}} utilize data labels and exploits the structure underlying {{the data for}} optimizing dictionary elements. An example of <b>unsupervised</b> dictionary <b>learning</b> is sparse coding, which aims to learn basis functions (dictionary elements) for data representation from unlabeled input data. Sparse coding {{can be applied to}} learn overcomplete dictionaries, where the number of dictionary elements is larger than the dimension of the input data. Aharon et al. proposed algorithm K-SVD for learning a dictionary of elements that enables sparse representation.|$|R
500|$|Kenji Doya {{has argued}} that the cerebellum's {{function}} is best understood not in terms of the behaviors it affects, but the neural computations it performs; the cerebellum consists {{of a large number of}} more or less independent modules, all with the same geometrically regular internal structure, and therefore all, it is presumed, performing the same computation. If the input and output connections of a module are with motor areas (as many are), then the module will be involved in motor behavior; but, if the connections are with areas involved in non-motor cognition, the module will show other types of behavioral correlates. Thus the cerebellum has been implicated in the regulation of many differing functional traits such as affection. emotion and behavior. The cerebellum, Doya proposes, is best understood as predictive action selection based on [...] "internal models" [...] of the environment or a device for supervised learning, in contrast to the basal ganglia, which perform reinforcement learning, and the cerebral cortex, which performs <b>unsupervised</b> <b>learning.</b>|$|E
2500|$|Huang, Te-Ming; Kecman, Vojislav; and Kopriva, Ivica (2006); , in Supervised, Semi-supervised, and <b>Unsupervised</b> <b>Learning,</b> Springer-Verlag, Berlin, Heidelberg, 260 pp.96 illus., Hardcover, ...|$|E
2500|$|In <b>unsupervised</b> <b>learning,</b> {{some data}} [...] is given {{and the cost}} {{function}} to be minimized, that can be any function of the data [...] and the network's output [...]|$|E
30|$|<b>Unsupervised</b> (or deep) <b>learning</b> methods [12].|$|R
50|$|In <b>unsupervised</b> feature <b>learning,</b> {{features}} are learned with unlabeled input data. Examples include dictionary learning, independent component analysis, autoencoders, matrix factorization, and {{various forms of}} clustering.|$|R
40|$|The {{success of}} machine {{learning}} algorithms generally depends on intermediate data representation, called features that disentangle the hidden factors of variation in data. Moreover, machine learning models are required to be generalized, {{in order to reduce}} the specificity or bias toward the training dataset. <b>Unsupervised</b> feature <b>learning</b> is useful in taking advantage of large amount of unlabeled data, which is available to capture these variations. However, learned features are required to capture variational patterns in data space. In this dissertation, <b>unsupervised</b> feature <b>learning</b> with sparsity is investigated for sparse and local feature extraction with application to lung segmentation, interpretable deep models, and Alzheimer 2 ̆ 7 s disease classification. Nonnegative Matrix Factorization, Autoencoder and 3 D Convolutional Autoencoder are used as architectures or models for <b>unsupervised</b> feature <b>learning.</b> They are investigated along with nonnegativity, sparsity and part-based representation constraints for generalized and transferable feature extraction...|$|R
2500|$|Tu, K., and Honavar, V. (2008). <b>Unsupervised</b> <b>Learning</b> of Probabilistic Context-Free Grammar using Iterative Biclustering[...] In: International Colloquium on Grammatical Inference (ICGI-2008). Springer-Verlag Lecture Notes in Computer Science vol. 5278 pp.224–237.|$|E
2500|$|In {{the late}} 1940s, D.O. Hebb created a {{learning}} hypothesis {{based on the}} mechanism of neural plasticity that {{is now known as}} Hebbian learning. Hebbian learning is an <b>unsupervised</b> <b>learning</b> rule. This evolved into models for long term potentiation. Researchers started applying these ideas to computational models in 1948 with Turing's B-type machines [...]|$|E
2500|$|The Socially Intelligent Machines Lab of the [...] Georgia Institute of Technology {{researches}} {{new concepts}} of guided teaching interaction with robots. The {{aim of the}} projects is a social robot that learns task and goals from human demonstrations without prior knowledge of high-level concepts. These new concepts are grounded from low-level continuous sensor data through <b>unsupervised</b> <b>learning,</b> and task goals are subsequently learned using a Bayesian approach. These concepts {{can be used to}} transfer knowledge to future tasks, resulting in faster learning of those tasks. The results are demonstrated by the robot Curi who can scoop some pasta from a pot onto a plate and serve the sauce on top.|$|E
40|$|This paper gives {{a review}} of the recent {{developments}} in deep <b>learning</b> and <b>unsupervised</b> feature <b>learning</b> for time-series problems. While these techniques have shown promise for modeling static data, such as computer vision, applying them to time-series data is gaining increasing attention. This paper overviews the particular challenges present in time-series data and provides {{a review of}} the works that have either applied time-series data to <b>unsupervised</b> feature <b>learning</b> algorithms or alternatively have contributed to modifications of feature learning algorithms {{to take into account the}} challenges present in time-series data...|$|R
40|$|Unsupervised image {{classification}} {{is the process}} by which each image in a dataset is identified {{to be a member of}} one of the inherent categories present in the image collection without the use of labelled training samples. Unsupervised categorisation of images relies on <b>unsupervised</b> machine <b>learning</b> algorithms for its implementation. This paper identifies clustering algorithms and dimension reduction algorithms as the two main classes of <b>unsupervised</b> machine <b>learning</b> algorithms needed in unsupervised image categorisation, and then reviews how these algorithms are used in some notable implementation of unsupervised {{image classification}} algorithms...|$|R
30|$|In this paper, we {{are going}} to {{illustrate}} our fine-grained hand segmentation method which leverages <b>unsupervised</b> online <b>learning</b> pattern to robustly segment the hand in pixel-level from egocentric video.|$|R
