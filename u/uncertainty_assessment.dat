459|653|Public
50|$|At that conference, Jim Wayman, Antonio Possolo and Tony Mansfield {{delivered}} a paper (No.59) entitled Fundamental issues in biometric performance testing: A modern statistical and philosophical framework for <b>uncertainty</b> <b>assessment.</b>|$|E
50|$|In {{physical}} experiments uncertainty analysis, or experimental <b>uncertainty</b> <b>assessment,</b> {{deals with}} assessing the uncertainty in a measurement. An experiment {{designed to determine}} an effect, demonstrate a law, or estimate the numerical value of a physical variable {{will be affected by}} errors due to instrumentation, methodology, presence of confounding effects and so on. Experimental uncertainty estimates are needed to assess the confidence in the results. A related field is design of experiments.|$|E
50|$|ENABLE is {{a history}} {{matching}} and <b>uncertainty</b> <b>assessment</b> software designed {{to be used in}} the oil and gas industry. It optimizes plans and reduces costs by accelerating history matching process and improving reservoir understanding. ENABLE is also used to quantify production estimates under reservoir uncertainty. As oil and gas resources are increasingly recovered from reservoirs whose behavior has considerable uncertainty the need to measure the uncertainty is increasing.Existing reservoir simulation projects accelerate significantly and ENABLE provides assisted history matching and uncertainty forecasts.|$|E
40|$|This CIRIA guide {{provides}} a framework for good practice in the <b>assessment</b> of <b>uncertainty</b> in fluvial flood risk mapping. <b>Uncertainty</b> <b>assessments</b> involve subjective assessments and the framework reveals and makes clear those assessments at each stage of the modelling process. The framework makes explicit what {{has in the past}} been kept implicit...|$|R
40|$|The Intergovernmental Panel on Climate Change has {{developed}} a novel framework for assessing and communicating uncertainty in the findings published in their periodic assessment reports. But how should these <b>uncertainty</b> <b>assessments</b> inform decisions? We take a formal decision-making perspective to investigate how scientific input formulated in the IPCC’s novel framework might inform decisions in a principled way through a normative decision model...|$|R
40|$|The European {{approach}} {{for the development}} of nuclear data for fusion technology applications is presented. Related R&D activities are conducted by the Consortium on Nuclear Data Development and Analysis for Fusion to satisfy the nuclear data needs of the major projects including ITER, the Early Neutron Source (ENS) and DEMO. Recent achievements are presented in the area of nuclear data evaluations, benchmarking and validation, nuclear model improvements, and <b>uncertainty</b> <b>assessments...</b>|$|R
30|$|Box–Behnken {{design was}} adopted for the {{production}} <b>uncertainty</b> <b>assessment</b> given the most-likely model of the cyclic GAGD process, which resulted after quantifying the geological <b>uncertainty</b> <b>assessment.</b>|$|E
40|$|Acknowledgements Table of {{contents}} List of symbols List of figures List of tables Abstract 1 Introduction 2 Methodology for integrated <b>uncertainty</b> <b>assessment</b> in groundwater modelling 3 Sensitivity to prior model probabilities {{and value of}} prior knowledge {{in the assessment of}} conceptual model uncertainty 4 on the value of conditioning data to reduce conceptual model uncertainty in groundwater modelling 5 Application of the integrated <b>uncertainty</b> <b>assessment</b> approach to account for conceptual and scenario uncertainties in groundwater modelling 6 Integrated <b>uncertainty</b> <b>assessment</b> for the regional aquifer Pampa del Tamarugal - Northern Chile 7 Conclusions Bibliography Appendices Curriculum Vitae List of publicationsnrpages: 156 status: publishe...|$|E
40|$|A new <b>uncertainty</b> <b>assessment</b> {{method is}} {{proposed}} to characterize non-statistical uncertainties in precision measurement. The proposed method {{is based on}} grey system theory {{to address the problem}} involved in <b>uncertainty</b> <b>assessment</b> where the sampling size is small and the distribution of the data is unknown. The advantage of the proposed approach is that the requirements of the statistics based methods are removed. In the proposed method, an accumulated true size vector and an accumulated measurement data vector are established to reduce the effects of the errors in the measurement and in the numerical calculation. The <b>uncertainty</b> <b>assessment</b> is based on the l(infinity) norm of the difference between the accumulation of the sorted measurement data vector and that of the true size vector. A number of computational and experimental tests were carried out. The results demonstrated the effectiveness and consistency of the proposed method in non-statistical <b>uncertainty</b> <b>assessment,</b> compared with the existing methods...|$|E
40|$|For a high {{dimensional}} {{field of}} random variables, global correlation {{is defined as}} the ratio of average covariance and average variance, and its elementary properties are studied. Global correlation is used to harmonize <b>uncertainty</b> <b>assessments</b> at global and local scales. It can be estimated by the correlation of random aggregations of fixed size of disjoint sets of random variables. Illustrative applications are given using crop loss per county per year and forest carbon...|$|R
40|$|Geological {{investigations}} {{are carried out}} under particularly high <b>uncertainties.</b> <b>Assessment</b> of these <b>uncertainties</b> {{is very important for}} project planning. This paper presents a methodology to evaluate uncertainty associated with geological structures such as ore deposits and aquifers. In order to assess the uncertainty of the geological systems observed, fuzzy clustering and spatial measures are used. Then, the heterogeneous zones are evaluated using conditional probabilities. The posterior probabilities obtained from testing data provide useful information for assessing the uncertainty. ...|$|R
40|$|Abstract Background The Bayesian {{approach}} is now widely recognised as a proper framework for analysing risk in health care. However, the traditional text-book Bayesian {{approach is}} in many cases difficult to implement, as {{it is based on}} abstract concepts and modelling. Methods The essential points of the risk analyses conducted according to the predictive Bayesian approach are identification of observable quantities, prediction and <b>uncertainty</b> <b>assessments</b> of these quantities, using all the relevant information. The risk analysis summarizes the knowledge and lack of knowledge concerning critical operations and other activities, and give in this way a basis for making rational decisions. Results It is shown that Bayesian risk analysis can be significantly simplified and made more accessible compared to the traditional text-book Bayesian approach by focusing on predictions of observable quantities and performing <b>uncertainty</b> <b>assessments</b> of these quantities using subjective probabilities. Conclusion The predictive Bayesian approach provides a framework for ensuring quality of risk analysis. The approach acknowledges that risk cannot be adequately described and evaluated simply by reference to summarising probabilities. Risk is defined by the combination of possible consequences and associated uncertainties. </p...|$|R
40|$|Results from towing-tank {{experiments}} for David Taylor Model Basin model 5512 {{are presented}} for resistance, sinkage and trim, wave profile, and nominal wake tests and <b>uncertainty</b> <b>assessment.</b> The resistance and sinkage and trim data are for Froude numbers 0. 05 - 0. 45 and free-model condition. The wave profiles and nominal wake data are for Froude numbers 0. 28 and 0. 41 and fixed-model condition (at the dynamic sinkage and trim for each Fr). The test design, measurement systems, and <b>uncertainty</b> <b>assessment</b> are described. The <b>uncertainty</b> <b>assessment</b> methodology rigorously follows the AIAA Standard S- 071 - 1995. The results are discussed {{with regard to}} the data trends and uncertainties, including Fr effects. Future work is mentioned. The data contributes to the surface-ship resistance and propulsion model-scale database for computational fluid dynamics validation, as part of an international collaborative project betwee...|$|E
40|$|The paper compares two {{different}} experimental methods {{to establish the}} traceability of freeform measurements on coordinate measuring machines: 1) <b>uncertainty</b> <b>assessment</b> using Modular Freeform Gauges, and ii) <b>uncertainty</b> <b>assessment</b> using Uncalibrated Objects. The first approach is an application to freeform geometries of the method described in ISO TS 15530 - 3 based on comparisons. The second approach is inspired by the procedure currently being developed within ISO TC 213, involving repeated measurements of a given object in different orientations with variation of measuring parameters etc. The feasibility of the two approaches for freeform geometries is demonstrated through the calibration of a turbine blade...|$|E
40|$|To enable {{properly}} sized software project {{budgets and}} plans, {{it is important}} to be able to assess the uncertainty of the estimates of most likely effort required to complete the project. Previous studies show people in general, as well as software professionals, tend to be overconfident when assessing uncertainty over estimated effort. This thesis explores the possibility of learning more realistic <b>uncertainty</b> <b>assessment</b> with the use of outcome feedback. Two experiments, with favorable learning environments, were set up to investigate the issue. The first study focused on whether people in general possess the ability to learn more realistic uncertainty assessment; the second how much, and how, software developers learn to improve <b>uncertainty</b> <b>assessment.</b> The results indicate that people in general are well calibrated initially, and highly capable of adjusting towards realism given favorable learning conditions; i. e. frequent and relevant feedback on performance. In the software engineering setting, using experienced software developers, there was, in comparison, observed a lower degree of learning realism in effort <b>uncertainty</b> <b>assessment.</b> There was found that a necessary condition for improvement of uncertainty assessments on effort estimates may be the use of explicitly formulated <b>uncertainty</b> <b>assessment</b> strategies. In contrast, intuition-based <b>uncertainty</b> <b>assessment</b> strategies may lead to little or no learning. The implications found for the industry and further research was: (I) For learning to occur, the learning process may need to be aided by explicitly stated learning strategies, and frequent reminders of the goal of the learning session. (II) There must be given special attention to the framing of the probability measures used to state uncertainty over effort. Check for adequate understanding of the concept of probability and uncertainty, give proper explanations of these terms, and issue reminders of the agreed upon definitions at regular intervals during the time of learning. It seems beneficial to support mathematical probability definitions with natural language descriptions, and oral consensus through debate of these definitions. (III) Feedback should be given in such a way that: (1) several kinds of feedback is used and issued frequently, as a minimum it should be given at naturally occurring places; (2) the possibility of subjective interpretations on performance is avoided as much as possible; (3) it can be directly transferable as input to future uncertainty assessments, i. e. framing of the appearance to visually mach the <b>uncertainty</b> <b>assessment</b> process to come, history based tendencies should be pointed out. (IV) There are different qualities and learning strategies that are effective for learning the skill of “know how” versus learning “how uncertain is”. The design and framing, of learning environment and feedback, should therefore reflect how learning <b>uncertainty</b> <b>assessment</b> is best obtained when this is the purpose...|$|E
40|$|Investigating and {{mitigating}} <b>uncertainties</b> in the <b>assessment</b> of Scottish Nephrops norvegicus populations using simulated underwater television data N. Campbell, H. Dobby, and N. Bailey Campbell, N., Dobby, H., and Bailey, N. 2009. Investigating and mitigating <b>uncertainties</b> in the <b>assessment</b> of Scottish Nephrops norvegicu...|$|R
40|$|International audienceWhen {{transistor}} noise characterization becomes {{a difficult task}} at millimeter wave range, the accuracy on the resulting noise parameters is not or poorly discussed. Within this context, this paper aims to present a methodology to estimate the uncertainties related to the extraction of the four noise parameters when using in situ tuner techniques, in the 130 - 170 GHz frequency range. B 9 MW SiGe HBT from STMicroelectonics {{was used as a}} test vehicle for the noise characterization and <b>uncertainties</b> <b>assessment...</b>|$|R
40|$|During {{the last}} few years, the IRSN has {{developed}} a level 2 PSA for French 900 MWe PWRs. The last version of the study has been completed in 2003. This version {{is going to be}} updated to take into account hydrogen recombiners installation and modifications envisaged for third decennial visit of these plants. The objective {{of this paper is to}} comment the methodological approach of the IRSN for <b>uncertainties</b> <b>assessment</b> in level 2 PSA, with examples for accident progression event tree and releases assessment. 1...|$|R
40|$|I enjoyed {{reading this}} paper which makes clear, in a {{classical}} and engineeristic per-spective, the main {{problems related to}} flood forecasting. I generally like the classical approach to hydrology, which aims to identify design variables (in this case flood fore-casts) {{with the support of}} advanced theory. I decided to write this short comment because I am missing a more dedicated treat-ment of <b>uncertainty</b> <b>assessment</b> in flood forecasting. The author rightly points out that <b>uncertainty</b> <b>assessment</b> is very important (page 4677, lines 25 - 30) in order to provide end users with a complete information, but the recent developments about forecast uncertainty estimation are not mentioned. Actually, the literature propose...|$|E
40|$|In this article, a {{probabilistic}} fusion {{concept for}} road extraction from multi-aspect SAR images, which incorpo-rates sensor geometry and context information, is proposed. Before fusion, {{the uncertainty of}} each extracted line segment is assessed by means of Bayesian probability theory. This assessment is performed on attribute-level {{and is based on}} predefined probability density functions learned from training data. In the first part the impor-tance of global and local context information and the benefit of incorporating sensor geometry within the fusion module are discussed. The second part concentrates on the analysis of the <b>uncertainty</b> <b>assessment</b> of the line segments. Finally, some results regarding the <b>uncertainty</b> <b>assessment</b> of the line segments using real SAR images are presented. ...|$|E
40|$|Abstract 1 Ensuring the {{accuracy}} of intelligence assessments is made difficult by the pervasiveness of uncertainty in intelligence information and the demand to fuse information from multiple sources. This paper describes Infusiun, a model-based software tool for information fusion and <b>uncertainty</b> <b>assessment</b> in intelligence analysis...|$|E
40|$|Probabilistic {{inversion}} {{is used to}} take expert <b>uncertainty</b> <b>assessments</b> about observable model outputs {{and build}} from them a distribution on the model parameters that captures the uncertainty expressed by the experts. In this paper we look at ways to use minimum information methods to do this, focussing in particular {{on the problem of}} ensuring consistency between expert assessments about differing variables, either as outputs from a single model, or potentially as outputs along a chain of models. The paper shows how such a problem can be structured and then illustrates the method with an example involving atmospheric dispersion and deposition...|$|R
40|$|Demographic {{forecasts}} {{are inherently}} uncertain. Nevertheless, an appropriate description of this uncertainty {{is a key}} underpinning of informed decision making. In recent decades various methods {{have been developed to}} describe the uncertainty of future populations and their structures, but the uptake of such tools amongst the practitioners of official population statistics has been lagging behind. In this letter we revisit the arguments for the practical uses of <b>uncertainty</b> <b>assessments</b> in official population forecasts, and address their implications for decision making. We discuss essential challenges, both for the forecasters and forecast users, and make recommendations for the official statistics community...|$|R
50|$|JCGM 106:2012. Evaluation of {{measurement}} data - The role {{of measurement}} <b>uncertainty</b> in conformity <b>assessment.</b>|$|R
40|$|We {{investigated}} the Austrian national {{greenhouse gas emission}} inventory to review the reliability and usability of such inventories. The overall uncertainty of the inventory (95 % confidence interval) is just over 10 % of emissions, with nitrous oxide (N 2 O) from soils clearly providing the largest impact. Trend uncertainty - the difference between 2 years - is only about five percentage points, as important sources like soil N 2 O {{are not expected to}} show different behavior between the years and thus exhibit a high covariance. The result is very typical for industrialized countries - subjective decisions by individuals during <b>uncertainty</b> <b>assessment</b> are responsible for most of the discrepancies among countries. Thus, <b>uncertainty</b> <b>assessment</b> cannot help to evaluate whether emission targets have been met. Instead, a more rigid emission accounting system that allows little individual flexibility is proposed to provide harmonized evaluation uninfluenced by the respective targets. Such an accounting system may increase uncertainty in terms of greenhouse gas fluxes to the atmosphere. More importantly, however, it will decrease uncertainty in intercountry comparisons and thus allow for fair burden sharing. Setting of post-Kyoto emission targets will require the independent evaluation of achievements. This can partly be achieved by the validation of emission inventories and thorough <b>uncertainty</b> <b>assessment...</b>|$|E
40|$|This work {{describes}} {{an error in}} the <b>uncertainty</b> <b>assessment</b> of uncertainties of the Total-Reflection X-ray Fluorescence technique Ref [1]. A confusion, between the precision and accuracy of a measurement produced an incomplete {{evaluation of the uncertainties}} of the technique. Comment: We found a conceptual error in an overall procedur...|$|E
40|$|It {{could be}} argued that {{activity}} measurements of radioactive substances should be under statistical control, considering that the measurand is unambiguously defined, the radioactive decay processes are theoretically well understood and the measurement function can be derived from physical principles. However, comparisons invariably show a level of discrepancy among activity standardisation results that exceeds expectation from uncertainty evaluations. Also decay characteristics of radionuclides determined from different experiments show unexpected inconsistencies. Arguably, the problem lies mainly in incomplete <b>uncertainty</b> <b>assessment.</b> Of the various reasons leading to incomplete <b>uncertainty</b> <b>assessment,</b> from human failure to limitations to the state-of-the-art knowledge, a selection of cases is discussed in which imperfections in the modelling of the measurement process can lead to unexpectedly large underestimations of uncertainty. JRC. D. 4 -Standards for Nuclear Safety, Security and Safeguard...|$|E
40|$|We {{present a}} method that {{combines}} uncertain air quality measurements with uncertain secondary information from an atmospheric dispersion model. The method combines external drift kriging and a measurement error (ME) model, and uses Bayesian techniques for inference. An illustration with simulated data shows what can theoretically be expected. The method is flexible for assigning different error variances to both the primary information and secondary information at each location. Next, we address actual NO 2 data collected at an urban and a rural site in the Netherlands. <b>Uncertainty</b> <b>assessments</b> in terms of exceeding air quality standards are given. The study shows that biased uncertain secondary information can be used successfully in a spatial interpolation study at the national scal...|$|R
40|$|Extracting {{quantitative}} information from both airborne and orbital sensors data requires {{information about their}} absolute calibration. The most common in-flight absolute calibration method is based on radiometric data collected from a reference surface located on the Earth. The first step of that method is the surface characterization, which involves radiometric measurements to determine the average Reflectance Factor of the surface. Frequently these reference surfaces are relatively large and the radiometric measurements have been performed by instruments, which must be calibrated. Thus, analyzing the instruments conditions and their respective contributions to the final uncertainty of the measurements experiments {{have to be performed}} in laboratory. The objective {{of this paper is to}} describe some in-lab instruments calibration procedures including the <b>uncertainties</b> <b>assessment.</b> Pages: 19 - 2...|$|R
40|$|The PSA 2 {{work package}} (PSA 2 WP) {{is a part}} of the Joined Programme Activity of the European Severe Accident Network (SARNET) related to level 2 PSA methodologies. The general {{objectives}} of this work package is to provide a comparison of the different methodologies used or under development for level 2 PSA application by the partners involved in the work package and to promote their harmonization. The PSA 2 WP is organized into three main topics: methodologies in general, methodologies for <b>uncertainties</b> <b>assessment,</b> and dynamic reliability methods. The different tasks initially defined for these three topics are shortly described and the partners involved identified. Attention is then paid on the methodologies used so far by the different partners to assess th...|$|R
30|$|The optimal set of injection, soaking, and {{production}} periods {{along with the}} minimum bottom hole pressure in production wells were set as nominal values in the geological <b>uncertainty</b> <b>assessment.</b> Since there are nine quantiles from permeability and K_v/K_h, {{the total number of}} simulations in the geological uncertainty was 81 (9 ^ 2 = 81).|$|E
40|$|Bayesian methods, and {{particularly}} Markov chain Monte Carlo (MCMC) techniques, are extremely useful in <b>uncertainty</b> <b>assessment</b> and parameter estimation of hydrologic models. However, MCMC algorithms {{can be difficult}} to implement successfully due to the sensitivity of an algorithm to model initialization and complexity of the parameter space. Many hydrologic studies, even relatively simple conceptualizations, ar...|$|E
40|$|This {{technical}} report describes the <b>uncertainty</b> <b>assessment</b> on scattering parameter measurements obtained from different {{vector network analyzer}} calibration algorithms in WR- 10 millimeter waveguide (from 75 to 110 GHz). The calibration algorithms considered for this exercise are Thru-Reflect-Line and Quick Short-Open-Load-Thru. The latter {{turned out to be}} a suitable traceable calibration method for waveguide measurements...|$|E
40|$|Probabilistic climate information, {{including}} climate forecasts, often rely on {{time series}} data of prognostic variables (Y, eg. rainfall or yield), represented as cumulative distribution probabilities functions (CDFs) or their complement, probability of exceeding functions (POEs). Useful information for decision-making is then derived from such distributions and expressed as Y percentiles or {{the probability of}} Y exceeding a certain threshold c (Pr[Y> c]). Such estimates are frequently reported without any measure of uncertainty. The degree of uncertainty associated with such estimates depends {{on the length of}} the time series and their internal variability. Lack of <b>uncertainty</b> <b>assessments</b> can lead to misguided beliefs about the true performance of the forecast systems, possibly resulting in inappropriate actions by the decision maker (Potts et al. 1996; Jolliffe 2004; Maia et al. 2006). However, even when uncertainty estimates are provided, these are often based on methods that rely on assumptions of data being normally distributed. This is in spite of the well-known fact that distributions of important climate variables, such as rainfall, are notoriously skewed, particularly in areas with strong seasonality (eg. high frequencies of ?zero? rainfall amounts). As an alternative for Normal-based procedures, we therefore propose the use of distribution free methods for constructing percentile and POE confidence limits as described in Hahn and Meeker (1991) and implemented into ?The Capability Procedure?? of the SAS® System. Such distribution-free tools are particularly useful for spatial <b>uncertainty</b> <b>assessments</b> that would otherwise require a tedious, location-by-location checking of assumptions regarding underlying probability distributions (Maia et al., 2006). Here, we discuss the rationale, advantages and limitations of both, parametric and nonparametric approaches. We illustrate the use of distribution-free methods by assessing the uncertainty of percentiles and POEs estimates for 3 -monthly rainfall series from selected locations in Australia and the Southeast of South America. 200...|$|R
40|$|Provider {{profiling}} (ranking, 2 ̆ 2 league tables 2 ̆ 2) is {{prevalent in}} health services research. Similarly, comparing educational institutions and identifying differentially expressed genes depend on ranking. Effective ranking procedures must be structured by a hierarchical (Bayesian) model and {{guided by a}} ranking-specific loss function, however even optimal methods can perform poorly and estimates {{must be accompanied by}} <b>uncertainty</b> <b>assessments.</b> We use the 1998 - 2001 Standardized Mortality Ratio (SMR) data from United States Renal Data System (USRDS) as a platform to identify issues and approaches. Our analyses extend Liu et al. (2004) by combining evidence over multiple years via an AR(1) model; by considering estimates that minimize errors in classifying providers above or below a percentile cutpoint in addition to those that minimize rank-based, squared-error loss; by considering ranks based on the posterior probability that a provider 2 ̆ 7 s SMR exceeds a threshold; by comparing these ranks to those produced by ranking MLEs and ranking P-values associated with testing whether a provider 2 ̆ 7 s SMR = 1; by comparing results for a parametric and a non-parametric prior; by reporting on a suite of uncertainty measures. Results show that MLE-based and hypothesis test based ranks are far from optimal, that uncertainty measures effectively calibrate performance; that in the USRDS context ranks based on single-year data perform poorly, but that performance improves substantially when using the AR(1) model; that ranks based on posterior probabilities of exceeding a properly chosen SMR threshold are essentially identical to those produced by minimizing classification loss. These findings highlight areas requiring additional research and the need to educate stakeholders on the uses and abuses of ranks; on their proper role in science and policy; on the absolute necessity of accompanying estimated ranks with <b>uncertainty</b> <b>assessments</b> and ensuring that these uncertainties influence decisions...|$|R
40|$|AbstractThis paper {{addresses}} {{the problem of}} exchanging <b>uncertainty</b> <b>assessments</b> in multi-agent systems. Since {{it is assumed that}} each agent might completely ignore the internal representation of its partners, a common interchange format is needed. We analyze the case of an interchange format defined by means of imprecise probabilities, pointing out the reasons of this choice. A core problem with the interchange format concerns transformations from imprecise probabilities into other formalisms (in particular, precise probabilities, possibilities, belief functions). We discuss this so far little investigated question, analyzing how previous proposals, mostly regarding special instances of imprecise probabilities, would fit into this problem. We then propose some general transformation procedures, which take also account of the fact that information can be partial, i. e. may concern an arbitrary (finite) set of events...|$|R
