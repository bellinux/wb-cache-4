52|10000|Public
5000|$|As {{defined by}} the Codex Alimentarius Commission and adopted by {{international}} food safety commissions, food safety risk assessment is [...] "The scientific evaluation of known or potential adverse health effects resulting from human exposure to foodborne hazards." [...] The {{most important aspect of}} risk assessment in relation to food safety is that it should be rooted in scientific data. Sources of data should be assembled in a systematic manner and should stem from valid scientific studies and communities across the world. A proper risk assessment can be described as being objective and unbiased, with absolute transparency. When at all possible, the assessment should remain independent of risk management as to preserve the integrity of the science and not have influence from regulatory policy and values. All assumptions made throughout the assessment should be well documented by the risk manager and should strive to be as objective, biologically realistic, and consistent as possible. As with any risk assessment performed, incomplete data or gaps in information create degrees of variability and uncertainty. In accounting for these factors, an extensive description of uncertainties in the risk estimate and their origins should be provided, as well as, descriptions of how assumptions being made can increase or decrease the <b>uncertainty</b> <b>of</b> <b>results</b> in the risk assessment. To increase the validity of a risk assessment, it is recommended that the assessment remain open for peer review and editing by food safety and science communities. A proper risk assessment is a constantly revolving process consisting of the following steps: (i) hazard identification, (ii) hazard characterization, (iii) exposure assessment, and (iv) risk characterization.|$|E
40|$|Abstract- The {{problems}} associated with measurements and determination of volume resistivity were presented in the paper. Special {{attention was paid to}} the measurements carried out on extremely high resistivity materials. Space charge influence was discussed as the most important factor affecting the <b>uncertainty</b> <b>of</b> <b>results</b> of the resistivity measurements...|$|E
40|$|The paper studies {{consequences}} of incomplete information to <b>uncertainty</b> <b>of</b> <b>results</b> of stochastic optimization. Stochastic characteristics of optimized system are evaluated from observed data, moreover, the data may be incomplete. Namely, {{we consider the}} random censoring of observations frequently encountered in time-to-event (of lifetime) studies. The analysis of uncertainty will be based both on theoretical properties of estimated stochastic characteristics and on simulated examples...|$|E
2500|$|The level <b>of</b> <b>uncertainty</b> <b>of</b> the <b>resulting</b> {{estimates}} depends significantly on {{the source}} category and the pollutant. Some examples: ...|$|R
40|$|The {{measurement}} <b>uncertainty</b> <b>of</b> the <b>result</b> <b>of</b> {{total phosphorus}} determinations in wastewater was evaluated. Total phosphorus was determined spectrometrically using ammonium molybdate (ISO 6878, 1998). The major sources <b>of</b> <b>uncertainty</b> <b>of</b> the <b>result</b> <b>of</b> measurement {{were identified as}} contributions from the linear least squares calibration, repeatability, homogeneity of the sample, storage conditions, and recovery. Identification and evaluation <b>of</b> sources <b>of</b> <b>uncertainty</b> was followed by combined uncertainty calculations. The {{results show that the}} major source <b>of</b> <b>uncertainty</b> arose from the calibration curve, thus leading to the conclusion that calibration is the target operation for reducing the measurement <b>uncertainty</b> <b>of</b> this determination...|$|R
30|$|A {{key feature}} of the {{proposed}} simulation-based SEMD method {{is that it can}} integrate various patterns between the simulated extrema. Furthermore, the <b>uncertainty</b> <b>of</b> the <b>resulting</b> IMFs can be evaluated on the basis of several sets of simulations.|$|R
40|$|Interpretation of Doppler-Shift {{spectroscopy}} {{measurement of}} neutral beams species content requires an assumption about distribution of populations of thin structure of excited state (hydrogen n= 3). In the paper several effects caused mixing of sublevel population are discussed and correction factors for different models of thin structure population are calculated. Such mixing {{can lead to}} tens percent <b>uncertainty</b> <b>of</b> <b>results</b> of measurements of beam species content. A possible way for experimental verification of sublevel population mixing is proposed...|$|E
40|$|The {{estimation}} of the uncertainty associated to analytical methods is {{necessary in order to}} establish the comparability of results. Multiresidue analytical methods lack very often of information about <b>uncertainty</b> <b>of</b> <b>results</b> with likely implications when results are compared with maximum residue levels (MRL) established by regulations. An adequate identification and {{estimation of}} each uncertainty source allows to laboratories to establish the accuracy of results and to balance with time-consuming and costs. © 2002 Elsevier Science B. V. All rights reserved...|$|E
40|$|BACKGROUND: Statistical {{heterogeneity}} {{can increase}} the <b>uncertainty</b> <b>of</b> <b>results</b> and reduce the quality of evidence derived from systematic reviews. At present, it is uncertain what the major factors are that account for heterogeneity in meta-analyses of analgesic adjuncts. Therefore, {{the aim of this}} review was to identify whether various covariates could explain statistical heterogeneity and use this to improve accuracy when reporting the efficacy of analgesics. METHODS: We searched for reviews using MEDLINE, EMBASE, CINAHL, AMED, and the Cochrane Database of Systematic Reviews. First, we identified the existence of considerable statistical heterogeneity (I 2 > 75...|$|E
3000|$|An {{error in}} a/t, which was {{neglected}} {{in the present}} evaluation, also causes <b>uncertainty</b> <b>of</b> the <b>results.</b> A postulated error of 20 % in a/t results in a relatively small error of about 5 and 2.5 % in [...]...|$|R
50|$|There {{are many}} robust linear algebra methods that can solve for {{the values of}} (x,y,z), such as Gaussian Elimination. Chapter 15 in Numerical Recipes {{describes}} several methods to solve linear equations and estimate the <b>uncertainty</b> <b>of</b> the <b>resulting</b> values.|$|R
40|$|Background: Providing quality {{insurance}} for radiation therapy implies {{high level of}} precision for determining absorbed dose, and therefore high level of precision for fitting radial dose function. 3 - 5   degree polynomials provide required precision, however, <b>uncertainty</b> <b>of</b> their coefficients may cause substantial errors. Aim: To investigate dependence of errors of calculating radial dose function with consideration <b>of</b> <b>uncertainty</b> <b>of</b> coefficients from different degrees of fitting polynomial. Materials and methods: Calculations were performed with software package GEANT 4. 9. 6. Geometry and materials of the source correspond to the model BEBIGCo 0. A 86. Spectral structure of the source corresponds to the NuDat 2. 6 database. Statistical processing was done using nonlinear least-square method. Results: Values of the radial dose function of Cobalt source for brachytherapy were calculated for given geometry. Conducted comparison of precisions of 3 to 5 degree polynomial approximations and possible <b>uncertainties</b> <b>of</b> <b>results</b> <b>of</b> radial dose function calculations. Conclusion: Withrequired precision of 25 % and higher at the radius of 10 cm the optimal choice for radial dose approximation is the 3 rd degree polynomial.  </p...|$|R
40|$|The {{volume of}} {{research}} specifically directed at lodging {{real estate investment}} trusts (REITs) is slender, although numerous studies have been conducted on REITs generally. Studies of REITs generally have found that regulatory requirements disperse ownership and focus management’s attention on its position as the shareholders’ agents. While REITs have carried more <b>uncertainty</b> <b>of</b> <b>results</b> than conventional real estate investments, they remain a vehicle for relatively small investors to participate in large real estate holdings. Despite the presence and apparent success of lodging REITs, no study has specifically addressed which ownership format is most suited to the hotel industry...|$|E
40|$|On {{the basis}} of data {{collected}} at club level, it can be suggested that most Scottish elite football teams were not profit-maximizers. The lack of price competition, the existence of free entry for some spectators, the playing of uneconomic friendly and minor cup fixtures, the relative under-utilization of ground facilities, and the employment policies and wage structures of clubs point to utility taking precedence over profits. Even if the clubs had been profit-maximizers, this would have required collusion at the Scottish Football League (SFL) level {{in order to produce}} <b>uncertainty</b> <b>of</b> <b>results</b> at both match and championship level, but the members of the SFL seem to have been willing to tolerate a high degree of inequality in playing success...|$|E
40|$|Change {{in adult}} health {{following}} medical priority rehousing Sirs, Blackman et al. 1 report an informative longitudinal {{study in which}} housing and environmental conditions and health-related factors are compared in a cohort of over 100 adults before and after medical priority rehousing. Contrasting data are also presented for a control group of similar size and base-line SF- 36 status whose applications for rehousing were unsuc-cessful. Unfortunately, the presentation and interpretation of the findings falls short in several respects. It is now the policy of many leading health-related journals such as the British Medical Journal that confidence intervals, rather than p values, should be the main expression of the <b>uncertainty</b> <b>of</b> <b>results</b> caused b...|$|E
40|$|Uncertainty {{analysis}} was used to back the development of H 2 /air and wet CO/air combustion mechanisms. The Leeds Methane Oxidation Mechanism was updated {{on the basis of the}} latest literature data. <b>Uncertainties</b> <b>of</b> the simulation <b>results,</b> caused by the <b>uncertainties</b> <b>of</b> the kinetic parameters and the heat of formation data, were analysed. The methods used were local uncertainty analysis and Monte Carlo Analysis with Latin Hypercube Sampling. There was always satisfactory agreement between the simulation results and the bulk experimental data, but in some cases the <b>uncertainties</b> <b>of</b> the simulation <b>results</b> were large...|$|R
40|$|The article {{describes}} literary knowledge briefly and presents {{an example of}} the application of a statistical, probability approach to the estimations <b>of</b> the <b>uncertainties</b> <b>of</b> the <b>results</b> <b>of</b> the qualitative GC/MS chemical analyses of the samples of the fire debris from the seat of fire for the content of accelerants...|$|R
40|$|We {{present an}} {{improved}} calculation of B→ light pseudoscalar formfactors from light-cone sum rules, including one-loop radiative corrections to twist- 2 and twist- 3 contributions, and leading order twist- 4 corrections. The total theoretical <b>uncertainty</b> <b>of</b> our <b>results</b> at zero momentum transfer is 10 to 13...|$|R
40|$|In monitoring, data {{collection}} and analysis, especially for pervasive and dynamic systems, the data collected (the results of observations) can be affected by non-negligible errors and consequently they can lead to misleading or nonoptimal decisions. Although guidelines and good practises to properly collect the results exist and are often applied, the <b>uncertainty</b> <b>of</b> <b>results</b> {{and the quality of}} the measuring system is rarely discussed. Written by a 1 st year PhD student at the University of Florence, this paper constitutes a research direction for the development of the PhD program: the paper presents a preliminary investigation and two examples towards the assessment of measuring systems and results to provide trusted observations. 1...|$|E
40|$|The {{influence}} of applied loads between 0. 09807 N and 0. 9807 N on measured values of micro-hardness was evaluated by Meyer’s index n, proportional specimen resistance model (PSR) and Hays – Kendall methods, Total Dispersion Zone and Analysis of Variance (ANOVA). The measurement was repeated 6 times {{using the same}} hardness reference block with standard hardness Hc = 327 HV 0. 05 as a sample. The {{influence of}} the load on the measured value of micro-hardness is statistically significant, {{and the relationship between}} applied load and micro-hardness manifests reverse indentation size effect (ISE) for most of “measurements”. The high value of the <b>uncertainty</b> <b>of</b> <b>results</b> can affect the existence and nature of ISE, especially at low loads...|$|E
30|$|Rapid {{revolution}} in digital video has brought many applications {{at home in}} affordable cost. The volume of digital data has been increasing rapidly due to the wide usage of multimedia applications {{in the areas of}} education, entertainment, business, medicine etc. One of the major applications is sports video analysis. Sports videos attract majority of people due to their capability of producing thrill as well as <b>uncertainty</b> <b>of</b> <b>results.</b> Hence, there has been an enormous increase of such video contents on the Internet. Video summarization helps to address these needs by developing a condensed version of full-length videos [1 – 3]. Extraction of important events and creation of summaries do not only make the video compact but also make it possible to deliver over low-bandwidth networks.|$|E
5000|$|According to the Guidelines for Evaluating and Expressing the <b>Uncertainty</b> <b>of</b> NIST Measurement <b>Results,</b> the {{following}} conditions need to be fulfilled {{in the establishment of}} repeatability: ...|$|R
40|$|We {{present an}} {{improved}} calculation of B [...] > light pseudoscalar form factors from light-cone sum rules, including one-loop radiative corrections to twist- 2 and twist- 3 contributions, and leading-order twist- 4 corrections. The total theoretical <b>uncertainty</b> <b>of</b> our <b>results</b> at zero momentum transfer is 10 to 13...|$|R
40|$|On {{the present}} work {{the quality of}} joint of {{dissimilar}} materials was inspected using ultrasonic waves. Various ultrasonic techniques and sample characteristics were analyzed to develop suitable configuration for the inspection of joint of dissimilar materials. Using CIVA software the sample was designed and virtual ultrasonic inspections performed using conventional, focused and phased array transducers with a frequency range from 3, 5 MHz to 10 MHz. Suitable type and frequencies of ultrasonic transducer {{as well as the}} side of the sample from which the inspection has to be performed are selected according to <b>results</b> <b>of</b> CIVA modelling. Using Omniscan measurement system and phased array transducers of 3, 5 MHz and 5 MHz the delaminations between dissimilar joints were found experimentally. The lengths of delaminations and their depths were measured and compared to theoretically calculated values. Using Tecscan measurement system and immersion transducers the location of defects was determined. The <b>uncertainties</b> <b>of</b> <b>results</b> <b>of</b> ultrasonic testing were estimated...|$|R
40|$|New CAD {{system is}} {{developed}} {{in order to}} provide computer aided design of pressure differential flowmeters for fluid energy carriers and to simplify implementation of new normative documents (ISO 5167. 1, 2, 3, 4 - 2003 and GOST 8. 586. 1, 2, 3, 4, 5 - 2005). This program meets the requirements of the new Standards and provides accomplishment of the following tasks: verification of conditions (constraints) for application of the pressure differential method according to the requirements of new Standards; calculation of parameters of primary device, pipe straight lengths and the whole flowmeter according to the requirements of new Standards; calculation of <b>uncertainty</b> <b>of</b> <b>results</b> of fluid flowrate and volume measurement; design of optimal flowmeter as to the accuracy of measurement...|$|E
40|$|This {{theme of}} {{research}} has been developed around three areas:•Technologies: mechatronic systems and their actuation systems include power transmission elements from different fields, which are usually treated independently. The goal of my research is to synthesize them in a coupled manner. •Models: the models to set up in order to design a system are of different natures. Simulation models are used to calculate all quantities that influence {{the choice of the}} components. Evaluation models enable lifetime or reliability to be calculated. Estimation models give rapid access to component characteristics, generally obtained after detailed design. •Methods: to optimize a design, it is advantageous to develop meta-models to replace time consuming simulations, to analyse the <b>uncertainty</b> <b>of</b> <b>results,</b> and to help the implementation of the design procedures...|$|E
40|$|AbstractThe {{accomplishment}} of building investments is a long-lasting process resulting in considerable financial outlays. Crucial decisions concerning construction solutions, technology of execution, organization of construction works are {{taken at the}} stage of planning and programming. At that time {{it is possible to}} predict the budget of an undertaking when it comes to preliminary cost estimation. However, taking accidentality, <b>uncertainty</b> <b>of</b> <b>results,</b> appearance of factors that were not planned earlier into account, {{it is important to know}} the total value of works. The solutions adopted at the stage of planning are reflected in costs in life cycle and determine the kind and range of possible risks. PERT approach used in the paper allows to present the costs of separate stages of the whole undertaking in ranges along with the probability of appearance...|$|E
30|$|Today, {{applications}} of modeling and regression {{methods have been}} widely spread in different fields of the science. These methods were applied for increasing the quality of forecasts using the sample data sets from experiments and different investigations. Appropriate use of these methods is very useful for resolve <b>of</b> <b>uncertainties</b> and <b>results</b> <b>of</b> predictions, when obtain of information is difficult and sometimes impossible.|$|R
50|$|The {{rules of}} {{significance}} arithmetic are an approximation based on statistical rules {{for dealing with}} probability distributions. See the article on propagation <b>of</b> <b>uncertainty</b> for these more advanced and precise rules. Significance arithmetic rules rely {{on the assumption that}} the number of significant figures in the operands gives accurate information about the <b>uncertainty</b> <b>of</b> the operands and hence the <b>uncertainty</b> <b>of</b> the <b>result.</b> For an alternative see interval arithmetic.|$|R
40|$|A {{new method}} of {{measurements}} of angular correlations in neutron decay is proposed. It excludes {{the need of}} precise spectroscopy of decay products and thus promises to make the systematic <b>uncertainties</b> <b>of</b> the <b>results</b> much lower than in experiments carried out up to day. Comment: 22 pages, including 8 figure...|$|R
40|$|Introduction: Statistical {{heterogeneity}} {{can increase}} the <b>uncertainty</b> <b>of</b> <b>results</b> and reduce the quality of evidence derived from systematic reviews. At present, it is uncertain what are the major factors that account for heterogeneity in meta-analyses of analgesic adjuncts. Therefore, {{the aim of this}} review was to identify whether various covariates could explain statistical heterogeneity and use this to improve accuracy when reporting the efficacy of analgesics. Methods: We searched for reviews using MEDLINE, EMBASE, CINAHL, AMED and Cochrane Database of Systematic Reviews. Firstly, we identified the existence of considerable statistical heterogeneity. Secondly, we conducted meta-regression analysis for the outcome of 24 -hour morphine consumption using baseline risk and other covariates. Finally, we constructed a league table of analgesic adjuncts assuming a fixed consumption of postoperative morphine. Results: We included 344 randomized controlled trials with 28, 130 participants. 91...|$|E
40|$|The {{design of}} {{synoptic}} monitoring strategies for soils is usually {{strongly influenced by}} the need to be representative of large areas. This can result in less attention being paid to estimating analytical error leading to <b>uncertainty</b> <b>of</b> <b>results.</b> To understand the relative benefits of intensive and extensive monitoring of soils, sampling was carried out at different resolutions and using different sampling strategies in an area of North Italy [1] (Pavia Province, Lombardy). A sampling strategy that incorporated an estimation of measurement uncertainty, was found preferable to more extensive sampling without estimation of overall measurement quality. A soil monitoring system should incorporate estimates of uncertainty from all sources including the primary sampling, sample preparation and chemical analysis. It must also include contributions from systematic errors rather than from random errors alone. JRC. DDG. H. 7 -Land management and natural hazard...|$|E
40|$|In {{traditional}} tutoring system, {{the quality}} assessment {{of student learning}} is identified by hard division regardless of the <b>uncertainty</b> <b>of</b> <b>results.</b> In the paper, Cloud M odel T heory was applied to build student model in Intelligent Tutoring System. And a quality assessment approach of student learning based on cloud m odel was proposed. S tudents’ test score s are regarded as cloud drop let s, and data is discret ized according to contribution of the cloud concept in the research. Further more, cloud transform algorithm is introduced to compute membership cloud. Finally, maximum determination algorithm is used to obtain more actual grade division of learning quality. Experimental results show that membership concept can reflect not only t he mastery level of knowledge points but also the stability and psychology {{in the process of}} student learning. The study will help improve the efficiency of Intelligent Tutoring System. </p...|$|E
40|$|Life Cycle Assessment (LCA) is a {{methodology}} {{to assess the}} environmental impacts associated with a product over its life cycle. Most often, an average scenario of production at the national scale is designed and assessed, using average data. However, stakeholders are facing two difficulties to interpret these LCA results: the meaning of average scenario at this scale and the <b>uncertainty</b> <b>of</b> the LCA <b>results</b> expected to be large but rarely accounted for. While stochastic modelling is widely recognised as very relevant to address the <b>uncertainty</b> <b>of</b> LCA <b>results,</b> it has been scarcely applied to LCA studies on agricultural products and no harmonised practice exists. Taking the Global Warming Potential (GWP) of one kg of NZ milk as a case study, we used stochastic modelling to quantify the <b>uncertainty</b> <b>of</b> LCA <b>results</b> <b>of</b> an agricultural product and discussed different assumptions on the characterisation of the distribution functions of the input variables. A refining process for the average scenario, focussing on the key input variables was proposed to produce more meaningful results for decision makers. In our case study, the refining process focussed on the on-farm dry matter intake by cows and the nitrous oxide emission factor from the nitrogen excreted. Th...|$|R
40|$|The summary <b>of</b> the <b>results</b> <b>of</b> our next-to-next-to-leading fits of Tevatron {{experimental}} data for xF_ 3 structure {{function of the}} ν N deep-inelastic scattering is given. The special {{attention is paid to}} the extraction of twist- 4 contributions and demonstration of the interplay between these effects and higher-order perturbative QCD corrections. The factorization and renormalization scale <b>uncertainties</b> <b>of</b> the <b>results</b> obtained are analyse...|$|R
40|$|The article {{considers}} {{different aspects}} of the Russian State and Terek Cossacks interaction as a mechanism of organization of their life activities in the changing conditions of XVIII-XIX centuries, caused by the Empire politics. The emphasis is put on the study of the mechanisms of this interaction and the <b>uncertainty</b> <b>of</b> its <b>results...</b>|$|R
