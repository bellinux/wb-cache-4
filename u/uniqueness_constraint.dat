44|57|Public
5000|$|ID: the {{effective}} {{value of the}} attribute must be a valid identifier, and {{it is used to}} define and anchor to the current element the target of references using this defined identifier (including as document fragment identifiers that may be specified at end of an URI after a [...] "#" [...] sign); it is an error if distinct elements in the same document are defining the same identifier; the <b>uniqueness</b> <b>constraint</b> also implies that the identifier itself carries no other semantics and that identifiers must be treated as opaque in applications; note that XML also predefines the standard pseudo-attribute [...] "xml:id" [...] with this type, without needing any declaration in the DTD, so the <b>uniqueness</b> <b>constraint</b> also applies to these defined identifiers when they are specified anywhere in a XML document.|$|E
50|$|The camera {{attributes}} must be known, {{focal length}} and distance apart etc., and a calibration done. Once this is completed the {{systems can be}} used to sense the distances of objects by triangulation. Finding the same singular physical point in the two left and right images is known as the correspondence problem. Correctly locating the point gives the computer the capability to calculate the distance that the robot or camera is from the object. On the BH2 Lunar Rover the cameras use five steps: a bayer array filter, photometric consistency dense matching algorithm, a Laplace of Gaussian (LoG) edge detection algorithm, a stereo matching algorithm and finally <b>uniqueness</b> <b>constraint.</b>|$|E
50|$|Keys {{provide the}} means for {{database}} users and application software to identify, access and update information in a database table. There may be several keys in any given table. For example, two distinct keys in a table of employees might be employee number and login name. The enforcement of a key constraint (i.e. a <b>uniqueness</b> <b>constraint)</b> in a table is also a data integrity feature of the database. The DBMS prevents updates that would cause duplicate key values and thereby ensures that tables always comply with the desired rules for uniqueness. Proper selection of keys when designing a database is therefore {{an important aspect of}} database integrity.|$|E
40|$|Business rules {{should be}} {{validated}} by business domain experts, and hence specified {{in a language}} easily understood by business people. This is the fourteenth {{in a series of}} articles on expressing business rules formally in a high-level, textual language. The first article [2] discussed criteria for a business rules language, and verbalization of simple <b>uniqueness</b> and mandatory <b>constraints</b> on binary associations. Article two [3] examined hyphen-binding, and verbalization of internal <b>uniqueness</b> <b>constraints</b> that span a whole association, or that apply to n-ary associations. Article three [4] covered verbalization of basic external <b>uniqueness</b> <b>constraints.</b> Article four [5] considered relational-style verbalization of external <b>uniqueness</b> <b>constraints</b> involving nesting or long join paths, as well as attribute-style verbalization of <b>uniqueness</b> <b>constraints</b> and simple mandatory constraints. Article five [6] discussed verbalization of mandatory constraints on roles of n-ary associations, and disjunctive mandatory constraints (also known as inclusiveor constraints) over sets of roles. Article six [7] considered verbalization of value constraints. Article seven [8] examined verbalization of subset constraints. Article eight [9] discussed verbalization of equality constraints. Article nine [10] covered verbalization of exclusion constraints. Article ten [11] dealt with verbalization of internal frequency constraints on single roles. Article eleven [12] considered verbalization of multi-role, and external, frequency constraints. Article twelve [13] discussed verbalization of rin...|$|R
40|$|In {{this article}} the Uniquest Algorithm (the "quest for uniqueness"), {{defined in the}} Predicator Model, is {{discussed}} in depth. The Predicator Model is a general platform for object-role models. The Uniquest Algorithm is a constructive formal definition of the semantics of <b>uniqueness</b> <b>constraints.</b> As such, it facilitates the implementation in so-called CASE-tools. The Uniquest Algorithm provides a systematic approach for the interpretation of complex <b>uniqueness</b> <b>constraints.</b> This interpretation process is easily traced, using an extra formalism, called the Object Relation Network (ORN). The ORN is a directed graph with labelled edges, representing an object-role information structure. Intermediate results that are {{outside the scope of}} the information structure at hand, are represented elegantly as an ORN. A number of theoretical and practical examples prove the power of the Uniquest Algorithm. In these examples we will encouter complex <b>uniqueness</b> <b>constraints,</b> that are missed easily. The Un [...] ...|$|R
40|$|An {{alternative}} {{formulation of}} the multigroup common factor model with minimal <b>uniqueness</b> <b>constraints</b> is considered. This alternative formulation {{is based on a}} simple identification constraint that is related to the standard maximum likelihood constraint used in single-group common factor analysis. It is argued that the alternative formulation leads to less technical difficulties in applications than earlier formulations of this multigroup common factor model. Furthermore, associated tests for various measurement invariance constraints across groups are proposed, such as an omnibus test for the absence of uniform bias. By means of an empirical example, the fitting of several multigroup common factor models with minimal <b>uniqueness</b> <b>constraints</b> and the testing for measurement invariance over groups are demonstrated. The nesting of multigroup confirmatory factor models under the multigroup common factor model with minimal <b>uniqueness</b> <b>constraints</b> is also discussed. Finally, a small study is performed to investigate the drop in power to detect uniform bias in using the multigroup common factor model with minimal uniqueness constrains instead of a confirmatory special case. The results of the study show a small drop in power under all research conditions...|$|R
5000|$|First, to {{establish}} a password p with server Steve, client Carol picks a small random salt s, and computes x = H(s, p), v = g. Steve stores v and s, indexed by , as Carol's password verifier and salt. x is discarded because it {{is equivalent to the}} plaintext password p. This step is completed before the system is used as part of the user registration with Steve. Note that the salt s is shared and exchanged to negotiate a session key later so the value could be chosen by either side but is done by Carol so that she can register , s and v in a single registration request. (Note server Steve should enforce a <b>uniqueness</b> <b>constraint</b> on all [...] "v" [...] to protect against someone stealing his database observing which users have identical passwords.) ...|$|E
40|$|In recent years, {{a number}} of {{proposals}} {{have been made to}} extend conventional conceptual data modeling techniques with concepts for modeling complex object structures. Among the most prominent proposed concepts is the concept of collection type. A collection type is an object type of which the instances are sets of instances of an other object type. A drawback of the introduction of such a new concept is that the formal definition of the technique involved becomes considerably more complex. In this paper a new kind of constraint, the extensional <b>uniqueness</b> <b>constraint,</b> is introduced, providing an alternative treatment of complex object types. The formal definition of this constraint type is presented, the advantages of its introduction are discussed, and its consequences for, among others, identification schemes are elaborated. Keywords: Conceptual Data Modeling, ER, NIAM, OO, <b>Uniqueness</b> <b>constraint,</b> Extensional <b>uniqueness</b> <b>constraint,</b> Identification, Collection type. Classification: AMS [...] ...|$|E
40|$|International audienceIn this paper, we {{consider}} sparse decomposition (SD) of twodimensional (2 D) signals on overcomplete dictionaries with separable atoms. Although, this {{problem can be}} solved by converting it to the SD of one-dimensional (1 D) signals, this approach requires {{a tremendous amount of}} memory and computational cost. Moreover, the <b>uniqueness</b> <b>constraint</b> obtained by this approach is too restricted. Then in the paper, we present an algorithm to be used directly for sparse decomposition of 2 D signals on dictionaries with separable atoms. Moreover, we will state another <b>uniqueness</b> <b>constraint</b> for this class of decomposition. Our algorithm is obtained by modifying the Smoothed L 0 (SL 0) algorithm, and hence we call it two-dimensional SL 0 (2 D-SL 0) ...|$|E
5000|$|In practice, some <b>uniqueness</b> <b>constraints</b> {{are needed}} for and/or [...] In , the [...] {{function}} uses corner constraints by default, {{which means that the}} top R rows of [...] is set to [...] RR-VGLMs were proposed in 2003.|$|R
40|$|Description Logics (DLs) with {{concrete}} domains are {{a useful tool}} in many applica-tions. To further enhance the expressive power of such DLs, it has been proposed to add database-style key constraints. Up to now, however, only <b>uniqueness</b> <b>constraints</b> have been considered in this context, thus neglecting the second fun-damental family of key constraints: functional dependencies. In this paper, we consider the basic DL {{with concrete}} domains ALC(D), extend it with functional dependencies, and analyze {{the impact of this}} extension on the decidability and complexity of reasoning. Though intuitively the expressivity of functional depen-dencies seems weaker than that of <b>uniqueness</b> <b>constraints,</b> we are able to show that the former have a similarly severe impact on the computational properties: reasoning is undecidable in the general case, and NExpTime-complete in some slightly restricted variants of our logic...|$|R
40|$|We {{establish}} the equivalence of: (1) the logical implication {{problem for a}} description logic dialect called DLClass that includes a concept constructor for expressing <b>uniqueness</b> <b>constraints,</b> (2) the logical implication problem for path functional dependencies (PFDs), and (3) the problem of answering queries in deductive databases with limited use of successor functions. As a consequence, we settle an open problem concerning lower bounds for the PFD logical implication problem and show that a regularity condition for DLClass that ensures low order polynomial time decidability for its logical implication problem is tight. 1 Introduction Description Logics (DL) have found many applications in information systems [3]. They also address problems in query optimization and information integration [1, 8, 9]. In such applications, it becomes essential to capture knowledge that relates to various kinds of <b>uniqueness</b> <b>constraints.</b> For example, in a hypothetical patient management system, [...] ...|$|R
40|$|Abstract: Psychophysical {{studies have}} shown that image features, under certain conditions, can give rise to {{multiple}} visible binocular matches. These findings are difficult to reconcile with the traditional interpretation of the <b>uniqueness</b> <b>constraint.</b> A new interpretation, a conditional <b>uniqueness</b> <b>constraint,</b> is proposed that allows multiple matching of similar primitives when a one-to-one correspondence does not exist locally within corresponding image regions, but prohibits it when a one-to-one correspondence does exist. A cooperative network model and an implementation are also described, where this constraint is enforced at each network node by a simple inhibitory (dual) AND-gate mechanism. The model performs with high accuracy {{for a wide range of}} stimuli, including multiple transparent surfaces, and seems able to account for several aspects of human binocular matching that previous models have not been able to account for. ...|$|E
40|$|Abstract We {{present a}} {{coherence}} condition for a boolean complete description logic with feature inversions {{and a very}} general form of <b>uniqueness</b> <b>constraint</b> that enables, among other things, the capture of unary functional dependencies. The condition is sufficiently weak to allow the transfer of relational and emerging object-oriented normalization techniques while still ensuring that the associated logical implication problem remains DEXPTIME-complete...|$|E
40|$|In {{this paper}} an {{solution}} for multi-threaded handling of iTask webrequests is being proposed. This solution preserves {{much of the}} original iTask system and aimed to preserve the <b>uniqueness</b> <b>constraint</b> present on the ”World ” object. There are however several issues with this solution {{that need to be}} solved before attempting implementation. This paper should therefore be considered as a proof of concept than a detailed guid...|$|E
5000|$|Facilities for {{defining}} <b>uniqueness</b> <b>constraints</b> and referential integrity are more powerful: unlike the ID and IDREF constraints in DTDs, {{they can be}} scoped to any part of a document, can be of any data type, can apply to element as well as attribute content, and can be multi-part (for example the combination of first name and last name must be unique).|$|R
40|$|Abstract. Representing parthood {{relations}} in ORM has received little attention, despite its added-value of the semantics at the conceptual level. We introduce a high-level taxonomy {{of types of}} meronymic and mereological relations, use it to construct a decision procedure to determine which type of part-whole role is applicable, and incrementally add mandatory and <b>uniqueness</b> <b>constraints.</b> This enables the conceptual modeller to develop models that are closer to the real-world subject domain semantics, hence improve quality of the software. ...|$|R
40|$|Prior work on {{computing}} queries from materialized views {{has focused}} on views defined by expressions consisting of selection, projection, and inner joins, with an optional aggregation on top (SPJG views). This paper provides the first view matching algorithm for views that may also contain outer joins (SPOJG views). The algorithm relies on a normal form for SPOJ expressions and does not use bottomup syntactic matching of expressions. It handles any combination of inner and outer joins, deals correctly with SQL bag semantics and exploits not-null <b>constraints,</b> <b>uniqueness</b> <b>constraints</b> and foreign key constraints. ...|$|R
40|$|We {{examine the}} stereo {{correspondence}} {{problem in the}} presence of slanted scene surfaces. In particular, we highlight a previously overlooked geometric fact: a horizontally slanted surface (i. e. having depth variation {{in the direction of the}} separation of the two cameras) will appear horizontally stretched in one image as compared to the other image. Thus, while corresponding two images, N pixels on a scanline in one image may correspond to a different number of pixels M in the other image. This leads to three important modifications to existing stereo algorithms: (a) due to unequal sampling, intensity matching metrics such as the popular Birchfield-Tomasi procedure must be modified, (b) unequal numbers of pixels in the two images must be allowed to correspond to each other, and (c) the <b>uniqueness</b> <b>constraint,</b> which is often used for detecting occlusions, must be changed to a 3 D <b>uniqueness</b> <b>constraint.</b> This paper discusses these new constraints and provides a simple scanline based matching algorithm for illustration. We experimentally demonstrate test cases where existing algorithms fail, and how the incorporation of these new constraints provides correct results. Experimental comparisons of the scanline based algorithm with standard data sets are also provided. 1...|$|E
40|$|A multi-resolution image {{matching}} technique {{based on}} multiwavelets {{followed by a}} coarse to fine strategy is presented. The technique addresses the estimation of optimal corresponding points and the corresponding disparity maps {{in the presence of}} occlusion, ambiguity and illuminative variations in the two perspective views taken by two different cameras or at different ligh t ing conditions. The problem of occlusion and ambiguity is addressed by a geometric topological refining approach along with the <b>uniqueness</b> <b>constraint</b> whereas the illuminative variation is dealt by using windowed normalized correlation...|$|E
40|$|Given {{two images}} of a scene, the problem of finding a map {{relating}} the points in the two images {{is known as the}} correspondence problem. Stereo correspondence is a special case in which corresponding points lie on the same row in the two images; optical flow is the general case. In this thesis, we argue that correspondence is inextricably linked to other problems such as depth segmentation, occlusion detection and shape estimation, and can-not be solved in isolation without solving each of these problems concurrently within a compositional framework. We first demonstrate the relationship between correspondence and segmentation in a world devoid of shape, and propose an algorithm based on con-nected components which solves these two problems simultaneously by matching image pixels. Occlusions are found by using the <b>uniqueness</b> <b>constraint,</b> which forces one pixel in the first image to match exactly one pixel in the second image. Shape is then introduced into the picture, and it is revealed that a horizontally slanted surface is sampled differently by the two cameras of a stereo pair, creating images of different width. In this scenario, we show that pixel matching must be replaced by interval matching, to allow intervals of different width in the two images to correspond. A new interval <b>uniqueness</b> <b>constraint</b> i...|$|E
40|$|We {{establish}} the equivalence of: (1) the logical implication {{problem for a}} description logic dialect called DLClass that includes a concept constructor for expressing <b>uniqueness</b> <b>constraints,</b> (2) the logical implication problem for path functional dependencies (PFDs), and (3) the problem of answering queries in deductive databases with limited use of successor functions. As a consequence, we settle an open problem concerning lower bounds for the PFD logical implication problem and show that a regularity condition for DLClass that ensures low order polynomial time decidability for its logical implication problem is tight...|$|R
40|$|A stereo {{algorithm}} is presented that optimizes a maximum likelihood cost function. The maximum likelihood cost function assumes that corresponding {{features in the}} left and right images are Normally distributed about a common true value and consists of a weighted squared error term if two features are matched or a (xed) cost if a feature is determined to be occluded. The stereo algorithm nds the set of correspondences that maximize the cost function subject to ordering and <b>uniqueness</b> <b>constraints.</b> The stereo {{algorithm is}} independent of the matching primitives. However, for the experiments described in this paper, matching is performed on the individual pixel intensities. Contrary to popular belief, the pixel-based stereo appears to be robust for a variety of images. It also has the advantages of (i) providing a dense disparity map, (ii) requiring no feature extraction and (iii) avoiding the adaptive windowing problem of area-based correlation methods. Because feature extraction and windowing are unnecessary, avery fast implementation is possible. Experimental results reveal that good stereo correspondences can be found using only ordering and <b>uniqueness</b> <b>constraints,</b> i. e. without local smoothness constraints. However, it is shown that the original maximum likelihood stereo algorithm exhibits multiple global minima. The dynamic programming algorithm is guaranteed to nd one, but not necessarily the same one for each epipolar scanline causing erroneou...|$|R
40|$|New <b>uniqueness</b> <b>constraints</b> for {{database}} {{tables and}} validations for corresponding models; new RSpec tests; several bug fixes. This release implements several value and <b>uniqueness</b> <b>constraints</b> on database tables aimed at {{ensuring that the}} BETYdb database will not contain multiple rows describing the same citation, site, species, treatment, or other entity. In conjunction with the database changes, new model validations {{have been added to}} several of the models so that potentially invalid database insertions or updates are caught at the Rails level before the insertion or update is attempted on the database. Second, several bugs existed that prevented access to many pages of the site [...] especially pages for editing database entities. Many of these were introduced by changes to the routing file that eliminated most wildcard routes. These bugs have now been fixed and several new tests have been added to ensure these bugs don't re-appear. Changes Pertinent to PEcAn Users Administrators need to do a database migration. See "Database Changes" below. Summary of Changes New Features New Validations to Prevent Duplicate Rows and Missing Data Bug Fixes Many pages that were previously inaccessible are once again reachable. Prior to this update, attempting to visit these pages yielded the following error: The page you were looking for doesn't exist. You may have mistyped the address or the page may have moved. Affected pages included (for example) the Covariate editing page the Model editing page the Modeltype editing page the Species editing page the Traits editing page Steps Needed for Upgrade Database Changes Administrators need to do database migrations! One migration has been added that new value and <b>uniqueness</b> <b>constraints</b> and also adds some convenience functions for implementing these constraints. The database version for this release is 20150202220519. Status of RSpec Tests All tests continue to pass when run in the default environment and can be run using the command bundle exec rspec Every effort has been made to make this command idempotent: You should be able to run the tests multiple times without reloading the test fixtures between runs. Complete details for running the rspec tests are on the updated Wiki page at [URL]...|$|R
40|$|Multiple {{explanatory}} frameworks may {{be required}} to provide an adequate account of human cognition. This paper embeds the classical account within a neural network framework, exploring the encoding of syntactically-structured objects over the synchronic-diachronic characteristics of networks. Synchronic structure is defined in terms of temporal binding and the superposition of states. To accommodate asymmetric relations, synchronic structure is subject to the type <b>uniqueness</b> <b>constraint</b> (TUC). The nature of synchronic structure is shown to underlie X-bar theory that characterizes the phrasal structure of human languages. The derivation differentiates core X-bar properties from language-specific properties...|$|E
40|$|Slightly {{modified}} version, as {{it appeared}} in Striegnitz, K. et al. (Eds.) Special Issue: The Language Sections of the ESSLLI- 01 Student Session, Human Language Technology Theses. 2002. “Incomplete” definite descriptions (i. e. descriptions that violate the <b>uniqueness</b> <b>constraint)</b> have been offered various accounts in semantics. Among them, the so-called ellipsis account, which analyzes “the F” as elliptical for “the F which is that F”. I begin by arguing that the objections raised against this account have not been conclusive, {{and go on to}} supply a new argument against it, which consists in showing such demonstrative completions to be semantically redundant...|$|E
40|$|An {{efficient}} {{technique of}} stereo matching to compute dense disparity map is presented. The technique is point -oriented and uses multiple windows {{to enhance the}} strategy of finding a best match. In addit ion to that, a novel technique for adapting the window size based on multiple windows is discussed. The window size is recursively adapted using normalized sum of squared differences similarity measure and also {{the output of the}} multi-windowing method. Several constraints like epipolar constraint, <b>uniqueness</b> <b>constraint</b> and mutual correspondence constraint are also incorporated into the algorithm to prune bad matches. The results of the technique which has been tested on real images are also presented...|$|E
40|$|To {{obtain a}} {{correctly}} integrated schema which {{corresponds to the}} modeled real world, integrity constraints {{have to be considered}} in the schema integration process. Previous publications on this area focused on the integration of object constraints as well as certain class <b>constraints</b> like <b>uniqueness</b> <b>constraints.</b> In this paper, we discuss the ability to integrate aggregation constraints which are integrity constraints that are based on aggregate functions like min, max, avg, sum, and count. For this purpose, we define two fundamental integration properties: decomposability and composability of integrity constraints. By means of these properties we are then able to conclude whether an aggregation constraint can be integrated or not...|$|R
40|$|Many data-management {{applications}} require integrating {{data from}} a variety of sources, where different sources may refer to the same real-world entity in different ways and some may even provide erroneous data. An important task in this process is to recognize and merge the various references that refer to the same entity. In practice, some attributes satisfy a uniqueness constraint—each real-world entity (or most entities) has a unique value for the attribute (e. g., business contact phone, address, and email). Traditional techniques tackle this case by first linking records that are likely to refer to the same real-world entity, and then fusing the linked records and resolving conflicts if any. Such methods can fall short for three reasons: first, erroneous values from sources may prevent correct linking; second, the real world may contain exceptions to the <b>uniqueness</b> <b>constraints</b> and always enforcing uniqueness can miss correct values; third, locally resolving conflicts for linked records may overlook important global evidence. This paper proposes a novel technique to solve this problem. The key component of our solution is to reduce the problem into a k-partite graph clustering problem and consider in clustering both similarity of attribute values and the sources that associate a pair of values in the same record. Thus, we perform global linkage and fusion simultaneously, and can identify incorrect values and differentiate them from alternative representations of the correct value from the beginning. In addition, we extend our algorithm to be tolerant to a few violations of the <b>uniqueness</b> <b>constraints.</b> Experimental results show accuracy and scalability of our technique. 1...|$|R
40|$|Abstract. In the {{relational}} model of data the Boyce-Codd-Heath nor-mal form, commonly just known as Boyce-Codd normal form, guarantees {{the elimination of}} data redundancy in terms of functional dependencies. For efficient means of data processing the industry standard SQL per-mits partial data and duplicate rows of data to occur in database sys-tems. Consequently, the combined class of <b>uniqueness</b> <b>constraints</b> and functional dependencies is more expressive than the class of functional dependencies itself. Hence, the Boyce-Codd-Heath normal form is not suitable for SQL databases. We characterize the associated implication problem of the combined class {{in the presence of}} NOT NULL constraints axiomatically, algorithmically and logically. Based on these results we are able to establish a suitable normal form for SQL. ...|$|R
40|$|Abstract. Determining shape from stereo {{has often}} been posed as a global {{minimization}} problem. Once formulated, the minimization problems are then solved {{with a variety of}} algorithmic approaches. These approaches include techniques such as dynamic programming min-cut and alpha-expansion. In this paper we show how an algorithmic technique that constructs a discrete spatial minimal cost surface can be brought to bear on stereo global minimization problems. This problem can then be reduced to a single min-cut problem. We use this approach to solve a new global minimization problem that naturally arises when solving for three-camera (trinocular) stereo. Our formulation treats the three cameras symmetrically, while imposing a natural occlusion cost and <b>uniqueness</b> <b>constraint.</b> ...|$|E
40|$|In this paper, {{the depth}} cue {{due to the}} {{assumption}} of texture uniqueness is reviewed. The spatial direction over which a similarity measure is optimized, {{in order to establish}} a stereo correspondence, is considered and methods to increase the precision and accuracy of stereo reconstructions are presented. It is further presented that the proposed method is quite robust to projective distortions due to less accurate camera parameters, possibly obtained through selfcalibration. An efficient implementation of the above methods is also offered, based on a scale-space treatment of the data. The above contributions are integrated in a generic and parallelizable implementation of the <b>uniqueness</b> <b>constraint</b> to observe speedup and increase in the fidelity of surface reconstruction. 1...|$|E
40|$|This paper {{presents}} a quasi-dense matching algorithm between images based on match propagation principle. The algorithm starts from {{a set of}} sparse seed matches, then propagates to the neighboring pixels by the best- rst strategy, and produces a quasidense disparity map. The quasi-dense matching aims at broad modeling and visualization applications which rely heavily on matching information. Our algorithm is robust to initial sparse match outliers due to the best- rst strategy; It is ecient {{in time and space}} as it is only output sensitive; It handles half-occluded areas because of the simultaneous enforcement of newly introduced discrete 2 D gradient disparity limit and the <b>uniqueness</b> <b>constraint.</b> The properties of the algorithm are discussed and empirically demonstrated...|$|E
40|$|Abstract. In this paper, {{we present}} an {{algorithm}} for estimating disparity for images containing large textureless regions. We propose a fast and efficient region growing algorithm for estimating the stereo disparity. Though we present results on ice images, the algorithm {{can be easily}} used for other applications. We modify the first-best region growing algorithm using relaxed <b>uniqueness</b> <b>constraints</b> and matching for sub-pixel values and slant surfaces. We provide an efficient method for matching multiple windows using a linear transform. We estimate the parameters required by the algorithm automatically based on initial correspondences. Our method was tested on synthetic, benchmark and real outdoor data. We quantitatively demonstrated that our method performs well in all three cases. ...|$|R
40|$|In this paper, {{we propose}} a {{symmetric}} stereo model to handle occlusion in dense two-frame stereo. Our occlusion reasoning is directly {{based on the}} visibility constraint that is more general than both ordering and <b>uniqueness</b> <b>constraints</b> used in previous work. The visibility constraint requires occlusion in one image and disparity in the other to be consistent. We embed the visibility constraint within an energy minimization framework, resulting in a symmetric stereo model that treats left and right images equally. An iterative optimization algorithm is used to approximate the minimum of the energy using belief propagation. Our stereo model can also incorporate segmentation as a soft constraint. Experimental results on the Middlebury stereo images show that our algorithm is state-of-the-art. ...|$|R
40|$|In the {{relational}} model of data the Boyce-Codd-Heath normal form, commonly just known as Boyce-Codd normal form, guarantees {{the elimination of}} data redundancy in terms of functional dependencies. For efficient means of data processing the industry standard SQL permits partial data and duplicate rows of data to occur in database systems. Consequently, the combined class of <b>uniqueness</b> <b>constraints</b> and functional dependencies is more expressive than the class of functional dependencies itself. Hence, the Boyce-Codd-Heath normal form is not suitable for SQL databases. We characterize the associated implication problem of the combined class {{in the presence of}} NOT NULL constraints axiomatically, algorithmically and logically. Based on these results we are able to establish a suitable normal form for SQL...|$|R
