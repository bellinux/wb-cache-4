1253|5046|Public
5000|$|Mapping: Solr maps the <b>user</b> <b>query</b> to the {{documents}} {{stored in the}} database to find the appropriate result.|$|E
5000|$|Reference Assistance — {{locating}} numeric or geospatial datasets containing measurable {{variables on}} a particular topic or group of topics, {{in response to a}} <b>user</b> <b>query.</b>|$|E
50|$|This way, any {{subsequent}} page {{request from}} this user {{will carry the}} same query string , {{making it possible to}} establish that all these pages have been viewed by the same <b>user.</b> <b>Query</b> strings are often used in association with web beacons.|$|E
40|$|Radiologists {{frequently}} {{search the}} Web to find {{information they need}} to improve their practice, and knowing the types of information they seek could be useful for evaluating Web resources. Our goal was to develop an automated method to categorize unstructured <b>user</b> <b>queries</b> using a controlled terminology and to infer the type of information users seek. We obtained the query logs from two commonly used Web resources for radiology. We created a computer algorithm to associate RadLex-controlled vocabulary terms with the <b>user</b> <b>queries.</b> Using the RadLex hierarchy, we determined the high-level category associated with each RadLex term to infer the type of information users were seeking. To test the hypothesis that the term category assignments to <b>user</b> <b>queries</b> are non-random, we compared the distributions of the term categories in RadLex with those in <b>user</b> <b>queries</b> using the chi square test. Of the 29, 669 unique search terms found in <b>user</b> <b>queries,</b> 15, 445 (52 %) could be mapped to one or more RadLex terms by our algorithm. Each query contained an average of one to two RadLex terms, and the dominant categories of RadLex terms in <b>user</b> <b>queries</b> were diseases and anatomy. While the same types of RadLex terms were predominant in both RadLex itself and <b>user</b> <b>queries,</b> the distribution of types of terms in <b>user</b> <b>queries</b> and RadLex were significantly different (p[*]<[*] 0. 0001). We conclude that RadLex can enable processing and categorization of <b>user</b> <b>queries</b> of Web resources and enable understanding the types of information users seek from radiology knowledge resources on the Web...|$|R
40|$|Entity-oriented {{retrieval}} aims {{to return}} a list of relevant entities rather than documents to provide exact answers for <b>user</b> <b>queries.</b> The nature of entity-oriented retrieval requires identifying the semantic intent of <b>user</b> <b>queries,</b> i. e., understanding the semantic role of query terms and determining the semantic categories which indicate the class of target entities. Existing methods {{are not able to}} exploit the semantic intent by capturing the semantic relationship between terms in a query and in a document that contains entity related information. To improve the understanding of the semantic intent of <b>user</b> <b>queries,</b> we propose concept-based retrieval method that not only automatically identifies the semantic intent of <b>user</b> <b>queries,</b> i. e., Intent Type and Intent Modifier but introduces concepts represented by Wikipedia articles to <b>user</b> <b>queries.</b> We evaluate our proposed method on entity profile documents annotated by concepts from Wikipedia category and list structure. Empirical analysis reveals that the proposed method outperforms several state-of-the-art approaches...|$|R
40|$|Accessing online {{information}} {{remains an}} inexact science. While valuable {{information can be}} found, typically many irrelevant documents are also retrieved and many relevant ones are missed. Terminology mismatches between the <b>user's</b> <b>query</b> and document contents is a main cause of retrieval failures. Expanding a <b>user's</b> <b>query</b> with related words can improve search performance, but the problem of identifying related words remains. This research uses corpus linguistics techniques to automatically discover word similarities directly from {{the contents of the}} untagged TREC database and to incorporates that information in the PRISE information retrieval system. The similarities are calculated based on the contexts in which a set of target words appear. Using these similarities, <b>user</b> <b>queries</b> are automatically expanded, resulting in conceptual retrieval rather than requiring exact word matches between queries and documents. 1. INTRODUCTION Expanding a <b>user's</b> <b>query</b> with related terms can improve searc [...] ...|$|R
50|$|Variations of the tf-idf {{weighting}} scheme {{are often used}} by search engines as a central tool in scoring and ranking a document's relevance given a <b>user</b> <b>query.</b> tf-idf can be successfully used for stop-words filtering in various subject fields including text summarization and classification.|$|E
50|$|The {{founders of}} Imense are Dr Christopher Town and Dr David Sinclair. In their {{academic}} lives they developed thefirst 'Ontological Query Evaluation Language' (OQUEL) for image retrieval, which mapped a plain text <b>user</b> <b>query</b> onto a query over automatically recognized visual content in a corpus of images.|$|E
50|$|Midomi and Soundhound both utilize Query by Humming, or QbH. This is {{a branch}} off of {{acoustic}} fingerprints, {{but is still}} a musical retrieval system. After receiving a user generated hummed melody, which is the input query, and returns a ranked list of songs that is closest to the <b>user</b> <b>query.</b>|$|E
5000|$|... {{create an}} {{environment}} in which <b>user</b> <b>queries</b> may be formulated intuitively ...|$|R
3000|$|In above subsections, we {{presented}} the indexing strategy and the structures {{to store the}} precomputed results. Now {{we are ready to}} study the process of answering <b>user</b> <b>queries.</b> We classify the <b>user</b> <b>queries</b> into three types: (1) Single selectivity query, (2) Drill-down query, (3) Roll-up query as we can see in the following models: [...]...|$|R
40|$|In {{this paper}} {{we present a}} short survey of fuzzy and Semantic {{approaches}} to Knowledge Extraction. The goal of such approaches is to define flexible Knowledge Extraction Systems {{able to deal with}} the inherent vagueness and uncertainty of the Extraction process. It has long been recognised that interactivity improves the effectiveness of Knowledge Extraction systems. Novice <b>user's</b> <b>queries</b> is the most natural and interactive medium of communication and recent progress in recognition is making it possible to build systems that interact with the user. However, given the typical novice <b>user's</b> <b>queries</b> submitted to Knowledge Extraction systems, it is easy to imagine that the effects of goal recognition errors in novice <b>user's</b> <b>queries</b> must be severely destructive on the system's effectiveness. The experimental work reported in this paper shows that the use of classical Knowledge Extraction techniques for novice <b>user's</b> <b>query</b> processing is robust to considerably high levels of goal recognition errors. Moreover, both standard relevance feedback and pseudo relevance feedback can be effectively employed to improve the effectiveness of novice <b>user's</b> <b>query</b> processing. Comment: arXiv admin note: substantial text overlap with arXiv: 1206. 0925, arXiv: 1206. 161...|$|R
50|$|Document {{retrieval}} {{is defined}} as the matching of some stated <b>user</b> <b>query</b> against a set of free-text records. These records could be any type of mainly unstructured text, such as newspaper articles, real estate records or paragraphs in a manual. User queries can range from multi-sentence full descriptions of an information need to a few words.|$|E
50|$|Archie {{changed all}} that. It {{combined}} a script-based data gatherer, which fetched site listings of anonymous FTP files, {{with a regular}} expression matcher for retrieving file names matching a <b>user</b> <b>query.</b> (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.|$|E
5000|$|Many {{queries are}} short and query terms are noisy. As an example, in the KDDCUP 2005 dataset, queries {{containing}} 3 words are most frequent (22%). Furthermore, 79% queries {{have no more}} than 4 words. A <b>user</b> <b>query</b> often has multiple meanings. For example, [...] "apple" [...] can mean a kind of fruit or a computer company. [...] "Java" [...] can mean a programming language or an island in Indonesia. In the KDDCUP 2005 dataset, most of the queries contain more than one meaning. Therefore, only using the keywords of the query {{to set up a}} vector space model for classification is not appropriate.|$|E
50|$|Different {{implementations}} of CBIR {{make use}} {{of different types of}} <b>user</b> <b>queries.</b>|$|R
5000|$|... audit all <b>users</b> <b>queries,</b> so <b>users</b> using system {{incorrectly}} can {{be investigated}} ...|$|R
40|$|International audienceObtaining {{correct and}} {{relevant}} {{information at the}} right time to <b>user's</b> <b>query</b> is quite a difficult task. This becomes even complex, if the query terms have many meanings and occur in different varieties of domain. This paper presents a fuzzy-ontology based information retrieval system that determine the semantic equivalence between terms in a query and terms in a document by relating the synonyms of query terms with those of document terms. Hence, documents could be retrieved based on the meaning of query terms. The challenge has been that surface form does not sufficiently retrieve relevant document to <b>user's</b> <b>query.</b> However, the results presented showed that the Fuzzy-Ontology Information Retrieval system successfully retrieve relevant documents to <b>user's</b> <b>query.</b> This is irrespective of different meaning and varieties of domain. The System was tested on words with different meanings and some set of <b>user's</b> <b>query</b> from varied domains...|$|R
50|$|Distributed {{computing}} {{projects such}} as SETI@home and the Great Internet Mersenne Prime Search, as well as Internet-dependent applications like Google Earth, rely primarily on client-side operations. They initiate a connection with the server (either {{in response to a}} <b>user</b> <b>query,</b> as with Google Earth, or in an automated fashion, as with SETI@home), and request some data. The server selects a data set (a server-side operation) and sends it back to the client. The client then analyzes the data (a client-side operation), and, when the analysis is complete, displays it to the user (as with Google Earth) and/or transmits the results of calculations back to the server (as with SETI@home).|$|E
50|$|Certain search services, {{including}} many Meta search engines, request individual contextual information from users {{to increase the}} precision of returned documents. Inquirus 2 is a Meta search engine {{that acts as a}} mediator between the <b>user</b> <b>query</b> and other search engines. When searching on Inquirus 2, users enter a query and specify constraints such as the information need category, maximum number of hits, and display formats. For example, a user looking for research papers can specify documents with “references” or “abstracts” to be rated higher. If another user is searching for general information on the topic rather than research papers, they can specify the GenScore attribute to have a heavier weight.|$|E
50|$|Q-go was {{a privately}} owned {{international}} company {{that specializes in}} semantic search SaaS, based on Natural Language Processing technology. The technology provides relevant answers to users in response to queries on a company’s internet website or corporate intranet, formulated in natural sentences or keyword input alike. It integrates automatic statistical reporting of <b>user</b> <b>query</b> behavior for businesses that want to monitor what kinds of questions their customers are asking. This is in order to adjust content to provide the appropriate information for customers and to reduce the load on traditional customer service ports of call, such as call centers and answers by email. RightNow Technologies acquired Q-go for $34 million on January 18, 2011. Rightnow was subsequently acquired by Oracle Corporation.|$|E
40|$|Instuitionistic {{fuzzy logic}} is widely {{accepted}} method to analyse the imprecise and vague data. There are number of database management systems (DBMS) {{are available to}} facilitate the users to store and organize {{the data for the}} future purpose. DBMS lacks to understand the <b>user</b> <b>queries</b> in distributed environment. Sql is a popular querying language to fetch data depend upon the <b>user</b> <b>queries.</b> The structure of the sql query is designed for precise <b>queries</b> from the <b>user</b> but it will return error for vague queries. As the business extends from one part of the world to the other language should be a barrier for the non- English speakers. The research is to find the solution for the problems exist in the sql to translate the <b>user</b> <b>queries.</b> The proposed model will answer all kind of <b>user</b> <b>queries</b> and tries solve the vagueness problem using fuzzy logic...|$|R
40|$|Obtaining {{correct and}} {{relevant}} {{information at the}} right time to <b>user’s</b> <b>query</b> is quite a difficult task. This becomes even complex, if the query terms have many meanings and occur in different varieties of domain. This paper presents a fuzzy-ontology based information retrieval system that determine the semantic equivalence between terms in a query and terms in a document by relating the synonyms of query terms with those of document terms. Hence, documents could be retrieved based on the meaning of query terms. The challenge has been that surface form does not sufficiently retrieve relevant document to <b>user’s</b> <b>query.</b> However, the results presented showed that the Fuzzy-Ontology Information Retrieval system successfully retrieve relevant documents to <b>user’s</b> <b>query.</b> This is irrespective of different meaning and varieties of domain. The System was tested on words with different meanings and some set of <b>user’s</b> <b>query</b> from varied domains...|$|R
40|$|Purpose: Advanced {{usage of}} Web Analytics tools allows {{to capture the}} content of <b>user</b> <b>queries.</b> Despite their {{relevant}} nature, the manual analysis of large volumes of <b>user</b> <b>queries</b> is problematic. This paper demonstrates the potential of using information extraction techniques and Linked Data to gather {{a better understanding of}} the nature of <b>user</b> <b>queries</b> in an automated manner. Design/methodology/approach: The paper presents a large-scale case-study conducted at the Royal Library of Belgium consisting of a data set of 83 854 queries resulting from 29 812 visits over a 12 month period of the historical newspapers platform BelgicaPress. By making use of information extraction methods, knowledge bases and various authority files, this paper presents the possibilities and limits to identify what percentage of end users are looking for person and place names. Findings: Based on a quantitative assessment, our method can successfully identify the majority of person and place names from <b>user</b> <b>queries.</b> Due to the specific character of <b>user</b> <b>queries</b> and the nature of the knowledge bases used, a limited amount of queries remained too ambiguous to be treated in an automated manner. Originality/value: This paper demonstrates in an empirical manner both the possibilities and limits of gaining more insights from <b>user</b> <b>queries</b> extracted from a Web Analytics tool and analysed with the help of information extraction tools and knowledge bases. Methods and tools used are generalisable and can be reused by other collection holders. Comment: Preprint (17 pages, 3 figures...|$|R
50|$|In 2011, {{the company}} {{acquired}} Q-go for $34 million. Q-go {{was founded in}} 1999. It specialized in semantic search service, based on natural language processing technology, providing relevant answers to queries on a company’s Internet website or corporate intranet, formulated in natural sentences or keyword input alike. It integrated automatic statistical reporting of <b>user</b> <b>query</b> behavior for businesses that want to monitor what kinds of questions their customers are asking so they can adjust content to provide the appropriate information for customers and to reduce the load on traditional customer service ports of call, such as call centers and answers by email. The technology has been implemented and deployed {{in a range of}} industries, including banking, insurance, pension, telecommunications and logistics, as well as several government agencies.|$|E
50|$|Function {{points were}} defined in 1979 in Measuring Application Development Productivity by Allan Albrecht at IBM. The {{functional}} user {{requirements of the}} software are identified and each one is categorized into one of five types: outputs, inquiries, inputs, internal files, and external interfaces. Once the function is identified and categorized into a type, it is then assessed for complexity and assigned a number of function points. Each of these functional user requirements maps to an end-user business function, such as a data entry for an Input or a <b>user</b> <b>query</b> for an Inquiry. This distinction {{is important because it}} tends to make the functions measured in function points map easily into user-oriented requirements, but it also tends to hide internal functions (e.g. algorithms), which also require resources to implement.|$|E
50|$|The {{vocabulary}} {{mismatch between}} user created queries and relevant documents in a corpus causes the term mismatch problem in information retrieval. Zhao and Callan (2010) were {{perhaps the first}} to quantitatively study the vocabulary mismatch problem in a retrieval setting. Their results show that an average query term fails to appear in 30-40% of the documents {{that are relevant to}} the <b>user</b> <b>query.</b> They also showed that this probability of mismatch is a central probability in one of the fundamental probabilistic retrieval models, the Binary Independence Model. They developed novel term weight prediction methods that can lead to potentially 50-80% accuracy gains in retrieval over strong keyword retrieval models. Further research along the line shows that expert users can use Boolean Conjunctive Normal Form expansion to improve retrieval performance by 50-300% over unexpanded keyword queries.|$|E
40|$|This paper proposes an {{evolutionary}} approach for searching information collections in a distributed environment. This approach, which {{is given a}} set of information collections and <b>user</b> <b>queries,</b> finds the best possible matching between the information collections and <b>user</b> <b>queries</b> by {{taking into account the}} factors from both <b>user</b> <b>queries</b> perspective and collections’ perspective. The contribution of each factor in a searching model is indicated by a weighted parameter derived by a genetic algorithm. The values of these parameters are used to determine the contribution of the corresponding factors to the effectiveness of selection. Some preliminary results are demonstrated. Although inconclusive, they provide directions for possibly fruitful research and applications...|$|R
5000|$|Private {{information}} retrieval with multiple databases {{can be achieved}} with information-theoretic privacy for the <b>user's</b> <b>query.</b>|$|R
5000|$|Relevance {{refers to}} Google's {{algorithms}} attempt to surface the listings that best match the <b>user's</b> <b>query.</b>|$|R
40|$|Information Retrieval IR systems store a {{large volume}} of {{unstructured}} data and provide search results for a <b>user</b> <b>query.</b> The performance of the IR systems depends upon the relevancy of the search results with <b>user</b> <b>query.</b> Page ranking algorithms are used to assign rank to the retrieved results for a <b>user</b> <b>query.</b> Page ranking algorithms are mainly categories in to web structure mining and web content mining. In literature many page ranking algorithms have been proposed to improve the relevancy of search results for a <b>user</b> <b>query.</b> In this paper a new hybrid page ranking algorithm using web structure mining and web content mining has been proposed. The algorithm is implemented and tested on a test data results shows that the new proposed algorithm performs better than the existing algorithm...|$|E
40|$|Abstract. Passage {{retrieval}} {{consists in}} identifying short but informative runs {{of a long}} text, given a specific <b>user</b> <b>query.</b> We discuss the sources of evidence that help choosing likely high-quality passages, such as relevance to the <b>user</b> <b>query</b> and self-containedness. These measures {{are different from the}} traditional information retrieval procedure due {{to the use of the}} context of the passage. ...|$|E
40|$|The WorldWideWeb (WWW) {{is a huge}} {{conservatory}} of web pages. Search Engines are key {{applications that}} fetch web pages for the <b>user</b> <b>query.</b> In the current generation web architecture, search engines treat keywords provided by the user as isolated keywords without considering {{the context of the}} <b>user</b> <b>query.</b> This results in a lot of unrelated pages or links being displayed to the user. Semantic Web is based on the current web with a revised framework to display a more precise result set as response to a <b>user</b> <b>query.</b> The current web pages need to be annotated by finding relevant meta data to be added to each of them, so that they become useful to Semantic Web search engines. Semantic Look explores the context of <b>user</b> <b>query</b> by processing the Semantic information recorded in the web pages. It is compared with an existing algorithm called OntoLook and it is shown that Semantic Look is a better optimized search engine by being more than twice as fast as OntoLook. Comment: 12 page...|$|E
40|$|Information {{integration}} systems {{provide a}} uniform query interface {{to a set}} of sources. One of the key challenges for an information integration system is to provide maximally complete answers to <b>user</b> <b>queries</b> and to execute <b>user</b> <b>queries</b> efficiently. We describe an approach to map recursive datalog programs into a streaming, dataflow execution system. We show that our method can be used in conjunction with the Inverse Rules algorithm to create a new information integration system that can provide maximally complete answers to <b>user</b> <b>queries</b> and efficiently execute those queries. Our preliminary results show that in addition to generating maximally complete answers, we obtain performance improvements ranging from 8 % to 24. 3 % over datalog execution...|$|R
40|$|Abstract — In {{this paper}} we tackle {{the problem of}} {{allowing}} applications to request different data at different rates from different sensors of the same sensor network while still being able to run the sensor network in an efficient manner. Our approach is to merge an arbitrary number of <b>user</b> <b>queries</b> into a network query. By doing this, traffic is minimised and the sensors have better energy consumption behavior than if all <b>user</b> <b>queries</b> would have been directly sent to the network. In the paper we describe the algorithms for the transformation of queries and the resulting data streams. We also provide an extensive performance evaluation of the algorithms using sets of over hundred overlapping <b>user</b> <b>queries</b> executing on the same sensor network. I...|$|R
50|$|It can let Google {{scan the}} book under the Library Project and display snippets in {{response}} to <b>user</b> <b>queries.</b>|$|R
