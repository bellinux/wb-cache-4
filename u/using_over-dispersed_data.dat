0|46|Public
40|$|The Conway-Maxwell-Poisson (COM-Poisson) is a {{generalization}} of the Poisson distribution which can model both under-dispersed and <b>over-dispersed</b> <b>data.</b> However, the distribution, moments, and MLE cannot be computed in closed form. This paper describes computational schemes and handy approximations for the COM-Poisson...|$|R
40|$|In {{traffic safety}} analysis, {{a large number}} of {{distributions}} have been proposed to analyze motor vehicle crashes. Among those distributions, the traditional Poisson and Negative Binomial (NB) distributions have been the most commonly used. Although the Poisson and NB models possess desirable statistical properties, their application on modeling motor vehicle crashes are associated with limitations. In practice, traffic crash <b>data</b> are often <b>over-dispersed.</b> On rare occasions, they have shown to be under-dispersed. The <b>over-dispersed</b> and under-dispersed <b>data</b> can lead to the inconsistent standard errors of parameter estimates using the traditional Poisson distribution. Although the NB {{has been found to be}} able to model <b>over-dispersed</b> <b>data,</b> it cannot handle under-dispersed data. Among those distributions proposed to handle over-dispersed and under-dispersed datasets, the Conway-Maxwell-Poisson (COM-Poisson) and double Poisson (DP) distributions are particularly noteworthy. The DP distribution and its generalized linear model (GLM) framework has seldom been investigated and applied since its first introduction 25 years ago. The objectives of this study are to: 1) examine the applicability of the DP distribution and its regression model for analyzing crash data characterized by over- and under-dispersion, and 2) compare the performances of the DP distribution and DP GLM with those of the COM-Poisson distribution and COM-Poisson GLM in terms of goodness-of-fit (GOF) and theoretical soundness. All the DP GLMs in this study were developed based on the approximate probability mass function (PMF) of the DP distribution. Based on the simulated data, it was found that the COM-Poisson distribution performed better than the DP distribution for all nine mean-dispersion scenarios and that the DP distribution worked better for high mean scenarios independent of the type of dispersion. <b>Using</b> two <b>over-dispersed</b> empirical datasets, the results demonstrated that the DP GLM fitted the <b>over-dispersed</b> <b>data</b> almost the same as the NB model and COM-Poisson GLM. With the use of the under-dispersed empirical crash data, it was found that the overall performance of the DP GLM was much better than that of the COM-Poisson GLM in handling the under-dispersed crash data. Furthermore, it was found that the mathematics to manipulate the DP GLM was much easier than for the COM-Poisson GLM and that the DP GLM always gave smaller standard errors for the estimated coefficients...|$|R
40|$|Count data {{regression}} models are used when {{the dependent variables}} are non-negative integers. The standard count data models are limited in their ability in handling the data distributions. Poisson and negative binomial distributions are commonly used in count models. Poisson distributions assumes equidispersed data (variance equals to the mean); and negative binomial {{regression models}} <b>over-dispersed</b> <b>data</b> (variance greater than the mean). While there are studies (Liu and Cela 2008; Tin 2008) provide zero-inflated and hurdle count data models in SAS, no study has provided a SAS program that allows for a comprehensive list of data distributions and modeling strategies. This paper presents a SAS ® macro program that allows {{for a wide variety}} of count data distributions which can be used to model both underand <b>over-dispersed</b> <b>data.</b> In addition, our SAS ® macro program can handle data that has excess zeros (zero-inflated) in the sample. This SAS ® macro is flexible in allowing one to estimate a variety of count regression models including: zero-inflated, hurdle, censored, truncated, finite mixture, semi-parametric, squared polynomial expansion, and generalized heterogeneous. We demonstrate this SAS ® macro procedure by applying it to the number of takeover bids received by targeted firms. We also evaluate count models performance using goodness-of-fit test, Vuong’s test, and information criteria test. 1...|$|R
40|$|This article explores a Bayesian {{analysis}} of a generalization of the Poisson distribution. By choice of a second parameter v, both under-dispersed and <b>over-dispersed</b> <b>data</b> can be modeled. The Conway-Maxwell-Poisson distribution forms an exponential family of distributions, so it has sufficient statistics of fixed dimension as the sample size varies, and a conjugate family of prior distributions. The article displays and proves a necessary and sufficient condition on the hyperparameters of the conjugate family for the prior to be proper, and it discusses methods of sampling from the conjugate distribution. An elicitation program to find the hyperparameters from the predictive distribution is also discussed...|$|R
40|$|The {{objectives}} {{of this study}} are to: 1) examine the applicability of the double Poisson (DP) generalized linear model (GLM) for analyzing motor vehicle crash data characterized by over-and under-dispersion and 2) compare the performance of the DP GLM with the Conway-Maxwell-Poisson (COM-Poisson) GLM in terms of goodness-of-fit and theoretical soundness. The DP distribution has seldom been investigated and applied since its first introduction two decades ago. The hurdle of applying the DP is related to its normalizing constant (or multiplicative constant) which is not available in closed form. This study proposed a new method to approximate the normalizing constant of the DP with high accuracy and reliability. The DP GLM and COM-Poisson GLM were developed <b>using</b> two observed <b>over-dispersed</b> datasets and one observed under-dispersed dataset. The modeling results indicate that the DP GLM with its normalizing constant approximated by the new method can handle crash data characterized by over- and under-dispersion. Its performance is comparable to the COM-Poisson GLM in terms of goodness-of-fit (GOF), although COM-Poisson GLM provides a slightly better fit. For the <b>over-dispersed</b> <b>data,</b> the DP GLM performs similar to the NB GLM. Considering the fact that the DP GLM can be easily estimated with inexpensive computation and that it is simpler to interpret coefficients, it offers a flexible and efficient alternative for researchers to model the count data...|$|R
30|$|The two-step {{approach}} to change point identification {{described in this}} paper {{has the advantage of}} building on control charts that may be already in place in practice. An alternative may be to retain the two-step approach but to use a Bayesian framework in both stages. There is now a substantial literature on Bayesian formulation of control charts and extensions such as monitoring processes with varying parameters (Feltz and Shiau 2001), <b>over-dispersed</b> <b>data</b> (Bayarri and Garcia-Donato 2005), start-up and short runs (Tsiamyrtzis and Hawkins 20052008). A further alternative is to consider a fully Bayesian, one-step approach, in which both the monitoring of the in-control process and the retrospective or prospective identification of changes is undertaken in the one analysis. This is the subject of further research.|$|R
40|$|In this paper, a new {{generalized}} counting {{process with}} Mittag-Leffler inter-arrival time distribution is introduced. This new {{model is a}} generalization of the Poisson process. The computational intractability is overcome by deriving the Mittag-Leffler count model using polynomial expansion. The hazard function of this new model is a decreasing function of time, so that the distribution displays negative duration dependence. The model is applied to a data on interarrival times of customers in a bank counter. This new count model can be simulated by Markov Chain Monte-Carlo (MCMC) methods, using Metropolis- Hastings algorithm. Our new model has many nice features such as its closed form nature, computational simplicity, ability to nest Poisson, existence of moments and autocorrelation {{and can be used}} for both equi-dispersed and <b>over-dispersed</b> <b>data...</b>|$|R
30|$|This example {{demonstrates}} {{the suitability of}} the sCMP class {{to serve as an}} exploratory tool for count <b>data</b> modeling. For <b>over-dispersed</b> <b>data</b> examples, the negative binomial distribution is generally expected to be a good model to describe the distribution. The sCMP class of distributions contains the negative binomial (and geometric) distribution as a special case; accordingly, it is not necessarily expected for the sCMP distribution to outperform simpler distributions but rather to demonstrate that the sCMP distribution offers insights regarding model considerations. Indeed, applying the sCMP model to these over-dispersed examples motivated consideration of the geometric distribution, {{which turned out to be}} an optimal model. Accordingly, while one may not consider the geometric distribution to be a viable model a priori, the sCMP showed why the geometric model is viable.|$|R
3000|$|As {{noted in}} the <b>over-dispersed</b> <b>data</b> example, we are limited {{in our ability to}} {{estimate}} m because it is a natural number. We opt for this formulation as it holds true to the form that generalizes the construction of the three special case models (negative binomial, Poisson, and binomial) as sums of their respective special case distributions associated with the CMP distribution (namely, the geometric, Poisson, and Bernoulli models). For example, the negative binomial pmf is often described as the probability of observing y failures before the nth success in a series of Bernoulli trials, or as a sum of n geometric random variables. Yet, the negative binomial distribution can alternatively be derived via a Poisson-gamma mixture, in which case the parameter n is a real number. As Hilbe (2008) notes, “there is no compelling mathematical reason to limit this parameter to integers." [...] (page 82). Future work considers broadening the sCMP formulation to likewise allow for real-valued m and any associated implications from such a definition.|$|R
40|$|The aim of {{this work}} is to present the {{reflections}} and proposals derived from the first Workshop of the SISMEC STATDENT working group on statistical methods and applications in dentistry, held in Ancona (Italy) on 28 th September 2011. STATDENT began as a forum of comparison and discussion for statisticians {{working in the field}} of dental research in order to suggest new and improve existing biostatistical and clinical epidemiological methods. During the meeting, we dealt with very important topics of statistical methodology for the analysis of dental data, covering the analysis of hierarchically structured and <b>over-dispersed</b> <b>data,</b> the issue of calibration and reproducibility, as well as some problems related to survey methodology, such as the design and construction of unbiased statistical indicators and of well conducted clinical trials. This paper gathers some of the methodological topics discussed during the meeting, concerning multilevel and zero-inflated models for the analysis of caries data and methods for the training and calibration of raters in dental epidemiology. </p...|$|R
50|$|GAMLSS is {{especially}} suited for modelling a leptokurtic or platykurtic and/or positively or negatively skewed response variable. For count type response variable data {{it deals with}} over-dispersion by <b>using</b> proper <b>over-dispersed</b> discrete distributions. Heterogeneity also is dealt with by modelling the scale or shape parameters using explanatory variables. There are several packages written in R related to GAMLSS models.|$|R
40|$|Forecasting for a {{time series}} of low counts, such as {{forecasting}} the number of patents to be awarded to an industry, is an important research topic in socio-economic sectors. Recently (2004), Freeland and McCabe introduced a Gaussian type stationary correlation model-based forecasting which appears to work well for the stationary time series of low counts. In practice, however, it may happen that the time series of counts will be non-stationary and also the series may contain over-dispersed counts. To develop the forecasting functions {{for this type of}} non-stationary <b>over-dispersed</b> <b>data,</b> the paper provides an extension of the stationary correlation models for Poisson counts to the non-stationary correlation models for negative binomial counts. The forecasting methodology appears to work well, for example, for a US time series of polio counts, whereas the existing Bayesian methods of forecasting appear to encounter serious convergence problems. Further, a simulation study is conducted to examine the performance of the proposed forecasting functions, which appear to work well irrespective of whether the time series contains small or large counts. Copyright © 2008 John Wiley & Sons, Ltd. ...|$|R
40|$|In this paper, {{we compare}} the {{performances}} of several models for tting <b>over-dispersed</b> binary <b>data.</b> The distribution models considered in this study include the binomial (BN), the beta- binomial (BB), the multiplicative binomial (MBM), the Com-Poisson binomial (CPB) and the double binomial (DBM) models. Applications of these models to several well known data sets exhibiting under-dispersion and over-dispersion were considered in this paper. We applied these models to two frequency data sets and two data sets with covariates that have been variously analysed in the literature. The rst relates to the Portuguese version of Duke Religiosity Index {{in a sample of}} 273 (202 women, 71 Male) postgraduate students of the faculty of Medicine of University of Sao Paulo. The second set that employs the Generalize Linear Model (GLM) is the correlated binary data which studies the cardiotoxic e ects of doxorubicin chemoteraphy on the treatment of acute lymphoblastic leukemia in childhood. In the rst data set, we have a single covariate, Sex (0, 1) and two covariates in the second data set (dose and time). Our results indicate that all the models considered here (excluding the binomial) behave reasonably well in modeling <b>over-dispersed</b> binary <b>data</b> with or without covariates, although both the multiplicative binomial and the double binomial models slightly behave better for thes...|$|R
40|$|Background: Air {{pollution}} in Darwin, Northern Australia, {{is dominated by}} smoke from seasonal fires in the surrounding savanna that burn {{during the dry season}} from April to November. Our aim was to study the association between particulate matter {{less than or equal to}} 10 microns diameter (PM 10) and daily emergency hospital admissions for cardio-respiratory diseases for each fire season from 1996 to 2005. We also investigated whether the relationship differed in indigenous Australians; a disadvantaged population sub-group. Methods: Daily PM 10 exposure levels were estimated for the population of the city from visibility data using a previously validated model. We <b>used</b> <b>over-dispersed</b> Poisson generalized linear models with parametric smoothing functions for time and meteorology to examine the association between admissions and PM 10 up to three days prior. An interaction between indigenous status and PM 10 was included to examine differences in the impact on indigenous people. Results: We found both positive and negative associations and our estimates had wide confidence intervals. There were generally positive associations between respiratory disease and PM 10 but not with cardiovascular disease. An increase of 10 μg/m 3 in same-day estimated ambient PM 10 was associated with a 4. 81...|$|R
40|$|A simple {{population}} {{model for}} Robben Island penguins is considered which incorporates fitting to both moult counts and tag data. The latter are now fit using a multinomial likelihood {{which is the}} method used in program MARK. Probability intervals on survival rates are more reliable than those obtained previously <b>using</b> an <b>over-dispersed</b> Poisson likelihood. Furthermore, the incorporation of a prior for relative undercounts of juveniles in the moult counts generally moves penalized likelihood estimates for adult survival rate away from an upper constraint boundary...|$|R
40|$|We {{introduce}} negative binomial matrix factorization (NBMF), {{a matrix}} factorization technique specially designed for analyzing <b>over-dispersed</b> count <b>data.</b> It {{can be viewed}} as an extension of Poisson matrix factorization (PF) perturbed by a multiplicative term which models exposure. This term brings a degree of freedom for controlling the dispersion, making NBMF more robust to outliers. We show that NBMF allows to skip traditional pre-processing stages, such as binarization, which lead to loss of information. Two estimation approaches are presented: maximum likelihood and variational Bayes inference. We test our model with a recommendation task and show its ability to predict user tastes with better precision than PF...|$|R
40|$|Clustered data, {{multiple}} observations {{collected on}} the same experimental unit, is common in epidemiological studies. Bivariate outcome data is often the result of interest in two correlated response variables. An efficient method is presented for dealing with bivariate outcomes when one outcome is continuous {{and the other is}} a count using a simple transformation to handle <b>over-dispersed</b> Poisson <b>data.</b> A multilevel analysis was performed on data from the National Health Interview Survey (NHIS) with body mass index (BMI) and the number of cigarettes smoked per day (NCS) as responses. Results show that these random effects models yield misleading results in cases where the data is not transformed...|$|R
40|$|Managing for stand {{structural}} {{complexity in}} northern hardwood forests {{has been proposed}} as a method for promoting microhabitat characteristics important to eastern red-backed salamanders (Plethodon cinereus). We evaluated the effects of alternate, structure-based silvicultural systems on red-backed salamander populations at two research sites in northwestern Vermont. Treatments included two uneven-aged approaches (single-tree selection and group-selection) and one unconventional approach, termed ‘‘structural complexity enhancement’’ (SCE), that promotes development of late-successional structure, including elevated levels of coarse woody debris (CWD). Treatments were applied to 2 ha units and were replicated two to four times depending on treatment. We surveyed red-backed salamanders with a natural cover search method of transects nested within vegetation plots 1 year after logging. Abundance estimates corrected for detection probability were calculated from survey data with a binomial mixture model. Abundance estimates differed between study areas and were influenced by forest structural characteristics. Model selection was conducted using Akaike Information Criteria, corrected for <b>over-dispersed</b> <b>data</b> and small sample size (QAICc). We found no difference in abundance {{as a response to}} treatment as a whole, suggesting that all of the uneven-aged silvicultural systems evaluated can maintain salamander populations after harvest. However, abundance was tied to specific structural habitat attributes associated with study plots within treatments. The most parsimonious model of habitat covariates included site, relative density of overstory trees, and density of more-decayed and less-decayed downed CWD. Abundance responded positively to the density of downed, well-decayed CWD and negatively to the density of poorly decayed CWD and to overstory relative density. CWD volume was not a strong predictor of salamande...|$|R
30|$|To {{measure the}} city-wide Pokémon Go effect we {{considered}} {{the availability of}} the game as a city intervention, which started {{on the day of the}} launch of the game. Our hypothesis is that the following days would show an effect of the game if people went out, regardless of being players or not, and this effect would show on the number of people connected at each zone of the city. To do so, we used Negative Binomial Regression (NB) [7, 13] applied to our dataset at 1 -minute intervals during a day. The NB regression model has been used frequently to analyze <b>over-dispersed</b> count <b>data,</b> i.e., when the variance is much larger than the mean, contrary to the Poisson model [14].|$|R
40|$|In this paper, {{we propose}} a new zero {{inflated}} distribution, namely, the zero inflated negative binomial-generalized exponential (ZINB-GE) distribution. The new distribution {{is used for}} count data with extra zeros and is an alternative for <b>data</b> analysis with <b>over-dispersed</b> count <b>data.</b> Some characteristics of the distribution are given, such as mean, variance, skewness, and kurtosis. Parameter estimation of the ZINB-GE distribution uses maximum likelihood estimation (MLE) method. Simulated and observed data are employed to examine this distribution. The {{results show that the}} MLE method seems to have high efficiency for large sample sizes. Moreover, the mean square error of parameter estimation is increased when the zero proportion is higher. For the real data sets, this new zero inflated distribution provides a better fit than the zero inflated Poisson and zero inflated negative binomial distributions. ...|$|R
40|$|This paper {{provides}} {{a survey of}} count regression models in SAS®, which incorporates various count data modeling strategies. Our estimation procedure allows {{for a wide variety}} of count data distributions which can be used to model both under- and <b>over-dispersed</b> count <b>data.</b> We also provide estimation procedures that estimate count data that has excess zeros (zero-inflated) in the sample. A SAS ® macro is provided to allow one to estimate a variety of count regression models including: zero-inflated, hurdle, censored, truncated, finite mixture, semi-parametric, squared polynomial expansion, and generalized heterogeneous. Model evaluations are carried out via statistical tests, including goodness-of-fit, Vuong’s test, and information criteria. This paper {{provides a}} SAS macro program that incorporates a comprehensive list of count regression models and evaluates the performance of various count models. 1...|$|R
40|$|Resource specialisation, {{although}} a fundamental component of ecological theory, is employed in disparate ways. Most definitions derive from simple counts of resource species. We build on {{recent advances in}} ecophylogenetics and null model analysis to propose a concept of specialisation that comprises affinities among resources {{as well as their}} co-occurrence with consumers. In the distance-based specialisation index (DSI), specialisation is measured as relatedness (phylogenetic or otherwise) of resources, scaled by the null expectation of random use of locally available resources. Thus, specialists use significantly clustered sets of resources, whereas generalists <b>use</b> <b>over-dispersed</b> resources. Intermediate species are classed as indiscriminate consumers. The effectiveness of this approach was assessed with differentially restricted null models, applied to a data set of 168 herbivorous insect species and their hosts. Incorporation of plant relatedness and relative abundance greatly improved specialisation measures compared to taxon counts or simpler null models, which overestimate the fraction of specialists, a problem compounded by insufficient sampling effort. This framework disambiguates the concept of specialisation with an explicit measure applicable to any mode of affinity among resource classes, and is also linked to ecological and evolutionary processes. This will enable a more rigorous deployment of ecological specialisation in empirical and theoretical studies. Field work was supported by FAPESP grants to T. M. L. (94 / 02837 - 2 and Biota/Fapesp 98 / 05085 – 2). T. M. L., P. I. P. and M. A. N. also receive research fellowships from CNPq. This study is part of L. R. J. 's doctoral thesis submitted to the Ecology Program at the University of Campinas, with support by Fapesp (predoctoral grant 09 / 54806 - 0) ...|$|R
40|$|This article proposes tree-structured {{logistic}} regression modeling for <b>over-dispersed</b> binomial <b>data.</b> Recursive partitioning {{is performed using}} a combination of statistical tests and residual analysis. The splitting criterion in cross-validation is based on the deviance function. A nested grid algorithm to estimate the bootstrap parameters is developed. The regression tree procedure provides a new approach to explore the relationship between the binomial response and explanatory variables in detail. The proposed procedure is applied to model the relationship between the incidence of malformation, and dose and fetal weight using data from a developmental experiment conducted at the National Center for Toxicological Research. A conditional Gaussian chain model is used to account for the effect of fetal weight by dose. 1 Introduction Recently, tree-based methods have been developed by many researchers. The tree-structured approaches are used for classification (Breiman et al., 19 [...] ...|$|R
40|$|We model DNA count data as a {{multiple}} change point problem, {{in which the}} data are divided in to different segments by {{an unknown number of}} change points. Each segment is supposed to be generated by unique distribution characteristics inherent to the underlying process. In this paper, we propose {{a modified version of the}} Cross-Entropy (CE) method, which utilizes Beta distribution to simulate locations of change points. Several stopping criterions are also discussed. The proposed CE method applies on <b>over-dispersed</b> count <b>data,</b> in which the observations are distributed as independent Negative Binomial. Furthermore, we incorporate the Bayesian Information Criterion to identify the optimal number of change points within the CE method while not fixing the maximum number of change points in the data sequence. We obtain estimates for the artificial data by using the modified CE method and compare the results with the general CE method, which utilizes normal distribution to simulate locations of the change points. The methods are applied to a real DNA count data set in order to illustrate the usefulness of the proposed modified CE method. 8 page(s...|$|R
40|$|AbstractWe propose an infinitesimal {{dispersion}} index for Markov counting processes. We show that, under standard moment existence conditions, a process is infinitesimally (over-) equi-dispersed if, and only if, it is simple (compound), i. e.  it increases in jumps of one (or more) unit(s), even though infinitesimally equi-dispersed processes might be under-, equi- or <b>over-dispersed</b> <b>using</b> previously studied indices. Compound processes arise, for example, when introducing continuous-time white noise to {{the rates of}} simple processes resulting in Lévy-driven SDEs. We construct multivariate infinitesimally over-dispersed compartment models and queuing networks, suitable for applications where moment constraints inherent to simple processes do not hold...|$|R
40|$|This paper {{documents}} {{the performance of}} a Bayesian Conway-Maxwell-Poisson (COM-Poisson) generalized linear model (GLM). This distribution was originally developed {{as an extension of the}} Poisson distribution in 1962 and has a unique characteristic, in that it can handle both under-dispersed and <b>over-dispersed</b> count <b>data.</b> Previous work by the authors lead to the development of a dual-link GLM based on the COM-Poisson distribution and applied this model to analyzing power system reliability and motor vehicle crash data. Parameter estimation for this model is done within the Bayesian framework using Markov Chain Monte Carlo methods. The objectives of this paper are to (1) characterize the parameter estimation accuracy of the Markov Chain Monte Carlo (MCMC) implementation of the COM GLM and (2) estimate the computational burden of this MCMC implementation. We use simulated datasets to assess the performance of the COM GLM. The results of the study indicate that the COM GLM is flexible enough to model under-, equi- and over-dispersed datasets with different sample mean values. The results also show that the MCMC implementation of the COM GLM yields accurate parameter estimates. Furthermore, we show that a previously suggested asymptoti...|$|R
40|$|BackgroundAir {{pollution}} in Darwin, Northern Australia, {{is dominated by}} smoke from seasonal fires in the surrounding savanna that burn {{during the dry season}} from April to November. Our aim was to study the association between particulate matter {{less than or equal to}} 10 microns diameter (PM 10) and daily emergency hospital admissions for cardio-respiratory diseases for each fire season from 1996 to 2005. We also investigated whether the relationship differed in indigenous Australians; a disadvantaged population sub-group. MethodsDaily PM 10 exposure levels were estimated for the population of the city from visibility data using a previously validated model. We <b>used</b> <b>over-dispersed</b> Poisson generalized linear models with parametric smoothing functions for time and meteorology to examine the association between admissions and PM 10 up to three days prior. An interaction between indigenous status and PM 10 was included to examine differences in the impact on indigenous people. ResultsWe found both positive and negative associations and our estimates had wide confidence intervals. There were generally positive associations between respiratory disease and PM 10 but not with cardiovascular disease. An increase of 10 μg/m 3 in same-day estimated ambient PM 10 was associated with a 4. 81 % (95 %CI: - 1. 04 %, 11. 01 %) increase in total respiratory admissions. When the interaction between indigenous status and PM 10 was assessed a statistically different association was found between PM 10 and admissions three days later for respiratory infections of indigenous people (15. 02 %; 95 %CI: 3. 73 %, 27. 54 %) than for non-indigenous people (0. 67 %; 95 %CI: - 7. 55 %, 9. 61 %). There were generally negative estimates for cardiovascular conditions. For non-indigenous admissions the estimated association with total cardiovascular admissions for same day ambient PM 10 and admissions was - 3. 43 % (95 %CI: - 9. 00 %, 2. 49 %) and the estimate for indigenous admissions was - 3. 78 % (95 %CI: - 13. 4 %, 6. 91 %), although ambient PM 10 did have positive (non-significant) associations with cardiovascular admissions of indigenous people two and three days later. ConclusionWe observed positive associations between vegetation fire smoke and daily hospital admissions for respiratory diseases that were stronger in indigenous people. While this study was limited by the use of estimated rather than measured exposure data, the results are consistent with the currently small evidence base concerning this source of air pollution...|$|R
40|$|Abstract Background Air {{pollution}} in Darwin, Northern Australia, {{is dominated by}} smoke from seasonal fires in the surrounding savanna that burn {{during the dry season}} from April to November. Our aim was to study the association between particulate matter {{less than or equal to}} 10 microns diameter (PM 10) and daily emergency hospital admissions for cardio-respiratory diseases for each fire season from 1996 to 2005. We also investigated whether the relationship differed in indigenous Australians; a disadvantaged population sub-group. Methods Daily PM 10 exposure levels were estimated for the population of the city from visibility data using a previously validated model. We <b>used</b> <b>over-dispersed</b> Poisson generalized linear models with parametric smoothing functions for time and meteorology to examine the association between admissions and PM 10 up to three days prior. An interaction between indigenous status and PM 10 was included to examine differences in the impact on indigenous people. Results We found both positive and negative associations and our estimates had wide confidence intervals. There were generally positive associations between respiratory disease and PM 10 but not with cardiovascular disease. An increase of 10 μg/m 3 in same-day estimated ambient PM 10 was associated with a 4. 81 % (95 %CI: - 1. 04 %, 11. 01 %) increase in total respiratory admissions. When the interaction between indigenous status and PM 10 was assessed a statistically different association was found between PM 10 and admissions three days later for respiratory infections of indigenous people (15. 02 %; 95 %CI: 3. 73 %, 27. 54 %) than for non-indigenous people (0. 67 %; 95 %CI: - 7. 55 %, 9. 61 %). There were generally negative estimates for cardiovascular conditions. For non-indigenous admissions the estimated association with total cardiovascular admissions for same day ambient PM 10 and admissions was - 3. 43 % (95 %CI: - 9. 00 %, 2. 49 %) and the estimate for indigenous admissions was - 3. 78 % (95 %CI: - 13. 4 %, 6. 91 %), although ambient PM 10 did have positive (non-significant) associations with cardiovascular admissions of indigenous people two and three days later. Conclusion We observed positive associations between vegetation fire smoke and daily hospital admissions for respiratory diseases that were stronger in indigenous people. While this study was limited by the use of estimated rather than measured exposure data, the results are consistent with the currently small evidence base concerning this source of air pollution. </p...|$|R
40|$|We propose an infinitesimal {{dispersion}} index for Markov counting processes. We show that, under standard moment existence conditions, a process is infinitesimally (over-) equi-dispersed if, and only if, it is simple (compound), i. e. Â it increases in jumps of one (or more) unit(s), even though infinitesimally equi-dispersed processes might be under-, equi- or <b>over-dispersed</b> <b>using</b> previously studied indices. Compound processes arise, for example, when introducing continuous-time white noise to {{the rates of}} simple processes resulting in Lévy-driven SDEs. We construct multivariate infinitesimally over-dispersed compartment models and queuing networks, suitable for applications where moment constraints inherent to simple processes do not hold. Continuous time Counting Markov process Birth-death process Environmental stochasticity Infinitesimal over-dispersion Simultaneous events...|$|R
40|$|Abstract Background Alcohol {{consumption}} {{is commonly used}} as a primary outcome in randomized alcohol treatment studies. The distribution of alcohol {{consumption is}} highly skewed, particularly in subjects with alcohol dependence. Methods In this paper, we will consider the use of count models for outcomes in a randomized clinical trial setting. These include the Poisson, over-dispersed Poisson, negative binomial, zero-inflated Poisson and zero-inflated negative binomial. We compare the Type-I error rate of these methods {{in a series of}} simulation studies of a randomized clinical trial, and apply the methods to the ASAP (Addressing the Spectrum of Alcohol Problems) trial. Results Standard Poisson models provide a poor fit for alcohol consumption data from our motivating example, and did not preserve Type-I error rates for the randomized group comparison when the true distribution was over-dispersed Poisson. For the ASAP trial, where the distribution of alcohol consumption featured extensive over-dispersion, there was little indication of significant randomization group differences, except when the standard Poisson model was fit. Conclusion As with any analysis, it is important to choose appropriate statistical models. In simulation studies and in the motivating example, the standard Poisson was not robust when fit to <b>over-dispersed</b> count <b>data,</b> and did not maintain the appropriate Type-I error rate. To appropriately model alcohol consumption, more flexible count models should be routinely employed. </p...|$|R
40|$|Group Randomized Trials (GRTs) {{randomize}} {{groups of}} people to treatment or control arms instead of individually randomizing subjects. When each subject has a binary outcome, <b>over-dispersed</b> binomial <b>data</b> may result, quantified as an intra-cluster correlation (ICC). Typically, GRTs have a small number, bin, of independent clusters, each {{of which can be}} quite large. Treating the ICC as a nuisance parameter, inference for a treatment effect can be done using quasi-likelihood with a logistic link. A Wald statistic, which, under standard regularity conditions, has an asymptotic standard normal distribution, can be used to test for a marginal treatment effect. However, we have found in our setting that the Wald statistic may have a variance less than 1, resulting in a test size smaller than its nominal value. This problem is most apparent when marginal probabilities are close to 0 or 1, particularly when n is small and the ICC is not negligible. When the ICC is known, we develop a method for adjusting the estimated standard error appropriately such that the Wald statistic will approximately have a standard normal distribution. We also propose ways to handle non-nominal test sizes when the ICC is estimated. We demonstrate the utility of our methods through simulation results covering a variety of realistic settings for GRTs. Copyright © 2010 John Wiley & Sons, Ltd...|$|R
40|$|Next-generation {{sequencing}} technologies {{provide a}} revolutionary tool for generating gene expression data. Starting with a fixed RNA sample, they construct {{a library of}} millions of differentially abundant short sequence tags or "reads", which constitute a fundamentally discrete measure {{of the level of}} gene expression. A common limitation in experiments using these technologies is the low number or even absence of biological replicates, which complicates the statistical analysis of digital gene expression data. Analysis of this type of data has often been based on modified tests originally devised for analysing microarrays; both these and even de novo methods for the analysis of RNA-seq data are plagued by the common problem of low replication. We propose a novel, non-parametric Bayesian approach for the analysis of digital gene expression data. We begin with a hierarchical model for modelling <b>over-dispersed</b> count <b>data</b> and a blocked Gibbs sampling algorithm for inferring the posterior distribution of model parameters conditional on these counts. The algorithm compensates for the problem of low numbers of biological replicates by clustering together genes with tag counts that are likely sampled from a common distribution and using this augmented sample for estimating the parameters of this distribution. The number of clusters is not decided a priori, but it is inferred along with the remaining model parameters. We demonstrate the ability of this approach to model biological data with high fidelity by applying the algorithm on a public dataset obtained from cancerous and non-cancerous neural tissues...|$|R
40|$|Modelling crash {{data has}} been an {{integral}} part of the research done in highway safety. Different tools have been suggested by researchers to analyze crash data. One such tool, which was recently proposed, is the Negative Binomial Generalized Exponential (NB-GE) distribution. As the name suggests, it is a combination of Negative Binomial and Generalized Exponential distribution. This distribution has three parameters and can handle <b>over-dispersed</b> crash <b>data</b> which are characterized by a large number of zeros and/or long tail. This research seeks to develop a generalized linear model (GLM) for NB-GE distribution and discuss its applications in crash data analysis. The NB-GE GLM was applied to two over-dispersed crash datasets and its performance was compared to Negative Binomial-Lindley (NB-L) and Negative Binomial (NB) models using various statistical measures. It was found that NB-GE performs almost as well as NB-L model and performs much better than the NB model. This research tried to determine the percentage of zeroes and the dispersion in the dataset where the NB-GE model is recommended over the NB model for ranking sites. Datasets were simulated for different scenarios. It was found that for high dispersion the NB-GE model performs better than the NB model when the percentage of zero counts in the dataset is greater than 80 %. When dataset has lower than 80 % zeroes then NB model and NB-GE model perform similarly. Hence for lower percentages NB model would be preferred as it is simpler and easier to use...|$|R
40|$|BACKGROUND: Alcohol {{consumption}} {{is commonly used}} as a primary outcome in randomized alcohol treatment studies. The distribution of alcohol {{consumption is}} highly skewed, particularly in subjects with alcohol dependence. METHODS: In this paper, we will consider the use of count models for outcomes in a randomized clinical trial setting. These include the Poisson, over-dispersed Poisson, negative binomial, zero-inflated Poisson and zero-inflated negative binomial. We compare the Type-I error rate of these methods {{in a series of}} simulation studies of a randomized clinical trial, and apply the methods to the ASAP (Addressing the Spectrum of Alcohol Problems) trial. RESULTS: Standard Poisson models provide a poor fit for alcohol consumption data from our motivating example, and did not preserve Type-I error rates for the randomized group comparison when the true distribution was over-dispersed Poisson. For the ASAP trial, where the distribution of alcohol consumption featured extensive over-dispersion, there was little indication of significant randomization group differences, except when the standard Poisson model was fit. CONCLUSION: As with any analysis, it is important to choose appropriate statistical models. In simulation studies and in the motivating example, the standard Poisson was not robust when fit to <b>over-dispersed</b> count <b>data,</b> and did not maintain the appropriate Type-I error rate. To appropriately model alcohol consumption, more flexible count models should be routinely employed. National Institute on Alcohol Abuse and Alcoholism (R 01 -AA 12617); Smith College Summer Research Program; Howard Hughes Medical Institut...|$|R
40|$|The Poisson-gamma (PG) and Poisson-lognormal (PLN) {{regression}} models {{are among the}} most popular means for motor-vehicle crash data analysis. Both models belong to the Poisson-hierarchical family of models, which provides a straightforward framework for interpretation of parameters. Over the last two decades, highway safety researchers have increasingly favored a full Bayesian approach to estimation of Poisson-hierarchical models due to its theoretical and computational advantages. While numerous studies have compared the overall performance of alternative Bayesian Poisson-hierarchical models, little research has addressed the impact of model choice on the expected crash frequency prediction at individual sites. This dissertation takes a microscopic approach to comparing the models? predictions and strives to identify possible trends e. g., that an alternative model?s prediction for sites with certain conditions tends to be higher (or lower) than that from another model. The practical importance of such trends is reflected most clearly when alternative models are utilized to identify hazardous highway sites (e. g., roadway segments, intersection, etc.) by ranking the sites with respect to their expected crash frequency. In addition to the PG and PLN models, this research formulates a new member of the Poisson-hierarchical family of models: the Poisson-inverse gamma (PIGam). The PIGam model was of special interest because of the heavy tail of the inverse gamma distribution and the conjectured potential of the PIGam model in dealing with highly <b>over-dispersed</b> <b>data.</b> Four field datasets (from Toronto, Texas, Michigan and Indiana) covering a wide range of over-dispersion characteristics were selected for analysis. This study discovered that the disparities between the alternative models predictions are mainly associated with the sites where the observed crash frequency is significantly larger or smaller than expected for a site with similar traffic and physical characteristics. For both scenarios, it was demonstrated that the PIGam model tends to predict a higher expectation for crash frequency than would the PLN and PG models, in order. In consequence, sites with unusually high number of observed crashes are likely to be ranked higher (in terms of expected crash frequency) when the PIGam model is used instead of the PLN model, and similarly when the PLN model is used instead of the PG model. Furthermore, the disparities between alternative model predictions were found to be even more important when the calibrated models were applied to predict crash frequency at sites with no observed crash count. For all four datasets, the PIGam model tended to predict higher expected crash frequencies than did the PLN and PG models, in order. Finally, a comparison between the models goodness-of-fit using the deviance information criterion (DIC) refuted the conjecture that models with heavy-tailed distributions will certainly perform better as the <b>data</b> become more <b>over-dispersed.</b> The author believes that the relative goodness-of-fit of alternative models to a given dataset is too complicated to be reliably predicted before actually fitting the models. However, the study demonstrated that models with similar measures of goodness-of-fit may predict considerably different crash frequencies at individual sites. This dissertation identified the relationships between alternative models? predictions at individual sites and described the resulting practical implications of choosing one model over another...|$|R
40|$|Abstract: In the {{analysis}} of the count data, the Poisson model becomes overtly restrictive in the case of <b>over-dispersed</b> or under-dispersed <b>data.</b> When count data are under-dispersed, specific models such as generalized linear models (GLM) are proposed. Other examples are the zero-inflated Poisson model (ZIP) and zero-truncated Poisson model (ZTP), which have been used in literature to deal with an excess or absence of zeros in count data. Thus having a knowledge of the probability of zeros and its estimation in Poisson distribution can be significant and useful. Some estimation problems with unknown parameter cannot attain minimum risk where the sample size is fixed. To resolve this captivity, working with a sequential sampling procedure can be useful. In this paper, we consider sequential point estimation of the probability of zero in Poisson distribution. Second order approximations to the expected sample size and the risk of the sequential procedure are derived as the cost per observations tends to zero. Finally, a simulation study is given...|$|R
