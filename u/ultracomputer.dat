35|2|Public
50|$|This {{class of}} {{networks}} {{has been built}} into the Illinois Cedar Multiprocessor, into the IBM RP3, and into the NYU <b>Ultracomputer.</b>|$|E
50|$|The NYU <b>Ultracomputer</b> is a {{significant}} processor design {{in the history of}} parallel computing. The system has N processors, N memories and an N log N message-passing switch connecting them. The switch uses an innovative fetch-and-add instruction which will combine references from several processors into a single reference, to reduce memory contention.|$|E
50|$|Parsytec {{has become}} known in the late 1980s and early 1990s as a {{manufacturer}} of transputer-based parallel systems. Products ranged from a single transputer plug-in board for the IBM PC up to large massively-parallel systems with thousands of transputers (or processors, respectively) such as the Parsytec GC. Some sources call the latter <b>ultracomputer</b> sized, scalable multicomputers (smC).|$|E
40|$|A general {{method for}} {{parallelism}} of some dynamic programming algorithms on VLSI {{was presented in}} [6]. We present, a general method for parallelisation for the same class of problems on more powerful parallel computers. The method is demonstrated on three typical dynamic programming problems: computing the optimal order of matrix multiplications, the optimal binary search tree and optimal triangulation of polygons (see[1, 2]). For these problems the dynamic programming approach gives algorithms having a similar structure. They {{can be viewed as}} straight line programs of size O(n 3). the general method of parallelisation of such programs described by Valiant et al [16] then leads directly to algorithms working in log 2 time with O(n 9) processors. However we adopt an alternative approach and show that a special feature of dynamic programming problems can be used. They can be thought as generalized parsing problems: find a tree of the optimal decomposition of the problem into smaller subproblems. A parallel pebble game on fees [10, 11] is used to decrease the number of processors and to simplify the structure of the algorithms. We show that the dynamic programming problems considered can be computed in log 2 n time using n 6 /log(n) processors on a parallel random access machine without write conflicts (CREW P-RAM). The main operation is essentially matrix multiplication, which is easily implementable on parallel computers with a fixed interconnection network of processors (<b>ultracomputers,</b> in the sense of [15]). Hence the problems considered also can be computed in log 2 n time using n 6 processors on a perfect shuffle computer (PSC) or a cube connected computer (CCC). An extension of the algorithm from [14] for the recognition of context-free languages on PSC and CCC can be used. If the parallel random access machine with concurrent writes (CRCW P-RAM is used then the minimum of m numbers can be determined in constant time (see [8]) and consequently the parallel time for the computation of dynamic programming problems can be reduced from log 2 (n) to log(n). We investigate also the parallel computation of Fees realising the optimal cost of dynamic programming problems...|$|R
5000|$|Jacob Theodore [...] "Jack" [...] Schwartz (January 9, 1930 - March 2, 2009) was an American mathematician, {{computer}} scientist, {{and professor}} of computer science at the New York University Courant Institute of Mathematical Sciences. He was the designer of the SETL programming language and started the NYU <b>Ultracomputer</b> project. He founded the New York University Department of Computer Science, chairing it from 1964 to 1980.|$|E
50|$|Snir {{received}} a Ph.D. in Mathematics from the Hebrew University of Jerusalem in 1979, worked at NYU on the NYU <b>Ultracomputer</b> project in 1980-1982, {{and worked at}} the Hebrew University of Jerusalem in 1982-1986, before joining IBM. Marc Snir was {{a major contributor to}} the design of the Message Passing Interface. He has published numerous papers and given many presentations on computational complexity, parallel algorithms, parallel architectures, interconnection networks, parallel languages and libraries and parallel programming environments.|$|E
40|$|Schwartz {{introduced}} {{the idea of}} an <b>ultracomputer</b> and reviewed various basic algorithms for such an ensemble of processors containing "shuffle" interconnections. Later, Harrison and Kalos {{introduced the}} idea of an N-dimensional <b>ultracomputer</b> (N-D UC), which is well suited for implementing many algorithms. The main result of this note is that an N-D UC can simulate the right/left and shuffle/unshuffle connections of its 1 -dimensional counterpart in N and 2 N steps, respectively. Thus, for each N, an ND UC is asymptotically as fast as a 1 -D UC of the same size. 1. Introduction Schwartz introduced {{the idea of an}} <b>ultracomputer</b> and reviewed various basic algorithms for such an ensemble of processors containing "shuffle" interconnections. Later, Harrison and Kalos introduced the idea of an N-dimensional <b>ultracomputer</b> (N-D UC), which is well suited for implementing many algorithms. The main result of this note is that an N-D can simulate the right/left ans shuffle/unshuffle connections of its [...] ...|$|E
40|$|Efficient interprocess {{synchronization}} {{tools for}} MIMD shared memory computers are critically {{important for the}} success of such systems. As members of the <b>Ultracomputer</b> Project at NYU, the authors have been participating in the continuing development of such algorithms. From their work, partially represented in this paper, they have observed that many of the more efficient algorithms utilize the storage and manipulation of more than one state variable in a single machine integer using fetch-and-add (see [GLR 83]). This paper describes this technique and presents several new implementations of busy-waiting synchronization functions which utilize this facility. 1. Introduction The NYU <b>Ultracomputer</b> is a shared memory MIMD computer (see [Gottlieb 86]). Each processor has a local cache for private data and code, and is provided access to an interleaved shared memory via a logarithmic multistage interconnection network. To avoid contention in the memory interface network, the <b>ultracomputer</b> [...] ...|$|E
40|$|This note {{describes}} a (presently implemented) <b>ultracomputer</b> emulator called PLUS which uses the multitasking and preprocessing features of PL/I {{to support a}} recursive <b>ultracomputer</b> programming style. Since the emulation to be described as written in PL/I, the powerful debugging features of the PL/I checkout compiler, as well as PL/I's separate compilation facility are available. This note also describes the emulation system and furnish a "User's Guide". In a subsequent part II we will discuss the system's implementation and prove both correctness and freedom from deadlock. 1. Introduction [UC] introduced {{the concept of an}} <b>ultracomputer</b> and reviewed various basic algorithms for such an ensemble of processors containing "shuffle" [Clos, Benes, and Stone] interconnections. [UCN 3] described a style for programming ultracomputers and rewrote several of the basic algorithms in this style. This new style forbids recursion so the recursive algorithms of [UC] appeared in iterative form [...] ...|$|E
40|$|The NYU <b>Ultracomputer</b> is {{a shared}} memory MIMD {{parallel}} computer design to contain thousands of processors connected by an Omega network to a like number of memory modules. A new coordination primitive fetch-and-add is defined {{and the network}} is enhanced to combine simultaneous requests, including fetch-and-adds, directed at the same memory location. The present report, an edited amalgam of papers I have coauthored {{with other members of}} the <b>Ultracomputer</b> project, discusses our work on architecture, programming models, system software, hardware and VLSI design, performance analysis, and simulation studies. 1. Introduction Since its inception in 1979, the NYU <b>Ultracomputer</b> project has studied the problem of how the fruits of the ongoing microelectronic revolution could be best used to solve problems having computational demands so vast as to make them inaccessible using current technology. Like others, we have always believed that parallelism provides the key new ingredient. With the pro [...] ...|$|E
40|$|The NYU <b>Ultracomputer</b> is {{a shared}} memory MIMD {{parallel}} computer design to con-tain thousands of processors connected by an Omega network to a like number of memory modules. A new coordination primitive fetch-and-add is defined and the net-work is enhanced to combine simultaneous requests, including fetch-and-adds, {{directed at the}} same memory location. The present report, an edited amalgam of papers I have coauthored {{with other members of}} the <b>Ultracomputer</b> project, discusses our work on ar-chitecture, programming models, system software, hardware and VLSI design, perfor-mance analysis, and simulation studies...|$|E
40|$|The NYU <b>Ultracomputer</b> is {{a shared}} memory MIMD {{parallel}} computer design to contain thousands of processors connected by an Omega network to a like number of memory modules. A new coordination primitive fetch-and-add is defined {{and the network}} is enhanced to combine simultaneous requests, including fetch-and-adds, directed at the same memory location. The present report, an edited amalgam of papers I have coauthored {{with other members of}} the <b>Ultracomputer</b> project, discusses our work on architecture, programming models, system software, hardware and VLSI design, performance analysis, and simulation studies. This work was supported in part by the Applied Mathematical Sciences Program of the US Department of Energy under contract DE-AC 02 - 76 ER 03077 and grant DE-FG 02 - 88 ER 25052 and in part by the National Science Foundation under grants DCR- 8413359 and DCR- 8320085. 1. Introduction Since its inception in 1979, the NYU <b>Ultracomputer</b> project has studied the problem of how the fruits of the o [...] ...|$|E
40|$|Ultracomputers are assemblages of {{processors}} {{that are}} able to operate concurrently and can exchange data through communication lines in, say, one cycle of operation. For physical reasons, the fan in/out of the processors must be limited. This imposes restric-tions on the possible communication schemes. I to have the <b>ultracomputer</b> operate efficiently as a whole, it is desirable that arbitrary exchanges of information between the processors can be effected in small number of data shifts. If a really huge <b>ultracomputer</b> is built, {{it would be nice if}} it could be constructed by coupling smaller ultracomputers, which in turn are assembled from still smaller ultra-computers, and so on. It will be shown that the latter desire conflicts to a certain extent with the earlier one. 1...|$|E
40|$|Serialization {{of memory}} access {{can be a}} {{critical}} bottleneck in shared memory parallel computers. The NYU <b>Ultracomputer,</b> a large-scale MIMD (Multiple Instruction stream, Multiple Data stream) shared memory architecture, {{may be viewed as}} a column of processors and a column of memory modules connected by a rectangular network of enhanced two by two buffered crossbars. These VLSI nodes enable the network to combine multiple requests directed at the same memory location. Such requests include a new coordination primitive, fetch-and-add, which permits task coordination to be achieved in a highly parallel manner. Processing within the network is used to reduce serialization at the memory modules. To avoid large network latency, the VLSI network nodes must be high-performance components. Design tradeoffs between architectural features, asymptotic performance requirements, cycle time and packaging limitations are complex. This report sketches the <b>Ultracomputer</b> architecture and discusses the iss [...] ...|$|E
40|$|Multistage {{interconnection}} {{networks are}} conventionally composed of 2 x 2 switching elements which perform only permutation functions. A {{number of networks}} have been built which provide more powerful switch functions involving replication and reduction of information, including the NYU <b>Ultracomputer</b> and several copy networks. Here we investigate these machines as a group, to see how replication and reduction are interrelated, and what other issues they involve. * This work supported by Bell Communications Research (Morristown, NJ), under the DAWN Project. 2 1. Introduction Multistage interconnection networks are composed of series of stages of regularly connectioned switching elements (usually 2 x 2). In conventional architectures, these networks support only permutation functions in their switch elements, although several notable exceptions have capitalized on other switch functions. These include the reduction capability of the NYU <b>Ultracomputer</b> [GotGK 83] and the replication capabilit [...] ...|$|E
40|$|Memory system {{congestion}} due to serialization of {{hot spot}} accesses can adversely affect {{the performance of}} interprocess coordination algorithms. Hardware and software techniques have been proposed to reduce this congestion and thereby provide superior system performance. The combining networks of Gottlieb et al. automatically parallelize concurrent hot spot memory accesses, improving the performance of algorithms that poll {{a small number of}} shared variables. We begin by debunking one of the performance claims made for the NYU <b>Ultracomputer.</b> Specifically, a gap in its simulation coverage hid a design flaw in the combining switches that seriously impacts the performance of busy wait polling in centralized coordination algorithms. We then debug the system by correcting the design and closing the simulation gap, after which we are able to duplicate the original claims of excellent performance on busy wait polling. Specifically our simulations show that, with the revised design, the <b>Ultracomputer</b> readers-writers and barrier algorithms achieve performance comparable to the highly regarded MCS algorithms. I...|$|E
40|$|Batcher's bitonic sort (cf. Knuth, v. III, pp. 232 ff) is a sorting network, {{capable of}} sorting n inputs in Q((log n) 2) stages. When adapted to {{conventional}} computers, it {{gives rise to}} an algorithm that runs in time Q(n(log n) 2). The method can also be adapted to ultracomputers (Schwartz [1979]) to exploit their high degree of parallelism. The resulting algorithm will take time Q((log N) 2) for ultracomputers of "size" N. The implicit constant factor is low, so that even for moderate values of N the <b>ultracomputer</b> architecture performs faster than the Q(N log N) time conventional architecture can achieve. The purpose of this note is to describe the adapted algorithm. After some preliminaries a first version of the algorithm is given whose correctness is easily shown. Next, this algorithm is transformed to make it suitable for an <b>ultracomputer.</b> 1. Introduction Batcher's bitonic sort (cf. Knuth, v. III, pp. 232 ff) is a sorting network, capable of sorting n inputs in Q((log n [...] ...|$|E
40|$|This report {{describes}} the combining switch {{that we have}} implemented {{for use in the}} 16 Θ 16 processor/memory interconnection network of the NYU <b>Ultracomputer</b> prototype. Packaging, message types and message formats are described. Details are given about the internal logic of the two component types used in the network, including the systolic combining queue, the wait buffer and the ALU. Systolic combining queue designs with greater combining capability are sketched. 1 Combining switch architecture A combining switch has been fabricated for use in theΩ network of the NYU <b>Ultracomputer</b> prototype [3]. A 2 Θ 2 switch node is composed of four each of two types of custom VLSI chips: forward path and return path components (Figure 1). Control for the switch is distributed as tri-state selection logic in each chip. The routing information is included in the message. Flow control avoids the necessity of acknowledgement for messages and allows pre-computation of signals that cont [...] ...|$|E
40|$|This paper {{describes}} a further {{extension of the}} combining fetch-and-add used in shared memory multiprocessors such as the NYU <b>Ultracomputer</b> and the IBM RPn. This extension permits the elimination of busy-waiting {{in a number of}} important parallel operations. These operations include barrier synchronization, stack and queue operations, full/empty bits, and group lock; most of these operations become one or two singleinstruction operations for the PE. The hardware necessary is compatible with the design of the switches which are used in the <b>Ultracomputer,</b> and in fact are implemented as fetch-and-add instructions with an increment of unity. 1. Summary In this paper we describe a relatively minor architectural addition to the combining fetch-and-add instruction which has been used on a number of shared memory machines. The main idea is to provide a (micro-) programmable interface between the network and the memory. This programmable interface (or memory controller), which we refer to as th [...] ...|$|E
40|$|We {{desire to}} permute N items w 0 [...] ., w N - 1, in an <b>ultracomputer</b> {{containing}} P processing elements (PEs), PE 0, [...] . PE P - 1. Under the assumpution that N+P and that w i e PE i, Schwartz gives the following worst case analyses: The static permutation algorithm requires 4 log P - 3 data communication steps. It is easily seen that for both algorithms the average case behavior closely approximates the worst case. Here {{we present a}} data motion algorithm oriented toward average case rather than worst case performance, and supply an argument suggesting that the following average number of data communication steps required is approximately 3 log P. 1. Introduction [UC] introduced {{the idea of an}} <b>ultracomputer</b> and reviewed algorithms for two permutation problems: The "static permutation problem", in which an algorithm is tailored to each specific permutation p (given in advance), and the "dynamic permutation problem", in which one algorithm must effect all permutations (i. e., the permuta [...] ...|$|E
40|$|Multistage {{interconnection}} {{networks are}} conventionally composed of 2 x 2 switching elements which perform only permutation functions. A {{number of networks}} have been built which provide more powerful switch functions involving replication and reduction of information, including the NYU <b>Ultracomputer</b> and several copy networks. Here we investigate these machines as a group, to see how replication and reduction are interrelated, and what other issues they involve...|$|E
40|$|ABSTRACT: Multistage {{interconnection}} {{networks are}} conventionally composed of 2 x 2 switching elements which perform only permutation functions. A {{number of networks}} have been built which provide more powerful switch functions involving replication and reduction of information, including the NYU <b>Ultracomputer</b> and several copy networks. Here we investigate these machines as a group, to see how replication and reduction are interrelated, and what other issues they involve. * This work supported by Bell Communications Research (Morristown, NJ), under the DAWN Project. ...|$|E
40|$|Molasses is a {{simulator}} for {{a complete}} <b>Ultracomputer</b> system, containing processors, memory modules, and a multi-stage interconnection network supporting combining. The simulated machine is quite close to the Ultra- 3 prototype. A large number of parameters may be set by the user to alter {{the characteristics of the}} simulated machine and the data to be collected. Molasses relies on careful implementation, parallelism, checkpoint/restart, and specialized modes of simulation to achieve high performance. Performance may be maximized by automatically building a custom version of Molasses with most parameters fixed to constant values; this allows the compiler to generate superior code, typically doubling performance. Although the preliminary version of Molasses does not yet simulate the Ultra- 3 processors, its good performance and flexibility have already made it useful for network studies...|$|E
40|$|A new {{formalism}} {{is given}} for read-modify-write (RMW) synchronization operations. This formalism {{is used to}} extend the memory reference combining mechanism, introduced in the NYU <b>Ultracomputer,</b> to arbitrary RMW operations. A formal correctness proof of this combining mechanism is given. General requirements for the practicality of combining are discussed. Combining is shown to be practical for many useful memory access operations. This includes memory updates of the form mem_val := mem_val op val, where op need not be associative, {{and a variety of}} synchronization primitives. The computation involved is shown to be closely related to parallel prefix evaluation. 1. INTRODUCTION Shared memory provides convenient communication between processes in a tightly coupled multiprocessing system. Shared variables can be used for data sharing, information transfer between processes, and, in particular, for coordination and synchronization. Constructs such as the semaphore introduced by Dijkstra in [...] ...|$|E
40|$|We {{present an}} {{extension}} to the FORTRAN language {{that allows the}} user to specify parallelism by means of clearly defined, nestable blocks. The implementation achieves compilerindependence through a portable preprocessor. High performance is obtained by prespawning processes and relying {{on a set of}} run-time routines to manage a self-scheduling allocation scheme. The resulting system, which we call ParFOR, lends itself to the exploitation of finegrained parallelism because of its low scheduling overhead. It encourages the elimination of explicit process synchronization, thereby enhancing the readability of the source program. In addition, ParFOR provides a variety of modes and compile-time options that are useful for performance measurement and debugging. Finally, we present an evaluation of system efficiency including timing results for several parallel applications running on the eight-processor <b>Ultracomputer</b> prototype. 1. Introduction It has been shown that parallel programming can b [...] ...|$|E
40|$|An outsider's view of {{the current}} state of {{dataflow}} research is presented. Two related observations are made. First, the field has matured from emphasizing proof of concept to pragmatic concerns needed to obtain high performance. Second, dataflow research is moving closer to mainstream parallel processing. Several examples of dataflow research supporting these observations are given. They include two-stage enabling rules and other eclectic scheduling disciplines, hardware and software throttling of parallelism, non-functional and (possibly) non-deterministic execution semantics, traditional memory management, and von Neumann compatibility. ________________________________ Supported in part by Department of Energy contract DE-FG 02 - 88 ER 025053. 1. Introduction This report offers an outsiders {{view of the}} current state of dataflow research, emphasizing material presented at the Eilat workshop. My own research, {{as a part of the}} New York University <b>Ultracomputer</b> project, concerns more tra [...] ...|$|E
40|$|In 1982, {{a report}} {{dealing with the}} nation's {{research}} needs in high-speed computing called for increased access to supercomputing resources for the research community, research in computational mathematics, and increased research in the technology base needed {{for the next generation}} of supercomputers. Since that time a number of programs addressing future generations of computers, particularly parallel processors, have been started by U. S. government agencies. The present paper provides a description of the largest government programs in parallel processing. Established in fiscal year 1985 by the Institute for Defense Analyses for the National Security Agency, the Supercomputing Research Center will pursue research to advance the state of the art in supercomputing. Attention is also given to the DOE applied mathematical sciences research program, the NYU <b>Ultracomputer</b> project, the DARPA multiprocessor system architectures program, NSF research on multiprocessor systems, ONR activities in parallel computing, and NASA parallel processor projects...|$|E
40|$|In shared-memory multiprocessors, {{combining}} networks {{serve to}} eliminate hot spots due to concurrent {{access to the}} same memory location. Examples are the NYU <b>Ultracomputer,</b> the IBM RP 3 and the Fluent Machine. We present a problem that occurs when one tries to implement the Fluent Machine's network nodes with network chips that do not know their position within the network. We formulate the problem mathematically and present two solutions. The first solution requires some additional hardware around nodes that can be put outside network chips. The second solution requires a minor modification of the routing algorithm, but one can prove that there is no performance loss. Keywords: Computer Architecture, combining networks, butterfly networks 1 Introduction In machines with emulated shared memory, combining networks serve two purposes. First, they route memory access requests between processors and memory modules. Second, they merge concurrent accesses of several processors to one memory ce [...] ...|$|E
40|$|This note {{describes}} a proposed extension to {{the architecture of}} shared memory multiprocessors with combining fetch-and-add operations, such as the NYU <b>Ultracomputer</b> and the IBM RPn. The extension involves addition of {{a small amount of}} hardware between the network and the memory, which permits the efficient implementation of a number of parallel operations. Examples are given. 1. Introduction It has been shown [GGKMRS,GLR] that a number of important operations can be made completely parallel using the combining fetch-and-add (F&A) operation. These include parallel queue operations and semaphores, More recently, Dimitrovsky [D 85] has shown that parallel garbage collection can be implemented using F&A, and in [D 86] has proposed a new synchronization mechanism, the group lock, which can also be implemented using F&A. However, the implementation of these operations in terms of fetch-and-add, though bounded in time independent of the number of processors, is generally slow (in some cases of [...] ...|$|E
40|$|Multi-stage Interconnection Networks (MINs) {{have been}} {{demonstrated}} {{to be one of}} the most cost-effective and useful communication media between processors and memory modules in parallel processing systems. In the last decade, several multiprocessor systems have been built based on this architecture. Though MINs are fairly flexible in handling varieties of traffic loads, the performance will be degraded by hotspot traffic. This is commonly known as tree saturation effect. This situation becomes worse with increasing system size. The existing combining network approach, by NYU <b>Ultracomputer,</b> is considered {{to be one of the}} most effective ways to alleviate the tree saturation problem. However, the combining network approach offers no performance advantage for non-hotspot traffic. This paper proposes a new approach to alleviate the tree saturation problem and at the same time improve performance of the MIN. In the proposed approach, each switch has certain packet diverting capabi [...] ...|$|E
40|$|Abstract. The {{power of}} shared-memory in models of {{parallel}} computation is studied, and a novel distributed data structure that {{eliminates the need}} for shared memory without significantly increasing the run time of the parallel computation is described. More specifically, it is shown how a complete network of processors can deterministically simulate one PRAM step in O(l~gn(loglogn) ~) time when both models use n processors {{and the size of the}} PRAM’s shared memory is polynomial in n. (The best previously known upper bound was the trivial O(n)). It is established that this upper bound is nearly optimal, and it is proved that an on-line simulation of T PRAM steps by a complete network of processors requires a (T(logn/loglogn)) time. A simple consequence of the upper bound is that an <b>Ultracomputer</b> (the currently feasible generalpurpose parallel machine) can simulate one step of a PRAM (the most convenient parallel model to program) in @(log n) 210 g log n) steps...|$|E
40|$|During {{the final}} stage of {{computer}} system design and implementation, using some flexible and convenient tools can save {{a lot of time and}} efforts. For example, when all subcomponents of a design are carefully placed on a board, and the design schematics and some necessary data are given, a wiring tool is strongly needed so that the wiring can be done easily and automatically. However, it is a practical question how to route each signal wire for many components on any given board under various constraints and complicated situations. This report describes a general-purpose wiring tool which meets such a kind of demand, and is used particularly for <b>Ultracomputer</b> prototyping and implementations. The most important part of the wiring tool is how to solve the routing problem for a signal net. We reduce it to a Traveling Salesman Problem (TSP) with various constraints. The wiring tool employs a heuristic and fast TSP algorithm that is able to produce optimal results in high probabilities. The [...] ...|$|E
40|$|We {{investigate}} {{the construction and}} application of parallel software caches in shared memory multiprocessors. In contrast to maintaining a private cache for each thread, a parallel cache allows the re-use of results of lengthy computations by other threads. This is especially important in irregular applications where the re-use of intermediate results by scheduling is not possible. Example applications are the computation of intersections between a scanline and a polygon in computational geometry, and the computation of intersections between rays and objects in ray tracing. A parallel software cache {{is based on a}} readers/writers lock, i. e. as long as no thread alters the cache data structure, multiple threads may read simultaneously. If a thread wants to alter the cache because of a cache miss, it waits until all other threads have left the data structure, then it can update the contents of the cache. Other threads can access the cache only after the writer has finished its work. To increase utilization, the cache has a number of slots that can be locked separately. We {{investigate the}} tradeoff between slot size, search time in the cache, and the time to re-compute a cache entry. Another major difference between sequential and parallel software caches is the replacement strategy. We adapt classic replacement strategies such as LRU and random replacement for parallel caches. As execution platform, we use the SB-PRAM, but the concepts might be portable to machines such as NYU <b>Ultracomputer,</b> Tera MTA, and Stanford DASH...|$|E
40|$|Ihavebeen {{fortunate}} {{to carry out}} the research for this dissertation in the stimulating and supportive environment of the <b>Ultracomputer</b> Research Laboratory. Iwould like to thank my advisor, Allan Gottlieb, Ultra's director, for his role in creating that environment aswell as for his careful reading of several drafts of my dissertation. As part of my work for Ultra, I have enjoyed fruitful research collaborations with Richard Kenner, my long-term partner in VLSI design, Ora Percus, my mentor in stochastic analysis and queueing theory, Yue-sheng Liu, who helped me formalize my thinking about switch types as well as simulating network behavior, Jan Edler, who was always willing to add another feature to his network simulator, and Ron Bianchini, who makes hardware work. In the computer science department as a whole, I would like to thank Alan Siegel, for reading my dissertation and making helpful comments, Ernie Davis and Richard Wallace, for serving on my committee, Elaine Weyuker, whose literature review seminar was very helpful in building both my con dence and competence to do computer science research, and Anina Karmen-Meade, who often helped me navigate NYU's bureaucratic labyrinths. Most of all, my gratitude goes to my husband, Tom Du, for his faith and support at all times, and to my children, Rachael Evans, Keelan Evans and Timothy Du. No matter how the research is going, my wonderful family always lls my life with joy. ii Content...|$|E
40|$|There is {{a phrase}} {{widely used in}} Chinese fortune-telling: “From {{south-east}} come people of great help. ” In my limited life experience, helpful people seem to come from all directions. There {{are so many people}} who helped me in so many ways, the appreciation I can express on paper can only show the tip of the iceberg. First and foremost, I thank my advisor Alan Siegel for guiding me, both in school and in real life, through quite a few whimsical years of my life. I often find myself quoting his words of wisdom to my junior fellows. I also thank Allan Gottlieb, Eric Freudenthal, and other colleagues in the <b>UltraComputer</b> Laboratory for some very stimulating research experience; Arthur Goldberg for showing me the way to the world of GroupWare and the Internet; Malcolm Harrison, Robert Dewar, Benjamin Goldberg, Edmond Schoenberg, and other knowledgeable minds in the Language Laboratory for broadening my views; Richard Cole, often through short and wise comments, setting a vivid example as a research scholar; my dear friends Charlie Repetti, Nai-Wei Hsue, Hseuming Chen and David Solomon for feeding me mental stimuli and connections to industry; and last but not least, Peter Barnett for teaching me the responsible perspectives towards many things. My parents Shing-Jeng Jai and Chio-Jane Hu brought me into this world and nurtured me through difficult times. My wife Angela kept me motivated. Without their love, I would not have lasted so long in this endeavor. My brothers Robin and Jackson are always setting good examples for me to follow. My admiration toward them can not be described by words...|$|E
40|$|Architecture reconfiguration, {{the ability}} of a system to alter the active {{interconnection}} among modules, has a history of different purposes and strategies. Its purposes develop from the relatively simple desire to formalize procedures that all processes have in common to reconfiguration for the improvement of fault-tolerance, to reconfiguration for performance enhancement, either through the simple maximizing of system use or by sophisticated notions of wedding topology to the specific needs of a given process. Strategies range from straightforward redundancy by means of an identical backup system to intricate structures employing multistage interconnection networks. The present discussion surveys the more important contributions to developments in reconfigurable architecture. The strategy here is in a sense to approach the field from an historical perspective, with the goal of developing a more coherent theory of reconfiguration. First, the Turing and von Neumann machines are discussed from the perspective of system reconfiguration, and it is seen that this early important theoretical work contains little that anticipates reconfiguration. Then some early developments in reconfiguration are analyzed, including the work of Estrin and associates on the 2 ̆ 2 fixed plus variable 2 ̆ 2 restructurable computer system, the attempt to theorize about configurable computers by Miller and Cocke, and the work of Reddi and Feustel on their restructable computer system. The discussion then focuses on the most sustained systems for fault tolerance and performance enhancement that have been proposed. An attempt will be made to define fault tolerance and to investigate some of the strategies used to achieve it. By investigating four different systems, the Tandern computer, the C. vmp system, the Extra Stage Cube, and the Gamma network, the move from dynamic redundancy to reconfiguration is observed. Then reconfiguration for performance enhancement is discussed. A survey of some proposals is attempted, then the discussion focuses on the most sustained systems that have been proposed: PASM, the DC architecture, the Star local network, and the NYU <b>Ultracomputer.</b> The discussion is organized around a comparison of control, scheduling, communication, and network topology. Finally, comparisons are drawn between fault tolerance and performance enhancement, in order to clarify the notion of reconfiguration and to reveal the common ground of fault tolerance and performance enhancement as well as the areas in which they diverge. An attempt is made in the conclusion to derive from this survey and analysis some observations on the nature of reconfiguration, as well as some remarks on necessary further areas of research...|$|E
