295|10000|Public
25|$|Average-case {{complexity}}: This is {{the complexity}} of solving the problem on an average. This complexity is only defined {{with respect to a}} probability distribution over the inputs. For instance, if all inputs of the same size are assumed to be equally likely to appear, the average case complexity can be defined with respect to the <b>uniform</b> <b>distribution</b> <b>over</b> all inputs of size n.|$|E
2500|$|Transition {{matrices}} {{that are}} symmetric (P'ij=P'ji or P(s′,s)=P(s,s′)) always have detailed balance. [...] In these cases, a <b>uniform</b> <b>distribution</b> <b>over</b> the states is an equilibrium distribution. [...] For continuous systems with detailed balance, {{it may be}} possible to continuously transform the coordinates until the equilibrium distribution is uniform, with a transition kernel which then is symmetric. [...] In the case of discrete states, {{it may be possible}} to achieve something similar by breaking the Markov states into appropriately-sized degenerate sub-states.|$|E
2500|$|By {{the upper}} bound on burst error {{detection}} (...) , we know that a cyclic code can not detect all bursts of length [...] However cyclic codes can indeed detect most bursts of length [...] The reason is that detection fails only when the burst is divisible by [...] Over binary alphabets, there exist [...] bursts of length [...] Out of those, only [...] are divisible by [...] Therefore, the detection failure probability is very small (...) assuming a <b>uniform</b> <b>distribution</b> <b>over</b> all bursts of length [...]|$|E
5000|$|... #Caption: A {{horseshoe}} vortex {{caused by a}} (purely theoretical) <b>uniform</b> lift <b>distribution</b> <b>over</b> an aircraft’s wing ...|$|R
40|$|Abstract — The {{range of}} the map P � (IC(P), H(P)) is determined. Here, P denotes a distribution, IC(P) its index of {{coincidence}} and H(P) its entropy. Let M 1 +(n) be the set of probability <b>distributions</b> <b>over</b> an n-letter alphabet. <b>Uniform</b> <b>distributions</b> <b>over</b> k-subsets are denoted Uk. We consider entropy H(P) and divergence D(P ‖Q) {{as well as the}} index of coincidence, IC(P) = ∑ p 2 i (known from cryptoanalysis). The measure of roughness is MRn(P) = IC(P) − 1 /n and the relative measure of roughness is MR n (P) = MRn(...|$|R
40|$|AbstractWe {{present a}} new method for testing {{compatibility}} of candidate edges in the greedy triangulation, and new {{results on the}} rank of edges in various triangulations. Our edge test requires O(1) time for test and update, O(n) space, and O(n) time to initialize. Based on these results, we present fast greedy triangulation algorithms with expected case running time of O(n log n) for <b>uniform</b> <b>distributions</b> <b>over</b> convex regions. While algorithms with O(n) expected case running times exist, the algorithms presented here are simpler to implement and work well in practice...|$|R
2500|$|The WoS method {{provides}} an efficient way of sampling the first exit {{point of a}} Brownian motion from the domain, by remarking that for any -sphere centred on , the first point of exit of [...] out of the sphere has a <b>uniform</b> <b>distribution</b> <b>over</b> its surface. Thus, it starts with [...] equal to , and draws the largest sphere [...] centered on [...] and contained inside the domain. The first point of exit [...] from [...] is uniformly distributed on its surface. By repeating this step inductively, the WoS provides a sequence [...] of positions of the Brownian motion.|$|E
50|$|The <b>uniform</b> <b>distribution</b> <b>over</b> any convex set.|$|E
5000|$|... #Caption: Comparing the VaR, CVaR and EVaR for the <b>uniform</b> <b>distribution</b> <b>over</b> the {{interval}} (0,1) ...|$|E
50|$|Exact t-designs over quantum states {{cannot be}} {{distinguished}} from the <b>uniform</b> probability <b>distribution</b> <b>over</b> all states when using t copies of a state from the probability distribution. However in practice even t-designs {{may be difficult to}} compute. For this reason approximate t-designs are useful.|$|R
40|$|Hit-and-run, a {{class of}} MCMC {{samplers}} that converges to general multivariate distributions, {{is known to be}} unique in its ability to mix fast for <b>uniform</b> <b>distributions</b> <b>over</b> convex bodies. In particular, its rate of convergence to a <b>uniform</b> <b>distribution</b> is of a low order polynomial in the dimension. However, when the body of interest is difficult to sample from, typically a hyperrectangle is introduced that encloses the original body, and a one-dimensional acceptance/rejection is performed. The fast mixing analysis of hit-and-run does not account for this one-dimensional sampling that is often needed for implementation of the algorithm. Here we show that the effect {{of the size of the}} hyperrectangle on the efficiency of the algorithm is only a linear scaling effect. We also introduce a variation of hit-and-run that accelerates the sampler, and demonstrate its capability through a computational study...|$|R
40|$|Thin film filters used in {{fibre optics}} {{communications}} are strictly defined, {{which requires a}} totally <b>uniform</b> thickness <b>distribution</b> <b>over</b> the coated substrate. In this paper we firstly analyse the thickness distribution related to chamber geometry and source emission function. Chamber layout and source function are optimised for large area uniformity...|$|R
5000|$|Let X,Y be two {{independent}} random variables with discrete <b>uniform</b> <b>distribution</b> <b>over</b> the set [...] Then ...|$|E
5000|$|If k exemplars {{are chosen}} (without replacement) from a {{discrete}} <b>uniform</b> <b>distribution</b> <b>over</b> the set {1, 2, ..., N} with unknown upper bound N, the MVUE for N is ...|$|E
50|$|A set of vectors {{is said to}} be in {{isotropic}} position if the <b>uniform</b> <b>distribution</b> <b>over</b> {{that set}} is in isotropic position. In particular, every orthonormal set of vectors is isotropic.|$|E
40|$|An {{algorithm}} inferring a boolean {{linear code}} from noisy patterns received by a noisy channel, {{under the assumption}} of <b>uniform</b> occurrence <b>distribution</b> <b>over</b> the codewords, and an upper bound {{to the amount of}} data are presented. A vector quantizer is designed from the noisy patterns, choosing the obtained codebook as code approximation...|$|R
40|$|A {{statistical}} mechanical {{framework for}} analyzing random linear vector channels {{is presented in}} a large system limit. The framework {{is based on the}} assumptions that the left and right singular value bases of the rectangular channel matrix are generated independently from <b>uniform</b> <b>distributions</b> <b>over</b> Haar measures and the eigenvalues of ^ T asymptotically follow a certain specific distribution. These assumptions make it possible to characterize the communication performance of the channel utilizing an integral formula with respect to, which is analogous to the one introduced by Marinari et. al. in J. Phys. A 27, 7647 (1994) for large random square (symmetric) matrices. A computationally feasible algorithm for approximately decoding received signals based on the integral formula is also provided. Comment: Submitted to PHYSCOMNET 0...|$|R
50|$|The {{simplest}} {{approach is}} to add one to each observed number of events including the zero-count possibilities. This is sometimes called Laplace's Rule of Succession. This approach is equivalent to assuming a <b>uniform</b> prior <b>distribution</b> <b>over</b> the probabilities for each possible event (spanning the simplex where each probability is between 0 and 1, and they all sum to 1).|$|R
5000|$|With [...] {{standing}} for the dirac delta. The {{important thing to}} note {{here is that the}} bond position vector has a <b>uniform</b> <b>distribution</b> <b>over</b> a sphere of radius , our constant bond length.|$|E
5000|$|It {{asserts that}} for any {{sequence}} of random [...] matrices whose entries are independent and identically distributed random variables, all with mean zero and variance equal to , the limiting spectral distribution is the <b>uniform</b> <b>distribution</b> <b>over</b> the unit disc.|$|E
50|$|For small squares it is {{possible}} to generate permutations and test whether the Latin square property is met. For larger squares, Jacobson and Matthews' algorithm allows sampling from a <b>uniform</b> <b>distribution</b> <b>over</b> the space of n × n Latin squares.|$|E
40|$|Abstract. In {{this paper}} we {{introduce}} a new Monte Carlo method for sampling lattice self-avoiding walks. The method, which we call “GAS ” (Generalised Atmospheric Sampling), samples walks along weighted sequences by implementing elementary moves generated by the positive, negative and neutral atmospheric statistics of the walks. A realised sequence is weighted such that the average weight of states of length n {{is proportional to the}} number of self-avoiding walks from the origin cn. In addition, the method also self-tunes to sample from <b>uniform</b> <b>distributions</b> <b>over</b> walks of lengths in an interval [0, nmax]- this implementation will be called “flatGAS ” (flat histogram GAS). We show how to implement flatGAS using both generalised and endpoint atmospheres of walks, and analyse our data to obtain estimates of the growth constant and entropic exponent of self-avoiding walks in the square and cubic lattice...|$|R
3000|$|The {{computational}} {{complexity of}} the proposed robust algorithm can be counted roughly as follows. We address the robust resource allocation problem with two steps, i.e., relay selection and power allocation separately. First, we allocate subcarriers to relays assuming <b>uniform</b> power <b>distribution</b> <b>over</b> subcarriers. Each subcarrier selects the best relay (i.e., the relay with highest ρ _k^i^*) for itself with computational complexity K. We assume T [...]...|$|R
40|$|In this paper, the probability-guaranteed H ∞ finite-horizon {{filtering}} {{problem is}} investigated {{for a class}} of nonlinear time-varying systems with uncertain parameters and sensor saturations. The system matrices are functions of mutually independent stochastic variables that obey <b>uniform</b> <b>distributions</b> <b>over</b> known finite ranges. Attention {{is focused on the}} construction of a time-varying filter such that the prescribed H ∞ performance requirement can be guaranteed with probability constraint. By using the difference linear matrix inequalities (DLMIs) approach, sufficient conditions are established to guarantee the desired performance of the designed finite-horizon filter. The time-varying filter gains can be obtained in terms of the feasible solutions of a set of DLMIs that can be recursively solved by using the semi-definite programming method. A computational algorithm is specifically developed for the addressed probability-guaranteed H ∞ finite-horizon filtering problem. Finally, a simulation example is given to illustrate the effectiveness of the proposed filtering scheme...|$|R
5000|$|Let [...] be the <b>uniform</b> <b>distribution</b> <b>over</b> the [...] opening {{values for}} {{security}} parameter k.A commitment scheme is respectively perfect, statistical, or computational hiding, if for all [...] the probability ensembles [...] and [...] are equal, statistically close, or computationally indistinguishable.|$|E
5000|$|Note that, in {{the above}} model (and also the one below), the prior {{distribution}} of the initial state [...] is not specified. Typical learning models correspond to assuming a discrete <b>uniform</b> <b>distribution</b> <b>over</b> possible states (i.e. no particular prior distribution is assumed).|$|E
5000|$|A random {{variable}} [...] over [...] is a k-wise independent space if, for all index sets [...] of size , the marginal distribution [...] is exactly {{equal to the}} <b>uniform</b> <b>distribution</b> <b>over</b> [...]That is, for all such [...] and all strings , the distribution [...] satisfies [...]|$|E
3000|$|... [...]) {{over the}} set a+ 1,a+ 2,…,m {{using the method}} in [41], which gives almost <b>uniform</b> {{sampling}} of <b>distributions</b> <b>over</b> the corresponding set. Let η =(∑ _i=a+ 1 ^mit_i-t̅)/(∑ _i=a+ 1 ^mit_i-∑ _i= 0 ^ait_i)> 0. Then, we get a distribution (η [...]...|$|R
40|$|We {{present a}} {{multiple}} pass streaming algorithm for learning the density {{function of a}} mixture of k <b>uniform</b> <b>distributions</b> <b>over</b> rectangles (cells) in ^d, for any d> 0. Our learning model is: samples drawn according to the mixture are placed in arbitrary order in a data stream that may only be accessed sequentially by an algorithm with a very limited random access memory space. Our algorithm makes 2 ℓ+ 1 passes, for any ℓ> 0, and requires memory at most Õ(ϵ^- 2 /ℓk^ 2 d^ 4 +(2 k) ^d). This exhibits a strong memory-space tradeoff: a few more passes significantly lowers its memory requirements, thus trading {{one of the two}} most important resources in streaming computation for the other. Chang and Kannan chang 06 first considered this problem for d= 1, 2. Our learning algorithm is especially appropriate for situations where massive data sets of samples are available, but practical computation with such large inputs requires very restricted models of computation...|$|R
40|$|This article {{deals with}} random {{projections}} applied as a data reduction technique for Bayesian regression analysis. We show sufficient {{conditions under which}} the entire d-dimensional distribution is approximately preserved under random projections by {{reducing the number of}} data points from n to k∈ O(poly(d/ε)) in the case n≫ d. Under mild assumptions, we prove that evaluating a Gaussian likelihood function based on the projected data instead of the original data yields a (1 +O(ε)) -approximation in terms of the ℓ_ 2 Wasserstein distance. Our main result shows that the posterior distribution of Bayesian linear regression is approximated up to a small error depending on only an ε-fraction of its defining parameters. This holds when using arbitrary Gaussian priors or the degenerate case of <b>uniform</b> <b>distributions</b> <b>over</b> R^d for β. Our empirical evaluations involve different simulated settings of Bayesian linear regression. Our experiments underline that the proposed method is able to recover the regression model up to small error while considerably reducing the total running time...|$|R
5000|$|The {{scale of}} each part [...] {{relative}} to a reference frame is modeled by a Gaussian density with parameters [...] Each part {{is assumed to be}} independent of other parts. The background model [...] assumes a <b>uniform</b> <b>distribution</b> <b>over</b> scale, within a range [...]|$|E
5000|$|Suppose {{there are}} two bidders, Alice and Bob, whose valuations a and b are drawn from a Continuous <b>uniform</b> <b>distribution</b> <b>over</b> the {{interval}} 0,1. Then, it is a Bayesian-Nash equilibrium when each bidder bids exactly half his/her value: Alice bids [...] and Bob bids [...]|$|E
5000|$|A {{function}} [...] {{is called}} an -merger if for every set of random variables [...] distributed over , {{at least one}} of which is uniform, the distribution of [...] has smooth min-entropy [...] The variable [...] denotes the <b>uniform</b> <b>distribution</b> <b>over</b> [...] bits, and represents a truly random seed.|$|E
50|$|The {{samples of}} a white noise signal may be {{sequential}} in time, or arranged along one or more spatial dimensions. In digital image processing, the pixels {{of a white}} noise image are typically arranged in a rectangular grid, and {{are assumed to be}} independent random variables with <b>uniform</b> probability <b>distribution</b> <b>over</b> some interval. The concept can be defined also for signals spread over more complicated domains, such as a sphere or a torus.|$|R
40|$|Low megavoltage electrons, {{because of}} their limited penetration, have been found very useful in the {{treatment}} of generalized superficial malignancies. However, because of the complexity of the human body contour, it is extremely difficult to achieve <b>uniform</b> dose <b>distribution</b> <b>over</b> the entire body surface. To achieve this, various techniques ranging from two to six fields have been used. In this paper, we discuss the disadvantages of these techniques and describe a new technique, “the rotation technique,” which is superior...|$|R
5000|$|Based on {{the results}} above a simple {{centralized}} trust value computing algorithm can be written. Note that we assume that all the local trust values for the whole network are available and present in the matrix C. We also note that, if the equation shown above converges, we can replace the initial vector [...] by a vector [...] that is an m-vector representing <b>uniform</b> probability <b>distribution</b> <b>over</b> all m peers. The basic EigenTrust algorithm is shown below: ...|$|R
