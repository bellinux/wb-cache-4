25|56|Public
50|$|When {{a network}} {{is in this}} condition, it settles into a stable state where traffic demand is high but little <b>useful</b> <b>throughput</b> is available, packet delay and loss occur and quality of service is {{extremely}} poor.|$|E
50|$|Channel {{bandwidth}} may {{be confused}} with useful data throughput (or goodput). For example, a channel with x bps may not necessarily transmit data at x rate, since protocols, encryption, and other factors can add appreciable overhead. For instance, much internet traffic uses the transmission control protocol (TCP), which requires a three-way handshake for each transaction. Although in many modern implementations the protocol is efficient, it does add significant overhead compared to simpler protocols. Also, data packets may be lost, which further reduces the useful data throughput. In general, for any effective digital communication, a framing protocol is needed; overhead and effective throughput depends on implementation. <b>Useful</b> <b>throughput</b> is {{less than or equal}} to the actual channel capacity plus implementation overhead.|$|E
3000|$|... grid {{topology}} for 10 randomly {{generated traffic}} profiles. In each profile, we randomly chose twelve source and destination node pairs to generate UDP (User Datagram Protocol) sessions. Each has the transmission demand uniformly distributed between 1 [*]Mbps and 5 [*]Mbps. Then we change every flow's rate proportionally until the network can satisfy 90 % of the injected traffic. The metric we examine {{is the total}} <b>useful</b> <b>throughput</b> across all sessions.|$|E
40|$|This paper {{argues for}} a new {{approach}} to building Byzantine fault tolerant replication systems. We observe that although recently developed BFT state machine replication protocols are quite fast, they don’t tolerate Byzantine faults very well: a single faulty client or server is capable of rendering PBFT, Q/U, HQ, and Zyzzyva virtually unusable. In this paper, we (1) demonstrate that existing protocols are dangerously fragile, (2) define a set of principles for constructing BFT services that remain useful even when Byzantine faults occur, and (3) apply these principles to construct a new protocol, Aardvark. Aardvark can achieve peak performance within 40 % of that of the best existing protocol in our tests and provide a significant fraction of that performance when up to f servers and any number of clients are faulty. We observe <b>useful</b> <b>throughputs</b> between 11706 and 38667 requests per second for a broad range of injected faults. ...|$|R
40|$|We {{report the}} results of a study of the {{performance}} of engineered nonhierarchical and hierarchical routing net-works under overloads, using analytical and simulation models. We also examine the effect of one control, name-ly, trunk reservation for first-routed traffic. Based on our results, we recommend that such a control be considered in the introduction of non hierarchical routing networks in order to ensure maximum <b>useful</b> network <b>throughput</b> under all traffic conditions. 1...|$|R
50|$|Shaders {{that are}} {{designed}} to be executed directly on the GPU became <b>useful</b> for high <b>throughput</b> general processing because of their stream programming model; this {{led to the development of}} compute shaders running on similar hardware (see also: GPGPU).|$|R
40|$|International audienceWe {{introduce}} P 3 -DCF, a new {{quality of}} service (QoS) mechanism, within the MAC protocol in IEEE 802. 11. P 3 -DCF enhances the distributed coordination function (DCF) for prioritized service in IEEE 802. 11. A novel function, the per-packet priority function, integrated to DCF establishes not only a per-flow differentiation but schedules the packets with an earliest deadline first discipline as well. The simulations carried out show that the enhanced MAC protocol satisfies the maximum tolerable latency of real-time traffics and performs efficient flow differentiation. Moreover, the performance improvement is achieved without affecting the <b>useful</b> <b>throughput...</b>|$|E
40|$|In {{recent years}} {{a number of}} TCP {{variants}} have emerged to optimise some aspect of data transport where high delay-bandwidth product paths are common. We evaluate a different scenario - latency-sensitive UDP-based traffic sharing a consumer-grade 'broadband' link {{with one or more}} TCP flows. In particular we compare Linux implementations of NewReno, H-TCP and CUBIC. We find that dynamic latency fluctuations induced by each TCP variant is a more significant differentiator than 'goodput' (<b>useful</b> <b>throughput),</b> and that CUBIC induces far more latency than either H-TCP or NewReno when multiple TCP flows are active concurrently. This potential for 'collateral damage' should influence future efforts to re-design TCP for widespread deployment...|$|E
40|$|Abstract — We {{introduce}} a sustainable system design for image processing applications by prototyping a Sobel edge-detection approach suitable for harsh operating environments. The resulting Reconfigurable Adaptive Redundancy System (RARS) is demonstrated on a Xilinx Virtex- 4 device with the JTAG port {{used to monitor}} the system status using an autonomous supervision process to maintain high system throughput. Evolutionary refurbishment of faulty modules by means of intrinsic Genetic Algorithms (GAs) is also utilized when the system performance declines below a pre-defined threshold. Finally, dynamic partial reconfiguration is utilized to reduce the bitstream transfer time and thus improve {{the performance of the}} GA. This results in an autonomous sustainable approach which supplies <b>useful</b> <b>throughput</b> at a degraded rate even during the repair period...|$|E
40|$|International audienceThis paper {{discusses}} {{the definition of}} simple dimensioning rules for high speed IP access networks carrying data traffic. We notably provide formulas relating capacity, demand and performance allowing dimensioning for a target quality of service {{expressed in terms of}} <b>useful</b> per-flow <b>throughput.</b> These formulas derive from a data traffic model equivalent of the Engset model for telephone access networks. Performance is shown to be largely independent of precise traffic characteristics. The key dimensioning parameter is offered traffic defined as the average data rate a user would generate in the absence of congestion...|$|R
50|$|The {{orthogonality}} criterion only {{holds for}} columns (1 and 2), (1 and 3), (2 and 4) and (3 and 4). Crucially, however, the code is full-rate and still only requires linear processing at the receiver, although decoding is slightly {{more complex than}} for orthogonal STBCs. Results show that this Q-STBC outperforms (in a bit-error rate sense) the fully orthogonal 4-antenna STBC over a good range of signal-to-noise ratios (SNRs). At high SNRs, though (above about 22 dB in this particular case), the increased diversity offered by orthogonal STBCs yields a better BER. Beyond this point, {{the relative merits of}} the schemes have to be considered in terms of <b>useful</b> data <b>throughput.</b>|$|R
40|$|This {{invention}} {{provides a}} eukaryotic cell that stably expresses exogenous, ectopic SID- 1 to confer enhanced polynucleotide, e. g. siRNA or dsRNA, uptake. Thus, in one aspect, this invention provides a eukaryotic cell which stably expresses exogenous SID- 1 polynucleotide and is. The cells stably expressing SID- 1 are particularly <b>useful</b> for high <b>throughput</b> screening of gene activity using RNA interference. Methods for producing {{and using the}} cells also are provided in this application. published_or_final_versio...|$|R
40|$|Spatial Classification Multiple Access (SCMA) is {{introduced}} {{as an example}} of using the radio connectivity among nodes for the dynamic establishment of distributed transmission schedules in wireless multi hop wireless networks. The shared channel is organized into transmission frames whose length in number of time slots is defined solely by the need to avoid hidden-terminal interference, rather than some arbitrary number of time slots related to network size. SCMA is shown to attain feasible transmission schedules within a finite time; and is compared with representative examples of traditional approaches to medium access control (MAC) based on contention, transmission scheduling, and reservations. The results of the analysis show that SCMA attains higher packet delivery ratio, lower average end-to-end delays, and better <b>useful</b> <b>throughput</b> than traditional MAC protocols. ...|$|E
40|$|Despite recent {{advances}} in the microelectronics technology, the implementation of high-throughput decoders for LDPC codes remains a challenging task. This paper aims at summarising the top-down design flow of a decoder for a structured LDPC code compliant with the WWiSE proposal for WLAN. Starting from the system performance analysis with finite-precision arithmetic, a high-throughput architecture is presented as an enhancement of the state-of-the-art solutions, and its VLSI design detailed. The envisaged architecture is also very flexible as it supports several code rates with no significant hardware overhead. The overall decoder, synthesised on 0. 18 μm standard cells CMOS technology, showed remarkable performances: small implementation loss (0. 2 dB down to BER = 10 - 8), low latency (less than 6. 0 μs), high <b>useful</b> <b>throughput</b> (up to 940 Mbps) and low complexity (about 375 Kgates) ...|$|E
40|$|This paper {{presents}} a behavioral fairness-based resource allocation (RA) algorithm for uplink transmission of a multiuser orthogonal frequency division multiplexing system. The aim of fairness- based RA is to boost users' willingness {{to start and}} keep cooperating {{with each other and}} behave well to keep their trust and reputation high. An interesting notion of differentiated streams is also included in the proposed RA, which can fine-tune RA result to transfer excess bandwidth and power from real-time streams to non-real-time streams, i. e. increase <b>useful</b> <b>throughput.</b> In order to reduce the complexity of RA, a set of contiguous subchannels are grouped into a chunk to allocate resource chunk by chunk. The effects of users' contribution and the number of subchannels grouped in a chunk on the average throughput is analyzed and shown by numerical simulation...|$|E
50|$|A {{widespread}} {{practical application}} is emerging in so-called Quantum Dot TVs. It {{must be noted}} however, that these new TV sets are still LCD TVs as far as picture generation is concerned. QDs are used to improve the LED backlighting—light from a blue LED is converted by QDs to relatively pure red and green, so that this combination of blue, green and red light incurs less absorption of unwanted colors by the color filters behind the LCD screen, thereby increasing <b>useful</b> light <b>throughput</b> and providing a better color gamut. The first manufacturer shipping TVs of this kind was Sony in 2013 as Triluminos, Sony's trademark for the technology. At the Consumer Electronics Show 2015, Samsung Electronics, LG Electronics, TCL Corporation and Sony showed QD-enhanced LED-backlighting of LCD TVs.|$|R
40|$|International audienceTransform-based block {{implementation}} of digital filters is <b>useful</b> for high <b>throughput</b> filtering due to inherent parallelism and complexity reduction provided {{by using the}} fast transforms. In basic form, for example the overlap-save implementation, the block digital filter (BDF) is represented by a vector. In this paper, the basic form of block filtering and the optimal design of BDF are described. Therefore, we propose a generalization of the block digital filtering where the BDF is represented by a matrix. This generalized form and its corresponding optimal BDF design are developed. The generalized BDF allows reducing the global distortion of the block filtering...|$|R
40|$|Abstract — Network coding {{has been}} shown to be <b>useful</b> for <b>throughput</b> and {{reliability}} in various network topologies, under a fixed-rate, point-to-multipoint wireless network model. We study the effect of introducing a wireless network model where link capacity depends on the network geometry and the signal to interference and noise ratio. In particular, we compare strategies with and without network coding on a multicast network with and without fading, and on single-user multiple path networks with fading. For the multicast network without fading, we find that the network geometry affects which scheme attains higher throughput. For the case with fading, we compare the throughput-outage probability curves achieved by network coding and repetition schemes. For the multiple path networks, we further consider the case where multiple simultaneous transmissions of identical information signals can be combined at a receiver. We find that the relative performance of the schemes we consider depends on the network geometry, the ratio of signal to noise power, whether multiple simultaneous transmissions can be combined, and the operating point on the throughput-outage probability curve. I...|$|R
40|$|A {{frequency}} division/demand assigned {{multiple access}} (FD/DAMA) network architecture for the first-generation land mobile satellite services is presented. Rationales and technical approaches are described. In this architecture, each mobile subscriber must follow a channel access protocol {{to make a}} service request to the network management center before transmission for either open-end or closed-end services. Open-end service requests will be processed on a blocked call cleared basis, while closed-end requests will be processed on a first-come-first-served basis. Two channel access protocols are investigated, namely, a recently proposed multiple channel collision resolution scheme which provides a significantly higher <b>useful</b> <b>throughput,</b> and the traditional slotted Aloha scheme. The number of channels allocated for either open-end or closed-end services can be adaptively changed according to aggregated traffic requests. Both theoretical and simulation results are presented. Theoretical results have been verified by simulation on the JPL network testbed...|$|E
40|$|As an {{enhancement}} of the state-of-the-art solutions, a high-throughput architecture of a decoder for structured LDPC codes {{is presented in}} this paper. Thanks to the peculiar code definition and to the envisaged architecture featuring memory paging, the decoder is very flexible, {{and the support of}} different code rates is achieved with no significant hardware overhead. A top-down design flow of a real decoder is reported, starting from the analysis of the system performance in finite-precision arithmetic, up to the VLSI implementation details of the elementary modules. The synthesis of the whole decoderon 0. 18 mu m standard cells CMOS technology showed remarkable performances: small implementation loss (0. 2 dB down to BER = 10 (- 8)), low latency (less than 6. 0 mu s), high <b>useful</b> <b>throughput</b> (up to 940 Mbps) and low complexity (about 375 Kgates) ...|$|E
40|$|Abstract — Many {{studies on}} {{measurement}} and characterization of wireless LANs {{have been performed}} recently. Most of these measurements have been conducted from the wired portion of the network based on wired monitoring or SNMP statistics. In this paper we argue that traffic measurements from a wireless vantage point in the network are more appropriate than wired measurements or SNMP statistics, to expose the wireless medium characteristics {{and their impact on}} the traffic patterns. While it is easier to make consistent measurements in the wired part of a network, such measurements can not observe the significant vagaries present in the wireless medium itself. As a consequence constructing an accurate measurement system from a wireless vantage point is important but usually quite difficult due to the noisy wireless channel. In our work we have explored the various issues in implementing such a system to monitor traffic in an IEEE 802. 11 based wireless network. We show the effectiveness of the wireless monitoring by quantitatively comparing it with SNMP and measurements at wired vantage points. We also show the analysis of a typical computer science department network traffic using the wireless monitoring technique. Our analysis reveals rich information about the PHY/MAC layers of the IEEE 802. 11 protocol such as the typical traffic mix of different frame types, their temporal characteristics, correlation with the user activities and the error characteristics of the wireless medium. Moreover, we identify anomalies in the operation of the IEEE 802. 11 MAC protocol. Our results show excessive retransmissions of some management frame types reducing the <b>useful</b> <b>throughput</b> of the wireless network. We also find that some features of the protocol, which were designed to reduce the retransmission errors, are not used. In addition, most of the clients fail to adapt the data rate according to the signal condition between them and the access point, which further reduce the <b>useful</b> <b>throughput.</b> I...|$|E
40|$|Transform-based block {{implementation}} of digital filters is <b>useful</b> for high <b>throughput</b> filtering due to inherent paral-lelism and complexity reduction provided {{by using the}} fast transforms. In basic form, for example the overlap-save im-plementation, the block digital filter (BDF) is represented by a vector. In this paper, the basic form of block filtering and the optimal design of BDF are described. Therefore, we pro-pose a generalization of the block digital filtering where the BDF is represented by a matrix. This generalized form and its corresponding optimal BDF design are developed. The generalized BDF allows reducing the global distortion of the block filtering. Index Terms — Block digital filtering, filter design, overlap-save, filtering distortion...|$|R
40|$|As future {{technologies}} push towards higher clock rates, traditional scheduling {{techniques that}} are based on wake-up and select from an instruction window fail to scale due to their circuit complexities. Speculative instruction schedulers can significantly reduce logic on the critical scheduling path, but can suffer from instruction misscheduling that can result in wasted issue opportunities. Misscheduled instructions can spawn other misscheduled instructions, only to be replayed over again and again until correctly scheduled. These “tornadoes ” in the speculative scheduler are characterized by extremely low <b>useful</b> scheduling <b>throughput</b> and a high volume of wasted issue opportunities. The impact of tornadoes becomes even more severe when using Simultaneous Multithreading. Misschedulings from one thread can occupy {{a significant portion of the}} processor issue bandwidth, effectively starving other threads. In this paper, we propose Zephyr, an architecture that inhibits the formation of tornadoes. Zephyr makes use of existing load latency prediction techniques as well as coarse-grain FIFO queues to buffer instructions before entering scheduling queues. On average, we observe a 23 % improvement in IPC performance, 60 % reduction in hazards, 41 % reduction in occupancy, and 48 % reduction in the number of replays compared with a baseline scheduler...|$|R
40|$|Until recently, {{the design}} of packet {{dropping}} adversary identification protocols that are robust to both benign packet loss and malicious behavior {{has proven to be}} surprisingly elusive. In this paper, we propose a secure and practical packetdropping adversary localization scheme that is robust (in the sense described earlier) and simultaneously achieves a high detection rate and low communication and storage overhead – the three key performance metrics for such protocols in realistic settings. Other recent work optimizes either the detection rate or the communication overhead only. In this paper, we systematically explore the design space of acknowledgment-based protocols to identify a packet dropping adversary on a forwarding path. In particular, we investigate a set of primitive protocols, each exemplifying a design dimension, and examine the underlying tradeoff between the performance metrics. For each primitive protocol, we present both upper and lower performance bounds via theoretical analysis and average-case results via simulations. Additionally, our protocols provide a provable, guaranteed level of <b>useful</b> end-to-end <b>throughput</b> in the presence of multiple adversarial nodes on a given path. We conclude that the proposed PAAI- 1 protocol outperforms other related schemes in terms of practicality. 1...|$|R
40|$|Consider a {{model of}} a {{dynamical}} queue with deterministic arrival and service rates, where the service rate depends on the server utilization history. This proposed queueing model occurs in many practical situations. for example in human-in-the-loop systems where widely accepted empirical laws describe human performance as a function of mental arousal, which increases when the human is working on a task and decreases otherwise. Formal methods for task management in state-dependent dynamical queues are gathering increasing attention to improve the efficiency of such systems. The focus of this research is hence to design maximally stabilizing task release control policies to maximize the <b>useful</b> <b>throughput</b> of such a system. Assuming that the error probability of a server is also related to its state., the <b>useful</b> <b>throughput</b> can be defined as the number of successfully completed tasks per unit time. Monitoring of both service and error rates is particularly typical in the realm of human-in-the-loop and production systems. This research focuses on developing policies to minimize both these penalty measures. For a server with deterministic service rate, the optimal policy is found to be a threshold policy that releases a task to the server only when the server state is {{less than or equal to}} a certain threshold. Assuming homogeneous tasks that bring in the same deterministic amount of work to be done, it can be shown that an appropriate threshold policy is maximally stabilizing and that this threshold value can be uniquely determined. This work is then further extended to the case when the server behaves stochastically and verified using simulation. Finally, a proof-of-concept experiment is proposed and developed to test the feasibility of the proposed theoretical policies in real-world settings. The experiment consisted of completing multiple-choice verbal analogy questions and the results confirm the effect of workload control in improving human performance. by Christine Chiu Hsia Siew. Thesis (S. M.) [...] Massachusetts Institute of Technology, Dept. of Aeronautics and Astronautics, 2011. Cataloged from PDF version of thesis. Includes bibliographical references (p. 108 - 111) ...|$|E
40|$|International audienceIn {{this paper}} a new {{equalizer}} for Single Carrier FDMA (SCFDMA) [1][2] is described. This signal is currently proposed for mobile radio uplink communications in 3 G Long Term Evolution (3 G LTE). In this system, equalization is performed {{thanks to the}} addition of a Cyclic Prefix (CP), as in OFDM system, and pilots regularly inserted for channel estimation which leads to the <b>useful</b> <b>throughput</b> degradation. To reduce this degradation a new structure of blind time domain equalizer, shared in two parts, is proposed in this paper : The filtering itself which is a linear time domain filtering performed before any demodulation device in the receiver side, and the MMSE criterion algorithm performed in the time domain after user selection and demodulation. The results are very convincing both in terms of Mean Square Error and in throughput gain compared to the performances of Zero-Forcing technique(ZF) used in literature...|$|E
40|$|International audienceThis paper {{presents}} the MPC parallel computer and its MPI implementation {{performed at the}} Laboratoire LIP 6 of Univ. Pierre and Marie Curie, Paris. MPC is a low cost and high performance parallel computer using standard PC main-boards as processing nodes connected through the specific FastHSL board to a high speed communication network using HSL 1 Gbits/s serial links, IEEE 1355 compliant. Two Asics are presented: RCUBE which is the HSL network router and PCI-DDC the network controller implementing the Direct Deposit State Less receiver protocol. The software part of the MPC parallel computer consists of 2 zero-copy layers leading to a latency of 5 to 40 /spl mu/s and a throughput of 490 Mbits/s. An efficient MPI implementation based on MPICH is presented and evaluated on an MPC parallel computer. Measures show a latency of 26 /spl mu/s and an <b>useful</b> <b>throughput</b> of 450 Mbits/s This paper presents also a performances study of the MPI implementation on the MPC computer. This reveals Several possible optimizations to reduce the overall MPI transfer latency on the MPC Computer...|$|E
40|$|It is {{suspected}} that apart from tick-borne encephalitis virus several additional European Arboviruses {{such as the}} sandfly borne Toscana virus, sandfly fever Sicilian virus and sandfly fever Naples virus, mosquito-borne Tahyna virus, Inkoo virus, Batai virus and tick-borne Uukuniemi virus cause aseptic meningo-encephalitis or febrile disease in Europe. Currently, the microarray technology is developing rapidly {{and there are many}} efforts to apply it to infectious diseases diagnostics. In order to arrive at an assay system <b>useful</b> for high <b>throughput</b> analysis of samples from aseptic meningo-encephalitis cases the authors developed a combined multiplex ligation-dependent probe amplification and flow-through microarray assay for the detection of European Bunyaviruses. These results show that this combined assay indeed is highly sensitive, and specific for the accurate detection of multiple viruses...|$|R
40|$|Nuclear receptors (NRs) {{are closely}} {{associated}} with various major diseases such as cancer, diabetes, inflammatory disease, and osteoporosis. Therefore, NRs have become a frequent target for drug development. During {{the process of developing}} drugs against these diseases by targeting NRs, we are often facing a problem: Given a NR and chemical compound, can we identify whether they are really in interaction with each other in a cell? To address this problem, a predictor called “iNR-Drug” was developed. In the predictor, the drug compound concerned was formulated by a 256 -D (dimensional) vector derived from its molecular fingerprint, and the NR by a 500 -D vector formed by incorporating its sequential evolution information and physicochemical features into the general form of pseudo amino acid composition, and the prediction engine was operated by the SVM (support vector machine) algorithm. Compared with the existing prediction methods in this area, iNR-Drug not only can yield a higher success rate, but is also featured by a user-friendly web-server established at [URL] which is particularly useful for most experimental scientists to obtain their desired data in a timely manner. It is anticipated that the iNR-Drug server may become a <b>useful</b> high <b>throughput</b> tool for both basic research and drug development, and that the current approach may be easily extended to study the interactions of drug with other targets as well...|$|R
40|$|Post-translational {{modifications}} (PTMs) play crucial {{roles in}} various cell functions and biological processes. Protein hydroxylation is {{one type of}} PTM that usually occurs at the sites of proline and lysine. Given an uncharacterized protein sequence, which site of its Pro (or Lys) can be hydroxylated and which site cannot? This is a challenging problem, not only for in-depth understanding of the hydroxylation mechanism, but also for drug development, because protein hydroxylation is closely relevant to major diseases, such as stomach and lung cancers. With the avalanche of protein sequences generated in the post-genomic age, it is highly desired to develop computational methods to address this problem. In view of this, a new predictor called “iHyd-PseAAC” (identify hydroxylation by pseudo amino acid composition) was proposed by incorporating the dipeptide position-specific propensity into the general form of pseudo amino acid composition. It was demonstrated by rigorous cross-validation tests on stringent benchmark datasets that the new predictor is quite promising and may become a <b>useful</b> high <b>throughput</b> tool in this area. A user-friendly web-server for iHyd-PseAAC is accessible at [URL] Furthermore, {{for the convenience of}} the majority of experimental scientists, a step-by-step guide on how to use the web-server is given. Users can easily obtain their desired results by following these steps without the need of understanding the complicated mathematical equations presented in this paper just for its integrity...|$|R
40|$|Abstract—Good {{performance}} and efficiency, {{in terms of}} high quality of service and resource utilization for example, are important goals in a cloud environment. Through extensive measurements of an n-tier application benchmark (RUBBoS), we show that overall system performance is surprisingly sensitive to appropriate allocation of soft resources (e. g., server thread pool size). Inappropriate soft resource allocation can quickly degrade overall application performance significantly. Concretely, both under-allocation and over-allocation of thread pool can lead to bottlenecks in other resources because of non-trivial dependencies. We have observed some non-obvious phenomena due to these correlated bottlenecks. For instance, the number of threads in the Apache web server can limit the total <b>useful</b> <b>throughput,</b> causing the CPU utilization of the C-JDBC clustering middleware to decrease as the workload increases. We provide a practical iterative solution approach to this challenge through an algorithmic combination of operational queuing laws and measurement data. Our results show that soft resource allocation plays {{a central role in}} the performance scalability of complex systems such as n-tier applications in cloud environments. Keywords-bottleneck, configuration, n-tier, parallel processing, scalability, and soft resourc...|$|E
40|$|This paper {{proposes a}} new semi blind {{equalizer}} structure for Single Carrier FDMA (SC-FDMA) [1][2]. SC-FDMA is currently adopted for mobile radio uplink communications in 3 G Long Term Evolution (3 G LTE). In this system, a Zero-Forcing(ZF) equalization is performed {{thanks to the}} addition of a Cyclic Prefix (CP), as in OFDM system, and pilots regularly inserted for channel estimation which lead to the <b>useful</b> <b>throughput</b> degradation. To reduce this degradation we propose in this paper a new structure of semi blind equalizer in time domain based on [3]. A ZF criterion and a simple Minimum Mean Square Error (MMSE) are combined {{to reduce the number of}} pilots inserted and to outperform the ZF optimum mean square error. This equalizer is shared in two parts: The filtering itself which is a linear time domain filtering performed before any demodulation device in the receiver side, and the criterion algorithm performed in the time domain after user selection and demodulation which is sometimes a ZF criterion and sometimes MMSE criterion. The results are very convincing both in terms of Mean Square Error and in throughput gain compared to the performances of Zero- Forcing technique used in literature...|$|E
30|$|The {{relationship}} between fairness and throughput is more relevant {{for our purposes}} than the {{relationship between}} fairness and probing rate because {{we are interested in}} characterizing a practically <b>useful</b> <b>throughput</b> region. Figure 3 b shows how short-term fairness horizon changes as a function of throughput. At low throughputs, short-term fairness horizon does not depend on d. As the throughput increases, there is a sharp increase in the short-term fairness horizon. The maximum value of the throughput where short-term fairness can be satisfied decreases as d increases. The reason behind this behavior is that the nodes are more dependent on each other in densely connected networks at high throughputs. When the average throughput in the network is low, transmission of a node is rarely prevented by its neighbors. So, nodes behave almost independently and short-term fairness does not depend on the global properties of the system such as the degree. As the probing rates increase, dependence between nodes increases. A node frequently finds the channel busy since at least one of its neighbors is already transmitting. This phenomenon is more apparent in higher degree networks because nodes are more densely connected. So, the nodes in higher-degree topologies starve for a long time at high probing rates that are required for achieving high throughputs.|$|E
40|$|Protocols {{to assess}} kinase {{activity}} generally include radioactive methods, fluorescent polarization {{technology and the}} use of specific antibodies. Here, a simple, effective, non radioactive method to measure kinase activity of immunoprecipitated proteins is described. Cdk 4, a cell cycle dependent enzyme, was immunoprecipitated from whole cell extracts and used in kinase reactions. This system has been developed taking advantage of the kinase-Glo reagent (Promega), based on ATP depletion technology, but with a wider range of applications. The original aim of the commercial kit is the evaluation of kinase activity of highly purified enzymes, while this system enabled the evaluation of native kinases, retrieved by immunoprecipitation. This method was highly homogeneous and did not require any kind of separation or purification as well. Moreover, it was suitable for basic research and may be <b>useful</b> for low-medium <b>throughput</b> pharmaceutical screening of chemical libraries...|$|R
40|$|Ó The Author(s) 2011. This {{article is}} {{published}} with open access at Springerlink. com Abstract It is suspected that apart from tick-borne encephalitis virus several additional European Arboviruses {{such as the}} sandfly borne Toscana virus, sandfly fever Sicilian virus and sandfly fever Naples virus, mosquitoborne Tahyna virus, Inkoo virus, Batai virus and tick-borne Uukuniemi virus cause aseptic meningo-encephalitis or febrile disease in Europe. Currently, the microarray technology is developing rapidly {{and there are many}} efforts to apply it to infectious diseases diagnostics. In order to arrive at an assay system <b>useful</b> for high <b>throughput</b> analysis of samples from aseptic meningo-encephalitis cases the authors developed a combined multiplex ligation-dependent probe amplification and flow-through microarray assay for the detection of European Bunyaviruses. These results show that this combined assay indeed is highly sensitive, and specific for the accurate detection of multiple viruses...|$|R
40|$|Self-assembling {{proteins}} forming amyloid fibrils {{are promising}} {{candidates for the}} fabrication of biomaterials, due to the chemical and mechanical stability of their structures. Among potential applications, their use as platforms for enzyme immobilization is rapidly gathering attention. In this work, we demonstrate that {{the production of the}} enzyme glutathione-S-transferase (GST) fused to the class I hydrophobin Vmh 2 from Pleurotus ostreatus represents an invaluable tool for the development of self-immobilizing enzymes <b>useful</b> for high <b>throughput</b> analyses. The proposed immobilization strategy is versatile since it can be applied, in principle, to every recombinant protein able to refold from Escherichia coli inclusion bodies. A GST based biosensor has been developed to quantify toxic compounds, such as the pesticides molinate and captan, in aqueous environmental samples. The main advantages of this sensor include simplicity and speed of preparation, high sensitivity, reusability, and accuracy...|$|R
