453|178|Public
5|$|Watson uses IBM's DeepQA {{software}} and the Apache UIMA (<b>Unstructured</b> <b>Information</b> Management Architecture) framework. The system {{was written in}} various languages, including Java, C++, and Prolog, and runs on the SUSE Linux Enterprise Server 11 operating system using the Apache Hadoop framework to provide distributed computing.|$|E
25|$|UIMA: The UIMA (<b>Unstructured</b> <b>Information</b> Management Architecture) is a {{component}} framework for analyzing unstructured content such as text, {{audio and video}} – originally developed by IBM.|$|E
2500|$|Prolog {{has been}} used in Watson. Watson uses IBM's DeepQA {{software}} and the Apache UIMA (<b>Unstructured</b> <b>Information</b> Management Architecture) framework. The system was written in various languages, including Java, C++, and Prolog, and runs on the SUSE Linux Enterprise Server 11 operating system using Apache Hadoop framework to provide distributed computing. Prolog is used for pattern matching over natural language parse trees. The developers have stated: [...] "We required a language in which we could conveniently express pattern matching rules over the parse trees and other annotations (such as named entity recognition results), and a technology that could execute these rules very efficiently. We found that Prolog was the ideal choice for the language due to its simplicity and expressiveness." ...|$|E
5000|$|Contextual: They may understand, identify, and extract contextual {{elements}} such as meaning, syntax, time, location, appropriate domain, regulations, user’s profile, process, task and goal. They may draw on multiple sources of information, including both structured and <b>unstructured</b> digital <b>information,</b> as well as sensory inputs (visual, gestural, auditory, or sensor-provided).|$|R
5000|$|Communication can {{be thought}} of as <b>unstructured</b> {{interchange}} of <b>information.</b> A phone call or an IM Chat discussion are examples of this.|$|R
40|$|Processing {{massive amounts}} of <b>unstructured</b> textual <b>information</b> {{is one of the}} most {{critical}} requisites of today’s computers. Unfortunately, selecting, processing, and interpreting textual data is not as explicit as the number crunching tasks computer’s were originally designed to solve. Most, if not all text analysis systems require some amount of data fo...|$|R
50|$|Semaphore Classification Server {{uses the}} model {{structure}} from Semaphore Ontology Editor and auto classifies <b>unstructured</b> <b>information</b> assets by applying metadata tags to the <b>unstructured</b> <b>information.</b>|$|E
50|$|IBM has {{developed}} software, called UIMA for <b>Unstructured</b> <b>Information</b> Management Architecture, {{that can be}} used for analysis of <b>unstructured</b> <b>information.</b> It can perhaps help perform trend analysis across documents, determine the theme and gist of documents, allow fuzzy searches on unstructured documents.|$|E
5000|$|UIMA ( [...] ), {{short for}} <b>Unstructured</b> <b>Information</b> Management Architecture, is an OASIS {{standard}} [...] for content analytics, originally developed at IBM. It provides a component software architecture for the development, discovery, composition, and deployment of multi-modal analytics {{for the analysis}} of <b>unstructured</b> <b>information</b> and integration with search technologies.|$|E
40|$|Human memory at {{its best}} can perform {{astonishing}} feats - the tiniest snippet of information can trigger whole chains of associations, ending at an item long-believed forgotten. While modern information systems excel at systematic manipulation of structured or semi-structured information or even vast repositories of <b>unstructured</b> textual <b>information,</b> they are still far from these capabilities...|$|R
40|$|One of {{the biggest}} {{challenges}} facing financial research and trading organizations is how to well exploit <b>unstructured</b> financial <b>information</b> such as textual announcements. The automatic classification of this type of data poses many challenges for learning systems because the feature vector used to represent a document must capture some of the complex semantics of natural language...|$|R
40|$|Topic {{modeling}} techniques {{help people}} {{to understand what is}} talking about in a corpus, and dramatically improve humans work on academic or business productivity. Although these topic modeling techniques can usually handle topic information represented by word frequencies well, they cannot deal with other <b>unstructured</b> text <b>information</b> such as timestamps, sentiments, or opinions. However, real applications often have needs to explain topics using both structured and <b>unstructured</b> text <b>information</b> together. When texts have their timestamps, it is necessary to identify different topics at different timestamps and how they evolve overtime. When sentiments or opinions are included in texts, it is necessary to identify human’s opinion on certain aspects. Toward these challenges, novel models are explored in this thesis to solve problems of topic evolution and aspect-level sentiment analysis. Two novel models towards topic evolution as follows: (1) The first model, through exploiting social networks in blogsphere, can accurately predict what topic...|$|R
50|$|Vivisimo {{software}} supported both {{structured and}} <b>unstructured</b> <b>information.</b>|$|E
50|$|Allying patent {{analysis}} and informatic tools offers {{an overview of}} the environment through value-added visualisations. As patents contain structured and <b>unstructured</b> <b>information,</b> visualisations fall in two categories. Structured data can be rendered with data mining in macrothematic maps and statistical analysis. <b>Unstructured</b> <b>information</b> can be shown in like clouds, cluster maps and 2D keyword maps.|$|E
50|$|A document-driven DSS manages, retrieves, and manipulates <b>unstructured</b> <b>information</b> in {{a variety}} of {{electronic}} formats.|$|E
50|$|Early case {{assessment}} {{software is}} typically used by attorneys, corporate legal departments, risk managers, forensics teams, IT professionals and independent consultants {{to help them}} analyze <b>unstructured</b> electronically stored <b>information.</b>|$|R
40|$|Context-specific {{document}} recommender systems rely on {{the accurate}} identification of context descriptors from <b>unstructured</b> textual <b>information</b> to identify highly relevant documents. In this paper, we propose two term-weighting measures, “normal distance ” and “adjusted inverse polysemy”, to enable the retrieval of relevant documents with higher precision. We analyze {{the performance of the}} proposed measures and present results with respect to a domain-specific corpus...|$|R
40|$|Our {{approach}} to generate recommendations for similar artists follows a recent tradition of authors tackling the problem not with content-based audio analysis. Following this novel procedure {{we rely on}} the acquisition, filtering and condensing of <b>unstructured</b> text-based <b>information</b> {{that can be found}} in the web. The beauty of this approach lies in the possibility to access so-called cultural metadata that is the agglomeration of several independent-originally subjective- perspectives about music. 1...|$|R
50|$|ECM {{tools and}} {{strategies}} allow {{the management of}} an organization's <b>unstructured</b> <b>information,</b> wherever that information exists.|$|E
5000|$|Medical Records Track - Goal: {{to explore}} methods for searching <b>unstructured</b> <b>information</b> found in patient medical records.|$|E
5000|$|<b>Un{{structure}}d</b> <b>information</b> {{might have}} some structure (semi-structured) or even be highly structured but {{in ways that are}} unanticipated or unannounced.|$|E
40|$|Positioning systems, {{combined}} with inexpensive communication technologies, open interesting possibilities to implement real-time applications that monitor vehicles and support decision making. This paper first discusses basic requirements for proactive real-time monitoring applications. Then, it describes how to structure and geo-reference <b>unstructured</b> text <b>information</b> {{available on the}} Internet, {{with a focus on}} road conditions change and using available geocoding services. Lastly, the paper outlines an application that monitors a fleet of trucks and incorporates proactive features...|$|R
40|$|The MyView project aims at the {{integration}} of both structured and <b>unstructured</b> bibliographic <b>information</b> from a diversity of heterogeneous Internet repositories like electronic journals and traditional libraries. Based on the user's individual information need MyView maintains a personalized warehouse for bibliographic data in a unified scheme, which is locally available for browsing, ad hoc queries and analysis. This paper gives {{an overview of the}} project, emphasizes research issues and describes {{the current state of the}} implementation...|$|R
30|$|It {{is obvious}} that the {{generated}} RDF in [21] is based on 1) the structural relationships among object resources, and 2) the structural metadata in[*]<[*]meta[*]>[*]tags that can be easily acquired from the web document. However, the <b>unstructured</b> semantic <b>information,</b> which might be more valuable for users, cannot be extracted and expressed in the RDF diagram by such approach. The query on[*]<[*]meta[*]>[*]tag information such as “Finding all documents with UTF- 8 character encoding” might not be helpful for users with question answering purposes.|$|R
50|$|Unstructured data (or <b>unstructured</b> <b>{{information}})</b> {{refers to}} information that either {{does not have}} a pre-defined data model or is not organized in a pre-defined manner. <b>Unstructured</b> <b>information</b> is typically text-heavy, but may contain data such as dates, numbers, and facts as well. This results in irregularities and ambiguities that make it difficult to understand using traditional programs as compared to data stored in fielded form in databases or annotated (semantically tagged) in documents.|$|E
50|$|Microsoft - Microsoft in {{partnership}} with Hortonworks offers the HDInsights tool {{which is used to}} analyse <b>unstructured</b> <b>information</b> provided by data aggregators.|$|E
5000|$|UIMA: The UIMA (<b>Unstructured</b> <b>Information</b> Management Architecture) is a {{component}} framework for analyzing unstructured content such as text, {{audio and video}} - originally developed by IBM.|$|E
40|$|Abstract. The paper {{discusses}} {{problems in}} automated processing and classification of <b>unstructured</b> text <b>information</b> and suggests {{a new approach}} based on the multi-agent technology. The approach was applied for one of UK insurance companies to analyze 25000 documents related to car insurance domain, leading to development of a system, capable to analyze documents, classify them into hierarchical semantic structure and build a template, which includes suitable parts of all similar documents. The paper describes the system, presents testing results and discusses perspectives...|$|R
40|$|Abstract—Positioning systems, {{combined}} with inexpensive communication technologies, open interesting possibilities to implement real-time applications that monitor vehicles and support decision making. This paper first discusses basic requirements for proactive real-time monitoring applications. Then, it describes how to structure and geo-reference <b>unstructured</b> text <b>information</b> {{available on the}} Internet, {{with a focus on}} road conditions change and using available geocoding services. Lastly, the paper outlines an application that monitors a fleet of trucks and incorporates proactive features. Keywords—vehicle monitoring; proactive behavior; Twitter analysis. I...|$|R
40|$|The thesis {{presented}} in this paper tackles selected issues in <b>unstructured</b> peer-to-peer <b>information</b> retrieval (P 2 PIR) systems, using world knowledge for solving P 2 PIR problems. A first part uses so-called reference corpora for estimating global term weights such as IDF instead of sampling them from the distributed collection. A second part of the work will be dedicated to the question of query routing in unstructured P 2 PIR systems using peer resource descriptions and world knowledge for query expansion...|$|R
50|$|One {{application}} of semantic intelligence is {{the management of}} <b>unstructured</b> <b>information,</b> leveraging semantic technology. These applications tackle R&D, sales, marketing and security for activities that include Knowledge Management, Customer Care and Corporate Intelligence.|$|E
50|$|Similar to the Chandler PIM, the Haystack system unifies {{handling}} {{different types}} of <b>unstructured</b> <b>information.</b> This information has a common representation in RDF that is presented to users in a configurable human-readable way.|$|E
50|$|Compound term {{processing}} allows {{statistical information}} retrieval applications to perform matching using multi-word concepts. This {{can improve the}} quality of search results and also allows <b>unstructured</b> <b>information</b> to be automatically classified with semantic metadata.|$|E
40|$|A {{large amount}} of {{structured}} information is buried in <b>unstructured</b> text. <b>Information</b> extraction systems can extract structured relations from the documents and enable sophisticated, SQL-like queries over <b>unstructured</b> text. <b>Information</b> extraction systems are not perfect and their output has imperfect precision and recall (i. e., contains spurious tuples and misses good tuples). Typically, an extraction system has a set of parameters {{that can be used}} as “knobs ” to tune the system to be either precision- or recall-oriented. Furthermore, the choice of documents processed by the extraction system also affects the quality of the extracted relation. So far, estimating the output quality of an information extraction task has been an ad hoc procedure, based mainly on heuristics. In this article, we show how to use Receiver Operating Characteristic (ROC) curves to estimate the extraction quality in a statistically robust way and show how to use ROC analysis to select the extraction parameters in a principled manner. Furthermore, we present analytic models that reveal how different document retrieval strategies affect the quality of the extracted relation. Finally, we present our maximum likelihood approach for estimating, on the fly, the parameters required by our analytic models to predict the runtime and the output quality of each execution plan. Our experimental evaluation demonstrates that our optimization approach predicts accurately the output quality and selects the fastest execution plan that satisfies the output quality restrictions...|$|R
40|$|XML {{is being}} {{incorporated}} into the foundation of E-business data applications. This paper addresses {{the problem of the}} freeform information that stored in any organization and how XML with using this new approach will make the operation of the search very efficient and time consuming. This paper introduces new solution and methodology that has been developed to capture and manage such <b>unstructured</b> freeform <b>information</b> (multi information) depending on the use of XML schema technologies, neural network idea and object oriented relational database, in order to provide a practical solution for efficiently management multi freeform information system...|$|R
40|$|The {{availability}} of map interfaces and location-aware devices makes a growing amount of <b>unstructured,</b> geo-referenced <b>information</b> {{available on the}} Web. In particular, over twelve million geo-referenced photos are now available on Flickr, a popular photo-sharing website. We show a method to analyze the Flickr data and generate aggregate knowledge {{in the form of}} “representative tags ” for arbitrary areas in the world. We display these tags on a map interface in an interactive web application along with images associated with each tag. We then use the implicit feedback of the aggregate user interactions with the tags and images to learn which images best describe the area shown on the map...|$|R
