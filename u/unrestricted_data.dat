21|38|Public
40|$|Conference PaperThe {{problem of}} speaker {{detection}} in audio databases is {{addressed in this}} paper. Gaussian mixture modeling is used to build target speaker and background models. A detection algorithm based on a likelihood ratio calculation is applied to estimate target speaker segments. Evaluation procedures are defined in detail for this task. Results are given for different subsets of the HUB 4 broadcast news database. For one target speaker, with the data restricted to high quality speech segments, the segment miss rate is approximately 7 %. For <b>unrestricted</b> <b>data,</b> the segment miss rate is approximately 27 %. In both cases the segment false alarm rate is 4 or 5 per hour. For two target speakers with <b>unrestricted</b> <b>data,</b> the segment miss rate is approximately 63 % with about 27 segment false alarms per hour. The decrease in performance for two target speakers is largely associated with short speech segments in the two target speaker test data which are undetectable in the current configuration of the detection algorithm...|$|E
40|$|This {{paper is}} drawn {{from the use of}} data envelopment {{analysis}} (DEA) in helping a Portuguese bank to manage the performance of its branches. The bank wanted to set targets for the branches on such variables as growth in number of clients, growth in funds deposited and so on. Such variables can take positive and negative values but apart from some exceptions, traditional DEA models have hitherto been restricted to non-negative data. We report on the development of a model to handle <b>unrestricted</b> <b>data</b> in a DEA framework and illustrate the use of this model on data from the bank concerned...|$|E
40|$|This paper reports an {{overview}} of the evaluation campaign results of the IWSLT 2005 workshop 1. The BTEC corpus, which consists of typical travel domain phrases, was used. Data for the five language pairs Arabic/Chinese/Japanese/Korean to English and English to Chinese was prepared. To study how much the amount of the training data and how much different training and decoding approaches contribute to the performance, a supplied data and an <b>unrestricted</b> <b>data</b> track were introduced. In addition, translation results were evaluated not only for text input but also speech recognition output. 19 systems from 17 organizations participated in the evaluation. All machine translation results were evaluated using automatic evaluation metrics. The most popular track, translating text form Chinese to English, was graded by 3 humans in terms of Fluency, Adequacy and Meaning Maintenance. The correlation between automatic evaluation metrics an...|$|E
25|$|The DHS Program {{works to}} provide survey data for program managers, health care providers, policymakers, country leaders, researchers, {{members of the}} media, and others who can act to improve public health. The DHS Program {{distributes}} <b>unrestricted</b> survey <b>data</b> files for legitimate academic research at no cost.|$|R
50|$|They {{suggested}} that reanalyses {{should therefore be}} subject to the same regulations as sponsor analyses, such as registering analysis plans. They argued against completely <b>unrestricted</b> access to <b>data,</b> but in favour of broader access. AllTrials is not calling for completely unrestricted access to raw data, so the scope of disagreements is limited to what restrictions should be in place.|$|R
40|$|Periodic autoregressions are characterised by autoregressive {{structures}} that {{vary with the}} season. If a time series is periodically integrated, one needs a seasonally varying differencing filter to remove the stochastic trend. When the periodic regression model contains constants and trends with <b>unrestricted</b> parameters, the <b>data</b> can show diverging seasonal deterministic trends. In this paper we derive explicit expressions for parameter restrictions that result in common deterministic trends under periodic trend stationarity and periodic integration. ...|$|R
40|$|ALEPH: an EBMT {{system based}} on the {{preservation}} of proportional analogies between sentences across languages We designed, implemented and assessed ALEPH, a pure example-based machine translation system. It strictly does not make any use of variables, templates or training, {{does not have any}} explicit transfer component, and does not require any preprocessing of the aligned examples. It relies on a specific operation: the resolution of analogical equations, that neutralizes translation divergences in an elegant way. Starting only from theoretical results, a system that is state-of-the-art with the top IWSLT 2004 results could be built in six month time. Evaluated on the <b>Unrestricted</b> <b>Data</b> track of IWSLT 2004, our system achieved second place in CE, and third place in JE (with best BLEU for this latter track). For this year’s evaluation campaign, the features of the system allowed its immediate application to all possible language pairs in the C-STAR tracks. 1...|$|E
40|$|Introduction The ALEPH machine {{translation}} system (Lep- age and Denoual, 2005) is an example-based {{machine translation}} system that strictly {{does not make}} any use of variables, templates or patterns, {{does not have any}} explicit transfer component, and does not require any training or preprocessing of the aligned examples, a knowledge that is, of course, indispensable. Its characteristic is that it relies solely on the use of a linguistically justified operation, proportional analogy. 2 Comparison with other systems We assessed the ALEPH system using the IWSLT 2004 tasks in both Japanese-English and Chinese-English directions. As we used a bicorpus of 160, 000 examples (the C-STAR Basic Traveler's Expressions Corpus or C-STAR BTEC (Takezawa et al., 2002)) our results should be compared with those of the <b>Unrestricted</b> <b>Data</b> track reported in the proceedings of the evaluation workshop (Akiba et al., 2004). In this track, no restrictions were imposed on linguistic resources. As for tools, t...|$|E
40|$|In the {{framework}} of the IHP-IV project H- 2 - 2 three workshops have been conducted bringing together scientists of different countries, disciplines and attitudes: (i) STORM surges, river flow and combined effects; STORM 91 (Hamburg, Germany, 1991); (ii) sea level changes and their consequences for hydrology and water management; SEACHANGE 93 (Noordwijkerhout, The Netherlands, 1993); (iii) water related problems in low lying coastal areas; HYDROCOAST 95 (Bangkok, Thailand, 1995). Workshop contributions are compiled in the proceedings and in state of art reports. As a result proposals are given to improve hazard reduction: An international centre of coastal resources and phenomena should be established in South East Asia, and training activities should be organized. For a world wide network of institutes working in this field comparable data sets and <b>unrestricted</b> <b>data</b> exchange are required. The cooperation between the UNESCO Division of Water Sciences and the IOC should be strengthened. (WEN) SIGLEAvailable from TIB Hannover: RO 2673 (8) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekDEGerman...|$|E
40|$|Abstract — Fluid simulation, {{proposed}} here, runs on an <b>unrestricted</b> octree <b>data</b> structure {{which uses}} mesh refinement techniques to enable {{higher level of}} detail and solves Navier Stokes equations for multiple fluids. We also show our solution for floating objects and unmoving obstacles by setting up the boundry conditions and solving the velocity of objects using a simple rigid body solver. We propose technique for discretizing the Poisson equation on octree grid. The resulting linear system is symmetric positive definite enabling the use of fast solution methods. The standard approximation to the Poisson equation on an octree grid results in a non-symmetric linear system which is more computationally challenging. To solve the advection of fluid we use the semi-Lagrangian characteristic tracing technique. To track the fluid surface we use Volume of Fluid (VOF) method. I...|$|R
40|$|We {{apply the}} method of {{constrained}} asset share estimation (CASE) to test the mean-variance efficiency (MVE) of the stock market. This method allows conditional expected returns to vary in relatively <b>unrestricted</b> ways. The <b>data</b> estimate reasonably the price of risk, and, in some cases, the MVE model is valuable in explaining expected equity returns. Unlike with most tests of MVE. we can put an explicit interpretation on the alternative hypothesis [...] a general linear Tobin portfolio choice model. We reject the restrictions implied by MVE. ...|$|R
40|$|We {{present a}} method for {{simulating}} water and smoke on an <b>unrestricted</b> octree <b>data</b> structure exploiting mesh refinement techniques to capture the small scale visual detail. We propose a new technique for discretizing the Poisson equation on this octree grid. The resulting linear system is symmetric positive definite enabling the use of fast solution methods such as preconditioned conjugate gradients, whereas the standard approximation to the Poisson equation on an octree grid results in a non-symmetric linear system which is more computationally challenging to invert. The semi-Lagrangian characteristic tracing technique is used to advect the velocity, smoke density, and even the level set making implementation on an octree straightforward. In the case of smoke, we have multiple refinement criteria including object boundaries, optical depth, and vorticity concentration. In the case of water, we refine near the interface {{as determined by the}} zero isocontour of the level set function...|$|R
40|$|International audienceLinguistic {{description}} and language modelling {{need to be}} formally sound and complete while still being supported by data. We present a linguistic framework that bridges such formal and descriptive requirements, based on the representation of syntactic information by means of local properties. This approach, called Property Grammars, provides a formal basis for the description of specific characteristics as well as entire constructions. In contrast with other formalisms, all information is represented {{at the same level}} (no property playing a more important role than another) and independently (any property being evaluable separately). As a consequence, a syntactic description, instead of a complete hierarchical structure (typically a tree), is a set of multiple relations between words. This characteristic is crucial when describing <b>unrestricted</b> <b>data,</b> including spoken language. We show in this paper how local properties can implement any kind of syntactic information and constitute a formal framework for the representation of constructions (seen as a set of interacting properties). The Property Grammars approach thus offers the possibility to integrate the description of local phenomena into a general formal framework...|$|E
40|$|It is {{pertinent}} {{to note that}} trade in agricultural commodities still dominates the export scene of the African economies. Moreover, the agricultural sector constitutes {{a significant part of}} the whole economy and employs a considerable proportion of the labour force. Furthermore, increasing agricultural exports is an intermediate step toward restoring external balance of payments equilibrium, a central component of most economic structural adjustment programmes (ESAP) initiated in the 1980 s and 1990 s. Against this backdrop, the major objective of this research is to increase our understanding of the specification and estimation of agricultural commodity trade models as well as to provide veritable tools for trade policy analysis. More specifically, the study aims at building a set of dynamic, theory-based econometric models which are able to capture both short-run and long-run effects of income and price changes, and which can be used for prediction and policy simulation under alternative assumed conditions In this study, the methodology to be adopted will be a relatively <b>unrestricted,</b> <b>data</b> determined, econometric modeling approach based on the vector autoregressive process, which is very popular for modeling multiple time series. Further, econometric models will be constructed for SSA‘s agricultura...|$|E
40|$|We prove global {{stability}} of Minkowski {{space for the}} Einstein vacuum equations in harmonic (wave) coordinate gauge for the set of restricted data coinciding with Schwartzschild solution {{in the neighborhood of}} space-like infinity. The result contradicts previous beliefs that wave coordinates are "unstable in the large" and provides an alternative approach to the stability problem originally solved (for <b>unrestricted</b> <b>data,</b> in a different gauge and with a precise description of the asymptotic behavior at null infinity) by D. Christodoulou and S. Klainerman. Using the wave coordinate gauge we recast the Einstein equations as a system of quasilinear wave equations and, in absence of the classical null condition, establish a small data global existence result. In our previous work we introduced the notion of a eak nul condition and showed that the Einstein equations in wave coordinates satisfy this condition. The result of this paper relies on this observation and combines it with the vector field method based on the symmetries of the standard Minkowski space. In a forthcoming paper we will address the question of {{stability of}} Minkowski space for the Einstein vacuum equations in wave coordinates for all "small" asymptotically flat data and the case of the Einstein equations coupled to a scalar field. Comment: 64 pages, no figures, submitted, CMP Minor changes and corrections adde...|$|E
50|$|PASDA is an {{open data}} portal—meaning that is {{provides}} open, free, <b>unrestricted</b> access to <b>data</b> in multiple formats. PASDA provides data storage, data access and retrieval, and metadata services free of charge to its data providers because access to data drives economic development, conservation efforts, and collaboration. The data made available through PASDA is provided by data partners to encourage the widespread sharing of geospatial data, eliminate the creation of redundant data sets, and to further build an inventory (through the development and hosting of metadata) of available data relevant to the Commonwealth.|$|R
50|$|The Conservation Geoportal was {{intended}} to support the principles and objectives of the Conservation Commons. At its simplest, it encourages organizations and individuals alike to ensure open access to data, information, expertise and knowledge related to the conservation of biodiversity. The Conservation Commons is {{the expression of a}} cooperative effort of non-governmental organizations, international and multi-lateral organizations, governments, academia, and the private sector, to improve open access to and <b>unrestricted</b> use of, <b>data,</b> information and knowledge related to the conservation of biodiversity with the belief that this will contribute to improving conservation outcomes.|$|R
40|$|In this paper, {{we present}} APOLN (Analizador Parcial de Oraciones en Lenguaje Natural) : a partial parser of {{unrestricted}} natural language sentences based on finite -state techniques. Partial parsing {{has been used}} in several applications: syntactic parsing of <b>unrestricted</b> texts, <b>data</b> extraction systems, machine translation, solving the attachment ambiguity, speech recognition systems, text summarization, etc. The main attractiveness of partial parsing is that is able to handle unrestricted sentences, that contain lexical errors or that present constructions not accepted by the defined grammar. Partial parsing is an alternative to the definition of wide coverage grammars whose definition is an expensive and complex task and that present well-known problems such as overgeneration, undergeneration and ambiguity. We present APOLN as a tool {{that can be used to}} construct syntactically annotated corpora from lexically tagged corpora. We also present the results (precision and recall rates) of applying APOLN on unrestricted Spanish corpora, and how tagging errors influence the performance of the parser. 2...|$|R
40|$|With {{convolutional}} {{neural networks}} revolutionizing the computer vision field {{it is important}} to extend the capabilities of neural-based systems to dynamic and <b>unrestricted</b> <b>data</b> like graphs. Doing so not only expands the applications of such systems, but also provide more insight into improvements to neural-based systems. Currently most implementations of graph neural networks are based on vertex filtering on fixed adjacency matrices. Although important for a lot of applications, vertex filtering restricts the applications to vertex focused graphs and cannot be efficiently extended to edge focused graphs like social networks. Applications of current systems are mostly limited to images and document references. Beyond the graph applications, this work also explored the usage of convolutional neural networks for intelligent character recognition in a novel way. Most systems define Intelligent Character Recognition as either a recurrent classification problem or image classification. This achieves great performance in a limited environment but does not generalize well on real world applications. This work defines intelligent Character Recognition as a segmentation problem which we show to provide many benefits. The goal of this work was to explore alternatives to current graph neural networks implementations as well as exploring new applications of such system. This work also focused on improving Intelligent Character Recognition techniques on isolated words using deep learning techniques. Due to the contrast between these to contributions this documents was divided into Part I focusing on the graph work, and Part II focusing on the intelligent character recognition work...|$|E
40|$|Performance {{and range}} {{requirements}} for next-generation rotary-wing aircraft have sparked renewed {{interests in the}} coaxial rotor configuration, augmented with lift and/or thrust compounding. Often, thrust augmentation is provided {{in the form of}} a propeller or jet engine to counteract the airframe and rotor drag in high speed forward flight. A notional X 2 TD coaxial compound configuration has been chosen to perform numerical simulations in forward flight with CFD-CSD coupling. The delta loose coupling method is used to couple the CFD and CSD models. Using the CFD results to correct the reduced order aerodynamics in this loose coupling framework will drive toward a deeper understanding of rotor-rotor and rotor-fuselage interactions in the forward flight regime. Using <b>unrestricted</b> <b>data</b> of the X 2 TD flight test program the in-house CSD code (PRASADUM) was validated against both CAMRAD II and flight test data results. Helios, using both Overflow and NSU 3 D as near-body solvers was used as the CFD solver for the CFD-CSD coupling framework. The CFD-CSD coupling framework was used for several key flight conditions of the X 2 TD, namely 55, 100, and 150 knots. A comparison study at both 55 and 150 knots was conducted between an isolated coaxial rotor system case, and 3 other cases incorporating three different fuselage models to the CFD analysis: a simple fuselage body, a complex fuselage body containing horizontal and vertical stabilizers, and lastly the complex fuselage body with the added inclusion of the rotor mast...|$|E
40|$|An IES Working Paper on {{the design}} of <b>unrestricted</b> <b>data</b> unit. The {{maturing}} spatial information sciences have led to greater free flow of spatial information. More than ever before, scientists within and between scientific disciplines appreciate the need to exchange environmental information to avert irreversible environmental disasters, hence the development of environmental information systems. Existing ecological classification schema are seen as an impediment to environmental data exchange between scientific disciplines. However, this paper will show that the perceived different ecological classification systems, though different, are not as incompatible as they might appear to be. It will be shown that the perceived problem of classification schema incompatibility is one of object definition and data structuring and lack of adequately structured meta-data. The advent of the Internet and it's associated technologies has led to immense possibilities for data exchange. Coupled with those opportunities are perhaps equally frightening possibilities of the use of data of undefined quality obtained from remote databases lacking adequate documentation on the data -sets. The development of an elegant data model based on the concept of object hierarchies and their associated behavioral attributes enables the capture, storage, and retrieval of data objects in a way that enables the aggregation of the objects into several ecological classification schema. Such a framework would facilitate the exchange of data between scientists and nations with seemingly different ecological classification systems. By carefully capturing meta-data incorporating it, and propagating it through the different hierarchical schema via the development of supporting logical model constructs, it is hoped that the data model will promote the informed multiple use of data from differently focused ecological classification and aggregation schemes from distributed sources...|$|E
50|$|The Conservation Commons is the {{expression}} of a cooperative effort of non-governmental organizations, international and multi-lateral organizations, governments, academia, and the private sector, to improve open access to and <b>unrestricted</b> use of, <b>data,</b> information and knowledge related to the conservation of biodiversity, with the belief that this will contribute to improving conservation outcomes. At its simplest, it encourages organizations and individuals alike to ensure open access to data, information, expertise and knowledge related to the conservation of biodiversity. The goal of the Conservation Commons is to promote conscious, effective, and equitable sharing of knowledge resources to advance conservation.|$|R
40|$|Abstract—Parallel memory modules {{can be used}} to {{increase}} memory bandwidth and feed a processor with only necessary data. Arbitrary stride access capability with interleaved memories is described in previous research where the skewing scheme is changed at run time according to the currently used stride. This paper presents the improved schemes which are adapted to parallel memories. The proposed novel parallel memory architecture allows conflict free accesses with all the constant strides which has not been possible in prior application specific parallel memories. Moreover, the possible access locations are <b>unrestricted</b> and the <b>data</b> patterns have equal amount of accessed data elements as the number of memory modules. The complexity is evaluated with resource counts. I...|$|R
40|$|Abstract This paper reconstructs multivariate {{functions}} from scattered data {{by a new}} multiscale technique. The reconstruction uses standard {{methods of}} interpolation by positive definite reproducing kernels in Hilbert spaces. But it adopts techniques from wavelet theory and shift–invariant spaces to construct {{a new class of}} kernels as multiscale superpositions of shifts and scales of a single compactly supported function ϕ. This means that the advantages of scaled regular grids are used to construct the kernels, while the advantages of <b>unrestricted</b> scattered <b>data</b> interpolation are maintained after the kernels are constructed. Using such a multiscale kernel, the reconstruction method interpolates at given scattered data. No manipulations of the data (e. g. thinning or separation into subsets of certain scales) are needed. Then, the multiscale structure of the kernel allows to represent the interpolant on regular grids on all scales involved, with cheap evaluation due to the compact support of the function ϕ, and with a recursive evaluation technique if ϕ is chosen to be refinable. There also is a wavelet–like data reduction effect, if a suitable thresholding strategy is applied to the coefficients of the interpolant when represented over a scaled grid. Various numerical examples are presented, illustrating the multiresolution and data compression effects. ...|$|R
40|$|A Doctoral Thesis. Submitted in partial {{fulfillment}} of the requirements for the award of Doctor of Philosophy of Loughborough University. This thesis examines banking efficiency and the productivity of thirteen transition Central and Eastern European banking systems during 1998 - 2003 using Data Envelopment Analysis (DEA). It proposes a non-parametric methodology for non-radial Russell output efficiency measure of banking firms, incorporating risk as an undesirable output. In addition, the proposed efficiency measure handles <b>unrestricted</b> <b>data,</b> i. e. both positive and negative. The Luenberger productivity index is suggested, which is applicable to technology where the desirable and undesirable outputs are jointly produced, and are possibly negative. Furthermore, the thesis addresses the main issue {{in the literature on}} banking performance measurement, which concerns the lack of consistency in the conceptual and theoretical considerations in describing the banking production process. Consequently, a metaanalysis tool, to examine the choice of inputs and outputs definitions in the banking efficiency literature, is suggested. In addition, the performance measures are estimated using three alternative definitions of the banking production process focusing on the risk and environmental dimensions of bank efficiency and productivity, with further comparative analysis using bootstrapping and kernel density techniques. Overall, the empirical results suggest that in Central and Eastern Europe Czech, Hungarian and Polish banks were the most technical efficient banks and the banking risk was mainly affected by external environmental factors during the analyzed period. Productivity analysis implies that the main driver of productivity change in the Central and Eastern European banks is the technological improvement. As meta-analysis revealed, the choice of particular approach of describing the banking production process is determined not by the availability of particular input or output variable information but the concepts of researcher's theoretical considerations. Statistical tests and density analysis indicate that efficiency scores, returns parameters and productivity indexes are sensitive to the choice of particular approaches...|$|E
40|$|Background. The Strategic Advisory Group of Experts on Immunization (SAGE) has {{recommended}} introduc-tion {{of at least}} 1 dose of inactivated poliovirus vaccine (IPV) at ≥ 14 weeks of age through the routine immunization program in countries currently not using IPV. Methods. We analyzed all available <b>unrestricted</b> <b>data</b> obtained from the Demographic and Health Surveys since 2005 in sub-Saharan Africa (31 countries) and in South and Southeast Asia (9 countries) to determine coverage of the following injectable vaccines delivered through the routine immunization schedule: diphtheria-tetanus-pertussis vaccine dose 1 (DTP 1), DTP 2, DTP 3, and measles vaccine. Coverage with these vaccines {{was used as a}} proxy mea-sure of likely 1 - and 2 -dose IPV coverage. Results. Coverage with 1 dose of IPV is expected to be lowest when offered with DTP 3 (median coverage, 73 %) and highest when offered with DTP 1 (median coverage, 90 %). The median DTP 1 -DTP 3 drop-out rate was 14 %, which equates to an additional 12 million children not receiving IPV if IPV is offered with DTP 3, rather than with DTP 1. An increased geographical clustering of children who have not received IPV is expected in sub-Saharan Africa and Asia if IPV is offered with DTP 3, rather than with DTP 1. Coverage with 2 doses of IPV is expected to be lowest if IPV is administered with DTP 3 and measles vaccine (69 %) and highest if administered with DTP 1 and DTP 2 (84 %). Conclusions. Coverage with 1 dose of IPV is expected to be lowest if it is administered at the DTP 3 visit. At present, there is insufficient evidence to determine whether the SAGE-recommended IPV schedule for the polio endgame would maximize population immunity to type 2 poliovirus...|$|E
40|$|This thesis {{examines}} banking {{efficiency and}} {{the productivity of}} thirteen transition Central and Eastern European banking systems during 1998 - 2003 using Data Envelopment Analysis (DEA). It proposes a non-parametric methodology for non-radial Russell output efficiency measure of banking firms, incorporating risk as an undesirable output. In addition, the proposed efficiency measure handles <b>unrestricted</b> <b>data,</b> i. e. both positive and negative. The Luenberger productivity index is suggested, which is applicable to technology where the desirable and undesirable outputs are jointly produced, and are possibly negative. Furthermore, the thesis addresses the main issue {{in the literature on}} banking performance measurement, which concerns the lack of consistency in the conceptual and theoretical considerations in describing the banking production process. Consequently, a metaanalysis tool, to examine the choice of inputs and outputs definitions in the banking efficiency literature, is suggested. In addition, the performance measures are estimated using three alternative definitions of the banking production process focusing on the risk and environmental dimensions of bank efficiency and productivity, with further comparative analysis using bootstrapping and kernel density techniques. Overall, the empirical results suggest that in Central and Eastern Europe Czech, Hungarian and Polish banks were the most technical efficient banks and the banking risk was mainly affected by external environmental factors during the analyzed period. Productivity analysis implies that the main driver of productivity change in the Central and Eastern European banks is the technological improvement. As meta-analysis revealed, the choice of particular approach of describing the banking production process is determined not by the availability of particular input or output variable information but the concepts of researcher's theoretical considerations. Statistical tests and density analysis indicate that efficiency scores, returns parameters and productivity indexes are sensitive to the choice of particular approaches. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|E
40|$|Smartphones {{have the}} {{capability}} of enhancing {{many aspects of the}} continuum of surgical care by providing an efficient means of multimedia communication among surgeons and healthcare personnel. The ability for mobile Internet and email access, along with features such as built-in cameras and video calling, allow surgeons to rapidly access, send and receive patient information without being restricted by issues of connectivity. Smartphones create an <b>unrestricted</b> network of <b>data</b> sharing, improving the flexibility of patient consultation, timeliness of preoperative preparation, efficiency of post-operative care and the effectiveness of a surgical team. Furthermore, smartphones provide mobile access to a multitude of surgical resources to bolster continued surgical education. This article presents a review of the current literature on the utilizations of smartphones in surgery, discusses their benefits and limitations, and addresses the possibility of incorporating smartphones into the protocol of surgical care...|$|R
50|$|One of the {{critical}} features of the Argo model is that of global and <b>unrestricted</b> access to <b>data</b> in near real-time. When a float transmits a profile it is quickly converted to a format that can be inserted on the GTS (Global Telecommunications System). The GTS is operated by the World Meteorological Organisation, or WMO, specifically {{for the purpose of}} sharing data needed for weather forecasting. Thus all nations who are members of the WMO receive all Argo profiles within a few hours of the acquisition of the profile. Data are also made available through ftp and WWW access via two Argo Global Data Centres (or GDACs), one in France and one in the US. About 90% of all profiles acquired are made available to global access within 24 hours, with the remaining profiles becoming available soon thereafter.|$|R
40|$|This paper {{describes}} a concurrent programming language, Aldwych, and its underlying model. The {{basis of the}} language is that the concurrency is provided by shared single-assignment variables, {{each of which is}} constrained by the terms of the language to have exactly one writer though it may have several readers. Through derived forms, this simple model provides the basis of a flexible concurrent objectoriented language. Introduction Underneath various concurrent programming models lie differences in how processes share <b>data.</b> <b>Unrestricted</b> read/write access to shared store leads to the classic problems of concurrency and mechanisms such as semaphores for mutual exclusion, discussion of which still dominates concurrency as conventionally taught [Ma & Kr 99]. Shared rewriteable store remains an essential part of concurrency in Java, with the need for its synchronized methods though access to store is restricted by object-orientation. The popular Linda model [Ca & Ge 89] has no direct [...] ...|$|R
40|$|We {{present the}} far-UV {{spectrum}} of the quasar HS 1700 + 6416 taken with FUSE. This QSO provides the second line of sight with the HeII absorption resolved into a Ly alpha forest structure. Since HS 1700 + 6416 is slightly less redshifted (z= 2. 72) than HE 2347 - 4342, we only probe the post-reionization phase of HeII, seen {{in the evolution of}} the HeII opacity, which is consistent with a simple power law. The HeII/HI ratio eta is estimated using a line profile-fitting procedure and an apparent optical depth approach, respectively. The expected metal line absorption in the far-UV is taken into account as well as molecular absorption of galactic H_ 2. About 27 % of the eta values are affected by metal line absorption. In order to investigate the applicability of the analysis methods, we create simple artificial spectra based on the statistical properties of the HI Ly alpha forest. The analysis of the artificial data demonstrates that the apparent optical depth method as well as the line profile-fitting procedure lead to confident results for restricted data samples only (12. 0 < log N(HI) < 13. 0). The reasons are saturation in the case of the apparent optical depth and thermal line widths in the case of the profile fits. Furthermore, applying the methods to the <b>unrestricted</b> <b>data</b> set may mimic a correlation between eta and the strength of the HI absorption. For the restricted data samples a scatter of 10 - 15 % in eta would be expected even if the underlying value is constant. The observed scatter is significantly larger than expected, indicating that the intergalactic radiation background is indeed fluctuating. In the redshift range 2. 58 < z < 2. 72, where the data quality is best, we find eta ~ 100, suggesting a contribution of soft sources like galaxies to the UV background. Comment: 18 pages, 14 figures, accepted for publication in A&...|$|E
40|$|The thesis titled, Cyberspace, Surveillance, Law and Privacy {{analyses}} {{the implications}} of state sponsored cyber surveillance on the exercise of the human right to privacy of communications and data privacy of individuals, subject to untargeted interception of digital communications. The principle aim of the thesis is to assess the legality of mass cyber surveillance of the Five Eyes alliance, {{with an emphasis on}} the United States and the United Kingdom. The study also considers the growing trend among the law enforcement agencies to access data without consent located in foreign jurisdictions without recourse to the Mutual Legal Assistance arrangements. The objective of the thesis is to demonstrate that these activities breach states’ human rights obligations under the international human rights frameworks and to show the unprecedented impact that surveillance technologies continue to have on this right. The research also highlights the inadequate protection of privacy in the internet. This leads to the evaluation of a number of possible legal solutions on the international level to the problem of mass surveillance, since the internet is a global environment designed for <b>unrestricted</b> <b>data</b> flows among jurisdictions and therefore facilitates continued violation of privacy of communications and data privacy. The thesis finds that bearing in mind (a) the highly politicised nature of the internet governance discourse, (b) the reluctance of states to subject peacetime espionage to international law regulation through a legally binding treaty, (c) the fact that international human rights law relating to privacy of communications is in need of modernization, (d) the reluctance of states to commit to a legally binding cyber treaty, (e) the slow pace with which customary cyber international law rules emerge and (f) the tendencies of states on the domestic level towards the introduction of draconian surveillance legislation at the expense of privacy, any progress in this regard at this stage will be piecemeal and likely to be achieved through a combination of the updating of the existing international and regional human rights and data protection instruments and soft law agreements...|$|E
40|$|The {{combined}} {{utilization of}} climate scenarios, climate models and Geographic Information Systems [GIS] represent {{the most reliable}} tools to spatially determine the potential impacts of climate change in the near to end-of-century time horizons. With affordable computation, massive and open online courses, open access journals, cloud-based data visualization platforms, countless repositories of environmental and climate data available through the Internet, and the Free and Open Source [FOS] software movement, geospatial analysis is becoming an increasingly accessible field for professional researchers, the technically inclined, and the general public. I present here a Master's thesis that has been developed primarily using FOS software and openly accessible environmental data with few usage restrictions. The analysis is a multi-criteria-based suitability and climate categorization of Southern Quebec for European V. vinifera wine grape viticulture. Using several openly available GIS data sources, I identify and categorize the wine regions of Quebec according {{to a series of}} climate metrics developed specifically for wine studies. My analysis is based on both NASA Daymet present-day satellite-observed climate grids (Thornton et al., 2015) and ClimateNA (Wang, Hamann, Spittlehouse, & Carroll, 2016), a statistically downscaled gridded data set of 30 -year climate normals extending from years 1980 to 2100 for two climate change scenarios, the Representative Concentration Pathways scenarios [RCP] 4. 5 and 8. 5 (Moss et al., 2010). I perform my analysis by examining the results of these viticultural metrics and climate variables both at the regional scale and at locations of presently-operating vineyards. All results are determined spatially using QGIS (QGIS Development Team, 2016) and other Open Source GNU/Linux utilities (Debian Project, 2015). My results show that present-day Saint Lawrence Seaway Valley barely exceeds the needed thermal suitability threshold for V. vinifera viticulture with most of Montérégie and Estrie at or below most “Cool Climate” categorizations and other agricultural zones are located well below climatic suitability for European viticulture. For future projections both RCP scenarios mirror an increase of 200 growing degree-days [GDDs, ºC] from 1981 - 2010 to the 2011 - 2040 period and strongly diverging for periods afterwards. Results using the RCP 4. 5 “Stabilization” show present-day vineyard locations may experience an increase in climate region category by roughly one or two climate categories (“Temperate” and “Warm”), while the RCP 8. 5 “Business as Usual” scenario shows some present vineyard locations may become unsuitably hot with “Warm” viticultural climates extending above 50 ºN. I also present an extended literature review and methodology chapter that summarizes and explores my experience in employing almost exclusively FOS software and <b>unrestricted</b> <b>data.</b> This chapter is structured in a non-traditional fashion and is meant to provide an introductory background and discussion of the history of Open Source/Data/Access and Open Government movements. An extended methodology explores FOS software, Open Data resources, and showcases an example methodology for an agriculturally-focused FOS-GIS analysis. While the FOS movement is not presently capable of replacing all proprietary tools or present models of knowledge dissemination, Open Source approaches and a fostering of the Open ecosystem can be greatly beneficial for both the individual and at societal levels...|$|E
40|$|The {{need for}} {{integrating}} applications is growing {{as a consequence}} of organisational demands and enabling technologies, in particular the Web and enterprise software packages. In this Chapter, we introduce the basic concepts of application integration, discuss a number of the most important issues in the area and outline promising research directions: process libraries, methodologies for process integration, adaptive and flexible process enactment, and moving business logic from systems to processes. 1. Introduction As organisations are becoming increasingly dependent on information technology, the need for integrating applications is growing. As an answer to this need, technologies in Enterprise Application Integration (EAI) have been proposed. EAI can be defined as "the <b>unrestricted</b> sharing of <b>data</b> and business processes among any connected applications and data sources in the enterprise", [1]. The demand for EAI is driven by many forces, where {{one of the most important}} is th [...] ...|$|R
40|$|Is Africa’s {{current growth}} {{reducing}} inequality? What {{are the implications}} of growth on output performances in Africa? Does the effect of Africa’s growth on sectorial output have any implication for inequality in Africa? The study investigates the effect of shocks on a set of macroeconomic variables on inequality (measured by life expectancy) and the implication of this on sectors that are perceived to provide economic empowerment in form of employment for people living in the African countries in our sample. Studies already find that growth in many African countries has not been accompanied with significant improvement in employment. Therefore inequality is subject to a counter cyclical trend in production levels when export destination countries experience a recession. The study also provides insight on the effect of growth on sectorial output for three major sectors in the African economy with the intent of analyzing the impact of growth on sectorial development. The method used in this study is Panel Vector Autoregressive (PVAR) estimation and the obvious advantage of this method {{lies in the fact that}} it allows us to capture both static and dynamic interdependencies and to treat the links across units in an <b>unrestricted</b> fashion. <b>Data</b> is obtained from World Bank (WDI) Statistics for the period 1985 to 2012 (28 years) for 10 African Countries. Our main findings confirm strong negative relationship between GDP growth and life expectancy and also for GDP and the services and manufacturing sector considering the full sample...|$|R
40|$|Background This document, the Public Access Plan (the Plan) for the Department of Energy (DOE or Department), {{including}} the National Nuclear Security Administration, presents the Department of Energy’s plan for increasing {{access to the}} results of the research it supports in response to the February 22, 2013, Office of Science and Technology Policy (OSTP) Memorandum, “Increasing Access to the Results of Federally Funded Scientific Research. ” 1 This Plan outlines the Department’s approach to implementing the objectives of the OSTP memorandum. Through policies and systems discussed in this Plan, scholarly publications and datasets resulting from research directly arising from DOE funding can become more readily accessible to the public, setting the stage for increased innovation, commercial opportunities, and accelerated scientific breakthroughs. With regard to unclassified and otherwise unrestricted 2 research in scientific publications, the Department proposes a new policy and tool for providing access to peer-reviewed scholarly publications and associated metadata in which publishers retain their rights under copyright to the Version of Record (VoR). Both the policy and tool will be applied to scholarly publications resulting from unclassified and otherwise unrestricted research supported by the Department. With regard to unclassified and otherwise <b>unrestricted</b> scientific <b>data</b> in digital formats, the Department proposes a set of principles and requirements to be adopted by all DOE offices supporting open research. Implementing strategies and timelines may differ across the Department depending on the specific communities supported and funding mechanisms used by each office...|$|R
