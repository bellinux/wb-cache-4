147|1131|Public
25|$|A {{histogram}} can {{be thought}} of as a simplistic kernel density estimation, which uses a kernel to smooth frequencies over the bins. This yields a smoother probability density function, which will in general more accurately reflect distribution of the <b>underlying</b> <b>variable.</b> The density estimate could be plotted as an alternative to the histogram, and is usually drawn as a curve rather than a set of boxes.|$|E
25|$|Histograms give a rough {{sense of}} {{the density of the}} {{underlying}} distribution of the data, and often for density estimation: estimating the probability density function of the <b>underlying</b> <b>variable.</b> The total area of a histogram used for probability density is always normalized to 1. If the length of the intervals on the x-axis are all 1, then a histogram is identical to a relative frequency plot.|$|E
25|$|If {{the test}} {{statistic}} follows a Student's t-distribution in {{the null hypothesis}} – which is common where the <b>underlying</b> <b>variable</b> follows a normal distribution with unknown scaling factor, then the test {{is referred to as}} a one-tailed or two-tailed t-test. If the test is performed using the actual population mean and variance, rather than an estimate from a sample, it would be called a one-tailed or two-tailed Z-test.|$|E
50|$|A {{statistical}} endeavor, by contrast, uses a {{model that}} relates the game results to <b>underlying</b> <b>variables</b> representing the ability of each player.|$|R
25|$|Semidefinite {{programming}} (SDP) is a subfield of convex optimization {{where the}} <b>underlying</b> <b>variables</b> are semidefinite matrices. It is a generalization of linear and convex quadratic programming.|$|R
5000|$|If [...] and [...] {{are taken}} to be {{imperfect}} measurements of <b>underlying</b> <b>variables</b> [...] and [...] with independent errors, then [...] measures the true correlation between [...] and [...]|$|R
2500|$|If one {{chooses to}} reject {{counterfactual}} definiteness, reality {{has been made}} smaller, {{and there is no}} non-locality problem. On the other hand, one is thereby introducing irreducible or intrinsic randomness into our picture of the world: randomness that cannot be [...] "explained" [...] as merely the reflection of our ignorance of <b>underlying,</b> <b>variable,</b> physical quantities. Non-determinism becomes a fundamental property of nature.|$|E
2500|$|ROV {{is usually}} used when {{the value of}} a project is {{contingent}} on the value of some other asset or <b>underlying</b> <b>variable.</b> (For example, the viability of a mining project is contingent on the price of gold; if the price is too low, management will abandon the mining rights, if sufficiently high, management will develop the ore body. Again, a DCF valuation would capture only one of these outcomes.) Here: (1) using financial option theory as a framework, the decision to be taken is identified as corresponding to either a call option or a put option; (2) an appropriate valuation technique is then employed – usually a variant on the Binomial options model or a bespoke simulation model, while Black Scholes type formulae are used less often; see Contingent claim valuation. (3) The [...] "true" [...] value of the project is then the NPV of the [...] "most likely" [...] scenario plus the option value. (Real options in corporate finance were first discussed by Stewart Myers in 1977; viewing corporate strategy as a series of options was originally per Timothy Luehrman, in the late 1990s.) [...] See also Option pricing approaches under Business valuation.|$|E
5000|$|... where x(t) is an ×1 vector of {{functions}} of an <b>underlying</b> <b>variable</b> , x(t) is the vector of first derivatives of these functions, and A is an [...] matrix, of which all elements are constants.|$|E
30|$|Appendix 4 {{describes}} the conditional posterior distributions underlying the respective {{elements of the}} model; namely of the latent <b>underlying</b> <b>variables,</b> the factor loadings, the direct coefficients, the cutpoints, the latent factors and the indirect coefficients.|$|R
5000|$|For pairs from an {{uncorrelated}} bivariate normal distribution, {{the sampling}} distribution {{of a certain}} function of Pearson's correlation coefficient follows Student's t-distribution with degrees of freedom n − 2. Specifically, if the <b>underlying</b> <b>variables</b> have a bivariate normal distribution, the variable ...|$|R
30|$|Obviously, the Logsum elasticities are {{artificial}} in {{the sense}} that we cannot directly link these with <b>underlying</b> LoS <b>variables.</b> Hence, it is necessary to link the sensitivity of the Logsum to the sensitivity of the <b>underlying</b> LoS <b>variables,</b> which can be interpreted.|$|R
5000|$|In {{descriptive}} statistics terms, one measures a {{goodness of fit}} of a normal model to the data - if the fit is poor then the data are not well modeled in that respect by a normal distribution, without making a judgment on any <b>underlying</b> <b>variable.</b>|$|E
5000|$|Using the {{abbreviated}} syntax {{means that}} the <b>underlying</b> <b>variable</b> is no longer available from inside the class. As a result, the [...] portion of the property must be present for assignment. Access can be restricted with a -specific access modifier.public class Student { public string Name { get; private set; }} ...|$|E
5000|$|In {{statistical}} models applied to psychometrics, congeneric reliability [...] ("rho C") is {{the reliability of}} a unidimensional congeneric measurement model. It {{can be interpreted as}} an indicator of how reliably the items of such a measurement model reflect the same <b>underlying</b> <b>variable.</b> Synonymous terms are composite reliability, unidimensional omega and Raju (1977) coefficient).|$|E
40|$|Abstract — Spatial {{dependencies}} tend {{to introduce}} correlations among parameter values obtained from test structures. These spatial correlations obscure the parameter correlations caused by common <b>underlying</b> <b>variables</b> and make process diagnosis more difficult. In this work, linear regression method {{has been used}} to generate the spatial models for both wafer and chip level to investigate their significance. It is found that the wafer level regression model does not have a strong wafer level dependence on the ring oscillator devices. On the other hand, at the chip level, goodness of fit calculated for certain device structures exhibit strong spatial dependence on the linear regression model. This proves that it is essential to consider different levels of spatial dependence before making conclusion on the variations caused by <b>underlying</b> <b>variables</b> and parameter. Index Terms—Spatial analysis, linear regression, least square method I...|$|R
40|$|While the {{relationship}} between volatility and risk is central {{to much of the}} financial literature it has not been incorporated systematically into assessment of sovereign debt sustainability. This paper attempts to fill this gap by studying how the probability distribution of sovereign debt to GDP ratios depends on the stochastic properties of <b>underlying</b> macroeconomic <b>variables.</b> Using the right hand-tail of the distribution as a measure of the risk we are able to show how the volatility of the <b>underlying</b> <b>variables</b> as well as potential interactions between them influence country risk. Macroeconomic volatility, debt dynamics, sovereign spreads...|$|R
40|$|This paper {{introduces}} asymmetric {{impulse response}} functions and asymmetric variance decompositions. It is shown how the <b>underlying</b> <b>variables</b> {{can be transformed}} into cumulative positive and negative changes in order to estimate the impulses to an asymmetric innovation. An application is provided to demonstrate how the propagation mechanism of these asymmetric impulses and responses operates. ...|$|R
50|$|In {{trading of}} fixed income {{securities}} (bonds), various measures of bond duration are used analogously to the delta of an option. The closest analogue to the delta is DV01, which is the reduction in price (in currency units) for an increase of one basis point (i.e. 0.01% per annum) in the yield (the yield is the <b>underlying</b> <b>variable).</b>|$|E
5000|$|If one {{chooses to}} reject {{counterfactual}} definiteness, reality {{has been made}} smaller, {{and there is no}} non-locality problem. On the other hand, one is thereby introducing irreducible or intrinsic randomness into our picture of the world: randomness that cannot be [...] "explained" [...] as merely the reflection of our ignorance of <b>underlying,</b> <b>variable,</b> physical quantities. Non-determinism becomes a fundamental property of nature.|$|E
50|$|A {{histogram}} can {{be thought}} of as a simplistic kernel density estimation, which uses a kernel to smooth frequencies over the bins. This yields a smoother probability density function, which will in general more accurately reflect distribution of the <b>underlying</b> <b>variable.</b> The density estimate could be plotted as an alternative to the histogram, and is usually drawn as a curve rather than a set of boxes.|$|E
50|$|The {{binomial}} {{pricing model}} traces {{the evolution of}} the option's key <b>underlying</b> <b>variables</b> in discrete-time. This is done by means of a binomial lattice (tree), for a number of time steps between the valuation and expiration dates. Each node in the lattice represents a possible price of the underlying at a given point in time.|$|R
40|$|Surveys {{can be a}} {{rich source}} of information. However, the {{extraction}} of <b>underlying</b> <b>variables</b> from the analysis of mixed categoric and numeric survey data is fraught with complications when using grouping techniques such as clustering or ordination. Here I present a new strategy to deal with classification of households into clusters, and identification of cluster membership for new households. The strategy relies on probabilistic methods for identifying <b>variables</b> <b>underlying</b> the clusters. It incorporates existing methods that (i) help determine the optimal cluster number, (ii) directly identify <b>variables</b> <b>underlying</b> clusters, and (iii) identify the variables important for classifying new cases into existing clusters. The strategy uses the R statistical software, which is freely accessible to anyone. nominal; cluster; typology; statistics; data analysis; decision tree; grouping...|$|R
40|$|The weak {{convergence}} of the empirical process of strong mixing or associated random variables is studied in LP(0, 1). We find minimal rates of convergence to zero of the mixing coefficients or the covariances, in either case, supposing stationarity of the <b>underlying</b> <b>variables.</b> The rates obtained improve, for p not too large, the corresponding {{results in the}} classical D(0, 1) framework. [URL]...|$|R
50|$|Histograms give a rough {{sense of}} {{the density of the}} {{underlying}} distribution of the data, and often for density estimation: estimating the probability density function of the <b>underlying</b> <b>variable.</b> The total area of a histogram used for probability density is always normalized to 1. If the length of the intervals on the x-axis are all 1, then a histogram is identical to a relative frequency plot.|$|E
50|$|In {{mathematical}} finance, convexity {{refers to}} non-linearities {{in a financial}} model. In other words, {{if the price of}} an <b>underlying</b> <b>variable</b> changes, the price of an output does not change linearly, but depends on the second derivative (or, loosely speaking, higher-order terms) of the modeling function. Geometrically, the model is no longer flat but curved, and the degree of curvature is called the convexity.|$|E
5000|$|The {{context tree}} {{weighting}} method (CTW) is a lossless compression and prediction algorithm by [...] The CTW algorithm {{is among the}} very few such algorithms that offer both theoretical guarantees and good practical performance (see, e.g. [...] ).The CTW algorithm is an “ensemble method,” mixing the predictions of many <b>underlying</b> <b>variable</b> order Markov models, where each such model is constructed using zero-order conditional probability estimators.|$|E
40|$|During {{the last}} half century, a number of {{individuals}} and institutions, including the Food and Agriculture Organization of the United Nations (FAO) and IFPRI, have engaged in projections of future food demand, supply, and related variables. In this brief, Alex McCalla and Cesar Revoredo compare projections with real-life outcomes. Projections forecast outcomes {{on the basis of}} certain underlying factors. If such forecasted outcomes are undesirable, changes may be made in the underlying factors so that the projections may not, in fact, come to pass. Many projections serve this precise goal. Therefore, the success of projections may not be that they match actual outcomes but that they avoid such outcomes by promoting action to change <b>underlying</b> <b>variables.</b> Unlike predictions, which are successful only if they match actual outcomes, projections that differ from actual outcomes may reflect either poor projection models or changes in <b>underlying</b> <b>variables,</b> possibly caused by the projections themselves. BriefNon-PRIFPRI 1; Hunger; 2020 DG...|$|R
40|$|The polychoric {{correlation}} coefficient {{is a measure}} of association between two ordinal variables. It {{is based on the assumption}} that two latent bivariate normally distributed random variables generate couples of ordinal scores. Categories of the two ordinal variables correspond to intervals of the corresponding continuous variables. Thus, measuring the association between ordinal variables means estimating the product moment correlation between the <b>underlying</b> normal <b>variables</b> (Olsonn, 1979). When the hypothesis of la- tent bivariate normality is empirically or theoretically implausible, other dis- tributional assumptions can be made. In this paper a new and more °exible {{polychoric correlation}} coe±cient is proposed assuming that the <b>underlying</b> <b>variables</b> are skew-normally distributed (Roscino, 2005). The skew normal (Azzalini and Dalla Valle, 1996) is a family of distributions which includes the normal distribution as a special case, but with an extra parameter to reg- ulate the skewness. As for the original polychoric correlation coe±cient, the new coe±cient was estimated by the maximization of the log-likelihood func- tion with respect to the thresholds of the continuous variables, the skewness and the correlation parameters. The new coe±cient was then tested on sam- ples from simulated populations di®ering in the number of ordinal categories and the distribution of the <b>underlying</b> <b>variables.</b> The results were compared with those of the original polychoric correlation coe±cient...|$|R
40|$|Abstract. The “General Linear Reality” view of {{the social}} world {{endorsed}} by analysis models assuming (<b>underlying)</b> continuous <b>variables</b> that are normally distributed is still prevailing in most of social and behavioral research. In this article both assumptions are discussed, arguing that it might often be better to treat (latent) variables as fundamentally categorical and showing that the assumption of linear relationships may mislead researchers and lead them to accept response effects {{that may not be}} there or are of a rather different nature. Keywords: categorical/categorized <b>variables,</b> <b>underlying</b> latent <b>variables,</b> Pearson-Yule dispute, ordinal/linear relationships, (extreme) response effects, measurement equivalenc...|$|R
5000|$|In VB.NET 2010, Auto Implemented {{properties}} can {{be utilized}} {{to create a}} property without having to use the Get and Set syntax. Note that a hidden variable is created by the compiler, called , to correspond with the Property [...] Using another variable within the class named [...] would result in an error. Privileged access to the <b>underlying</b> <b>variable</b> is available from within the class.|$|E
50|$|If {{the test}} {{statistic}} follows a Student's t-distribution in {{the null hypothesis}} - which is common where the <b>underlying</b> <b>variable</b> follows a normal distribution with unknown scaling factor, then the test {{is referred to as}} a one-tailed or two-tailed t-test. If the test is performed using the actual population mean and variance, rather than an estimate from a sample, it would be called a one-tailed or two-tailed Z-test.|$|E
50|$|Regression {{analysis}} {{controls for}} {{other relevant variables}} by including them as regressors (explanatory variables). This helps to avoid false inferences of causality due {{to the presence of}} a third, <b>underlying,</b> <b>variable</b> that influences both the potentially causative variable and the potentially caused variable: its effect on the potentially caused variable is captured by directly including it in the regression, so that effect will not be picked up as an indirect effect through the potentially causative variable of interest.|$|E
5000|$|... (iv) And, does a {{spurious}} relationship {{exist in}} the model because an <b>underlying</b> causative <b>variable</b> is itself time-trending? ...|$|R
40|$|A multivariate {{comparison}} of morphometric differences was undertaken on populations of Pontophilus norvegicus from four Atlantic and one Mediterranean locations. Multiple discriminant analysis revealed clear morphometric differences between Atlantic and Mediterranean populations, with the Atlantic populations exhibiting {{a low degree}} of separation. The <b>underlying</b> <b>variables</b> responsible for this discrimination are shown {{not to have any}} operational taxonomic utility and, hence, no sub-specific status is attached to the respective populations...|$|R
3000|$|Unlike a policymaker, a {{theoretician}} {{is interested}} in estimating the specific <b>underlying</b> <b>variables</b> that affect empirical discriminability. For example, empirical ROC data will decrease toward the diagonal line of chance performance the more that (1) the memory signals for targets and foils overlap and/or (2) the confidence criteria vary from decision to decision (Wickelgren & Norman, 1966). Area under the curve measures (both non-parametric pAUC and parametric A [...]...|$|R
