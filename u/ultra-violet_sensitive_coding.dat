0|79|Public
5000|$|Theoretical <b>sensitive</b> <b>coding,</b> that is, {{generating}} theoretical strong concepts {{from the}} data to explain the phenomenon researched; ...|$|R
40|$|The COMPASS {{experiment}} at the SPS accelerator at CERN uses a {{large scale}} Ring Imaging CHerenkov detector (RICH) to identify pions, kaons and protons in a wide momentum range. For the data taking in 2006, the COMPASS RICH has been upgraded in the central photon detection area (25 % of the surface) with a new technology to detect Cherenkov photons at very high count rates of several 10 ^ 6 per second and channel and a new dead-time free read-out system, which allows trigger rates up to 100 kHz. The Cherenkov photons are detected by an array of 576 visible and <b>ultra-violet</b> <b>sensitive</b> multi-anode photomultipliers with 16 channels each. The upgraded detector showed an excellent performance during the 2006 data taking. Comment: Proceeding of the IPRD 06 conference (Siena, Okt. 06...|$|R
40|$|SafeStack, {{initially}} {{proposed as}} {{a key component of}} Code Pointer Integrity (CPI), separates the program stack into two distinct regions to provide a safe region for <b>sensitive</b> <b>code</b> pointers. SafeStack can prevent buffer overflow attacks that overwrite <b>sensitive</b> <b>code</b> pointers, e. g., return addresses, to hijack control flow of the program, and has been incorporated into the Clang project of LLVM as a C-based language front-end. In this paper, we propose and implement SafeStack+, an enhanced dual stack LLVM plug-in that further protects programs from data-flow hijacking. SafeStack+ locates data flow sensitive variables on the unsafe stack that could potentially affect evaluation of branching conditions, and adds canaries of random sizes and values to them to detect malicious overwriting. We implement SafeStack+ as a plug-in on LLVM 3. 8 and perform extensive experiments to justify a lazy checking mechanism that adds on average 3. 0 % of runtime and 5. 3 % of memory overhead on top of SafeStack on SPEC CPU 2006 benchmark programs. Our security analysis confirms that SafeStack+ is effective in detecting data-flow hijacking attacks...|$|R
40|$|The COMPASS {{experiment}} at the SPS accelerator at CERN uses a {{large scale}} Ring Imaging CHerenkov detector (RICH) to identify pions, kaons and protons in a wide momentum range. For the data taking in 2006, the COMPASS RICH has been upgraded in the central photon detection area (25 % of the surface) with a new technology to detect Cherenkov photons at very high count rates of several 10 ^ 6 per second and channel and a new dead-time free read-out system, which allows trigger rates up to 100 kHz. The Cherenkov photons are detected by an array of 576 visible and <b>ultra-violet</b> <b>sensitive</b> multi-anode photomultipliers with 16 channels each. The upgraded detector showed an excellent performance during the 2006 data taking. Particle identification at high rates is a central aspect of many present and future experiments in high-energy particle physics. The COMPASS experiment at the SPS accelerator at CERN uses {{a large scale}} Ring Imaging CHerenkov detector (RICH) to identify pions, kaons and protons in a wide momentum range. For the data taking in 2006, the COMPASS RICH has been upgraded in the central photon detection area (25 % of the surface) with a new technology to detect Cherenkov photons at very high count rates of several 106 s− 1 per channel and a new dead-time free read-out system, which allows trigger rates up to 100 kHz. The Cherenkov photons are detected by an array of 576 visible and <b>ultra-violet</b> <b>sensitive</b> multi-anode photomultipliers with 16 channels each. Lens telescopes of fused silica lenses have been designed and built to focus the Cherenkov photons onto the individual photomultipliers. The read-out electronics of the PMTs {{is based on the}} MAD 4 amplifier-discriminator chip and the dead-time free high resolution F 1 -TDC. The 120 ps time resolution of the digital card guarantees negligible background from uncorrelated physical events. In the outer part of the detector, where the particle rates are lower, the present multi-wire proportional chambers (MWPC) with Cesium Iodide photo-cathodes have been upgraded with a new read-out electronic system based on the APV preamplifier and shaper ASIC with analog pipeline and sampling ADCs. The project was fully designed and implemented in the period November 2004 until May 2006. The upgraded detector showed an excellent performance during the 2006 data taking: the number of detected Cherenkov photons per ring was increased from 14 to above 60 at saturation. The time resolution was improved from about 3 microseconds to about one nanosecond which allows an excellent suppression of the background photons from uncorrelated events...|$|R
50|$|The {{general idea}} is to {{identify}} <b>sensitive</b> <b>code</b> from application data by analyzing the source code. Once this is done the different data is separated and placed in different modules. When assuming that each module has total control over the sensitive information it contains, {{it is possible to}} specify when and how should leave the module. An example is a cryptographic module that can prevent keys from ever leaving the module unencrypted.|$|R
40|$|International audienceFault attack {{represents}} one of the serious threat against Java Card security. It consists in physical perturbation of chip components that cause an unusual behavior in the execution of Java Card applications. This perturbation allows to introduce faults on Java Card appli- cation, with the aim to reveal a secret information that are stored in the card or grant an undesired authorization. This paper presents a method- ology to recognize the <b>sensitive</b> <b>code</b> to the fault attack in the Java Card applications. It is based on concept from textcategorization and machine learning...|$|R
40|$|Diverse {{groups of}} people use edged-weapons (i. e. knives, spears, swords) professionally. The {{training}} received affects how the edged-weapon is used and {{the area of the}} body targeted. There is a growing body of information available on the internet which is aimed at the training individuals in offensive knife attacks. This poster aims to raise awareness of this issue and highlight how a trained individual modifies an attack sequence depending on their victim’s posture and the protective clothing worn. A male trained in the Filipino martial arts discipline of Eskrima performed attack techniques on a static mannequin covered with a long sleeved upper body garment and leggings, a police custodian helmet and a HG 1 + KR 1 police body armour. In some simulated attacks the target was also dressed in a police high-visibility tactical vest on top of the body armour. High-speed video was used to capture each simulated attack and the impact location on the torso recorded using <b>ultra-violet</b> <b>sensitive</b> liquid applied to the weapon. Target posture was modified by adjusting the arm position of the mannequin. In a second series of experiments a PermaGel™ male target torso was used so that penetrating damage could be assessed. Data collected identified the change of attack due to victim’s posture and vulnerability of the neck, underarm area and groi...|$|R
40|$|Abstract—Code Completion helps {{developers}} learn APIs and frees {{them from}} remembering every detail. In this paper, we describe a novel technique called CSCC (Context <b>Sensitive</b> <b>Code</b> Completion) {{for improving the}} performance of API method call completion. CSCC is context sensitive in that it uses new sources of information as {{the context of a}} target method call. CSCC indexes method calls in code examples by their contexts. To recommend completion proposals, CSCC ranks candidate methods by the similarities between their contexts and the context of the target call. Evaluation using a set of subject systems and five popular state-of-the-art techniques suggests that CSCC performs better than existing type or example-based code completion systems. We also investigate how the different contextual elements of the target call benefit CSCC. I...|$|R
40|$|The Planning Domain Definition Language (PDDL) {{represents}} {{a standard for}} definitions of planning domains and problems. Researchers and designers often make semantic and syntax errors due to the language’s complexity. At the same time, {{it is hard to}} read and work with larger documents in PDDL. We have developed a tool called PDDL Studio, which is aimed at aiding in creation and inspection of PDDL documents. The editor’s main features are: 1) PDDL parser capable of localizing syntax and semantic errors, 2) PDDL syntax highlighting, 3) context <b>sensitive</b> <b>code</b> completion and hints- similar to Microsoft’s IntelliSense for declarative languages, 4) code collapsing, 5) PDDL document management, and 6) planner integration. Our PDDL Editor also features a PDDL Parser tool, which {{can be used as a}} standalone parser for other projects...|$|R
40|$|Abstract. Code bases contain often {{millions}} {{lines of}} code. Code recommendation systems ease programming by proposing developers mined and extracted {{use cases of}} a code base. Currently, recommender systems are based on hardcoded sets what makes it complicate to adapt them. Another research area is adaptable live detection of code clones. We advance clone detection and code recommender systems by presenting utilization of our Hypermodelling approach to realize an alternative technique. This approach uses Data Warehousing technology that scales for big data and allows for flexible and adaptable queries of source code. We present the generic idea to advance recommendation and clone detection based on queries and to evaluate our application with industry source code. Consequently, recommender systems and clone detection can be customized with flexible queries via Hypermodelling. This enables further research about more complex clone detection and context <b>sensitive</b> <b>code</b> recommendation...|$|R
40|$|To save {{memory and}} improve speed, {{vectorial}} data such as images and signals are often represented as strings of discrete symbols (i. e., sketches). Charikar (2002) proposed a fast approximate method for finding neighbor pairs of strings by sorting and scanning {{with a small}} window. This method, which we shall call “single sorting”, is applied to locality <b>sensitive</b> <b>codes</b> and prevalently used in speed-demanding web-related applications. To improve on single sorting, we propose a novel method that employs blockwise masked sorting. Our method can dramatically {{reduce the number of}} candidate pairs which have to be verified by distance calculation in exchange with an increased amount of sorting operations. So it is especially attractive for high dimensional dense data, where distance calculation is expensive. Empirical results show the efficiency of our method in comparison to single sorting and recent fast nearest neighbor methods...|$|R
40|$|International audienceIsolated Execution Environments (IEEs), such as ARM TrustZone and Intel SGX, {{offer the}} {{possibility}} to execute <b>sensitive</b> <b>code</b> in isolation from other malicious programs, running on the same machine, or a potentially corrupted OS. A key feature of IEEs {{is the ability to}} produce reports binding cryptographically a message to the program that produced it, typically ensuring that this message {{is the result of the}} given program running on an IEE. We present a symbolic model for specifying and verifying applications that make use of such features. For this we introduce the SℓAPIC process calculus, that allows to reason about reports issued at given locations. We also provide tool support, extending the SAPIC/TAMARIN toolchain and demonstrate the applicability of our framework on several examples implementing secure outsourced computation (SOC), a secure licensing protocol and a one-time password protocol that all rely on such IEEs...|$|R
40|$|Software {{developers}} face {{a number}} of challenges when creating applications that attempt to keep important data confidential. Even diligent use of correct software design and implementation practices, can allow secrets to be exposed through a single flaw {{in any of the}} privileged code on the platform, code which may have been written by thousands of developers from hundreds of organizations throughout the world. Intel is developing innovative security technology that allows software developers control of the security of <b>sensitive</b> <b>code</b> and data by creating trusted domains within applications to protect critical information during execution and at rest. This paper will show how protection of private information, including enterprise rights management, video chat, trusted financial transactions, among others, has been demonstrated using this technology. Examples will include both protection of local processing and the establishment of secure communication with cloud services. It will illustrate useful software design patterns that can be followed to create many additional types of trusted software solutions...|$|R
40|$|Unlike {{benchmarks}} {{that focus}} on performance or reliability evaluations, a benchmark for computer security must neces-sarily include <b>sensitive</b> <b>code</b> and data. Because these artifacts could damage systems or reveal personally identifiable infor-mation about the users affected by cyber attacks, publicly dis-seminating such a benchmark raises several scientific, ethical and legal challenges. We propose the Worldwide Intelligence Network Environment (WINE), a security-benchmarking ap-proach based on rigorous experimental methods. WINE in-cludes representative field data, collected worldwide from 240, 000 sensors, for new empirical studies, and it will enable the validation of research on all the phases in the lifecycle of security threats. We tackle the key challenges for security benchmarking by designing a platform for repeatable experi-mentation on the WINE data sets and by collecting the meta-data required for understanding the results. In this paper, we review the unique characteristics of the WINE data, we dis-cuss why rigorous benchmarking will provide fresh insights on the security arms race and we propose a research agenda for this area. 1...|$|R
5000|$|The Commission report {{stated that}} the year 1989 {{signified}} [...] "the perpetuation of the general political trend of indulging the Tamil militants on Indian soil and tolerance of their wide-ranging criminal and anti-national activities." [...] The report also alleged that LTTE leaders in Jaffna were in possession of <b>sensitive</b> <b>coded</b> messages exchanged between the Union government and the state government of DMK. [...] "There is evidence to show that, during this period, {{some of the most}} vital wireless messages were passed between the LTTE operatives based in Tamil Nadu and Jaffna. These messages, which were decoded later, are directly related to the assassination of Rajiv Gandhi," [...] the report stated. The Congress subsequently brought down the United Front (UF) government of I K Gujral after the report was leaked in November 1998. The party also demanded the removal of DMK from the UF government, arguing that it had {{played a key role in}} the death of Rajiv Gandhi.|$|R
40|$|An {{important}} security {{challenge is}} to protect the execution of security-sensitive code on legacy systems from malware that may infect the OS, applications, or system devices. Prior work experienced a tradeoff between the level of security achieved and efficiency. In this work, we leverage the features of modern processors from AMD and Intel to overcome the tradeoff to simultaneously achieve a high level of security and high performance. We present TrustVisor, a special-purpose hypervisor that provides code integrity as well as data integrity and secrecy for selected portions of an application. TrustVisor achieves a high level of security, first because it can protect <b>sensitive</b> <b>code</b> at a very fine granularity, and second because it has a very small code base (only around 6 K lines of code) that makes verification feasible. TrustVisor can also attest the existence of isolated execution to an external entity. We have implemented TrustVisor to protect security-sensitive code blocks while imposing less than 7 % overhead on the legacy OS and its applications in the common case. ...|$|R
40|$|A {{data set}} {{composed}} of 1141 proteins representative of all eukaryotic protein sequences in the Swiss-Prot Protein Knowledge base was coded by seven physicochemical properties of amino acid residues. The resulting numerical profiles were submitted to correlation analysis after {{the application of}} a linear (simple mean) and a nonlinear (Recurrence Quantification Analysis, RQA) filter. The main RQA variables, Recurrence and Determinism, were subsequently analyzed by Principal Component Analysis. The RQA descriptors showed that (i) within protein sequences is embedded specific information neither present in the codes nor in the amino acid composition and (ii) the most <b>sensitive</b> <b>code</b> for detecting ordered recurrent (deterministic) patterns of residues in protein sequences is the Miyazawa-Jernigan hydrophobicity scale. The most deterministic proteins in terms of autocorrelation properties of primary structures were found (i) to be involved in protein−protein and protein−DNA interactions and (ii) to display a significantly higher proportion of structural disorder with respect to the average data set. A study of the scaling behavior of the average determinism with the setting parameters of RQA (embedding dimension and radius) allows for the identification of patterns of minimal length (six residues) as possible markers of zones specifically prone to inter- and intramolecular interactions...|$|R
40|$|Intel(R) Software Guard eXtensions (SGX) is a {{hardware-based}} {{technology for}} ensuring security of sensitive data from disclosure or modification that enables user-level applications to allocate protected areas of memory called enclaves. Such memory areas are cryptographically protected even from code running with higher privilege levels. This memory protection {{can be used}} to develop secure and dependable applications, but the technology has some limitations: ($i$) the code of an enclave is visible at load time, ($ii$) libraries used by the code must be statically linked, and ($iii$) the protected memory size is limited, demanding page swapping to be done when this limit is exceeded. We present DynSGX, a privacy preserving tool that enables users and developers to dynamically load and unload code to be executed inside SGX enclaves. Such a technology makes possible that developers use public cloud infrastructures to run applications based on <b>sensitive</b> <b>code</b> and data. Moreover, we present a series of experiments that assess how applications dynamically loaded by DynSGX perform in comparison to statically linked applications that disregard privacy of the enclave code at load time. Comment: Paper accepted on CloudCom 201...|$|R
40|$|International audienceMalicious Android {{applications}} use clever {{techniques to}} hide their real intents from the user and avoid detection by security tools. They resort to code obfuscation and dynamic loading, or wait for special events on the system like reboot or WiFi activation. Therefore, promising approaches aim to locate, study and execute specific parts of Android applications in order to monitor for suspicious behavior. They rely on Control Flow Graphs (CFGs) to obtain execution paths towards <b>sensitive</b> <b>codes.</b> We claim here that these CFGs are incomplete {{because they do not}} take into consideration implicit control flow calls, i. e., those that occur when the Android framework calls a method implemented in the application space. This article proposes a practical tool, GPFinder, exposing execution paths towards any piece of code considered as suspicious. GPFinder takes the Android framework into account and considers explicit and implicit control flow calls to build CFGs. Using GPFinder, we give global characteristics of application CFGs by studying a dataset of 14, 224 malware and 2, 311 goodware samples. We evaluate that 72. 69 % of the analyzed malicious samples have at least one suspicious method reachable only through implicit calls...|$|R
40|$|Enforcing {{integrity}} and confidentiality of users' application code and data is a challenging mission that any software developer {{working on an}} online production grade service is facing. Since cryptology is not a widely understood subject, people {{on the cutting edge}} of research and industry are always seeking for new technologies to naturally expand the security of their programs and systems. Intel Software Guard Extension (Intel SGX) is an Intel technology for developers who are looking to protect their software binaries from plausible attacks using hardware instructions. The Intel SGX puts <b>sensitive</b> <b>code</b> and data into CPU-hardened protected regions called enclaves. In this project we leverage the Intel SGX to produce a secure cryptographic library which keeps the generated keys inside an enclave restricting use and dissemination of confidential cryptographic keys. Using enclaves to store the keys we maintain a small Trusted Computing Base (TCB) where we also perform computation on temporary buffers to and from untrusted application code. As a proof of concept, we implemented hashes and symmetric encryption algorithms inside the enclave where we stored hashes, Initialization Vectors (IVs) and random keys and open sourced the code ([URL]...|$|R
40|$|Device drivers, {{which are}} {{normally}} implemented as kernel code, pose stability problems since bugs in the drivers cause kernel crashes. Running device drivers as unprivileged user-level code {{has often been}} proposed {{as a solution to}} increase the robustness of the system. However, moving the entire driver to user space brings down the performance of the system. An alternative approach would be to retain the performance critical code in the kernel and move the less performance <b>sensitive</b> <b>code</b> to the user space. In this project, we propose a scheme for factorization of driver code based on performance. In our split driver, work {{that needs to be done}} fast such as device I/O and interrupt handling is retained in the kernel space. Work that is less common and can afford to be done slower such as configuration or statictics collection is moved to the user space. We implemented this scheme on PCnet 32 network driver and measured the performance overhead incurred by moving some of the driver functions to user space. We found the performance overhead to be less than a factor of 2. Also, the performance of the critical operations retained in the kernel was not affected by this factorization. ...|$|R
40|$|Title: Tool for editing PDDL {{projects}} Author: Miroslav Chomut Department / Institute: Department of Software and Computer Science Education Supervisor of {{the bachelor}} thesis: Mgr. Tomáš Plch Abstract: The Planning Domain Definition Language (PDDL) {{is one of}} standard languages used for defining planning domains and problems. PDDL is a syntactically complex language therefore developers often make syntax and semantic errors. Working with larger PDDL files is time consuming. Unlike imperative programming languages (e. g. C#, C++), there is no suitable widespread tool for editing PDDL. Our goal is to provide PDDL developers with tool for comfortable editing, which is known from tools for imperative programming languages (e. g. Microsoft Visual Studio). This thesis describes our project called PDDL Studio, which is capable of a) syntax highlighting, b) context <b>sensitive</b> <b>code</b> completion, c) error detection with Interactive Error Table, d) code collapsing, e) project management, f) XML export and import features, g) planner integration and h) common editor features (e. g. Bracket Matching, Line Counter). Our tool is multiplatform and {{it is designed to}} increase efficiency of PDDL developers and code readability. Keywords: Problem Domain Definition Language, Integrated Development Environment, editor, syntax checking, [...] ...|$|R
40|$|Abstract Background Of {{the eight}} human herpes viruses, varicella-zoster virus, which causes {{chickenpox}} and zoster, {{has a unique}} epidemiology. Primary infection is much less common in children in the tropics compared with temperate areas. This results in increased adult susceptibility causing outbreaks, for example in health-care workers migrating from tropical to temperate countries. The recent demonstration that there are different genotypes of varicella-zoster virus and their geographic segregation into tropical and temperate areas suggests a distinct, yet previously unconsidered climatic factor {{may be responsible for}} both the clinical and molecular epidemiological features of this virus infection. Presentation of the hypothesis Unlike other human herpes viruses, varicella-zoster virus does not require intimate contact for infection to occur indicating that transmission may be interrupted by a geographically restricted climatic factor. The factor with the largest difference between tropical and temperate zones is ultra-violet radiation. This could reduce the infectiousness of chickenpox cases by inactivating virus in vesicles, before or after rupture. This would explain decreased transmissibility in the tropics and why the peak chickenpox incidence in temperate zones occurs during winter and spring, when ultra-violet radiation is at its lowest. The evolution of geographically restricted genotypes is also explained by ultra-violet radiation driving natural selection of different virus genotypes with varying degrees of resistance to inactivation, tropical genotypes being the most resistant. Consequently, temperate viruses should be more sensitive to its effects. This is supported by the observation that temperate genotypes are found in the tropics only in specific circumstances, namely where ultra-violet radiation has either been excluded or significantly reduced in intensity. Testing the Hypothesis The hypothesis is testable by exposing different virus genotypes to ultra-violet radiation and quantifying virus survival by plaque forming units or quantitative mRNA RT-PCR. Implications of the hypothesis The ancestral varicella-zoster virus, most probably a tropical genotype, co-migrated with man as he left Africa approximately 200, 000 years ago. For this virus to have lost the selective advantage of resistance to ultra-violet radiation, the hypothesis would predict that the temperate, <b>ultra-violet</b> <b>sensitive</b> virus should have acquired another selective advantage as an evolutionary trade-off. One obvious advantage could be an increased reactivation rate as zoster to set up more rounds of chickenpox transmission. If this were so, the mechanism responsible for resistance to ultra-violet radiation might also be involved in reactivation and latency. This could then provide the first insight into a genetic correlate of the survival strategy of this virus. </p...|$|R
40|$|An {{increasingly}} promising {{and widespread}} topic {{of research in}} distributed computing is the mobile agent paradigm: code travelling and performing computations on remote hosts in an au-tonomous manner. One {{of the biggest challenges}} faced by this new paradigm is security. The issue of protecting <b>sensitive</b> <b>code</b> and data carried by a mobile agent against tampering from a malicious host is particularly hard but important. Based on secure multi-party computation, a re-cent research direction shows the feasibility of a software-only solution to this problem, which had been deemed impossible by some researchers previously. The best result prior to this dissertation is a single-agent protocol which requires the participation of a trusted third party. Our research employs multi-agent protocols to eliminate the trusted third party, resulting in a protocol with minimum trust assumptions. This dissertation presents one of the first formal definitions of secure mobile agent computation, in which the privacy and integrity of the agent code and data as well as the data provided by the host are all protected. We present secure protocols for mobile agent computation against static, semi-honest or malicious adversaries without relying on any third party or trusting any specific participant in the system. The security of our protocols is formally proven through standard proo...|$|R
40|$|Subthreshold signal {{detection}} {{is an important}} task for animal survival in complex environments, where noise increases both the external signal response and the spontaneous spiking of neurons. The mechanism by which neurons process the coding of signals is not well understood. Here, we propose that coincidence detection, {{one of the ways}} to describe the functionality of a single neural cell, can improve the reliability and the precision of {{signal detection}} through detection of presynaptic input synchrony. Using a simplified neuronal network model composed of dozens of integrate-and-fire neurons and a single coincidence-detector neuron, we show how the network reads out the subthreshold noisy signals reliably and precisely. We find suitable pairing parameters, the threshold and the detection time window of the coincidence-detector neuron, that optimize the precision and reliability of the neuron. Furthermore, it is observed that the refractory period induces an oscillation in the spontaneous firing, but the neuron can inhibit this activity and improve the reliability and precision further. In the case of intermediate intrinsic states of the input neuron, the network responds to the input more efficiently. These results present the critical link between spiking synchrony and noisy signal transfer, which is utilized in coincidence detection, resulting in enhancement of temporally <b>sensitive</b> <b>coding</b> scheme...|$|R
40|$|Existing code update {{protocols}} target {{efficiency and}} assume correct behavior from participating sensor nodes. This work aims for the progressive, resource <b>sensitive</b> verification of <b>code</b> updates in sensor networks {{to ensure that}} unauthorized updates from malicious nodes are not propagated, while correct updates continue to be efficiently disseminated...|$|R
40|$|Mobile {{systems have}} become widely adopted by users to perform {{sensitive}} operations ranging from on-line payments {{for personal use}} to remote access to enterprise assets. Thus, attacks on mobile devices can cause significant loss to user's personal data {{as well as to}} valuable enterprise assets. In order to mitigate risks arising from attacks, various approaches have been proposed including the use of Trusted Execution Environment (TEE) to isolate and protect the execution of <b>sensitive</b> <b>code</b> {{from the rest of the}} system, e. g. applications and other software. However, users remain at risk of exploits via several types of software vulnerabilities - indicating that enterprises have failed to deliver the required protection, despite the use of existing isolation technologies. In this paper, we investigate Samsung KNOX and its usage of TEE as being the current technology providing secure containers. First, we study how KNOX uses TEE and perform analysis on its design consideration from a system vulnerabilities perspective. Second, we analyse and discuss recent attacks on KNOX and how those attacks exploit system vulnerabilities. Finally, we present new shortcomings emerging from our analysis of KNOX architecture. Our research exhibits that system vulnerabilities are the underlying cause of many attacks on systems and it reveals how they affect fundamental design security principles when the full potential of TEE is not exploited...|$|R
40|$|A depth {{image is}} {{three-dimensional}} (3 D) information used for virtual view synthesis in 3 D video system. In depth coding, the object boundaries {{are hard to}} compress and severely affect the rendering quality since they are <b>sensitive</b> to <b>coding</b> errors. In this paper, we propose a depth boundary reconstruction filter and utilize it as an in-loop filter to code the depth video. The proposed depth boundary reconstruction filter is designed considering occurrence frequency, similarity, and closeness of pixels. Experimental results demonstrate that the proposed depth boundary reconstruction filter is useful for efficient depth coding as well as high-quality 3 D rendering...|$|R
40|$|Application {{size and}} {{complexity}} are {{the underlying cause}} of numerous security vulnerabilities in code. In order to mitigate the risks arising from such vulnerabilities, various techniques have been proposed to isolate the execution of <b>sensitive</b> <b>code</b> {{from the rest of}} the application and from other software on the platform (e. g. the operating system). However, even with these partitioning techniques, it is not immediately clear exactly how they can and should be used to partition applications. What overall partitioning scheme should be followed; what granularity of the partitions should be. To some extent, this is dependent on the capabilities and performance of the partitioning technology in use. For this work, we focus on the upcoming Intel Software Guard Extensions (SGX) technology as the state-of-the-art in this field. SGX provides a trusted execution environment, called an enclave, that protects the integrity of the code and the confidentiality of the data inside it from other software, including the operating system. We present a novel framework consisting of four possible schemes under which an application can be partitioned. These schemes range from coarse-grained partitioning, in which the full application is included in a single enclave, through ultra-fine partitioning, in which each application secret is protected in an individual enclave. We explain the specific security benefits provided by each of the partitioning schemes and discuss how the performance of the application would be affected. To compare the different partitioning schemes, we have partitioned OpenSSL using four different schemes. We discuss SGX properties together with the implications of our design choices in this pape...|$|R
40|$|AbstractWith the advancements {{in digital}} technology, {{the threat of}} unimaginable level of {{duplicating}} and illegal reproducing of software also increases. Therefore the piracy rate is increasing proportionally. This scenario has clearly placed the threat for the software manufacturers and leads {{to the development of}} numerous software protection techniques. The numerous software protection techniques have been developed and one of such software protection techniques is code obfuscation. The code obfuscation is a mechanism for hiding the original algorithm, data structures or the logic of the code, or to harden or protect the code (which is considered as intellectual property of the software writer) from the unauthorized reverse engineering process. In general, code obfuscation involves hiding a program's implementation details from an adversary, i. e. transforming the program into a semantically equivalent (same computational effect) program, which is much harder to understand for an attacker. None of the current code obfuscation techniques satisfy all the obfuscation effectiveness criteria to resistance the reverse engineering attacks. Therefore the researchers as well as the software industries are trying their best to apply newer and better obfuscation techniques over their intellectual property in a regular process. But unfortunately, software code is not safe, i. e. still it can be cracked. This paper presents some of the obfuscation methods, which can help to protect the <b>sensitive</b> <b>code</b> fragments of any software, without alteration of inherent functionalities of the software. The proposed obfuscation techniques are implemented in assembly level code, with taking care of the theory of optimizing transformations. The assembly code represents the data dependencies and comfort to analyse the data after disassembling the executable as compared to the decompiled code...|$|R
40|$|We {{present a}} novel algorithm, Compact Kd-Trees (CompactKdt), that {{achieves}} state-of-the-art performance in searching large scale object image collections. The algorithm uses {{an order of}} magnitude less storage and computations by making use of both the full local features (e. g. SIFT) and their compact binary signatures to build and search the K-Tree. We compare classical PCA dimensionality reduction to three methods for generating compact binary representations for the features: Spectral Hashing, Locality Sensitive Hashing, and Locality <b>Sensitive</b> Binary <b>Codes.</b> CompactKdt achieves significant performance gain over using the binary signatures alone, and comparable performance to using the full features alone. Finally, our experiments show significantly better performance than the state-of-the-art Bag of Words (BoW) methods with equivalent or less storage and computational cost...|$|R
40|$|Fine-grained program counter-based {{memory access}} control {{mechanisms}} {{can be used to}} enhance low-level machine models to become the target of secure (fully abstract) com-pilation schemes. A secure compilation scheme reduces the power of a low-level attacker with code injection privileges to that of a high-level attacker which generally does not have such privileges. The existing trace semantics for a fine-grained program counter-based memory access control mechanism is not fully abstract, thus the protection mechanism it models cannot be used as the target of a provably secure compilation scheme. This paper shows why is such a fully abstract trace semantics needed, and proposes a correction to the existing trace semantics that makes it fully abstract and thus capable of supporting a secure compilation scheme. Low-level machine code offers virtually no protection mechanism from an attacker that has code injection privileges, who is free to read sensible data and disrupt the execution flow with malicious code. A way to defend against these kind of attacks is by employing a fine-grained program counter-based memory {{access control mechanisms}} (FPMAC). The idea behind recent FPMACs implementations [3, 5, 8, 9], is to run <b>sensitive</b> <b>code</b> in isolation, so that malicious low-level code cannot tamper with it. Although details of these works differ, the FPMAC protection mechanism can be summarized as follows. The memory is logically divided into a protected and an unprotected section. Protected memory is further divided into a code and a data section. The code section contains a number of entry points: addresses which unprotected memory instructions can jump to and execute. The data section is accessible only from the protected section. The following table provides a representation of the access control model enforced by the protection mechanism...|$|R
40|$|In {{this work}} we {{categorize}} comprehensively image quality measures, extend measures defined for gray scale images to their multispectral case, and propose novel image quality measures. They are categorized into pixel difference-based, correlation-based, edge-based, spectral-based, context based and HVS-based (Human Visual System-based) measures. Furthermore we compare these measures statistically for still image compression applications. The statistical {{behavior of the}} measures and their sensitivity to coding artifacts are investigated via Analysis of Variance techniques. Their similarities or differences have been illustrated by plotting their Kohonen maps. Measures that give consistent scores across an image class and that are <b>sensitive</b> to <b>coding</b> artifacts are pointed out. It {{has been found that}} measures based on phase spectrum, on multiresolution distance or HVS filtered mean square error are computationally simple and are more responsive to coding artifacts...|$|R
40|$|Microscopic {{analysis}} of malicious code (malware) requires {{the aid of}} a variety of powerful tools. Chief among them is a debugger that enables runtime binary analysis at an instruction level. One of the important services provided by a debugger is the ability to stop execution of code at an arbitrary point during runtime, using breakpoints. Software breakpoints support an unlimited number of breakpoint locations by changing the code being debugged {{so that it can be}} interrupted during runtime. Most, if not all, malware are very <b>sensitive</b> to <b>code</b> modification with self-modifying and/or self-checking (SM-SC) capabilities, rendering the use of software breakpoints limited in their scope. Hardware breakpoints supported by the underlying processor, on the other hand, use a subset of the processor register set and exception mechanisms to provide breakpoints that do not entai...|$|R
40|$|The {{study of}} the {{expansion}} of a laser produced lithium plasma using spatially and temporally resolved imaging and spectroscopic diagnostic techniques is described. The diagnostic system consists of three separate components: a 2. 2 m grazing incidence spectrometer (coupled to an Extreme <b>Ultra-Violet</b> (EUV) <b>sensitive</b> photodiode array), a recently developed fast-frame photography apparatus comprising a CCD camera coupled to a gated image intensifier, and a newly developed shadowgraphy apparatus consisting {{of a combination of}} a Nd:YAG pumped dye laser and a CCD camera. The development and capabilities of the diagnostic techniques used to characterise the plasma expansion are outlined. Furthermore, the characterisation of new or additional instrumental parameters pertinent to the quantitative interpretation of the experimental data is explored. Using the 2. 2 m grazing incidence spectrometer, temperature and density profile estimates for a laser produced lithium plasma are inferred. Photoabsorption spectra using this instrument and a newly developed model, for the 1 s 2 -> 1 snp (n = 4, 5, 6 and 7) in Li+, are synthesised for the first time. Employing the fast-frame photography technique, species velocities and corresponding temperature estimates are obtained. Additionally, excited state density distributions are extracted by application of the Abel transform. Finally, the shadowgraph technique is used to furnish electron density distribution information. In all cases plasma parameters, determined using the diagnostic techniques proposed, are correlated with novel computer codes developed, based on established plasma expansion models. The thesis concludes with a description of future work with an emphasis on prospective extensions to the diagnostic techniques developed...|$|R
40|$|Conference Name: 2013 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2013. Conference Address: Manchester, United kingdom. Time:October 13, 2013 - October 16, 2013. IEEE; IEEE Systems, Man, and Cybernetics Society; State Grid Energy Research Institute; IET; PowerpegImage-based 3 D human pose {{recovery}} is usually conducted by retrieving relevant poses with image features. However, it suffers from high dimensionality of image features and low efficiency of retrieving process. In this paper, we propose {{a novel approach}} to recover 3 D human poses from silhouettes. This approach improves traditional methods by adopting locality <b>sensitive</b> sparse <b>coding</b> in the retrieving process. It incorporates a local similarity preserving term into the objective of sparse coding, which groups similar silhouettes to alleviate the instability of sparse codes. The experimental results demonstrate {{the effectiveness of the}} proposed method. ? 2013 IEEE...|$|R
