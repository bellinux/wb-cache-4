10000|32|Public
5|$|Given an iid {{random sample}} {x1, …, x'n} of size n from a <b>univariate</b> {{distribution}} with the {{cumulative distribution function}} F(x;θ0), where θ0 ∈ Θ is an unknown parameter to be estimated, let {x(1), …, x(n)} be the corresponding ordered sample, that {{is the result of}} sorting of all observations from smallest to largest. For convenience also denote x(0) = −∞ and x(n+1) = +∞.|$|E
5|$|In statistics, maximum spacing {{estimation}} (MSE or MSP), or maximum {{product of}} spacing estimation (MPS), {{is a method}} for estimating the parameters of a <b>univariate</b> statistical model. The method requires maximization of the geometric mean of spacings in the data, which are {{the differences between the}} values of the cumulative distribution function at neighbouring data points.|$|E
5|$|Selection has {{different}} effects on traits. Stabilizing selection acts {{to hold a}} trait at a stable optimum, and in the simplest case all deviations from this optimum are selectively disadvantageous. Directional selection favours extreme values of a trait. The uncommon disruptive selection also acts during transition periods when the current mode is sub-optimal, but alters the trait {{in more than one}} direction. In particular, if the trait is quantitative and <b>univariate</b> then both higher and lower trait levels are favoured. Disruptive selection can be a precursor to speciation.|$|E
25|$|Previously, {{this article}} {{discussed}} the <b>univariate</b> median, when the sample or population had one-dimension. When the dimension is two or higher, {{there are multiple}} concepts that extend {{the definition of the}} <b>univariate</b> median; each such multivariate median agrees with the <b>univariate</b> median when the dimension is exactly one.|$|E
25|$|Kernel density {{estimation}} <b>univariate</b> kernel density estimation.|$|E
25|$|Both <b>univariate</b> {{and multivariate}} cases {{need to be}} considered.|$|E
25|$|A {{probability}} distribution whose sample space is {{the set of}} real numbers is called <b>univariate,</b> while a distribution whose sample space is a vector space is called multivariate. A <b>univariate</b> distribution gives the probabilities of a single random variable taking on various alternative values; a multivariate distribution (a joint {{probability distribution}}) gives the probabilities of a random vector—a list {{of two or more}} random variables—taking on various combinations of values. Important and commonly encountered <b>univariate</b> probability distributions include the binomial distribution, the hypergeometric distribution, and the normal distribution. The multivariate normal distribution is a commonly encountered multivariate distribution.|$|E
25|$|SAS System {{includes}} <b>univariate</b> {{probability mass}} function and distribution function.|$|E
25|$|A general {{formula for}} {{differential}} resolvents of arbitrary <b>univariate</b> polynomials {{is given by}} Nahay's powersum formula.|$|E
25|$|Note {{that the}} ARMA {{model is a}} <b>univariate</b> model. Extensions for the multivariate case are the vector {{autoregression}} (VAR) and Vector Autoregression Moving-Average (VARMA).|$|E
25|$|For <b>univariate</b> {{distributions}} {{that are}} symmetric about one median, the Hodges–Lehmann estimator is a robust and highly efficient estimator {{of the population}} median.|$|E
25|$|In {{addition}} to <b>univariate</b> distributions, characteristic functions {{can be defined}} for vector or matrix-valued random variables, and can also be extended to more generic cases.|$|E
25|$|Mathematica {{supports}} the <b>univariate</b> Poisson distribution as PoissonDistribution, and the bivariate Poisson distribution as MultivariatePoissonDistribution, including computation of probabilities and expectation, sampling, parameter estimation and hypothesis testing.|$|E
25|$|Sklar's Theorem {{states that}} any multivariate joint {{distribution}} {{can be written}} in terms of <b>univariate</b> marginal distribution functions and a copula which describes the dependence structure between the variables.|$|E
25|$|Sparse grids were {{originally}} developed by Smolyak for the quadrature of high-dimensional functions. The method is always {{based on a}} one-dimensional quadrature rule, but performs a more sophisticated combination of <b>univariate</b> results.|$|E
25|$|Key {{and quick}} <b>univariate</b> (single parameter) {{statistics}} based on output levels, with release based on geography. (e.g. local authority, wards and parish/ community) tabulated by population count and percentages and accessible by post code.|$|E
25|$|There {{are also}} {{formulas}} for cubic and quartic polynomials {{which can be}} used in the same way. However, there are no algebraic formulas in terms of the coefficients that apply to all <b>univariate</b> polynomials of a higher degree, by the Abel–Ruffini theorem.|$|E
25|$|The Poisson {{distribution}} {{is a special}} case of discrete compound Poisson distribution (or stuttering Poisson distribution) with only a parameter. The discrete compound Poisson distribution can be deduced from the limiting distribution of <b>univariate</b> multinomial distribution. It is also a special case of a compound Poisson distribution.|$|E
25|$|In the {{continuous}} <b>univariate</b> case above, the reference measure is the Lebesgue measure. The {{probability mass function}} of a discrete random variable is the density {{with respect to the}} counting measure over the sample space (usually the set of integers, or some subset thereof).|$|E
25|$|The {{marginal}} median {{is defined}} for vectors defined {{with respect to}} a fixed set of coordinates. A marginal median is defined to be the vector whose components are <b>univariate</b> medians. The marginal median is easy to compute, and its properties were studied by Puri and Sen.|$|E
25|$|Pearson's {{system of}} {{continuous}} curves. A system of continuous <b>univariate</b> probability distributions {{that came to}} {{form the basis of}} the now conventional continuous probability distributions. Since the system is complete up to the fourth moment, it is a powerful complement to the Pearsonian method of moments.|$|E
25|$|The {{notion of}} normal distribution, {{being one of}} the most {{important}} distributions in probability theory, has been extended far beyond the standard framework of the <b>univariate</b> (that is one-dimensional) case (Case 1). All these extensions are also called normal or Gaussian laws, so a certain ambiguity in names exists.|$|E
25|$|In DBM, highly {{non-linear}} registration algorithms are used, and {{the statistical}} analyses are not {{performed on the}} registered voxels but on the deformation fields used to register them (which requires multivariate approaches) or derived scalar properties thereof, which allows for <b>univariate</b> approaches. One common variant—sometimes referred to as Tensor-based morphometry (TBM)is based on the Jacobian determinant of the deformation matrix.|$|E
25|$|For a <b>univariate</b> {{polynomial}}, p(x), {{the factor}} theorem states that a is a {{root of the}} polynomial (that is, p(a) = 0, also called a zero of the polynomial) {{if and only if}} (x – a) is a factor of p(x). The other factor in such a factorization of p(x) can be obtained by polynomial long division or synthetic division.|$|E
25|$|The <b>univariate</b> noncentral hypergeometric {{distribution}} may be derived alternatively as a conditional {{distribution in the}} context of two binomially distributed random variables, for example when considering the response to a particular treatment in two different groups of patients participating in a clinical trial. An important application of the noncentral {{hypergeometric distribution}} in this context is the computation of exact confidence intervals for the odds ratio comparing treatment response between the two groups.|$|E
25|$|In mathematics, a Sylvester matrix is {{a matrix}} {{associated}} to two <b>univariate</b> polynomials with coefficients {{in a field}} or a commutative ring. The entries of the Sylvester matrix of two polynomials are coefficients of the polynomials. The determinant of the Sylvester matrix of two polynomials is their resultant, which is zero when the two polynomials have a common root (in case of coefficients in a field) or a non-constant common divisor (in case of coefficients in an integral domain).|$|E
25|$|Statistical Parametric Mapping (SPM) is {{a method}} for {{determining}} whether the activation of a particular brain region changes between experimental conditions, stimuli, or over time. The essential idea is simple, and consists of two major steps: first, one performs a <b>univariate</b> statistical test on each individual voxel between each experimental condition. Second, one analyzes the clustering of the voxels that show statistically significant differences, and determines which brain regions exhibit different levels of activation under different experimental conditions.|$|E
25|$|There are general {{algorithms}} {{which always}} produce the complete factorization of any polynomial, in either one variable (the <b>univariate</b> case) or several variables (the multivariate case); see Factorization of polynomials. These algorithms are implemented {{and are available}} in most computer algebra systems. They involve advanced properties of polynomials, and are too complicated for hand-written computation. There are also a few elementary methods which are well–suited for hand-written computation, and do not always allow finding the complete factorization in degree higher than four.|$|E
25|$|If K is a field, and K a <b>univariate</b> {{polynomial}} ring, then a K-module M is a K-module with {{an additional}} action of x on M that commutes with the action of K on M. In other words, a K-module is a K-vector space M combined with a linear map from M to M. Applying the Structure theorem for finitely generated modules over a principal ideal domain to this example shows {{the existence of the}} rational and Jordan canonical forms.|$|E
25|$|Some {{but not all}} {{polynomial}} equations with rational coefficients have {{a solution}} that is an algebraic expression with {{a finite number of}} operations involving just those coefficients (that is, it can be solved algebraically). This can be done for all such equations of degree one, two, three, or four; but for degree five or more it can be solved for some equations but not for all. A large amount of research has been devoted to compute efficiently accurate approximations of the real or complex solutions of a <b>univariate</b> algebraic equation (see Root-finding algorithm) and of the common solutions of several multivariate polynomial equations (see System of polynomial equations).|$|E
2500|$|... where [...] is the (<b>univariate,</b> central) hypergeometric {{distribution}} probability.|$|E
2500|$|... is a <b>univariate</b> {{algebraic}} (polynomial) equation with integer coefficients and ...|$|E
2500|$|Johnson, N. L., Kotz, S., Balakrishnan, N. (1970), Continuous <b>Univariate</b> Distributions, Volume 2, Wiley.|$|E
2500|$|The R package [...] {{includes}} the <b>univariate</b> {{probability mass function}} and random variable generating function.|$|E
2500|$|Theorem (Gil-Pelaez). For a <b>univariate</b> random {{variable}} X, if x is a continuity point of FX then ...|$|E
2500|$|Probability binning is a non-gating {{analysis}} method in which flow cytometry data is split into quantiles on a <b>univariate</b> basis.|$|E
