6|10000|Public
40|$|Abstract: This work {{introduces}} a novel information visualization technique for mobile devices through Augmented Reality (AR). A painting boundary detector and a features extraction modules {{have been implemented}} to compute paintings signatures. The computed signatures are matched using a linear weighted combination of the extracted features. The detected boundaries and the features are exploited to compute the homography transformations. The homography transformations are used to introduce a novel <b>user</b> <b>interaction</b> <b>technique</b> for AR. Three different user interfaces have been evaluated using standard usability methods. ...|$|E
40|$|Today’s {{increasing}} design complexity requires {{innovation in}} both debug automation, and user interaction. We present here a novel query-based method that uses assertions as queries {{to assist in}} the domain of hardware debugging. Our approach is generally applicable to hardware debug, and is independent of the particulars of the input design or the assertion description language. We show how assertions drive the debugging process in order to diagnose the cause of the faulty behavior through the unique techniques of trace slicing, and trace dicing. We also present here a new visualization and <b>user</b> <b>interaction</b> <b>technique</b> specifically designed for complex systems with numerous component interactions and abundant control flows. The visualization approach leverages design and assertion spans to present the debugging information to the user in a refined detail of abstraction and permit adequate interaction. An experimental debugging system incorporating assertion-based debug guidance has been implemented. Experimental results for productivity enhancements from a case study will be presented...|$|E
40|$|This paper {{describes}} a prototype application based on smartphone devices which supports independent learners within a mobile environment. The techniques employed allow children to rapidly {{gain access to}} a large repository of multimedia information {{through the use of}} a camera equipped mobile phone or smartphone. More specifically, the use of visual codes (Rohs M. et al., 2004) attached to locations and objects within a museum and park in Lancaster enable the retrieval of Web based information to be triggered by capturing images using the integrated CCD camera. Moreover, the location and orientation of the phone are used as contextual parameters in order to control the specific information to be retrieved by the system. The prototype described in this paper is currently under evaluation by groups of children in Lancaster in order to evaluate the use of this platform for teaching and learning. Our aim was to establish whether or not this <b>user</b> <b>interaction</b> <b>technique</b> could be harnessed for education based applications targeted at young children...|$|E
40|$|In this paper, a {{framework}} allowing {{to extend the}} applicability of natural <b>user</b> <b>interaction</b> <b>techniques</b> to existing programs is presented. Body gestures captured by a depth camera are mapped to application commands, and a wide set of common desktop applications can be controlled without any code rewritin...|$|R
40|$|This book {{discusses}} {{how human}} language techniques can be embedded in humancomputer dialogue to make "talking computers" conversational. It uses an interdisciplinary approach to explain application of speech technologies to computer interfaces. Chapters progress from physiological and psychological components of speech, language, and hearing to technical principles of speech synthesis and voice recognition to <b>user</b> <b>interaction</b> <b>techniques</b> to employ such technologies effectively. Areas covered include...|$|R
40|$|This paper {{presents}} {{a series of}} new augmented reality <b>user</b> <b>interaction</b> <b>techniques</b> to support the capture and creation of 3 D geometry of large outdoor structures, {{part of an overall}} concept we have named construction at a distance. We use information about the user's physical presence, along with hand and head gestures, to allow the user to capture and create the geometry of objects that are orders of magnitude larger than themselves, with no prior information or assistance...|$|R
40|$|This paper {{describes}} a prototype application which enables the real-time monitoring and visualization of large Wide Area Networks (WANs) using smartphone devices. The techniques employed allow field engineers to rapidly {{gain access to}} a large information repository {{through the use of}} a camera equipped mobile phone. More specifically, the use of visual codes [11] attached to networking hardware and infrastructure cabling enables the real-time visualization of network traffic and statistics to be triggered by the capturing of images from a personal device. Moreover, the location and orientation of the phone are used as contextual parameters in order to control the specific information to be retrieved. The prototype described in this paper is currently under evaluation by Information Systems Services (ISS) which is responsible for network support across Lancaster University, the student residences network and also a large regional WAN spanning the whole of the North West of England. Our aim was to establish whether or not this <b>user</b> <b>interaction</b> <b>technique</b> could be harnessed for a real world application that would benefit field engineers who are responsible for maintaining a live production network interconnecting tens of thousands of hosts...|$|E
40|$|Modern {{handheld}} {{devices such}} as smart phones and tablets have become popular for capturing and viewing panoramic images, but navigation in these devices is a challenging task due to small screen size. In this thesis work, investigation is done for {{similarities and differences between}} two user interaction techniques of one-dimensional cubical panorama images. This work is carried out on a fourth generation Apple iPod Touch. The first <b>user</b> <b>interaction</b> <b>technique</b> swipe (drag-based panning) is developed using touch-gestures, while the second technique look-around (device-orientation panning) is developed using motion-sensors such as gyroscope and accelerometer for device-orientation. Both these techniques were implemented for touch-screen devices for an angle of 360 degrees.      In the swipe technique, the navigation is done by using finger movements on the screen, while in the look-around based panning technique the navigation is done by moving the device direction physically in space through an angle of 360 degrees. The experiments were conducted on 20 participants for the two panning techniques. The results have shown that the look-around based panning technique offers a better user interaction compared to swipe based panning technique. Look-around based panning technique is natural and more free to move physically in space. Swipe based panning technique is easy to use in some situations and it does not require any physical space to move around the system. ...|$|E
40|$|This {{thesis is}} {{concerned}} with the geometrical modeling of organs to perform medical image analysis tasks. The thesis is divided in two main parts devoted to model linear vessel segments and the left ventricle of the heart, respectively. Chapters 2 to 4 present different aspects of a model-based technique for semi-automated quantification of linear vessel segments from 3 -D Magnetic Resonance Angiography (MRA). Chapter 2 {{is concerned with}} a multiscale filter for the enhancement of vessels in 2 -D and 3 -D angiograms. Chapter 3 applies the filter developed in Chapter 2 to determine the central vessel axis in 3 -D MRA images. This procedure is initialized using an efficient <b>user</b> <b>interaction</b> <b>technique</b> that naturally incorporates the knowledge of the operator about the vessel of interest. Also in this chapter, a linear vessel model is used to recover the position of the vessel wall in order to carry out an accurate quantitative analysis of vascular morphology. Prior knowledge is provided in two main forms: a cylindrical model introduces a shape prior while prior knowledge on the image acquisition (type of MRA technique) is used to define an appropriate vessel boundary criterion. In Chapter 4 an extensive in vitro and in vivo evaluation of the algorithm introduced in Chapter 3 is described. Chapters 5 to 7 change the focus to 3 D cardiac image analysis from Magnetic Resonance Imaging. Chapter 5 presents an extensive survey, a categorization and a critical review of the field of cardiac modeling. Chapter 6 and Chapter 7 present successive refinements of a method for building statistical models of shape variability with particular emphasis on cardiac modeling. The method is based on an elastic registration method using hierarchical free-form deformations. A 3 D shape model of the left and right ventricles of the heart was constructed. This model contains both the average shape of these organs as well as their shape variability. The methodology presented in the last two chapters could also be applied to other anatomical structures. This has been illustrated in Chapter 6 with examples of geometrical models of the nucleus caudate and the radius...|$|E
40|$|This paper {{discusses}} {{the development of}} a prototype human-computer interaction (HCI) environment for the user-led exploration of time and place. It draws on earlier research to develop <b>user</b> <b>interaction</b> <b>techniques</b> using Kinect for Windows and the Kinect SDK. Two specific visual techniques are applied - a lens and a slider - for gesture-based manipulation of content, supported by aural commands that facilitate simplified switching between techniques. Code samples are included to illustrate the development and realisation of the techniques and their reapplication using Kinect...|$|R
40|$|Cooperative {{manipulation}} {{refers to}} the simultaneous manipulation of a virtual object by multiple users in an immersive virtual environment. This paper describes a framework supporting the development of collaborative manipulation techniques, and example techniques we have tested within this framework. We describe the modeling of cooperative <b>interaction</b> <b>techniques,</b> methods of combining simultaneous user actions, and the awareness tools used to provide the necessary knowledge of partner activities during the cooperative interaction process. Our framework {{is based on a}} Collaborative Metaphor concept that defines rules to combine <b>user</b> <b>interaction</b> <b>techniques.</b> The combination is based on the separation of degrees of freedom between two users. Finally, we present novel combinations of two <b>interaction</b> <b>techniques</b> (Simple Virtual Hand and Ray-casting) ...|$|R
40|$|Abstract. This paper {{studies the}} virtual product model and <b>user</b> <b>interaction</b> <b>techniques</b> for virtual scene of 3 DSMAX, VRML and WEB {{technology}} {{to build a}} phone virtual display system, so that the user can navigate the phone structure from different perspectives, conduct phone local remote zoom, move and rotation and {{to learn more about}} the structure of their components, the various parts of the physical shape and assembly characteristics by assembling animation. The phone virtual display system treats VRML as the core, uses JavaScript largely and shows virtual product designing method...|$|R
40|$|Impromptu is {{a mobile}} audio device which uses {{wireless}} Internet Protocol (IP) to access novel computer-mediated voice communication channels. These channels show {{the richness of}} IP-based communication as compared to conventional mobile telephony, adding audio processing and storage in the network, and flexible, user-centered call control protocols. These channels may be synchronous, asynchronous, or eventtriggered, or even change modes {{as a function of}} other user activity. The demands of these modes plus the need to navigate with an entirely non-visual user interface are met with a number of audio-oriented <b>user</b> <b>interaction</b> <b>techniques...</b>|$|R
40|$|The {{objective}} of this minicourse is to give an overview of 3 D <b>User</b> <b>Interaction</b> <b>Techniques</b> {{on the basis of}} state-of-the-art technologies supported by multiple application examples. Areas influencing 3 D <b>User</b> <b>Interaction,</b> such as 3 D display systems, 3 D input devices and others are also considered. It is expected that attendees of the course {{will have the opportunity to}} get a comprehensive overview of existing approaches and future trends and will be able to immediately apply some ideas in their everyday work. Students, research scientists and developers, who would like to get acquainted with the area of 3 D <b>user</b> <b>interaction</b> in detail, learn about state-of the-art technologies, approaches and ongoing projects, and reflect upon new application ideas...|$|R
40|$|We {{present a}} new {{application}} for graph drawing {{in the context}} of graphical model-based system design, where manual placing of graphical items is still state-of-the-practice. The KIELER framework aims at improving this by oﬀering novel <b>user</b> <b>interaction</b> <b>techniques,</b> enabled by automatic layout of the diagrams. In this paper we present extensions of the well-known hierarchical layout approach, originally suggested by Sugiyama et al. to support port constraints, hyperedges, and compound graphs in order to layout diagrams of data ﬂow languages. A case study and experimental results show that our algorithm is well suited for application in interactive user interfaces. ...|$|R
40|$|Synthetic {{environment}} {{equipped with}} user interfaces intuitive for direct 3 D shape modification by non-designer stake-holders was proposed as a collaboration tool for design concept communication in dynamic prototyping of product design in {{early stages of}} the design process. After a survey of 3 D <b>user</b> <b>interaction</b> <b>techniques</b> for SE, a simple user interface with hierarchical visual menu was proposed and a proof-of-concept implementation of it was tested to be intuitive with experiment, which serves as a base for further study to verify the hypothetical benefits of SE aided dynamic prototyping in terms of communication efficiency and accuracy...|$|R
40|$|We {{previously}} introduced DIMPLE, {{a software}} environment allowing the run-time {{creation of a}} physically dynamic, haptically-enabled virtual scene using the Open Sound Control (OSC) protocol [18]. Object properties could be requested over OSC to allow modulation of parameters for sound synthesis or visualization. Examples were given of PureData patches that controlled such scenes, making haptic virtual musical instruments (VMI) accessible to a visual programming environment. However, scenes were limited to simple 3 D objects and haptic interaction was limited to pushing on objects and their constraints. Here we present various recent developments to enhance DIMPLE for more intricate haptic sensations and to allow {{a wider variety of}} <b>user</b> <b>interaction</b> <b>techniques.</b> ...|$|R
40|$|This paper {{presents}} a versatile- write once, use everywhere- approach of standardizing {{the development of}} three-dimensional <b>user</b> <b>interaction</b> <b>techniques.</b> In order to achieve a platform and application independent implementation of 3 D <b>interaction</b> <b>techniques</b> (ITs), we propose to implement the related techniques directly in the tracking middleware. Therefore a widely used tracking framework was extended by a Python binding to allow straight forward scripting of ITs. We cluster existing 3 D ITs, into those which can be fully, partly or not implemented in the tracking middleware. A number of examples demonstrate how various <b>interaction</b> <b>techniques</b> can quickly and efficiently be implemented in the middleware and are therefore fully independent of the underlying application. We hint at how this approach {{can be used to}} decouple menu system control from the application with the final goal to help establishing standards for 3 D interaction...|$|R
40|$|In rapidly {{changing}} pervasive environments that {{are rich in}} resources, there will be numerous opportunities for end users to carry out tasks by causing two or more devices or resources to interoperate together, often in unplanned ways. Empirical {{studies have shown that}} users find such interoperation tasks hard to manage using current <b>interaction</b> <b>techniques,</b> particularly via devices with small, resource-poor user interfaces. In addition, existing software architectures make such situations difficult to address in a principled, scaleable way. We propose that a new <b>user</b> <b>interaction</b> principle called Direct Combination- and the <b>user</b> <b>interaction</b> <b>techniques,</b> software architecture, and analysis techniques that support it- have significant potential to reduce the time, attention and mental effort required by users to carry out such tasks in ubiquitous environments. When Direct Combination is applied to spontaneous <b>interactions,</b> the <b>user</b> interface can be made relatively simple, and the amount of search required by the user to specify desired actions can be greatly reduced. We present Direct Combination (DC) and the new <b>interaction</b> <b>techniques</b> it gives rise to for pervasive environments...|$|R
40|$|Abstract: This work {{describes}} a methodology {{that supports the}} design and implementation of software modules, which represent the individual and collaborative three-dimensional interaction process phases. The presented methodology integrates three modeling approaches: Petri Nets, a collaborative manipulation model based on the combination of single <b>user</b> <b>interaction</b> <b>techniques</b> taxonomy, and object-oriented programming concepts. The combination of these elements allows for the description of interaction tasks, the sequence of interaction processes being controlled by Petri Nets with the codes generated automatically. By the integration of these approaches, the present work addresses not only the entire development cycle of both individual and collaborative three-dimensional interaction, but also the reuse of developed interaction blocks in new virtual environment projects...|$|R
30|$|Cooperative {{manipulation}} {{refers to}} the simultaneous manipulation of a virtual object by multiple users in an immersive virtual environment (VE). In this work, we present techniques for cooperative manipulation based on existing single-user techniques. We discuss methods of combining simultaneous user actions, based on the separation of degrees of freedom between two users, and the awareness tools used to provide the necessary knowledge of the partner activities during the cooperative interaction process. We also present a framework for supporting the development of cooperative manipulation techniques, {{which are based on}} rules for combining single <b>user</b> <b>interaction</b> <b>techniques.</b> Finally, we report an evaluation of cooperative manipulation scenarios, the results indicating that, in certain situations, cooperative manipulation is more efficient and usable than singleuser manipulation.|$|R
40|$|Current {{consumer}} media production is laborious, tedious, and produces unsatisfying results. To address this problem, Active Capture leverages media production knowledge, computer vision and audition algorithms, and <b>user</b> <b>interaction</b> <b>techniques</b> to automate direction and cinematography and thus enables the automatic production of annotated, high quality, reusable media assets. Active Capture {{is part of}} a new computational media production paradigm that transforms media production from a manual mechanical process into an automated computational one that can produce mass customized and personalized media integrating video of non-actors. The implemented system automates the process of capturing a non-actor performing two simple reusable actions (“screaming ” and “turning her head to look at the camera”) and automatically integrates those shots into various commercials and movie trailers...|$|R
40|$|In this paper, we {{describe}} simple techniques for object group manipulation, an important operation in <b>user</b> <b>interaction</b> with a Virtual Environment. All presented manipulation techniques exploit constraints to simplify <b>user</b> <b>interaction.</b> The <b>techniques</b> {{are based on}} how humans perceive groups and afford direct manipulation of such groups. Furthermore, we introduce two new intuitive ways to create {{a whole group of}} objects: drag-add and random drag-add. Finally, we present an evaluation of the presented techniques. 1...|$|R
40|$|Abstract. Rapid {{advances}} in graphics generation capabilities, display technologies, and <b>user</b> <b>interaction</b> technologies have broadened {{the range of}} possibilities for information display and exploration. Taking advantage of these developments, we can build virtual reality environments that support immersive and highly engaging ways to experience the battlefield or monitor activity on the battlefield. Successful use of these environments requires a careful assessment of {{the demands of the}} planning, decision-making, or problem-solving task, and matching these demands against the features possible in various virtual environment installations. Further, the resulting applications must provide <b>user</b> <b>interaction</b> <b>techniques</b> that allow the virtual world’s participants to build a cognitive map of the virtual battlefield, travel easily throughout the virtual world, and interact with battlefield objects. Virtual reality offers an intuitive and immersive experience for data-intensive decision-making tasks. In this chapter, we explore the issues involved in using such nontraditional display and <b>user</b> <b>interaction</b> environments for tasks relevant to the Army domain. We are particularly interested in matching the display and interaction environment {{to the demands of the}} particular task or problem-solving process. 2 Virtual Reality Environment...|$|R
40|$|Object Picking in Virtual Environments {{is one of}} {{the main}} tasks along with travel and manipulation. It deals with {{indicating}} objects and has been implemented with multiple techniques, each trying to improve performance measures such as accuracy, speed and task completion time. Various <b>interaction</b> <b>techniques</b> have been developed for interactive 3 D environments. This paper presents an effect of Visual Conflicts on 3 D object Picking Process in Non Immersive virtual Environment. There are a few fundamental pointing-based <b>user</b> <b>interaction</b> <b>techniques</b> for performing pick process. One of these techniques is based on ray casting, which makes selections by determining intersections of objects with the mouse click. In this paper I describe about an object picking algorithm with embedded coding and present a simple yet effective application-independent for 3 D input devices. We also discuss a differential geometry based surface constraint that can be applied to the 3 D cursor position for improving points matching...|$|R
40|$|We {{describe}} {{a demonstration of}} four novel <b>interaction</b> <b>techniques</b> for a cubic head-coupled 3 D display. The interactions illustrated include: viewing a static scene, navigating through a large landscape, playing with colliding objects inside a box, and stylus-based manipulation of objects. <b>Users</b> experience new <b>interaction</b> <b>techniques</b> for 3 D scene manipulation in a cubic display...|$|R
40|$|Abstract. A recent {{trend in}} modern high {{performance}} computing (HPC) system architectures employs “lean ” compute nodes running a lightweight operating system (OS). Certain {{parts of the}} OS {{as well as other}} system software services are moved to service nodes in order to increase performance and scalability. This paper examines the impact of this HPC system architecture trend on HPC “middleware ” software solutions, which traditionally equip HPC systems with advanced features, such as parallel and distributed programming models, appropriate system resource management mechanisms, remote application steering and <b>user</b> <b>interaction</b> <b>techniques.</b> Since the approach of keeping the compute node software stack small and simple is orthogonal to the middleware concept of adding missing OS features between OS and application, the role and architecture of middleware in modern HPC systems needs to be revisited. The result is a paradigm shift in HPC middleware design, where single middleware services are moved to service nodes, while runtime environments (RTEs) continue to reside on compute nodes...|$|R
40|$|We present Opportunistic Controls, a {{class of}} <b>user</b> <b>interaction</b> <b>techniques</b> for {{augmented}} reality (AR) applications that support gesturing on, and receiving feedback from, otherwise unused affordances already present in the domain environment. Oppor-tunistic Controls leverage characteristics of these affordances to provide passive haptics that ease gesture input, simplify gesture recognition, and provide tangible feedback to the user. 3 D wid-gets are tightly coupled with affordances to provide visual feed-back and hints about the functionality of the control. For example, a set of buttons is mapped to existing tactile features on domain objects. We describe examples of Opportunistic Controls that we have designed and implemented using optical marker tracking, combined with appearance-based gesture recognition. We present {{the results of a}} user study in which participants performed a simu-lated maintenance inspection of an aircraft engine using a set of virtual buttons implemented both as Opportunistic Controls and using simpler passive haptics. Opportunistic Controls allowed participants to complete their tasks significantly faster and were preferred over the baseline technique...|$|R
40|$|Interactive {{graphical}} editors {{provide good}} means for system design by {{the application of}} visual languages (VL). Unfortunately, applications of visual languages {{for the design of}} large systems seem not to be practicable because of the resulting huge and complicated graphical picture. This paper introduces a notion of Multi-View Design Environments (MVDEs) that try to overcome this problem. The principles of an MVDE can be applied to any VL that can be mapped to a graph-like internal representation. The so-called Graph Model is a basis for the data processing in an MVDE. It enables the applying of the Multi-View- Technique to large designs by their redundant decomposition. An MVDE allows the creation of views on the fly while keeping consistency among all views. Sophisticated <b>user</b> <b>interaction</b> <b>techniques,</b> including direct manipulation and hypertext navigation support, makes the MVDE easy to learn and to use. The paper finishes with a short description of the implementation of two projects which have used the MVDE-approach and a comparison to related work...|$|R
40|$|Graphical user {{interfaces}} have limitations {{in terms of}} the information bandwidth they provide between users and systems. This can impede the redesign of systems previously based on more physical media: information may be less appropriately displayed, and shared cognition between users can be reduced. However, in parallel with research on new <b>user</b> <b>interaction</b> <b>techniques,</b> a more systematic use of visual design techniques can relieve those limitations. This article explores some of those techniques and how they can be applied, through a design experiment. Virtuosi and DigiStrips are two user interface prototypes developed within a research program on air traffic control workstations, which make use of touch screens and served as a basis for research on the use of graphical design techniques in {{user interfaces}}. This paper describes the lessons learnt in that experience and argues that techniques such as animation, font design, careful use of graphical design techniques can augment the p [...] ...|$|R
40|$|As mobile {{devices are}} {{constantly}} evolving, new requirements are emerging regarding their screens: bigger displaying surfaces, enriched <b>user</b> <b>interactions,</b> enhanced robustness, adaptability, [...] . many requirements foldable screens {{would provide a}} way to fulfill. For example, they would allow to enjoy bigger displaying surfaces by simply using unfold operations, preserving the small dimensions these devices require in mobile situations while maximizing the display surface in fixed situations. In order to test and evaluate such device capabilities, prototypes must be built. However physical prototypes elaboration remains a complex, time consuming and expensive process due to the multiple new technologies involved. This thesis addresses this issue by allowing the creation of virtual rather that physical prototypes. It is achieved by developing UIFolder, a software tool allowing {{the definition of the}} prototype physical structure, folding capabilities and <b>user</b> <b>interactions</b> <b>techniques.</b> An extension of the Yoshizawa-Randlett origami notation system is proposed to graphically represents the prototype characteristics. Then the prototype definition is rendered in a virtual world allowing users to manipulate the virtual prototype. Thus this 3 D prototype representation can be used to test and evaluate its usage...|$|R
5000|$|From the <b>user's</b> perspective, an <b>interaction</b> <b>technique</b> {{is a way}} {{to perform}} a single {{computing}} task and can be informally expressed with user instructions or usage scenarios. For example, [...] "to delete a file, right-click on the file you want to delete, then click on the delete item".|$|R
40|$|This paper {{presents}} the concept, working prototype and design {{space of a}} two-handed, hybrid spatial user interface for minimally immersive desktop VR targeted at multi-dimensional visualizations. The user interface supports dual button balls (6 DOF isotonic controllers with multiple buttons) which automatically switch between 6 DOF mode (xyz + yaw,pitch,roll) and planar- 3 DOF mode (xy + yaw) upon contacting the desktop. The mode switch automatically switches a button ball‟s visual representation between a 3 D cursor and a mouse-like 2 D cursor while also switching the available <b>user</b> <b>interaction</b> <b>techniques</b> (ITs) between 3 D and 2 D ITs. Further, the small form factor of the button ball allows the user to engage in 2 D multi-touch or 3 D gestures without releasing and re-acquiring the device. We call the device and hybrid interface the HyFinBall interface which is an abbreviation for „Hybrid Finger Ball. ‟ We describe the user interface (hardware and software), the design space, as well as preliminary results of a formal user study. This is done in th...|$|R
40|$|After {{successfully}} integrating desktop-based concepts into drivers’ cockpits, it is {{now time}} to investigate options towards integrating 3 D <b>user</b> <b>interaction</b> <b>techniques.</b> Such <b>interaction</b> <b>techniques</b> are becoming increasingly important to handle the ever-growing wealth of information that cars can provide to their drivers regarding traffic, road and car conditions, as well as entertainment and communication facilities. However, since drivers have to focus primarily on the road, they cannot lend their full attention to the user interface. Thus, common requirements and rules for interface design need to be extended. In this paper, we present and discuss the implication of such constraints on 3 D user interfaces, listing some upcoming options for integrating 3 D interfaces into cars. In addition, we present the concept that cars are no longer mainly mechanical objects. Rather they are complex computer systems with very particular input and output devices and mobile functionality. Following this new view, we reconsider familiar control devices of cars, such as the steering wheel and the gas and brake pedals as input and output devices to a very special three-dimensional computer application with strong connections to the real environment. In this light, such interaction devices serve as valuable examples of well-designed 3 D user interfaces for computer-based navigation...|$|R
40|$|Abstract. The use of {{immersive}} virtual {{environments for}} industrial applications has now reached maturity {{in a number}} of areas including oil and gas exploration and the visualisation of complex scientific data sets. However, there are still a number of applications which benefit greatly from the use of three-dimensional immersive virtual environments, but technological limitations, <b>user</b> <b>interaction</b> <b>techniques</b> and cost still prohibit general acceptance of this technology for use as an everyday tool. This paper presents the use of instrumented objects in immersive virtual environments to aid haptic feedback for applications including, but not limited to, off-line robot programming, assembly planning, assembly training and teleoperation of remotely operated vehicles. The paper describes the motivation behind the use of instrumented objects for manipulation tasks in immersive virtual environments and presents findings from previous research which compares interaction tasks between a mouse and keyboard, six degree of freedom interaction device, an instrumented object and the real task. Findings show that using an instrumented object for object manipulation offers significantly improved performance over conventional <b>interaction</b> <b>techniques.</b> Implications of this approach including the advantages and limitations are discussed. The paper presents this work in an application context of programming robotic devices via immersive virtual environments and discusses the benefits of this approach and identifies where similar successes can be achieved...|$|R
40|$|Interactive visual {{analytic}} systems {{enable users}} to discover insights from complex data. Users can express and test hypotheses via <b>user</b> <b>interaction,</b> leveraging their domain expertise and prior knowledge to guide and steer the analytic {{models in the}} system. For example, semantic <b>interaction</b> <b>techniques</b> enable systems {{to learn from the}} <b>user's</b> <b>interactions</b> and steer the underlying analytic models based on the user's analytical reasoning. However, an open challenge is how to not only steer models based on the dimensions or features of the data, but how to add dimensions or attributes to the data based on the domain expertise of the user. In this paper, we present a technique for inferring and appending dimensions onto the dataset based on the prior expertise of the user expressed via <b>user</b> <b>interactions.</b> Our <b>technique</b> enables <b>users</b> to directly manipulate a spatial organization of data, from which both the dimensions of the data are weighted, and also dimensions created to represent the prior knowledge the user brings to the system. We describe this technique and demonstrate its utility via a use case...|$|R
