638|2152|Public
25|$|In November 2005 the OpenUsability {{project in}} {{cooperation}} with the Berlin-based Relevantive AG conducted a <b>usability</b> <b>test</b> of the German Wikipedia. The study focused on finding information and included a set of recommendations to change the MediaWiki interface. In February 2006, the open usability project led a second test which focused on the experience of new editors. The reports were published in English.|$|E
25|$|Usability: Aspects of the Drupal 6 {{administration}} interface were {{confusing and}} intimidating to some, particularly for new administrators. According to Dries Buytaert, Drupal 7 addressed 90% {{of the problems}} identified by usability tests conducted at the Universities of Minnesota and Baltimore. To achieve this, Acquia (the company founded by the project lead of Drupal) hired user experience designer Mark Boulton {{to work with the}} Drupal community to design an improved user interface for Drupal's administration interface. The majority of his team's design work has been implemented by the community in Drupal 7. The 2011 <b>usability</b> <b>test</b> results from the University of Minnesota Office of Information Technology show that all of the major usability problems identified in Drupal 6 are either vastly improved or non-existent in Drupal 7. However, some new usability problems were identified. Since the release of Drupal 7 there are now various distributions and applications to enhance the Back-end Usability of Drupal such as Drupal Gardens, Open Enterprise and Mitkom Builder.|$|E
50|$|The three click {{rule has}} been {{challenged}} by <b>usability</b> <b>test</b> results, which {{have shown that the}} number of clicks needed to access the desired information affects neither user satisfaction, nor success rate.|$|E
40|$|This paper investigates {{different}} <b>usability</b> <b>testing</b> methods, {{and details}} a final <b>usability</b> <b>testing</b> procedure for the PolyXpress application. <b>Usability</b> <b>testing</b> varies greatly and {{depends on the}} size, scope, and budget of a project. Jakob Nielsen, a usability expert, says <b>usability</b> <b>testing</b> generally involves finding test users, asking the users to perform representative tasks with the design, and observing/documenting what happens. There are three main types of <b>usability</b> <b>testing</b> methods: traditional, discount, and going-out-of-business. The methods range in scale, complexity, and cost. This paper specifically details the unique combination of the three main methods that should {{be used as the}} <b>usability</b> <b>testing</b> procedure for PolyXpress. PolyXpress <b>usability</b> <b>testing</b> should consist of recruiting five to fifteen test users, who are lead by a facilitator reading from a script, and should be documented using a GoPro camera with a Head Strap Mount, screen capture video using Display Recorder, and written notes. This paper also includes a list of recommendations for PolyXpress from preliminary <b>usability</b> <b>testing...</b>|$|R
40|$|Usability {{is one of}} {{the most}} {{important}} aspects of Information Technology. Usability plays a vital role in this industry, where organizations thrive to ensure utmost satisfaction of their end-users in regard to the experience of using their product. The systems may be a website or a software application. To measure user satisfaction, the method of <b>usability</b> <b>testing</b> can be performed. Performing <b>usability</b> <b>testing</b> gives a clear picture of difficulties that would be faced by potential target users. There are different types of <b>usability</b> <b>testing</b> such as Task-based <b>usability</b> <b>testing,</b> open ended <b>usability</b> <b>testing,</b> remote <b>usability</b> <b>testing</b> etc. The important point here is about deciding upon the most appropriate type of testing technique to get the accurate user satisfaction level. This study is mainly focused to answer the following research question: What is the relationship between the task-based and open ended <b>usability</b> <b>testing,</b> in terms of measuring satisfaction? System Usability Scale (SUS) has been used to measure the satisfaction of the users in this study. For this we used two websites performing task-based <b>usability</b> <b>testing</b> and open ended <b>usability</b> <b>testing</b> respectively. This study had involved twenty eight different participants. Participants are divided into two groups, one group to perform open ended <b>usability</b> <b>testing</b> and another for task-based <b>usability</b> <b>testing</b> for both the websites. This study has produced following results; Open-ended testing tended to produce higher SUS-ratings for the tested system. The results in this study showed that users performing open-ended <b>usability</b> <b>testing</b> gave positive responses for both the websites in terms of user satisfaction. Open-ended <b>usability</b> <b>testing</b> is an exploratory testing, where the testing is based on different aspects such as user interface of the system, design etc. Task-based <b>usability</b> <b>testing</b> is goal based where users have to complete the given task without fail. This method drew lower scores when compared to open-ended <b>usability</b> <b>testing</b> for the tested systems from the attained results. Nevertheless that task-based testing attained lower SUS scores, it is fairly straight forward than open-ended testing to measure efficiency and effectiveness. The above results have been discussed in detail. This study has finally concluded that to measure the usability of a system it is recommended to practice both the open-ended and task-based <b>usability</b> <b>testing</b> techniques...|$|R
40|$|This study {{examines}} how librarians are experiencing <b>usability</b> <b>testing</b> {{and how their}} observations are influencing library instruction. A survey of instruction librarians illustrates how <b>usability</b> <b>testing</b> and library instruction are connected. Survey results prove instruction librarians are involved in <b>usability</b> <b>testing.</b> Furthermore, their participation in usability studies has led instruction librarians to alter their instructional methods. An overwhelming majority changed one or more instructional tools {{as a result of}} <b>usability</b> <b>testing,</b> and many reported creating new instructional resources. The authors add their own insights as both instruction librarians and participants in <b>usability</b> <b>testing...</b>|$|R
5000|$|Synchronous {{usability}} testing methodologies involve {{video conferencing}} or employ remote application sharing {{tools such as}} WebEx. WebEx and GoToMeeting are {{the most commonly used}} technologies to conduct a synchronous remote <b>usability</b> <b>test.</b> However, synchronous remote testing may lack the immediacy and sense of [...] "presence" [...] desired to support a collaborative testing process. Moreover, managing inter-personal dynamics across cultural and linguistic barriers may require approaches sensitive to the cultures involved. Other disadvantages include having reduced control over the testing environment and the distractions and interruptions experienced by the participants' in their native environment. One of the newer methods developed for conducting a synchronous remote <b>usability</b> <b>test</b> is by using virtual worlds.|$|E
50|$|Setting up a <b>usability</b> <b>test</b> {{involves}} carefully {{creating a}} scenario, or realistic situation, wherein the person performs {{a list of}} tasks using the product being tested while observers watch and take notes (dynamic verification). Several other test instruments such as scripted instructions, paper prototypes, and pre- and post-test questionnaires are also used to gather feedback on the product being tested (static verification). For example, to test the attachment function of an e-mail program, a scenario would describe a situation where a person needs to send an e-mail attachment, and ask {{him or her to}} undertake this task. The aim is to observe how people function in a realistic manner, so that developers can see problem areas, and what people like. Techniques popularly used to gather data during a <b>usability</b> <b>test</b> include think aloud protocol, co-discovery learning and eye tracking.|$|E
50|$|In November 2005 the OpenUsability {{project in}} {{cooperation}} with the Berlin-based Relevantive AG conducted a <b>usability</b> <b>test</b> of the German Wikipedia. The study focused on finding information and included a set of recommendations to change the MediaWiki interface. In February 2006, the open usability project led a second test which focused on the experience of new editors. The reports were published in English.|$|E
40|$|The paper motivates {{the need}} to acquire methodological {{knowledge}} for involving children as <b>test</b> users in <b>usability</b> <b>testing.</b> It introduces a methodological framework for delineating comparative assessments of <b>usability</b> <b>testing</b> methods for children participants. This framework consists in three dimensions: (1) assessment criteria for <b>usability</b> <b>testing</b> methods, (2) characteristics describing <b>usability</b> <b>testing</b> methods and, finally, (3) characteristics of children that may impact upon {{the process and the}} result of <b>usability</b> <b>testing.</b> Two comparative studies are discussed {{in the context of this}} framework along with implications for future research...|$|R
40|$|The {{purposes}} of this article are to describe <b>usability</b> <b>testing</b> and introduce designs and methods of <b>usability</b> <b>testing</b> research as it relates to upper-limb prosthetics. This article defines usability, describes usability research, discusses research approaches to and designs for <b>usability</b> <b>testing,</b> and highlights a variety of methodological considerations, including sampling, sample size requirements, and <b>usability</b> metrics. <b>Usability</b> <b>testing</b> is compared with other types of study designs used in prosthetic research...|$|R
40|$|<b>Usability</b> <b>testing</b> is {{a dynamic}} process {{that can be used}} {{throughout}} the process of developing interactive multimedia software. The purpose of <b>usability</b> <b>testing</b> is to find problems and make recommendations to improve the utility of a product during its design and development. For developing effective interactive multimedia software, dimensions of <b>usability</b> <b>testing</b> were classified into the general categories of: learnability; performance effectiveness; flexibility; error tolerance and system integrity; and user satisfaction. In the process of <b>usability</b> <b>testing,</b> evaluation experts consider the nature of users and tasks, tradeoffs supported by the iterative design paradigm, and real world constraints to effectively evaluate and improve interactive multimedia software. Different methods address different purposes and involve a combination of user and <b>usability</b> <b>testing,</b> however, <b>usability</b> practitioners follow the seven general procedures of <b>usability</b> <b>testing</b> for effective multimedia development. As the knowledge about <b>usability</b> <b>testing</b> grows, evaluation experts will be able to choose more effective and efficient methods and techniques that are appropriate to their goals...|$|R
5000|$|In {{usability}} engineering, a {{focus group}} is a survey method to collect the views of users on software or a website. This marketing method {{can be applied to}} computer products to better understand the motivations of users and their perception of the product. Unlike other methods of ergonomics, focus group implies several participants: users or future users of the application. The focus group can only collect subjective data, not objective data {{on the use of the}} application as the <b>usability</b> <b>test</b> for example.|$|E
50|$|In 1979, {{she founded}} the Document Design Centre at the American Institutes for Research in Washington D.C. and {{remained}} there as Director for thirteen years. The {{aim of the}} DDC was to streamline workplace documents for government agencies and major private companies by developing online document template models. She founded {{one of the first}} independent <b>usability</b> <b>test</b> laboratories in the United States of America in 1985, monitoring users who would test new user interfaces and document templates for the multinationals such as IBM and Hewlett-Packard among others.|$|E
50|$|Usability {{testing is}} the most common method used by {{designers}} to test their designs. The basic idea behind conducting a <b>usability</b> <b>test</b> is to check whether the design of a product or brand works well with the target users. While carrying out usability testing, two things are being tested for: Whether the design of the product is successful and if it is not successful, how can it be improved. While designers are testing, they are testing the design and not the user. Also, every design is evolving. The designers carry out usability testing at every stage of the design process.|$|E
40|$|This paper {{discusses}} a {{user research}} method {{the authors have}} refined over several years: field <b>usability</b> <b>testing.</b> Field <b>usability</b> <b>testing</b> combines techniques from traditional laboratory <b>usability</b> <b>testing</b> and condensed contextual inquiry, itself an adaptation of traditional contextual inquiry methods. The authors describe two approaches or models of field usability testing: ethnographic and structured. Three case histories illustrate the method, giving examples of the ethnographic model and the structured model. Keywords: user research, field research, <b>usability</b> <b>testing,</b> ethnograph...|$|R
40|$|System {{usability}} can {{be measured}} through various methods. One {{of the more important}} and widely -employed techniques is &#x 2032;usability testing&#x 2032;, where asks, number of users, evaluators, and other factors are the main -elements. This paper reviews <b>usability</b> <b>testing</b> together with current issues that can influence <b>usability</b> <b>testing</b> results, both negatively and positively. It also reviews web <b>usability</b> <b>testing.</b> In addition, in this paper, <b>usability</b> <b>testing</b> in the future is considered in order that improvements may be made...|$|R
40|$|Abstract- This paper {{stresses}} the importance of <b>usability</b> <b>tests</b> in user acceptance, performance, and satisfaction. We refer to methods, techniques, and evaluators used in <b>usability</b> <b>testing.</b> Then, we focus on the importance of planning a test and conducting it, attending to ethical issues. Finally, we describe <b>usability</b> <b>tests</b> conducted to an hyperdocument...|$|R
50|$|It {{is worth}} noting that Nielsen does not {{advocate}} stopping after a single test with five users; his point is that testing with five users, fixing the problems they uncover, and then testing the revised site with five different users is a better use of limited resources than running a single <b>usability</b> <b>test</b> with 10 users. In practice, the tests are run once or twice per week during the entire development cycle, using three to five test subjects per round, and with the results delivered within 24 hours to the designers. The number of users actually tested {{over the course of the}} project can thus easily reach 50 to 100 people.|$|E
5000|$|Subjects-in-tandem (also called co-discovery) is the {{pairing of}} {{subjects}} in a <b>usability</b> <b>test</b> to gather important {{information on the}} ease of use of a product. Subjects tend to discuss the tasks they have to accomplish out loud and through these discussions observers learn where the problem areas of a design are. To encourage co-operative problem-solving between the two subjects, and the attendant discussions leading to it, the tests can be designed to make the subjects dependent on each other by assigning them complementary areas of responsibility (e.g. for testing of software, one subject may be {{put in charge of}} the mouse and the other of the keyboard.) ...|$|E
50|$|Asynchronous methodologies include {{automatic}} {{collection of}} user's click streams, user logs of critical incidents that occur while {{interacting with the}} application and subjective feedback on the interface by users. Similar to an in-lab study, an asynchronous remote <b>usability</b> <b>test</b> is task-based and the platforms allows researchers to capture clicks and task times. Hence, for many large companies, this allows researchers to better understand visitors' intents when visiting a website or mobile site. Additionally, this style of user testing also provides an opportunity to segment feedback by demographic, attitudinal and behavioral type. The tests are {{carried out in the}} user's own environment (rather than labs) helping further simulate real-life scenario testing. This approach also provides a vehicle to easily solicit feedback from users in remote areas quickly and with lower organizational overheads. In recent years, conducting usability testing asynchronously has also become prevalent and allows testers to provide feedback in their free time and from the comfort of their own home.|$|E
50|$|Simply {{gathering}} {{opinions on}} an object or document is market research or qualitative research rather than <b>usability</b> <b>testing.</b> <b>Usability</b> <b>testing</b> usually involves systematic observation under controlled conditions {{to determine how}} well people can use the product. However, often both qualitative and <b>usability</b> <b>testing</b> are used in combination, to better understand users' motivations/perceptions, {{in addition to their}} actions.|$|R
40|$|Purpose of thesis was to {{conclude}} {{if there is}} an obvious gain in <b>testing</b> the <b>usability</b> of ecommerce website in foreign language. Hypothesis was, that <b>usability</b> <b>testing</b> in foreign language might show usability issues on the website, which wouldn't be found otherwise. We started with presenting <b>usability</b> <b>testing</b> methods and choosing which methods to use in this thesis. We have decided to perform laboratory <b>usability</b> <b>testing</b> in combination with surveys. In the next chapter we presented the problem of using ecommerce websites in foreign language and explained the process and decisions made regarding the testing itself. In the chapter titled "Testing Results" we have presented the results of <b>usability</b> <b>testing</b> gained by tests and surveys. In the last chapter we have suggested improvements based on those findings. We have found that <b>usability</b> <b>testing</b> in foreign language can have benefits as opposed to <b>usability</b> <b>testing</b> in native language only...|$|R
40|$|AbstractConventional <b>usability</b> <b>testing</b> {{is usually}} {{conducted}} with several individual participants. In recent years, however, group <b>usability</b> <b>testing</b> is gradually gaining attention. Such approach involves several-to-many participants performing tasks simultaneously, with one to several testers observing and {{interacting with the}} participants. This approach is able to generate many useful data within {{a short period of}} time. In light with the need to further improve the approach, this paper presents a modified version of a group <b>usability</b> <b>testing</b> and how it can be feasibly used to evaluate the usability of a non-immersive virtual reality-based learning environment. The proposed modified group approach aims to minimize the possibility of data loss during the <b>usability</b> <b>testing</b> process. The effectiveness and efficiency of this modified method was compared to the original approach of group <b>usability</b> <b>testing.</b> The results indicate that the modified group <b>usability</b> <b>testing</b> is more effective and efficient than the original approach as it can collect more critical and significant data with lesser time, cost and effort consumption...|$|R
5000|$|Usability: Aspects of the Drupal 6 {{administration}} interface {{were seen}} to be confusing and intimidating to some, particularly for new administrators. According to Dries Buytaert, Drupal 7 addressed 90% of the problems identified by usability tests conducted at the Universities of Minnesota and Baltimore. To achieve this, Acquia (the company founded by the project lead of Drupal) hired user experience designer Mark Boulton {{to work with the}} Drupal community to design an improved user interface for Drupal's administration interface. The majority of his team's design work has been implemented by the community in Drupal 7. The 2011 <b>usability</b> <b>test</b> results from the University of Minnesota Office of Information Technology show that all of the major usability problems identified in Drupal 6 are either vastly improved or non-existent in Drupal 7. However, some new usability problems were identified. Since the release of Drupal 7 there are now various distributions and applications to enhance the Back-end Usability of Drupal such as Drupal Gardens, Open Enterprise and Mitkom Builder.|$|E
50|$|He {{stayed on}} to manage Windows 2.0 and shifted the {{interface}} back to overlapping windows. It became the platform {{for one of}} the first significant applications to run on Windows, Microsoft Excel. By then a company called Aldus had also created a Windows version of their popular page layout product, Pagemaker, which took advantage of a printer driver for the HP LaserJet printer. At the time most printers were dot matrix. By that time, there was a Joint Development Agreement with IBM (who still rejected licensing Windows) where Microsoft and IBM would work together on a new OS called OS/2 that would include its own window manager called Presentation Manager. The UIs for Windows and OS/2 Presentation Manager had to be kept in sync so users could move smoothly between them and Tandy became the liaison to negotiating features between the products {{in an effort to keep}} them operational compatible. When Windows 2.0 shipped in November 1987, Tandy proposed the creation of a new group at Microsoft that would do usability testing, app interface design, publish UI guidelines, and create prototypes of new UIs. Subsequently he founded Microsoft's first <b>usability</b> <b>test</b> labs and wrote most of the guidelines for designing Windows applications that were published by Microsoft Press. he was also a featured speaker on application interface design at early user interface conferences.|$|E
40|$|For {{more than}} a decade, the number of <b>usability</b> <b>test</b> {{participants}} {{has been a major}} theme of debate among usability practitioners and researchers keen to improve <b>usability</b> <b>test</b> performance. This paper provides evidence suggesting that the focus be shifted to task coverage instead. Our data analysis of nine commercial <b>usability</b> <b>test</b> teams participating in the CUE- 4 study revealed no significant correlation between the percentage of problems found or of new problems and number of test users, but correlations of both variables and number of user tasks used by each usability team were significant. The role of participant recruitment on <b>usability</b> <b>test</b> performance and future research directions are discussed...|$|E
40|$|<b>Usability</b> <b>testing</b> {{has a long}} history. In {{its early}} form, it was {{conducted}} with many individual participants much like traditional research experiments. With the advent of discount usability engineering techniques, fewer participants were required (5 - 7 versus 30 - 50) and protocols were simplified. The evolution from “many to few ” in <b>usability</b> <b>testing</b> has become the standard in formative testing. What is the next tool in our toolbox? This paper uses a case study to introduce a formative method called “group <b>usability</b> <b>testing.</b> ” It involves several to many participants individually, but simultaneously, performing tasks, with one to several testers observing and interacting with participants. The idea for group <b>usability</b> <b>testing</b> arose as an answer to limited time resources {{and the availability of}} many users gathered together in one place. Data characteristics, benefits, and drawbacks of group <b>usability</b> <b>testing</b> are discussed. Additionally, this method is compared/contrasted with individual <b>usability</b> <b>testing,</b> co-discovery, task-based focus groups, and cooperative <b>usability</b> <b>testing.</b> Keywords Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Copyright 2006, UPA. Group <b>usability</b> <b>testing,</b> collaborative design and evaluation technique, <b>usability</b> <b>testing,</b> formative <b>usability</b> method 13...|$|R
50|$|<b>Usability</b> <b>testing</b> {{focuses on}} {{measuring}} a human-made product's capacity {{to meet its}} intended purpose. Examples of products that commonly benefit from <b>usability</b> <b>testing</b> are foods, consumer products, web sites or web applications, computer interfaces, documents, and devices. <b>Usability</b> <b>testing</b> measures the <b>usability,</b> or ease of use, of a specific object or set of objects, whereas general human-computer interaction studies attempt to formulate universal principles.|$|R
40|$|Storm, J., & Kostons, D. (2009). Usability at the CELSTEC MediaLab. Presentation {{about the}} <b>usability</b> <b>testing,</b> <b>usability</b> {{research}} and User Centered Design {{conducted at the}} CELSTEC MediaLab. July, 7, 2009, Heerlen, The Netherlands: Open University of the Netherlands. Presentation about the <b>usability</b> <b>testing,</b> <b>usability</b> research and User Centered Design conducted at the CELSTEC MediaLab...|$|R
40|$|Abstract: {{conducting}} sufficient <b>usability</b> <b>test</b> requires {{planning and}} attention to the evaluation details. In common, <b>usability</b> <b>test</b> methods for software take into considerations, planning usability questions, selecting a representative sample and recruiting participants, and preparing the test materials and actual test environment. Several issues were reported while choosing the suitable <b>usability</b> <b>test</b> method for mobile applications, especially for indicating the way for conducting the test. Therefore, this paper aims to demonstrate the most used testing methods for the evaluation purposes of mobile applications...|$|E
30|$|CH {{developed}} the methodology and the mathematical models, programmed VAO Checker, {{carried out the}} <b>usability</b> <b>test,</b> analyzed the results and drafted the manuscript. MH assisted the literature review and the <b>usability</b> <b>test.</b> SC offered suggestion and guidance to the research. All authors read and approved the final manuscript.|$|E
40|$|Do {{you want}} to improve the {{usability}} of your library website, but {{feel that it is}} too difficult, time-consuming, or expensive? Usability Testing: A Practical Guide for Librarians will teach you how {{to make the case for}} usability testing, define your audience and their goals, select a usability testing method appropriate for your particular context, plan for an in-house <b>usability</b> <b>test,</b> conduct an effective in-house <b>usability</b> <b>test,</b> analyze <b>usability</b> <b>test</b> results, and create and implement a plan for ongoing, systematic usability testing. Step-by-step instructions, along with a myriad of examples...|$|E
40|$|Even if {{usability}} {{is becoming}} a hot topic both in industry and academic research, very little {{work has been done}} on the modeling and formalizing of empirical studies, in particular on <b>usability</b> <b>testing</b> processes. Process modeling is fundamental, not only for process maturity, standardization, and customization to different types of empirical studies, but also for the automation and integration of <b>usability</b> <b>testing</b> in the mainstream of software development lifecycle. Moreover, there is a big gap between <b>usability</b> <b>testing</b> practices and <b>usability</b> tools. Most of current tools used in <b>usability</b> <b>testing</b> cover only a few steps and activities of the testing process. We lack integrated toolsets that can support the whole <b>usability</b> <b>testing</b> process. Within this context, we reviewed the existing <b>usability</b> <b>testing</b> process models while highlighting their limitations. We then proposed a ten-step, well-defined and structured process that combines all strengths and overcomes the drawbacks of current processes. We also demonstrated how such a well-defined process can be embedded into a web-based wizard, WizUse, to assist usability professionals managing, customizing, and conducting <b>usability</b> <b>tests.</b> To validate our approach, we also conducted a series of tests to demonstrate that the proposed process model and WizUse are a suitable approach {{to bridge the gap between}} current usability practices and tool...|$|R
40|$|The aim {{with this}} thesis is to {{investigate}} two different {{methods used to}} <b>test</b> <b>usability</b> on web-sites to determine {{the advantages and disadvantages}} of two methods and compare the results. The two selected methods are the think aloud method and asynchronous remote method. Re-mote <b>usability</b> <b>testing</b> has existed for more than ten years but the possibilities with this meth-od needs to be explored more. To achieve the purpose of this thesis a case study was conducted on an e-commerce site and for the online remote <b>usability</b> <b>testing</b> software was used. The results of this study show that think aloud <b>usability</b> <b>testing</b> encounter more issues with the website than the remote <b>usability</b> <b>testing...</b>|$|R
40|$|This paper {{presents}} part of {{the research}} result from the project eCAALYX – Enhanced Complete Ambient Assisted Living Experiment in which the authors {{were responsible for the}} design and evaluation of a TV-based user interface for older adults with chronic conditions. The curiosity to understand the impact of performing <b>usability</b> <b>testing</b> had in older adults led the authors to evaluate the feelings of happiness and fatigue before and after <b>usability</b> <b>testing.</b> This evaluation was conducted five times in a set of eight <b>usability</b> <b>tests</b> performed over thirteen weeks. Results show that these two feelings improved in the <b>usability</b> <b>tests.</b> In this paper, the authors develop an understanding of what may justify this improved self-sense of happiness and fatigue; following principles that favor older adults wellness during <b>usability</b> <b>testing</b> may be the main reason for these results...|$|R
