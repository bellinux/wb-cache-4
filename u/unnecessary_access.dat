20|74|Public
50|$|The SNS is {{characterized}} as being national, universal, general and free. It is national {{as it should}} be provided nationwide, although presently it still only covers Continental Portugal. It is universal as all Portuguese citizens and foreign residents have access to it. It is general as it encompasses the whole range of healthcare, including the health surveillance and promotion, the disease prevention, the diagnosis and treatment of patients and the social and medical rehabilitation. It is free, as the system is publicly funded, with the health services being tendentiously free of charge for the users. However, some fees are charged, not in order to finance the system but serving mainly to moderate and filter <b>unnecessary</b> <b>access</b> to the services (e.g. to avoid that a person with a minor injury go to the hospital's emergency department instead of going to a local primary health care unit).|$|E
40|$|The {{number of}} {{information}} resource security incidents {{and the resulting}} cost of business disruption and service restoration at Texas A&M University-Kingsville (TAMUK) continue to escalate. Implementing security procedures, blocking <b>unnecessary</b> <b>access</b> to networks and computers, improving user security awareness, and early detection and mitigation of informatio...|$|E
40|$|The {{number of}} {{computer}} security incidents {{and the resulting}} cost of business disruption and service restoration continue to escalate. By implementing appropriate security procedures, blocking <b>unnecessary</b> <b>access</b> to networks, computers and applications, improving user security awareness, and early detection and mitigation of security incidents {{are some of the}} actions tha...|$|E
40|$|A {{distributed}} text database {{consists of}} multiple individual text collections. When a query is posed to a distributed text database, signi cant computational resources {{can be saved}} by identi ng the individual collections that {{are the most likely}} to contain answers, as <b>unnecessary</b> <b>accesses</b> to the other collections will be avoided. In this paper we explore the potential of one approach to selecting collections: ranking them according to the content of each collection's lexicon. We outline principles on which such ranking might be based and how its performance can be evaluated. Experiments with two sets of text collections show that use of lexicons to select collections can be eective, but depends on how performance is measured...|$|R
40|$|Tile-based {{rasterization}} {{was recently}} proposed {{as a solution}} for en-abling three dimensional computer graphics on low-power mobile devices. In tile-based rasterization the screen is decomposed into independent parts, called tiles, that are processed sequentially by a simple embedded graphics accelerator. By keeping these tiles small, on-chip memory {{can be used to}} store intermediate rasterization val-ues. This reduces a major source of power dissipation when com-pared to a conventional graphics pipeline, the external traffic be-tween the rasterization hardware and the framebuffer. In the GRAphics AcceLerator (GRAAL) project a tile-based raster-ization solution utilizing a scenebuffer resident in external memory was proposed. The scenebuffer contains the rasterization instruc-tions needed to process all the screen tiles and is reused for the ras-terization of every tile. Several sorting algorithms were introduced to prevent <b>unnecessary</b> <b>accesses</b> to the graphics accelerator...|$|R
5000|$|Bloom {{proposed}} the technique for applications where {{the amount of}} source data would require an impractically large amount of memory if [...] "conventional" [...] error-free hashing techniques were applied. He gave {{the example of a}} hyphenation algorithm for a dictionary of 500,000 words, out of which 90% follow simple hyphenation rules, but the remaining 10% require expensive disk accesses to retrieve specific hyphenation patterns. With sufficient core memory, an error-free hash could be used to eliminate all unnecessary disk accesses; on the other hand, with limited core memory, Bloom's technique uses a smaller hash area but still eliminates most <b>unnecessary</b> <b>accesses.</b> For example, a hash area only 15% of the size needed by an ideal error-free hash still eliminates 85% of the disk accesses - an 85-15 form of the Pareto principle.|$|R
40|$|The {{number of}} {{computer}} security incidents {{and the resulting}} cost of business disruption and service restoration continue to escalate. Some actions {{that can be taken}} to reduce the risk and drive down the cost of security incidents are implementing solid security procedures, blocking <b>unnecessary</b> <b>access</b> to networks and computers, improving user security awareness, and earl...|$|E
40|$|In {{light of}} {{worldwide}} concerns regarding the emission of green house gases, researchers {{have begun to}} look at {{ways to reduce the}} carbon footprint of Information Technology (IT) infrastructure. This paper contributes to this effort by proposing a scheduler that switches off <b>unnecessary</b> <b>Access</b> Points (APs), up to 88 % of deployed APs, while ensuring end-users continue to enjoy good coverage and performance...|$|E
40|$|In a multi-tenanted {{collaboration}} system {{there is}} the need to monitor traffic within the system to avoid congestion and encourage collaborations to maintain adequate resources. This gave rise to the need for adequate pricing to minimize <b>unnecessary</b> <b>access,</b> while encouraging visitors to the network. The researchers believe that inasmuch as there is need to restrain access through pricing, in the Educational collaboration system, there is need to encourage more genuine visitors. Since the more they stay in the network, the merrier. Hence, the need to settle these costs offline than charging the visitors. The writer recommended syndication between the ultimate beneficiaries, such as governments, individuals and corporate bodies, etc...|$|E
40|$|International audienceThe Particle-in-Cell (PIC) method allows solving partial {{differential}} equation through simulations, with important applications in plasma physics. To simulate thousands {{of billions of}} particles on clusters of multicore machines, prior work has proposed hybrid algorithms that combine domain decomposition and particle decomposition with carefully optimized algorithms for handling particles processed on each multicore socket. Regarding the multicore processing, existing algorithms either suffer from suboptimal execution time, due to sorting operations or use of atomic instructions, or suffer from suboptimal space usage. In this paper, we propose a novel parallel algorithm for two-dimensional PIC simulations on multicore hardware that features asymptotically-optimal memory consumption, and does not perform <b>unnecessary</b> <b>accesses</b> to the main memory. In practice, our algorithm reaches 65 % of the maximum bandwidth, and shows excellent scalability on the classical Landau damping and two-stream instability test cases...|$|R
40|$|Application-specific instruction-set {{extensions}} (custom instructions) help embedded processors achieve higher performance. Most custom instructions offering significant performance benefit re-quire multiple input operands. Unfortunately, RISC-style embedded processors {{are designed}} to sup-port at most two input operands per instruction. This data bandwidth problem {{is due to the}} limited number of read ports in the register file per instruction as well as the fixed-length instruction encod-ing. We propose to overcome this restriction by exploiting the data forwarding feature present in processor pipelines. With minimal modifications to the pipeline and the instruction encoding along with cooperation from the compiler, we can supply up to two additional input operands per custom instruction. Experimental results indicate that our approach achieves 87 â€“ 100 % of the ideal perfor-mance limit for standard benchmark programs. Additionally, our scheme saves 25 % energy on an average by avoiding <b>unnecessary</b> <b>accesses</b> to the register file. ...|$|R
40|$|We {{introduce}} the Coarse-Grain Out-of-Order (CG- OoO) general purpose processor designed to achieve close to In-Order processor energy while maintaining Out-of-Order (OoO) performance. CG-OoO is an energy-performance proportional general purpose architecture that scales {{according to the}} program load. Block-level code processing {{is at the heart}} of the this architecture; CG-OoO speculates, fetches, schedules, and commits code at block-level granularity. It eliminates <b>unnecessary</b> <b>accesses</b> to energy consuming tables, and turns large tables into smaller and distributed tables that are cheaper to access. CG-OoO leverages compiler-level code optimizations to deliver efficient static code, and exploits dynamic instruction-level parallelism and block-level parallelism. CG-OoO introduces Skipahead issue, a complexity effective, limited out-of-order instruction scheduling model. Through the energy efficiency techniques applied to the compiler and processor pipeline stages, CG-OoO closes 64 % of the average energy gap between the In-Order and Out-of-Order baseline processors at the performance of the OoO baseline. This makes CG-OoO 1. 9 x more efficient than the OoO on the energy-delay product inverse metric. Comment: 11 page...|$|R
30|$|There {{are several}} {{important}} features of CUDA that require careful attention when programming GPUs. First, memory transfers between CPU and GPU must be minimized {{due to the}} high latency of transferring data over the bus. Accesses to GM should be coalesced whenever possible, and SM {{should be used to}} avoid <b>unnecessary</b> <b>access</b> to GM. Grouping threads in multiples of a warp facilitates coalescing of data with GM and helps to enhance GPU utilization. The programmer typically profiles the application extensively to fine tune performance and identify bottlenecks. To summarize, a GPU programmer should design kernels to spread the workload as much as possible throughout the GPU, read from GM in a coalesced manner to an SM, instantiate sufficient numbers of threads per block (TPB), and write back results to GM in a coalesced manner.|$|E
40|$|A {{semantic}} caching scheme {{suitable for}} wrappers wrapping web sources is presented. Since the web sources have typically weaker querying capabilities than conventional databases, existing semantic caching schemes cannot be applied directly. A seamlessly integrated query translation and capability mapping between the wrappers and web sources in semantic caching is described. In addition, {{an analysis on}} the match types between the user's input query and cached queries is presented. Semantic knowledge acquired from the data {{can be used to}} avoid <b>unnecessary</b> <b>access</b> to the web sources by transforming the cache miss to the cache hit. A polynomial time algorithm based on the proposed query matching technique is presented to find the best matched query in the cache. Experimental results reveal the effectiveness of the proposed semantic caching scheme. 1 Introduction Web databases allow users to pose queries to distributed and heterogeneous web sources. Such systems usually consist of three com [...] ...|$|E
40|$|We {{review the}} current status {{as well as the}} risks and {{benefits}} of a recently developed DNA test of risk for Alzheimer's disease: the apolipoprotein E genotype. While apolipoprotein E genotypes may indicate a degree of susceptibility, the gene is neither necessary nor sufficient to cause the disease; thus, many questions remain. Because risk prediction is not straightforward, practical issues related to the testing of complex diseases like Alzheimer's and to the ethical, legal, and social implications of genetic tests require careful consideration and unambiguous answers. The use of apolipoprotein E genotyping in patients with Alzheimer's disease should be limited to research centers, and additional studies are strongly recommended. Apolipoprotein E genotypes should not be available to third parties such as insurers or employers until genotypic risks are fully understood. National policies that encourage scientific investigation while maintaining individual privacy and limiting <b>unnecessary</b> <b>access</b> to genetic information should be immediately developed...|$|E
40|$|Abstract: Unlike {{relational}} {{tables in}} a database, data sources on the Web typically {{can only be}} accessed in limited ways. In particular, some of the source fields may be required as input and thus need to be mandatorily filled in order to access the source. Answering queries over sources with access limitations is a complex task that requires a possibly recursive evaluation even when the query is non-recursive. After reviewing the main techniques for query answering in this context, {{in this article we}} consider the impact of functional and inclusion dependencies on dynamic query optimization under access limitations. In particular, we address the implication problem for functional dependencies and simple full-width inclusion dependencies, and prove that it can be decided in polynomial time. Then we provide necessary and sufficient conditions, based on the dependencies together with the data retrieved at a certain step of the query answering process, that allow avoiding <b>unnecessary</b> <b>accesses</b> to the sources...|$|R
40|$|In {{the last}} two years we have {{developed}} improved techniques for indexing and retrieval of text data, including algorithms for inversion, for compression of the data and index, and for economical ranking. These techniques were, however, tested on relatively small databases. In this paper we describe our experiences in scaling these techniques up to a large (2 Gb) heterogeneous text database. Our experiments show that compression performance does not degrade with the increase in size, and that response times remain small, confirming that our techniques are suitable for large volumes of data. 1 Introduction Large document collections present many problems for practical information retrieval. To avoid <b>unnecessary</b> <b>accesses</b> to the text of the collection during query evaluation, comprehensive indexes are required. It must also be possible to create and access these indexes in a reasonable amount of time, and to store them, and the data itself, in a reasonable amount of space. In {{the last two}} [...] ...|$|R
40|$|Unlike {{relational}} {{tables in}} a database, data sources on the Web typically {{can only be}} accessed in limited ways. In particular, some of the source fields may be required as input and thus need to be mandatorily filled in order to access the source. Answering queries over sources with access limitations is a complex task that requires a possibly recursive evaluation even when the query is non-recursive. After reviewing the main techniques for query answering in this context, {{in this article we}} consider the impact of functional and inclusion dependencies on dynamic query optimization under access limitations. In particular, we address the implication problem for functional dependencies and simple full-width inclusion dependencies, and prove that it can be decided in polynomial time. Then we provide necessary and sufficient conditions, baseon the dependencies together with the data retrieved at a certain step of the query answering process, that allow avoiding <b>unnecessary</b> <b>accesses</b> to the sources...|$|R
40|$|We {{present a}} new {{semantic}} caching scheme suitable for wrappers in web databases. Since the web sources in web databases have typically weaker querying capabilities than conventional databases, {{it is not}} trivial to apply existing semantic caching schemes directly. We provide a seamlessly integrated query translation and capability mapping between the wrappers and web sources in the semantic caching to cope with such difficulties and describe several related issues. In addition, an analysis on the match types between the user's input query and queries stored in the cache is presented. We show how to use semantic knowledge acquired from the data to avoid <b>unnecessary</b> <b>access</b> to web sources by transforming the cache miss to the cache hit. Further, a polynomial time algorithm based on the extended and knowledge-based matching is proposed {{to find the best}} matched query in the cache. Finally, experimental results are presented to illustrate the effectiveness of our proposed semantic caching sch [...] ...|$|E
40|$|An {{intelligent}} semantic caching scheme {{suitable for}} web sources is presented. Since web sources typically have weaker querying capabilities than conventional databases, existing semantic caching schemes cannot be directly applied. Our proposal {{takes care of}} the difference between the query capabilities of an end user system and web sources. In addition, an analysis on the match types between a user's input query and cached queries is presented. Based on this analysis, we present an algorithm that finds the best matched query under different circumstances. Furthermore, a method to use semantic knowledge, acquired from the data, to avoid <b>unnecessary</b> <b>access</b> to web sources by transforming the cache miss to the cache hit is presented. To verify the effectiveness of the proposed semantic caching scheme, we first show how to generate synthetic queries exhibiting different levels of semantic localities. Then, using the test sets, we show that the proposed query matching technique is an efficient and effective way for semantic caching in web databases...|$|E
40|$|Top-k join queries {{have become}} very {{important}} in many important areas of computing. One of the most efficient algorithms for top-k join queries is the Rank-Join algorithm [17] [18]. However, there are many cases where Rank-Join does much <b>unnecessary</b> <b>access</b> to the input data sources. In this report, we first show {{that there are many}} cases where Rank-Joinâ€™s stopping mechanism is not efficient, and it does much unnecessary accesses to the input data sources. Then, we propose JTop, a family of much more efficient algorithms for top-k queries. We prove that our algorithms always perform less work than Rank-Join, and thus are more efficient. We also show that the performance of our algorithms can be O(n) times better than that of Rank-Join where n is the number of data items in the database. We evaluated the performance of our algorithms through experimentation over databases with different distributions. The results show that over the tested databases our algorithms significantly outperform Rank-Join...|$|E
40|$|As {{the number}} of cores {{increases}} on chip multiprocessors, coherence is fast becoming a central issue for multi-core performance. This is exacerbated {{by the fact that}} interconnection speeds are not scaling well with technology. This paper describes mechanisms to accelerate coherence for a multi-core architecture that has multiple private L 2 caches and a scalable point-to-point interconnect between cores. These techniques exploit the differences in geometry between chip multiprocessors and traditional multiprocessor architectures. Directory-based protocols have been proposed as a scalable alternative to snoop-based protocols. In this paper, we discuss implementations of coherence for CMPs and propose and evaluate a novel directory-based coherence scheme to improve the performance of parallel programs on such processors. Proximity-aware coherence accelerates read and write misses by initiating cache-to-cache transfers from the spatially closest sharer. This has the dual benefit of eliminating <b>unnecessary</b> <b>accesses</b> to off-chip memory, and minimizing the distance over which communicated data moves across the network. The proposed schemes result in speedups up to 74. 9 % for our workloads...|$|R
40|$|Existing work {{on similar}} {{sequence}} matching {{has focused on}} either whole matching or range subsequence matching. In this paper, we present novel methods for ranked subsequence matching under time warping, which finds top-k subsequences most similar to a query sequence from data sequences. To {{the best of our}} knowledge, this is the first and most sophisticated subsequence matching solution mentioned in the literature. Specifically, we first provide a new notion of the minimum-distance matching-window pair (MDMWP) and formally define the mdmwp-distance, a lower bound between a data subsequence and a query sequence. The mdmwp-distance can be computed prior to accessing the actual subsequence. Based on the mdmwp-distance, we then develop a ranked subsequence matching algorithm to prune <b>unnecessary</b> subsequence <b>accesses.</b> Next, to reduce random disk I/Os and bad buffer utilization, we develop a method of deferred group subsequence retrieval. We then derive another lower bound, the window-group distance, {{that can be used to}} effectively prune <b>unnecessary</b> subsequence <b>accesses</b> during deferred group-subsequence retrieval. Through extensive experiments with many data sets, we showcase the superiority of the proposed methods...|$|R
5000|$|... #Caption: Bloom filter used {{to speed}} up answers in a {{key-value}} storage system. Values are stored on a disk which has slow access times. Bloom filter decisions are much faster. However some <b>unnecessary</b> disk <b>accesses</b> are made when the filter reports a positive (in order to weed out the false positives). Overall answer speed is better with the Bloom filter than without the Bloom filter. Use of a Bloom filter for this purpose, however, does increase memory usage.|$|R
40|$|Restricting {{the access}} {{of a web}} server to system {{resources}} limits the potential damage caused to those resources through exploitation of web server vulnerabilities. However, allowing the web server to access the required resources enables the web server to provide expected functionality. This combination of denying <b>unnecessary</b> <b>access</b> and allowing required access results in providing web server functionality while limiting damage. To demonstrate this, we hosted the Apache web server on Security-Enhanced Linux, an operating system that enforces a mandatory access control policy. By tailoring the Security-Enhanced Linux policy, {{we were able to}} control interaction between the Apache web server and other processes and files on the system. The policy dictates that Apache is only allowed to display web pages and perform limited functions that support the display of web pages. This work demonstrates the following. âˆ‘ Security-Enhanced Linux is capable of supporting commonly used applications. âˆ‘ Security-Enhanced Linux can confine applications so that a reduced level of risk is achieved when making applications available. âˆ‘ Although confined, these applications provide the functionality expected by users...|$|E
40|$|Given a large component-based program, {{it may be}} {{very complex}} to {{identify}} an optimal access-control policy, allowing the program to execute with no authorization failures and no violations of the Principle of Least Privilege. This paper presents a novel combination of static and dynamic analysis for automatic determination of precise accesscontrol policies for programs that will be executed on Stack-Based Access Control systems, such as Java and the Common Language Runtime (CLR). The static analysis soundly models {{the execution of the}} program taking into account native methods, reflection, and multi-threaded code. The dynamic analysis interactively refines the potentially conservative results of the static analysis, with no need for writing or generating test cases or for restarting the system if an authorization failure occurs during testing, and no risk of corrupting the underlying system on which the analysis is performed. We implemented the analysis framework presented by this paper in an analysis tool for Java programs, called Access-Control Explorer (ACE). ACE allows for automatic, safe, and precise identification of access-right requirements and library-code locations that should be made privilegeasserting to prevent client code from requiring <b>unnecessary</b> <b>access</b> rights. This paper presents experimental results obtained on large production-level applications. ...|$|E
40|$|Datacubes are {{efficient}} structures used {{to represent}} multidimensional aggregates at various levels. Quite often multiple datacubes are predefined and computed in order to assist analytical queries in Decision support systems. However being statically defined structures, they suffer from some inherent problems. Firstly, conventional datacubes are highly inefficient when created over sparse data. Next, datacubes in traditional approaches address &quot;fixed &quot; or predefined queries, and are useless with truly adhoc queries that would require expensive join operations with the fact table. Furthermore, traditional cubes are defined based on nonrecursive SPJ views, hence incapable of accommodating &quot;recursive &quot; OLAP queries. In order to address these issues, we adopt and extend an Object-Oriented Data Warehousing (OODW) approach to datacubes. We represent the Cube notion as a generic Class, thereby semantically enriching the static cube structure and inter-cell relationships via Oids. More specifically, by constructing semantic cube hierarchies and defining class views, {{we are able to}} compute/derive new cubes efficiently by avoiding <b>unnecessary</b> <b>access</b> to the lower level detailed tables. Also, recursive cubes can be defined and computed within the same framework. Algorithms are developed to compute specialized dynamic Cubes in a multi-Fact environment, taking into account a given a set of predefined OLAP queries. We validate our approach with an experimental evaluation of our algorithms, which also provides a foundation fo...|$|E
40|$|Main memory {{database}} management systems have become essential for response-time-bounded applications, {{such as those}} in telecommunications systems or Internet, where users frequently access a table in order to get information or check whether an element exists, and require the response to be as fast as possible. Continuous data growth is making it unafordable to keep entire relations in memory and some commercial applications provide two different engines to handle data in-memory and on-disk separately. However, these systems assign each table to one of these engines, forcing large relations to be kept on secondary storage. In this paper we present TwinS|a hybrid {{database management}} system that allows managing hybrid tables, i. e. tables partially managed by both engines. Our objective is twofold: first, to allow large tables that do not fit in the memory to partially benefit from in-memory management techniques and, second, to provide a way to discard <b>unnecessary</b> <b>accesses</b> to both memory and disk. Overall, we show that we can reduce response time when accessing a large table in the database. All our experiments have been run on a dual-engine DBMS: IBM-SolidDB. Postprint (published version...|$|R
40|$|Modern {{high-performance}} processors {{access the}} branch target buffer (BTB) every cycle to speculate branch target addresses. This aggressive approach improves performance as {{it results in}} early identification of target addresses. However, unfortunately, such accesses, quite often, are unnecessary {{as there is no}} control flow instruction among those fetched. In this work we introduce Speculative BTB Access (SABA) to address this design inefficiency. SABA relies on a simple power efficient structure, referred to as the SABAfilter, to identify cycles where there is no control flow instruction among those fetched, at least one cycle in advance. By identifying such cycles and eliminating <b>unnecessary</b> BTB <b>accesses</b> we reduce BTBâ€™s power dissipation (and therefore power density). SABA comes with zero timing overhead as it makes decisions regarding future cycles and therefore does not impact critical path delay. Our study shows that, by using SABA, it is possible to eliminate {{more than half of the}} <b>unnecessary</b> BTB <b>accesses</b> while paying a very low performance cost (average: 0. 7 %). We also study how variations in SABA-filter configuration, branch predictor configuration and processor execution bandwidth impact power savings and performance slowdown for a SABAenhanced processor. ...|$|R
5000|$|... #Caption: [...] An {{approximate}} member query (AMQ) filter used {{to speed}} up answers in a key-value storage system. Key-value pairs are stored on a disk which has slow access times. AMQ filter decisions are much faster. However some <b>unnecessary</b> disk <b>accesses</b> are made when the filter reports a positive (in order to weed out the false positives). Overall answer speed is better with the AMQ filter than without it. Use of an AMQ filter for this purpose, however, does increase memory usage.|$|R
40|$|Due to {{the high}} {{complexity}} of objects and queries and also due to extremely large data vol-umes, spatial database systems impose stringent requirements {{on the performance of}} query processing. For improving query performance the following two properties are an absolute necessity: (i) a fast spatial access to the objects and (ii) a fast processing of geometric oper-ations. It has been convincingly demonstrated and it is generally accepted that a fast spatial access can only be achieved by integrating spatial access methods (SAMs) into spatial data-base systems. However, the huge potential that SAMs open for global clustering in combi-nation with set I/O, thus improving the performance of set-oriented access to large amounts of objects, has rarely been investigated. Furthermore, SAMs must be exploited adequately for improving expensive operations such as the spatial join. Property (ii), fast processing of complex geometric operations, is achieved by the following two building blocks: approxi-mations and decompositions. Good approximations provide an efficient filtering by avoiding <b>unnecessary</b> <b>access</b> and operations on the exact representation. The decomposition of com-plex spatial objects into simple components substitutes the expensive execution of a compu-tational geometry algorithm for the complex object by multiple executions of simple and fast computational geometry algorithms for simple components. In this paper, we investigate the above mentioned potentials for improving the query performance in spatial database systems in detail. ...|$|E
40|$|Database {{evolution}} {{is the process}} of updating the schema of a database or data warehouse (schema evolution) and e-volving the data to the updated schema (data evolution). It is often desired or necessitated when changes occur to the data or the query workload, the initial schema was not carefully designed, or more knowledge of the database is known and a better schema is concluded. The Wikipedia database, for example, has had more than 170 versions in the past 5 years [8]. Unfortunately, although much research has been done on the schema evolution part, data evolution has long been a prohibitively expensive process, which essentially evolves the data by executing SQL queries and re-constructing indexes. This prevents databases from being flexibly and frequently changed based on the need and forces schema designers, who cannot afford mistakes, to be highly cautious. Techniques that enable efficient data evolution will undoubtedly make life much easier. In this paper, we study the efficiency of data evolution, and discuss the techniques for data evolution on column oriented databases, which store each attribute, rather than each tuple, contiguously. We show that column oriented databases have a better potential than traditional row oriented databases for supporting data evolution, and propose a novel data-level data evolution framework on column oriented databases. Our approach, as suggested by experimental evaluations on real and synthetic data, is much more efficient than the query-level data evolution on both row and column oriented databases, which involves <b>unnecessary</b> <b>access</b> of irrelevant data, materializing intermediate results and re-constructing indexes...|$|E
40|$|Abstract. The {{commercial}} aircraft operational life extension considerations are increasing with more shorter and longer range aircraft operating {{than ever in}} history. Aircraft Maintenance Repair and Overhaul companies (MROs) serve {{a significant portion of}} the after-sales market. Though more aircraft are operating, the increasing after-sales market, particularly maintenance still remains very competitive. Cheaper labour forces existing in emerging economies and the arrival of next generation aircraft (Airbus A 350 and Boeing 787 Dreamliner promising reduced maintenance costs) will cause additional pressures for MROs to operate efficiently and satisfy airline operators. One fundamental requirement for airliner operators will always remain apparent and that is the need to maximise their aircraft usage. State of the art, automated NDT applications for aircraft often also discussed within the context of structural health monitoring (SHM) are slowly developing and provide the prospect of reducing maintenance related down time, potentially increasing fleet usage. The questions of where and how these automated NDT and SHM technologies can be used on aircraft to bring about such benefits have yet to be fully addressed. The paper describes a method of analysing scheduled maintenance interval blocks to identify time critical areas. Automated NDT solutions with embedded sensors and faster decision support at such locations could contribute to elimination of <b>unnecessary</b> <b>access</b> and efficient information relayed to maintenance, repair and material handling teams. The method proposed was preliminarily applied to a short range aircraft across different types of scheduled maintenance interval blocks. Results highlighted significant benefits are likely to occur within shorter, more frequent scheduled maintenance intervals where inspections were intensive but specific to an area...|$|E
40|$|Abstractâ€”This paper {{presents}} {{an approach that}} reuses data stored in the frame memory and in the motion estimation (ME) internal buffer to avoid <b>unnecessary</b> memory <b>accesses</b> and redundant ME computations for MPEG video encoders. This work employs a macroblock bitmap table, which can be easily maintained, to locate the reusable data. The experimental {{results show that the}} proposed scheme is particularly efficient in low motion video sequences, approximately saving 18 % of the frame memory accesses as well as about 16 % of the ME computations without any sacrifice in the image quality...|$|R
50|$|In a 2015 paper, the National Institute Against Discrimination, Xenophobia and Racism (INADI) {{called for}} {{recognition}} {{of the rights of}} intersex people to bodily integrity and autonomy in medical decisions. INADI called for the deferral of medically <b>unnecessary</b> interventions, and <b>access</b> to health for all intersex people regardless of prior medical treatment.|$|R
40|$|In {{this paper}} we {{address the problem of}} {{efficiently}} evaluating containment (i. e., subset, equality, and superset) queries over set-valued data. We pro-pose a novel indexing scheme, the Ordered Inverted File (OIF) which, differ-ently from the state-of-the-art, indexes set-valued attributes in an ordered fashion. We introduce query processing algorithms that practically treat containment queries as range queries over the ordered postings lists of OIF and exploit this ordering to quickly prune <b>unnecessary</b> page <b>accesses.</b> OIF is simple to implement and our experiments on both real and synthetic data show that it greatly outperforms the current state-of-the-art methods for al...|$|R
