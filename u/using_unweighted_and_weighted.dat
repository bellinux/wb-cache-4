4|10000|Public
40|$|This study {{examines}} {{the extent of}} nonresponse bias in online advertising conversion studies. Two indicators (i. e., conversion rates and travel expenditure) assessing the tourism advertising effectiveness were compared <b>using</b> <b>unweighted</b> <b>and</b> <b>weighted</b> data sets. The {{results of this study}} using 24 locations throughout the U. S. confirm the conclusions of previous studies, showing consistent overestimates in advertising effectiveness. Several methodological and managerial implications of these findings are discussed...|$|E
30|$|All dense stereo {{matching}} algorithms use some method of measuring {{the similarity of}} pixels between the two image views. Typically, a matching function is computed at each pixel for all disparities under consideration. The simplest matching functions assume {{that there is little}} or no luminance difference between corresponding left/right pixels, but more robust methods may allow for (explicitly or implicitly) radiometric changes and/or noise. Common pixel-based matching functions include absolute differences, squared differences, or sampling-insensitive absolute differences [27]. Common window-based matching functions include the sum of absolute or squared differences (SAD, SSD), normalized cross-correlation (NCC), and rank and census transforms [28]. Some matching functions can be implemented efficiently <b>using</b> <b>unweighted</b> <b>and</b> <b>weighted</b> median filters [29, 30]. More complicated similarity measures are possible and have included mutual information or approximate segment-wise mutual information as used in the layered stereo approach of Zitnick [31]. Some methods not only try to employ new combined matching functions but also propose secondary disparity refinement to further remove the remaining outliers [32].|$|E
40|$|Significant {{differences}} were found between opioid takers and non-takers among referrals to Jervis Street Drug Advisory and Treatment Centre, Dublin, for the following variables: Sex, Age, Employment Status, Prior Court Appearance, Solitary drug taking. First drug taken abroad. Injection history. Physical complications associated with drug taking, and Maternal psychiatric status (for referrals aged under 21). Further analysis indicated 6 types of abuse associated with particular personal and social circumstances: (1) Minor tranquillisers, (2) Barbiturates only or with minor tranquillisers, (3) Opioids only or mainly, (4) Cannabis and/or L. S. D. only, (5) Poly-abuse excluding opioids, and (6) Poly-abuse including opioids. In broad social terms, these 6 groups may be seen as representing psychiatric or personal inadequacy-groups (1), (2) and (3), subcultural deviance-(5) and (6), and subcultural hedonism in the context of poor family relationships-(4). Groups most at risk were considered to be in the following order-(2), (3), (6),(1). Finally, cluster analyses, <b>using</b> <b>unweighted</b> <b>and</b> <b>weighted</b> variables, were employed to test the significance of distinctions based on types (3), (4), (5) and (6). In general, similar groups were obtained...|$|E
30|$|The CC {{standard}} data were studied <b>using</b> <b>unweighted</b> linear regression <b>and</b> <b>weighted</b> linear regression with a weighting factor of 1 /x, 1 /√x, and 1 /x 2. The data analysis {{was carried out}} using Microsoft ® Excel ® 2016 MSO (64 -bit).|$|R
40|$|This study {{examines}} {{the relationship between}} the level of disclosure and its determinants, more specifically those relating to corporate governance mechanisms. The theoretical framework of the relationship between governance and the level of disclosure is proposed by the agency theory. For a sample of non-financial listed Tunisian companies for a period of 2004 - 2009. We built an index to approximate the extent of disclosure. We also <b>used</b> an <b>unweighted</b> <b>and</b> <b>weighted</b> index based on the views of financial information users (banker, Analyst, Financial Market Board, shareholders). The results show that the level of disclosure is explained by the size, leverage, profitability, duality, concentration of ownership and control quality as measured by the number of auditors and the presence of Big 4. Key words: Weighted disclosure index, corporate governance, board of directors, ownershi...|$|R
40|$|OBJECTIVE: To {{assess whether}} {{ultrasonography}} (US) is reliable {{for the evaluation}} of inflammatory and structural abnormalities in patients with knee osteoarthritis (OA). METHODS: Thirteen patients with early knee OA were examined by 11 experienced sonographers during 2 days. Dichotomous and semiquantitative scoring was performed on synovitis characteristics in various aspects of the knee joint. Semiquantitative scoring was done of osteophytes at the medial and lateral femorotibial joint space or cartilage damage of the trochlea and on medial meniscal damage bilaterally. Intra- and interobserver reliability were computed by <b>use</b> of <b>unweighted</b> <b>and</b> <b>weighted</b> kappa coefficients. RESULTS: Intra- and interobserver reliability scores were moderate to good for synovitis (mean kappa 0. 67 and 0. 52, respectively) as well as moderate to good for the global synovitis (0. 70 and 0. 50, respectively). Mean intra- and interobserver reliability kappa for cartilage damage, medial meniscal damage and osteophytes ranged from fair to good (0. 55 and 0. 34, 0. 75 and 0. 56, 0. 73 and 0. 60, respectively). CONCLUSIONS: Using a standardised protocol, dichotomous and semiquantitative US scoring of pathological changes in knee OA can be reliable...|$|R
40|$|BACKGROUND: High-quality {{non-invasive}} imaging of {{the deep}} venous system in the thorax is challenging, but nevertheless required for diagnosis of vascular pathology {{as well as for}} patient selection and preoperative planning for endovascular procedures. PURPOSE: To compare the diagnostic quality of Gadofosveset-enhanced thoracic magnetic resonance venography, seven consecutive patients with suspected or known disease affecting the central thoracic veins were compared to seven consecutive magnetic resonance venography using conventional gadolinium-based contrast agents. MATERIALS AND METHODS: Diagnostic capability, defined as the ability to assess vessel patency and pathologic conditions, for the major thoracic deep venous segments was assessed by two-independent readers. Both reviewers rated the overall subjective image quality on a four-graded scale, and inter-rater variability was analyzed <b>using</b> <b>unweighted</b> <b>and</b> <b>weighted</b> Cohen's kappa values. RESULTS: Diagnostic capability was generally considerably higher in the Gadofosveset group for all examined vessel segments. The overall images quality rating was significantly higher for the Gadofosveset group with a mean rating of 2. 9 and 2. 7 for the two-independent readers, compared to 1. 2 and 1. 0 for the control croup. Inter-rater variability showed less variability for the Gadofosveset group with a quadratic-weighted Cohen's Kappa value of 0. 58 compared to 0. 36 for the control group. CONCLUSION: Our results show that Gadofosveset-enhanced magnetic resonance venography of the central thoracic veins is a reliable technique in clinical routine practice that results in diagnostic images, superior to conventional gadolinium-based contrast medium...|$|E
40|$|An {{important}} and difficult problem in computer vision {{is to determine}} 2 D image feature correspondences over a set of images. In this paper, two new affinity measures for image points and lines from different images are presented, and are <b>used</b> to construct <b>unweighted</b> <b>and</b> <b>weighted</b> bipartite graphs. It is shown that the image feature matching problem {{can be reduced to}} an unweighted matching problem in the bipartite graphs. It is further shown that the problem can be formulated as the general maximum-weight bipartite matching problem, thus generalizing the above unweighted bipartite matching technique. Keywords: image feature matching, bipartite matching, image sequence analysis, computer vision 1 INTRODUCTION A fundamental and important problem in computer vision is to acquire 3 D models of objects and scenes from a set of images. The basic principles involved in 3 D model acquisition are feature matching and triangulation, with the two commonly used types of image features being points an [...] ...|$|R
40|$|Certification of {{the phase}} {{compositions}} {{of the three}} NIST Reference Clinkers will be based upon more than one independent method. The current reference values were established using an optical microscope examination, with additional optical microscope data taken from an ASTM C 1356 round robin. The present X-ray powder diffraction (XRD) study provides the second, independent estimate of the phase abundance. Reitveld refinement of the powder diffraction data allowed calculation {{of a set of}} best-fit reference patterns and their scale factors. Because of significant contrast in the linear absorption coefficients of ferrite and periclase, relative to the estimated mean matrix linear absorption coefficient, the scale factors were adjusted for microabsorption effects. The XRD data agree with the optical data with the exception of aluminate. This disagreement may reflect the difficulty in resolving this fine-sized phase using the optical microscope. The XRD data did show greater precision than replicate measurements by microscopy. Measurements from different sources, laboratories, instruments, and from different methods can exhibit significant between-method variability, as well as distinct within-method variances. The data sets were treated <b>using</b> both <b>unweighted</b> <b>and</b> <b>weighted</b> schemes to establish the bestconsensus values and to provide meaningful uncertainties. While the mean values of individual phase abundance do not vary, the 95 % uncertainty level values do. The Mandel-Paule-Vangel-Rukhin method of combining the data sets is favored as this method produces a weighted mean whose weighting scheme does not necessarily skew the consensus value {{in the direction of the}} large number of XRD values, and that takes between- as well as within-method variation into account...|$|R
40|$|A {{large number}} of {{pedestrian}} fatalities were reported in China since the 1990 s, however the exposure of pedestrians in public traffic has never been measured quantitatively using in-depth accident data. This study aimed to investigate the association between the impact speed and risk of pedestrian casualties in passenger vehicle collisions based on real-world accident cases in China. The cases were selected from a database of in-depth investigation of vehicle accidents in Changsha-IVAC. The sampling criteria were defined as (1) the accident was a frontal impact that occurred between 2003 and 2009; (2) the pedestrian age was above 14; (3) the injury according to the Abbreviated Injury Scale (AIS) was 1 +; (4) the accident involved passenger cars, SUVs, or MPVs; and (5) the vehicle impact speed can be determined. The selected IVAC data set, which included 104 pedestrian accident cases, was weighted based on the national traffic accident data. The logistical regression models of the risks for pedestrian fatalities and AIS 3 + injuries were developed in terms of vehicle impact speed <b>using</b> the <b>unweighted</b> <b>and</b> <b>weighted</b> data sets. A multiple logistic regression model on the risk of pedestrian AIS 3 + injury was developed considering the age and impact speed as two variables. It {{was found that the}} risk of pedestrian fatality is 26 % at 50 km/h, 50 % at 58 km/h, and 82 % at 70 km/h. At an impact speed of 80 km/h, the pedestrian rarely survives. The weighted risk curves indicated that the risks of pedestrian fatality and injury in China were higher than that in other high-income countries, whereas the risks of pedestrian casualty was lower than in these countries 30 years ago. The findings could have a contribution to better understanding of the exposures of pedestrians in urban traffic in China, and provide background knowledge for the development of strategies for pedestrian protection. (C) 2010 Elsevier Ltd. All rights reserved...|$|R
40|$|The genera Speiropsis and Xylomyces are anomorph fungi. The taxonomic {{address for}} the fungi has been unclear. In this study, {{observation}} of morphological traits indicates {{that they have}} a unique pattern of mycelia with dark-brown to black colour and thick-walled hyphae. The same culture patterns of certain fungi isolated from freshwater habitats in Thailand were selected from BIOTEC Culture Collection (BCC, Thailand), while more species were added from Centraalbureau voor Schimmelcultures (CBS, Netherlands). These fungi were composed of Jahnula spp. (2 -celled ascospores), Brachiosphaera tropicalis (hyaline and 4 - 5 armed conidia), S. pedatospora (hyaline and branches conidia) and Xylomyces sp. (dematiaceous and fusiform chlamydospores). This study was undertaken to confirm the taxonomic address for S. pedatospora and Xylomyces based on phylogenetics relationships as inferred from their ITS rDNA sequence data by <b>using</b> MP (<b>unweighted</b> <b>and</b> successive <b>weighted</b> MP), NJ, ML and Bayesian analysis. Phylogenic analysis revealed that isolates of S. pedatospora (2 strains) {{was a member of the}} Order Jahnulales and clustered with Jahnula spp. (5 strains) and B. tropicalis (4 strains) with > 82 % bootstrap support and 100 % posterior probabilities. Four isolates of X. chlamydosporus, X. elegans and X. aquaticus were shown to be polyphyletic within the Jahnulales and Pleoporales. The MP and NJ showed the same topology as in the Jahnulales clade obtained by ML analysis...|$|R
40|$|Diabetes {{mellitus}} (DM) is {{a leading}} cause of death in the United States (U. S.) and adversely impacts both individuals and the nation’s economy. While there are many known risk factors for DM, exploring additional risk factors for DM is necessary to reduce the prevalence of the disease. This study examined the association between PM 2. 5 and ozone with DM prevalence in the U. S. between 2002 and 2008. This was a semi-ecologic study that used annual average, county-level exposure data from the CDC’s National Public Health Tracking Network (Tracking Network) and cross-sectional, individual-level outcome and covariate data from the Selected Metropolitan/Micropolitan Area Risk Trends of Behavioral Risk Factor Surveillance System (SMART BRFSS). <b>Unweighted</b> <b>and</b> <b>weighted</b> associations between PM 2. 5 and ozone with DM among SMART BRFSS counties by the four U. S. regions (Northeast, Midwest, South, West) were analyzed. Final analysis included 863, 096 individuals and the results are presented in change in prevalence of DM per 10 -unit increase in annual average county-level PM 2. 5 (μg/m 3) or ozone (parts per billion, ppb) concentration estimates. This study found statistically significant associations between PM 2. 5 and ozone with DM after adjusting for confounders in the final unweighted model (PM 2. 5 : prevalence ratio (PR) = 1. 13; 95 % Confidence Interval (CI) = 1. 07, 1. 20; ozone: PR= 1. 04; 95 % CI= 1. 01, 1. 07). Using the final weighted model, the association between PM 2. 5 and DM was attenuated (PR= 1. 05; 95 % CI= 0. 99, 1. 12) while the association between ozone and DM remained (PR= 1. 07; 95 % CI= 1. 03, 1. 11). Regional variations in the association between PM 2. 5 and ozone with DM <b>using</b> the final <b>unweighted</b> <b>and</b> <b>weighted</b> models were also found with the strongest association for PM 2. 5 and DM in the Midwest (unweighted PR= 1. 24; 95 % CI= 1. 13, 1. 36; weighted PR= 1. 23; 95 % CI= 1. 03, 1. 46), and for ozone and DM being in the Northeast (unweighted PR= 1. 12; 95 % CI= 1. 03, 1. 21; weighted PR= 1. 15; 95 % CI= 1. 03, 1. 28). In summary, this study found that per every 10 -unit increase in annual average county-level concentration estimates of PM 2. 5 and ozone, prevalence of DM increased between 5 % to 7 % and these associations varied by U. S. region. These findings suggest that interventions in reducing exposures to PM 2. 5 and ozone could help reduce the DM burden. ...|$|R
40|$|Studies {{examining}} {{the extent of}} disclosure of NPOs remain sparse, yet {{it is essential to}} ensure accountability to stakeholders. This study was initiated to elicit stakeholders’ information needs from charity organisations. The research objectives include: to measure the extent of disclosure of information by charity organisation in Malaysia; to understand the motivating factors that drives the donors to donate; to determine reasons charity organisations disclose information about their organization and to examine the organisational-specific attributes affecting the extent of information disclosed. A Charity Organisations Reporting Index (ChORI) instrument was developed to reliably measure the extent of information disclosure. Stakeholder theory and resource dependence theory were used {{in the development of the}} index. A complete set of ChORI items were compiled and used in a survey involving 117 institutional donors. The objective of the survey was to obtain the weight of importance of the items. The extent of disclosure was then measured by <b>using</b> both the <b>unweighted</b> <b>and</b> <b>weighted</b> index of the information items. A content analysis of each organisation’s annual returns resulted in a disclosure score for each organisation. The annual returns for the financial year 2009 of 101 charity organisations were examined in order to identify the extent of disclosures provided by each organisation. The dependent variable of the study, extent of disclosure, was the ratio of an organisation’s total disclosure score to the organisation’s total possible disclosure score. Through semi-structure interviews, it was found that motivating factors that drive the donors to donate are the altruism and strategic profit maximisation motivating factors. On the other hand, accountability, transparency and legitimacy appeared to be the upmost reasons for disclosure of information from the perspective of the charity. Hierarchical regression analysis was used to determine the effect of organization-specific attributes which include internal and external governance mechanism, financial and non-financial performance, and organisational type, i. e. funded or nonfunded charity organisations. The empirical results of the study revealed that external governance mechanism and financial performance have significant positive effect on the extent of disclosure. Moreover, external governance mechanism variable was found to be a more important determinant of information disclosure. Results also revealed that public-funded charity organisations were more likely to disclose more information. This is most probably due to more stringent disclosure requirements imposed by the resource providers to public-funded charity organisations. A major contribution for future research in the field is the disclosure index instrument, ChORI which was developed in this study to measure the extent of disclosure. This study also provides a guide for best practice in charity reporting. Recommendations are made in terms of ways to improve charity disclosure for better accountability and transparency. Both managerial and policy implications are also discussed...|$|R
3000|$|... is non-compromised. They {{carry out}} the private <b>unweighted</b> <b>and</b> <b>weighted</b> trust computation, respectively, without {{limitations}} imposed on U [...]...|$|R
3000|$|... [...]. The AP and WAP {{protocols}} {{compute the}} average <b>unweighted</b> <b>and</b> <b>weighted</b> trust, respectively. The generalized MPKP and MPWP protocols relax {{the assumption that}} U [...]...|$|R
3000|$|This paper {{extends the}} schemes of [9] by {{introducing}} the MPKP and MPWP protocols that compute average <b>unweighted</b> <b>and</b> <b>weighted</b> {{trust in the}} general case, even when the initiator U [...]...|$|R
40|$|An {{exploratory}} {{study was conducted}} {{to evaluate the effectiveness of}} indices proposed by different investigators to relate vehicle vibrations to passenger comfort. The indices considered included criteria for sinusoidal vibrations, <b>unweighted</b> <b>and</b> <b>weighted</b> amplitude exceedance counts, the integral of the <b>unweighted</b> <b>and</b> <b>weighted</b> power spectral density and absorbed power. These functions were initially examined analytically to determine the manner in which they each weighed vibration amplitude and frequency. Similarities among them are noted. Index values were then computed from measured vibrations and compared with the associated comfort ratings. The data for these comparisons were obtained from ride comfort evaluations of passenger trains...|$|R
30|$|To {{detail the}} basic {{parameters}} {{that provide a}} general view on the networks, we present <b>unweighted</b> <b>and</b> <b>weighted</b> degree distributions, <b>and</b> the distributions of the differences of resulting degrees of the connected node pairs in this section.|$|R
40|$|In {{this paper}} we propose two {{algorithms}} for solving both <b>unweighted</b> <b>and</b> <b>weighted</b> constrained two-dimensional two-staged cutting stock problems. The problem is called two-staged cutting problem because each produced (sub) optimal cutting pattern is realized by using two cut-phases. OPTIMIZATION; PROGRAMMING; STOCKS...|$|R
3000|$|..., can be {{compromised}} by the adversary, and we introduce the Multiple Private Keys <b>and</b> the <b>Weighted</b> protocols (MPKP <b>and</b> MPWP) for computing average <b>unweighted</b> <b>and</b> <b>weighted</b> trust, respectively. Moreover, the Csed Protocol (CEBP) extends the PKEBP in this case. The computation {{of all our}} algorithms requires the transmission of O(n) (possibly large) messages.|$|R
40|$|We discuss data {{structures}} and their methods of analysis. In particular, we treat the <b>unweighted</b> <b>and</b> <b>weighted</b> dictionary problem, self-organizing data structures, persistent data structures, the union-find-split problem, priority queues, the nearest common ancestor problem, {{the selection and}} merging problem, and dynamization techniques. The methods of analysis are worst, average and amortized case...|$|R
40|$|We {{investigate}} the relative merit of phase-based methods [...] -mean phase coherence, <b>unweighted</b> <b>and</b> <b>weighted</b> phase lag index [...] -for estimating {{the strength of}} interactions between dynamical systems from empirical time series which are affected by common sources and noise. By numerically analyzing the interaction dynamics of coupled model systems, we compare these methods to each other {{with respect to their}} ability to distinguish between different levels of coupling for various simulated experimental situations. We complement our numerical studies by investigating consistency and temporal variations of the strength of interactions within and between brain regions using intracranial electroencephalographic recordings from an epilepsy patient. Our findings indicate that the <b>unweighted</b> <b>and</b> <b>weighted</b> phase lag index are less prone to the influence of common sources but that this advantage may lead to constrictions limiting the applicability of these methods...|$|R
40|$|We {{deal with}} <b>unweighted</b> <b>and</b> <b>weighted</b> {{enumerations}} of lozenge tilings of a hexagon with side lengths a; b + m; c; a + m; b; c + m, where an equilateral triangle of side length m {{has been removed}} from the center. We give closed formulas for the plain enumeration and for a certain (1) -enumeration of these lozenge tilings...|$|R
40|$|The {{widely used}} χ 2 {{homogeneity}} test for comparing histograms(unweighted) is modified for cases involving <b>unweighted</b> <b>and</b> <b>weighted</b> histograms. Numerical examples illustrate {{an application of}} the method for the case of histograms with a small statistics of events and also for large statistics of events. This method {{can be used for}} the comparison of simulated data histograms against experimental data histograms. ...|$|R
40|$|A {{weighted}} multivariate signed-rank test {{is introduced}} for {{an analysis of}} multivariate clustered data. Observations in different clusters may then get different weights. The test provides a robust and efficient alternative to normal theory based methods. Asymptotic theory is developed to find the approximate p-value {{as well as to}} calculate the limiting Pitman efficiency of the test. A conditionally distribution-free version of the test is also discussed. The finite-sample behavior of different versions of the test statistic is explored by simulations and the new test is compared to the <b>unweighted</b> <b>and</b> <b>weighted</b> versions of Hotelling's T 2 test and the multivariate spatial sign test introduced in [D. Larocque, J. Nevalainen, H. Oja, A weighted multivariate sign test for cluster-correlated data, Biometrika 94 (2007) 267 - 283]. Finally, a real data example is used to illustrate the theory. primary, 62 H 15, 62 G 10 secondary, 62 E 20 Clustered observations Paired observations Intra-cluster correlation Multivariate location problem Wilcoxon signed-rank test <b>Unweighted</b> <b>and</b> <b>weighted</b> testing U-statistics...|$|R
40|$|We {{review a}} family of closely related query {{learning}} algorithms for <b>unweighted</b> <b>and</b> <b>weighted</b> tree automata, {{all of which are}} based on adaptations of the minimal adequate teacher (MAT) model by Angluin. Rather than presenting new results, the goal is to discuss these algorithms in sufficient detail to make their similarities and differences transparent to the reader interested in grammatical inference of tree automata...|$|R
40|$|This article reviews {{aspects of}} {{significance}} testing. Problems of detection {{of a specific}} signal with background noise from observed Poisson counts of events {{is used as a}} basic example throughout. In particular we discuss issues of using alternative test-statistics, unbinned likelihood fits, <b>and</b> comparing <b>unweighted</b> <b>and</b> <b>weighted</b> histograms. We point at the possibility of adding simultaneous confidence intervals to the statistical toolbox normally used by particle physicists...|$|R
40|$|We study two single-machine {{scheduling}} problems: Minimizing the <b>weighted</b> <b>and</b> <b>unweighted</b> {{number of}} tardy units, when release times are present. Fast strongly polynomial algorithms are given for both problems: For problems with n jobs, we give algorithms which require O(n log n) and O(n 2) steps, for the <b>unweighted</b> <b>and</b> <b>weighted</b> problems respectively. Our results also imply {{an extension of}} the family of very efficiently solvable transportation problems, as well as these which are greedily solvable using the “Monge sequence ” idea. 1...|$|R
40|$|AbstractWe present {{improved}} parameterized algorithms for {{the feedback}} vertex set problem on both <b>unweighted</b> <b>and</b> <b>weighted</b> graphs. Both algorithms run in time O(5 kkn 2). The algorithms construct a feedback vertex set of size at most k (in the weighted case this set is of minimum weight among the feedback vertex sets of size at most k) {{in a given}} graph G of n vertices, or report that no such feedback vertex set exists in G...|$|R
40|$|AbstractWe {{consider}} the p-center problem on tree graphs where the customers are modeled as continua subtrees. We address <b>unweighted</b> <b>and</b> <b>weighted</b> models {{as well as}} distances with and without addends. We prove that a relatively simple modification of Handler’s classical linear time algorithms for <b>unweighted</b> 1 - <b>and</b> 2 -center problems with respect to point customers, linearly solves the <b>unweighted</b> 1 - <b>and</b> 2 -center problems with addends of the above subtree customer model. We also develop polynomial time algorithms for the p-center problems based on solving covering problems and searching over special domains...|$|R
40|$|Using the SIS {{model on}} <b>unweighted</b> <b>and</b> <b>weighted</b> networks, we {{consider}} the disease localization phenomenon. In contrast to the well-recognized {{point of view that}} diseases infect a finite fraction of vertices right above the epidemic threshold, we show that diseases can be localized on a finite number of vertices, where hubs and edges with large weights are centers of localization. Our results follow from the analysis of standard models of networks and empirical data for real-world networks. Comment: 5 pages, 3 figure...|$|R
40|$|This project aims at the {{consolidation}} of the data from integrated fieldwork in Swabian Alb test area since 1996 to 2013 {{as well as the}} height systems computations. Reliable data were checked by height differences and gravity values, after that they were grouped into 5 closed loops with 56 out of 121 observed points. Potential differences were computed from height differences which acquired from spirit levelling and gravity value, then least square adjustment was adopted. Observation equation (A-matrix) and condition equation (B-matrix) were applied in the adjustment, a weight matrix was also assigned in the adjustment. Geopotential differences were computed based on the Helmert orthometric height at point 580, then geopotenial numbers of the other points were computed by adding the geopotential number with the adjusted potential differences, then height systems could be determined as well as height corrections. In the closed loop adjustment especially adjustment of several loops with many data, condition equation adjustment is preferred because of the smaller size of design matrix compared to the observation equations and the advantage of condition equations over observation equations is that loop misclosures can be determined by condition equation. The results from both equations are the same. The difference of the geopotential numbers between <b>unweighted</b> <b>and</b> <b>weighted</b> adjustment is up to 0. 0129 m 2 /s 2 and the difference of height systems between <b>unweighted</b> <b>and</b> <b>weighted</b> adjustment is up to 0. 0013 meter or 1. 3 millimeter, so the height system computations were not significantly affected by the assigned weight. The difference of height corrections between <b>unweighted</b> <b>and</b> <b>weighted</b> adjustment is up to 10 - 8 m so the assigned weights did not affect height correction results. Normal corrections give the smallest values while dynamic corrections give the largest values because the test area is located at latitude 48. 485 ° instead of latitude 45 ° so the correction values are quite large...|$|R
40|$|We present {{predictions}} of the <b>unweighted</b> <b>and</b> <b>weighted</b> double spin asymmetries related to the transversal helicity distribution g_ 1 T and the longitudinal transversity distribution h_ 1 L^, two of eight leading-twist transverse momentum dependent parton distributions (TMDs) or three-dimensional parton distribution functions (3 dPDFs), in the polarized proton-antiproton Drell-Yan process at typical kinematics on the Facility for Antiproton and Ion Research (FAIR). We conclude that FAIR is ideal to access the new 3 dPDFs towards a detailed picture of the nucleon structure. Comment: 6 latex pages, 5 figures, version for publication in EPJ...|$|R
40|$|AbstractThe Wiener {{polynomial}} of a graph G is a {{generating function}} for the distance distribution dd(G) =(D 1,D 2,…,Dt), where Di {{is the number}} of unordered pairs of distinct vertices at distance i from one another and t is the diameter of G. We use the Wiener polynomial and several related generating functions to obtain generating functions for distance distributions of <b>unweighted</b> <b>and</b> <b>weighted</b> graphs that model certain large classes of computer networks. These provide a straightforward means of computing distance and timing statistics when designing new networks or enlarging existing networks...|$|R
40|$|The SAS system V. 8 {{implements}} the computation of <b>unweighted</b> <b>and</b> <b>weighted</b> kappa statistics as {{an option}} in the FREQ procedure. A major limitation of this implementation is that the kappa statistic can only be evaluated {{when the number of}} raters is limited to 2. Extensions to the case of multiple raters due to Fleiss (1971) have not been implemented in the SAS system. A SAS macro called MAGREE. SAS, that can handle the case of multiple raters is available at the SAS Institute’s web site (check it a...|$|R
30|$|We propose new {{efficient}} trust computation {{schemes that}} can replace {{any of the}} above schemes. Our schemes enable the initiator to compute <b>unweighted</b> (additive) <b>and</b> <b>weighted</b> (non additive) trust with low communication complexity of O(n) (large) messages.|$|R
