1|1368|Public
40|$|Choosing {{the size}} of a case-control study is a basic design problem. Recently, O'Neill (1) {{proposed}} confidence interval estimation of the odds ratio as a basis for sample size determination in the unmatched design. Using recent findings of Breslow (2) and of Connett et al. (3), this paper extends O'Neill's results to the case-control study employing matching of controls to cases. Data from the case-control study with R controls matched on a covariate vector X = (Xi, X 2, [...] .) for each case are presented in table 1. The odds ratio is assumed constant over the levels of X (4). The assumption should be tested once the data are in hand (5). Presence and absence of the risk agent is denoted by Au Ao and of the disease under study by Du Do- Here, zwT is the frequency with which the case shows the outcome level A, while T controls out of the R matched controls show the outcome level Ai (for v = 0, l; T = 0, 1, [...] ., R). The Z,Tprovide a basis for statistical inference on. SAMPLE SIZE REQUIREMENTS As in O'Neill's formulation, approximate 1 — a confidence limits on are given by exp|/J ± Zo(Var p) 1 / 2 (1) Here, Za is the value of the <b>unit</b> <b>normal</b> <b>variate</b> exceeded with probability a/ 2, and / 3 = In ^ is the estimated log odds ratio. However, the estimator / 3 and its variance in the matched case-control design are different from those used by O'Neill for the unmatched design. Instead...|$|E
40|$|AbstractIn {{this paper}} first a {{characterization}} of the multivariate skew normal distribution is given. Then the joint moment generating functions of two quadratic forms, and a linear compound and a quadratic form in skew <b>normal</b> <b>variates,</b> have been derived and conditions for their independence are given. Distribution of the ratios of quadratic forms in skew <b>normal</b> <b>variates</b> has also been studied...|$|R
40|$|We offer a {{procedure}} for generating a gamma variate as the cube of a suitably scaled <b>normal</b> <b>variate.</b> It is fast and simple, assuming {{one has a}} fast way to generate normal variables. In brief: generate a <b>normal</b> <b>variate</b> x and a uniform variate U until In(U) 1. The gamm{{a procedure}} is particularly fast for C implementation if the <b>normal</b> <b>variate</b> is generated in-line, via the #define feature. We include such an inline version, based on our ziggurat method. With it, and an inline uniform generator, gamma variates can be produced in 400 MHz CPUs at better than 1. 3 million per second, with the parameter a changing from call to call. Categories and Subject Descriptors: G. 4 [Mathematics of Computing]: Mathematical Software; 1. 6 [Computing Methodologies]: Simulation and Modeling. link_to_OA_fulltex...|$|R
3000|$|Two sets of n {{standard}} <b>Normal</b> <b>variates</b> are randomly generated (Nd(0, 1), Nh(0, 1)) and column {{bound to}} form matrix [N] [...]...|$|R
5000|$|Let z [...] (z1, …, zN)T be a vector whose {{components}} are N independent standard <b>normal</b> <b>variates</b> (which can be generated, for example, {{by using the}} Box-Muller transform).|$|R
3000|$|... and z is a {{standard}} <b>normal</b> <b>variate.</b> In addition, if the assets have mean returns, μ, the portfolio return may be accessed {{with the knowledge of}} three numbers m=a [...]...|$|R
5000|$|... where Z is a {{standard}} <b>normal</b> <b>variate,</b> independent of both Z1 and Z2, ε is a zero-mean error and d is a parameter. From these relationships, the associated RMM quantile function is (Shore, 2011): ...|$|R
50|$|The polar form {{requires}} 3/2 multiplications, 1/2 logarithm, 1/2 square root, and 1/2 division {{for each}} <b>normal</b> <b>variate.</b> The {{effect is to}} replace one multiplication and one trigonometric function with a single division and a conditional loop.|$|R
5000|$|By {{the normal}} {{approximation}} to a binomial {{this is the}} squared of one standard <b>normal</b> <b>variate,</b> and hence is distributed as chi-squared with 1 degree of freedom. Note that the denominator is one standard deviation of the Gaussian approximation, so can be written ...|$|R
5000|$|This {{method of}} {{producing}} a pair of independent standard <b>normal</b> <b>variates</b> by radially projecting a random point on the unit circumference to a distance given by the square root of a chi-square-2 variate is called the polar method for generating a pair of normal random variables, ...|$|R
40|$|A b s t r a c t Ratios of {{quadratic}} {{forms in}} <b>normal</b> <b>variates</b> arise in many econometrie and statistical applications. Their exact distribution can be computed using e. g. a procedure due to Imhof (1961). In this paper two examples arising in dynamic models will be considered. First, {{the distribution of}} a test of linear restrictions on the coeffi-cients of a regression model with autocorrelated errors will be analyzed. Second, {{the distribution of the}} sample autocorrelations of a series generated by an autoregressive- integrated- moving average model will be investigated. In both cases, the exact distribution will be compared with the (approximate) distributions used in applied work. Some remarks on the usefulness of the exact distribution of quotients of quadratic forms in <b>normal</b> <b>variates</b> conclude the paper...|$|R
40|$|It {{is shown}} that matrix quotients of submatrices of a spherical matrix are {{distributed}} as matrix Cauchy. This generalizes known results for scalar ratios of independent <b>normal</b> <b>variates.</b> The derivations are simple and {{make use of}} the theory of invariant measures on manifolds. Cauchy quotients invariant measures matrix variates spherical distributions...|$|R
40|$|Grant No. AFOSR- 62 [...] 148 Methods of {{construction}} of cumulative sum control charts for folded <b>normal</b> <b>variates</b> are described. These charts {{are likely to}} be useful Then the sign of an approximately normally distributed quantity is lost in measurement. Some assessment is given of the information lost by omission of the sign...|$|R
5000|$|Expanding Yk, as {{given on}} the right-hand-side, into a Taylor series around zero, {{in terms of}} powers of Z (the {{standard}} <b>normal</b> <b>variate),</b> and then taking expectation on both sides, assuming that cZ<<η so that η+cZ≈η, an approximate simple expression for the k-th non-central moment, based on the first six terms in the expansion, is: ...|$|R
5000|$|Generate {{independent}} gamma , and <b>normal</b> [...] <b>variates,</b> {{independently of}} past random variates.|$|R
40|$|In {{this paper}} we extend {{the result of}} Bateman (1949) to obtain the joint {{conditional}} characteristic function of a sum of squares of n noncentral squares of independent <b>normal</b> <b>variates</b> and s 1 of their linear combinations given that these variates lie in a subspace of dimension n - s 2. Characteristic function Chi-bar square Subspace...|$|R
40|$|Strongly {{reproductive}} exponential {{models with}} affine dual foliations {{are known to}} allow of a decomposition analogous to the standard decomposition theorem for Chi-squared distributed quadratic forms in <b>normal</b> <b>variates.</b> It is shown that when the components are identically distributed, then necessarily each component follows the gamma law. affine dual foliations Choquet-Deny theorem decomposition Gamma distribution independence natural exponential family...|$|R
5000|$|... where [...] is {{standard}} <b>normal</b> random <b>variate.</b> The exponential random variate is : ...|$|R
5000|$|The {{basic form}} {{requires}} two multiplications, 1/2 logarithm, 1/2 square root, and one trigonometric function for each <b>normal</b> <b>variate.</b> On some processors, the cosine and sine {{of the same}} argument can be calculated in parallel using a single instruction. Notably for Intel-based machines, one can use the fsincos assembler instruction or the expi instruction (usually available from C as an intrinsic function), to calculate complex ...|$|R
5000|$|Note that ε1 {{represents}} uncertainty (measurement imprecision or otherwise) in {{the explanatory}} variables (included in the LP). This is {{in addition to}} uncertainty associated with the response (ε2). Expressing ε1 and ε2 in terms of standard <b>normal</b> <b>variates,</b> Z1 and Z2, respectively, having correlation ρ, and conditioning Z2 | Z1=z1 (Z2 given that Z1 is equal to a given value z1), we may write {{in terms of a}} single error, ε: ...|$|R
40|$|This article extends and amplifies {{on results}} from a paper of over forty years ago. It {{provides}} software for evaluating the density and distribution functions of the ratio z/w for any two jointly <b>normal</b> <b>variates</b> z, w, and provides details on methods for transforming a general ratio z/w into a standard form, (a+x) /(b+y), with x and y independent standard normal and a, b non-negative constants. It discusses handling general ratios when, in theory, none of the moments exist yet practical considerations suggest there should be approximations whose adequacy can be verified {{by means of the}} included software. These approximations show that many of the ratios of <b>normal</b> <b>variates</b> encountered in practice can themselves be taken as normally distributed. A practical rule is developed: If a < 2. 256 and 4 < b then the ratio (a+x) /(b+y) is itself approximately normally distributed with mean µ = a/(1. 01 b −. 2713) and variance σ 2 = (a 2 + 1) /(b 2 +. 108 b − 3. 795) − µ 2...|$|R
40|$|This paper {{uses the}} {{approach}} of Im, Pesaran and Shin [Im, K. S., Pesaran, M. H., Shin, Y, 2003. Testing for unit roots in heterogeneous panels. Journal of Economics 115, 53 - 74. ] to propose seasonal unit root tests for dynamic heterogeneous panels. The standardised t-bar and F-bar statistics are simply averages of the HEGY tests across groups. These statistics converge to standard <b>normal</b> <b>variates.</b> (C) 2004 Elsevier B. V. All rights reserved. ...|$|R
40|$|Using Weyl's {{formula for}} {{the volume of}} the tube about a {{manifold}} in the unit sphere, we show that the distribution of the squared length of the projection of the <b>normal</b> <b>variate</b> to any smooth convex cone is a mixture of chi-squared distributions and we give the explicit formulas for the weights. We also give the application of our work to circular cones. Weyl's formula Tube volume Chi-bar squared distribution Projection Convex cone...|$|R
40|$|We suggest several goodness-of-fit (GOF) methods {{which are}} {{appropriate}} with Type-II right censored data. Our {{strategy is to}} transform the original observations from a censored sample into an approximately i. i. d. sample of <b>normal</b> <b>variates</b> and then perform a standard GOF test for normality on the transformed observations. A simulation study with several well known parametric distributions under testing reveals the sampling properties of the methods. We also provide theoretical analysis of the proposed method...|$|R
40|$|Abstract In {{this paper}} {{we examine the}} ¯nite-sample {{properties}} of the approximate maximum likelihood estimate (MLE) of the fractional di®erencing parameter d in an ARFIMA(p, d, q) model based on the wavelet coe±cients. Ignoring wavelet coe±cients of higher order of resolution, the remaining wavelet coe±cients approximate a sample of independently and identically distributed <b>normal</b> <b>variates</b> with homogeneous variance within each level. The approximate MLE performs satisfactorily and provides a robust estimate for which the short memory component need not be speci¯ed...|$|R
40|$|Consistency of the {{bootstrap}} second moments {{does not}} usually follow from the proofs of {{consistency of the}} distribution of the bootstrap. Here it is shown that the convergence of the bootstrap distribution to a <b>normal</b> <b>variate</b> implicitly defines a consistent estimator for the asymptotic second moments. The estimator is based on the L-estimation of the scale parameter of arbitrary linear combinations of the bootstrap sequence and uses Classical Minimum Distance techniques to impose the positive semi-definiteness restrictions. Copyright 2005 Royal Economic Society...|$|R
40|$|In this paper, {{we examine}} the finite-sample {{properties}} of the approximate maximum likelihood estimate (MLE) of the fractional differencing parameter d in an ARFIMA(p, d, q) model based on the wavelet coefficients. Ignoring wavelet coefficients of higher order of resolution, the remaining wavelet coefficients approximate a sample of independently and identically distributed <b>normal</b> <b>variates</b> with homogeneous variance within each level. The approximate MLE performs satisfactorily and provides a robust estimate for which the short memory component need not be specified...|$|R
5000|$|Note {{that these}} {{formulas}} do {{not depend on}} the particular <b>unit</b> <b>normal</b> [...] used (there exist two <b>unit</b> <b>normals</b> to any surface at a given point, pointing in opposite directions, so one of the <b>unit</b> <b>normals</b> is the negative of the other one).|$|R
5000|$|Another form is as the {{divergence}} of the <b>unit</b> <b>normal.</b> A <b>unit</b> <b>normal</b> {{is given}} by [...] and the mean curvature is ...|$|R
40|$|Log {{periodogram}} (LP) regression {{is shown}} to be consistent {{and to have a}} mixed normal limit distribution when the memory parameter d= 1. Gaussian errors are not required. The proof relies on a new result showing that asymptotically infinite collections of discrete Fourier transforms (dft 2 ̆ 7 s) of a short memory process at the fundamental frequencies {{in the vicinity of the}} origin can be treated as asymptotically independent <b>normal</b> <b>variates,</b> provided one does not include too many dft 2 ̆ 7 s in the collection...|$|R
40|$|In {{this thesis}} {{we present a}} {{solution}} for the problem of predicting the chemical and physical properties of paper from spectrometric data. We used a data set that consists of over 1000 samples of paper. For each sample 15 chemical and physical properties and its near-infrared spectra were measured. We used the following machine learning methods to predict the properties of paper: linear regression, pace regression, a nearest neighbor-based model, regression trees, a support vector machine, principal component regression, partial least squares regression, a multi-layer perceptron, and a radial basis function network. The prediction task {{turned out to be}} linear. Therefore, linear regression, principal component regression, and partial least squares regression gave the best results. Many outside factors affect the spectra and cause different types of interference. We used the following spectra preprocessing methods to remove the interference and improve the predictions: absorbance transformation, Kubelka-Munk transformation, multiplicative scatter correction, standard <b>normal</b> <b>variate</b> transformation, spectra derivation and orthogonal signal correction. We also investigated how preprocessing affects the machine learning methods. The results show that most preprocessing methods improve the models' predictions. The standard <b>normal</b> <b>variate</b> transformation and multiplicative scatter correction gave the best results. We tried to further improve the predictions with calibration. However, calibration did not improve the predictions...|$|R
40|$|Measurement error {{effect on}} the power of control charts for zero {{truncated}} Poisson distribution and ratio of two Poisson distributions are recently studied by Chakraborty and Khurshid (2013 a) and Chakraborty and Khurshid (2013 b) respectively. In this paper, in addition to the expression for the power of control chart for ZTBD based on standardized <b>normal</b> <b>variate</b> is obtained, numerical calculations are presented to see the effect of errors on the power curve. To study the sensitivity of the monitoring procedure, average run length (ARL) is also considered...|$|R
40|$|Classification of {{data that}} arise as signals or images often {{requires}} a standard-ization step so that information extracted from biologically equivalent signals can be quantified for comparison across classes. Differences in global trend, total en-ergy, high-frequency noise and/or local background can arise from variabilities due to instrumentation or conditions during data collection. This article considers some common ways in which such variation is adjusted for and introduces a generalization of the popular “standard <b>normal</b> <b>variate</b> ” transformation. Examples from three types of spectroscopy data illustrate the method and its properties...|$|R
40|$|Wallace has {{proposed}} {{a new class of}} pseudo-random generators for <b>normal</b> <b>variates.</b> These generators do not require a stream of uniform pseudo-random numbers, except for initialisation. The inner loops are essentially matrix-vector multiplications and are very suitable for implementation on vector processors or vector/parallel processors such as the Fujitsu VPP 300. In this report we outline Wallace's idea, consider some variations on it, and describe a vectorised implementation RANN 4 which is more than three times faster than its best competitors (the Polar and Box-Muller methods) on the Fujitsu VP 2200 and VPP 300...|$|R
40|$|Sample size can be {{calculated}} from many online calculators or tables. But {{the use of these}} instruments is rational only when we understand our input data and the concept behind them completely. Terminologies like confidence interval, confidence limit, standard error of mean, margin of error, standard <b>normal</b> <b>variate,</b> power, significance level etc. and extent to which population size or chances of occurrence of an outcome can affect our sample size remain to be well understood before using these software solutions. [Int J Basic Clin Pharmacol 2012; 1 (3. 000) : 223 - 224...|$|R
40|$|Visible-near-infrared hyperspectral imaging {{was tested}} for its {{suitability}} for monitoring the moisture content (MC) of wood samples during natural drying. Partial least-squares regression (PLSR) prediction of MC {{was performed on}} the basis of average reflectance spectra obtained from hyperspectral images. The validation showed high prediction accuracy. The results were compared concerning the PLSR prediction of MC mapping from raw spectra and standard <b>normal</b> <b>variate</b> (SNV) treatment. SNV pretreatment leads to the best results for visualizing the MC distribution in wood. Hyperspectral imaging has a high potential for monitoring the water distribution of wood. (Résumé d'auteur...|$|R
