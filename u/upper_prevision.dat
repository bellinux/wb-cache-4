12|22|Public
50|$|A {{unification}} {{of many of}} the above mentioned imprecise probability theories was proposed by Walley, although this is in no way the first attempt to formalize imprecise probabilities. In terms of probability interpretations, Walley’s formulation of imprecise probabilities is based on the subjective variant of the Bayesian interpretation of probability. Walley defines upper and lower probabilities as special cases of upper and lower previsions and the gambling framework advanced by Bruno de Finetti. In simple terms, a decision maker’s lower prevision is the highest price at which the decision maker is sure he or she would buy a gamble, and the <b>upper</b> <b>prevision</b> is the lowest price at which the decision maker is sure he or she would buy the opposite of the gamble (which is equivalent to selling the original gamble). If the upper and lower previsions are equal, then they jointly represent the decision maker’s fair price for the gamble, the price at which the decision maker is willing to take either side of the gamble. The existence of a fair price leads to precise probabilities.|$|E
40|$|We {{study the}} {{relation}} between possibility measures and the theory of imprecise probabilities, and argue that possibility measures have {{an important part in}} this theory. It is shown that a possibility measure is a coherent upper probability if and only if it is normal. A detailed comparison is given between the possibilistic and natural extension of an upper probability, both in the general case and for upper probabilities defined on a class of nested sets. We prove in particular that a possibility measure is the restriction to events of the natural extension of a special kind of upper probability, defined on a class of nested sets. We show that possibilistic extension can be interpreted in terms of natural extension. We also prove that when either the upper or the lower cumulative distribution function of a random quantity is specified, possibility measures very naturally emerge as the corresponding natural extensions. Next, we go from upper probabilities to upper previsions. We show that if a coherent <b>upper</b> <b>prevision</b> defined on the convex cone of all non-negative gambles is supremum preserving, then it must {{take the form of a}} Shilkret integral associated with a possibility measure. But at the same time, we show that such a supremum preserving <b>upper</b> <b>prevision</b> is never coherent unless it is the vacuous <b>upper</b> <b>prevision</b> with respect to a non-empty subset of the universe of discourse...|$|E
40|$|We {{study the}} {{relation}} between possibility measures and the theory of imprecise probabilities. It is shown that a possibility measure is a coherent upper probability iff it is normal. We also prove that a possibility measure is the restriction to events of the natural extension of {{a special kind of}} upper probability, defined on a class of nested sets. Next, we go from upper probabilities to upper previsions. We show that if a coherent <b>upper</b> <b>prevision</b> defined on the convex cone of all positive gambles is supremum preserving, then it must {{take the form of a}} Shilkret integral associated with a possibility measure. But at the same time, we show that a supremum preserving <b>upper</b> <b>prevision</b> is not necessarily coherent! This makes us look for alternative extensions of possibility measures that are not necessarily supremum preserving, through natural extension. ...|$|E
40|$|AbstractThis paper compares four {{measures}} {{that have been}} advocated as models for uncertainty in expert systems. The measures are additive probabilities (used in the Bayesian theory), coherent lower (or <b>upper)</b> <b>previsions,</b> belief functions (used in the Dempster-Shafer theory) and possibility measures (fuzzy logic). Special emphasis {{is given to the}} theory of coherent lower <b>previsions,</b> in which <b>upper</b> and lower probabilities, expectations and conditional probabilities are constructed from initial assessments through a technique of natural extension. Mathematically, all the measures can be regarded as types of coherent lower or <b>upper</b> <b>previsions,</b> and this perspective gives some insight into the properties of belief functions and possibility measures. The measures are evaluated according to six criteria: clarity of interpretation; ability to model partial information and imprecise assessments, especially judgements expressed in natural language; rules for combining and updating uncertainty, and their justification; consistency of models and inferences; feasibility of assessment; and feasibility of computations. Each of the four measures seems to be useful in special kinds of problems, but only lower and <b>upper</b> <b>previsions</b> appear to be sufficiently general to model the most common types of uncertainty...|$|R
40|$|AbstractA new {{approach}} to constructing generalised probabilities is proposed. It {{is based on the}} models using lower and <b>upper</b> <b>previsions,</b> or equivalently, convex sets of probability measures. Our approach uses sets of Markov operators in the role of rules preserving desirability of gambles. The main motivation being the operators of conditional expectations which are usually assumed to reduce riskiness of gambles. Imprecise probability models are then obtained in the ways to be consistent with those desirability preserving rules. The consistency criteria are based on the existing interpretations of models using imprecise probabilities. The classical models based on lower and <b>upper</b> <b>previsions</b> are shown to be a special class of the generalised models. Further, we generalise some standard extension procedures, including the marginal extension and independent products, which can be defined independently of the existing procedures known for standard models...|$|R
40|$|We define strong monads of {{continuous}} (lower, <b>upper)</b> <b>previsions,</b> and of forks, modeling both probabilistic and non-deterministic choice. This is an elegant alternative to recent proposals by Mislove, Tix, Keimel, and Plotkin. We show that our monads are sound and complete, {{in the sense}} that they model exactly the interaction between probabilistic and (demonic, angelic, chaotic) choice...|$|R
40|$|This article {{introduces}} {{a new way}} of understanding subjective probability and its generalization to lower and <b>upper</b> <b>prevision.</b> Instead of asking whether a person is willing to pay given prices for given risky payoffs, we ask whether the person believes he can {{make a lot of money}} at those prices. If not [...] -if the person is convinced that no strategy for exploiting the prices can make him very rich in the long run [...] -then the prices measure his subjective uncertainty about the events involved. This ne...|$|E
40|$|In {{this paper}} {{the theory of}} {{coherent}} imprecise previsions is applied to risk measurement. We introduce the notion of coherent risk measure defined on an arbitrary set of risks, showing {{that it can be}} considered a special case of coherent <b>upper</b> <b>prevision.</b> We also prove that our definition generalizes the notion of coherence for risk measures defined on a linear space of random numbers, given in literature. Consistency properties of Value-at-Risk (V aR), currently one of the most used risk measures, are investigated too, showing that it does not necessarily satisfy a weaker notion of consistency called ‘avoiding sure loss’. We introduce sufficient conditions for VaR to avoid sure loss and to be coherent. Finally we discuss ways of modifying incoherent risk measures into coherent ones...|$|E
40|$|This {{study is}} about {{developing}} some further ideas in imprecise probability models of financial risk measures. A financial risk measure {{has been interpreted}} as an <b>upper</b> <b>prevision</b> of imprecise probability, which through the conjugacy relationship {{can be seen as}} a lower prevision. The risk measures selected in the study are value-at-risk (VaR) and conditional value-at-risk (CVaR). The notion of coherence of risk measures is explained. Stocks that are traded in the financial markets (the risky assets) are seen as the gambles. The study makes a determination through computation from actual assets data whether the risk measure assessments of gambles (assets) are coherent as an imprecise probability. It is observed that coherence of assessments depends on the asset's returns distribution characteristic...|$|E
40|$|Coherent <b>upper</b> {{and lower}} <b>previsions</b> are {{becoming}} more and more popular as a mathematical model for robust valuations under uncertainty. Likewise, the mathematically equivalent class of coherent risk measures is attracting a lot attention in mathematical finance. In this paper, we show that a misinterpretation of <b>upper</b> <b>previsions</b> demands a closer examination of the basis of the theory of imprecise previsions. As a consequence, we obtain a new interpretation of coherent lower previsions as fair prices, a class of coherent variability measures, and a new type of conditioning for coherent lower previsions...|$|R
40|$|AbstractIn this paper, we {{show that}} {{coherent}} <b>upper</b> and lower <b>previsions</b> as well as coherent risk measures are only meaningful {{under the assumption that}} one starts with initial wealth being constantly 0. This implies at least for coherent <b>upper</b> and lower <b>previsions</b> a correction of their interpretation, especially coherent <b>upper</b> <b>previsions</b> turn out to represent infimum short selling prices instead of infimum selling prices and coherent lower previsions represent fair prices. We elaborate this meaning of coherent lower previsions by identifying a class of coherent variability measures and present a way to extend coherence to all possible situations of initial wealth. Since a coherent risk measure is the negative of a coherent lower prevision, all results presented in this paper can easily be reformulated in terms of risk measures. Finally, we sketch how corresponding results can be obtained when replacing coherence by convexity...|$|R
40|$|We explore {{some little}} {{investigated}} {{aspects of the}} well known betting scheme defining coherent lower or <b>upper</b> <b>previsions</b> in terms of admissible gains. A limiting situation (lose-or-draw) where the supremum of some gain is zero is discussed, deriving a gambler’s gain evaluations and comparing {{the differences between the}} imprecise and precise prevision cases. Then, the correspondence of the betting scheme for imprecise previsions with real-world situations is analysed, showing how the gambler’s profit objectives may compel him to select certain types of bets...|$|R
40|$|In {{this paper}} {{coherent}} risk measures and other currently used risk measures, notably Value-at-Risk (V aR), are studied {{from the perspective}} of the theory of coherent imprecise previsions. We introduce the notion of coherent risk measure defined on an arbitrary set of risks, showing that it can be considered a special case of coherent <b>upper</b> <b>prevision.</b> We also prove that our definition generalizes the notion of coherence for risk measures defined on a linear space of random numbers, given in literature. We also show that Value-at-Risk does not necessarily satisfy a weaker notion of coherence called ‘avoiding sure loss ’ (ASL), and discuss both sufficient conditions for V aR to avoid sure loss and ways of modifying V aR into a coherent risk measure...|$|E
40|$|In {{this paper}} we analyze, mainly in a finitary setting, the {{consistency}} properties of fuzzy possibilities, interpreting them as instances of upper previsions and applying the basic notions of avoiding sure loss and coherence from {{the theory of}} imprecise probabilities. It ensues that fuzzy possibilities always avoid sure loss, but satisfy the stronger coherence condition only in a special case. Their natural extension, i. e. their least–committal correction to a coherent <b>upper</b> <b>prevision,</b> is determined. The same analysis is then performed when min {{is replaced by a}} T–norm (or seminorm) in the definition of fuzzy possibility, showing that the consistency properties and also the natural extension remain the same. Some “closure ” properties are also discussed, which are guaranteed to hold if the T–norm is continuous, and are satisfied by (ordinary) possibilities too...|$|E
40|$|AbstractThis article {{introduces}} {{a new way}} of understanding subjective probability and its generalization to lower and <b>upper</b> <b>prevision.</b> Instead of asking whether a person is willing to pay given prices for given risky payoffs, we ask whether the person believes he can {{make a lot of money}} at those prices. If not––if the person is convinced that no strategy for exploiting the prices can make him very rich in the long run––then the prices measure his subjective uncertainty about the events involved. This new understanding justifies Peter Walley’s updating principle, which applies when new information is anticipated exactly. It also justifies a weaker principle that is more useful for planning because it applies even when new information is not anticipated exactly. This weaker principle can serve as a basis for flexible probabilistic planning in event trees...|$|E
40|$|AbstractThe personalist {{conception}} of probability is often explicated {{in terms of}} betting rates acceptable to an individual. A common approach, that of de Finetti for example, assumes that the individual {{is willing to take}} either side of the bet, so that the bet is “fair” from the individual’s point of view. This can sometimes be unrealistic, and leads to difficulties in the case of conditional probabilities or previsions. An alternative conception is presented in which it is only assumed that the collection of acceptable bets forms a convex cone, rather than a linear space. This leads to the more general {{conception of}} an <b>upper</b> conditional <b>prevision.</b> The main concerns of the paper are with the extension of <b>upper</b> conditional <b>previsions.</b> The main result is that any <b>upper</b> conditional <b>prevision</b> is the <b>upper</b> envelope of a family of additive conditional previsions...|$|R
40|$|AbstractThe article {{considers}} estimating a parameter θ in an imprecise probability model (P¯θ) θ∈Θ {{which consists}} of coherent <b>upper</b> <b>previsions</b> P¯θ. After {{the definition of a}} minimum distance estimator in this setup and a summarization of its main properties, the focus lies on applications. It is shown that approximate minimum distances on the discretized sample space can be calculated by linear programming. After a discussion of some computational aspects, the estimator is applied in a simulation study consisting of two different models. Finally, the estimator is applied on a real data set in a linear regression model...|$|R
5000|$|Perhaps {{the most}} {{straightforward}} generalization is {{to replace a}} single probability specification with an interval specification. Lower and upper probabilities, denoted by [...] and , or more generally, lower and <b>upper</b> expectations (<b>previsions),</b> aim to fill this gap: ...|$|R
40|$|The {{relationship}} is studied between possibility and necessity measures dened on arbitrary spaces, {{the theory of}} imprecise probabilities, and elementary random set theory. It is shown how special random sets {{can be used to}} generate normal possibility and necessity measures, as well as their natural extensions. This leads to interesting alternative formulas for the calculation of these natural extensions. Keywords|Upper probability, <b>upper</b> <b>prevision,</b> coherence, natural extension, possibility measure, random sets. I. Introduction P OSSIBILITY measures were introduced by Zadeh [1] in 1978. In his view, these supremum preserving set functions are a mathematical representation of the information conveyed by typical armative statements in natural language. For recent discussions of this interpretation within the behavioural framework of the theory of imprecise probabilities, we refer to [2], [3], [4]. Supremum preserving set functions can also be found in the literature under a number o [...] ...|$|E
40|$|The paper {{discusses}} {{integration in}} possibility theory, both in an ordinal {{and in a}} numerical (behavioral) context. It is shown that in an ordinal context, the fuzzy integral has {{an important part in}} at least three areas: the extension of possibility measures to larger domains, the construction of product measures from marginals and the denition of conditional possibilities. In a numerical (behavioral) context, integration can be used to extend upper probabilities to upper previsions. It is argued that the role of the fuzzy integral in this context is limited, as it can only be used to dene a coherent <b>upper</b> <b>prevision</b> if the associated upper probability is 0 - 1 -valued, in which case it moreover coincides with the Choquet integral. These results are valid for arbitrary coherent upper probabilities, and therefore also relevant for possibility theory. It follows from the discussion that in a numerical context, the Choquet integral is better suited than the fuzzy integral for producing coherent upper previsions starting from possibility measures. At the same time, alternative expressions for the Choquet integral associated with a possibility measure are derived...|$|E
40|$|AbstractThis paper {{presents}} {{a summary of}} Peter Walley’s theory of coherent lower previsions. We introduce three representations of coherent assessments: coherent lower and <b>upper</b> <b>previsions,</b> closed and convex sets of linear previsions, and sets of desirable gambles. We show also how the notion of coherence {{can be used to}} update our beliefs with new information, and a number of possibilities to model the notion of independence with coherent lower previsions. Next, we comment on the connection with other approaches in the literature: de Finetti’s and Williams’ earlier work, Kuznetsov’s and Weischelberger’s work on interval-valued probabilities, Dempster–Shafer theory of evidence and Shafer and Vovk’s game-theoretic approach. Finally, we present a brief survey of some applications and summarize the main strengths and challenges of the theory...|$|R
40|$|AbstractData-based {{decision}} theory under imprecise probability {{has to deal}} with optimization problems where direct solutions are often computationally intractable. Using the Γ-minimax optimality criterion, the computational effort may significantly be reduced {{in the presence of a}} least favorable model. Buja [A. Buja, Simultaneously least favorable experiments. I. Upper standard functionals and sufficiency, Zeitschrift für Wahrscheinlichkeitstheorie und Verwandte Gebiete 65 (1984) 367 – 384] derived a necessary and sufficient condition for the existence of a least favorable model in a special case. The present article proves that essentially the same result is valid in case of general coherent <b>upper</b> <b>previsions.</b> This is done mainly by topological arguments in combination with some of Le Cam’s decision theoretic concepts. It is shown how least favorable models could be used to deal with situations where the distribution of the data as well as the prior is allowed to be imprecise...|$|R
50|$|The {{allowance}} for imprecision, or {{a gap between}} a decision maker's <b>upper</b> and lower <b>previsions,</b> is the primary difference between precise and imprecise probability theories. Interestingly, such gaps arise naturally in betting markets which happen to be financially illiquid due to asymmetric information.|$|R
40|$|Abstract. The right {{interpretation}} of subjective probability {{is implicit in}} the theories of upper and lower odds, and <b>upper</b> and lower <b>previsions,</b> developed, respectively, by Cedric Smith (1961) and Peter Walley (1991). On this interpretation you are free to assign contingent events the probability 1 (and thus to employ conditionalization {{as a method of}} probability revision) without becoming vulnerable to a weak Dutch book...|$|R
50|$|Imprecise {{probability}} {{is understood}} {{in a very}} wide sense. It {{is used as a}} generic term to cover all mathematical models which measure chance or uncertainty without sharp numerical probabilities. It includes both qualitative (comparative probability, partial preference orderings, …) and quantitative modes (interval probabilities, belief functions, <b>upper</b> and lower <b>previsions,</b> …). Imprecise probability models are needed in inference problems where the relevant information is scarce, vague or conflicting, and in decision problems where preferences may also be incomplete http://www.sipta.org/.|$|R
50|$|The Society is now {{concerned}} with many activities around {{the theme of}} imprecise probabilities.Imprecise probability is understood in a very wide sense. It {{is used as a}} generic term to cover all mathematical models which measure chance or uncertainty without sharp numerical probabilities. It includes both qualitative (comparative probability, partial preference orderings,...) and quantitative models (interval probabilities, belief functions, <b>upper</b> and lower <b>previsions,...).</b> Imprecise probability models are needed in inference problems where the relevant information is scarce, vague or conflicting, and in decision problems where preferences may also be incomplete.|$|R
40|$|The paper {{deals with}} a {{possibilistic}} imprecise second-order probability model. It is argued that such models appear naturally {{in a number of}} situations. They lead to the introduction of a new type of previsions, called possibilistic previsions, which formally generalise coherent <b>upper</b> and lower <b>previsions.</b> The converse problem is also looked at: given a possibilistic prevision, under what conditions can it be generated by a second-order possibility distribution? This leads to the definition of normality, representability and natural extension of possibilistic previsions. Finally, some attention is paid to the special class of full possibilistic previsions, which can be formally related to Zadeh's fuzzy probabilities. The results have immediate applicability in decision making and statistical reasoning...|$|R
40|$|The {{generalized}} Bayes’ rule (GBR) {{can be used}} {{to conduct}} ‘quasi-Bayesian’ analyses when prior beliefs are represented by imprecise probability models. We describe a procedure for deriving coherent imprecise probability models when the event space consists of a finite set of mutually exclusive and exhaustive events. The procedure is based on Walley’s theory of <b>upper</b> and lower <b>prevision</b> and employs simple linear programming models. We then describe how these models can be updated using Cozman’s linear programming formulation of the GBR. Examples are provided to demonstrate how the GBR can be applied in practice. These examples also illustrate the effects of prior imprecision and prior-data conflict on the precision of the posterior probability distribution. Copyright Springer 2005 imprecise probability, generalized Bayes’ rule, second-order probability, quasi-Bayesian analysis,...|$|R
40|$|AbstractAlthough {{financial}} risk measurement is a largely investigated research area, {{its relationship with}} imprecise probabilities has been mostly overlooked. However, risk measures {{can be viewed as}} instances of <b>upper</b> (or lower) <b>previsions,</b> thus letting us apply the theory of imprecise previsions to them. After a presentation of some well known risk measures, including Value-at-Risk or VaR, coherent and convex risk measures, we show how their definitions can be generalized and discuss their consistency properties. Thus, for instance, VaR may or may not avoid sure loss, and conditions for this can be derived. This analysis also makes us consider a very large class of imprecise previsions, which we termed convex previsions, generalizing convex risk measures. Shortfall-based measures and Dutch risk measures are also investigated. Further, conditional risks can be measured by introducing conditional convex previsions. Finally, we analyze the role in risk measurement of some important notions in the theory of imprecise probabilities, like the natural extension or the envelope theorems...|$|R
40|$|In {{most of the}} cases, imprecise {{probability}} {{is represented}} by means of probability intervals, <b>upper</b> and lower <b>previsions,</b> or credal sets (closed and convex sets of probability distributions). However, there is a language based on sets of desirable gambles [4, 2, 1] that presents some advantages. First, it is more general than the other models, {{and for this reason}} it was advocated by [3] as the unifying theory of imprecise probability. The second reason, is that in spite of this generality, many of the most important concepts and results are more easily expressed, justified, and proved using desirable gambles. This talk is devoted to discuss and support the use of this model. I shall illustrate with examples the representation of a variety of situations in which there is uncertainty and partial ignorance. Then, I shall concentrate in the concepts of natural extension, conditioning, and independence, showing variations of the basic axioms for different notions of conditioning. A very important property of this model is the possibility of expressing how to make conditioning to events of probability equal to zero. I shall discuss the implication...|$|R
40|$|Uncertainty exists {{frequently}} in {{our knowledge of}} the real world. Two forms of uncertainty are considered. One is variability coming from stochasticity. The other is epistemic uncertainty, also called 2 nd order uncertainty and other names as well. Often it comes from ignorance or imprecision. In principle, this kind of uncertainty can be reduced by additional empirical data;Stochasticity is well studied in the field of probability theory. A variety of methods have been developed to address epistemic uncertainty. Some of these approaches are confidence limits, discrete convolutions, probabilistic arithmetic, Monte Carlo simulation, copulas, stochastic dominance, clouds, and distribution envelope determination. Belief and plausibility curves, <b>upper</b> and lower <b>previsions,</b> left and right envelopes and probability boxes designate an important type of representation for bounded uncertainty about distribution;Some methods combine probability theory and interval Mathematics; Intervals have the potential for bounding the result of an operation. Discretization error coming from discretizing distributions may be bounded by intervals. Distribution envelope determination (DEnv) uses interval based analysis. If the dependency is not specified, result bounds will include the entire range of possible dependencies. These bounds will be wider than if a particular dependency is specified. I have worked on new algorithms to process the dependency relationships. Pearson correlation can be used to improve the results, for example. Also partial dependence information might be available in the form of unimodality or of probability over a specified area of a joint distribution. If this information is used in the calculation, more accurate results can be obtained than that without using this information. Another situation is uncertainty about the parameters of a distribution. All these topics are researched in this work. They are implemented in the software we call Statool;Based on the developed methods, uncertainty can be flexibly considered and added into models. This can make the model closer to real situations. One problem posed by Sandia National Laboratory is studied in this work. Other applications include Pert networks, decision models and others...|$|R

