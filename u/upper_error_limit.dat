0|1968|Public
40|$|Many {{problems}} in signal processing and statistical inference involve finding sparse solution to some underdetermined linear system of equations. This {{is also the}} application condition of compressive sensing (CS) which can find the sparse solution from the measurements {{far less than the}} original signal. In this paper, we propose l 1 - and l 2 -norm joint regularization based reconstruction framework to approach the original l 0 -norm based sparseness-inducing constrained sparse signal reconstruction problem. Firstly, it is shown that, by employing the simple conjugate gradient algorithm, the new formulation provides an effective framework to deduce the solution as the original sparse signal reconstruction problem with l 0 -norm regularization item. Secondly, the <b>upper</b> reconstruction <b>error</b> <b>limit</b> is presented for the proposed sparse signal reconstruction framework, and it is unveiled that a smaller reconstruction error than l 1 -norm relaxation approaches can be realized by using the proposed scheme in most cases. Finally, simulation results are presented to validate the proposed sparse signal reconstruction approach...|$|R
40|$|Abstract. Sensor {{assignment}} {{is an important}} issue in the collaborative tracking, a good sensor assignment method can improve the efficiency of collaborative tracking. This paper proposes a H∞ filtering based sensor assignment modeling method. The method uses relation of set theory to establish the sensor target assignment scheme model, combines the sensor assignment scheme with the filter, regards each sensor detect program as an H ∞ filtering, and chooses the best scheme according to <b>error</b> <b>upper</b> <b>limit</b> γ of H ∞ filtering. This modeling method does not need assumption of statistical characteristics of the target...|$|R
40|$|We {{report an}} {{experimental}} {{determination of the}} diamagnetic correction to the ^ 9 Be^+ ground state hyperfine constant A. We measured A = - 625 008 837. 371 (11) Hz at a magnetic field B of 4. 4609 T. Comparison with previous results, obtained at lower values of B (0. 68 T and 0. 82 T), yields the diamagnetic shift coefficient k = 2. 63 (18) × 10 ^- 11 T^- 2, where A(B) =A_ 0 × (1 +k B^ 2). The zero-field hyperfine constant A_ 0 is determined to be - 625 008 837. 044 (12) Hz. The g-factor ratio g_I^'/g_J is determined to be 2. 134 779 852 7 (10) × 10 ^- 4, which {{is equal to the}} value measured at lower B to within experimental <b>error.</b> <b>Upper</b> <b>limits</b> are placed on some other corrections to the Breit-Rabi formula. The measured value of k agrees with theoretical estimates. Comment: 13 pages, 10 figures. Accepted by Physical Review...|$|R
40|$|AbstractIn {{this paper}} {{two-point}} boundary value problems for systems of second-order differential equations with constant coefficients are studied {{in terms of}} the solution of an algebraic matrix equation related to the problem. The interplay between the differential and the algebraic problems is used to obtain approximate solutions and <b>upper</b> <b>error</b> bounds of the boundary value problem in terms of approximate solutions and <b>upper</b> <b>error</b> bounds of the algebraic matrix equation...|$|R
40|$|This paper explores error {{estimation}} in feed-forward {{neural network}} classifiers from a geometrical perspec-tive. Metrics, expressed as functions of the convex hull, are introduced and developed to capture the geometrical constraints across the single neuron classifier. An <b>upper</b> <b>error</b> bound is found {{as a function of}} convex hull jointed-ness and the underlying conditional probability distribu-tions. A lower bound of the <b>upper</b> <b>error</b> bound is shown to be only a function of n, the number of samples used to form the convex hull. ...|$|R
5000|$|William Shakespeare; Comedy of <b>Errors</b> (<b>Limited</b> Editions Club NY, 1939) ...|$|R
40|$|International audienceThe paper {{presents}} a posteriori error {{estimates for the}} mixed discontinuous Galerkin approximation of the stationary Stokes problem. We consider anisotropic finite element discretizations, i. e. elements with very large aspect ratio. Our analysis covers two- and three-dimensional domains. Lower and <b>upper</b> <b>error</b> bounds are proved with minimal assumptions on the meshes. The lower error bound is uniform {{with respect to the}} mesh anisotropy. The <b>upper</b> <b>error</b> bound depends on a proper alignment of the anisotropy of the mesh which is a common feature of anisotropic error estimation. In the special case of isotropic meshes, the results simplify, and <b>upper</b> and lower <b>error</b> bounds hold unconditionally. The numerical experiments confirm the theoretical predictions and show the usefulness of the anisotropic error estimator...|$|R
40|$|This paper investigates multiwavelength retrievals of median equivolumetric drop {{diameter}} D(sub 0) {{suitable for}} drizzle and light rain, through collocated 355 -/ 527 -nm Micropulse Lidar Network (MPLNET) observations collected during precipitation occurring 9 May 2012 at the Goddard Space Flight Center (GSFC) project site. By applying a previously developed retrieval technique for infrared bands, the method exploits the differential backscatter by liquid water at 355 and 527 nm for water drops larger than approximately 50 micrometers. In {{the absence of}} molecular and aerosol scattering and neglecting any transmission losses, {{the ratio of the}} backscattering profiles at the two wavelengths (355 and 527 nm), measured from light rain below the cloud melting layer, can be described as a color ratio, which is directly related to D(sub 0). The uncertainty associated with this method is related to the unknown shape of the drop size spectrum and to the measurement error. Molecular and aerosol scattering contributions and relative transmission losses due to the various atmospheric constituents should be evaluated to derive D(sub 0) from the observed color ratio profiles. This process is responsible for increasing the uncertainty in the retrieval. Multiple scattering, especially for UV lidar, is another source of error, but it exhibits lower overall uncertainty with respect to other identified error sources. It is found that the total <b>error</b> <b>upper</b> <b>limit</b> on D(sub 0) approaches 50 %. The impact of this retrieval for long-term MPLNET monitoring and its global data archive is discussed...|$|R
40|$|This paper {{presents}} an a posteriori error analysis for a coupled continuum pipe-flow/Darcy model in karst aquifers. We consider a unified anisotropic finite element discretization (i. e. elements with very large aspect ratio). Our analysis covers two-dimensional domains, conforming and nonconforming discretizations {{as well as}} different elements. Many examples of finite elements that are covered by analysis are presented. From the finite element solution, the error estimators are constructed {{and based on the}} residual of model equations. Lower and <b>upper</b> <b>error</b> bounds form the main result with minimal assumptions on the elements. The lower error bound is uniform with respect to the mesh anisotropy in the entire domain. The <b>upper</b> <b>error</b> bound depends on a proper alignment of the anisotropy of the mesh which is a common feature of anisotropic error estimation. In the special case of isotropic meshes, the results simplify, and <b>upper</b> and lower <b>error</b> bounds hold unconditionally. Comment: 26 pages, 3 figures, 1 tablea...|$|R
40|$|Monte Carlo {{integration}} {{is often used}} for antialiasing in rendering processes. Due to low sampling rates only expected error estimates can be stated, and the variance can be high. In this article quasi-Monte Carlo methods are presented, achieving a guaranteed <b>upper</b> <b>error</b> bound and a convergence rate essentially as fast as usual Monte Carlo...|$|R
40|$|AbstractIn {{this paper}} an {{algorithm}} for solving coupled differential matrix systems is presented. By means of algebraic transformations the original problem is transformed in another one {{for which the}} successive approximation method is available. Explicit expressions of the approximate solutions, <b>upper</b> <b>error</b> bounds of them and convergence conditions in terms of data are given...|$|R
40|$|Abstract] 'Materia' {{describes}} {{a model of}} matter that accurately describes many commonly registered phenomenon {{and can be used}} to calculate important physical constants to a high degree of precesion. Some of the important physical constants and particle masses accurately calculated are : (i). The elementary charge quantum e. (materia pgs 221 - 224, accuracy : match upto 11 sig. digits with CODATA recommended value, within CODATA <b>error</b> <b>limits).</b> (ii). The mass of the neutron. (materia pgs 170 - 174, accuracy : match upto 9 sig. digits with CODATA recommended value, within CODATA <b>error</b> <b>limits).</b> (iii) The mass of the W and Z bozons. (materia pgs 121 - 125, W bozon accuracy : match upto 5 sig. digits with PDG recommended value, within PDG <b>error</b> <b>limits)</b> (materia pgs 125 - 130, Z bozon accuracy : match upto 4 sig. digits with PDG recommended value, within PDG <b>error</b> <b>limits).</b> (iv). The Tau particle mass. (materia pgs 175 - 185, accuracy : match upto 6 sig. digits with PDG recommended value, within PDG <b>error</b> <b>limits).</b> (v). The mass of the electron. (materia pgs 206 - 210, accuracy : match upto 10 sig. digits with CODATA recommended value, within CODATA <b>error</b> <b>limits).</b> This paper is lengthy (399 pgs) so please be patient with the review process. But to assure that you are not wasting your time on something useless I have included quickreference sections in this paper for important equations and formulae presented in this paper on Materia:pgs 11 - 12 and quickreference sections for important concepts and laws presented in this paper on Materia:pgs 13 - 22. I hope this would simplify and facitilate the review process. Please review the paper completely before coming to conclusions as the paper introduces many new concepts and one concept may be dependent on other concepts (which are not yet introduced in the paper). So {{it is very important to}} review the whole paper before coming to conclusions. I hope you will find the paper important and interesting enough to justify the lengthy review process. Thanking You, Yours Faithfully, SD. (materia@vsnl. net...|$|R
30|$|We {{obtain the}} <b>upper</b> <b>error</b> bound of the {{algorithm}} (1.9) {{for the independent}} and identical samples with 1 ≤ q ≤ 2. We decomposed the error quantity into the approximation error, the hypothesis error and the sample error and obtained their <b>upper</b> bounds using <b>error</b> analysis techniques developed in learning theory. In some practical applications, we may often encounter the non-i.i.d. sampling processes such as weakly dependent or non-identical processes; see [13, 15, 20]. It may be interesting to continue our error analysis for the non-i.i.d. samples.|$|R
40|$|This paper places {{models of}} {{language}} evolution {{within the framework}} of information theory. We study how signals become associated with meaning. If there is a probability of mistaking signals for each other, then evolution leads to an error limit: increasing the number of signals does not increase the fitness of a language beyond a certain <b>limit.</b> This <b>error</b> <b>limit</b> can be overcome by word formation: a linear increase of the word length leads to an exponential increase of the maximum fitness. We develop a general model of word formation and demonstrate the connection between the <b>error</b> <b>limit</b> and Shannon's noisy coding theorem...|$|R
3000|$|... a posteriori <b>upper</b> <b>error</b> bounds for {{the error}} {{estimates}} of the control, the state, and the co-state. Then we also obtain sharper a posteriori error estimates for the control approximation and error estimates in the L^ 2 norm for the state and co-state on the boundary. Finally, we give a conclusion and some possible future work in Section  4.|$|R
40|$|OSInternational audienceWe {{present a}} unified {{approach}} to build error estimators based on H(div) -reconstructed fluxes on the primal mesh, {{inspired by the}} hypercircle method. Here, the transport equation is considered and discretized by discontinuous Galerkin, nonconforming and conforming finite elements. We describe the local computation of fluxes on patches, obtain <b>upper</b> <b>error</b> bounds and show some numerical tests...|$|R
40|$|The paper {{presents}} a posteriori error estimators for the stationary Stokes problem. We consider anisotropic finite element discretizations (i. e. elements with very large aspect ratio) where conventional, isotropic error estimators fail. Our analysis covers two- and three-dimensional domains, conforming and nonconforming discretizations {{as well as}} different elements. This large variety of settings requires different approaches and results in different estimators. Furthermore many examples of finite element pairs that are covered by the analysis are presented. Lower and <b>upper</b> <b>error</b> bounds form the main result with minimal assumptions on the elements. The lower error bound is uniform {{with respect to the}} mesh anisotropy with the exception of nonconforming 3 D discretizations made of pentahedra or hexahedra. The <b>upper</b> <b>error</b> bound depends on a proper alignment of the anisotropy of the mesh which is a common feature of anisotropic error estimation. In the special case of isotropic meshes, the results simplify, and <b>upper</b> and lower <b>error</b> bounds hold unconditionally. Some of the corresponding results seem to be novel (in particular for 3 D domains), and cover element pairs of practical importance. The numerical experiments confirm the theoretical predictions and show the usefulness of the anisotropic error estimators...|$|R
5000|$|For multi-class k-NN classification, Cover and Hart (1967) prove an <b>upper</b> bound <b>error</b> rate of ...|$|R
40|$|AbstractWe {{present a}} method for {{computing}} a posteriori error bounds for piecewise linear nonconforming approximate solution of elliptic equations. The <b>upper</b> <b>error</b> estimator is a quadratic function of free parameters. Optimal error bounds are obtained by solving a quadratic minimization problem. The feature of the method is that error estimates do not require the high regularity for the exact solution of the original problem...|$|R
40|$|The problem {{considered}} is {{interval estimation}} of the stress - strength reliability R = P(Xθ and λ respectively and a common location parameter μ. Several types of asymptotic, approximate and bootstrap intervals are investigated. Performances are investigated using simulation techniques and compared in terms of attainment of the nominal confidence level, symmetry of lower and <b>upper</b> <b>error</b> rates, and expected length. Recommendations concerning their usage are given...|$|R
30|$|Indeed, {{the lack}} of a gold {{standard}} for the quantification of microcirculatory perfusion and the large heterogeneity of the results (and its associated potential for major <b>error)</b> <b>limits</b> the application of this technology in a clinical setting for the time being.|$|R
5000|$|Class A: The <b>error</b> <b>limits</b> of {{this class}} of glass {{volumetric}} equipment is specified DIN EN ISO 9712. It applies both to Class A itself and to other classes that have an additional [...] "A" [...] designation such as Class AS.|$|R
40|$|We {{present the}} {{temperature}} dependence of absorption and reduced scattering coefficients of 1. 8 % Intralipid measured by frequency-domain photon-migration spectroscopy between 710 and 850 nm. These measurements {{were made in}} the physiologically relevant 30 to 40 °C temperature range. The temperature coefficients for absorption were consistent during heating and cooling and follow closely other reported results. The change in absorption coefficient at 740 nm suggests that a minimum temperature change of 4 °C is observable within the <b>error</b> <b>limits.</b> We found that the reduced scattering coefficient shows a hysteresis with temperature at 740 nm. The temperature coefficient for reduced scattering determined from heating cycle measurements agrees with theory and other measurements within the <b>error</b> <b>limits.</b> 6 page(s...|$|R
40|$|This paper {{presents}} a posteriori error {{estimates for the}} hp-version of the boundary element method. We discuss two first kind integral operator equations, namely Symm's integral equation and the integral equation with a hypersingular operator. The computable <b>upper</b> <b>error</b> bounds indicate an algorithm for the automatic hp-adaptive mesh-refinement. The efficiency of this method is shown by numerical experiments yielding almost optimal convergence even {{in the presence of}} corner singularities...|$|R
40|$|Abstract—The {{analysis}} {{of stability and}} robustness of fuzzy rea-soning {{is an important issue}} in areas like intelligent systems and fuzzy control. An interesting aspect is to what extent the pertur-bation of input in a fuzzy reasoning scheme causes the oscillation of the output. In particular, when the <b>error</b> <b>limits</b> (restrictions) of the input values are given, what the <b>error</b> <b>limits</b> of the output values are. In this correspondence, we estimate the upper and lower bounds of the output error affected by the perturbation parame-ters of the input, and obtain the limits of the output values when the input values range over some interval in many fuzzy reasoning schemes under compositional rule of fuzzy inference (CRI). Index Terms—Error estimation, fuzzy reasoning, fuzzy set, in-terval perturbation, simple perturbation. I...|$|R
40|$|Abstract. We {{demonstrate}} how different characterization techniques, X-ray diffractometry and re-flectometry, spectroscopic ellipsometry, Auger electron spectroscopy, secondary ion mass spectros-copy, and transmission electron microscopy, {{can be used}} to analyze the layer properties of typical SiGe Hetero-Bipolar Transistor (HBT) structures. For three different HBT’s the parameters of Si cap and total SiGe layer thickness, maximum Ge content, and shape of the Ge gradient part are measured and the <b>error</b> <b>limits</b> of the different techniques are discussed. We show that the values obtained agree very well within the <b>error</b> <b>limits.</b> Concerning layer thickness an achievable accuracy of about 1 nm is realistic and reproducible in a routine process. The highest accuracy in Ge content determination of about 0. 5 % can be realized by XRD and well-calibrated SE...|$|R
40|$|Abstract—This paper {{presents}} a new algorithm using semidefinite programming (SDP) relaxation to design IIR digital filters in the minimax sense. Unlike traditional design algorithms which try to directly minimize the <b>error</b> <b>limit,</b> the proposed algorithm employs a bisection searching procedure {{to locate the}} minimum <b>error</b> <b>limit</b> of the approximation error. Given a fixed <b>error</b> <b>limit</b> at each iteration, the SDP relaxation technique is adopted to formulate the design problem in a convex form. In practice, the true minimax design cannot be always obtained. Thus, a regularized feasibility problem is adopted in the bisection searching procedure. The stability of designed filters can also be guaranteed by adjusting the regularization coefficient. Unlike other iterative design methods, the proposed algorithm tries to find a feasible solution at each iteration of the iterative procedure within a feasible set defined by the relaxed constraints. This feasible set is not restricted within the neighborhood of a given point obtained from the previous iteration. Thus, the proposed method can avoid being trapped in the locally minimum point. Four examples are to be {{presented in this paper}} to demonstrate the effectiveness of the proposed method. Index Terms—Bisection searching procedure, infinite impulse response (IIR) digital filter, minimax design, rank minimization, semidefinite programming (SDP) relaxation, trace approximation. I...|$|R
40|$|To {{introduce}} to {{the students}} the operation of various electronic Instruments which are {{used to measure the}} electronic parameters. Course Outcomes: Students would be able to: 1. understand operation of different instruments. 2. describe different terminology related to measurements. 3. understand the principles of various types of transducers and sensors. UNIT-I: Electro Mechanical instruments and their characteristics: Static characteristics, Dynamic Characterist ics. Errors: Gross error, systematic <b>error,</b> Random <b>error,</b> <b>limiting</b> <b>error,</b> Probable error...|$|R
3000|$|Beyond the a posteriori error {{analysis}} of [7], the additional stabilisation {{term in the}} discretisation of this paper causes an additional difficulty in that the Galerkin orthogonality does not hold for the natural residual. Inspired from novell developments in the a posteriori error control of elliptic PDEs motivated by inexact solve [14 – 17], this section presents some guaranteed <b>upper</b> <b>error</b> bound for the discretisation at hand for any approximation u [...]...|$|R
40|$|This paper {{presents}} <b>Limited</b> <b>Error</b> Fitness (LEF), {{a modification}} {{to the standard}} supervised learning approach in Genetic Programming (GP), in which an individual's fitness score is based on how many cases remain uncovered in the ordered training set after the individual exceeds an <b>error</b> <b>limit.</b> The training set order and the <b>error</b> <b>limit</b> are both altered dynamically {{in response to the}} performance of the fittest individual in the previous generation. LEF allows standard GP to readily solve the Boolean Even N Parity problem (a very hard classification problem for GP) for N= 6 and N= 7 with a population size of 400, otherwise, Automatically Defined Functions, a more powerful representation, and much larger populations, are required for GP to solve for N? 5. Individual fitness evaluations run more quickly, but LEF usually requires many more generations. Also a smaller population size allows GP to be run on smaller computers at a reasonable speed. LEF changes the dynamics of GP, preventing premat [...] ...|$|R
40|$|The {{purpose of}} the present work was to {{determine}} the size sample in order to estimate soluble solid contents in yellow melon plot. Samples of fifty fruits were taken of fifteen yellows melon plots grown in Mossoró- Assu Agricultural Pole. Evaluation for total soluble solid contents was done in all fruit samples. The procedures used for sample size estimation were bootstrap method and classical expression with 5 % and 10 % of <b>error</b> <b>limit.</b> The sample sizes estimated by bootstrap method, in average, yielded estimates close to those observed in the classical expression with 10 % of <b>error</b> <b>limit,</b> although in ten plots these sample sizes were greater in the bootstrap method. The sample size recommended to estimate the total soluble solid contents in cultivated plots of melon fruits is fifteen...|$|R
5000|$|... #Subtitle level 3: Parameter <b>errors,</b> {{confidence}} <b>limits,</b> residuals etc.|$|R
30|$|Where {{possible}} each {{experiment was}} done in triplicate and {{means and standard deviations}} calculated using Microsoft Excel. Survival curves were plotted as the means with standard deviations as error bars. In order to allow plotting survival curves on a logarithmic scale, because zero cannot be plotted on a logarithmic scale, one was added to each mean viable count. In some cases error bars were obscured by the graph symbols and in others only <b>upper</b> <b>error</b> bars were plotted.|$|R
40|$|A new {{approach}} for computing <b>upper</b> <b>error</b> bounds for reduced-order models of linear time-varying systems is presented. It {{is based on}} a transformation technique of the Hankel singular values using positive-real, odd incremented functions. By applying such time-varying functions, the singular values to be removed can be forced to become equal and constant, {{so that they can be}} reduced. Two variations of this method are proposed: one for finite-time horizons and the other for infinite-time problems including periodic systems...|$|R
40|$|We obtain error bounds for {{monotone}} approximation schemes of {{a particular}} Isaacs equation. This {{is an extension of}} the theory for estimating errors for the Hamilton-Jacobi-Bellman equation. For obtaining <b>upper</b> <b>error</b> bound, we consider the ``Krylov regularization'' of the Isaacs equation to build an approximate sub-solution of the scheme. To get lower error bound we extend the method of Barles and Jakobsen which consists in introducing a switching system whose solutions are local super-solutions of the Isaacs equation...|$|R
40|$|In {{this study}} we use {{disturbed}} Newton-like methods to approximate a locally unique solution of a nonlinear equation containing a nondifferentiable term. We choose the linear operators to be inverted at {{every step of the}} computation of the Newton iterates to be associated not only with the differentiable part of the equation as it has happened so far but also with the nondifferentiable part. This way the <b>upper</b> <b>error</b> bounds on the distances are smaller than before...|$|R
