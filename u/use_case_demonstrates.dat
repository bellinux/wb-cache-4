14|10000|Public
40|$|Abstract. neXtProt {{provides}} a comprehensive knowledgebase on human proteins complemented by an extensive cross incorporation of annotations from many databases. With {{the diversity of}} published data, provenance information becomes critical to providing reliable and trustworthy services to scientists, thus the tracking of provenance in open, decentralized systems is especially important. Since the nanopublication system addresses many of these challenges, we have developed the neXtProt Linked Data by serializing in RDF/XML annotations specific to neXtProt and started employing the nanopublication model to give appropriate attribution to all data. Specifically, a <b>use</b> <b>case</b> <b>demonstrates</b> the handling of post-translational modification (PTM) data modeled as nanopublications to illustrate how the different levels of provenance and data quality thresholds can be captured in this model...|$|E
40|$|We present MiningZinc, a novel {{system for}} con- straint-based pattern mining. It {{provides}} a declarative approach to data mining, where a user specifies {{a problem in}} terms of constraints and the system employs advanced techniques to efficiently find solutions. Declarative programming and modeling are common in artificial intelligence and in database systems, {{but not so much}} in data mining; by building on ideas from these communities, MiningZinc advances the state-of-the-art of declarative data mining significantly. Key components of the MiningZinc system are (1) a high-level and natural language for formalizing constraint-based itemset mining problems in models, and (2) an infrastructure for executing these models, which supports both specialised mining algorithms as well as generic constraint solving systems. A <b>use</b> <b>case</b> <b>demonstrates</b> the generality of the language, as well as its flexibility towards adding and modifying constraints and data, as well as the use of different solution methods. status: publishe...|$|E
40|$|Cloud {{computing}} offers services which {{promise to}} meet continuously increasing computing demands {{by using a}} large number of networked resources. However, data heterogeneity remains a major hurdle for data interoperability and data integration. In this context, a Knowledge as a Service (KaaS) approach has been proposed with the aim of generating knowledge from heterogeneous data and making it available as a service. In this paper, a Collaborative Knowledge as a Service (CKaaS) architecture is proposed, with the objective of satisfying consumer knowledge needs by integrating disparate cloud knowledge through collaboration among distributed KaaS entities. The NIST cloud computing reference architecture is extended by adding a KaaS layer that integrates diverse sources of data stored in a cloud environment. CKaaS implementation is domain-specific; therefore, this paper presents its application to the disaster management domain. A <b>use</b> <b>case</b> <b>demonstrates</b> collaboration of knowledge providers and shows how CKaaS operates with simulation models...|$|E
30|$|New <b>use</b> <b>cases</b> to <b>demonstrate</b> {{these two}} mechanisms.|$|R
50|$|The ONOS {{software}} {{has been}} used as a platform that applications have been written on top of or has been integrated into other projects. A number of <b>use</b> <b>cases</b> <b>demonstrate</b> how the software is being used today—including global research networking deployments, multilayer network control, and central office re-designed as a datacenter.|$|R
40|$|International audiencePharmML 1 is an XML-based {{exchange}} format 2 - 4 {{created with}} a focus on nonlinear mixed-effect (NLME) models used in pharmacometrics, 5, 6 but providing a very general framework that also allows describing mathematical and statistical models such as single-subject or nonlinear and multivariate regression models. This tutorial provides an overview of the structure of this language, brief suggestions on how to work with it, and <b>use</b> <b>cases</b> <b>demonstrating</b> its power and flexibility. © 2017 ASCPT All rights reserved...|$|R
40|$|Due to {{its wide}} use in personal, but most importantly, {{professional}} contexts, email represents a valuable {{source of information}} that can be harvested for understanding, reengineering and repurposing undocumented business processes of companies and institutions. Towards this aim, a few researchers investigated the problem of extracting process oriented information from email logs in order to take benefit of the many available process mining techniques and tools. In this paper we go further in this direction, by proposing a new method for mining process models from email logs that leverage unsupervised machine learning techniques with little human involvement. Moreover, our method allows to semi-automatically label emails with activity names, {{that can be used for}} activity recognition in new incoming emails. A <b>use</b> <b>case</b> <b>demonstrates</b> the usefulness of the proposed solution using a modest in size, yet real-world, dataset containing emails that belong to two different process models. Comment: 18 pages, 6 figure...|$|E
40|$|Abstract. The {{development}} of Linked Data {{provides the opportunity}} for databases to supply extensive volumes of biological data, information, and knowledge in a machine interpretable format to make previously isolated data silos interoperable. To increase ease of use, often databases incorporate annotations from several different resources. Linked Data can overcome many formatting and identifier issues that prevent data interoperability, but the extensive cross incorporation of annotations between databases makes the tracking of provenance in open, decentralized systems especially important. With the diversity of published data, provenance information becomes critical to providing reliable and trustworthy services to scientists. The nanopublication system addresses many of these challenges. We have developed the neXtProt Linked Data by serializing in RDF/XML annotations specific to neXtProt and started employing the nanopublication model to give appropriate attribution to all data. Specifically, a <b>use</b> <b>case</b> <b>demonstrates</b> the handling of post-translational modification (PTM) data modeled as nanopublications to illustrate the how the different levels of provenance and data quality thresholds can be captured in this model...|$|E
40|$|Hundreds of {{thousands}} of pedestrians are involved in severe traffic accidents every year world-wide. Reasons for these accidents include complex and highly dynamic traffic situations where views are obstructed or unexpected movement occurs. Driver assistance systems are a valid option for increasing pedestrian safety by enhancing the awareness of complex traffic situations and identifying potential dangers. In this work, we present a collision avoidance system based on smart video surveillance and car-to-infrastructure communication. We use a distributed system of monocular cameras to determine the position of both vehicles and pedestrians in realtime. In addition, we utilize standard car- 2 -x communication technology (ETSI ITS G 5) to provide all position detections to the vehicles, thus enabling complex use cases such as warning cascades to drivers in case of oncoming dangers. A detailed evaluation of the proposed system and collision warning <b>use</b> <b>case</b> <b>demonstrates</b> the suitability as assistance system for human drivers. We also show that automatic braking systems would lead to drastic performance improvements due to a significant reduction of reaction times...|$|E
40|$|Traceability and {{rationale}} {{management are}} highly important—especially in distributed collaborative software development projects {{due to a}} lack of mutual awareness and informal coordination among the participating stakeholders. Therefore this papers presents a tool for extracting, visualizing, and analyzing the relationships between requirements and other artifacts, activities as well as users within a distributed software project using a collaborative development environment. Underlying requirements and the conceptual design of this tool are based on several real-world <b>use</b> <b>cases</b> <b>demonstrating</b> the respective value contribution of the tool’s functionality...|$|R
50|$|The ODRL language, {{currently}} at Version 2.0, defines a comprehensive information policy framework through publication of two specifications: the ODRL Version 2.0 Core Model, and ODRL Version 2.0 Common Vocabulary. Included within the ODRL documentation {{are a number}} of basic <b>use</b> <b>cases</b> <b>demonstrating</b> how to implement policy expressions using the Core Model with terms from the Common Vocabulary. ODRL is fully extensible and provides a mechanism for new communities to extend and/or deprecate the ODRL Common Vocabulary used in conjunction with the Core Model.|$|R
30|$|Recontextualization {{can be used}} as {{a trigger}} for self-* {{operations}} that cannot misinterpret events. This is in contrast to a VM trying to analyse when an event had occurred via its own internal procedures, for example based on when its external IP address changes. A distributed file system <b>use</b> <b>case</b> <b>demonstrated</b> the feasibility of our approach. In the <b>use</b> <b>case</b> recontextualization is <b>used</b> to reconfigure the VM to recover from performance degradation experienced as a result of VM migration. Although the degradation is partly synthetic and correlated to the network settings used, we demonstrate that performance gains can be achieved by runtime optimization triggered by recontextualization.|$|R
40|$|It is {{essential}} to understand the operation sequences of a production system when designing or changing it. This paper will demonstrate how the software tool Sequence Planner (SP) not only supports this understanding by sequence visualization, but also improves the solution using optimization and verification. SP is a tool for modeling and analyzing automation systems. The tool has been developed since 2007 with an initial focus on supporting engineers when developing control code for programmable logical controllers. Today, SP is a micro-service architecture, usable in various areas like runtime control, online monitoring, energy optimization, and even emergency department patient planning. This paper presents a use case at an automotive company, where the operation sequences in {{a large number of}} automated robot stations, need to be modified. SP, together with virtual commissioning tools, automates this modification by identifying, optimizing, verifying and simulating operation sequences, and then updates the robot and control programs. This <b>use</b> <b>case</b> <b>demonstrates</b> the strength of SP and its architecture and how it is used for integrated virtual preparation and commissioning...|$|E
40|$|Using Web {{applications}} {{with multiple}} modalities, and especially speech {{is a difficult}} problem. For instance, the navigation differs from GUI based navigation, since in speech, the user interface has to be serialized. Current web technologies make applications usable only with one modality at the time (HTML vs. Voice XML). This paper presents an idea of using XForms as the UI description language, and automatically creating UIs for different modalities based on a single XForms descrip-tion. Three approaches of realizing speech UIs are de-scribed, and dialog-based approach is chosen to suit best the automatical creation of a speech UI. There are several features in XForms, which make the task easier compared to HTML forms. XForms pro-vides datatypes to the filled data, and it supports dynamic changes in the UI based on user input. The paper also describes an implementation of this idea. The implementation {{is now part of}} the open-source X-Smiles XML browser. The implementation allows multimodal interaction on any XForms. It uses Sphinx- 4 and FreeTTS as the ASR and TTS implementations, re-spectively. A use case was developed to demonstrate the func-tionality of the implementation. The use case is a airline ticket reservation system. The <b>use</b> <b>case</b> <b>demonstrates</b> in-put and output of several datatypes, as well as navigation within a form. 1...|$|E
40|$|Web {{crawlers}} {{are increasingly}} used for focused {{tasks such as}} the extraction of data from Wikipedia or the analysis of social networks like last. fm. In these cases, pages are far more uniformly structured than in the general Web and thus crawlers can use the structure of Web pages for more precise data extraction and more expressive analysis. In this demonstration, we present a focused, structurebased crawler generator, the “Not so Creepy Crawler ” (nc 2). What sets nc 2 apart, is that all analysis and decision tasks of the crawling process are delegated to an (arbitrary) XML query engine such as XQuery or Xcerpt. Customizing crawlers just means writing (declarative) XML queries that can access the currently crawled document {{as well as the}} metadata of the crawl process. We identify four types of queries that together suffice to realize a wide variety of focused crawlers. We demonstrate nc 2 with two applications: The first extracts data about cities from Wikipedia with a customizable set of attributes for selecting and reporting these cities. It illustrates the power of nc 2 where data extraction from Wiki-style, fairly homogeneous knowledge sites is required. In contrast, the second <b>use</b> <b>case</b> <b>demonstrates</b> how easy nc 2 makes even complex analysis tasks on social networking sites, here exemplified by last. fm...|$|E
40|$|Abstract. A virtual {{observatory}} {{will not}} only enhance many current scientific investigations, {{but it will also}} enable entirely new scientific explorations due to both the federation of vast amounts of multiwavelength data and the new archival services which will, as a necessity, be developed. The detailing of specific science <b>use</b> <b>cases</b> is important in order to properly facilitate the development of the necessary infrastructure of a virtual observatory. The understanding of high velocity clouds is presented as an example science <b>use</b> <b>case,</b> <b>demonstrating</b> the future synergy between the data (either catalog or images), and the desired analysis in the new paradigm of a virtual observatory. 1...|$|R
40|$|Summary: This {{deliverable}} describes Web-enabled public showcases {{for public}} dissemination and {{presentation of the}} project. The showcase is built around publicly available, Web-based CODE prototypes. The purpose of each prototype is briefly described followed by typical <b>use</b> <b>case</b> descriptions <b>demonstrated</b> along a typical <b>use</b> <b>case...</b>|$|R
30|$|Create <b>use</b> <b>cases</b> which <b>demonstrate</b> the {{triggering}} of a blockchain-based P 2 P energy trade {{based on}} measurements {{from a home}} smart meter in order {{to carry out a}} full end-to-end simulation with hardware in the loop.|$|R
40|$|The {{numerous}} {{public data}} resources make integrative bioinformatics experimentation increasingly important in life sciences research. However, it is severely {{hampered by the}} way the data and information are made available. The semantic web approach enhances data exchange and integration by providing standardized formats such as RDF, RDF Schema (RDFS) and OWL, to achieve a formalized computational environment. Our semantic web-enabled data integration (SWEDI) approach aims to formalize biological domains by capturing the knowledge in semantic models using ontologies as controlled vocabularies. The strategy is to build a collection of relatively small but specific knowledge and data models, which together form a ‘personal semantic framework’. This can be linked to external large, general knowledge and data models. In this way, the involved scientists are familiar with the concepts and associated relationships in their models and can create semantic queries using their own terms. We studied the applicability of our SWEDI approach {{in the context of a}} biological use case by integrating genomics data sets for histone modification and transcription factor binding sites. Results: We constructed four OWL knowledge models, two RDFS data models, transformed and mapped relevant data to the data models, linked the data models to knowledge models using linkage statements, and ran semantic queries. Our biological <b>use</b> <b>case</b> <b>demonstrates</b> the relevance of these kinds of integrative bioinformatics experiments. Our findings show high startup costs for the SWEDI approach, but straightforward extension with similar data...|$|E
40|$|We {{live in the}} {{information}} age. Data has become an essential asset for most everyday situations and business interactions. The need to share data, to generate information, and create new knowledge from that data is common to all fields of research and all economic activity. Managing data is a critical, and sometimes costly, process. When not properly defined, data might become incomplete, inconsistent or, even worse, unusable. Requirements for data evolve and we must define new data or update existing data over the entire data lifecycle. Evolving data requirements {{is an important issue}} and a technological challenge as {{it is not possible to}} define, in advance, information structures that meet requirements you do not yet know. Specifying information requirements is particularly challenging in domains such as manufacturing where information exchange involves many actors and sharing across multiple functions and software applications. As a result, it becomes hard to find a common information structure for representing data. The challenge is even bigger when a temporal aspect has to be considered since it requires the ability to extend {{the information}} structure dynamically over time. One area within the manufacturing domain that we have identified with these characteristics is Product Lifecycle Management (PLM). PLM involves many global actors using a myriad of software applications that perform a series of product management functions that can last from weeks to decades. Because the mechanism to extend models is static by its nature, requiring numerous updates of the initial information model, this operation is expensive in cost and time, and requires and understanding of the entire initial model to ensure correct extensions are developed. This research presents an alternative based on dynamic customization of information models in the context of PLM, by leveraging existing PLM standards and frameworks, and using emerging semantic web technologies such as OWL, SPARQL and SPIN. Following a state of the art in Chapter 2, Chapter 3 defines technical requirements used to evaluate existing PLM standards and frameworks. Based on the analysis of this evaluation, Chapter 4 presents new framework components for defining dynamically customizable information models for PLM. In chapter 5 these components are integrated together into a framework, and a <b>use</b> <b>case</b> <b>demonstrates</b> the efficiency of the framework. Chapter 6 concludes the research and introduces ideas for future research. Pas de résumé en françai...|$|E
40|$|Pas de résumé en françaisWe {{live in the}} {{information}} age. Data has become an essential asset for most everyday situations and business interactions. The need to share data, to generate information, and create new knowledge from that data is common to all fields of research and all economic activity. Managing data is a critical, and sometimes costly, process. When not properly defined, data might become incomplete, inconsistent or, even worse, unusable. Requirements for data evolve and we must define new data or update existing data over the entire data lifecycle. Evolving data requirements {{is an important issue}} and a technological challenge as {{it is not possible to}} define, in advance, information structures that meet requirements you do not yet know. Specifying information requirements is particularly challenging in domains such as manufacturing where information exchange involves many actors and sharing across multiple functions and software applications. As a result, it becomes hard to find a common information structure for representing data. The challenge is even bigger when a temporal aspect has to be considered since it requires the ability to extend {{the information}} structure dynamically over time. One area within the manufacturing domain that we have identified with these characteristics is Product Lifecycle Management (PLM). PLM involves many global actors using a myriad of software applications that perform a series of product management functions that can last from weeks to decades. Because the mechanism to extend models is static by its nature, requiring numerous updates of the initial information model, this operation is expensive in cost and time, and requires and understanding of the entire initial model to ensure correct extensions are developed. This research presents an alternative based on dynamic customization of information models in the context of PLM, by leveraging existing PLM standards and frameworks, and using emerging semantic web technologies such as OWL, SPARQL and SPIN. Following a state of the art in Chapter 2, Chapter 3 defines technical requirements used to evaluate existing PLM standards and frameworks. Based on the analysis of this evaluation, Chapter 4 presents new framework components for defining dynamically customizable information models for PLM. In chapter 5 these components are integrated together into a framework, and a <b>use</b> <b>case</b> <b>demonstrates</b> the efficiency of the framework. Chapter 6 concludes the research and introduces ideas for future research. DIJON-BU Doc. électronique (212319901) / SudocSudocFranceF...|$|E
40|$|This paper {{presents}} {{the current state}} of development of GlamMap, a visualisation tool that displays library metadata on an interactive, computer-generated geographic map. The focus in the paper is on the most crucial improvement achieved {{in the development of the}} tool: GlamMapping Trove. The visualisation of Trove’s sixty-million book records is possible thanks to an improved database structure, more efficient data retrieval, and more scalable visualisation algorithms. The paper analyses problems encountered in visualising massive datasets, describes remaining challenges for the tool, and presents a <b>use</b> <b>case</b> <b>demonstrating</b> GlamMap’s ability to serve researchers in the history of ideas...|$|R
40|$|Collective {{intelligence}} {{is currently a}} hot topic within the Web and Geoinformatics communities. Research into ways of producing advances with collective {{intelligence is}} becoming increasingly popular. This article introduces a novel approach to collective intelligence {{with the use of}} geographic knowledge discovery to determine spatially referenced patterns and models from the Geospatial Web which are used for supporting decisions. The article details the latest Web 2. 0 technologies which make geographic knowledge discovery from the Geospatial Web possible to produce advanced collective intelligence. The process is explored and illustrated in detail, and <b>use</b> <b>cases</b> <b>demonstrate</b> the potential usefulness. Finally, potential pitfalls are discussed...|$|R
40|$|We {{present the}} design and {{implementation}} of a real-time 3 D graphics library for image-based Constructive Solid Geometry (CSG). This major approach of 3 D modeling has not been supported by real-time computer graphics until recently. We explain two essential image-based CSG rendering algorithms, and we introduce an API that provides a compact access to their complex functionality and implementation. As an important feature, the CSG library seamlessly integrates application-defined 3 D shapes as primitives of CSG operations to ensure high adaptability and openness. We also outline optimization techniques to improve the performance {{in the case of}} complex CSG models. A number of <b>use</b> <b>cases</b> <b>demonstrate</b> potential applications of the library. ...|$|R
40|$|Sensor data plays a {{significant}} role in our life. Sensors are devices that can sense our environment. This sensor data is part of the Sensor Web and needs a vast data infrastructure design, because many organisations are involved, the abundance of different sensors and because of the big volume of data and near-real time data. The extent to which sensor data can be used depends on a variety of factors, among them the observations of the sensor, the data about the sensor itself and interoperability between sensor systems. Standards will lead the way to a well-functioning interoperable Sensor Web, meaning all the systems can communicate with and understand each other. Therefore, the sensor standards need to be discoverable, interoperable and usable in their own domain. Currently this is not yet the case. There is not sufficient information available about the gaps and overlaps in the Sensor Web {{and there are a lot}} of different standards and therefore the systems are not capable of communicating to each other. Currently the standards are made and maintained by different organizations. There is one cluster of standards assembled by Open Geospatial Consortium (OGC), called Sensor Web Enablement (SWE), which forms the spine of the Sensor Web, but there are also other non-OGC standards that are useful for the Sensor Web, such as the IEEE standards and W 3 C's Semantic Sensor Network. A <b>use</b> <b>case</b> <b>demonstrates</b> both the usefulness and the gaps and overlaps of the standards in the Sensor Web, because it sketches a real-life situation in which standards can bring interoperability. The Smart Emission project in Nijmegen is a suitable candidate to assist in answering questions regarding sensor standards. In Nijmegen there is a citizen request for air quality data. The Smart Emission Project was initiated to respond to this request. The project is a collaboration between different organizations to to keep the city liveable by incorporating citizen participation in the Sensor Web. Sensors that measure different environmental indicators namely air quality, sound pollution, and meteorological data are placed all over Nijmegen to sense the city. The plan is to provide the data to the citizen for both viewing and downloading via applications. The project team is currently using OGC sensor standards, namely the SensorThingsApi and Sensor Observation Service, but is interested in alternatives, such as for remote access to the sensor, and can profit from research on sensor standards. The requirements for implementation in the use case are extracted from meetings with citizens, network maintainers, experts in the Sensor Web and project members. Validation of requirements from the Smart Emission project demonstrates weaknesses and strong points in the current standards and how they can be used in a combination to provide the right data to the citizen. Validation is done through a data model based mainly on Observations and Measurements. Results show that not all requirements for the use case are met and that standardisation is not achieved for every requirement. To improve standardisation for future projects the adapted Open Systems Interconnection (OSI) model in combination with a digital sensor portal is suggested to be used. The OSI model, adapted from computer science and used for data interoperability on the Internet is an ordering system and can indicate the right standard for the right usage. It is a layered system in which data is added in all the five layers. The chosen data flows through this system and ends up with relevant information for the user and the system. The adapted OSI-model is changed on several aspects. The layers are different than the original OSI-model. Furthermore division between standards is made by categorizing them on implementation method. Using the Adapted OSI-model will fill the gaps and demonstrate the overlaps in the Sensor Web make it more interoperable. In this thesis data modeling of a use case has been used to analyse sensor standards. The results show that in the current situation the sensor standards are insufficient ordered, interoperable and harmonised. The Adapted OSI-model can bring order and the model can open up the way towards a more interoperable sensor web. Architecture and The Built EnvironmentOTBGeomatic...|$|E
40|$|Abstract—Understanding {{information}} and gaining insights about information usually means {{interacting with the}} displayed information. Utilizing the new capabilities of smart meeting rooms, visual outputs of different information representation applications are presented according to the user’s needs. In this paper we present smart interaction management. This interaction approach enables the users to interact with all dis-played views, utilizing the novel capabilities of these environments and besides, to traditionally interact with applications using her local device. We further show two <b>use</b> <b>cases</b> <b>demonstrating</b> typical applications of our approach in such multi-display environments: (1) to modify the arrangement and layout of views and (2) {{to interact with the}} displayed information within a view. I...|$|R
40|$|The Chem 2 Bio 2 RDF portal is a Linked Open Data (LOD) portal for systems {{chemical}} biology aiming for facilitating drug discovery. It converts around 25 different datasets on genes, compounds, drugs, pathways, side effects, diseases, and MEDLINE/PubMed documents into RDF triples and links {{them to other}} LOD bubbles, such as Bio 2 RDF, LODD and DBPedia. The portal is based on D 2 R server and provides a SPARQL endpoint, but adds on few unique features like RDF faceted browser, user-friendly SPARQL query generator, MEDLINE/PubMed cross validation service, and Cytoscape visualization plugin. Three <b>use</b> <b>cases</b> <b>demonstrate</b> the functionality and usability of this portal. Comment: 8 pages, 10 figure...|$|R
40|$|A virtual {{observatory}} {{will not}} only enhance many current scientific investigations, {{but it will also}} enable entirely new scientific explorations due to both the federation of vast amounts of multiwavelength data and the new archival services which will, as a necessity, be developed. The detailing of specific science <b>use</b> <b>cases</b> is important in order to properly facilitate the development of the necessary infrastructure of a virtual observatory. The understanding of high velocity clouds is presented as an example science <b>use</b> <b>case,</b> <b>demonstrating</b> the future synergy between the data (either catalog or images), and the desired analysis in the new paradigm of a virtual observatory. Comment: 6 pages, 4 figures, uses newpasp. sty (included). To be published in the proceedings of the conference "Virtual Observatories of the Future," editors R. J. Brunner, S. G. Djorgovski, and Alex S. Szalay, ASP Conference Series, Volume 22...|$|R
40|$|We {{propose to}} {{demonstrate}} Direct Code Execution (DCE), {{a framework that}} enables to execute nearly unmodified ap-plications and Linux Kernel code jointly with the ns- 3 simu-lator. DCE allows therefore fully deterministic reproducibil-ity of network experiments. DCE also supports larger scale scenarios than real-time emulators by using simulation time dilatation. In this demonstration, we will showcase two main scenarios: (1) a basic example describing how to integrate in DCE the Data Center TCP (DCTCP) Linux kernel patch, and then how to customize this protocol and run it on differ-ent scenarios; (2) a more advanced <b>use</b> <b>case</b> <b>demonstrating</b> how to benefit from DCE to build a rich and realistic eval-uation environment for Software Defined Wireless Networks based on Open vSwitch and the NOX SDN controller...|$|R
40|$|In {{this paper}} we {{argue for the}} use of Unstructured Supplementary Service Data (USSD) as a {{platform}} for universal cell phone applications. We examine over a decade of ICT 4 D research, analyzing how USSD can extend and complement current uses of IVR and SMS for data collection, messaging, information access, social networking and complex user initiated transactions. Based on these findings we identify situations when a mobile based project should consider using USSD with increasingly common third party gateways over other mediums. This analysis also motivates the design and implementation of an open source library for rapid development of USSD applications. Finally, we explore three USSD <b>use</b> <b>cases,</b> <b>demonstrating</b> how USSD opens up a design space not available with IVR or SMS...|$|R
40|$|Abstract. In this paper, {{we present}} a {{multilingual}} matching approach aiming at building matches between terms belonging to multilingual thesauri. The approach {{is presented as a}} variant of the schema matching problem and present its evalua-tion on domain-specific <b>use</b> <b>cases</b> by <b>demonstrating</b> the viability of the proposed technique for facing the multilingual thesaurus matching approach. ...|$|R
40|$|The present {{contribution}} describes {{approaches for}} visualizing building related data (temperature, energy use, etc.). A web based visualization framework is presented {{and a number}} of <b>use</b> <b>cases</b> are <b>demonstrated</b> (i. e. three-dimensional building browsing). Usability is optimized for diverse screen sizes, input methods (i. e. touch screen), and application logics. Finally, guidelines for further user interface development are presented...|$|R
40|$|We {{focus on}} visual {{analysis}} of space- and time-referenced categorical data, which describe possible states of spatial (geographical) objects or locations and their changes over time. The {{analysis of these}} data is difficult as there are only limited possibilities to analyze the three aspects (location, time and category) simultaneously. We present a new approach which interactively combines (a) visualization of categorical changes over time; (b) various spatial data displays; (c) computational techniques for task-oriented selection of time steps. They provide an expressive visualization with regard to either the overall evolution over time or unusual changes. We apply our approach on two <b>use</b> <b>cases</b> <b>demonstrating</b> its usefulness {{for a wide variety}} of tasks. We analyze data from movement tracking and meteorologic areas. Using our approach, expected events could be detected and new insights were gained...|$|R
40|$|While deliberating {{and making}} decisions, {{participants}} in urban development processes need {{easy access to}} the pertinent content scattered among different plans. A Planning Markup Language (PML) has been proposed to represent the underlying structure of plans in an XML-compliant way. However, PML currently covers only textual information and lacks specifications about graphic information used in plans. To fill in this gap, this dissertation develops a PML extension, termed PMLGraphics, with the capacity of marking up graphic content of plans in a ???plan usable??? way. The development of the PMLGraphics can significantly impact how plans are made and used in planning practice. The PMLGraphics is built on theoretical research on ontology of graphic representations in plan documents and relationships between different entities of plan content (i. e. text, single graphic and graphic group). The ontology of graphic representations includes typical graphic types in plans, representation methods used by graphics, and classification of intended plan information conveyed by graphics. The proposed PMLGraphics has three components: document metadata that summarizes general information of plan documents, document structure that outlines hierarchical structure of topics in plans, and document content that defines sets of elements to mark up plan content in text, single graphic, and graphic group, as well as relationships between these three content entities. To test the feasibility of the PMLGraphics, three plans are encoded and a prototype for using the PMLGraphics is designed and implemented. Three hypothetical <b>use</b> <b>cases,</b> which simulate scenarios in practical planning processes, are created to test the PMLGraphics capabilities. The <b>use</b> <b>cases</b> <b>demonstrate</b> the feasibility and applicability of the PMLGraphics in accessing graphic content scattered in different plans made by different agents. The significance of the PMLGraphics for planning participants is that, as <b>use</b> <b>cases</b> <b>demonstrate,</b> graphic plan content accessed through PMLGraphics would have been harder to find, if found at all, using hardcopy sources or using electronic files without graphic markup...|$|R
40|$|Let us {{consider}} a simplified emergency <b>use</b> <b>case</b> to <b>demonstrate</b> {{the potential of}} Geoinformation-services (GI services) in a cross-border region: A case of foot-and-mouth disease has occurred on a Dutch farm. A veterinary from the Dutch environmental authority needs to inform his German colleague of its occurrence. We shall assume that {{the locations of the}} barns or fields have bee...|$|R
40|$|We {{present a}} general visual {{analytics}} architecture that is constructed and implemented to effectively analyze un-structured social media {{data on a}} large scale. Pipelined based on a high-performance cluster configuration, MPI processing, and interactive visual analytics interfaces, our architecture, I-SI, closely integrates data-driven analyti-cal methods and user-centered visual analytics. It creates a coherent analysis environment for identifying event structures, geographical distributions, and key indicators of emerging events. This environment can support moni-toring, analyzing, and responding to latent information extracted from social media. We have applied the I-SI ar-chitecture to collect social media data, analyze the data {{on a large scale}} and uncover the latent social phenomena. To demonstrate the efficacy and applicability of I-SI, we describe several social media <b>use</b> <b>cases</b> in multiple do-mains that were evaluated by experts. The <b>use</b> <b>cases</b> <b>demonstrate</b> that I-SI can benefit a range of users by construct-ing meaningful event structures and identifying precursors to critical events within a rich, evolving set of topics...|$|R
