5|10000|Public
40|$|The {{problem of}} {{recognizing}} and locating isolated objects from their images is addressed, {{and a complete}} procedure is described. The technique {{is based on a}} template matching in the log-polar amplitude spectrum domain and in the space domain. This makes it possible to take properly into account both spectral features and spatial details and to measure with accuracy the scale factor, the orientation, and the absolute position of the object. To reduce the computational burden, pairs of tomographic projections are <b>used</b> <b>in</b> <b>both</b> <b>domains.</b> As an example, the double tomographic method is applied to the classical problem of the automatic reading of printed characters, with the additional specification of unknown size and orientation, and shows good result...|$|E
40|$|Multimedia {{information}} systems are getting commonplace as kiosk systems in public areas or as online {{information systems}} in the Internet. This paper describes {{the development of a}} municipal information system that can be <b>used</b> <b>in</b> <b>both</b> <b>domains.</b> In the public area an ATM based client/ server structure is used which provides gateways for Internet access. The system offers retrieval of authority information using intuitive and associative access modes. We discuss architectural models and tools which {{can be used as a}} system environment. For content representation HTML is extended in order to reflect the designers' impression of such a system. 1 Introduction Ulm and its surrounding regions are developing an information infrastructure for their inhabitants. An ATM-based backbone network shall allow private and commercial users to deploy new telecommunication and telecooperation services at a low participation cost. In order to enforce the development and acceptance of the entire undertaking a [...] ...|$|E
40|$|Abstract More than 20 {{years of}} {{theoretical}} development and practical {{experience in the}} field of Conceptual Information Systems have made available a wide variety of structure and procedures to gain new knowledge from data or to present it in a user-friendly way, by restructuring the data in a conceptual way to help the user interpret and understand the meaning. Even longer, Database Theory has helped develop highly efficient database systems, processing daily huge amounts of data. However, both theories can profit from a cooperation: on the one hand, data and database modeling methodologies could be applied to the building of Conceptual Information System, the connection between the presented conceptual structures and the original data can be clarified. On the other hand, database theory may profit from the experience and ideas for more user-centered interfaces to the stored data, as well as profit from the translation of theoretical results. In this paper, we present the first necessary steps to perform a translation between the languages <b>used</b> <b>in</b> <b>both</b> <b>domains.</b> For this purpose, we introduce basic notions from Database Theory with a focus on the operations, which are basic for a first application: a more formal way to describe the process of Relational Scaling [PW 99] and the transformation of data for Conceptual Information Systems in general. Conversely, we present an approach for a standard problem of database theory by using methods from Formal Concept Analysis. Finally, we discuss the next steps needed for the integration of these two theories. ...|$|E
40|$|AbstractLanguage use or {{choice at}} the border often {{presents}} a multitude of patterns. The language chosen is often determined by {{the participants in the}} communicative act. At the Malaysia-Thai border, where most of the population are Malay but of Thai and/or Malaysian nationalities, the language choices of the speakers present an interesting pattern. This paper aims to present the language choice at the Malaysia-Thai border via questionnaires, interviews and observations. The findings show {{that there might be a}} geographical demarcation line separating the respondents but the language they <b>use</b> <b>in</b> <b>both</b> <b>domains</b> is determined by their ethnicity...|$|R
40|$|The paper {{presents}} a real-time speaker identification {{system based on}} {{the analysis of the}} audio track of a video stream. The system has been employed in the context of automatic video segmentation. It <b>uses</b> features evaluated <b>in</b> <b>both</b> <b>domains</b> of time and frequency. Their combined use significantly improved the performance of the system...|$|R
40|$|Sustainability as {{a concept}} for both ecology and economy {{urgently}} needs {{to be put into}} practice. Problems arise when one is searching for common ground, as ecological and environmental issues cannot be expressed sufficiently in monetary units and economists traditionally disregard ecological and environmental values. Instead of looking for common parameters alone, it is recommended that risk assessments be <b>used</b> for non-sustainability <b>in</b> <b>both</b> <b>domains,</b> though <b>in</b> a concise and decision-oriented way...|$|R
40|$|This {{research}} {{seeks to}} investigate how application of Human Factors techniques {{could be used to}} improve performance resulting from the use of technical traffic management and SCOOT validation systems. The systems <b>used</b> <b>in</b> <b>both</b> <b>domains</b> have historically been developed without consideration given to the social factors important to their use, designs instead being based solely on technical constraints. In the first stages of the project traffic management is investigated through conduction of a literature review covering the objectives, functions and constraints acting upon Traffic Management Centres (TMCs) in road, rail, maritime and air domains. Congestion management is then considered in urban road TMCs through application of the Event Analysis of Systematic Teamwork (EAST) method based on observational data collected from four TMCs, Bristol, Cardiff, Dorset and Nottingham, in which the tasks, social agents, information and relationships between these elements are considered. The EAST method is then expanded to enable investigation into TMCs’ resilience, providing further knowledge about the domain. The later stages of the project are concerned with SCOOT validation, the process by which adaptively controlled traffic lights using SCOOT are set up to reflect real traffic conditions. The domain, using the current PC SCOOT Urban Traffic Control system, is assessed through Cognitive Work Analysis (CWA) with the findings used to propose areas suitable for development. One of these areas, STOC validation, is then developed further by applying Ecological Interface Design to develop an alternative display addressing limitations with PC SCOOT’s display. This concept display is then evaluated through two empirical experiments examining performance compared to traditional displays and investigating the role of experience within the domain. Finally, by using insights obtained into the STOC validation process an automated STOC selection algorithm is developed which has the potential to redefine how STOC validation is conducted...|$|E
40|$|The {{proliferation}} of Internet-connected mobile and situated digital devices {{combined with the}} ubiquity of online collaboration and interaction exposes the need to review the ownership models for data and digital infrastructures that increasingly perform as politicised resources in everyday life. Viewing geography as an important aspect to the socio-cultural context within which potential new forms of ‘bottom-up’ online participation are performed, this thesis analyses the practices surrounding the ownership of, {{as well as the}} participation in urban planning through the various information communication technologies (ICTs) encountered in decisions affecting the material context of cities. In two ethnographic studies of information systems in municipal planning, technology-supported citizen participation is analysed. First, participation records for 597 citizens in a three-year planning process in Lancaster (UK) are used to reconstruct the geographic patterns of participation in relation to places. Then, through 21 participant interviews, the genealogy of municipal planners’ establishment of an infrastructure for participation is outlined and associated practices of participation analysed. Finally, as a critique of possible technical interventions, the challenges of linking various actors’ practices through geospatial technologies are scrutinised in two cases from Helsinki (Finland) and Aarhus (Denmark). From each study recommendations for design interventions are drawn. The findings suggest that ‘local’ participation draws on the materiality of various places. We find that formal participation processes and infrastructures used accounted poorly for the spatial constellation of material context and local actors who exerted a low influence within established formal participation process. To develop technical interventions that support distributing ownership of participation to various local groups within established institutional practices, human computer interactionists need to carefully consider established rules and roles <b>used</b> <b>in</b> <b>both</b> <b>domains,</b> the formal institutions and the many informally-organised actors involved. It is suggested that planners’ role shifts beyond that of a mediator towards that of a facilitator for local actors’ ownership of participation processes, wherein the need for economies of scale and technological compatibility in applying technical interventions may perform as boundaries for sustainable technical interventions. It suggests the scope for third parties to aid this process...|$|E
40|$|A Doctoral Thesis. Submitted in partial {{fulfilment}} of {{the requirements}} for the award of Doctor of Philosophy at Loughborough University. There have been few theoretical advancements {{in the theory of}} light-scattering from particles since the days of Rayleigh and of those who have come later {{in the early twentieth century}} such as Mie, Lorenz and Born. The lack of progressive advancements in this area is a clear indication of the difficulties faced. However, the advancement of optical sensor technology and computer systems, representing thus an empirical advancement, have managed to produce effective means for collecting large amounts of high quality data in a relatively short time. Such difference in advancement between these two approaches signifies the necessity for establishing generic approximations and tools that can be <b>used</b> effectively <b>in</b> <b>both</b> <b>domains.</b> [Continues. ...|$|R
5000|$|One {{theoretical}} framework {{that has been}} used to explain negative spillover is called the role scarcity hypothesis (...) [...] The main argument here is that since people have a limited, fixed amount of resources (e.g., energy, time), problems may arise when different roles draw on these same resources. For example, when both family and work roles draw on the scarce resource of time, it is likely that one of these roles is compromised {{due to a lack of}} available time.A different framework, the role expansion hypothesis (...) , has been used to explain positive spillover. According to this hypothesis, individuals generate resources (e.g., positive mood, skills) and opportunities from the multiple roles they are engaged in. These, in turn, can be <b>used</b> <b>in</b> <b>both</b> life <b>domains</b> to improve functioning and promote growth (...) [...]|$|R
50|$|SYNTAX handles most {{classes of}} {{deterministic}} (unambiguous) grammars (LR, LALR, RLR {{as well as}} general context-free grammars. The deterministic version has been <b>used</b> <b>in</b> operational contexts (e.g., Ada), and is currently <b>used</b> <b>both</b> <b>in</b> the <b>domain</b> of compilation. The non-deterministic features include an Earley parser generator used for natural language processing. Parsers generated by SYNTAX include powerful error recovery mechanisms, and allow the execution of semantic actions and attribute evaluation on the abstract tree or on the shared parse forest.|$|R
40|$|Several {{studies have}} {{investigated}} the encoding and perception of emotional expressivity in music performance. A relevant question concerns how {{the ability to communicate}} emotions in music performance is acquired. In accordance with recent theories on the embodiment of emotion, we suggest here that both the expression and recognition of emotion in music might at least in part rely on knowledge about the sounds of expressive body movements. We test this hypothesis by drawing parallels between musical expression of emotions and expression of emotions in sounds associated with a non-musical motor activity: walking. In a combined production-perception design, two experiments were conducted, and expressive acoustical features were compared across modalities. An initial performance experiment tested for similar feature <b>use</b> <b>in</b> walking sounds and music performance, and revealed that strong similarities exist. Features related to sound intensity, tempo and tempo regularity were identified as been <b>used</b> similarly <b>in</b> <b>both</b> <b>domains.</b> Participants <b>in</b> a subsequent perception experiment were able to recognize both non-emotional and emotional properties of the sound-generating walkers. An analysis of the acoustical correlates of behavioral data revealed that variations in sound intensity, tempo, and tempo regularity were likely used to recognize expressed emotions. Taken together, these results lend support the motor origin hypothesis for the musical expression of emotions. QC 20150126 </p...|$|R
40|$|Autoregressive (AR) {{models are}} {{commonly}} {{obtained from the}} linear autocorrelation of a discrete-time signal to obtain an all-pole estimate of the signal's power spectrum. We {{are concerned with the}} dual, frequency-domain problem. We derive the relationship between the discrete-frequency linear autocorrelation of a spectrum and the temporal envelope of a signal. In particular, we focus on the real spectrum obtained by a type-I odd-length discrete cosine transform (DCT-Io) which leads to the all-pole envelope of the corresponding symmetric squared Hilbert temporal envelope. A compact linear algebra notation for the familiar concepts of AR modeling clearly reveals the dual symmetries between modeling in time and frequency domains. By <b>using</b> AR models <b>in</b> <b>both</b> <b>domains</b> <b>in</b> cascade, we can jointly estimate the temporal and spectral envelopes of a signal. We model the temporal envelope of the residual of regular AR modeling to efficiently capture signal structure in the most appropriate domain...|$|R
40|$|Abstract—Autoregressive (AR) {{models are}} {{commonly}} {{obtained from the}} linear autocorrelation of a discrete-time signal to obtain an all-pole estimate of the signal’s power spectrum. We {{are concerned with the}} dual, frequency-domain problem. We derive the relationship between the discrete-frequency linear autocorrelation of a spectrum and the temporal envelope of a signal. In particular, we focus on the real spectrum obtained by a type-I odd-length discrete cosine transform (DCT-Io) which leads to the all-pole envelope of the corresponding symmetric squared Hilbert temporal envelope. A compact linear algebra notation for the familiar concepts of AR modeling clearly reveals the dual symmetries between modeling in time and frequency domains. By <b>using</b> AR models <b>in</b> <b>both</b> <b>domains</b> <b>in</b> cascade, we can jointly estimate the temporal and spectral envelopes of a signal. We model the temporal envelope of the residual of regular AR modeling to efficiently capture signal structure in the most appropriate domain. Index Terms—Autoregressive (AR) modeling, frequency-domain linear prediction (FDLP), Hilbert envelope, linear prediction in spectral domain (LPSD), temporal noise shaping (TNS). I...|$|R
40|$|The pursuit {{for higher}} {{performance}} {{at a lower}} cost is driving rapid progress in the field of packaged digital systems. As the complexity of interconnects and packages increases, and the rise and fall time of the signal decreases, the electromagnetic effects in distributed passive structures become an important factor in determining the system performance. Hence {{there is a need to}} accurately simulate these parasitic electromagnetic effects that are observed in the signal distribution network (SDN) and the power delivery network (PDN) of an electronic system. The accurate simulation of high-speed systems requires information on the high frequency transient currents that are injected into the power distribution network causing simultaneous switching noise. Existing techniques for determining these transient currents are not sufficiently accurate. Furthermore existing transient simulation techniques suffer from two major drawbacks: 1) they are not scalable and hence cannot be applied to large sized systems, and 2) the time domain simulations violate causality. This dissertation addresses the above-mentioned problems in the domain of high-speed packaging. It proposes a new technique to accurately extract the transient switching noise currents in high-speed digital systems. The extracted switching noise currents can be <b>used</b> <b>in</b> <b>both</b> the frequency <b>domain</b> and the time domain to accurately simulate simultaneous switching noise. The dissertation also proposes a methodology for the transient co-simulation of the SDN and the PDN in high-speed digital systems. The methodology enforces causality on the transient simulation and can be scaled to perform large sized simulations. The validity of the proposed techniques has been demonstrated by their application on a variety of real-world test cases. Ph. D. Committee Chair: Swaminathan, Madhavan; Committee Member: Chatterjee, Abhijit; Committee Member: Davis, Jeffrey; Committee Member: Keezer, David; Committee Member: Sitaraman, sures...|$|R
40|$|Indiana University-Purdue University Indianapolis (IUPUI) Electronic Fuel Control (EFC) valve regulates {{fuel flow}} to the {{injector}} fuel supply line in the Cummins Pressure Time (PT) fuel system. The EFC system controls the fuel flow {{by means of a}} variable orifice that is electrically actuated. The supplier of the EFC valves inspects all parts before they are sent out. Their inspection test results provide a characteristic curve which shows the relationship between pressure and current provided to the EFC valve. This curve documents the steady state characteristics of the valve but does not adequately capture its dynamic response. A dynamic test procedure is developed in order to evaluate the performance of the EFC valves. The test itself helps to understand the effects that proposed design changes will have on the stability of the overall engine system. A by product of this test is the ability to evaluate returned EFC valves that have experienced stability issues. The test determines whether an EFC valve is faulted or not before it goes out to prime time use. The characteristics of a good valve and bad valve can be observed after the dynamic test. In this thesis, a mathematical model has been combined with experimental research to investigate and understand the behavior of the characteristics of different types of EFC valves. The model takes into account the dynamics of the electrical and mechanical portions of the EFC valves. System Identification has been addressed to determine the transfer functions of the different types of EFC valves that were experimented. Methods have been <b>used</b> <b>both</b> <b>in</b> frequency <b>domain</b> as well as time domain. Also, based on the characteristic patterns exhibited by the EFC valves, fuzzy logic has been implemented for the use of pattern classification...|$|R
40|$|Master of ScienceDepartment of Computing and Information SciencesDoina CarageaSeveral {{computational}} biology and bioinformatics problems involve DNA sequence classification using {{supervised machine learning}} algorithms. The performance of these algorithms is largely dependent {{on the availability of}} labeled data and the approach used to represent DNA sequences as feature vectors. For many organisms, the labeled DNA data is scarce, while the unlabeled data is easily available. However, for a small number of well-studied model organisms, large amounts of labeled data are available. This calls for domain adaptation approaches, which can transfer knowledge from a source domain, for which labeled data is available, to a target domain, for which large amounts of unlabeled data are available. Intuitively, one approach to domain adaptation can be obtained by extracting and representing the features that the source domain and the target domain sequences share. Latent Dirichlet Allocation (LDA) is an unsupervised dimensionality reduction technique that has been successfully used to generate features for sequence data such as text. In this work, we explore the use of LDA for generating predictive DNA sequence features, that can be <b>used</b> <b>in</b> <b>both</b> supervised and <b>domain</b> adaptation frameworks. More precisely, we propose two dimensionality reduction approaches, LDA Words (LDAW) and LDA Distribution (LDAD) for DNA sequences. LDA is a probabilistic model, which is generative in nature, and is used to model collections of discrete data such as document collections. For our problem, a sequence {{is considered to be a}} "document" and k-mers obtained from a sequence are "document words". We use LDA to model our sequence collection. Given the LDA model, each document can be represented as a distribution over topics (where a topic can be seen as a distribution over k-mers). In the LDAW method, we use the top k-mers in each topic as our features (i. e., k-mers with the highest probability); while in the LDAD method, we use the topic distribution to represent a document as a feature vector. We study LDA-based dimensionality reduction approaches for both supervised DNA sequence classification, as well as domain adaptation approaches. We apply the proposed approaches on the splice site predication problem, which is an important DNA sequence classification problem in the context of genome annotation. In the supervised learning framework, we study the effectiveness of LDAW and LDAD methods by comparing them with a traditional dimensionality reduction technique based on the information gain criterion. In the domain adaptation framework, we study the effect of increasing the evolutionary distances between the source and target organisms, and the effect of using different weights when combining labeled data from the source domain and with labeled data from the target domain. Experimental results show that LDA-based features can be successfully used to perform dimensionality reduction and domain adaptation for DNA sequence classification problems...|$|R
40|$|Software Product Lines (SPL) and Runtime Adaptation (RTA) have {{traditionally}} been distinct research areas addressing different problems and with different communities. Despite the differences, there are also underlying commonalities with synergies that are worth investigating <b>in</b> <b>both</b> <b>domains,</b> potentially leading to more systematic variability support <b>in</b> <b>both</b> <b>domains.</b> Accordingly, this paper analyses commonality and differences of variability management between SPL and RTA and presents an initial discussion and our perspective on the feasibility of integrating variability management <b>in</b> <b>both</b> areas...|$|R
3000|$|... [...]. By {{assuming}} {{that the number of}} CG iterations is predetermined and identical <b>in</b> <b>both</b> <b>domains,</b> we can <b>use</b> the ratio [...]...|$|R
40|$|Reinforcement {{learning}} and evolutionary strategy {{are two major}} approaches in addressing complicated control problems. Both have strong biological basis {{and there have been}} recently many advanced techniques <b>in</b> <b>both</b> <b>domains.</b> <b>In</b> this paper, we present a thorough comparison between {{the state of the art}} techniques <b>in</b> <b>both</b> <b>domains</b> <b>in</b> complex continuous control tasks. We also formulate the parallelized version of the Proximal Policy Optimization method and the Deep Deterministic Policy Gradient method. Comment: NIPS 2017 Deep Reinforcement Learning Symposiu...|$|R
40|$|Text {{classification}} {{is widely}} <b>used</b> <b>in</b> many realworld applications. To obtain satisfied classification performance, most traditional data mining methods require lots of labeled data, {{which can be}} costly <b>in</b> terms of <b>both</b> time and human efforts. In reality, {{there are plenty of}} such resources in English since it has the largest population in the Internet world, which is not true in many other languages. In this paper, we present a novel transfer learning approach to tackle the cross-language text classification problems. We first align the feature spaces <b>in</b> <b>both</b> <b>domains</b> utilizing some on-line translation service, which makes the two feature spaces under the same coordinate. Although the feature sets <b>in</b> <b>both</b> <b>domains</b> are the same, the distributions of the instances <b>in</b> <b>both</b> <b>domains</b> are different, which violates the i. i. d. assumption in most traditional machine learning methods. For this issue, we propose an iterative feature and instance weighting (Bi-Weighting) method for domain adaptation. We empirically evaluate the effectiveness and efficiency of our approach. The experimental results show that our approach outperforms some baselines including four transfer learning algorithms. ...|$|R
30|$|The paper {{does not}} {{demonstrate}} {{the impact of}} recent technologies in terms of intermodal activities despite these being significant components of rail freight volume and revenue <b>in</b> <b>both</b> <b>domains.</b> The paper also has very limited details on the commercial and competitive realities faced by railways <b>in</b> <b>both</b> <b>domains</b> including rail-on-rail, waterway and road based competition and how this is managed by the train operators. The paper makes superficial reference to the “Blocking Problem” but fail to develop what this implies and how the rail freight operators develop strategies to mitigate this.|$|R
40|$|Tese de mestrado, História e Filosofia das Ciências, Universidade de Lisboa, Faculdade de Ciências, 2010 O Nobel da Física Richard Feynman dizia nas suas Lectures nos anos sessenta que é {{important}}e ter consciência que na física de hoje não temos um conhecimento do que seja a energia. Outros físicos têm salientado a mesma dificuldade. O conceito de energia é também usado em explicações de processos fisiológicos. Neste contexto, o significado de energia é uma questão em aberto. Um estudo recente (Coelho 2009) mostra que os descobridores da energia não encontraram nada que não possa ser destruído ou criado mas antes um princípio de equivalência entre quantidades físicas que não tinham sido até então ligadas. Então surge a questão se esta ideia de equivalência pode ser útil na compreesão da energia nas ciências da vida. Neste contexto será tratada a contribuição de Mayer para o princípio da conservação da energia. A ideia deste princípio tem origem numa observação clínica. Jenstsch 1916 salientou que não havia evidência para a tese de Mayer baseada naquela observação: o sangue venoso é mais escuro nas regiões frias do que nos trópicos porque nas regiões frias se consome mais oxigénio para manter a temperatura do corpo. Isto conduziu à questão se Mayer observou o que ele diz ter observado. Relativamente a esta questão, algumas hipóteses são apresentadas e discutidas nos capítulos 4 e 5 da presente dissertação. No livro de 1845, Mayer generalizou pela primeira vez a ideia de conservação da energia à fisiologia e biologia. Em alguns casos, havia uma investigação empírica. Em muitos casos, os argumentos de Mayer para a generalização são baseados em analogias entre os fenómenos inorgânicos e orgânicos. Os modelos teóricos ou conceptuais usados por Mayer em ambos os domínios são abordados no capítulo 1. The Nobel Laureate Richard Feynman {{said in his}} Lectures in the 60 s that it {{is important}} to realize that we have no knowledge in physics today of what energy is. Other physicists have pointed out the same difficulty. The concept of energy is also <b>used</b> <b>in</b> explanations of physiological processes. In this context, the meaning of energy is an open question. A recent study (Coelho 2009) shows that the discoverers of energy did not find anything which can neither be destroyed nor created but rather a principle of equivalence between physical quantities which had not been connected until then. The question whether this idea of equivalence can be useful in understanding energy in life sciences then arises. In this context, Mayer’s contribution to the principle of energy conservation is dealt with. The idea of this principle has its origin in a clinical observation. Jentsch 1916 pointed out that there was no evidence for Mayer’s thesis based on that observation: the venous blood is darker in cold regions than in the tropics because in cold regions the consuming of oxygen is greater in order to maintain the temperature of the body. This leads to the question of whether Mayer observed what he said he had. Concerning this, some hypotheses are presented and discussed in the chapters 4 and 5 of the present dissertation. In the 1845 book, Mayer generalized the idea of the conservation of energy to physiology and biology, for the first time. In some cases, there was empirical research. In most cases, Mayer’s arguments for that generalisation are based on analogies between inorganic and organic phenomena. The theoretical or conceptual models <b>used</b> by Mayer <b>in</b> <b>both</b> <b>domains</b> are dealt with in the chapter 1...|$|R
25|$|The {{most notable}} supersecondary {{structure}} is a five chain beta sheet that {{is composed of}} a β-meander and a β-α-β clockwise unit. It is present <b>in</b> <b>both</b> <b>domains</b> suggesting that the protein arose from gene duplication.|$|R
30|$|The designations of TLO and FLO {{are meant}} to be applied to each {{subchannel}} individually; but, as seen from the waveforms' time-frequency representation and their positioning, the absence of their mutual overlapping <b>in</b> <b>both</b> <b>domains</b> is not implied.|$|R
30|$|Requirements {{engineering}} plays a {{key role}} <b>in</b> <b>both</b> <b>domain</b> engineering and application engineering. In domain engineering, the requirements of the domain must be defined as common and variable requirements. In application engineering, the requirements for individual products of the SPL are defined by reusing the domain requirements.|$|R
40|$|The {{process of}} the {{acquisition}} of an agreed, shared task model {{as a means to}} structure interaction between expert users and knowledge engineers is described. The role existing (generic) task models play in this process is illustrated for two <b>domains</b> of application, <b>both</b> <b>domains</b> requiring diagnostic reasoning. <b>In</b> <b>both</b> <b>domains</b> different levels of interaction between an expert user and a diagnostic reasoning system are distinguished. ...|$|R
50|$|Kuncel {{and colleagues}} {{investigated}} the predictive {{validity of the}} MAT <b>in</b> <b>both</b> academic and occupational settings. Their meta-analytic study indicated that the MAT is a valid predictor <b>in</b> <b>both</b> <b>domains</b> and that it measures the same abilities as other cognitive ability instruments. Selected validity coefficients from the study are presented in the table below.|$|R
50|$|NMF is an {{instance}} of nonnegative quadratic programming (NQP), just like the support vector machine (SVM). However, SVM and NMF are related at a more intimate level than that of NQP, which allows direct application of the solution algorithms developed for {{either of the two}} methods to problems <b>in</b> <b>both</b> <b>domains.</b>|$|R
40|$|Studies of the DNA-replication {{machinery}} of Archaea have revealed striking similarities {{to that of}} eukaryotes. Indeed, it appears that in most cases Archaea possess a simplified version of the eukaryotic replication apparatus. Studies of Archaea are therefore shedding light on the fundamental processes of DNA replication <b>in</b> <b>both</b> <b>domains</b> of life...|$|R
5000|$|We {{can collect}} {{function}} germs together to construct equivalence classes. One standard equivalence is A-equivalence. We say that two function germs [...] are A-equivalent if there exist diffeomorphism germs [...] and [...] such that : {{there exists a}} diffeomorphic change of variable <b>in</b> <b>both</b> <b>domain</b> and range which takes f to g.|$|R
40|$|Abstract. ABC is a public-domain {{system for}} logic {{synthesis}} and formal verification of binary logic circuits appearing in synchronous hardware designs. ABC combines scalable logic transformations based on And-Inverter Graphs (AIGs), {{with a variety}} of innovative algorithms. A focus on the synergy of sequential synthesis and sequential verification leads to improvements <b>in</b> <b>both</b> <b>domains.</b> This paper introduces ABC, motivates its development, and illustrates its <b>use</b> <b>in</b> formal verification...|$|R
40|$|This paper {{presents}} {{the formulation of}} Port- Controlled Hamiltonian Models from Bond Graphs. For this, the general abstract field representation of BG and its associated standard implicit form are <b>used</b> <b>in</b> order to establish a connection among variables <b>in</b> <b>both</b> <b>domains.</b> This correspondence can be useful to translate energy- and powershaping control system design methods available on one domain into the other. Some examples illustrate the techniques and formulæ presented...|$|R
40|$|This article compares {{methods and}} {{techniques}} <b>used</b> <b>in</b> software engineering with the ones used for handling electronic documents. It shows the common features <b>in</b> <b>both</b> <b>domains,</b> {{but also the}} differences and it proposes an approach which extends the field of document manipulation to document engineering. It shows also in what respect document engineering is different from software engineering. Therefore specific techniques must be developped for building integrated environments for document engineering...|$|R
40|$|This article {{examines}} the expression of number features in relation to (but as distinct from) person features in ASL. We consider parallelisms in the spatial instantiation of agreement features in the nominal and verbal domains, which {{may be seen as}} natural consequences of the existence, <b>in</b> <b>both</b> <b>domains,</b> of functional projections of person, number, and aspect...|$|R
40|$|AbstractExpanding the {{prevalent}} within-domain perspective, {{the present study}} investigated how students' domain-specific ability self-concepts relate to the value they attach to school. With a longitudinal design and a sample of N= 1592 lower secondary school students from n= 82 classes in different educational tracks, we tested the hypothesis that mathematics and verbal self-concept interact in predicting how students value school. In addition to statistically significant main effects, structural equation modeling revealed the expected latent interaction effect. Response surface methodology demonstrated that students valued school more highly when their ability self-concepts were high <b>in</b> <b>both</b> <b>domains</b> rather than just one; a single low self-concept predisposed students to attach less value to school just as much as low self-concepts <b>in</b> <b>both</b> <b>domains</b> did. Helping all students frame attainable goals, thereby providing them with opportunities to experience success across domains, might increase the value they attach to school...|$|R
