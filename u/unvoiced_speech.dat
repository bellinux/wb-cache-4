156|26|Public
50|$|<b>Unvoiced</b> <b>speech</b> {{recognition}} recognizes {{speech by}} observing the EMG activity of muscles associated with speech. It is targeted {{for use in}} noisy environments, and may be helpful for people without vocal cords and people with aphasia.|$|E
50|$|HVXC uses Linear {{predictive}} coding (LPC) with block-wise adaptation every 20ms. The LPC parameters are transformed to Line spectral pair (LSP) coefficients, which are jointly quantized. The LPC residual signal {{is classified as}} either voiced or unvoiced. In the case of voiced speech, the residual is coded in a parametric representation (operating as a vocoder), while {{in the case of}} <b>unvoiced</b> <b>speech,</b> the residual waveform is quantized (thus operating as hybrid speech codec).|$|E
50|$|In {{implementation}} of the source-filter model of speech production, the sound source, or excitation signal, is often modelled as a periodic impulse train, for voiced speech, or white noise for <b>unvoiced</b> <b>speech.</b> The vocal tract filter is, in the simplest case, approximated by an all-pole filter, where the coefficients are obtained by performing linear prediction to minimize the mean-squared error in the speech signal to be reproduced. Convolution of the excitation signal with the filter response then produces the synthesised speech.|$|E
5000|$|One {{systemic}} {{exception is}} that the consonant clusters ds and dš do not change into ts and tš (although d tends to be <b>unvoiced</b> in normal <b>speech</b> in such clusters): ...|$|R
3000|$|In this section, {{we first}} {{consider}} {{the problem of}} modeling the time-trajectory of a sequence of K consecutive LSF parameters. These LSF parameters correspond to a given (all voiced or <b>unvoiced)</b> section of <b>speech</b> signal [...]...|$|R
3000|$|We artificially {{introduce}} shimmer in {{the test}} waveforms to understand the effect of increased shimmer on ASR performance. Pulse positions representing glottal closures are extracted for each test waveform. From {{the location of the}} pulse positions, the voiced and <b>unvoiced</b> segments in <b>speech</b> are determined. To simulate shimmer effects, the speech samples [...]...|$|R
40|$|While a lot {{of effort}} has been made in {{computational}} auditory scene analysis to segregate voiced speech from monaural mixtures, <b>unvoiced</b> <b>speech</b> segregation has not received much attention. <b>Unvoiced</b> <b>speech</b> is highly susceptible to interference due to its relatively weak energy and lack of harmonic structure, and hence makes its segregation extremely difficult. This paper proposes a new approach to segregation of <b>unvoiced</b> <b>speech</b> from nonspeech interference. The proposed system first removes estimated voiced speech, and the periodic part of interference based on cross-channel correlation. The resultant interference becomes more stationary and we estimate the noise energy in unvoiced intervals using segregated speech in neighboring voiced intervals. Then <b>unvoiced</b> <b>speech</b> segregation occurs in two stages: segmentation and grouping. In segmentation, we apply spectral subtraction to generate time–frequency segments in unvoiced intervals. <b>Unvoiced</b> <b>speech</b> segments are subsequently grouped based on frequency characteristics of <b>unvoiced</b> <b>speech</b> using simple thresholding as well as Bayesian classification. The proposed algorithm is computationally efficient, and systematic evaluation and comparison show that our approach considerably improves the performance of <b>unvoiced</b> <b>speech</b> segregation...|$|E
40|$|<b>Unvoiced</b> <b>speech</b> {{separation}} {{is an important}} and challenging problem that has not received much attention. We propose a CASA based approach to segregate <b>unvoiced</b> <b>speech</b> from nonspeech interference. As <b>unvoiced</b> <b>speech</b> does not contain periodic signals, we first remove the periodic portions of a mixture including voiced speech. With periodic components removed, the remaining interference becomes more stationary. We estimate the noise energy in unvoiced intervals {{on the basis of}} segregated voiced speech. Spectral subtraction is employed to extract time-frequency segments in unvoiced intervals, and we group the segments dominated by <b>unvoiced</b> <b>speech</b> by simple thresholding or Bayesian classification. Systematic evaluation and comparison show that the proposed method considerably improves the <b>unvoiced</b> <b>speech</b> segregation performance under various SNR conditions...|$|E
30|$|The inverse ZCR and the {{short-term}} power are combined for ZRMSE (14). In {{contrast to the}} ZCR, it is capable to detect voiced speech. For voiced speech, the value is high as the power typically is high for voiced speech. On the other hand, the high ZCR for <b>unvoiced</b> <b>speech</b> prevents a detector from detecting <b>unvoiced</b> <b>speech.</b> This behavior can be exploited to distinguish between voiced and <b>unvoiced</b> <b>speech.</b>|$|E
40|$|In this paper, a {{new method}} for making v/uv {{decision}} is developed {{which uses a}} multi-feature v/uv classification algorithm based on the analysis of cepstral peak, zero crossing rate, and autocorrelation function (ACF) peak of short-time segments of the speech signal by using some clustering methods. This v/uv classifier achieved excellent results for identification of voiced and <b>unvoiced</b> segments of <b>speech...</b>|$|R
40|$|Abstract Analogue {{amplitude}} modulation radios {{are used for}} air/ground voice communication between aircraft pilots and controllers. The identification of the aircraft, so far always transmitted verbally, could be embedded as a watermark in the speech signal and thereby prevent safety-critical misunderstandings. The {{first part of this}} paper presents an overview on this watermarking application. The second part proposes a speech watermarking algorithm that embeds data in the linear prediction residual of <b>unvoiced</b> narrowband <b>speech</b> at a rate of up to 2 kbit/s. A bit synchroniser is developed which enables the transmission over analogue channels and which reaches the optimal limit within one to two percentage points in terms of raw bit error rate. Simulations show the robustness of the method for the AWGN channel. ...|$|R
40|$|The {{purpose of}} this paper is to {{investigate}} the auditory discrimination skill of Malay children using computer-based method. Currently, most of the auditory discrimination assessments are conducted manually by Speech-Language Pathologist. These conventional tests are actually general tests of sound discrimination, which do not reflect the client's specific speech sound errors. Thus, we propose computer-based Malay auditory discrimination test to automate the whole process of assessment as well as to customize the test according to the specific speech error sounds of the client. The ability in discriminating voiced and <b>unvoiced</b> Malay <b>speech</b> sounds was studied for the Malay children aged between 7 and 10 years old. The study showed no major difficulty for the children in discriminating the Malay speech sounds except differentiating /g/-/k/ sounds. Averagely the children of 7 years old failed to discriminate /g/-/k/ sounds...|$|R
30|$|The {{transform}} coefficients {{based on}} the spectral characteristics of <b>unvoiced</b> <b>speech</b> signals are nearly uniformly distributed in the frequency domain with no obvious decay. Consequently, the sparsity of <b>unvoiced</b> <b>speech</b> signal {{with respect to the}} DCT basis is undesirable. Furthermore, we have not found a satisfactory sparsifying matrix for <b>unvoiced</b> <b>speech</b> signals. Therefore, the usual practice in the framework of CS is to apply the scheme to entire speech signals and not to distinguish voiced speech signals and <b>unvoiced</b> <b>speech</b> signals in advance. Moreover, we find that the overall performance has not been greatly influenced, which can be verified by the simulation results in the following subsection. The reason is that the proportion of voiced speech is more than seventy percent and voiced speech bears dominating information of speech. Certainly, it is of great significance for us to seek to construct a basis or a redundant dictionary for <b>unvoiced</b> <b>speech</b> signals, which is the focus of our future work.|$|E
40|$|<b>Unvoiced</b> <b>speech</b> poses a big {{challenge}} to current monaural speech segregation systems. It lacks harmonic structure and is highly susceptible to interference due to its relatively weak energy. This paper describes {{a new approach to}} segregate <b>unvoiced</b> <b>speech</b> from nonspeech interference. The system first estimates a voiced binary mask, and then performs <b>unvoiced</b> <b>speech</b> segregation in two stages: segmentation and grouping. In segmentation, timefrequency units labeled as 0 in the voiced binary mask are first used to estimate the noise energy and spectral subtraction is then performed to generate time-frequency segments in unvoiced intervals. Based on the type of noise, unvoiced segments are grouped either by selecting segments consistent with those generated by onset/offset analysis or by Bayesian classification of acoustic-phonetic features. Systematic evaluation and comparison show that the proposed approach improves the performance of <b>unvoiced</b> <b>speech</b> segregation considerably. Index Terms — <b>Unvoiced</b> <b>speech</b> segregation, nonspeech interference, spectral subtraction, onset/offset analysis, Bayesian classification 1...|$|E
30|$|After {{the removal}} of {{periodic}} signals, the mixture is composed of only <b>unvoiced</b> <b>speech</b> and a periodic interference. Then, this mixture is segmented by spectral subtraction. Finally, in order to extract only <b>unvoiced</b> <b>speech</b> segments and to remove residual noise, a grouping is carried out.|$|E
30|$|In this work, {{the audio}} signal used for {{experimentation}} is speech signal. Generally, the speech signal has been classified into voiced and unvoiced parts. The voiced {{part of speech}} consists of high-energy and low-frequency component whereas the <b>unvoiced</b> part of <b>speech</b> contains low-energy and high-frequency component [34]. The voiced frames are selected for embedding watermark bits because the distortion created in high-energy frames are less audible compared to low-energy frames. Further, the modification in DCT coefficients of high-energy frames introduces minimal distortion compared to low-energy frames and hence provides better scope of embedding the watermark.|$|R
40|$|Electrical & Computer Engineering ii Co-channel speech {{occurs when}} one speaker’s speech is {{corrupted}} by another speaker’s speech. A co-channel detection system could provie information to suspend the operation of a speech processing system whose operation would be degraded if it were processing co- hannel speech. Two new methods of co-channel speech detection, one based on cyclostationarity and the other based on wavelet transform are developed. Detection of co- hannel speech in this thesis refers to the detection of co-chan el voiced speech, as {{it is not possible}} to detect <b>unvoiced</b> co-channel <b>speech.</b> Cyclostationarity based co-channel speech detection reveals that at least 60 % of co-channel speech is correctly detected for different combinations of speech, e. g., male-male female-f male etc., with false alarms of approximately 16 %. Investigation of the wavelet transform based co-channel speech detection reveals that at least 94 % of co-channel speech is correctly detected with false alarms of approximately 28 % making both methods tools fo...|$|R
40|$|Co-channel speech {{occurs when}} one speaker’s speech is {{corrupted}} by another speaker’s speech. A co-channel detection system could provide information to suspend the operation of any speech processing system whose operation would be degraded if it were processing co-channel speech. In this paper we present two new methods of co-channel speech detection, one based on cyclostationarity and the other based on wavelet transform. Detection of co-channel speech in this paper refers to the detection of co-channel voiced speech, as {{it is not yet}} possible to detect <b>unvoiced</b> co-channel <b>speech.</b> Cyclostationary-based co-channel speech detection reveals that at least 65 % of co-channel speech is correctly detected for different combinations of speech, e. g., male-male, female-female etc., with false alarms of approximately 24 %. Investigation of the wavelet transform based co-channel speech detection reveals that at least 94 % of co-channel speech is correctly detected with false alarms of approximately 28 % making both methods tools for detecting co-channel speech...|$|R
40|$|Abstract — In this paper, {{we present}} {{statistical}} approaches to enhance body-conducted <b>unvoiced</b> <b>speech</b> for silent speech communication. A body-conductive microphone called nonaudible murmur (NAM) microphone is effectively {{used to detect}} very soft <b>unvoiced</b> <b>speech</b> such as NAM or a whispered voice while keeping speech sounds emitted outside almost inaudible. However, body-conducted <b>unvoiced</b> <b>speech</b> is difficult to use in human-to-human speech communication because it sounds unnatural and less intelligible owing to the acoustic change caused by body conduction. To address this issue, voice conversion (VC) methods from NAM to normal speech (NAMto-Speech) and to a whispered voice (NAM-to-Whisper) are proposed, where the acoustic features of body-conducted <b>unvoiced</b> <b>speech</b> are converted into those of natural voices in a probabilistic manner using Gaussian mixture models (GMMs). Moreover, these methods are extended to convert not only NAM but also a body-conducted whispered voice (BCW) as another type of body-conducted <b>unvoiced</b> <b>speech.</b> Several experimental evaluations are conducted to demonstrate {{the effectiveness of the}} proposed methods. The experimental results show that 1) NAM-to-Speech effectively improves intelligibility but it causes degradation of naturalness owing to the difficulty of estimating natural fundamental frequency contours from unvoiced speech; 2) NAM-to-Whisper significantly outperforms NAM-to-Speech in terms of both intelligibility and naturalness; and 3) a single conversion model capable of converting both NAM and BCW is effectively developed in our proposed VC methods. Index Terms — silent speech, body-conducted <b>unvoiced</b> <b>speech,</b> voice conversion, nonaudible murmur, whispered voice. I...|$|E
30|$|Onset/offset {{detection}} {{yields a}} set of segments, usually containing voiced, <b>unvoiced</b> <b>speech,</b> and interference.|$|E
40|$|The {{conversion}} method from Non-Audible Murmur (NAM) {{to ordinary}} speech {{based on the}} statistical voice conversion (NAM-to-Speech) has been proposed towards realization of “silent speech telephone. ” Although NAM-to-Speech converts NAM to intelligible voices with similar quality to speech, {{there is still a}} large problem, i. e., difficulties of the F 0 estimation from <b>unvoiced</b> <b>speech.</b> In order to avoid this problem, we propose a conversion method from NAM to whisper that is a familiar and intelligible <b>unvoiced</b> <b>speech</b> (NAM-to-Whisper). Moreover, we enhance NAM-to-Whisper so that multiple types of body-transmitted <b>unvoiced</b> <b>speech</b> such as NAM and Body Transmitted Whisper (BTW) are accepted as input voices. We evaluate the performance of the proposed conversion method. Experimental results demonstrate that 1) intelligibility and naturalness of NAM are significantly improved by NAM-to-Whisper, 2) NAM-to-Whisper outperforms NAM-to-Speech, and 3) we can train a single conversion model successfully converting both NAM and BTW to the target voice. Index Terms: silent speech telephone, body transmitted <b>unvoiced</b> <b>speech,</b> voice conversion, F 0 estimation, whispe...|$|E
40|$|Abstract — Gaussian process (GP) {{model is}} a {{flexible}} nonparametric Bayesian method that is widely used in regression and classification. In this paper we present a probabilistic method where we solve voice activity detection (VAD) and speech enhancement in a single framework of GP regression, modeling clean speech by a GP smoother. Optimized hyperparameters in GP models lead us to a novel VAD method since learned lengthscale parameters in covariance functions are much different between voiced and <b>unvoiced</b> frames. Clean <b>speech</b> is estimated by posterior means in GP models. Numerical experiments confirm the validity of our method. I...|$|R
40|$|Previous {{research}} on automatic laughter detection has mainly {{been focused on}} audio-based detection. In this study we present an audiovisual approach to distinguishing laughter from speech and we show that the integration of audio and visual information leads to improved performance over single-modal approaches. We consider two cases, one that we discriminate between laughter and speech and one that we discriminate between voiced laughter, <b>unvoiced</b> laughter and <b>speech.</b> When tested on 207 audiovisual sequences, depicting spontaneously displayed (as opposed to posed) laughter and speech episodes, in a person independent way the proposed audiovisual approach achieves an F 1 rate of over 90 % and a classification rate of over 80 %. ...|$|R
40|$|Local Binary Patterns (LBP) {{have been}} used in 2 -D image {{processing}} for applications such as texture segmentation and feature detection. In this paper a new 1 -dimensional local binary pattern (LBP) signal processing method is presented. Speech systems such as hearing aids require fast and computationally inexpensive signal processing. The practical use of LBP based speech processing is demonstrated on two signal processing problems: - (i) signal segmentation and (ii) voice activity detection (VAD). Both applications use the underlying features extracted from the 1 -D LBP. The proposed VAD algorithm demonstrates the simplicity of 1 -D LBP processing with low computational complexity. It is also shown that distinct LBP features are obtained to identify the voiced and the <b>unvoiced</b> components of <b>speech</b> signal...|$|R
40|$|In this thesis, we {{analyze the}} {{complexity}} {{involved in the}} production of <b>unvoiced</b> <b>speech</b> signals with measures from nonlinear dynamics and chaos theory. Previous research successfully characterized some speech signals as chaotic. However, in this dissertation, we use multifractal measures to postulate the presence of various fractal regimes present in the attractors of <b>unvoiced</b> <b>speech</b> signals. We extend prior work which used only correlation dimension D 2 and Lyapunov Exponents to analyze some speech sounds. We capture the chaotic properties of <b>unvoiced</b> <b>speech</b> signals in the embedded vector space more succinctly by not only estimating the correlation dimension D 2, but also estimating the generalized dimension Dq. The (non-constant) generalized dimension were estimated from phase space reconstructed vectors of single scalar variable realization of <b>unvoiced</b> <b>speech</b> signals. The largest of those dimensions is an indicator of the minimum dimension required in the phase space of any realistic dynamic model of speech signals. Results of the generalized dimension estimation support the hypothesis that <b>unvoiced</b> <b>speech</b> signals indeed have multifractal measures. The multifractal analysis also reveals that <b>unvoiced</b> <b>speech</b> signals exhibit low-dimensional chaos as well as 2 ̆ 2 soft 2 ̆ 2 turbulence. This is in contrast to the opinion that <b>unvoiced</b> <b>speech</b> signals are generated from what is technically known as 2 ̆ 2 hard 2 ̆ 2 turbulent flow, in which the dimension of a dynamical model is very high. <b>Unvoiced</b> <b>speech</b> signals may actually be generated from 2 ̆ 2 soft 2 ̆ 2 turbulent flow. In this dissertation, we explore the relationship between the estimated generalized dimension Dq and the singularity spectrum ƒ(α). Existing algorithms for accurately estimated the resulting singularity spectrum ƒ(α) from the samples of generalized dimensions Dq of a multifractal chaotic time series use either (a) linear interpolation of the known, coarsely sampled, Dq values or (b) a finely sampled Dq curve obtained at great computational/experimental expense. Also, in conventional techniques the derivative in the expression for Legendre transform necessary to go from Dq to ƒ(α) is approximated using first order centered difference equation. Finely sampling the Dq is computationally intensive and the simple linear approximations to interpolation and differentiation give erroneous end points in the ƒ(α) curve. We propose using standard min-max filter design methods to more accurately interpolate between known samples of the Dq values and compute the differentiation needed to evaluate the Legendre transform. We use optimum (min-max) interolators and differentiators designed with the Parks-McClellan algorithm. We have computed the generalized dimensions and singularity spectrum of 20 <b>unvoiced</b> <b>speech</b> sounds from the ISOLET database. The results not only indicated multifractality of certain <b>unvoiced</b> <b>speech</b> sounds, but also may lead to nonlinear maps that may be useful in improving the nonlinear dynamical modeling of speech sounds. This new approach to ƒ(α) singularity spectrum calculation exhibits computational reduction and improved accuracy. The proposed method also provides estimates of the generalized dimensions at D∞ and D-∞ which are almost impossible to obtain from real data with limited number of data samples. Also, the asymmetric spread of α values with the corresponding ƒ(α) around the maximum of ƒ(α) reveal the inhomogeneity in the attractors of <b>unvoiced</b> <b>speech</b> signals just like the variations in the Dq values. The asymmetric spread of α values may also be an indication that the turbulent energy fields generated during <b>unvoiced</b> <b>speech</b> production are made of non-homogeneous fractals...|$|E
40|$|Unvoiced-voiced {{portions}} of cochannel speech contain considerable amounts of both voiced and <b>unvoiced</b> <b>speech</b> {{and play a}} significant role in separation. Motivated by recent developments in separation of speech from nonspeech noise, we propose a classification-based approach for unvoiced-voiced speech separation. A new feature set consisting of pitch-based features and gammatone frequency cepstral coefficients is proposed to represent the characteristics of a time-frequency unit. The cepstral features do not rely on pitch and are thus more robust than the pitch-based features to pitch estimation errors. Speaker-independent support vector machines are trained for classification. Results based on the TIMIT corpus show that the proposed algorithm significantly improves <b>unvoiced</b> <b>speech</b> segregation compared to a recent algorithm. Index Terms — Cochannel speech separation, <b>unvoiced</b> <b>speech,</b> voiced speech, unit-level features, classification 1...|$|E
30|$|Hu and Wang [18] used CASA {{system to}} {{segregate}} <b>unvoiced</b> <b>speech</b> using segregated voiced signals. At first, this system removes estimated voiced {{speech and the}} periodic part of interference based on cross-channel correlation. Then, it estimates interference energy by averaging mixture energy in neighboring voiced intervals. <b>Unvoiced</b> <b>speech</b> segregation is decomposed in two stages: segmentation and grouping. In fact, the estimated interference is used by spectral subtraction to extract unvoiced segments, which are then grouped by either simple thresholding or Bayesian classification.|$|E
40|$|This paper {{describes}} {{a study of}} noise-robust voice activity detection (VAD) utilizing the periodicity of the signal, full band signal energy and high band to low band signal energy ratio. Conventional VADs are sensitive to a variably noisy environment especially with low SNR, and also result in cutting off <b>unvoiced</b> regions of <b>speech</b> as well as random oscillating of output VAD decisions. To overcome these problems, the proposed algorithm first identifies voiced regions of speech and then differentiates unvoiced regions from silence or background noise using the energy ratio and total signal energy. The performance of the proposed VAD algorithm is tested on real speech signals. Comparisons confirm that the proposed VAD algorithm outperforms the conventional VAD algorithms, especially {{in the presence of}} background noise...|$|R
40|$|A new {{algorithm}} {{of direct}} time domain fundamental frequency estimation (DFE) and voiced/unvoiced (V/UV) classification of speech signal {{is presented in}} this paper. The DFE algorithm consists of spectral shaping, detection of significant extremes based on adaptive thresholding, and actual frequency estimation under several truth criteria. We propose a majority criterion for V/UV classification based on the detected frequencies consistency evaluation. Performance of the algorithm is tested on the Speecon database and compared to the Praat modified autocorrelation algorithm. In comparison to the Praat, the results indicate better properties of the DFE for clean speech and speech corrupted by additive noise to SNR about 10 dB. For lower SNR, sensitivity of the DFE to the speech component decreases rapidly while Praat fails to differentiate noise and <b>unvoiced</b> parts of <b>speech</b> from voiced parts. 1...|$|R
40|$|A b s t r a c t An {{enhanced}} sinusoidal model, {{which employs}} the time-varying amplitudes of three components {{to track the}} fast dynamicd vari-ations during the transition speech segments. and exploits the redundancies between the near-neighborhood components to re-duce the number of sinusoidal components to a maximum of?O with high synthesized quality is presented. Many components can be determined by linear prediction of the dominant and funda-mental components, thereby {{reducing the number of}} the param-eters required to be transmitted and the corresponding bit rate. This approach improves the synthesized quality of the <b>unvoiced</b> and transition <b>speech</b> segmenrs. An optimal algorithm for extracting dominant frequencies by formats and pitches is compared with a DFT method. The effects on the synthesis quality of the number of the time-varyin...|$|R
30|$|For <b>unvoiced</b> <b>speech</b> segregation, Hu–Wang system {{employed}} a multi-scale onset and offset analysis for <b>unvoiced</b> <b>speech</b> segmentation, which makes both voiced and unvoiced speeches correctly segmented in[8]. After voiced speech removal, acoustic-phonetic features are then {{used in a}} classification stage to distinguish unvoiced segments from interference[9, 10]. Hu and Wang[11] proposed a new CASA approach for unvoiced segregation based on spectral substraction. To further group the target signal across time, Shao proposed a CASA system comprised of both simultaneous and sequential organizations systematically in[12].|$|E
40|$|Abstract—Cochannel (two-talker) speech {{separation}} is predominantly addressed using pretrained speaker dependent models. In this paper, we propose an unsupervised approach to separating cochannel speech. Our approach follows {{the two main}} stages of computational auditory scene analysis: segmentation and grouping. For voiced speech segregation, the proposed system utilizes a tandem algorithm for simultaneous grouping and then unsupervised clustering for sequential grouping. The clustering is performed by a search to maximize the ratio of between- and within-group speaker distances while penalizing within-group concurrent pitches. To segregate <b>unvoiced</b> <b>speech,</b> we first produce <b>unvoiced</b> <b>speech</b> segments based on onset/offset analysis. The segments are grouped using the complementary binary masks of segregated voiced speech. Despite its simplicity, our approach produces significant SNR improvements {{across a range of}} input SNR. The proposed system yields competitive performance in comparison to other speaker-independent and model-based methods. Index Terms—Computational auditory scene analysis (CASA), cochannel speech separation, sequential grouping, unsupervised clustering, <b>unvoiced</b> <b>speech</b> segregation. I...|$|E
40|$|In this paper, {{we focus}} on methods for {{estimating}} the a posteriori proba-bility of a signal segment being voiced which employ a harmonic signal model. Fisher et al. [1] present two likelihood functions for voiced and <b>unvoiced</b> <b>speech</b> from which the posterior probability can be derived. However, due to the chosen models, the a posteriori probability of a signal segment being voiced does not go to 0 % in <b>unvoiced</b> <b>speech.</b> Thus, a novel algorithm is proposed, which incorporates the expected <b>unvoiced</b> <b>speech</b> energy and allows for obtaining low probabilities. Further, it explicitly models the statistics of the segment energy and employs a state-of-the-art noise tracker. Experiments which were conducted on the TIMIT database for different noise types and noise levels show that the proposed method results in lower over-estimation and under-estimation of the voicing probability as compared to [1]. Index Terms — voiced-unvoiced decision, likelihood ratio test, harmonic model, voicing determination, a posteriori probability 1...|$|E
40|$|Perceptual {{harmonic}} cepstral coefficients (PHCC) {{are proposed}} as features to extract for speech recognition. Pitch estimation and classification into voiced, <b>unvoiced,</b> and transitional <b>speech</b> are {{performed by a}} spectro-temporal auto-correlation technique. A peak picking algorithm is then employed to precisely locate pitch harmonics. A weighting function, which depends on the classification and the pitch harmonics, {{is applied to the}} power spectrum and ensures accurate representation of the voiced speech spectral envelope. The harmonics weighted power spectrum undergoes mel-scaled band-pass filtering, and the logenergy of the filters' output is discrete cosine transformed to produce cepstral coefficients. For perceptual considerations, within-filter cubic-root amplitude compression is applied to reduce amplitude variation without compromise of the gain invariance properties. Experiments show substantial recognition gains of PHCC over MFCC, with 48 % and 15 % error rate reduction for the Mandarin digit database and E-set, respectively...|$|R
40|$|A {{comparative}} {{evaluation of}} several pitch determination algorithms (PDAs) is presented. Fundamental frequency estimates, F 0, are compared with laryngeal frequency estimates, Lx. An algorithm is presented which enables Lx contours to be generated from laryngograph data. We seek {{the most accurate}} method of F 0 extraction in order to minimise errors propagating into subsequent prosodic analysis. The super resolution pitch determinator [3] performs well relative to the other PDAs studied. Modifications made to this algorithm are described, which radically {{reduce the number of}} gross F 0 errors and improve the classification of voiced and <b>unvoiced</b> sections of <b>speech.</b> The raw F 0 contours produced by this enhanced algorithm are processed to form schematised contours used in computer aided intonation teaching. The series of processes used in the schematisation is described. Keywords: Pitch tracking, Intonation, Language teaching 1 INTRODUCTION The fundamental frequency of speech plays an imp [...] ...|$|R
30|$|In DWT decomposition, by the {{restriction}} of Heisenberg’s uncertainty principle, the spatial resolution and spectral resolution of high-frequency band become poor, thus limiting {{the application of}} DWT. In particular, there are some problems with the basic DWT-based thresholding method when it is applied to noisy speech {{for the purpose of}} enhancement. An important shortcoming is the shrinkage of the <b>unvoiced</b> frames of <b>speech</b> which contain many noise-like speech components leading to a degraded speech quality. On the other hand, in WP decomposition, since both the approximation and the detail coefficients are decomposed into two parts at each level of decomposition, a complete binary tree with superior frequency localization can be achieved. Thus, in the context of noisy speech enhancement, this particular feature of the WP decomposition provides better discriminability of speech coefficients among those of the noise and is indeed useful for enhancing speech in the presence of noise.|$|R
