24|8|Public
25|$|Dean, who {{had been}} {{suffering}} with a severe bout of the flu for several days, attended a post-caucus rally for his volunteers at the Val-Air Ballroom in West Des Moines, Iowa and delivered his concession speech, aimed at cheering up those in attendance. Dean was shouting over the cheers of his enthusiastic audience, but the crowd noise was being filtered out by his <b>unidirectional</b> <b>microphone,</b> leaving only his full-throated exhortations audible to the television viewers. To those at home, he seemed to raise his voice out of sheer emotion.|$|E
2500|$|A <b>unidirectional</b> <b>microphone</b> is {{primarily}} sensitive to sounds from only one direction. The diagram above illustrates {{a number of}} these patterns. The microphone faces upwards in each diagram. The sound intensity for a particular frequency is plotted for angles radially from 0 to 360°. (Professional diagrams show these scales and include multiple plots at different frequencies. The diagrams given here provide only an overview of typical pattern shapes, and their names.) ...|$|E
2500|$|The {{most common}} <b>unidirectional</b> <b>microphone</b> is a {{cardioid}} microphone, {{so named because}} the sensitivity pattern is [...] "heart-shaped", i.e. a cardioid. The cardioid family of microphones are commonly used as vocal or speech microphones, since they are good at rejecting sounds from other directions. In three dimensions, the cardioid is shaped like an apple centred around the microphone which is the [...] "stem" [...] of the apple. The cardioid response reduces pickup from the side and rear, helping to avoid feedback from the monitors. Since these directional transducer microphones achieve their patterns by sensing pressure gradient, putting them {{very close to the}} sound source (at distances of a few centimeters) results in a bass boost due to the increased gradient. This is known as the proximity effect. The SM58 has been the most commonly used microphone for live vocals for more than 50 years demonstrating the importance and popularity of cardioid mics.|$|E
40|$|In {{a recent}} paper [1] the {{proof-of-concept}} {{of a novel}} approach for the localization of sound source was demonstrated. The method relies {{on the use of}} <b>unidirectional</b> <b>microphones</b> and amplitude-based signals' features to extract information about the direction of the incoming sound. By intersecting the directions identified by a pair of <b>unidirectional</b> <b>microphones,</b> the position of the emitting source can be identified. In this study we expand the work presented in that paper by assessing the effectiveness of the approach for the localization of an acoustic source in an indoor setting. As the method relies on the accurate knowledge of the microphones directivity, analytical expression of the acoustic sensors polar pattern were derived by testing them in an anechoic chamber. Then an experiment was conducted in a classroom-type environment by using an array of three <b>unidirectional</b> <b>microphones.</b> The ability to locate the position of a commercial speaker placed at different position is discussed. It is believed that this method may pave the road toward a new generation of reduced size sound detectors and localizers...|$|R
40|$|In {{a recent}} paper [P. Rizzo, G. Bordoni, A. Marzani, and J. Vipperman, “Localization of Sound Sources by Means of <b>Unidirectional</b> <b>Microphones,</b> Meas. Sci. Tech., 20, 055202 (12 pp), 2009] the {{proof-of-concept}} of an approach for the localization of acoustic sources was presented. The method {{relies on the}} use of <b>unidirectional</b> <b>microphones</b> and amplitude-based signals’ features to extract information about the direction of the incoming sound. By intersecting the directions identified by a pair of microphones, the position of the emitting source can be identified. In this paper we expand the work presented previously by assessing the effectiveness of the approach for the localization of an acoustic source in an indoor setting. As the method relies on the accurate knowledge of the microphones directivity, analytical expression of the acoustic sensors polar pattern were derived by testing them in an anechoic chamber. Then an experiment was conducted in an empty laboratory by using an array of three <b>unidirectional</b> <b>microphones.</b> The ability to locate the position of a commercial speaker placed at different positions in the room is discussed. The objective {{of this study is to}} propose a valid alternative to the common application of spaced arrays and therefore to introduce a new generation of reduced size sound detectors and localizers. The ability of the proposed methodology to locate the position of a commercial speaker placed at different positions in the room was evaluated and compared to the accuracy provided by a conventional time delay estimate algorithm...|$|R
40|$|Usage of {{cellular}} phones and {{small form factor}} devices as PDAs and other handhelds has been increasing rapidly. Their use is varied, with scenarios such as communication, internet browsing, audio and video recording {{just to name a}} few. This requires better sound capturing system as the sound source is already at larger distance from the device’s microphone. In this paper we propose sound capture system for small devices which uses two <b>unidirectional</b> <b>microphones</b> placed back-toback close to each other. The processing part consists of beamformer and a non-linear spatial filter. The speech enhancement processing achieves an improvement of 0. 39 MOS points in the perceptual sound quality and 10. 8 dB improvement in SNR. Index Terms: sound capture, speech enhancement, beamforming...|$|R
50|$|The Soundfield {{microphone}} is {{an audio}} microphone composed of four closely spaced subcardioid or cardioid (<b>unidirectional)</b> <b>microphone</b> capsules {{arranged in a}} tetrahedron. It was invented by Michael Gerzon and Peter Craven, and is a part of, but not exclusive to, Ambisonics, a surround sound technology. It can function as a mono, stereo or surround sound microphone, optionally including height information.|$|E
5000|$|A <b>unidirectional</b> <b>microphone</b> is {{primarily}} sensitive to sounds from only one direction. The diagram above illustrates {{a number of}} these patterns. The microphone faces upwards in each diagram. The sound intensity for a particular frequency is plotted for angles radially from 0 to 360°. (Professional diagrams show these scales and include multiple plots at different frequencies. The diagrams given here provide only an overview of typical pattern shapes, and their names.) ...|$|E
50|$|Some {{acoustic}} cameras use two-dimensional acoustic mapping, {{which uses}} a <b>unidirectional</b> <b>microphone</b> array (e.g. a rectangle of microphones, all facing the same direction). Two-dimensional acoustic mapping works best when the surface to be examined is planar and the acoustic camera {{can be set up}} facing the surface perpendicularly. However, the surfaces of real-world objects are not often flat, and it is not always possible to optimally position the acoustic camera.|$|E
5000|$|The {{observational}} mode of documentary {{developed in}} the wake of documentarians returning to Vertovian ideals of truth, along with the innovation and evolution of cinematic hardware in the 1960s. In Dziga Vertov's Kino-Eye manifestoes, he declared, [...] "I, a camera, fling myself along…maneuvering in the chaos of movement, recording movement, startling with movements of the most complex combinations." [...] (Michelson, O’Brien, & Vertov 1984) The move to lighter 16mm equipment and shoulder mounted cameras allowed documentarians to leave the anchored point of the tripod. Portable Nagra sync-sound systems and <b>unidirectional</b> <b>microphones,</b> too, freed the documentarian from cumbersome audio equipment. A two-person film crew could now realize Vertov’s vision and sought to bring real truth to the documentary milieu. Unlike the subjective content of poetic documentary, or the rhetorical insistence of expositional documentary, observational documentaries tend to simply observe, allowing viewers to reach whatever conclusions they may deduce. Pure observational documentarians proceeded under some bylaws: no music, no interviews, no scene arrangement of any kind, and no narration. The fly-on-the-wall perspective is championed, while editing processes utilize long takes and few cuts. Resultant footage appears as though the viewer is witnessing first-hand the experiences of the subject: they travel with Bob Dylan to England in D.A. Pennebaker's Dont Look Back sic (1967,) suffer the stark treatment of patients at the Bridgewater State Hospital in Frederick Wiseman's Titicut Follies (1967,) and hit the campaign trail with John F. Kennedy and Hubert Humphrey in Robert Drew’s Primary (1960.) ...|$|R
30|$|Large Farsdat [22] {{includes}} about 140 h {{of speech}} signals, all segmented and labeled in word level. This corpus is uttered by 100 speakers {{from the most}} common dialects of the Persian language. Each speaker utters 20 - 25 pages of text from various subjects. In contrast with small Farsdat, which is recorded in a quiet and reverberation-free room, large Farsdat is recorded in office environment. Four <b>microphones,</b> a <b>unidirectional</b> desktop <b>microphone,</b> two lapel microphones and a headset microphone are used to record the speech signals. All the speech signals in this corpus are recorded using two microphones simultaneously, the desktop microphone is used {{in all of the}} recording sessions and each of the other three microphones is used in about one-third of the sessions. Totally, the desktop microphone is used for about 70 h of recorded speech and the other three microphones are used for the 70 remaining hours. The average SNR of the desktop microphone is about 28 dB. The sampling rate is 16 kHz for the whole corpus.|$|R
40|$|Abstract Background To date, {{methods used}} to assess cough have been {{primarily}} subjective, and only broadly reflect the impact of chronic cough and/or chronic cough therapies on quality of life. Objective assessment of cough has been attempted, but early techniques were neither ambulatory nor feasible for long-term data collection. We evaluated a novel ambulatory cardio-respiratory monitoring system with an integrated <b>unidirectional,</b> contact <b>microphone,</b> and report here the results from a study of patients with COPD who were videotaped in a quasi-controlled environment for 24 continuous hours while wearing the ambulatory system. Methods Eight patients with a documented history of COPD with ten {{or more years of}} smoking (6 women; age 57. 4 ± 11. 8 yrs.; percent predicted FEV 1 49. 6 ± 13. 7 %) who complained of cough were evaluated in a clinical research unit equipped with video and sound recording capabilities. All patients wore the LifeShirt ® system (LS) while undergoing simultaneous video (with sound) surveillance. Video data were visually inspected and annotated to indicate all cough events. Raw physiologic data records were visually inspected by technicians who remained blinded to the video data. Cough events from LS were analyzed quantitatively with a specialized software algorithm to identify cough. The output of the software algorithm was compared to video records on a per event basis {{in order to determine the}} validity of the LS system to detect cough in COPD patients. Results Video surveillance identified a total of 3, 645 coughs, while LS identified 3, 363 coughs during the same period. The median cough rate per patient was 21. 3 coughs·hr - 1 (Range: 10. 1 cghs·hr - 1 – 59. 9 cghs·hr - 1). The overall accuracy of the LS system was 99. 0 %. Overall sensitivity and specificity of LS, when compared to video surveillance, were 0. 781 and 0. 996, respectively, while positive- and negative-predictive values were 0. 846 and 0. 994. There was very good agreement between the LS system and video (kappa = 0. 807). Conclusion The LS system demonstrated a high level of accuracy and agreement when compared to video surveillance for the measurement of cough in patients with COPD. </p...|$|R
5000|$|Radially {{asymmetric}} {{systems may}} also be modeled with polar coordinates. For example, a microphone's pickup pattern illustrates its proportional response to an incoming sound from a given direction, and these patterns can be represented as polar curves. The curve for a standard cardioid microphone, the most common <b>unidirectional</b> <b>microphone,</b> can be represented as r [...] 0.5 + 0.5sin(ϕ) at its target design frequency. The pattern shifts toward omnidirectionality at lower frequencies.|$|E
50|$|Dean, who {{had been}} {{suffering}} with a severe bout of the flu for several days, attended a post-caucus rally for his volunteers at the Val-Air Ballroom in West Des Moines, Iowa and delivered his concession speech, aimed at cheering up those in attendance. Dean was shouting over the cheers of his enthusiastic audience, but the crowd noise was being filtered out by his <b>unidirectional</b> <b>microphone,</b> leaving only his full-throated exhortations audible to the television viewers. To those at home, he seemed to raise his voice out of sheer emotion.|$|E
5000|$|The first {{integration}} between mobile phone and Palm (PDA Personal Device Assistant) occurred in 1999, {{as a result}} of an Italian-lead project submitted to the action line V1.1 CPA1 [...] "Integrated application platforms and services" [...] 5th Framework Program of the European Community (project number IST1999-11100).The project, called MTM (Multimedia Terminal Mobile), was a multimedia platform, including both phone and PDA features; it also integrated the first miniature camera and a <b>unidirectional</b> <b>microphone</b> for video conferencing and commands interpretation through voice recognition. The creator and coordinator of the project, Alessandro Pappa, worked in a team with other European partners: ...|$|E
40|$|The Coriolanus Online {{project began}} {{life as a}} virtual {{mobility}} project between Coventry University (UK) and the University of Tampere (Finland). This project explored international co-operation in acting {{in a foreign language}} within a system that promotes a more environmentally sustainable model, eliminating the need to move large groups of student actors across the globe for rehearsals, workshops and even performances. Since 1995, the degree programme of Acting in the University of Tampere has sought new ways of investigating performance through the use of foreign languages. Each year class has either produced an entire play or at least scenes from a play in a language that none of the performers were previously familiar with. This technique encourages actors to make conscious use of the body to communicate meaning and to focus on the materiality of the text and the physicality of the voice. It can improve articulation, adding energy to the act of speaking. It can also {{have a positive effect on}} body awareness. Using a foreign language may somewhat liberate the fixed subjectivity intertwined to the mother tongue. It can help to break away from some habitual ways of using and perceiving the body and thus even be the key for creating genuine presence on stage. Taking the text of Shakespeare's Coriolanus as a basis for study, students from both institutions worked on a small section of the script (3 : 3) in both Finnish and English. A 'virtual rehearsal space' was created in both locations through the re-purposing of H. 323 [...] -videoconferencing technology and the use of large rear projection screens, high speed internet connections and <b>unidirectional,</b> hypercardioid <b>microphones.</b> Skype and Adobe Connect were also used for self-directed student rehearsal, peer to peer learning and discussion and were also used to provide a series of seminars and lectures contextualising Renaissance theatre, Finnish theatre culture and the theories underpinning acting in a foreign language. The scene chosen from Coriolanus also allowed the opportunity to investigate the concept of the citizen in the 'mobile' age, the relationship between politicians and the people and their ability to engage in meaningful political dialogue through computermediated exchanges...|$|R
40|$|This {{repository}} {{contains the}} datasets {{used in the}} article "Shared Acoustic Codes Underlie Emotional Communication in Music and Speech - Evidence from Deep Transfer Learning" (Coutinho & Schuller, 2017). In that article four different data sets were used: SEMAINE, RECOLA, ME 14 and MP (acronyms and datasets described below). The SEMAINE (speech) and ME 14 (music) corpora {{were used for the}} unsupervised training of the Denoising Auto-encoders (domain adaptation stage) - only the audio features extracted from the audio files in these corpora were used and are provided in this repository. The RECOLA (speech) and MP (music) corpora were used for the supervised training phase - both the audio features extracted from the audio files and the Arousal and Valence annotations were used. In this repository, we provide the audio features extracted from the audio files for both corpora, and Arousal and Valence annotations for some of the music datasets (those that the author of this repository is the data curator). Below, you can find description of the various corpora, the details about the data stored in this repository and information on how to obtain the rest of the data used by Coutinho and Schuller (2017). SEMAINE (speech) The SEMAINE corpus (McKeown, Valstar, Cowie, Pantic & Schroder, 2012) was developed specifically to address the task of achieving emotion-rich interactions, and it is adequate for this task as it comprises a wide range of emotional speech. It includes video and speech recordings of spontaneous interactions between human and emotionally stereotyped `characters'. Coutinho & Schuller (2017) used a subset of this database (called Solid-SAL). The Solid-SAL dataset is freely available for scientific research purposes (see [URL] This repository includes the audio features used in Coutinho & Schuller (2017) (under features/SEMAINE). RECOLA (speech) The RECOLA database (Ringeval, Sonderegger, Sauer & Lalanne, 2013) consists of multimodal recordings (audio, video, and peripheral physiological activity) of spontaneous dyadic interactions between French adults. Coutinho & Schuller (2017) used the RECOLA-Audio module which consists of the audio recordings of each participant in the dyadic phase of the task. In particular, they used the non-segmented high-quality audio signals (WAV format, 44. 1 kHz, 16 bits), obtained through <b>unidirectional</b> headset <b>microphones,</b> of the first five minutes of each interaction. Annotations consist of time-continuous ratings of the level of Arousal and Valence dimensions of emotion perceived by each rater while seeing and listening the audio-visual recordings of each participant task. The publicly available annotated dataset includes only part of the data which amounts to a total number of 23 instances. The time frame length used by Coutinho & Schuller (2017) is 1 s (the original annotations were downsampled). This repository includes the audio features used in Coutinho & Schuller (2017) (under features/RECOLA). To obtain the annotations you should contact the author of the original study (see [URL] for further details). ME 14 (music) The MediaEval ``Emotion in Music'' task is dedicated to the estimation of Arousal and Valence scores continuously in time and value for song excerpts from the Free Music Archive. Coutinho and Schuller (2017) used the whole corpus (development and test sets for the 2014 challenge) which includes 1, 744 songs belonging to 11 musical styles [...] Soul, Blues, Electronic, Rock, Classical, Hip-Hop, International, Folk, Jazz, Country, and Pop (maximum of five songs per artist). This repository includes the audio features used in Coutinho & Schuller (2017) (under features/ME 14). The full dataset (including annotations) can be obtained from [URL] MP (music) This is a corpus compiled specifically for this work described in Coutinho & Schuller (2017) using data collected in four previous studies. It consists of emotionally diverse full music pieces from a variety of musical styles (Classical and contemporary Western Art, Baroque, Bossa Nova, Rock, Pop, Heavy Metal, and Film Music). Annotations were obtained in controlled laboratory experiments whereby the emotional character of each piece was evaluated time-continuously in terms of levels of Arousal and Valence perceived by listeners (ranging between 35 to 52 in the four studies). In what follows, some details about the various studies are described. 	MPDB 1 : This subset of the MP corpus consists of the data reported by Korhonen (2004), and gently made available by the author. This dataset includes six full (or long excerpts) music pieces ranging from 151 s to 315 s in length (only classical music). Each piece was annotated by 35 participants (14 females). The time series correspondents to each music piece were collected at 1 Hz. The golden standard for each piece was computed by averaging the individual time series across all raters. This repository includes the audio features used in Coutinho & Schuller (2017) (under features/MP/DB 1). To obtain the labels please contact the author of the original study. 	MPDB 2 : The dataset by Coutinho & Cangelosi (2011) includes 9 full pieces (43 s to 240 s long) of classical music (romantic repertoire) annotated by 39 subjects (19 females). Values were recorded every time the mouse was moved with a precision of 1 ms. The resultant timeseries were then resampled (moving average) to a synchronous rate of 1 Hz. The golden standard for each piece was computed by averaging the individual time series across all raters. This repository includes the audio features (under features/MP/DB 2) and labels (under annotations/MP/DB 2) used in Coutinho & Schuller (2017). 	MPDB 3 : This dataset was collected by Coutinho & Dibben (2012) and it consists of 8 pieces of film music (84 s to 130 s long) taken from the late 20 th century Hollywood film repertoire. Emotion ratings were given by 52 participants (26 females). The annotation procedure, data processing, and golden standard calculations were identical to MPDB 2. This repository includes the audio features (under features/MP/DB 3) and labels (under annotations/MP/DB 3) used in Coutinho & Schuller (2017). 	MPDB 4 : This dataset was collected by Grewe, Nagel, Kopiez and Altenmüller (2007), and gently made available by the authors. It includes seven music pieces (127 s to 502 s in length) of heterogeneous styles (e. g., Rock, Pop, Heavy Metal, Classical). Each music piece was annotated by 38 participants (29 females) using an identical methodology to MPDB 2 and MPDB 3. Data processing and golden standard calculations were also identical. This repository includes the audio features (under features/MP/DB 4) used in Coutinho & Schuller (2017). To obtain the labels contact the authors of the original study Bibliography Coutinho, E., & Cangelosi, A. (2011). Musical emotions: predicting second-by-second subjective feelings of emotion from low-level psychoacoustic features and physiological measurements. Emotion, 11 (4), 921. Coutinho, E., & Dibben, N. (2013). Psychoacoustic cues to emotion in speech prosody and music. Cognition & Emotion, 27 (4), 658 - 684. Coutinho E, Schuller B (2017) Shared acoustic codes underlie emotional communication in music and speech—Evidence from deep transfer learning. PLoS ONE 12 (6) : e 0179289. [URL] org/ 10. 1371 /journal. pone. 0179289. Grewe, O., Nagel, F., Kopiez, R., Altenmüller, E. (2007). Emotions over time: synchronicity and development of subjective, physiological, and facial affective reactions to music. Emotion, 7 (4), pp. 774 - 788. DOI: 10. 1037 / 1528 - 3542. 7. 4. 774. Korhonen, M. (2004). Modeling Continuous Emotional Appraisals of Music Using System Identification. Available from: [URL] McKeown, G., Valstar, M., Cowie, R., Pantic, M., Schroder, M. (2012). The SEMAINE Database: Annotated Multimodal Records of Emotionally Colored Conversations between a Person and a Limited Agent. IEEE Transactions on Affective Computing, 3, pp. 5 - 17. DOI: [URL] Ringeval, F., Sonderegger, A., Sauer, J. & Lalanne, D. (2013). Introducing the RECOLA Multimodal Corpus of Remote Collaborative and Affective Interactions. In Proceedings of the 2 nd International Workshop on Emotion Representation, Analysis and Synthesis in Continuous Time and Space (EmoSPACE 2013), Shanghai, China. IEE...|$|R
5000|$|The {{most common}} <b>unidirectional</b> <b>microphone</b> is a {{cardioid}} microphone, {{so named because}} the sensitivity pattern is [...] "heart-shaped", i.e. a cardioid. The cardioid family of microphones are commonly used as vocal or speech microphones, since they are good at rejecting sounds from other directions. In three dimensions, the cardioid is shaped like an apple centred around the microphone which is the [...] "stem" [...] of the apple. The cardioid response reduces pickup from the side and rear, helping to avoid feedback from the monitors. Since these directional transducer microphones achieve their patterns by sensing pressure gradient, putting them {{very close to the}} sound source (at distances of a few centimeters) results in a bass boost due to the increased gradient. This is known as the proximity effect. The SM58 has been the most commonly used microphone for live vocals for more than 50 years demonstrating the importance and popularity of cardioid mics.|$|E
5000|$|The dismal results caused Gephardt to {{drop out}} and later endorse Kerry. Carol Moseley Braun also dropped out, {{endorsing}} Howard Dean. Besides the impact of coming in third, Dean was further hurt by a speech he gave at a post-caucus rally. Dean was shouting over the cheers of his enthusiastic audience, but the crowd noise was being filtered out by his <b>unidirectional</b> <b>microphone,</b> leaving only his full-throated exhortations audible to the television viewers. To those at home, he seemed to raise his voice out of sheer emotion.The incessant replaying of the [...] "Dean Scream" [...] by the press became a debate {{on the topic of}} whether Dean was the victim of media bias. The scream scene was shown approximately 633 times by cable and broadcast news networks in just four days following the incident, a number that does not include talk shows and local news broadcasts. However, those who were in the actual audience that day insist that they were not aware of the infamous [...] "scream" [...] until they returned to their hotel rooms and saw it on TV.|$|E
30|$|In this section, we {{expand the}} RIR model from Section 3 {{and examine the}} {{response}} for perception by a <b>unidirectional</b> <b>microphone</b> array.|$|E
30|$|Let us {{assume that}} the reverberant signal impinges on a <b>unidirectional</b> <b>microphone</b> array, rather than a single {{omnidirectional}} microphone. Such an array can be composed of several directional microphone elements or alternately by applying beamforming techniques with a few closely spaced omnidirectional microphones [19, 20]. The overall source-to-microphone response {{can be described as}} a convolution of the RIR (2) with the response of the corresponding directional microphone. The acoustic response of a directional microphone (or beamformer) is time-invariant and is defined only by the frequency and angle of the arriving signal.|$|E
40|$|Automatic speech {{recognition}} {{has become a}} standard feature on many consumer electronics and automotive products, and {{the accuracy of the}} decoded speech has improved dramatically over time. Often, designers of these products achieve accuracy by employing microphone arrays and beamforming algorithms to reduce interference. However, beamforming microphone arrays are too large for small form factor products such as smart watches. Yet these small form factor products, which have precious little space for tactile user input (i. e. knobs, buttons and touch screens), would benefit immensely from a user interface based on reliably accurate automatic {{speech recognition}}. This thesis proposes a solution for interference mitigation that employs blind source separation with a compact array of commercially available <b>unidirectional</b> <b>microphone</b> elements. Such an array provides adequate spatial diversity to enable blind source separation and would easily fit in a smart watch or similar small form factor product. The solution is characterized using publicly available speech audio clips recorded for the purpose of testing automatic speech recognition algorithms. The proposal is modelled in different interference environments and the efficacy of the solution is evaluated. Factors affecting the performance of the solution are identified and their influence quantified. An expectation is presented for the quality of separation as well as the resulting improvement in word error rate that can be achieved from decoding the separated speech estimate versus the mixture obtained from a single <b>unidirectional</b> <b>microphone</b> element. Finally, directions for future work are proposed, which have the potential to improve the performance of the solution thereby making it a commercially viable product...|$|E
40|$|In northeastern Brazil, most well-preserved Atlantic Rainforest {{fragments}} {{are situated}} within privately-owned lands or sugarcane properties, {{mainly in the}} states of Alagoas and Pernambuco. On the lands of São José Sugar Mill (USJ), in Igarassu (Pernambuco), there are 106 forest patches totaling 6, 660 ha, surrounded by sugarcane fields. The main goal {{of this study was}} to determine the species richness in five forest remnants within the USJ property. We completed 77 hours of observation and recording of bird voices, using binoculars and a tape recorder with a <b>unidirectional</b> <b>microphone.</b> A total of 184 bird species were registered, among which 15 were under risk of extinction, (e. g.) Cercomacra laeta sabinoi, Pyriglena leuconota pernambucensis and Myrmeciza rufi cauda soror. Some species that were observed and collected in the same region in the 1940 s, such as Ramphastos vitellinus, Pteroglossus aracari and Lipaugus vociferans, were not registered during the study period...|$|E
30|$|In {{order to}} examine our {{proposed}} approach in a real environment, we performed speech recordings in a lecture hall of size 15 × 10 × 6 m, using six microphone clusters (with a 3 -m spacing between adjacent clusters), each composed of four <b>unidirectional</b> <b>microphone</b> units and each facing 90 ° apart. For the purpose of analyzing the performance of our proposed measure, we placed the microphone clusters on a line along the hall. The speaker in the experiment moved along the line, advancing from the first array toward the sixth. We divided the speech recordings such that every time the speaker {{was in front of}} one array or in between two arrays, a separate speech segment was defined. Then, for every active speech segment, we measured the power ratio (18) and calculated its temporal mean separately. Since we could not restore the reference DRR (or C 50) precisely, we chose to demonstrate here a qualitative analysis of the results.|$|E
40|$|One of the {{problems}} for all speech input is the necessity for the talker to be encumbered by a head. mounted, hand-held, or fixed position microphone. An intelfigent, electronically-aimed <b>unidirectional</b> <b>microphone</b> would overcome this problem. Array techniques hold the best promise to bring such a system to practicality. The development of a robust algorithm to determine {{the location of a}} talker is a fundamental issue for a microphone-array system. Here, a two-step talker-location algorithm is introduced. Step 1 is a rather conventional filtered cross-correlation method; the cross-correlation between some pair of microphones is determined to high accuracy using a somewhat novel, fast interpolation on the sampled data. Then, using the fact that the delays for a point source should fit a hyperbola, a best hyperbolic fit is obtained using nonlinear optimization. A method which fits the hyperbola directly to peak-picked delays is shown to be far less robust than an algorithm which fits the hyperbola in the cross-correlation space. An efficient, global nonlinear optimization technique, Stochastic region Contraction (SRC) is shown to yield highly accurate (> 90 %), and computationally efficient, results for a normal ambient...|$|E
40|$|Assessing pigs’ {{welfare is}} one of the most {{challenging}} subjects in intensive pig farming. Animal vocalization analysis is a noninvasive procedure and may be used as a tool for assessing animal welfare status. The objective of this research was to identify stress conditions in piglets reared in farrowing pens through their vocalization. Vocal signals were collected from 40 animals under the following situations: normal (baseline), feeling cold, in pain, and feeling hunger. A <b>unidirectional</b> <b>microphone</b> positioned about 15 cm from the animals’ mouth was used for recording the acoustic signals. The microphone was connected to a digital recorder, where the signals were digitized at the 44, 100 Hz frequency. The collected sounds were edited and analyzed. The J 48 decision tree algorithm available at the Weka® data mining software was used for stress classification. It was possible to categorize diverse conditions from the piglets’ vocalization during the farrowing phase (pain, cold and hunger), with an accuracy rate of 81. 12 %. Results indicated that vocalization might be an effective welfare indicator, and it could be applied for assessing distress from pain, cold and hunger in farrowing piglets...|$|E
40|$|ABSTRACT: Among the {{challenges}} of pig farming in today's competitive market, there is factor of the product traceability that ensures, among many points, animal welfare. Vocalization is a valuable tool to identify situations of stress in pigs, {{and it can be}} used in welfare records for traceability. The objective of this work was to identify stress in piglets using vocalization, calling this stress on three levels: no stress, moderate stress, and acute stress. An experiment was conducted on a commercial farm in the municipality of Holambra, São Paulo State, where vocalizations of twenty piglets were recorded during the castration procedure, and separated into two groups: without anesthesia and local anesthesia with lidocaine base. For the recording of acoustic signals, a <b>unidirectional</b> <b>microphone</b> was connected to a digital recorder, in which signals were digitized at a frequency of 44, 100 Hz. For evaluation of sound signals, Praat software was used, and different data mining algorithms were applied using Weka software. The selection of attributes improved model accuracy, and the best attribute selection was used by applying Wrapper method, while the best classification algorithms were the k-NN and Naive Bayes. According to the results, it was possible to classify the level of stress in pigs through their vocalization. 201...|$|E
40|$|In {{order to}} reach higher broiler performance, farmers target losses reduction. One way to make this {{possible}} is by rearing sexed broilers as male and female present diverse performance due to their physiological differences. Birds from different genetic strain also have a distinct performance at the same age. Considering that sexed flocks may present higher performance this study aimed to identify one-day-old chicks&# 8217; sex throughout their vocalization. This research also investigated the possibility of identifying the genetic strain by their vocalization attributes. A total of 120 chicks, half of them were from Cobb® genetic strain {{and the other half}} from Ross® genetic strain. From each group, a total of 30 were males and 30 females, which were previously separated by sex using their secondary physiological characteristics at the hatchery. Vocalizations audio recording was done inside a semi-anechoic chamber using a <b>unidirectional</b> <b>microphone</b> connected to an audio input of a digital recorder. Vocalizations were recorded for two minutes. Acoustic characteristics of the sounds were analyzed being calculated the fundamental frequency Pitch, the sound intensity, the first formant, and second formant. Results indicated that the vocalizations of both sexes could be identified by the second formant, and the genetic strain was detected by both the second formant and the Pitch...|$|E
40|$|Su Xingming 苏兴明, an {{elementary}} school teacher in Judu village, was my informant. I elicited more than 500 vocabulary words on July 31, 2012. Unfortunately, the static noise from the Minghai H 15 <b>unidirectional</b> <b>microphone</b> was quite frustrating to me. I was always making sure my informant spoke loud and close enough to the microphone as to speak significantly louder than the static. However, I still successfully collected excellent data, as Su Xingming was a highly cooperative and literate informant who had worked with field linguists before. He had personally known Li Jinfang and his graduates students Kang Zhongde and Li Xia. Unlike all of the other ethnic Gelao villages I had been to, in Judu, the Gelao language was actually used far more in daily conversation than Chinese. Even the village officials often spoke White Gelao to each other. They took me to the Judu elementary school, where I was to work with my informant, have my meals, and stay for the night. I then played Jerold Edmondson’s recordings of northern Vietnam’s White Gelao to them, and then said that much of it was intelligible, except for some variations in tone and lexicon. Location: Judu village 居都村, Qingkou township 箐口乡, Liuzhi Special District, Liupanshui, Guizhou, Chin...|$|E
40|$|An {{acoustic}} spectroscopic {{approach to}} detect contents within different packaging, with substantially wider applicability than other currently available subsurface spectroscopies, is presented. A frequency-doubled Nd:YAG (neodymium-doped yttrium aluminum garnet) pulsed laser (13 ns pulse length) operated at 1 Hz {{was used to}} generate the sound field of a two-component system {{at a distance of}} 50 cm. The acoustic emission was captured using a <b>unidirectional</b> <b>microphone</b> and analyzed in the frequency domain. The focused laser pulse hitting the system, with intensity above that necessary to ablate the irradiated surface, transferred an impulsive force which led the structure to vibrate. Acoustic airborne transients were directly radiated by the vibrating elastic structure of the outer component that excited the surrounding air in contact with. However, under boundary conditions, sound field is modulated by the inner component that modified the dynamical integrity of the system. Thus, the resulting frequency spectra are useful indicators of the concealed content that influences the contributions originating from the wall of the container. High-quality acoustic spectra could be recorded from a gas (air), liquid (water), and solid (sand) placed inside opaque chemical-resistant polypropylene and stainless steel sample containers. Discussion about effects of laser excitation energy and sampling position on the acoustic emission events is reported. Acoustic spectroscopy may complement the other subsurface alternative spectroscopies, severely limited by their inherent optical requirements for numerous detection scenarios...|$|E

