2|93|Public
50|$|Using the best-fit {{values for}} its orbit, it is {{expected}} to come to perihelion in 2109. It has been observed 28 times over 3 oppositions and has an <b>uncertainty</b> <b>code</b> of 5. As of 2016, is 46.1 AU from the Sun. The body's spectral type as well as its rotation period remain unknown.|$|E
40|$|In {{this paper}} a method is {{demonstrated}} {{which allows the}} user to carry out uncertainty calculations in arbitrary 3 -dimensional radiation transport problems in an easy and straightforward manner. Although 3 D uncertainty tools do exist their use is very limited because they are tied to 3 D deterministic radiation transport codes, which are not commonly used because of their complexity. The method we present makes use of the 3 D Monte Carlo code MCNP, which is world-wide {{the most commonly used}} radiation transport code. Using standard options of MCNP we produce a sensitivity profile, which is converted to an uncertainty value. This conversion may be carried out by a suitable <b>uncertainty</b> <b>code.</b> In the current work we used the modular code SUSD. Because of the minimal effort needed to carry out these analyses, 3 -D uncertainty calculations may be performed as standard practice. Design calculations for advanced nuclear systems (such as Generation-IV reactors and ADS systems) may greatly benefit from this development. Increased insight may be obtained from neutronics design calculations, as uncertainty bands may be associated to critical design parameters. Note, however, that proper covariance data are an essential ingredient in the analysis. Unfortunately these data are rather scarce. We strongly support the current initiatives to produce high-quality covariance datalibraries for all relevant isotopes. ...|$|E
5000|$|The {{traditional}} {{version of}} HTML allowed <b>uncertainties</b> in <b>code</b> review, which means even {{there are some}} mistakes in the web pages, browser will still display the contents through error detection and correction. [...] "Nowadays, there {{would be at least}} one mistake existing in over 99% HTML web pages. [...] " [...] estimated by the Google senior programmer Mark Pilgrim.|$|R
30|$|Rateless coding is an {{efficient}} way to combat channel <b>uncertainty.</b> The <b>code</b> rate automatically adapts to the channel quality through the use of incremental redundancy. Unlike conventional coding schemes, a rateless encoder can generate codewords with arbitrary length [12]. Therefore, very long codewords can be used as needed rather than employing a fixed rate code [13]. This is an effective solution for time-varying communications channels.|$|R
3000|$|In this article, we {{proposed}} a parameters’ optimization scheme in order to eliminate <b>uncertainties</b> when selecting <b>coding</b> parameters. Compare with existing methods, our approach builds a relationship map of the quantization parameter QP [...]...|$|R
40|$|Within the {{framework}} of the OECD/NEA Expert Group on Reactor-based Plutonium disposition (TFRPD), fuel modeling code benchmarks for MOX fuel were initiated. This paper summarizes the calculation results provided by the contributors for the first two fuel performance benchmark problems. A limited sensitivity study of the effect of the rod power <b>uncertainty</b> on <b>code</b> predictions of fuel centerline temperature and fuel pin pressure also was performed and is included in the paper...|$|R
40|$|This paper {{describes}} {{the status of}} the 2004 edition of the HITRAN molecular spectroscopic database. The HITRAN compilation consists of several components that serve as input for radiative transfer calculation codes: individual line parameters for the microwave through visible spectra of molecules in the gas phase; absorption cross-sections for molecules having dense spectral features, i. e., spectra in which the individual lines are unresolvable; individual line parameters and absorption cross-sections for bands in the ultra-violet; refractive indices of aerosols; tables and files of general properties associated with the database; and database management software. The line-by-line portion of the database contains spectroscopic parameters for 39 molecules including many of their isotopologues. The format of the section of the database on individual line parameters of HITRAN has undergone the most extensive enhancement in almost two decades. It now lists the Einstein A-coefficients, statistical weights of the upper and lower levels of the transitions, a better system for the representation of quantum identifications, and enhanced referencing and <b>uncertainty</b> <b>codes.</b> In addition, there is a provision for making corrections to the broadening of line transitions due to line mixing. (C) 2005 Elsevier Ltd. All rights reserved...|$|R
40|$|The UMAE (Uncertainty Methodology {{based on}} Accuracy Extrapolation) was planned and used (i. e. {{prediction}} of <b>uncertainty</b> in <b>code</b> {{application to the}} analysis of accidents in Nuclear power Plants) having in mind data from Integral Test Facilities (ITF). This document puts the basis for expanding the domain of applicability and of qualification of UMAE to the Separate Effect Test Facilities (SETF). The concerned phenomenon is the Critical Heat Flux (CHF) : data were obtained from the Paul Scherrer Institute (PSI) in Switzerland...|$|R
40|$|AbstractThe {{objectives}} of paper are {{the application of}} uncertainty and sensitivity analysis methods in atmospheric dispersion modeling to case study for predicting the dispersion of pollutants in the atmospheric environment. Gaussian Plume Model is used to study the impact of meteorology on the dispersion of emissions from industrial source complex. Quantitative uncertainty analysis has become a common component of risk assessments. Uncertainties were defined a priori {{in each of the}} following variables: wind speed, wind direction and pollutant emission rate. In order to get information about the <b>uncertainty</b> of computer <b>code</b> results, a number of code runs have to be performed using tolerance limits method. Monte Carlo method is used for propagating <b>uncertainty</b> across of <b>code.</b> Spearman rank correlation coefficient is used as sensitivity measure...|$|R
40|$|The {{proposed}} {{recommendations for}} implementing constructive signal code constructions in modems used in radio relay and satellite communications. Developed documentation support for {{implementation of the}} proposed modems in the production process of PJSC "ELMIZ. " The study {{is based on the}} idea of the synthesis signal-code constructions for use in satellite and microwave transmission with desired properties by criteria information efficiency. This makes it possible to unify the system used for the types of signals and information transmitted at different speeds, ensuring efficient use of available frequency and power resources. Writing team for the first time resolved <b>uncertainty</b> <b>coding</b> theorem for the Shannon discrete channels with given accuracy through precise numerical value in relation to the block and continuous codes, and first tested for performance by the border with Multi-channel signals in the structure of signal-code constructions (SCC). The paper used new methods for solving applied problem with new innovative ideas and concepts, combined with the fundamental principles of the theory of noise immunity multiposition signals, noise-immune coding theory, information theory, Shannon theorem. The end result is excess options and advanced codes multiposition signals combined in SCC. Proved and patented new technology of information transmission using multiposition signals combined with speed and noise immunity codes while performing a modulation / coding. ???????????? ???????????? ??? ?????????????? ?????????? ????????? ??????? ??????????? ? ???????, ??????????? ? ????????????? ? ??????????? ?????. ??????????? ???????????? ????????????? ?? ????????? ???????????? ??????? ? ???????????????? ??????? ??? ???????. ? ?????? ?????? ???????? ???? ??????? ?????????-??????? ??????????? ??? ?????????? ? ??????????? ? ????????????? ????? ? ????????? ?????????? ?? ????????? ?????????????? ?????????????. ??? ???? ??????????? ????????????? ??????? ? ??????????????? ????????? ? ?????? ??????????, ??????? ?????????? ? ?????? ?????????, ??????????? ??????????? ????????????? ????????? ????????? ? ?????????????? ????????. ????????? ??????????? ??????? ?????? ???????????????? ?????? ??????????? ??????? ??? ?????????? ??????? ? ???????? ?????????????? ????? ?????? ?????????????? ??????????? ?? ??????? ? ??????????? ?????, ? ????? ??????? ??????????? ?? ???????????? ??????? ?????????????????? ??????? ? ????????????????? ????????? ? ????????? ?????????-??????? ??????????? (???). ? ?????? ???????????? ????? ?????? ??????? ?????????? ?????? ? ??????????? ????? ???????????? ???? ? ?????????, ? ????????? ? ???????????????? ????????? ?????? ?????????????????? ???????????????? ????????, ?????? ????????????????? ???????????, ?????? ??????????, ?????? ???????. ???????? ??????????? ???????? ????????? ?????????? ????? ? ??????????? ???????????????? ????????, ???????????? ? ???. ?????????? ? ????????????? ????? ?????????? ???????? ?????????? ? ?????????????? ???????????????? ???????? ? ????????? ?? ??????????? ?????????????????? ?????? ? ????????????? ??????????? ???????? ????????? / ???????????...|$|R
40|$|Jet and dijet {{rates in}} pA {{collisions}} at LHC energies are computed using a NLO Monte Carlo <b>code.</b> <b>Uncertainties</b> {{due to the}} choice of scale, jet-finding algorithm, nucleon parton densities and nuclear modifications of nucleon parton densities are analyzed. Predictions for different pA collisions at LHC energies are presented...|$|R
5000|$|Objects such as [...] with a {{condition}} <b>code</b> (<b>Uncertainty</b> Parameter U) of E where the eccentricity is assumed are considered lost. [...] has an Uncertainty Parameter of 8, {{and the next}} good chance to observe the asteroid may not be until November 2044 when the uncertainty allows it to pass somewhere between 0.03-0.19 AU from Earth.|$|R
40|$|We have {{developed}} a new model for calculating the expected yield of cosmic-ray spallation neutrons in a Cargo Container Counter, and we have benchmarked the model against measurements made with several existing large neutron counters. We also developed two versions of a new measurement <b>uncertainty</b> prediction <b>code</b> based on Microsoft Excel spreadsheets. The codes calculate the minimum detectability limit for the Cargo Container Counter for either neutron singles or doubles counting, and also propagate the uncertainties associated with efficiency normalization flux monitors and cosmic ray flux monitors. This paper will describe the physics basis for this analysis, and the results obtained for several different counter designs...|$|R
40|$|In {{the common}} {{approach}} to the generic camera calibration (GCC), one uses dense ray coding (with e. g. active grids displayed on an LCD screen) {{in order to find}} the ray origin and direction for each camera pixel independently. While applicable to many types of imaging sensors, the GCC fails to describe the local differential properties of ray bundles that are important in e. g. studying the geometry of infinitesimal scene changes via optical flow. In this report, we investigate the alternative {{approach to the}} GCC where the camera ray origins and directions are assumed to be differentiable functions of the sensor position. In particular, we present a novel calibration technique based on finite element method that unites the ray update and bundle adjustment stages of the common GCC and accommodates arbitrary anisotropic <b>coding</b> <b>uncertainties</b> and non-planar <b>coding</b> surfaces. The accuracy and the stability of the resulting smooth generic camera calibration (sGCC) algorithm are verified based on some non-trivial synthetic examples...|$|R
40|$|In {{this paper}} we propose {{modifications}} for the learning rules of Marshall's EXIN (excitatory + inhibitory) neural network model in order to decrease its computational complexity and understand {{the role of the}} weight updating learning rules in correctly encoding familiar, superimposed and ambiguous input patterns. The MEXIN (Modified EXIN) models introduce mixtures of competitive and Hebbian updating rules. In this case, only the weights of the unit with highest activation are updated. Hence, the MEXIN networks require less computation than the original EXIN model. A number of simulations are carried out with the aim of showing how the models respond to overlapping, superimposed and ambiguous patterns. Keywords: EXIN networks, anti-hebbian learning, competitive learning, <b>uncertainty,</b> distributed <b>coding...</b>|$|R
40|$|Jet and dijet {{rates in}} AB {{collisions}} at LHC energies are computed using a NLO Monte Carlo <b>code.</b> <b>Uncertainties</b> due to nuclear modifications of nucleon parton densities are analyzed. Predictions for different AB collisions at LHC energies are presented. Comment: LaTeX 2 e, 6 pages, uses enclosed cernrep. cls, 2 figures included using graphicx; {{contribution to the}} Yellow Report on Hard Probes in Heavy Ion Collisions at the LH...|$|R
40|$|A {{method to}} {{determine}} both the correspondences {{and the structure}} from the camera location is presented. Straight image segments are used as features. The location <b>uncertainty</b> is <b>coded</b> using a probabilistic model. The finite length of the image segments is considered, so a more restrictive equation (respect the usage of infinite straight lines) is used, and hence the spurious rejection is improved. The probabilistic modelling derives all the location uncertainty from image error and from camera location error. Thus, the uncertainty is fixed from a physical basis, simplifying the tuning for the matching thresholds. Furthermore, covariance matrices representing the reconstruction location error are also computed. Experimental results with real images for a trinocular system, and for a sequence of images are presented...|$|R
40|$|A {{computer}} {{package is}} presented for the integrated risk assessment of accidental releases of hazardous substances. DECARA provides an integrated risk analysis including source-term strength evaluation, {{estimation of the}} hazardous cloud dispersion and quantification of health impacts. Multiple accidents, each with a certain probability of occurrence can be handled. Dispersion of heavier as well as lighter-than-air gases released instantaneously or continuously, can be simulated. Time-varying release rates are possible. Uncertainty analysis can be performed including both parameter and modelling <b>uncertainty.</b> The <b>code</b> calculates the unconditional fatality probability at any point around the site of release. Isorisk curves or maximum individual risk versus distance from the source can be generated. The computer package is portable and available for running in personal computer...|$|R
40|$|I argue {{here that}} the problem of <b>coding</b> <b>uncertainty</b> in environments with many sources of {{variation}} in terms of a probabilistic inferential system is now, for the most part, solved thanks to recent developments in influence diagrams. In this paper I outline why I believe this to be the case and review some of the more important features of this methodology which have enabled such complex probability systems to be elicited and to work efficiently...|$|R
40|$|The best-estimate {{calculation}} {{results from}} complex system codes {{are affected by}} approximations that are un-predictable {{without the use of}} computational tools that account for the various sources of <b>uncertainty.</b> The <b>Code</b> with (the capability of) Internal Assessment of Uncertainty (CIAU) has been proposed by the University of Pisa to realize the integration between a qualified best-estimate thermal- hydraulic system code and a suitable uncertainty methodology and to supply proper uncertainty bands each time a nuclear power plant transient scenario is calculated. In the frame of a cooperation with The Pennsylvania State University, the CIAU code has been recently extended to evaluate the uncertainty of coupled 3 -D neutronics/thermal-hydraulics calculations. The result of this effort is CIAU-TN. The derivation of the methodology and the adopted qualification processes (“internal” and “external”) are described in the paper...|$|R
5000|$|Due to the {{proximity}} of its orbit to Earth and its estimated size, this object has been classified as a potentially hazardous asteroid (PHA) by the Minor Planet Center in Cambridge, Massachusetts. In November 2006 there were 823 PHAs known. , there are 1261 PHAs known. [...] {{was removed from the}} Sentry Risk Table on October 10, 2002. A Doppler observation has helped produce a well known trajectory with a condition <b>code</b> (<b>Uncertainty</b> Parameter U) of 0.|$|R
40|$|Graduation date: 2013 Access {{restricted}} to the OSU Community, at author's request, from Dec. 5, 2012 - Dec. 5, 2013 This study is intended to investigate and propose a systematic method for uncertainty quantification for the computer <b>code</b> validation application. <b>Uncertainty</b> quantification has gained more and more attentions in recent years. U. S. Nuclear Regulatory Commission (NRC) {{requires the use of}} realistic best estimate (BE) computer code to follow the rigorous Code Scaling, Application and Uncertainty (CSAU) methodology. In CSAU, the Phenomena Identification and Ranking Table (PIRT) was developed to identify important <b>code</b> <b>uncertainty</b> contributors. To support and examine the traditional PIRT with quantified judgments, this study proposes a novel approach, the Quantified PIRT (QPIRT), to identify important code models and parameters for uncertainty quantification. Dimensionless analysis to code field equations to generate dimensionless groups (Π groups) using code simulation results serves as the foundation for QPIRT. Uncertainty quantification using DAKOTA code is proposed in this study based on the sampling approach. Nonparametric statistical theory identifies the fixed number of code run to assure the 95 percent probability and 95 percent confidence in the <b>code</b> <b>uncertainty</b> intervals...|$|R
40|$|Young {{people with}} conduct {{disorder}} often experience histories of psychosocial adversity and socioeconomic insecurity. For these individuals, real-world future outcomes {{are not only}} delayed in their delivery but also highly uncertain. Under such circumstances, accentuated time preference (extreme favoring of the present over the future) is a rational response to the everyday reality {{of social and economic}} transactions. Building on this observation, the author sets out the hypothesis that the exaggerated temporal discounting displayed by individuals with conduct disorder reported by White et al. (2014) is an adaptation to chronic exposure to psychosocial insecurity during development. The author postulates that this adaptation leads to (a) a decision-making bias whereby delay and <b>uncertainty</b> are <b>coded</b> as inseparable characteristics of choice outcomes and/or (b) reprogramming of the brain networks regulating intertemporal decision making. Future research could explore the putative role of environmental exposures to adversity in the development of exaggerated temporal discounting in conduct disorder as well as the mediating role of putative cognitive and neurobiological adaptations. <br/...|$|R
40|$|We discuss {{graphical}} methods {{which may}} be employed generically for uncertainty and sensitivity analysis. This field is rather new, and the literature reveals {{very little in the}} way of theoretical development. Perhaps it is the nature of these methods that one simply `sees' what is going on. Apart from statistical reference books [Cleveland, 1993] which focus on visualizing data, the main sources for graphical methods are software packages. Our focus is visualization to support uncertainty and sensitivity analysis. A simple problem for illustrating four generic graphical techniques, namely tornado graphs, radar plots, multiple scatter plots, and cobweb pots. <b>Uncertainty</b> analysis <b>codes,</b> with or without graphic facilities, were benchmarked in a recent workshop of the technical committee Uncertainty Modelling of the European Safety and Reliability Association. The report [Cooke 1997] contains descriptions and references to the codes, as well as simple test problems. An extended version of this paper will appear in [Saltelli appearing]...|$|R
40|$|Quantifying <b>uncertainties</b> on <b>code</b> outputs is an {{important}} step for code-based design and scenario development. Because of the high computational cost of plasma edge transport simulations, the propagation of uncertainties on input parameters quickly becomes intractable. The paper starts with a short overview of current concepts to deal with this issue. A practical in-parts adjoint approach to sensitivity calculation is then applied for computationally efficient uncertainty propagation. The cumbersome derivation-by-hand of the sensitivity expressions is avoided, while the computational cost is roughly kept independent of the number of uncertain parameters. Exemplarily sensitivities of the outer strike point temperature and a heat load objective are calculated for a WEST case. Transport coefficients, boundary condition parameters, rate coefficients, as well as uncertain parameters in the magnetic equilibrium calculation are considered. The sensitivities are verified to be accurate, while the computational cost to compute the entire sensitivity matrix is equivalent to only one additional plasma edge simulation for each output quantity of interest. Furthermore, several logical trends are observed in the sensitivities...|$|R
40|$|Abstract. This paper {{provides}} baseline {{results for}} the medical automatic annotation task of CLEF 2007 applying the image retrieval in medical applications (IRMA) -based algorithms initially used for the corresponding tasks in 2005 and 2006, with identical parameterization. Three classifiers based on global image features are combined within a nearest neighbor (NN) approach: texture histograms and two distance measures, which are applied on down-scaled versions of the original images and model common variabilities in the image data, such as translation, radiation dose, and local deformations. According to the classification scheme introduced in 2007, which uses the hierarchical structure of the coding scheme for the categorization, the baseline classifier yields a score of 51. 29 and 52. 54 when the full codes are reported for 1 -NN and 5 -NN, respectively. This corresponds to error rates of 20. 0 % and 18. 0 % (rank 18 among 68 submited runs), respectively. Improvements addressing the code hierarchy and optionally reporting <b>uncertainty</b> at <b>code</b> levels were not obtained. However, we can conclude that 2007 was a slightly easier task than the previous years. ...|$|R
40|$|Uncertainty {{analysis}} is {{the evaluation of}} the distribution of a computer code output Y given <b>uncertainty</b> about the <b>code’s</b> true inputs X. When we sum the outputs of a number of different codes, it is important to account for the different sources of uncertainty that can arise. An estimate of the biospheric carbon flux for England and Wales requires the outputs of many different computer codes to be aggregated. In this report, the techniques needed for output aggregation will be discussed and the process of producing estimates of the carbon flux will be reviewed. This includes an independent normal approximation to a Dirichlet distribution for proportion parameters. ...|$|R
40|$|Computational {{capability}} {{has been}} developed to calculate sensitivity coefficients of generalized responses with respect to cross-section data in the SCALE code system. The focus {{of this paper is}} the implementation of generalized perturbation theory (GPT) for one-dimensional and two-dimensional deterministic neutron transport calculations. GPT is briefly summarized for computing sensitivity coefficients for reaction rate ratio responses within the existing framework of the TSUNAMI sensitivity and <b>uncertainty</b> (S/U) analysis <b>code</b> package in SCALE. GPT provides the capability to analyze generalized responses related to reactor analysis, such as homogenized cross-sections, relative powers, and conversion ratios, as well as measured experimental parameters such as 28 ρ (epithermal/thermal 238 U capture rates) in thermal benchmarks and fission ratios such a...|$|R
40|$|Although the {{approach}} for licensing of Nuclear Power Plants (NPP) differs considerably between countries, the USNRC Regulatory Guide (RG) 1. 70 [1] constitutes {{the starting point}} for preparing the safety analysis report in several countries, including Argentina. Licensing tends to move from a “conservative ” to a risk informed approach. This implies that for safety analysis best estimate codes, together with an assessment of the <b>uncertainties</b> of <b>code</b> calculations (best estimate plus uncertainty, BEPU) are preferred over a conservative bounding case analysis using penalizing codes and boundary conditions. The project Atucha 2 NPP (CNA 2) was started officially in 1979. Siemens-KWU won the assignment to construct a pressurized heavy water reactor (PHWR) featuring 745 MWe (2160 MWth). The work on site began in 1981. The construction license was issued based on a Preliminary Safety Analyses Report (PSAR), which formerly followed RG 1. 70 [1], but whose content is based in large parts on German legislation. For example, the safety systems are designed with n+ 2 redundancy (single failure plus repair case). In contrary to both, the German and US licensing approach, the Argentine Regulatory Authority ARN places stron...|$|R
40|$|Fire {{simulation}} {{codes are}} powerful tools {{for use in}} risk-informed and performance-based approaches for risk assessment. Given increasing use of fire simulation code results, accounting for the uncertainty inherent in fire simulation codes is becoming more important than ever. This research presents a "white-box" methodology {{with the goal of}} accounting for uncertainties resulting from simulation <b>code.</b> <b>Uncertainties</b> associated with the input variables used in the codes as well as the uncertainties associated with the sub-models and correlations used inside the simulation code are accounted for. A Bayesian estimation approach is used to integrate all evidence available and arrive at an estimate of the uncertainties associated with a parameter of interest being estimated by the simulation code. Two example applications of this methodology are presented...|$|R
40|$|In {{the frame}} of {{developmental}} assessment and code validation, a post-test calculation of the test QUENCH- 07 was performed with ATHLET-CD. The system code ATHLET-CD is being developed for best-estimate simulation of accidents with core degradation and for evaluation of accident management procedures. It applies the detailed models of the thermal-hydraulic code ATHLET in an efficient coupling with dedicated models for core degradation and fission products behaviour. The first step of the work was the simulation of the test QUENCH- 07 applying the modelling options recommended in the code User’s Manual (reference calculation). The global results of this calculation showed a good agreement with the measured data. This calculation was complemented by a sensitivity analysis in order to investigate {{the influence of a}} combined variation of code input parameters on the simulation of the main phenomena observed experimentally. Results of this sensitivity analysis indicate that the main experimental measurements lay within the uncertainty range of the corresponding calculated values. Among the main contributors to the <b>uncertainty</b> of <b>code</b> results are the heat transfer coefficient due to forced convection to superheated steam-argon mixture, the thermal conductivity of the shroud isolation and the external heater rod resistance. Uncertainties on modelling of B 4 C oxidation do not affect significantly the total calculated hydrogen release rates. 1...|$|R
40|$|This paper {{describes}} {{an overview of}} two Functional Reliability (FR) assessment performed {{for some of the}} Passive Safety Systems (PSS) of a CAREM-like integral advance reactor. The FR can be understood as the success probability of a PSS to fulfill a safety function. The safety function fulfillment is characterized and quantified by a Performance Indicator (PI), which is a measure of how far the system is of the core damage or of a design criterion breaching. PI uncertainties were estimated by means of the Reliability Method for Passive Safety (RMPS), which takes into account the relevant parameters uncertainties. These assessments were made with two different main goals: design verification of the Isolation Condensers (IC), and design improvement of the Medium Pressure Injection System (MPIS). For the first goal, a Station Black-Out (SBO) event was proposed, and for the second one, it was proposed a Small Break Loss of Coolant Accident plus a SBO (SB-LOCA + SBO) event. For this assessment were considered <b>uncertainties</b> in <b>code</b> parameters, besides <b>uncertainties</b> in engineering parameters (design, construction, operation and maintenance), in order to perform Monte Carlo simulations based on a Best Estimate (B-E) plant model (RELAP 5). Response surfaces based on PI were used for improving the FR accuracy of these PSS. Their predictio...|$|R
40|$|Accurate {{mapping of}} species {{distributions}} {{is a fundamental}} goal of modern biogeography, both for basic and applied purposes. This is commonly done by plotting known species occurrences, expert-drawn range maps or geographical estimations derived from species distribution models. However, all three kinds of maps are implicitly subject to uncertainty, due to the quality and bias of raw distributional data, the process of map building, and the dynamic nature of species distributions themselves. Here we review the main sources of <b>uncertainty</b> suggesting a <b>code</b> of good practices {{in order to minimize}} their effects. Specifically, we claim that uncertainty should be always explicitly taken into account and we propose the creation of maps of ignorance to provide information on where the mapped distributions are reliable and where they are uncertain. © The Author(s) 2011...|$|R
40|$|An {{uncertainty}} and sensitivity {{analysis for the}} simulation of a station blackout scenario in the Jules Horowitz Reactor (JHR) is presented. The JHR is a new material testing reactor under construction at CEA on the Cadarache site, France. The thermal-hydraulic system code CATHARE is applied to investigate {{the response of the}} reactor system to the scenario. The {{uncertainty and}} sensitivity study was based on a statistical methodology for <b>code</b> <b>uncertainty</b> propagation, and the 'Uncertainty and Sensitivity' platform URANIE was used. Accordingly, the input uncertainties relevant to the transient, were identified, quantified, and propagated to the code output. The results show that the safety criteria are not exceeded and sufficiently large safety margins exist. In addition, the most influential input uncertainties on the safety parameters were found by making use of a sensitivity analysis...|$|R
40|$|A Performance-Based Wind Engineering (PBWE) {{framework}} {{for assessing the}} serviceability limit state of tall buildings is considered. The immediate focus is on {{a key component of}} this procedure which involves creating an appropriate coupling interface between two independent fluid and structural solvers. The fluid solver (ANSYS FLUENT) uses the finite volume method to model the wind field and predict the wind-induced loads. The structural solver (Strand 7) applies the finite element method to model the resulting dynamic response. The requirement of the coupling algorithm is to appropriately consider the interaction phenomena involving the wind and the tall building. Clearly, no single coupling algorithm exists which may be appropriate for all fluid-structure interaction problems; the choice depends on an appropriate level of detail and accuracy in a given simulation setting and has {{a significant impact on the}} computational demand and accuracy. The decision on the level of complexity for this particular coupling problem is related to the inherently probabilistic context of the PBWE framework and its ability to account for a whole range of <b>uncertainties.</b> The <b>code</b> for the coupling interface has been developed using Microsoft VBA and its functionality will be demonstrated using a case study of a standard 49 -storey tall building. Attention will be focused on its capabilities and the potential consequences of its limitations...|$|R
40|$|We {{have studied}} the effects of random laser speckle and self-mixing {{interference}} on TDLS based gas measurements made using integrating spheres. Details of the theory and TDLS apparatus are given in Part 1 of this paper and applied here to integrating spheres. Experiments have been performed using two commercial integrating spheres with diameters of 50 mm and 100 mm {{for the detection of}} methane at 1651 nm. We have calculated the expected levels of laser speckle related uncertainty, considered to be the fundamental limiting noise, and imaged subjective laser speckle in a sphere using different sized apertures. For wavelength modulation spectroscopy, noise equivalent absorbances (NEAs) of around 5 × 10 - 5 were demonstrated in both cases, corresponding to limits of detection of 1. 2 ppm methane and 0. 4 ppm methane respectively. Longer-term drift was found to be at an NEA of 4 × 10 - 4. This lies within our broad range of expectations. For a direct spectral scan with no wavelength dither, a limit of detection of 75 ppm or fractional measured power uncertainty of 310 - 3 corresponded well with our prediction for the objective speckle <b>uncertainty.</b> PACS <b>codes</b> 07. 07. Df sensors – chemical 42. 62. Fi laser spectroscopy 42. 30. Ms Speckle and moire patterns 42. 79. -e optical instruments, equipment and techniques 2...|$|R
