14|49|Public
2500|$|Ultimately, {{the purpose}} of {{distinct}} operating modes for the CPU is to provide hardware protection against accidental or deliberate corruption of the system environment (and corresponding breaches of system security) by software. Only [...] "trusted" [...] portions of system software are allowed to execute in the unrestricted environment of kernel mode, and then, in paradigmatic designs, only when absolutely necessary. All other software executes {{in one or more}} user modes. If a processor generates a fault or exception condition in a user mode, in most cases system stability is unaffected; if a processor generates a fault or exception condition in kernel mode, most operating systems will halt the system with an <b>unrecoverable</b> <b>error.</b> When a hierarchy of modes exists (ring-based security), faults and exceptions at one privilege level may destabilize only the higher-numbered privilege levels. Thus, a fault in Ring 0 (the kernel mode with the highest privilege) will crash the entire system, but a fault in Ring 2 will only affect rings 3 and beyond and Ring 2 itself, at most.|$|E
5000|$|Alternately, if {{the thread}} holding the mutex is deleted (perhaps {{due to an}} <b>unrecoverable</b> <b>error),</b> the mutex can be {{automatically}} released.|$|E
50|$|Previous {{drivers were}} fully {{implemented}} in kernel mode, whereas WDDM is implemented partly in user mode. If the user mode area fails with an <b>unrecoverable</b> <b>error,</b> it will, at the most, cause the application to quit unexpectedly instead {{of producing a}} blue screen error as it would in previous driver models.|$|E
25|$|Relays {{following}} the decode-and-forward strategy overhear transmissions from the source, decode them {{and in case}} of correct decoding, forward them to the destination. Whenever <b>unrecoverable</b> <b>errors</b> reside in the overheard transmission, the relay can {{not contribute to the}} cooperative transmission.|$|R
40|$|Today’s {{data storage}} systems are {{increasingly}} adopting low-cost disk drives that have higher capacity but lower reliability, leading to more frequent rebuilds {{and to a}} higher risk of <b>unrecoverable</b> media <b>errors.</b> We propose an efficient intradisk redundancy scheme to enhance the reliability of RAID systems. This scheme introduces an additional level of redundancy inside each disk, on top of the RAID redundancy across multiple disks. The RAID parity provides protection against disk failures, whereas the proposed scheme aims to protect against media-related <b>unrecoverable</b> <b>errors.</b> In particular, we consider an intradisk redundancy architecture that is based on an interleaved parity-check coding scheme, which incurs only negligible I/O performance degradation. A comparison between this coding scheme and schemes based on traditional Reed–Solomon codes and single-parity-check codes is conducted by analytical means. A new model is developed to capture the effect of correlated <b>unrecoverable</b> sector <b>errors.</b> The probability of an unrecoverable failure associated with these schemes is derived for the new correlated model, {{as well as for the}} simpler independent error model. We also derive closed-form expressions for the mean time to data loss of RAID- 5 and RAID- 6 systems in the presence of <b>unrecoverable</b> <b>errors</b> and disk failures. W...|$|R
25|$|Go was {{initially}} released with exception handling explicitly omitted, with the developers {{arguing that it}} obfuscated control flow. Later, the exception-like / mechanism {{was added to the}} language, which the Go authors advise using only for <b>unrecoverable</b> <b>errors</b> that should halt the entire process.|$|R
50|$|The {{instruction}} set is variable-width and extensible, {{so that more}} encoding bits can always be added. Space for the 128-bit stretched version of the ISA was reserved, because 60 years of industry experience {{has shown that the}} most <b>unrecoverable</b> <b>error</b> in {{instruction set}} design is a lack of memory address space. , the 128-bit ISA remains undefined intentionally, because there is yet so little practical experience with such large memory systems.|$|E
50|$|On IBM mainframes, {{the term}} wait state is {{used with a}} {{different}} meaning. A wait state refers to a CPU being halted, possibly due {{to some kind of}} serious error condition (such as an <b>unrecoverable</b> <b>error</b> during operating system to IPL). A wait state is indicated by bit 14 of the PSW being set to 1, with other bits of the PSW providing a wait state code giving a reason for the wait. In z/Architecture mode, the wait state code is found in bits 116-127.|$|E
50|$|Most {{algorithms}} {{for mutual}} exclusion {{are designed with}} the assumption that no failure occurs while a process is running inside the critical section. However, in reality such failures may be commonplace. For example, a sudden loss of power or faulty interconnect might cause a process in a critical section to experience an <b>unrecoverable</b> <b>error</b> or otherwise be unable to continue. If such a failure occurs, conventional, non-failure-tolerant mutual exclusion algorithms may deadlock or otherwise fail key liveness properties. To deal with this problem, several solutions using crash-recovery mechanisms have been proposed.|$|E
5000|$|The Guru Meditation is {{an error}} notice {{displayed}} by early {{versions of the}} Commodore Amiga computer when they crashed. It {{is analogous to the}} [...] "Blue Screen of Death" [...] in Microsoft Windows operating systems, or a kernel panic in Unix. It has later been used as a message for <b>unrecoverable</b> <b>errors</b> in software such as Varnish and VirtualBox.|$|R
40|$|Today’s {{data storage}} systems are {{increasingly}} adopting lowcost disk drives that have higher capacity but lower reliability, leading to more frequent rebuilds {{and to a}} higher risk of <b>unrecoverable</b> media <b>errors.</b> We propose a new XORbased intra-disk redundancy scheme, called interleaved parity check (IPC), to enhance the reliability of RAID systems that incurs only negligible I/O performance degradation. The proposed scheme introduces an additional level of redundancy inside each disk, on top of the RAID redundancy across multiple disks. The RAID parity provides protection against disk failures, while the proposed scheme aims to protect against media-related <b>unrecoverable</b> <b>errors.</b> We develop a new model capturing the effect of correlated <b>unrecoverable</b> sector <b>errors</b> and subsequently use it to analyze the proposed scheme as well as the traditional redundancy schemes based on Reed-Solomon (RS) codes and single-parity-check (SPC) codes. We derive closed-form expressions for the mean time to data loss (MTTDL) of RAID 5 and RAID 6 systems in the presence of <b>unrecoverable</b> <b>errors</b> and disk failures. We then combine these results for a comprehensive characterization of the reliability of RAID systems that incorporate the proposed IPC redundancy scheme. Our results show that in the practical case of correlated errors, the proposed scheme provides the same reliability as the optimum albeit more complex RS coding scheme. Finally, the throughput performance of incorporating the intra-disk redundancy on various RAID systems is evaluated by means of event-driven simulations. A detailed description of these contributions is given in [1]...|$|R
50|$|After {{initially}} omitting exceptions, the exception-like / mechanism {{was eventually}} {{added to the}} language, which the Go authors advise using for <b>unrecoverable</b> <b>errors</b> {{such as those that}} should halt an entire program or server request, or as a shortcut to propagate errors up the stack within a package (but not across package boundaries; there, error returns are the standard API).|$|R
5000|$|Libcwd {{provides}} several macros {{that are}} easily extensible, allowing {{the user to}} basically do anything that one can normally do with ostreams. However, if one just wants to write debug output, two macros will suffice: Dout and DoutFatal. The latter {{is to be used}} for fatal debug output, after which the application needs to be terminated. For example: if (error) DoutFatal(dc::fatal, [...] "An <b>unrecoverable</b> <b>error</b> occurred.");The difference with Dout is that when the application is compiled without debug code, the macro Dout (...) is replaced with nothing, while DoutFatal (...) is replaced with code that prints its output and terminates (in a way that the user can define).|$|E
5000|$|Ultimately, {{the purpose}} of {{distinct}} operating modes for the CPU is to provide hardware protection against accidental or deliberate corruption of the system environment (and corresponding breaches of system security) by software. Only [...] "trusted" [...] portions of system software are allowed to execute in the unrestricted environment of kernel mode, and then, in paradigmatic designs, only when absolutely necessary. All other software executes {{in one or more}} user modes. If a processor generates a fault or exception condition in a user mode, in most cases system stability is unaffected; if a processor generates a fault or exception condition in kernel mode, most operating systems will halt the system with an <b>unrecoverable</b> <b>error.</b> When a hierarchy of modes exists (ring-based security), faults and exceptions at one privilege level may destabilize only the higher-numbered privilege levels. Thus, a fault in Ring 0 (the kernel mode with the highest privilege) will crash the entire system, but a fault in Ring 2 will only affect rings 3 and beyond and Ring 2 itself, at most.|$|E
40|$|We {{investigate}} {{the maximum number}} of intersections between two polygons with p and q vertices, respectively, in the plane. The cases where p or q is even or the polygons {{do not have to be}} simple are quite easy and already known, but when p and q are both odd and both polygons are simple, the problem is more difficult. The conjectured maximum is (p- 1) (q- 1) + 2 for all odd p and q. Comment: This paper has been withdrawn by the author due to an <b>unrecoverable</b> <b>error</b> in the proof of Lemma 2. In fact, a counterexample to an even weaker version of Lemma 2 has been found by the autho...|$|E
40|$|Abstract — Task {{learning}} in robotics requires repeatedly executing the same actions in different states {{to learn the}} model of the task. However, in real-world domains, there are usually sequences of actions that, if executed, may produce <b>unrecoverable</b> <b>errors</b> (e. g. breaking an object). Robots should avoid repeating such errors when learning, and thus explore the state space in a more intelligent way. This requires identifying dangerous action effects to avoid including such actions in the generated plans, {{while at the same time}} enforcing that the learned models are complete enough for the planner not to fall into dead-ends. We thus propose a new learning method that allows a robot to reason about dead-ends and their causes. Some such causes may be dangerous action effects (i. e., leading to <b>unrecoverable</b> <b>errors</b> if the action were executed in the given state) so that the method allows the robot to skip the exploration of risky actions and guarantees the safety of planned actions. If a plan might lead to a dead-end (e. g., one that includes a dangerous action effect), the robot tries to find an alternative safe plan and, if not found, it actively asks a teacher whether the risky action should be executed. This method permits learning safe policies as well as minimizing <b>unrecoverable</b> <b>errors</b> during the learning process. Experimental validation of the approach is provided in two different scenarios: a robotic task and a simulated problem from the international planning competition. Our approach greatly increases success ratios in problems where previous approaches had high probabilities of failing. I...|$|R
40|$|Multipath {{propagation}} is {{a common}} source of error in radio occultation experiments in dense atmospheres. If not correctly detected and mapped into the ray asymptote structure, multipath eects produce <b>unrecoverable</b> <b>errors</b> in the inverse problem for the refractive index prole. Raytracing {{can be used to}} clearly illustrate multi-path caused by interacting signals following closely spaced paths in the atmosphere. Such dynamic signals occur in connection with sharp variations in refractivity with height and result in multivalued frequency with time. Closed-loop radio receivers based on phase-locked loops that are designed to receive single-valued frequency signals will fail to track these dynamic multivalued signals. SUMMARY The increasing use of radio occultation to determine atmospheric structure and climate motivates a continuing examination of data collection, processing, and interpretation methods in the presence of atmospheric multi-path propagation. Multipath propagation {{is a common}} source of error in radio occultation experiments in dense atmospheres. If not correctly detected and mapped into the ray asymptote structure, multipath eects produce <b>unrecoverable</b> <b>errors</b> in the inverse problem for the refractive index prole. In order to use backprojection methods to mitigate multipath eects in the refractive index prole, it is im...|$|R
40|$|Two schemes {{proposed}} {{to cope with}} unrecoverable or latent media errors and enhance the reliability of RAID systems are examined. The first scheme is the established, widely used disk scrubbing scheme, which operates by periodically accessing disk drives to detect media-related <b>unrecoverable</b> <b>errors.</b> These errors are subsequently corrected by rebuilding the sectors affected. The second scheme is the recently proposed intradisk redundancy scheme which uses a further level of redundancy inside each disk, {{in addition to the}} RAID redundancy across multiple disks. Analytic results are obtained assuming Poisson arrivals of random I/O requests. Our results demonstrate that the reliability improvement due to disk scrubbing depends on the scrubbing frequency and the workload of the system, and may not reach the reliability level achieved by a simple IPC-based intra-disk redundancy scheme, which is insensitive to the workload. In fact, the IPC-based intra-disk redundancy scheme achieves essentially the same reliability as that of a system operating without <b>unrecoverable</b> sector <b>errors.</b> For heavy workloads, the reliability achieved by the scrubbing scheme can be orders of magnitude less than that of the intra-disk redundancy scheme...|$|R
40|$|This letter {{presents}} an analytical model that jointly exploits the buffer dynamics {{of both the}} sending and receiving nodes to find the optimum number of byte-level and packet-level forward error correction (FEC) units for real-time multimedia transmission over wireless networks. The proposed analytical model first provides an optimum number of FEC units required at the byte-level, and then chooses the number of FEC units at the packet-level based on current channel and network conditions. The accuracy of the proposed model is dependent on two parameters: the variable deadline-time at the byte-level and fixed round-trip time (RTT) delay at the packet-level. Numerical results demonstrate {{the effectiveness of the}} model in reducing the <b>unrecoverable</b> <b>error</b> probability, which is achieved when the byte-level FEC scheme is supplemented by the packet-level FEC scheme...|$|E
40|$|Cosmic {{radiation}} induced soft errors {{have emerged}} as a key challenge in computer system design. The exponential increase in the transistor count will drive the per chip fault rate sky high. New techniques for detecting errors in the logic and memories that allow meeting the desired failures in-time (FIT) budget in future chip multiprocessors (CMPs) are essential. Among the two major contributors towards soft error rate, silent data corruption (SDC) and detected <b>unrecoverable</b> <b>error</b> (DUE), DUE is the largest. Moreover, processors can experience a super-linear increase in DUE when {{the size of the}} write-back cache is doubled. This paper targets the DUE problem in write-back data caches. We analyze the cost of protection against single bit and multi-bit upsets into caches. Our results show that the proposed mechanism can reduce the DUE to “ 0 ” with minimum area, power and performance overheads. Peer ReviewedPostprint (published version...|$|E
40|$|The {{emergence}} of online crowdsourcing {{services such as}} Ama-zon Mechanical Turk, presents us huge opportunities to dis-tribute micro-tasks at an unprecedented rate and scale. Un-fortunately, the high verification cost and the unstable em-ployment relationship give rise to opportunistic behaviors of workers, which in turn exposes the requesters to quality risks. Currently, most requesters rely on redundancy to identify the correct answers. However, existing techniques cannot separate the true (<b>unrecoverable)</b> <b>error</b> rates from the (recoverable) biases that some workers exhibit, which would lead to in-correct assessment of worker quality. Furthermore, massive redundancy is expensive, increasing significantly the cost of crowdsourced solutions. In this paper, we present an algorithm that can easily sepa-rate the true error rates from the biases. Also, we describe how to seamlessly integrate the existence of “gold ” data for learning the quality of workers. Next, we bring up an ap-proach for actively testing worker quality in order to quicky identify spammers or malicious workers. Finally, we present experimental results to demonstrate the performance of our proposed algorithm. 1...|$|E
40|$|Trabajo presentado a la International Conference on Intelligent Robots and Systems celebrada en Hamburgo (Alemania) del 28 de septiembre al 2 de octubre de 2015. Task {{learning}} in robotics requires repeatedly executing the same actions in different states {{to learn the}} model of the task. However, in real-world domains, there are usually sequences of actions that, if executed, may produce <b>unrecoverable</b> <b>errors</b> (e. g. breaking an object). Robots should avoid repeating such errors when learning, and thus explore the state space in a more intelligent way. This requires identifying dangerous action effects to avoid including such actions in the generated plans, {{while at the same time}} enforcing that the learned models are complete enough for the planner not to fall into dead-ends. We thus propose a new learning method that allows a robot to reason about dead-ends and their causes. Some such causes may be dangerous action effects (i. e., leading to <b>unrecoverable</b> <b>errors</b> if the action were executed in the given state) so that the method allows the robot to skip the exploration of risky actions and guarantees the safety of planned actions. If a plan might lead to a dead-end (e. g., one that includes a dangerous action effect), the robot tries to find an alternative safe plan and, if not found, it actively asks a teacher whether the risky action should be executed. This method permits learning safe policies as well as minimizing <b>unrecoverable</b> <b>errors</b> during the learning process. Experimental validation of the approach is provided in two different scenarios: a robotic task and a simulated problem from the international planning competition. Our approach greatly increases success ratios in problems where previous approaches had high probabilities of failing. This work was supported by CSIC project MANIPlus 201350 E 102. D. Martínez is also supported by the Spanish Ministry of Education, Culture and Sport via a FPU doctoral grant (FPU 12 - 04173). Peer Reviewe...|$|R
40|$|We {{consider}} a simple model of higher order, functional computation over the booleans. Then, we enrich {{the model in}} order to encompass non-termination and <b>unrecoverable</b> <b>errors,</b> taken separately or jointly. We show that the models so defined form a lattice when ordered by the extensional collapse situation relation, introduced in order to compare models {{with respect to the}} amount of "intensional information" that they provide on computation. The proofs are carried out by exhibiting suitable applied λ-calculi, and by exploiting the fundamental lemma of logical relations...|$|R
40|$|At the Optical Computing Lab in the Jet Propulsion Laboratory (JPL) {{a binary}} {{holographic}} data storage system was designed and tested with methods of recording and retrieving the binary information. Levels of error correction {{were introduced to the}} system including pixel averaging, thresholding, and parity checks. Errors were artificially introduced into the binary {{holographic data storage}} system and were monitored {{as a function of the}} defect area fraction, which showed a strong influence on data integrity. Average area fractions exceeding one quarter of the bit area caused <b>unrecoverable</b> <b>errors.</b> Efficient use of the available data density was discussed. ...|$|R
40|$|Crowdsourcing services, such as Amazon Mechanical Turk, {{allow for}} easy {{distribution}} of small tasks {{to a large}} number of workers. Unfortunately, since manually verifying the quality of the submitted results is hard, malicious workers often take advantage of the verification difficulty and submit answers of low quality. Currently, most requesters rely on redundancy to identify the correct answers. However, redundancy is not a panacea. Massive redundancy is expensive, increasing significantly the cost of crowdsourced solutions. Therefore, we need techniques that will accurately estimate the quality of the workers, allowing for the rejection and blocking of the low-performing workers and spammers. However, existing techniques cannot separate the true (<b>unrecoverable)</b> <b>error</b> rate from the (recoverable) biases that some workers exhibit. This lack of separation leads to incorrect assessments of a worker’s quality. We present algorithms that improve the existing state-of-the-art techniques, enabling the separation of bias and error. Our algorithm generates a scalar score representing the inherent quality of each worker. We illustrate how to incorporate cost-sensitive classification errors in the overall framework and how to seamlessly integrate unsupervised and supervised techniques for inferring the quality of the workers. We present experimental results demonstrating the performance of the proposed algorithm under a variety of settings. 1...|$|E
40|$|We {{construct}} {{a new class}} of quantum error-correcting codes for a bosonic mode, which are advantageous for applications in quantum memories, communication, and scalable computation. These “binomial quantum codes” are formed from a finite superposition of Fock states weighted with binomial coefficients. The binomial codes can exactly correct errors that are polynomial up to a specific degree in bosonic creation and annihilation operators, including amplitude damping and displacement noise as well as boson addition and dephasing errors. For realistic continuous-time dissipative evolution, the codes can perform approximate quantum error correction to any given order in the time step between error detection measurements. We present an explicit approximate quantum error recovery operation based on projective measurements and unitary operations. The binomial codes are tailored for detecting boson loss and gain errors by means of measurements of the generalized number parity. We discuss optimization of the binomial codes and demonstrate that by relaxing the parity structure, codes with even lower <b>unrecoverable</b> <b>error</b> rates can be achieved. The binomial codes are related to existing two-mode bosonic codes, but offer the advantage of requiring only a single bosonic mode to correct amplitude damping as well as the ability to correct other errors. Our codes are similar in spirit to “cat codes” based on superpositions of the coherent states but offer several advantages such as smaller mean boson number, exact rather than approximate orthonormality of the code words, and an explicit unitary operation for repumping energy into the bosonic mode. The binomial quantum codes are realizable with current superconducting circuit technology, and they should prove useful in other quantum technologies, including bosonic quantum memories, photonic quantum communication, and optical-to-microwave up- and down-conversion...|$|E
40|$|Nowadays {{ubiquitous}} connectivity, portable computing, pervasive sensing, novel interfaces, {{cheap and}} fast computing units, and advances in robotic devices and actuators are changing our lives, our living environments, and our social interaction. To truly benefit {{the elderly and}} fragile population, commodities based on these novel technologies need to be autonomous and interactive, and must be capable of anticipating user needs, managing complex and unforeseen situations on their own, seamlessly interfacing with casual end-users, and gracefully terminating their functioning when <b>unrecoverable</b> <b>errors</b> occur. Our main aim {{is to provide a}} model for developing a multi-agent system integrated into a medical social network. It must provide a tool for developing assistive services to support elderly patients with disabilities in their daily life. Peer ReviewedPostprint (published version...|$|R
40|$|Many {{sources of}} {{information}} relevant to computer vision and machine learning tasks are often underused. One example is the similarity between the elements from a novel source, such as a speaker, writer, or printed font. By comparing instances emitted by a source, we help ensure that similar instances are given the same label. Previous approaches have clustered instances prior to recognition. We propose a probabilistic framework that unifies similarity with prior identity and contextual information. By fusing information sources in a single model, we eliminate <b>unrecoverable</b> <b>errors</b> that result from processing the information in separate stages and improve overall accuracy. The framework also naturally integrates dissimilarity information, which has previously been ignored. We demonstrate with an application in printed character recognition from images of signs in natural scenes. 1...|$|R
40|$|New {{applications}} such as conferencing and co-operative systems operate in a group environment. Many networks have multicast capabilities, yet, a lack of transport protocols and services that offer end-to-end group communication support can be observed. End-to-end specific issues in this context are communication integrity {{and the problem of}} reliability versus scalability. Full reliability is required by many applications. This usually generates high management overhead and restricts the number of receivers. On the other hand, some applications do not require guaranteed reliability, and a small number of <b>unrecoverable</b> <b>errors</b> can be acceptable as long as a high number of participants can be accommodated. The M-Connection Service presented in this paper provides end-to-end support for distributed multimedia systems. Four service classes to support discrete as well as continuous media have been developed. Further, different reliability schemes to satisfy the requirements of vari [...] ...|$|R
40|$|International audienceDistributed {{computing}} infrastructures {{are commonly}} used through scientific gateways, but operating these gateways requires important human intervention to handle operational incidents. This paper presents a self-healing process that quantifies incident degrees of workflow activities from metrics measuring long-tail effect, application efficiency, data transfer issues, and site-specific problems. These metrics are simple enough to be computed online and they make little assumptions on the application or resource characteristics. From their degree, incidents are classified in levels and associated to sets of healing actions that are selected based on association rules modeling correlations between incident levels. We specifically study the long-tail effect issue, and propose a new algorithm to control task replication. The healing process is parametrized on real application traces acquired in production on the European Grid Infrastructure. Experimental results obtained in the Virtual Imaging Platform show that the proposed method speeds up execution up to a factor of 4, consumes up to 26 % less resource time than a control execution and properly detects <b>unrecoverable</b> <b>errors...</b>|$|R
40|$|In {{a series}} of papers which are either {{published}} [A. R. Hadjesfandiari and G. F. Dargush, Couple stress theory for solids, Int. J. Solids Struct. 48, 2496 - 2510, 2011; A. R. Hadjesfandiari and G. F. Dargush, Fundamental solutions for isotropic size-dependent couple stress elasticity, Int. J. Solids Struct. 50, 1253 - 1265, 2013] or available as preprints Hadjesfandiari and Dargush have reconsidered the linear indeterminate couple stress model. They are postulating a certain physically plausible split in the virtual work principle. Based on this postulate they claim that the second-order couple stress tensor must always be skew-symmetric. Since they use an incomplete set of boundary conditions in their virtual work principle their statement contains <b>unrecoverable</b> <b>errors.</b> This is shown by specifying their development to the isotropic case. However, their choice of constitutive parameters is mathematically possible and still yields a well-posed boundary value problem. Comment: arXiv admin note: text overlap with arXiv: 1504. 0086...|$|R
40|$|Tracking {{targets in}} forward-looking {{infrared}} (FLIR) video sequences taken from airborne platforms is a challenging task. Several tracking failure modes can occur; in particular, discontinuities due to platform's motion can produce {{the so called}} ego-motion failure leading to <b>unrecoverable</b> <b>errors</b> in tracking the target. A novel ego-motion compensation technique for UAVs (unmanned aerial vehicles) is proposed. Data received from the autopilot {{can be used to}} predict the motion of the platform, thus allowing to identify a smaller region of the image (subframe) where the candidate target has to be searched for in the next frame of the sequence. The presented methodology is compared with a recently robust algorithm for automatic target tracking; experimental results show that the proposed motion estimation approach helps to improve performance both in terms of frames processed per second (targets are searched in smaller regions) and in terms of robustness (targets are correctly tracked for all the sequence's frames...|$|R
5000|$|<b>Unrecoverable</b> read <b>errors</b> (URE) {{present as}} sector read failures, {{also known as}} latent sector errors (LSE). The {{associated}} media assessment measure, <b>unrecoverable</b> bit <b>error</b> (UBE) rate, is typically guaranteed {{to be less than}} one bit in 1015 for enterprise-class drives (SCSI, FC, SAS or SATA), and less than one bit in 1014 for desktop-class drives (IDE/ATA/PATA or SATA). Increasing drive capacities and large RAID 5 instances have led to the maximum error rates being insufficient to guarantee a successful recovery, due to the high likelihood of such an error occurring on one or more remaining drives during a RAID set rebuild. [...] When rebuilding, parity-based schemes such as RAID 5 are particularly prone to the effects of UREs as they affect not only the sector where they occur, but also reconstructed blocks using that sector for parity computation. Thus, an URE during a RAID 5 rebuild typically leads to a complete rebuild failure.|$|R
5000|$|In December 1990, Microsoft {{released}} Windows 3.0a. This version {{contained an}} improved {{ability to move}} pieces of data greater than 64KB (the original release could only manipulate one segment of RAM at a time). It also improved stability by reducing <b>Unrecoverable</b> Application <b>Errors</b> (UAEs) associated with networking, printing, and low-memory conditions. This version appears as [...] "Windows 3.00a" [...] in Help/About Windows system dialogs.|$|R
40|$|One of {{the most}} {{prominent}} data quality problems is the existence of duplicate records. Current duplicate elimination procedures usually produce one clean instance (repair) of the input data, by carefully choosing the parameters of the duplicate detection algorithms. Finding the right parameter settings can be hard, and in many cases, perfect settings do not exist. Furthermore, replacing the input dirty data with one possible clean instance may result in <b>unrecoverable</b> <b>errors,</b> for example, identification and merging of possible duplicate records in health care systems. In this paper, we treat duplicate detection procedures as data processing tasks with uncertain outcomes. We concentrate on a family of duplicate detection algorithms that are based on parameterized clustering. We propose a novel uncertainty model that compactly encodes the space of possible repairs corresponding to different parameter settings. We show how to efficiently support relational queries under our model, and to allow new types of queries on the set of possible repairs. We give an experimental study illustrating the scalability and the efficiency of our techniques in different configurations. 1...|$|R
40|$|Provisioning a {{scalable}} communication {{architecture in}} self-reconfiguring (SR) modular robots {{is one of}} the major challenges in their large-scale implementation in hardware. The infrared and hardwired communication systems usually employed are complex, are unreliable, and can lead to <b>unrecoverable</b> <b>errors.</b> Wireless mesh networks offer a robust and versatile solution but do not scale in terms of bandwidth and latency. By exploiting the unique properties of modular robots, we address the scalability problem in this paper. We propose a novel multi-radio, multi-channel mesh network architecture that guarantees fixed bandwidth and latency for inter-module communication, independent of the robot size. We present experimental results using modified ZigBee radios in a well-controlled setup that is free from unpredictable properties of radio propagation such as multi-path interference. By eliminating variables that would otherwise be present in a practical environment, we are able to precisely validate the principles and assumptions of our architecture. Using results from our experiments as a baseline we also qualitatively show that our approach has in-built protection from multi-path interference. ...|$|R
