49|10000|Public
2500|$|Analysis and <b>updating</b> <b>of</b> <b>data</b> {{contained}} in the Roads Database ...|$|E
50|$|Wastage {{of time in}} manual <b>updating</b> <b>of</b> <b>data</b> {{and taking}} print out is minimized.|$|E
5000|$|...The {{opening of}} a new section of road that is {{classified}} as a national road thereby replacing the old route*Re-alignments to existing National Roads*Changes to the classification of roads.*Analysis and <b>updating</b> <b>of</b> <b>data</b> contained in the Roads Database ...|$|E
5000|$|... exchanging {{versions}} or <b>updates</b> <b>of</b> <b>data</b> between servers (often {{known as}} anti-entropy); and ...|$|R
30|$|Strategy 1. If X is {{a series}} data with short limited time, {{then there is no}} need to log the <b>updates</b> <b>of</b> <b>data.</b>|$|R
50|$|The TransbaseCD {{database}} option can use {{read-only storage}} media such as CD, DVD or Blu-ray Discs. In addition, a persistent disk cache can be utilized to store data for performance improvement and/or for <b>updates</b> <b>of</b> <b>data</b> supplied originally as read-only media.|$|R
50|$|It {{allows for}} control and {{automation}} of all basic appliance-related {{services in a}} home: e.g., remote control of appliance operation, energy or load management, remote diagnostics and automatic maintenance support to appliances, downloading and <b>updating</b> <b>of</b> <b>data,</b> programs and services (possibly from the Internet).|$|E
50|$|These {{types of}} data {{structures}} are particularly common in logical and functional programming, and in a purely functional program all data is immutable, so all data structures are automatically fully persistent. Persistent data structures can also be created using in-place <b>updating</b> <b>of</b> <b>data</b> and these may, in general, use less time or storage space than their purely functional counterparts. Purely functional data structures are persistent data structures that completely avoid the use of mutable state, but can often still achieve attractive amortized time complexity bounds.|$|E
40|$|A {{simulation}} algorithm concerning problem B of 09 CUMCM is given. By {{introducing the}} concept of bed vector, clinic matrix, arrangement matrix and priority vector, the information about clinic, bed and arrangement will be stored and analyzed via MATLAB. By the algorithm the continuously <b>updating</b> <b>of</b> <b>data</b> has been simulated successfully...|$|E
40|$|Abstract. Marine {{environment}} {{data warehouse}} can store massive data. After {{the full amount}} <b>of</b> historical <b>data</b> has been initially loaded, the incremental update mode must be applied to ensure timely <b>updates</b> <b>of</b> <b>data.</b> In this paper, {{in view of the}} marine environment data warehouse’s characteristics, such as massive data amount, large number <b>of</b> historical <b>data</b> and low update frequency, a complete set of mechanisms for incremental <b>update</b> <b>of</b> marine environment <b>data</b> warehouse was proposed to greatly improve the operating efficiency <b>of</b> marine environment <b>data</b> warehouse. I...|$|R
30|$|It can be {{highlighted}} {{that all}} recommender systems are backboned on passive data warehouse. No consideration of real-time feature is involved. However, such a characteristic is fundamental in many domains {{such as social}} media where the speed of producing information is highly expedited and requires an instantaneous <b>update</b> <b>of</b> <b>data</b> warehouse.|$|R
40|$|This {{document}} {{presents an}} <b>update</b> <b>of</b> <b>data</b> available for Gulf <b>of</b> Cadiz hake. <b>Data</b> include: Landings, length distributions, catch-at-age and weight-at-age, abundance and effort trends and maturity information. Trawlers make {{the majority of}} catches in this fishery and catches have decreased since 2003. However, the mean length of the catch has increased {{in the same time}} period...|$|R
30|$|For {{analysis}} {{of the features of}} the region, such as the area, center of gravity, bounding box, or perimeter, some features should be collected when each pixel is scanned. When position of P(y,x) is judged, the data can be accumulated from the beginning to the tail of the run, and the accumulated data can be used for <b>updating</b> <b>of</b> <b>data</b> table.|$|E
30|$|However, {{in order}} to gain the best out of the {{standardisation}} and interoperability process, the majority of the countries should agree on certain legal aspects. In the case of LADM this would be common codelists for a cross-boundary approach. Other constrains need to be addressed as well, for example the maintenance and <b>updating</b> <b>of</b> <b>data,</b> as this makes a land administration system more reliable and secure.|$|E
40|$|Abstract — In this paper, {{we present}} a first step for {{designing}} a privacy leakage diagnosis and protection system using two privacy definitions and a new definition, and then evaluate a prototype program. The diagnosis is based on major notions of privacy: k-anonymity and (c, l) -diversity. Furthermore, the diagnosis include another method that analyze sensitivity of each attribute values. The prototype program realizes a computation time of less than 1 ms for the diagnosis and <b>updating</b> <b>of</b> <b>data.</b> Thus, it provides a privacy-leakage level within a feasible computation time...|$|E
5000|$|Deletion time - time {{required}} for the <b>update</b> <b>of</b> the <b>data</b> structure when an input element is deleted; ...|$|R
5000|$|Insertion time - time {{required}} for the <b>update</b> <b>of</b> the <b>data</b> structure when one more input element is added; ...|$|R
40|$|Motivated by {{frequent}} {{demand and}} capacity imbalances in the European airspace network, the SATURN project (‘Strategic Allocation of Traffic Using Redistribution in the Network’) is examining realistic {{ways to use}} market-based demand-management mechanisms to redistribute air traffic. Building upon the earlier data management report, this document provides an <b>update</b> <b>of</b> <b>data</b> management activities that run throughout the project...|$|R
40|$|In {{the area}} of {{documentation}} and registration of the Collections of the National Historical Museum there is a responsibility to update and monitor the financial position of objects forming the collections. In 2011 the area was doomed to making a survey of visu collections. The two main objectives were to update overall inventory information and regularize the situation of objects without patrimonial number dial. Together the collections survey was implemented the National Heritage Inventory. The SeNIP is a suite of applications that enables continuous <b>updating</b> <b>of</b> <b>data</b> heritage objects and the progressive inclusion of digital images. These two projects involved a challenge in {{the area of}} documentation as they are tools to collect, describe, classify and disseminate historical heritage content...|$|E
40|$|A {{regional}} observation {{system is}} a tool for municipal planning. With a base in Geographic Information System and through a database that system can identify problem areas in the municipalities and support the process of sustainable solutions. The database contains data about the infrastructure, the real situation of the land utilization plan, {{the use of the}} areas in the municipality and the buildings and their users. Together with several other programs it’s possible to interpret this data and make a good planning {{for the future of the}} city. Examples like the city Leipzig, Germany, demonstrate that these systems have application to the various departments within the municipalities, where there may be create new possibilities for use. One of the problems may be the cost of data collection, maintenance and <b>updating</b> <b>of</b> <b>data...</b>|$|E
40|$|International audienceUbiquitous Health Telemonitoring Systems collect {{low level}} data {{with the aim}} to {{ameliorate}} the health condition of patients. Models from data mining are created to compute indicators regarding their status and activity (habits, abnormalities). Models can also help generate feedbacks and recommendations for patients {{as well as for}} remote formal and informal care givers. Essential features are that the models can be easily updated whenever new information is available and that data generated from the models can be readily accessible as well as sensed data. This paper addresses the challenge of conveniently incorporating in a Ubiquitous Health Telemonitoring System the creation, the use, and the <b>updating</b> <b>of</b> <b>data</b> mining models. We conducted first runs and generated results showing the feasibility as well as the effectiveness of the system...|$|E
50|$|An {{alternative}} approach to event driven architectures {{is to increase}} the refresh cycle <b>of</b> an existing <b>data</b> warehouse to update the data more frequently. These real-time data warehouse systems can achieve near real-time <b>update</b> <b>of</b> <b>data,</b> where the data latency typically is in the range from minutes to hours. The analysis <b>of</b> the <b>data</b> is still usually manual, so the total latency is significantly different from event driven architectural approaches.|$|R
40|$|Between 1999 and 2002 an {{up-to-date}} {{checklist of}} the Italian Vascular Flora was accomplished following a convention {{funded by the}} Dipartimento di Biologia Vegetale, University "La Sapienza" of Rome, and the Ministero dell'Ambiente e della Tutela del Territorio, Direzione Conservazione della Natura. The first paper edition of the checklist is currently in print and required a further <b>update</b> <b>of</b> <b>data,</b> which ended in 2004...|$|R
50|$|Switch based devices, as {{the name}} suggests, reside in the {{physical}} switch hardware used to connect the SAN devices. These also sit between the hosts and storage but may use different techniques to provide the metadata mapping, such as packet cracking to snoop on incoming I/O requests and perform the I/O redirection. It {{is much more difficult}} to ensure atomic <b>updates</b> <b>of</b> metadata in a switched environment and services requiring fast <b>updates</b> <b>of</b> <b>data</b> and metadata may be limited in switched implementations.|$|R
40|$|Abstract: This paper {{discusses}} some of {{the linguistic}} problems encountered during {{the development of the}} User Specialty Languages (USL) system, an information system that accepts a subset of German or English as input for query, analysis, and <b>updating</b> <b>of</b> <b>data.</b> The system is regarded as a model for portions of natural language that are relevant to interactions with a data base. The model provides insight into the functioning of language and the linguistic behavior of users who must communicate with a machine in order to obtain information. The aim of application independence made it necessary to approach many problems from a different angle than in most comparable systems. Rather than a full treatment of the linguistic capacity of the system, details of phenomena such as time handling, coordination, quantification, and possessive pronouns are presented. The solutions that have been implemented are described, and open questions are pointed out...|$|E
40|$|Abstract. Requirements on {{embedded}} systems have {{increased over the}} years leading to an increased complexity of software and, consequently, higher development and maintenance costs. One {{major part of the}} cost is the way data is engineered and maintained in the system so that resources, such as CPU, are used efficiently. There exist various ways of optimizing CPU usage by controlling how often calculations on data are performed. These include (i) algorithms that ensure <b>updating</b> <b>of</b> <b>data</b> ondemand, i. e., when data becomes outdated, and/or (ii) appropriate eventcondition-action rules implementing active behavior in the system and, thereby ensuring that calculations, specified by actions, are carried out when an event satisfying the condition occurs. Adding active behavior ensures that time-triggered {{embedded systems}} can also respond to events that occur aperiodically. One way of adding on-demand and active behavior to the system is by re-engineering the existing software. This is a time-consuming an...|$|E
40|$|As {{large-scale}} {{digital libraries}} become more available and complete, {{not to mention}} more numerous, it is clear {{there is a need}} for services that can draw together and perform inference calculations on the metadata produced. However, the traditional Relational Database Management System (RDBMS) model, while efficiently constructed and optimised for many business structures, does not necessarily cope well with issues of concurrent data updates and retrieval at the scale of hundreds of thousands of papers. At the same time the growth of RDF and the increasing interest in Semantic Web technologies perhaps begins to present a viable alternative at a scalable, practical level. This paper considers a specific application of large-scale metadata analysis and conducts scalability tests using real-world data. It concludes that RDF technologies are both a scalable and performance-realistic alternative to traditional RDBMS approaches. It also shows that for relationship-based queries on large-scale metadata stores, RDF technologies can significantly out-perform traditional RDBMS approaches by allowing both retrieval and <b>updating</b> <b>of</b> <b>data</b> in a timely manner...|$|E
50|$|Formation of an adjusted, {{identified}} overall air situation {{based on}} the information on the local air situation of the different radar stations. Continuous <b>updating</b> <b>of</b> the <b>data.</b>|$|R
5000|$|Situational {{awareness}} systems rely on periodic <b>updates</b> <b>of</b> positional <b>data</b> to help users locate friendly forces, {{as long as}} {{the responses}} are timely and not masked by terrain ...|$|R
30|$|Reading and <b>updating</b> <b>of</b> {{remotely}} hosted <b>data</b> entities {{associated with}} a trade item.|$|R
40|$|Updating XML data is {{very wide}} area, which must solve {{a number of}} {{difficult}} problems. From designing language with sufficient expressive power to the XML data repository able to apply the changes. Ways {{to deal with them}} are few. From this perspective, is this work very closely dedicated only to the language XQuery. Thus, its extension for updates, for which the candidate recommendation by the W 3 C were published only recently. Another specialization of this work is to focus only on the XML data stored in the object­relational database with that repository will enforce the validity of documents to the scheme described in XML Schema. This requirement, combined with the possibility of <b>updating</b> <b>of</b> <b>data</b> in the repository is on the contradictory requirements. In this thesis is designed language based on XQuery language, designed and implemented evaluating of the update queries of the language on the store and a description and implementation of the store in object­relational database...|$|E
40|$|Abstract. As {{large-scale}} {{digital libraries}} become more available and complete, {{not to mention}} more numerous, it is clear {{there is a need}} for services that can draw together and perform inference calculations on the metadata produced. However, the traditional Relational Database Management System (RDBMS) model, while efficiently constructed and optimised for many business structures, does not necessarily cope well with issues of concurrent data updates and retrieval at the scale of hundreds of thousands of papers. At the same time the growth of RDF and the increasing interest in Semantic Web technologies perhaps begins to present a viable alternative at a scalable, practical level. This paper considers a specific application of large-scale metadata analysis and conducts scalability tests using real-world data. It concludes that RDF technologies are both a scalable and performance-realistic alternative to traditional RDBMS approaches. It also shows that for relationship-based queries on large-scale metadata stores, RDF technologies can significantly out-perform traditional RDBMS approaches by allowing both retrieval and <b>updating</b> <b>of</b> <b>data</b> in a timely manner. ...|$|E
40|$|High-temperature {{environmental}} attack often {{limits the}} useful service {{life of the}} hot section components in gas turbines, for aircraft, marine and industrial applications. High-temperature coatings are mandatory to obtain acceptable service life, but the life of these coatings often determines the refurbishment intervals. This paper addresses the use of computerized data bases and expert systems for high-temperature corrosion and high-temperature coatings, which {{have not always been}} useful for other material problems. The principal problems with material data bases are assessing the quality of the data; describing the materials, test conditions and test results; as well as the more common problems of data retrieval, correction of errors, and <b>updating</b> <b>of</b> <b>data.</b> Expert systems have generally not achieved expert performance, but are useful for people having less expertise than the program. Several expert systems are discussed including one that is currently being used by electric utilities to select high-temperature coatings for industrial gas turbine blades. Finally, in developing data bases and expert systems, the rapid prototyping process is recommended, along with a careful consideration of the end users, the available data and the available expertise...|$|E
40|$|Flow Java {{integrates}} {{single assignment}} variables (logic variables) into Java. This paper presents and compares three implementation strategies for single assignment variables in Flow Java. One strategy uses forwarding and dereferencing {{while the two}} others are variants of Taylor's scheme. The paper introduces how to adapt Taylor's scheme for a concurrent language based on operating system threads, token equality, and <b>update</b> <b>of</b> <b>data</b> structures. Evaluation <b>of</b> the strategies clarifies that the key issue for e#ciency is reducing memory usage...|$|R
50|$|A major <b>update</b> <b>of</b> the <b>data</b> in BRENDA is {{performed}} twice a year. Besides the upgrade of its content, improvements {{of the user}} interface are also incorporated into the BRENDA database.|$|R
50|$|Geographic {{information}} {{is an essential}} component of land administration through the collection and <b>updating</b> <b>of</b> cadastral <b>data</b> and through the development of information systems that allow the management of land activity.|$|R
