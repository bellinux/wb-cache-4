9|14|Public
40|$|Earlier work {{demonstrated}} that quantum-mechanical machinery {{can be used}} to transmit classical bits without a minimal <b>unavoidable</b> <b>energy</b> cost per bit. It is shown that a very minor variation of the earlier proposal also works for qubits. In addition, some casual remarks about error control in quantum parallelism are provided...|$|E
30|$|When {{including}} {{oxy-fuel combustion}} into the SAGD configuration, the electricity {{demanded by the}} ASU and CPU leads to an increase of GT output of 46.5  %. This enlargement enables the HRSG to generate higher steam rate and thus, save fuel in the natural gas boiler. The cost in terms of fuel is only 9  % higher than the reference case. This also leads {{to an increase in}} the ratio between the power generation and the fuel input. This parameter must be carefully considered, not forgetting that the higher fuel supply leads to an <b>unavoidable</b> <b>energy</b> penalty compared with the reference case.|$|E
40|$|Most animals {{must travel}} to find food, {{incurring}} an <b>unavoidable</b> <b>energy</b> and time cost. Economic theory predicts, and experimental work confirms, that within species, increasing the distance traveled {{each day to}} find food has negative fitness consequences, decreasing {{the amount of energy}} invested in maintenance, repair, and reproduction. Here, we show that this relationship between daily distance traveled and reproductive success is fundamentally different between species and over evolutionary time in many lineages. Phylogenetically controlled analyses of 161 eutherian mammals indicate that, after controlling for body mass, evolutionary increases in the daily distance traveled are associated with corresponding increases in both total fertility (number of offspring per lifetime) and total offspring mass (grams of offspring per lifetime). This suggests that over evolutionary time, increasing travel distance is often part of a strategy for procuring more food energy and not necessarily a response to decreased food availability. These results have important implications for ecological comparisons among species, including assessments of habitat quality based on locomotor behavior...|$|E
40|$|AbstractThe {{evaluation}} of energy efficiency in water supply systems should account for both actual energy consumed and how efficiently such energy is spent. This work proposes the new concept of <b>Unavoidable</b> Minimum <b>Energy,</b> as the reference for defining an energy efficiency indicator. The {{aim is to}} search for possible optimal network configurations that minimize energy consumption and maximize the energy efficiency, acting on the main structural parameters of the system (pipe diameters, leakage rate) and considering the pump efficiency as well. The optimization process is carried out by coupling the heuristic algorithm GHEST with the EPANET solver and applied to a literature synthetic case study...|$|R
40|$|Summary A {{contact between}} two rough {{surfaces}} subjected to normal and shear loads is an <b>unavoidable</b> source of <b>energy</b> dissipation. It {{has been observed}} in experimental studies at Sandia National Laboratories {{that the loss of}} energy undergoes certain evolution when contact is subjected to cyclic loading. In the present work a simple model is developed that suggests a hypothesis that two rough surfaces brought into contact tend to lock up with a number of cycles resulting in a reduction of a total energy loss...|$|R
40|$|Density {{functional}} {{theory has}} been used to probe the structures and solution dynamics of a series of polyaromatic hydrocarbon complexes of the 12 -electron fragment {Rh(PiBu(3)) (2) }(+). These studies suggest that the strength of the binding of the metal to the hydrocarbon surface is controlled by the electronic demands of both the metal (16 -electron configuration) and ligand (maximum retention of aromaticity). In cases where these two factors can be satisfied simultaneously an energetically isolated equilibrium structure emerges and haptotropic shifts are blocked. In cases where a compromise between the requirements of the metal and ligand is <b>unavoidable</b> the potential <b>energy</b> surface is rather flatter, leading to rapid haptotropic shifts between near iso-energetic isomers...|$|R
40|$|Wireless sensor nodes {{capable of}} {{acquiring}} and transmitting biosignals are increasingly important to address future needs in healthcare monitoring. One {{of the main}} issues in designing these systems is the <b>unavoidable</b> <b>energy</b> constraint due to the limited battery lifetime, which strictly limits the amount of data that may be transmitted. Compressed Sensing (CS) is an emerging technique for introducing low-power, real-time compression of the acquired signals before transmission. The recently developed rakeness approach is capable of further increasing CS performance. In this paper we apply the rakeness-CS technique to enhance compression capabilities for electroencephalographic (EEG) signals, and particularly for Evoked Potentials (EP), which are recordings of the neural activity evoked by the presentation of a stimulus. Simulation results demonstrate that EPs are correctly reconstructed using rakeness-CS with a compression factor of 16. Additionally, some interesting denoising capabilities are identified: the high-frequency noise components are rejected and the 60 Hz power line noise is decreased by more than 20 dB {{with respect to the}} state-of-the-art filtering when rakeness-CS techniques are applied to the EEG data stream...|$|E
40|$|Nanoscale {{localization}} {{of electromagnetic}} fields near metallic nanostructures underpins the fundamentals and applications of plasmonics. The <b>unavoidable</b> <b>energy</b> loss from plasmon decay, initially {{seen as a}} detriment, has now expanded the scope of plasmonic applications to exploit the generated hot carriers. However, quantitative understanding of the spatial localization of these hot carriers, akin to electromagnetic near - field maps, has been elusive. Here we spatially map hot - electron - driven reduction chemis try with 15 nanometre resolution {{as a function of}} time and electromagnetic field polarization for different plasmonic nanostructures. We combine experiments employing a six - electron photo - recycling process that modify the terminal group of a self - assembled monolayer on plasmonic silver nanoantennas, with theoretical predictions from first - principles calculations of non - equilibrium hot - carrier transport in these systems. The resulting localization of reactive regions, determined by hot carrier transport from high - field regions, paves the way for improving efficiency in hot - carrier extraction science and nanoscale regio - selective surface chemistry...|$|E
40|$|AbstractTo {{employee}} clustering algorithms in multi-hop data forwarding mechanism, Hot-spot {{problem will}} cause unbalanced energy dissipation among the cluster {{heads in the}} network. Unequal clustering technique promotes even energy dissipation only in inter-cluster communications not in intra-cluster communication. An Energy-efficient Clustering Algorithm (EECA) is introduced to avoid these problems in edge-based wireless sensor networks. The main aim of the presented algorithm is to avoid hot-spot problem by balancing uniform energy utilization among networked cluster heads. EECA constructs uneven size clusters in different levels to enable uniform energy expenditure among cluster heads. Data delivery {{is one of the}} important and <b>unavoidable</b> <b>energy</b> consuming operation in any sensor networks. To balance energy consumption load among data transmission routes, a multi-hop data forwarding protocol is introduced. Here, source node selects a relaying node who has minimum hop count to base station with more energy reserves and relayed less number of packets. Extensive experimental results prove that the presented algorithm overcome the congestion problem in the network by uniform distribution of energy consumption and enhances network's lifetime...|$|E
40|$|Recently, the {{increasing}} energy demand has caused dramatic consumption {{of fossil fuels}} and <b>unavoidable</b> raising <b>energy</b> prices. Moreover, environmental effect of fossil fuel led to the need of using renewable energy (RE) to meet the rising energy demand. Unpredictability and {{the high cost of}} the renewable energy technologies are the main challenges of renewable energy usage. In this context, the integration of renewable energy sources to meet the energy demand of a given area is a promising scenario to overcome the RE challenges. In this study, a novel approach is proposed for optimal design of hybrid renewable energy systems (HRES) including various generators and storage devices. The ?-constraint method has been applied to minimize simultaneously the total cost of the system, unmet load, and fuel emission. A particle swarm optimization (PSO) -simulation based approach has been used to tackle the multi-objective optimization problem. The proposed approach has been tested on a case study of an HRES system that includes wind turbine, photovoltaic (PV) panels, diesel generator, batteries, fuel cell (FC), electrolyzer and hydrogen tank. Finally, a sensitivity analysis study is performed to study the sensibility of different parameters to the developed model. Scopu...|$|R
40|$|The {{resource}} {{quality and}} the temporal generation pattern of variable renewable energy sources vary significantly across Europe. In this paper spatial distributions of renewable assets are explored which exploit this heterogeneity to lower the total system costs for {{a high level of}} renewable electricity in Europe. Several intuitive heuristic algorithms, optimal portfolio theory and a local search algorithm are used to find optimal distributions of renewable generation capacities that minimise the total costs of backup, transmission and renewable capacity simultaneously. Using current cost projections, an optimal heterogeneous distribution favours onshore wind, particularly in countries bordering the North Sea, which results in average electricity costs that are up to 11 % lower than for a homogeneous reference distribution of renewables proportional to each country's mean load. The reduction becomes even larger, namely 18 %, once the transmission capacities are put to zero in the homogeneous reference distribution. Heuristic algorithms to distribute renewable capacity based on each country's wind and solar capacity factors are shown to provide a satisfactory approximation to fully optimised renewable distributions, while maintaining the benefits of transparency and comprehensibility. The sensitivities of the results to changing costs of solar generation and gas supply {{as well as to the}} possible cross-sectoral usage of <b>unavoidable</b> curtailment <b>energy</b> are also examined. Comment: 16 pages, 14 figure...|$|R
40|$|M. A. University of Hawaii at Manoa 2011. Includes bibliographical references. The {{current study}} {{explored}} how individuals perceive bias in various authorities making arguments for {{one side of}} a controversial topic (in this case, anthropogenic global warming [AGW]). Recent theorizing on the nature of belief suggests that <b>unavoidable</b> time and <b>energy</b> constraints demand that individuals resolve their predicaments economically by relying upon authorities to provide useful information on topics of interest (Hardin, 2009). Emotional indicators of belief may to facilitate the decision-making process by highlighting the plausibility of information in light of past experiences (Leicester, 2008). The present study explored perceptions of bias in authorities based upon three factors: a.) credentials of the presenting authority, b.) a prime casting climate change as either a scientific or moral issue, and c.) agreement of the authority's argument with participants' prior climate change beliefs. Analysis revealed a significant interaction between argument agreement and authority, suggesting that individuals selectively consider an authority's credentials when perceiving bias, depending on the authority's agreement with one's prior beliefs. Participants were unable to recall many, if any, resources by which they had learned about AGW. As well, higher levels of belief certainty and moral engagement in AGW were associated with more lenient treatment of agreeing authorities and harsher treatment of disagreeing authorities. These findings suggest that individuals may dismiss inconvenient scientific opinions when a more agreeable argument is presented, even when such an argument is presented by one with few valid credentials...|$|R
30|$|The {{continuous}} physical {{stress in}} ultra-endurance performances {{leads to an}} <b>unavoidable</b> <b>energy</b> deficit (Knechtle 2013). In most ultra-endurance exercises, energy intake is lower than energy expenditure (Bescós et al. 2012 [a], [b]; Bircher et al. 2006; Enqvist et al. 2010; Francescato et al. 2002; Linderman et al. 2004). Ultra-endurance sports such as 24 -hour bike races require large caloric intakes in order to match the expenditures required to complete these prolonged events (Bescós et al. 2012 [a], [b]; Bircher et al. 2006; Francescato et al. 2002; Linderman et al. 2004; White et al. 1984). The type, timing and amount of foods ingested {{as well as the}} co-ingestion of ergogenic aids such as caffeine are important factors directly linked to sport performance in endurance events (Eberle 2007; Jeukendrup 2014; Laursen et al. 2001; Maughan et al. 2011; Ormsbee et al. 2014; Peters 2003). At present, comprehensive dietary macronutrient recommendations for ultra-endurance and adventure races are missing. As a result, race recommendations for carbohydrate (Burke 2002; Burke et al. 2011; Jeukendrup 2004, 2011, 2014; Lambert et al. 2003; Millard-Stafford et al. 2008; Peters 2003), protein (Ormsbee et al. 2014; Ranchordas 2012) and fat (Ormsbee et al. 2014; Lambert 2003) intake have been extrapolated from studies on traditional endurance events.|$|E
40|$|M. Ing. Southern Africa {{faces an}} {{increasingly}} serious shortage of potable water {{and will continue}} to do so for the foreseeable future. Desalination is a process whereby dissolved solids are removed out of a contaminated water source to produce potable water. However, desalination carries an <b>unavoidable</b> <b>energy</b> cost per unit of potable water produced. Minimising this energy cost is an important goal towards making desalination a practical technology for widespread use. A desalination process is proposed by the author which is based on the injection of a brine spray into vacuum conditions. Although a complete desalination process is proposed, only the core components of the desalination process are investigated in the present study. The physical processes taking place in the core components are complex. Computational fluid dynamics is the numerical tool used to investigate the processes taking place in the core components. A commercial computational fluid dynamics code, augmented with user-programming, provides a simulation model for the core components. Due to the complexity of the investigated desalination process not all of its physical aspects are accounted for in the simulation. An analytical as opposed to experimental verification of the simulation is performed. The simulation model is used to perform a number of parametric tests. These tests are used to numerically investigate the effects of a number of process variables on the core components. The results of these parametric tests are presented and discusse...|$|E
40|$|From several PV-demonstration programmes, real {{performance}} {{data and the}} range in utilizing the systems are obtained. Nevertheless, for a particular system, no precise assessment and interpretation of the real system behaviour can be given: The quantification and {{the separation of the}} avoidable and of the <b>unavoidable</b> <b>energy</b> losses in the system is not to perform from the monitored data alone. Aim of this project was the interpretation of real operation data of PV-systems, the calculation of all steps of energy conversion and hence the determination of the energy losses in particular PV-systems as well as the presentation of a measure for energy production and system performance. This task includes a brief survey of the optimization potential of the energetic performance. The method choosen was an energy flow analysis, carried out on selected demonstration plants of the MuD-programme. For this reason, detailed simulation models were used. The results of the simulation calculations gives the measure to interprete the monitored system performance. In the course of the project, the analytical evaluation has shown a large ability in the assessment and interpretation of real system perforamances. Since the concept of the evaluation is transferable to many PV-systems, it represents a base for further applications in this field and in the field of the online-system control of PV-plants. (orig.) In verschiedenen Demonstrationsprogrammen werden aus Langzeitmessungen Groessenordnung und Spannbreite der Nutzungsgrade photovoltaischer Anlagen ermittelt. Fuer eine spezifische Anlage ist damit jedoch keine praezise Einschaetzung bzw. Interpretation des Systemverhaltens moeglich, da die Quantifizierung der unvermeidbaren und der vermeidbaren Energieverluste im System nicht aus den Messdaten allein erfolgen kann. Ziel dieses Projektes war die Interpretation des reellen Betriebs von PV-Anlagen, die Berechnung der Zusammensetzung und der Bandbreite der Energieverluste im spezifischen PV-System sowie die Bereitstellung eines Massstabes (Sollwerte) fuer die Energieproduktion und die Anlagenausnutzung. Damit ist auch die Untersuchung einer moeglichen Optimierung der energetischen Ausbeute verknuepft. Arbeitsmethode war die Energieflussanalyse, die an ausgewaehlten Demonstrationsanlagen des MuD-Programms durchgefuehrt wurde. Die Analyse erfolgte durch Verwendung detaillierter Simulationsmodelle. Die Ergebnisse der Simulationsrechnungen dienten gleichzeitig als Massstab zur Beurteilung des gemessenen Betriebsverhaltens. Die entwickelte analytische Auswertung fuehrte zu einer hohen Interpretierbarkeit des reellen Anlagenverhaltens und stellt damit wegen der Uebertragbarkeit des Konzeptes die Basis fuer weitere Anlagenauswertungen und fuer den Einsatz in die Online-Kontrolle von PV-Anlagen dar. (orig.) Available from TIB Hannover: F 96 B 605 +a / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekSIGLEBundesministerium fuer Bildung, Wissenschaft, Forschung und Technologie, Bonn (Germany) DEGerman...|$|E
40|$|In {{the highly}} {{structured}} solar corona, resonant absorption is an <b>unavoidable</b> mechanism of <b>energy</b> transfer from global transverse MHD waves to local azimuthal Alfvén waves. Due to its localised nature, a direct detection of this mechanism is extremely difficult. Yet, {{it is the}} leading theory explaining the observed fast damping of the global transverse waves. However, at odds with this theoretical prediction, recent observations indicate that in the low amplitude regime such transverse MHD waves can also appear decay-less, a yet unsolved phenomenon. Recent numerical work has shown that Kelvin-Helmholtz instabilities (KHI) often accompany transverse MHD waves. In this work, we combine 3 D MHD simulations and forward modelling to show that for currently achieved spatial resolution and observed small amplitudes, an apparent decay-less oscillation is obtained. This effect results from the combination of periodic brightenings produced by the KHI and the coherent motion of the KHI vortices amplified by resonant absorption. Such effect is especially clear in emission lines forming at temperatures that capture the boundary dynamics rather than the core, and reflects the low damping character of the local azimuthal Alfvén waves resonantly coupled to the kink mode. Due to phase mixing, the detected period can {{vary depending on the}} emission line, with those sensitive to the boundary having shorter periods than those sensitive to the loop core. This allows to estimate the density contrast at the boundary. Comment: 8 pages, 4 figures; Accepted for publication in The Astrophysical Journal Letter...|$|R
40|$|Earth's {{most recent}} major {{extinction}} episode, the Quaternary Megafauna Extinction, claimed two-thirds of mammal genera and one-half {{of species that}} weighed > 44 kg between ≈ 50, 000 and 3, 000 years ago. Estimates of megafauna biomass (including humans as a megafauna species) for before, during, and after the extinction episode suggest that growth of human biomass largely matched the loss of non-human megafauna biomass until ≈ 12, 000 years ago. Then, total megafauna biomass crashed, because many non-human megafauna species suddenly disappeared, whereas human biomass continued to rise. After the crash, the global ecosystem gradually recovered into a new state where megafauna biomass was concentrated around one species, humans, instead of being distributed across many species. Precrash biomass levels were finally reached just before the Industrial Revolution began, then skyrocketed above the precrash baseline as humans augmented the energy available to the global ecosystem by mining fossil fuels. Implications include (i) an increase in human biomass (with attendant hunting and other impacts) intersected with climate change to cause the Quaternary Megafauna Extinction and an ecological threshold event, after which humans became dominant in the global ecosystem; (ii) with continued growth of human biomass and today's unprecedented global warming, only extraordinary and stepped-up conservation efforts will prevent {{a new round of}} extinctions in most body-size and taxonomic spectra; and (iii) a near-future biomass crash that will unfavorably impact humans and their domesticates and other species is <b>unavoidable</b> unless alternative <b>energy</b> sources are developed to replace dwindling supplies of fossil fuels...|$|R
40|$|The 20 th century saw an {{enormous}} worldwide {{growth of the}} housing stock. In particular the building boom after WW-II, during which the housing stock in most countries was multifolded, focussed {{the attention of the}} housing sector primarily to the planning and realisation of new construction; the consciousness of the enormous maintenance and management task to come was still a far cry. The begin of the 21 st century shows a completely different situation that urges for a paradigm shift. New construction in most western countries has faded down below an annual production of 1 % of the existing stock, and often well below. Parallel to this, the ageing existing stock draws growing attention. The investments in major repairs, renovation, adaptation and redevelopment count at present for a total turn-over well beyond that of new construction. The awareness grows that life cycle extension of the existing stock is <b>unavoidable.</b> Improving the <b>energy</b> efficiency to the required standards of tomorrow will give this awareness a strong extra boost. Though the change from new addition to the adaptation and transformation of the existing stock is well under way, the consequences of this paradigm shift seem still to be largely neglected. Large parts of the construction and real estate trade seem to stick to business as usual: new constriction, if not on greenfields then in brownfields. The knowledge about how and when to successfully maintain, manage, adapt, transform and redesign has still a way to go. The paper illustrates the paradigm shift in Western Europe, explores the consequences for the management of the housing stock and concludes with recommendations for housing and building policies. OTB Research Institute for the Built Environmen...|$|R
40|$|Beside {{the usage}} of {{regenerative}} energy sources {{it is necessary to}} increase the efficiency of fossil fuel power plants to preserve these available fossil resources. As they were thermal engines, the improvement of power plants causes higher process temperatures. To protect the materials being used, like reactor walls or catalysts, at high temperatures, an improvement of gas cleaning is required. The GuD combined process using gas- and steam turbine is achieving 60 % of degree of efficiency. IGCC Plant e. g. this combined process is applied with a subsequent gasification of the fuel. To minimize the <b>unavoidable</b> losses of <b>energy</b> during the gas cleaning, it is necessary to maximize the cleaning temperature. However, soption-based removal of heavy metals at high temperatures is widely less investigated than alkali or sulphur removal by sorption. The intention {{of this study was to}} determine the capability of available sulphur and alkali sorption materials for the removal of heavy metals. The examined heavy metals in this work were arsenic, cadmium, selenium, lead and zinc. The used sorbent materials were part from the groups of silicates, alumosilicates, zeolites, lime, fly ashes and some synthetic materials like titania or CaSiO 3. Initially the release and the sorption behaviour were calculated with thermodynamic software. Therefore coals with different ranks and different heavy metal contents were assumed. These coals were gasified both in a fluidised bed gasifier and in an entrained flow gasifier. The results of these calculations were compared with the results of laboratory experiments at a molecular beam mass spectrometer (MBMS) to determine sorption kinetics. With a MBMS it is possible to measure the gas purity downstream the sorbent bed without condensation of the impurities. The sorption mechanisms were then investigated by XRD and SEM in additional exposure tests...|$|R
40|$|Development of {{efficient}} light-harvesting technologies {{hinges on}} {{our understanding of}} the fundamental physics of light-harvesting in both natural and artificial systems. This work addresses the following topics, i.) the mechanism underlying the remarkably efficient electronic energy transfer in natural light harvesting antennas, ii.) a femtosecond time-resolved photonumeric technique to quantitatively characterize transient chemical species. A non-adiabatic model for photosynthetic energy transfer in light harvesting antennas is proposed. Light harvesting antennas use a set of closely spaced pigment molecules held in a controlled relative geometry by a protein. It is shown that in the Fenna-Matthews-Olson (FMO) antenna protein, the antenna found in green sulfur bacteria, the excited state electronic energy gaps are resonant with a quantum of vibrational energy on its pigment, bacteriochlorophyll a. Through a dimer model loosely based on FMO, it is shown that such a resonance leads to an <b>unavoidable</b> nested non-adiabatic <b>energy</b> funnel on the excited states of photosynthetic antennas. The non-adiabatic model presented here leads to enhanced vibrational oscillations on the ground electronic state of these antennas, the 2 D spectroscopic signatures and oscillation frequencies of which are consistent with all the reported 2 D signatures of long-lived oscillations, including the ones that are not explained by prior models of excited state electronic energy transfer. Extensions that account for both resonant and near-resonant pigment vibrations suggest that photosynthetic energy transfer presents a novel design in which electronic energy transfer proceeds non-adiabatically through clusters of vibrations with frequencies distributed around electronic energy gaps. The latter part of the thesis presents absolute measurements of femtosecond pump-probe signal strength. The experiments demonstrate quantitative time-resolved measurement of absolute number of excited state molecules. Based on these measurements, an all-optical technique that simultaneously determines concentration and extinction coefficient of an unknown sample is presented. Unlike prior such analytical techniques, the present photonumeric method does not require any sample isolation, physical handling or in situ calibrant. In principle, the experimental and theoretical framework developed allows extensions towards characterization of transient chemical species...|$|R
40|$|Electricity sector {{decarbonisation}} {{is widely}} seen as a fundamental step in the global fight against climate change. The need to secure this transition {{is compounded by the}} prospective use of electrification to deliver economy-wide carbon reductions, especially in harder to address sectors like heat and transport. No agreement has yet been reached on the best decarbonisation approach. Empirical evidence is required to guide a transition that not only succeeds in delivering a ‘truly’ low carbon electricity source, but also prevents wider environmental issues being exacerbated. This research portfolio examines the low carbon transition of electricity systems in a UK context. The energy and environmental implications in response to different decarbonisation approaches were evaluated using Life Cycle Assessment (LCA) and related methods. Potential UK low carbon electricity systems were investigated via three socio-technical energy scenarios, known as the Transition Pathways. Key factors were identified, which may impact the future environmental performance of UK electricity, such as supply chain dynamics, policy shifts, and new entrant technologies, were investigated to assess their consequences on decarbonisation targets. This research exemplifies the guiding principles of LCA as a valuable proactive tool in shaping superior future decarbonisation and wider environmental policies. A key finding of this thesis was the importance of whole life cycle accounting of power sector GHG emissions, including upstream impacts which are often overlooked by governmental bodies. Hence, current decarbonisation policies may lead to a shift in practices and the adoption of production routes with unintended negative effects upstream. In this work, the upstream gas emissions for future supplies increase significantly (rising 2. 7 to 3. 4 times current mix per MJ supplied) and are foreseen to be highly influential on the future electricity systems analysed. Increased influx of biomethane leads to a substantial reduction in direct fossil emissions (up to 10. 6 million tonnes of CO 2 eq), and is found to be critical in offsetting rising upstream emissions. The roll-out of carbon capture and storage was also found to be instrumental in the success of the pathways. The electricity system transitions assessed achieved differing, yet significant, levels of decarbonisation (between 75 - 85 % reductions on 1990 levels on a lifecycle basis). Nevertheless, these were often achieved at the expense of wider environmental impacts, suggesting trade-offs were <b>unavoidable.</b> The civic-led <b>energy</b> transition resulted in the greatest associated environmental benefits, realising the greatest reduction in 13 of 18 environmental categories assessed, compared to the 2008 levels. It was also the only pathway to decouple electricity supply from fossil fuel use. Reliance on metal resources was seen to steadily increase in response to a developing renewable energy sector, rising 23 - 75 % from the 2008 baseline system. The presented results, models and data are transparently presented for others in the field to build upon, and scrutinise their implications for wider decarbonisation strategies within and outside of the electricity sector...|$|R
40|$|The {{goal of the}} SThG® {{project is}} to recycle, to store and to manage <b>unavoidable</b> and {{renewable}} <b>energies</b> in the residential building. The aim of the work performed is to fill the lack of research on global sizing of systems, which is a barrier {{to the evolution of}} the energy balance of the building. Indeed, available tools are not adapted to make optimized choices and to size systems as they are very time-consuming. That is why an alternative model has been found. This work can be divided into two main steps. The first one, consists in doing a simplified model of a building and its systems. The second one in applying optimization to have the best choice and the better sizing of those systems according to energy and economic criterions. To solve this problem a model has been developed and two blocks can be identified. The first block is composed of two main points: the establishment of a physical model and the physical optimization. First, a macroscopic model of a building and its systems (recovery, storage and generation) has been created. This model is as much as generic as possible in order to represent a huge part of systems. But, it is also precise to ensure a good description of the real way they work and to allow the link to systems after the physical optimization during block two. Secondly, a Global Pattern Search optimization is used to find the best physical solution according to the constraints of the situation. The second block permits to find solutions in a discrete data basis of systems. These two blocks allow a durability of the tool, indeed the mechanism of optimization depends on the physical model. It will be possible to enrich the data basis of the second block without reconsidering the first one. Thanks to this work, a consistent choice and sizing of systems can be found. Le projet Smart Thermogène Grid® s’intègre dans la dynamique actuelle d’économie d’énergie en s’intéressant à la mise en œuvre de solutions de récupération et de stockage des énergies fatales et renouvelables dans le bâtiment résidentiel. L’objectif de ce travail de thèse est de proposer une approche globale permettant le choix et le dimensionnement de ces systèmes. Ce manque freine leur intégration au sein du bâtiment ainsi que l’évolution du bilan énergétique. Dans ce contexte, une modélisation de type « boites grises » qui se base sur un assemblage de modules (récupérations sur air extrait, sur eaux usées et solaire, stockages géothermique et en cuve) a été développée. Ce type de démarche permet une simplification importante du modèle complet et une réduction considérable du nombre de paramètres à déterminer. Il devient alors possible de mettre en place une méthode d’optimisation sous contrainte de type GPS afin d’aboutir à un dimensionnement physique. Une solution discrète est ensuite proposée grâce à une recherche dans des bases de données. Il est ainsi possible de répondre à la problématique et de proposer un dimensionnement cohérent du SThG®...|$|R
40|$|This {{dissertation}} collects three {{essays on}} the economics of global warming and climate policies. The papers, each of whom {{can be read as}} a stand‑alone essay, are arranged in a way that goes from a more radical, more general approach to a more pragmatic, more specific one. The first essay deals with the very essence of global warming: is it a global public bad? Does its nature justify, or even require, international collective action in order to let the external costs be internalized? In order to provide an answer, a Coasian approach is undertaken. The starting point is an original interpretation of the socalled Coase Theorem, which is derived from Forte (2007). Forte argues that the symmetry underlying the theorem can only be held true in the short run. In the long run, however, the symmetry ceases to exist because, among the other reasons, a different return on invested capital may emerge under a different initial allocation of rights. It follows that the initial allocation of rights does matter, even in a transactions costs‑free world. An intuitive consequence of this, is that the long‑term consequences of the initial allocation should be considered. In the case of pollution, this may mean that it is not always efficient to follow the Polluter Pays Principle. In fact, while pollution is a negative externality, the venture that causes it may also generate positive externalities. When this is the case, by imposing to the polluter the cost of getting rid of pollution, the positive externality may be lost together with the negative one. The present paper argues that this is precisely the case of global warming. Greenhouse gases emissions – which are suspected of causing man‑made global warming, as opposed to natural climate change – are, at the present state of technology, an <b>unavoidable</b> byproduct of <b>energy</b> production and economic growth. Cutting emissions – as is requested by many stakeholders as well as by international treaties such as the Kyoto Protocol – might result in curbing economic growth, and by so doing it might impose a social cost that is greater than the avoided cost from global warming. The econometric evidence on the costs of global warming and the costs of climate policies, {{as well as on the}} respective benefits, is still unclear. Hence, the present scenario is characterized by a deep uncertainty on the side of the costs and benefits of collective action (or the lack thereof), and by a lack of cost‑efficient technological alternatives to the current technologies, particularly in the energy and transportation sectors. By applying the Forte interpretation of the Coase Theorem, it may be argued that—when this is more efficient—it might well be the case to let the cost of pollution (or the cost of eliminating pollution) bear on the polluted party, instead of the polluter. Since the polluted party, as far as global warming is concerned, is future generations, this means that a case can be made against climate change mitigation. Temperatures might be left free to grow (that is, carbon dioxide might be left free to accumulate in the atmosphere) until cost‑efficient technologies become available. Policies should instead focus on accelerating the process of technological innovation, and on developing adaptation measures in order to better face the effects, rather than addressing the alleged causes, of global warming. The second paper looks at the existing patterns from greenhouse gases (GHGs) emissions, widely suspected of contributing to global warming. Assuming that some sort of political action is to be taken, and that some result may follow, it is noted that emissions – both in aggregate, on a per capita basis, and in terms of carbon intensity, i. e. the ratio between total emissions and gross domestic product (GDP) – are stabilizing or slightly declining in most developed countries. On the contrary, total emissions, per capita emissions, and carbon intensity are dramatically increasing in the developing world. A higher carbon intensity is interpreted as a proxy of a more obsolete technology. An analogy is then made with the pattern governing other pollutants, that is the so‑called Environmental Kuznets Curve. Empirical evidence shows that as a general rule in most countries pollutants have increased at first, then peaked and decreased as GDP has grown. This phenomenon has been theoretically understood as a consequence of the increased concentrations of pollutants that made them ever more intolerable on the one hand, and the increased availability of wealth to be invested in newer technologies and/or in innovative investments on the other hand. It is unclear, however, whether or not carbon emissions are following such a bell‑shaped curve, too. Further investigations into this pattern have suggested that GDP growth is not the only independent variable. The existence of free market institutions also matter, in that this allows GDP growth and creates a more favourable environment for investments. An empirical measure of the existence of free market institutions has been gleaned by the Index of Economic Freedom, published yearly by the Heritage Foundation and the Wall Street Journal. A panel dataset has been built with data regarding the Index of Economic Freedom, its subcomponents, total GHGs emissions, and a number of other macroeconomic and environmental indicators. A model has been built that relates GHGs emissions with economic freedom, controlling for one or more of the above‑mentioned variables. A significant, negative correlation has been found, which means that economic freedom – along with other factors – may explain part of the difference in carbon intensity between countries. The correlation is stronger for lower values of economic freedom, consistently with other evidence that correlates economic freedom with economic growth. There are theoretical reasons to believe that the correlation may be a sign of a causal link, even though the empirical evidence is still not enough to support such a claim. If the causal link should be proven true, a policy consequence would be that–all else being equal–increasing economic freedom might lead to a reduced carbon intensity in the developing world, which is expected to account for an increasing share of global emissions in the next few decades. If this is correct, promoting economic freedom could be an effective, no‑regret way to contain future emissions. The third paper focuses on the European Union’s climate policies. A first assessment is made by looking at the stated objectives of the policy, i. e. limiting temperature growth within 2 degrees above the pre‑industrial levels, and the broader context of GHGs emissions. It is shown that the EU is responsible for a relatively small share of world emissions, which is going to decline if the present trends continue. Under this reasonable assumption, the impact that European efforts may or will have on world emissions is negligible, as is their possible consequences on temperatures rise and global warming. This means that EU policies, absent an international cooperation on curbing emissions, can’t hold vis‑à‑vis any cost‑benefit analysis, however low is the “cost” side. The existing policies are not only unlikely to deliver a measurable environmental benefit: they are also working very poorly. The most important European policy is the Emissions Trading Scheme (ETS), a cap & trade scheme that covers some 12, 000 facilities in all Europe. In the ETS First Phase (2005 ‑ 2007), emissions in the areas covered by the ETS did actually rise. There is evidence that this is at least partly a consequence of an over‑allocation that happened in the initial stage of the process. The Second Phase (2008 ‑ 2012) is expected to deliver more substantial emissions cuts, even though it is not yet clear whether ETS or the economic crisis will be the major driver. As to the Third Phase (2013 ‑ 2020) new rules are to be implemented, under which a growing number of allowances (starting from 30...|$|R

