0|10000|Public
3000|$|In Eq. (8), a {{convergence}} pressure, p k, was incorporated. It not only mitigates <b>the</b> <b>non-convergence</b> at high pressures but also provides reliable estimation for supercritical components.|$|R
5000|$|... #Caption: This graph {{demonstrates}} <b>the</b> <b>non-convergence</b> of <b>the</b> expanding spheres {{method for}} calculating the Madelung constant for NaCl {{as compared to}} the expanding cubes method, which is convergent.|$|R
30|$|As is {{mentioned}} in Section 2, Jacobian matrix {{is a powerful}} approach to judge <b>the</b> <b>non-convergence</b> phenomena of dynamical system [24]. A dynamic system is unstable <b>under</b> <b>the</b> condition that each eigenvalue absolute of the Jacobian matrix is larger than 1. Lv and Zhang [8] has found {{that a lot of}} chaotic behaviors are represented in the interval λ ∈ [1, 2.32588]. Accordingly, we use STM to modify the eigenvalue of Jacobian matrix of Equation (5) <b>under</b> <b>the</b> condition and get the controlled MCA Equation (8) without changing the value and location of unstable fixed points.|$|R
3000|$|... and can {{represent}} the best linear approximation to a differentiable function near a given point. It is generally be utilized to judge <b>the</b> <b>non-convergence</b> phenomena. Further, When the spectral {{radius of the}} Jacobian matrix of the dynamical system (1) is smaller than 1, i.e., ρ(J) < 1, the convergence of dynamical system can be obtained and the fixed point is attracted. If the spectral radius of Jacobian matrix of dynamical system (1) is larger than 1, i.e., ρ(J) > 1, the fixed point will lose its attracting property in the specific parameter interval and the dynamical system produces instability. After a few iterations, the iterative solutions could present <b>the</b> <b>non-convergence</b> phenomena, such as periodic oscillation, bifurcation, and even chaos.|$|R
40|$|This paper {{investigates the}} {{feasibility}} of a monetary union in Southern Africa Development Community (SADC) by looking at evidence of nominal exchange rate and inflation convergence. Using a methodology based on estimating time varying parameters, <b>the</b> evidence suggests <b>non-convergence.</b> <b>The</b> <b>non-convergence</b> of nominal exchange rate and consumer price inflation suggests that presently, the chances of SADC member countries satisfying some form of Maastricht-type criteria is quite low. Inflation and Exchange Rate Convergence in SADC. ...|$|R
40|$|The {{presence}} of some missing outcomes in randomized studies often complicates {{the estimation of}} measures of effect, even in well designed randomized controlled trials. The process may be complicated further when the efficacy rates are close to 0 % or 100 % as the standard binomial model is susceptible to model <b>non-convergence.</b> <b>The</b> main <b>objective</b> {{of this study was}} to compare the performance of multiple imputation (MI) and Complete Case analysis for dealing with missing binary outcomes when modeling a risk difference. Firstly, however, the binomial regression COPY method and the Cheung’s modified Ordinary Least Squares (OLS) method were examined using simulation processes for their appropriateness in risk difference modeling. It was found that the number of copies (for the COPY method) required to minimize <b>non-convergence</b> coincided with <b>the</b> number of copies that gave the most biased estimates of the true efficacy difference while increasing the number of copies made <b>the</b> problems of <b>non-convergence</b> and bias worse; using Cheung’s method, however, there was 100 % convergence with unbiased estimates of effect size. Simulation methods were used to compare the performance of complete case (CC) analysis and several multiple imputation (MI) models for handling missing outcome data over a wide range of efficacy environments and missing value assumptions. When outcomes were missing at random (MAR) or completely at random (MCAR), MI analyses that included treatment group membership in the imputation calculations yielded unbiased estimates of efficacy differences. The CC method was found to be as good, and often better, than MI methods when outcomes were MAR or MCAR, with coverage close to 95 % in many situations – but neither CC nor MI produced unbiased estimates of effect difference when outcomes were missing not at random (MNAR). It was concluded that CC and MI methods are equally good in terms of producing unbiased estimates of effect difference in most missing outcome situations, but applying the intention to treat principle (ITT) which requires all randomized patients to be included in the primary analysis of a RCT, MI should be adopted as the analysis method of first choice, accompanied by a secondary CC analysis for sensitivity purposes (i. e. to investigate the extent of any likely bias) ...|$|R
40|$|This work {{fits within}} a broader {{framework}} of bilingual language development {{in relation to}} the Interface Hypothesis. In this study, I focus on a specific population of bilingual speakers — L 2 advanced speakers of English from different L 1 backgrounds, using specific interface structures — pronouns and reflexives in Picture Noun Phrases (PNPs, e. g. Sheldon showed Leonard a picture of him/himself) as windows into <b>the</b> <b>non-convergence</b> bilingual speakers exhibit at the interface structures. Specifically, this work investigates <b>the</b> residual <b>non-convergence</b> in advanced L 2 speakers for structures involving interfaces between syntax and semantics/pragmatics, centering around two accounts: crosslinguistic influence and processing inefficiency. This study lends support to a model of L 2 anaphora resolution in which different weighs and directions of structural and non-structural information are considered on a gradient basis...|$|R
40|$|In this paper, {{we provide}} a {{rigorous}} quantum mechanical derivation for the coherent photon transport {{characteristics of a}} two-level atom coupled to a waveguide without linearizing the coupling coefficient between {{the light and the}} atom. We propose a novel single frequency sampling method that allows us to treat the singularities in real space scattering potential despite <b>the</b> <b>non-convergence</b> property. We also study <b>the</b> conditions <b>under</b> which <b>the</b> linearization of the coupling coefficient is an accurate assumption. This allows us to confirm and expand on the findings of the existing literature while obtaining the dynamic electronic polarizability for the two-level atom confined to a 1 -D waveguide while using an interaction Hamiltonian with rotating-wave approximation. Comment: This manuscript contains 3 figure...|$|R
40|$|Abstract—Approximate {{message passing}} is an {{iterative}} algo-rithm for compressed sensing and related applications. A solid theory about {{the performance and}} convergence of the algorithm exists for measurement matrices having iid entries of zero mean. However, it was observed by several authors that for more general matrices the algorithm often encounters convergence problems. In this paper we identify the reason of <b>the</b> <b>non-convergence</b> for measurement matrices with iid entries and non-zero mean {{in the context of}} Bayes optimal inference. Finally we demonstrate numerically that when the iterative update is changed from parallel to sequential the convergence is restored. I...|$|R
40|$|AbstractIn this paper, the multistability of a {{class of}} Amari’s α-divergence based nonnegative matrix {{factorization}} learning algorithms is analyzed. The analysis results show that invariant sets for the update algorithms can be constructed. In these invariant sets, <b>the</b> <b>non-convergence</b> of <b>the</b> discussed algorithms can be guaranteed. Based on Lyapunov’s stability theorem, the local convergence of this class of learning algorithms is proved {{in the domain of}} their update rules. In the simulation, the analysis results are applied to image representation. Experiment results demonstrate that selecting suitable initial data for different applications of these nonnegative matrix factorization algorithms is very important...|$|R
40|$|Approximate {{message passing}} is an {{iterative}} algorithm for compressed sensing and related applications. A solid theory about {{the performance and}} convergence of the algorithm exists for measurement matrices having iid entries of zero mean. However, it was observed by several authors that for more general matrices the algorithm often encounters convergence problems. In this paper we identify the reason of <b>the</b> <b>non-convergence</b> for measurement matrices with iid entries and non-zero mean {{in the context of}} Bayes optimal inference. Finally we demonstrate numerically that when the iterative update is changed from parallel to sequential the convergence is restored. Comment: 5 pages, 3 figure...|$|R
40|$|This article {{reveals the}} {{discrepancy}} and <b>the</b> <b>non-convergence,</b> still significant, between the Romanian agriculture and the agricultures of the EU states. Following the restitution {{of the agricultural}} land to the former owners {{at the beginning of}} 1990 s, the small size holdings have reached a significant percentage in the Romanian agriculture. Although the total number of agricultural holdings in Romania diminished with 626, 000 between 2005 - 2013, Romania is one of the EU countries with the most numerous agricultural holdings. In 2013, it had {{more than a third of}} the total number of agricultural holdings of the EU (33. 5...|$|R
40|$|The {{paper is}} {{the second part of}} a 2 -part paper. The first part focused on the issues of data {{structure}} and fast difference operation. The second studies <b>the</b> <b>non-convergence</b> of <b>the</b> alternating sum of volumes (ASV) process. An ASV is a series of convex components joined by alternating union and difference operations. It is desirable that an ASV series be finite. However, such is not always the case - the ASV algorithm can be nonconvergent. The paper investigates the causes of this nonconvergence, and finds and proves the conditions responsible for it. Linear time algorithms are then developed for detection. © 1991...|$|R
40|$|An interest-driven {{account of}} Embryonic Stem Cell Research would, given the {{considerable}} financial and scientific concerns, likely predict regulations to converge towards permissive policies. However, across Western Europe, national regulations of embryonic stem-cell research vary considerably, from general bans to permissive policies. There {{is a lack}} of systematic accounting for <b>the</b> <b>non-convergence,</b> and <b>the</b> sparse attempts at explanation are contradictory. Drawing on qualitative comparative analysis and configurational causality, we assess the interaction of a number of explanatory factors. Our empirical analysis reveals the importance of one factor in particular, path-dependence, insofar as prior policies on assisted reproduction exert a strong and systematic effect on the subsequent regulation of ESCR...|$|R
40|$|This {{study is}} based on an {{exceptionally}} large and automatically filtered data set containing most of the quoted prices on Reuters over 7 years. We employ semi-parametric extremal analysis. A bias reduction is attained by bootstrapping on resamples. The empirical results demonstrate the existence of the unconditional second moment of the distribution but <b>the</b> <b>non-convergence</b> of <b>the</b> fourth moment. Studies of cross-rates among European Monetary System currencies show a smaller tail index indicating a higher probability on extreme returns relative to the scale. The theory is subsequently applied to calculating the probabilities on as yet unseen extreme returns. This provides information to the treasurers of currency desks...|$|R
40|$|The close-coupling {{method for}} electron/positron–atom {{scattering}} promises {{to give a}} complete description of the scattering process provided the space of target states is properly spanned. This paper will discuss {{the structure of the}} equations, emphasising questions of stability associated with the expansions over sets of target states. For electron–atom scattering, the character of the solution is discussed and a simple example is given to illustrate <b>the</b> <b>non-convergence</b> of <b>the</b> half-shell T-matrix. This lack of convergence can be fixed by application of the symmetrisation boundary condition, leading to new forms of the equations. For positron–atom scattering, the standard equations yield convergent half-shell T-matrices for all but the largest calculations...|$|R
40|$|AbstractTwo {{difficulties}} {{connected with}} the solution of Laplace’s equation around an object inside an infinite circular cylinder are resolved. One difficulty is <b>the</b> <b>non-convergence</b> of Fourier transforms used, in earlier publications, to obtain the general solution, and the second difficulty concerns the existence of apparently different expressions for the solution. By using a Green’s function problem as an easily analyzed model problem, we show that, in general, Fourier transforms along the cylinder axis exist only {{in the sense of}} generalized functions, but when interpreted as such, they lead to correct solutions. We demonstrate the equivalence of the corrected solution to a different general solution, also previously published, but we point out that the two solutions have different numerical properties...|$|R
40|$|AbstractTwo recently-proposed {{methods for}} {{estimating}} the m frequencies of a trigonometric signal using Szegö polynomials of fixed degree k>m consist of multiplying {{the moments of}} the n-truncated periodogram by the moments of the Poisson kernel and the wrapped Gaussian, respectively, {{in an effort to}} address <b>the</b> <b>non-convergence</b> of <b>the</b> polynomials as n→∞. These methods are seen to be equivalent to convolution of point masses with approximate identities, suggesting a general method. We characterize the limit polynomial for the case when the approximate identity is the Fejér kernel, extending recent results of the author for the case of the Poisson kernel. Moreover, the limit is seen to be the same as in the former case...|$|R
40|$|We {{carry out}} {{simulations}} of gravitationally unstable disks using smoothed particle hydrodynamics(SPH) and the novel Lagrangian meshless finite mass (MFM) scheme in the GIZMO code (Hopkins 2015). Our {{aim is to}} understand the cause of <b>the</b> <b>non-convergence</b> of <b>the</b> cooling boundary for fragmentation reported in the literature. We run SPH simulations with two different artificial viscosity implementations, and compare them with MFM, which does not employ any artificial viscosity. With MFM we demonstrate convergence of the critical cooling time scale for fragmentation at β_crit = 3 [...] Non-convergence persists in SPH codes, although it is significantly mitigated with schemes having reduced artificial viscosity such as inviscid SPH (ISPH) (Cullen & Dehnen 2010). We show how <b>the</b> <b>non-convergence</b> problem is caused by artificial fragmentation triggered by excessive dissipation of angular momentum in domains with large velocity derivatives. With increased resolution such domains become more prominent. Vorticity lags behind density due to numerical viscous dissipation in these regions, promoting collapse with longer cooling times. Such effect is shown to be dominant over the competing tendency of artificial viscosity to diminish with increasing resolution. When the initial conditions are first relaxed for several orbits, the flow is more regular, with lower shear and vorticity in non-axisymmetric regions, aiding convergence. Yet MFM is the only method that converges exactly. Our findings are of general interest as numerical dissipation via artificial viscosity or advection errors can also occur in grid-based codes. Indeed for the FARGO code values of β_crit significantly higher than our converged estimate {{have been reported in}} the literature. Finally, we discuss implications for giant planet formation via disk instability. Comment: please check the ApJ versio...|$|R
30|$|This {{article focuses}} on the chaotic {{dynamics}} analysis, and especially chaos control of Douglas's minor component analysis algorithm. Periodic oscillation, bifurcation, and chaotic behaviors are discussed {{on the basis of}} the chaos theory, and the Lyapunov exponent and the Jacobian matrix reflecting the dynamic property of non-linear system are analyzed. Furthermore, the chaotic phenomena of Douglas' MCA algorithm under some conditions can be controlled and transformed into a stable system with STM of chaos feedback control, and the convergence solutions can be achieved in the original chaotic intervals. Generally, exploring the chaotic dynamic behavior of Douglas's MCA is a good path to understand the essential reasons for <b>the</b> <b>non-convergence</b> in MCA method, and it is helpful to extend the effective application of the MCA and related methods.|$|R
40|$|Fossil fuels {{have been}} {{fundamental}} {{to the evolution of}} humanity's technology and way of life for the last two centuries. However, their use has two main disadvantages: (a) the climate change produced by the release of carbon dioxide (CO 2) during their combustion; and (b) their limited reserves. In the last decades, society has tried to counter these problems by potentiating the use of renewable energies instead of fossil fuels. The Ocean Grazer (OG) {{is one of the many}} proposed projects to develop the renewable energy eld further. Developed by the University of Groningen, the OG platform is expected to extract and store multiple forms of renewable energy, of which wave energy is its primary source. The main innovation provided by the OG is a novel Wave Energy Converter (WEC) technology denominated MP 2 PTO, which aims to adapt to the di erent wave pro les so the maximum energy content can always be extracted. In the last years, one of the many research lines in the group has been the study of the wave energy extraction process in an experimental setup called the wave tank, which contains a prototype of the multi-piston PTO concept used in the OG-WEC. More precisely, in the last few months, experimental measurements of the velocity of the particles within the tank have been carried out using the Digital Particle Image Velocimetry (DPIV) technique, taking into account di erent scenarios that allow investigating the in uence of the prototype in the process of energy extraction. In this work, the validation of these experimental measurements of the kinetic energy contained in the OG group's wave tank experimental setup is approached by the use of a computational uid dynamics (CFD) simulation software developed by the University of Groningen called ComFLOW. In a rst phase of the validation process, a time domain analysis of the simulation results is done. In this analysis, <b>the</b> <b>non-convergence</b> of <b>the</b> simulation is detected. However, the results of the simulations are compared to the experimental measurements leading to a qualitative coincidence of the evolution of the content of energy along the tank but not quantitative. In a second phase, <b>the</b> <b>non-convergence</b> of <b>the</b> simulation is studied by means of a frequency domain analysis of the wave tank's water surface height. This study allows the detection of a strong mesh-dependant wave re ection in the simulation's wave tank, which is the reason behind <b>the</b> <b>non-convergence</b> of <b>the</b> simulation. Consequently, due to this non-convergence, no validation of the experimental measurements can be fully con rmed even though the behaviour of the kinetic energy along the simulation tank is very similar to the experimental measurements. Finally, further research paths are proposed in the Conclusion Section...|$|R
40|$|One of the {{difficulties}} of the three-dimensional (3 -D) eddy current finite element methods is to solve large finite element equations economically. In this paper a 3 -D eddy-current finite element model using a four component formulation of complex variables to study skewed rotor induction motors is described. An iterative process among the four specific components during the solution of large algebraic equations is presented. The proposed method overcomes <b>the</b> <b>non-convergence</b> problems when <b>the</b> ICCG method or the shifting ICCG method is used directly. The algorithm also requires much less computer storage compared with the Gaussian elimination method. Department of Electrical EngineeringIndustrial CentreAuthor name used in this publication: S. L. HoAuthor name used in this publication: W. N. FuAuthor name used in this publication: H. C. WongH. C. Wong, Industrial Centr...|$|R
40|$|Abstract. Despite noise injecting during {{training}} {{has been demonstrated}} with success in enhancing the fault tolerance of neural network, theoretical analysis on the dynamic of this noise injection-based online learning algorithm has far from complete. In particular, the convergence proofs for those algorithms have not been shown. In this regards, this paper presents an empirical study on <b>the</b> <b>non-convergence</b> properties of injecting weight noises {{during training}} a multilayer perceptron, and an online learning algorithm called SNIWD (simultaneous noise injection and weight decay) to overcome such non-convergence problem. Simulation results show that SNIWD is able to improve the convergence and enforce small magnitude on the network parameters (input weights, input biases and output weights). Moreover, SNIWD is {{able to make the}} network have similar fault tolerance ability as using pure noise injection approach. ...|$|R
40|$|The way {{and extent}} to which {{differences}} in economic systems and stages of development, {{and the impact of}} institutional changes affected the political economy and fiscal systems of regions, or vice versa, is the overall theme of this volume. One major problem is <b>the</b> <b>non-convergence</b> of economic regions, financial networks, political borders and fiscal systems. The question is whether a set of variables is supra-regional, interregional, regional, local or even a mix of all of these. These questions have broad implications for our understanding of urban society and the relations between town and countryside. This volume contains studies about economic, financial and political structures, and developments in different regions of the Low Countries and the Lower Rhine area in a regional comparative perspective during the Late Middle Ages and the Early Modern Period...|$|R
40|$|Revealing hidden {{features}} in unlabeled data is called unsupervised feature learning, which {{plays an important}} role in pretraining a deep neural network. Here we provide a statistical mechanics analysis of the unsupervised learning in a restricted Boltzmann machine with binary synapses. A message passing equation to infer the hidden feature is derived, and furthermore, variants of this equation are analyzed. A statistical analysis by replica theory describes the thermodynamic properties of the model. Our analysis confirms an entropy crisis preceding <b>the</b> <b>non-convergence</b> of <b>the</b> message passing equation, suggesting a discontinuous phase transition as a key characteristic of the restricted Boltzmann machine. Continuous phase transition is also confirmed depending on the embedded feature strength in the data. <b>The</b> mean-field result <b>under</b> <b>the</b> replica symmetric assumption agrees with that obtained by running message passing algorithms on single instances of finite sizes. Interestingly, in an approximate Hopfield model, the entropy crisis is absent, and a continuous phase transition is observed instead. We also develop an iterative equation to infer the hyper-parameter (temperature) hidden in the data, which in physics corresponds to iteratively imposing Nishimori condition. Our study provides insights towards understanding the thermodynamic properties of the restricted Boltzmann machine learning, and moreover important theoretical basis to build simplified deep networks. Comment: 24 pages, 9 figures, results adde...|$|R
40|$|Submitted to {{the journal}} of Numerical AlgorithmsThe implicitly {{restarted}} Arnoldi method (IRAM) computes some eigenpairs of large sparse non Hermitian matrices. However, {{the size of the}} subspace in this method is chosen empirically. A poor choice of this size could lead to <b>the</b> <b>non-convergence</b> of <b>the</b> method. In this paper we propose a technique to improve the choice of the size of subspace. This approach, called multiple implicitly restarted Arnoldi method with nested subspaces (MIRAMns) is based on the projection of the problem on several nested subspaces instead of a single one. Thus, it takes advantage of several different sized subspaces. MIRAMns updates the restarting vector of an IRAM by taking the eigen- information of interest obtained in all subspaces into account. With almost the same complexity as IRAM, according to our experiments, MIRAMns improves the convergence of IRAM...|$|R
40|$|AbstractIn this paper, we {{developed}} the wavelet based decoupled method for the numerical solution of elastohydrodynamic lubrication problems. The standard decoupled method with Newton-generalized minimum residual procedure performs poorly or may break down {{when it is}} used to solve such type of problems. Zargari et al. (2007) presented decoupled and coupled methods, in which for the limitations of decoupled method for some set of physical parameters and slight variation in these values <b>the</b> <b>non-convergence</b> solution was tabulated. The wavelet based decoupled technique is used to overcome these limitations. Residual errors are presented in comparison with the existing methods to demonstrate the versatility and applicability of the proposed method. And also investigations of the effects of couple stress fluids on elastohydrodynamic lubrication behavior in smooth and rough contacts at low-speed-high load and high-speed-low load conditions are discussed. The elastohydrodynamic lubrication characteristics computed for couple stress fluids are found to have strong dependence on couple stress parameter...|$|R
40|$|In this communication, the {{convergence}} of the 1 /t and Wang - Landau algorithms in the calculation of multidimensional numerical integrals is analyzed. Both simulation methods are applied {{to a wide variety}} of integrals without restrictions in one, two and higher dimensions. The errors between the exact and the calculated values of the integral are obtained and the efficiency and accuracy of the methods are determined by their dynamical behavior. The comparison between both methods and the simple sampling Monte Carlo method is also reported. It is observed that the time dependence of the errors calculated with 1 /t algorithm goes as N^{- 1 / 2 } (with N the MC trials) in quantitative agreement with the simple sampling Monte Carlo method. It is also showed that the error for the Wang - Landau algorithm saturates in time evidencing <b>the</b> <b>non-convergence</b> of <b>the</b> methods. The sources for the error are also determined. Comment: 8 pages, 5 figure...|$|R
40|$|In {{generalized}} linear models, {{such as the}} {{logistic regression}} model, maximum likelihood estimators are well-known to be biased at smaller sample sizes. When the number of dose levels or replications per dose is small, bias in the maximum likelihood estimates can lead to very misleading results and the model often fails to converge. In order to correct the bias present in the maximum likelihood estimates and <b>the</b> problem of <b>non-convergence,</b> <b>the</b> penalized maximum likelihood estimator is considered. Simulations compare the fit and empirical confidence levels of inferences made from the maximum likelihood and penalized maximum likelihood based model...|$|R
30|$|There is a {{consensus}} {{in the developed world}} empirical literature that innovation enhances labour productivity (Griffin et al. 2006; Griffith et al. 2004; Mairesse and Mohnen 2010; Mairesse et al. 2005; OECD 2009). However, evidence in the developing world literature is rather contradicting; for example, in Sub-Saharan Africa (SSA), Chowdhury and Wolf (2003) argue that innovation (proxied by Information Communication and Technology (ICT) 6) dampens labour productivity among SMEs. Similarly, Goedhuys et al. (2008) show that product or process innovation has no significant impact on labour productivity. On the other hand, Esselaar et al. (2007) show that innovation (proxied by ICT) enhances labour productivity among SMEs. <b>The</b> <b>non-convergence</b> in <b>the</b> innovation and labour productivity relationship is equally prevalent among empirical studies in other developing economies. For example, empirical studies in Latin America have argued that innovation has no impact on labour productivity (Perez et al. 2005; Benavente 2006; Raffo et al. 2008; Crespi and Zuniga 2012). On the contrary, Raffo et al. (2008) show that product innovation has a significant impact on labour productivity.|$|R
30|$|<b>The</b> <b>non-convergence</b> in {{relationship}} between innovation and firm {{performance in the}} developing world is in contradiction to the consensus in developed world literature in which innovation is argued to enhance labour productivity (Griffin et al. 2006; Griffith et al. 2004; Mairesse and Mohnen 2010; Mairesse et al. 2005; OECD 2009). This perhaps suggests that there is room to further explore the innovation-labour productivity relationship in developing countries. It is in that regard that this paper undertakes to explain the relationship between innovation and labour productivity using the 2013 WBES data for Uganda. Consistent with Lin and Chen (2007), we measure innovation using whether a firm engaged in process, organisational, product and marketing innovation as opposed to ICT utilization which Chowdhury and Wolf (2003) and Esselaar et al. (2007) used in explaining labour productivity in East Africa and SSA respectively. This paper further contributes to empirical literature by exploring the potential of complimentary among process, product, marketing and organisational innovations in their relationship with labour productivity (Appendix 1 and Appendix 2).|$|R
40|$|Digital {{images from}} diverse medical imaging {{modalities}} and from different imaging times are becoming an indispensable information resource for making clinical decisions. Image registration is an enabling technique for more fully utilizing the embedded heterogeneous image information. However, {{in addition to}} the complex differences and deformations inherent in the medical images, the increasing scope, resolution, and dimensionality of imaging pose significant challenges in this medical arena. Wavelets have shown great potential in multi-scale registration due to their superior capacity for representing image information at different resolutions and spatial frequencies. However, the application of wavelets in registration is hindered by their lack of rotation- and translation-invariance. To overcome this obstacle, this paper proposes a non-iterative hierarchical registration method based on points of interest which are extracted automatically from wavelet decompositions. The proposed algorithm for two-dimensional monomodal medical images has been validated by experiments on phantom data and clinical imaging data. This proposed non-iterative method provides a computationally efficient registration, as well as assists in avoiding <b>the</b> <b>non-convergence</b> problem. Department of Electronic and Information Engineerin...|$|R
40|$|The {{objective}} {{of this study is}} to determine the mixed-mode I+II interlaminar fracture toughness at high mode I content for a 0 °/ 45 ° ply interface. Mixed-mode I+II delamination tests on unidirectional and multidirectional specimens were performed using ADCB and AMMF methods. The procedure used for choosing the stacking sequence resulted in desirable propagation behaviors of the delamination. There was no change of delamination plane, an acceptable crack front profile, no initial specimen curvature, and no energy dissipation through global specimen damage. For data reduction, the experimental compliance method was the most consistent in determining the total energy release rate. Finite element simulation proved to be an indispensable analysis tool. A resin-rich interlayer was modeled at the delamination interface to eliminate <b>the</b> <b>non-convergence</b> behavior of <b>the</b> energy release rate. For delamination between plies of different orientations, the mode decomposition could only be analyzed by the finite element method. The ADCB configuration offered a very high mode I content (90 - 95 %) while the AMMF offered a moderately high mode I (70 - 78 %) ...|$|R
40|$|A {{graph theory}} {{oriented}} algorithm for optimal ultimate pit limit design is developed. Mathematical proofs of optimality and convergence are given. The algorithm {{works on a}} 3 -D block mine model and formulates the model into a directed graph consisting many trees. The vertices in the graph are identified with the blocks in the model and the imposed arcs in the graph represent pit slope constraints. The formation of each directed tree is based more on the ore-waste support concept than geometric constraints alone. The algorithm efficiently handles the joint support and re-allocation problems. The theoretical proof shows that the new algorithm is consistently faster than the well known Lerchs-Grossmann's (LG) algorithm, {{which is the only}} algorithm developed in the past one-quarter century capable of producing a true optimum pit limit. The case study results show that the new algorithm is able to generate the optimal ultimate pit limit for a model with 80 columns x 80 rows x 40 levels on an IBM PC AT 80286 microcomputer in 115 minutes. The indirect comparison was made between the results of the new algorithm and the results obtained by P. Huttagosol (1988, 1989) using the LG algorithm. P. Huttagosol optimized a smaller mine model than the one optimized by the new algorithm in 535 minutes of VAX 8600 CPU time. The comparison between 535 minutes of VAX 8600 CPU time for a smaller model with 115 minutes PC AT processing time for a bigger model clearly indicates that the new algorithm is significantly faster than the LG algorithm. This study also investigates both proposed mathematical optimization approaches and the popular trial and error "pushback" approach to long range mine planning. Both the theoretical analysis and numerical examples demonstrate it is impossible to obtain the optimal solution to mine production scheduling by the approach combining the Lagrangian relaxation with the ultimate pit limit algorithm. <b>The</b> <b>non-convergence</b> due to redundant optimal solutions and <b>the</b> <b>non-convergence</b> due to <b>the</b> requirement of advanced stripping are identified with the proposed approach. The investigation clarifies the long-time misunderstood concept and proves the impossibility of such a research direction itself. Finally, some problem solving techniques which play important roles in the computerized mine planning and grade control are developed and discussed. Specifically, they are: (1) point-in-polygon algorithm, (2) polygon area algorithm, (3) polygon clipping algorithm, (4) blast hole data collection, validation and database maintenance, and (5) the interactive graphics ore-waste delineation...|$|R
30|$|As in {{many other}} {{countries}} all over the world, all the models predict better mortality for women as mortality experience for women has less fluctuations. In addition, {{it is important to}} highlight the use of these seven models in abridged life tables and the results found despite <b>the</b> <b>non-convergence</b> of some models. In this study, the models presented problems of convergence with the cohort effect with both R-packages for men, except the APC model. The convergence problem for mortality models with cohort effect has been pointed out by other authors such as Debón et al. (2010), Hunt and Villegas (2015), and Kennes (2017). In this study, we would like to remark that the cohort effect presents problems of estimation of the parameters on abridged life tables as cohorts represent subsets of five cohorts with different numbers of observations. On the other hand, the CBD model demonstrated very bad behavior for infants and advanced ages. Therefore, the comparison was carried out by fitting LC, LC 2, and APC. In summary, we can conclude that the LC 2 model provides a better fit for both sexes, although the improvement of LC 2 on LC is mostly for intermediate ages.|$|R
40|$|In recent work, we have {{developed}} a variational principle for large N multi-matrix models based on the extremization of non-commutative entropy. Here, we test the simplest variational ansatz for our entropic variational principle with Monte-Carlo measurements. In particular, we study the two matrix model with action Tr[m^ 2 2 (A_ 1 ^ 2 + A_ 2 ^ 2) - 1 4 [A_ 1,A_ 2]^ 2] {{which has not been}} exactly solved. We estimate the expectation values of traces of products of matrices and also those of traces of products of exponentials of matrices (Wilson loop operators). These are compared with a Monte-Carlo simulation. We find that the simplest wignerian variational ansatz provides a remarkably good estimate for observables when m^ 2 is of order unity or more. For small values of m^ 2 the wignerian ansatz is not a good approximation: the measured correlations grow without bound, reflecting <b>the</b> <b>non-convergence</b> of matrix integrals defining the pure commutator squared action. Comparison of this ansatz with the exact solution of a two matrix model studied by Mehta is also summarized. Here the wignerian ansatz is a good approximation both for strong and weak coupling. Comment: 21 pages, 8 figure...|$|R
