41|1|Public
5000|$|Triangular warning LEDs {{at the top}} {{and bottom}} of the shutter speed scale, to {{indicate}} over- or <b>under-exposure.</b>|$|E
5000|$|Optical {{proximity}} correction uses computational {{methods to}} counteract the effects of diffraction-related blurring and <b>under-exposure</b> by modifying on-mask geometries with means such as: ...|$|E
50|$|In {{the current}} ILO Classification system, {{the reader is}} first asked to grade {{radiographic}} quality. There are four technical grades: (1) Good; (2) Acceptable, with no technical defect likely to impair classification; (3) Acceptable, with some technical defect but still adequate; and (4) Unacceptable. Quality defects include over- or <b>under-exposure,</b> underinflation, artifacts, improper positioning, and others.|$|E
40|$|Although solar {{ultraviolet}} (UV) yields favourable health effects, chronic (occupational) UV {{exposure is}} responsible of most epithelial skin cancers. The dose-response between sun exposure and skin cancer occurrence {{is not yet}} fully understood. Exposure data is limited because individual exposure assessment is tedious, costly and challenging (prone to epidemiological bias, limited to short-term exposure and specific exposure setting, and cannot account for the large intra-individual (anatomical) variations in UV doses received). A multidisciplinary team gathering competencies in meteorology, 3 D computing sciences, public health and exposure sciences, developed a numeric simulation tool (SimUVEx) to predict anatomical UV exposure. SimUVEx uses 3 D computer graphics techniques to compute UV doses based on postural information and ambient UV measurements. It allows assessment of specific exposure scenarios, taking into account body surface inclination, orientation to the sun, shading from other body parts, and radiation component (direct, diffuse, reflected). Selected applications of SimUVEx to estimate the contribution of occupational UV exposure to the burden of squamous cell carcinoma, to measure the contribution of diffuse UV radiation to exposure patterns of various body sites, {{and to develop a}} regression model for predicting UV exposure ratios without individual measurements by anatomical site are presented. So far, outputs from SimUVEx have enabled to (i) build exposure scenarii (using for instance population-based survey of frequent outdoor occupations), (ii) produce body site-specific reference doses for common outdoor occupational and leisure activities and (iii) identify high-risk situations with respect to over- and <b>under-exposures</b> to sunlight. SimUVEx has contributed to improve our understanding of exposure patterns and identify potential shortcomings in current sun protection recommendations. Current developments, perspectives and limitations of SimUVEx will be discussed...|$|R
50|$|Maddy {{stays with}} Carla and Rosa. A doctor {{confirms}} that she's never had SCID, just an underdeveloped immune system from <b>under-exposure</b> due {{to living in}} filtered air her whole life. Her mother later tells her that, after Maddy's father and brother {{died in a car}} crash, Maddy was all she had left and she wanted to protect her and keep her safe.|$|E
50|$|The ASA {{standard}} {{underwent a}} major revision in 1960 with ASA PH2.5-1960, when the method to determine film speed was refined and previously applied safety factors against <b>under-exposure</b> were abandoned, effectively doubling the nominal speed of many black-and-white negative films. For example, an Ilford HP3 {{that had been}} rated at 200 ASA before 1960 was labeled 400 ASA afterwards without any change to the emulsion. Similar changes were applied to the DIN system with DIN 4512:1961-10 and the BS system with BS 1380:1963 in the following years.|$|E
50|$|The Mamiya ZM, {{introduced}} in 1982, was essentially an advanced {{version of the}} ZE-2, {{with some of the}} features of the ZE-X. It was the last Mamiya 135-film camera produced. It had an aperture-priority automatic time control, based on center-weighted TTL readings, an automatic shutter-speed range from 4 seconds to 1/1000, and a manual range from 2 seconds to 1/1000. Visual and audio signals indicated over- or <b>under-exposure,</b> pending battery failure, or excessive camera shake. Metering modes, shutter release, self-timer, manual time settings and the ergonomics of the camera body were also improved.|$|E
5000|$|DeFazio {{has served}} as the {{director}} of photography on numerous music videos, 35 mm and high-definition movies, some of which have been screened at notable film festivals, including Sundance Film Festival (Crossing, Roam) and the Cannes International Film Festival (9:30). He has shot movies for Hollywood directors and producers Joe Dante (director of Gremlins), Paul Schrader (director of American Gigolo and screenwriter of Taxi Driver and Raging Bull), and Roger Corman (producer of Death Race, The Little Shop of Horrors) among others. His second feature, Undoing was singled out by the Los Angeles Film Festival for its [...] "striking imagery," [...] and Tobe Roberts from Cinema Without Borders called his work [...] "stylistically shot and emotionally charged." [...] While DeFazio's diverse background in the visual arts offers him a unique perspective and approach to cinematography, his work has been criticized for its overuse of color-drenching, over-exposure, and <b>under-exposure.</b>|$|E
5000|$|Meyrowitz {{states that}} prior to the {{saturation}} of television in society, our political leaders had been treated as a [...] "mystified presence", at a status above the common citizen, as {{it was easier to}} control the flow of information that represented who they were and what they did. [...] Although television is a useful tool for our politicians in trying to create this status, it [...] "tends to mute differences between levels of social class". Meyrowitz terms this [...] "a double-edge sword", as over-exposure of a political leader diminishes their power, with their continuous presence making them seem more ordinary and less mystified. [...] This over-exposure is difficult to balance with <b>under-exposure,</b> as without media presence a leader has minimal power over people, yet with exceeding presence they lose power. Because of the immediacy of information to the common citizen about all issues of society, they are now able to closely inspect the image of our leaders, creating a demystification of their presence.|$|E
5000|$|If the {{resulting}} shadows {{are unable to}} be processed to acceptable noise or tonal range, one has simply encountered a situation that cannot be taken with a single shot, and consideration could be given to HDR (High Dynamic Range) techniques requiring multiple exposures. If the latter is infeasible due to scene or camera motion, on may revert to the technique of exposing to the important highlights (abbr. ETTIH), which is in fact just a slight generalization of ETTR. With ETTR, exposure is maximized up to the constraint of preserving all highlights at the right edge of the histogram (a constraint implied by the hard saturation of the digital sensor). With ETTIH, exposure is maximized a bit further; the constraint is relaxed so that only the highlights deemed important are preserved before the right edge of the histogram, and the highlights deemed unimportant may fall into the saturation zone of the sensor. Typical examples of unimportant highlights include the sun, other very bright light-sources, and sharp-edged specular highlights like chrome car bumpers in the sun; however, one should avoid blowing areas with smooth luminosity gradients, for instance the sky around the sun, because these likely lead to visible sensor saturation artefacts (banding). Many cameras have over-exposure [...] "blinkies", showing the location of blown highlights which is not evident from the histogram. Some cameras also have <b>under-exposure</b> [...] "blinkies" [...] as an exposure aid. Mirror-less cameras often have [...] "preview zebras", which is another aid for being aware of and correcting over-exposure.|$|E
40|$|The narrow {{therapeutic}} window for calcineurin inhibitors (CNI) remains {{a challenge in}} the management of transplant recipients, with <b>under-exposure</b> risking acute rejection and over-exposure resulting in acute and chronic impairment of graft function. Optimisation of CNI exposure by computerised dosing and different monitoring strategies have long been an area of interest in the transplant literature (1, 2) ...|$|E
40|$|A {{retrospective}} {{study was conducted}} to assess our 10 -year experience of therapeutic drug monitoring (TDM) of linezolid in a large patient population to establish whether conventional dosing may result in adequate drug exposure in the majority of patients. Patients included in this study underwent TDM of linezolid trough concentration (Cmin) during treatment with conventional doses of 600 mg every 12 hr in the period between January 2007 and June 2016. The desired range of Cmin was set between 2 and 7 mg/L (<b>under-exposure,</b> Cmin 7 mg/L). Multivariate logistic regression analysis investigated variables potentially correlated with linezolid Cmin. One thousand forty-nine patients had 2484 linezolid Cmin assessed during treatment with conventional doses. Median (IQR) linezolid Cmin was 5. 08 mg/L (2. 78 - 8. 52 mg/L). Linezolid Cmin were within the desired range in 50. 8 % of cases (1262 / 2484). Over-exposure (n= 821; 33 %) occurred much more frequently than <b>under-exposure</b> (n= 401; 16. 2 %), and was severe (> 20 mg/L) in 3. 9 % of cases (98 / 2484). Linezolid over-exposure was significantly associated with CrCLC-G estimates â‰¤ 40 mL/min (OR 1. 463; 95 % CI 1. 124 - 1. 904, P = 0. 005). Linezolid <b>under-exposure</b> resulted significantly associated with CrCLC-G estimates > 100 mL/min (OR 3. 046; 95 % CI 2. 234 - 4. 152, P < 0. 001. Linezolid Cmin was not correlated linearly to CrCLC-G (R(2) = 0. 061). Variability in renal function explained only partially the very wide interindividual linezolid Cmin variability. Our study suggests that TDM could represent a valuable approach in optimizing linezolid exposure in the majority of patients. This article is protected by copyright. All rights reserved...|$|E
40|$|The {{original}} publication {{is available}} at [URL] study of the rates of repeat radiography in two hospitals in the Cape Peninsula was undertaken. The relationship between these rates {{and the type of}} examination as well as the contributing factors was established. The total frequencies for the two hospitals varied from 10, 1 % to 14 %. The predominant reasons for 78 % of repeat radiographs were over-exposure, <b>under-exposure</b> and positioning. Publishers' versio...|$|E
40|$|This paper {{introduces}} temporal image fusion. The proposed technique builds upon {{previous research}} in exposure fusion and expands it {{to deal with}} the limited Temporal Dynamic Range of existing sensors and camera technologies. In particular, temporal image fusion enables the rendering of long-exposure effects on full frame-rate video, as well as the generation of arbitrarily long exposures from a sequence of images of the same scene taken over time. We explore the problem of temporal <b>under-exposure,</b> and show how it can be addressed by selectively enhancing dynamic structure. Finally, we show that the use of temporal image fusion together with content-selective image filters can produce a range of striking visual effects on a given input sequence...|$|E
40|$|We {{present a}} simple and usable noise model for the raw-data of digital imaging sensors. This signal-dependent noise model, which gives the pointwise standard-deviation of the noise as a {{function}} of the expectation of the pixel raw-data output, is composed of a Poissonian part, modeling the photon sensing, and Gaussian part, for the remaining stationary disturbancies in the output data. We further explicitly take into account the clipping of the data (over- and <b>under-exposure),</b> faithfully reproducing the nonlinear response of the sensor. We propose an algorithm for the fully automatic estimation of the model parameters given a single noisy image. Experiments with synthetic images as well as with real raw-data from various sensors prove the practical applicability of the method and the accuracy of the proposed model...|$|E
40|$|Abstractâ€”This paper {{introduces}} temporal image fusion. The proposed technique builds upon {{previous research}} in exposure fusion and expands it {{to deal with}} the limited Temporal Dynamic Range of existing sensors and camera technologies. In particular, temporal image fusion enables the rendering of long-exposure effects on full frame-rate video, as well as the generation of arbitrarily long exposures from a sequence of images of the same scene taken over time. We explore the problem of temporal <b>under-exposure,</b> and show how it can be addressed by selectively enhancing dynamic structure. Finally, we show that the use of temporal image fusion together with content-selective image filters can produce a range of striking visual effects on a given input sequence. Index Termsâ€”Exposure fusion temporal blending image pro-cessing computational photography I...|$|E
40|$|Abstract. When taking {{pictures}} using a commodity camera {{in a scene}} with strong or harsh lighting, such as a sunny day outdoors, we often see a loss of highlight details (over-exposure) in some bright regions {{and a loss of}} shadow details (<b>under-exposure)</b> in some dark regions. In this paper, we developed a wavelet tight frame based approach to reconstruct a well-exposed image with better visibility on details from the one with over/under-exposed regions. There are two modules in the proposed approach: one in lightness channel that in-paints the clipped lightness and adjustes image contrast; and the other in chromatic channels that in-paints the saturated colour regions. The experiments showed that our method can effectively repair over/under-exposed regions and it performed better than other existing methods on tested real photographs...|$|E
40|$|Abstract. The {{prevalence}} of social network sites and smartphones {{has led to}} many people sharing their locations with others. Privacy concerns are seldom ad-dressed by these services; the default privacy settings may be either too restrictive or too lax, resulting in <b>under-exposure</b> or over-exposure of location information. One mechanism for alleviating over-sharing is through personalised privacy set-tings that automatically change according to users â€™ predicted preferences. In this paper, we use data collected from a location-sharing user study (N = 80) to in-vestigate whether users â€™ willingness to share their locations can be predicted. We find that while default settings match actual users â€™ preferences only 68 % of the time, machine-learning classifiers can predict up to 85 % of users â€™ preferences. Using these predictions instead of default settings would reduce the over-exposed location information by 40 %. ...|$|E
40|$|This paper proposes {{the first}} {{end-to-end}} deep framework for {{high dynamic range}} (HDR) imaging of dynamic scenes with large-scale foreground motions. In state-of-the-art deep HDR imaging such as [13], the problem is formulated as an image composition problem, by first aligning input images using optical flows which are still error-prone due to occlusion and large motions. In our end-to-end approach, HDR imaging is formulated as an image translation problem and no optical flows are used. Moreover, our simple translation network can automatically hallucinate plausible HDR details {{in the presence of}} total occlusion, saturation and <b>under-exposure,</b> which are otherwise almost impossible to recover by conventional optimization approaches. We perform extensive qualitative and quantitative comparisons to show that our end-to-end HDR approach produces excellent results where color artifacts and geometry distortion are significantly reduced compared with existing state-ofthe-art methods. Comment: Submitted to CVPR 201...|$|E
40|$|Using global {{contrast}} enhancement, low contrast image can {{be improved}} in its quality globally. The enhanced output image, with such type of enhancement, {{may not have the}} noise and ringing artifacts. However, it may have over-exposure on some parts of the image and <b>under-exposure</b> on some other parts of the image when too high contrast gain occurs. Besides these, the enhanced output image may lack of local details. On the other hand, using local contrast enhancement, the local details of an image can be better defined. However, local contrast enhancement may produce the output image with noise and ringing artifact when too much contrast gain occurs. Besides these, it may be poor in global contrast. For some images, applying the local contrast enhancement along with global contrast enhancement is much better than that of global contrast enhancement only or local contrast enhancement only...|$|E
40|$|Abstract â€” In our fast going life, {{digital cameras}} and smart phones {{have been widely}} used to acquire photographs. In fact, digital cameras and smart phones have a limited dynamic range, which is very much lower than that our eyes can perceive. So, the {{pictures}} taken in high dynamic range scenes often exhibit <b>under-exposure</b> or over-exposure artifacts in shadow or highlight regions. Here, an image fusion based approach, called classified exposure image fusion, is proposed for enhancement of image. First, a function following the F-stop concept in photography is designed to generate several pseudo images having different intensity. After that, a classified image fusion method, which blends pixels in distinct luminance classes using different fusion functions, is proposed to produce a fused image in which every image region can be properly exposed. Terms â€” Classified exposure image fusion, contrast enhancement, exposure fusion and image fusion. I...|$|E
40|$|A common {{component}} in the mispricing of ETFs can arise from limits to arbitrage. I find strong evidence of commonality among of international country ETFs. Differences in systematic risk between an ETF and its underlying index can explain some of this effect. While non-synchronicity alone cannot explain it, it does amplify the effect. I then investigate whether this commonality can be a channel of contagion between ETFs. Consistent with this hypothesis I find that extreme shocks to financial markets are followed by large changes in the systematic risks of ETFs. Extreme shocks to U. S. market returns or volatility generally amplify the over- and <b>under-exposure</b> to subsequent U. S. and regional market movements that already exists in normal times. These findings imply not only that ETF returns are excessively volatile in comparison to their underlying index, but also that local risk matters for the pricing of these ETFs, particularly in adverse markets...|$|E
30|$|However, as {{in other}} feature-based methods, the ZNCC shows {{meaningful}} differences only for well-contrasted and highly textured regions. Hence, feature-based methods often give incorrect results in low-contrast regions where the pixel values {{are about to be}} saturated due to over- or <b>under-exposure</b> and also in low-textured regions. For these regions, we exploit the IMF between the images, which was successfully used in [13, 14, 24, 25]. In this paper, the IMF is estimated from regions having high ZNCC only, because other regions are saturated or moving object regions that have low credibility in estimating the IMF. Then, the pixels lying outside the IMF tolerance are considered pixels on the faint moving objects. To determine the ghost map in this region, we also develop an optimization technique, which yields less noisy results than conventional IMF-based thresholding methods. Experimental results show that the proposed method constructs plausible ghost maps and hence yields pleasing HDR images without noticeable ghost artifacts.|$|E
40|$|There {{is often}} {{more than one way}} to select tonal {{adjustment}} for a photograph, and different individuals may prefer different adjustments. However, selecting good adjustments is challenging. This thesis describes a method to predict whether a given tonal rendition is acceptable for a photograph, which we use to characterize its range of acceptable adjustments. We gathered a dataset of image "acceptability" over brightness and contrast adjustments. We find that unacceptable renditions can be explained in terms of over-exposure, <b>under-exposure,</b> and low contrast. Based on this observation, we propose a machine-learning algorithm to assess whether an adjusted photograph looks acceptable. We show that our algorithm can differentiate unsightly renditions from reasonable ones. Finally, we describe proof-of-concept applications that use our algorithm to guide the exploration of the possible tonal renditions of a photograph. by Ronnachai Jaroensri. Thesis: S. M., Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, 2015. Cataloged from PDF version of thesis. Includes bibliographical references (pages 57 - 58) ...|$|E
40|$|Abstract â€” In {{our daily}} life, digital cameras and smart phones {{have been widely}} used to take pictures. However, digital cameras and smart phones have a limited dynamic range, which is much lower than that human eyes can perceive. Thus, the {{photographs}} taken in high dynamic range scenes often exhibit <b>under-exposure</b> or over-exposure artifacts in shadow or highlight regions. In this study, an image fusion based approach, called classified virtual exposure image fusion (CVEIF), is proposed for image enhancement. First, a function imitating the F-stop concept in photography is designed to generate several virtual images having different intensity. Then, a classified image fusion method, which blends pixels in distinct luminance classes using different fusion functions, is proposed to produce a fused image in which every image region is well exposed. Experimental results on four different kinds of generic images, including a normal image, a low-contrast images, a backlight image, and a dark scene image, have shown that the proposed CVEIF approach produced more pleasingly enhanced images than other methods 1. Index Terms â€” Classified virtual exposure image fusion, contrast enhancement, exposure fusion, image fusion. I...|$|E
40|$|In {{the field}} of still and moving {{pictures}} digital imaging {{one of the most}} important factors affecting the quality of the acquisi-tion is a correct exposure. This factor is especially important for video surveillance. It is particularly applicable in the case of CCTV cameras often operating in low-light conditions or blinded by an external light source. It is quite common that the lighting conditions are well below the adaptive capacity of a camera. Therefore, it is necessary to continuously monitor the level of exposure to determine image quality. It should be noted that the level of exposure is not expressed using quan-titative parameters of acquisition (being sometimes meaning-less from the perceived quality assessment point of view) but is measured directly on the image. The purpose of the pre-sented research was to develop a No Reference (NR) metric assessing video Quality of Experience (QoE) affected with the exposure distortion. It was presumed that both over- and <b>under-exposure</b> degrade QoE, so the research task was to de-rive a proper mapping function between QoE and exposure. The presented exposure metric is unique and no similar re-search was found in the literature. 1...|$|E
40|$|Single image dehazing {{has been}} a {{challenging}} problem due to its ill-posed nature. While most of the existing single image based dehazing algorithms address this issue by introducing certain assumptions and priors into the haze imaging model, the imaging process of imaging devices has been seldom taken into account, such as white balance and metering. In general, consumer photos are taken with AWB (Auto White Balance). Hence, color temperature in a foggy scenario may not be correctly detected, which results in color distortion; and the whole scene looks brighter, which leads to <b>under-exposure</b> during the imaging process. In this paper, we propose to handle these two issues by applying white balance correction and decomposing an image into two component images, reflex lightness image and ambience illumination image. We devise an improved dark channel prior based algorithm to dehaze the reflex lightness image and the exposure adjustment is estimated from the ambience illumination image. Finally, a high quality haze- free image is produced by refining {{the brightness of the}} preliminarily dehazed image with the estimated exposure adjustment. Experimental results with a benchmark dataset demonstrate that our approach outperforms the state-of-the-art, in terms of contrast and color fidelity. Department of Electronic and Information EngineeringRefereed conference pape...|$|E
40|$|One of the {{important}} challenges {{for the development of}} science communication concerns the current problems with the <b>under-exposure</b> of null results. I suggest that each article published in a top scientific journal can get tagged (online) with attempts to replicate. As such, a future reader of an article will also be able to see whether replications have been attempted and how these turned out. Editors and/or reviewers decide whether a replication is of sound quality. The authors of the main article have the option to review the replication and can provide a supplementary comment with each attempt that is added. After 5 or 10 years, and provided enough attempts to replicate, the authors of the main article get the opportunity to discuss/review their original study in light of the outcomes of the replications. This approach has two important strengths: 1) The approach would provide researchers with the opportunity to show that they deliver scientifically thorough work, but sometimes just fail to replicate the result that others have reported. This can be especially valuable for the career opportunities of promising young researchers; 2) perhaps even more important, the visibility of replications provides an important incentive for researchers to publish findings only if they are sure that their effects are reliable (and thereby reduce the influence of "experimenter degrees of freedom" or even outright fraud). The proposed approach will stimulate researchers to look beyond the point of publication of their studies...|$|E
40|$|Cu is an {{essential}} nutrient for man, but can be toxic if intakes are too high. In sensitive populations, marginal over- or <b>under-exposure</b> can have detrimental effects. Malnourished children, the elderly, and pregnant or lactating females may be susceptible for Cu deficiency. Cu status and exposure in the population can currently not be easily measured, as neither plasma Cu nor plasma cuproenzymes reflect Cu status precisely. Some blood markers (such as ceruloplasmin) indicate severe Cu depletion, but do not inversely respond to Cu excess, and are not suitable to indicate marginal states. A biomarker of Cu is needed that is sensitive to small changes in Cu status, and that responds to Cu excess as well as deficiency. Such a marker will aid in monitoring Cu status in large populations, and will help to avoid chronic health effects (for example, liver damage in chronic toxicity, osteoporosis, loss of collagen stability, or increased susceptibility to infections in deficiency). The advent of high-throughput technologies has enabled us to screen for potential biomarkers in the whole proteome of a cell, not excluding markers that have no direct link to Cu. Further, this screening allows us {{to search for a}} whole group of proteins that, in combination, reflect Cu status. The present review emphasises the need to find sensitive biomarkers for Cu, examines potential markers of Cu status already available, and discusses methods to identify a novel suite of biomarker...|$|E
40|$|Seafood {{consumption}} is worryingly low {{in young children}} in Scotland despite current dietary goals targeting an increase in oily-fish intake and global recommendations to regularly include fish into the diet of infants. The aims {{of this study were}} to investigate the seafood inclusion in information resources and to investigate the beneficiary and cautionary messages for seafood during early years feeding. A review of early years feeding resources issued by Government, National Health Service (NHS), and reputable charities was conducted in March 2013. A list of published resources issued by health visitors in Scotland and searches of Tayside and Grampian NHS libraries, and from the publication websites for NHS Health Scotland, and the Department of Health (DH) were used to identify resources. Non-parametric comparative analysis was conducted to assess the occurrence of beneficiary and cautionary messages made on seafood to other food types (meat, poultry and vegetable). Thirteen early years feeding resources targeted towards parents were identified. Significantly more beneficiary messages were cited for vegetables whilst a significantly higher number of cautionary messages were cited for seafood than other food types. Seafood was the only food type to receive more cautionary than beneficiary claims. The prominence of negative seafood messages may be deterring parents from including seafood into the diet of their infant during early years feeding. The <b>under-exposure</b> to seafood during early years, when taste and food acceptance is developed, may impact on the future acceptance and consumption of this healthful food. Publisher PDFNon peer reviewe...|$|E
40|$|Objectives Aluminum {{step wedge}} (ASW) {{equivalent}} radiodensity (eRD) {{has been used}} to quantify restorative material's radiodensity. The aim {{of this study was to}} evaluate the effects of image acquisition control (IAC) of a digital X-ray system on the radiodensity quantification under different exposure time settings. Materials and Methods Three 1 -mm thick restorative material samples with various opacities were prepared. Samples were radiographed alongside an ASW using one of three digital radiographic modes (linear mapping (L), nonlinear mapping (N), and nonlinear mapping and automatic exposure control activated (E)) under 3 exposure time settings (underexposure, normal-exposure, and overexposure). The ASW eRD of restorative materials, attenuation coefficients and contrasts of ASW, and the correlation coefficient of linear relationship between logarithms of gray-scale value and thicknesses of ASW were compared under 9 conditions. Results The ASW eRD measurements of restorative materials by three digital radiographic modes were statistically different (p = 0. 049) but clinically similar. The relationship between logarithms of background corrected grey scale value and thickness of ASW was highly linear but attenuation coefficients and contrasts varied significantly among 3 radiographic modes. Varying exposure times did not affect ASW eRD significantly. Conclusions Even though different digital radiographic modes induced large variation on attenuation of coefficient and contrast of ASW, E mode improved diagnostic quality of the image significantly under the <b>under-exposure</b> condition by improving contrasts, while maintaining ASW eRDs of restorative materials similar. Under the condition of this study, underexposure time may be acceptable clinically with digital X-ray system using automatic gain control that reduces radiation exposure for patient...|$|E
30|$|This paper {{presents}} an algorithm for compositing a {{high dynamic range}} (HDR) image from multi-exposure images, considering inconsistent pixels for the reduction of ghost artifacts. In HDR images, ghost artifacts may appear when there are moving objects while taking multiple images with different exposures. To prevent such artifacts, {{it is important to}} detect inconsistent pixels caused by moving objects in consecutive frames and then to assign zero weights to the corresponding pixels in the fusion process. This problem is formulated as a binary labeling problem based on a Markov random field (MRF) framework, the solution of which is a binary map for each exposure image, which identifies the pixels to be excluded in the fusion process. To obtain the ghost map, the distribution of zero-mean normalized cross-correlation (ZNCC) of an image with respect to the reference frame is modeled as a mixture of Gaussian functions, and the parameters of this function are used to design the energy function. However, this method does not well detect faint objects that are in low-contrast regions due to over- or <b>under-exposure,</b> because the ZNCC does not show much difference in such areas. Hence, we obtain an additional ghost map for the low-contrast regions, based on the intensity relationship between the frames. Specifically, the intensity mapping function (IMF) between the frames is estimated using pixels from high-contrast regions without inconsistent pixels, and pixels out of the tolerance range of the IMF are considered moving pixels in the low-contrast regions. As a result, inconsistent pixels in both the low- and high-contrast areas are well found, and thus, HDR images without noticeable ghosts can be obtained.|$|E
40|$|Background: SSG&PM over 17 days is {{recommended}} as {{first line treatment}} for visceral leishmaniasis in eastern Africa, but is painful and requires hospitalization. Combination regimens including AmBisome and miltefosine are safe and effective in India, {{but there are no}} published data from trials of combination therapies including these drugs from Africa. Methods: A phase II open-label, non-comparative randomized trial was conducted in Sudan and Kenya to evaluate the efficacy and safety of three treatment regimens: 10 mg/kg single dose AmBisome plus 10 days of SSG (20 mg/kg/day), 10 mg/kg single dose AmBisome plus 10 days of miltefosine (2. 5 mg/kg/day) and miltefosine alone (2. 5 mg/kg/day for 28 days). The primary endpoint was initial parasitological cure at Day 28, and secondary endpoints included definitive cure at Day 210, and pharmacokinetic (miltefosine) and pharmacodynamic assessments. Results: In sequential analyses with 49 â€“ 51 patients per arm, initial cure was 85 % (95 % CI: 73 â€“ 92) in all arms. At D 210, definitive cure was 87 % (95 % CI: 77 â€“ 97) for AmBisome + SSG, 77 % (95 % CI 64 â€“ 90) for AmBisome + miltefosine and 72 % (95 % CI 60 â€“ 85) for miltefosine alone, with lower efficacy in younger patients, who weigh less. Miltefosine pharmacokinetic data indicated <b>under-exposure</b> in children compared to adults. Conclusion: No major safety concerns were identified, but point estimates of definitive cure were less than 90 % for each regimen so none will be evaluated in Phase III trials in their current form. Allometric dosing of miltefosine in children needs to be evaluated. Trial Registration: The study was registered with ClinicalTrials. gov, number NCT 0106744...|$|E
40|$|Background and Aims <b>Under-exposure</b> to seafood {{during early}} years feeding, when taste and food {{acceptance}} is developed, may {{impact on the}} future development of a healthy diet. The {{aim of this study}} was to investigate the inclusion of seafood in commercial baby food products and baby and toddler cookbooks, and the occurrence of beneficiary and cautionary information on seafood in the cookbooks. Methods A survey was conducted of all commercial pre-prepared baby food main-meal products in Scotland from September-December 2012. The primary food type within each product, (vegetables, poultry, meat, and seafood), nutritional composition, and ingredient contribution were collected. A survey of Amazonâ€™s top 20 best-selling baby and toddler cookbooks was conducted in June 2013. The types and varieties of the different food types cited in addition to recipes, beneficiary claims and cautionary information was recorded. Results Seafood (n= 13 (3. 8 %)) was significantly underrepresented as a main-meal product compared to poultry (103 (30. 2 %)), meat (121 (35. 5 %)) and vegetables (104 (30. 5 %)). Similarly, seafood-based main-meal recipes were significantly lower than vegetable recipes however were not significantly different to poultry and meat recipes. Cautionary claims in the cookbooks were significantly higher for seafood than other food types. Conclusions Parents who predominantly wean their infant using commercial products are may face challenges in sourcing a suitable range of products to enable the inclusion of seafood. Parents who predominantly home-cook have greater exposure to seafood in recipes however, this may be counteracted by the prominence of negative seafood messages, deterring them from including this healthful food into the diet of their infant. Publisher PDFNon peer reviewe...|$|E
40|$|SSG&PM over 17 days is {{recommended}} as {{first line treatment}} for visceral leishmaniasis in eastern Africa, but is painful and requires hospitalization. Combination regimens including AmBisome and miltefosine are safe and effective in India, {{but there are no}} published data from trials of combination therapies including these drugs from Africa. A phase II open-label, non-comparative randomized trial was conducted in Sudan and Kenya to evaluate the efficacy and safety of three treatment regimens: 10 mg/kg single dose AmBisome plus 10 days of SSG (20 mg/kg/day), 10 mg/kg single dose AmBisome plus 10 days of miltefosine (2. 5 mg/kg/day) and miltefosine alone (2. 5 mg/kg/day for 28 days). The primary endpoint was initial parasitological cure at Day 28, and secondary endpoints included definitive cure at Day 210, and pharmacokinetic (miltefosine) and pharmacodynamic assessments. In sequential analyses with 49 - 51 patients per arm, initial cure was 85 % (95 % CI: 73 - 92) in all arms. At D 210, definitive cure was 87 % (95 % CI: 77 - 97) for AmBisome + SSG, 77 % (95 % CI 64 - 90) for AmBisome + miltefosine and 72 % (95 % CI 60 - 85) for miltefosine alone, with lower efficacy in younger patients, who weigh less. Miltefosine pharmacokinetic data indicated <b>under-exposure</b> in children compared to adults. No major safety concerns were identified, but point estimates of definitive cure were less than 90 % for each regimen so none will be evaluated in Phase III trials in their current form. Allometric dosing of miltefosine in children needs to be evaluated. The study was registered with ClinicalTrials. gov, number NCT 01067443...|$|E
30|$|We have {{proposed}} an HDR image fusion algorithm with reduced ghost artifacts, by detecting inconsistent pixels in the high-contrast and low-contrast regions separately. To detect inconsistent pixels in high-contrast areas, a ZNCC measure is used {{based on the}} observation that the ZNCC histogram displays a unimodal distribution in static regions, whereas it has a multimodal shape in dynamic regions. A cost function based on the parameters of these probability distributions is designed, whose minimization yields the ghost map for the highly contrasted region. To detect the ghost map in the low-contrast region, the IMF is first estimated using pixels from the high-contrast regions having no moving objects. Next, a cost function that encodes the IMF compliance of the pixel pairs is designed, whose minimization gives the ghost map for the low-contrast areas. The overall ghost map {{is defined as the}} logical operation of these two maps, and the ghost pixels are excluded from the fusion process. Since the proposed algorithm can find faint moving objects in areas where the pixel values are about to be saturated due to over- and <b>under-exposure,</b> it provides satisfactory HDR outputs with no noticeable ghost artifacts. However, the proposed method has limitations in correcting moving foreground object when it is saturated in the reference frame (FigureÂ  15), because they are simply excluded from the fusion process. In this case, we have to manually select a reference frame that has well-exposed foreground objects, which can degrade the fusion results due to the narrower well-exposed background region than in the reference. Otherwise, we need to correct the inconsistent pixels instead of simply excluding them, which is a very challenging problem, especially when the moving foreground object is not consistently detected in each frame due to saturation, noise, or non-rigid motion.|$|E
