17|130|Public
5000|$|The {{previous}} paragraph can {{be generalized}} to any variance: given a variable (such as an <b>unbiased</b> <b>error</b> variable) , evaluating the error function at [...] describes the probability of &epsilon; falling in the range x. This is used in statistics to predict behavior of any sample {{with respect to the}} population mean. This usage is similar to the Q-function, which in fact can be written in terms of the error function.|$|E
5000|$|In statistics, it {{is common}} to have a {{variable}} [...] and its unbiased estimator [...] The error is then defined as [...] This makes the error a normally distributed random variable with mean 0 (because the estimator is unbiased) and some variance this is written as [...] For the case where , i.e. an <b>unbiased</b> <b>error</b> variable , erf(x) describes the probability of the error &epsilon; falling in the range x; in other words, the probability that the absolute error is no greater than x. This is true for any random variable with distribution but the application to error variables is how the error function got its name.|$|E
30|$|For future work, {{additional}} {{learning machine}} algorithms and data sets should be investigated. First, {{we would like}} to integrate new recommended techniques to the framework. After that, parameter tuning should be implemented automatically in the framework for each learning algorithm in order to improve the performance. The framework could also be enhanced by automating additional statistical analysis {{as a part of the}} evaluation process. Finally, novel <b>unbiased</b> <b>error</b> metric, such as standardized accuracy (SA), should be used for comparison of models.|$|E
30|$|We use the {{following}} approach: For a dataset with n occupancy strings, each cell of an error matrix depicts the Hamming Distance between any two occupancy strings – {{the number of}} mismatching characters (Hamming 1950). To normalize, we divide value in each cell by the length of occupancy string. The error matrix is a symmetric matrix of size n 2 which helps in systematically injecting <b>unbiased</b> <b>errors</b> in the occupancy data.|$|R
30|$|The delta {{algorithm}} provides superior results regarding improved accuracy, reduced variability, <b>unbiased</b> <b>errors,</b> reduced complexity, {{and fast}} computation. The solution is lightweight {{and can be}} implemented in existing SQL query language or similar code {{without the need for}} external software packages. Additionally, the solution is designed {{in a manner that is}} potentially highly scalable allowing adaptation to numerous applications of anomaly detection, such as outage estimation, outage impact, billing reconciliation, and record validation. The solution’s measured performance relative to the performance of well-known ARIMA models provides evidence of the solution’s ability to provide high accuracy with minimal computing overhead.|$|R
5000|$|... #Caption: Expected {{error in}} the mean of A for a sample of n data points with sample bias {{coefficient}} ρ. The <b>unbiased</b> standard <b>error</b> plots as the ρ=0 diagonal line with log-log slope -½.|$|R
40|$|We {{consider}} a parameter choice method (called the minimum bound method) for regularization of linear ill-posed problems {{that was developed}} by Raus and Gfrerer for the case with continuous, deterministic data. The method is adapted and analysed in a discrete, stochastic framework. It is shown that asymptotically, {{as the number of}} data points approaches infinity, the method (with a constant set to 2) behaves like an <b>unbiased</b> <b>error</b> method, which selects the parameter by minimizing a certain unbiased estimate of the expected squared error in the regularized solution. The method is also shown to be weakly asymptotically optimal, in that the 'expected' estimate achieves the optimal rate of convergence with repect to the expected squared error criterion and it has the optimal rate of decay...|$|E
40|$|In multivariate {{regression}} {{problems with}} multiple responses, there often exist unobserved covariates which are {{correlated with the}} responses. It is possible to estimate these covariates via factor analytic methods, but calculating <b>unbiased</b> <b>error</b> variance estimates after adjusting for latent factors requires assigning appropriate degrees of freedom to the estimated factors. Many ad-hoc solutions to this problem have been proposed without the backup of a careful theoretical analysis. Using recent results from random matrix theory, we derive an expression for degrees of freedom. Our estimate gives a principled alternative to ad-hoc approaches in common use. Extensive simulation results show excellent agreement between the proposed estimator and its theoretical value. When we apply the methods to a microarray dataset, with 2 estimated latent factors, our estimate assigns between 2. 18 and 2. 99 degrees of freedom, depending on which response is under consideration...|$|E
40|$|This study {{compared}} 4, 594 {{student responses}} {{from three different}} sur,-eys of incoming students at the University of South Florida (USF) with data from Florida's State University System (SUS) admissions files to determine what proportion of error occurs in the survey responses. Specifically, the study investigated the amount of measurement error in student responses to questions about application and admission activities to universities {{other than the one}} at which they were enrolling. A literature review is included that examines the problem of measurement error in other studies that used survey or self-report measures. The study found that considerable measurement error can exist in self-report measures even when a subject is reporting simple factual information; in this case the level of <b>unbiased</b> <b>error</b> was about 4 percent, and biased error was about 20 percent. The amount of error was directly proportional t...|$|E
40|$|Abstract—Wireless Sensor Networks {{are well}} suited for track-ing targets {{carrying}} RFID tags in indoor environments. Tracking based on the received signal strength indication (RSSI) {{is by far the}} cheapest and simplest option, but suffers from secular biases due to effects of multi-path, occlusions and decalibration, as well as large <b>unbiased</b> <b>errors</b> due to measurement noise. We propose a novel algorithm that solves these problems in a distributed, scalable and power-efficient manner. Firstly, our proposal includes a tandem incremental estimator that learns and tracks the radio environment of the network, and provides this knowledge {{for the use of the}} tracking algorithm, which eliminates the secular biases due to radio occlusions etc. Secondly, we reduce the <b>unbiased</b> tracking <b>error</b> by exploiting the co-dependencies in the motion of several targets (as in crowds or herds) via a fully distributed and tractable particle filter. We thereby extract a significant “diversity gain ” while still allowing the network to scale seamlessly to a large tracking area. In particular, we avoid the pitfalls of network congestion and severely shortened battery lifetimes that plague procedures based on the joint multi-target probability density...|$|R
30|$|The {{training}} algorithm {{of random}} forest is called bootstrap algorithm or bagging technique to select n feature randomly from m features, i.e. to create random samples, this model trains the new sample to out of bag sample(1 / 3 rd of the data) {{used to determine}} the <b>unbiased</b> OOB <b>error.</b>|$|R
40|$|Estimating the {{generalization}} error {{is one of}} the key ingredients of supervised learning since a good generalization error estimator can be used for model selection. An <b>unbiased</b> generalization <b>error</b> estimator called the subspace information criterion (SIC) is shown to be useful for model selection, but its range of application is limited to linear learning methods. In this paper, we extend SIC to be applicable to non-linear learning...|$|R
40|$|In {{this paper}} the {{performance}} of different information criteria for simultaneous model class and lag order selection is evaluated using simulation studies. We focus {{on the ability of}} the criteria to distinguish linear and nonlinear models. In the simulation studies, we consider three different versions of the commonly known criteria AIC, SIC and AICc. In addition, we also assess {{the performance of}} WIC and evaluate the impact of the error term variance estimator. Our results confirm the findings of different authors that AIC and AICc favor nonlinear over linear models, whereas weighted versions of WIC and all versions of SIC are able to successfully distinguish linear and nonlinear models. However, the discrimination between different nonlinear model classes is more difficult. Nevertheless, the lag order selection is reliable. In general, information criteria involving the <b>unbiased</b> <b>error</b> term variance estimator overfit less and should be preferred to using the usual ML estimator of the error term variance. © 2016 by De Gruyter...|$|E
40|$|Abstract: Almost {{everywhere}} {{unique identifiers}} like keys {{are required for}} joining data in databases or information systems. If keys are absent or corrupted the supplement of data extracted from different sources is difficult. The question is: Does the data contained in a single record belong to another record, or not? This leads to a classification problem {{with at least two}} classes: identical and not identical. Classifying pairs of records needs some preprocessing. The first step is to detect suitable common properties of the different sources. Secondly, to allow comparisons the values of the records are transformed to this common properties. Finally, the classification is performed on an almost finite subset R ⊂ IR k, the range of an appropriate comparison function. Given two data sets, a random sample of pairs is used for detecting similarities, rules or classification criteria. Different classification techniques can be applied to classify the pairs of the sources in order to link them or not. <b>Unbiased</b> <b>error</b> rates can be estimated by cross validation. The procedure is illustrated by an example of a library database from the internet. ...|$|E
40|$|Several {{prominent}} {{methods have}} been developed for the crucial selection of the parameter in regularization of linear ill-posed problems with discrete, noisy data. The discrepancy principle (DP), minimum bound (MB) method and generalized cross-validation (GCV) are known to be at least weakly asymptotically optimal with respect to appropriate loss functions as the number n of data points approaches infinity. We compare these methods in three other ways. First, n is taken to be fixed and. using a discrete Picard condition, upper and lower bounds on the 'expected' DP and MB estimates are derived in terms of the optimal parameters with respect to the risk and expected error. Next, we define a simple measure of the variability of a practical estimate and. for each of the five methods, determine its asymptotic behaviour. The results are that the asymptotic stability of GCV is the same as for the unbiased risk method and is superior to that of DP, which is better than for MB and an <b>unbiased</b> <b>error</b> method. Finally, the results of numerical simulations of the five methods demonstrate that the theoretical conclusions hold in practice...|$|E
40|$|Abstract. A {{well-known}} result by Stein {{shows that}} regularized estimators with small bias often yield better estimates than unbiased estimators. In this paper, we adapt this spirit to model selection, and propose regularizing <b>unbiased</b> generalization <b>error</b> estimators for stabilization. We trade a small bias {{in a model}} selection criterion against a larger variance reduction which has the beneficial effect of being more precise on a single training set. ...|$|R
5000|$|... #Caption: For a {{value that}} is sampled with an <b>unbiased</b> {{normally}} distributed <b>error,</b> the above depicts {{the proportion of}} samples that would fall between 0, 1, 2, and 3 standard deviations {{above and below the}} actual value.|$|R
40|$|The optimal state {{estimator}} for linear systems {{described by}} functional differential equations is constructed. The problem is solved by deriving equations for the <b>unbiased</b> estimation <b>error</b> {{as well as}} for its covariance. A functional of the estimation error at the terminal time is minimized by an optimal choice of the gain matrices of the estimator. It is accomplished by the application of the matrix maximum principle. The applicability of the optimal estimator is illustrated by an example...|$|R
40|$|Instabilities {{and long}} term shifts in seasons, whether induced by natural drivers or human activities, pose great {{disruptive}} threats to ecological, agricultural, and social systems. Here, we propose, measure, and explore two fundamental markers of location-sensitive seasonal variations: the Summer and Winter Teletherms [...] -the on-average annual dates of the hottest and coldest days of the year. We analyse daily temperature extremes recorded at 1218 stations across the contiguous United States from 1853 [...] 2012, and observe large regional variation with the Summer Teletherm falling up to 90 days after the Summer Solstice, and 50 days for the Winter Teletherm after the Winter Solstice. We show that Teletherm temporal dynamics are substantive with clear {{and in some cases}} dramatic shifts reflective of system bifurcations. We also compare recorded daily temperature extremes with output from two regional climate models finding considerable though relatively <b>unbiased</b> <b>error.</b> Our work demonstrates that Teletherms are an intuitive, powerful, and statistically sound measure of local climate change, and that they pose detailed, stringent challenges for future theoretical and computational models. Comment: Manuscript: 13 pages, 8 Figures; Supplementary: 19 pages, 21 Figure...|$|E
40|$|Growth of {{the leopard}} grouper, Mycteroperca rosacea (Streets, 1877), was {{analyzed}} {{in its natural}} habitat. Age determination {{was based on the}} reading of otoliths, and the method was validated under three main criteria: (1) proportionality, (2) seasonality, and (3) concordance with another method. Otolith growth is proportional to organism growth, with a slight degree of allometry, and the otolith registers the growth of the individual, even at advanced ages. The opaque growth zone in the otolith is deposited once a year, between July and October. Thus, taken together, one opaque and one hyaline mark represent an annual cycle. Back-calculated lengths-at-age agreed reasonably well with observed lengths-at-age at the time of capture, considering that back-calculated lengths represent an exact age (birthday), and observed lengths are taken at an intermediate age between birthdays. Fish length and otolith age data were fitted to the von Bertalanffy growth function by two methods: (1) linear regression (Ford-Walford and Beverton), using transformed data, and (2) nonlinear regression, by iteration. Although the nonlinear regression gave a fit with <b>unbiased</b> <b>error,</b> parameters resulting from linear regressions had a better biological meaning for the species. The resulting parameters were compared with those reported for other species of the family Serranidae...|$|E
40|$|We have {{developed}} novel molecular methods using a stool sample, which contains intact sloughed colon cells, to quantify colonic gene expression profiles. In this study, {{our goal was}} to identify diagnostic gene sets (combinations) for the noninvasive classification of different phenotypes. For this purpose, the effects of a legume-enriched, low glycemic index, high fermentable fiber diet was evaluated in subjects with four possible combinations of risk factors, including insulin resistance and a history of adenomatous polyps. In a randomized crossover design controlled feeding study, each participant (a total of 23; 5 – 12 per group) consumed the experimental diet (1. 5 cups of cooked dry beans) and a control diet (isocaloric average American diet) for 4 weeks with a 3 -week washout period between diets. Using prior biological knowledge, the complexity of feature selection was reduced to perform an exhaustive search on all allowable feature (gene) sets of size 3, and among these, 27 had (<b>unbiased)</b> <b>error</b> estimates of 0. 15 or less. Linear discriminant analysis was successfully used to identify the best single genes and two- to three-gene combinations for distinguishing subjects with insulin resistance, a history of polyps, or exposure to a chemoprotective legumerich diet. These results support our premise that gene products (RNA) isolated from stool have diagnostic value in terms of assessing colon cancer risk...|$|E
40|$|An error {{analysis}} for mesospheric profiles retrieved from absorptive occultation data has been performed, starting with realistic error assumptions as {{would apply to}} intensity data collected by available high-precision UV photodiode sensors. Propagation of statistical errors was investigated through the complete retrieval chain from measured intensity profiles to atmospheric density, pressure, and temperature profiles. We assumed <b>unbiased</b> <b>errors</b> as the occultation method is essentially self-calibrating and straight-line propagation of occulted signals as we focus on heights of 50 – 100 km, where refractive bending of the sensed radiation is negligible. Throughout the analysis the errors were characterized at each retrieval step by their mean profile, their covariance matrix and their probability density function (pdf). This furnishes, compared to a variance-only estimation, a much improved insight into the error propagation mechanism. We applied the procedure to a baseline analysis {{of the performance of}} a recently proposed solar UV occultation sensor (SMAS – Sun Monitor and Atmospheric Sounder) and provide, using a reasonable exponential atmospheric model as background, results on error standard deviations and error correlation functions of density, pressure, and temperature profiles. Two different sensor photodiode assumptions are discussed, respectively, diamond diodes (DD) with 0. 03...|$|R
30|$|In {{line with}} the {{findings}} presented above we hypothesized {{that there should be}} method effects associated with negatively worded items. Thus, achieving a model with an acceptable goodness of fit, <b>unbiased</b> standard <b>errors</b> and parameter estimates would require controlling for the method effects {{associated with the use of}} negatively worded items in our data set. We also expect to find a relationship between the country-level model fit due to method effect associated with the negatively worded items and country-level performance on assessment.|$|R
40|$|Abstract. Systematic {{approach}} for {{the transmission line}} positive sequence parameters, temperature, and sag based on wavelet analysis to detect error is developed in this work. <b>Unbiased</b> (random/Gaussian) <b>error</b> such as, transient meter failures, transient meter malfunction, and measurements captured during system transients, are inherently {{in the form of}} large abrupt change of short duration in a measurement-sequence. These should be detected before the data is used because their presence will lead to insecure and unstable of power grid. The test results of the proposed method based on data of Sichuan power grid are presented...|$|R
40|$|In the AGEMAP {{genomics}} study, {{researchers were}} interested in detecting genes related to age {{in a variety of}} tissue types. After not finding many age-related genes in some of the analyzed tissue types, the study was criticized for having low power. It is possible that the low power is due to the presence of important unmeasured variables, and indeed we find that a latent factor model appears to explain substantial variability not captured by measured covariates. We propose including the estimated latent factors in a multiple regression model. The key difficulty in doing so is assigning appropriate degrees of freedom to the estimated factors to obtain <b>unbiased</b> <b>error</b> variance estimators and enable valid hypothesis testing. When the number of responses is large relative to the sample size, treating the estimated factors like observed covariates leads to a downward bias in the variance estimates. Many ad-hoc solutions to this problem have been proposed in the literature without the backup of a careful theoretical analysis. Using recent results from random matrix theory, we derive a simple, easy to use expression for degrees of freedom. Our estimate gives a principled alternative to ad-hoc approaches in common use. Extensive simulation results show excellent agreement between the proposed estimator and its theoretical value. Applying our methodology to the AGEMAP genomics study, we found an order of magnitude {{increase in the number of}} significant genes. Although we focus on the AGEMAP study, the methods developed in this paper are widely applicable to other multivariate models, and thus are of independent interest. Comment: 34 pages, 8 figures; includes supplementary materia...|$|E
40|$|In this study, the bioelectrical {{impedance}} analysis (BIA), skinfold thickness measurement (STM) and dual-energy X-ray absorptiometry (DXA), as {{a reference}} method, were compared {{with each other in}} the assessment of body composition in elderly (62 - 72 year-old) Finnish women (n= 93). BIA had better agreement with DXA in the assessment of fat free mass (FFM, R^ 2 = 0. 70, Sres= 2. 1) and fat mass (FM, R^ 2 = 0. 93, Sres= 2. 3) than the STM (FFM, R^ 2 = 0. 62, Sres= 2. 4; FM, R^ 2 = 0. 89,Sres= 2. 8). There was quite a large variation in the estimates when different BIA prediction equations were used. The equation developed in this study, FFM (kg) =- 128. 06 + 1. 85 x BMI- 0. 63 x weight + 1. 07 x height - 0. 03 x resistance + 10. 0 x waist-hip ratio, yielded a small and <b>unbiased</b> <b>error</b> (0. 5 &# 0177; 1. 6 kg), with a small residual standard deviation (R^ 2 = 0. 83, Sres= 1. 6). However, error associated with the estimate of FM was positively related to the degree of FM. BIA(Heitmann) equation yielded unbiased estimates of both FFM and FM (FFM, R^ 2 = 0. 77, Sres= 1. 8; FM, R^ 2 = 0. 95, Sres= 1. 9). Both the STM and BIA (manufacturer's equation) resulted in error which was statistically significant and positively correlated with FFM and FM. These results indicate that BIA prediction equations, chosen with care, can improve the performance of equations based upon anthropometric measurements alone in the assessment of body composition in elderly women...|$|E
30|$|Despite the advantages, SVMs models {{lack the}} {{capability}} of detecting the contributing factors {{and the use of}} all the variables as input makes the estimation inefficient. As suggested by Yu et al. [17], variable selection procedure is needed prior to the SVMs estimation. Meanwhile, by selecting variables it is able to solve the over-fitting issues. Hence, random forest was employed to select the contributing factors, as it is well known for selecting significant contributing variables from a set of factors [27]. The strategy of random forest is that every tree is built with several factors, so a particular tree grows from a bootstrap aggregate sample, part of the cases is discarded and they will not be used {{in the development of the}} trees. The left-out cases are called Out-Of-Bag (OOB) data. The OOB data turn to validate the built trees with an <b>unbiased</b> <b>error</b> estimate as well as the important level estimations of variables. To test whether the attempted numbers of trees are sufficient to reach relatively stable results, the plot of OOB error rate against various tree numbers is developed. The optimal number of trees is the one having the minimum OOB error rate along with a nearly constant error rate. A wrapper MATLAB file interface to C code used in R package random forest [28] was employed to select the contributing factors. The tool provides the ‘mean decrease in Gini index’ method to select contributing variables. A higher magnitude implies a higher variable importance. Hassan et al. [27] chose several variables with higher scores (approximately 50 % of the scores for all variables in total) than the remaining variables. In this paper, the variables which score higher than the mean score of all the variables were selected for the modeling process.|$|E
40|$|We {{extend the}} scope of the corrected-score method studied by Nakamura (1990) and Stefanski (1989) to a large class of {{generalized}} linear measurement error models. Example applications to rare-event logistic regression and extreme-value binary regression are presented. Bernoulli number Extreme-value distribution Generalized linear model Logistic regression Measurement <b>error</b> <b>Unbiased</b> score...|$|R
40|$|This note {{concerns}} kernel density estimation at a point. It {{is shown}} that under {{a wide variety}} of circumstances there exist non-zero bandwidths which annihilate the bias in such estimation. Properties of data-driven bandwidth selectors, estimating bias annihilating bandwidths, are discussed. Bandwidth selection Local smoothing Mean squared <b>error</b> <b>Unbiased</b> estimation...|$|R
40|$|Abstract:Diagnosis of {{diseases}} from the database available {{is one of}} the vital and intricate jobs in medicine. With the advent of time, people {{are becoming more and more}} vulnerable to several diseases due to several reasons. One of the most frequently found disease all across the globe is the heart disease. Almost 60 % of world population become victim of this disease. In this paper, we are trying to find the most probable factors that may be responsible for a person suffering from heart disease. The whole process of data mining is being carried out on the data available for the patients suffering from heart disease. Rattle data mining tool is being used for performing the tasks of analyzing the data of the heart patients. The data is being partitioned into training and testing datasets. The next steps namely clustering and modeling is performed on the training datasets. The testing dataset is used to obtain the <b>unbiased</b> <b>errors.</b> We also find out the correlation of the attributes being used in the present study. After finding the relationship of several attributes of the datasets of the heart patients we give a detailed explanation through the use of rattle data mining tool. Finally, the optimal heart parameters related to heart problem are found out for quick and correct diagnosis...|$|R
40|$|Abstract: Our work in {{this paper}} is {{motivated}} by an elementary but also fundamental and highly practical observation – that uncertainty in constructing a network graph Ĝ, as an approximation (or estimate) of some true graph G, manifests as errors in the status of (non) edges that must necessarily propagate to any summaries η(G) we seek. Mimicking the common practice of using plug-in estimates η(Ĝ) as proxies for η(G), {{our goal is to}} characterize the distribution of the discrepencyD = η(Ĝ) −η(G), in the specific case where η(·) is a subgraph count. In the empirically relevant setting of large, sparse graphs with low-rate measurement errors, we demonstrate under an independent and <b>unbiased</b> <b>error</b> model and for the specific case of counting edges that a Poisson-like regime maintains. Specifically, we show that the appropriate limiting distribution is a Skellam distribution, rather than a normal distribution. Next, because dependent errors typically can be expected when counting subgraphs in practice, either {{at the level of the}} edges themselves or due to overlap among subgraphs, we develop a parallel formalism for using the Skellam distribution in such cases. In particular, using Stein’s method, we present a series of results leading to the quantification of the accuracy with which the difference of two sums of dependent Bernoulli random variables may be approximated by a Skellam. This formulation is general and likely of some independent interest. We then illustrate the use of these results in our original context of subgraph counts, where we examine (i) the case of counting edges, under a simple dependent error model, and (ii) the case of counting chains of length 2 under an independent error model. We finish with a discussion of various open problems raised by our work...|$|E
40|$|The Weapon's Identification Task (WIT; Payne, 2001) {{was used}} in {{conjunction}} with electroencephalographic recording to examine neural responses to racially biased versus <b>unbiased</b> <b>errors</b> in a paradigm pioneered by Amodio et al. (2004). The task was extended to examine four different kinds of stereotypes: Black-negative, Black-positive, Asian-negative, and Asian-positive. This extension was more effective for Black than Asian stereotypes; reaction time and accuracy data confirmed stereotype facilitation effects for the Black faces only. The predicted effect of a heightened response to racially biased <b>errors</b> relative to <b>unbiased</b> ones (first reported by Amodio et al., 2004) was not found: we did not see a larger-magnitude error-related negativity (ERN), an event-related potential (ERP) component, following the commission of racially biased errors. However, the data revealed consistent effects of racial information on responding, including differences in neural responses to minority versus White face primes and differences between Black and Asian blocks. A follow-up analysis of the Black blocks revealed an interaction that reached significance in the opposite direction of the ERN effect found in Amodio et al. (2004; 2006; 2008). Taken together, our results did not replicate the "Amodio Effect," but did demonstrate the sensitivity of the ERN to racial information. Hyper-monitoring in the presence of Black face primes and hypersensitivity to errors on Black face trials are discussed as possible explanations for our findings...|$|R
40|$|International audienceIn this paper, {{a unified}} H_∞ dynamic {{observer}} (UO) is proposed {{for a class}} of linear systems {{in the presence of}} unknown inputs and disturbances. It generalizes the existing results on the proportional observer (PO), the proportional integral observer (PIO) and the dynamic observer (DO). The design approach of the UO is derived from the solution of the linear matrix inequality (LMI), based on the new dynamic observer formulation and the solution of the algebraic constraints obtained from the <b>unbiased</b> estimation <b>error.</b> A numerical example is provided to show the applicability and the performance of our results...|$|R
40|$|A {{proportion}} estimation {{procedure is}} presented which requires only on set of ground truth data {{for determining the}} error matrix. The error matrix is then used to determine an <b>unbiased</b> estimate. The <b>error</b> matrix is shown to be {{directly related to the}} probability of misclassifications, and is more diagonally dominant with {{the increase in the number}} of passes used...|$|R
40|$|We {{report on}} SEE and TID tests of highly scaled Samsung 2 Gbits flash memories. Both in-situ and biased {{interval}} irradiations {{were used to}} characterize {{the response of the}} total accumulated dose failures. The radiation-induced failures can be categorized as followings: single event upset (SEU) read errors in biased and <b>unbiased</b> modes, write <b>errors,</b> and single-event-functional-interrupt (SEFI) failures...|$|R
40|$|A QR-decomposition based {{algorithm}} is presented for <b>unbiased,</b> equation <b>error</b> adaptive IIR filtering. The {{algorithm is}} based on casting the adaptive IIR filtering in a mixed Least Squares - Total Least Squares (LS-TLS) framework. This formulation is shown to be equivalent to the minimization of the mean-square equation error subject to a unit norm constraint on the denominator parameter vector. An efficient implementation of the mixed LS-TLS solution is achieved {{through the use of}} back substitution and inverse iteration. Unbiasedness of the system parameter estimates is established for the mixed LS-TLS solution in the case of uncorrelated output noise, and the algorithm is shown to converge to this solution...|$|R
