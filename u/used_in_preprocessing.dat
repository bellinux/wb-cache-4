19|10000|Public
50|$|Anomaly {{detection}} {{is applicable}} {{in a variety}} of domains, such as intrusion detection, fraud detection, fault detection, system health monitoring, event detection in sensor networks, and detecting Eco-system disturbances. It is often <b>used</b> <b>in</b> <b>preprocessing</b> to remove anomalous data from the dataset. In supervised learning, removing the anomalous data from the dataset often results in a statistically significant increase in accuracy.|$|E
40|$|Biometric {{helps in}} {{identifying}} the individual's physiological also behavioral attributes. In the currentbiometric system fingerprint is very well known one. As a means of access control and criminal identification,Automated Fingerprint Identification Systems (AFIS) are widely utilized. The objective of this review paper is tosummarize the well- known approaches <b>used</b> <b>in</b> <b>Preprocessing,</b> Minutiae Extraction and Post processing ofFingerprint systems...|$|E
40|$|Abstract — In {{this review}} paper, it is planned {{to review and}} compare the {{different}} methods of diagnosing brain tumor through MRI <b>used</b> <b>in</b> <b>preprocessing</b> and segmentation techniques. In preprocessing and enhancement stage is used to eliminate the noise and high frequency components from MRI image. In this paper, various Preprocessing and Enhancement Technique, Segmentation Algorithm and their performance have been studied and compared...|$|E
40|$|In {{this paper}} {{we present a}} {{quantitative}} comparisons of different independent component analysis (ICA) algorithms in order to investigate their potential <b>use</b> <b>in</b> <b>preprocessing</b> (such as noise reduction and feature extraction) the electroencephalogram (EEG) data for early detection of Alzhemier disease (AD) or discrimination between AD (or mild cognitive impairment, MCI) and age-match control subjects...|$|R
40|$|We {{describe}} many {{vantage points}} on the Baire metric and its <b>use</b> <b>in</b> clustering data, or its <b>use</b> <b>in</b> <b>preprocessing</b> and structuring data {{in order to support}} search and retrieval operations. In some cases, we proceed directly to clusters and do not directly determine the distances. We show how a hierarchical clustering can be read directly from one pass through the data. We offer insights also on practical implications of precision of data measurement. As a mechanism for treating multidimensional data, including very high dimensional data, we use random projections. Comment: 17 pages, 45 citations, 2 figure...|$|R
40|$|This paper {{presents}} an evaluation framework for coreference resolution geared towards interpretability for higher-level applications. Three application scenarios for coreference resolution are outlined and metrics for them are devised. The metrics provide detailed system analysis and aim at measuring the potential benefit of <b>using</b> coreference systems <b>in</b> <b>preprocessing...</b>|$|R
40|$|A {{variable}} elimination rule {{allows the}} polynomialtime identification of certain variables whose elimination {{does not affect}} the satisfiability of an instance. Variable elimination in the constraint satisfaction problem (CSP) can be <b>used</b> <b>in</b> <b>preprocessing</b> or during search to reduce search space size. We show that there are essentially just four variable elimination rules defined by forbidding generic sub-instances, known as irreducible patterns, in arc-consistent CSP instances. One of these rules is the Broken Triangle Property, whereas the other three are novel...|$|E
30|$|COD-CLARANS (clustering with {{obstructed}} distance {{based on}} CLARANS) (Tung et al. 2001) {{was the first}} clustering algorithm that takes into consideration the presence of obstacle entities. Although COD-CLARANS generates good clustering results, there are several major problems with this algorithm. Since COD-CLARANS {{is an extension of}} CLARANS algorithm, it suffers from similar drawbacks as CLARANS. In addition, COD-CLARANS cannot handle outliers. Also the overall efficiency of the algorithm is very low because the model <b>used</b> <b>in</b> <b>preprocessing</b> for determining visibility and building the spatial join index would need to be significantly changed. Third, if the dataset has varying densities, COD-CLARANS’s micro-clustering approach may not be suitable for the sparse clusters.|$|E
40|$|Data {{obtained}} by HCMM satellite over a complex area in eastern Spain were evaluated {{and found to}} be most useful in studying macrostructures in geology and in analyzing marine currents, layers, and areas (although other satellites provide more data). The upper scale to work with HCMM data appears to be 1 : 2. 000. 000. Techniques <b>used</b> <b>in</b> <b>preprocessing,</b> processing, and analyzing imagery are discussed as well as methods for pattern recognition. Surface temperatures obtained for soils, farmlands, forests, geological structures, and coastal waters are discussed. Suggestions are included for improvements needed to achieve better results in geographic areas similar to the study area...|$|E
40|$|We {{study the}} {{calculation}} of the largest pairwise intersections in a given set family. We give combinatorial and algorithmic results both for the worst case and for set families where the frequencies of elements follow a power law, as words in texts typically do. The results can be <b>used</b> <b>in</b> faster <b>preprocessing</b> routines <b>in</b> a simple approach to multi-document summarization...|$|R
40|$|High {{dimensional}} data classification becomes challenging task because data are large, complex to handle, heterogeneous and hierarchical. In {{order to reduce}} the data set without affecting the classifier accuracy. The feature selection plays a vital role in large datasets and which increases the efficiency of classification to choose the important features for high dimensional classification, when those features are irrelevant or correlated. Therefore feature selection is considered to <b>use</b> <b>in</b> <b>preprocessing</b> before applying classifier to a data set. Thus this good choice of feature selection leads to the high classification accuracy and minimize computational cost. Though different kinds of feature selection methods are investigate for selecting and fitting features, the best algorithm should be preferred to maximize the accuracy of the classification. The proposed Hybrid kernel Improved Support Vector Machine (HISVM) classifier is used to train the parameters and optimized using Enhanced Grey wolf Optimization (EGWO). The Novel approach aimed to select minimum number of features and providing high classification accuracy...|$|R
40|$|High-dimensional data poses {{a severe}} {{challenge}} for data mining. Feature selection is a frequently <b>used</b> technique <b>in</b> <b>preprocessing</b> high-dimensional data for successful data mining. Traditionally, feature selection {{is focused on}} removing irrelevant features. However, for high-dimensional data, removing redundant features is equally critical. In this paper, we provide a study of feature redundancy in high-dimensional data and propose a novel correlation-based approach to feature selection within the filter model. The extensive empirical study using real-world data shows that the proposed approach is efficient and effective in removing redundant and irrelevant features...|$|R
40|$|International audienceVariable or value {{elimination}} in a {{constraint satisfaction}} problem (CSP) can be <b>used</b> <b>in</b> <b>preprocessing</b> or during search to reduce search space size. A variable elimination rule (value elimination rule) allows the polynomial-time identification of certain variables (domain elements) whose elimination, without {{the introduction of}} extra compensatory constraints, {{does not affect the}} satisfiability of an instance. We show that there are essentially just four variable elimination rules and three value elimination rules defined by forbidding generic sub-instances, known as irreducible existential patterns, in arc-consistent CSP instances. One of the variable elimination rules is the already-known Broken Triangle Property, whereas the other three are novel. The three value elimination rules can all be seen as strict generalisations of neighbourhood substitution...|$|E
40|$|Variable or value {{elimination}} in a {{constraint satisfaction}} problem (CSP) can be <b>used</b> <b>in</b> <b>preprocessing</b> or during search to reduce search space size. A variable elimination rule (value elimination rule) allows the polynomial-time identification of certain variables (domain elements) whose elimination, without {{the introduction of}} extra compensatory constraints, {{does not affect the}} satisfiability of an instance. We show that there are essentially just four variable elimination rules and three value elimination rules defined by forbidding generic sub-instances, known as irreducible existential patterns, in arc-consistent CSP instances. One of the variable elimination rules is the already-known Broken Triangle Property, whereas the other three are novel. The three value elimination rules can all be seen as strict generalisations of neighbourhood substitution. Comment: A full version of an IJCAI' 13 paper to appear in Journal of Computer and System Sciences (JCSS...|$|E
40|$|Abstract. This paper {{presents}} our {{newly developed}} wireless system for person verification {{based on the}} face verification technology. In our system, a handheld device cooperating with a client-server wireless network is used. Due to the limitation of the handheld device's computing power, most computations will be distributed to a remote control server. There are two main problems for implementing the system. One is the variations in lighting and background conditions. The other is that the camera on the handheld device can not be calibrated with the system in advance. For calibration on line, we use a three-point localization scheme for extracting appropriate face region according to the information of eyes and mouth. Furthermore, statistic based illumination normalization is <b>used</b> <b>in</b> <b>preprocessing</b> to decrease illumination influence under variant lighting conditions. Experiment shows the proposed system provides users a more flexible and feasible way {{to interact with the}} verification system through handheld device. ...|$|E
40|$|The {{problem of}} {{evaluating}} clustering algorithms {{and their respective}} computer programs for <b>use</b> <b>in</b> a <b>preprocessing</b> step for classification is addressed. In clustering for classification the probability of correct classification is suggested as the ultimate measure of accuracy on training data. A means of implementing this criterion and a measure of cluster purity are discussed. Examples are given. A procedure for cluster labeling {{that is based on}} cluster purity and sample size is presented...|$|R
40|$|International audienceIn this article, we {{consider}} the k-edge connected subgraph problem from a polyhedral point of view. We introduce further classes of valid inequalities for the associated polytope and describe sufficient conditions for these inequalities to be facet defining. We also devise separation routines for these inequalities and discuss some reduction operations that can be <b>used</b> <b>in</b> a <b>preprocessing</b> phase for the separation. Using these results, we develop a Branch-and-Cut algorithm and present some computational results...|$|R
30|$|Gauss {{filtering}} method [24]: Gauss filter {{is a kind}} {{of linear}} smoothing filter, which is suitable for filtering Gaussian white noise. Gauss filter method has been widely <b>used</b> <b>in</b> the <b>preprocessing</b> stage of image processing. The pixel value of each pixel in the image is calculated by Gaussian filtering operation, the result is obtained by weighting the pixel gray value and other pixel gray values in the neighborhood of the pixel itself, and the weighted average weighting coefficient is obtained by sampling and normalizing the two-dimensional discrete Gaussian function.|$|R
40|$|We report {{preliminary}} {{results of an}} effort to use variants of the Hidden Markov Models developed by speech researchers to characterize persistence and recurrence of atmospheric circulation patterns in a 36 year record of Northern Hemisphere 700 -mb geopotential heights. Using a cross validation scheme, we fit autoregressive hidden Markov models (ARHMMs) with a range of complexities, varying the autoregressive order and the number of hidden states. As the autoregressive order increases from zero to four, the optimal number of hidden states decreases from more than eight to one, i. e., simple autoregressive models are better than multi-state models when the autoregressive order is at least four. We believe that the failure to find regime-like behavior is an artifact of a temporal low-pass filter <b>used</b> <b>in</b> <b>preprocessing</b> the data and that more careful work is indicated. 1 Introduction There is an ongoing interest in characterizing large scale atmospheric circulation patterns. The microscopic [...] ...|$|E
40|$|Quality {{evaluation}} {{is an important}} factor in food processing industries using the computer vision system where human inspection systems provide high variability. In many countries food processing industries aims at producing defect free food materials to the consumers. Human evaluation techniques suffer from high labour costs, inconsistency and variability. Thus this paper provides various steps for identifying defects in the food material using the computer vision systems. Various steps in computer vision system are image acquisition, Preprocessing, image segmentation, feature identification and classification. The proposed framework provides the comparison of various filters where the hybrid median filter was selected as the filter with the high PSNR value and is <b>used</b> <b>in</b> <b>preprocessing.</b> Image segmentation techniques such as Colour based binary Image segmentation, Particle swarm optimization are compared and image segmentation parameters such as accuracy, sensitivity, specificity are calculated and found that colour based binary image segmentation is well suited for food quality evaluation. Finally this paper provides an efficient method for identifying the defected parts in food materials...|$|E
40|$|Local Binary Pattern (LBP) is texture {{operator}} <b>used</b> <b>in</b> <b>preprocessing</b> for object detection, tracking, {{face recognition}} and fingerprint matching. Many of these applications are performed on embedded devices, which poses {{limitations on the}} implementation complexity and power consumption. As LBP features are computed pixelwise, high performance is required for real time extraction of LBP features from high resolution video. This paper presents an application-specific instruction processor for LBP extraction. The compact, yet powerful processor is capable of extracting LBP features from 1280 × 720 p (30 fps) video with a reasonable 304 MHz clock rate. With a low power consumption and an area of less than 16 k gates the processor is suitable for embedded devices. Experiments present resource and power consumption measured on an FPGA board, along with processor synthesis results. In terms of latency, our processor requires 17. 5 × less clock cycles per LBP feature than a workstation implementation and only 2. 0 × more than a hardwired ASIC. Index Terms — Feature extraction, Video signal processing, Digital signal processors, Image texture analysi...|$|E
40|$|Multichannel {{recordings}} of EEG data during various mental tasks are processed using two popular methods, independent component analysis (ICA) and matching pursuit (MP). The results are fed {{to a time}} delay neural network (TDNN) for classification of each mental task. Based {{on the results of}} the test sets, we analyzed the effectiveness of ICA and MP methods for <b>use</b> <b>in</b> EEG <b>preprocessing</b> and TDNN classification. It is shown that ICA is more effective than MP in lowering the neural network classification error; however this advantage is not significant...|$|R
40|$|In {{this paper}} we {{introduce}} a parallel algorithm for thinning gray scale images. The algorithm {{is based on}} repeatedly conditionally eroding the gray level objects in the image until a one pixel thick pattern is obtained along {{the center of the}} high intensity region. Erosion conditions are devised to assure preserving connectivity. To properly handle objects with hollows, gray level gradient is <b>used</b> <b>in</b> the <b>preprocessing</b> phase to detect significant hollows. This allows processing of realistic nontrivial gray level objects. Results of applying the algorithm on a variety of images will be shown...|$|R
40|$|Feature {{selection}} is frequently <b>used</b> <b>in</b> data <b>preprocessing</b> for data mining. It decreases number of features, removes irrelevant or noisy data, and increases mining performance such as predictive accuracy and comprehensibility. This work investigates active sampling in feature selection in a lter model setting. Three versions of active sampling are proposed and empirically evaluated: two employ class {{information and the}} other utilizes feature variance. They are applied to a widely used, ecient feature selection algorithm Relief. In comparison with random sampling, we conduct extensive experiments with benchmark data sets...|$|R
40|$|For a {{real time}} face {{recognition}} system, restraints like orientation, lighting and pose {{are the major}} challenges to be addressed. In the proposed work, to eliminate the variations due to pose, lighting and features to some extent, Gabor wavelets are <b>used</b> <b>in</b> <b>preprocessing</b> of human face image. Principal Component Analysis (PCA) has been widely adopted as the unpretentious potential face recognition algorithm which extracts low dimensional feature vectors from face. Linear Discriminant analysis (LDA) is applied on the reduced features from PCA, to get more discriminating features. The classification is done using distance measure classifiers and Support Vector Machine. Here, the train dataset is considered in randomized fashion, means the database contains a set of images of an individual in which different set of train dataset is generated randomly to choose the trained set that gives high rate of recognition. This will support in plummeting the overall database size and upsurge {{the enactment of the}} system. The system has been successfully tested on ORL face database with 400 frontal images corresponding to 40 different subjects of variable illumination and facial expressions. The proposed system gives a better recognition rate when compared to other standard techniques and achieves better accuracy with increased number of features. Keywords [...] Face recognition; Gabor Wavelets; PCA; LDA; Randomized Dataset I...|$|E
40|$|Introduction: Because of huge {{impacts of}} “OMICS” {{technologies}} in life sciences, many researchers aim to implement such high throughput approach to address cellular and/or molecular functions {{in response to}} any influential intervention in genomics, proteomics, or metabolomics levels. However, in many cases, use of such technologies often encounters some cybernetic difficulties in terms of knowledge extraction from a bunch of data using related softwares. In fact, there is little guidance upon data mining for novices. The main goal {{of this article is}} to provide a brief review on different steps of microarray data handling and mining for novices and at last to introduce different PC and/or web-based softwares that can be <b>used</b> <b>in</b> <b>preprocessing</b> and/or data mining of microarray data. Methods: To pursue such aim, recently published papers and microarray softwares were reviewed. Results: It was found that defining the true place of the genes in cell networks is the main phase in our understanding of programming and functioning of living cells. This can be obtained with global/selected gene expression profiling. Conclusion: Studying the regulation patterns of genes in groups, using clustering and classification methods helps us understand different pathways in the cell, their functions, regulations and the way one component in the system affects the other one. These networks can act as starting points for data mining and hypothesis generation, helping us reverse engineer...|$|E
40|$|World Wide Web {{is a huge}} {{repository}} of web pages and links. It provides abundance information for the Internet users. The growth of web is incredible {{as it can be}} seen in present days. Users ’ accesses are recorded in web logs. From the user’s perspective, {{it is very difficult to}} extract useful knowledge from the huge amount of information and secondly, it is also difficult to extract for the users to access relevant information efficiently. From business point of view, the webmasters and administrators find it difficult to organize the contents of the websites to cater to the needs of the users. Both the problems can be solved if the web navigation behavior of a user can be understood. One way to extract such information is to use Web Usage Mining. Web Usage Mining consists of preprocessing, pattern discovery and pattern analysis. Preprocessing is the step which transforms the raw log file into a form that is more suitable for mining. Four steps are <b>used</b> <b>in</b> <b>preprocessing,</b> they are, data cleaning, user identification, session identification and formatting the result to suit the clustering algorithm. The clustering technique used in this paper is Fuzzy-Possibilistic C-Means clustering technique. In pattern analysis, this paper uses Fast Adaptive Neuro-Fuzzy Inference System (FANFIS). The experimental results suggest that the proposed technique for web log mining results in better prediction of user behaviors when compared to the existing web usage mining techniques. ...|$|E
40|$|Many local {{consistency}} properties {{have been}} exploited in solving Constraint Satisfaction Problems. The objec-tive {{is to reduce}} the search space and consequently improve search methods. It has been shown that maintaining arc-consistency during search is very useful in solving CSPs. The use of stronger local consistency forms (like path con-sistency) is still limited since they need complicated data structures to be managed and the constraint graph may be modied. In this paper, we propose a possible way to get benets from <b>using,</b> <b>in</b> a <b>preprocessing</b> step, a partial form of path consistency and arc consistency based on support intervals notion. 1...|$|R
40|$|Abstract- Electric {{power is}} an {{important}} part in economic development. Moreover, an accurate load forecast can make a financing planning, power supply strategy and market research planned effectively. This paper used the fuzzy logic system to predict the regional electric power load. To design the fuzzy prediction system, the correlation-based clustering algorithm and TSK fuzzy model were used. Also, to improve the prediction system's capability, the moving average technique and relative increasing rate were <b>used</b> <b>in</b> the <b>preprocessing</b> procedure. Finally, using four regional electric power load in Taiwan, this paper verified the performance of the proposed system and demonstrated its effectiveness an...|$|R
40|$|The {{quality of}} images {{generated}} by volume rendering strongly {{depends on the}} applied continuous reconstruction method. Recently, {{it has been shown}} that the reconstruction of the underlying function can be improved by a discrete prefiltering. In volume rendering, however, an accurate gradient reconstruction also plays an important role as it provides the surface normals for the shading computations. Therefore, in this paper, we propose prefiltering schemes in order to increase the accuracy of the estimated gradients yielding higher image quality. We search for discrete prefilters of minimal support which can be efficiently <b>used</b> <b>in</b> a <b>preprocessing</b> as well as on the fly...|$|R
40|$|This licentiate thesis {{deals with}} {{automatic}} syntactic analysis, or parsing, of natural languages. A parser constructs the syntactic analysis, which it learns {{by looking at}} correctly analyzed sentences, known as training data. The general topic concerns manipulations of the training data {{in order to improve}} the parsing accuracy. Several studies using constituency-based theories for natural languages in such automatic and data-driven syntactic parsing have shown that training data, annotated according to a linguistic theory, often needs to be adapted in various ways in order to achieve an adequate, automatic analysis. A linguistically sound constituent structure is not necessarily well-suited for learning and parsing using existing data-driven methods. Modifications to the constituency-based trees in the training data, and corresponding modifications to the parser output, have successfully been applied to increase the parser accuracy. The topic of this thesis is to investigate whether similar modifications in the form of tree transformations to training data, annotated with dependency-based structures, can improve accuracy for data-driven dependency parsers. In order to do this, two types of tree transformations are in focus in this thesis. %This is a topic that so far has been less studied. The first one concerns non-projectivity. The full potential of dependency parsing can only be realized if non-projective constructions are allowed, which pose a problem for projective dependency parsers. On the other hand, non-projective parsers tend, among other things, to be slower. In order to maintain the benefits of projective parsing, a tree transformation technique to recover non-projectivity while using a projective parser is presented here. The second type of transformation concerns linguistic phenomena that are possible but hard for a parser to learn, given a certain choice of dependency analysis. This study has concentrated on two such phenomena, coordination and verb groups, for which tree transformations are applied in order to improve parsing accuracy, in case the original structure does not coincide with a structure that is easy to learn. Empirical evaluations are performed using treebank data from various languages, and using more than one dependency parser. The results show that the benefit of these tree transformations <b>used</b> <b>in</b> <b>preprocessing</b> and postprocessing to a large extent is language, treebank and parser independent...|$|E
40|$|Our {{motivation}} is increased bronchoscopic diagnostic yield and optimized preparation, for navigated bronchoscopy. In navigated bronchoscopy, virtual 3 D airway visualization {{is often used}} to guide a bronchoscopic tool to peripheral lesions, synchronized with the real time video bronchoscopy. Visualization during navigated bronchoscopy, the segmentation time and methods, differs. Time consumption and logistics are two essential aspects that need to be optimized when integrating such technologies in the interventional room. We compared three different approaches to obtain airway centerlines and surface. CT lung dataset of 17 patients were processed in Mimics (Materialize, Leuven, Belgium), which provides a Basic module and a Pulmonology module (beta version) (MPM), OsiriX (Pixmeo, Geneva, Switzerland) and our Tube Segmentation Framework (TSF) method. Both MPM and TSF were evaluated with reference segmentation. Automatic and manual settings allowed us to segment the airways and obtain 3 D models as well as the centrelines in all datasets. We compared the different procedures by user interactions such as number of clicks needed to process the data and quantitative measures concerning the quality of the segmentation and centrelines such as total length of the branches, number of branches, number of generations, and volume of the 3 D model. The TSF method was the most automatic, while the Mimics Pulmonology Module (MPM) and the Mimics Basic Module (MBM) resulted in the highest number of branches. MPM is the software which demands the least number of clicks to process the data. We found that the freely available OsiriX was less accurate compared to the other methods regarding segmentation results. However, the TSF method provided results fastest regarding number of clicks. The MPM was able to find the highest number of branches and generations. On the other hand, the TSF is fully automatic and it provides the user with both segmentation of the airways and the centerlines. Reference segmentation comparison averages and standard deviations for MPM and TSF correspond to literature. The TSF is able to segment the airways and extract the centerlines in one single step. The number of branches found is lower for the TSF method than in Mimics. OsiriX demands the highest number of clicks to process the data, the segmentation is often sparse and extracting the centerline requires the use of another software system. Two of the software systems performed satisfactory with respect to be <b>used</b> <b>in</b> <b>preprocessing</b> CT images for navigated bronchoscopy, i. e. the TSF method and the MPM. According to reference segmentation both TSF and MPM are comparable with other segmentation methods. The level of automaticity and the resulting high number of branches plus the fact that both centerline and the surface of the airways were extracted, are requirements we considered particularly important. The in house method has the advantage of being an integrated part of a navigation platform for bronchoscopy, whilst the other methods can be considered preprocessing tools to a navigation system...|$|E
40|$|As {{demand for}} {{real-time}} image processing increases, {{the need to}} improve the efficiency of image processing systems is growing. The process of image segmentation is often <b>used</b> <b>in</b> <b>preprocessing</b> stages of computer vision systems to reduce image data and increase processing efficiency. This dissertation introduces a novel image segmentation approach known as leap segmentation, which applies a flexible definition of adjacency to allow groupings of pixels into segments which need not be spatially contiguous and thus can more accurately correspond to large surfaces in the scene. Experiments show that leap segmentation correctly preserves an average of 20 % more original scene pixels than traditional approaches, while using the same number of segments, and significantly improves execution performance (executing 10 x - 15 x faster than leading approaches). Further, leap segmentation is shown to improve the efficiency of a high-level vision application for scene layout analysis within 3 D scene reconstruction. The benefits of applying image segmentation in preprocessing are not limited to single-frame image processing. Segmentation is also often applied in the preprocessing stages of video analysis applications. In the second contribution of this dissertation, the fast, single-frame leap segmentation approach is extended into the temporal domain to develop a highly-efficient method for multiple-frame segmentation, called video leap segmentation. This approach is evaluated for use on mobile platforms where processing speed is critical using moving-camera traffic sequences captured on busy, multi-lane highways. Video leap segmentation accurately tracks segments across temporal bounds, maintaining temporal coherence between the input sequence frames. It is shown that video leap segmentation can be applied with high accuracy to the task of salient segment transformation detection for alerting drivers to important scene changes that may affect future steering decisions. Finally, while research efforts in the field of image segmentation have often recognized the need for efficient implementations for real-time processing, many of today’s leading image segmentation approaches exhibit processing times which exceed their camera frame periods, making them infeasible for use in real-time applications. The third research contribution of this dissertation focuses on developing fast implementations of the single-frame leap segmentation approach for use on both single-core and multi-core platforms as well as on both high-performance and resource-constrained systems. While the design of leap segmentation lends itself to efficient implementations, the efficiency achieved by this algorithm, as in any algorithm, is can be improved with careful implementation optimizations. The leap segmentation approach is analyzed in detail and highly optimized implementations of the approach are presented with in-depth studies, ranging from storage considerations to realizing parallel processing potential. The final implementations of leap segmentation for both serial and parallel platforms are shown to achieve real-time frame rates even when processing very high resolution input images. Leap segmentation’s accuracy and speed make it a highly competitive alternative to today’s leading segmentation approaches for modern, real-time computer vision systems. Ph. D...|$|E
40|$|Superpixel {{segmentation}} {{is widely}} <b>used</b> <b>in</b> the <b>preprocessing</b> step of many applications. Most of existing methods {{are based on}} a photometric criterion combined to the position of the pixels. In {{the same way as the}} Simple Linear Iterative Clustering (SLIC) method, based on k-means segmentation, a new algorithm is introduced. The main contribution lies on the definition of a new distance for the construction of the superpixels. This distance takes into account both the surface normals and a similarity measure between pixels that are located on the same planar surface. We show that our approach improves over-segmentation, like SLIC, i. e. the proposed method is able to segment properly planar surfaces...|$|R
40|$|A {{combined}} geometric and radiometric {{processing chain}} for hyperspectral data is presented. This paper describes the ortho-rectification {{solution for the}} geometric {{part of the whole}} problem. A parametric geocoding approach (PARGE) has been chosen. The geometrical model strictly considers all navigational parameters engaging a forward transformation methodology. For the implementation, a number of auxiliary data calibration and tuning possibilities are shown together with the work flow of the currently applied processor. Results of the procedure for HyMap and AVIRIS hyperspectral imagery are analyzed with respect to their absolute accuracy and coregistration errors. The geocoding procedure will be <b>used</b> <b>in</b> standard <b>preprocessing</b> chains <b>in</b> combination with the atmospheric correction procedure ATCOR 4 for current and future hyperspectral instruments. ...|$|R
30|$|In recent years, many {{different}} techniques and studies are conducted on road line detection, {{mainly caused by}} the current interest in advanced driving assistance systems and autonomous driving systems. Road line detection algorithms in the literature mostly consist of two stages, preprocessing and detection stages. <b>In</b> <b>preprocessing</b> stage, different image processing techniques are <b>used</b> <b>in</b> order to provide enhanced data for detection task. <b>Preprocessing</b> methods <b>in</b> the literature can be exemplified as follows. Somasundaram et al. [15] use transformation from RGB colour space to HSV colour space for reducing redundancy. Morphological filtering is <b>used</b> <b>in</b> [16]. In [17], canny edge detector is used to indicate and emphasis road lines, and in [18], Gaussian smoothing is used to eliminate noise. Transformation to binary image is <b>used</b> <b>in</b> [19]. Inverse perspective mapping and road segmentation methods are <b>used</b> as <b>preprocessing</b> <b>in</b> [20, 21]. A study conducted by Jung et al. [22] proposed constructing spatiotemporal images which exploits the temporal dependency of the video frames.|$|R
