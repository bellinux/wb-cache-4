732|7203|Public
25|$|In test-driven {{development}} <b>unit</b> <b>tests</b> {{are written}} before the code and the code {{is not considered}} complete until all tests complete successfully.|$|E
25|$|Boys will be {{assessed}} regularly {{through the year}} by means of end of <b>unit</b> <b>tests</b> {{in the majority of}} subjects and end of year exams will be sat during {{the latter part of the}} Summer Term.|$|E
25|$|The Field Test Group {{conducted}} {{a number of}} tests and exercise to evaluate {{the recommendations of the}} other groups. Using one battle group, resources from two others, and 150 aircraft (125 helicopters and 25 fixed-wing), the group conducted 40 field tests. Three of these were week-long tests; evaluating the ability of troops to function in swampy areas in Georgia, testing the concept in counter-guerrilla exercises in western Virginia, and finally a withdrawal under pressure scenario conducted in the Fort Bragg area. The tests used over 11,000 flying hours, many at low altitude and simulating combat conditions as much as possible. Smaller <b>unit</b> <b>tests</b> (16 in all) were conducted before these exercises, and a number of smaller tests took {{place at the same time}} to evaluate tactical mobility, firepower, reconnaissance, and logistics operations.|$|E
50|$|Test {{methods are}} {{declared}} as such by decorating a <b>unit</b> <b>test</b> method with the TestMethod attribute. The attribute {{is used to}} identify methods that contain <b>unit</b> <b>test</b> code. Best practices state that <b>unit</b> <b>test</b> methods should contain only <b>unit</b> <b>test</b> code.|$|R
50|$|<b>Unit</b> <b>testing</b> is the {{cornerstone}} of extreme programming, which relies on an automated <b>unit</b> <b>testing</b> framework. This automated <b>unit</b> <b>testing</b> framework can be either third party, e.g., xUnit, or created within the development group.|$|R
50|$|<b>Unit</b> <b>testing</b> is {{commonly}} automated, but {{may still be}} performed manually. The IEEE does not favor one over the other. The objective in <b>unit</b> <b>testing</b> is to isolate a unit and validate its correctness. A manual approach to <b>unit</b> <b>testing</b> may employ a step-by-step instructional document. However, automation is efficient for achieving this, and enables the many benefits listed in this article. Conversely, if not planned carefully, a careless manual <b>unit</b> <b>test</b> case may execute as an integration test case that involves many software components, and thus preclude the achievement of {{most if not all}} of the goals established for <b>unit</b> <b>testing.</b>|$|R
500|$|Cobra uses {{indentation}} and {{a similar}} syntax. Cobra's [...] "Acknowledgements" [...] document lists Python first among languages that influenced it. However, Cobra directly supports design-by-contract, <b>unit</b> <b>tests,</b> and optional static typing.|$|E
2500|$|NEDU {{describes}} {{its mission}} as: [...] "The Navy Experimental Diving <b>Unit</b> <b>tests</b> and evaluates diving, hyperbaric, and other life-support systems and procedures, and conducts {{research and development}} in biomedical and environmental physiology. NEDU provides technical recommendations based upon knowledge and experience, to Commander, Naval Sea Systems Command to support operational requirements of our armed forces." ...|$|E
2500|$|The {{school is}} {{affiliated}} to the CBSE board of India. <b>Unit</b> <b>tests</b> are held quarterly {{in addition to}} half yearly and annual examinations. Students appear for AISSCE (10th) and AISSE (12th) along with other CBSE affiliated schools in India. Student to staff ratio is about 2.5:1 and student to teaching staff ratio is 10:1 which is far {{above the national average}} of India. Staff are recruited by central government from all over India. [...] Students are offered Science, Arts and Commerce subjects in 11th and 12th classes. [...] School curriculum includes seven periods of 40 minutes each. [...] Daily three hours of compulsory prep is included in a routine for students to concentrate on studies.|$|E
50|$|The PHP <b>Unit</b> <b>Testing</b> Framework {{is one of}} the xUnit {{family of}} <b>unit</b> <b>testing</b> frameworks. A variety of assertions are available.|$|R
40|$|This paper {{presents}} a dependency injection based, <b>unit</b> <b>testing</b> methodology for <b>unit</b> <b>testing</b> components, or actors, involved in discrete event based computer network simulation via an xUnit testing framework. The fundamental purpose of discrete event based computer network simulation is verification of networking protocols used in physical–not simulated–networks. Thus, use of rigorous <b>unit</b> <b>testing</b> and test driven development methodologies mitigates risk of modeling the wrong system. We validate the methodology through {{the design and}} implementation of OPNET-Unit, an xUnit style <b>unit</b> <b>testing</b> application for an actor oriented discrete event based network simulation environment, OPNET Modeler...|$|R
5000|$|<b>Unit</b> <b>testing</b> and {{integration}} testing. One {{study found that}} the average defect detection rates of <b>unit</b> <b>testing</b> {{and integration}} testing are 30% and 35% respectively.|$|R
50|$|In test-driven {{development}} (TDD), {{which is}} frequently {{used in both}} extreme programming and scrum, <b>unit</b> <b>tests</b> are created before the code itself is written. When the tests pass, that code is considered complete. The same <b>unit</b> <b>tests</b> are run against that function frequently as the larger code base is developed either as the code is changed or via an automated process with the build. If the <b>unit</b> <b>tests</b> fail, it {{is considered to be}} a bug either in the changed code or the tests themselves. The <b>unit</b> <b>tests</b> then allow the location of the fault or failure to be easily traced. Since the <b>unit</b> <b>tests</b> alert the development team of the problem before handing the code off to testers or clients, it is still early in the development process.|$|E
5000|$|Unit {{testing is}} {{provided}} {{as an integral}} part of Laravel, which itself contains <b>unit</b> <b>tests</b> that detect and prevent regressions in the framework. <b>Unit</b> <b>tests</b> can be run through the provided [...] command-line utility.|$|E
5000|$|Parameterized <b>unit</b> <b>tests</b> (PUTs) are {{tests that}} take parameters. Unlike {{traditional}} <b>unit</b> <b>tests,</b> {{which are usually}} closed methods, PUTs take any set of parameters. PUTs have been supported by TestNG, JUnit and various [...]NET test frameworks. Suitable parameters for the <b>unit</b> <b>tests</b> may be supplied manually {{or in some cases}} are automatically generated by the test framework. Testing tools like QuickCheck exist to generate test inputs for PUTs.|$|E
50|$|PHP <b>Unit</b> <b>Testing</b> Framework is a <b>unit</b> <b>testing</b> {{framework}} that enables developers to discover bugs {{and in turn}} drive down {{the costs associated with}} developing PHP software.|$|R
40|$|This thesis {{explains}} what <b>unit</b> <b>testing</b> is and why <b>unit</b> <b>testing</b> for ontologies is a helpful process for an ontological engineer {{in order to}} verify an ontology. In {{the course of that}} we clarify what an ontology is and present some common methodologies for ontological engineering. In a first step we define exactly in which state of the development process <b>unit</b> <b>testing</b> can be helpful. <b>Unit</b> <b>testing</b> is highly dependent on the used logical language, which is underlying the used ontology language. Therefore, in a second step, we examine some logical languages and detail those cases where the use of <b>unit</b> <b>testing</b> makes sense in order to avoid engineering mistakes. The theoretical results of this thesis are used to implement a <b>unit</b> <b>testing</b> prototype for WSML-Flight and WSML-Rule ontologies, based on Eclipse. Acknowledgements First, I thank my advisor Holger Lausen. He tought me how to write a thesis and guided me into the right direction. Furthermore, I would like to than...|$|R
40|$|This paper {{presents}} {{an analysis of}} the <b>unit</b> <b>testing</b> approach developed and used by the Core Flight Software System (CFS) product line team at the NASA Goddard Space Flight Center (GSFC). The goal of the analysis is to understand, review, and recommend strategies for improving the CFS' existing <b>unit</b> <b>testing</b> infrastructure as well as to capture lessons learned and best practices that can be used by other software product line (SPL) teams for their <b>unit</b> <b>testing.</b> The results of the analysis show that the core and application modules of the CFS are <b>unit</b> <b>tested</b> in isolation using a stub framework developed by the CFS team. The application developers can <b>unit</b> <b>test</b> their code without waiting for the core modules to be completed, and vice versa. The analysis found that this <b>unit</b> <b>testing</b> approach incorporates many practical and useful solutions such as allowing for <b>unit</b> <b>testing</b> without requiring hardware and special OS features in-the-loop by defining stub implementations of dependent modules. These solutions are worth considering when deciding how to design the testing architecture for a SPL. © 2012 Elsevier B. V. All rights reserved...|$|R
50|$|<b>Unit</b> <b>tests</b> are {{so named}} because they each test one unit of code. A complex module may have a {{thousand}} <b>unit</b> <b>tests</b> and a simple module may have only ten. The <b>unit</b> <b>tests</b> used for TDD should never cross process boundaries in a program, let alone network connections. Doing so introduces delays that make tests run slowly and discourage developers from running the whole suite. Introducing dependencies on external modules or data also turns <b>unit</b> <b>tests</b> into integration tests. If one module misbehaves in a chain of interrelated modules, {{it is not so}} immediately clear where to look for the cause of the failure.|$|E
50|$|Smoke {{tests can}} be broadly {{categorized}} as functional tests or as <b>unit</b> <b>tests.</b> Functional tests exercise the complete program with various inputs. <b>Unit</b> <b>tests</b> exercise individual functions, subroutines, or object methods. Functional tests may comprise a scripted series of program inputs, possibly even with an automated mechanism for controlling mouse movements. <b>Unit</b> <b>tests</b> {{can be implemented}} either as separate functions within the code itself, or else as a driver layer that links to the code without altering the code being tested.|$|E
5000|$|Testing: support {{classes for}} writing <b>unit</b> <b>tests</b> and {{integration}} tests ...|$|E
40|$|<b>Unit</b> <b>testing,</b> {{a common}} step in {{software}} development, presents a challenge. When produced manually, <b>unit</b> <b>test</b> suites are often insufficient to identify defects. The main {{alternative is to}} use one of a variety of automatic <b>unit</b> <b>test</b> generation tools: these are able to produce and execute a large number of test inputs that extensively exercise the <b>unit</b> under <b>test.</b> However, without a priori specifications, developers need to manually verify the outputs of these test executions, which is generally impractical. To reduce this cost, <b>unit</b> <b>test</b> selection techniques may be used to help select a subset of automatically generated test inputs. Then developers can verify their outputs, equip them with test oracles, and put them into the existing test suite. In this paper, we present the operational violation approach for <b>unit</b> <b>test</b> selection, a black-box approach without requiring a priori specifications. The approach dynamically generates operational abstractions from executions of the existing <b>unit</b> <b>test</b> suite. Any automatically generated tests violating the operational abstractions are identified as candidates for selection. In addition, these operational abstractions can guide test generation tools to produce better tests. To experiment with this approach, we integrated the use of Daikon (a dynamic invariant detection tool) and Jtest (a commercial Java <b>unit</b> <b>testing</b> tool). An experiment is conducted to assess this approach. 1...|$|R
50|$|The book {{illustrates}} {{the use of}} <b>unit</b> <b>testing</b> {{as part of the}} methodology, including examples in Java and Python. One section includes using test-driven development to develop a <b>unit</b> <b>testing</b> framework.|$|R
50|$|Lazy Systematic <b>Unit</b> <b>Testing</b> is a {{software}} <b>unit</b> <b>testing</b> method {{based on the}} two notions of lazy specification, the ability to infer the evolving specification of a unit on-the-fly by dynamic analysis, and systematic testing, the ability to explore and <b>test</b> the <b>unit's</b> state space exhaustively to bounded depths. A testing toolkit JWalk exists to support lazy systematic <b>unit</b> <b>testing</b> in the Java programming language.|$|R
50|$|Initialization {{and cleanup}} methods {{are used to}} prepare <b>unit</b> <b>tests</b> before running and {{cleaning}} up after <b>unit</b> <b>tests</b> have been executed. Initialization methods are declared as such by decorating an initialization method with the TestInitialize attribute, while cleanup methods are declared as such by decorating a cleanup method with the TestCleanup attribute.|$|E
5000|$|Students have weekly tests/four <b>unit</b> <b>tests,</b> half yearly {{and annual}} exams.|$|E
50|$|An {{elaborate}} {{hierarchy of}} <b>unit</b> <b>tests</b> does not equal integration testing. Integration with peripheral units {{should be included}} in integration tests, but not in <b>unit</b> <b>tests.</b> Integration testing typically still relies heavily on humans testing manually; high-level or global-scope testing can be difficult to automate, such that manual testing often appears faster and cheaper.|$|E
50|$|<b>Unit</b> <b>testing.</b> White-box {{testing is}} done during <b>unit</b> <b>testing</b> {{to ensure that}} the code is working as intended, before any {{integration}} happens with previously tested code. White-box <b>testing</b> during <b>unit</b> <b>testing</b> catches any defects early on and aids in any defects that happen later on after the code is integrated {{with the rest of the}} application and therefore prevents any type of errors later on.|$|R
5000|$|Recurring Bug Pattern - Expose a bug via a <b>unit</b> <b>test.</b> Run that <b>unit</b> <b>test</b> {{as part of}} a {{standard}} build from that moment on. This ensure that the bug will not recur.|$|R
5000|$|The Art of <b>Unit</b> <b>Testing</b> is a 2009 book by Roy Osherove {{which covers}} <b>unit</b> <b>test</b> writing for software. It's written with [...]NET Framework examples, but the {{fundamentals}} {{can be applied}} by any developer.|$|R
50|$|Developer tools include data logging, pretty-printer, profiler, {{contract}} programming, and <b>unit</b> <b>tests.</b>|$|E
50|$|Later elaborations of {{the concept}} {{introduced}} build servers, which automatically ran the <b>unit</b> <b>tests</b> periodically or even after every commit and reported the results to the developers. The use of build servers (not necessarily running <b>unit</b> <b>tests)</b> had already been practised by some teams outside the XP community. Nowadays, many organisations have adopted CI without adopting all of XP.|$|E
5000|$|Some {{programming}} languages directly {{support unit}} testing. Their grammar allows the direct declaration of <b>unit</b> <b>tests</b> without importing a library (whether third party or standard). Additionally, the boolean {{conditions of the}} <b>unit</b> <b>tests</b> can be expressed in the same syntax as boolean expressions used in non-unit test code, such as {{what is used for}} [...] and [...] statements.|$|E
40|$|AbstractThe study aims at {{investigating}} empirically {{the ability}} of a Quality Assurance Indicator (Qi), a metric that we proposed in a previous work, to predict different levels of <b>unit</b> <b>testing</b> effort of classes in object-oriented systems. To capture the <b>unit</b> <b>testing</b> effort of classes, we used four metrics to quantify various perspectives related to the code of corresponding <b>unit</b> <b>test</b> cases. Classes were classified, according to the involved <b>unit</b> <b>testing</b> effort, in five categories (levels). We collected data from two open source Java software systems (ANT and JFREECHART) for which JUnit test cases exist. In order to explore {{the ability of}} the Qi metric to predict different levels of the <b>unit</b> <b>testing</b> effort of classes, we decided to explore the possibility of using the Multinomial Logistic Regression (MLR) method. The performance of the Qi metric has been compared to the performance of three well-known source code metrics related respectively to size, complexity and coupling. Results suggest that the MLR model based on the Qi metric is able to accurately predict different levels of the <b>unit</b> <b>testing</b> effort of classes...|$|R
40|$|Westinghouse Fuel Manufacturing in Västerås, Sweden, {{manufactures}} {{fuel rods}} for nuclear plants. Manufacturing-IT is a software development section at Westinghouse Fuel Manufacturing. This thesis involves {{the development of}} a <b>unit</b> <b>testing</b> methodology (UTM) for the Manufacturing-IT section, which currently does not follow a well-defined software test process. By evaluating different <b>unit</b> <b>testing</b> best practices and UTM design issues collected from literature, articles, papers and the Internet, a UTM document was developed. The UTM document was developed according to requirements from Manufacturing-IT and as an extension to existing documents within the Westinghouse organization. The UTM was evaluated by applying the methodology in a case study. A single unit within a production control system in the rod manufacturing workshop at the Westinghouse fuel factory in Västerås was tested. Asides from evaluating the UTM, the case study was intended to find software tools that could simplify the <b>unit</b> <b>testing</b> process, and to test the production control system unit thoroughly. The 182 test cases designed and implemented revealed 28 faults in the <b>tested</b> <b>unit.</b> NUnit was chosen to be the tool for automated <b>unit</b> <b>testing</b> in the UTM. The results from the case study indicate that the methods and other <b>unit</b> <b>testing</b> process related activities included in the UTM document developed are applicable to <b>unit</b> <b>testing.</b> However, adjustments and further evaluation will be needed in order to enhance the UTM. The UTM developed in this thesis is a first step towards a structured testing process for the Manufacturing-IT section and the UTM document will be used at the Manufacturing-IT section. By using the methods and other <b>unit</b> <b>testing</b> process related activities in the UTM developed in this thesis, any company or individual with similar requirements for a UTM as Manufacturing-IT, and that performs <b>unit</b> <b>testing</b> in an unstructured way, may benefit in that a more structured <b>unit</b> <b>testing</b> process is achieved...|$|R
40|$|This paper {{presents}} {{an analysis of}} the <b>unit</b> <b>testing</b> approach developed and used by the Core Flight Software (CFS) product line team at the NASA GSFC. The goal of the analysis is to understand, review, and recommend strategies for improving the existing <b>unit</b> <b>testing</b> infrastructure as well as to capture lessons learned and best practices that can be used by other product line teams for their <b>unit</b> <b>testing.</b> The CFS <b>unit</b> <b>testing</b> framework is designed and implemented as a set of variation points, and thus testing support is built into the product line architecture. The analysis found that the CFS <b>unit</b> <b>testing</b> approach has many practical and good solutions that are worth considering when deciding how to design the testing architecture for a product line, which are documented in this paper along with some suggested improvements...|$|R
