0|9755|Public
40|$|In {{this work}} we {{consider}} the problem of efficient object-class recognition in large image collections. We are specifically interested in scenarios where the classes to be recognized are not known in advance. The motivating application is “object-class search by example ” where a <b>user</b> provides at <b>query</b> <b>time</b> a small set of training images defining an arbitrary novel category and the system must retrieve from a large databas...|$|R
40|$|Data {{integration}} approaches mostly {{attempt to}} resolve semantic uncertainty and conflicts between data sources during the data integration process. In some application areas, this is impractical or even prohibitive. We propose a probabilistic XML approach that allows storage and query-ing of uncertain data. It requires only minimal user involve-ment during data integration, because most semantic uncer-tainty and conflicts {{can be resolved}} by exploiting <b>user</b> feed-back on <b>query</b> results, thus effectively postponing <b>user</b> in-volvement to <b>query</b> <b>time</b> when a <b>user</b> already interacts with the system. We show that repeated feedback gradually im-proves the factual correctness of integrated data. ...|$|R
40|$|AbstractUsers of location-based {{services}} (LBSs) {{may have}} serious privacy concerns when using these technologies since their location can be utilized by adversaries to infer privacy-sensitive information about them. In this work, we analyze the mainstream anonymity solutions proposed for LBSs based on k-anonymity, {{and point out}} that these do not follow the safe assumptions as per the original definition of k-anonymity. We propose an alternative anonymity property, LBS (k,T) -anonymity, that ensures anonymity of a <b>user's</b> <b>query</b> against an attacker who knows about the issuance of the <b>user</b> <b>query</b> within a <b>time</b> window. We evaluate {{the vulnerability of the}} approaches in the literature to this type of attack that we believe is very basic and important, and assess the performance of our proposed algorithm for achieving LBS (k,T) -anonymity in terms of providing optimal solution...|$|R
40|$|AbstractIn this paper, {{a dynamic}} load-balancing scheme is {{developed}} for implementation in an agent-based distributed system. Load balancing is achieved via agent migration from heavily loaded nodes to lightly loaded ones. The credit based concept {{is used for}} the dual objective of: (1) the selection of candidate agents for migration, and (2) the selection of destination nodes. This represents an elaboration of previous research work aimed at the selection of agents only. Multiple linear regression is used to achieve our dual objective. A complexity analysis for the proposed system operation is performed, the complete operation is O(n 2) where n is the number of agents on the local host. The proposed system is implemented using JADE (Java Agent DEvelopment Framework), the multiple linear regression operation is performed using R Tool. The experimental results show a modified system operation in terms of reduced <b>user’s</b> <b>query</b> response <b>time,</b> by implementing agent load balancing...|$|R
40|$|In recent years, the {{management}} and processing of data streams has become a topic of active research in several fields of computer science such as, distributed systems, database systems, and data mining. A data stream {{can be thought of}} as a transient, continuously increasing sequence of data. In data streams' applications, because of online monitoring, answering to the <b>user's</b> <b>queries</b> should be <b>time</b> and space efficient. In this paper, we consider the special requirements of indexing to determine the performance of different techniques in data stream processing environments. Stream indexing has main differences with approaches in traditional databases. Also, we compare data stream indexing models analytically that can provide a suitable method for stream indexing...|$|R
40|$|The first {{generation}} of search engines caused relatively few legal problems in terms of copyright. They merely retrieved text data from the web and displayed short text-snippets in reply to a specific <b>user</b> <b>query.</b> Over <b>time,</b> search engines have become efficient retrieval tools, which have shifted from a reactive response mode ('user pull') to pro-actively proposing options ('user push'). Moreover, they will soon be organising and categorising {{of all sorts of}} audio-visual in-formation. Due to these transformations, search engines are becom-ing fully-fledged information portals, rivalling traditional media. This will cause tensions with traditional media and content owners. As premium audiovisual content is generally more costly to produce and commercially more valuable than text-based content, one may expect copyright litigation problems to arise in the future. Given this perspective, this article briefly introduces search engine technology and business rationale and then summarizes the nature of current copyright litigation. The copyright debate is then put in the audio-visual context with a view to discussing elements for future policies. JRC. DDG. J. 4 -Information Societ...|$|R
40|$|The {{need for}} pattern {{discovery}} in long time series data led researchers to develop algorithms for similarity search. Most {{of the literature}} about time series focuses on algorithms that index time series and bring the data into the main storage, thus providing fast information retrieval on large time series. This paper reviews {{the state of the}} art in visualizing time series, and focuses on techniques that enable <b>users</b> to interactively <b>query</b> <b>time</b> series. Then it presents TimeSearcher 2, a tool that enables users to explore multidimensional data using coordinated tables and graphs with overview+detail, filter the time series data to reduce the scope of the search, select an existing pattern to find similar occurrences, and interactively adjust similarity parameters to narrow the result set. This tool is an extension of previous work, TimeSearcher 1, which uses graphical timeboxes to interactively <b>query</b> <b>time</b> series data...|$|R
40|$|Abstract This work {{presents}} an information retrieval model developed {{to deal with}} hyperlinked environments. The model is based on belief networks and provides a framework for combining information extracted from {{the content of the}} documents with information derived from cross-references among the documents. The information extracted from the content of the documents is based on statistics regarding the keywords in the collection {{and is one of the}} basis for traditional information retrieval (IR) ranking algorithms. The information derived from cross-references among the documents is based on link refer-ences in a hyperlinked environment and has received in-creased attention lately due to the success of the Web. We discuss a set of strategies for combining these two types of sources of evidential information and experiment with them using a reference collection extracted from the Web. The results show that this type of combination can improve the retrieval performance without requiring any extra information from the <b>users</b> at <b>query</b> <b>time.</b> In our experiments, the improvements reach up to 59 % in terms of average precision figures...|$|R
40|$|Abstract We {{outline the}} need for search engines to provide user {{feedback}} on the expected <b>time</b> for a <b>query,</b> describe a scheme for learning a model of <b>query</b> <b>time</b> by observing sample queries, and discuss the results obtained {{for a set of}} actual <b>user</b> <b>queries</b> on a document collection using the MG search engine. ...|$|R
40|$|Abstract. Recently, several {{approaches}} that mine frequent XML query patterns and cache their {{results have been}} proposed to improve <b>query</b> response <b>time.</b> However, frequent XML query patterns mined by these approaches ignore the temporal sequence between <b>user</b> <b>queries.</b> In this paper, we {{take into account the}} temporal features of <b>user</b> <b>queries</b> to discover association rules, which indicate that when a user inquires some information from the XML document, she/he will probably inquire some other information subsequently. We cluster XML queries according to their semantics first and then mine association rules between the clusters. Moreover, not only positive but also negative association rules are discovered to design the appropriate cache replacement strategy. The experimental results showed that our approach considerably improved the caching performance by significantly reducing the <b>query</b> response <b>time.</b> ...|$|R
40|$|Existing search {{services}} rely {{solely on}} a query's occurrence in the document collection to locate relevant documents. They typically do not perform any task or topic-based analysis of queries using other available resources, and do not leverage changes in <b>user</b> <b>query</b> patterns over <b>time.</b> In this paper provided within a set of techniques and metrics for performing temporal analysis on query logs. The metrics proposed for our log analysis are shown to be reasonable and informative, {{and can be used}} to detect changing trends and patterns in the query stream, thus providing valuable data to a search service. We continue with an algorithm for automatic topical classification of web queries. Results are presented showing that our classification approach can be successfully applied to {{a significant portion of the}} query stream, making it possible for search services to leverage it for improving search effectiveness and efficiency. &# 13; &# 13; Keywords- Web mining, Classify Query Technique, Cluster...|$|R
40|$|We propose an {{augmented}} Web {{space and}} its query language to support geographical querying and sequential plan creation utilizing a digital {{city that is}} a city-based information space on the Internet. The augmented Web space involves {{a new approach to}} integrate the World Wide Web (WWW) and a geographic information system (GIS). The augmented Web space consists of home pages (HP), hyperlinks, and generic links that represent geographical relations between HPs. The generic links are created dynamically using geographical evaluation functions included in a <b>user's</b> search <b>query</b> each <b>time</b> one is issued. A query also includes a path expression showing how to navigate the HPs, hyperlinks, and generic links. Since the path expression is an extended regular expression, we can describe an arbitrary sequence of users' search actions for navigating the augmented Web space. We have applied the proposed augmented Web space to Digital City Kyoto, a city information service system that is accessed through a 3 D walk-through implementation and a map-based interface. Each <b>time</b> a <b>user's</b> <b>query</b> is issued through the 3 D and 2 D interfaces, Digital City Kyoto creates an augmented Web space, and navigates the Web information space based on the path expression in the query. 1...|$|R
40|$|In this paper, {{we propose}} a new {{strategy}} for optimizing the placement of bin boundaries to minimize the cost of query evaluation using bitmap indices with binning. For attributes {{with a large number}} of distinct values, often the most efficient index scheme is a bitmap index with binning. However, this type of index {{may not be able to}} fully resolve some <b>user</b> <b>queries.</b> To fully resolve these queries, one has to access parts of the original data to check whether certain candidate records actually satisfy the specified conditions. We call this procedure the candidate check, which usually dominates the total <b>query</b> processing <b>time.</b> Given a set of <b>user</b> <b>queries,</b> we seek to minimize the total time required to answer the queries by optimally placing the bin boundaries. We show that our dynamic programming based algorithm can efficiently determine the bin boundaries. We verify our analysis with some real <b>user</b> <b>queries</b> from the Sloan Digital Sky Survey. For queries that require significant amount of time to perform candidate check, using our optimal bin boundaries reduces the candidate check time by a factor of 2 and the total <b>query</b> processing <b>time</b> by 40 %...|$|R
40|$|Introduction The {{conceptual}} indexing and {{retrieval system}} at Sun Microsystems Laboratories (Woods et al., 2000) {{is designed to}} help people find specific answers to specific questions in unrestricted text. It uses a combination of syntactic, semantic, and morphological knowledge, together with taxonomic subsumption techniques, to address differences in terminology between a <b>user's</b> <b>queries</b> and the material that may answer them. At indexing time, the system builds a conceptual taxonomy of all the words and phrases in the indexed material. This taxonomy is based on the morphological structure of words, the syntactic structure of phrases, and semantic relations between meanings of words that it knows in its lexicon. At <b>query</b> <b>time,</b> the system automatically retrieves any concepts that are subsumed by (i. e., are more specific than) the <b>user's</b> <b>query</b> terms, according to this taxonomy. The system uses a penalty-based relaxation-ranking method to locate and rank potentially relevant passages i...|$|R
40|$|Taux d'acceptation (13 %) International audienceThis paper {{presents}} SHIRI-Querying, {{an approach}} for semantic search in semistructured documents. We propose {{a solution to}} tackle incompleteness and imprecision of annotations at <b>querying</b> <b>time.</b> This solution relies on two elementary reformulation types that use the notion of aggregation and the documents structure. We present the dynamic algorithm (DREQ) which combines these elementary transformations to construct ordered reformulations of <b>user</b> <b>queries.</b> Experimentations on two real datasets show that reformulations increase greatly the recall and that the precision is better for the first returned answers...|$|R
40|$|Clustering is {{important}} task for any recommendation system. Clustering method suggested by many researchers for search engine optimization. Search engine help user for better searching by <b>user’s</b> <b>query</b> recommendation. Clustering is helpful for finding actual relation between different queries {{which are not}} same as they seems. But do clustering of <b>user</b> <b>query</b> is also a difficult task because of user enters lots of type and varying <b>queries.</b> Many <b>time</b> these <b>queries</b> may very short to get their real meaning and also can generate different meanings. Any single query may have various meaning on other hand many different query words may have common meaning for searching contents. Lots of clustering methods are given in last decades for search engine optimization but these methods unable to proper utilization various information hidden in <b>user</b> <b>query</b> log. This paper gives a novel clustering approach based on to identify query similarity and apply SOM clustering for effective clustering results. We propose a novel similarity matrix for <b>user</b> <b>queries</b> by uses of URL clicked by user trough searching results. Text similarity and time similarity are also measure for calculating similarity between two queries. This method shows good results within clustering performance to compare with other existing methods. 1...|$|R
40|$|Hybrid pre-query term {{expansion}} using Latent Semantic Analysis Latent semantic {{retrieval methods}} (unlike vector space methods) take the document and query vectors and map {{them into a}} topic space to cluster related terms and documents. This produces a more precise retrieval but also a long <b>query</b> <b>time.</b> We present a new method of document retrieval which allows us to process the latent semantic information into a hybrid Latent Semantic-Vector Space query mapping. This mapping automatically expands the <b>users</b> <b>query</b> based on the latent semantic information in the document set. This expanded query is processed using a fast vector space method. Since we have the latent semantic data in a mapping, {{we are able to}} store and retrieve vector information in the same fast manner that the vector space method offers. Multiple mappings are combined to produce hybrid latent semantic retrieval which provide precision results 5 % greater than the vector space method and fast <b>query</b> <b>times.</b> ...|$|R
40|$|Abstract. It is {{rare for}} data’s history to include {{computational}} processes alone. Even when software generates data, users ultimately decide to execute software procedures, choose their configuration and inputs, reconfigure, halt and restart processes, and so on. Understanding the provenance of data thus involves understanding the reasoning of users behind these decisions, but demanding that users explicitly document decisions could be intrusive if implemented naively, and impractical in some cases. In this paper, therefore, we explore {{an approach to}} transparently deriving the provenance of <b>user</b> decisions at <b>query</b> <b>time.</b> The <b>user</b> reasoning is simulated, and if {{the result of the}} simulation matches the documented decision, the simulation is taken to approximate the actual reasoning. The plausibility of this approach requires that the simulation mirror human decision-making, so we adopt an automated process explicitly modelled on human psychology. The provenance of the decision is modelled in Open Provenance Model (OPM), allowing it to be queried {{as part of a larger}} provenance graph, and an OPM profile is provided to allow consistent querying of provenance across user decisions...|$|R
40|$|Source {{selection}} {{is one of}} the foremost challenges for searching deep-web. For a <b>user</b> <b>query,</b> source selection involves selecting a subset of deep-web sources expected to provide relevant answers to the <b>user</b> <b>query.</b> Existing source selection models employ query-similarity based local measures for assessing source quality. These local measures are necessary but not sufficient as they are agnostic to source trustworthiness and result importance, which, given the autonomous and uncurated nature of deep-web, have become indispensible for searching deep-web. SourceRank provides a global measure for assessing source quality based on source trustworthiness and result importance. Source-Rank’s effectiveness has been evaluated in single-topic deep-web environments. The goal of the thesis is to extend sourcerank to a multi-topic deep-web environment. Topic-sensitive sourcerank is introduced as an effective way of extending sourcerank to a deep-web environment containing a set of representative topics. In topic-sensitive sourcerank, multiple sourcerank vectors are created, each biased towards a representative topic. At <b>query</b> <b>time,</b> using the topic of query keywords, a query-topic sensitive, composite sourcerank vector is computed as a linear combination of these pre-computed biased sourcerank vectors. Extensiv...|$|R
40|$|Radiologists {{frequently}} {{search the}} Web to find {{information they need}} to improve their practice, and knowing the types of information they seek could be useful for evaluating Web resources. Our goal was to develop an automated method to categorize unstructured <b>user</b> <b>queries</b> using a controlled terminology and to infer the type of information users seek. We obtained the query logs from two commonly used Web resources for radiology. We created a computer algorithm to associate RadLex-controlled vocabulary terms with the <b>user</b> <b>queries.</b> Using the RadLex hierarchy, we determined the high-level category associated with each RadLex term to infer the type of information users were seeking. To test the hypothesis that the term category assignments to <b>user</b> <b>queries</b> are non-random, we compared the distributions of the term categories in RadLex with those in <b>user</b> <b>queries</b> using the chi square test. Of the 29, 669 unique search terms found in <b>user</b> <b>queries,</b> 15, 445 (52 %) could be mapped to one or more RadLex terms by our algorithm. Each query contained an average of one to two RadLex terms, and the dominant categories of RadLex terms in <b>user</b> <b>queries</b> were diseases and anatomy. While the same types of RadLex terms were predominant in both RadLex itself and <b>user</b> <b>queries,</b> the distribution of types of terms in <b>user</b> <b>queries</b> and RadLex were significantly different (p[*]<[*] 0. 0001). We conclude that RadLex can enable processing and categorization of <b>user</b> <b>queries</b> of Web resources and enable understanding the types of information users seek from radiology knowledge resources on the Web...|$|R
40|$|The rapid {{emergence}} of XML {{as a standard}} for data exchange over the Web has led to considerable interest in the problem of securing XML documents. In this context, query eva]- uation engines need to ensure that <b>user</b> <b>queries</b> only use and return XML data the user is al- lowed to access. These added access control checks can considerably increase <b>query</b> evaluation <b>time.</b> In this paper, we consider the problem of optimizing the secure evaluation of XML twig queries...|$|R
40|$|Abstract—This {{work is a}} {{proposal}} for an efficient yet secure XML access control enforcement which has been specifically designed to support fine-grained security policy. Without a structural summary, to ensure that <b>user</b> <b>queries</b> only use and return XML data the user is allowed to access can be quite inefficient in checking <b>user</b> <b>queries</b> and in traversing for XPath queries with the self-or-descendant axis(“//”). In this paper, we propose the SQ-Filter based on metadata in the DTD. SQ-Filter is a pre-processing method to rewrite <b>user</b> <b>query</b> by extending/eliminating query tree nodes and by injecting operators combining set of nodes from the <b>user</b> <b>query</b> point of view. Experimental results clearly demonstrate that our approach is efficient...|$|R
40|$|In data {{integration}} efforts, portal development in particular, much development time {{is devoted to}} entity resolution. Often advanced similarity measurement techniques are used to remove semantic duplicates or solve other semantic conflicts. It proves impossible, however, to automatically {{get rid of all}} semantic problems. An often-used rule of thumb states that about 90 % of the development effort is devoted to semi-automatically resolving the remaining 10 % hard cases. In an attempt to significantly decrease human effort at {{data integration}} time, we have proposed an approach that strives for a ‘good enough’ initial integration which stores any remaining semantic uncertainty and conflicts in a probabilistic database. The remaining cases are to be resolved with <b>user</b> feedback during <b>query</b> <b>time.</b> The main contribution of this paper is an experimental investigation of the effects and sensitivity of rule definition, threshold tuning, and user feedback on the integration quality. We claim that our approach indeed reduces development effort — and not merely shifts the effort—by showing that setting rough safe thresholds and defining only a few rules suffices to produce a ‘good enough’ initial integration that can be meaningfully used, and that user feedback is effective in gradually improving the integration quality...|$|R
40|$|Entity-oriented {{retrieval}} aims {{to return}} a list of relevant entities rather than documents to provide exact answers for <b>user</b> <b>queries.</b> The nature of entity-oriented retrieval requires identifying the semantic intent of <b>user</b> <b>queries,</b> i. e., understanding the semantic role of query terms and determining the semantic categories which indicate the class of target entities. Existing methods {{are not able to}} exploit the semantic intent by capturing the semantic relationship between terms in a query and in a document that contains entity related information. To improve the understanding of the semantic intent of <b>user</b> <b>queries,</b> we propose concept-based retrieval method that not only automatically identifies the semantic intent of <b>user</b> <b>queries,</b> i. e., Intent Type and Intent Modifier but introduces concepts represented by Wikipedia articles to <b>user</b> <b>queries.</b> We evaluate our proposed method on entity profile documents annotated by concepts from Wikipedia category and list structure. Empirical analysis reveals that the proposed method outperforms several state-of-the-art approaches...|$|R
40|$|We {{present an}} {{approach}} for optimizing {{the performance of}} information agents by materializing useful information. A critical problem with information agents, particularly those gathering and integrating information from Web sources is a high <b>query</b> response <b>time.</b> This is because the data needed to answer <b>user</b> <b>queries</b> is present across several different Web sources (and in several pages within a source) and retrieving,extracting and integrating the data is time consuming. We address this problem by materializing useful classes of information and defining them as auxiliary data sources for the information agent. The key challenge here is to identify the content and schema of the classes of information that {{would be useful to}} materialize. We present an algorithm that identifies such classes by analyzing patterns in <b>user</b> <b>queries.</b> We describe an implementation of our approach and experiments in progress. We also discuss other important problems that we will address in optimizing information [...] ...|$|R
40|$|Data {{warehouse}} maintenance algorithms usually work off-line, {{making the}} warehouse unavailable to users. However, since most organizations require continuous operation, we need {{be able to}} perform the updates online, concurrently with <b>user</b> <b>queries.</b> To guarantee that <b>user</b> <b>queries</b> access a consistent view of the warehouse, online update algorithms introduce redundancy in order to store multiple versions of the data objects that are being changed. In this paper, we present an online warehouse update algorithm, that stores multiple versions of data as separate rows (vertical redundancy). We compare our algorithm to another online algorithm that stores multiple versions within each tuple by extending the table schema (horizontal redundancy). We have implemented both algorithms on top of an Informix Dynamic Server and measured their performance under varying workloads, focusing on their impact on <b>query</b> response <b>times.</b> Our experiments show that, except for a limited number of cases, [...] ...|$|R
40|$|Information Retrieval IR systems store a {{large volume}} of {{unstructured}} data and provide search results for a <b>user</b> <b>query.</b> The performance of the IR systems depends upon the relevancy of the search results with <b>user</b> <b>query.</b> Page ranking algorithms are used to assign rank to the retrieved results for a <b>user</b> <b>query.</b> Page ranking algorithms are mainly categories in to web structure mining and web content mining. In literature many page ranking algorithms have been proposed to improve the relevancy of search results for a <b>user</b> <b>query.</b> In this paper a new hybrid page ranking algorithm using web structure mining and web content mining has been proposed. The algorithm is implemented and tested on a test data results shows that the new proposed algorithm performs better than the existing algorithm...|$|R
40|$|In social {{networks}} such as Orkut, www. orkut. com, {{a large portion}} of the <b>user</b> <b>queries</b> refer to names of other people. Indeed, more than 50 % of the queries in Orkut are about names of other users, with an average of 1. 8 terms per <b>query.</b> Further, the <b>users</b> usually search for people with whom they maintain relationships in the network. These relationships can be modelled as edges in a friendship graph, a graph in which the nodes represent the users. In this context, search ranking can be modelled as a function that depends on the distances among users in the graph, more specifically, of shortest paths in the friendship graph. However, applica-tion of this idea to ranking is not straightforward because the large size of modern {{social networks}} (dozens of millions of users) prevents efficient computation of shortest paths at <b>query</b> <b>time.</b> We overcome this by designing a ranking for-mula that strikes a balance between producing good results and reducing <b>query</b> processing <b>time.</b> Using data from the Orkut social network, which includes over 40 million users, we show that our ranking, augmented by this new signal, produces high quality results, while maintaining <b>query</b> pro-cessing <b>time</b> small...|$|R
40|$|Abstract — Image {{retrieval}} and re-ranking as per <b>user</b> image <b>query</b> {{has become}} the popular and effective of image retrieval techniques. Similar <b>user</b> <b>query</b> and click through log {{is important for the}} success of an image search engine. User search goal analysis will also enhance user experience of a search engine. Using this as a base and leveraging click logs we propose a new design in this paper. In this paper, we focus on designing a new machine learning approach for auto classification and grouping similar <b>user</b> <b>queries</b> for image search system to address a specific kind of image search. Our approach finds most relevant images for a user based on a given <b>user</b> <b>query.</b> Here, our focus is to evaluate the effective association between <b>User</b> <b>Queries</b> and Click through data and customizes search results according to each individual preferences/interests. We also present a ranking procedure to score the images that are retrieved using the proposed approach...|$|R
30|$|The {{idea of the}} {{approach}} is to recommend personalized MDX <b>queries</b> to the <b>user.</b> The <b>queries</b> are adapted to user’s preferences and objectives of analysis. Preferences of the user are detected implicitly using collaborative filtering technique by comparing the current <b>user</b> <b>query</b> with previous queries triggered by former users and recorded in a log file. The comparison between the current <b>user</b> <b>query</b> and previous queries is performed using the spatio-semantic similarity measure.|$|R
40|$|Sponsored search {{systems are}} tasked with {{matching}} queries to relevant advertisements. The current state-of-the-art matching algorithms expand the <b>user’s</b> <b>query</b> {{using a variety}} of external resources, such as Web search results. While these expansion-based algorithms are highly effective, they are largely inefficient and cannot be applied in real-time. In practice, such algorithms are applied offline to popular queries, with the results of the expensive operations cached for fast access at <b>query</b> <b>time.</b> In this paper, we describe an efficient and effective approach for matching ads against rare queries that were not processed offline. The approach builds an expanded query representation by leveraging offline processing done for related popular queries. Our experimental results show that our approach significantly improves the effectiveness of advertising on rare queries with only a negligible increase in computational cost...|$|R
40|$|Accessing online {{information}} {{remains an}} inexact science. While valuable {{information can be}} found, typically many irrelevant documents are also retrieved and many relevant ones are missed. Terminology mismatches between the <b>user's</b> <b>query</b> and document contents is a main cause of retrieval failures. Expanding a <b>user's</b> <b>query</b> with related words can improve search performance, but the problem of identifying related words remains. This research uses corpus linguistics techniques to automatically discover word similarities directly from {{the contents of the}} untagged TREC database and to incorporates that information in the PRISE information retrieval system. The similarities are calculated based on the contexts in which a set of target words appear. Using these similarities, <b>user</b> <b>queries</b> are automatically expanded, resulting in conceptual retrieval rather than requiring exact word matches between queries and documents. 1. INTRODUCTION Expanding a <b>user's</b> <b>query</b> with related terms can improve searc [...] ...|$|R
40|$|We {{present a}} {{semantic}} caching approach for optimizing {{the performance of}} information mediators. A critical problem with information mediators, particularly those gathering and integrating information from Web sources is a high <b>query</b> response <b>time.</b> This is because the data needed to answer <b>user</b> <b>queries</b> is present across several different Web sources (and in several pages within a source) and retrieving, extracting and integrating the data is time consuming. We address this problem using a semantic caching approach, where we cache useful classes of information and define them as auxiliary data sources for the information mediator. The key challenge here is to identify the content and schema of the classes of information that {{would be useful to}} cache as the auxiliary sources. We present an algorithm that identifies such classes by analyzing patterns in <b>user</b> <b>queries.</b> The algorithm utilizes the kr system loom to reason about classes of information to cache. We describe an implementation of [...] ...|$|R
5000|$|... {{create an}} {{environment}} in which <b>user</b> <b>queries</b> may be formulated intuitively ...|$|R
3000|$|In above subsections, we {{presented}} the indexing strategy and the structures {{to store the}} precomputed results. Now {{we are ready to}} study the process of answering <b>user</b> <b>queries.</b> We classify the <b>user</b> <b>queries</b> into three types: (1) Single selectivity query, (2) Drill-down query, (3) Roll-up query as we can see in the following models: [...]...|$|R
40|$|In {{this paper}} {{we present a}} short survey of fuzzy and Semantic {{approaches}} to Knowledge Extraction. The goal of such approaches is to define flexible Knowledge Extraction Systems {{able to deal with}} the inherent vagueness and uncertainty of the Extraction process. It has long been recognised that interactivity improves the effectiveness of Knowledge Extraction systems. Novice <b>user's</b> <b>queries</b> is the most natural and interactive medium of communication and recent progress in recognition is making it possible to build systems that interact with the user. However, given the typical novice <b>user's</b> <b>queries</b> submitted to Knowledge Extraction systems, it is easy to imagine that the effects of goal recognition errors in novice <b>user's</b> <b>queries</b> must be severely destructive on the system's effectiveness. The experimental work reported in this paper shows that the use of classical Knowledge Extraction techniques for novice <b>user's</b> <b>query</b> processing is robust to considerably high levels of goal recognition errors. Moreover, both standard relevance feedback and pseudo relevance feedback can be effectively employed to improve the effectiveness of novice <b>user's</b> <b>query</b> processing. Comment: arXiv admin note: substantial text overlap with arXiv: 1206. 0925, arXiv: 1206. 161...|$|R
