0|10000|Public
3000|$|... which signify {{additional}} capital {{on top of}} the originally computed external assets e that is held by the core and periphery banks, respectively. As before, the <b>updated</b> <b>amount</b> <b>of</b> the external assets is then divided into liquid and illiquid assets according to the parameter ρ, and the simulations are run.|$|R
40|$|This NOC {{application}} is provided to <b>update</b> the description <b>of</b> <b>amounts</b> <b>of</b> material handled, and to <b>update</b> the calculation <b>of</b> potential for emissions and resultant calculation of offsite TEDE. This NOC also includes an <b>updated</b> description <b>of</b> the various emission units at WSCF, including use of portable tanks to receive and remove liquid waste contaminated with {{low levels of}} radioactive contamination. The resultant, adjusted estimate for TEDE to the hypothetical MEI due to all combined unabated emissions from WSCF is 1. 4 E- 02 millirem per year. The total adjusted estimate for all combined abated emissions is 2. 8 E- 03 millirem per year. No single emission unit at the WSCF Complex exceeds a potential (unabated) offsite dose of 2. 7 E- 03 millirem per year...|$|R
40|$|This paper {{presents}} {{a method for}} simulating color aging of leaves. Our technique is inspired by natural processes. We consider the flow of fluid in the leaf, and its evaporation through stomata. To model this, we use three maps describing the stomata and venation distributions and the fluid flow, respectively. We conduct a simulation that uses these maps to <b>update</b> the <b>amount</b> <b>of</b> fluid sourced into, diffused, and sinked out of the plant structure. The <b>amount</b> <b>of</b> fluid present at each location is used to control {{the color of the}} leaf or petal. (a) (b...|$|R
30|$|Next, Step  4 {{solves the}} hour-m {{scheduling}} problem and Step  5 fixes {{the values of}} the hour-m generator and PEV-charging scheduling variables only. The values of decision variables for hours (m+ 1) through (m+T) are determined in subsequent iterations of the algorithm. Step  6 computes the cost incurred in hour m, based on {{the values of the}} hour-m decision variables, which are fixed in Step  5. Finally, Step  7 <b>updates</b> the <b>amount</b> <b>of</b> charging energy that remains unserved for vehicles that arrived before hour m, based on the PEV-charging decisions determined by the hour-m scheduling model.|$|R
40|$|We studied which factors {{affect the}} {{adherence}} to password policies by users within a government agency. The agency’s functions are of such nature that a potential information leakage can endanger peoples’ lives. The factors we studied were resetting passwords, <b>updating</b> passwords, <b>amount</b> <b>of</b> passwords, passwords complexity, user’s fear of forgetting passwords, organizational cul- ture and informal routines. Five semi-structured {{interviews were conducted}} with employees in the organization. We found that four factors affected the users’ behaviour in the studied department: resetting passwords, updating passwords, the organizational culture and informal routines. Validerat; 20120621 (anonymous...|$|R
25|$|ACS {{sends out}} {{a large number of}} virtual ant agents to explore many {{possible}} routes on the map. Each ant probabilistically chooses the next city to visit based on a heuristic combining the distance to the city and the <b>amount</b> <b>of</b> virtual pheromone deposited on the edge to the city. The ants explore, depositing pheromone on each edge that they cross, until they have all completed a tour. At this point the ant which completed the shortest tour deposits virtual pheromone along its complete tour route (global trail <b>updating).</b> The <b>amount</b> <b>of</b> pheromone deposited is inversely proportional to the tour length: the shorter the tour, the more it deposits.|$|R
40|$|In {{the last}} years more and more {{applications}} make use of 3 d building models. Automatic reconstruction {{is essential for the}} acquisition and <b>updating</b> <b>of</b> the huge <b>amount</b> <b>of</b> data. For many applications it is useful to be able to transform the object representation automatically in order to generalize the geometry. For this task a structurally rich description of the object is needed. We presented a reconstruction procedure which models the structure of facades. This method is based on a facade grammar and a reversible jump Markov Chain Monte Carlo process. For the stochastic process we need prior knowledge about facades. The distribution of facade elements can influence the proposal of rules. Until now we made general assumptions about these distributions. In this paper we analyze facades from a set of facade images and derive prior facade information which support the modelling process. 1. 1 Motivation...|$|R
40|$|Abstract — Proteins {{are highly}} {{flexible}} and large amplitude deformations of their structure, also called slow dynamics, are often decisive to their function. We present a two-level rendering approach that enables visualization of slow dynamics of large protein assemblies. Our approach is aligned with a hierarchical model of large scale molecules. Instead <b>of</b> constantly <b>updating</b> positions <b>of</b> large <b>amounts</b> <b>of</b> atoms, we <b>update</b> {{the position and}} rotation of residues, i. e., higher level building blocks of a protein. Residues are represented by one vertex only indicating its position and additional information defining the rotation. The atoms in the residues are generated on-the-fly on the GPU, exploiting the new graphics hardware geometry shader capabilities. Moreover, we represent the atoms by billboards instead of tessellated spheres. Our representation is then significantly faster and pixel precise. We demonstrate the usefulness of our new approach {{in the context of}} our collaborative bioinformatics project. Index Terms—Molecular visualization, hardware acceleration, protein dynamics. ...|$|R
40|$|Automated {{road network}} {{extraction}} from remotely sensed imagery is of {{importance in the}} context of road databases creation, refinement and <b>updating.</b> Substantial <b>amount</b> <b>of</b> research has been carried out to extract road network from satellite imagery in the photogrammetric and computer vision communities. However, little research has been conducted on utility of multi-temporal satellite images {{in the context of}} road extraction. This paper first proposes a simple scheme to detect vehicles using high resolution multi-temporal images of a geographic location and uses the detected road seeds in Fourier descriptor based road tracking algorithm to extract the road network. Performance of the proposed method on CARTOSAT- 2 images is discussed. Index Terms — Object detection, Road extraction, Fourier descriptor, CARTOSAT- 2, High resolution satellite imager...|$|R
40|$|Abstract:––In this Paper, Optimization {{of error}} is {{considered}} using power adaptation algorithm while transmission {{is carried out}} The Maximum Power Adaptation algorithm(MAPAA) is based on <b>updating</b> the <b>amount</b> <b>of</b> power transmitted for each bit according to its importance in the image quality {{as measured by the}} mean-square error. This Maximum Power Adaptation Algorithm is used to find the optimum power distribution vector such that the RootMeanSquareError (RMSE) is minimized subject to the constraints that the power per bit is kept constant considering Maximum Root Mean Square Error. The above optimization is done for RMSE regardless of the average probability of bit error and the PAPR is kept below a certain limit. Maximum Power Adaptation Algorithm shows better performance compared to the case of Conventional power adaptation while reducing the Peak-to-Average Power (PAPR) Ratio...|$|R
50|$|DSDV {{requires}} a regular <b>updates</b> <b>of</b> its routing tables, which uses up battery {{power and a}} small <b>amount</b> <b>of</b> bandwidth even when the network is idle.|$|R
40|$|Set-oriented {{constructs}} for {{forward chaining}} rule-based systems {{are presented in}} this paper. These constructs allow arbitrary <b>amounts</b> <b>of</b> data to be matched and changed within the execution of a single rule. Second order tests on the data can {{be included in the}} match. The ability of a single rule to directly access all of the data to be manipulated eliminates the need for unwieldy control mechanisms and marking schemes. Adding this expressivity to rule-based languages enhances their value and capabilities as database programming languages since operations on entire relations can now be clearly specified, thus providing the database management system an opportunity to use its ability to <b>update</b> large <b>amounts</b> <b>of</b> data. Additionally, these set-oriented constructs can provide a basis for more efficient implementations of rule-based systems, for both the traditional memory-based systems and the emerging disk-based ones. The work described has been implemented using an extended version of the Rete network algorithm...|$|R
40|$|Abstract: In data-warehousing {{there is}} large <b>amount</b> <b>of</b> data can {{store in the}} database. For {{maintaining}} the updated database it takes more time. For more scalability it can take more processing time also. So we introduce a new framework to achieve this problem. In this we included grouping and partitioning to schedule the tasks <b>of</b> <b>updating</b> more number <b>of</b> jobs in less time with respect to execution time and utilization time. In Data warehousing when dealing with large <b>amount</b> <b>of</b> database, {{there are so many}} problems <b>of</b> <b>updating</b> <b>of</b> the database. When any transaction done on the database it takes more amore <b>amount</b> <b>of</b> time to <b>update...</b>|$|R
40|$|Abstract Concept {{hierarchies}} greatly help in {{the organization}} and reuse of information and are widely used {{in a variety of}} information systems applications. In this paper, we describe a method for efficiently storing and querying data organized into concept hierarchies and dispersed over a DHT. In our method, peers individually decide on the level of indexing according to the granularity of the incoming queries. Roll-up and drill-down operations are performed on a pernode basis in order to minimize the required bandwidth for answering queries on variable aggregation levels. We motivate our approach by applying it on a large-scale Grid system: Specifically, we apply our fully decentralized scheme that creates, queries and <b>updates</b> large volumes <b>of</b> hierarchical data on-line and replace the traditional centralized and strictly indexed information systems. Our extensive experimental results support this argument on many diverse configurations: Our system proves very efficient in skewed workloads, both over single and multiple hierarchy levels at the same time. It adapts to sudden changes in popularity and effectively stores and <b>updates</b> large <b>amounts</b> <b>of</b> data at very low cost...|$|R
40|$|Concept {{hierarchies}} greatly help in {{the organization}} and reuse of information and are widely used {{in a variety of}} information systems applications. In this paper, we describe a method for efficiently storing and querying data organized into concept hierarchies and dispersed over a DHT. In our method, peers individually decide on the level of indexing according to the granularity of the incoming queries. Roll-up and drill-down operations are performed on a per-node basis in order to minimize the required bandwidth for answering queries on variable aggregation levels. We motivate our approach by applying it on a large-scale Grid system: Specifically, we apply our fully decentralized scheme that creates, queries and <b>updates</b> large volumes <b>of</b> hierarchical data on-line and replace the traditional centralized and strictly indexed information systems. Our extensive experimental results support this argument on many diverse configurations: Our system proves very efficient in skewed workloads, both over single and multiple hierarchy levels at the same time. It adapts to sudden changes in popularity and effectively stores and <b>updates</b> large <b>amounts</b> <b>of</b> data at very low cost. © 2010 Springer Science+Business Media, LLC...|$|R
40|$|The {{present report}} on the European Ephemeroptera {{database}} is an outcome of the Eurolimpacs project, Work Package 7 ‘Indicators of Ecosystem Health’. It is an <b>update</b> <b>of</b> the activities presented in the deliverable 189 : ‘Indicator value database for Ephemeroptera Phase I Report’ (Armanini et al., 2007) and deals with the compilation of an autoecological matrix for mayflies from literature data. In the Phase I report, the approach and methods to derive the autoecological information from the literature were described in details and here we present {{an overview of the}} full results of the activity, in terms <b>of</b> <b>amount</b> <b>of</b> information gained. ...|$|R
40|$|Trial-to-trial {{variability}} {{in decision making}} {{can be caused by}} {{variability in}} information processing as well as by variability in response caution. In this paper, we study which neural components code for trial-to-trial adjustments in response caution using a new computational approach that quantifies response caution on a single-trial level. We found that the frontostriatal network <b>updates</b> the <b>amount</b> <b>of</b> response caution. In particular, when human participants were required to respond quickly, we found a positive correlation between trial-to-trial fluctuations in response caution and the hemodynamic response in the presupplementary motor area and dorsal anterior cingulate. In contrast, on trials that required a change from a speeded response mode to a more accurate response mode or vice versa, we found a positive correlation between response caution and hemodynamic response in the anterior cingulate proper. These results indicate that for each decision, response caution is set through corticobasal ganglia functioning, but that individual choices differ according to the mechanisms that trigger changes in response caution...|$|R
5000|$|June 8 - Major League Baseball {{released}} an <b>update</b> <b>of</b> All Star Game voting. The Cubs received the four highest <b>amount</b> <b>of</b> votes among NL teams: Anthony Rizzo, Kris Bryant, Dexter Fowler, and Ben Zobrist. Addison Russell is also leading at SS.|$|R
40|$|This Paper conducts on a {{simulation}} {{to estimate the}} administrative investment necessary for maintaining and <b>updating</b> the infrastructure <b>of</b> present-day Japanese Prefectures. The field of the analyzed infrastructure is four fields (the road, water service, drainage, and the trash disposal facility). In this text, <b>amount</b> <b>of</b> an administrative investment necessary to estimate the administrative investment done in the past, and to maintain the infrastructure stock of the current state by the simulation in addition was estimated. It has been understood that maintenance and <b>update</b> <b>of</b> the infrastructure are the pressing issues {{as a result of}} these. Especially, maintenance and <b>update</b> <b>of</b> the infrastructure in the metropolitan area and a further provision of social capital in the local area will become a problem in the future. administrative investment, simulation, maintenance and <b>update</b> <b>of</b> the infrastructure...|$|R
40|$|Abstract. Marine {{environment}} {{data warehouse}} can store massive data. After the full <b>amount</b> <b>of</b> historical data has been initially loaded, the incremental update mode must {{be applied to}} ensure timely <b>updates</b> <b>of</b> data. In this paper, {{in view of the}} marine environment data warehouse’s characteristics, such as massive data <b>amount,</b> large number <b>of</b> historical data and low update frequency, a complete set of mechanisms for incremental <b>update</b> <b>of</b> marine environment data warehouse was proposed to greatly improve the operating efficiency of marine environment data warehouse. I...|$|R
30|$|<b>Updating</b> <b>of</b> variables: <b>Updating</b> <b>of</b> weights is made on {{each step}} {{increment}} as per SQP procedure and continues from step 5 (b).|$|R
50|$|The ISO 3166/MA updates ISO 3166-3 when necessary. The <b>updating</b> <b>of</b> ISO 3166-3 {{is totally}} {{dependent}} on the <b>updating</b> <b>of</b> ISO 3166-1.|$|R
50|$|Refresher {{training}} {{is a form}} <b>of</b> <b>updating</b> military knowledge <b>of</b> the reservist troops. After one has completed the conscription service, {{he or she can}} be called for refresher training for some <b>amount</b> <b>of</b> days.|$|R
40|$|If the <b>amount</b> <b>of</b> {{physical}} activity in a society increased, it should improve public health; but uncertainties remain about how to achieve this. Professor Philip Baker from the Queensland University of Technology in Australia describes {{the findings from the}} January 2015 <b>update</b> <b>of</b> the Cochrane review of the evidence on community-wide interventions...|$|R
40|$|Leading {{agent-based}} trust models address {{two important}} needs. First, they show how an agent may estimate the trustworthiness of another agent based on prior interactions. Second, they show how agents may share their knowledge {{in order to}} cooperatively assess the trustworthiness of others. However, in real-life settings, information relevant to trust is usually obtained piecemeal, not all at once. Unfortunately, the problem of maintaining trust has drawn little attention. Existing approaches handle trust updates in a heuristic, not a principled, manner. This paper builds on a formal model that considers probability and certainty as two dimensions of trust. It proposes a mechanism using which an agent can <b>update</b> the <b>amount</b> <b>of</b> trust it places in other agents on an ongoing basis. This paper shows via simulation that the proposed approach (a) provides accurate estimates of the trustworthiness of agents that change behavior frequently; and (b) captures the dynamic behavior of the agents. This paper includes an evaluation based on a real dataset drawn from Amazon Marketplace, a leading e-commerce site. 1. ...|$|R
40|$|This {{technical}} note {{presents the results}} of applying the T-Check method in an initial investigation of cloud computing. In this report, three hypotheses are examined: (1) an organization can use its existing infrastructure simultaneously with cloud resources with relative ease; (2) cloud computing environments provide ways to continuously <b>update</b> the <b>amount</b> <b>of</b> resources allocated to an organization; and (3) it is possible to move an application 2 ̆ 7 s resources between cloud computing providers, with varying levels of effort required. From the T-Check investigation, the first hypothesis is partially sustained and the last two hypotheses are fully sustained within the context specified for the investigation. From an engineering perspective, cloud computing is a distributed computing paradigm that focuses on providing a wide range of users with distributed access to virtualized hardware and/or software infrastructure over the internet. From a business perspective, it is the availability of computing resources that are scalable and billed on a usage basis. While scalability is the primary tenet of cloud computing, a host of other advantages are advertised as being inherently obtained through cloud computing...|$|R
50|$|List {{shows the}} ten best skaters {{based on the}} <b>amount</b> <b>of</b> points during the regular season. If two or more skaters are tied (i.e. same <b>amount</b> <b>of</b> points, goals and played games), all of the tied skaters are shown. <b>Updated</b> as <b>of</b> February 28, 2017.|$|R
30|$|Now {{with the}} network constructed, the Bayesian reasoner can infer probabilities of nodes given evidence. <b>Update</b> <b>of</b> {{beliefs in the}} network propagates belief <b>updating</b> <b>of</b> {{properties}} in the probabilistic ontology.|$|R
40|$|Abstract. While {{architectural}} design {{went into the}} time of BIM, it meant not only the <b>update</b> <b>of</b> tools and methods, but also the <b>update</b> <b>of</b> design procedures and thinking model. In the meantime, while engineering construction went into the time of BIM, it meant not only the <b>update</b> <b>of</b> design delivery methods, but also the <b>update</b> <b>of</b> interactive mode and consequence among the links of entire industry. Therefore, {{there are so many}} obstacles to overcome and it depends on the rapid upgrade of information technology application and management standards of the related enterprises as well as the entire industry...|$|R
50|$|The Eisenhower Foundation has {{released}} two <b>updates</b> <b>of</b> the National Violence Commission, {{as well as}} <b>updates</b> <b>of</b> the Kerner Riot Commission. Eisenhower Foundation President Alan Curtis edited the Foundation’s 15 year <b>update</b> <b>of</b> the Violence Commission, published by Yale University Press in 1985. Curtis and Eisenhower Foundation Trustee Elliott Currie, Professor of Criminology, Law and Society at the University of California, Irvine, co-authored the Foundation’s 30 year update in 1999.|$|R
3000|$|... in Algorithm  2 {{needs to}} {{carefully}} be chosen to ensure fast convergence <b>of</b> the <b>update</b> <b>of</b> instant transmit power p_i^(t) [...]. It is {{also noted that}} the <b>update</b> <b>of</b> instant transmit power p_i^(t) [...] for upstream SU v [...]...|$|R
30|$|In the {{standard}} BP algorithm, all value r_ij^l should be updated before updating all value g_ij^l. We observe that, since the sub-codes Pe′ in transmission vectors {{should not be}} connected {{to each other as}} shown by (4), all value r_ij^l for <b>updating</b> g_ij^(l+ 1) <b>of</b> kth sub-code have already been updated before <b>updating</b> r_ij^l <b>of</b> the (k+ 1)th sub-code, 1 ≤k≤(Ng− 1). Therefore, the <b>updating</b> <b>of</b> g_ij^(l+ 1) for kth sub-code can be computed in parallel with the <b>updating</b> <b>of</b> r_ij^l for (k+ 1)th sub-code. In the high-speed serial decoding (HSSD), the check-node updating and the symbol-node <b>updating</b> <b>of</b> the HSSD are carried out alternately as follows.|$|R
30|$|<b>Update</b> <b>of</b> {{external}} components.|$|R
40|$|In {{this article}} the novel {{clustering}} and regression methods TEDACluster and TEDAPredict methods are described additionally to recently proposed evolving classifier TEDAClass. The algorithms for classification, clustering and regression {{are based on}} the recently proposed AnYa type fuzzy rule based system. The novel methods use the recently proposed TEDA framework capable of recursive processing <b>of</b> large <b>amounts</b> <b>of</b> data. The framework is capable of computationally cheap exact <b>update</b> <b>of</b> data per sample, and can be used for training `from scratch'. All three algorithms are evolving that is they are capable of changing its own structure during the update stage, which allows to follow the changes within the model pattern...|$|R
50|$|There was {{an attempt}} made at adding A. praecox to the 2006 <b>update</b> <b>of</b> the National Pest Plant Accord (NPPA) but the {{application}} was declined. The 2011-2012 <b>update</b> <b>of</b> the NPPA attracted debate when it was again recommended for addition.|$|R
40|$|The recent advancements in {{cellular}} mobile technology and smart phone usage have opened opportunities for researchers and commercial companies to develop ubiquitous low cost localization systems. Radio frequency (RF) fingerprinting {{is a popular}} positioning technique which uses radio signal strength (RSS) values from already existing infrastructures to provide satisfactory user positioning accuracy in indoor and densely built outdoor urban areas where Global Navigation Satellite System (GNSS) signal is poor and hard to reach. However a major requirement for the RF fingerprinting to maintain good localization accuracy is the collection and <b>updating</b> <b>of</b> large training database. The Minimization of Drive Tests (MDT) functionality proposed by 3 GPP LTE Release 10 & 11 has enabled cellular operators to autonomously gather and <b>update</b> necessary <b>amount</b> <b>of</b> RF fingerprint samples by utilizing their subscriber user equipments (UEs). The main objective of this thesis is to propose a framework for RF fingerprint positioning (RFFP) of outdoor UEs using MDT data and to further improve its performance capability to provide better localization. In the first part only LTE base-station (BS) RSS values were used to improve grid-based RF fingerprint positioning (G-RFFP) by using novel approaches: using overlapped grid-cell layouts (GCL), weighting based grid-cell unit selection and Artificial Intelligence based G-RFFP method. In the second part real measurement RSS values from LTE BS and WLAN access points (APs) were utilized and a generic measurement method referred to as GMDT was proposed to correlate WLAN RSS to LTE RSS measurements and its significance to RFFP was studied using a partial fingerprint matching technique. To remove the computational cost associated with training data preprocessing a new cluster-based RF fingerprint positioning (C-RFFP) method was proposed. This thesis provides {{a good source of}} information and novel techniques for cellular operators to build a low cost RF fingerprint positioning system which can deliver acceptable results in emergency user localizatio...|$|R
