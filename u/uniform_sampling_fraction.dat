0|2706|Public
50|$|The {{above is}} called the Paley-Wiener-Levinson theorem, which generalize WSK <b>sampling</b> theorem from <b>uniform</b> <b>samples</b> to non <b>uniform</b> <b>samples.</b> Both of them can {{reconstruct}} a band-limited signal from those samples, respectively.|$|R
40|$|Appropriate {{sampling}} of training points {{is one of}} the primary factors affecting the fidelity of surro-gate models. This paper investigates the relative advantage of probability-based <b>uniform</b> <b>sampling</b> over distance-based <b>uniform</b> <b>sampling</b> in training surrogate models whose system inputs follow a distribution. Using the probability of the inputs as the metric for <b>sampling,</b> the probability-based <b>uniform</b> <b>sample</b> points are obtained by the inverse transform sampling. To study the suitability of probability-based <b>uniform</b> <b>sampling</b> for surrogate modeling, the Mean Squared Error (MSE) of a monomial form is for-mulated based on the relationship between the squared error of a surrogate model and the volume or hypervolume per sample point. Two surrogate models are developed respectively using the same number of probability-based and distance-based <b>uniform</b> <b>sample</b> points to approximate the same system. Their fidelities are compared using the monomial MSE function. When the exponent of the monomial function is between 0 and 1, the fidelity of the surrogate model trained using probability-based <b>uniform</b> <b>sampling</b> is higher than that of the other one trained using distance-based <b>uniform</b> <b>sampling.</b> When the exponent is greater than 1 or less than 0, the fidelity comparison is reversed. This theoretical conclusion is suc-cessfully verified using standard test functions and an engineering application...|$|R
40|$|Random {{sampling}} is {{an appealing}} approach to build synopses of large data streams because random samples {{can be used}} for a broad spectrum of analytical tasks. Users are often interested in analyzing only the most recent fraction of the data stream in order to avoid outdated results. In this paper, we focus on sampling schemes that sample from a sliding window over a recent time interval; such windows are a popular and highly comprehensible method to model recency. In this setting, the main challenge is to guarantee an upper bound on the space consumption of the sample while using the allotted space efficiently at the same time. The difficulty arises {{from the fact that the}} number of items in the window is unknown in advance and may vary significantly over time, so that the <b>sampling</b> <b>fraction</b> has to be adjusted dynamically. We consider <b>uniform</b> <b>sampling</b> schemes, which produce each sample of the same size with equal probability, and stratified sampling schemes, in which the window is divided into smaller strata and a <b>uniform</b> <b>sample</b> is maintained per stratum. For <b>uniform</b> <b>sampling,</b> we prove that it is impossible to guarantee a minimum sample size in bounded space. We then introduce a novel sampling scheme called bounded priority sampling (BPS), which requires only bounded space. We derive a lower bound on the expected sample size and show that BPS quickly adapts to changing data rates. For stratified sampling, we propose a mergebased stratification scheme (MBS), which maintains strata of approximately equal size. Compared to naive stratification, MBS has the advantage that the sample is evenly distributed across the window, so that no part of the window is over- or underrepresented. We conclude the paper with a feasibility study of our algorithms on large real-world datasets...|$|R
50|$|In <b>sampling</b> theory, <b>sampling</b> <b>fraction</b> is {{the ratio}} of sample size to {{population}} size or, {{in the context of}} stratified sampling, {{the ratio of}} the sample size {{to the size of the}} stratum.The formula for the <b>sampling</b> <b>fraction</b> iswhere n is the sample size and N is the population size. If the <b>sampling</b> <b>fraction</b> is less than 5% or 0.05, then the finite population multiplier might be ignored.|$|R
5000|$|... #Subtitle level 3: Standardized and <b>uniform</b> <b>sample</b> {{processing}} ...|$|R
5000|$|Slice sampling: This method {{depends on}} the {{principle}} that one can sample from a distribution by sampling uniformly from the region under the plot of its density function. It alternates <b>uniform</b> <b>sampling</b> in the vertical direction with <b>uniform</b> <b>sampling</b> from the horizontal 'slice' defined by the current vertical position.|$|R
40|$|Different methodologies of <b>uniform</b> <b>sampling</b> {{over the}} {{rotation}} group, SO(3), for building unbiased 2 D shape models from 3 D objects are introduced and reviewed in this chapter. State-of-the-art non <b>uniform</b> <b>sampling</b> approaches are discussed, and <b>uniform</b> <b>sampling</b> methods using Euler angles and quaternions are introduced. Moreover, since presented work is oriented to model building applications, {{it is not}} limited to general discrete methods to obtain uniform 3 D rotations, but also from a continuous point of view in the case of Procrustes Analysis. Peer ReviewedPostprint (published version...|$|R
30|$|We used PerMANOVA (implemented in the R-vegan {{function}} adonis) {{to distinguish}} the amount of variation explained by the <b>sample</b> <b>fraction,</b> the domestication status (wild or cultivated) or the interaction between these factors. Approximately 10  % of the total variation was explained by these factors, and <b>sample</b> <b>fraction,</b> domestication status and their interaction all made a significant contribution. Of these, the interaction between domestication status and <b>sample</b> <b>fraction</b> {{was the most important}} factor explaining 4.7  % of the variation (Additional file 5 : Table S 4). Thus it seems that there may be differences in the way that bacterial communities are distributed among <b>sample</b> <b>fractions,</b> depending on the domestication status of the host plants.|$|R
40|$|It {{is shown}} that, by {{suitable}} random normalisation, the spacings (differences between order statistics) of exponential and <b>uniform</b> <b>samples</b> are independent. These normalised spacings {{can be used}} for consecutive discordancy testing when scale (and possibly also location) parameters are unknown. Independent spacings outliers consecutive discordancy testing exponential <b>sample</b> <b>uniform</b> <b>sample...</b>|$|R
40|$|Abstract—Discrete {{time signal}} {{is nothing but}} samples {{continuous}} time signal and this process is called as sampling. If inter-sampling distance is constant it is called as <b>uniform</b> <b>sampling</b> and if distance is unequal, it is non-uniform sampling. Irrespective of type of sampling, reconstruction is possible if sampling rate do not exceed Nyquist rate. Sinc interpolation {{is one of the}} methods which results in reconstruction of original signal. In this paper we will see how to reconstruct input from non-uniform and <b>uniform</b> <b>samples</b> using sinc interpolation and why it is said that perfect reconstruction from non-uniform samples is more difficult than <b>uniform</b> <b>samples.</b> ...|$|R
40|$|The Bernoulli sieve is an {{infinite}} occupancy scheme obtained by allocating {{the points of}} a <b>uniform</b> $[0, 1]$ <b>sample</b> over {{an infinite}} collection of intervals made up by successive positions of a multiplicative random walk independent of the <b>uniform</b> <b>sample.</b> We prove a law of the iterated logarithm {{for the number of}} non-empty (occupied) intervals as the size of the <b>uniform</b> <b>sample</b> becomes large. Comment: submitted for publication, 11 page...|$|R
40|$|This paper {{considers}} the combined problem of allocation and stratification {{in order to}} minimise the variance of the expansion estimator of a total, {{taking into account that}} the population is finite. The proof of necessary minimum variance conditions utilises the Kuhn-Tucker Theorem. Stratified simple random sampling with non-negligible <b>sampling</b> <b>fractions</b> is an important design in sample surveys. We go beyond limiting assumptions that have often been used in the past, such as that the stratification equals the study variable or that the <b>sampling</b> <b>fractions</b> are small. We discuss what difference the <b>sampling</b> <b>fractions</b> will make for stratification. In particular, in many surveys the <b>sampling</b> <b>fraction</b> equals one for some strata. The main theorem of this article is applied to a business population...|$|R
40|$|Tail index {{estimation}} depends for its accuracy on {{a precise}} {{choice of the}} <b>sample</b> <b>fraction,</b> i. e. the number of extreme order statistics on which the estimation is based. A complete solution to the <b>sample</b> <b>fraction</b> selection is given {{by means of a}} two step subsample bootstrap method. This method adaptively determines the <b>sample</b> <b>fraction</b> that minimizes the asymptotic mean squared error. Unlike previous methods, prior knowledge of the second order parameter is not required. In addition, we are able to dispense with the need for a prior estimate of the tail index which already converges roughly at the optimal rate. The only arbitrary choice of parameters is the number of Monte Carlo replications. tail index;bias;bootstrap;mean squared error;optimal extreme <b>sample</b> <b>fraction...</b>|$|R
40|$|Classical Respondent-Driven Sampling (RDS) estimators {{are based}} on a Markov Process model in which {{sampling}} occurs with replacement. Given that respondents generally cannot be interviewed more than once, this assumption is counterfactual. We join recent work by Gile and Handcock in exploring the implications of the sampling-with-replacement assumption for bias of RDS estimators. We differ from previous studies in examining a wider range of <b>sampling</b> <b>fractions</b> and in using not only simulations but also formal proofs. One key finding is that RDS estimates are surprisingly stable even in the presence of substantial <b>sampling</b> <b>fractions.</b> Our analyses show that the sampling-with-replacement assumption is a minor contributor to bias for <b>sampling</b> <b>fractions</b> under 40 %, and bias is negligible for the 20 % or smaller <b>sampling</b> <b>fractions</b> typical of field applications of RDS...|$|R
5000|$|The naive Monte Carlo {{approach}} is to sample points uniformly on Ω: given N <b>uniform</b> <b>samples,</b> ...|$|R
40|$|In this paper, {{we explore}} {{the use of}} Markov chain {{sampling}} techniques for applications in probabilistic robustness of control systems. First, we analyze the general hit-and-run (HR) method for <b>uniform</b> <b>sampling</b> in convex bodies, and discuss several key {{issues related to the}} so called mixing rate of this process and to Hoeffding-type inequalities for dependent samples. Then, we apply the HR method for <b>uniform</b> <b>sampling</b> in the interior of a generic LMI feasible set. Two specific applications of this latter problem which are relevant in probabilistic robust control are studied: the uniform generation of stable transfer functions bounded in the H∞ norm, and <b>uniform</b> <b>sampling</b> in matrix spectral (maximum singular value) norm balls...|$|R
40|$|We {{study the}} problem of {{approximately}} answering aggregation queries using sampling. We observe that <b>uniform</b> <b>sampling</b> performs poorly when {{the distribution of the}} aggregated attribute is skewed. To address this issue, we introduce a technique called outlier-indexing. <b>Uniform</b> <b>sampling</b> is also inefective for queries with low selectivity. We rely on weighted sampling based on workload information to overcome this shortcoming. We demonstrate that a combination of outlier-indexing with weighted sampling can be used to answer aggregation queries with significantly reduced approximation error compared to either <b>uniform</b> <b>sampling</b> or weighted sampling alone. We discuss the implementation of these techniques on Microsoft's SQL Server; and present experimental results that demonstrate the merits of our techniques...|$|R
3000|$|... {{respectively}} for the BFS and the <b>uniform</b> <b>sample,</b> {{in agreement}} with recent studies by Facebook [18, 19].|$|R
40|$|For response-adaptive {{clinical}} trials using a generalized Friedman’s urn design, we derive the asymptotic normality of the <b>sample</b> <b>fraction</b> {{assigned to each}} treatment under staggered entry and delayed response. The rate of convergence and {{the law of the}} iterated logarithm are obtained for both the urn composition and the <b>sample</b> <b>fraction.</b> Some applications are also discussed...|$|R
40|$|Abstract—The {{knowledge}} of channel statistics, {{as a result}} of random fading, interference, and primary user activities, can be very helpful for a secondary user in making sound opportunistic spectrum access decisions in a cognitive radio network. It is therefore desirable to be able to efficiently and accurately estimate channel statistics, even for resource constrained secondary users like wireless sensors. In this paper we focus on the traditional ML (maximum likelihood) estimator. However, rather than using equal or uniform sampling/sensing intervals as is typically done, we introduce a random sampling/sensing based ML estimation strategy. The randomization of the sampling intervals allows us to catch channel variations on a finer (time) granularity; the associated likelihood function is also more sensitive to channel variations. Consequently, this scheme significantly reduces the average sampling rate compared to <b>uniform</b> <b>sampling.</b> Analysis and simulation both show that random <b>sampling</b> significantly outperforms <b>uniform</b> <b>sampling</b> at low sampling rate. We further propose a randomized <b>uniform</b> <b>sampling</b> scheme which achieves a better tradeoff between good performance of random sampling and the low complexity of <b>uniform</b> <b>sampling.</b> I...|$|R
40|$|ISBN: 978 - 3 - 642 - 17652 - 4 International audienceWe {{consider}} {{the problem of}} <b>uniform</b> <b>sampling</b> in large scale open systems. <b>Uniform</b> <b>sampling</b> is a fundamental schema that guarantees that any individual in a population has the same probability to be selected as sample. An important issue that seriously hampers the feasibility of <b>uniform</b> <b>sampling</b> in open and large scale systems is the inevitable presence of malicious nodes. In this paper we show that restricting the number of requests that malicious nodes can issue and allowing for a full knowledge of {{the composition of the}} system is a necessary and sufficient condition to guarantee <b>uniform</b> and ergodic <b>sampling.</b> In a nutshell, a <b>uniform</b> and ergodic <b>sampling</b> guarantees that any node in the system is equally likely to appear as a sample at any non malicious node in the system and that infinitely often any nodes have a non null probability to appear as a sample at any honest nodes...|$|R
40|$|AbstractTail index {{estimation}} depends for its accuracy on {{a precise}} {{choice of the}} <b>sample</b> <b>fraction,</b> i. e., the number of extreme order statistics on which the estimation is based. A complete solution to the <b>sample</b> <b>fraction</b> selection is given {{by means of a}} two-step subsample bootstrap method. This method adaptively determines the <b>sample</b> <b>fraction</b> that minimizes the asymptotic mean-squared error. Unlike previous methods, prior knowledge of the second-order parameter is not required. In addition, we are able to dispense with the need for a prior estimate of the tail index which already converges roughly at the optimal rate. The only arbitrary choice of parameters is the number of Monte Carlo replications...|$|R
3000|$|... is {{employed}} in [9, 10] {{to achieve the}} optimum sensing performance due to the homogeneous nature of the random field. However, due to the inherent randomness of the energy harvesting process, <b>uniform</b> <b>sampling</b> might be infeasible given that {{there might not be}} sufficient energy to perform sensing operations at certain time periods. The performance of <b>uniform</b> <b>sampling</b> with conventional power supply can then serve as a lower bound for systems with energy harvesting devices.|$|R
40|$|Abstract—A {{major problem}} in the {{application}} of particle filters to multiple target tracking is the loss of modes due to sample degeneracy. A particle filter based on the notion of <b>uniform</b> <b>sampling</b> is developed to address this issue. The proposed particle filter approximates <b>uniform</b> <b>sampling</b> in an auxiliary variable framework. Simulation results show that the proposed method provides significantly improved mode retention compared to a particle filter using the optimal importance density...|$|R
30|$|In the following, we briefly {{discuss the}} two {{statistical}} sampling methods adopted in this work, namely the breadth-first-search and the <b>uniform</b> <b>sampling.</b>|$|R
3000|$|... {{behave like}} an i.i.d. <b>uniform</b> <b>sample,</b> {{and in the}} sequel we shall need to {{restrict}} ourselves to an interval of the form [...]...|$|R
40|$|We {{study the}} usage of various {{definitions}} of <b>sampling</b> <b>fractions</b> in understanding electron shower shapes in a sampling multilayer electromagnetic calorimeter. We show that the <b>sampling</b> <b>fractions</b> obtained by the conventional definition (I) of (average observed energy in layer) /(average deposited energy in layer) will not give the best energy resolution for the calorimeter. The {{reason for this is}} shown to be the presence of layer by layer correlations in an electromagnetic shower. The best resolution is obtained by minimizing the deviation from the total input energy using a least squares algorithm. The 'sampling fractions' obtained by this method (II) are shown to give the best resolution for overall energy. We further show that the method (II) <b>sampling</b> <b>fractions</b> are obtained by summing the columns of a non-local {lambda} tensor that incorporates the correlations. We establish that the <b>sampling</b> <b>fractions</b> (II) cannot be used to predict the layer by layer energies and that one needs to employ the full {lambda} tensor for this purpose. This effect is again a result of the correlations...|$|R
3000|$|While {{the number}} of {{identical}} communities between the two sets obtained by using, respectively, BFS and <b>uniform</b> <b>sampling,</b> is not high (i.e., respectively, [...]...|$|R
40|$|Abstract — We extend Valiant’s {{probably}} approximately correct (PAC) {{model of}} learning to Markov decision processes (MDPs) and dynamic games. The work {{is related to the}} simulation-based estimation of value functions for discounted reward MDPs. We obtain <b>uniform</b> <b>sample</b> complexity results for MDP simulation. We also obtain <b>uniform</b> <b>sample</b> complexity results for Markov games with a finite number of players. The results can be extended to the case when the states are partially observable and policies are non-stationary and have memory. I...|$|R
40|$|Abstract. We discuss <b>uniform</b> <b>sampling</b> {{algorithms}} {{that are}} based on stochastic growth methods, using sampling of extreme configurations of polymers in simple lattice mod-els as a motivation. We shall show how a series of clever enhancements to a fifty-odd year old algorithm, the Rosenbluth method, led to a cutting-edge algorithm capable of <b>uniform</b> <b>sampling</b> of equilibrium statistical mechanical systems of polymers in situa-tions where competing algorithms failed to perform well. Examples range from collapsed homo-polymers near sticky surfaces to models of protein folding. ...|$|R
50|$|<b>Uniform</b> <b>sampling</b> without replacement: is {{the most}} used {{approach}} for this purpose, where every point has the same probability of being included in the sample.|$|R
5000|$|... #Caption: As we {{approach}} the optimum the probability of finding further improvements through <b>uniform</b> <b>sampling</b> decreases towards zero if the sampling-range d is kept fixed.|$|R
25|$|Analysts {{should be}} mindful that the samples remain truly random as the <b>sampling</b> <b>fraction</b> grows, lest <b>sampling</b> bias be introduced.|$|R
40|$|Random {{sampling}} {{has become}} a critical tool in solving massive matrix problems. For linear regression, a small, manageable set of data rows can be randomly selected to approximate a tall, skinny data matrix, improving processing time significantly. For theoretical performance guarantees, each row must be sampled with probability proportional to its statistical leverage score. Unfortunately, leverage scores are difficult to compute. A simple alternative is to sample rows uniformly at random. While this often works, <b>uniform</b> <b>sampling</b> will eliminate critical row information for many natural instances. We take {{a fresh look at}} <b>uniform</b> <b>sampling</b> by examining what information it does preserve. Specifically, we show that <b>uniform</b> <b>sampling</b> yields a matrix that, in some sense, well approximates a large fraction of the original. While this weak form of approximation is not enough for solving linear regression directly, it is enough to compute a better approximation. This observation leads to simple iterative row sampling algorithms for matrix approximation that run in input-sparsity time and preserve row structure and sparsity at all intermediate steps. In addition to an improved understanding of <b>uniform</b> <b>sampling,</b> our main proof introduces a structural result of independent interest: we show that every matrix can be made to have low coherence by reweighting a small subset of its rows...|$|R
40|$|<b>Uniform</b> <b>sampling</b> in {{networks}} is at {{the core}} of a wide variety of randomized algorithms. Random sampling can be performed by modeling the system as a graph with associated transition probabilities and defining a corresponding Markov chain (MC). A random walk of prescribed minimum length, performed on this graph, yields a stationary distribution, and the corresponding random sample. This sample, however, is not uniform when network nodes have a nonuniform degree distribution. This poses a significant practical challenge since typical large scale, real-world, unstructured networks tend to have non-uniform degree distributions, e. g., power-law degree distribution in unstructured peer-to-peer networks. In this paper we present a distributed algorithm that enables efficient <b>uniform</b> <b>sampling</b> in large real-world networks. Specifically, we prescribe necessary conditions for <b>uniform</b> <b>sampling</b> in such networks and present distributed algorithms that satisfy these requirements. We empirically evaluate the performance of our algorithm in comparison to known algorithms. We also quantify, in context of the presented algorithms, the performance parameters in <b>uniform</b> <b>sampling</b> that are most relevant in a distributed setting – computational complexity, number of network messages, and the uniformity of the sampling. Detailed experimental results are used to support our claims relating to performance improvements of our algorithm. ...|$|R
40|$|Objective: Understand {{impacts of}} {{adaptive}} sampling {{and get to}} know more about pbrt Developing environment: Windows OS is recommended; you can use it linux or other OS Requirements: 1) Given a user specified ray budget (e. g., 10 K sample counts), distribute those ray samples over pixels of the view space according to an adaptive or error metric. A simple metric would be variance metric. 2) Compare its visual quality and runtime performance with a simple <b>uniform</b> <b>sampling.</b> 3) Measure MSE between your result and a reference that is generated by a high number of sample counts. Do the same thing for a result generated by the <b>uniform</b> <b>sampling</b> in the same sample count. 4) Also, measure and compare the runtime cost of your method against that of <b>uniform</b> <b>sampling</b> 5) Which method is better? Explain why you think so. Explain your adaptive metric...|$|R
