2|24|Public
30|$|Several of the {{students}} expressed that {{they were concerned about}} having to redo everything that they had taped. The students also referred to taping being a boring assignment and commented that it never seemed to end. The teacher’s ability to create a meaningful learning situation was also hampered by the interruption arising from the site manager’s decision to have the architect act as an instructor. The architect demonstrated the same thing that the teacher had done earlier, which {{the students}} experienced as an <b>unnecessary</b> <b>instruction</b> that only decreased their motivation. The consequences of this arrangement were a decreased motivation among the students and nothing getting done that day.|$|E
40|$|Abstract—In this paper, {{we focus}} on {{low-power}} design tech-niques for high-performance processors at the architectural and compiler levels. We focus mainly on developing methods for re-ducing the energy dissipated in the on-chip caches. Energy dis-sipated in caches represents a substantial portion in the energy budget of today’s processors. Extrapolating current trends, this portion {{is likely to increase}} in the near future, since the devices devoted to the caches occupy an increasingly larger percentage of the total area of the chip. We propose a method that uses an additional minicache located between the I-Cache and the central processing unit (CPU) core and buffers instructions that are nested within loops and are continuously otherwise fetched from the I-Cache. This mechanism is combined with code modifications, through the compiler, that greatly simplify the required hardware, eliminate <b>unnecessary</b> <b>instruction</b> fetching, and consequently reduce signal switching activity and the dissipated energy. We show that the additional cache, dubbed L-Cache, is much smaller and simpler than the I-Cache when the compiler assumes the role of allocating instructions to it. Through simulation, we show that for the SPECfp 95 benchmarks, the I-Cache remains dis-abled most of the time, and the “cheaper ” extra cache is used in-stead. We also propose different techniques that are better adapted to nonnumeric nonloop-intensive code. Index Terms—Low-power design, memory, power-consumption model, special low-power 99...|$|E
5000|$|Instruction creep, {{the gradual}} and {{unmanaged}} addition of <b>unnecessary</b> <b>instructions</b> ...|$|R
40|$|Pre-execution systems {{reduce the}} impact of cache misses and branch mispredictions by forking a slice, a code {{fragment}} derived from the program, in advance of frequently mispredicted branches and frequently missing loads in order to either resolve the branch or prefetch the load. Because <b>unnecessary</b> <b>instructions</b> are omitted the slice reaches the branch or load before the main thread does, for loads this time margin can reduce or even eliminate cache miss delay...|$|R
5000|$|Elimination of <b>unnecessary</b> branch <b>instructions</b> {{can make}} the {{execution}} of necessary branches, {{such as those that}} make up loops, faster by lessening the load on branch prediction mechanisms.|$|R
40|$|A dynamic {{instruction}} trace often {{contains many}} <b>unnecessary</b> <b>instructions</b> {{that are required}} only by the unexecuted portion of the program. Hot-cold optimization (HCO) is a technique that realizes this peeormance opportunity. HCO uses profile information to partition each routine into frequently executed (hot) and infrequently executed (cold) parts. Unnecessary operations in the hot portion are removed, and compensation code is added on transitions from hot to cold as needed. We evaluate HCO on a collection of large Windows NT applications. HCO is most effective on the programs that are call intensive and have flat profiles, providing a 3 - 896 reduction in path length beyond conventional optimization. 1...|$|R
40|$|Abstract- Program {{generators}} {{are usually}} aimed for {{the generation of}} program source code. This paper introduces the idea of software source code generation and its execution on demand that we refer to as Autogeneration. Autogeneration avoids the generation of program files by using the possibility of scripting languages to evaluate program code from variables. There are several features that could be achieved by Autogeneration. Some of them are program update during its execution, optimized code without temporarily <b>unnecessary</b> <b>instructions</b> and introspection of the generation process for development purposes. An example of a web application for database content management that is implemented as an autogeneration process is presented and discussed. I...|$|R
5000|$|Causes <b>unnecessary</b> caching of <b>{{instruction}}s</b> {{into the}} CPU instruction cache - which also decreases data locality.|$|R
50|$|To {{decide whether}} Atari's {{replica of the}} 10NES, the Rabbit, was {{infringing}} on Nintendo's copyright, the court analyzed whether there was substantial similarity between the two systems. The court stated that Nintendo incorporated creative organization and sequencing unnecessary to the lock and key function, and that Nintendo could protect the unique arrangement of programming instructions to create a purely arbitrary data stream. Nintendo also produced expert testimony showing {{that there were a}} multitude of different ways to generate a data stream to unlock the NES. Atari's unlocking program also contained elements of the 10NES that were unnecessary for unlocking. Atari's argument that these <b>unnecessary</b> <b>instructions</b> were included to ensure future compatibility was rejected.|$|R
60|$|Hetty {{looked about}} her, and, as it happened, {{the glare of}} {{sunlight}} flung back from the snow was in her eyes. Still, she could dimly see the trail dip over {{what seemed to be}} the edge of a gully close ahead, and she knew the descent to the creek in its bottom was a trifle perilous. She was, however, fearless and a trifle obstinate, and Clavering had, unfortunately, already ventured to give her what she considered quite <b>unnecessary</b> <b>instructions</b> as to the handling of the team. There had also been an indefinite change in his attitude towards her during the last week or two, which the girl, without exactly knowing why, resented and this appeared a fitting opportunity for checking any further presumption.|$|R
40|$|People {{verbally}} {{express themselves}} {{in a variety of}} ways, yet the same patterns consistently appear within the words of a person who is being deceptive. This research provides insight and demonstrates that similar patterns exist within malicious software that do not commonly exist within benign software, and can be used to help determine that the software is malicious or untrustworthy. The patterns that have been shown to exist within deceptive language were investigated to determine whether similar patterns exist within malware. Tests were performed to determine whether malware is more likely to consistently contain a higher or lower frequency of each <b>instruction,</b> contain <b>unnecessary</b> <b>instructions,</b> lack instructions to match each other, or is more compact and stealthy than benign software. 310 malware files were obtained from VirusShare and 200 viruses were generated by VCL 32 (Virus Creation Laboratory) and Mass Code Generator (MPCGEN) to compare against 276 clean Windows 7 application files. The results showed that malware can be differentiated from benign software by searching for patterns within the disassembled code. These patterns will allow stronger protections to be developed and show promise to prevent future systems from being compromised...|$|R
50|$|The shared {{weakness}} of protectors and virtualizers {{is that they}} impact performance, either by requiring decryption or by introducing <b>unnecessary</b> CPU <b>instructions.</b> To reduce the overhead code virtualizers are often only used to secure the critical parts of the code base, such as those interfacing with the gamestate and rendering.|$|R
5000|$|Architecture of DX 200 allows live {{migration}} {{as well as}} {{software update}} during live operation. Unlike in many other switching platforms, DX 200 performs live software update without code patching. Therefore, running code is not polluted by <b>unnecessary</b> jump <b>instructions.</b> Furthermore, as opposed to [...] "integration guessing" [...] of various software patches, DX 200 architecture makes proper integration testing of software components possible.|$|R
40|$|A well {{articulated}} {{educational program}} provides students {{an opportunity to}} develop to their highest potential without <b>unnecessary</b> duplication of <b>instruction</b> and delay in attaining their educational and career objectives. To provide articulation between North Carolina's public system of elementary, secondary and post secondary schools, the State Bodrd of Education appointed a committe...|$|R
5000|$|Under {{a moving}} block system, {{computers}} calculate a [...] "safe zone" [...] around each moving train {{that no other}} train is allowed to enter. The system depends on knowledge of the precise location and speed and direction of each train, which is determined {{by a combination of}} several sensors: active and passive markers along the track, and trainborne speedometers; (GPS systems cannot be relied upon because they do not work in tunnels). With a moving block setup, lineside signals are <b>unnecessary,</b> and <b>instructions</b> are passed directly to the trains. This has the advantage of increasing track capacity by allowing trains to run closer together while maintaining the required safety margins.|$|R
40|$|Pre-execution systems {{reduce the}} impact of cache misses and branch mispredictions by forking a slice, a code {{fragment}} derived from the program, in advance of frequently mispredicted branches and frequently missing loads in order to either resolve the branch or prefetch the load. Because <b>unnecessary</b> <b>instructions</b> are omitted the slice reaches the branch or load before the main thread does, for loads this time margin can reduce or even eliminate cache miss delay. Published results have shown significant improvements for some benchmarks, {{on the order of}} 20 %, with many showing at least single-digit improvements. These studies left unexamined two system parameters that one would expect pre-execution to be sensitive to: fetch rate and reorder buffer size. Higher fetch rate would allow the main thread to reach the troublesome load sooner, but would not affect the slice and so the slice’s margin is reduced. Studies have shown large potential margins for slices, but the fetch rate effect has not been measured. A second system parameter is reorder buffer size. A larger reorder buffer would allow a system to hide more of the miss latency that preexecution reduces. To test the sensitivity to these factors pre-execution schemes were simulated on systems with varying fetch rates and reorder buffer sizes. Results show that higher fetch rate does not reduce pre-execution speedup in most benchmarks. Reorder buffer size sensitivity varies, some benchmarks are insensitive to reorder buffer size increases beyond 256 entries, but still benefit from preexecution, the benefit {{due in large part to}} prefetching those loads that provide values for frequently mispredicted branches. The benchmarks that are sensitive to reorder buffer size are also the ones that benefit most ∗ Presented at the Workshop on Duplicating, Deconstructing, and Debunking, held in conjunction with the 30 th Annual Internationa...|$|R
40|$|We {{show how}} to {{translate}} a {{call by value}} functional language to a RISC architecture in a uniform way that encompasses register alloca tion and spill code placement avoids <b>unnecessary</b> copy <b>instructions</b> provides short circuit translation of Boolean expressions and can make use of inter procedural information The translation is directed by the source language structure It uses higher order functional programming extensively Prelimi nary measurements suggest that this method can compete with graph colouring the framework in which most contemporary register allocators are cast The translation is implemented in the ML Kit a region inference based SML compiler On average our back end compiles our benchmarks to code that runs in {{of the time of}} the code generated by SML NJ version. Eje: Conferencia latinoamericana de programación funciona...|$|R
40|$|In the {{foreword}} to {{this book}} {{the statement is}} made that “ {{there is no scientific}} doubt that breast feeding is best. ” The role of the nurse in promoting breast feeding is of the greatest importance. In order to do this the nurse needs guidance based on facts. This book provides that advice. It is an attractive, well-written book containing all the information needed without <b>unnecessary</b> details. Clear <b>instructions</b> are given on all aspects of breast feeding from the antenatal period to the time of weaning the infant from the breast. I recommend this book to all members of the health team...|$|R
25|$|To {{prevent such}} a situation, the Admiralty issued {{instructions}} on 27 June, which allowed the convoy {{to be turned}} back temporarily in order to shorten {{the distance to the}} nearest Allied base. In the event, enemy surface movements took place later than expected, making these <b>instructions</b> <b>unnecessary.</b> The Admiralty also instructed the safety of the convoy from surface attack to the westward of Bear Island depended on Allied surface forces, while to the eastward it was to be met by Allied submarines. Furthermore, the convoy's cruiser covering force was not to go east of Bear Island, unless the convoy was threatened by the presence of a surface force which the cruiser force could fight, nor to go beyond 25° East under any circumstance.|$|R
40|$|Traditional {{data flow}} {{analysis}} methods {{are designed for}} sequential programs. Hence they may fail when applied to control flow parallel imperative programs that share memory and {{are based on the}} MIMD computer model. Current approaches mostly use a copy-in/copy-out semantics, when entering/exiting a process, to model shared memory. To avoid this restriction, this paper extends the notion of program execution paths. Selecting some specific paths out of the set of all possible paths, allows to give simple data flow equations which are proved to be equal to the meet over all path solution. Since these data flow equations are extensions of the sequential ones, they fit very well to the traditional optimization methods. An example shows that the code generator of a compiler as well as a reordering assembler needs this kind of data flow analysis to avoid <b>unnecessary</b> memory barrier <b>instructions</b> and to produce correct instruction reorderings, respectively. Another paper is currently under work ( [...] ...|$|R
40|$|In this paper, the Scheduled Dataflow (SDF) architecturea {{decoupled}} memory/execution, multithreaded architecture using nonblocking threadsis {{presented in}} detail and evaluated against Superscalar architecture. Recent {{focus in the}} field of new processor architectures is mainly on VLIW (e. g., IA- 64), superscalar, and superspeculative designs. This trend allows for better performance, but at the expense of increased hardware complexity and, possibly, higher power expenditures resulting from dynamic instruction scheduling. Our research deviates from this trend by exploring a simpler, yet powerful execution paradigm that is based on dataflow and multithreading. A program is partitioned into nonblocking execution threads. In addition, all memory accesses are decoupled from the thread's execution. Data is preloaded into the thread's context (registers) and all results are poststored after the completion of the thread's execution. While multithreading and decoupling are possible with control-flow architectures, SDF makes it easier to coordinate the memory accesses and execution of a thread, as well as eliminate <b>unnecessary</b> dependencies among <b>instructions.</b> We have compared the execution cycles required for programs on SDF with the execution cycles required by programs on SimpleScalar (a superscalar simulator) by considering the essential aspects of these architectures {{in order to have a}} fair comparison. The result...|$|R
40|$|This paper {{presents}} {{an evaluation of}} a non-blocking, decoupled memory/execution, multithreaded architecture known as the Scheduled Dataflow (SDF). Recent focus {{in the field of}} new processor architectures is mainly on VLIW (e. g. IA- 64), superscalar and superspeculative designs. This trend allows for better performance at the expense of increased hardware complexity, and possibly higher power expenditures resulting from dynamic instruction scheduling. Our research deviates from this trend by exploring a simpler, yet powerful execution paradigm that is based on dataflow and multithreading. A program is partitioned into non-blocking execution threads. In addition, all memory accesses are decoupled from the thread's execution. Data is pre-loaded into the thread's context (registers), and all results are post-stored after the completion of the thread's execution. While multithreading and decoupling are possible with control-flow architectures, we believe that the non-blocking and functional nature of SDF, make it easier to coordinate the memory accesses and execution of a thread, as well as eliminate <b>unnecessary</b> dependencies among <b>instructions.</b> In this paper we compare the execution cycles required for programs on SDF with the execution cycles required by programs on SimpleScalar (a Superscalar simulator) ...|$|R
40|$|In {{this paper}} the Scheduled Dataflow (SDF) {{architecture}} - a decoupled memory/execution, multithreaded architecture using non-blocking threads - {{is presented in}} detail and evaluated against Superscalar architecture. Recent focus {{in the field of}} new processor architectures is mainly on VLIW (e. g. IA- 64), superscalar and superspeculative designs. This trend allows for better performance but at the expense of increased hardware complexity, and possibly higher power expenditures resulting from dynamic instruction scheduling. Our research deviates from this trend by exploring a simpler, yet powerful execution paradigm that is based on dataflow and multithreading. A program is partitioned into non-blocking execution threads. In addition, all memory accesses are decoupled from the thread's execution. Data is pre-loaded into the thread's context (registers), and all results are post-stored after the completion of the thread's execution. While multithreading and decoupling are possible with control-flow architectures, SDF makes it easier to coordinate the memory accesses and execution of a thread, as well as eliminate <b>unnecessary</b> dependencies among <b>instructions.</b> We have compared the execution cycles required for programs on SDF with the execution cycles required by programs on SimpleScalar (a superscalar simulator) by considering the essential aspects of these architectures {{in order to have a}} fair comparison. The results show that our architecture can outperform the superscalar. SDF performance scales better with the number of functional units and allow for a good exploitation of Thread Level Parallelism (TLP) and available chip area...|$|R
40|$|Recent {{syntactic}} {{research suggests}} that adjectives, adverbs and prepositional modifiers appear in fixed word orders, although the inventories of such elements vary across languages (Cinque, 1994, 1999; Stringer, Burghardt, Seo & Wang, in press). This paper focuses on the pedagogical implications of this syntactic research, and presents the initial findings of {{an investigation into the}} L 2 acquisition of the syntax of prepositional modifiers. Learners of English who achieve ILR Level 4 are expected not to make errors with orderings of modifiers despite the paucity of instruction and the infrequency of multiple modifiers in the input. The identification of what exactly facilitates acquisition is important in the design of advanced instructional materials to hasten the developmental process. Two experiments shed light on whether target-like accuracy depends on (i) frequency of exposure to the hierarchy itself; or (ii) acquisition of the lexical semantics of the individual modifiers, such that the hierarchy is naturally manifested. Following instruction on the meanings of individual modifiers, a computer-animated narrative was presented to 121 learners of English and 20 native-speaker controls, and a preference task and grammaticality judgment task were administered. Mixed design ANOVAs reveal that with minor exceptions, accuracy scores were significantly above chance and remarkably similar across the two experiments, across L 1 s, and, surprisingly, across proficiency levels. The results reveal prior understanding of universal principles of syntax, in evidence {{from the beginning of the}} acquisition process. The implication for high-advanced levels of instruction is that explicit teaching of syntactic hierarchies of modifiers is <b>unnecessary,</b> and advanced <b>instruction</b> should focus on contextualized vocabulary rather than word order...|$|R
40|$|The Board of Education did {{issue an}} order last night closing all schools• Some people are noted to {{have failed to}} {{understand}} yesterday’s closing orders to have included theaters, churches, lodge meetings, lectures, card parties, and dances• The closing orders are not to apply to 20 or more people in a restaurant; they do apply, however, to saloons and pool halls• Inspectors were sent late today to inspect rooming house, second hand stores, and business places in the North Side to determine their sanitary condition; unclean places will be placarded with sign reading “Unfit for human habitation”• P. J. Kealy, President of the Metropolitan, issued an order late today requiring that conductors {{limit the number of}} standing passengers on street cars to a “reasonable number;” later, “reasonable” was defined as 20 to 25; this, however, either followed or preceded a conflict with Gannon• Gannon, leading the city’s campaign, had telephoned P. J. Kealy, President of the Metropolitan, today instructing him {{to limit the number of}} standing passengers to 20; Kealy refused to cooperate; when Gannon asked Chief Godley to have his policemen enforce the order, but officials at the health department, including Bullock, Henry Benjamin, and President Motley, refused to support Gannon and countermanded his request• The details of the exchange between Gannon and Bullock are provided by the Star: first Bullock asked Gannon to wait for Motley’s permission before submitting written instructions to the police force, but then returned declaring the <b>instructions</b> <b>unnecessary</b> and saying that a railway official had called him complaining about persecution at the hands of Gannon; Gannon replied by calling attention to the poor sanitation conditions seen earlier in the day and the fact that only 2 women were assigned to keep 75 cars clean; Gannon declared that he was going ahead with his inspection anyway• Later, Motley said he would not support Gannon in his efforts, but saying he might “if the limit is made a reasonable number. It may be less than twenty or more than twenty;” Motley also said: “We don’t want the police department—we don’t need the police department [...] . If the street care company shows a disposition to comply with our requests it is not necessary to call in the police. The police are needed only when there are riots and disorder. Until such conditions exist we will go ahead ourselves enforcing the city ordinances without the police department’s help. We’re not going to cause the street car company any trouble in this matter. We are not going to start a row. ”Newspaper article...|$|R

