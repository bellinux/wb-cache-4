5|42|Public
5000|$|The second {{typology}} (according to {{the nature}} of the <b>underlying</b> <b>amount)</b> is determined by the needs of users of the variance information and may include e.g.: ...|$|E
5000|$|The term {{notional}} principal contract (NPC) {{is a term}} of art used by U.S. {{federal income}} tax professionals for contracts based on an underlying notional amount (other financial services professionals refer to such NPCs under the more general heading [...] "swaps," [...] although not all swaps are NPCs). The reason the <b>underlying</b> <b>amount</b> is [...] "notional" [...] is because neither party to the NPC is required to actually hold the property comprising the <b>underlying</b> <b>amount.</b> NPCs involve two parties who agree contractually to pay each other amounts at specified times, based on the underlying notional amount. The simplest example of an NPC is a so-called interest rate swap, in which one party (Party A) pays the other party (Party B) an amount each quarter determined by multiplying a floating, market-determined interest rate (e.g., LIBOR) by the notional amount; and Party B pays Party A on the same date an amount determined by multiplying a fixed interest rate by the notional amount.|$|E
5000|$|For U.S. {{federal income}} tax purposes, {{currently}} applicable Treasury Regulations define a Notional Principal Contract as [...] "a financial instrument that provides for the payment of amounts by one party to another at specified intervals calculated by reference to a specified index upon a notional principal amount in exchange for specified consideration or a promise to pay similar amounts." [...] (Treas. Reg. § 1.446-3(c)(1)(i)). [...] It is not defined in the Internal Revenue Code itself. Treasury Regulations § 1.446-3 provides extensive and detailed rules governing the taxation of NPCs in the United States. While the Treasury Regulations provide examples of contracts that are treated as NPCs, including [...] "interest rate swaps, currency swaps, basis swaps, interest rate caps, interest rate floors, commodity swaps, equity swaps, and similar agreements." [...] (Treas. Reg. § 1.446-3(c)(1)(i)), there are significant limitations on the types of contracts that may be treated as NPCs, particularly contracts that provide for an actual delivery of the <b>underlying</b> <b>amount.</b>|$|E
50|$|Variances can {{be divided}} {{according}} to their effect or nature of the <b>underlying</b> <b>amounts.</b>|$|R
40|$|Objectives: To {{identify}} potential factors predicting recurrence after talc pleurodesis for malignant pleural effusion. Methods: Retrospective {{study of}} two cohorts of consecutive patients undergoing surgery for malignant pleural effusion (T VATS/talc pleurodesis number of procedures n = 41, IPC VATS/indwelling pleural catheter n = 41). Data analysed: ASA, performance status, <b>underlying</b> histology, <b>amount</b> of effusion drained at initial procedure, morbidity, recurrence of effusion (RecEff), redo surgery due to recurrence. The definition of RecEff {{was based on}} imaging (at least CxR) and related report issued by a senior radiologist. Median radiological follow-up of both groups was identical (8 months). Statistical analysis was done through SPSS ver 21. Results: The RecEff rate was 20...|$|R
40|$|Conjoint {{analysis}} (CA) {{is concerned}} with estimating consumer choice behavior for products and services and/or underlying preference structures {{as a function of}} tangible specifications such as the cost of a product and duration of a service. Although constraints influence consumers’ preferences by forcing them to make specific trade-off across attributes, and affect what can be offered to the consumers, they have received little attention in the conjoint analysis literature. Such constraints have implications for CA design and the practicality of discrete attribute levels. This paper discusses the principles <b>underlying</b> mixture <b>amount</b> designs and compares the approach to the traditional fractional factorial designs in examples of travel time allocation to four different activities subject to time constraints of individuals...|$|R
40|$|Summary. Reconstruction {{of signal}} {{transduction}} network models based on incomplete information about network structure and dynamical behaviour {{is a major}} challenge in current systems biology. In particular, interactions within signalling networks are frequently characterised by partially unknown protein phosphorylation and dephosphorylation cascades at a submolecular description level. For prediction of promising network candidates, reverse engineering techniques typically enumerate the reaction search space. Considering an <b>underlying</b> <b>amount</b> of phosphorylation sites, this implies a potentially exponential number of individual reactions in conjunction with corresponding protein activation states. To manage the computational complexity, we extend P systems with string-objects by a subclass for protein representation able to process wild-carded together with specific information about protein binding domains and their ligands. This variety of reactants works together with assigned term-rewriting mechanisms derived from discretised reaction kinetics. We exemplify the descriptional capability and flexibility of the framework by discussing model candidates for the circadian clock formed by the KaiABC oscillator found in the cyanobacterium Synechococcus elongatus. A simulation study of its dynamical behaviour demonstrates effects of superpositioned protein abundance courses based on regular expressions corresponding to dedicated protein activation states. ...|$|E
40|$|Comparative {{kinetics}} of the uremic toxin p-cresol versus creatinine in rats {{with and}} without renal failure. Backgroundp-cresol, which is extensively metabolized into p-cresylglucuronide in the rat, is related to several biochemical and physiologic alterations in uremia and is not removed adequately by current hemodialysis strategies. The knowledge of its in vivo kinetic behavior could be helpful to improve the current removal strategies. MethodsWe investigated the kinetic behavior of intravenously injected p-cresol (10 mg/kg) in rats with normal and decreased renal function, and compared the results with those obtained for creatinine (60 mg/kg) under similar conditions. Renal failure was obtained by 5 / 6 nephrectomy. Both p-cresol and p-cresylglucuronide were analyzed using reversed-phase high-performance liquid chromatography (RP-HPLC). The relation between the p-cresylglucuronide peak height and the <b>underlying</b> <b>amount</b> of p-cresol was determined after hydrolysis of the glucuronide with β-glucuronidase. We calculated urinary excretion of p-cresol {{with and without}} taking p-cresylglucuronide into account. In addition, total, renal, and non-renal clearance, half-life, and volume of distribution were calculated for p-cresol. ResultsOver a 4 -hour period, p-cresol serum concentration showed only a minimal decline in rats with decreased renal function (t 1 / 2 = 11. 7 ± 0. 4 hours), compared to rats with normal renal function (t 1 / 2 = 1. 4 ± 0. 7 hours). A similar observation was made for p-cresylglucuronide. In rats with normal renal function, 21. 0 ± 10. 0 % of the injected p-cresol was excreted in urine as p-cresol and 60. 7 ± 25. 0 % as p-cresylglucuronide; in rats with renal failure, the respective amounts were 6. 7 ± 7. 5 % and 32. 0 ± 25. 3 % (P < 0. 05 vs. normal renal function) (total recovery 81. 81 ± 31. 07 % vs. 38. 50 ± 32. 09 %, P < 0. 05). The volume of distribution of p-cresol was approximately 4 times larger than that of creatinine, but was not significantly affected by renal failure. Not only renal, but also non-renal and total clearance, were much lower in rats with decreased renal function. ConclusionThe present data sheds a light on the kinetic behavior of p-cresol in uremic patients; the large volume of distribution, especially, might explain the inadequate dialytic removal of p-cresol. In addition, {{a substantial amount of}} p-cresol is removed by metabolism, and both renal and non-renal clearance are disturbed in uremia...|$|E
40|$|Correspondence {{issued by}} the General Accounting Office with an {{abstract}} that begins "GAO performed detailed tests of transactions that represent the <b>underlying</b> basis of <b>amounts</b> distributed to the Unemployment Trust Fund (UTF) and reviewed key reconciliations of the Internal Revenue Service records to the Department of the Treasury records. GAO found that the net federal unemployment tax revenue distributed to UTF for fiscal year 2001 {{is supported by the}} underlying records. ...|$|R
40|$|Correspondence {{issued by}} the Government Accountability Office with an {{abstract}} that begins "We performed the procedures which we agreed to perform and with which the Department of Labor (DOL) concurred soley to assist the DOL in ascertaining whether the net federal unemployment tax (FUTA) revenue distributed to the Unemployment Trust Fund (UTF) {{for the fiscal year}} ended September 30, 2006, is supported by the underlying records. The procedures we agreed to perform relate to (1) transactions that represent <b>underlying</b> basis of <b>amounts</b> distributed to the UTF and (2) key reconciliations of the Internal Revenue Service records to the Department of the Treasury Records. ...|$|R
40|$|Effective {{operation}} of cognitive radios (CRs) requires sensing the spectrum and dynamic {{adaptation of the}} available resources accord-ing to the sensed information. Although sensing and resource al-location are coupled, most existing designs optimize each of the tasks separately. This work optimizes them jointly for an under-lay CR paradigm. The formulation considers that secondary users adapt their power and rate based on the available imperfect chan-nel state information, while {{taking into account the}} cost associated with acquiring such an information. The objective of the optimiza-tion is twofold: maximize the (sum-rate) performance of the CR and protect the primary users through an average interference constraint. Designing the sensing in our <b>underlay</b> paradigm <b>amounts</b> to decide what channel/frequency slots are sensed at every time instant. Par-tial observability of the channel state (due to noisy and outdated in-formation) calls for (Bayesian) sequential estimators {{to keep track of the}} interference channel gains, as well as for dynamic program-ming tools to design the optimal schemes. Together with the optimal schemes, a simple approximate solution is also developed. Index Terms — Cognitive radio, underlay paradigm, sensing, dual decomposition, sequential estimation, dynamic programming...|$|R
5000|$|In 2005 {{they became}} one of the first {{telephony}} carriers to introduce the concept of [...] "Dynamic Bandwidth" [...] as an official product. [...] Dynamic Bandwidth works by transmitting voice and data over a T1 connection using ATM as the <b>underlying</b> protocol. The <b>amount</b> of available bandwidth for data [...] "expands" [...] by 64k for each voice line not in use. When a voice line goes off-hook, the bandwidth reserved for that voice line becomes solely available for use with that line until the call ends, when it becomes usable for data again. Using this concept, a T1 that would normally provide 768k of data and 12 voice lines can provide a full 1.536 Mbit/s connection when no voice lines are in use.|$|R
40|$|We {{present a}} new, general method for {{normalizing}} two-channel microarray data, partially drawing on ideas from two widely used approaches. Whereas the ANOVA approach carefully distinguishes different sources of signal and bias through explicit terms in its model, the MA-plot based approach {{takes into account}} the fact that sources of bias may be intensity-dependent. However, both approaches suffer from serious drawbacks, as we have shown in previous work. The fixed (non-intensity-dependent) coefficients in the ANOVA approach tend to under- or over-fit the data, and the MA-plot based approach assumes that all intensity-dependent trends are due to unwanted bias, each leading to inaccurate normalization in fairly common scenarios. Our proposed approach, called eCADS, captures the strengths of these previous approaches, while avoiding their weaknesses. We replace the fixed coefficients in the ANOVA model with functions of <b>underlying</b> RNA <b>amount,</b> thereby incorporating intensity-dependent relationships like those evident in MA-plots. The normalization method fits this 2 ̆ 2 functional ANOVA 2 ̆ 2 model and subtracts off terms representing bias to retain the biological signal of interest. By requiring a simple balance in experimental design, we show that our method preserves differential expression relationships in expectation. A consequence of this work is the statistical justification of a more efficient dye-swap design that requires only one array per sample pair. We demonstrate our new method on an experiment measuring expression in developing mice...|$|R
40|$|Correspondence {{issued by}} the Government Accountability Office with an {{abstract}} that begins "We assisted the Department of Labor in ascertaining whether the net federal unemployment tax (FUTA) revenue distributed to the Unemployment Trust Fund (UTF) {{for the fiscal year}} ended September 30, 2004, is supported by the underlying records. We evaluated fiscal year 2004 activity affecting distributions to the UTF. In performing the agreed-upon procedures, we conducted our work in accordance with U. S. generally accepted government auditing standards, which incorporate financial audit and attestation standards established by the American Institute of Certified Public Accountants. These standards also provide guidance for performing and reporting the results of agreed-upon procedures. The procedures we agreed to perform include (1) detailed tests of transactions that represent the <b>underlying</b> basis of <b>amounts</b> distributed to the UTF and (2) review of key reconciliations of the Internal Revenue Service records to the Department of the Treasury records. ...|$|R
30|$|The Internet of Things (IoT) is {{a recent}} {{paradigm}} that gained momentum by proposing to extend the Internet {{to a variety of}} things – such as Radio Frequency IDentification (RFID) tags, sensors, actuators or other smart technological components (e.g. smartphones) – which allows to foresee novel usages and applications where these things cooperate together with people, to achieve a common goal [1]. Similar vision in Ubiquitous computing led early thinkers [2] to propose the use of Semantic Web technologies in order to automate this process by enhancing interoperability between these things. Hence few projects revolved around this idea. In particular, Chen et al. [3] as well as Katasonov et al. [4] coupled the use of ontologies with agents to allow heterogeneous devices to cooperate. Singh et al. [5] proposed a framework enabling the collaboration of different pervasive computing environments (called AS, standing for Autonomous Systems) by using Semantic Web technologies. More recently Pfisterer et al. [6] proposed an architecture aiming to open the access to sensors, allowing enhancing integration of data and services of heterogeneous sensors and facilitating novel applications. However, the recent interest in providing digital extensions for these things has led to billions of connected devices offering their services or data through different platforms such as Cosma, Nimbitsb, ThinkSpeakc or Thingworxd. Considering that such connected devices may be mobile or may become unavailable, the <b>underlying</b> huge <b>amount</b> of heterogeneous and continuously changing data or services offered by these distributed connected devices, bring scalability issues.|$|R
40|$|It {{is common}} to define a change in health status or in a disease state {{on the basis of}} a {{sustained}} rise (or decline) in a biomarker over time. However, such observations are often subject to important variability unrelated to the underlying biologic process. The authors propose a method to evaluate rules that define an event on the basis of consecutive increases (or decreases) in the observations, given the presence of random variation. They examine how well these rules correctly identify a truly rising biomarker trajectory and, conversely, how often they can recognize a truly stable series or a slowly rising series. The method relies on simulation of realistic, sophisticated data sets that accurately reflect the systematic and random variations observed in marker series. These flexible, empirically based simulations enable estimation of the sensitivity and specificity of rules of consecutive rises as a function of the <b>underlying</b> trend, <b>amount</b> of random variation, and schedule of measurements (frequency and duration of follow-up). The authors illustrate the approach with postradiotherapy series of prostate-specific antigen, where three consecutive rises in prostate-specific antigen indicate treatment failure; the data are described by using a Bayesian hierarchical changepoint model. The method is particularly flexible and could be applied to evaluate other rules that purport to accurately detect upturns (downturns) in other noisy data series, including other medical data or other application areas. Bayesian hierarchical model; changepoint; Markov chain Monte Carlo; noise; prostate-specific antigen; sensitivit...|$|R
40|$|International audienceThe 2008 {{revision}} of the IEEE- 754 standard, which governs floating-point arithmetic, recommends that a certain set of elementary functions should be correctly rounded. Successful attempts for solving the Table Maker's Dilemma in binary 64 {{made it possible to}} design CRlibm, a library which offers correctly rounded evaluation in binary 64 of some functions of the usual libm. It evaluates functions using a two step strategy, which relies on a folklore heuristic that is well spread in the community of mathematical functions designers. Under this heuristic, one can compute the distribution of the lengths of runs of zeros/ones after the rounding bit {{of the value of the}} function at a given floating-point number. The goal of this paper is to change, whenever possible, this heuristic into a rigorous statement. The <b>underlying</b> mathematical problem <b>amounts</b> to counting integer points in the neighborhood of a curve, which we tackle using so-called exponential sums techniques, a tool from analytic number theory...|$|R
40|$|International audienceIn {{order to}} {{elucidate}} the mechanisms <b>underlying</b> the large <b>amount</b> of RAPD polymorphism found in 1990 {{in a population}} of the selfing annual Medicago truncatula GAERTN. (Fabaceae), we have analysed most of the individuals (n = 363) from the same population 6 years later using microsatellite loci. We confirm {{the result of the}} earlier study, namely that this population is very polymorphic and highly subdivided, with approximately 37 % of the variance distributed among subpopulations, only 50 m apart one from another. We use standard F-statistics analyses, linkage disequilibria, minimum spanning network, multilocus assignment tests and spatial autocorrelation analyses to test the hypotheses that spatial structure and outcrossing events are involved in maintaining the large amount of genetic diversity at the level of each subpopulation. Interestingly, fine-scale spatial structure could be observed in only one subpopulation suggesting that other mechanisms are acting elsewhere. To the best of our knowledge, this is the first study of fine spatial genetic structure in a predominantly selfing species...|$|R
40|$|Copy number {{variants}} (CNVs) <b>underlie</b> {{a significant}} <b>amount</b> of genetic diversity and disease. CNVs {{can be detected}} {{by a number of}} means, including chromosomal microarray analysis (CMA) and whole-genome sequencing (WGS), but these approaches suffer from either limited resolution (CMA) or are highly expensive for routine screening (both CMA and WGS). As an alternative, we have developed a next-generation sequencing-based method for CNV analysis termed SMASH, for short multiply aggregated sequence homologies. SMASH utilizes random fragmentation of input genomic DNA to create chimeric sequence reads, from which multiple mappable tags can be parsed using maximal almost-unique matches (MAMs). The SMASH tags are then binned and segmented, generating a profile of genomic copy number at the desired resolution. Because fewer reads are necessary relative to WGS to give accurate CNV data, SMASH libraries can be highly multiplexed, allowing large numbers of individuals to be analyzed at low cost. Increased genomic resolution can be achieved by sequencing to higher depth...|$|R
40|$|Correspondence {{issued by}} the General Accounting Office with an {{abstract}} that begins "GAO assisted the Department of Transportation determine whether the net excise tax revenue distributed to the Airport and Airway Trust Fund (AATF) for fiscal year 2000 {{was supported by the}} underlying records. GAO agreed to (1) perform detailed tests of transactions that represent the <b>underlying</b> basis of <b>amounts</b> distributed to AATF, (2) review the Internal Revenue Service's (IRS) quarterly AATF certifications, (3) review the Department of the Treasury Financial Management Service adjustments to AATF for FY 2000, (4) review the Office of Tax Analysis process for estimating amounts to be distributed to AATF for the fourth quarter of FY 2000, (5) compare net excise tax distributions to AATF during FY 2000 and amounts reported in the financial statements prepared by the Bureau of the Public Debt for AATF and the Federal Aviation Administration's consolidated financial statements, and (6) review key reconciliations of IRS records to Treasury records. ...|$|R
40|$|Traditionally, {{political}} polarization {{has been}} an important topic of analysis on the social and political sciences, but in recent years, due to the paradigm shift of political participation and the digital record that it generates associated to the universalization of the Internet and the emergence of online social networks, {{it can be seen that}} different scienti c disciplines related to complex networks have focused on the study of political polarization, elections predictions or protests. In this context, this work consists of an extense analysis of the society behavior on a social network when there is a tense situation, using as case of study an ongoing conflict in Spain, the catalan independence. This conflict, besides the political tension that generates, provides us a unique ground truth for user classi cation. Taking advantage of this fact, we perform two strategies to classify ideologically opposite users and several analyses to detect political communities, study the political polarization <b>underlying</b> this great <b>amount</b> of data and evaluate temporal dynamics...|$|R
40|$|We {{quantify}} {{sources of}} variation in annual job earnings data collected by the Survey of Income and Program Participation (SIPP) {{to determine how much}} of the variation is the result of measurement error. Jobs reported in the SIPP are linked to jobs reported in a new administrative database, the Detailed Earnings Records (DER) drawn from the Social Security Administration’s Master Earnings File, a universe file of all earnings reported on W- 2 tax forms. As a result of the match, each job potentially has two earnings observations per year: survey and administrative. Unlike previous validation studies, both of these earnings measures are viewed as noisy measures of some <b>underlying</b> true <b>amount</b> of annual earnings. While the existence of survey error resulting from respondent mistakes or misinterpretation is widely accepted, the idea that administrative data is also error-prone is new. Possible sources of employer reporting error, employee under-reporting of compensation such as tips, and general differences between how earnings may be reported on tax forms and in surveys, necessitates the discarding of the assumption that administrative data is a “true ” measure of the quantity collected by the survey. In addition, errors in matching SIPP and DER jobs, a necessary task in any use of administrative data, also contribute to measurement error in both earnings variables. Exploiting the presence of individuals with multiple jobs and shared employers over time, we estimate an econometric model that includes random person and firm effects as well as a common error component shared by SIPP and SSA earnings. We do not impose ancillary orthogonal design assumptions...|$|R
50|$|Delta {{is always}} {{positive}} for long calls and negative for long puts (unless they are zero). The total delta {{of a complex}} portfolio of positions on the same underlying asset can be calculated by simply taking {{the sum of the}} deltas for each individual position - delta of a portfolio is linear in the constituents. Since the delta of underlying asset is always 1.0, the trader could delta-hedge his entire position in the underlying by buying or shorting the number of shares indicated by the total delta. For example, if the delta of a portfolio of options in XYZ (expressed as shares of the underlying) is +2.75, the trader would be able to delta-hedge the portfolio by selling short 2.75 shares of the underlying. This portfolio will then retain its total value regardless of which direction the price of XYZ moves. (Albeit for only small movements of the <b>underlying,</b> a short <b>amount</b> of time and not-withstanding changes in other market conditions such as volatility and the rate of return for a risk-free investment).|$|R
40|$|Constitutional {{doctrine}} is often {{shaped by the}} details of the constitutional text. Under the Necessary and Proper Clause, the Supreme Court first considers whether a law is “necessary” and then whether it is “proper. ” Some justices have urged the same approach for the Cruel and Unusual Punishments Clause: first ask if the punishment is “cruel,” then if it is “unusual. ” That each clause has two requirements seems obvious, and it is has been the assumption <b>underlying</b> vast <b>amounts</b> of scholarship. That assumption is incorrect. This Article argues that “necessary and proper” and “cruel and unusual” are best read as instances of hendiadys. Hendiadys is a figure of speech in which two terms, separated by a conjunction, work together as a single complex expression. It is found in many languages, including English: e. g., “rise and shine,” “nice and fat,” “cakes and ale,” “open and notorious. ” When “cruel and unusual” is read as a hendiadys, the clause does not prohibit punishments that merely happen to be both cruel and unusual. Rather, it prohibits punishments that are unusually cruel, i. e., innovative in their cruelty. If “necessary and proper” is read as a hendiadys, then the terms are not separate requirements for congressional action. The word “necessary” requires a close relationship between a statute and the constitutional power it is carrying into execution, while “proper” instructs us not to interpret “necessary” in its strictest sense. “Proper” also reminds us that the incidental power Congress is exercising must belong to an enumerated power. To read each of these constitutional phrases as a hendiadys, though seemingly novel, actually aligns closely with the early interpretations, including the interpretation of the Necessary and Proper Clause in McCulloch v. Maryland. The readings offered here solve a number of puzzles, and they better capture the subtlety of these clauses...|$|R
40|$|Two decades ago, {{a chapter}} on {{aviation}} with this title might have focused on physical aspects of human performance, on representing the control processes involved in flying. There {{has been such a}} fundamental change in our knowledge and techniques that this chapter focuses almost exclusively on cognitive processes. The main aims are to show that relatively few general principles <b>underlie</b> the huge <b>amount</b> of information relevant to interface design, and that context is a key concept in understanding human behavior. Classical interface human factors/ergonomics consists of a collection of useful but mainly disparate facts and a simple model of the cognitive processes underlying behavior— that these processes consist of independent information–decision–action or if–then units. (The combined term human factors/ergonomics is used, shortened to HF/E, because these terms have different shades of meaning in different countries. Cognitive processing is the unobservable processing between arrival of stimuli at the senses and initiating an action.) Classic HF/E tools are powerful aids for interface design, but they make an inadequate basis for designing to support complex tasks. Pilots and air traffic controllers are highly trained and able people. Their behavior is organized and goal-directed, and they add knowledge to th...|$|R
40|$|We study ideals in Hall algebras of monoid {{representations}} on pointed sets {{corresponding to}} certain {{conditions on the}} representations. These conditions include the property that the monoid act via partial permutations, that the representation possess a compatible grading, and conditions on {{the support of the}} module. Quotients by these ideals lead to combinatorial Hopf algebras which can be interpreted as Hall algebras of certain sub-categories of modules. In the case of the free commutative monoid on n generators, we obtain a co-commutative Hopf algebra structure on n-dimensional skew shapes, whose <b>underlying</b> associative product <b>amounts</b> to a "stacking" operation on the skew shapes. The primitive elements of this Hopf algebra correspond to connected skew shapes, and form a graded Lie algebra by anti-symmetrizing the associative product. We interpret this Hopf algebra as the Hall algebra of a certain category of coherent torsion sheaves on A_/ F_ 1 ^n supported at the origin, where F_ 1 denotes the field of one element. This Hopf algebra may be viewed as an n-dimensional generalization of the Hopf algebra of symmetric functions, which corresponds to the case n= 1...|$|R
40|$|Cornelia de Lange Syndrome (CdLS) is a {{multi-organ}} system {{birth defects}} disorder linked, {{in at least}} half of cases, to heterozygous mutations in the NIPBL gene. In animals and fungi, orthologs of NIPBL regulate cohesin, a complex of proteins that is essential for chromosome cohesion and is also implicated in DNA repair and transcriptional regulation. Mice heterozygous for a gene-trap mutation in Nipbl were produced and exhibited defects characteristic of CdLS, including small size, craniofacial anomalies, microbrachycephaly, heart defects, hearing abnormalities, delayed bone maturation, reduced body fat, behavioral disturbances, and high mortality (75 - 80 %) during the first weeks of life. These phenotypes arose despite a decrease in Nipbl transcript levels of only ∼ 30 %, implying extreme sensitivity of development to small changes in Nipbl activity. Gene expression profiling demonstrated that Nipbl deficiency leads to modest but significant transcriptional dysregulation of many genes. Expression changes at the protocadherin beta (Pcdhb) locus, as well as at other loci, support the view that NIPBL influences long-range chromosomal regulatory interactions. In addition, evidence is presented that reduced expression of genes involved in adipogenic differentiation may <b>underlie</b> the low <b>amounts</b> of body fat observed both in Nipbl+/- mice and in individuals with CdLS...|$|R
40|$|Correspondence {{issued by}} the Government Accountability Office with an {{abstract}} that begins "As requested, this correspondence discusses our findings on whether the net excise tax revenue distributed to the Airport and Airway Trust Fund (AATF) {{for the fiscal year}} ended September 30, 2011, is supported by the underlying records. The Department of Transportation's Inspector General is responsible for the adequacy of these agreed-upon procedures to meet the inspector's objectives, and we make no representation in that respect. The procedures we agreed to perform were related to (1) transactions that represent the <b>underlying</b> basis of <b>amounts</b> distributed from the general fund to the AATF during fiscal year 2011, (2) the Internal Revenue Service's (IRS) quarterly AATF excise tax receipt certifications prepared during fiscal year 2011, (3) the U. S. Department of the Treasury's Financial Management Service adjustments to AATF excise tax distributions during fiscal year 2011, (4) the U. S. Department of the Treasury's Office of Tax Analysis's (OTA) estimates of excise tax amounts to be distributed to the AATF for the fourth quarter of fiscal year 2011, (5) adjustments to the AATF for tax on kerosene used in aviation during fiscal year 2011, and (6) the amount of net excise taxes distributed to the AATF during fiscal year 2011. ...|$|R
40|$|Correspondence {{issued by}} the Government Accountability Office with an {{abstract}} that begins "We assisted the Department of Transportation in ascertaining whether the net excise tax revenue distributed to the Airport and Airway Trust Fund (AATF) {{for the fiscal year}} ended September 30, 2004, is supported by the underlying records. In performing the agreed-upon procedures, we conducted our work in accordance with U. S. generally accepted government auditing standards, which incorporate financial audit and attestation standards established by the American Institute of Certified Public Accountants. The procedures we agreed to perform were (1) transactions that represent the <b>underlying</b> basis of <b>amounts</b> distributed to the AATF, (2) the Internal Revenue Service's (IRS) quarterly AATF certifications, (3) the Department of the Treasury's Financial Management Service adjustments to the AATF for fiscal year 2005, (4) IRS's precertification of receipts for the second and third quarters of fiscal year 2005, (5) certain procedures of the Department of the Treasury's Office of Tax Analysis' (OTA) estimation procedures affecting excise tax distributions to the AATF for the fourth quarter of fiscal year 2005, and other procedures including (6) the net amount of fiscal year 2005 excise taxes distributed to the AATF, (7) transactions that represent total IRS tax revenue receipts and refunds, and (8) key reconciliations of IRS records to Treasury records. ...|$|R
40|$|The {{understanding}} of melanoma malignancy mechanisms {{is essential for}} patient survival, because melanoma is responsible for ca. 75 % of deaths related to skin cancers. Enhanced formation of invadopodia and extracellular matrix (ECM) degradation are two important drivers of cell invasion, and actin dynamics facilitate protrusive activity by providing a driving force to push through the ECM. We focused {{on the influence of}} epidermal growth factor (EGF), hepatocyte growth factor (HGF) and transforming growth factor β (TGFβ) on melanoma cell invasiveness, since they are observed in the melanoma microenvironment. All three factors stimulated invasion of A 375 and WM 1341 D cells derived from primary tumor sites. In contrast, only EGF and HGF stimulated invasion of WM 9 and Hs 294 T cells isolated from lymph node metastases. Enhanced formation of invadopodia and ECM degradation <b>underlie</b> the increased <b>amount</b> of invasive cells after stimulation with the tested agents. Generally, a rise in invasive potential was accompanied by a decrease in actin polymerization state (F:G ratio). The F:G ratio remained unchanged or was even increased in metastatic cell lines treated with TGFβ. Our findings indicate that the effects of stimulation with EGF, HGF and TGFβ on melanoma cell invasiveness could depend on melanoma cell progression stage.  </p...|$|R
40|$|Several {{envelope}} proteins of Bordetella pertussis demonstrated {{differences in}} electrophoretic mobility, depending upon solubilization temperature before sodium dodecyl sulfate-polyacrylamide gel electrophoresis. These proteins were exposed {{on the cell}} surface as judged by their accessibility to radiolabeling with 125 I. Monoclonal antibodies {{to two of the}} heat-modifiable proteins (Mrs of 18, 000 and 91, 000) reacted with intact cells in immunofluorescence microscopy experiments, also indicating surface exposure of these two proteins. Two-dimensional gel electrophoresis revealed that two heat-modifiable proteins (a major protein with an Mr of 38, 000 and one with an Mr of 18, 000) migrated as higher-Mr moieties when solubilized at low temperatures (25 degrees C). Three proteins (Mrs of 91, 000, 32, 000, and 30, 000) and possibly a fourth (31, 000) migrated as lower-Mr species when solubilized at 25 degrees C, as revealed in the two-dimensional gel system; these three proteins were found only in virulent B. pertussis and were not detected in a phase IV avirulent strain nor in a strain modulated to phenotypic avirulence by growth in nicotinic acid. The 38, 000 molecular-weight protein (38 K protein) and a 25 K protein were found to be noncovalently associated with the <b>underlying</b> peptidoglycan. Small <b>amounts</b> of the 91 K and 18 K proteins were also found associated with peptidoglycan...|$|R
40|$|With a 1999 ASCA observation, PG 2112 + 059 became notable as {{the first}} Broad Absorption Line (BAL) quasar found to exhibit a typical radio-quiet quasar X-ray {{continuum}} <b>underlying</b> a large <b>amount</b> of intrinsic absorption. We present a recent Chandra ACIS-S 3 observation of PG 2112 + 059 that demonstrates remarkable spectral and luminosity variability since that time. In addition to {{a decrease in the}} continuum normalization by a factor of ~ 3. 5, the absorption column density has apparently increased substantially, and a strong feature in the Fe K alpha region has appeared. Concurrent HST STIS data compared with archival HST data from earlier epochs show evidence for variability of the continuum (up to a factor of ~ 1. 7 in the ultraviolet), and in some absorption features of the CIV 1549 BAL since 1992; however, the OVI BAL structure is consistent with a 1995 observation. We also present evidence for Ly beta [...] OVI 1037. 62 and Ly alpha [...] NV 1242. 80 line-locked absorption systems, supporting the assumption that ultraviolet line pressure is driving the BAL outflow. Whereas ultraviolet BALs typically exhibit only modest equivalent-width variability over timescales of years, the dramatic X-ray variability of PG 2112 + 059 suggests that X-ray spectral variability studies of BAL quasars have great potential for probing the physics of quasar winds. Comment: 30 pages, 10 figures, uses AASTeX. Accepted for publication in the Astrophysical Journa...|$|R
40|$|Primate-specific {{segmental}} duplications {{are considered}} important in human disease and evolution. The inability {{to distinguish between}} allelic and duplication sequence overlap has hampered their characterization as well as assembly and annotation of our genome. We developed a method whereby each public sequence is analyzed at the clone level for overrepresentation within a whole-genome shotgun sequence. This test {{has the ability to}} detect duplications larger than 15 kilobases irrespective of copy number, location, or high sequence similarity. We mapped 169 large regions flanked by highly similar duplications. Twenty-four of these hot spots of genomic instability have been associated with genetic disease. Our analysis indicates a highly nonrandom chromosomal and genic distribution of recent segmental duplications, with a likely role in expanding protein diversity. Initial analyses of the human genome sequence have identified a large amount of interspersed as well as tandem segmental duplications (1 – 3). These observations raise the possibility that segmental duplications may have {{played a significant role in}} gene and genome evolution compared with whole-genome duplication models (4). Furthermore, segmental duplications may <b>underlie</b> a greater <b>amount</b> of human phenotypic variation and disease than was previously recognized (5, 6). Unfortunately, duplicated regions of the genome are marginalized within both private and public assemblies (7). The overarching problem stems from the inability of current assembly strategies to differentiate highly similar duplicated sequence from true overlaps that remain unassembled. Using computational methods, we have developed a simple statistical test to determine whether a given stretch of sequence is duplicated based on its overrepresentation and average sequence identity within a random sample o...|$|R
40|$|Correspondence {{issued by}} the Government Accountability Office with an {{abstract}} that begins "We have performed the procedures contained in this report, which we agreed to perform and with which the Inspector General (IG) of the Department of Transportation concurred, solely to assist that office in ascertaining whether the net excise tax revenue distributed to the Airport and Airway Trust Fund (AATF) {{for the fiscal year}} ended September 30, 2004, is supported by the underlying records. We evaluated fiscal year 2004 activity affecting distributions to the AATF. The adequacy of the procedures to meet the IG's objectives is the IG's responsibility, and we make no representation in that respect. The procedures we agreed to perform were (1) detailed tests of transactions that represent the <b>underlying</b> basis of <b>amounts</b> distributed to the AATF, (2) review of the Internal Revenue Service's (IRS) quarterly AATF certifications, (3) review of the Department of the Treasury's Financial Management Service adjustments to the AATF for fiscal year 2004, (4) review of IRS's precertification 1 of receipts for the second and third quarters of fiscal year 2004, (5) review of certain procedures of the Department of the Treasury's Office of Tax Analysis' (OTA) estimation procedures affecting excise tax distributions to the AATF for the fourth quarter of fiscal year 2004, and other procedures including (6) compiling and reporting the net amount of fiscal year 2004 excise taxes distributed to the AATF, (7) detailed tests of transactions that represent total IRS tax revenue receipts and refunds, and (8) review of key reconciliations of IRS records to Treasury records. ...|$|R
40|$|Klebsiella pneumoniae, a Gram-negative bacterium, is {{normally}} associated with pneumonia {{in patients with}} weakened immune systems. However, {{it is also a}} prevalent nosocomial infectious agent that can be found in infected surgical sites and combat wounds. Many of these clinical strains display multidrug resistance. We have worked with a clinical strain of K. pneumoniae that was initially isolated from a wound of an injured soldier. This strain demonstrated resistance to many commonly used antibiot-ics but sensitivity to carbapenems. This isolate was capable of forming biofilms in vitro, contributing to its increased antibiotic resistance and impaired clearance. We were interested in determining how sublethal concentrations of carbapenem treatment specifically affect K. pneumoniae biofilms both in morphology and in genomic expression. Scanning electron microscopy showed striking morphological differences between untreated and treated biofilms, including rounding, blebbing, and dimpling of treated cells. Comparative transcriptome analysis using RNA sequencing (RNA-Seq) technology identified a large number of open reading frames (ORFs) differentially regulated in response to carbapenem treatment at 2 and 24 h. ORFs upregulated with carbapenem treatment included genes involved in resistance, as well as those coding for antiporters and autoinducers. ORFs downregulated included those coding for metal transporters, membrane biosynthesis proteins, andmotility proteins. Quantita-tive real-time PCR validated the general trend of some of these differentially regulated ORFs. Treatment of K. pneumoniae bio-films with sublethal concentrations of carbapenems induced a wide range of phenotypic and gene expression changes. This study reveals some of the mechanisms <b>underlying</b> how sublethal <b>amounts</b> of carbapenems could affect the overall fitness and patho...|$|R
40|$|To improve {{both the}} {{efficiency}} and accuracy of video semantic recognition, we can perform feature selection on the extracted video features to select a subset of features from the high-dimensional feature set for a compact and accurate video data representation. Provided the number of labeled videos is small, supervised feature selection could fail to identify the relevant features that are discriminative to target classes. In many applications, abundant unlabeled videos are easily accessible. This motivates us to develop semisupervised feature selection algorithms to better identify the relevant video features, which are discriminative to target classes by effectively exploiting the information <b>underlying</b> the huge <b>amount</b> of unlabeled video data. In this paper, we propose a framework of video semantic recognition by semisupervised feature selection via spline regression (SFSR). Two scatter matrices are combined to capture both the discriminative information and the local geometry structure of labeled and unlabeled training videos: A within-class scatter matrix encoding discriminative information of labeled training videos and a spline scatter output from a local spline regression encoding data distribution. An ℓ-norm is imposed as a regularization term on the transformation matrix to ensure it is sparse in rows, making it particularly suitable for feature selection. To efficiently solve SFSR, we develop an iterative algorithm and prove its convergency. In the experiments, three typical tasks of video semantic recognition, such as video concept detection, video classification, and human action recognition, are used {{to demonstrate that the}} proposed SFSR achieves better performance compared with the state-of-the-art methods...|$|R
