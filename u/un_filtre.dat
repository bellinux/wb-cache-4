11|3|Public
40|$|Transport of {{excitations}} (phonons and quasiparticles) in superconducting tinMISE EN EVIDENCE, EN REGIME DE FORTE EXCITATION, DU DEGRE DE FRAGILITE DU SYSTEME QUASI-PARTICULES CONDENSAT DE PAIRES. LES MATERIAUX DE GRANDE DIFFUSIVITE ELECTRONIQUE ONT CANTONNE L'ETUDE A DES SITUATIONS DE QUASI-HOMOGENEITE. EN CE QUI CONCERNE LES PHONONS BALISTIQUES, DES RESULTATS PRECIS ONT ETE OBTENUS: L'ETAT SUPRACONDUCTEUR A T= 0 AGIT COMME <b>UN</b> <b>FILTRE</b> PARFAIT DES FREQUENCES JUSQU'A LA CASSURE DES PAIRES DONT LE SEUIL ET L'ANISOTROPIE SONT AINSI MESURABLES; EN FRANCHANT THERMIQUEMENT L'INTERACTION AVEC LES QUASI-PARTICULES, LES DUREES DE VIES LIMITEES PAR LES ELECTRONS ONT ETE OBTENUES MODE PAR MODE. DES MESURES SUR LE NIOBIUM ONT PERMIS DE DONNER UNE VALEUR MOYENNE DU POTENTIEL DE DEFORMATION. APPROCHE SYNTHETIQUE DES MODES POSSIBLES DE LA CONDUCTIVITE ELECTRONIQUE PAR UNE METHODE D'EQUATIONS DE TRANSPORT COUPLEES. CONCLUSIO...|$|E
40|$|ESSEC Working paper. Document de Recherche ESSEC / Centre de {{recherche}} de l'ESSEC ISSN : 1291 - 9616 DR 09004 This original {{study examines}} {{the potential of}} a spatiotemporal autoregressive Local (LSTAR) approach in modelling transaction prices for the housing market in inner Paris. We use a data set from the Paris Region notary office (“Chambre des notaires d'Île-de-France”) which consists of approximately 250, 000 transactions units between {{the first quarter of}} 1990 and the end of 2005. We use the exact X [...] Y coordinates and transaction date to spatially and temporally sort each transaction. We first choose to use the spatiotemporal autoregressive (STAR) approach proposed by Pace, Barry, Clapp and Rodriguez (1998). This method incorporates a spatiotemporal filtering process into the conventional hedonic function and attempts to correct for spatial and temporal correlative effects. We find significant estimates of spatial dependence effects. Moreover, using an original methodology, we find evidence of a strong presence of both spatial and temporal heterogeneity in the model. It suggests that spatial and temporal drifts in households socio-economic profiles and local housing market structure effects are certainly major determinants of the price level for the Paris Housing Market. Cette étude originale évalue l'apport d'une modélisation spatio-temporelle autorégressive (STAR) pour expliquer l'évolution des prix des transactions de logements sur Paris et sa première couronne. Nous utilisons la méthode STAR introduite par Pace, Barry, Clapp and Rodriguez (1998), qui incorpore <b>un</b> <b>filtre</b> spatio-temporel à la fonction hédonique standard. La recherche indique une forte présence d'interdépendances spatiales et temporelles dans ces sous-marchés qui semblent déterminantes dans l'analyse des prix immobiliers de logements à Paris...|$|E
40|$|LES MODULATIONS LINEAIRES DE TYPE MAQ OU MDP SONT SOUVENT TRANSMISES A DES DEBITS TELS QUE LE TEMPS SYMBOLE EST BIEN INFERIEUR A L'ETALEMENT TEMPOREL DU CANAL. LES SYSTEMES DE TRANSMISSION FONT DONC USAGE DE SEQUENCES D'APPRENTISSAGE POUR PERMETTRE AU RECEPTEUR D'ESTIMER LE CANAL ET CORRIGER LES PERTURBATIONS QUI EN RESULTENT. DES EFFORTS TRES IMPORTANTS ONT ETE CONSENTIS CES VINGT DERNIERES ANNEES POUR TENTER DE S'AFFRANCHIR DES SEQUENCES D'APPRENTISSAGE DANS LE BUT D'AMELIORER L'EFFICACITE SPECTRALE. ON PARLE ALORS DE DEMODULATION AUTODIDACTE OU AVEUGLE. LA PLUPART DES EGALISEURS AVEUGLES ACTUELS SONT DE TYPE TRANSVERSE ET SONT DONC MAL ADAPTES AUX CANAUX TRES SELECTIFS. L'OBJET DE CETTE THESE CONSISTE A ETUDIER L'APPLICATION DES STRUCTURES RECURSIVES AU CONTEXTE DE L'EGALISATION AVEUGLE. DANS CE CADRE, LES TROIS RESULTATS SUIVANTS ONT ETE ETABLIS : (I) POUR LES VALEURS DE RSB PERMETTANT L'OUVERTURE DE L'IL EN SORTIE DE L'EGALISEUR, LE CRITERE DE GODARD CONDUIT A <b>UN</b> <b>FILTRE</b> PROCHE DE CELUI DE WIENER QUI EST LA REFERENCE DANS CE TYPE DE PROBLEME; IL N'EST DONC PAS UTILE DE CONSIDERER D'AUTRES CRITERES DANS LA PHASE D'ACCROCHAGE; (II) L'OPTIMISATION GLOBALE D'UNE STRUCTURE RECURSIVE ENTRAINE L'APPARITION DE COMPENSATIONS POLE-ZERO AUTOUR DU CERCLE UNITE; C'EST POURQUOI LES EGALISEURS CORRESPONDANTS NE PEUVENT PAS ETRE RETENUS POUR UNE REALISATION PRATIQUE; (III) LES MEILLEURS RESULTATS SONT OBTENUS AVEC UNE STRUCTURE BI-RECURSIVE (CAUSALE ET ANTICAUSALE) IMITANT LE FILTRE DE WIENER ET DANS LESQUELLES LE NUMERATEUR ET LE DENOMINATEUR DE LA FONCTION DE TRANSFERT SONT OPTIMISES SEPAREMENT. POUR PERMETTRE LE SUIVI DE CANAUX A FORT ETALEMENT DOPPLER, LES DONNEES SONT TRAITEES PAR BLOCS. UNE CAMPAGNE D'ESSAIS SYSTEMATIQUES A ETE REALISEE POUR VALIDER CETTE SOLUTION ET LA COMPARER AUX SOLUTIONS ANTERIEURES. LES RESULTATS MONTRENT QU'ELLE FIGURE PARMI LES PLUS PERFORMANTES. CERGY-ENSEA (951272302) / SudocRENNES 1 -BU Sciences Philo (352382102) / SudocSudocFranceF...|$|E
40|$|International audienceThe Vietoris–Rips {{filtration}} is {{a versatile}} tool in topological data analysis. It is {{a sequence of}} simplicial complexes built on a metric space to add topological structure to an otherwise disconnected set of points. It is widely used because it encodes useful information about the topology of the underlying metric space. This information is of-ten extracted from its so-called persistence diagram. Unfortunately, this filtration is often too large to construct in full. We show how to construct an O(n) -size filtered simplicial complex on an n-point metric space such that its persistence diagram is a good approximation {{to that of the}} Vietoris–Rips filtration. This new filtration can be constructed in O(n log n) time. The constant factors in both the size and the run-ning time depend only on the doubling dimension of the metric space and the desired tightness of the approximation. For the first time, this makes it computationally tractable to approximate the persistence di-agram of the Vietoris–Rips filtration across all scales for large data sets. We describe two different sparse filtrations. The first is a zigzag filtration that removes points as the scale increases. The second is a (non-zigzag) filtration that yields the same persistence diagram. Both methods are based on a hierarchical net-tree and yield the same guar-antees. La filtration de Vietoris-Rips est un outil très versatile en analyse topologique des données. C'est une séquence de complexes simpliciaux construits sur une métrique pour ajouter de la structure topologique à un nuage de points. Malheureusement, cette filtration est souvent trop large pour tenir entièerement en mémoire. Nous montrons comment construire <b>un</b> complexe simplicial <b>filtré</b> de taille O(n) à partir d'un espace métrique fini composé de n points, de manièere à ce que le diagramme de persistance de ce complexe filtré soit une bonne approximation de celui de la filtration de Vietoris-Rips...|$|R
40|$|This thesis {{addresses}} imitative digital {{sound synthesis}} of acoustically viable instruments with support of expressive, high-level control parameters. A general model is provided for quasi-harmonic instruments that reacts coherently with its acoustical equivalent when control parameters are varied. The approach builds upon recording-based methods and uses signal transformation techniques to manipulate instrument sound signals {{in a manner}} that resembles the behavior of their acoustical equivalents using the fundamental control parameters intensity and pitch. The method preserves the inherent quality of discretized recordings of a sound of acoustic instruments and introduces a transformation method that retains the coherency with its timbral variations when control parameters are modified. It is thus meant to introduce parametric control for sampling sound synthesis. The objective of this thesis is to introduce a new general model representing the timbre variations of quasi-harmonic music instruments regarding a parameter space determined by the control parameters pitch as well as global and instantaneous intensity. The model independently represents the deterministic and non-deterministic components of an instrument's signal and an extended source-filter model will be introduced for the former to represent the excitation and resonance characteristics of a music instrument by individual parametric filter functions. The latter component will be represented using a classic source-filter approach using filters with similar parameterization. All filter functions are represented using tensor-product B-splines to support for multivariate control variables. An algorithm will be presented for the estimation of the model's parameters that allows for the joint estimation of the filter functions of either component in a multivariate surface-fitting approach using a data-driven optimization strategy. This procedure also includes smoothness constraints and solutions for missing or sparse data and requires suitable data sets of single note recordings of a particular musical instrument. Another original contribution of the present thesis is an algorithm for the calibration of a note's intensity by means of an analysis of crescendo and decrescendo signals using the presented instrument model. The method enables the adjustment of the note intensity of an instrument sound coherent with the relative differences between varied values of its note intensity. A subjective evaluation procedure is presented to assess the quality of the transformations obtained using a calibrated instrument model and independently varied control parameters pitch and note intensity. Several extends of sound signal manipulations will be presented therein. For the support of inharmonic sounds as present in signals produced by the piano, a new algorithm for the joint estimation of a signal's fundamental frequency and inharmonicity coefficient is presented to extend the range of possible instruments to be manageable by the system. The synthesis system will be evaluated in various ways for sound signals of a trumpet, a clarinet, a violin and a piano. Dans cette thèse un système de synthèse sonore imitative sera présenté, applicable à la plupart des instruments de quasi-harmoniques. Le système se base sur les enregistrements d’une note unique qui représentent une version quantifiée de l'espace de timbre possible d'un instrument par rapport à sa hauteur et son intensité. Une méthode de transformation permet alors de générer des signaux sonores de valeurs continues des paramètres de contrôle d'expression qui sont perceptuellement cohérent avec ses équivalents acoustiques. Un modèle paramétrique de l'instrument se présente donc basé sur <b>un</b> modèle de <b>filtre</b> de source étendu avec des manipulations distinctes sur les harmoniques d’un signal et ses composantes résiduelles. Une procédure d'évaluation subjective sera présentée afin d’évaluer une variété de résultats de transformation par une comparaison directe avec des enregistrements non modifiés, afin de comparer la perception entre les résultats synthétiques et leur équivalents acoustiques...|$|R
40|$|La modélisation des systèmes dynamiques requiert la prise en compte d’incertitudes liées à l’existence inévitable de bruits (bruits de mesure, bruits sur la dynamique), à la méconnaissance de certains phénomènes perturbateurs mais également aux incertitudes sur la valeur des paramètres (spécification de tolérances, phénomène de vieillissement). Alors que certaines de ces incertitudes se prêtent bien à une modélisation de type statistique comme {{par exemple}} ! les bruits de mesure, d’autres se caractérisent mieux pa ! r des bornes, sans autre attribut. Dans ce travail de thèse, motivés par les {{observations}} ci-dessus, nous traitons le problème de l’intégration d’incertitudes statistiques et à erreurs bornées pour les systèmes linéaires à temps discret. Partant du filtre de Kalman Intervalle (noté IKF) développé dans [Chen 1997], nous proposons des améliorations significatives basées sur des techniques récentes de propagation de contraintes et d’inversion ensembliste qui, contrairement aux mécanismes mis en jeu par l’IKF, permettent d’obtenir un résultat garanti tout en contrôlant le pessimisme de l’analyse par intervalles. Cet algorithme est noté iIKF. Le filtre iIKF a la même structure récursive que le filtre de Kalman classique et délivre un encadrement de tous les estimés optimaux et des matrices de covariance possibles. L’algorithme IKF précédent évite quant à lui le problème de l’inversion des matrices intervalles, ce qui lui vaut de perdre des solutions possibles. Pour l’iIKF, nous proposons une méthode originale garantie pour l’inversion des matrices intervalle qui couple l’algorithme SIVIA (Set Inversion via Interval Analysis) et un ensemble de problèmes de propagation de contraintes. Par ailleurs, plusieurs mécanismes basés sur la propagation de contraintes sont également mis en œuvre pour limiter l’effet de surestimation due à la propagation d’intervalles dans la structure récursive du <b>filtre.</b> <b>Un</b> algorithme de détection de défauts basé sur iIKF est proposé en mettant en œuvre une stratégie de boucle semi-fermée qui permet de ne pas réalimenter le filtre avec des mesures corrompues par le défaut dès que celui-ci est détecté. A travers différents exemples, les avantages du filtre iIKF sont exposés et l’efficacité de l’algorithme de détection de défauts est démontré. ABSTRACT : In this thesis, {{a new approach to}} estimation problems under the presence of bounded uncertain parameters and statistical noise has been presented. The objective is to use the uncertainty model which appears as the most appropriate for every kind of uncertainty. This leads to the need to consider uncertain stochastic systems and to study how the two types of uncertainty combine : statistical noise is modeled as the centered gaussian variable and the unknown but bounded parameters are approximated by intervals. This results in an estimation problem that demands the development of mixed filters and a set-theoretic strategy. The attention is drawn on set inversion problems and constraint satisfaction problems. The former is the foundation of a method for solving interval equations, and the latter can significantly improve the speed of interval based arithmetic and algorithms. An important contribution of this work consists in proposing an interval matrix inversion method which couples the algorithm SIVIA with the construction of a list of constraint propagation problems. The system model is formalized as an uncertain stochastic system. Starting with the interval Kalman filtering algorithm proposed in [Chen 1997] and that we name the IKF, an improved interval Kalman filtering algorithm (iIKF) is proposed. This algorithm is based on interval conditional expectation for interval linear systems. The iIKF has the same structure as the conventional Kalman filter while achieving guaranteed statistical optimality. The recursive computational scheme is developed in the set-membership context. Our improvements achieve guaranteed interval inversion whereas the original version IKF [Chen 1997] uses an instance (the upper bound) of the interval matrix to avoid the possible singularity problems. This point of view leads to a sub-optimal solution that does not preserve guaranteed results, some solutions being lost. On the contrary, in the presence of unknown-but-bounded parameters and measurement statistical errors, our estimation approach {{in the form of the}} iIKF provides guaranteed estimates, while maintaining a computational burden comparable to that of classic statistical approaches. Several constraint based techniques have also been implemented to limit the overestimation effect due to interval propagation within the interval Kalman filter recursive structure. The results have shown that the iIKF out puts bounded estimates that enclose all the solutions consistent with bounded errors and achieves good overestimation control. iIKF is used to propose a fault detection algorithm which makes use of a Semi-Closed Loop strategy which does not correct the state estimate with the measure as soon as a fault is detected. Two methods for generating fault indicators are proposed : they use the a priori state estimate and a threshold based on the a posteriori and a priori covariance matrix, respectively, and check the consistency against the measured output. Through different examples, the advantages of the iIKF with respect to previous versions are exhibited and the efficiency of the iIKF based Semi-Closed Loop fault detection algorithm is clearly demonstrated...|$|R
40|$|LES SIMULATIONS LES REQUIERENT LE DEVELOPPEMENT ET L'UTILISATION DE MODELES DE SOUS-MAILLE POUR DECRIRE L'EFFET DES ECHELLES TURBULENTES NON-RESOLUES. AU COURS DE NOTRE ETUDE, NOUS AVONS PROPOSE UNE NOUVELLE FORMULATION FONDEE SUR L'EQUATION DE TRANSPORT DE LA VARIABLE D'AVANCEMENT C. CETTE GRANDEUR, BIEN DEFINIE, PEUT ETRE FACILEMENT EXTRAITE DE SIMULATIONS NUMERIQUES DIRECTES (TESTS A PRIORI) OU MESUREE EXPERIMENTALEMENT (TESTS A PRIORI POUR DEVELOPPER DES MODELES OU A POSTERIORI POUR VALIDER LES SIMULATIONS). L'EQUATION DE TRANSPORT DE LA VARIABLE D'AVANCEMENT EST FILTREE AVEC <b>UN</b> <b>FILTRE</b> LES PLUS LARGE QUE LE MAILLAGE DE CALCUL POUR POUVOIR RESOUDRE NUMERIQUEMENT LA VARIABLE D'AVANCEMENT FILTREE $. CETTE OPERATION INTRODUIT DES GRANDEURS NON FERMEES QU'IL FAUT MODELISER. TOUTES CES GRANDEURS ONT ETE ETUDIEES, A LA FOIS ANALYTIQUEMENT, A PARTIR DE SIMULATIONS NUMERIQUES DIRECTES ET A PARTIR DE MESURES EXPERIMENTALES. CETTE ANALYSE NOUS A CONDUIT A PROPOSER UNE FORMULATION BASEE SUR LA NOTION DE DENSITE DE SURFACE DE FLAMME DE SOUS-MAILLE. UN POINT IMPORTANT CONCERNE LA CAPACITE DU MODELE PROPOSE A PREDIRE LA PROPAGATION D'UNE FLAMME LAMINAIRE PLANE. IL APPARAIT ALORS QUE LE FLUX CONVECTIF NON RESOLU COMPORTE UNE CONTRIBUTION LAMINAIRE QUI NE PEUT ETRE NEGLIGEE. UN TERME DIFFUSIF A EGALEMENT ETE INCORPORE A NOTRE MODELE POUR GERER, DANS LES SIMULATIONS PRATIQUES, L'EPAISSEUR DE LA FLAMME RESOLUE. FINALEMENT, NOTRE MODELE A ETE IMPLANTE DANS LE CODE AVBP. TOUT D'ABORD, LA CAPACITE DU MODELE A DEGENERER CORRECTEMENT VERS LES SITUATIONS LAMINAIRES A ETE VERIFIEE PAR LA SIMULATION D'UNE FLAMME PLANE, MODIMENSIONNELLE, STATIONNAIRE. DES SIMULATIONS BI-DIMENSIONNELLES ONT ENSUITE ETE CONDUITES ET ONT MONTRE UN BON COMPORTEMENT DU MODELE, COMPARATIVEMENT A L'APPROCHE BASEE SUR L'EPAISSISSEMENT ARTIFICIEL DE LA FLAMME. LES DONNEES EXPERIMENTALES QUANTITATIVES DISPONIBLES POUR VALIDER UN TEL MODELE SONT, MALHEUREUSEMENT, ENCORE PEU NOMBREUSES, MAIS LES PREMIERS TESTS S'AVERENT TRES PROMETTEURS. IL FAUT SOULIGNER QUE CES CALCULS LES ONT PERMIS DE METTRE EN EVIDENCE L'APPARITION, DANS CERTAINS CAS, DE TRANSPORT TURBULENT DE TYPE CONTRE-GRADIENT, AU MOINS A L'ECHELLE RESOLUE, MALGRE L'UTILISATION D'UNE MODELISATION DE TYPE GRADIENT DES FLUX NON-RESOLUS. CHATENAY MALABRY-Ecole {{centrale}} (920192301) / SudocSudocFranceF...|$|E
40|$|Neodymium ion is a {{potential}} candidate to realize an optical amplifier for the second optical telecommunications window, in parts owing to its strong absorption band accessible by commercial power laser diodes. Yet, 1. 3 mm emission competes with the 1. 06 mm transition emitted from the same metastable energy level {{and in order to}} increase performances of neodymium doped optical amplifiers this 1. 06 mm emission has to be eliminated. Therefore, we propose a structure including a filter based on a tilted Bragg grating distributed on the whole length of the amplifier. The design of this amplifier is based on a thin layer of lanthanum trifluoride (LaF 3) doped with neodymium (Nd 3 +). A spectroscopic study of Nd 3 +:LaF 3 has been carried out. The aim {{of this study is to}} analyse some parameters having an influence upon the amplifier performances, particularly the polarisation of 1. 03 mm emission and the effect of Nd 3 + concentration. Thereafter a fabrication process has been developed. This process is inventive by many points: first by the association of a polymeric material to realise passive optical functions and monocrystalline Nd 3 +:LaF 3 as an active material, and secondly by the development of generic technologies for the low cost fabrication of optical functions such as nanoimprint lithography using very low viscosity thermocurable polymers and poli-dicing. Surface treatments improving the adhesion on polymers and ionic crystals have also been developed. Finally, the gain of a micro-amplifier at 1. 3 mm has been modelled and conditions to obtain a high gain and a good efficiency have been defined. After this modelling, a structure is proposed to realise the amplifier. L'ion neodyme est un candidat potentiel pour realiser un amplificateur optique pour la seconde bande de telecommunications optiques, en particulier du fait de sa bande d'absorption accessible aux diodes laser du commerce. Cependant, l'emission a 1, 3 mm est en competition avec la transition a 1, 06 mm issue du meme niveau metastable. Pour optimiser les performances de l'amplificateur a base de neodyme, il faut eliminer cette emission. Nous proposons donc une structure integrant <b>un</b> <b>filtre</b> par reseau de Bragg incline et distribue sur toute la longueur de l'amplificateur. Cet amplificateur est concu a partir d'une couche mince de trifluorure de lanthane (LaF 3) dope neodyme (Nd 3 +). Nous avons mene une etude spectroscopique du LaF 3 :Nd 3 +. Durant cette etude, nous nous sommes attaches a etudier certains parametres importants pour l'amplificateur, notamment la polarisation de l'emission a 1, 3 mm et l'influence de la concentration en Nd 3 +. Nous avons ensuite developpe un procede de realisation original a plusieurs points de vue : tout d'abord par l'association d'un materiau polymere pour realiser les fonctions optiques passives et du LaF 3 :Nd 3 + monocristallin comme materiau actif, ensuite par le developpement de technologies generiques permettant de realiser des fonctions optiques a bas cout comme la nanoimpression utilisant une resine thermodurcissable tres fluide et le polisciage. Nous avons aussi mis au point des traitements de surface permettant d'ameliorer l'adherence sur un polymere et les cristaux ioniques. Nous avons enfin modelise le gain d'un micro amplificateur pour 1, 3 mm et determine les conditions a respecter pour avoir un gain efficace et un bon rendement. A la suite de quoi, nous proposons une structure pour realiser un tel amplificateur...|$|E
40|$|CETTE THESE A ETE REALISEE POUR REPONDRE A LA QUESTION SUIVANTE : LES INFORMATIONS APPORTEES PAR LES MARCHES A TERME DE COMMODITES ET LES TECHNIQUES DE COUVERTURE QU'ILS PROPOSENT PEUVENTELLES SERVIR A VALORISER UN GISEMENT PETROLIER ET DECIDER DE SA DATE D'EXPLOITATION ? LES TRAVAUX ONT DONC ETE CENTRES SUR LA VALORISATION D'UN BARIL DE PETROLE POUR UNE ECHEANCE LOINTAINE DE LIVRAISON. POUR Y PARVENIR, UN MODELE DE STRUCTURE PAR TERME DES PRIX DES COMMODITES A ETE EMPLOYE. PLUSIEURS ETAPES ONT SUCCESSIVEMENT ETE ABORDEES. LES THEORIES TRADITIONNELLES DES PRIX DES COMMODITES ONT D'ABORD ETE EXPLOREES, AFIN DE COMPRENDRE LES RELATIONS ENTRE PRIX AU COMPTANT ET PRIX A TERME. LES PRINCIPAUX MODELES DE STRUCTURE PAR TERME DES PRIX DES COMMODITES ONT ENSUITE ETE ETUDIES. A CE STADE DES TRAVAUX, LE CONSTAT SUIVANT A ETE ETABLI : SI LES THEORIES TRADITIONNELLES CONSTITUENT LE FONDEMENT THEORIQUE DE L'ELABORATION D'UN MODELE DE STRUCTURE PAR TERME DES PRIX, TOUS LES ENSEIGNEMENTS DE CES THEORIES N'ONT CEPENDANT PAS ETE EXPLOITES; LE COMPORTEMENT ASYMETRIQUE DE LA BASE A DES IMPLICATIONS SUR LE COMPORTEMENT DYNAMIQUE DU CONVENIENCE YIELD IGNOREES A CE JOUR. PARTANT DE CE CONSTAT, UN MODELE DE STRUCTURE PAR TERME DES PRIX DES COMMODITES, DANS LEQUEL LE CONVENIENCE YIELD A UN COMPORTEMENT ASYMETRIQUE, A ETE DEVELOPPE. CE MODELE A ETE COMPARE AUX DEUX MODELES LES PLUS EMPLOYES DANS LE DOMAINE DES COMMODITES. SES PERFORMANCES ONT ETE TESTEES EN EMPLOYANT <b>UN</b> <b>FILTRE</b> DE KALMAN. LES RESULTATS OBTENUS LAISSENT PENSER QUE L'HYPOTHESE D'ASYMETRIE DU CONVENIENCE YIELD EST VERIFIEE, BIEN QUE LA METHODE D'ESTIMATION DES PARAMETRES DU MODELE DOIVE ETRE AMELIOREE. LA TROISIEME PARTIE DE LA THESE EST CONSACREE A L'ETUDE DE PROJETS D'INVESTISSEMENT. L'APPORT DES MODELES DE STRUCTURE PAR TERME DES PRIX DANS CE TYPE D'ANALYSE EST MIS EN EVIDENCE A PARTIR D'UNE COMPARAISON AVEC LES METHODES TRADITIONNELLEMENT EMPLOYEES POUR VALORISER UN GISEMENT ET DECIDER DE SA DATE DE MISE EN EXPLOITATION. The {{aim of this}} pH-D {{thesis is}} to answer the {{following}} question : Is it possible to use the information offered by the commodity futures markets and the hedging instruments they propose to value a petroleum field and decide when it is suitable to invest ? A term structure model for commodity prices is developed to value a petroleum barrel for postponed delivery. We survey both the traditional theories of commodity prices (first part) and the main term structure models for futures prices (second part). The former allow understanding the relation between spot and futures prices and make up the theoretical foundations of the term structure models. Yet these models did not take into account one important result drawn from commodity prices theories, namely : the asymmetrical behavior of the basis has implications on the dynamic of the convenience yield. In order to bridge this gap, we propose a model in which the convenience yield has an asymmetrical behavior. Relying on the Kalman filter methodology, we compare its performance with that of the most common model in the field : the Schwartz’s model (1997). We reach two main conclusions. First, results tend to confirm the assumption that the dynamic of the convenience yield is asymmetrical. Second, the Kalman filter appears not to be the most convenient way to solve the inverse problem faced in term structure models of commodity prices. In the third part, the relevance of term structure models for investment decisions is tested by applying them to an empirical case study : the valuation of a petroleum field and the optimal timing for investment and production decisions. Comparison with standard methods leads us to conclude that these methods tend to rush investment and production decisions, because they do not consider the possibility of postponing them. Marchés à terme; Matières premières; Pétrole; Gestion des risques;...|$|E
40|$|In {{the first}} part of this thesis, we study the {{influence}} of the Southern Annular Mode (SAM) onto the Atlantic Meridional Overturning Circulation (AMOC) in a control simulation with the IPSLCM 4 model. We show that a positive phase of the SAM, corresponding to an intensification of the Westerlies south of 45 ̊S, leads to an acceleration of the AMOC after 8 years, via an atmospheric teleconnection, which is likely to be model-dependent. We also find a link between the SAM and the AMOC at a multidecadal time scale. Positive salt anomalies, created by a positive SAM, enter the South Atlantic from the Drake Passage and, more importantly, via the Aghulas leakage; they propagate northward, eventually reaching the northern North Atlantic where they decrease the vertical stratification and thus increase the AMOC. In the second part of this thesis, we study to what extent the Atlantic Multidecadal Oscillation (AMO) reflects the AMOC fluctuations, and what are the other signals that influence it in natural climate variability conditions and with the external forcings of the 20 th century. We use a dynamical filter, based on linear inverse modeling (LIM), to decompose the North Atlantic sea surface temperature (SST) field into a global trend, an El Nino Southern Oscillation-related part (ENSO), a part associated with Pacific decadal variability and a residual. In the historical simulation with the IPSLCM 5 climate model, removing the global trend from the AMO with LIM yields to better correlations with the AMOC than removing a linear trend or the global mean SST. We also find that removing the ENSO-related part from the AMO with LIM leads to small changes, and even better correlations with the AMOC. Additionaly removing the signal associated to Pacific decadal variability leads to considerable changes in the AMO, which becomes much less correlated to the AMOC. The robustness of these results is demonstrated by performing the LIM decomposition in control simulations with five different climate models. Besides, the LIM filter is applied to SST observations. Finally, we briefly study the impact of the volcanic forcing onto the links between the AMO and the AMOC in a simulation of the last millenium with the IPSLCM 4 climate model. Nous étudions d'abord l'influence du mode annulaire Sud (SAM) sur la circulation méridienne de retournement Atlantique (AMOC) dans une simulation de contrôle avec IPSLCM 4. Une phase positive du SAM, correspondant à une intensification des vents d'Ouest soufflant au Sud de 45 ° S, entraîne après 8 ans une accélération de l'AMOC, via une téléconnection atmosphérique peu réaliste. Une accélération de l'AMOC suit de 70 ans une phase positive du SAM, due à la propagation d'anomalies de sel du Sud vers le Nord de l'Atlantique. Ensuite, nous étudions dans quelle mesure l'Oscillation Multidécennale Atlantique (AMO) reflète les fluctuations de l'AMOC. Nous utilisons <b>un</b> <b>filtre</b> basé sur un modèle linéaire inverse (LIM) pour décomposer la température de surface de l'océan (SST) Atlantique Nord en une partie liée à la dérive globale, une à El Nino (ENSO), une à la variabilité de basse fréquence du Pacifique, et un résidu. Dans la simulation historique de IPSLCM 5, enlever la dérive globale de l'AMO avec LIM induit de meilleures corrélations avec l'AMOC que lorsque ce signal est soustrait par une dérive linéaire. Enlever l'influence de ENSO de l'AMO améliore très légérement ses corrélations avec l'AMOC, tandis qu'enlever la variabilité de basse fréquence du Pacifique les dégrade. La robustesse de ces résultats est vérifiée dans des simulations de contrôle avec 5 modèles. Cette déconstruction de l'AMO est aussi effectuée dans les observations. Enfin, l'impact du forçage volcanique sur les liens AMO-AMOC est étudié dans une simulation du dernier millénaire avec IPSLCM 4...|$|E
40|$|Wireless Body Area Networks (WBAN) {{refers to}} the family of “wearable” {{wireless}} sensor networks (WSN) used to collect personal data, such as human activity, heart rate, sleep sequences or geographical position. This thesis aims at proposing cooperative algorithms and cross-layer mechanisms with WBAN to perform large-scale individual motion capture and coordinated group navigation applications. For this purpose, we exploit the advantages of jointly cooperative and heterogeneous WBAN under full/half-mesh topologies for localization purposes, from on-body links at the body scale, body-to-body links between mobile users of a group and off-body links with respect to the environment and the infrastructure. The wireless transmission relies on an impulse radio Ultra-Wideband (IR-UWB) radio (based on the IEEE 802. 15. 6 standard), in order to obtain accurate peer-to-peer ranging measurements based on Time of Arrival (ToA) estimates. Thus, we address the problem of positioning and ranging estimation through the design of cross-layer strategies by considering realistic body mobility and channel variations. Our first contribution consists in the creation of an unprecedented WBAN measurement database obtained with real experimental scenarios for mobility and channel modelling. Then, we introduce a discrete-event (WSNet) and deterministic (PyLayers) co-simulator tool able to exploit our measurement database to help us on the design and validation of cooperative algorithms. Using these tools, we investigate the impact of nodes mobility and channel variations on the ranging estimation. In particular, we study the “three-way ranging” (3 -WR) protocol and we observed that the delays of 3 -WR packets {{have an impact on the}} distances estimated in function of the speed of nodes. Then, we quantify and compare the error with statistical models and we show that the error generated by the channel is bigger than the mobility error. In a second time, we extend our study for the position estimation. Thus, we analyze different strategies at MAC layer through scheduling and slot allocation algorithms to reduce the impact of mobility. Then, we propose to optimize our positioning algorithm with an extended Kalman filter (EKF), by using our scheduling strategies and the statistical models of mobility and channel errors. Finally, we propose a distributed-cooperative algorithm based on the analysis of long-term and short-term link quality estimators (LQEs) to improve the reliability of positioning. To do so, we evaluate the positioning success rate under three different channel models (empirical, simulated and experimental) along with a conditional algorithm (based on game theory) for virtual anchor choice. We show that our algorithm improve the number of positions estimated for the nodes with the worst localization performance. Les réseaux corporels (WBAN) se réfère aux réseaux de capteurs (WSN) "portables" utilisés pour collecter des données personnelles, telles que la fréquence cardiaque ou l'activité humaine. Cette thèse a pour objectif de proposer des algorithmes coopératifs (PHY/MAC) pour effectuer des applications de localisation, tels que la capture de mouvement et la navigation de groupe. Pour cela, nous exploitons les avantages du WBAN avec différentes topologies et différents types de liens: on-body à l'échelle du corps, body-to-body entre les utilisateurs et off-body par rapport à l'infrastructure. La transmission repose sur une radio impulsionnelle (IR-UWB), afin d'obtenir des mesures de distance précises, basées sur l’estimation du temps d'arrivée (TOA). Ainsi, on s’intéresse au problème du positionnement à travers de la conception de stratégies coopératives et en considérant la mobilité du corps et les variations canal. Notre première contribution consiste en la création d'une base de données obtenue avec de scénarios réalistes pour la modélisation de la mobilité et du canal. Ensuite, nous introduisons un simulateur capable d'exploiter nos mesures pour la conception de protocoles. Grâce à ces outils, nous étudions d’abord l'impact de la mobilité et des variations de canal sur l'estimation de la distance avec le protocole "three way-ranging" (3 -WR). Ainsi, nous quantifions et comparons l'erreur avec des modèles statistiques. Dans un second temps, nous analysons différentes algorithmes de gestion de ressources pour réduire l'impact de la mobilité sur l'estimation de position. Ensuite, nous proposons une optimisation avec <b>un</b> <b>filtre</b> de Kalman étendu (EKF) pour réduire l'erreur. Enfin, nous proposons un algorithme coopératif basé sur l'analyse d’estimateurs de qualité de lien (LQEs) pour améliorer la fiabilité. Pour cela, nous évaluons le taux de succès de positionnement en utilisant trois modèles de canaux (empirique, simulé et expérimental) avec un algorithme (basé sur la théorie des jeux) pour le choix des ancres virtuelles. Nous montrons que notre algorithme permet d’améliorer le nombre de positions estimées pour les noeuds ayant la pire performance de localisation...|$|E
40|$|LHCb {{is one of}} {{the four}} large {{experiments}} of the Large Hadron Collider (LHC) based at CERN. The LHCb experiment is taking data at its nominal design luminosity. However, in order to distinguish among models of new physics, higher luminosity is needed. So the LHCb collaboration intends to upgrade the detector during the planned long LHC shutdown in 2019 {{in order to be able}} to operate at a luminosity about 10 times the nominal one. The objective the present thesis is the development of an integrated solution for the analog signal processing in the electronic and hadronic calorimeters of the LHCb. The analogue signal processing will be performed by a shaper ASIC in the calorimeters Front End (FE) boards. The signal pulse from a photomultiplier tube is clipped at its base and, then, is transmitted through a 12 m 50 Ω coaxial cable to the FE board located in the crates at the calorimeter platform. In order to reduce the PMT ageing after the increase in luminosity, the gain has to be decreased by a factor 5 with respect to the present operation in order to keep the same average current. Therefore, the preamplifier input noise must be decreased accordingly so that the total input referred noise voltage is smaller than 1 nV/√Hz. Consequently, a 50 Ω termination resistor is not acceptable. The main requirements for the analogue FE of the calorimeter system include a cali- bration of 4 fC/ 2. 5 MeV per ADC count; a dynamic range of 12 bits; noise lower than 1 ADC cnt (ENC < 4 fC); and a spill-over residue level ± 1 % a linearity: < 1 %. The implementation of the ASIC includes: four analog channels with programmable values to control the key parameters and compensate for process variations; a dedicated Delay Locked Loop (DLL) to synchronize each channel signal phase and a digital interface using SPI protocol. The analog channel is designed with an input amplifier that includes an electronically cooled termination input stage. A passive line termination would induce too large a noise and is avoided. Afterwards an alternated switched differential signal paths scheme permits the integration of the signal with no dead time between consecutive events. Each path includes a pole-zero filter in order to compensate for cable effects, a switched integrator with capacitive feedback, a Track-and-Hold for a 12 -bit ADC and a MUX to select the correct sub-channel output signal. A fully differential signal processing is adopted in order to minimize the impact of common mode noise, which is important in a switched system. Each analog channel includes a delay line based on a DLL so the user can set a delay to compensate the delay introduced by PMT voltage settings, cable lengths or particle time of flight from the interaction point to the calorimeter cells. The DLL is adjusted by means of two control voltages to ensure that systematic process or environmental variations will not affect the channel time tuning. The radiation hardness expected from the selected technology (0. 35 µm AMS SiGe BiCMOS) is enough, but design techniques are used to ensure being able to tolerate SEUs, SETs and SELs. The design has been checked at different tests of a total of 30 pieces of the final pro- totype: at the laboratory using a signal obtained with a scope, with electron beams and ECAL channels in a dedicated facility at CERN, and its radiation hardness at Centre de Resources du Cyclotron at Louvain la Neuve. Dedicated boards were developed and the results are positive. L’LHCb és un dels quatre grans experiments del Gran Col·lisionador d’Hadrons al CERN. En aquest moment, l’experiment LHCb ja ha profunditzat en el seu programa de presa de dades, però, per tal de realitzar estudis destinats a distingir entre models de nova física es pretén actualitzar el detector el 2019 per tal d’operar amb una lluminositat uns deu cops superior a l’actual. L’objectiu de la present Tesi és el desenvolupament d’una solució integrada pel processat del senyal analògic en la actualització i millora de l’electrònica del Calorímetre, sub-detector de l’LHCb. Aquest processat es durà a terme a les plaques d’electrònica front-end (FE) mitjançant un ASIC que rep polsos que venen de tubs fotomultiplicadors (PM) a través de 12 m de cable coaxial de 50 Ohms. Els principals requeriments del FE analògic del Calorímetre inclouen una calibració de 4. 5 fc/ 2. 5 MeV per compte d’ADC, un rang dinàmic de 12 bits, un soroll menor a 1 compte d’ADC, un nivell de cua residual menor que 1 % i una desviació de la linearitat menor que 1 %. La implementació de l’ASIC inclou quatre canals analògics amb valors programables, una línia de retard (DLL) i una interfase digital utilitzant protocol SPI. El canal analògic disposa d’un amplificador d’entrada que inclou una terminació de línia activa. Desprès, un sistema de dos subcanals diferencials commutats permet la integració del senyal sense temps morts entre esdeveniments consecutius. Cada subcanal inclou <b>un</b> <b>filtre</b> de pol-zero, un integrador commutat, un Track-and-Hold i un multiplexor. Finalment, la DLL proporciona els senyals de rellotge per sincronitzar els canals. La pròpia resistència a la radiació que assegura el fet de fer servir la tecnologia escollida (AMS 0. 35 um BICMOS) i l’ús de tècniques específiques permet una tolerància del circuit integrat a SEUs, SETs i SELs, als nivells esperats al detector. El disseny ha estat verificat en diferents proves per un total de 30 prototips de la versió final del xip: primer, al laboratori, utilitzant un senyal obtingut amb un oscil·loscopi al detector; amb feixos d’electrons i canals del detector, a zones dedicades al CERN; i, la resistència a la radiació s’ha verificat al CRC a Louvain la Neuve, Bèlgica...|$|E
40|$|This is a {{contribution}} to combinatorial set theory, specifically to infinite Ramsey Theory, which deals with partitions of infinite sets. The basic pigeon hole principle states that for every partition of the set of all natural numbers in finitely many classes there is an infinite set of natural numbers that is included in some one class. Ramsey’s Theorem, which {{can be seen as}} a generalization of this simple result, is about partitions of the set [N]k of all k-element sets of natural numbers. It states that for every k ≥ 1 and every partition of [N]k into finitely many classes, there is an infinite subset M of N such that all k-element subsets of M belong to some same class. Such a set is said to be homogeneous for the partition. In Ramsey’s own formulation (Ramsey, [8], p. 264), the theorem reads as follows. Theorem (Ramsey). Let Γ be an infinite class, and μ and r positive numbers; and let all those sub-classes of Γ which have exactly r numbers, or, as we may say, let all r−combinations of the members of Γ be divided in any manner into μ mutually exclusive classes Ci (i = 1, 2,..., μ), so that every r−combination is a member of one and only one Ci; then assuming the axiom of selections, Γ must contain an infinite sub-class △ such that all the r−combinations of the members of △ belong to the same Ci. In [5], Neil Hindman proved a Ramsey-like result that was conjectured by Graham and Rotschild in [3]. Hindman’s Theorem asserts that if the set of all natural numbers is divided into two classes, one of the classes contains an infinite set such that all finite sums of distinct members of the set remain in the same class. Hindman’s original proof was greatly simplified, though the same basic ideas were used, by James Baumgartner in [1]. We will give new proofs of these two theorems which rely on forcing arguments. After this, we will be concerned with the particular partial orders used in each case, with the aim of studying its basic properties and its relations to other similar forcing notions. The partial order used to get Ramsey’s Theorem will be seen to be equivalent to Mathias forcing. The analysis of the partial order arising in the proof of Hindmans Theorem, which we denote by PFIN, will be object of the last chapter of the thesis. A summary of our work follows. In the first chapter we give some basic definitions and state several known theorems that we will need. We explain the set theoretic notation used and we describe some forcing notions that will be useful in the sequel. Our notation is generally standard, and when it is not it will be sufficiently explained. This work is meant to be self-contained. Thus, although most of the theorems recorded in this first, preliminary chapter, will be stated without proof, it will be duly indicated where a proof can be found. Chapter 2 is devoted to a proof of Ramsey’s Theorem in which forcing is used to produce a homogeneous set for the relevant partition. The partial order involved is isomorphic to Mathias forcing. In Chapter 3 we modify Baumgartner’s proof of Hindman’s Theorem to define a partial order, denoted by PC, from which we get by a forcing argument a suitable homogeneous set. Here C is an infinite set of finite subsets of N, and PC adds an infinite block sequence of finite subsets of natural numbers with the property that all finite unions of its elements belong to C. Our proof follows closely Baumgartner’s. The partial order PC is similar both to the one due to Matet in [6] and to Mathias forcing. This prompts the question whether it is equivalent to one of them or to none, which can only be solved by studying PC, which we do in chapter 4. In chapter 4 we first show that the forcing notion PC is equivalent to a more manageable partial order, which we denote by PFIN. From a PFIN- generic filter an infinite block sequence can be defined, from which, in turn, the generic filter can be reconstructed, roughly as a Mathias generic filter can be reconstructed from a Mathias real. In section 4. 1 we prove that PFIN is not equivalent to Matet forcing. This we do by showing that PFIN adds a dominating real, thus also a splitting real (see [4]). But Blass proved that Matet forcing preserves p-point ultrafilters in [2], from which follows that Matet forcing does not add splitting reals. Still in section 4. 1 we prove that PFIN adds a Mathias real by using Mathias characterization of a Mathias real in [7] according to which x ⊆ ω is a Mathias real over V iff x diagonalizes every maximal almost disjoint family in V. In fact, we prove that if D = (Di) i∈ω is the generic block sequence of finite sets of natural numbers added by forcing with PFIN, then both {minDi : i ∈ ω} and {maxDi : i ∈ ω} are Mathias reals. In section 4. 2 we prove that PFIN is equivalent to a two-step iteration of a σ-closed and a σ-centered forcing notions. In section 4. 3 we prove that PFIN satisfies Axiom A and in section 4. 4 that, as Mathias forcing, it has the pure decision property. In section 4. 5 we prove that PFIN does not add Cohen reals. So far, all the properties we have found of PFIN are also shared by Mathias forcing. The question remains, then, whether PFIN is equivalent to Mathias forcing. This we solve by first showing in section 5. 1 that PFIN adds a Matet real and then, in section 5. 2, that Mathias forcing does not add a Matet real, thus concluding that PFIN and Mathias forcing are not equivalent forcing notions. In the last, 5. 3, section we explore another forcing notion, denoted by M 2, which was introduced by Shelah in [9]. It is a kind of “product” of two copies of Mathias forcing, which we relate to denoted by M 2. Bibliography [1] J. E. Baumgartner. A short proof of Hindmanʼs theorem. Journal of Combinatorial Theory, 17 : 384 – 386, 1974. [2] A. Blass. Applications of superperfect forcing and its relatives. In Set theory and its applications. Lecture notes in Mathematics. Springer, Berlin., 1989. [3] R. L. Graham and B. L. Rothschild. Ramseyʼs theorem for n-parameter sets. Transaction American Mathematical Society, 159 : 257 – 292, 1971. [4] L. Halbeisen. A playful approach to Silver and Mathias forcing. Studies in Logic (London), 11 : 123142, 2007. [5] N. Hindman. Finite sums from sequences within cells of partition of N. Journal of Combinatorial Theory (A), 17 : 1 – 11, 1974. [6] P. Matet. Some filters of partitions. The Journal of Symbolic Logic, 53 : 540 – 553, 1988. [7] A. R. D. Mathias. Happy families. Annals of Mathematical logic, 12 : 59 – 111, 1977. [8] F. P. Ramsey. On a problem of formal logic. London Mathematical Society, 30 : 264 – 286, 1930. [9] S. Shelah and O. Spinas. The distributivity numbers of finite products of P(ω) /fin. Fundamenta Mathematicae, 158 : 81 – 93, 1998. Aquesta tesi és una contribució a la teoria combinatria de conjunts, específcament a la teoria de Ramsey, que estudia les particions de conjunts infinits. El principi combinatori bàsic diu que per a tota partició del conjunt dels nombres naturals en un nombre finit de classes hi ha un conjunt infinit de nombres naturals que està inclòs en una de les classes. El teorema de Ramsey [6], que hom pot veure com una generalització d'aquest principi bàsic, tracta de les particions del conjunt [N]k de tots els subconjunts de k elements de nombres naturals. Afirma que, per a cada k >/= 1 i cada partició de [N]k en un nombre finit de classes, existeix un subconjunt infinit de nombres naturals, M, tal que tots els subconjunts de k elements de M pertanyen a una mateixa classe. Els conjunts amb aquesta propietat són homogenis per a la partició. En [3], Neil Hindman va demostrar un resultat de tipus Ramsey que Graham i Rotschild havien conjecturat en [2]. El teorema de Hindman afirma que si el conjunt de nombres naturals es divideix en dues classes, almenys una d'aquestes classes conté un conjunt infinit tal que totes les sumes finites d'elements distints del conjunt pertanyen a la mateixa classe. La demostració original del Teorema de Hindman va ser simplificada per James Baumgartner en [1]. En aquesta tesi donem noves demostracions d'aquests dos teoremes, basades en la tècnica del forcing. Després, analitzem els ordres parcials corresponents i n'estudiem les propietats i la relació amb altres ordres coneguts semblants. L'ordre parcial emprat en la demostració del teorema de Ramsey és equivalent al forcing de Mathias, definit en [5]. L'ordre parcial que apareix en la prova del teorema de Hindman, que anomenem PFIN, serà l'objecte d'estudi principal de la tesi. En el primer capítol donem algunes definicions bàsiques i enunciem alguns teoremes coneguts que necessitarem més endavant. El segon capítol conté la demostració del teorema de Ramsey. Usant la tècnica del forcing, produïm un conjunt homogeni per a una partició donada. L'ordre parcial que utilitzem és equivalent al de Mathias. En el tercer capítol, modifiquem la demostració de Baumgartner del teorema de Hindman per definir un ordre parcial, que anomenem PC, a partir del qual, mitjançant arguments de forcing, obtenim el conjunt homogeni buscat. Aquí, C es un conjunt infinit de conjunts finits disjunts de nombres naturals, i PC afegeix una successió de conjunts finits de nombres naturals amb la propietat de que totes les unions finites de elements d'aquesta successió pertanyen al conjunt C. A partir d'aquesta successió és fàcil obtenir un conjunt homogeni per a la partició del teorema original de Hindman. L'ordre parcial PC és similar a l'ordre definit per Pierre Matet en [4] i també al forcing de Mathias. Per això, és natural preguntar-nos si aquests ordres són equivalents o no. En el quart capítol treballem amb un ordre parcial que és equivalent a PC i que anomenem PFIN. Mostrem que PFIN té les propietats següents: (1) A partir d'un filtre genèric per a PFIN obtenim una successió infinita de conjunts finits de nombres naturals. Com en el cas del real de Mathias, aquesta successi_o ens permet reconstruir tot el filtre genèric. (2) PFIN afegeix un real de Mathias, que és un "dominating real". Ara bé, si afegim un "dominating real" afegim també un "splitting real". Aquest fet ens permet concloure que PFIN no és equivalent al forcing de Matet, ja que el forcing de Matet no afegeix "splitting reals" (3) PFIN es pot veure com una iteració de dos ordres parcials, el primer dels quals és "sigma-closed" i el segon és "sigma-centered". (4) PFIN té la "pure decision property". (5) PFIN no afegeix reals de Cohen. En el cinquè capítol demostrem que PFIN afegeix un real de Matet i, finalment, que el forcing de Mathias no afegeix reals de Matet. Això és com demostrem que el forcing de Mathias i PFIN no són ordres equivalents. Al final del capítol donem una aplicació de PFIN. Demostrem que un cert ordre definit per Saharon Shelah en [7], que anomenem M 2, és una projecció de PFIN. Això implica que si G és <b>un</b> <b>filtre</b> PFIN-genèric sobre V, l'extensió V [G] conté també <b>un</b> <b>filtre</b> genèric per a M 2. L'ordre M 2 és una mena de producte de dues cópies del forcing de Mathias. REFERÈNCIES [1] J. E. Baumgartner. A short proof of Hindman's theorem, Journal of Combinatorial Theory, 17 : 384 - 386, (1974). [2] R. L. Graham and B. L. Rothschild. Ramsey's theorem for m-parameter sets, Transaction American Mathematical Society, 159 : 257 - 292, (1971). [3] N. Hindman. Finite sums from sequences within cells of partitions of N, Journal of Combinatorial Theory (A), 17 : 1 - 11, (1974). [4] P. Matet. Some _lters of partitions, The Journal of Symbolic Logic, 53 : 540 - 553, (1988). [5] A. R. D. Mathias. Happy families, Annals of Mathematical Logic, 12 : 59 - 111, (1977). [6] F. P. Ramsey. On a problem of formal logic, London Mathematical Society, 30 : 264 _D 286, 1930. [7] S. Shelah and O. Spinas. The distributivity numbers of finite products of P(!) =fin, Fundamenta Mathematicae, 158 : 81 _D 93, 1998...|$|E
40|$|L'adsorption en phase liquide sur charbon actif est un sujet très travaillé sur le plan expérimental et de plus en plus dans le domaine de la modélisation. Les tentatives de {{description}} des courbes de percée ou de fuite des filtres montrant la saturation du matériau adsorbant remontent aux travaux de BOHART et ADAMS en 1920. D'autres équations avec d'autres approximations ont été proposées {{par la suite}} (THOMAS 1944; DOLE et KLOLZ 1946), HUTCHINS (1973); plus récemment, WOLBORSKA (1989) ou CLARK (1987) ont proposé d'autres modèles. Nous avons essayé de faire le point sur ces différents modèles, de montrer leurs origines communes, souvent à partir des équations de BOHART et ADAMS, les approximations apportées limitant leur domaine d'application, les grandeurs qu'ils permettent de déterminer : capacité maximum d'adsorption, constante cinétique d'adsorption, vitesse de déplacement du front d'adsorption. De tous ces modèles, un seul (CLARK 1987) permet une bonne représentation des courbes de percée. Nous en avons proposé une linéarisaüon qui facilite la détermination des paramètres nécessaires au calcul des courbes de fuite. Tous ces modèles ont été testés sur les résultats expérimentaux obtenus pour l'adsorption d'un tensioactif anionique : le décylsulfonate de sodium et ceci sur cinq petites colonnes de hauteurs différentes de charbon actif. Le modèle de CLARK a également été appliqué à des résultats obtenus au laboratoire (El HANI, 1987) sur l'adsorption et la dégradation biologique des acides humiques sur <b>un</b> <b>filtre</b> de charbon de 1 m de haut, sur une période beaucoup plus longue (1 mois) et avec des lavages du filtre. Ce modèle {{permet de}} calculer la part qui n'est pas simplement de l'adsorption rapide (dégradation biologique et adsorption lente). Low concentrations of organic contaminants are not easily removed by conventional treatment methods, but activated carbon bas a good affinity for various organics and is used in batch or column reactors. Much has been written concerning the prediction {{of the performance of}} powdered activated carbon (PAC); adsorptive capacity and equilibrium isotherms determined in « batch » reactor are proposed to simulate the performance of PAC for single or bisolute systems (DUSART et al. 1990, SMITH 1991). Some investigators have attempted to simulate column performance with mathematical models and the aim of this work is to present the principal models and verify how the different models are applied to break-through curves; parameters which can be evaluated by the different equations will also be compared. As early as 1920 BOHART and ADAMS presented differential equations which govern the dynamics of the adsorption of vapours and gases on fixed beds and the final result, applied to the liquid-solid phase, yields the kinetic adsorption rate (k) and the maximum adsorption capacity (No) (eq. 3). By transposition to the liquid phase, we have calculated the concentration distribution in the bed (eq. 5) by using the kinetic constant k and the maximum adsorption capacity No obtained by equation 4; it was noted that only the low concentration range of the break-through curve can be used. Some approximations from DOLE and KLOTZ (1946) lead to the « Bed Depth/Service-Time (BDST) equation 7 proposed by HUTCHINS (1973); the service time of a column tb has a linear relationship with the bed depth Z (fig. 3). The activated carbon efficiency No can be estimated and the adsorption rate constant calculated from the slope and the y-intercept. Recently, WOLBORSKA (1989) proposed a rectilinear equation InC/Co = At + B (eq. 10) for the initial segment of the break-through curve. The form of this equation is similar to equation (4) obtained tram the BOHART-ADAMS hypothesis. The mass transfer coefficient, ßa, the maximum adsorption capacity and the migration velocity v (eq. 9) of the concentration fronts can be calculated from the constants A and B. The model developed by CLARK (1987) is based on the use of e mass-transfer concept in combination with the Freundlich isotherm (fig. 4). The originality of this modal, in comparison to the others, consists in the existence of the equilibrium concentration Ce and the driving force equilibrium « C-Ce ». The general equation is equation (14). Two parameters A and r are determined by regression equations; we proposed a simple method to calculate A and r by a linearization of the preceding equation (eq. 14). This is equation (16) In [(Co/C) n- 1 - 1] = In A -rt. Sodium decanesulfonate at a concentration of 20 mg ·l- 1 was used as influent and activated powdered carton (200 ≤ ø ≤ 315 µm) as the fixed bed adsorbent layer to illustrate the comparison between the different models. The linear flow rates were 3. 0 m. h- 1 and the five columns tested were 3. 1; 4. 0; 7. 5; 10. 2; 12. 5 cm high with a 1. 45 cm 2 cross section. The Freundlich isotherm equation (fig. 4) obtained in a batch system for an equilibrium time (t = 24 h for this activated carbon) gives a « n value » equal to 2. 38. Figure 2 presents the experimental break-through curves obtained for the different bed heights; by using equations (4 or 10) in the system (In C/Co, t) they are represented on the same figure by the dotted line. The agreement is only for the low values of C in the break-through curves. The coefficients A and B (table 1) are determined from the straight lines obtained with the low break-curve concentrations (fig. 1). The kinetic coefficient Sa, and the maximum capacity adsorption No are shown in table 1. The No value is similar to those obtained from the other equations. The migration velocity of the concentration fronts (r = 0, 133 cm · h- 1) is in good agreement with the experimental value (0, 128 cm · h- 1). The linearized Clark equation (16) gives a good representation of experimental results (fig. 6) alter the determination of A and r parameters (fig. 5 and table 2). With the use of the two parameters, the break-through curves have been recalculated (fig. 6) and compared to experimental results. Their is good agreement. The A parameter is related to the depth Z of the adsorbant : A = Bez ·. B value can be determined with the different columns (fig. 7). The Clark model can be applied to filers which have a biological activity; the results obtained in the laboratory by EL HANI (1987) for the adsorption of humic acids (10 mg · l- 1) on a 1 m granular activated carton bed were analyzed by the Clark equation (fig. 8). The initial concentrations of humic acids are never obtained in the effluent because of biological degradation and/or slow adsorption in mesopores. From the difference {{in the area of the}} two curves, it is possible to calculate the supplementary biological degradation. For 95 cm of activated carbon in the column and after 800 h, the biological degradation represents 55 % of the total elimination. The percentage is constant alter 35 cm depth of the activated carton in agreement with the electon microscopy study that showed that the flora was only present in the 10 first centimeters. The use of this model is facilitated by our linearization and the case of particular phenomena : biological degradation or desorption. in the case of successive muld adsorbates fixation (REYDEMANEUF et al. to be published) can be studied and compared to the only adsorption phenomena. In conclusion, nome of the tested models lead to different parameters by using low break-through curve concentrations or others with the whole range of experimental points, but only one (CLARK) gives a good description of the break-through curves in our actual knowledge...|$|E

