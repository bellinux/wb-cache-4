21|6|Public
5000|$|... 1915 : La Manœuvre amoureuse or <b>Une</b> <b>erreur</b> by Labruyère (short subject) ...|$|E
5000|$|William Matthey-Claudet. [...] " [...] <b>Une</b> <b>erreur</b> judicière? L' Affaire Popesco" [...] Imprimerie Montandon & Cie, Fleurier (Ntel).|$|E
5000|$|It lacks Matthew 16:2b-3. The {{text of the}} Pericope Adulterae (John 7:53-8:11) {{is marked}} as a doubtful. It has note at the margin: [...] "mais c'est <b>une</b> <b>erreur.</b> None avone verifie le passage avec soin et cette note n'y existe nulle part".|$|E
50|$|<b>Une</b> Comédie des <b>Erreurs,</b> 1943-1956 souvenirs et réflexions sur une étape de la {{construction}} européenne, Paris: Plon, 1978.|$|R
40|$|This talk is {{concerned}} with the following contrast, which illustrates a situation where the speaker has to say the same thing twice, or, in other words, has to be redundant. (1) a. Jean a fait <b>une</b> grosse <b>erreur.</b> Il ne la fera plus. John made a big mistake. He won't do it any more. b. # Jean a fait <b>une</b> grosse <b>erreur.</b> Il ne la fera pas. John made a big mistake. He won't do it. It is well-known that ne [...] . pas and ne [...] . plus only differ in presuppositional contents: contrarily to ne … pas which bears no presupposition, ne [...] . plus triggers a presupposition (that something was the case). They both share the same asserted content, namely the negation of what's in their scope. So the surprising fact here is that the speaker has to use the presuppositional negation, instead of the simple negation, even though the presuppositional content is not new, but already asserted in the discourse. In other terms, the speaker has to repeat some information: once it is asserted, and then it is presupposed. What we want to do in this talk is firstly to explain when (§ 1) and why (§ 2) a presupposition trigger becomes obligatory in the very case where it is redundant, and secondly to suggest tha...|$|R
40|$|The aim of {{this thesis}} is the multifractal {{analysis}} of selfsimilar functions {{and the study of}} the validity of the multifractal formalism. First, we determine the exact pointwise Hölder regularity for functions such that locally the graph is roughly a contraction of the global graph, modulo an error; then we compute the Hausdorff dimensions of the sets of points which have the same Hölder exponent; and finally we verify the conjectures of Frish and Parisi and the one of Arneodo, Bacry and Muzy, which relate these dimensions to some averaged quantities extracted from the function. We study different types of selfsimilarities, and prove (by reformuling some times) that the wavelet analysis is a good tool to study the validity of these relations. L'objet de cette thèse est l'analyse multifractale des fonctions autosimilaires et l'étude de la validité du formalisme multifractal. Il s'agit d'abord de déterminer la régularité Hölderienne ponctuelle exacte pour des fonctions dont le graphe localement est grossièrement une contraction du graphe complet, à <b>une</b> fonction <b>erreur</b> près; ensuite de calculer les dimensions de Hausdorff des ensembles de points où la fonction présente la même singularité; et enfin de vérifier les conjectures de Frish et Parisi et celle d'Arneodo, Bacry et Muzy, qui relient ces dimensions à des quantités moyennes extraites de la fonction. Nous étudions plusieurs types d'autosimilarités, et montrons (en reformulant parfois) que l'analyse par ondelettes permet d'étudier la validité de ces relations...|$|R
5000|$|... "Les ingénieurs ont de fréquentes {{occasions}} d'employer le fer en grandes parties, et c'est sur cette matière que plus d'un fonde l'espoir d'une architecture nouvelle. Je lui dis tout de suite, c'est là <b>une</b> <b>erreur.</b> Le fer est un moyen, ce ne sera jamais un principe." ...|$|E
5000|$|It also {{provides}} an allegorical journey after death, {{going from the}} expectation of heaven to shock at being sent to hell. This divides the song into two parts as the [...] "heaven" [...] section fades away and then the song begins again with [...] "Non! Pourquoi moi? C'est <b>une</b> <b>erreur!</b> Garde-moi, je suis noble de cœur!" [...] (No! Why me? It's a mistake! Keep me, I am noble at heart) as he is sent to hell.|$|E
5000|$|C'etait <b>une</b> <b>erreur</b> cet album, une connerie. Sur un plan personnel, j'avais plein de galères et je me suis amusé à jouer avec des samples, comme une récréation. Et on l'a sorti comme album. Mais quand je le réécoute aujourd'hui... Je l'assume, hein, il faut bien assumer ce qu'on fait, mais je ne le referais pas. (This {{album was}} a mistake, a dumb thing to do. I {{had a lot}} of {{personal}} problems at the time, and I kept myself entertained playing with these samples. And then they were released as an album. But when I listen to the album today... well, one has to stand behind one's own work, I guess, but I really wouldn't do it again.) Jean-Michel Jarre ...|$|E
40|$|PROSPECT, a widely-used leaf directional-hemispherical {{reflectance}} and transmittance model, currently {{treats the}} behaviour of chlorophylls a, b, carotenoids, and anthocyanins uniformly, leading to errors. A finer discrimination among these pigments in lightabsorbing plant tissues should improve the model. In this paper, a new calibration and validation phase of PROSPECT {{is performed using}} two comprehensive databases containing hundreds of leaves collected in temperate latitudes in Angers, France, and in a tropical environment in Hawaii, USA. Leaf biochemical (chlorophylls a, b, carotenoids, water, and dry matter) and optical properties (directional-hemispherical reflectance and transmittance measured from 400 nm to 2500 nm) were measured and used in the model development steps. The first step consists in providing distinct in vivo specific absorption coefficients for the leaf pigments using the Angers database. The model is then inverted to predict the biochemical content of intact leaves from both data sets. The main result of this preliminary study {{is that the new}} chlorophyll and carotenoid specific absorption coefficients are in good agreement with available in vitro absorption spectra, that the chlorophyll predictions are improved, and that the carotenoids are reasonably retrieved. RÉSUMÉ: PROSPECT, le modèle de réflectance et de transmittance directionnelle-hémisphérique des feuilles aujourd'hui le plus utilisé par la communauté scientifique, suppose que l'absorption de la lumière par les chlorophylles a, b, caroténoïdes, et anthocyanes est uniquement due aux chlorophylles, ce qui conduit à des <b>erreurs.</b> <b>Une</b> discrimination plus fine de ces pigments dans les tissu...|$|R
50|$|In 1944, de Gaulle {{decided that}} the Anglophile Massigli was too pro-British for his liking and demoted him to Ambassador to London. From August 1944 until June 1954, Massigli was the French Ambassador to the Court of St. James's. In August 1944, Massigli lobbied Winston Churchill for {{allowing}} a greater French role {{in the war in}} the Far East as the best way of ensuring that French Indochina stay French after World War II had ended. In November 1944, when Churchill visited Paris, he presented to de Gaulle his offer for an Anglo-French pact, which Massigli urged the General to accept, but which de Gaulle refused. During his time in Britain, Massigli was very much involved in the debates about the Cold War and European integration. Massigli was strongly opposed to the vision of European federation of Jean Monnet. Instead he urged the creation of an Anglo-French bloc which would {{serve as the basis for}} a federation of Europe. In 1954-1956, Massigli served as the Secretary-General of the Quai d'Orsay. As Secretary-General, Massigli played a major role behind the scenes in resolving the 1954 crisis in trans-Atlantic relations caused by the rejection by the French National Assembly of the European Defence Community treaty. In 1956, Massigil retired. His memoirs, <b>Une</b> Comédie des <b>Erreurs,</b> were published in 1978. He died in Paris on 3 February 1988 a month short of his 100th birthday.|$|R
40|$|International audienceTo be interpreted, prehistoric imagery must be {{evaluated}} in its archaeological and cultural contexts. The body of material data of which images and effigies {{are an integral}} part is particularly important in order to avoid anachronisms and ’anatopisms’ (confusion of geographical contexts). Based on contextualization at different scales, this article proposes a reinterpretation of two frequent motifs characterizing the third millennium BCE statue-menhirs (or ’stelae’, i. e. standing stones) from the Central Alps: the circular figures and the so-called ‘bandoliera’ figures. During the third millennium BCE, coinciding with Copper Age 2 and 3, ceremonial sites with groups of statue-menhirs appeared in the Central Alps alongside similar developments in other European regions. These Central Alpine monoliths from Valcamonica can be called the Camunnian group, and their rich imagery makes them outstanding. The typology of several artefacts allows this group’s evolution to be divided into three chronological phases (the usual prefix, III, will be omitted) : A 1 (2900 - 2500 / 2400 BCE), A 2 (2500 / 2400 - 2200 BCE) and A 3 (2200 - 2000 BCE). Only the A 1 imagery — the most problematic — will be considered in this article. At least two subsets of statue-menhirs can be recognized from their engraved images as well as the shape and size of the stone blocks employed. Set 1 bears images of daggers, ’halberds’, axes, belts, animals, and sometimes ploughing. Set 2 is only engraved with body ornaments. This dualistic pattern is also present in the neighbouring regions. Particularly in the Trentino-South Tyrol to the east, monoliths with ornaments also display sculpted breasts, suggesting the existence of a gendered distinction between male and female monoliths. The circle and ‘bandoliera’ images discussed in this article are typical of male iconography. The Camunnian group shows several distinctive features compared with statue-menhirs of neighbouring regions. The erect boulders have an abstract human shape suggesting a representation of individuals. However, at the same time, anthropomorphism is unclear or absent from the actual imagery. It follows that engraved images, not being directly connected with recognizable body parts, are often difficult to interpret. That is the case with both the motifs we are dealing with in this article, which appear very abstract to present-day observers. The circular figures are engraved at the top of male statue-menhirs and, until now, have been interpreted as representations of the sun. This intuitive idea rests on the presupposition that the same set of formal attributes — a circle with or without rays — should have a universal meaning, independent of its cultural context. However, various lines of evidence from both the imagery itself and the archaeological context of the Central Alpine Copper Age suggest a different interpretation. First, there is the large morphological variability of the circular figures, which is inconsistent with a conventional, socially accepted representation of the sun. Secondly, these figures are circles, not discs, i. e. the reverse of the most widespread norm for representing the sun in known ethnographic contexts. Moreover, both a reappraisal of the social and ideological context characterizing the third millennium BCE Central Alps, and a comparison of the Camunnian monoliths with those of the neighbouring regions, strongly indicate that despite their relative abstraction in the Central Alps the statue-menhirs are effigies of individuals. This leads to a third objection: the so-called ‘sun disc’ is absent from all other Alpine regions, although sharing the same broad ideology. In these other regions, it is figurations of necklaces that appear at the top of statue-menhirs. Finally, dealing with an ideology where manipulation of funerary ‘relics’ is attested at the same ceremonial sites including erect stone bodies, any interpretation of statue-menhirs demands that attention be paid to the funerary context. In the Copper Age Central Alps this context can be equated with the so-called Civate funerary tradition, in which what we may call the ’culture of appearance’ clearly possessed an important social value. Body ornaments, particularly elements from necklaces, indeed predominate among the objects deposited with the dead, characterizing the Civate sites. When all the above lines of evidence are brought together, the Camunnian circular figures emerge as elements of the material culture of the groups involved, and more specifically necklaces. The second ambiguous image examined, the ‘bandoliera’ figure, is systematically engraved on two contiguous faces of the statue-menhir. In spite of this, it has been commonly interpreted as the representation in plan of a megalithic enclosure, based on comparison with dwelling or ceremonial sites far away from the Central Alps (including southern France, Portugal, or even Stonehenge). This motif is considered to be part of so-called ‘topographic’ imagery. However, such an interpretation is problematic on methodological grounds, being divorced from both its medium (a stone effigy of an individual) and its cultural context. No megalithic enclosures have so far been found in the Central Alps. Depending on the same body of data summarised above in connection with the circular figures, we propose to interpret the ‘bandoliera’ motif as a male body ornament — or part of equipment — worn on the shoulder. Contextualization of imagery enables a more robust body of evidence to be built in order to identify and interpret the motifs. In Alpine cases, notably from the Copper Age of Valcamonica, the interpretation of engraved imagery — and especially of ambiguous figures — needs {{to take into account the}} ontology of the erected monoliths and the cultural and archaeological context. There is always a risk, in the archaeology of ideological phenomena, of producing discourses or theories disconnected from the archaeological contextual base. We must not ascribe to the monoliths a meaning patently unsupported by the material evidence, be it universal sun images or megalithic plans. Contextualization on different scales is essential for an ’archaeology of images’ and is equally valuable to avoid misinterpretations. This paper has allowed new interpretations to be proposed for the Camunnian circular and ‘bandoliera’ motifs: images that, accordingly, belong in the social enhancement of bodily symbolic display — or the ‘culture of appearance’ — of the Copper Age groups involved. Pour être interprétées, les iconographies préhistoriques doivent être réinsérées dans leur contexte archéologique et culturel. À partir d’une contextualisation à différentes échelles, le présent article propose une relecture de deux images gravées sur les statues-menhirs centre-alpines du IIIe millénaire av. n. è., à savoir les figures circulaires et « a bandoliera ». Les premières sont gravées au sommet des statues-menhirs masculines et ont jusqu’alors été interprétées comme des figurations du soleil. Cette interprétation se base sur le présupposé selon lequel un même ensemble de caractéristiques formelles, un cercle radié notamment, possède une signification universelle, c'est-à-dire indépendante de son contexte de création. Or, plusieurs indices issus à la fois de l’iconographie elle-même et du contexte archéologique des Alpes centrales permettent d’envisager une lecture différente. La première objection que nous opposons à l'interprétation couramment admise est l'extrême variabilité morphologique de ces figures circulaires qui s'accorde mal avec une représentation conventionnelle du soleil. De plus, il s’agit bien de cercles et non de disques au contraire de ce qu’une représentation du soleil requiert en divers contextes culturels. Plus encore, la comparaison de ce que nous interprétons comme des individus de pierre, à partir notamment du contexte social et idéologique qui caractérise les Alpes centrales du IIIe millénaire, et en dépit de leur relative abstraction, aux autres corpus de statues-menhirs alpines suggère une seconde objection. Ces prétendues figures solaires sont absentes des autres secteurs géographiques alpins qui connaissent pourtant un phénomène idéologique semblable aux Alpes centrales. Le sommet des statues-menhirs accueille dans ces régions des figurations de colliers. Finalement, s'agissant de corps de pierre érigés sur des sites cérémoniels où des manipulations d’ossements humains sont attestées, l’interprétation des statues-menhirs requiert de prêter attention aux données funéraires qui permettent alors d’envisager une valorisation sociale importante des cultures de l’apparence dans les groupes chalcolithiques centre-alpins. En effet, sur les sites de tradition dite Civate les ornements corporels, et plus encore les éléments de colliers, prédominent parmi les objets déposés avec les défunts et en constituent la caractéristique essentielle. La contextualisation de l’iconographie permet ainsi la constitution d’un solide faisceau de données en faveur d'une relecture de ces figures. Nous proposons de reconnaître dans les cercles gravés au sommet des statues-menhirs masculines, des éléments de la culture matérielle des groupes chalcolithiques centre-alpins, des ornements corporels, et plus précisément de colliers. La lecture d'une seconde image ambiguë, la figure « a bandoliera », est ici également révisée. Couramment interprétée comme la représentation en plan d’un enclos mégalithique, par comparaison à des structures d’habitat issues de contextes forts éloignés des Alpes centrales, cette image est associée aux figurations qualifiées de « tpographiques ». Pourtant, une telle lecture constitue <b>une</b> réelle <b>erreur</b> méthodologique dans la mesure où elle extrait l’image non-seulement de son support (un corps de pierre) mais également de son contexte culturel. Aucun enclos mégalithique n’a jamais été découvert dans les Alpes centrales. Par conséquent, et en fonction du faisceau de données établi au sujet des figures circulaires, nous proposons de reconnaître dans la figure « a bandoliera », systématiquement gravée sur deux faces contigües, un ornement corporel propre à l’équipement masculin et porté sur l’épaule. Ainsi, l'interprétation de l'imagerie gravée et plus particulièrement de certaines figures ambiguës, se doit de prendre en considération la nature des blocs érigés et le contexte archéologique. Cette contextualisation à différentes échelles permet d’envisager de nouvelles interprétations de ces images qui apparaissent dès lors comme l’indice d’une valorisation sociale des cultures de l’apparence au sein des groupes humains chalcolithiques...|$|R
5000|$|The {{controversy}} next entered politics, {{influencing the}} debate on capital punishment in France, which culminated in criminal lawyer, and newly nominated Minister of Justice, Robert Badinter addressing the National Assembly in September 1981 to defend his bill to abolish capital punishment. He claimed, {{with regard to the}} Ranucci case, that there were: [...] "too many questions about his case, and that these questions were sufficient ... to condemn the death penalty". On the other hand, some journalists who covered the case refuted Perrault's miscarriage of justice theory. Christian Chardon, who covered the case for Le Nouveau Détective, wrote an article for the newspaper Minute titled [...] "Non ! L'affaire Ranucci n'est pas <b>une</b> <b>erreur</b> judiciaire" [...] ("No! The Ranucci case was not a miscarriage of justice") in late 1978, in which he recapped the key points of the case and argued for Ranucci's guilt. Chardon denied that Ranucci had been tortured as claimed during his trial. (In particular, he had accused Commissioner Gérard Alessandra, chief of the criminal section [...] "Nord" [...] in the Hôtel de Police de Marseille, {{who was in charge of}} the inquiry.) in late 1979, Jean Laborde published an article in Paris-Match which he titled [...] "Ranucci innocent ? Eh bien non !" [...] ("Ranucci innocent? Well no!"), also refuting Perrault's theory of Ranucci's innocence.|$|E
40|$|Liu Wenli : A Mistake by Voltaire {{concerning}} China. In Chapter 39 of the Siècle de Louis XIV, Voltaire {{was mistaken}} {{as to the}} succession of the Emperor Kang-hi. He was not responsible for this mistake, which {{can be explained by}} an error in a Chinese character which altered the Emperor's last wishes. Voltaire's information, which came from European missionaries' accounts, was insufficiant for a clear view of events in China. Wenli Liu. <b>Une</b> <b>erreur</b> de Voltaire sur la Chine. In: Dix-huitième Siècle, n° 14, 1982. Au tournant des Lumières : 1780 - 1820. pp. 421 - 422...|$|E
40|$|Some {{degree of}} error is {{inevitable}} in multi-agent bioassays regardless of design or measurement technology. Estimation error {{can be reduced}} post facto by exploiting the matrix partial ordering of the bioassay survival estimates. The standard method for this is order-restricted regression (ORR). If the joint action of the bioassay agents admits a tolerance-based interpretation, additional structure beyond matrix partial ordering is available, leading to a new method of error reduction. This tolerance-based error reduction (TBER) procedure almost always outperforms ORR. Like ORR, TBER applies to complete factorial bioassay designs and, using weighting, to incomplete designs. R ESUM E L'estimation des probabilites de survie d'etres vivants exposes adi#erentes doses d'agents toxiques est sujette a <b>une</b> <b>erreur</b> experimentale qui peut etre reduite post facto en exploitant certaines relations liant les probabilites de survie multivariees entre elles. Pour ce faire, on utilise generalement l [...] ...|$|E
40|$|Danielle Leeman - Compléments circonstanciels ou {{apposition}}s? Contemporary {{studies on}} apposition no longer take into account, {{as one of}} its characteristics, the traditional criterion of co-reference, understood as a virtual (lexical) or actual (referential) identity; from this point of view, certain adverbial complements may be considered as appositions. However, the study of numerous prepositional groups introduced by dans shows that the notion of co-reference is useful when it comes to distinguishing analytically between two types of functions an adverbial complement may have, in so far as its meaning may be circumstantial (e. g. of cause, manner, place) or else correspond to the verbal idea : thus, the decision to dissolve the National Assembly constitutes a mistake in a sentence such as Dans sa décision de dissoudre l'assemblée, Chirac a commis <b>une</b> <b>erreur.</b> Leeman Danielle. Compléments circonstanciels ou appositions ?. In: Langue française, n° 125, 2000. Nouvelles recherches sur l'apposition, sous la direction de Franck Neveu. pp. 18 - 29...|$|E
40|$|Bank of Canada {{working papers}} are {{theoretical}} or empirical works-in-progress on subjects in economics and finance. The views {{expressed in this}} paper {{are those of the}} authors. No responsibility for them should be attributed to the Bank of Canada. ISSN 1701 - 9397 © 2008 Bank of CanadaAcknowledgements We thank seminar participants at the Bank of Canada for their valuable comments. ii Authors ’ note: After completing this working paper, we realized a computer coding error, which led to an overstatement of transition costs due to imperfect credibility. The corrected results show that it takes at least 10 quarters of low credibility (as opposed to 2 quarters in the old version) for a policy change from inflation targeting to price-level targeting to be welfare reducing. The revised version of this paper with corrected transition costs is forthcoming as a Bank of Canada Working Paper. Note des auteurs: Après la rédaction de ce document de travail, nous avons relevé <b>une</b> <b>erreur</b> de programmation, qui a entraîné une surestimation de...|$|E
40|$|Après relecture <b>une</b> <b>erreur</b> est apparue dans le {{document}} et doit être retiréInternational audienceThe Ultra High Temperature Ceramics (UHTCs) are {{of great}} interest for different engineering sectors and notably the aerospace industry. Indeed, hypersonic flights, re-entry vehicles, propulsion applications and so on, require new materials that can perform in oxidizing or corrosive atmospheres at temperatures higher than 2000 °C and sometimes, for long life-time. To fulfil these requirements, UHTCs {{seems to be one}} of the most promising candidates and among this family, ZrB 2 and HfB 2 -based composites are the most attractive. Since 2006, Onera actively takes part in several programs to investigate such materials as well for hypersonic civil flights as for propulsion systems. Several manufacturing processes and compositions have been studied to assess the influence of the microstructure and the composition on mechanical properties and oxidation behaviour. For example, HfB 2 /SiC composites exhibit high mechanical properties at room temperature (Hv 10 = 17. 5 GPa and K 1 C = 6. 7 MPa. m 1 / 2) and high oxidation resistance above 1600 °C compared to traditional SiC-based ceramics thanks to the formation of multi-oxide scales composed of a refractory oxide (skeleton) and a glass component...|$|E
40|$|Abstract. The {{application}} of dilution gauging techniques to {{rivers and streams}} and other variable flows incurs a systematic error caused by the change in discharge with time. The discharge calculated from dilu-tion gauging results may be considered as {{an estimate of the}} stream discharge at the injection section or the sampling section, and the magnitude and sense of the error vary according to this choice. On the recession limb of a hydrograph and in other cases where the discharge may be expected to change smoothly and gradually, the equations derived from the residence time model previously proposed by the author may be linearized, and formulae of general applicability developed. In this paper formulae are presented for the error arising {{in the case of a}} constant rate tracer injection, and examples are given of their application to results from a number of different flow systems. It is shown that the error may be significant in two situations: where the rate of change of discharge is high and where poor mixing necessitates long gauging reaches and hence long tracer injections. Mesures par dilution sur la courbe de tarissement: 1. Méthode par injection de taux cons tan t Résumé. L'application des techniques de mesures par dilution aux rivières et ruisseaux e t aux autres débits variables provoque <b>une</b> <b>erreur</b> systématique occasionnée par le changement en débit avec le temps. Le débit calculé des résultats des mesures par dilution peuvent être considéré comme évaluation du débi...|$|E
40|$|Mastering {{the risk}} of {{thermally}} induced cracking in concrete structures {{need to know the}} exothermic hydration accompanying binders, particularly in the case of binders containing cement and pozzolan (silica fume or fly ash silico -alumineuse). The works presented here have is to model this exothermic depending on the composition of the concrete, after a bibliographic study and the realization of an experimental plan, including among other many tests Calorimeters supplementing the results find in literature. In a first step, we seek to predict the final magnitude of the exothermic. For this, it was considered, depending on the composition of the concrete {{and the nature of the}} binder, the final quantities of binders capable of reacting in the hydration and the accompanying exothermic hydration of these binders; we also propose a method of calculating the thermal capacity of the concrete depending on its temperature and the progress of hydration. The model proposes a result of this step gives the final elevation of temperature of concrete with a lower average error was 2 c on the sixty concretes tested. Was then studied the heat production kinetics in concrete. It is relatively well deducted from the kinetics of cement hydration, characterized by a standard test cement, and that of the pozzolanic reaction that appeared approximately constant for a given type of pozzolan. Furthermore, the study of the participation of a pozzolan resistance of concrete in compression led to propose an improvement of an existing model which was then valid on a wide set of experimental results taken from literature. This last step was used to compare the modes of participation of pozzolan has heat production on the one hand and the resistance of the other concrete. La maitrise des risques de fissuration d'origine thermique dans les structures en beton necessite de connaitre l'exothermie qui accompagne l'hydratation des liants, en particulier dans le cas de liants contenant du ciment et des pouzzolanes (fumee de silice ou cendre volante silico-alumineuse). Les travaux presentes ici ont consiste a modeliser cette exothermie en fonction de la composition du beton, apres une etude bibliographique et la realisation d'un plan experimental, comportant entre autres de nombreux essais calorimetriques completant les resultats trouves dans la litterature. Dans une premier temps, on a cherche a prevoir l'amplitude finale de l'exothermie. Pour cela, on a estime, en fonction de la composition du beton et de la nature des liants, les quantites finales de liants susceptibles de reagir dans l'hydratation, puis l'exothermie accompagnant l'hydratation de ces liants; on a egalement propose un mode de calcul de la capacite thermique du beton en fonction de sa temperature et de l'avancement de l'hydratation. Le modele propose a l'issue de cette etape donne l'elevation finale de temperature du beton avec <b>une</b> <b>erreur</b> moyenne inferieure a 2 c sur la soixantaine de betons testes. On a ensuite etudie la cinetique de production de chaleur dans le beton. Celle-ci se deduit relativement bien de la cinetique d'hydratation du ciment, caracterisee par un essai standard sur ciment, et de celle de la reaction pouzzolanique qui est apparue a peu pres constante pour un type de pouzzolane donne. Par ailleurs, l'etude de la participation des pouzzolanes a la resistance en compression du beton a conduit a proposer une amelioration d'un modele existant qu'on a ensuite valide sur un large ensemble de resultats experimentaux extraits de la litterature. Cette derniere etape a permis de comparer les modes de participation des pouzzolanes a la production de chaleur d'une part, et a la resistance du beton d'autre part...|$|E
40|$|La quantité de matière sèche et d’éléments minéraux exportée au cours de l’exploitation forestière est estimée pour 3 taillis d’espèces mélangées dans les Ardennes françaises. Des tarifs, régression linéaire, sont construits à partir d’échantillons. Nous montrons dans cet article la nécessité d’établir des tarifs de biomasse ou de mininéralomasse en {{fonction}} de chacun des facteurs étudiés. L’analyse de variance permet de classer, {{en fonction}} de la valeur du test F, les effets des facteurs ou de leur interaction soit : Espèce » Compartiment »> Station > Espèce-Station ≥ Espèce-Compartiment > Station-Compartiment. La nécessité d’utiliser les tarifs spécifiques à chaque espèce, à chaque station et pour chacun des compartiments pose le problème de l’optimisation de l’échantillonnage. Nous admettons que la variance générale estimée à partir de nos échantillons est le meilleur estimateur de la variance générale. Nous calculons ainsi le nombre minimum d’individus (arbres) à la probabilité de 5 p. 100 pour satisfaire à <b>une</b> <b>erreur</b> de 10 p. 100 sur la moyenne. Le nombre de 20 arbres échantillonnés par station et par espèce, se révèle insuffisant dans 17 p. 100 des cas. The intensification of forestry (including whole tree harvesting) {{raises the question}} of its consequences on site fertility. We evaluate the removal of mineral elements related to the degree of intensification by applying tariffs established by stratified sampling. Here we summarise the limitations to the application of these tariffs due to the variability of other factors : species, site, tree and its components. - Three coppices in the Primary Ardennes were sampled by : 1. Species : Oak (Quercus sessiliflora, birch (Betula verrucosa), and mountain ash (Sorbus aucuparia). 2. Site : « sol brun acide » with greater (80 cm) or lesser (40 cm) depths of loess over shales. 3. Tree component : leaves, branches, and three parts of the trunk cut at 7 cm, 4 cm and 1 cm diameters. - The dry weight of the material at 65 °C and major elements, N, P, K, Ca, Mg were determined. We used the following tariff model : Log (biomass) = a + b Log (circumference) and applied the correction for bias as used by Baskerville (1972). - The analyses of variance with one or more factors were measured {{to examine the effects of}} the factors. We used analysis of co-variance to compare the tariffs. Results : The qualitative examination of the variation in the chemical composition of the different components, showed that each is partly a function of site, but is primarly due to species (table 2). The interaction between species and site is low. We can classify the effects of, and interactions between, factors as a function of their F test value in the following order of decreasing magnitude : Species » tree component »> site > species X site ≥ species X component > station X component. The quantitative results (table 3) confirm that species is the most discriminating factor. There are also significant differences between the tariffs. These must be specific to species, site and tree component. The cost of these studies is high, so it was not possible to regroup the samples. Therefore, is it possible to increase the efficiency of the sampling strategy ? As no general law exists for the relationship between the parameter measured (circumference at 1. 30 m or C 130) and the mineral content of tree tissue, we calculated the minimum number of individuals, N, necessary by using the formula :(formule document ci-joint) at 5 p. 100 where L is the confidence limit at 10 p. 100 of the mean. The value of ơ is the value of the variance, and we admit that the best estimate is the variance estimated from our samples. Table 4 shows the results calculated for the different tree components. The variability is greater for mountain ash than for birch or oak. In our study, the choice ot 60 trees per species is theoretically insufficient in 17 p. 100 of the cases. At each site, for each species, we propose to sample 30 branches to estimate the mean at 10 p. 100, and 15 trees for the mean at 15 p. 100...|$|E
40|$|L'usage quasi systématique de fertilisants sur de grandes {{surfaces}} {{a conduit}} la majorité des aquifères superficiels à un grave niveau de contamination par les nitrates. Des essais de gestion environnementale de cette problématique agricole sont conduits à l'échelle du bassin versant afin d'estimer les flux de nitrates percolant vers la nappe. La présente étude reprend les résultats issus de la modélisation d'un bassin versant dans le but d'appréhender l'évolution de la concentration en nitrates dans les eaux de la nappe. L'importance des conditions hydrogéologiques dans les relations entre zones non saturée et saturée a été mise en évidence par la comparaison des concentrations calculées dans la zone non saturée et observées dans la nappe. En règle générale, les concentrations sont très semblables pour les zones proches des limites amont du bassin, et se différencient de plus en plus vers l'aval du système. Une dilution semble se produire entre les flux percolant des différentes zones non saturées et les flux d'eau et de nitrates s'écoulant dans l'aquifère. Afin de tester cette hypothèse, un modèle de dilution basé sur les flux d'eau et de nitrates dans les zones non saturée et saturée est développé. Appliqué sur l'axe d'écoulement principal du système, le modèle de dilution permet de reproduire adéquatement les concentrations observées dans la nappe à partir de celles calculées dans le sol avec <b>une</b> <b>erreur</b> maximale variant de 1 à 22 %. Le couplage d'un modèle environnemental pour la zone racinaire du sol avec un modèle de dilution simple peut permettre le calcul des concentrations en nitrates dans la zone saturée. Toutefois, la prise en compte des conditions hydrogéologiques du système est nécessaire à un calcul de dilution efficace basé sur les valeurs des flux de percolation. Pesticides and nitrates represent the {{main sources of}} aquifer contamination in agricultural zones. In many regions, nitrate concentration levels reach and exceed the water quality criteria (50 mg NO 3 /L). The increasing use of mineral fertilizers (which has doubled during the 20 last years) and the intensive exploitation of the aquifers for crop irrigation (1, 1 million ha in France) have led to groundwater contamination by nitrates. The dynamics (long-term persistence) and extensiveness (regional contamination) of this contamination make it a sensitive environmental issue. Comprehensive environmental management is {{needed in order to}} limit the increase of the concentration levels and to reduce the extent of the contaminated areas. During the last few years, research has been done in the field of watershed management, from laboratory experiments to field investigations. At the same time, numerous simulation models have been developed at different investigation scales. Banton et al. (1993) developed a model specifically devoted to environmental management. Their model, AgriFlux, is based on a mechanistic approach to the processes, using a stochastic method that takes into account the spatial variability of the parameters. AgriFlux calculates the nitrate concentrations as well as the water fluxes in the unsaturated zone. The concentrations in the unsaturated zone (obtained by modeling or measurement) are generally dissimilar to those observed in the saturated zone (i. e. in the aquifer) because the infiltration water is diluted in the aquifer water. This difference indicates that the concentrations in the unsaturated zone cannot be used to accurately evaluate the actual risk of groundwater contamination. Hydrogeological conditions such as the recharge limits, the flow direction and the flow rate should be incorporated into the evaluation. In this paper, the modeling results obtained previously (Dupuy et al., 1997) with AgriFlux for the La Jannerie watershed are used to determine the concentrations in the aquifer and to compare them with the concentrations measured in the observation wells. This watershed (160 ha) is used exclusively for agriculture. The fractured carbonate strata (Superior Oxfordian) constitute a phreatic aquifer with a vertical extension of about 20 m. First, the temporal evolution of the annual mean concentrations in the aquifer is compared with the evolution of the annual precipitation. The results show that the mean concentrations tend to follow precipitation levels. However, the differences observed at different locations in the watershed cannot be explained by these results. The spatial evolution of the concentrations from the upstream to the downstream part of the aquifer was studied in order to explain the concentration distribution in the watershed. On the main flow line, the concentrations observed from 1985 to 1989 show a decrease from the P 7 well (upstream) to the P 26 well (downstream). This phenomenon can be attributed to two factors. First, denitrification may occur in the aquifer during flow. However, it is recognized in literature that the denitrification rate is usually low and {{a long period of time}} is required to obtain a significant decrease in the nitrate level. The observed attenuation cannot be imputed to this factor alone. The second possible cause is related to the dilution of the nitrates in the water contained in the aquifer. In order to test this hypothesis, a dilution model was elaborated using the watershed division as indicated in Dupuy et al. (1997). In each area, the resulting concentration is obtained by diluting the fluxes of water and nitrate leaching in the unsaturated zone in the fluxes of water and nitrates flowing from the upstream area. The concentrations in the aquifer are calculated from upstream areas to downstream areas for the period between 1985 and 1989. The pattern of the concentration curves obtained in this manner agrees with the trend measured in the different wells. The results clearly show a decrease of the concentration in the aquifer water leached from the unsaturated zone. For the downstream area (well P 26), the calculated concentrations are higher than the observed ones. This difference could be due to the fact that the lateral fluxes (flow convergence into the median part) are not taken into account and the concentrations may thus be overestimated. However, the mean resulting error (12 %) remains low considering the lack of knowledge of the aquifer characteristics. It is therefore possible to accurately estimate the nitrate concentrations in the saturated zone from the concentrations simulated in the unsaturated zone using a simple dilution model. However, this method is only valid for simple hydrogeological conditions...|$|E
40|$|The {{world is}} {{day by day}} more computerized. There {{is more and more}} {{software}} running everywhere, from personal computers to data servers, and inside most of the new popularized inventions such as connected watches or intelligent washing machines. All of those technologies use software applications to perform the services they are designed for. Unfortunately, the number of software errors grows with the number of software applications. In isolation, software errors are often annoyances, perhaps costing one person a few hours of work when their accounting application crashes. Multiply this loss across millions of people and consider that even scientific progress is delayed or derailed by software error: in aggregate, these errors are now costly to society as a whole. There exists two techniques to deal with those errors. Bug fixing consists in repairing errors. Resilience consists in giving an application the capability to remain functional despite the errors. This thesis focuses on bug fixing and resilience in the context of exceptions. Exceptions are programming language constructs for handling errors. They are implemented in most mainstream programming languages and widely used in practice. We specifically target two problems:Problem # 1 : There is a lack of debug information for the bugs related to exceptions. This hinders the bug fixing process. To make bug fixing of exceptions easier, we will propose techniques to enrich the debug information. Those techniques are fully automated and provide information about the cause and the handling possibilities of exceptions. Problem # 2 : There are unexpected exceptions at runtime {{for which there is no}} error-handling code. In other words, the resilience mechanisms against exceptions in the currently existing (and running) applications is insufficient. We propose resilience capabilities which correctly handle exceptions that were never foreseen at specification time neither encountered during development or testing. In this thesis, we aim at finding solutions to those problems. We present four contributions to address the two presented problems. In Contribution # 1, we lies the foundation to address both problems. To improve the available information about exceptions, we present a characterization of the exceptions (expected or not, anticipated or not), and of their corresponding resilience mechanisms. We provide definitions about what is a bug when facing exceptions and what are the already-in-place corresponding resilience mechanisms. We formalize two formal resilience properties: source-independence and pure-resilience as well as an algorithm to verify them. We also present a code transformation that uses this knowledge to enhance the resilience of the application. Contribution # 2 aims at addressing the limitations of Contribution # 1. The limitations is that there are undecidable cases, for which we lack information to characterize them in the conceptual framework of Contribution # 1. We focus on the approaches that use the test suite as their main source of information as in the case of Contribution # 1. In this contribution, we propose a technique to split test cases into small fragments in order to increase the efficiency of dynamic program analysis. Applied to Contribution # 1, this solution improves the knowledge acquired by providing more information on more cases. Applied to other dynamic analysis techniques which also use test suites, we show that it improve the quality of the results. For example, one case study presented is the use of this technique on Nopol, an automatic repair tool. Contribution # 1 and # 2 are generic, they target any kind of exceptions. In order to further contribute to bug fixing and resilience, we need to focus on specific types of exceptions. This focus enables us to exploit the knowledge we have about them and further improve bug fixing and resilience. Hence, in the rest of this thesis, we focus on a more specific kind of exception: the null pointer dereference exceptions (NullPointerException in Java). Contribution # 3 focuses on Problem # 1, it presents an approach to make bug fixing easier by providing information about the origin of null pointer dereferences. We present an approach to automatically provide information about the origin of the null pointer dereferences which happen in production mode (i. e. those for which no efficient resilience mechanisms already exists). The information provided by our approach is evaluated w. r. t. its ability to help the bug fixing process. This contribution is evaluated other 14 real-world null dereference bugs from large-scale open-source projects. Contribution # 4 addresses Problem # 2, we present a way to tolerate the same kind of errors as Contribution # 3 : null pointer dereference. We first use dynamic analysis to detect harmful null dereferences, skipping the non-problematic ones. Then we propose a set of strategies able to tolerate this error. We define code transformations to 1) detect harmful null dereferences at runtime; 2) allow a runtime ehavior modification to execute strategies; 3) assess the correspondance between the modified behavior and the specifications. This contribution is evaluated other 11 real-world null dereference bugs from large-scale open-source projects. To sum up, this thesis studies the exceptions, their behaviors, the information one can gathered from them, the problems they may cause and the applicable solutions to those problems. Le monde est de plus en plus informatisé. Il y a de plus en plus de logiciels en cours d'exécution partout, depuis les ordinateurs personnels aux serveurs de données, et à l'intérieur de la plupart des nouvelles inventions connectées telles que les montres ou les machines à laver intelligentes. Toutes ces technologies utilisent des applications logicielles pour effectuer les taches pour lesquelles elles sont conçus. Malheureusement, le nombre d'erreurs de logiciels croît avec le nombre d'applications logicielles. Localement, <b>une</b> <b>erreur</b> logicielle est embêtante, elle peut coûter quelques heures de travail à une personne lorsque l'application crashe. Multipliez cette perte sur des millions de personnes et considérez que même les avancées scientifiques sont retardées ou empêchées par des erreurs de ce type: dans l'ensemble, ces erreurs sont coûteuses pour la société dans son ensemble. Il existe deux techniques pour faire face à ces erreurs. La réparation logicielle consiste à réparer ces erreurs manuellement. La résilience consiste à donner à une application la capacité de rester fonctionnelle malgré les erreurs. Cette thèse porte sur la correction des bugs et la résilience dans le contexte des exceptions. L'Exception est un mécanisme de gestion d'erreurs. Il est intégré dans la plupart des langages de programmation et largement utilisé dans la pratique. Dans cette thèse, nous ciblons spécifiquement deux problèmes:Problème n° 1 : Il ya un manque d'informations de débogage pour les bugs liés à des exceptions. Cela entrave le processus de correction de bogues. Pour rendre la correction des bugs liées aux exceptions plus facile, nous allons proposer des techniques pour enrichir les informations de débogage. Ces techniques sont entièrement automatisées et fournissent des informations sur la cause et les possibilités de gestion des exceptions. Problème n ° 2 : Il ya des exceptions inattendues lors de l'exécution pour lesquelles il n'y a pas de code pour gérer l'erreur. En d'autres termes, les mécanismes de résilience actuels contre les exceptions ne sont pas suffisamment efficaces. Nous proposons de nouvelles capacités de résilience qui gérent correctement les exceptions qui n'ont jamais été rencontrées avant. Dans cette thèse, nous nous efforçons de trouver des solutions à ces problèmes. Nous présentons quatre contributions pour résoudre les deux problèmes présentés. En résumé, cette thèse étudie les exceptions, leurs comportements, l'information qu'on peut recueillir auprès d'elles, les problèmes qu'elles peuvent causer et les solutions applicables à ces problèmes...|$|E
40|$|In Civil Aviation domain, to {{cope with}} the {{increasing}} traffic demand, research activities are pointed toward the optimization of the airspace capacity. Researches are thus ongoing on all Civil Aviation areas: Communication, Navigation, Surveillance (CNS) and Air Traffic Management (ATM). Focusing on the navigation aspect, the goals are expected to be met by improving performances of the existing services through the developments of new NAVigation AIDS (NAVAIDS) and the definition of new procedures based on these new systems. The Global Navigation Satellite System (GNSS) is recognized as a key technology in providing accurate navigation services with a worldwide coverage. A symbol of its importance, in civil aviation, can be observed in the avionics of new civil aviation aircraft since a majority of them are now equipped with GNSS receivers. The GNSS concept was defined by the International Civil Aviation Organization (ICAO). It, includes the provision of an integrity monitoring function by an augmentation system in addition to the core constellations. This is needed to meet all the required performance metrics of accuracy, integrity, continuity and availability which cannot be met by the stand-alone constellations such as GPS. Three augmentation systems have been developed within civil aviation: the GBAS (Ground Based Augmentation System), the SBAS (Satellite Based Augmentation System) and the ABAS (Aircraft Based Augmentation System). GBAS, in particular, is currently standardized to provide precision approach navigation services down to Category I (CAT I) using GPS or Glonass constellations and L 1 band signals. This service is known as GBAS Approach Service Type-C (GAST-C). In order to extend this concept down to CAT II/III service, research activities is ongoing to define the new service called a GAST-D. Among other challenges, the monitoring of the ionospheric threat is the area where the integrity requirement is not met. Thanks to the deployment of new constellations, Galileo and Beidou, and the modernization process of the existing ones, GPS and Glonass, the future of GNSS is envisaged to be Multi-Constellation (MC) and Multi-frequency (MF). In Europe, research activities have been focused on a Dual-Constellation (DC) GNSS and DC GBAS services based on GPS and Galileo constellationsthe robustness of the entire system against unintentional interference thanks to the use of measurements in two protected frequency bands,• the robustness against a constellation failure,• the accuracy improvement by using new signals with improved performance, and more satellites. any of the information included herein. Reprint with approval for publisher and with reference to source code only• improved detection of ionosphere anomalous condition thanks to the use of DF measurements. • mitigation of the residual ionospheric induced range errorThese last points in particular are considered as one of the biggest benefits brought by the DF GBAS. To overcome the problems experienced by Single-Frequency (SF) GBAS due to ionosphere anomalies, the use of two frequencies (Dual Frequency, DF) has been selected as a mean to improve ionosphere anomalies detection and to mitigate ionosphere residual errors. Advantages in using a DC/DF GBAS (GAST-F) system are, however, not only related to the integrity monitoring performance improvement. However, the use of new signals and a new constellation, does not bring only benefits. It also raises a series of challenges that have to be solved to fully benefit from the new concept. In this thesis, some challenges, related to DC/DF GBAS, have been investigated. One of them, rising from the use of new GNSS signals, is to determine the impact of error sources that are uncorrelated between the ground station and the aircraft and that induce an error on the estimated position. Using two frequencies, there is the possibility to form measurement combinations like Divergence-free (D-free) and Ionosphere-free (I-free) for which the errors impact has to be analyzed. In this thesis, the impact of the uncorrelated errors (noise and multipath as main sources) on ground measurements is analyzed. The aim is to compare the derived performances with the curve proposed in (RTCA Inc.; DO 253 -C, 2008) for the ground correction accuracy and derived for GPS L 1 C/A. Another issue raised by the use of DC/DF GBAS is the increased number of satellites and the presence of a second frequency. This leads to the constraint of having a big number of channels in GNSS receivers to track all available signals. Moreover, to broadcast a bigger number of corrections from the ground to the aircraft, the messages capacity has to be increased with respect to the current SF/SC GBAS. To solve this problem, some solutions have been proposed, one of these is the implementation of a satellite selection algorithm. In this PhD, the impact of some algorithms proposed in literature has been analyzed on a simulated DC GBAS system. The last analysis performed in this thesis regards some of the challenges in the integrity monitoring domain. GBAS has been validated, nowadays, only for GAST-C to provide CAT I service. Although almost the same architecture has been used to provide CAT II/III service within GAST-D (but with new monitors on the ground architecture), the concept to derive the airworthiness between the two services is totally different. This major difference is justified by the fact that for CAT III operations, requirements are more stringent than for CAT I. Despite all the efforts done, GAST-D for CATII/III has not been validated. The cause of the non-validation of GAST-D is the lack of integrity performances in monitoring the ionosphere anomalous activity with the proposed monitoring scheme. Even if for GAST-F, relying on DF combinations, the monitoring of the ionosphere could not represent the main issue and the integrity performances of current monitors may be sufficient to meet the requirements, two considerations have to be done• In case of loss of frequency, for GAST-F, the ionosphere monitoring presents the same condition as for GAST-D. If the latter is not validated, the fallback mode is GAST-C, limiting the availability of CAT II/III operations. • Improving the integrity performances of GAST-D will permit to create a system with an enhanced CAT II/III operations availability thanks to the use of GAST-D as fallback mode in case of frequency loss whenever GAST-F is used as primary mode. In case of GAST-D as primary mode, GAST-F can be considered the fallback mode for cases of ionosphere anomalous conditions. Considering previous conditions, the work done in this thesis has focused on the monitoring of the ionospheric conditions that are impeding GAST-D to be validated. A solution combining RAIM SC/SF GBAS differential corrections and foreseen GAST-D monitors is proposed. The combination of these integrity monitoring functions permit to get closer to GAST-D requirements for particular ionospheric scenarios where the maximum ionospheric induced range error can be assumed. The consideration of dual constellation in the same mix of integrity monitoring functions has also been studied, as a possible fallback mode of GAST-F when one of the two frequencies is lost. The ionosphere is not the only integrity issue for GAST-F. Other analysis have been done considering the impact of new signals or new processing modes on the existing monitors. Concerning this, the impact of a lower update rate, for the PRC and RRC, on the Excessive Acceleration (EA) monitor has been analyzed. The aim is to verify the feasibility of the monitor in extending the current update interval from 0. 5 seconds up to a proposed value of 2. 5 seconds. Dans le domaine de l'aviation civile les activités de recherche sont guidées par la volonté d’améliorer la capacité de l'espace aérien. En ce que concerne la navigation, les objectifs devraient être atteints par l'amélioration des performances des services existants grâce au développement des nouvelles aides à la navigation. La navigation par satellite, grâce au concept de Global Navigation Satellite System (GNSS), est reconnue comme une technologie clé pour fournir des services de navigation précis avec une couverture mondiale. Le GNSS comprend une fonction de surveillance de l'intégrité fournie par un système d’augmentation en plus de la constellation de base. Un entre eux est le système GBAS qui est actuellement standardisé pour fournir des services de navigation, comme l'approche de précision, jusqu’à la Catégorie I (CAT I) en utilisant les constellations GPS ou Glonass et des signaux dans la bande L 1. Ce service est connu sous le nom de GBAS Approach Service Type-C (GAST-C). Afin d'étendre ce concept jusqu'à des approche de précision CAT II/II (GAST-D), les activités de recherche sont en cours. Grâce au développement des nouvelles constellations, Galileo et Beidou, l'avenir du GNSS sera Multi-Constellation (MC) et Multi-Fréquence (MF). En Europe, les activités de recherche se sont concentrées sur un système GNSS Bi-Constellation (Dual-Constellation, DC) basé sur GPS et Galileo. Afin de surmonter les problèmes rencontrés par en fonctionnement Mono-Fréquence (Single-Frequency, SF) en présence d’anomalies ionosphériques, l'utilisation de deux fréquences (Dual-Frequency, DF) a été sélectionnée comme un moyen d'améliorer la détection des anomalies ionosphériques et d'atténuer les erreurs résiduelles ionosphériques. Cependant, l'utilisation de nouveaux signaux et d’une nouvelle constellation, n’apporte pas que des avantages. Elle soulève également une série de défis qui doivent être résolus de profiter pleinement de ce nouveau concept. Dans cette thèse, certains défis, liés à un système DC/DF GBAS ont été étudiés. Un d’entre eux, causé par l'utilisation de nouveaux signaux GNSS, est de déterminer l'impact des sources d'erreur qui sont décorrélées entre la station au sol et l'avion et qui induisent <b>une</b> <b>erreur</b> sur la position estimée. Un autre problème soulevé par l'utilisation de DC/DF GBAS est l'augmentation du nombre de satellites et la présence d'une deuxième fréquence. Cela nécessite un grand nombre de canaux dans les récepteurs pour poursuivre tous les signaux disponibles. Dans cette thèse, l'impact de certains algorithmes proposés dans la littérature a été analysé sur un système GBAS DC simulé. La dernière analyse effectuée dans cette thèse concerne le domaine de la surveillance de l'intégrité. Le travail effectué dans cette thèse a mis l'accent sur le suivi des conditions ionosphériques qui entravent la validation du GAST-D. Une solution combinant un algorithme de type RAIM et les moniteurs prévus en GAST-D est proposée. La prise en compte d’une deuxième constellation avec la même combinaison de fonctions de contrôle d'intégrité a également été étudiée, en tant que mode de repli possible de GAST-F lorsque l'une des deux fréquences est perdue. L'ionosphère n’est pas le seul problème d'intégrité pour GAST-F. D'autres analyses doivent être faites compte tenu de l'impact des nouveaux signaux ou de nouveaux modes de traitement sur les moniteurs existants. Sur ce sujet, l'impact d'un taux de mise à jour plus bas des corrections différentielles sur le moniteur « Excessive Acceleration » a été analysé...|$|E
40|$|With recent {{progress}} in computing, algorithmics and telecommunications, 3 D models are increasingly used in various multimedia applications. Examples include visualization, gaming, entertainment and virtual reality. In the multimedia domain 3 D {{models have been}} traditionally represented as polygonal meshes. This piecewise planar representation {{can be thought of}} as the analogy of bitmap images for 3 D surfaces. As bitmap images, they enjoy great flexibility and are particularly well suited to describing information captured from the real world, through, for instance, scanning processes. They suffer, however, from the same shortcomings, namely limited resolution and large storage size. The compression of polygonal meshes has been a very active field of research in the last decade and rather efficient compression algorithms have been proposed in the literature that greatly mitigate the high storage costs. However, such a low level description of a 3 D shape has a bounded performance. More efficient compression should be reachable through the use of higher level primitives. This idea has been explored to a great extent in the context of model based coding of visual information. In such an approach, when compressing the visual information a higher level representation (e. g., 3 D model of a talking head) is obtained through analysis methods. This can be seen as an inverse projection problem. Once this task is fullled, the resulting parameters of the model are coded instead of the original information. It is believed that if the analysis module is efficient enough, the total cost of coding (in a rate distortion sense) will be greatly reduced. The relatively poor performance and high complexity of currently available analysis methods (except for specific cases where a priori knowledge about the nature of the objects is available), has refrained a large deployment of coding techniques based on such an approach. Progress in computer graphics has however changed this situation. In fact, nowadays, an increasing number of pictures, video and 3 D content are generated by synthesis processing rather than coming from a capture device such as a camera or a scanner. This means that the underlying model in the synthesis stage can be used for their efficient coding without the need for a complex analysis module. In other words it would be a mistake to attempt to compress a low level description (e. g., a polygonal mesh) when a higher level one is available from the synthesis process (e. g., a parametric surface). This is, however, what is usually done in the multimedia domain, where higher level 3 D model descriptions are converted to polygonal meshes, if anything by the lack of standard coded formats for the former. On a parallel but related path, the way we consume audio-visual information is changing. As opposed to recent past and a large part of today's applications, interactivity is becoming a key element in the way we consume information. In the context of interest in this dissertation, this means that when coding visual information (an image or a video for instance), previously obvious considerations such as decision on sampling parameters are not so obvious anymore. In fact, as in an interactive environment the effective display resolution can be controlled by the user through zooming, there is no clear optimal setting for the sampling period. This means that because of interactivity, the representation used to code the scene should allow the display of objects in a variety of resolutions, and ideally up to infinity. One way to resolve this problem would be by extensive over-sampling. But this approach is unrealistic and too expensive to implement in many situations. The alternative would be to use a resolution independent representation. In the realm of 3 D modeling, such representations are usually available when the models are created by an artist on a computer. The scope of this dissertation is precisely the compression of 3 D models in higher level forms. The direct coding in such a form should yield improved rate-distortion performance while providing a large degree of resolution independence. There has not been, so far, any major attempt to efficiently compress these representations, such as parametric surfaces. This thesis proposes a solution to overcome this gap. A variety of higher level 3 D representations exist, of which parametric surfaces are a popular choice among designers. Within parametric surfaces, Non-Uniform Rational B-Splines (NURBS) enjoy great popularity as a wide range of NURBS based modeling tools are readily available. Recently, NURBS has been included in the Virtual Reality Modeling Language (VRML) and its next generation descendant eXtensible 3 D (X 3 D). The nice properties of NURBS and their widespread use has lead us to choose them as the form we use for the coded representation. The primary goal of this dissertation is the definition of a system for coding 3 D NURBS models with guaranteed distortion. The basis of the system is entropy coded differential pulse coded modulation (DPCM). In the case of NURBS, guaranteeing the distortion is not trivial, as some of its parameters (e. g., knots) have a complicated influence on the overall surface distortion. To this end, a detailed distortion analysis is performed. In particular, previously unknown relations between the distortion of knots and the resulting surface distortion are demonstrated. Compression efficiency is pursued at every stage and simple yet efficient entropy coder realizations are defined. The special case of degenerate and closed surfaces with duplicate control points is addressed and an efficient yet simple coding is proposed to compress the duplicate relationships. Encoder aspects are also analyzed. Optimal predictors are found that perform well across a wide class of models. Simplification techniques are also considered for improved compression efficiency at negligible distortion cost. Transmission over error prone channels is also considered and an error resilient extension defined. The data stream is partitioned by independently coding small groups of surfaces and inserting the necessary resynchronization markers. Simple strategies for achieving the desired level of protection are proposed. The same extension also serves the purpose of random access and on-the-fly reordering of the data stream. Avec les récents progrès de l'informatique et des télécommunications, les modèles 3 D sont de plus en plus utilisés dans les applications multimédia. La visualisation, les jeux, le divertissement et la réalité virtuelle comptent parmi les exemples les plus répandus. Dans le domaine du multimédia les modèles 3 D ont été traditionnellement représentés comme des maillages polygonaux. Cette représentation plane par morceaux, peut être vue comme l'analogue des images bitmap pour les surfaces 3 D. Comme les images bitmap, ils jouissent d'une grande flexibilité et sont particulièrement bien adaptés pour décrire des informations acquises depuis le monde réel, comme par exemple, lors d'un processus de balayage. Ils souffrent, cependant, des mêmes limitations, notamment une résolution limitée et un grand espace de stockage. La compression de maillages polygonaux est un domaine de recherche très actif depuis une décennie et des algorithmes de compression efficaces permettant de réduire fortement les besoins en place de stockage, ont été proposés dans la littérature. Cependant, cette description bas-niveau de formes 3 D a une performance limitée. Une compression plus efficace devrait être possible avec l'usage de primitives de plus haut niveau. Cette idée a été extensivement explorée dans le contexte du codage à base de modèles de l'information visuelle. Dans une telle approche, lors de la compression de l'information visuelle une représentation de plus haut niveau (par ex. un modèle 3 D d'une tête parlante) est obtenue par analyse. Ceci peut être vu comme un problème de projection inverse. Une fois cette tâche accomplie, les paramètres du modèle résultants sont codés à la place de l'information originale. Il est communément admis que si le module d'analyse est suffisamment efficace le coût total de codage (dans le sens débit distorsion) en sera largement réduit. La performance relativement basse et la haute complexité des méthodes d'analyse existantes (mis à part des cas spécifiques où une connaissance a priori de la nature des objets est disponible), a empêché un large déploiement des techniques de codage basées sur une telle approche. Le progrès dans le domaine de l'infographie (computer graphics) a néanmoins changé cette situation. En effet, de nos jours, un nombre croissant d'images, vidéos et contenu 3 D sont générés par procédés de synthèse au lieu de provenir d'un appareil de capture, tels une caméra ou un scanner. Cela signifie que le modèle sous-jacent dans le stade de synthèse peut être utilisé pour améliorer les performances de codage sans avoir besoin de recourir à un module d'analyse hautement complexe. En d'autres mots, ce serait <b>une</b> <b>erreur</b> que de vouloir essayer de compresser une description bas-niveau (par ex. un maillage polygonal) alors qu'une description de plus haut niveau est disponible dans le processus de synthèse (par ex. une surface paramétrique). Cela est, cependant, ce qui est couramment fait dans le domaine du multimédia, où des descriptions de modèles 3 D de haut niveau sont convertis en maillage polygonaux, ne serait-ce que par manque d'un format standard pour le codage de ceux-ci. Par ailleurs, la façon dont nous consommons l'information audiovisuelle est en train de changer. A l'opposé des anciennes applications et une grande partie des actuelles, l'interactivité est en train de devenir un élément clé de la manière dont nous consommons l'information. Dans le cadre de la présente dissertation, cela signifie que, lorsque nous codons une information visuelle (par ex. une image ou une vidéo), des considérations évidentes par le passé telles que la sélection des paramètres d'échantillonnage n'est plus aussi évidente qu'avant. En effet, à l'instar d'un environnement interactif où la résolution d'affichage effective peut être contrôlée par l'utilisateur à travers un zoom, il n'y a pas de choix optimal clairement défini pour les paramètres d'échantillonnage. Cela signifie qu'à cause de l'interactivité, la représentation utilisée pour coder la scène devrait permettre l'affichage des objets dans une large gamme de résolutions et, idéalement, jusqu'à l'infini. Une façon de résoudre ce problème serait l'utilisation d'un suréchantillonnage extensif. Néanmoins, cette approche est irréaliste et trop coûteuse à implanter dans beaucoup de situations. L'alternative serait d'utiliser une représentation indépendante de la résolution. Dans le domaine du modelage 3 D, lesdites représentations sont couramment disponibles lorsque les modèles sont crées par un artiste sur un ordinateur. Le sujet de cette dissertation est précisément la compression de modèles 3 D dans une forme de plus haut niveau. Le codage direct sous une telle forme devrait délivrer une performance débit distorsion améliorée tout en procurant un large degré d'indépendance de résolution. Il n'y a pas eu, jusqu'à ce jour, de travaux majeurs sur la compression efficace de telles représentations, telles que les surfaces paramétriques. Cette thèse propose une solution pour combler ce vide. Une variété de représentations 3 D de haut niveau existe, parmi lesquelles les surfaces paramétriques sont un choix répandu parmi les designers. Dans la famille des surfaces paramétriques, les B-Splines rationnelles non-uniformes (NURBS) jouissent d'une grande popularité étant donné qu'une large gamme d'outils basés sur les NURBS sont couramment disponibles. Récemment, les NURBS ont été ajoutées dans le Virtual Reality Modeling Language (VRML) ainsi que son descendant de nouvelle génération le eXtensible 3 D (X 3 D). Les bonnes propriétés des NURBS et leur utilisation largement répandue nous ont conduit à les choisir comme forme sous laquelle les modèles seront codés. Le but principal de cette dissertation est la définition d'un système de codage des modèles 3 D NURBS avec une distorsion garantie. La base du système est la modulation par impulsion et codage différentiel (DPCM) codée entropiquement. Dans le cas de NURBS, garantir la distorsion n'est pas évident, dès lors que certains de ses paramètres (par ex. noeuds) ont une influence compliquée sur la distorsion totale de la surface. A cette fin, une analyse détaillé de la distorsion est effectuée. En particulier, des relations jusqu'alors inconnues entre la distorsion des noeuds et la distorsion de la surface résultante est démontrée. L'efficacité de la compression est recherchée à chaque stade et des codeurs entropiques simples mais néanmoins efficaces sont définis. Le cas particulier de surfaces fermées et dégénérées avec des point de contrôle dupliqués est adressé et un codage simple et efficace est proposé pour compresser les relations de duplication. Les aspects de l'encodeur sont aussi analysés. Des prédicteurs optimaux ayant une bonne performance sur une large classe de modèles sont trouvés. Des techniques de simplification, ayant un coût négligeable sur la distorsion, sont aussi considérées pour une meilleure efficacité de compression. La transmission sur des canaux présentant un taux d'erreur non négligeable est aussi considérée est une extension de résilience aux erreurs est définie. Le train de données est morcelé en codant indépendamment des petits groupes de surfaces et en insérant les marqueurs de resynchronisation nécessaires. Des stratégies simples pour atteindre le niveau désiré de protection sont proposées. La même extension sert aussi pour l'accès aléatoire et le réordonnancement à la demande du train de données...|$|E
40|$|Strontium Bismuth Titanate {{is a very}} {{promising}} material for high temperature piezoelectric applications, its elevated ferroelectric phase transition (530 °C), linear piezoelectric properties under low field and relatively low room temperature conductivity (compared to others Bismuth Titanates) make it very attractive for precision sensors. However, under severe conditions (low frequency, high field, high temperature or low oxygen partial pressure) some of those advantages disappear. Piezoelectric response is dominated by charge drift in general becomes unstable. Above all, at high temperature and low oxygen partial pressure, a large conductivity increase reduces the piezoelectric efficiency of the material, in this work, electrical conductivity, piezoelectric properties and dielectric permittivity of SrBIT ceramic have been investigated in conditions of high temperature, low oxygen partial pressure, low to moderate driving field and frequency. As {{a result of this}} research, a better understanding of SrBIT properties was achieved. Thanks to a careful study of SrBIT processing as a bulk ceramic, a reproducible route was established. Many basic mechanisms leading to both SrBIT crystallization and sintering have been identified. It was demonstrated that a detailed knowledge of the exact processing conditions is required in order to achieve high quality material. DC conductivity measurements were carried out as a function of temperature, oxygen partial pressure and dopant concentration. It was found that the apparent activation energy for conduction for undoped SrBIT was 1 eV between 140 and 220 °C and 1. 5 eV between 450 and 700 °C. Decrease of the activation energy in the lower temperature range has been discussed considering grain boundary conductivity, as a transition from electronic to ionic conduction or as consequence of small polarons conduction. It was shown that lower activation energy resulted from Manganese doping (0. 5 eV), this was interpreted as either growing influence of grain boundary conductivity as dopant concentration increases or as shallow hole trap formation. DC conductivity measurements and acceptor/donor doping experiments demonstrated p-type conductivity in the low temperature range (up to 220 °C) as donor doping decreases conductivity, while oxygen partial pressure controlled measurements indicated n-type conductivity at higher temperature (above 700 °C). An electrical impedance analysis was performed with several equivalent circuits. The aim of these models was to simulate the impedance of SrBIT. The best approximation was found with a distributed element of Havriliak-Negami type. However, as the physical justification for this circuit was not clear, the investigation of the grain, grain boundary and electrode impedances was performed with multiple discrete parallel RC elements. With temperature, grain size and oxygen activity variations, the identification of three separate contributions as grain, grain boundary and electrode was realized. The anisotropy of conductivity and permittivity was demonstrated with textured material and both DC and AC analysis. With the master curves built for the electrical modulus, {{it was found that the}} impedance probably related to Bismuth oxide layers produces an additional high frequency are. From this finding and by comparing characteristic relaxation frequencies of undoped, 2 mol. % Mn and 4 mol. % Nb SrBIT, it was determined that conductivity is higher in the ab plane direction than in the c direction within both perovskite units and Bismuth oxide layers. With conductivity measurements under controlled oxygen partial pressure, it was found that an acceptor-based (intrinsic or extrinsic) model could be used to describe the electrical conductivity under controlled oxygen partial pressure of both undoped and 2 mol. % Mn doped SrBIT. However, as neither a pO 2 - 1 / 6 region (intrinsic oxygen vacancies compensated by electrons) for undoped SrBIT nor a pO 2 - 1 / 4 region (oxygen vacancies compensated by singly ionized acceptors) for Mn doped SrBIT were seen, it was concluded that the acceptorcontrolled model is not sufficient for a complete description of SrBIT. For this reason and in order to include the low oxygen partial pressure behavior of undoped SrBIT, a donor-based (intrinsic) model was also considered. The source of intrinsic donors would be in that case Bi 3 + cations sitting on Sr 2 + sites in the perovskite sublattice. Considering Bismuth vacancies as the negative compensating species, both pO 2 - 1 / 4 and pO 2 -independent regions could be predicted with the model. However, even if the donor-controlled model seems to better match conductivity measurements in the full PO 2) range, rejecting the acceptor-based model would be an error. It is actually not demonstrated that in SrBIT the concentration of exchanged Bismuth cations is always (all temperature, pressure) larger than the natural acceptor impurity concentration. It is very likely that the cation exchange is dependent of the oxygen partial pressure. It is also not proved as suggested in the literature that direct compensation between exchanged Strontium and Bismuth exists, reducing the net donor-excess. With the acceptor-controlled model, the mass-action constants for reduction and for ionization of intrinsic carriers across the band gap were determined. The band gap of SrBIT was estimated to be 3. 5 eV. The ionic conductivity of SrBIT was determined at high temperature with measurements under controlled oxygen partial pressure. It was found that the electrical conductivity of SrBIT is probably mixed (electronic and ionic) as the estimation of the transference number provided quite large values (t= 0. 8 at 800 °C). From electronic and ionic conductivity data, mixed conduction can actually be predicted in a large temperature range (above room temperature). The piezoelectric measurements using direct effect demonstrated that it is actually possible to unlock piezoelectrically active ferroelectric domain walls and create non-linear piezoelectric properties in undoped SrBIT. This occurs above a threshold elastic field, which is thermally activated. With a piezoelectric composite, it was demonstrated that the electromechanical coupling between two different phases creates a piezoelectric relaxation. This one could be positive or negative depending on the respective properties of each composite's component. It was shown experimentally that a small temperature change is sufficient to transform a positive relaxation into a negative one. While these experiments did not provide a detailed microstructural explanation for the piezoelectric relaxation observed in 2 mol. % Mn doped SrBIT, they gave a first insight into an original phenomenological approach. Microstructure and piezoelectric properties were related with the calculation of a piezoelectric relaxation composite made of two textured samples. This demonstrated that a piezoelectric relaxation may occur just because of anisotropy. Microstructural reproduction of this coupling is actually an important feature of Aurivillius phases. Le Titanate de Strontium Bismuth (abrégé SrBIT) est un matériau très prometteur pour des applications piézo-électriques à haute température. Grâce à une température de transition de phase ferro-électrique élevée (530 °C), à des propriétés piézo-électriques très stables sous faible champ et à une conductivité électrique relativement basse à température ambiante (comparée à d'autres Titanates de Bismuth), ce matériau est très intéressant pour réaliser des capteurs de précision. Cependant, sous conditions extrêmes (basse fréquence, champ élevé, haute température ou basse pression d'oxygène), ces avantages disparaissent et la réponse piézo-électrique, dominée par les dérives de charge, devient instable. Mais avant tout, à haute température et basse pression d'oxygène, l'accroissement très fort de la conductivité réduit considérablement l'effet piézo-électrique de SrBIT. Dans ce travail, la conductivité électrique, les propriétés piézo-électriques et la permittivité diélectrique de SrBIT ont été étudiées à haute température, basse pression partielle d'oxygène, faible champ et fréquence. Cette recherche a permis une meilleure compréhension des propriétés de SrBIT. Par une étude détaillée de la préparation de SrBIT sous forme de céramique massive, une méthode de préparation reproductible a été établie. Différents mécanismes élémentaires qui interviennent lors de la cristallisation ou du frittage ont été identifiés. Mais, il a été démontré également qu'une connaissance pointue des paramètres du procédé est requise pour garantir la préparation de matériaux de bonne qualité. Des mesures de conductivité électrique de type DC ont été effectuées en variant la température, la pression partielle d'oxygène et la teneur en dopant. L'énergie d'activation pour la conductivité déterminée lors de ce travail de thèse est de 1 eV entre 140 et 220 °C et de 1. 5 eV entre 450 et 700 °C. La diminution de l'énergie d'activation dans la gamme inférieure de température a été interprétée comme la conséquence d'une conductivité aux joints de grain prépondérante à basse température. Les hypothèses de la transition d'un régime électronique à un régime ionique ou la conduction par polarons ont également été considérées. La diminution de l'énergie d'activation (0. 5 eV) par le dopage de type accepteur de SrBIT a été interprétée comme soit l'influence grandissante de la conductivité des joints de grains soit le piégeage des trous dans des niveaux peu profonds. Les mesures de conductivité DC et les dopages de type donneur et accepteur ont démontré une conductivité de type p dans la gamme inférieure de température (jusqu'à 220 °C), car l'ajout d'une certaine quantité de Niobium (donneur) diminue la conductivité de SrBIT. Par contre, les mesures effectuées à plus haute température sous pression partielle d'oxygène contrôlée indiquent une conductivité de type n. L'analyse dc l'impédance électrique de SrBIT a été réalisée à l'aide de différents circuits équivalents. A l'aide de ces modèles, l'impédance de SrBIT a pu être simulées. La meilleure approximation a été trouvée avec un élément distribué de type Havriliak-Negami. Cependant, comme la justification physique de ce circuit n'est pas claire l'étude de l'impédance des grains, des joints de grains et de l'interface céramique-électrode a été effectuée avec plusieurs circuits équivalents de type discret. À l'aide de l'influence de la température, de la taille de grain et de la pression partielle d'oxygène, l'identification de l'impédance des grains, des joints de grains et de l'électrode a pu être établie. L'anisotropie de la conductivité et de la permittivité a été démontrée par l'analyse AC et DC de matériau texturé. Grâce à des courbes maîtresses construites pour le module électrique, l'impédance probablement liée aux couches d'oxyde de Bismuth a été observée à haute fréquence. Avec ce résultat et en comparant les fréquences de relaxation de SrBIT non-dopé, dopé accepteur (2 % mol Mn) et dopé donneur (4 % mol.), il a pu être démontré que la conductivité est plus grande dans la direction dite "plan ab" que dans la direction "c" pour les unités de structure pérovskite et pour les couches d'oxyde de Bismuth. Avec des mesures de conductivité DC à haute température (> 700 °C) sous pression partielle d'oxygène contrôlée, il a pu être démontré qu'un modèle basé sur un excès d'accepteurs pouvait être avantageusement utilisé pour décrire la conductivité de SrBIT non-dopé et dopé accepteur. Cependant, comme aucune région en pO 2 - 1 / 6 (compensation des lacunes d'oxygènes intrinsèques par des électrons) pour SrBIT non-dopé ou en O 2 - 1 / 4 (compensation des lacunes d'oxygènes par des accepteurs simplement ionisés) pour SrBIT dopé Mn n'ont été observée, il a été conclu qu'un modèle basé uniquement sur un excès d'accepteurs était insuffisant. C'est pourquoi, afin aussi d'inclure le comportement de SrBIT à très basse pression d'oxygène, un autre modèle, basé sur un excès intrinsèque de donneurs a également été considéré. Dans ce modèle, la source intrinsèque de donneur proviendrait d'un échange de site entre Bi 3 + des couches d'oxydes de Bismuth et Sr 2 + des unités pérovskite. En assumant la compensation des défauts positifs par des lacunes de Bismuth, une région de conductivité en pO 2 - 1 / 4 et une autre indépendante de la pression peuvent être prédites. Cependant, même si ce modèle semble mieux correspondre aux mesures de conductivité, rejeter le modèle accepteur serait <b>une</b> <b>erreur.</b> En effet, il n'est pas prouvé que la concentration de donneurs intrinsèque soit toujours supérieure (à n'importe quelle température ou pression) que la concentration d'impuretés. Il est par exemple très probable que l'échange de cations soit dépendant de la pression partielle d'oxygène. De plus, la compensation directe des deux cations échangés ne peut pas être exclue, ce qui réduirait l'excès net de donneur. Avec le modèle accepteur, les constantes d'équilibre pour la création de lacunes d'oxygène et pour l'ionisation de porteurs au travers de la bande interdite ont été déterminées. L'énergie de la bande interdite de SrBIT a été estimée à 3. 5 eV. Avec les mesures de conductivité sous pression partielle d'oxygène contrôlée, la conductivité ionique de SrBIT a été estimée à haute température. Ainsi, il est proposé que la conductivité de SrBIT est de type mixte (électronique et ionique), par exemple le nombre de transférence à 800 °C est de 0. 8. A partir des mesures de conductivité ionique et électronique, une conduction mixte sur une large gamme de température peut être prédite. Des mesures piézo-électriques ont démontré que le désancrage de parois de domaines ferroélectriques actives pour la piézo-électricité de SrBIT était possible, ce qui induit des propriétés piézo-électriques non linéaires. Ceci se produit au-dessus d'un seuil de champ élastique qui est activé thermiquement. Avec un composite piézo-électrique, la création d'une relaxation piézoélectrique par le couplage électro-mécanique de deux différentes phases a été démontré. Cette relaxation peut être positive ou négative selon les propriétés des matériaux constituant le composite. Par exemple, un petit écart de température transforme une relaxation positive en relaxation négative. Bien que ces expériences ne permettent pas d'expliquer les relaxations piézo-électriques observées pour SrBIT dopé Manganèse, elles donnent un premier aperçu d'une approche phénoménologique originale. Le lien entre la microstructure et la relaxation piézoélectrique a été établi par le calcul du coefficient piézo-électrique d'un composite constitué de deux échantillons texturés. Il a ainsi été démontré que les différences de propriétés dues à l'anisotropie de SrBIT suffisent à créer une relaxation. La reproduction interne à la microstructure de ce couplage est certainement une caractéristique importante des phases d'AuriviIlius...|$|E
40|$|As the {{development}} of animation industry, {{there are more and}} more requirements for the realistic rendering. Global illumination (GI) is an significant part of realistic rendering, and it has been focused by researchers for many years. Not only the lighting directly from the light sources but also the lighting reflected by other objects in the scene is required to be computed, so it's complicated to solve the GI problem that can be described as the rendering equation. Several algorithms can be used to solve this problem, such as Monte Carlo based ray tracing, photon mapping, many lights based approaches, point based global illumination and so on. This thesis is about Point Based Global Illumination (PBGI). PBGI is a popular rendering algorithm in movie and motion picture productions. This algorithm provides a diffuse global illumination solution by caching radiance in a mesh-less hierarchical data structure during a pre-process, while solving for the visibility over this cache, at rendering time, for each receiver, using a microbuffer, which is a localized depth and color buffer inspired from real time rendering environments. As a result, noise free ambient occlusion, indirect soft shadows and color bleeding effects are computed efficiently for high resolution image output and in a temporally coherent fashion. PBGI has attracted increasing attention nowadays because of its efficiency and noise free quality. However, there are still some problems, such as it can not simulation non-diffuse light transport, that makes it have limited applications. My thesis aims to solve these issues in PBGI and extend it to support more light transport path. Based on the spatial coherency, we propose a factorized solution of PBGI to make it more efficient by reusing the tree cut and the microbuffers. In PBGI, each receiver traverse the point cloud tree independently, but we observe that the similar receivers have the similar tree cut, that means there is redundancy during the traversal process. A similar model of the receivers is proposed at first, and then it is used to cluster the receivers. The point cloud tree is traversed for a cluster instead of for each receiver, and a cluster tree cut is obtained. The far nodes in the cluster tree cut are shared directly by all the receivers in this cluster without further traversing, while the near nodes are traversed for each receiver independently. A cluster microbuffer is proposed to solve for the visibility of the far nodes, while the receiver specific microbuffer is used to solve for the visibility of the near nodes and the refined nodes. The final microbuffer by combining these two microbuffers is convolved with the bidirectional reflection distribution funcion (BRDF) of the receiver to get the final indirect illumination. Our algorithm offers a significant rendering speed-up for a negligible and controllable approximation error and it inherits the temporal coherence of PBGI. We also propose a wavelet based solution to PBGI to compute the non-diffuse light transport. As only the diffuse lighting of the point is baked in PBGI, it can not simulate the non-diffuse light transport, such as caustics. PBGI tree nodes uses spherical harmonics (SH) to represent outgoing radiance. Unfortunately, even using a larger number of coefficients, SH are not able to capture high frequencies efficiently, which translates in our case to non-diffuse reflections or refractions. Consequently, caustics stemming from metals, plastics, glass and other reflective or refractive materials are not handled with classical PBGI frameworks. Even when ignoring the performance issue induced by a larger number of SH coefficients, ringing artifacts quickly appear. Compared with SH, haar wavelets support non-linear approximation, so the the representation is compact. So we propose to represent the outgoing radiance of the non-diffuse point with wavelet coefficients by sampling according to a cube map firstly and wavelet transforming each face of this cube map. The coefficients are further encoded hierarchically in the point cloud tree to decrease the memory usage, that means the coefficients themselves are wavelet transformed, generating two kinds of coefficients: node approximation coefficients and node detail coefficients. The node approximation coefficients are stored for the low level nodes (close to the root), and the node detail coefficients are stored for the high level nodes. To avoid storing the entire list of nodes vectors at any intermediate state, we compute this compressed representation during a post-order depth-first traversal of the PBGI tree. Further more, according to the artifacts problem that appears when there is high frequency BRDF or lighting in the scene, we propose to use the importance driven microbuffer. The importance function that includes the incoming lighting importance and the BRDF importance is used to drive the microbuffer, that means when one pixel has high frequency information (lighting or BRDF), it will be subdivided. Finally, our rendering algorithm allows to handle non-diffuse light transport, reproducing caustics with a similar quality to bidirectional path tracing for only a fraction of the computation time, with an intuitive control on the approximation error. Based on the previous two algorithms, we propose a view-tree based approach to compute the multiple bounces reflection. In PBGI, the indirect illumination of each point in the point cloud needs to be evaluated by traversing the point cloud tree and splatting the nodes in the tree-cut, so each point is treated a receiver. We propose to organize all the receivers into a view tree, and the point cloud tree is traversed for the view tree instead of for each receiver. The view tree approach is based on the observation some nodes in the point cloud tree contribute similarly to all the points in other nodes, that means we don¡¯t need to traverse for these points respectively but only for the node that contains these points. This is an extension to the factorized PGBI from one level (cluster) to a hierarchical structure (tree). Another problem for multiple bounces computation is how to evaluate the outgoing radiance from the incoming radiance efficiently. As the outgoing radiance needs to be computed for each sampling direction, the time complexity is O(n^ 4), where n× n represents the resolution of the hemisphere or square according to which the incoming direction and the outgoing direction are sampled. The wavelet representation is sparse that improves the performance, so we decide to wavelet transform the incoming radiance and the BRDF and multiply them in the frequency domain. We propose a novel outgoing radiance computation model by doing product between 4 D BRDF wavelet coefficients and 2 D incoming radiance wavelet coefficients. Finally, the point cloud tree with multiple bounces reflection stored is used to offer a preview rendering of the scene by utilizing the GPU computing efficiently. We can support scenes that include diffuse materials and all frequency glossy materials with a changing camera. My thesis improves and extends the PGBI algorithm so that it can be used in more applications. Comme le développement de l'industrie de l'animation, il ya de plus en plus d'exigences pour le rendu réaliste. L'illumination globale (IG) est une partie importante de rendu réaliste, et il a été porté par les chercheurs depuis de nombreuses années. Non seulement la lumière directement à partir des sources de lumière, mais aussi l'éclairage réfléchi par les autres objets de la scène doit être calculé, de sorte qu'il est compliqué de résoudre le problème de l'IG qui peut être décrit par l'équation de rendu. Plusieurs algorithmes peuvent être utilisés pour résoudre ce problème, comme le traçage basé Monte Carlo ray, le photon mapping, beaucoup de lumières approches fondées, le point sur la base de l'illumination globale et ainsi de suite. Cette thèse est basé sur des points sur Global Illumination (PBGI). PBGI est un algorithme de rendu populaire en cinéma et productions cinématographiques. Cet algorithme fournit une solution d'illumination globale diffuse en mettant en cache l'éclat dans une structure de données hiérarchique maille moins lors d'un pré-traitement, tout en résolvant pour la visibilité sur ce cache, au moment du rendu, pour chaque récepteur, en utilisant un microbuffer, qui est un localisée profondeur et tampon de couleurs inspirée des environnements de rendu en temps réel. En conséquence, le bruit ambiant sans occlusion, ombres douces indirects et des saignements de couleur effets sont calculés de manière efficace pour la sortie image haute résolution et d'une manière temporellement cohérente. PBGI a attiré une attention croissante de nos jours, en raison de sa qualité, sans l'efficacité et le bruit. Cependant, il ya encore quelques problèmes, comme il ne peut pas la simulation de transport de lumière non diffuse, ce fait avoir des applications limitées. Ma thèse vise à résoudre ces questions dans PBGI et l'étendre à soutenir chemin de transport de plus de lumière. Basé sur la cohérence spatiale, nous proposons une solution factorisée de PBGI à faire plus efficace en réutilisant la coupe des arbres et les microbuffers. Dans PBGI, chaque récepteur parcourir l'arborescence point de trouble indépendamment, mais nous observons que les récepteurs semblables ont la coupe d'arbre similaire, cela signifie qu'il ya une redondance au cours du processus de traversée. Un modèle similaire de récepteurs est proposé dans un premier temps, puis il est utilisé pour regrouper les récepteurs. L'arbre de nuage de points est traversé d'un cluster au lieu de pour chaque récepteur, et un arbre coupé de cluster est obtenu. Les nœuds loin dans l'arbre coupé du cluster sont partagés directement par tous les récepteurs de ce groupe sans autre déplacement, tandis que les nœuds proches sont traversés pour chaque récepteur indépendamment. Un microbuffer de cluster est proposé à résoudre pour la visibilité des nœuds loin, tandis que le microbuffer spécifique récepteur est utilisé à résoudre pour la visibilité des près de noeuds et les noeuds raffinés. Le microbuffer finale en combinant ces deux microbuffers est convoluée avec le funcion de distribution de réflexion bidirectionnelle (BRDF) du récepteur pour obtenir l'éclairage indirect finale. Notre algorithme offre un rendu accélération significative pour <b>une</b> <b>erreur</b> d'approximation négligeable et contrôlable et il hérite de la cohérence temporelle de PBGI. Nous proposons également une solution d'ondelettes basée à PBGI pour calculer les transport. As lumineuses non diffuses que l'éclairage diffus du point est cuit dans PBGI, il ne peut pas simuler la non-diffus transport léger, comme les caustiques. Nœuds d'arbre PBGI utilise des harmoniques sphériques (SH) pour représenter le rayonnement sortant. Malheureusement, même en utilisant un plus grand nombre de coefficients, SH ne sont pas en mesure de capturer efficacement les hautes fréquences, ce qui se traduit dans notre cas à des réflexions ou réfractions non-diffuses. Par conséquent, caustiques découlant de métaux, les plastiques, le verre et d'autres matériaux réfléchissants ou de réfraction ne sont pas traitées avec des cadres de PBGI classiques. Même lorsque ignorant la question de la performance induite par un plus grand nombre de coefficients SH, sonner artefacts apparaissent rapidement. Par rapport aux SH, les ondelettes de Haar supportent approximation non linéaire, de sorte que la représentation est compact. Donc, nous proposons de représenter le rayonnement sortant du point de non-diffuse avec des coefficients d'ondelettes par échantillonnage selon un cube map abord et ondelettes transformer chaque face de ce cube map. Les coefficients sont en outre codées hiérarchiquement dans l'arbre de nuages ​​de points pour réduire l'utilisation de mémoire, cela signifie que les coefficients sont eux-mêmes transformés en ondelettes, générer deux types de coefficients: noeud coefficients d'approximation et coefficients de détail de noeud. Les coefficients d'approximation noeud sont stockées pour les nœuds de bas niveau (à proximité de la racine), et les coefficients de détail de noeud sont stockées pour les nœuds de haut niveau. Pour éviter de stocker la liste complète des noeuds vecteurs à tout état intermédiaire, nous calculons cette représentation comprimé lors d'un post-order profondeur d'abord la traversée de l'arbre de PBGI. De plus, selon le problème des artefacts qui apparaît quand il ya BRDF haute fréquence ou de l'éclairage de la scène, nous proposons d'utiliser l'importance entraîné microbuffer. La fonction d'importance qui comprend l'importance de l'éclairage entrant et l'importance BRDF est utilisé pour entraîner le microbuffer, cela signifie quand un pixel dispose d'informations à haute fréquence (éclairage ou BRDF), elle sera découpée. Enfin, notre algorithme de rendu permet de gérer le transport de la lumière non diffuse, la reproduction caustiques avec une qualité similaire à la trajectoire bidirectionnelle traçage pour seulement une fraction du temps de calcul, avec un contrôle intuitif sur l'erreur d'approximation. Basé sur les deux algorithmes précédents, nous proposons une approche basée vue arbre pour calculer la rebonds réflexion multiple. Dans PBGI, l'éclairage indirect de chaque point dans le nuage de points doit être évaluée en parcourant l'arborescence point de trouble et splatting les nœuds de l'arbre-coupe, de sorte que chaque point est traité un récepteur. Nous proposons d'organiser tous les récepteurs dans un arbre de la vue, et l'arbre de nuage de points est traversée pour l'arbre de vue au lieu de pour chaque récepteur. L'approche de l'arbre de la vue est basée sur l'observation des noeuds de l'arbre de nuage de points contribuent de manière similaire à tous les points dans d'autres nœuds, cela signifie que nous ¡¯ t besoin de traverser pour ces points, respectivement, mais seulement pour le nœud qui contient ces points. Ceci est une extension de la PGBI factorisée d'un niveau (cluster) à une structure hiérarchique (arbre). Un autre problème pour de multiples rebonds calcul est comment évaluer le rayonnement sortant du rayonnement entrant efficacement. Comme le rayonnement sortant doit être calculé pour chaque direction d'échantillonnage, la complexité est en O (n ^ 4), où n times n représente la résolution de l'hémisphère ou carré selon laquelle le sens entrant et sortant direction sont échantillonnés. La représentation en ondelettes est clairsemée qui améliore la performance, donc nous avons décidé de la transformée en ondelettes le rayonnement entrant et la BRDF et les multiplier dans le domaine de fréquence. Nous proposons un modèle roman de calcul de rayonnement sortant en faisant produit entre 4 D BRDF coefficients d'ondelettes et 2 D radiance entrant coefficients d'ondelettes. Enfin, l'arbre de nuage de points avec de multiples rebonds réflexion stockée est utilisée pour offrir un rendu d'aperçu de la scène en utilisant efficacement le GPU computing. Nous pouvons soutenir scènes qui comprennent des matériaux diffuses et toutes les fréquences matériaux brillants avec une caméra changer. Ma thèse améliore et prolonge la PGBI algorithme de sorte qu'il peut être utilisé dans plusieurs applications...|$|E

