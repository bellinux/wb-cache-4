54|1|Public
500|$|For natural {{language}} plaintext, there will typically {{be only one}} plausible decryption, although for extremely short plaintexts, multiple candidates are possible. For example, the ciphertext MPQY could, plausibly, decrypt to either [...] "aden" [...] or [...] "know" [...] (assuming the plaintext is in English); similarly, [...] "ALIIP" [...] to [...] "dolls" [...] or [...] "wheel"; and [...] "AFCCP" [...] to [...] "jolly" [...] or [...] "cheer" [...] (see also <b>unicity</b> <b>distance).</b>|$|E
2500|$|Information theoretic {{concepts}} {{apply to}} cryptography and cryptanalysis. [...] Turing's information unit, the ban, {{was used in}} the Ultra project, breaking the German Enigma machine code and hastening the end of World War II in Europe. [...] Shannon himself defined an important concept now called the <b>unicity</b> <b>distance.</b> Based on the redundancy of the plaintext, it attempts to give a minimum amount of ciphertext necessary to ensure unique decipherability.|$|E
50|$|Ciphertexts {{greater than}} the <b>unicity</b> <b>distance</b> can be assumed to have only one {{meaningful}} decryption. Ciphertexts shorter than the <b>unicity</b> <b>distance</b> may have multiple plausible decryptions. <b>Unicity</b> <b>distance</b> is not {{a measure of how}} much ciphertext is required for cryptanalysis, but how much ciphertext is required for there to be only one reasonable solution for cryptanalysis.|$|E
40|$|Let P and Q be probabilities on the Borel [sigma]-algebra in a metric space (M, d). We {{prove that}} if the support of Q is finite and P verifies a certain {{continuity}} condition, then all the solutions of the Monge-Kantorovich mass transference problem between P and Q can be written as (X, H(X)) where X is any random element with distribution P and H only depends on P and Q. Monge-Kantorovich mass transference problem Wasserstein <b>distances</b> Mallows <b>distances</b> <b>unicity</b> metric spaces...|$|R
5000|$|... the intentionalredundancy of the {{convolutional}} coder {{reduces the}} <b>Unicity</b> <b>distance</b> of the encoded data and ...|$|E
5000|$|Claude Shannon {{defined the}} <b>unicity</b> <b>distance</b> in his 1949 paper [...] "Communication Theory of Secrecy Systems." ...|$|E
50|$|<b>Unicity</b> <b>distance</b> is {{a useful}} {{theoretical}} measure, but it doesn't say much about {{the security of a}} block cipher when attacked by an adversary with real-world (limited) resources. Consider a block cipher with a <b>unicity</b> <b>distance</b> of three ciphertext blocks. Although there is clearly enough information for a computationally unbounded adversary to find the right key (simple exhaustive search), this may be computationally infeasible in practice.|$|E
50|$|Another way to {{increase}} the <b>unicity</b> <b>distance</b> is {{to increase}} the number of possible valid sequences in the files as it is read. Since if for at least the first several blocks any bit pattern can effectively be part of a valid message then the <b>unicity</b> <b>distance</b> has not been reached. This is possible on long files when certain bijective string sorting permutations are used, such as the many variants of bijective Burrows-Wheeler transforms.|$|E
50|$|In both versions, the {{plaintext}} {{was first}} converted to digits {{by use of}} a straddling checkerboard rather than a Polybius square. This {{has the advantage of}} slightly compressing the plaintext, thus raising its <b>unicity</b> <b>distance</b> and also allowing radio operators to complete their transmissions quicker and shut down sooner. Shutting down sooner reduces the risk of the operator being found by enemy radio direction finders. Increasing the <b>unicity</b> <b>distance</b> increases strength against statistical attacks.|$|E
50|$|The <b>unicity</b> <b>distance</b> can {{equivalently}} {{be defined}} as the minimum amount of ciphertext required to permit a computationally unlimited adversary to recover the unique encryption key.|$|E
50|$|Basically {{the bigger}} the <b>unicity</b> <b>distance</b> the better. For a one time pad of {{unlimited}} size, given the unbounded entropy of the key space, we have , {{which is consistent with}} the one-time pad being unbreakable.|$|E
5000|$|... where U is the <b>unicity</b> <b>distance,</b> H(k) is {{the entropy of}} the key space (e.g. 128 for 2128 equiprobable keys, rather less if the key is a memorized pass-phrase). D {{is defined}} as the {{plaintext}} redundancy in bits per character.|$|E
50|$|Since M/N gets {{arbitrarily}} {{small as}} the length L {{of the message}} increases, there is eventually some L that {{is large enough to}} make the number of spurious keys equal to zero. Roughly speaking, this is the L that makes KM/N=1. This L is the <b>unicity</b> <b>distance.</b>|$|E
50|$|The <b>unicity</b> <b>distance</b> can be {{increased}} by reducing the plaintext redundancy. One {{way to do this}} is to deploy data compression techniques prior to encryption, for example by removing redundant vowels while retaining readability. This is a good idea anyway, as it reduces the amount of data to be encrypted.|$|E
5000|$|Cryptosystems often {{compress}} data (the [...] "plaintext") before encryption {{for added}} security. When properly implemented, compression greatly increases the <b>unicity</b> <b>distance</b> by removing patterns that might facilitate cryptanalysis. However, many ordinary lossless compression algorithms produce headers, wrappers, tables, or other predictable output that might instead make cryptanalysis easier. Thus, cryptosystems must utilize compression algorithms whose output {{does not contain}} these predictable patterns.|$|E
50|$|According to the <b>unicity</b> <b>distance</b> of English, 27.6 {{letters of}} {{ciphertext}} {{are required to}} crack a mixed alphabet simple substitution. In practice, typically about 50 letters are needed, although some messages can be broken with fewer if unusual patterns are found. In other cases, the plaintext can be contrived to have a nearly flat frequency distribution, and much longer plaintexts will then be required by the cryptanalyst.|$|E
50|$|Information theoretic {{concepts}} {{apply to}} cryptography and cryptanalysis. Turing's information unit, the ban, {{was used in}} the Ultra project, breaking the German Enigma machine code and hastening the end of World War II in Europe. Shannon himself defined an important concept now called the <b>unicity</b> <b>distance.</b> Based on the redundancy of the plaintext, it attempts to give a minimum amount of ciphertext necessary to ensure unique decipherability.|$|E
50|$|In cryptography, <b>unicity</b> <b>distance</b> is {{the length}} of an {{original}} ciphertext needed to break the cipher by {{reducing the number of}} possible spurious keys to zero in a brute force attack. That is, after trying every possible key, there should be just one decipherment that makes sense, i.e. expected amount of ciphertext needed to determine the key completely, assuming the underlying message has redundancy.|$|E
5000|$|For natural {{language}} plaintext, there will typically {{be only one}} plausible decryption, although for extremely short plaintexts, multiple candidates are possible. For example, the ciphertext MPQY could, plausibly, decrypt to either [...] "aden" [...] or [...] "know" [...] (assuming the plaintext is in English); similarly, [...] "ALIIP" [...] to [...] "dolls" [...] or [...] "wheel"; and [...] "AFCCP" [...] to [...] "jolly" [...] or [...] "cheer" [...] (see also <b>unicity</b> <b>distance).</b>|$|E
5000|$|A cipher whose key {{space is}} too small is subject to brute force attack with access to nothing but {{ciphertext}} by simply trying all possible keys. All that is needed is some way to distinguish valid plaintext from random noise, which is easily done for natural languages when the ciphertext is longer than the <b>unicity</b> <b>distance.</b> One example is DES, which only has 56-bit keys. All too common current examples are commercial security products that derive keys for otherwise impregnable ciphers like AES from a user-selected password. Since users rarely employ passwords with anything close to the [...] entropy of the cipher's key space, such systems are often quite easy to break in practice using only ciphertext. The 40-bit CSS cipher used to encrypt DVD video discs can always be broken with this method, as {{all that is needed}} is to look for MPEG-2 video data.|$|E
30|$|The <b>unicity</b> <b>distance</b> {{was defined}} in [34] {{as the amount}} of intercepted {{cryptogram}}s by the eavesdropper beyond which the eavesdropper can deduce the key. Equivalently, the cryptogram is undecipherable when the message length is less than the <b>unicity</b> <b>distance</b> [34]. So, to achieve information theoretic security the message should be enciphered by a new key before the eavesdropper gathers enough cryptograms for deciphering. To secure an entire OFDM transmission burst, the number of joint symbols within the channel decorrelation period must be less than the <b>unicity</b> <b>distance.</b>|$|E
40|$|We {{show that}} the {{estimate}} of the <b>unicity</b> <b>distance</b> for ciphertext-only attacks on the Y- 00 (or αη) cryptosystem key given by C. Ahn and K. Birnbaum in Phys. Lett. A 370 (2007) 131 - 135 is, at best, a lower bound. We also {{show that the}}ir unfavorable security comparison between Y- 00 and an additive stream cipher for ciphertext-only, statistical, and known-plaintext attacks is based on invalid extrapolations of Shannon’s random cipher analysis to a concrete cipher. We prove a general necessary and sufficient insecurity condition applicable both to αη and to ordinary ciphers that reveals that the arguments of Ahn and Birnabaum provide only lower bounds on the <b>unicity</b> <b>distance</b> against all these attacks that are not generally correct approximate estimates. We show that, strictly speaking, the <b>unicity</b> <b>distance</b> for any type of heterodyne attack on αη is infinite, so that any <b>unicity</b> <b>distance</b> estimate {{must be accompanied by}} an estimate of the probability with which the system is broken at that distance...|$|E
30|$|Consequently, we {{estimate}} the key-dependent JPEG 2000 PBHash {{to have a}} rather large <b>unicity</b> <b>distance.</b>|$|E
40|$|Key size {{becomes very}} {{important}} to a cryptographic algorithm according to Kerckhoffs law where a civilian cryptosystem shall depend fully on key secrecy. Currently, there are four passphrase generation methods: Sentence, acronym, diceware, and coinware. <b>Unicity</b> <b>distance</b> is the minimum size of ciphertext for unique decipherability of ciphertext when number of spurious keys is zero. A key with size less than <b>unicity</b> <b>distance</b> is good where there are spurious keys which allow a protection method using limited unsuccessful logins. Here, stronger forms of passphrases using textual semantic noises like punctuation marks, mnemonic substitution, misspelling, and associative morphing, which improve the key entropy, are proposed An ASCII mutual substitution table is presented together with its proof on information rate increment. Higher information rate has lower redundancy, and hence bigger <b>unicity</b> <b>distance</b> ensures encrypted keys the short cryptogram in a key vault, like Password Safe, cannot be cryptanalyzed within certain limited login attempts. Document Type: Proceedings Pape...|$|E
40|$|An image hash is a {{randomized}} compact representation of image content and finds applications in image authentication, image and video watermarking, and image similarity comparison. Usually, an image-hashing scheme {{is required to}} be robust and secure, and the security issue is particularly important in applications, such as multimedia authentication, watermarking, and fingerprinting. In this paper, we investigate the security of image hashing {{from the perspective of}} <b>unicity</b> <b>distance,</b> a concept pioneered by Shannon in one of his seminal papers. Using two recently proposed image-hashing schemes as representatives, we show that the concept of <b>unicity</b> <b>distance</b> can be adapted to evaluate the security of image hashing. Our analysis shows that the secret hashing key, or its equivalent form, can be estimated with high accuracy when the key is reused several dozen times. The estimated <b>unicity</b> <b>distance</b> determines the maximum number of key reuses in the investigated hashing schemes. A countermeasure of randomized key initialization is discussed to avoid key reuse and strengthen the security of robust image hashing...|$|E
3000|$|... [...]. We derive the <b>unicity</b> <b>distance</b> for JCMA {{based on}} this single assumption. For clarity of {{presentation}} we consider the case where a single bit is transmitted by each transmitter ([...] [...]...|$|E
3000|$|Secrecy is {{required}} for the entire data transmission burst. This means that the <b>unicity</b> <b>distance</b> must be larger than the number of received joint symbols during the channel decorrelation period ([...] [...]...|$|E
30|$|Recently, {{a method}} for {{measuring}} the security of robust image hashing algorithms has been proposed [16]. It is based on <b>unicity</b> <b>distance,</b> a concept pioneered by Shannon [43] in 1949, which states {{that the amount of}} uncertainty in an encryption key reduces with each observed clear-text and cipher-text pair. This means for image hashing, that the secret key can be estimated when the key is reused multiple times on different input images. In this case, the <b>unicity</b> <b>distance</b> of a hashing scheme determines how often (i.e., for how many different images) a key can be reused, before it can be uniquely determined.|$|E
3000|$|... [...]. It {{follows that}} for the given {{scenario}} the eavesdropper cannot effectively decode the messages from the nodes, even when the <b>unicity</b> <b>distance</b> has passed and security factors 1 – 3 described in Section 4.2 are compromised.|$|E
3000|$|... range), {{and we will}} {{determine}} the sensitivity against key modifications for the scheme in more detail to provide an approximation for an actual <b>unicity</b> <b>distance</b> value. In particular, we will investigate possibilities {{how to make the}} key-estimation procedure separable, that is, conduct key estimation for each decomposition level separately.|$|E
40|$|Shannon {{presented}} the concept `unicity distance' for describing {{the security of}} secret key encryption protocols against various ciphertext-only attacks. We develop this important concept of cryptanalysis into the quantum context, and find that there exist quantum encryption protocols reusing a short key have infinite <b>unicity</b> <b>distance.</b> Comment: 20 page...|$|E
3000|$|... {{conditional}} entropy will decrease. To determine the <b>unicity</b> <b>distance</b> of the image-hashing algorithm, the observed image-hash pairs are taken as the input to a key estimation algorithm. The output of this algorithm (i.e., the estimated secret key) is gradually refined {{with the increased}} number of observed image-hash pairs. It is expected that the estimated key gets {{closer and closer to}} the actual key [...]...|$|E
40|$|Newspaper cryptograms {{are usually}} easy to solve. They may get hard when they 2 ̆ 7 re made of words with unusual letter statistics. But {{no matter how}} hard they are, when you solve them, you can be sure you got the right answer. Cryptograms longer than about 30 letters (known to codebreakers as the 2 ̆ 7 <b>unicity</b> <b>distance</b> 2 ̆ 7) just don 2 ̆ 7 t have two solutions. Or do they...|$|E
40|$|In 1949, Shannon {{published}} the paper ”Communication theory of secrecy systems”. This constituted a foundational treatment {{and analysis of}} encryption systems. He transferred the methods of information theory, originally developed as a mathematical model for communication over ”noisy ” channels to the setting of cryptosystems. We give a brief introduction into his most outstanding ideas, such as the notions of perfect/provable security, and statistical/information-theoretic concepts like entropy, key equivocation, and <b>unicity</b> <b>distance...</b>|$|E
40|$|Steganography {{is about}} how to send secret message covertly. And the purpose of {{steganalysis}} is to not only detect {{the existence of the}} hidden message but also extract it. So far there have been many reliable detecting methods on various steganographic algorithms, while there are few approaches that can extract the hidden information. In this paper, the difficulty of extracting hidden information, which is essentially a kind of privacy, is analyzed with information-theoretic method in the terms of <b>unicity</b> <b>distance</b> of steganographic key (abbreviated stego key). A lower bound for the <b>unicity</b> <b>distance</b> is obtained, which shows the relations between key rate, message rate, hiding capacity and difficulty of extraction. Furthermore the extracting attack to steganography is viewed as a special kind of cryptanalysis, and an effective method on recovering the stego key of popular LSB replacing steganography in spatial images is presented by combining the detecting technique of steganalysis and correlation attack of cryptanalysis together. The analysis for this method and experimental results on steganographic software ``Hide and Seek 4. 1 " are both accordant with the information-theoretic conclusion. Comment: 8 pages, 3 figure...|$|E
30|$|We {{discuss a}} robust image {{authentication}} {{scheme based on}} a hash string constructed from leading JPEG 2000 packet data. Motivated by attacks against the approach, key-dependency is added by means of employing a parameterized lifting scheme in the wavelet decomposition stage. Attacks can be prevented effectively in this manner and {{the security of the}} scheme in terms of <b>unicity</b> <b>distance</b> is assumed to be high. Key-dependency however can lead to reduced sensitivity of the scheme. This effect has to be compensated by an increase of the hash length which in turn decreases robustness.|$|E
