171|67|Public
50|$|Lanczos {{resampling}} {{is based}} on a windowed sinc function as a practical upsampling filter approximating the ideal sinc function. Lanczos resampling is widely used in video <b>up-sampling</b> for digital zoom applications.|$|E
50|$|The {{objective}} {{model is}} tested {{on a wide}} variety of different frame-rates as used in TV applications (29.97 fps and 25 fps), in Interlaced video and Progressive scan mode at the resolution 1920⨉1080. Content of 1280⨉720 was included in testing by <b>up-sampling</b> it to 1920⨉1080, as this is the typical case for most consumer applications. Content with 24 fps was included in testing but re-played at 25 fps.|$|E
5000|$|The contourlet {{transform}} has {{a number}} of useful features and qualities, but it also has its flaws. One of the more notable variations of the contourlet transform was developed and proposed by da Cunha, Zhou and Do in 2006. The nonsubsampled contourlet transform (NSCT) was developed mainly because the contourlet transform is not shift invariant. [...] The reason for this lies in the <b>up-sampling</b> and down-sampling present in both the Laplacian Pyramid and the directional filter banks. The method used in this variation was inspired by the nonsubsampled wavelet transform or the stationary wavelet transform which were computed with the à trous algorithm.|$|E
3000|$|... {{and then}} <b>up-sample</b> it using bi-cubic {{interpolation}} {{by the same}} factor of s to obtain the low-frequency band image Y_l∈R^K. Then, we <b>up-sample</b> X [...]...|$|R
30|$|In {{order to}} carry out the {{bi-directional}} sub-pixel prediction {{it is not necessary to}} <b>up-sample</b> the reference frames, because they have been previously <b>up-sampled</b> before applying the forward and backward predictions.|$|R
5000|$|Up-sampling: {{the color}} bands are <b>up-sampled</b> {{to the same}} {{resolution}} as the panchromatic band; ...|$|R
50|$|Handwriting {{movements}} are fast, non-repetitive with a primary frequency around 5 Hz and a bandwidth of about 10 Hz. While sampling rates of 20 Hz would theoretically suffice, <b>up-sampling</b> {{will be needed}} to properly visualize the Lissajous-like handwriting and drawing strokes. Higher-than-necessary sampling rates such as 100 Hz are preferred as this would also allow low-pass filtering or smoothed data with reduced equipment and quantization noise by factor √(100/20) = √5. A laboratory computer {{will be needed to}} store, process, and visualize massive amounts of samples. It took more than 50 years for computers to be available in laboratories. Electronic analog computers were used until digital computers came within reach for research: Wang Laboratories, Digital Equipment Corporation (DEC), Apple Inc., IBM PC (Personal Computer), Norsk Data, Atari, Osborne Computer Corporation, and Data General. Sadly, most of these innovative mini and microcomputer companies have discontinued their operation.|$|E
5000|$|... 2006 {{was another}} key {{year in the}} {{development}} of the Cambridge Audio brand with the introduction of the Azur 840A integrated amplifier, 840E & W pre and power amplifiers and the 840C CD player, the first products that the company described as ‘affordable’ hi-end audio. [...] The 840A integrated amplifier also introduced Cambridge Audio’s unique and patented Crossover Displacement (Class-XD Amplifier) design which combined the performance of a Class A design with the efficiency of a class AB. The technology would go-on to help the 840A be selected as the 2007/2008 two-channel amplifier of the year at the prestigious EISA Awards. The 840C was the first product to feature another new technology that Cambridge Audio had developed with Swiss-based Anagram Technologies, Adaptive Time Filtering (ATF) <b>up-sampling.</b> [...] ATF up samples any digital signal - such as CD’s 16bit 44.1 kHz - to a 24bit, 384 kHz signal which presents a much more accurate audio soundwave. [...] Both Class XD and ATF remain proprietary technologies only found on Cambridge Audio hi-end products.|$|E
50|$|An {{alternative}} or {{variation of}} the contourlet transform was proposed by Lu and Do in 2006. This new proposed method {{was intended as a}} remedy to fix non-localized basis images in frequency. The issue with the original contourlet transform was that when the contourlet transform was used with imperfect filter bank filters aliasing occurs and the frequency domain resolution is affected. There are two contributing factors to the aliasing, the first is the periodicity of 2-D frequency spectra and the second is an inherent flaw in the critical sampling of the directional filter banks. This new method mitigates these issues by changing the method of multiscale decomposition. As mentioned before, the original contourlet used the Laplacian Pyramid for multiscale decomposition. This new method as proposed by Lu and Do uses a multiscale pyramid that can be adjusted by applying low pass or high pass filters for the different levels. This method fixes multiple issues, it reduces the amount of cross terms and localizes the basis images in frequency, removes aliasing and has proven in some instances more effective in denoising images. Though it fixes all of those issues, this method requires more filters than the original contourlet transform and still has both the <b>up-sampling</b> and down-sampling operations meaning it is not shift-invariant.|$|E
5000|$|In {{the above}} diagram, filters in each level are <b>up-sampled</b> {{versions}} of the previous (see figure below).|$|R
3000|$|R is {{the full}} {{resolution}} frames {{in the right}} view. Disparity estimation is employed between the <b>up-sampled</b> I [...]...|$|R
5000|$|Forward transform: the <b>up-sampled</b> color {{bands are}} {{transformed}} to an alternate color space (where intensity is orthogonal {{to the color}} information); ...|$|R
40|$|In a {{scalable}} video coding (SVC), spatial <b>up-sampling</b> {{of video}} sequences is an important process for spatial scalability. We propose an <b>up-sampling</b> method based on the type- 1 DCT. For further improvement of the upsampling method, we introduce an adaptive filtering method, which applies different weighting parameters to DCT coefficients. The proposed adaptive <b>up-sampling</b> method shows much improved PSNR {{in comparison with the}} recent H. 264 SVC <b>up-sampling</b> method. Index Terms — Adaptive filters, DCT, video codecs. 1...|$|E
30|$|R {{to obtain}} the {{disparity}} map. Considering the tradeoff of the <b>up-sampling</b> quality and the computational complexity, we select the bicubic interpolation as the <b>up-sampling</b> method. The disparity estimation in this method utilizes normalized mutual information (NMI) algorithm based on adaptive support weighting (ASW).|$|E
30|$|Quarter-pel <b>up-sampling</b> - To {{provide a}} more {{accurate}} estimation for the possible perspective deformations, the backward and forward reference frames are first up-sampled with the H. 264 /AVC quarter-pel <b>up-sampling</b> filter [1]; this is performed so that the next steps can benefit from increased precision reference frames.|$|E
5000|$|Output {{sampling}} frequency of 44.1 kHz (it can also accept 22.05-kHz samples - they are <b>up-sampled</b> to 44.1 kHz before output) ...|$|R
50|$|Clocking {{flexibility}} {{allows the}} McASP to receive and transmit at different rates. For example, the McASP can receive data at 48 kHz but output <b>up-sampled</b> data at 96 kHz or 192 kHz.|$|R
5000|$|Alignment: the <b>up-sampled</b> color {{bands and}} the {{panchromatic}} band are aligned to reduce artifacts due to mis-registration (generally, when the data {{comes from the}} same sensor, this step is usually not necessary); ...|$|R
40|$|Image <b>up-sampling</b> {{is found}} to be a very {{effective}} technique useful in today’s digital image processing applications or rendering devices. In image upsampling, an image is enhanced from a lower resolution to a higher resolution with the degree of enhancement depending upon application requirements. It is known that the traditional interpolation based approaches for <b>up-sampling,</b> such as Bilinear or Bicubic interpolation, blur the resultant images [1, 2]. Furthermore; in color imagery, these interpolation based <b>up-sampling</b> methods may have color infringing artifacts in the areas where the images contain sharp edges and fine textures. In this paper, we present an interesting <b>up-sampling</b> algorithm using 9 / 7 bi-orthogonal Spline filters based Discrete Wavelet Transform (DWT). The proposed method preserves much of the sharp edge features in the image, and lessens the amount of color artifacts. Effectiveness of the proposed algorithm has been demonstrated based on evaluation of PSNR and � Eab quality metrics of the original image and the reconstructed image...|$|E
40|$|Inspired by {{the recent}} {{advances}} of image super-resolution using convolutional neural network (CNN), we propose a CNN-based block <b>up-sampling</b> scheme for intra frame coding. A block can be down-sampled before being compressed by normal intra coding, and then up-sampled to its original resolution. Different from previous studies on down/up-sampling-based coding, the <b>up-sampling</b> methods in our scheme have been designed by training CNN instead of hand-crafted. We explore a new CNN structure for <b>up-sampling,</b> which features deconvolution of feature maps, multi-scale fusion, and residue learning, making the network both compact and efficient. We also design different networks for the <b>up-sampling</b> of luma and chroma components, respectively, where the chroma <b>up-sampling</b> CNN utilizes the luma information to boost its performance. In addition, we design a two-stage <b>up-sampling</b> process, the first stage being within the block-by-block coding loop, and the second stage being performed on the entire frame, so as to refine block boundaries. We also empirically study how to set the coding parameters of down-sampled blocks for pursuing the frame-level rate-distortion optimization. Our proposed scheme is implemented into the High Efficiency Video Coding (HEVC) reference software, and a comprehensive set of experiments have been performed to evaluate our methods. Experimental results show that our scheme achieves significant bits saving compared with HEVC anchor especially at low bit rates, leading to on average 5. 5 % BD-rate reduction on common test sequences and on average 9. 0 % BD-rate reduction on ultra high definition (UHD) test sequences. Comment: Accepted by IEEE Transactions on Circuits and Systems for Video Technolog...|$|E
40|$|Abstract — Unorganized point clouds {{obtained}} from 3 D shape acquisition devices usually present noises, outliers, and non-uniformities. In this article, we propose a framework to consoli-date unorganized points by an iterative procedure of interlaced down-sampling and <b>up-sampling</b> steps. After down-sampling and <b>up-sampling,</b> selection operations are conducted to remove outliers while preserving geometric details. The uniformity of points is improved {{by moving the}} down-sampled particles and the following refinement of point samples, and the missed regions are filled through surface extrapolation. Moreover, an adaptive sampling strategy is employed {{to speed up the}} iterations. Ex-perimental results demonstrate the effectiveness of the proposed point processing framework. Index Terms — Unorganized point clouds, repulsion operator, outlier removal, down-sampling, <b>up-sampling.</b> I...|$|E
3000|$|..., one {{for each}} <b>up-sampled</b> {{reference}} frame (backward and forward). The two warpings, W, obtained for each transform, correspond to two sets of perspective transform vectors, one set of vectors pointing from the WZ frame to X [...]...|$|R
40|$|Abstract — We {{consider}} the warping problem which appears in well-known image registration algorithms that use higher-order motion models. An implementation inspired by recent {{work and a}} new image registration algorithm are used for the analysis. Both approaches rely on frame-to-frame estimation. The key technique is the well-established gradient descent approach for the estimation of higher-order motion parameters. We show that using <b>up-sampled</b> input images in the last step of the algorithms improves {{the accuracy of the}} estimated motion parameters. It {{can be seen in the}} experimental results that the performance of the image registration algorithms increases significantly only by applying the gradient descent on <b>up-sampled</b> images in comparison to recent algorithms developed. I...|$|R
50|$|On the Apple TV (2nd generation), {{digital output}} audio is <b>up-sampled</b> to 48 kHz, {{including}} lossless CD rips at 44.1 kHz. Although {{this is a}} higher frequency and the difference is not audible in most cases, it means the audio is not 'bit perfect' which is often a goal for digital transmission of data.|$|R
40|$|<b>Up-sampling</b> and down-sampling are the {{two most}} used methods in {{balancing}} the data when dealing with two class imbalance problem. However, none of the existing approaches to class rebalance take into account class information (e. g. distribution, within and between class distances, imbalance factor). This study presents initial results of <b>up-sampling</b> methods based on various approaches to aggregation of class information such as spread, imbalance factor and the distance between the classes. Artificially generated data are used for experiments. The performance of each <b>up-sampling</b> method is evaluated with respect to how well the resulting data set reflects the underlying original class distribution, in terms of mean, standard deviation and (empirical) distribution function...|$|E
30|$|Step 6 : The reconstructed {{residual}} signal errrec {{is produced}} by applying the inverse WT and <b>up-sampling</b> process {{by a factor of}} two, respectively.|$|E
30|$|Before {{starting}} with sub-pixel prediction, the frames must be up-sampled to half-pixel accuracy {{by means of}} a 6 -tap filter, and then they are up-sampled to quarter-pixel accuracy {{by means of a}} bilinear filter. The <b>up-sampling</b> is carried out on the GPU because the frames are transferred to the GPU memory with full-pixel accuracy. It is faster to generate the frames with quarter-pixel accuracy than to transfer them. Three GPU kernels are needed since there are dependencies in the <b>up-sampling</b> process.|$|E
50|$|A CIC filter {{consists}} {{of one or}} more integrator and comb filter pairs. In the case of a decimating CIC, the input signal is fed through one or more cascaded integrators, then a down-sampler, followed by one or more comb sections (equal in number to the number of integrators). An interpolating CIC is simply the reverse of this architecture, with the down-sampler replaced with a zero-stuffer (<b>up-sampler).</b>|$|R
30|$|In the decoder part, four GICN {{structures}} are respectively {{connected to the}} corresponding decoder features, where the GICN- 1 mapping with Res- 2, GICN- 2 mapping with Res- 3, and so on. The GICN- 1 feature layer is composed of 32 128 [*]×[*] 128 feature maps, and the GICN- 2, GICN- 3, and GICN- 4 are all composed of 32 64 [*]×[*] 64 feature maps, in which it used the transpose convolution to <b>up-sample</b> with the kernel size of 3 [*]×[*] 3 and the strides 2. Deconv- 1 outputs 32 128 [*]×[*] 128 feature maps. The features consisting of the DeConv- 1 outputs and GICN- 1 used transpose convolution to <b>up-sample</b> with convolution kernel size of 3 [*]×[*] 3 and stride size of 2. In the Deconv- 2 output 32 256 [*]×[*] 256 features, the network generate the feature map with the same size of original image by using transpose convolution to the DeConv- 2 output {{with the size of}} 2 [*]×[*] 2, and stride of 2. At this point, we completed the multi-level feature pyramid feature extraction.|$|R
40|$|High-refresh-rate {{displays}} (e. g., 120 Hz) {{have recently}} become {{available on the}} consumer market and quickly gain on popularity. One of their aims {{is to reduce the}} perceived blur created by moving objects that are tracked by the human eye. However, an improvement is only achieved if the video stream is produced at the same high refresh rate (i. e. 120 Hz). Some devices, such as LCD TVs, solve this problem by converting low-refresh-rate content (i. e. 50 Hz PAL) into a higher temporal resolution (i. e. 200 Hz) based on two-dimensional optical flow. In our approach, we will show how rendered three-dimensional images produced by recent graphics hardware can be <b>up-sampled</b> more efficiently resulting in higher quality at the same time. Our algorithm relies on several perceptual findings and preserves the naturalness of the original sequence. A psychophysical study validates our approach and illustrates that temporally <b>up-sampled</b> video streams are preferred over the standard low-rate input by the majority of users. We show that our solution improves task performance on high-refresh-rate displays...|$|R
40|$|Depth map {{images are}} {{characterized}} by large homogeneous areas and strong edges. It has been observed that efficient compression of the depth map is achieved by applying a down-sampling operation prior to encoding. However, since high resolution depth maps are typically required for view synthesis, an <b>up-sampling</b> method that is able to recover the loss of information is needed within this framework. In this paper, an <b>up-sampling</b> algorithm that recovers the high frequency content of depth maps using a novel edge layer concept is proposed. This algorithm includes a method for extracting edge layers from the corresponding texture images, which are then used {{as part of a}} non-linear interpolation filter for depth map upsampling. In the present work, the <b>up-sampling</b> is applied as a post-processing operation to generate multiview output for display. Views synthesized with our up-sampled depth maps show the efficiency of our proposed technique relative to conventional interpolation filters...|$|E
30|$|Fourier {{transforming}} (FT) the de-rotated {{signals in}} the azimuth direction. It {{should be noted}} that <b>up-sampling</b> in azimuth may be required in the spotlight mode to avoid the Doppler spectrum aliasing.|$|E
40|$|Image <b>up-sampling</b> in the {{discrete}} cosine transform (DCT) domain is {{a challenging}} problem because DCT coefficients are de-correlated, {{such that it}} is nontrivial to estimate directly high-frequency DCT coefficients from observed low-frequency DCT coefficients. In the literature, DCT-based <b>up-sampling</b> algorithms usually pad zeros as high-frequency DCT coefficients or estimate such coefficients with limited success mainly due to the nonadaptive estimator and restricted information from a single observed image. In this paper, we tackle the problem of estimating high-frequency DCT coefficients in the spatial domain by proposing a learning-based scheme using an adaptive k-nearest neighbor weighted minimum mean squares error (MMSE) estimation framework. Our proposed scheme makes use of the information from precomputed dictionaries to formulate an adaptive linear MMSE estimator for each DCT block. The scheme is able to estimate high-frequency DCT coefficients with very successful results. Experimental {{results show that the}} proposed <b>up-sampling</b> scheme produces the minimal ringing and blocking effects, and significantly better results compared with the state-of-the-art algorithms in terms of peak signal-to-noise ratio (more than 1 dB), structural similarity, and subjective quality measurements. Department of Electronic and Information Engineerin...|$|E
40|$|We {{present a}} novel {{approach}} to the removal of artifacts generated by deep brain stimulation equipment in neural recordings. We show {{it is possible to}} free a recording of its stimulation artifacts by under-sampling the data with a scheme which does not include any of the unwanted artifacts: using cubic-spline interpolation to <b>up-sample</b> the resulting signal. The whole process has been tested using simulated data and recordings made during periods of deep brain stimulation...|$|R
40|$|Abstract — This paper {{proposes a}} new super-resolution (SR) scheme for {{landmark}} images by retrieving correlated web images. Using correlated web images significantly improves the exemplar-based SR. Given a low-resolution (LR) image, we extract local descriptors from its <b>up-sampled</b> version and bundle the descrip-tors {{according to their}} spatial relationship to retrieve correlated high-resolution (HR) images from the web. Though similar in content, the retrieved images are usually taken with different illumination, focal lengths, and shot perspectives, resulting in uncertainty for the HR detail approximation. To solve this problem, we first propose aligning these images to the <b>up-sampled</b> LR image through a global registration, which identifies the cor-responding regions in these images and reduces the mismatching. Second, we propose a structure-aware matching criterion and adaptive block sizes to improve the mapping accuracy between LR and HR patches. Finally, these matched HR patches are blended together by solving an energy minimization problem to recover the desired HR image. Experimental results demonstrate that our SR scheme achieves significant improvement compared with four state-of-the-art schemes {{in terms of both}} subjective and objective qualities. Index Terms — Super-resolution, image retrieval, web image, hallucination, landmark image. I...|$|R
40|$|Dense {{disparity}} map estimation from a high-resolution stereo {{image is}} a very difficult problem in terms of both matching accuracy and computation efficiency. Thus, an exhaustive disparity search at full resolution is required. In general, examining more pixels in the stereo view results in more ambiguous correspondences. When a high-resolution image is down-sampled, the high-frequency components of the fine-scaled image are at risk of disappearing in the coarse-resolution image. Furthermore, if erroneous disparity estimates caused by missing high-frequency components are propagated across scale space, ultimately, false disparity estimates are obtained. To solve these problems, we introduce an efficient hierarchical stereo matching method in two-scale space. This method applies disparity estimation to the reduced-resolution image, and the disparity result is then <b>up-sampled</b> to the original resolution. The disparity estimation values of the high-frequency (or edge component) regions of the full-resolution image are combined with the <b>up-sampled</b> disparity results. In this study, we extracted the high-frequency areas from the scale-space representation by using difference of Gaussian (DoG) or found edge components, using a Canny operator. Then, edge-aware disparity propagation was used to refine the disparity map. The experimental results show that the proposed algorithm outperforms previous methods...|$|R
