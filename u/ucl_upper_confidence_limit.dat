0|5518|Public
40|$|This report {{provides}} a general method of determining <b>upper</b> <b>confidence</b> <b>limits</b> {{for the failure}} probability of any combination of components when the failure history of the individual components is known. The assumptions made are that failures are independent and follow a binomial distribution. The relation between systems of <b>upper</b> <b>confidence</b> <b>limits</b> and operating characteristic curves for acceptance testing by attributes is also described. "Sandia Corporation contractors for U. S. Atomic Energy Commission" [...] Cover. "Systems Analysis. ""December 1957. "Includes bibliographical references (p. 36). This report {{provides a}} general method of determining <b>upper</b> <b>confidence</b> <b>limits</b> for the failure probability of any combination of components when the failure history of the individual components is known. The assumptions made are that failures are independent and follow a binomial distribution. The relation between systems of <b>upper</b> <b>confidence</b> <b>limits</b> and operating characteristic curves for acceptance testing by attributes is also described. Mode of access: Internet. This bibliographic record is available under the Creative Commons CC 0 public domain dedication. The University of Florida Libraries, as creator of this bibliographic record, has waived all rights to it worldwide under copyright law, including all related and neighboring rights, to the extent allowed by law...|$|R
40|$|On {{the basis}} of a {{negative}} binomial sampling scheme, we consider a uniformly most accurate <b>upper</b> <b>confidence</b> <b>limit</b> for a small but unknown proportion, such as the proportion of defectives in a manufacturing process. The optimal stopping rule, with reference to the twin criteria of the expected length of the confidence interval and the expected sample size, is investigated. The proposed confidence interval has also been compared with several others that have received attention in the recent literature. expected length, posterior quantile, negative binomial, sample size, score interval, uniformly most accurate, <b>upper</b> <b>confidence</b> <b>limit,...</b>|$|R
40|$|The Buehler 1 -[alpha] <b>upper</b> <b>confidence</b> <b>limit</b> is {{as small}} as possible, subject to the {{constraints}} that (a) its coverage probability never falls below 1 -[alpha] and (b) it is a non-decreasing function of a designated statistic T. We provide two new results concerning the influence of T on the efficiency of this <b>confidence</b> <b>limit.</b> Firstly, we extend the result of Kabaila (Statist. Probab. Lett. 52 (2001) 145) to prove that, for a wide class of Ts, the T which maximizes the large-sample efficiency of this <b>confidence</b> <b>limit</b> is itself an approximate 1 -[alpha] <b>upper</b> <b>confidence</b> <b>limit.</b> Secondly, there may be ties among the possible values of T. We provide the result that breaking these ties by a sufficiently small modification cannot decrease the finite-sample efficiency of the Buehler <b>confidence</b> <b>limit.</b> <b>Confidence</b> <b>upper</b> <b>limit</b> Reliability Biostatistics Discrete data Nuisance parameter...|$|R
40|$|Consider the {{reliability}} problem {{of finding a}} 1 -[alpha] <b>upper</b> (lower) <b>confidence</b> <b>limit</b> for [theta] the probability of system failure (non-failure), based on binomial data on the probability of failure of each component of the system. The Buehler 1 -[alpha] <b>confidence</b> <b>limit</b> is usually based on an estimator of [theta]. This <b>confidence</b> <b>limit</b> has the desired coverage properties. We prove that in large samples the Buehler 1 -[alpha] <b>upper</b> <b>confidence</b> <b>limit</b> based on an approximate 1 -[alpha] upper limit for [theta] is less conservative, whilst also possessing the desired coverage properties. Reliability <b>Confidence</b> <b>limit</b> Discrete data...|$|R
30|$|The average larval {{mortality}} {{data were}} subjected to probit analysis for calculating LC 50, LC 90, and other statistics at 95 % <b>confidence</b> <b>limits</b> of <b>upper</b> <b>confidence</b> <b>limit</b> and lower <b>confidence</b> <b>limit</b> and Chi-square values were calculated using the SPSS 11.5 (Statistical Package of Social Sciences) software. Results with P[*]<[*] 0.05 {{were considered to be}} statistically significant.|$|R
40|$|Competitive {{limits on}} the weakly {{interacting}} massive particle (WIMP) spin-independent scattering cross section are currently being produced by 76 Ge detectors originally designed to search for neutrinoless double beta decay, such as the Heidelberg-Moscow and IGEX experiments. In the absence of background subtraction, {{limits on the}} WIMP interaction cross section are set by calculating the <b>upper</b> <b>confidence</b> <b>limit</b> on the theoretical event rate, given the observed event rate. The standard analysis technique involves calculating the 90 % <b>upper</b> <b>confidence</b> <b>limit</b> {{on the number of}} events in each bin, and excluding any set of parameters (WIMP mass and cross-section) which produces a theoretical event rate for any bin which exceeds the 90 % <b>upper</b> <b>confidence</b> <b>limit</b> on the event rate for that bin. We show that, if {{there is more than one}} energy bin, this produces exclusion limits that are actually at a lower degree of confidence than 90 %, and are hence erroneously tight. We formulate criteria which produce true 90 % <b>confidence</b> exclusion <b>limits</b> in these circumstances, including calculating the individual bin <b>confidence</b> <b>limit</b> for which the overall probability that no bins exceeds this <b>confidence</b> <b>limit</b> is 90 % and calculating the 90 % minimum <b>confidence</b> <b>limit</b> on the number of bins which exceed their individual bin 90 % <b>confidence</b> <b>limits.</b> We then compare the limits on the WIMP cross-section produced by these criteria with those found using the standard technique, using data from the Heidelberg-Moscow and IGEX experiments. Comment: 6 pages, 3 figures, 3 tables, shortened version to appear in Phys. Rev. D, contents otherwise unchange...|$|R
30|$|The percent {{mortality}} was observed, and {{the average}} mortality data were subjected to Probit analysis for calculating LC 50 and, lower and <b>upper</b> <b>confidence</b> <b>limit</b> values at 95  % using the SPSS software package 9.0 ver. Results with p value of < 0.05 {{were considered to be}} statistically significant.|$|R
40|$|We {{consider}} the following question: "Is there a <b>confidence</b> <b>upper</b> <b>limit</b> with given minimum coverage properties which is better than all other <b>confidence</b> <b>upper</b> <b>limits</b> with the given minimum coverage properties?". We prove that for discrete data this question is answered in the negative when certain readily checked and commonly satisfied assumptions hold true. Reliability <b>Confidence</b> <b>upper</b> <b>limit</b> Discrete data...|$|R
40|$|We {{present a}} new and simple method for {{constructing}} a 1 -[alpha] <b>upper</b> <b>confidence</b> <b>limit</b> for [theta] {{in the presence of}} a nuisance parameter vector [psi], when the data is discrete. Our method is based on computing a P-value P{T[less-than-or-equals, slant]t} from an estimator T of [theta], replacing the nuisance parameter by the profile maximum likelihood estimate for [theta] known, and equating to [alpha]. We provide a theoretical result which suggests that, from the point of view of coverage accuracy, this is close to the optimal replacement for the nuisance parameter. We also consider in detail limits for the (i) slope parameter of a simple linear logistic regression, (ii) odds ratio in two-way tables, (iii) ratio of means for two Poisson variables. In all these examples the coverage performance of our upper limit is a dramatic improvement on the coverage performance of the standard approximate <b>upper</b> <b>limits</b> considered. <b>Upper</b> <b>confidence</b> <b>limit</b> Profile maximum likelihood estimator Coverage error Nuisance parameter...|$|R
40|$|We present {{improved}} {{lower and}} upper bounds {{for the time}} constant of first-passage percolation on the square lattice. For the case of lower bounds, a new method, using {{the idea of a}} transition matrix, has been used. Numerical results for the exponential and uniform distributions are presented. A simulation study is included, which results in new estimates and improved <b>upper</b> <b>confidence</b> <b>limits</b> of the time constants. ...|$|R
40|$|We {{discuss the}} problem of {{incorporating}} the uncertainty in the experimental sensitivity into the calculation of an <b>upper</b> <b>confidence</b> <b>limit</b> on a branching ratio or similar quantity. If the number of events is small or zero but without background, the correction to the usual result is given by a simple, easily applied formula. The case of an accurately known background also has a simple solution. ...|$|R
40|$|The {{objective}} {{of this study is}} to compare interval estimation methods for population means of positively skewed distributions. The estimation methods are the interval estimation method with student-t statistics, the interval estimation method with Johnson’s statistics, the interval estimation method with Hall’s statistics and the interval estimation method with Chen’s statistics. Log-normal distribution and Weibull distribution are considered. The measures of skewness under the consideration are 1. 0, 3. 0, 5. 0, respectively. The sample sizes are 10, 30, 50 and the confidence levels are 0. 95. The consideration has two steps. First, the confidence level of interval estimation methods are not lower than the determined confidence level value. The second is the comparision of mean of lower <b>confidence</b> <b>limit,</b> mean of <b>upper</b> <b>confidence</b> <b>limit</b> and mean of confidence interval length. The experimental data are generated by the Monte Carlo Simulation technique. The confidence level of interval estimation method with Bootstrap is higher than the non-boot-strap. The interval estimation method with Johnson’s statistics is the optimum estimation method for the <b>upper</b> <b>confidence</b> interval and two-tailed confidence interval. The interval estimation method with Chen’s statistics is the optimum estimation method for the lower confidence interval. Commonly, the confidence level of interval estimation methods for <b>upper</b> <b>confidence</b> interval are varied by the measure of skewness butthe confidence level of interval estimation methods for lower confidence interval and two-tailed confidence interval are converted by the measure of skewness. The mean of lower <b>confidence</b> <b>limit</b> is varied by the sample size, on the other hand, the mean of <b>upper</b> <b>confidence</b> <b>limit</b> and mean of confidence interval length are converted by the sample size...|$|R
40|$|Optimization {{methods have}} been widely applied in statistics. Among them, {{capability}} measuring techniques are widely applied in industries. Process Capability Indices (PCIs) have been proposed for measuring process production capability. This paper concentrates, the lower and <b>upper</b> <b>confidence</b> <b>limits</b> for the PCIs are obtained from generalized pivotal quantity of random model and also, use this confidence interval, the decision variables would seem to monitor the capability of production process of linear programming models and find {{the solution of the}} accepted models...|$|R
40|$|Environmental {{exposure}} measurements are, in general, positive {{and may be}} subject to left censoring, i. e. the measured value is less than a ''limit of detection''. In occupational monitoring, strategies for assessing workplace exposures typically focus on the mean exposure level or the probability that any measurement exceeds a limit. A basic problem of interest in environmental risk assessment is to determine if the mean concentration of an analyte is less than a prescribed action level. Parametric methods, used to determine acceptable levels of exposure, are often based on a two parameter lognormal distribution. The mean exposure level and/or an upper percentile (e. g. the 95 th percentile) are used to characterize exposure levels, and <b>upper</b> <b>confidence</b> <b>limits</b> are needed to describe the uncertainty in these estimates. In certain situations it is of interest to estimate the probability of observing a future (or ''missed'') value of a lognormal variable. Statistical methods for random samples (without non-detects) from the lognormal distribution are well known for each of these situations. In this report, methods for estimating these quantities based on the maximum likelihood method for randomly left censored lognormal data are described and graphical methods are used to evaluate the lognormal assumption. If the lognormal model is in doubt and an alternative distribution for the exposure profile of a similar exposure group is not available, then nonparametric methods for left censored data are used. The mean exposure level, along with the <b>upper</b> <b>confidence</b> <b>limit,</b> is obtained using the product limit estimate, and the <b>upper</b> <b>confidence</b> <b>limit</b> on the 95 th percentile (i. e. the upper tolerance limit) is obtained using a nonparametric approach. All of these methods are well known but computational complexity has limited their use in routine data analysis with left censored data. The recent development of the R environment for statistical data analysis and graphics has greatly enhanced the availability of high quality nonproprietary (open source) software that serves as the basis for implementing the methods in this paper...|$|R
40|$|Abstract We {{present a}} {{procedure}} for estimating Q 95 low flows in both gauged and ungauged catchments where Q 95 is the flow that is exceeded 95 % of the time. For {{each step of}} the estimation procedure, a number of alternative methods was tested on the Austrian data set by leave-one-out cross-validation, and the method that performed best was used in the final procedure. To maximise the accuracy of the estimates, we combined relevant sources of information including long streamflow records, short streamflow records, and catchment characteristics, according to data availability. Rather than deriving a single low flow estimate for each catchment, we estimated lower and <b>upper</b> <b>confidence</b> <b>limits</b> to allow local information to be incorporated in a practical application of the procedure. The components of the procedure consist of temporal (climate) adjustments for short records; grouping catchments into eight seasonality-based regions; regional regressions of low flows with catchment characteristics; spatial adjustments for exploiting local streamflow data; and uncertainty assessment. The results are maps of lower and <b>upper</b> <b>confidence</b> <b>limits</b> of low flow discharges for 21 000 sub-catchments in Austria. Key words low flows; droughts; regionalisation; prediction of ungauged catchments (PUB); seasonality index; catchment grouping; regional regression; climate variability adjustment; predictive uncertaint...|$|R
30|$|For {{the four}} core years, the results {{both for the}} {{analyses}} of averages over all three periods and for the analyses of data from individual Periods were a mix {{of positive and negative}} estimates (Fig.  3). Two of the negative ones (for egg area averaged across periods and for larval area in the After period) were significantly different from zero though only marginally (Fig.  3). For the other 22 cases, the confidence intervals were not as wide as for the fifth-year estimates. While <b>confidence</b> <b>limits</b> are specific to the estimated mean, we point out that 18 of the lower <b>confidence</b> <b>limits</b> were more negative than − 20 % and almost half of the <b>upper</b> <b>confidence</b> <b>limits</b> were greater than + 20 %.|$|R
30|$|The {{results for}} rape during years 1 – 4 (both for the {{analyses}} of averages over all three periods {{and for the}} analyses of data from individual Periods or the four core years) had a preponderance of positive estimates, i.e. {{cases in which the}} hives exposed to the treated crop were estimated to have performed better than the controls (19 / 24) (Fig.  4). None of the estimates was significantly different from zero and the <b>confidence</b> <b>limits</b> were very wide: 23 of the 24 lower <b>confidence</b> <b>limits</b> were more negative than − 20 %, with 15 more than − 50 %; 23 of the 24 <b>upper</b> <b>confidence</b> <b>limits</b> were greater than + 20 %, with 20 greater than + 50 %.|$|R
40|$|This report {{presents}} background {{concentrations of}} 17 trace elements in groundwater in the Netherlands. The {{three types of}} background concentrations distinguished are natural, semi-natural and regional. Natural background concentration is the concentration at locations without any human influence. Because these types of locations are {{not found in the}} Netherlands natural background concentrations for groundwater are, as a first approach, estimated by deriving the <b>upper</b> <b>confidence</b> <b>limit</b> of the median trace metal concentration at the above-mentioned locations with only diffuse pollution. Semi-natural background concentration is the concentration at locations not influenced by point sources of pollution. The locations may be influenced by diffuse pollution occurring on a national scale. This is determined by calculating the <b>upper</b> <b>confidence</b> <b>limit</b> of the 90 -percentile of the concentration at this type of locations. Regional background concentrations are concentrations occurring at locations with an elevated regional atmospheric deposition but without local point sources of pollution. Regional background concentrations have been derived for trace metals in groundwater found in the sands {{in the southern part of}} the Netherlands. The southern part of the Netherlands, with mainly sandy soil, has a well-known history of relatively high atmospheric trace metal deposition due to the presence of a zinc industry. In addition, this region has suffered from a high atmospheric acid deposition, mainly due to the presence of high intensive animal farming in this region (ammonia emission and deposition). The combination of a high deposition and vulnerable soils is probably the cause of the high trace-metal concentrations in groundwater. Regional background concentrations are determined by calculating the <b>upper</b> <b>confidence</b> <b>limit</b> of 90 -percentile of the concentration at these type of locations. Data from this region have not been used to derive natural and semi-natural background concentrations in groundwater in sandy soils for beryllium, cadmium, cobalt, nickel and zinc. Natural and semi-natural background concentrations are given for each of the soil types, with different background concentrations specified for different depths in the groundwater. The three soil types distinguished are sand, clay and peat. The three levels distinguished: are upper groundwater (> 5 m), shallow groundwater (ca. 10 m) and deep groundwater (ca. 25 m) ...|$|R
40|$|Abstract. The brittle is {{crippling}} {{the application}} of bioceramic. The compound bioceramic is a new biomaterial being widely applied in medical treatments and its fracture toughness is an important mechanical behaviors. In this paper, we introduce the manufacturing method of the compound bioceramic and experiment facilities for its fracture toughness, investigate its probability distribution for the experimental data and conduct the test for fit. We conclude that the experimental data for the toughness fracture of the compound bioceramic obey the two-parameter Weibull distribution, introduce the analyzing method for the <b>upper</b> <b>confidence</b> <b>limit</b> curve and lower <b>confidence</b> <b>limit</b> curve and study the reliability and confidence level of the fracture toughness of the compound bioceramic...|$|R
40|$|A common {{statistical}} {{issue in}} seed-quality control is {{to prove that}} the proportion of individuals showing an unwanted trait is less than a small threshold. Group testing can be used to reduce costs of assay and <b>upper</b> <b>confidence</b> <b>limits</b> for the proportion of detrimental individuals can be used for either estimation or hypothesis testing. A crucial problem of group testing is the appropriate choice of group size in dependence of the number of groups, an assumed true proportion, and the threshold. This paper reports on experimental design to achieve high power for tests or low confidence interval width. Two agricultural applications are presented for which experimental design is discussed...|$|R
40|$|A {{generalized}} {{least squares}} regression model {{was developed to}} estimate local harvest of the Western Arctic caribou (Rangifer tarandus granti) herd. This model provides herd and community level harvest based on community size, proximity of the herd to the village. The model utilizes community harvest survey information from the Alaska Department of Fish and Game, Subsistence Division and cooperation from the nonprofit organizations Maniliq and Kawerak. The model will assist in an annual selection of communities to survey. The predicted local resident harvest of the Western Arctic caribou herd is 14 700 with 95 % lower and <b>upper</b> <b>confidence</b> <b>limits</b> of 10 100 and 19 700 respectively...|$|R
40|$|In this paper, {{we develop}} an {{approach}} for optimizing the explicit binomial confidence interval recently derived by Chen et al. The optimization reduces conservativeness while guaranteeing prescribed coverage probability. 1 Explicit Formula of Chen et al. Let X be a Bernoulli random variable defined in probability space (Ω,F,Pr) with distribution Pr{X = 1 } = 1 − Pr{X = 0 } = p ∈ (0, 1). It {{is a frequent}} problem to construct a confidence interval for p based on n i. i. d. random samples of X. limit Recently, Chen et al. have proposed an explicit confidence interval in [1] with lower <b>confidence</b> and <b>upper</b> <b>confidence</b> <b>limit</b> Ln,δ = K...|$|R
40|$|The US Environmental Protection Agency (EPA) {{recommends}} {{the use of}} an <b>upper</b> <b>confidence</b> <b>limit</b> in making a decision of whether a possibly polluted environment needs clean-up. This decision rule, however, is frequently too conservative and {{does not take into account}} the costs and/or benefits from making a correct or a wrong decision. In this paper we propose an asymmetric loss function and a Bayesian decision rule for remediation actions. The new loss function accounts for both false-positive and false-negative errors possibly involved in a decision, and accommodates the needs of both the EPA and other parties involved. Asymmetric loss Costs and risks Action-level-concentration Superfund program...|$|R
40|$|The topical {{carcinogenicity}} to mouse skin {{of smoke}} condensates {{obtained from a}} tobacco substitute (NSM), alone or in combination with tobacco, has been compared with condensate from tobacco and with acetone, the solvent used. Sixteen different types of cigarette were {{used to make the}} condensates, and the age-standardized results have been analysed according to the Weibull distribution model. The results show that NSM condensate has less than 25 % of the potency of tobacco condensate (37 % at 95 % <b>upper</b> <b>confidence</b> <b>limit),</b> and that condensates from blends of NSM and tobacco are similarly reduced in activity. General pathology analysis failed to reveal abnormalities due to NSM...|$|R
40|$|The acute static renewal {{test of a}} botanical {{pesticide}} - azadirachtin for the freshwater catfish, Heteropneustes fossilis {{has been}} performed to determine the LC 50 values at different exposure period. The LC 50 values at various exposure periods are 173. 06 mg L - 1 for 24 h; 80. 69 mg L - 1 for 48 h; 58. 57 mg L - 1 for 72 h and 52. 35 mg L - 1 for 96 h. The <b>upper</b> <b>confidence</b> <b>limits</b> were 196. 87, 86. 91, 79. 20 and 70. 04 mg L - 1 for 24, 48, 72 and 96 h and lower <b>confidence</b> <b>limits</b> were 154. 01, 74. 24, 37. 33 and 33. 83 mg L - 1, respectively. These results indicate that azadirachtin exposure to the fish caused toxic effects. </p...|$|R
40|$|Background: Switching off air {{handling}} systems in operating theaters {{during periods of}} prolonged inactivity (eg, nights, weekends) can produce a substantial reduction of energy expenditure. However, little evidence is available regarding the effect of switching off the {{air handling}} system during periods of prolonged inactivity on the air quality in operating theaters during operational periods. The {{aim of this study}} is to determine the amount of time needed after restarting the ventilation system to return to a stable situation, with air quality at least equal to the situation before switching off the system. Methods: Measurements were performed in 3 operating theaters, all of them equipped with a unidirectional downflow(UDF) system. Measurements (particle counts of emitted particles with a particle size ≥ 0. 5 μm) were taken during the start-up of the ventilation system to determine when prespecified degrees of protectionwere achieved. Temperature readingswere taken to determine when a stable temperature difference between the periphery and the protected area was reached, signifying achievement of a stable condition. Results: After starting up the system, the protected area achieved the required degrees of protection within 20 minutes (95 % <b>upper</b> <b>confidence</b> <b>limit).</b> A stable temperature difference was achieved within 23 minutes (95 % <b>upper</b> <b>confidence</b> <b>limit).</b> Both findings lie well within the period of 25 minutes normally required for preparations before the start of surgical procedures. Conclusions: Switching off the ventilation systemduring prolonged inactivity (during the night andweekend) has no negative effect on the air quality in UDF operating theaters during normal operational hour...|$|R
5000|$|A {{tolerance}} interval is a statistical interval within which, with some confidence level, a specified proportion of a sampled population falls. [...] "More speciﬁcally, a 100×p%/100×(1−α) {{tolerance interval}} provides limits within {{which at least}} a certain proportion (p) of the population falls with a given level of conﬁdence (1−α)." [...] "A (p, 1−α) tolerance interval (TI) based on a sample is constructed {{so that it would}} include at least a proportion p of the sampled population with conﬁdence 1−α; such a TI is usually referred to as p-content − (1−α) coverage TI." [...] "A (p, 1−α) upper tolerance limit (TL) is simply an 1−α <b>upper</b> <b>confidence</b> <b>limit</b> for the 100 p percentile of the population." ...|$|R
5000|$|The {{determination}} of the confidence interval of Pc makes use of Student's t-test (t). The value of t depends {{on the number of}} data and the confidence level of the estimate of the confidence interval. Then, the lower (L) and <b>upper</b> (U) <b>confidence</b> <b>limits</b> of Pc in a symmetrical distribution are found from: ...|$|R
40|$|The paper {{develops}} {{and studies}} simultaneous confidence bounds that {{are useful for}} making low dose inferences in quantitative risk analysis. Application is intended for risk assessment studies where human, animal or ecological data are used to set safe low dose levels of a toxic agent, but where study information is limited to high dose levels of the agent. Methods are derived for estimating simultaneous, one-sided, <b>upper</b> <b>confidence</b> <b>limits</b> on risk for end points measured on a continuous scale. From the simultaneous confidence bounds, lower <b>confidence</b> <b>limits</b> on the dose {{that is associated with}} a particular risk (often referred to as a "bench-mark dose") are calculated. An important feature of the simultaneous construction is that any inferences that are based on inverting the simultaneous confidence bounds apply automatically to inverse bounds on the bench-mark dose. Copyright 2005 Royal Statistical Society. ...|$|R
40|$|The {{problem of}} {{assessing}} occupational exposure using the mean or an upper percentile of a log-normal distribution is addressed. Inferential methods for constructing an <b>upper</b> <b>confidence</b> <b>limit</b> for an <b>upper</b> percentile of a lognormal distribution and for finding confidence intervals for a lognormal mean based on samples with multiple detection limits are proposed. The pro-posed methods {{are based on}} the maximum likelihood estimates. They perform well with re-spect to coverage probabilities as well as power and are applicable to small sample sizes. The proposed approaches are also applicable for finding <b>confidence</b> <b>limits</b> for the percentiles of a gamma distribution. Computational details and a source for the computer programs are given. An advantage of the proposed approach is the ease of computation and implementation. Illustrative examples with real data sets and a simulated data set are given...|$|R
40|$|Shock is {{accompanied}} by generalised splanchnic hypoperfusion, and splanchnic organs like the pancreas can be damaged, as shown in animal experimental models and in humans, {{by the presence of}} high plasma concentrations of trypsin and other pancreatic enzymes. In order to design a radioimmunoassay technique (RIA) for the measurement of equine trypsin-like immunoreactivity (TLI) in biological fluids, trypsin was purified (with purity {{greater than or equal to}} 96 %) from the equine pancreas by extraction in an acid medium, ammonium sulfate precipitations, gel filtration chromatography and, after activation of trypsinogen into trypsin, affinity chromatography. Gel polyacrylamide electrophoresis showed a monomeric enzyme with a molecular weight of 27 kDa. The purified equine trypsin served for the immunisation of rabbits in order to obtain a specific antiserum, and the labelled antigen was prepared by iodination of equine trypsin with I- 125. The RIA was based on the binding of the antigen to the antibody followed by the separation of the antigen-antibody complex by immunoprecipitation in the presence of sheep anti-rabbit gammaglobulins and the assay of the radioactivity in the precipitate. The RIA showed good sensitivity, specificity, precision, accuracy and reproducibility. The reference mean value of TLI in the plasma of healthy horses (n = 20) was 30. 01 +/- 6. 84 ng/mL (<b>upper</b> <b>confidence</b> <b>limit</b> 50. 52 ng/mL; p < 0. 01). Three horses with non strangulating intestinal obstruction without shock showed TLI values within normal limits whereas 5 of 7 horses with strangulation obstruction showed TLI levels above the <b>upper</b> <b>confidence</b> <b>limit.</b> Further studies using the RIA and the enzymatic assay should be performed in order to confirm the role of the pancreas in equine intestinal obstruction. Peer reviewe...|$|R
30|$|A {{cut point}} factor of 2 {{was used for}} {{clinical}} sample analysis and approximates 2 to 3 standard deviations (SDs) above the mean of NC samples (data not shown). In order to align with industry benchmarks, the screening cut point was statistically derived by targeting a 5 % false-positive rate following industry guidelines to minimize potential false-negatives. The serum samples from 20 treatment-naive NHL individual samples and 30 normal human serum samples were analyzed 3 times each over 3  days. The assay cut point {{was derived from the}} 95 % <b>upper</b> <b>confidence</b> <b>limit</b> after removal of outliers. Because different cut point factors were obtained for the normal human serum and NHL sample populations, the NHL-specific cut point factor was used for clinical sample analysis.|$|R
40|$|The three-parameter {{lognormal}} distribution is {{the extension of}} the two-parameter {{lognormal distribution}} to meet the need of the biological, sociological, and other fields. Numerous research papers have been published for the parameter estimation problems for the lognormal distributions. The inclusion of the location parameter brings in some technical difficulties for the parameter estimation problems, especially for the interval estimation. This paper proposes a method for constructing exact confidence intervals and exact <b>upper</b> <b>confidence</b> <b>limits</b> for the location parameter of the three-parameter lognormal distribution. The point estimation problem is discussed as well. The performance of the point estimator is compared with the maximum likelihood estimator, which is widely used in practice. Simulation result shows that the proposed method is less biased in estimating the location parameter. The large sample size case is discussed in the paper...|$|R
30|$|To {{alleviate}} {{concerns about}} antimicrobial resistance pressure, new 5 -fluorouracil (5 -FU)-coated catheters were recently developed. The pyrimidine analogue 5 -FU is an antimetabolite drug. In concentrations well below {{those used in}} cancer therapy, 5 -FU {{has been shown to}} inhibit the growth of gram-positive and gram-negative bacteria and Candida species. In a single-blind, multicenter, noninferiority, randomized trial, catheters coated externally with 5 -FU were compared to catheters coated externally with chlorhexidine-silver-sulfadiazine (CH-SS). 5 -FU-coated catheters compared favorably to CH-SS-coated catheters in terms of catheter tip colonization (5 -FU-coated, 12 / 419 vs. CH-SS-coated, 21 / 398 catheters; difference, - 2.6 % with an <b>upper</b> <b>confidence</b> <b>limit</b> of - 0.13 %) and CR-BSI (5 -FU-coated, 0 / 65 episodes vs. CH-SS-coated, 2 / 71 episodes; difference, - 2.8 %; 95 % CI, - 10 % to + 3 %) [87].|$|R
40|$|Throughout fifty-three {{years of}} operations, an {{estimated}} 792, 000 Ci (29, 300 TBq) of tritium {{have been released}} to the atmosphere at the Livermore site of Lawrence Livermore National Laboratory (LLNL); about 75 % was tritium gas (HT) primarily from the accidental releases of 1965 and 1970. Routine emissions contributed slightly more than 100, 000 Ci (3, 700 TBq) HT and about 75, 000 Ci (2, 800 TBq) tritiated water vapor (HTO) to the total. A Tritium Dose Reconstruction was undertaken to estimate both the annual doses {{to the public for}} each year of LLNL operations and the doses from the few accidental releases. Some of the dose calculations were new, and the others could be compared with those calculated by LLNL. Annual doses (means and 95 % confidence intervals) to the potentially most exposed member of the public were calculated for all years using the same model and the same assumptions. Predicted tritium concentrations in air were compared with observed mean annual concentrations at one location from 1973 onwards. Doses predicted from annual emissions were compared with those reported in the past by LLNL. The highest annual mean dose predicted from routine emissions was 34 {micro}Sv (3. 4 mrem) in 1957; its <b>upper</b> <b>confidence</b> <b>limit,</b> based on very conservative assumptions about the speciation of the release, was 370 {micro}Sv (37 mrem). The <b>upper</b> <b>confidence</b> <b>limits</b> for most annual doses were well below the current regulatory limit of 100 {micro}Sv (10 mrem) for dose to the public from release to the atmosphere; the few doses that exceeded this were well below the regulatory limits of the time. Lacking the hourly meteorological data needed to calculate doses from historical accidental releases, ingestion/inhalation dose ratios were derived from a time-dependent accident consequence model that accounts for the complex behavior of tritium in the environment. Ratios were modified to account for only those foods growing {{at the time of the}} releases. The highest dose from an accidental release was calculated for a release of about 1, 500 Ci HTO that occurred in October 1954. The likely dose for this release was probably less than 360 {micro}Sv (36 mrem), but, because of many unknowns (e. g., release-specific meteorological and accidental conditions) and conservative assumptions, the uncertainty was very high. As a result, the <b>upper</b> <b>confidence</b> <b>limit</b> on the predictions, considered a dose that could not have been exceeded, was estimated to be 2 mSv (200 mrem). The next highest dose, from the 1970 accidental release of about 290, 000 Ci (10, 700 TBq) HT when wind speed and wind direction were known, was one-third as great. Doses from LLNL accidental releases were well below regulatory reporting limits. All doses, from both routine and accidental releases, were far below the level (3. 6 mSv [360 mrem] per year) at which adverse health effects have been documented in the literature...|$|R
40|$|The {{interval}} {{estimation of the}} survival function of the two-parameter exponential distribution {{on the basis of}} the progressively Type-II censored samples is investigated. Toward this end, the concept of the generalized confidence intervals (GCIs) is used and the lower and <b>upper</b> generalized <b>confidence</b> <b>limits</b> (GCLs) are obtained. It will be shown that the coverage probabilities of the GCLs are satisfactory using a simulation study. Finally, some concluding remarks are presented...|$|R
