2|14|Public
5000|$|DRC {{systems are}} not {{normally}} {{used to create a}} perfect inversion of the room's response because a perfect correction would only be valid at the location where it was measured: a few millimeters away the arrival times from various reflections will differ and the inversion will be imperfect. The imperfectly corrected signal may end up sounding worse than the <b>uncorrected</b> <b>signal</b> because the acausal filters used in digital room correction may cause pre-echo. Room correction filter calculation systems instead favor a [...] approach, and employ sophisticated processing to attempt to produce an inverse filter which will work over a usably large volume, and which avoid producing bad-sounding artifacts outside of that volume, at the expense of peak accuracy at the measurement location.|$|E
40|$|The {{correlation}} between the irradiance (as measured by the Active Cavity Radiometer Irradiance Monitor and the Earth Radiation Budget) as corrected for sunspot flux deficit (which is {{responsible for most of}} the variance of the <b>uncorrected</b> <b>signal)</b> and both the 205 nm flux (as measured by Nimbus 7) and a photometric facular index is discussed. The computer program which simulates two-dimensional convection in a compressible, stratified medium is described. Equipment which was acquired to perform high precision, white-light observations of sunspot areas, and procedures were tested. Analysis of observations of large scale convective heat inhomogeneities which were obtained in May 1985 was begun...|$|E
5000|$|In LS AAS {{background}} absorption {{can only}} be corrected using instrumental techniques, {{and all of them}} are based on two sequential measurements, firstly, total absorption (atomic plus background), secondly, background absorption only, and the difference of the two measurements gives the net atomic absorption. Because of this, and because of the use of additional devices in the spectrometer, the signal-to-noise ratio of background-corrected signals is always significantly inferior compared to <b>uncorrected</b> <b>signals.</b> It should also be pointed out that in LS AAS {{there is no way to}} correct for (the rare case of) a direct overlap of two atomic lines. In essence there are three techniques used for background correction in LS AAS: ...|$|R
50|$|The Cytac system {{underwent}} {{an enormous}} {{series of tests}} across the United States and offshore. Given the potential accuracy of the system, even {{minor changes to the}} groundwave synchronization were found to cause errors that could be eliminated - issues such as the number of rivers the signal crossed caused predictable delays that could be measured and then factored into navigation solutions. This led to a series of correction contours that could be added to the received signal to adjust for these concerns, and these were printed on the Cytac charts. Using prominent features on dams as target points, a series of tests demonstrated that the <b>uncorrected</b> <b>signals</b> provided accuracy on the order of 100 yards, while adding the correction contour adjustments reduced this to the order of ten yards.|$|R
50|$|It {{has already}} been {{mentioned}} that in HR-CS AAS lamp flicker noise is eliminated using correction pixels. In fact, any increase or decrease in radiation intensity that is observed {{to the same extent}} at all pixels chosen for correction is eliminated by the correction algorithm. This obviously also includes a reduction of the measured intensity due to radiation scattering or molecular absorption, which is corrected in the same way. As measurement of total and background absorption, and correction for the latter, are strictly simultaneous (in contrast to LS AAS), even the fastest changes of background absorption, as they may be observed in ET AAS, do not cause any problem. In addition, as the same algorithm is used for background correction and elimination of lamp noise, the background corrected signals show a much better signal-to-noise ratio compared to the <b>uncorrected</b> <b>signals,</b> which is also in contrast to LS AAS.|$|R
40|$|Nowadays, easily observe many peoples {{standing}} on the bus station {{in order to get}} their desired BUS but sometimes it take too much time for them to wait because many times there may be a traffic jam or some other problem with the Bus to reach at right at time. In this paper a new intelligent bus movement monitoring system and station reporting system based on GPS and RF Transceivers is presented. The neural networks is used to process raw, <b>uncorrected</b> <b>signals</b> received by GPS receiver, which is used to correct the raw signal and to obtain highly accurate position coordinate data. The neural network is trained with a particular GPS receiver. In addition to some fundamental functions such as real-time monitoring, some featured functions can tightly be combined to make the system compatible with the daily operations of any public transport scheme. The proposed method will surely improve the average accuracy of GPS signal reception at the bus station which led people to take decision either to wait for Bus or not...|$|R
40|$|International audienceIn electroencephalographic (EEG) {{measurements}} performed during functional Magnetic Resonance Imaging (fMRI), {{imaging and}} cardiac artefacts strongly contaminate the EEG signal. Several algorithms {{have been proposed}} to suppress these artefacts {{and most of them}} have shown important improvements with respect to <b>uncorrected</b> <b>signals.</b> However, the relative performances of these algorithms have not been properly assessed. In particular, it is not known to what extent such algorithms deteriorate the EEG signal of interest. In this study, we propose to cross-validate different methods proposed for artefact correction, using a forward model to generate EEG and MR-related artefacts. The methods are assessed under various experimental conditions (described in terms of EEG sampling rate, artefacts amplitude, frequency band of interest, etc.). Using experimental data, we also tested the performance of the correction methods for alpha rhythm imaging and for epileptic spike reconstruction. Results show that most of the methods allow the observation of the modulation of alpha rhythms and the identification of spikes, despite subtle differences between algorithms. They also show that over-filtering the data may degrade the EEG. Our results indicate that the optimal artefact removal technique should be chosen according to whether one is interested in fast (> 10 Hz) vs. slow (< 10 Hz) oscillations or in evoked vs. ongoing activity...|$|R
40|$|Förster {{resonance}} energy transfer (FRET) microscopy {{is commonly used}} to monitor protein interactions with filter-based imaging systems, which require spectral bleedthrough (or cross talk) correction to accurately measure energy transfer efficiency (E). The double-label (donor+acceptor) specimen is excited with the donor wavelength, the acceptor emission provided the <b>uncorrected</b> FRET <b>signal</b> and the donor emission (the donor channel) represents the quenched donor (qD), {{the basis for the}} E calculation. Our results indicate this is not the most accurate determination of the quenched donor signal as it fails to consider the donor spectral bleedthrough (DSBT) signals in the qD for the E calculation, which our new model addresses, leading to a more accurate E result. This refinement improves E comparisons made with lifetime and spectral FRET imaging microscopy as shown here using several genetic (FRET standard) constructs, where cerulean and venus fluorescent proteins are tethered by different amino acid linkers...|$|R
40|$|Imperfections in a lidar's overlap {{function}} lead to artefacts in the background, {{range and}} overlap-corrected lidar signals. These artefacts can erroneously {{be interpreted as}} an aerosol gradient or, in extreme cases, as a cloud base leading to false cloud detection. A correct specification of the overlap function is hence crucial {{in the use of}} automatic elastic lidars (ceilometers) for the detection of the planetary boundary layer or of low cloud. In this study, an algorithm is presented to correct such artefacts. It {{is based on the assumption}} of a homogeneous boundary layer and a correct specification of the overlap function down to a minimum range, which must be situated within the boundary layer. The strength of the algorithm lies in a sophisticated quality-check scheme which allows the reliable identification of favourable atmospheric conditions. The algorithm was applied to 2 years of data from a CHM 15 k ceilometer from the company Lufft. Backscatter signals corrected for background, range and overlap were compared using the overlap function provided by the manufacturer and the one corrected with the presented algorithm. Differences between corrected and <b>uncorrected</b> <b>signals</b> reached up to 45  % in the first 300  m above ground. The amplitude of the correction turned out to be temperature dependent and was larger for higher temperatures. A linear model of the correction as a function of the instrument's internal temperature was derived from the experimental data. Case studies and a statistical analysis of the strongest gradient derived from corrected signals reveal that the temperature model is capable of a high-quality correction of overlap artefacts, in particular those due to diurnal variations. The presented correction method has the potential to significantly improve the detection of the boundary layer with gradient-based methods because it removes false candidates and hence simplifies the attribution of the detected gradients to the planetary boundary layer. A particularly significant benefit can be expected for the detection of shallow stable layers typical of night-time situations. The algorithm is completely automatic and does not require any on-site intervention but requires the definition of an adequate instrument-specific configuration. It is therefore suited for use in large ceilometer networks...|$|R
40|$|Mechanical waves provide {{information}} about the stiffness and the condition of a medium; thus, changes in medium conditions can be inferred from changes in wave velocity and attenuation. Non-destructive testing (NDT) methods based on ultrasonic waves are often more economical, practical and faster than destructive testing. Multichannel analysis of surface waves (MASW) is a well-established surface wave method used for determination of the shear-wave profile of layered medium. The MASW test configuration is also applicable to assess the condition of concrete elements using appropriate frequency range. Both attenuation and dispersion of ultrasonic waves can be evaluated by this technique. In ultrasonic testing, the characterization of a medium requires the precise measurement of its response to ultrasonic pulses to infer the presence of defects and boundary conditions. However, any ultrasonic transducer attached to a surface affects the measured response; especially at high frequencies. On the other hand, ultrasonic transducers available for engineering application are mostly used to measure wave velocities (travel time method). Therefore, these transducers {{do not have a}} flat response in the required frequency range. Moreover, in the case of full-waveform methods, the recorded signals should be normalized with respect to the transfer functions of the transducers to obtain the real response of the tested specimen. The main objective of this research is to establish a comprehensive methodology based on surface wave characteristics (velocity, attenuation and dispersion) for condition assessment of cemented materials with irregular defects. To achieve the major objective, the MASW test configuration is implemented in the ultrasonic frequency range. The measured signals are subjected to various signal processing techniques to extract accurate information. In addition, a calibration procedure is conducted to determine the frequency response functions (FRF) of the piezoelectric accelerometers outside their nominal frequency range. This calibration is performed using a high-frequency laser vibrometer. This research includes three main studies. The first study introduces the calibration approach to measure the FRFs of the accelerometers outside of their flat frequency range. The calibrated accelerometers are then used to perform MASW tests on a cemented-sand medium. The original signals and the corrected ones by eliminating the effect of the FRFs are used to determine material damping of the medium. Although, the damping ratios obtained from different accelerometers are not same, the values from the corrected signals are found closer to the characteristic damping value compared to those from the <b>uncorrected</b> <b>signals.</b> The second study investigates the sensitivity of Rayleigh wave velocity, attenuation coefficient, material damping and dispersion in phase velocity to evaluate the sensitivity of these characteristics to the damage quantity in a medium. The soft cemented-sand medium is preferred as the test specimen so that well-defined shaped defects could be created in the medium. MASW test configuration is implemented on the medium for different cases of defect depth. The recorded signals are processed using different signal processing techniques including Fourier and wavelet transforms and empirical mode decomposition to determine the surface wave characteristics accurately. A new index, ‘dispersion index’, is introduced which quantifies the defect based on the dispersive behaviour. All surface wave characteristics are found capable of reflecting the damage quantity of the test medium at different sensitivity levels. In the final study, the condition assessment of six lab-scale concrete beams with different void percent is performed. The beam specimens involving Styrofoam pellets with different ratios are tested under ultrasonic and mechanical equipment. The assessment produce established in the second study with well-defined defects is pursed for the beams with irregular defects. Among the characteristics, attenuation, P and R-wave velocities and dispersion index are found as the promising characteristics for quantifying the defect volume...|$|R
40|$|Background: The Agilent microRNA {{microarray}} platform interrogates each microRNA {{with several}} copies of distinct oligonucleotide probes and integrates the results into a total gene signal (TGS), using a proprietary algorithm that {{makes use of}} the background subtracted signal. The TGS can be normalized between arrays, and the Agilent recommendation is either not to normalize or to normalize to the 75 th percentile signal intensity. The robust multiarray average algorithm (RMA) is an alternative method, originally developed to obtain a summary measure of mRNA Affymetrix gene expression arrays by using a linear model {{that takes into account}} the probe affinity effect. The RMA method has been shown to improve the accuracy and precision of expression measurements relative to other competing methods. There is also evidence that it might be preferable to use non-corrected signals for the processing of microRNA data, rather than background-corrected signals. In this study we assess the use of the RMA method to obtain a summarized microRNA signal for the Agilent arrays. Findings: We have adapted the RMA method to obtain a processed signal for the Agilent arrays and have compared the RMA summarized signal to the TGS generated with the image analysis software provided by the vendor. We also compared the use of the RMA algorithm with <b>uncorrected</b> and background-corrected <b>signals,</b> and compared quantile normalization with the normalization method recommended by the vendor. The pre-processin...|$|R
40|$|The {{brainstem}} {{is directly}} involved in controlling blood pressure, respiration, sleep/wake cycles, pain modulation, motor and cardiac output. As such it is of significant basic science and clinical interest. However, the brainstem’s location close to major arteries and adjacent pulsatile cerebrospinal fluid filled spaces, means {{that it is difficult}} to reliably record functional magnetic resonance imaging (fMRI) data from. These physiological sources of noise generate time varying signals in fMRI data, which if left <b>uncorrected</b> can obscure <b>signals</b> of interest. In this Methods Article we will provide a practical introduction to the techniques used to correct for the presence of physiological noise in time series fMRI data. Techniques based on independent measurement of the cardiac and respiratory cycles, such as retrospective image correction (RETROICOR, Glover et al., 2000), will be described and their application and limitations discussed. The impact of a physiological noise model, implemented in the framework of the general linear model, on resting fMRI data acquired at 3 T and 7 T is presented. Data driven approaches based such as independent component analysis (ICA) are described. MR acquisition strategies that attempt to either minimise the influence of physiological fluctuations on recorded fMRI data, or provide additional information to correct for their presence, will be mentioned. General advice on modelling noise sources, and its effect on statistical inference via loss of degrees of freedom, and non-orthogonality of regressors, is given. Lastly, different strategies for assessing the benefit of different approaches to physiological noise modelling are presented...|$|R
40|$|Arterial spin {{labelling}} (ASL) is {{a magnetic}} resonance imaging (MRI) based method that can measure quantitatively cerebral blood flow (CBF) by magnetically labelling the arterial blood that perfuse the brain tissue. The brain consists predominantly of three different components: white matter (WM), grey matter (GM) and cerebrospinal fluid (CSF), which differ in longitudinal relaxation times: T 1, and flow. Due to the low spatial resolution in ASL, a signal from a given voxel is likely to consist of a mixture of signals from the different components weighted by their volume fraction. For accurate flow measurements, this partial volume (PV) effect {{needs to be taken}} into account. Recently, a method for obtaining PV maps based on multi-exponential analysis of the T 1 recovery curves in an inversion recovery Look-Looker (IR LL) acquisition was proposed. In the current study, a modification of the IR LL sequence is presented. By introducing slice selective inversion pulses, magnetization history propagation between individual slices will be eliminated and thus significant reduction in acquisition time is enabled compared to sequences with non-selective inversions. Short acquisition time is an important factor to enable partial volume correction (PVC) in ASL in clinical applications since available scan time is limited. In {{the first part of the}} study, optimization of sequence parameters was performed based on both phantom and brain imaging. The optimal parameter combination was found to be RF pulses with flip angle = 4 deg, separated with &# 8710;TI = 400 ms and with spoiling gradient amplitude of 10 mT/m after each read-out. Correcting for $B_ 1 $-field inhomogeneity was shown not to be required and number of inversion times was set to n = 12. This provided viable PV maps with full-brain coverage with a total acquisition time of about 5 min, a 30 % reduction compared to the original IR LL sequence. In the second part of the study, PVC based on the PV maps from the optimal IR LL sequence was performed on ASL images. Two PVC methods, one based on thresholding and one based on a regression algorithm, were tested and compared with the raw <b>uncorrected</b> ASL <b>signals.</b> The ASL signals from the threshold-based PVC were found to be dependent of the voxel-wise PV fractions and thus suboptimal for PVC. In contrast, the regression analysis was capable of separating the tissue specific contributions and thus the resulting tissue specific ASL signals were independent of PV fractions. With reduced acquisition time, the applicability of the IR LL sequence for PVC in clinical ASL protocols has increased and can thus contribute to improvements of CBF measurements as a diagnostic and prognostic tool. </p...|$|R
40|$|Abstract Background The Agilent microRNA {{microarray}} platform interrogates each microRNA {{with several}} copies of distinct oligonucleotide probes and integrates the results into a total gene signal (TGS), using a proprietary algorithm that {{makes use of}} the background subtracted signal. The TGS can be normalized between arrays, and the Agilent recommendation is either not to normalize or to normalize to the 75 th percentile signal intensity. The robust multiarray average algorithm (RMA) is an alternative method, originally developed to obtain a summary measure of mRNA Affymetrix gene expression arrays by using a linear model {{that takes into account}} the probe affinity effect. The RMA method has been shown to improve the accuracy and precision of expression measurements relative to other competing methods. There is also evidence that it might be preferable to use non-corrected signals for the processing of microRNA data, rather than background-corrected signals. In this study we assess the use of the RMA method to obtain a summarized microRNA signal for the Agilent arrays. Findings We have adapted the RMA method to obtain a processed signal for the Agilent arrays and have compared the RMA summarized signal to the TGS generated with the image analysis software provided by the vendor. We also compared the use of the RMA algorithm with <b>uncorrected</b> and background-corrected <b>signals,</b> and compared quantile normalization with the normalization method recommended by the vendor. The pre-processing methods were compared in terms of their ability to reduce the variability (increase precision) of the signals between biological replicates. Application of the RMA method to non-background corrected signals produced more precise signals than either the RMA-background-corrected signal or the quantile-normalized Agilent TGS. The Agilent TGS normalized to the 75 % percentile showed more variation than the other measures. Conclusions Used without background correction, a summarized signal that takes into account the probe effect might provide a more precise estimate of microRNA expression. The variability of quantile normalization was lower compared with the normalization method recommended by the vendor. </p...|$|R
40|$|In susceptibility-weighted MRI, {{ignoring}} {{the magnetic field}} inhomogeneity can lead to severe reconstruction artifacts. Correcting {{for the effects of}} magnetic field inhomogeneity requires accurate fieldmaps. Especially in functional MRI, dynamic updates are desirable, since the fieldmap may change in time. Also, susceptibility effects that induce field inhomogeneity often have non-zero through-plane gradients, which, if <b>uncorrected,</b> can cause <b>signal</b> loss in the reconstructed images. Most image reconstruction methods that compensate for field inhomogeneity, even using dynamic fieldmap updates, ignore through-plane fieldmap gradients. Furthermore, standard optimization methods, like CG-based algorithms, may be slow to converge and recently proposed algorithms based on the Augmented Lagrangian (AL) framework have shown the potential to lead to more efficient optimization algorithms, especially in MRI reconstruction problems with non-quadratic regularization. In this work, we propose a computationally efficient, model-based iterative method for joint reconstruction of dynamic images and fieldmaps in single coil and parallel MRI, using single-shot trajectories. We first exploit the fieldmap smoothness to perform joint estimation using less than two full data sets and then we exploit the sensitivity encoding from parallel imaging to reduce the acquisition length and perform joint reconstruction using just one full k-space dataset. Subsequently, we extend the proposed method to account for the through-plane gradients of the field inhomogeneity. To improve the efficiency of the reconstruction algorithm we use a linearization technique for fieldmap estimation, which allows the use of the conjugate gradient algorithm. The resulting method allows for efficient reconstruction by applying fast approximations that allow the use of the conjugate gradient algorithm along with FFTs. Our proposed method can be computationally efficient for quadratic regularizers, but the CG-based algorithm is not directly applicable to non-quadratic regularization. To improve the efficiency of our method for non-quadratic regularization we propose an algorithm based on the augmented Lagrangian (AL) framework with variable splitting. This new algorithm can also be used for the non-linear optimization problem of fieldmap estimation without the need for the linearization approximation...|$|R
40|$|International audienceReceiver Autonomous Integrity Monitoring (RAIM) {{has been}} {{certified}} to provide lateral guidance in flight operations ranging from En-route to Non-Precision Approach (NPA). Recent {{developments in the}} RAIM algorithm science, namely Advanced RAIM (ARAIM), have suggested a future role in vertically guided operations down to LPV with a decision height of 200 ft [EU-U. S., 2012]. However, more stringent requirements {{as a result of}} the vertical guidance application question the external risk or trust that is placed on the constellation service provision and may require the partial reduction of this risk through the use of a ground segment, identifying and removing threats and providing data through an ISM (Integrity Support Message). This ground segment should ideally be light and low-cost so not to replicate that implemented for SBAS. In addition the ISM latency [Walter et al, 2012] should ideally be allowed as long as possible to obviate the challenging and expensive communications requirements as imposed, for example, on SBAS (6 sec Time to Alert). Furthermore, the ISM should be as simple as possible to ensure the data broadcast requirements can be met with a number of solutions from ATC, to local ground communications to GEO relay. Finally, the network should be light, in the sense of a sparse and global distribution of stations. In order to meet the defined role of the ground segment and its monitoring capability; three possible methodologies were identified [Milner et al, 2014]: No ground monitoring, Offline Monitoring, Real-Time Monitoring. The parameters of interest to this monitoring are the input parameters defined for the ARAIM baseline airborne algorithm as given below: • URA/SISA: Standard deviation of ranging measurement for integrity • URE/SISE: Standard deviation of ranging measurement for nominal accuracy/continuity • Bnom: Maximum nominal bias on ranging measurement • Psat: Prior probability of fault in satellite per approach • Pconst : Prior probability of fault affecting more than one satellite in constellation per approach This list of parameters contains the maximum nominal bias Bnom. A nominal bias is a fault-free bias, both to account for near-constant <b>uncorrected</b> errors (<b>signal</b> deformation and antenna bias) and non-Gaussian behaviour. However, some small nominal bias may be included in this Bnom parameter. Indeed, after application of all possible corrections, ionofree smoothed code ranges are affected by residual ephemeris plus satellite clock and payload group delay errors w. r. t constellation reference frame and clock. In the context of ARAIM, the residual ephemeris plus clock errors, residual tropospheric error, and multipath plus noise errors, are all assumed to be random errors overbounded by zero mean gaussian errors with known modeled variance. However, it is noted that the residual ephemeris plus satellite clock errors may include a long term bias. These ionofree smoothed code ranges are also affected by the receiver clock offset, defined as the common propagation delays from antenna to signal processing stages, also defined as the error identical to all measurements of the same constellation, which varies across constellations (time reference, signal) and the receiver design. Note that the receiver clock offset may include residual payload plus ephemeris delays identical to all satellites used in the navigation solution computation, so may vary depending on the set of satellites used in this computation. The iono-free nominal bias may then be defined as the permanent bias in excess of the residual error identical to all measurements of the same constellation, and from this definition may therefore depend on the receiver clock offset. A first paper has been issued to define properly the nominal bias and to characterize over the globe those biases for an ARAIM user [Macabiau et al, 2014]. Three possible types of sources of nominal bias were identified: nominal signal deformation, variation of SV antenna group delay with nadir angle, variation of user antenna group delay with Azimuth (Az) and Elevation (El) angles. Models used to characterize theses nominal bias contributions were proposed and fully defined. Assumptions were made at several levels of these models to try and reflect possible nominal situations of signal distortion, SV or user receiver antenna group delay variation. Initial work was presented on the ARAIM reference algorithm integrity monitoring performance to protect the ARAIM user against the impact of these nominal biases, driven by the ISM input Bnom value transmitted by the ground segment. The aim of this paper is therefore to update the analysis done on nominal bias affecting the ARAIM user, on the capacity of the ground monitoring network to provide a pertinent Bnom, and on the impact on the ARAIM user performance. This methodology allows determining possible restrictions on ARAIM user receiver characteristics. Based on the definition of the ARAIM user nominal bias expressed in [Macabiau et al, 2014] identifyng three possible sources of nominal bias (signal deformations, SV antenna, and user antenna), assumption and models definition are set in the first part of the paper. Then, we determine the impact of that defined nominal bias on the ARAIM user receiver range measurement and position estimate. Impact of nominal signal deformations is evaluated using a models derived from the bounding ICAO EWF threat model The evaluation considers different receiver configurations in terms of bandwidth and chip spacing, representing the regions that are proposed at RTCA/EUROCAE [Phelts et al., 2014 b] plus regions identified to induce a maximum ranging error due to nominal signal deformation. The analysis of the nominal bias obtained for the different configuration leads to the identification of suitable design requirements for the ARAIM user receiver. Impact of user antenna group delay variation as a function of Azimuth and Elevation is then addressed, considering several models for user antenna, including recent results for the model of a dualfrequency civil aviation antenna mounted on aircraft. Impact of SV antenna group delay variation is also assessed based on mathematical analysis of antenna group delay. Then, through simulation, we analyze the results of the implementation of these ground monitoring techniques for estimation of the Bnom bounds and we analyze the performance of these bounds with respect to the possible distribution of the ARAIM user nominal bias. Situations leading to extreme integrity situations have been identified and are analyzed with respect to current monitoring concept used in the ISM. This analysis will assess the sensitivity of the results with respect to the model used to define each nominal bias contributor. From these simulations, we finally conclude on the capacity of the ground to provide efficient Bnom coupled with some possible restrictions for the characteristics of the ARAIM user receiver...|$|R

