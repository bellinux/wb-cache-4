10|0|Public
40|$|A new, {{inexpensive}} and fast method {{for determining the}} magnetoelastic uniformity of amorphous magnetostrictive ribbons and wires is proposed. Such a method {{is based on the}} response of a magnetostrictive delay line (MDL) when a pulsed magnetic field acoustically excites the whole line. Results are given showing an improvement of the <b>unformity</b> after stress current annealin...|$|E
40|$|THE DEFINITION OF THE UNIFORM LINEAR GENERATOR IS GIVEN AND SOME OF THE MOSTLY USED TESTS TO EVALUATE THE <b>UNFORMITY</b> AND THE INDEPENDENCE OF THE OBTAINED DETERMINATIONS ARE LISTED. THE GENERATORS OF THE MAIN DISCRETE AND CONTINUOUS RANDOM VARIABLES ARE DEFINED. THE PROBLEM OF CALCULATING, THROUGH SIMULATION, SOME MOMENT W OF A RANDOM VARIABLE FUNCTION IS TAKEN INTO ACOCUNT. THE MONTE CARLO METHOD ENABLES THE MOMENT W TO BE ESTIMATED AND THE ESTIMATOR VARIANCE TO BE OBTAINED. SOME TECHNIQUES FOR THE CONSTRUCTION OF OTHER ESTIMATORS OF W WITH A REDUCED VARIANCE ARE INTRODUCED. NA-NOT AVAILABL...|$|E
30|$|Self-avoiding and self-repelling random walks are {{variants}} of random walks which bias the walk towards unvisited nodes [22]. The <b>unformity</b> in coverage of such random walks in 2 -d lattices has {{been pointed out}} in [28]. Our paper extends the analysis of self-repelling random walks presented in [28] for application in mobile ad-hoc networks that are modeled as time varying random geometric graphs. Further, we show that by complementing self-repelling random walks with a push phase, we can complete aggergation in O(N) time and messages. The idea of locally biasing random walks and its impact in speeding up coverage has been pointed out in [29] for static networks. Self-repelling random walks are different than the local bias technique presented in [29]. Moreover, we show how to improve the convergence of self-repelling random walks using a complementary push-phase and demonstrate our results on mobile networks.|$|E
40|$|Abstract. The Gamma-ray Large Area Space Telescope (GLAST) is a next {{generation}} high energy gamma-ray observatory due for launch in Fall 2007. The primary instrument is the Large Area Telescope (LAT), which will measure gamma-ray flux and spectra from 20 MeV to> 300 GeV {{and is a}} successor to the highly successful EGRET experiment on CGRO. The LAT will have better angular resolution, greater effective area, wider field of view and broader energy coverage than any previous experiment in this energy range. This poster will present performance estimates with particular emphasis on how these apply to studies of microquasars. The LAT’s scanning mode will provide unprecedented <b>unformity</b> of sky coverage and permit measurements of light curves for any source. We will show results from recent detailed simulations that illustrate {{the potential of the}} LAT to observe microquasar variability and spectra, including source sensitivity and ability to detect orbital modulation. PACS: 95. 75. -z, 95. 75. Mn, 95. 75. Wx, 95. 80. +p, 95. 85. P...|$|E
40|$|A {{corrugated}} {{quantum well}} infrared photodetector (C-QWIP) {{focal plane array}} (FPA) with cutoff at 11. 2 tm has been fabricated and characterized. The C-QWIP array uses total internal reflection to couple normal incident light into the pixels. The processing steps involve only one chemical etching, one optional reactive ion etching, and one ohmic contact metalization. The detector array has 256 x 256 pixel elements, indium bumped to a direct injection readout circuit. The photocurrent to dark current ratio measured in this FPA, on which the noise equivalent temperature difference (NEAT) depends, is consistent {{with that of a}} large area test sample. The array shows good responsivity <b>unformity</b> (cs) of 52 % with no extra leakage current resulted from array processing. The estimated NEAT ofthis array, excluding the readout noise, is 17 mK at I 63 K. The fact that this FPA can be operated at a temperature similar to those of standard QWIP arrays with much shorter wavelengths shows that the C-QWIP structure can greatly increase array performance...|$|E
40|$|The ~ [...] calcutus [9] is a paradigmatical process {{calculus}} for message-passing con-currency. Two processes with {{acquaintance of}} a given name {{can use it to}} interact with each other. Names themselves may be exchanged in communications, which can model modifications of the linkage structure among processes. These are the basic process constructs (using lower case for names and upper case for pro-cesses) : ~(b}. P, the output of b at a with P as continuation; a(b). P, an input at a with b placeholder for the name received in the input; P 1 I P 2, the parallel composition of the two processes; yaP, which makes name a local to P; and!P, which denotes a potentially-infinite number of copies of P in parallel. In this paper, we study the situation in which certain names are uniformly receptive. A name x is receptive in a process P if at any time P is able of offering an input at x (at least {{as long as there are}} processes that could send messages at x). The receptiveness of x is uniform if all inputs at x have the same continuation. Receptiveness ensures that any message sent at x can be immediately processed; <b>unformity</b> ensures that there is a unique way in which a message at x may b...|$|E
40|$|One of {{the main}} {{problems}} of observational cosmology {{is to determine the}} range in which a reliable measurement of galaxy correlations is possible. This corresponds to determine the shape of the correlation function, its possible evolution with redshift and the size and amplitude of large scale structures. Different selection effects, inevitably entering in any observation, introduce important constraints in the measurement of correlations. In the context of galaxy redshift surveys selection effects can be caused by observational techniques and strategies and by implicit assumptions used in the data analysis. Generally all these effects are taken into account by using pair-counting algorithms to measure two-point correlations. We review these methods stressing that they are based on the a-priori assumption that galaxy distribution is spatially homogeneous inside a given sample. We show that, when this assumption is not satisfied by the data, results of the correlation analysis are affected by finite size effects. In order to quantify these effects, we introduce a new method based on the computation of the gradient of galaxy counts along tiny cylinders. We show, by using artificial homogeneous and inhomogeneous point distributions, that this method is to identify redshift dependent selection effects and to disentangle them from the presence of large scale density fluctuations. We then apply this new method to several redshift catalogs and we find evidences that galaxy distribution, in those samples where selection effects are small enough, is characterized by power-law correlations with exponent γ= 0. 9 up to 20 Mpc/h followed by a change of slope that, in the range [20, 100] Mpc/h, corresponds to a power-law exponent γ= 0. 25. Whether a crossover to spatial <b>unformity</b> occurs at ∼ 100 Mpc/h cannot be clarified by the present data. Comment: 35 pages, Journal of Cosmology and Astroparticle Physics in the pres...|$|E
40|$|An active neutron fuel-rod scanner {{has been}} {{designed}} for the assay of fissile materials in mixed oxide fuel rods. A {sup 252 }Cf source {{is located at the}} center of the scanner very near the through-hole for the fuel rods. Spontaneous fission neutrons from the californium are moderated and induce fissions within the passing fuel rod. The rod continues past a combined gamma-ray and neutron shield where delayed gamma rays above 1 MeV are detected. The authors used the Monte Carlo neutron-photon (MCNP) code to design the scanner and review optimum materials and geometries. Applications of the rod scanner include accountability of fissile material for safeguards applications, quality control of the fissile content in a fuel rod, and the verification of reactivity potential for mixed oxide fuels. A passive neutron fuel-rod scanner {{has been designed}} for the assay of the plutonium in mixed oxide fuel rods. The {sup 240 }Pu-effective is measured by counting the spontaneous fission neutrons using a high-efficiency thermal-neutron detector. This passive neutron detector would be combined with a high-resolution gamma-ray system (HRGS) measurement to obtain the total plutonium from the plutonium isotopic ratios. A passive gamma-ray scanner has been designed for the measurement of the {sup 241 }Am and plutonium uniformity in mixed oxide fuel rods. The passive gamma-ray emissions from {sup 241 }Am (60 keV) and plutonium (150 - 400 keV) are used to verify the <b>unformity</b> of the fuel enrichment zones and to check for any pellets that are out of specification. The fuel rod is moved through the interior of an NaI(Tl) or a bismuth germanate detector to measure the passive gamma-ray emissions. A tungsten sleeve collimator is used in the through-hole to improve the pellet-to-pellet spatial resolution. The same detector is used to verify the plutonium uniformity in the pellets with a 13 -mm tungsten collimator. The low-resolution passive gamma system would be used in the unattended mode...|$|E
40|$|Case, JohnThe three content {{chapters}} of {{this doctoral dissertation}} involve each of the concepts Computability, Complexity, Constructivity & Provability from the title. One of these chapters is devoted to showing that un verifiable programs need not be obfuscated. An application casts some doubts on an interesting 1980 argument of Putnam's. It is also shown {{that there is an}} acceptable programming system, of course with infinitely many universal simulating programs, presented so that it has exactly one verifiable such universal program, and there is another acceptable system presented so that it has no verifiable universal programs. Another chapter was suggested by two functions in Rogers' book which are based on eventual, currently unknown patterns in the decimal expansion of ?. One of them is classically and not at all constructively provably computable, and there is no known algorithm for it. For the other it is unknown whether it is computable. A problem is that advances in relevant knowledge about these now unknown patterns in ? may destroy Rogers' examples. Presented in this chapter is a safer computable real to replace ? so that the associated first function retains its classically provable computability, but has un provability of the correctness of any particular program for it. For the second Rogers' function ? is replaced by a real with each bit linear-time computable in the length of its position, but with the associated second function provably un computable. The last chapter features some computability results which are shown to provably require non -constructivity, e. g., that the program equivalence problem for acceptable programming systems is not computably enumerable (c. e.). Characterized is how to divide this example problem into non-trivial cases of disjoint index sets, where showing each of these index sets to be non-c. e. has a kind of <b>unformity</b> not found for the full equivalence problem, and each set's being non-c. e. is of ostensibly lower degree of nonconstructivity. Lastly, some related results are presented for natural run-time bounded programming systems [...] with run time bounds {{all the way down to}} linear-time. This chapter suggests a Reverse Mathematics project to minimize non-constructivity here and elsewhere. University of Delaware, Department of Computer & Information SciencesPh. D...|$|E
40|$|INTRODUCTION: To achieve high {{precision}} prostate radiotherapy requires accurate delineation {{of the prostate}} combined with accurate targeting of treatment with image-guided techniques. MRI scans {{have been shown to}} have lower inter-observer variability in prostate contouring than CT scans. If dose planning could also be performed on MRI scans then uncertainties due to registration to a CT scan would be reduced, as well as the resources required to use two imaging modalities. The feasibility of dose planning directly on MRI scans is investigated in this study. METHODS: Ten patients treated at the Newcastle Mater Hospital had three 0. 9 × 7 mm gold markers implanted by a urologist under trans-rectal guidance. Each patient then underwent a planning CT with urethral contrast. The prostate was delineated on the CT for field definition as per our normal protocol. Patients were treated with daily on-line corrections using electronic portal images of the implanted markers. The patients also received a MRI scan in the treatment position following their planning CT. Several MRI sequences were utilized; a T 2 whole pelvis scan, a T 2 small field-of-view scan to visualise prostate borders, and a T 2 * gradient echo scan to visualise implanted markers. All scans were transferred to the Pinnacle treatment planning system. The CT and MRI scans were registered using bony anatomy. Dose plans were produced on both sets of scans. For the CT scans, plans were produced with full electron density information, a bulk uniform density of 1, and bulk density plus a density of 1. 3 assigned to the bone regions. For the MRI plans, uniform and uniform+bone densities were assigned to the scans and dose plans using the same beam arrangements produced. The doses to the ICRU point for the dose plans were then compared. RESULTS: Dose plans for two patients have been analyzed to date. Assigning a bulk uniform density to the CT scan was found to give average dose errors of 2. 7 % to the ICRU point compared to the full density plan. When the bulk density of bony anatomy was added, this was reduced to within 1 %. Bulk density MRI plans gave average dose errors of 3. 7 %, which was reduced to 2. 3 % with bulk density of bone added. [Figure 1. Example of bulk density CT and MRI dose plans. ] DISCUSSION & CONCLUSIONS: The CT results suggest that scans with bulk densities assigned produce reasonably accurate dose plans for prostate. By optimizing the densities used, further improvements may be achieved. However the errors when bulk densities were assigned to MRI scans were greater. This is due to differences in patient contour due to both MRI spatial uniformity and patient positioning differences. Futher work is required to quantify the errors due to spatial <b>unformity</b> differences with a rigid phantom...|$|E

