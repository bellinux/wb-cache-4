14|22|Public
5000|$|... where γ is the Greek letter gamma. For a CRT, the gamma {{that relates}} {{brightness}} to voltage is {{usually in the}} range 2.35 to 2.55; video look-up tables in computers usually adjust the system gamma to the range 1.8 to 2.2, {{which is in the}} region that makes a <b>uniform</b> <b>encoding</b> difference give approximately uniform perceptual brightness difference, as illustrated in the diagram {{at the top of this}} section.|$|E
5000|$|In {{the late}} 1980s, work began on {{developing}} a <b>uniform</b> <b>encoding</b> for a [...] "Universal Character Set" [...] (UCS) that would replace earlier language-specific encodings with one coordinated system. The {{goal was to}} include all required characters {{from most of the}} world's languages, as well as symbols from technical domains such as science, mathematics, and music. The original idea was to replace the typical 256-character encodings requiring 1 byte per character with an encoding using 216 = 65,536 values requiring 2 bytes per character. Two groups worked on this in parallel, the IEEE and the Unicode Consortium, the latter representing mostly manufacturers of computing equipment. The two groups attempted to synchronize their character assignments, so that the developing encodings would be mutually compatible. The early 2-byte encoding was usually called [...] "Unicode", but is now called [...] "UCS-2". UCS-2 differs from UTF-16 by being a constant length encoding and only capable of encoding characters of BMP, it is supported by many programs. However, [...] "UCS-2 should now be considered obsolete. It no longer refers to an encoding form in either 10646 or the Unicode Standard." ...|$|E
40|$|We {{present a}} {{formalization}} of the λσ⇑-calculus[9] in the Coq V 5. 8 system[6]. The principal axiomatized {{result is the}} confluence of this calculus. Furthermore we propose a <b>uniform</b> <b>encoding</b> for many-sorted first order term rewriting systems, on which we study the local confluence by critical pairs analysis. ...|$|E
40|$|Categorial {{unification}} grammars (CUGs) {{embody the}} essential properties of both unification and categorial grammar formalisms. Their efficient and <b>uniform</b> way of <b>encoding</b> linguistic knowledge in well-understood and widely used representations inakes them attractive for computational applications and for linguistic research...|$|R
40|$|Multiword Expressions {{present a}} {{challenge}} for language technology, given their flexible nature. Each type of multiword expression has its own characteristics, and providing a <b>uniform</b> lexical <b>encoding</b> for them is a difficult task to undertake. Nonetheless, {{in this paper we}} present an architecture for the lexical encoding of these expressions in a database, that takes into account their flexibility. This encoding extends in a straightforward manner the one required for simplex (single) words, and maximises the information contained for them in the description of multiwords. ...|$|R
40|$|Introduction: In {{a recent}} Computer column I deplored the {{continuing}} {{use in the}} telecommunications and computing industry of the ASCII and EBCDIC character sets, and condemned the incipient use of the Unicode scheme for text encoding, suggesting that something simpler was required to support, for example, simple interlingual text equipment applied to tasks such as e-mail. In the following I describe and advocate a basis for <b>uniform</b> text <b>encoding</b> systems, and possibly only one is needed, which will support digital use of all the world's writing systems both for encoding of text by people, and for practical digital interlingual storage and exchange of text, such as is sorely needed for Internet...|$|R
40|$|The {{compressing}} {{mapping of}} digital images {{have been proposed}} in the paper. This mapping has the property to decrease color and statistical redundancy of digital images. It is shown, that due to proposed mapping one could encode the chromatic components of the image using the adaptive <b>uniform</b> <b>encoding</b> with less than one byte length. When you are citing the document, use the following link [URL]...|$|E
40|$|We {{present a}} formalisation, in the theorem proving system Isabelle/HOL, of a linear type {{system for the}} pi calculus, {{including}} a proof of runtime safety of typed processes. The use of a <b>uniform</b> <b>encoding</b> of pi calculus syntax in a meta language, {{the development of a}} general theory of type environments, and the structured formalisation of the main proofs, facilitate the adaptation of the Isabelle theories and proof scripts to variations on the language and other type systems. Keywords: Types; pi calculus; automatic theorem proving; semantics. ...|$|E
40|$|We {{introduce}} {{an integrated}} tool for implementing and playing various diagnostic games. The tool uses a semantics hierarchy introduced in [6] to improve code sharing among di#erent diagnostic games {{and reduce the}} cost of introducing a new game. PlayGame synthesizes the winning strategy for a game using the evidence that is an abstract and <b>uniform</b> <b>encoding</b> of the proof computed by a checker, and hence instead of relying on a particular checker the tool works on a variety of checkers that can be extended to produce such an evidence. PlayGame implements a -calculus game and a full range of equivalence/preorder games on the Concurrency Workbench-New Century (CWB-NC) ...|$|E
40|$|Consider a {{distributed}} real-time {{program which}} is executed {{on a system}} with a limited set of hardware resources. Assume the program is required to satisfy some timing constraints, despite the occurrence of anticipated hardware failures. For efficient use of resources, scheduling decisions must be taken at run-time, considering deadlines, the load and hardware failures. The paper demonstrates how to reason about such dynamically scheduled programs {{in the framework of}} a timed process algebra and modal logic. The algebra provides a <b>uniform</b> process <b>encoding</b> of programs, hardware and schedulers, with an operational semantics of a process depending on the assumptions about faults. The logic specifies the timing properties of a process and verifies them via this fault-affected semantics, establishing fault-tolerance. The approach lends itself to application of existing tools and results supporting reasoning in process algebras and modal logics...|$|R
30|$|This study {{compared}} sequence complexity {{for both}} of the intermediate encodings. Interestingly, the complexity for BIN remains quite <b>uniform</b> over the <b>encoded</b> sequence, while LZW tends to have lower complexity scores {{in the front and}} higher scores {{in the rear of the}} sequence. Since LZW saves regular patterns in the front part to absorb them later in the rear end, there are not so many regular patterns in the end of the sequence. Also, structural complexity was compared with linguistic complexity (LC) and topological entropy (TE). They also showed similar behavior on intermediate encodings.|$|R
40|$|We {{present a}} method for testing the {{validity}} for any finite many-valued logic by using simple transformations into the validity problem for von Wright’s logic of elsewhere. The method provides a new origi-nal viewpoint on finite many-valuedness. Indeed, we present a <b>uniform</b> modal <b>encoding</b> of any finite many-valued logic that views truth-values as nominals. Improvements of the transformations are discussed and the translation technique is extended to any finite annotated logic. Using similar ideas, we conclude the paper by defining transforma-tions from the validity problem for any finite many-valued logic into TAUT (the validity problem for the classical propositional calculus). As already known, this sharply illustrates that reasoning within a finite many-valued logic can be naturally and easily encoded into a two-valued logic. All the many-one reductions in the paper are tight since they require only time in O(n. log n) and space in O(log n). Key-words: finite many-valued logic, modal logic of elsewhere, many-one reduction...|$|R
40|$|AbstractRegular-SAT is a {{constraint}} {{programming language}} between CSP and SAT that—by combining {{many of the}} good properties of each paradigm—offers a good compromise between performance and expressive power. Its similarity to SAT allows us to define a <b>uniform</b> <b>encoding</b> formalism, to extend existing SAT algorithms to Regular-SAT without incurring excessive overhead in terms of computational cost, and to identify phase transition phenomena in randomly generated instances. On the other hand, Regular-SAT inherits from CSP more compact and natural encodings that maintain more {{the structure of the}} original problem. Our experimental results—using a range of benchmark problems—provide evidence that Regular-SAT offers practical computational advantages for solving combinatorial problems...|$|E
40|$|We {{present a}} {{distributed}} algorithm for computing equilibria of heterogeneous nonmonotonic multi-context systems (MCS). The algorithm can be parametrized to compute only partial equilibria, {{which can be}} used for reasoning tasks like query answering or satisfiability checking that need only partial information and not whole belief states. Furthermore, caching is employed to cut redundant solver calls. As a showcase, we instantiate the MCS framework with answer set program contexts. To characterize equilibria of such MCS, we develop notions of loop formulas that enable reductions to the classical satisfiability problem (SAT). Notably, loop formulas for bridge rules between contexts and for the local contexts can be combined to a <b>uniform</b> <b>encoding</b> of an MCS into a (distributed) SAT instance. As a consequence, we can use SAT solvers for belief set building. We demonstrate this approach by an experimental prototype implementation, which uses an off-the-shelf SAT solver...|$|E
40|$|An {{important}} issue towards a broader acceptance of answer-set programming (ASP) is {{the deployment of}} tools which support the programmer during the coding phase. In particular, methods for debugging an answer-set program are recognised as a crucial step in this regard. Initial work on debugging in ASP mainly focused on propositional programs, yet practical debuggers need to handle programs with variables as well. In this paper, we discuss a debugging technique that is directly geared towards non-ground programs. Following previous work, we address the central debugging question why some interpretation is not an answer set. The explanations provided by our method are computed {{by means of a}} meta-programming technique, using a <b>uniform</b> <b>encoding</b> of a debugging request in terms of ASP itself. Our method also permits programs containing comparison predicates and integer arithmetics, thus covering a relevant language class commonly supported by all state-ofthe-art ASP solvers...|$|E
40|$|Category theory {{provides}} a <b>uniform</b> method of <b>encoding</b> mathematical structures and universal constructions with them. In {{this article we}} apply the method of additional structures on the objects of a category to deform a comonoid structure, used implicitly in all categories. To deform this comultiplication we consider internal categories in a monoidal category with some special properties. Then we consider structures over comonoids and show that deformed internal categories form a 2 -category. This provides the possibility to study, in a uniform way, different types of generalized multiplicative and comultiplicative structures of 2 -dimensional Topological Quantum Field Theory...|$|R
40|$|Within {{the quantum}} Darwinist {{framework}} introduced by W. H. Zurek (Nat. Phys., 5 : 181 - 188, 2009), observers obtain pointer-state information about quantum systems by interacting {{with a local}} sample of the surrounding environment, e. g. a local sample of the ambient photon field. Because the environment encodes such pointer state information uniformly and hence redundantly throughout its entire volume, the information is equally available to all observers regardless of their location. This framework {{is applied to the}} observation of stellar center-of-mass positions, which are assumed to be encoded by the ambient photon field {{in a way that is}} uniformly accessible to all possible observers. Assuming Landauer's Principle, constructing such environmental encodings requires (ln 2) kT per encoded bit. For the observed 10 ^ 24 stars and a <b>uniform</b> binary <b>encoding</b> of center-of-mass positions into voxels with a linear dimension of 5 km, the free energy required at the current CMB temperature T = 2. 7 K is ∼ 2. 5 · 10 ^- 27 kg · m^- 3, strikingly close to the observed value of Ω_Λρ_c. Decreasing the voxel size to (l_P) ^ 3 results in a free energy requirement 10 ^ 117 times larger. Comment: 12 pp; 3 fig...|$|R
40|$|Model-based {{diagnosis}} {{has traditionally}} operated on hardware systems. However, in most complex systems today, hardware is augmented with software functions {{that influence the}} system’s behavior. In this paper hardware models are extended to include the behavior of associated embedded software, resulting in more comprehensive diagnoses. Capturing the behavior of software is much more complex than that of hardware due to the potentially enormous state space of a program. This complexity is addressed by using probabilistic, hierarchical, constraint-based automata (PHCA) that allow the <b>uniform</b> and compact <b>encoding</b> of both hardware and software behavior. We introduce a novel approach that frames PHCA-based diagnosis as a soft constraint optimization problem over a finite time horizon. The problem is solved using efficient, decomposition-based optimization techniques. The solutions correspond to the most likely evolutions of the software-extended system. ...|$|R
40|$|Although {{most of the}} {{relevant}} dictionary productions of the recent past have relied on digital data and methods, there is little consensus on formats and standards. The Institute for Corpus Linguistics and Text Technology (ICLTT) of the Austrian Academy of Sciences has been conducting a number of varied lexicographic projects, both digitising print dictionaries and working {{on the creation of}} genuinely digital lexicographic data. This data was designed to serve varying purposes: machine-readability was only one. A second goal was interoperability with digital NLP tools. To achieve this end, a <b>uniform</b> <b>encoding</b> system applicable across all the projects was developed. The paper describes the constraints imposed on the content models of the various elements of the TEI dictionary module and provides arguments in favour of TEI P 5 as an encoding system not only being used to represent digitised print dictionaries but also for NLP purposes...|$|E
40|$|We {{present a}} formalisation, in the theorem proving system Isabelle/HOL, of a linear type {{system for the}} pi calculus, {{including}} a proof of runtime safety of typed processes. The use of a <b>uniform</b> <b>encoding</b> of pi calculus syntax in a meta language, {{the development of a}} general theory of type environments, and the structured formalisation of the main proofs, facilitate the adaptation of the Isabelle theories and proof scripts to variations on the language and other type systems. 1 Introduction Static type systems are an established feature of programming languages, and the range of type systems is proliferating. The general principle is that a program which has been typechecked (at compile time) is guaranteed to satisfy a particular runtime property: for example (traditionally) that all functions are called with the correct number and type of arguments, or (more exotically) that some security property, such as secrecy, holds. In order for typechecking to give a proper guarantee of [...] ...|$|E
40|$|Abstract. We study fusion and binding {{mechanisms}} in name passing process calculi. To this purpose, we introduce the U-Calculus, a process calculus with no I/O polarities and a unique form of binding. The latter {{can be used}} both to control the scope of fusions and to handle new name generation. This is achieved {{by means of a}} simple form of typing: each bound name x is annotated with a set of exceptions, that is names that cannot be fused to x. The new calculus is proven to be more expressive than pi-calculus and Fusion calculus separately. In U-Calculus, the syntactic nesting of name binders has a semantic meaning, which cannot be overcome by the ordering of name extrusions at runtime. Thanks to this mixture of static and dynamic ordering of names, U-Calculus admits a form of labelled bisimulation which is a congruence. This property yields a substantial improvement with respect to previous proposals by the same authors aimed at unifying the above two languages. The additional expressiveness of U-Calculus is also explored by providing a <b>uniform</b> <b>encoding</b> of mixed guarded choice into the choice-free sub-calculus. ...|$|E
40|$|The {{current work}} proposes {{a model for}} {{prediction}} of breast cancer using the classification approach in data mining. The proposed model is based on various parameters, including symptoms of breast cancer, gene mutation and other risk factors causing breast cancer. Mutations have been predicted in breast cancer causing genes {{with the help of}} alignment of normal and abnormal gene sequences; then predicting the class label of breast cancer (risky or safe) on the basis of IF-THEN rules, using Genetic Algorithm (GA). In this work, GA has used variable gene encoding mechanisms for chromosomes <b>encoding,</b> <b>uniform</b> population generations and selects two chromosomes by Roulette-Wheel selection technique for two-point crossover, which gives better solutions. The performance of the model is evaluated using the F score measure, Matthews Correlation Coefficient (MCC) and Receiver Operating Characteristic (ROC) by plotting points (Sensitivity V/s 1 - Specificity) ...|$|R
40|$|Model-based {{diagnosis}} {{has largely}} operated on hardware systems. However, in most complex systems today, hardware is augmented with software functions {{that influence the}} system’s behavior. In this paper, hardware models are extended to include the behavior of associated embedded software, resulting in more comprehensive diagnoses. Prior work introduced probabilistic, hierarchical, constraint-based automata (PHCA) to allow the <b>uniform</b> and compact <b>encoding</b> of both hardware and software behavior. This paper focuses on PHCA-based monitoring and diagnosis to ensure the robustness of complex systems. We introduce a novel approach that frames diagnosis over a finite time horizon as a soft constraint optimization problem (COP), allowing us to leverage an extensive body of efficient solution methods for COPs. The solutions to the COP correspond to the most likely evolutions of the complex system. We demonstrate our approach on a vision-based rover navigation system, and models of the SPHERES and Earth Observing One spacecraft...|$|R
40|$|We {{introduce}} a capability for online monitoring and diagnosis of stochastic systems with complex behavior. Our work complements offline verification techniques for embedded systems. In most complex systems today, hardware is augmented with software functions {{that influence the}} system’s behavior. In this paper hardware models are extended to include the behavior of associated embedded software, resulting in more comprehensive estimates of a system’s state trajectories. Capturing the behavior of software is much more complex than that of hardware due to the potentially enormous state space of a program. This complexity is addressed by using probabilistic, hierarchical, constraint-based automata (PHCA) that allow the <b>uniform</b> and compact <b>encoding</b> of both hardware and software behavior. We {{introduce a}} novel approach that frames PHCA-based diagnosis as a soft constraint optimization problem over a finite time horizon. The problem is solved using efficient, decomposition-based optimization techniques. The solutions correspond to the most likely evolutions of the software-extended system...|$|R
40|$|This paper {{proposes a}} corpus {{encoding}} standard {{that meets the}} needs of linguistic research using a variety of linguistic data structures. The standard was developed in SFB 441, a research project at the University of Tuebingen. The principal concern of SFB 441 are the empirical data structures which feed into linguistic theory building. SFB 441 consists of several projects, most of which are building corpora to empirically investigate various linguistic phenomena in various languages (e. g. modal verbs in German, forms of address and politeness in Russian). These corpora will form the components of the "Tuebingen collection of reusable, empirical, linguistic data structures (TUSNELDA) ". The TUSNELDA annotation standard aims at providing a <b>uniform</b> <b>encoding</b> scheme for all subcorpora and texts of TUSNELDA such that they can be processed with uniform standardized tools. To guarantee maximal reusability we use XML for encoding. Previous SGML standards for text encoding were provided by the Text Encoding Initiative (TEI) and the Expert Advisory Group on Language Engineering Standards (Corpus Encoding Standard, CES). The TUSNELDA standard is based on TEI and XCES (XML version of CES) but takes into account the specific needs of the SFB projects, i. e. the peculiarities of the examined languages and linguistic phenomena...|$|E
30|$|To {{overcome}} {{the lack of}} HDR objective metrics, LDR metrics, e.g., PSNR, were also used to evaluate HDR quality, especially in early HDR studies. However, LDR metrics are designed for gamma encoded images, typically having luminance values in the range 0.1 – 100 cd/m 2, while HDR images have linear values and are meant to capture a much wider range of luminance. Originally, gamma encoding was developed {{to compensate for the}} characteristics of cathode ray tube (CRT) displays, but it also takes advantage of the non-linearity in HVS to optimize quantization when encoding an image [16]. Under common illumination conditions, the HVS is more sensitive to relative differences between darker and brighter tones. According to Weber’s law, the HVS sensitivity approximately follows a logarithm function for light luminance values [17]. Therefore, in several studies, LDR metrics have been computed in the log domain to predict HDR quality. However, at the darkest levels, the HVS sensitivity is closer to a square-root behavior, according to Rose-DeVries law [18, 19]. To extend the range of LDR metrics and to consider the sensitivity of the HVS, Aydin et al. [20] have proposed the perceptually <b>uniform</b> (PU) <b>encoding.</b> Another approach to apply LDR metrics on HDR images was proposed in [21]. This technique consists in tone-mapping the HDR image to several LDR images with different exposure ranges and to take the average objective score computed on each exposure. However, this approach is more time consuming and requires more computational power, proportionally to the number of exposures.|$|R
30|$|A {{major problem}} with {{objective}} priors {{is that they}} are hard to defend—particularly in the context of research with LSAs. For example, perhaps the most extreme version of an objective prior is the so-called <b>uniform</b> prior which <b>encodes</b> total ignorance about the average value and precision of a parameter. For example, in the case of pre-primary education, using a uniform prior distribution ranging from -∞ to +∞ would say that all values of the causal effect across the real numbers are equally likely. It should be noted, however, that the Bayesian literature has developed a large number of objective (aka “reference”) priors that can also be used for Bayesian inference as well as comparisons to models that use subjective priors (e.g, Jeffreys’ prior or the maximum entropy prior); but these priors simply provide different ways of quantifying the notion of complete uncertainty about a causal effect. Nevertheless, I agree with Berger (2006), that reference priors should be used “in scenarios in which a subjective analysis is not tenable”, although I believe that these scenarios are now rare in the world of LSA.|$|R
40|$|This paper {{introduces}} {{and develops}} an algebra over triadic relations, that is, relations whose contents are only triples. In essence, the algebra is {{a variation of}} relational algebra, defined over relations with exactly three attributes and closed for {{the same set of}} relations. Ternary relations are important because they provide the minimal, and thus most <b>uniform,</b> way to <b>encode</b> semantics wherein metadata may be treated uniformly with regular data; this fact has been recognized in the choice of triples to formalize the “Semantic Web”. Indeed, algebraic definitions corresponding to certain of these formalisms will be shown as examples. An important aspect of this algebra is an encoding of triples, implementing a kind of reification. The algebra is shown to be equivalent, over non-reified values, to a restriction of Datalog and hence to a fragment of first order logic. Furthermore, the algebra requires only two operators if certain fixed infinitary constants (similar to Tarski’s identity) are present. In this case, all structure is represented only in the data, that is, in encodings represented by these infinitary constants. 1...|$|R
40|$|A {{well-known}} result by Palamidessi {{tells us}} that (the π-calculus with mixed choice) is more expressive than (its subset with only separate choice). The proof of this result argues with their different expressive power concerning leader election in symmetric networks. Later on, Gorla offered an arguably simpler proof that, instead of leader election in symmetric networks, employed the reducibility of incestual processes (mixed choices that include both enabled senders and receivers for the same channel) when running two copies in parallel. In both proofs, the role of breaking (initial) symmetries {{is more or less}} apparent. In this paper, we shed more light on this role by re-proving the above result - based on a proper formalization {{of what it means to}} break symmetries without referring to another layer of the distinguishing problem domain of leader election. Both Palamidessi and Gorla rephrased their results by stating that there is no <b>uniform</b> and reasonable <b>encoding</b> from into. We indicate how the respective proofs can be adapted and exhibit the consequences of varying notions of uniformity and reasonableness. In each case, the ability to break initial symmetries turns out to be essential. Comment: In Proceedings EXPRESS' 10, arXiv: 1011. 601...|$|R
40|$|At present, when an {{international}} communication across health systems is quite common, {{there is a}} growing need to establish a common language which could be used for an expert´s communications from different countries and disciplines. A <b>uniform</b> language for <b>encoding</b> a wide range of information is provided by classifications that belong to a Family of International Classifications of World Health Organization (WHO-FIC). International Classification of Functioning, Disability and Health (ICF) was officially approved in 2001 and it forms and a conceptual framework for describing the disability. In the Czech Republic the Communication No. 431 / 2009 Coll. came into effect from 1 July 2010; it introduces ICF classification into a clinical practice. The physiotherapy {{is a part of the}} comprehensive rehabilitation system and people with the disability form a target group of patients for this field of specialization. We use many specific methods to examine motion systems and all his parts, the most of them are subjective. Still, we must find a way how to code the information obtained during the examination under a single scheme and the universal language given by ICF. This thesis demonstrate an importance of ICF classification and it could be a suggestion how to use the classification for an evaluation of the patient´s functional status by selected examination methods in an everyday practice of physiotherapists...|$|R
40|$|Recent {{advances}} in video capturing and display technologies, {{along with the}} exponentially increasing demand of video services, challenge the video coding research community to design new algorithms able to significantly improve the compression performance of the current H. 264 /AVC standard. This target is currently gaining evidence with the standardization activities in the High Efficiency Video Coding (HEVC) project. The distortion models used in HEVC are mean squared error (MSE) and sum of absolute difference (SAD). However, they are widely criticized for not correlating well with perceptual image quality. The structural similarity (SSIM) index {{has been found to}} be a good indicator of perceived image quality. Meanwhile, it is computationally simple compared with other state-of-the-art perceptual quality measures and has a number of desirable mathematical properties for optimization tasks. We propose a perceptual video coding method to improve upon the current HEVC based on an SSIM-inspired divisive normalization scheme as an attempt to transform the DCT domain frame prediction residuals to a perceptually <b>uniform</b> space before <b>encoding.</b> Based on the residual divisive normalization process, we define a distortion model for mode selection and show that such a divisive normalization strategy largely simplifies the subsequent perceptual rate-distortion optimization procedure. We further adjust the divisive normalization factors based on local content of the video frame. Experiments show that the scheme can achieve significant gain in terms of rate-SSIM performance and better visual quality when compared with HEVC...|$|R
40|$|Abstract—Recent {{advances}} in video capturing and display technologies, {{along with the}} exponentially increasing demand of video services, challenge the video coding research community to design new algorithms able to significantly improve the compression performance of the current H. 264 /AVC standard. This target is currently gaining evidence with the standardization activities in the High Efficiency Video Coding (HEVC) project. The distortion models used in HEVC are mean squared error (MSE) and sum of absolute difference (SAD). However, they are widely criticized for not correlating well with perceptual image quality. The structural similarity (SSIM) index {{has been found to}} be a good indicator of perceived image quality. Meanwhile, it is computationally simple compared with other state-of-the-art perceptual quality measures and has a number of desirable mathematical properties for optimization tasks. We propose a perceptual video coding method to improve upon the current HEVC based on an SSIM-inspired divisive normalization scheme as an attempt to transform the DCT domain frame prediction residuals to a perceptually <b>uniform</b> space before <b>encoding.</b> Based on the residual divisive normalization process, we define a distortion model for mode selection and show that such a divisive normalization strategy largely simplifies the subsequent perceptual rate-distortion optimization procedure. We further adjust the divisive normalization factors based on local content of the video frame. Experiments show that the proposed scheme can achieve significant gain in terms of rate-SSIM performance when compared with HEVC. Index Terms—SSIM index; HEVC; rate distortion optimiza-tion; residual divisive normalization; I...|$|R
40|$|Dieser Beitrag ist mit Zustimmung des Rechteinhabers aufgrund einer (DFG geförderten) Allianz- bzw. Nationallizenz frei zugänglich. This {{publication}} is {{with permission}} of the rights owner freely accessible due to an Alliance licence and a national licence (funded by the DFG, German Research Foundation) respectively. A well-known result by Palamidessi tells us that πmix (the π-calculus with mixed choice) is more expressive than πsep (its subset with only separate choice). The proof of this result analyses their different expressive power concerning leader election in symmetric networks. Later on, Gorla offered an arguably simpler proof that, instead of leader election in symmetric networks, employed the reducibility of ‘incestual’ processes (mixed choices that include both enabled senders and receivers for the same channel) when running two copies in parallel. In both proofs, the role of breaking (initial) symmetries {{is more or less}} apparent. In this paper, we shed more light on this role by re-proving the above result – based on a proper formalization {{of what it means to}} break symmetries – without referring to another problem domain like leader election. Both Palamidessi and Gorla rephrased their results by stating that there is no <b>uniform</b> and reasonable <b>encoding</b> from πmix into πsep. We indicate how their proofs can be adapted and exhibit the consequences of varying notions of uniformity and reasonableness. In each case, the ability to break initial symmetries turns out to be essential. Moreover, by abandoning the uniformity criterion, we show that there indeed is a reasonable encoding. We emphasize its underlying principle, which highlights the difference between breaking symmetries locally instead of globally...|$|R
40|$|The {{distribution}} of colours in an image {{has proven to}} be very useful for object recognition. Building on Swain's colour indexing (1991), colour distributions are now an integral part of many recognition schemes. This {{is not to say that}} colour alone suffices but rather that colour is one important cue that aids recognition. In this paper we look at colour distribution based recognition from a rather novel image processing perspective. Specifically we view the {{distribution of}} colours in an image as an image and so recast colour distribution matching as a problem of image comparison. Two results are reported here. First that, by compressing images we can improve matching efficiency (recognize objects more quickly). Second, that the degree of compression (and so speedup) that is possible depends on the colour space on top of which the distribution images are built. The more <b>uniform</b> opponent colour <b>encoding</b> can be compressed more effectively compared with conventional rg-chromaticity encoding. We explain this in the following way: image processing is based on the assumption that all image locations are equal so, to treat colour space as an image, each colour location should also be equally likely. This is in fact approximately the case for the opponent chromaticity space. To validate our approach we repeated Swain's object recognition experiments. We show that the distribution of colours in an image (which Swain encoded with 4096 numbers) represented by 8 numbers (the projection coefficients onto an 8 -dimensional principal component basis) suffices to achieve the same (almost perfect) recognition rate. Our method delivers a 500 -fold speed up in indexing without loss of accuracy. This result scales to a second larger database of 140 image...|$|R
40|$|Rad se bavi izgledom i reprezentacijom vojnih uniformi, koje u Prvom svjetskom ratu postaju neizbježan i standardiziran ratni atribut. One su postale važan dio ratne ali i ukupne odjevne slike. Promjene ratnih uniformi istiskuju dotadašnju viziju uniforme i njezinu kodiranost, proizvodeći popunjavanje značenjskog prostora novim kodovima, a mogu se čitati na razini uniforme kao objekta, ali još više u retorici fotografske slike. Napredak industrijalizacije i masovne proizvodnje standardiziranih vojnih uniformi poklapa se s industrijom stvaranja i reproduciranja slike (fotografije) u tiskanim masovnim medijima, pa se sve informacije o promjenama uniforme čitaju kao promjene kodiranosti fotografske reprezentacije. Radom se želi pokazati vidljivim promjene koje su se dogodile u razvoju vojne uniforme kao objekta, kao i promjene kodiranosti unutar reprezentacije fotografske slike. {{became an}} {{inevitable}} and standardized wartime feature during WWI. Uniforms {{became an important}} part not only for wartime, but also as an overall clothing appearance. The changes in the military uniform style displaced the previous vision of the <b>uniform</b> and its <b>encoding,</b> thus saturating the semantic space with new codes, which can be read {{on the level of}} the uniform as an object, but even more so {{on the basis of the}} photographic image rhetoric. The progress of industrialization and mass production of standardized military uniforms coincided with the emergence of the industry of creating and reproducing images (i. e. photography) in the printed media, so all information about the changes in uniform styles can be read as the changes in the encoding of the photographic representation. The paper aims to shed light on the changes that took place in the development of the military uniform as an object and the changes of its encoding within the representation of the photographic image, and it is a part of a broader research, which is still underway, focused on military uniform, appearance, meaning as forms of clothing and representational image. The Centre for fashion and clothing research is doing research under the project “Male manifest: Construction of stereotypes and representations of ‘male fantasies’ – clothing, appearance, typology, and forms of social impact of male power during socialist Yugoslavia”. The project researches and problematizes the relationship between fashion and clothing and a significant part of it is dedicated to the establishment and representation of the male military uniform – with special focus on the visual rhetoric of a photographic image...|$|R

