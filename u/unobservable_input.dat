8|25|Public
3000|$|... {{is driven}} by the <b>unobservable</b> <b>input</b> forces now, we will provide an {{additional}} least-square filter to estimate the input forces and correct the estimation of state [...]...|$|E
40|$|We study a principal-agent model {{wherein the}} agent is better {{informed}} of the prospects of the project, and the project requires both an observable and <b>unobservable</b> <b>input.</b> We show (1) Performance pay may not be optimal, even if output is the only informative signal of an essential input; (2) Total surplus tends to be higher if one input is unobservable than if both inputs are observable; and (3) Bunching may arise amongst low and intermediate types. We explore the implications for push and pull programs used to encourage R&D activity, but our results have applications beyond this context...|$|E
40|$|This paper {{provides}} a general methodology for introducing capital accumulation into economies with private information and heterogeneous agents. The agents operate a stochastic neoclassical production technology with capital and labor input. I study a moral hazard economy with <b>unobservable</b> <b>input</b> (hidden action). I characterize the efficient allocation of capital, labor, and consumption in a stationary recursive competitive equilibrium. The economy is decentralized by the component planner approach developed by Atkeson and Lucas (1995). Accumulation of capital is facilitated by a "capital planner" {{who serves as}} a financial intermediary for the component planners. In the unique, feasible and non-degenerate stationary equilibrium, private information lowers the market-clearing interest rate below agents' discount rate. ...|$|E
40|$|We {{consider}} a general nn MIMO system excited by <b>unobservable</b> <b>inputs</b> that are spatially independent, cyclostationary with unknown statistics. Such a MIMO scenario appears in many applications, such as multi-user communications and separation of competing speakers in speech processing. A special {{case of this}} problem, i. e., a 22 system case, was recently addressed in [1] using frequency-domain correlations of the system output. In this paper we provide a set of conditions under which a general n n system is uniquely identifiable based on the second-order frequency-domain correlations of the system output. We provide a constructive proof for {{the uniqueness of the}} system solution, which could also serve as a basis for a practical algorithm for system identification...|$|R
40|$|In {{this paper}} a unified {{probabilistic}} framework for solving inverse {{problems in the}} presence of epistemic and aleatory uncertainty is presented. The aim is to establish a flexible theory that facilitates Bayesian data analysis in experimental scenarios as they are commonly met in engineering practice. Problems are addressed where learning about <b>unobservable</b> <b>inputs</b> of a forward model, e. g. reducing the epistemic uncertainty of fixed yet unknown parameters and/or quantifying the aleatory uncertainty of variable inputs, is based on processing response measurements. Approaches to Bayesian inversion, hierarchical modeling and uncertainty quantification are combined into a generic framework that eventually allows to interpret and accomplish this task as multilevel model calibration. A joint problem formulation, where quantities that are not of particular interest are marginalized out from a joint posterior distribution, or an intrinsically marginal formulation, which is based on an integrated likelihood function, can be chosen according to the inferential objective and computational convenience. Fully Bayesian probabilistic inversion, i. e. the inference the variability of <b>unobservable</b> model <b>inputs</b> across a number of experiments, is derived as a special case of multilevel inversion. Borrowing strength, i. e. the optimal estimation of experiment-specific unknown forward model inputs, is introduced as a means for combining information in inverse problems. Two related statistica...|$|R
40|$|A semiparametric multivariate fractionally cointegrated {{system is}} considered, {{integration}} orders possibly being unknown and I(0) <b>unobservable</b> <b>inputs</b> having nonparametric spectral density. Two {{estimates of the}} vector of cointegrating parameters [nu] are considered. One involves inverse spectral weighting {{and the other is}} unweighted but uses a spectral estimate at frequency zero. Both corresponding Wald statistics for testing linear restrictions on [nu] are shown to have a standard null [chi] 2 limit distribution under quite general conditions. Notably, this outcome is irrespective of whether cointegrating relations are "strong" (when the difference between integration orders of observables and cointegrating errors exceeds 1 / 2), or "weak" (when that difference is less than 1 / 2), or when both cases are involved. Finite-sample properties are examined in a Monte Carlo study and an empirical example is presented. Fractional cointegration Semiparametric model Unknown integration orders Standard inference...|$|R
40|$|This Paper {{studies the}} {{relationship}} between political wealth redistribution and the allocation of firm-ownership when production requires an <b>unobservable</b> <b>input.</b> The economy's wealth distribution affects the equilibrium interest rate and the allocation of entrepreneurial rents because wealth serves as a bonding device and determines agents’ ability and willingness to borrow. This leads to unconventional voting behaviour of the politically decisive middle class: the political preferences of middle and upper class voters coincide when redistribution only has an adverse interest-rate effect. Middle class voters vote with the lower class instead if redistribution enables them {{to get access to}} entrepreneurial rents. Technological change may in-duce dramatic changes in political outcomes and greater inequality pronounces the interest-rate effect and may lead to less redistribution. firm-ownership; inequality; moral hazard; redistibutive taxation...|$|E
40|$|This {{paper is}} a {{continuation}} of the work in [11] and [2] on the problem of estimating by a linear estimator, N <b>unobservable</b> <b>input</b> vectors, undergoing the same linear transformation, from noise-corrupted observable output vectors. Whereas in the aforementioned papers, only the matrix representing the linear transformation was assumed uncertain, here we are concerned with the case in which the second order statistics of the noise vectors (i. e., their covariance matrices) are also subjected to uncertainty. We seek a robust mean-squared error estimator immuned against both sources of uncertainty. We show that the optimal robust mean-squared error estimator has a special form represented by an elementary block circulant matrix, and moreover when the uncertainty sets are ellipsoidal-like, the problem of finding the optimal estimator matrix can be reduced to solving an explicit semidefinite programming problem, whose size is independent of N...|$|E
40|$|Abstract This paper {{studies the}} {{relationship}} between political wealth redistribu-tion and the allocation of ¯rm-ownership when production requires an <b>unobservable</b> <b>input.</b> The economy's wealth distribution a®ects the equilibrium interest rate and the allocation of entrepreneurial rents because wealth serves as a bonding device and de-termines agents ability and willingness to borrow. This leads to unconventional voting behavior of the politically decisive middle class: the political preferences of middle and upper class voters coincide when redistribution only has an adverse interest-rate e®ect. Middle class voters vote with the lower class instead if redistribution enables them {{to get access to}} entrepreneurial rents. Technological change may in-duce dramatic changes in political outcomes and greater inequality pronounces the interest-rate e®ect and may lead to less redistribution. We present some empirical evidence for the US that suggest that the proposed interest rate e®ect of inequality may be relevant in practice...|$|E
40|$|We {{present a}} novel frequency-domain {{framework}} for the identification of a multiple-input multiple-output (MIMO) system driven by white, mutually independent, <b>unobservable</b> <b>inputs.</b> The system frequency response is obtained based on singular value decomposition (SVD) of a matrix constructed based on the power-spectrum and slices of polyspectra of the system output. By appropriately selecting the polyspectra slices, we can create a set of such matrices, each of which could independently yield the solution, or they could all be combined in a joint diagonalization scheme to yield a solution with improved statistical performance. The freedom to select the polyspectra slices allows us to bypass the frequency-dependent permutation ambiguity that is usually associated with frequency domain SVD, {{while at the same}} time allows us compute and cancel the phase ambiguity. An asymptotic consistency analysis of the system magnitude response estimate is performed...|$|R
40|$|This {{research}} examines {{whether the}} fair value of mortgage servicing rights (MSRs) based on managerial inputs (Level 3) better reflects the cash flow and risk {{characteristics of the}} underlying assets than the fair value of MSRs based on market inputs (Level 2). Using mortgage servicing fees {{as a proxy for}} the underlying cash flows, we find that the valuation multiples for MSRs based on Level 3 inputs are more positively associated with the persistence of future servicing fees compared with the fair value of MSRs based on Level 2 inputs. We also document that only the valuation multiples based on Level 3 fair values are negatively associated with proxies for risk factors. Our results suggest that, although <b>unobservable</b> <b>inputs</b> are subject to managerial discretions, managers can generate higher quality fair value estimates than market inputs due to their information advantage, especially when the market for the underlying asset is inactive...|$|R
40|$|We {{consider}} {{the problem of}} identifying a n Θ n MIMO system excited by unobservable non-white and spatially independent inputs. Frequency-domain methods always associate a permutation ambiguity to the system frequency response, which unless computed and compensated for, results in an erroneous solution. We show that, from the permutation-dependent solution we can extract some quantities (invariances) which are independent of the permutation, and which, under certain conditions, {{can lead to the}} correct solution. I. Introduction The blind identification of a n Θ m Multiple-input multipleoutput (MIMO) system is of great importance in many applications, such as communications, biomedical engineering, seismology, etc [...] The goal of blind system identification is to identify an unknown system H(z), driven by n- <b>unobservable</b> <b>inputs,</b> based on the m system outputs, and subsequently use the system estimate to recover the input signals (sources). In this paper we deal with the case of c [...] ...|$|R
40|$|Objectives: To {{undertake}} a structured review and critical appraisal of methods for the model-based cost–utility analysis of screening programmes. Also to develop guidelines and an assessment checklist of good {{practice in the}} development of screening models. Data sources: Major electronic databases of healthcare and operational research literatures were searched up to June 2003. Review methods: Searches of the literature were undertaken to identify applied and methodological studies of economic evaluations of healthcare screening programmes. All applied screening models were also reviewed in three broad disease areas (cancer, cardiovascular disease and diabetes), as well as antenatal screening. A second-level review focused on particular aspects of the modelling process through case study assessments of screening models for three specific disease areas (colorectal cancer, abdominal aortic aneurysms and antenatal screening for haemoglobinopathies). A separate literature review of studies reporting the utility effects of screening was also undertaken. Guidelines and an assessment checklist for good practice for screening modelling were developed. Results: Few relevant methodological studies were identified, and no studies reporting direct empirical comparisons of alternative methodologies were retrieved. From the review of disease-based screening models, it was apparent that many alternative modelling methods had been applied, including some relatively new approaches that had not been widely disseminated. Natural history modelling is the preferred approach. Alternative modelling approaches were generally only used to extrapolate the observed effects of screening and were unsuitable for evaluating unobserved screening options. More complex model structures may incorporate important additional aspects of the disease natural history, although any benefits should outweigh the consequences of additional <b>unobservable</b> <b>input</b> parameters and increased preferred general method of evaluation for screening programmes. State transition models have generally been used to represent disease natural histories, with individual sampling models more prevalent than in treatment intervention evaluations. No comparative methodological studies were identified, so no empirical data were available to inform the relative merits of alternative methodologies. The defined guidelines and complexity in implementing the model. No direct comparisons of more detailed and less detailed screening model structures informed areas in which more realistic representations of the disease process may be most beneficial, so only general aspects of good practice could be defined. Two structural aspects that were not well handled by existing screening models included post-diagnosis disease progression and screening uptake. Most models described the former using historical mortality rates, rather than treatment models that are representative of current treatment patterns for different stages of the disease. Constant screening uptake rates were applied to all screening programmes and attendance was not linked to disease incidence or progression. Evidence exists to inform a more detailed representation of screening uptake. The most commonly applied modelling techniques were cohort Markov models and individual sampling simulation models. Individual sampling simulation models may provide more flexibility in their representation of a screening decision problem, but any benefits should outweigh the consequences of the need to assess both variability and uncertainty. Complex mathematical models describing input parameters as continuous variables have analysed the costeffectiveness of screening; these require further development to estimate the cost–utility of screening directly, or to inform a more detailed representation of the preclinical section of a natural history model (with a traditional state-based model describing pathways’ post-clinical presentation). Calibration is a common aspect of screening models, whereby models are fitted to observed data describing outputs of the model in order to populate unobserved input parameters. The review concluded that the estimation of a reference case input parameter set is not recommended. Conclusions: The review of methods for the modelbased cost–utility analysis of screening programmes identified the natural history modelling approach as theassessment checklist are informed, therefore, by theoretical interpretations of the impact of alternative approaches to different components of the modelling process when applied to the cost–utility analysis of screening programmes. Further research is needed into methods with the potential to improve the accuracy of screening models, and to respond to the needs of model users...|$|E
40|$|To {{undertake}} a structured review and critical appraisal of methods for the model-based cost-utility analysis of screening programmes. Also to develop guidelines and an assessment checklist of good {{practice in the}} development of screening models. Major electronic databases of healthcare and operational research literatures were searched up to June 2003. Searches of the literature were undertaken to identify applied and methodological studies of economic evaluations of healthcare screening programmes. All applied screening models were also reviewed in three broad disease areas (cancer, cardiovascular disease and diabetes), as well as antenatal screening. A second-level review focused on particular aspects of the modelling process through case study assessments of screening models for three specific disease areas (colorectal cancer, abdominal aortic aneurysms and antenatal screening for haemoglobinopathies). A separate literature review of studies reporting the utility effects of screening was also undertaken. Guidelines and an assessment checklist for good practice for screening modelling were developed. Few relevant methodological studies were identified, and no studies reporting direct empirical comparisons of alternative methodologies were retrieved. From the review of disease-based screening models, it was apparent that many alternative modelling methods had been applied, including some relatively new approaches that had not been widely disseminated. Natural history modelling is the preferred approach. Alternative modelling approaches were generally only used to extrapolate the observed effects of screening and were unsuitable for evaluating unobserved screening options. More complex model structures may incorporate important additional aspects of the disease natural history, although any benefits should outweigh the consequences of additional <b>unobservable</b> <b>input</b> parameters and increased complexity in implementing the model. No direct comparisons of more detailed and less detailed screening model structures informed areas in which more realistic representations of the disease process may be most beneficial, so only general aspects of good practice could be defined. Two structural aspects that were not well handled by existing screening models included post-diagnosis disease progression and screening uptake. Most models described the former using historical mortality rates, rather than treatment models that are representative of current treatment patterns for different stages of the disease. Constant screening uptake rates were applied to all screening programmes and attendance was not linked to disease incidence or progression. Evidence exists to inform a more detailed representation of screening uptake. The most commonly applied modelling techniques were cohort Markov models and individual sampling simulation models. Individual sampling simulation models may provide more flexibility in their representation of a screening decision problem, but any benefits should outweigh the consequences of the need to assess both variability and uncertainty. Complex mathematical models describing input parameters as continuous variables have analysed the cost-effectiveness of screening; these require further development to estimate the cost-utility of screening directly, or to inform a more detailed representation of the preclinical section of a natural history model (with a traditional state-based model describing pathways' post-clinical presentation). Calibration is a common aspect of screening models, whereby models are fitted to observed data describing outputs of the model in order to populate unobserved input parameters. The review concluded that the estimation of a reference case input parameter set is not recommended. The review of methods for the model-based cost-utility analysis of screening programmes identified the natural history modelling approach as the preferred general method of evaluation for screening programmes. State transition models have generally been used to represent disease natural histories, with individual sampling models more prevalent than in treatment intervention evaluations. No comparative methodological studies were identified, so no empirical data were available to inform the relative merits of alternative methodologies. The defined guidelines and assessment checklist are informed, therefore, by theoretical interpretations of the impact of alternative approaches to different components of the modelling process when applied to the cost-utility analysis of screening programmes. Further research is needed into methods with the potential to improve the accuracy of screening models, and to respond to the needs of model users. J Karnon, E Goyder, P Tappenden, S McPhie, I Towers, J Brazier and J Mada...|$|E
40|$|The {{measurement}} of fair values, {{particularly in the}} absence of quoted prices in active markets, is complex and difficult to verify. This paper examines whether banks use fair value estimates based on <b>unobservable</b> <b>inputs</b> (i. e., Level 3) to manage earnings during the 2008 financial crisis. Using a sample of 291 U. S. bank holding companies, we find that banks use discretionary Level 3 gains or losses to smooth earnings. We benchmark our findings against loan loss provisions (LLP), but we do not find consistent evidence that banks use LLP to smooth earnings, mainly because better corporate governance mechanisms effectively reduce discretion. However, better corporate governance does not reduce measurement discretion in Level 3. This finding suggests that monitoring mechanisms prevent excessive discretion for loans—which are measured at amortized cost—but have yet to develop to prevent similar discretion for investments that are measured at fair value...|$|R
40|$|A new {{approach}} {{to examine the relationship}} between the excess of forecast based on characteristics toward management forecast and business risk is provided in this research at companies listed on the stock exchange in Tehran. The customary (traditional (approach is based on the regression of management forecast errors of past years. Therefore, the observable and <b>unobservable</b> <b>inputs,</b> such as managements, incentive misalignment, are used to predict management forecast errors. In this study, the future earnings is at first estimated by using characteristics including earnings per share, loss indicator, Neg. accruals per share, Pos. accruals per share,asset growth, dividend indicator (non-payment of the dividend), Book-to-market value, share price and dividend per share for companies. Based on that, a criterion (CO) for estimating the earnings forecast error was developed, which is the alternative forecasted errors. One should notice that, business risk is considered as a measure of company performance. In this study, measures of business risk are volatility of earnings and dividend ratio. Research findings show that, there is a significant relationship between the CO and volatility of earnings, on the contrary there is no significant relationship between this criteria and dividend rati...|$|R
40|$|A semiparametric bivariate fractionally cointegrated {{system is}} considered, {{integration}} orders possibly being unknown and I (0) <b>unobservable</b> <b>inputs</b> having nonparametric spectral density. Two kinds of {{estimate of the}} cointegrating parameter Î½ are considered, one involving inverse spectral weighting and the other, unweighted statistics with a spectral estimate at frequency zero. We establish under quite general conditions the asymptotic distributional properties of the estimates of Î½, both in case of â 8 ̆ 09 ̆cstrong cointegration â 8 ̆ 09 ̆d (when the difference between integration orders of observables and cointegrating errors exceeds 1 / 2) and in case of â 8 ̆ 09 ̆cweak cointegrationâ 8 ̆ 09 ̆d (when that difference is less than 1 / 2), which includes the case of (asymptotically) stationary observables. Across both cases, the same Wald test statistic has the same standard null Ï 8 ̆ 7 2 limit distribution, irrespective of whether integration orders are known or estimated. The regularity conditions include unprimitive ones on the integration orders and spectral density estimates, but we check these under more primitive conditions on particular estimates. Finite-sample properties are examined in a Monte Carlo study...|$|R
40|$|International audienceIn {{this paper}} a unified {{probabilistic}} framework for solving inverse {{problems in the}} presence of epistemic and aleatory uncertainty is presented. The aim is to establish a flexible theory that facilitates Bayesian data analysis in experimental scenarios as they are commonly met in engineering practice. Problems are addressed where learning about <b>unobservable</b> <b>inputs</b> of a forward model, e. g. reducing the epistemic uncertainty of fixed yet unknown parameters and/or quantifying the aleatory uncertainty of variable inputs, is based on processing response measurements. Approaches to Bayesian inversion, hierarchical modeling and uncertainty quantification are combined into a generic framework that eventually allows to interpret and accomplish this task as multilevel model calibration. A joint problem formulation, where quantities that are not of particular interest are marginalized out from a joint posterior distribution, or an intrinsically marginal formulation, which is based on an integrated likelihood function, can be chosen according to the inferential objective and computational convenience. Fully Bayesian probabilistic inversion, i. e. the inference the variability of <b>unobservable</b> model <b>inputs</b> across a number of experiments, is derived as a special case of multilevel inversion. Borrowing strength, i. e. the optimal estimation of experiment-specific unknown forward model inputs, is introduced as a means for combining information in inverse problems. Two related statistical models for situations involving finite or zero model/measurement error are devised. Multilevel-specific obstacles to Bayesian posterior computation via Markov chain Monte Carlo are discussed. The inferential machinery of Bayesian multilevel model calibration and its underlying flow of information are studied {{on the basis of a}} system from the domain of civil engineering. A population of identically manufactured structural elements serves as an exemplary system for examining different experimental settings from the standpoint of uncertainty quantification and reduction. In a series of tests the material variability throughout the ensemble of specimens, the entirety of specimen-specific material properties and the measurement error level are inferred under various uncertainties in the problem setup...|$|R
40|$|With {{the recent}} {{financial}} crisis {{that have happened}} and the global move towards fair value accounting financial {{institutions such as the}} IASB saw fit to increase the mandatory disclosure requirements by implementing IFRS 13 to regulated fair value accounting in IFRS. The implementation of IFRS 13 means that many of the old standards in regards to fair value will be replaced, for example investment properties reporting under IAS 40. Furthermore IFRS 13 redefines the classification system for assets and liabilities.   The {{purpose of this study is}} to investigate whether these changes have any influence on investment decisions. This has been done by adopting a qualitative abductive descripto-explanatory approach to our research, and our empirical data was gathered through semi-structured interviews with academics and audit professionals.   The analysis of our empirical data suggests that the implementation of IFRS 13 and its increased disclosure requirements have been useful to investor’s decision making. IFRS 13 accomplishes this through its increased clarity in financial reporting. However investors should be mindful whilst investing in companies utilizing Level 3 valuation techniques because they use estimates of <b>unobservable</b> <b>inputs</b> and because such estimates are hard to control they are prone to bias, error, and manipulation...|$|R
40|$|ABSTRACT: Using {{disclosure}} {{mandated by}} Statement of Financial Accounting Standards (FAS) 157 “Fair Value Measurements, ” I evaluate {{the concern that}} fair value estimates for assets and liabilities not traded in active markets, aka mark-to-model, are too unreliable {{to be used in}} financial reporting. I document a significant positive association between stock prices and fair values of net assets measured using unadjusted market prices (Level 1), other observable inputs (Level 2), and significant <b>unobservable</b> <b>inputs</b> (Level 3). While the estimated coefficients on the mark-to-model estimates (Levels 2 and 3) are consistently lower than those on the mark-to-market fair values (Level 1), the difference is significant only for Level 3 net assets. Furthermore, even at its maximum, the difference does not exceed 35 % of the coefficient on Level 1 net assets. Additional analysis suggests that the valuation gap is more pronounced for firms with lower equity capital and fewer financial experts on the Audit Committee, as well as for companies that develop their mark-to-model estimates internally. I also document a significant positive association between Level 3 net gains and both quarterly returns and returns for the three-day period surrounding the filing of Form 10 -Q. Collectively, the results suggest that equity investors find mark-to-model fair value estimates sufficiently reliable to be reflected in firm value...|$|R
50|$|This was a reoccurring {{problem in}} the current {{financial}} crisis. Since the crisis unfolded, fair value assets held by banks increasingly became Level 3 <b>inputs</b> (<b>unobservable).</b> Ultimately, most of the assets held by financial institutions were either not subject to fair value, or did not impact the income statement or balance sheet accounts.|$|R
40|$|In {{light of}} the {{uncertainties}} about valuation highlighted by the 2007 – 08 market turbulence, this chapter provides an empirical examination of the potential procyclicality that fair value accounting (FVA) methods could introduce in bank balance sheets. The chapter finds that, while weaknesses in the FVA methodology may introduce unintended volatility and procyclicality, thus requiring some enhancements, {{it is still the}} preferred accounting framework for financial institutions. It concludes that capital buffers, forward-looking provisioning, and more refined disclosures can help to mitigate the procyclicality of FVA. The analysis presented does not preclude that there are other dimensions to FVA that are relevant and that, after further scrutiny, may indicate the need for additional refinements to the FVA methodology. Going forward, the valuation approaches for accounting, prudential measures, and risk management need to be reconciled and will require adjustments on the part of all parties. Since the 2007 market turmoil surrounding complex structured credit products, fair value accounting (FVA) and its application through the business cycle has been a topic of considerable debate. As the illiquidity of certain products became more severe, financial institutions turned increasingly to model-based valuations that, despite increased disclosure requirements, were nevertheless accompanied by growing opacity in the classification of products across the fair value (FV) spectrum. Moreover, under stressed liquidity conditions, financial institutions made wider use of <b>unobservable</b> <b>inputs</b> in their valuations, increasing uncertainty among financial institutions, supervisors, and investors regarding the valuation of financial products under such conditions. It has been during this period that the procyclical impact of FVA on bank balance sheets Note: This chapter was written by a team led b...|$|R
40|$|RESEARCH OBJECTIVES: The aim of {{this study}} was to {{investigate}} how do auditors attain the necessary assurance regarding fair value estimates. Audit risks concerning fair value estimates, the auditors' response to address those risks and specifically how do auditors cope with the challenges of verifiability, uncertainty and management bias in their audit work were all of particular interest. This study also examined how the new IFRS 13 fair value measurement standard would possibly impact the current practice of auditing fair values. METHODOLOGY: This study was done using qualitative interviews. Data was gathered by interviewing 5 auditors working for Big 4 audit firms (2 partners and 3 senior managers). Each respondent had at least 9 years of experience in auditing fair value estimates under IFRS. The interview was structured into two themes, and they were all recorded and extracted into writing. RESULTS: The primary audit risks concerning fair value estimates were accuracy, completeness, valuation and management override of controls. Internal control testing was considered only for items whose measurement process is either automated or is largely based on observable inputs. For estimates measured using <b>unobservable</b> <b>inputs,</b> auditors rely mainly on substantive testing, with substantive analytical procedures being most common. The auditors conduct the procedure by forming their own understanding on what the estimate should be by benchmarking the key inputs used behind the measurement against market data and actively using valuation experts. The new IFRS 13 standard did not have any major impact on auditing fair value. The minor impacts were simplifying the auditors work when looking for guidance and easing the argumentation process with the client. The standard's new disclosure requirements could help to shift part of the risk and responsibility concerning the estimates to the readers and users of the financial statement...|$|R
40|$|Background: The large {{reorganisation}} {{of financial}} {{instruments in the}} US banking sector prior to the recent financial crisis and the effects related to the crisis, raise concerns of similar accounting disclosures in Europe. The valuation of the Level 3 financial instruments is based on <b>unobservable</b> <b>inputs</b> and the instruments shall be valued at their fair value, in which information asymmetry may present itself through the subjectivity in the valuation mechanism. Research scope: The study is built {{on the notion that}} a high amount of Level 3 financial instruments results in a higher cost of capital. In relation to the main objective we have included control variables representing an overview of a bank’s business. The control variables are also subject to an in depth analysis. Research design: The correlation between Level 3 instruments and the cost of capital is examined through a statistical research, using CDS as a proxy for the cost of capital. The study consists of approximately 50 listed banks actively operating in the European Union, reflecting {{a large proportion of the}} asset base within the banking sector. The Level 3 variable as well as the control variables is examined through a linear regression analysis. Limitations: The study is limited to banks within the European Union as they are subject to the same economic regulation. The amendments to IFRS 7 were implemented in January of 2009 and as such the study encompasses both of the available years in order to establish a sound base of analysis. Empirical findings: We find no significant relationship between the amount of Level 3 financial instruments and the banks cost of capital. However, for 2010 the multiple regression analysis present depicts a significant relationship regarding the control variables as well as exhibiting a correlation to the cost of capital. Further research: We propose that future research include information observed over a longer period of time as well as examines an extended economical area due to the difference in the results received...|$|R
40|$|This thesis investigates whether U. S. banks&# 146; {{assets and}} liabilities, {{reported}} using Fair Value Accounting (FVA) under SFAS 157 Fair Value Measurement, {{are associated with}} information asymmetry among equity investors during the 2008 Global Financial Crisis. Using bid-ask spread {{as a proxy for}} information asymmetry and controlling for bank size, profitability, default risk and capital adequacy, I find that bid-ask spread is positively and significantly associated with total fair value net assets and net assets measured using Level 1 inputs (unadjusted observable inputs in active markets), Level 2 inputs (other indirect observable inputs), and Level 3 <b>inputs</b> (<b>unobservable</b> <b>inputs</b> reflecting firm&# 146;s own assumptions and models), as specified in SFAS 157. On the other hand, the U. S. Securities and Exchange Commission (2008) has alleged that large loan loss provisions, determined based on managerial internal information and discretion, {{played a significant role in}} bank failures in the Global Financial Crisis. I, therefore, examine whether banks&# 146; loan loss provisions, specifically Provisions for Loan Losses (PLL) appearing on the income statement and Allowance for Loan Losses (ALL) appearing on the balance sheet, are associated with information asymmetry. I find that both PLL and ALL are positively and significantly associated with bid-ask spread, with higher standardized coefficients than that of fair value net assets. As the economic condition kept worsening during the Global Financial Crisis, I also find that both fair value net assets and loan loss provisions are constantly and positively associated with information asymmetry across the four quarters of 2008. The results are robust using constant samples, analysing fair value assets and liabilities separately, using a sample of non-bank firms, and extending the dataset to the fiscal year 2009. In short, both fair value net assets and loan loss provisions are associated with information asymmetry among equity investors during the 2008 Global Financial Crisis, with loan loss provisions having the stronger of the two effects...|$|R
40|$|This paper {{presents}} an active output feedback fault-tolerant model predictive control (MPC) scheme for systems with sensor faults. The proposed control scheme actively steers {{the system in}} order to prevent loss of observability caused by a sensor fault. To this end, the standard tracking objective of the MPC controller is augmented with an observability cost term which strongly penalizes <b>unobservable</b> state and <b>input</b> trajectories. A numerical example illustrates the use of the proposed approach on a target estimation and tracking control problem with faulty sensors...|$|R
40|$|Despite {{the influx}} of {{measures}} which can be customized {{to the demands of}} each business unit (e. g., customer satisfaction surveys and quality indices), many firms have been dogged in their reliance on standardized measures (e. g., conventional financial metrics) in performance evaluation. In this paper, we consider one justification: though customized measures may more accurately target the goals of a particular unit, standardized measures may offer more meaningful opportunities for relative performance evaluation. Standardized measures have a commonality in errors which is naturally absent among measures targeted to each circumstance. This commonality allows learning about one measure from another and, thus, the construction of more efficient proxies for <b>unobservable</b> employee <b>inputs.</b> The use of comparative evaluation schemes is not without its challenges, since it may induce unwanted coordination by those being evaluated. Even with such gaming concerns, standardized measures can still be preferred, but the requirements are more stringent...|$|R
40|$|Asset value {{volatility}} is at {{the heart}} of the capital structure optimisation theory as proposed by Leland (1994). Asset value and volatility are two key <b>unobservable</b> <b>inputs</b> required for pricing equity and debt using structural models that are based on option pricing theory. These inputs are normally calculated using equity prices, book value of debt and leverage. However, when tested empirically, the prediction errors of structural models suggest that such calculations of asset value and volatility may be inaccurate. Furthermore, in alI structural models it is assumed that asset value volatility is constant over time, which may also lead to inaccurate credit spread predictions. This is the first empirical study into bond-implied asset'values and volatility. This study uses three-year time series of daily bond- and equity prices for 36 Western-European companies to derive bo?d-implied asset values and asset value volatilities. Bond-implied values were calculated using a structural credit model developed by Leland and Toft (1996). The values were obtained by simultaneously solving a system of two equations and two unknowns. The two equations were the value of a bond and the value of equity; the. two unknowns were asset value and asset value volatility. An equity-based method for calculating asset value and volatility was used to generate an alternative data set for the same daily observations. The two sets of data for each firm were compared to determine whether there are statistically significant differences between bond-implied and equity-based asset values and volatility, and whether bond-implied volatility is stable over time. Additionally,. the correlation between bond-implied volatility and theibond's time to maturity was tested to determine whether there is a constant relationship between these two variables. TIlls study shows that there are significant difference~ between asset values and asset value volatilities derived from bond prices and those derived using equity values and leverage. Additionally, it is shown that bond-implied asset value volatility is not constant over time. The relationship between bond-implied asset value volatility and a bond's time to maturity as measured by their correlation coefficient was not uniform across the sample, in some cases demon,strating strong positive correlation, in other cases strong negative correlation and in some cases no strong relationship at alI. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|Abstract: Two {{ubiquitous}} {{problems in}} econometric models of production systems are aggregation {{and whether the}} variables are empirically observable. Aggregation is both an unavoidable and a useful data management strategy. Aggregation increases the statistical precision of economic variables such as the yield per acre of a crop in a field, farm, or region. Many production processes are subject to production shocks. As a result, planned outputs and realized product prices are both <b>unobservable</b> when <b>inputs</b> are committed. Expectations processes are notoriously difficult to model, especially when working with aggregate data or decision makers who are averse to risk. This paper derives the complete class of input demand systems that are exactly aggregable, can be specified and estimated with observable data, and are consistent with economic theory for all risk preferences. We extend this to a new, general, and flexible class of input demand systems {{that can be used}} to nest and test for aggregation, global economic regularity, functional form, and flexibility. Almost all existing input demand systems are restricted cases of this class. Key Words: Aggregation, input demands, observable variables, ran...|$|R
40|$|The {{problem of}} {{parameter}} estimation and blind deconvolution of AR systems with independent nonstationary binary inputs is considered. The estimation procedure consists of applying an MA filter (equalizer) to the observed data and adjusting {{the parameters of}} the filter so as to minimize a criterion that measures the binariness of its output. The output sequence itself serves as an estimate of the <b>unobservable</b> binary <b>input</b> of the AR system. Without assuming stationarity of the inputs, it is shown that the proposed method produces a consistent estimator of the AR system, not only in the sense of converging to the true parameter as the sample size increases, but also attaining the true parameter of the AR system for sufficiently large sample size. For noisy data, the estimation criterion is modified based upon an asymptotic analysis of the effect of the noise. It is shown that the modified criterion is also consistent (in the usual sense) and its variability depends upon the filtered noise. Some simulation results are also presented to demonstrate the performance of the proposed method for parameter estimation as well as for blind deconvolution...|$|R
40|$|Understanding {{the choices}} that each {{individual}} in the population makes regarding daily plans and activity participation behavior is crucial to forecasting spatial-temporal travel demand in the region. In this dissertation, we develop a comprehensive mathematical/statistical framework to infer and replicate travel behavior of individuals {{in terms of their}} socio-demographic profiles. The framework comprises series of distinct modules that employ statistical segmentation, Bayesian econometrics, data mining, and optimization techniques to predict individuals' activity types, activity frequencies, and the travel linkages that make them possible. The key advantages of the model are: first, providing the likely content of activity agenda as part of the inference procedure; second, integrating transportation network topology within activity scheduling step; and third, capability of integrating modal components. The data used for the analysis is the California Household Travel Survey data, 2000 - 2001, (Caltrans, 2002). After preprocessing (which includes queries to match, clean, and prepare data), the final cleaned data is consisted of activity patterns of 26, 269 individuals. In the model-building process, we initially cluster individuals in the sample based on their reported (one-day) activity patterns. Later, we argue and demonstrate that clustering activity/travel patterns in terms of such activity characteristics as type, duration, scheduling, and location can be an effective tool to capture preferential distributions of arrival time, departure time, and duration, which are <b>unobservable</b> <b>inputs</b> to activity-based travel models. Representative patterns are found based on two measures of dissimilarities between activity patterns, Sequence Alignment Method and Agenda dissimilarity, resulting in 8 clusters. A decision tree based on socio-demographics of individuals is fitted to infer the cluster to which each individual belongs. Inference on agenda formation in each cluster is based on ensemble of three different modules [...] "multivariate probit model," "Markov chains with conditional random fields," and "adaptive boosting" [...] applied to individuals within each cluster. In each of these modules, the inputs are socio-demographic attributes of individuals, and the outputs are discrete outcomes indicating participation in each activity type. Arrival time and activity duration inference for each activity type in each cluster, is performed using the adaptive boosting algorithm. Having identified the type of activities, and their arrival time and duration, activities are scheduled in the agenda using two approaches: decision rules, and Household Activity Pattern Problem (HAPP: a variation of pickup and delivery problem with time windows, (Recker, 1995)). Testing the entire modeling system on an out-of-sample population [...] 15 % of the entire sample [...] shows that the model is able to predict on average 80. 3 % of daily activities of individuals; correct activities during 867 minutes of 1080 awake minutes in a day was predicted...|$|R
40|$|Fair value {{accounting}} aims {{to establish}} a three-level hierarchy that distinguishes (1) readily observable measurement inputs from (2) less readily observable measurement <b>inputs</b> and (3) <b>unobservable</b> measurement <b>inputs.</b> Level 3 longevity valued assets will pose unique valuation risks once securitised pools of these alternative asset classes come to market as investment vehicles for pension plans and individual retirement accounts. No uniform framework is available to assure consistent fair market valuation and transparency for investor decision-making. Applying existing international auditing standards and analytical procedures (IFRS 13) will offer a platform upon which fund managers, their auditors and actuaries can agree upon uniform valuation and presentation guidelines. Application of these quasi-governmental standards will bring future liquidity to otherwise illiquid capital market instruments. This paper presents a valuation methodology consistent with fair value accounting and auditing standards. The methodology incorporates the longevity predictive modelling of Stallard {{in a form that}} is compatible with Bayes Factor weighted average valuation techniques based on the study by Kass and Raftery. The methodology is applicable to fair valuation of life settlement portfolios where the combination of too few large death benefit policies and large variances in individual life expectancy estimates currently challenge accurate valuation and periodic re-valuation. ...|$|R
40|$|In {{the first}} essay I {{estimate}} production functions of multiproduct firms when technologies are product-specific but inputs are observable {{only at the}} firm-level. I provide an estimation strategy that solves for the <b>unobservable</b> <b>inputs</b> while correcting for the well-known simultaneity, collinearity and omitted price problems in production function estimation. The key insights of the estimation strategy are, first, using output demand estimates in identifying the product-level input allocations and production functions, and second, using an inverse of the production function to control for endogeneity. The second essay describes the biases that arise when production functions are estimated under the standard assumption of a firm-level technology, while the true technologies are product-specific. The assumption of a firm-level technology implies that the technology parameters are identical across the various goods produced in the industry, and that a multiproduct firm produces all of its output with a single technology. To examine {{the implications of these}} simplifying assumptions, I estimate a firm-level production function on a dataset generated of an industry where two types of goods are produced with product-specific Cobb-Douglas production functions. I find that the biases in the estimated firm-level parameters are substantial even when the true product-specific technologies are very similar. The directions and the magnitudes of the biases are determined by intricate functions of the true product-specific technologies and the product scopes of the firms in the industry. The estimated productivity levels have a relatively low correlation with the true firm-level productivity levels when the firms' product scopes are heterogeneous, as they usually are. The third essay estimates the range of productivity gains achieved by information technology investments in the Finnish manufacturing sector. The contribution is to provide estimates of IT's productivity effects while accounting for some of the key characteristics of IT, i. e., that returns to IT depend on previous IT or complementary investments, come with lags, and, due to the aforementioned factors, are heterogeneous across firms and over time. I find that the productivity effects of IT range from negative to positive. For example, most firms obtain a negative productivity effect in the first year after the investment, which may be due to disruption in the production process caused by the implementation of the IT investment. Two years after the IT investment was made, most firms attain a positive productivity effect. In the third year after the investment, almost all firms gain a positive productivity effect. The estimation results suggest that the common practice of estimating a single output elasticity for an IT stock that is constructed as a linear function of the IT investments is unlikely to provide a truthful description of the productivity effects of IT...|$|R

