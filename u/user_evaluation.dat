1053|1573|Public
2500|$|Limited <b>User</b> <b>Evaluation</b> - Tests were to use co-ed {{teams of}} 16 {{soldiers}} to determine each weapon’s probability {{and quality of}} hit, time of first trigger pull, and mobility and portability in an operational environment. [...] They were to engage targets at short and long range, {{as well as in}} close-quarters. [...] Targets were to be stationary, mobile, and exposed for various times.|$|E
50|$|Two {{production}} grade MAAREECH {{systems have}} been developed and <b>user</b> <b>evaluation</b> trials completed on board two Indian Navy ships.|$|E
5000|$|When the QUIS was developed, a {{large number}} {{questionnaires}} concerning user subjective satisfaction had been developed. However, few of these exclusively focused on <b>user</b> <b>evaluation</b> of the interface itself. This was the motivation {{for the development of}} the QUIS ...|$|E
40|$|In this paper, we {{identify}} trends about, benefits from, and barriers to performing <b>user</b> <b>evaluations</b> in software engineering research. From a corpus of over 3, 000 papers spanning ten years, {{we report on}} various subtypes of <b>user</b> <b>evaluations</b> (e. g., coding tasks vs. questionnaires) and relate <b>user</b> <b>evaluations</b> to paper topics (e. g., debugging vs. technology transfer). We identify the external measures of impact, such as best paper awards and citation counts, that are correlated {{with the presence of}} <b>user</b> <b>evaluations.</b> We complement this with a survey of over 100 researchers from over 40 different universities and labs in which {{we identify}} a set of perceived barriers to performing <b>user</b> <b>evaluations.</b> Categories and Subject Descriptors H. 1. 2 [Informatio...|$|R
40|$|<b>User</b> <b>evaluations</b> of {{information}} systems are frequently used as measures of MIS success, since {{it is extremely difficult}} to get objective measures of system performance. However, <b>user</b> <b>evaluations</b> have been appropriately criticized as lacking a clearly articulated theoretical basis for linking them to systems effectiveness, and almost no research has been found that explicitly tests the link between <b>user</b> <b>evaluations</b> of systems and objectively measured performance. In this paper, we focus on <b>user</b> <b>evaluations</b> of task-technology fit for mandatory use systems and develop theoretical arguments for the link to individual performance. This is then empirically tested in a controlled experiment with objective performance measures and carefully validated <b>user</b> <b>evaluations.</b> Statistically significant support for the link is found for one measure of performance but not for a second. These findings are consistent with others which found that users are not necessarily accurate reporters of key constructs related to use of IS, specifically that self reporting is a poor measure of actual utilization. The possibility that <b>user</b> <b>evaluations</b> have a stronger link to performance when users receive feedback on their performance is proposed. Implication...|$|R
40|$|Active {{interactive}} {{genetic algorithms}} (aiGAs) rely on actively optimizing synthetic fitness functions. In interactive genetic algorithms (iGAs) framework, <b>user</b> <b>evaluations</b> {{provide the necessary}} input for synthesizing a reasonably accurate surrogate fitness function that models <b>user</b> <b>evaluations</b> or, in other words, his/her decision preferences. <b>User</b> <b>evaluations</b> collected via tournament selection only provide partial-ordering relations between solutions. Active iGAs assemble a partial-ordering graph of <b>user</b> <b>evaluations.</b> In such a directed graph, any contradictory evaluation provided by the user introduces a cycle in the graph. This property is explored in this paper to measure {{the consistency of the}} evaluations provided by the user along the evolutionary process. The consistency measures are applied to a real-world problem, the weight tuning of the cost function involved in corpus-based text-to-speech synthesis. Results show the usefulness of such measures to identify inconsistent users during the evolutionary tuning process, and successfully the number of evaluations required by more than half. ...|$|R
50|$|Limited <b>User</b> <b>Evaluation</b> - Tests were to use co-ed {{teams of}} 16 {{soldiers}} to determine each weapon’s probability {{and quality of}} hit, time of first trigger pull, and mobility and portability in an operational environment. They were to engage targets at short and long range, {{as well as in}} close-quarters. Targets were to be stationary, mobile, and exposed for various times.|$|E
5000|$|Two Active Denial Systems were {{developed}} under a Defense Department [...] "Advanced Concept Technology Demonstration" [...] Program (now known as Joint Concept Technology Demonstration Program) from 2002 to 2007. Unlike typical weapons development {{programs in the}} Defense Department, ACTDs/JCTDs are not focused on optimizing the technology; rather they are focused on rapidly assembling the technology in a configuration suitable for <b>user</b> <b>evaluation.</b>|$|E
50|$|In May 2011, General James Amos of the U.S. Marine Corps {{approved}} {{the conclusion of}} the Limited <b>User</b> <b>Evaluation</b> (LUE), and ordered the replacement of the M249 LMG by the M27. Fielding of the approximately 6,500 M27 units was expected to be completed in the summer of 2013, at a cost of $13 million. Each M27 gunner was to be equipped with around twenty-two 30-round magazines of the type currently in use with the M16 and M4 carbine approximating the combat load of an M249 SAW gunner; although the M27 gunner would not be expected to carry all 22 magazines. The individual combat load would be determined at the unit level and was expected to vary by unit, based on results of evaluations conducted by the four infantry battalions and one light armored reconnaissance battalion that participated in the Limited <b>User</b> <b>Evaluation.</b> Though program officials were aware that switching from the belt-fed M249 would result in a loss of suppressive fire capability, Charles Clark III, of the Marine Corps' Combat Development and Integration Office, cited the substantially increased accuracy of the M27 as a significant factor in the decision to replace the M249.|$|E
40|$|There {{are many}} <b>user</b> {{experience}} <b>evaluation</b> methods {{available in the}} literature. Four different <b>user</b> experience <b>evaluation</b> methods were reviewed. However, most <b>user</b> experience <b>evaluation</b> methods measured different dimensions of user experience. Therefore, Norman's emotional design is proposed as a common conceptual framework for user experience. <b>User</b> experience <b>evaluation</b> method should measure emotional response related to visceral, behavioural and reflective. Thus, a user experience testing {{is designed to provide}} holistic view of a product. Other challenges faced in <b>user</b> experience <b>evaluation</b> methods are also discussed in detail...|$|R
30|$|User {{responses}} directory {{includes the}} <b>user</b> <b>evaluations</b> of twelve subjects {{and the mean}} subjective responses.|$|R
5000|$|Goodhue, Dale L. [...] "Understanding <b>user</b> <b>evaluations</b> of {{information}} systems." [...] Management science 41.12 (1995): 1827-1844.|$|R
50|$|The Global Observer Joint Capabilities Technology Demonstration (JCTD) {{program had}} {{the goal of}} helping solve the {{capability}} gap in persistent ISR and communications relay for the US military and homeland security. The Global Observer JCTD demonstrated a new stratospheric, extreme endurance UAS that could be transitioned for post-JCTD development, extended <b>user</b> <b>evaluation,</b> and fielding. The program was a joint effort with the U.S. Department of Defense, Department of Homeland Security, and AeroVironment that started in September 2007, to culminate in a Joint Operational Utility Assessment (JOUA) in 2011.|$|E
50|$|Often the {{end users}} {{may not be}} able to provide a {{complete}} set of application objectives, detailed input, processing, or output requirements in the initial stage. After the <b>user</b> <b>evaluation,</b> another prototype will be built based on feedback from users, and again the cycle returns to customer evaluation. The cycle starts by listening to the user, followed by building or revising a mock-up, and letting the user test the mock-up, then back. There is now a new generation of tools called Application Simulation Software which help quickly simulate application before their development.|$|E
50|$|By 1998, Shyena {{was ready}} for trials, and it was tested 24 times by the NSTL from 1998 to 2000. During trials, thrust was laid on {{monitoring}} of various factors through four computers fitted on board Shyena. <b>User</b> <b>evaluation</b> tests with designed and engineered models of the TAL took place in 2003-2005, following which the Navy was convinced of the system's capabilities, {{and the fact that}} 95 per cent of the components were indigenous except a few integrated circuits and sensors, and ordered 25 units, and is likely to order more. The TAL is currently being manufactured by Bharat Dynamics Limited at its Visakhapatnam unit.|$|E
5000|$|Goodhue {{research}} interests {{are in the}} field of [...] "Data management in large organizations, Task-Technology Fit, <b>User</b> <b>evaluations,</b> and IS success, Management of IS" ...|$|R
40|$|The {{objective}} of this thesis is to obtain knowledge regarding how effective <b>user</b> centred <b>evaluation</b> methods are and how <b>user</b> centred <b>evaluations</b> are conducted by IT professionals. This will be achieved by exploring <b>user</b> centred <b>evaluation</b> in experimental and practical settings. The knowledge gained in these studies should inspire suggestions for further research and suggestions for improvements on the <b>user</b> centred <b>evaluation</b> activity. Two experimental studies were conducted. One compares the results from using three <b>user</b> centred <b>evaluation</b> methods, and the other examines two factors while conducting heuristic evaluation. The {{results show that the}} think-aloud evaluation method was the most effective method in finding realistic usability problems of the three methods. The number of critical problems found during think-aloud evaluation increases, if heuristic evaluation is conducted prior to the think-aloud evaluations. Further, two studies of <b>user</b> centred <b>evaluation</b> in practical setting...|$|R
3000|$|... refine {{and improve}} the {{recommendation}} approach considering people with common interests; <b>users</b> <b>evaluation</b> of BROAD-RSI recommendations; evaluation of users’ interaction in the BROAD-RSI through their logs.|$|R
40|$|This {{research}} aims {{to identifying}} {{the relationship between}} task-technology fit and <b>user</b> <b>evaluation,</b> that influences individual performance. The hypothesis are; (1) The characteristic of system information will affect <b>user</b> <b>evaluation</b> of task-technology fit; (2) The characteristic of task will affect <b>user</b> <b>evaluation</b> of task-technology fit; (3) The characteristic of individual ability will affect <b>user</b> <b>evaluation</b> of task-technology fit; (4) The interaction between task, technology and individual will affect <b>user</b> <b>evaluation</b> of task-technology fit (5) <b>User</b> <b>evaluation</b> will affect individual performance. Research subject consist of 135 manager from manufactures and services industry which implemented system information technology. The data are analysed by regression (multiple and tinier). The interaction on hypothesis 4 are analysed by moderated regression analysis. Research finding shows; partially, the characteristic of information system, task and individual ability will affect <b>user</b> <b>evaluation.</b> When the three variables on task-technology fit make an interaction effect therefore user evaluations {{will have a better}} result. Another findings show <b>user</b> <b>evaluation</b> has a positive significant effect to individual performance. The conclusion from this research is if <b>user</b> <b>evaluation</b> have a better result because of task-technology fit therefore individual performance will Improved indirectly. Key Word : System information technology, <b>User</b> <b>Evaluation,</b> Task-Technology Fit, Moderated Regression analysed, Individual Performance...|$|E
40|$|<b>User</b> <b>evaluation</b> is {{generally}} performed {{early in the}} development process to reveal usability problems, design flaws, and errors and correct them before deploying. Due to the short iterations of agile development, implementing <b>user</b> <b>evaluation</b> {{as part of the}} development process is a challenge that is often neglected. In a previous work, we proposed an approach that would enable the integration of <b>user</b> <b>evaluation</b> throughout the development process, by managing and automating <b>user</b> <b>evaluation</b> activities from with the integrated development environment (IDE). In this work, we focus on a case study in which small-sized agile software teams, made up of students in an annual software engineering project course, applied our integrated <b>user</b> <b>evaluation</b> approach for developing their software projects. The feedbacks from these agile teams show the intuitiveness and effectiveness of our integration approach...|$|E
40|$|A {{software}} system {{needs to be}} maintained through continuous enhancements and improvements. To understand the types of enhancements to be made, a <b>user</b> <b>evaluation</b> exercise is often conducted to determine the system’s weaknesses and limitations. This paper reports on the outcomes of a <b>user</b> <b>evaluation</b> survey carried out on an electronic Malaysian Sign Language Dictionary, e-Sign Dictionary. The evaluation was conducted using a questionnaire survey focusing on the functions and features of e-Sign Dictionary. A total of 45 respondents comprising deaf school teacher, deaf students, {{and the general public}} participated in the <b>user</b> <b>evaluation.</b> The results show that out of 45 respondents, 36 (81 %) respondents rated e-Sign Dictionary as a good or very good system. The results of the <b>user</b> <b>evaluation</b> would be used as guidelines to enhance and improve the functions and features of e-Sign Dictionary. </p...|$|E
50|$|Its fielding will {{be delayed}} until summer after the {{completion}} of another round of <b>user</b> <b>evaluations</b> ordered by MARCORSYSCOM. The Marine Corps plans on fielding 108,000.|$|R
40|$|<b>User</b> <b>evaluations</b> {{have gained}} {{increasing}} importance in visualization research {{over the past}} years, as in many cases these evaluations are {{the only way to}} support the claims made by visualization researchers. Unfortunately, recent literature reviews show that in comparison to algorithmic performance evaluations, the number of <b>user</b> <b>evaluations</b> is still very low. Reasons for this are the required amount of time to conduct such studies together with the difficulties involved in participant recruitment and result reporting. While it could be shown that the quality of evaluation results and the simplified participant recruitment of crowdsourcing platforms makes this technology a viable alternative to lab experiments when evaluating visualizations, the time for conducting and reporting such evaluations is still very high. In this paper, we propose a software system, which integrates the conduction, the analysis and the reporting of crowdsourced <b>user</b> <b>evaluations</b> directly into the scientific visualization development process. With the proposed system, researchers can conduct and analyze quantitative evaluations on a large scale through an evaluation-centric user interface with only a few mouse clicks. Thus, it becomes possible to perform iterative evaluations during algorithm design, which potentially leads to better results, as compared to the time consuming <b>user</b> <b>evaluations</b> traditionally conducted {{at the end of the}} design process. Furthermore, the system is built around a centralized database, which supports an easy reuse of old evaluation designs and the reproduction of old evaluations with new or additional stimuli, which are both driving challenges in scientific visualization research. We will describe the system's design and the considerations made during the design process, and demonstrate the system by conducting three <b>user</b> <b>evaluations,</b> all of which have been published before in the visualization literature...|$|R
2500|$|On 6 June 2013, the House Armed Services Committee {{passed an}} {{amendment}} to the 2014 budget that would prevent the Army from cancelling the IC program before <b>user</b> <b>evaluations.</b> [...] Committee members voted unanimously for the amendment that would require <b>user</b> <b>evaluations,</b> a business case analysis, and reports back to congressional defense committees before a final decision is made. [...] If passed into law, it would not take effect until 1 October 2013, which gave the Army four months to decide the fate of the program without violating a congressional directive.|$|R
40|$|We {{report on}} two new portals for searching MEDLINE/PubMed with {{handheld}} devices, PICO (Patient, Intervention, Comparison, Outcome) and a WAP (Wireless Application Protocol) browser interface. Early <b>user</b> <b>evaluation</b> and user feedback will be discussed. We also include an updated report of <b>user</b> <b>evaluation</b> of established search tools for handheld devices {{included in the}} first release...|$|E
40|$|This paper {{presents}} an evaluation methodology {{to reveal the}} relationships between the attributes of software products, practices applied during the development phase and the <b>user</b> <b>evaluation</b> of the products. For the case study, the games sector has been chosen due to easy access to the <b>user</b> <b>evaluation</b> of this type of software products. Product attributes and practices applied during the development phase have been collected from the developers via questionnaires. <b>User</b> <b>evaluation</b> results were collected from a group of independent evaluators. Two bipartite networks were created using the gathered data. The first network maps software products to the practices applied during the development phase and the second network maps the products to the product attributes. According to the links, similarities were determined and subgroups of products were obtained according to selected development phase practices. By this way, the effect of development phase on the <b>user</b> <b>evaluation</b> has been investigated...|$|E
40|$|This paper {{describes}} the findings {{for an international}} user study investigating cultural applicability of <b>user</b> <b>evaluation</b> methods. The case study evaluates cultural differences in understanding of a virtual campus website across four culturally different user groups by using the same methods for each group. Findings suggest that some <b>user</b> <b>evaluation</b> methods are less applicable than others are for a culturally diverse user base...|$|E
40|$|Part 1 : Long and Short PapersInternational audienceIn a nutshell. This {{tutorial}} comprehensively covers important <b>user</b> experience (UX) <b>evaluation</b> {{methods and}} {{opportunities and challenges}} of UX evaluation {{in the area of}} entertainment and games. The course is an ideal forum for attendees to gain insight into state-of-the art <b>user</b> experience <b>evaluation</b> methods, going way-beyond standard usability and <b>user</b> experience <b>evaluation</b> approaches in area of human-computer interaction. It surveys and assesses the efforts of <b>user</b> experience <b>evaluation</b> of the gaming and human computer interaction communities during the last 10 years...|$|R
30|$|Although AR {{has been}} studied for over 40 years, {{only in the last}} decade it began to be {{formally}} evaluated [23, 24, 68]. One of the reasons why it took so long to have <b>user</b> <b>evaluations</b> may be a lack of knowledge on how to properly evaluate AR experiences and design experiments [24]. Dünser et al. [24] claim that {{there seems to be a}} lack of understanding regarding the need of doing studies and the right motivation for carrying them. If <b>user</b> <b>evaluations</b> are conducted out of incorrect motivation or if empirical methods are not properly applied, the findings are of limited value or can even be misleading.|$|R
40|$|M-learning is {{considered}} {{as the next}} form of e-learning using mobile technologies to facilitate education for teachers and learners. Students {{need to keep in}} touch with their education services anytime regardless the place. Engaging the m-learning services in the Malaysian higher education will improve the availability of education. This paper discusses the development and <b>user’s</b> <b>evaluation</b> of Student’s Mobile Information Prototype (SMIP). The study aims to utilize mobile learning services to facilitate education for students in the higher education environment. The Design Science Research Methodology (DSRM) was adapted to develop the SMIP. Results of <b>user’s</b> <b>evaluation</b> on the SMIP indicate that most of the participants highly agreed on Perceived Usefulness, Perceived Ease of Use, Learnability, Functionality, and Didactic Efficiency...|$|R
40|$|Abstract—We {{formulate}} a novel problem of summarising entities with limited presentation budget on entity-relationship knowledge graphs and propose an efficient algorithm for solving this problem. The algorithm has been implemented {{together with a}} visualising tool. Experimental <b>user</b> <b>evaluation</b> of the algorithm wasconductedonreallarge semanticknowledgegraphsextracted from the web. The reported results of experimental <b>user</b> <b>evaluation</b> are promising and encourage to continue the work on improving the algorithm. I...|$|E
40|$|Abstract- This paper {{describes}} a touch-based PDA interface for mobile robot teleoperation and the objective <b>user</b> <b>evaluation</b> results. The interface {{is composed of}} three screens; the Vision-only screen, the Sensor-only screen, and the Vision with sensory overlay screen. The Vision-only screen provides the robot’s camera image. The Sensor-only screen provides the ultrasonic and laser range finder sensory information. The Vision with sensory overlay screen provides the image and the sensory information in concert. A <b>user</b> <b>evaluation</b> was conducted. Thirty-novice users drove a mobile robot using the interface. Participants completed three tasks, one with each screen. The {{purpose of this paper}} is to present the <b>user</b> <b>evaluation</b> results related to the collected objective data...|$|E
3000|$|Supplementary {{material}} {{consisting of}} the subjective <b>user</b> <b>evaluation</b> results can be downloaded from the following link: [URL] [...]...|$|E
40|$|This chapter {{gives an}} {{overview}} of research that describes user experiences with different types of energy-efficient buildings, focusing on indoor climate, technical operation, user attitudes and general satisfaction. Energy-efficient buildings are often rated better than conventional buildings on indoor climate, but on digging deeper, users have different concerns. The varying results from the <b>user</b> <b>evaluations</b> reflect {{that the quality of}} buildings differs. However, the complaints may also be a result of inappropriate use. The main aim of this chapter is to give guidelines for further research, based on existing <b>user</b> <b>evaluations</b> of energy-efficient buildings. Three important areas for further research on <b>user</b> <b>evaluations</b> could be identified. First, {{there is a shortage of}} research that takes into account the social context for evaluation; the social environment, the process of moving into an energy-efficient building and prior knowledge of environmental issues influence evaluation of the buildings. Energy-efficient buildings may also require specific architectural solutions and further research should consider architectural and aesthetic aspects in the evaluation. Research on the use and operation of energy-efficient buildings is increasing, but there is still a need to give more detailed attention to different ways of providing information and training in operation and use...|$|R
40|$|<b>User</b> {{experience}} <b>evaluation</b> {{is currently}} discussed widely {{both in the}} academia and industry. In this paper we discuss the use of questionnaires in <b>user</b> experience <b>evaluation</b> {{from the point of}} view of both academic research and practical system and service development in industry. We describe the current challenges and future needs we have identified in questionnaire usage, such as the need for agreed metrics for <b>user</b> experience <b>evaluation,</b> mobile questionnaires, modular questionnaires and creating questionnaire toolboxes for researchers and practitioners...|$|R
40|$|<b>User</b> <b>evaluations</b> of dialogs from a spoken dialog system (SDS) can be {{directly}} used to gauge the system’s performance. However, it is costly to obtain manual evaluations of a large corpus of dialogs. Semi-supervised learning (SSL) provides a possible solution. This process learns from {{a small amount of}} manually labeled data, together with a large amount of unlabeled data, and can later be used to perform automatic labeling. We conduct comparative experiments among SSL approaches, classical regression and supervised learning in evaluation of dialogs from CMU’s Let’s Go Bus Information System. Two typical SSL methods, namely co-training and semi-supervised support vector machine (S 3 VM), are found to outperform the other approaches in automatically predicting <b>user</b> <b>evaluations</b> of unseen dialogs in the case of low training rate...|$|R
