0|10|Public
50|$|A desktop <b>uniprocessor</b> {{entry-level}} <b>server.</b> It {{replaced the}} DECsystem 3100. Code name MIPSMATE.|$|R
50|$|There {{were two}} {{models of the}} 21070, the DECchip 21071 and the DECchip 21072. The 21071 was {{intended}} for workstations whereas the 21072 was intended for high-end workstations or low-end <b>uniprocessor</b> <b>servers.</b> The two models differed in memory subsystem features: the 21071 has a 64-bit memory bus and supports 8 MB to 2 GB of parity-protected memory whereas the 21072 has a 128-bit memory bus and supports 16 MB to 4 GB of ECC-protected memory.|$|R
50|$|Although SunSoft {{stated in}} its initial Solaris 2 press release their intent to {{eventually}} support both SPARC and x86 systems, {{the first two}} Solaris 2 releases, 2.0 and 2.1, were SPARC-only. An x86 version of Solaris 2.1 was released in June 1993, about 6 months after the SPARC version, as a desktop and <b>uniprocessor</b> workgroup <b>server</b> operating system. It included the Wabi emulator to support Windows applications. At the time, Sun also offered the Interactive Unix system that it had acquired from Interactive Systems Corporation. In 1994, Sun released Solaris 2.4, supporting both SPARC and x86 systems from a unified source code base.|$|R
40|$|This paper {{introduces}} {{network interface}} data caching, {{a new technique}} to reduce local interconnect traffic on networking servers by caching frequently-requested content on a programmable network interface. The operating system on the host CPU determines which data to store in the cache and for which packets it should use data from the cache. To facilitate data reuse across multiple packets and connections, the cache only stores application-level response content (such as HTTP data), with application-level and networking headers generated by the host CPU. Network interface data caching can reduce PCI traffic by up to 57 % on a prototype implementation of a <b>uniprocessor</b> web <b>server.</b> This traffic reduction results in up to 31 % performance improvement, leading to a peak server throughput of 1571 Mb/s...|$|R
40|$|This paper {{presents}} a new asynchronous program-ming library (libasync-smp) that allows event-driven ap-plications {{to take advantage}} of multiprocessors by run-ning code for event handlers in parallel. To control the concurrency between events, the programmer can spec-ify a color for each event: events with the same color (the default case) are handled serially; events with dif-ferent colors can be handled in parallel. The program-mer can incrementally expose parallelism in existing event-driven applications by assigning different colors to computationally-intensive events that do not share muta-ble state. An evaluation of libasync-smp demonstrates that ap-plications achieve multiprocessor speedup with little pro-gramming effort. As an example, parallelizing the cryp-tography in the SFS file server required about 90 lines of changed code in two modules, {{out of a total of}} about 12, 000 lines. Multiple clients were able to read large cached files from the libasync-smp SFS server running on a 4 -CPU machine 2. 5 times as fast as from an unmod-ified <b>uniprocessor</b> SFS <b>server</b> on one CPU. Applications without computationally intensive tasks also benefit: an event-driven Web server achieves 1. 5 speedup on four CPUs with multiple clients reading small cached files. ...|$|R
40|$|This thesis {{examines}} web-server architectures for static workloads on both uniprocessor and multiprocessor {{systems to}} determine the key factors affecting their performance. The architectures examined are event-driven (userver) and pipeline (WatPipe). As well, a thread-per-connection (Knot) architecture is examined for the uniprocessor system. Various workloads are tested {{to determine the}}ir effect {{on the performance of}} the servers. Significant effort is made to ensure a fair comparison among the servers. For example, all the servers are implemented in C or C++, and support sendfile and edge-triggered epoll. The existing servers, Knot and userver, are extended as necessary, and the new pipeline-server, WatPipe, is implemented using userver as its initial code base. Each web server is also tuned to determine its best configuration for a specific workload, which is shown to be critical to achieve best server performance. Finally, the server experiments are verified to ensure each is performing within reasonable standards. The performance of the various architectures is examined on a uniprocessor system. Three workloads are examined: no disk-I/O, moderate disk-I/O and heavy disk-I/O. These three workloads highlight the differences among the architectures. As expected, the experiments show the amount of disk I/O is the most significant factor in determining throughput, and once there is memory pressure, the memory footprint of the server is the crucial performance factor. The peak throughput differs by only 9 - 13 % among the best servers of each architecture across the various workloads. Furthermore, the appropriate configuration parameters for best performance varied based on workload, and no single server performed the best for all workloads. The results show the event-driven and pipeline servers have equivalent throughput when there is moderate or no disk-I/O. The only difference is during the heavy disk-I/O experiments where WatPipe's smaller memory footprint for its blocking server gave it a performance advantage. The Knot server has 9 % lower throughput for no disk-I/O and moderate disk-I/O and 13 % lower for heavy disk-I/O, showing the extra overheads incurred by thread-per-connection servers, but still having performance close to the other server architectures. An unexpected result is that blocking sockets with sendfile outperforms non-blocking sockets with sendfile when there is heavy disk-I/O because of more efficient disk access. Next, the performance of the various architectures is examined on a multiprocessor system. Knot is excluded from the experiments as its underlying thread library, Capriccio, only supports uniprocessor execution. For these experiments, it is shown that partitioning the system so that server processes, subnets and requests are handled by the same CPU is necessary to achieve high throughput. Both N-copy and new hybrid versions of the <b>uniprocessor</b> <b>servers,</b> extended to support partitioning, are tested. While the N-copy servers performed the best, new hybrid versions of the servers also performed well. These hybrid servers have throughput within 2 % of the N-copy servers but offer benefits over N-copy such as a smaller memory footprint and a shared address-space. For multiprocessor systems, it is shown that once the system becomes disk bound, the throughput of the servers is drastically reduced. To maximize performance on a multiprocessor, high disk throughput and lots of memory are essential...|$|R
40|$|Server-based {{resource}} reservation protocols (e. g., periodic and bandwidth-sharing servers) {{have the}} ad-vantage of providing temporal isolation between sub-systems co-executing upon a shared processing plat-form. However, {{for many of}} these protocols, tempo-ral isolation is often obtained at the price of over-provisioned reservations. Other more fine-grained ap-proaches such as real-time calculus (RTC) permit a pre-cise characterization of the resources required by a sub-system via demand-curve interfaces. An important, un-solved challenge for subsystems specified by demand-curve interfaces is the development of admission con-trol algorithms. Admission control is required to en-sure that the demand-curve specified by the interface is never violated. In this paper, we take an initial step towards addressing this challenge by designing an ad-mission controller for a simple, “single-step ” demand curve. Our approach utilizes and extends techniques originally proposed for admission control in the settings of aperiodic jobs upon dedicated <b>uniprocessors</b> and bandwidth-sharing <b>servers.</b> In future work, we hope to use these techniques as the basis for constructing admission controllers for complex, arbitrary demand-curve interfaces. 1...|$|R
40|$|Run-time {{monitoring}} {{has been}} applied in software-intensive systems to detect run-time constraint violations and trigger system recovery actions. Uncontrolled monitoring activities may, however, delay detection of a violation for an unbounded time and, worse, affect the original system 2 ̆ 7 s schedulability. In this dissertation, we introduce the concept of predictable run-time monitoring, which demands a bound on detection latency while ensuring temporal non-interference by the monitoring process. We present off-line analysis techniques that can 1) predict the maximum detection latency and the maximum (event) queue length with fixed-priority scheduling on uniprocessor systems, and 2) control the processor resource for monitor tasks and target tasks. ^ To support run-time monitoring on rapidly growing multi-core applications, we extend the <b>uniprocessor</b> Deferrable <b>Server</b> algorithm to the Synchronized Deferrable Server algorithm for fixed-priority multi-core or multiprocessor systems. The Synchronized Deferrable Server algorithm combines partitioned and global multiprocessor scheduling in one system, and can efficiently utilize processor bandwidth for run-time monitoring. We present off-line analysis techniques that can bound from above the maximum detection latency of a monitor task scheduled by the Synchronized Deferrable Server algorithm. Given a fixed amount of processor bandwidth {{for a set of}} Synchronized Deferrable Servers running on different cores, we compare three different schemes of allocating bandwidth to each of these servers. We show that evenly allocating bandwidth is 2 ̆ 2 better 2 ̆ 2 than other allocation schemes in terms of a task set 2 ̆ 7 s schedulability. ^ In addition to the predictable run-time monitoring theories, we implement the deferrable server algorithm as a Linux kernel module that provides hard real-time performance based on Xenoma, which cooperates with the Linux kernel to provide a real-time execution environment. This kernel module allows the implementation of predictable run-time monitoring on hard real-time systems. ...|$|R

