36|65|Public
50|$|The <b>zettabyte</b> is a {{multiple}} of the unit byte for digital information. The prefix zetta indicates multiplication by the seventh power of 1000 or 1021 in the International System of Units (SI). A <b>zettabyte</b> is one sextillion (one long scale trilliard) bytes. The unit symbol is ZB.|$|E
5000|$|... 1 <b>zettabyte</b> is {{equivalent}} to 8 million years of UHD 8K video format.|$|E
5000|$|The binary {{approximation}} of the zetta-, or 1,000,000,000,000,000,000,000 multiplier. 1,180,591,620,717,411,303,424 bytes = 1 <b>zettabyte</b> (or zebibyte).|$|E
5000|$|The world's {{technological}} capacity to receive information through one-way broadcast networks was 0.432 <b>zettabytes</b> of (optimally compressed) information in 1986, 0.715 in 1993, 1.2 in 2000, and 1.9 (optimally compressed) <b>zettabytes</b> in 2007 (this is the informational equivalent to every person on earth receiving 174 newspapers per day).|$|R
5000|$|ZFS {{allows for}} a maximum storage {{capacity}} of about 256 quadrillion <b>zettabytes.</b>|$|R
5000|$|Research {{from the}} University of California, San Diego reports that in 2008, Americans {{consumed}} 3.6 <b>zettabytes</b> of information.|$|R
5000|$|A <b>zettabyte</b> (ZB) is a {{quantity}} of information or information storage capacity equal to 1021 bytes, 10007 bytes, 1,000 exabytes or 1 sextillion bytes. The zebibyte (ZiB) is a related unit that uses a binary prefix, and means 10247 bytes.|$|E
50|$|The {{combined}} {{space of}} all computer hard drives {{in the world}} was estimated at approximately 160 exabytes in 2006. , the entire World Wide Web was estimated to contain close to 500 exabytes. This is one half <b>zettabyte.</b> This has increased rapidly however, as Seagate Technology reported selling a total capacity of 330 exabytes of hard drives during the 2011 Fiscal Year.|$|E
5000|$|In 2014, May joined Matthew Zwolenski (CTO Aust-NZ, EMC), Colin Fairweather and Rod Tucker as expert panelists in big-data, smart cities, {{advanced}} {{broadband networks}} and smart devices at 2020: Smart Cities, <b>Zettabyte</b> Data and 200 billion things. [...] May's {{contribution to the}} understanding of [...] "The Internet of Things", sustaining future smart and sustainable industries is also present in his published writings that cover diverse topics such as organisational culture and knowledge management to [...] "Designing for the Digitally Pervasive World".|$|E
3000|$|Volume (size) This {{characteristic}} {{represents the}} quantity of data (usually measured in Terabytes to <b>Zettabytes)</b> gathered by organizations from several locations.|$|R
5000|$|GUID Partition Table (GPT) {{allows for}} a maximum disk and {{partition}} size of 7.02 <b>zettabytes,</b> or 5.946 zebibytes, when using 512-byte sectors.|$|R
5000|$|In 2013, {{one expert}} {{estimated}} {{that the amount of}} data generated worldwide would reach 4 <b>zettabytes</b> {{by the end of that}} year.|$|R
50|$|The {{bandwidth}} {{made possible}} by optical networking technologies enabled {{the rapid growth of}} the Internet and will allow it to continue to grow. The demand for bandwidth is driven primarily by Internet Protocol (IP) traffic, which includes video services, telemedicine, social networking, Web 2.0 applications that are transaction-intensive, and cloud-based computing. At the same time, machine-to-machine and the scientific community require support for the large-scale exchange of data. The Cisco Visual Networking Index predicts global IP traffic will be more than a <b>zettabyte</b> (10^21 bytes) in 2016. By 2018, the Index predicts, a million minutes worth of video content will cross the network every second, all transmitted by optical networks.|$|E
5000|$|Scenes from Enthiran, {{particularly}} one {{known as}} the [...] "Black Sheep" [...] scene, have been parodied in subsequent films, including Mankatha (2011), Osthe (2011), Singam II (2013), {{as well as in}} the Telugu films Dookudu (2011) and Nuvva Nena (2011). In the film, Chitti often introduces himself by stating the clock rate of his central processing unit, which is 1 terahertz (1012 hertz), and his random-access memory limit, which is 1 <b>zettabyte</b> (1021 bytes). This introduction dialogue, which is spoken by Chitti as [...] "Hi, I'm Chitti, speed 1 terahertz, memory 1 zettabyte" [...] became popular. Rajinikanth featured in a cameo role as Chitti in the science-fiction film Ra.One (2011).|$|E
40|$|Abstract — We {{introduce}} flexible end-to-end {{data integrity}} for storage systems, which enables each component along the I/O path (e. g., memory, disk) to alter its protection scheme {{to meet the}} performance and reliability demands of the system. We apply this new concept to <b>Zettabyte</b> File System (ZFS) and build Zettabyte-Reliable ZFS (Z 2 FS). Z 2 FS provides dynamical tradeoffs between performance and protection and offers <b>Zettabyte</b> Reliability, which is one undetected corruption per <b>Zettabyte</b> of data read. We develop an analytical framework to evaluate reliability; the protection approaches in Z 2 FS are built upon {{the foundations of the}} framework. For comparison, we implement a straightforward End-to-End ZFS (E 2 ZFS) with the same protection scheme for all components. Through analysis and experiment, we show that Z 2 FS is able to achieve better overall performance than E 2 ZFS, while still offering <b>Zettabyte</b> Reliability. I...|$|E
5000|$|Research {{from the}} University of Southern California reports that in 2007, {{humankind}} successfully sent 1.9 <b>zettabytes</b> of information through broadcast technology such as televisions and GPS.|$|R
50|$|It is {{estimated}} that the world's capacity to store information has increased from 2.6 (optimally compressed) exabytes in 1986, to some 5,000 exabytes in 2014 (5 <b>zettabytes).</b>|$|R
5000|$|According to International Data Corporation, {{the total}} amount of global data was {{expected}} to grow to 2.7 <b>zettabytes</b> during 2012. This is an increase of 48% from 2011.|$|R
30|$|Among these {{challenges}} in various aspects [2 – 8], high-volume data distribution and user mobility support {{are the two}} representative demands urgently to be addressed. In terms of Internet traffic, according to the statistics estimated by Cisco [9], annual global IP traffic will surpass the <b>zettabyte</b> (1000 exabytes) threshold in 2016, and the two <b>zettabyte</b> threshold in 2019, at the same time, globally, consumer Internet video traffic will be 80 % of all consumer Internet traffic in 2019, up from 64 % in 2014. With respect to the mobile traffic, Cisco [9] also assesses that mobile data traffic will increase tenfold between 2014 and 2019, and that global mobile data traffic will grow three times faster than fixed IP traffic from 2014 to 2019. These observations and predictions have motivated two concepts {{for the improvement of}} Internet, namely Information-Centric Networking [10] and IP Mobility Management [11].|$|E
40|$|Part 4 : CryptographyInternational audienceAccording {{to several}} recent studies, the global IP {{communication}} and digital storage have already surpassed the <b>zettabyte</b> threshold ($$ 10 ^{ 21 }$$ bytes). The Internet entered the <b>zettabyte</b> {{era in which}} fast and secure computations are important more than ever. One solution for certain types of computations, that may offer a speedup up to several orders of magnitude, is the incremental cryptography. While the idea of incremental crypto primitives is not new, so far its potential has not been fully exploited. In this paper, we define two incremental hash functions iSHAKE 128 and iSHAKE 256 based on the recent NIST proposal for SHA- 3 Extendable-Output Functions SHAKE 128 and SHAKE 256. We describe two practical implementation scenarios of the newly introduced hash functions and compare them with the already known tree-based hash scheme. We show the trends of efficiency gains {{as the amount of}} data increases in comparison to the standard tree-based incremental schemes. Our proposals iSHAKE 128 and iSHAKE 256 provide security against collision attacks of 128 and 256 bits, respectively...|$|E
40|$|Several {{scientific}} fields, including Astrophysics, Astroparticle Physics, Cosmology, Nuclear and Particle Physics, and Research with Photons, are estimating {{that by the}} 2020 decade {{they will}} require data handling systems with data volumes approaching the <b>Zettabyte</b> distributed amongst as many as 10 (18) individually addressable data objects (Zettabyte-Exascale systems). It may be convenient or necessary to deploy such systems using multiple physical sites. This paper describes {{the findings of a}} working group composed of experts from severa...|$|E
50|$|At {{the time}} when Zettabox was founded, {{the total amount of}} {{worldwide}} data saved on networks had surpassed 1,000 Exabytes and entered a new era of sizing storage - <b>Zettabytes</b> (1 x 1021).|$|R
40|$|International {{audience}} 4 <b>zettabytes</b> (4 billion terabytes) of data {{generated in}} 2013, 44 <b>zettabytes</b> predicted for 2020 and 185 <b>zettabytes</b> for 2025. These figures are staggering and perfectly illustrate {{this new era}} of data deluge. Data {{has become a major}} economic and social challenge. The speed of processing of these data is the weakest link in a computer system: the storage system. It is therefore crucial to optimize this operation. During the last decade, storage systems have experienced a major revolution: the advent of flash memory. Flash Memory Integration: Performance and Energy Issues contributes {{to a better understanding of}} these revolutions. The authors offer us an insight into the integration of flash memory in computer systems, their behavior in performance and in power consumption compared to traditional storage systems. The book also presents, in their entirety, various methods for measuring the performance and energy consumption of storage systems for embedded as well as desktop/server computer systems. We are invited on a journey to the memories of the future...|$|R
50|$|The world's {{technological}} capacity to store information grew from 2.6 (optimally compressed) exabytes in 1986 to 15.8 in 1993, over 54.5 in 2000, and to 295 (optimally compressed) exabytes in 2007, and some 5 <b>zettabytes</b> in 2014. This is the informational equivalent to 1.25 stacks of CD-ROM {{from the earth}} to the moon in 2007, and the equivalent of 4,500 stacks of printed books from the earth to the sun in 2014.The world's {{technological capacity}} to receive information through one-way broadcast networks was 432 exabytes of (optimally compressed) information in 1986, 715 (optimally compressed) exabytes in 1993, 1.2 (optimally compressed) <b>zettabytes</b> in 2000, and 1.9 <b>zettabytes</b> in 2007.The world's effective capacity to exchange information through two-way telecommunication networks was 281 petabytes of (optimally compressed) information in 1986, 471 petabytes in 1993, 2.2 (optimally compressed) exabytes in 2000, 65 (optimally compressed) exabytes in 2007, and some 100 exabytes in 2014.The world's technological capacity to compute information with humanly guided general-purpose computers grew from 3.0 × 10^8 MIPS in 1986, to 6.4 x 10^12 MIPS in 2007.|$|R
40|$|As {{storage system}} s becom e ever larger andm orecom plex, file system s and other storage {{software}} needs tom ove away from static configuration and m nual perform nce tuning and towards dynam c configuration and autom tic run-tim perform nce tuning. The <b>Zettabyte</b> File System (ZFS) includesm any self-tuning and self- m naging algorithm. In this paper we present {{three of those}} algorithm : dynam c striping, autom atic block size selection, and autom tic filenam based perform nce tuning...|$|E
40|$|In {{this paper}} we descr: e two quality of ser ice (QoS) {{management}} models, gr[mar based QoS and existential QoS, {{in the context}} of stor[] systems. We compar the two models in ter[of gener[L 3 y, implementation complexity, and, most impor antly, ease of administ rW ion. We descr] e one implementation of existential QoS, in the <b>Zettabyte</b> File System. We conclude that the existential QoS model pr ovides mor flexibility and configur ability, eliminates many di#cult design p r blems, and simplifies administr tion. 1...|$|E
40|$|In {{this paper}} we {{describe}} a new file system that provides strong data integrity guarantees, simple administration, and immense capacity. We show {{that with a}} few changes to the traditional high-level file system architecture — including a redesign of the interface between the file system and volume manager, pooled storage, a transactional copy-onwrite model, and self-validating checksums — we can eliminate many drawbacks of traditional file systems. We describe a general-purpose productionquality file system based on our new architecture, the <b>Zettabyte</b> File System (ZFS). Our new architecture reduces implementation complexity, allows new performance improvements, and provides several useful new features almost as a side effect. ...|$|E
50|$|One {{of the key}} {{focal points}} of the report was the {{inundation}} of the archives with {{a tidal wave of}} information. By 2010 this represented more than 1 <b>zettabytes</b> of data or 1.8 trillion gigabytes.|$|R
5000|$|The name at {{one point}} was said to stand for [...] "Zettabyte File System", but by 2006 was no longer {{considered}} to be an abbreviation. [...] A ZFS file system can store up to 256 quadrillion <b>zettabytes</b> (ZB).|$|R
5000|$|... 256-bit {{processors}} {{could be}} used for addressing directly up to 2256 bytes. Already 2128 (128-bit) would greatly exceed the total data stored on Earth as of 2010, which has been estimated to be around 1.2 <b>zettabytes</b> (over 270 bytes).|$|R
40|$|Abstract. One of {{the crucial}} factors for {{enabling}} fast and secure computations in the <b>Zettabyte</b> era {{is the use of}} incremental cryptographic primitives. For files ranging from several megabytes up to hundreds of gigabytes, incremental cryptographic primitives offer speedup factors measured in multiple orders of magnitude. In this paper we define two incremental hash functions iSHAKE 128 and iSHAKE 256 based on the recent NIST proposal for SHA- 3 Extendable-Output Functions SHAKE 128 and SHAKE 256. We give two practical implementation aspects of a newly introduced hash functions and compare them with already known tree based hash scheme. We show the trends of efficiency gains as the amount of data increases in comparisons between our proposed hash functions and the standard tree based incremental schemes. Our proposals have the security levels against collision attacks of 128 and 256 bits...|$|E
40|$|The {{accelerated}} {{evolution and}} {{explosion of the}} Internet and social media is generating voluminous quantities of data (on <b>zettabyte</b> scales). Paramount amongst the desires to manipulate and extract actionable intelligence from vast big data volumes {{is the need for}} scalable, performance-conscious analytics algorithms. To directly address this need, we propose a novel MapReduce implementation of the exemplar-based clustering algorithm known as Affinity Propagation. Our parallelization strategy extends to the multilevel Hierarchical Affinity Propagation algorithm and enables tiered aggregation of unstructured data with minimal free parameters, in principle requiring only a similarity measure between data points. We detail the linear run-time complexity of our approach, overcoming the limiting quadratic complexity of the original algorithm. Experimental validation of our clustering methodology on a variety of synthetic and real data sets (e. g. images and point data) demonstrates our competitiveness against other state-of-the-art MapReduce clustering techniques...|$|E
40|$|The {{amount of}} data moved over the Internet per year has already {{exceeded}} the Exabyte scale and soon will hit the <b>Zettabyte</b> range. To support this massive {{amount of data}} movement across the globe, the networking infrastructure {{as well as the}} source and destination nodes consume immense amount of electric power, with an estimated cost measured in billions of dollars. Although considerable amount of re-search has been done on power management techniques for the networking infrastructure, there has not been much prior work focusing on energy-aware data transfer algorithms for minimizing the power consumed at the end-systems. We in-troduce novel data transfer algorithms which aim to achieve high data transfer throughput while keeping the energy con-sumption during the transfers at the minimal levels. Our ex-perimental results show that our energy-aware data transfer algorithms can achieve up to 50 % energy savings with the same or higher level of data transfer throughput...|$|E
40|$|An IDC {{estimate}} put {{the size}} of the “digital universe ” at 0. 18 <b>zettabytes</b> in 2006, and is forecasting a tenfold growth by 2011 to 1. 8 <b>zettabytes.</b> [1] • The New York Stock Exchange generates about one terabyte of new trade data per day. • Facebook hosts approximately 10 billion photos, taking up one petabyte of storage. • The Internet Archive stores around 2 petabytes of data, and is growing at a rate of 20 terabytes per month. Two problems we can raise from above: 1. Reliable and high throughput storage for these datasets 2. Analysis on these datasets and obtain results in reasonable time Hadoop answered with HDFS and MapReduc...|$|R
50|$|IDC and EMC {{project that}} data {{will grow to}} 40 <b>zettabytes</b> by 2020, {{resulting}} in a 50-fold growth {{from the beginning of}} 2010. The Computer World magazine states that unstructured information might account for more than 70%-80% of all data in organizations.|$|R
50|$|One {{yottabyte}} (YB) is a unit {{of digital}} information or information storage capacity that contains one septillion bytes or 1,000 <b>zettabytes.</b> The yobibyte (YiB) is a related unit that uses a binary prefix, and means 1,0248 bytes, which is approximately 1.2 septillion bytes.|$|R
