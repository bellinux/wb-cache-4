0|523|Public
50|$|Also, a Harvard {{architecture}} {{machine has}} distinct code and data <b>address</b> spaces: <b>instruction</b> <b>address</b> <b>zero</b> {{is not the}} same as data <b>address</b> <b>zero.</b> <b>Instruction</b> <b>address</b> <b>zero</b> might identify a twenty-four bit value, while data <b>address</b> <b>zero</b> might indicate an eight-bit byte that is not part of that twenty-four bit value.|$|R
40|$|Reducing <b>address</b> {{arithmetic}} <b>instructions</b> by optimization {{of address}} offset assignment greatly improves {{the performance of}} DSP applications. However, minimizing address operations alone may not directly reduce code size and schedule length for multiple functional units DSPs. In this paper, we exploit address assignment and scheduling for application with loops on multiple functional units DSPs. Array transformation is used in our approach to leverage the indirect addressing modes provided {{by most of the}} DSP architectures. An algorithm, <b>Address</b> <b>Instruction</b> Reduction Loop Scheduling (AIRLS), is proposed. The algorithm utilizes the techniques of rotation scheduling, address assignment and array transformation to minimize both <b>address</b> <b>instructions</b> and schedule length. Compared to the list scheduling, AIRLS shows an average reduction of 35. 4 % in schedule length and an average reduction of 38. 3 % in <b>address</b> <b>instructions.</b> Compared to the rotation scheduling, AIRLS shows an average reduction of 19. 2 % in schedule length and 39. 5 % in the number of <b>address</b> <b>instructions...</b>|$|R
40|$|One of the {{important}} issues in embedded system design is to optimize program code for the microprocessor to be stored in ROM. In this paper, we propose an integrated approach to the DSP address code generation problem for minimizing the number of <b>addressing</b> <b>instructions.</b> Unlike previous works in which code scheduling and offset assignment are performed sequentially without any interaction between them, our work tightly couples offset assignment problem with code scheduling to exploit scheduling on minimizing <b>addressing</b> <b>instructions</b> more effectively. We accomplish this by developing a fast but accurate two-phase procedure which, for a sequence of code schedules, finds a sequence of memory layouts with minimum <b>addressing</b> <b>instructions.</b> Experimental results with benchmark DSP programs show improvements of 13 %- 33 % in the address code size over Solve-SOA/GOA [7]...|$|R
40|$|A {{microprocessor}} system {{is provided with}} added memories to expand its address spaces beyond its address word length capacity by using indirect <b>addressing</b> <b>instructions</b> of a type having a detectable operations code and dedicating designated address spaces of memory {{to each of the}} added memories, one space to a memory. By decoding each operations code of instructions read from main memory into a decoder to identify indirect <b>addressing</b> <b>instructions</b> of the specified type, and then decoding the address that follows in a decoder to determine which added memory is associated therewith, the associated added memory is selectively enabled through a unit while the main memory is disabled to permit the instruction to be executed on the location to which the effective address of the indirect <b>address</b> <b>instruction</b> points, either before the indirect address is read from main memory or afterwards, depending on how the system is arranged by a switch...|$|R
5000|$|... 0 <b>address</b> arithmetic, <b>Zero</b> <b>address</b> arithmetic, a {{computer}} architecture feature where assignment to a physical address space is deferred until programming statement execution time ...|$|R
25|$|Counter machine – {{the most}} {{primitive}} and reduced theoretical {{model of a}} computer hardware. Lacks indirect <b>addressing.</b> <b>Instructions</b> are in the finite state machine {{in the manner of}} the Harvard architecture.|$|R
5000|$|One of {{the reasons}} for the 8051s {{popularity}} is its range of operations on single bits. Bits are always specified by absolute addresses; there is no register-indirect or indexed <b>addressing.</b> <b>Instructions</b> that operate on single bits are: ...|$|R
50|$|A Change <b>Address</b> Mode (CAM) <b>instruction</b> {{switched}} between 2-, 3- and 4-character {{address mode}}s.The address mode specified {{the number of}} characters needed for each operand <b>address</b> in <b>instructions.</b>|$|R
50|$|The {{instruction}} set implicitly subdivides the data format {{into the same}} fields as type A instructions: prefix, decrement, tag and <b>address.</b> <b>Instructions</b> exist to modify each of these fields in a data word without changing {{the remainder of the}} word.|$|R
40|$|PC {{instruction}} memory, {{fetch instruction}} Register numbers register file, read registers Depending on instruction class Use ALU to calculate Arithmetic result Memory address for load/store Branch target address Access data memory for load/store PC target address or PC + 4 3 Abstract / Simplified View Two types of functional units: elements that operate on data values (combinational) elements that contain state (sequential) 4 PC <b>address</b> <b>instruction</b> instruction memory data memory address data registers data register # register # register # ALU Abstract / Simplified View Cannot just join wires together Use multiplexers 5 PC <b>address</b> <b>instruction</b> instruction memory data memory address data registers data register # register # register # ALU Recall...|$|R
5000|$|... +------+ | nop | {{execute the}} {{following}} instruction +------+ [...] (Effective PC <b>address</b> = next <b>instruction</b> <b>address)</b> ...|$|R
50|$|The machine {{instructions}} can {{be grouped}} into six categories: accumulator instructions, branch instructions, memory reference <b>instructions,</b> <b>address</b> register <b>instructions,</b> scratchpad register instruction, miscellaneous instructions (interrupt, input, output, indirect scratchpad register, load, and store).|$|R
5000|$|Addressing modes include Immediate (operand in instruction), Direct or [...] "Symbolic" [...] (operand <b>address</b> in <b>instruction),</b> Register (operand in {{workspace}} register), Register Indirect (operand {{address in}} workspace register) {{with or without}} auto-increment, Indexed (operand <b>address</b> in <b>instruction</b> indexed with workspace register content), and Program Counter Relative.|$|R
50|$|With no byte <b>addressing</b> <b>instructions</b> at all, code had to {{be written}} to pack and shift {{characters}} into words. The very large words, and comparatively small amount of memory, meant that programmers would frequently economize on memory by packing data into words at the bit level.|$|R
5000|$|... +----+------------------------------+ |jump| offset | jump {{relative}} +----+------------------------------+ [...] (Effective PC <b>address</b> = next <b>instruction</b> <b>address</b> + offset, offset may be negative) ...|$|R
5000|$|... +------+-----+-----+ |skipEQ| reg1| reg2| {{skip the}} {{following}} instruction if reg1=reg2 +------+-----+-----+ [...] (Effective PC <b>address</b> = next <b>instruction</b> <b>address</b> + 1) ...|$|R
50|$|<b>Zero</b> <b>address</b> {{arithmetic}} is {{a feature}} of a few innovative computer architectures, whereby the assignment to a physical address space is deferred until programming statement execution time. It eliminates the link step of conventional compile and link architectures, and more generally relocation.|$|R
50|$|In {{both the}} {{original}} version and 1A, clocks for Program Store and Call Store were operated out of phase, so one would be delivering data while the other was still accepting an <b>address.</b> <b>Instruction</b> decoding and execution were pipelined, to allow overlapping processing of consecutive instructions in a program.|$|R
5000|$|The {{instruction}} set implicitly subdivides the data format {{into the same}} fields as type A instructions: prefix, decrement, tag and <b>address.</b> <b>Instructions</b> exist to modify each of these fields in a data word without changing {{the remainder of the}} word though the Store Tag instruction was not implemented on the IBM 704.|$|R
5000|$|Relocation is {{typically}} {{done by the}} linker at link time, {{but it can also}} be done at run time by a relocating loader, or by the running program itself. Some architectures avoid relocation entirely by deferring address assignment to run time; this is known as <b>zero</b> <b>address</b> arithmetic.|$|R
5000|$|An {{incrementing}} {{counter that}} {{keeps track of}} the memory <b>address</b> of the <b>instruction</b> {{that is to be}} executed next or in other words, holds the <b>address</b> of the <b>instruction</b> to be executed next.|$|R
5000|$|... 0003 NOOP 00 0000 0000 No-operation <b>{{instruction}},</b> next <b>instruction</b> <b>address</b> is 0000 0000 HALT 01 0000 8000 Halt, next <b>instruction</b> <b>address</b> is {{the console}} (this Halt instruction was stored in 0000 by the STD instruction above) ...|$|R
5000|$|A two-byte {{instruction}} specialized {{for program}} looping {{is new to}} the Z80. DJNZ (Decrement Jump if Non-Zero) takes a signed 8-bit displacement as an immediate operand. The B register is decremented. If the result is nonzero then program execution jumps relative to {{the address of the}} PC plus the displacement. The flags remain unaltered. To perform an equivalent loop on an 8080 would require separate decrement and jump (to a two-byte absolute <b>address)</b> <b>instructions,</b> and the flag register would be altered.|$|R
5000|$|A {{label to}} the left of an {{instruction}} mnemonic is converted to the memory <b>address</b> the <b>instruction</b> or data is stored at. i.e. loopstart INP ...|$|R
40|$|Communication {{components}} (<b>address,</b> <b>instruction,</b> {{and data}} buses and associated hardware like I/O pins, pads, and buffers) are contributing increasingly to the area/cost and power consumption of microprocessor systems. To decrease costs due to address buses, we propose to use narrow widths for underutilized buses (hardware-only compression) to transmit information in multiple cycles. We analyze performance and power consumption overheads of hardware-only compression and investigate {{the use of}} “address concatenation ” to mitigate performance loss and address offsets and XORs to reduce power consumption overheads. ...|$|R
40|$|PC → {{instruction}} memory, {{fetch instruction}} � Register numbers → register file, read registers � Depending on instruction class � Use ALU to calculate � Arithmetic result � Memory address for load/store � Branch target address � Access data memory for load/store � PC ← target address or PC + 4 3 Abstract / Simplified View data PC <b>address</b> <b>instruction</b> instruction memory register # registers register # register # ALU address data memory data � Two types of functional units: elements that operate on data values (combinational) elements that contain state (sequential) 4 Abstract / Simplified View data PC <b>address</b> <b>instruction</b> instruction memory register # registers register # register # ALU address data memory data � Cannot just join wires together � Use multiplexers 5 Recall: Logic Design Basics � Information encoded in binary � Low voltage = 0, High voltage = 1 � One wire per bit � Multi-bit data encoded on multi-wire buses � Combinational element � Operate on data � Output {{is a function}} of input � State (sequential) elements � Store information 6 Sequential Elements � Register: stores data in a circuit � Uses a clock signal to determine when to update the stored value � Edge-triggered: update when Clk changes from 0 to...|$|R
50|$|To {{work around}} this difficulty, most {{assembly}} languages (including the LMC) combine the mnemonics with labels. A label {{is simply a}} word {{that is used to}} either name a memory <b>address</b> where an <b>instruction</b> or data is stored, or to refer to that <b>address</b> in an <b>instruction.</b>|$|R
50|$|In {{the early}} {{instances}} {{of the architecture}} (System/360 and early System/370), the <b>instruction</b> <b>address</b> was 24 bits; in later instances (XA/370), the <b>instruction</b> <b>address</b> was 31 bits plus a mode bit (24 bit addressing mode if zero; 31 bit addressing mode if one) {{for a total of}} 32 bits.|$|R
40|$|A major hurdle {{of recent}} x 86 superscalar {{processor}} designs is limited instruction issue rate {{due to the}} overly complex x 86 instruction formats. To alleviate this problem, the machine states must be preserved and the <b>instruction</b> <b>address</b> routing paths must be simplified. We propose an <b>instruction</b> <b>address</b> queue, whose queue size has been estimated to handle saving of <b>instruction</b> <b>addresses</b> with three operations: allocation, access, and retirement. The <b>instruction</b> <b>address</b> queue will supply the stored <b>instruction</b> <b>addresses</b> as data for three mechanisms: changing instruction flow, updating BTB, and handling exceptions. It {{can also be used}} for internal snooping to solve self-modified code problems. Two CISC hazards in the x 86 architectures, the variable instruction length and the complex addressing mode, have been considered in this design. Instead of the simple full associative storing method in lower degree (< 4) superscalar systems, the line-offset method is used in this address queue. This will reduce by 1 / 3 the storage space for a degree- 5 superscalar x 86 processor with even smaller access latency. We use synthesis tools to analyze the design, and show that it produces optimized results. Because the address queue design can keep two different line <b>addresses</b> in an <b>instruction</b> access per cycle, this method can be extended for designing a multiple instruction block issue system, such as the trace processor...|$|R
40|$|The {{memory system}} stores {{information}} comprising primarily instructions and data and secondarily address information, such as cache tag fields. It {{interacts with the}} processor by supporting related traffic (again comprising <b>addresses,</b> <b>instructions,</b> and data). Continuing exponential growth in processor performance, combined with technology, architecture, and application trends, place enormous demands on the memory system to permit this information storage and exchange at a high-enough performance (i. e., to provide low latency and high bandwidth access to large amounts of information). This paper comprehensively analyzes the redundancy in the information (<b>addresses,</b> <b>instructions,</b> and data) stored and exchanged between the processor and the memory system and evaluates the potential of compression in improving performance of the memory system. Analysis of traces obtained with Sun Microsystems’ Shade simulator simulating SPARC executables of nine integer and six floating-point programs in the SPEC CPU 2000 benchmark suite yield impressive results. Well-designed compression schemes may provide benefits in performance that far outweigh the extra time and logic for compression and decompression. This will be more so in the future since the speed and size of logic (which {{will be used to}} perform compression/decompression) are improving and are projected to improve at a much higher rate compared to those of interconnect (which will be used to communicate the information), both on-chip and off-chip. Keyword...|$|R
2500|$|Simple {{addressing}} modes with complex <b>addressing</b> performed by <b>instruction</b> sequences ...|$|R
50|$|The primary design {{consultant}} for the Librascope computer was Stan Frankel, a Manhattan Project veteran {{and one of the}} first programmers of ENIAC. He designed a usable computer with a minimal amount of hardware. The single <b>address</b> <b>instruction</b> set had only 16 commands. Not only was the main memory on magnetic drum, but so were the CPU registers, timing information and the master bit clock, each on a dedicated track. The number of vacuum tubes were kept to a minimum by using solid-state diode logic, a bit-serial architecture and multiple usage of each of the 15 flip-flops.|$|R
5000|$|Dick and Carey made a {{significant}} contribution to the instructional design field by championing a systems view of instruction, in contrast to defining instruction as the sum of isolated parts. The model <b>addresses</b> <b>instruction</b> as an entire system, focusing on the interrelationship between context, content, learning and instruction. According to Dick and Carey, [...] "Components such as the instructor, learners, materials, instructional activities, delivery system, and learning and performance environments interact with each other and work together to bring about the desired student learning outcomes". The components of the Systems Approach Model, also known as the Dick and Carey Model, are as follows: ...|$|R
5000|$|IP/EIP/RIP: Instruction pointer. Holds {{the program}} counter, the current <b>instruction</b> <b>address.</b>|$|R
5000|$|... 64-bit {{processor}} {{status register}} (PSW), {{which includes a}} 24-bit <b>Instruction</b> <b>Address</b> ...|$|R
50|$|As a result, {{a greater}} number of {{programs}} were able to utilize the enhanced direct page addressing mode versus legacy processors that only included the <b>zero</b> page <b>addressing</b> mode.|$|R
