1|62|Public
40|$|The {{design and}} {{performances}} {{of a high}} dynamic range DC-AC current sensor utilizing Giant Magneto-Impedance (GMI) are presented. The sensor {{is based on a}} GMI element with negative feedback. The sensing element is a 30 μm diameter GMI Co-based amorphous wire. It is curled to a toroidal core of 2 cm diameter. A bias magnetic field of about 650 A/m is applied to the GMI element to obtain an asymmetric GMI effect. A strong negative feedback is introduced to ensure linearity in a wide dynamic range. Analog conditioning electronics was fully developed. This includes a square wave oscillator based on an inverter trigger; a peak detector and a high gain amplifier with <b>zero</b> <b>adjust.</b> The GMI element is driven at a 3 MHz frequency and 5 mA peak-to-peak current. The closed-loop operations are investigated and the performances of the sensor are presented. DC current measurements are performed. The sensor exhibits good sensitivity and very good linearity, free from hysteresis, in a wide dynamic range of ± 40 A. The sensitivity is about 0. 24 V/A and the linearity error is about 0. 02 % of the full scale (FS). The hysteresis error is smaller than the measurement accuracy. AC current measurements using the developed sensor have also been successfully achieved. The sensor bandwidth in closed-loop was about 1. 7 kHz...|$|E
40|$|In certain {{applications}} involving count data, it {{is sometimes}} found that zeros are observed with a frequency significantly higher (lower) than predicted by the assumed model. Examples of such applications are cited in the literature from engineering, manufacturing, eco-nomics, public health, epidemiology, psychology, sociology, political science, agriculture, road safety, species abundance, use of recrea-tional facilities, horticulture and criminology. In this article, a <b>zero</b> <b>adjusted</b> generalized Poisson distribution is studied and a score test is developed, with and without covariates, to determine whether suc...|$|R
40|$|In actuarial and {{insurance}} literatures, several researchers suggested generalized linear regression models (GLM) for modeling claim costs {{as a function}} of risk factors. The modeling of claim costs involving both zero and positive claims experience has been carried out by fitting the claim costs collectively using Tweedie model. However, the probability of zero claims in Tweedie model is not allowed to be fitted explicitly {{as a function of}} explanatory variables. The {{purpose of this article is}} to propose the application of <b>Zero</b> <b>Adjusted</b> Gamma (ZAGA) and <b>Zero</b> <b>Adjusted</b> Inverse Gaussian (ZAIG) regression models for modeling both zero and positive claim costs data. The models are fitted to the Malaysian motor insurance claims experiences which are divided into three types namely Third Party Bodily Injury (TPBI), Own Damage (OD) and Third Party Property Damage (TPPD). The fitted models show that both claim probability and claim cost are affected by either the same or different explanatory variables. The fitted models also allow the relative risk of each rating factor to be compared and the low or high risk vehicles to be identified, not only for the claim cost but also for the claim probability. The AIC and BIC indicate that ZAIG regression is the best model for modeling both positive and zero claim costs for all claim types...|$|R
5000|$|If d is not <b>zero,</b> the {{algorithm}} <b>adjusts</b> C(x) {{so that a}} recalculation of d would be zero: ...|$|R
40|$|Compositional {{data are}} met in many {{different}} fields, such as economics, archaeometry, ecology, geology and political sciences. Regression where the dependent variable is a composition is usually carried out via a log-ratio transformation of the composition or via the Dirichlet distribution. However, when there are zero values in the data these two ways are not readily applicable. Suggestions for this problem exist, {{but most of them}} rely on substituting the zero values. In this paper we adjust the Dirichlet distribution when covariates are present, in order to allow for zero values to be present in the data, without modifying any values. To do so, we modify the log-likelihood of the Dirichlet distribution to account for zero values. Examples and simulation studies exhibit the performance of the <b>zero</b> <b>adjusted</b> Dirichlet regression. Comment: Research article consisting of 18 pages, 4 figure...|$|R
40|$|In {{the wake}} of the largest M&A wave in history, it is {{appropriate}} to assess the evidence on the profitability of this activity. One popular view is that merger activity is highly unprofitable. Does research sustain this view? This paper reflects on what it means for M&A to “pay ” and summarizes the evidence from 130 studies from 1971 to 2001. The review comments on various research approaches, and highlights findings for the broad activity as well as niches of special note. The mass of research suggests that target shareholders earn sizable positive marketreturns, that bidders (with interesting exceptions) earn <b>zero</b> <b>adjusted</b> returns, and that bidders and targets combined earn positive adjusted returns. On balance, one should conclude that M&A does pay. But the broad dispersion of findings around a zero return to buyers suggests that executives should approach this activity with caution...|$|R
50|$|Some {{countries}} or states have already implemented {{some of these}} ideas through Vision <b>Zero</b> networks. Pay-as-you-drive <b>adjusts</b> insurance costs according to when and where the person drives.|$|R
40|$|It {{is thought}} that {{variation}} in natural light levels affect people with Seasonal Affective Disorder (SAD). Several meteorological factors related to luminance can be forecast but {{little is known about}} which factors are most indicative of worsening SAD symptoms. The aim of this meteorological analysis is to determine which factors are linked to SAD symptoms. The symptoms of 291 individuals with SAD in and near Groningen have been evaluated over the period 2003 - 2009. Meteorological factors linked to periods of low natural light (sunshine, global radiation, horizontal visibility, cloud cover and mist) and others (temperature, humidity and pressure) were obtained from weather observation stations. A Bayesian <b>zero</b> <b>adjusted</b> auto-correlated multilevel Poisson model was carried out to assess which variables influence the SAD symptom score BDI-II. The outcome of the study suggests that the variable sunshine duration, for both the current and previous week, and global radiation for the previous week, are significantly linked to SAD symptoms...|$|R
40|$|For oranges {{intended}} for European markets, it is strongly advisable to find preservation methods increasing shelf-life and favouring {{the reduction of}} use of chemical antifungal products, both for reasons of environmental sustainability and {{to reduce the risk}} to health. In this paper results related to imazalil fungicide treatment of oranges fruits are reported. The experiments were carried out by comparing the traditional dipping of fruit with an innovative pilot plant process, designated “thin film” (TF), which is designed to reduce fungicide/water mixture concentration. Two film thickness (1 and 3 mm) and two temperatures (20 and 45 °C) were investigated. The incidence of decay was > 5 mg kg- 1 making the oranges unsuitable for the market. Therefore, TF treatment can be considered a useful method for maintaining high quality of citrus fruit and controlling green and blue mould both during cold storage and shelf-life period. A correlation was found between the overall decay incidence % and fungicide residue at time <b>zero</b> (<b>adjusted</b> R 2 of 0. 98) and the fungicide destruction rate logarithmically depended on its initial dose...|$|R
40|$|In motor {{insurance}} pricing based on risk of policyholder, modeling claim {{is the most}} important step. The modeling includes two main models there are model which relates to event of claims and model the cost of claims submitted to insurance companies. Most studies modeling the cost of claims involving only the amount of claims which are positive, i. e. when an accident happens and then the policyholder filed a claim with the claims cost is greater than zero. In one period of insurance, there’re policyholders who have not had an accident and there’re policyholders who had an accident but does not have claim, in this case is said to the claims cost is zero. This paper investigate the implementation ZAIG (<b>Zero</b> <b>Adjusted</b> Inverse Gaussian) regression on the model of automobile insurance claims that involve the cost of claims that zero and positive use data supported by Insurance Services Malaysia (ISM) Berhard. By regression ZAIG note that both the event and the average of claim cost significantly affected by the premium...|$|R
40|$|We {{present a}} series of {{high-resolution}} cosmological simulations 1 of galaxy formation to z = 0, spanning halo masses ∼ 108 − 1013 M, and stellar masses ∼ 104 − 1011 M. Our simulations include fully explicit treatment of the multi-phase ISM & stellar feedback. The stellar feedback inputs (energy, momentum, mass, and metal fluxes) are taken directly from stellar population models. These sources of feedback, with <b>zero</b> <b>adjusted</b> parameters, reproduce the observed relation between stellar and halo mass up to Mhalo ∼ 1012 M. We predict weak redshift evolution in the M∗−Mhalo relation, consistent with current constraints to z> 6. We find that the M∗−Mhalo relation is insensitive to numerical details, but is sensitive to feedback physics. Simulations with only supernova feedback fail to reproduce ob-served stellar masses, particularly in dwarf and high-redshift galaxies: radiative feedback (photo-heating and radiation pressure) is necessary to destroy GMCs and enable efficient coupling of later supernovae to the gas. Star formation rates agree well with the observed Kennicutt relation at all redshifts. The galaxy-averaged Kennicutt relation {{is very different from}} the numerically imposed law for converting gas into stars, and is determined by self-regulation vi...|$|R
40|$|Background: Currently, {{there is}} no {{standardised}} tool used to capture morbidity following abdominal aortic aneurysm (AAA) repair. The aim of this prospective observational study was to validate the Postoperative Morbidity Survey (POMS) according to its two guiding principles: to only capture morbidity substantial enough to delay discharge from hospital {{and to be a}} rapid, simple screening tool. Methods: A total of 64 adult patients undergoing elective infrarenal AAA repair participated in the study. Following surgery, the POMS was recorded daily, by trained research staff with the clinical teams blinded, until hospital discharge or death. We modelled the data using Cox regression, accounting for the competing risk of death, with POMS as a binary time-dependent (repeated measures) internal covariate. For each day for each patient, ‘discharged’ (yes/no) was the event, with the elapsed number of days post-surgery as the time variable. We derived the hazard ratio for any POMS morbidity (score 1 – 9) vs. no morbidity (<b>zero),</b> <b>adjusted</b> for type of repair (endovascular versus open), age and aneurysm size. Results: The hazard ratio for alive discharge with any POMS-recorded morbidity versus no morbidity was 0. 130 (95...|$|R
50|$|The fixed bits at bit {{positions}} 1, 3 and 5, and carry, parity, <b>adjust,</b> <b>zero</b> {{and sign}} flags are inherited from an even earlier architecture, 8080. The adjust flag {{used to be}} called auxiliary carry bit in 8080 and half-carry bit in the Zilog Z80 architecture.|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedThe possibility of designing constrained adaptive finite impulse response digital filters is investigated as {{motivated by a}} study of adaptive noise cancellation. The first constraint considered consists of a fixed angle between filter zeros and is implemented in a master-slave approach in {{which one of the}} <b>zeros</b> is <b>adjusted</b> adaptively and the others follow subject to the constraint. The second constraint considered is a linear constraint on the filter weights and is implemented by augmenting the error equation with Lagrangean multipliers. Simulation indicate that the approach is feasible. [URL] United States Nav...|$|R
40|$|Fully {{adaptive}} {{feedforward control}} {{algorithm is proposed}} for general multi-channel active noise control (ANC) when all the noise transmission channels are uncertain. To reduce the actual canceling error, two kinds of virtual errors are introduced and are forced into <b>zero</b> by <b>adjusting</b> three adaptive FIR filter matrices in an on-line manner, which can result in the canceling at the objective points. Unlike other conventional approaches, the proposed algorithm does not need exact identification of the secondary paths, and so requires neither any dither signals nor the PE property of the source noises, {{which is a great}} advantage of the proposed adaptive approach. 1...|$|R
40|$|A new direct {{adaptive}} algorithm for feedforward active {{noise control}} (ANC) is proposed in a general case when the primary and secondary path responses are all uncer-tain. In {{order to reduce the}} actual canceling error, the two artificial errors are introduced and are forced into <b>zero</b> by <b>adjusting</b> three adaptive FIR filters in an on-line manner. The distinctive feature of the method is that the adjusted parameters do not have to converge to the true FIR parameters but some constants, then the reduction of the two errors can attain the canceling at the objec-tive point. The effectiveness of the proposed algorithm is validated in experiments using an air duct. 1...|$|R
40|$|Abstract—A compact {{open-loop}} resonator with multispurious suppression is proposed. When excited symmetrically, the res-onator shows two tunable transmission <b>zeros.</b> By <b>adjusting</b> the open-end gap capacitance, {{one of the}} zeros {{is placed}} near the passband {{and the other is}} tuned to collocate with the leading two degenerated higher order resonances, so that the circuit has a sharp transition as well as a wide upper stopband. The experi-mental circuit shows the first spurious peak occurs at four times the passband frequency (4), and the measurement shows good agreement with the theoretic prediction. Index Terms—Bandpass filter (BPF), microstrip circuit, spu-rious suppression, stepped-impedance resonator (SIR), transmis-sion zero. I...|$|R
40|$|A new direct {{adaptive}} {{algorithm is}} proposed for multichan-nel active noise control (ANC) and sound reproduction (SR) when primary and secondary path dynamics are all uncertain and changeable. To attenuate the canceling errors or repro-duction errors, two kinds of virtual error vectors are intro-duced and are forced into <b>zero</b> by <b>adjusting</b> three adaptive FIR matrix filters in an on-line manner. It is shown that the convergence of the canceling or reproduction errors to zero can be attained at the objective points. The proposed algo-rithm can tune an inverse controller matrix directly without need of explicit identification of the secondary path channels, and requires neither any dither signals nor the PE property of the source signals for the identifiability. 1...|$|R
40|$|A new frequency-domain direct {{adaptive}} {{approach is}} proposed for general multichannel {{active noise control}} (ANC) when both of the primary and secondary path channels are uncertain and changeable. To reduce the cancelling errors, two kinds of virtual error vectors are introduced and are forced to <b>zero</b> by <b>adjusting</b> three adaptive FIR filter matrices in an online manner, by which the convergence of the actual cancelling error to zero can be attained at the objective points. Unlike other conventional approaches, the proposed algorithm can give an inverse controller directly without need of explicit identification of the secondary path channels. The proposed algorithm can be implemented in the frequencydomain to reduce its computational complexity. 1...|$|R
40|$|We {{present a}} series of {{high-resolution}} cosmological simulations of galaxy formation to z= 0, spanning halo masses ~ 10 ^ 8 - 10 ^ 13 M_sun, and stellar masses ~ 10 ^ 4 - 10 ^ 11. Our simulations include fully explicit treatment of both the multi-phase ISM (molecular through hot) and stellar feedback. The stellar feedback inputs (energy, momentum, mass, and metal fluxes) are taken directly from stellar population models. These sources of stellar feedback, with <b>zero</b> <b>adjusted</b> parameters, reproduce the observed relation between stellar and halo mass up to M_halo~ 10 ^ 12 M_sun (including dwarfs, satellites, MW-mass disks, and small groups). By extension, this leads to reasonable agreement with the stellar mass function for M_star 6. We find that the M_star-M_halo relation is insensitive to numerical details, but is sensitive to the feedback physics. Simulations with only supernova feedback fail to reproduce the observed stellar masses, particularly in dwarf and high-redshift galaxies: radiative feedback (photo-heating and radiation pressure) is necessary to disrupt GMCs and enable efficient coupling of later supernovae to the gas. Star formation rates agree well with the observed Kennicutt relation at all redshifts. The galaxy-averaged Kennicutt relation {{is very different from}} the numerically imposed law for converting gas into stars in the simulation, and is instead determined by self-regulation via stellar feedback. Feedback reduces star formation rates considerably and produces a reservoir of gas that leads to rising late-time star formation histories significantly different from the halo accretion history. Feedback also produces large short-timescale variability in galactic SFRs, especially in dwarfs. Many of these properties are not captured by common 'sub-grid' galactic wind models. Comment: 23 pages, 13 figures, accepted to MNRAS. Revised to match published version. For movies of the simulations here, see [URL] or the FIRE project site [URL]...|$|R
40|$|ASML {{produces}} TwinScan NXT {{machines that}} are used {{for the production of}} microchips. The machines ensure that an accurate pattern of DUV-light passes a lens and that it is projected as accurate as possible on the wafer. To ensure that {{the focal point of the}} converged DUV-light falls exactly onto the wafer, the leveling functionality is of great importance. That is, placing the wafer in the correct depth of focus by rotating the wafer and moving the wafer up or down during exposure. In order to meet the imaging requirements, the performance is investigated by analyzing errors of the machines at customers' site, considering one-year data. The most important errors are A, B, C and D. To reduce the total unscheduled down (USD) time of those errors, we should focus on reducing the mean USD time for errors A and C; and focus on reducing the frequency for errors B andD where these last two errors are likely to be solved together. Different nominal customer-related variables are considered as possible causes of USD times such as location, system type or type of sensors. After applying hierarchical clustering and multidimensional scaling, the variable set is reduced. This set is used to model the USD time of one error: B. Significant differences in USD times are found, showed by the robust and distribution free rank tests: Wilcoxon and Kruskal-Wallis test. To discover interesting patterns among variables, regression models are applied. The linear regression model and generalized linear model not seem to be the right model to the data. The <b>zero</b> <b>adjusted</b> exponential model seems to be the correct model and show that AG type, location and FSM flex package are the most important explanatory variables. This directs to a potential root cause where ASML is working further upon. Applied Mathematic...|$|R
40|$|A novel virtual error {{approach}} is proposed for fully adap-tive feedforward compensation/equalization, which {{is useful in}} nonlinear active noise control and predistortion for nonlin-ear high power amplifier (HPA). To attenuate the compensa-tion error, two kinds of virtual error are introduced and are forced into <b>zero</b> by <b>adjusting</b> three nonlinear adaptive filters in an on-line manner. It is shown that the convergence of the compensation error to zero can be assured by forcing the vir-tual errors to zero separately. The proposed method can ad-just the predistorter directly without identification of a post-inverse model of HPA as adopted in previous predistortion methods. The effectiveness of the proposed virtual error ap-proach is validated in numerical simulation with comparison to an ordinary nonlinear filtered-x algorithm in the adaptive predistortion for nonlinear HPA used in OFDM communica-tion systems. 1...|$|R
40|$|Effective density {{dependent}} pairing {{forces of}} <b>zero</b> range are <b>adjusted</b> on gap values in T= 0, 1 channels calculated with the Paris force in symmetric nuclear matter. General {{discussions on the}} pairing force are presented. In conjunction with the effective k-mass the nuclear pairing force seems to need very little renormalization in the T= 1 channel. The situation in the T= 0 channel is also discussed. Comment: 6 pages, 8 figures, {{to be published in}} PR...|$|R
5000|$|Windage plays a {{significant}} role, with the effect increasing with wind speed or {{the distance of}} the shot. The slant of visible convections near the ground can be used to estimate crosswinds, and correct the point of aim. All adjustments for range, wind, and elevation can be performed by aiming off the target, called [...] "holding over" [...] or Kentucky windage. Alternatively, the scope can be adjusted so that the point of aim is changed to compensate for these factors, sometimes referred to as [...] "dialing in". The shooter must remember to return the scope to <b>zeroed</b> position. <b>Adjusting</b> the scope allows for more accurate shots, because the cross-hairs can be aligned with the target more accurately, but the sniper must know exactly what differences the changes will have on the point-of-impact at each target range.|$|R
40|$|We analyze {{transport}} of local magnetization and develop schemes to control transport behavior in finite spin- 1 / 2 Heisenberg chains and spin- 1 / 2 Heisenberg two-leg ladders at <b>zero</b> temperature. By <b>adjusting</b> parameters in the Hamiltonians, these quantum systems may show both integrable and chaotic limits. We provide examples of chaotic systems leading to diffusive and to ballistic transport. In addition, methods of coherent quantum control to induce {{a transition from}} diffusive to ballistic transport are proposed. Comment: 8 pages, 5 figure...|$|R
40|$|Effective density-dependent pairing {{forces of}} <b>zero</b> range are <b>adjusted</b> on gap values in T = 0, 1 {{channels}} calculated with the Paris force in symmetric nuclear matter. General {{discussions on the}} pairing force are presented. In conjunction with the effective k mass the nuclear pairing force seems to need very little renormalization in the T = 1 channel. The situation in the T = 0 channel is also discussed. This work was supported by DGICYT-IN 2 P 3 agreement and by DGICYT (Spain) under Contract No. PB 95 / 0123. Peer reviewe...|$|R
40|$|When using a Photo-elastic {{modulator}} (PEM) {{in combination}} with a coherent light source, {{in addition to the}} modulation of the phase, Fabry-Perot interference in the PEM’s optical head induces large offsets in the 1 ω and 2 ω detector signals. A Jones matrix which describes both effects simultaneously, was derived for a single axis PEM and used to find an expression for the detector signal for two different MO Kerr setups. The effect of the PEM tilt angle, polarizer angle, analyzer angle, and retardation, on the detector signal offsets show that offsets can be <b>zeroed</b> by <b>adjusting</b> PEM tilt angle, polarizer angle, and retardation. This strategy will allow one to avoid large offset drifts due to the small retardation, intensity, and beam direction fluctuations caused by lab temperature fluctuations. In addition, it will enable one to measure in the most sensitive range of the lock-in amplifiers further improving the signal to noise ratio of the setup...|$|R
40|$|Horizontal slot waveguides {{based on}} {{graphene}} {{have been considered}} an attractive structure for optical waveguide modulators for transverse magnetic (TM) modes. Graphene {{is embedded in the}} slot region of a horizontal slot waveguide. If graphene were treated as an isotropic material and its dielectric constant were made close to <b>zero</b> by <b>adjusting</b> its Fermi level, the surface-normal electric field component of the fundamental TM mode of a horizontal slot waveguide might be highly enhanced in graphene. This could cause a large increase in the attenuation coefficient of the mode. This is called the epsilon-near-zero (ENZ) effect. This paper discusses that graphene needs to be treated as an anisotropic material that has an almost real surface-normal dielectric constant component. Then, the ENZ effect does not exist. Approximate analytic expressions and numerical simulation are used for the discussion, and they demonstrate that horizontal slot waveguides are not appropriate for graphene-based modulators for TM modesopen...|$|R
40|$|Abstract. We {{describe}} atoms by a pseudo-relativistic {{model that}} has its origin {{in the work of}} Chandrasekhar. We prove that the leading energy correction for heavy atoms, the Scott correction, exists. It turns out to be lower than in the non-relativistic description of atoms. Our proof is valid up to and including the critical coupling constant. It is based on a renormalization of the energy whose <b>zero</b> level we <b>adjust</b> to be the ground-state energy of the corresponding non-relativistic problem. This allows us to roll the proof back – by relatively simple technical means – to results for the Schrödinger operator. 1...|$|R
40|$|Abstract. Error {{correcting}} output codes (ECOC) {{represent a}} successful extension of binary classifiers {{to address the}} multiclass problem. Lately, the ECOC framework was extended from the binary to the ternary case to allow classes to be ignored by a certain classifier, allowing in this way {{to increase the number}} of possible dichotomies to be selected. Nevertheless, the effect of the zero symbol by which dichotomies exclude certain classes from consideration has not been previously enough considered in the definition of the decoding strategies. In this paper, we show that by a special treatment procedure of <b>zeros,</b> and <b>adjusting</b> the weights at the rest of coded positions, the accuracy of the system can be increased. Besides, we extend the main state-of-art decoding strategies from the binary to the ternary case, and we propose two novel approaches: Laplacian and Pessimistic Beta Density Probability approaches. Tests on UCI database repository (with different sparse matrices containing different percentages of zero symbol) show that the ternary decoding techniques proposed outperform the standard decoding strategies. ...|$|R
40|$|The {{adjusted}} factor Ga is {{a generic}} friction loss correction factor for pipelines with multiple outlets. The adjusted factor Ga {{can be applied}} to pipelines with or without outflow at the downstream end. Furthermore, this factor {{can be applied to}} a pipeline where the first outlet is at a full outlet spacing or a fractional outlet spacing from the pipeline inlet. When the outflow at the downstream end is reduced to <b>zero,</b> the <b>adjusted</b> factor Ga reduces to the adjusted factor Fa. If the first outlet is positioned one outlet spacing from the pipeline inlet, the factor Ga reduces to G. Finally, if both the outflow is zero and the first outlet is one outlet spacing from the pipeline inlet, the adjusted factor Ga reduces to a close approximation of the well known factor F. The adjusted factor Ga {{is a function of the}} number of outlets along the pipeline, the location of the first outlet from the pipeline inlet, the outflow ratio, and the velocity exponent of the head loss formula...|$|R
40|$|The {{coefficient}} alpha of the Rashba spin-orbit interaction is calculated in an asymmetric quantum well consisting of Ga 0. 47 In 0. 53 As (well), Al 0. 48 In 0. 52 As (left barrier), and AlxGa 1 -xAsySb 1 -y (right barrier) {{as a function}} of the external electric field perpendicular to the well E-z(ex) which is controlled by the gate voltage. This {{coefficient alpha}}, which depends on the band offset, can be tuned to be <b>zero</b> by <b>adjusting</b> the Al fraction x in the right barrier layer to the optimum value x(0) in the case where the wave function vanishes at the left heterointerface. Such a composition-adjusted asymmetric quantum well is proposed as a structure in which the magnitude of a can be switched by changing the polarity of E-z(ex). The calculation shows that, when vertical bar x - x(0) vertical bar 40 for a large enough vertical bar E-z(ex) vertical bar (vertical bar E-z(ex) vertical bar > 10 (7) V/m for a well width of 20 nm), which results in the on/off spin-relaxation-rate ratio exceeding 10 (3) in the Dyakonov-Perel mechanism...|$|R
40|$|Abstract—A high-selectivity {{microstrip}} wideband {{bandpass filter}} with six transmission zeros using transversal signal-interaction concepts is proposed. A fifth-order wide passband with six transmission zeros (0 – 2 f 0, f 0 is center {{frequency of the}} passband) can be realized two transmission paths. The bandwidth and locations of the transmission <b>zeros</b> can be <b>adjusted</b> conveniently by changing the characteristic impedances of open stub and coupling coefficients of the open/shorted coupled lines. A prototype of planar wideband bandpass filter with 3 -dB fractional bandwidth 43. 3 % (2. 33 – 3. 63 GHz) is designed and fabricated. The measured and simulated results both indicate good performances of high selectivity and wideband harmonic suppression. 1...|$|R
30|$|The {{design of}} {{double-sided}} pyramid grating structure is adopted {{to promote the}} overall light absorption of the silicon solar cell, and it can also realize the <b>zero</b> reflection by <b>adjusting</b> the parameters. However, for the effective light absorption of CS part, it does not increase with the enhancement of the overall light absorption. For the front surface pyramid gratings, the suggested ratio of P 1 /D 1 is less than 1.4 and H 1 is between 10 and 600  nm, and for the rear surface pyramid gratings, there is {{little effect on the}} effective light absorption enhancement, so no rear gratings are necessary. Therefore, the innovation and optimized design of the front surface texture is a big trend for further improvement of solar cell efficiency.|$|R
40|$|The spin Hall effect (SHE) {{of light}} in layered {{nanostructures}} is investigated theoretically in this paper. A general propagation model describing the spin-dependent transverse splitting in the SHE of light is established {{from the viewpoint of}} classical electrodynamics. We show that the transverse displacement of wave-packet centroid can be tuned to either a negative or a positive value, or even <b>zero,</b> by just <b>adjusting</b> the structure parameters, suggesting that the SHE {{of light in}} layered nanostructures can be enhanced or suppressed in a desired way. The inherent secret behind this interesting phenomenon is the optical Fabry-Perot resonance in the layered nanostructure. We believe that these findings will open the possibility for developing new nano-photonic devices. Comment: 6 pages, 3 figure...|$|R
40|$|Electrical {{impedance}} tomography (EIT) is a {{fast and}} cost-effective technique {{to provide a}} tomographic conductivity image of a subject from boundary current–voltage data. This paper proposes a time and memory efficient method for solving a large scale 3 D EIT inverse problem using a parallel conjugate gradient (CG) algorithm. The 3 D EIT system {{with a large number}} of measurement data can produce a large size of Jacobian matrix; this could cause difficulties in computer storage and the inversion process. One of challenges in 3 D EIT is to decrease the reconstruction time and memory usage, at the same time retaining the image quality. Firstly, a sparse matrix reduction technique is proposed using thresholding to set very small values of the Jacobian matrix to <b>zero.</b> By <b>adjusting</b> the Jacobian matrix into a sparse format, the element with zeros would be eliminated, which results in a saving of memory requirement. Secondly, a block-wise CG method for parallel reconstruction has been developed. The proposed method has been tested using simulated data as well as experimental test samples. Sparse Jacobian with a block-wise CG enables the large scale EIT problem to be solved efficiently. Image quality measures are presented to quantify the effect of sparse matrix reduction in reconstruction results...|$|R
