25|60|Public
5000|$|For Microsoft Word Office Remote offers <b>zoom</b> <b>control</b> and scrolling.|$|E
5000|$|Colour {{video camera}} with pan and <b>zoom</b> <b>control</b> {{in the front}} and back.|$|E
50|$|A pan-tilt-zoom camera (PTZ camera) is {{a camera}} that {{is capable of}} remote {{directional}} and <b>zoom</b> <b>control.</b>|$|E
5000|$|Linda has an Earth-2 counterpart, Dr. Light, sent by Zoom to {{kill the}} Flash. This Linda is a {{super-powered}} thief on Earth-2 who is coerced into helping Zoom. She tries {{to kill the}} Earth-1 Linda and assume her identity to escape <b>Zoom's</b> <b>control.</b> She is defeated by the Flash, but escapes captivity without her costume.|$|R
5000|$|The lens {{features}} a compact construction colloquially {{referred to as}} a [...] "pancake lens" [...] and a micromotor-driven power <b>zoom</b> <b>controlled</b> by a switch {{on the side of the}} lens. Given the lens' focal length of 16-50mm, it is generally regarded as a multipurpose lens, though underperforms in low light situations due to its slower variable minimum aperture of f/3.5-5.6.|$|R
5000|$|Widgets: A {{number of}} widgets {{over the map}} include a {{navigator}} widget, map type (map, satellite & hybrid) controller and a <b>zoom</b> level <b>control.</b>|$|R
5000|$|... #Caption: Modern {{stereomicroscope}} optical design.A - Objective B - Galilean telescopes (rotating objectives) C - <b>Zoom</b> <b>control</b> D - Internal objective E - Prism F - Relay lens G - Reticle H - Eyepiece ...|$|E
50|$|Notes from TopXNotes on a Macintosh can be synced to an iPhone, iPad, or iPod touch A <b>zoom</b> <b>control</b> on the iPhone accesses larger text. TopXNotes Touch {{can create}} {{notes on a}} mobile device while on the go, and those notes can be synced back to the Macintosh using a home Wi-Fi.|$|E
50|$|The DVX100 also {{features}} two XLR audio inputs, another rare feature for cameras in its price range. It includes a 4-pin FireWire port, {{as well as}} S-Video and RCA in and out ports. It features manual and servo zoom, with a second <b>zoom</b> <b>control</b> and record button {{on top of the}} handle for recording from low angles.|$|E
5000|$|Typical image {{manipulation}} <b>controls</b> (<b>zoom,</b> pan, brightness/contrast, magnifier) ...|$|R
2500|$|As noted above, the {{drag-and-drop}} Pegman icon is {{the primary}} user interface element used by Google to connect Maps to Street View. [...] His name comes from his resemblance to a clothespeg. When not in use, Pegman sits atop the Google Maps <b>zoom</b> <b>controls.</b> [...] Occasionally Pegman [...] "dresses up" [...] for special events or is joined by peg friends in Google Maps. When dragged into Street View near Area 51, he becomes a flying saucer. When viewing older views, the Pegman in the minimap changes to Doc Brown from Back to the Future.|$|R
40|$|Abstract. In {{this paper}} we {{evaluate}} techniques for browsing photographs on small displays. We present two new interaction techniques that replace conventional scrolling and <b>zooming</b> <b>controls.</b> Via a single user action, scrolling and <b>zooming</b> are inter-dependently <b>controlled</b> with AutoZoom and independently controlled with GestureZoom. Both techniques were evaluated in a large-scale, 72 -subject usability experiment alongside a conventional thumbnail grid image browser. Performance {{with the new}} techniques was at least as good as that with the standard thumbnail grid, even though none of the subjects had prior experience with such systems. In {{a number of cases}} – such as finding small groups of photos or when seeking for images containing small details – the new techniques were significantly faster than the conventional approach. In addition, AutoZoom and GestureZoom supported significantly more accurate identification of subsets of photographs. Subjects also reported lower levels of physical and cognitive effort and frustration with the new techniques in comparison to the thumbnail grid browser. ...|$|R
5000|$|The user {{interface}} {{is much easier}} to use with a ring selector similar to other compact digital cameras. The <b>zoom</b> <b>control</b> {{is in the form of}} a ring surrounding the shutter button, with the parallax control now in the form of a slider on top of the camera. 2D/3D selection is through a single button, as is the video mode.|$|E
50|$|Commands in the DOS {{version of}} Skyglobe are mostly keystroke-based, and by default the {{available}} keys are {{listed on the}} screen for reference. Keys exist to adjust the viewer's location, viewing direction, and time (all of which are shown on the display by default), and to control how many objects are rendered. There is also a <b>zoom</b> <b>control,</b> and a function to search for particular objects. If the object being searched for is not currently above the horizon but {{will be in the}} next 24 hours, Skyglobe will adjust the time appropriately.|$|E
5000|$|For example, a lens for APS-C format (18&times;24 mm) with a {{focal length}} of 40 mm, might be {{described}} as [...] "60 mm (35 mm equivalent)." [...] Although its true focal length remains 40 mm, its angle of view is equivalent to that of a 60 mm lens on a 35 mm format (24&times;36 mm) camera. Another example is the lens of the 2/3 inch format Fujifilm X10, which is marked with its true zoom range [...] "7.1-28.4 mm" [...] but has 35 mm-equivalent <b>zoom</b> <b>control</b> markings ranging from [...] "28" [...] to [...] "112".|$|E
5000|$|ENG {{cameras are}} larger and heavier (helps dampen small movements), and usually {{supported}} by a camera shoulder support or shoulder stock on the camera operator's shoulder, taking the weight off the hand, which is freed to operate the <b>zoom</b> lens <b>control.</b>|$|R
40|$|We {{present a}} new {{interaction}} paradigm for digital cameras aimed at making interactive imaging algorithms accessible on these devices. In our system, the user creates visual cues {{in front of}} the lens during the live preview frames that are continuously processed before the snapshot is taken. These cues are recognized by the camera’s image processor to control the lens or other settings. We design and analyze visionbased camera interactions, including focus and <b>zoom</b> <b>controls,</b> and argue that the vision-based paradigm offers a new level of photographer control needed {{for the next generation of}} digital cameras. ACM Classification: H 5. 2 [Information interfaces and presentation]: User Interfaces. - Input devices and strategies. General terms...|$|R
40|$|In this paper, {{we present}} TiltZoom, a {{collection}} of tilt-based interaction techniques designed for easy one-handed zooming on mobile devices. TiltZoom represents novel gestural interaction techniques, implemented using rate-of-rotation readings from a gyroscope, a sensor commonly embedded on current generation smart phones. We designed and experimented three variants of TiltZoom - Tilt Level, Tilt and Hold and Flip Gesture. The design decisions for all three variants are discussed in this paper and their performance, as well as subjective user experience are evaluated and compared against conventional touch-based zooming techniques. TiltZoom {{appears to be a}} worthy addition to current established collection of gesture-based mobile interaction techniques for <b>zooming</b> <b>controls,</b> especially when user has only one hand available when moving about...|$|R
5000|$|Autonomous {{cameras are}} cameras that can direct {{themselves}} in their environment. There has been some recent work using this approach. In work from Denzler et al., the motion of a tracked object is modeled using a Kalman filter while the focal length that minimizes the uncertainty in the state estimations {{is the one that}} is used. A stereo set-up with two zoom cameras was used. A handful of papers have been written for <b>zoom</b> <b>control</b> and do not deal with total object-camera position estimation. An attempt to join estimation and control in the same framework {{can be found in the}} work of Bagdanov et al., where a Pan-Tilt-Zoom camera is used to track faces. [...] Both the estimation and control models used are ad hoc, and the estimation approach is based on image features rather than 3D properties of the target being tracked.|$|E
5000|$|The 14-140mm lens is a {{35mm camera}} {{equivalent}} focal length of 28mm wide-angle to a 280mm telephoto with manual <b>zoom</b> <b>control.</b> This lens is called a [...] "super zoom" [...] lens {{because it has a}} 10x magnification ratio as opposed to the more common and traditional zooms which tend to be in the 3x to 4x range. On larger sensor cameras (APS-C or larger), zoom lens tend to be large and heavy. The micro four thirds sensor provides some advantages in allowing a smaller, lighter, more compact zoom lens design. Even at a hefty (for MFT system lenses) weight at 460 grams, this lens is still relatively compact, includes in-lens optical stabilization and auto focusing, and very good (for a super zoom lens) optical performance. Nevertheless, the video optimized 14-140mm lens has been criticized as being too expensive for a kit lens, costing as much or more as the camera body. When the successor GH2 camera was introduced, Panasonic offered as another option, a much less expensive (and less capable) 3x zoom 14-42mm kit lens, in addition to the 14-140mm 10x zoom lens combination.|$|E
5000|$|Sliding {{the lens}} cover {{and the camera}} turns itself on very quickly and the front [...] "Z5" [...] {{lettering}} illuminates. The all-metal-bodied camera is then ready to take pictures in seconds. External controls around the camera are straightforward {{and easy to use}} for normal everyday users. However, the buttons are almost flat against the back panel making them a bit difficult to assume they were pressed. For more advanced settings, users must use Fujifilm's menu system. The rear of the camera has a large 2.5 in LCD TFT display with 230,000 pixels that display images in high quality and a high frame rate of 60 frame/s (Frame rate can be changed from 15, 30, and 60). <b>Zoom</b> <b>control</b> is actuated by the thumb and can zoom up to 18.5× (3× optical and 6.2× digital). The Fujinon lens is sharp compared to other ultra slim folded optic cameras from other manufacturers and takes vivid images. The Z5fd's autofocus system is quicker than one expects from a camera of this price and range. The face detection system locates faces by highlighting them in a green box. Up to ten different faces can be detected at one time for easy focusing. Pressing the small shutter release button to take photos.|$|E
5000|$|The {{external}} controls of the Series 1 were also mechanically {{more complex than}} the Zoomar. Most early zooms had separate twist control rings to vary the focus and focal length - a [...] "two touch" [...] zoom. The Series 1 used a single control ring: twist to focus, push-pull to zoom - a [...] "one touch" [...] zoom. For a short time, about 1980-1985, one-touch zooms were the dominant type, because of their ease of handling. However, the arrival of interchangeable lens autofocus cameras in 1985 with the Minolta Maxxum 7000 (Japan; called Alpha 7000 in Japan, 7000 AF in Europe) necessarily forced the decoupling of focusing and <b>zooming</b> <b>controls</b> and two touch zooms made an instant comeback.|$|R
50|$|Stellarium's multi-lingual {{interface}} features <b>zoom,</b> time <b>control,</b> in-built scripting {{to record}} and playback shows, fisheye projection for planetarium domes, spheric mirror projection for personal domes, telescope control, equatorial and azimuthal grids, twinkling and shooting stars, simulated eclipses, landscapes and other deep sky objects.|$|R
40|$|We {{report on}} a virtual {{environment}} for natural immersive exploration of extremely detailed surface models on light field displays. Our specialized 3 D user interface allows casual users to inspect 3 D objects at various scales, integrating panning, rotating, and <b>zooming</b> <b>controls</b> into a single low-degree-of-freedom operation, while {{taking into account the}} requirements for comfortable viewing on a light field display hardware. Specialized multiresolution structures, embedding a fine-grained per-patch spatial index within a coarse-grained patch-based mesh structure, are exploited for fast batched I/O, GPU accelerated rendering, and user-interaction-system-related geometric queries. The capabilities of the system are demonstrated by the interactive inspection of a giga-triangle dataset on a large scale 35 MPixel light field display controlled by wired or vision-based devices. 83 - 9...|$|R
40|$|A {{parametric}} camera {{model and}} calibration procedures are developed for an outdoor active camera system with pan, tilt and <b>zoom</b> <b>control.</b> Unlike traditional methods, active camera motion {{plays a key}} role in the calibration process, and no special laboratory setups are required. Intrinsic parameters are estimated automatically by fitting parametric models to the optic flow induced by rotating and zooming. No knowledge of 3 D scene structure is needed. Extrinsic parameters are calculated by actively rotating the camera to sight a sparse set of surveyed landmarks over a virtual hemispherical field of view, yielding a wellconditioned pose estimation problem. 1. Introduction This paper develops a parametric projection model for the intrinsic (lens) and extrinsic (pose) parameters of a camera with active pan, tilt and <b>zoom</b> <b>control.</b> Calibration procedures are presented for estimating intrinsic parameters by fitting parametric models to the optic flow induced by rotating and zooming the camera. [...] ...|$|E
40|$|In a {{laboratory}} experiment on multiscale pointing, we compared one-handed vs. two-handed input for two zoomcontrol devices, a first-order mouse mini-joystick vs. a zero-order mouse wheel. Using a recent method of quantifying multiple degree-of-freedom (DOF) input coordination to evaluate pan-zoom parallelism, we confirm previous work [1] showing that multiscale pointing performance strongly {{depends on the}} degree of pan-zoom parallelism. The new finding is that two-handed input and first-order <b>zoom</b> <b>control</b> allow more input parallelism, thereby increasing performance speed...|$|E
40|$|Abstract [...] Fuzzy {{logic is}} an {{innovative}} technology to design solutions for multi-parameter and non-linear control problems. It uses human experience and experimental results {{rather than a}} mathematical model for {{the definition of a}} control strategy. As a result, it often delivers solutions faster than conventional control design techniques. Today, fuzzy technology is a multibillion-dollar industry spanning the entire globe, from automatic transmission of car engines to <b>zoom</b> <b>control</b> of hand-held cameras. "The bottom line objective is to develop a robust and intelligent automation system”...|$|E
50|$|The QuickTake 100 was {{released}} in 1994 as an easy-to-use digital camera that connected to any Macintosh computer by way of an Apple serial cable. The camera was capable of storing eight photos at 640×480 resolution, 32 photos at 320×240 resolution, or a mixture of both sizes. All photos were at 24-bit color. The camera had a built-in flash, but no focus or <b>zoom</b> <b>controls.</b> Other than downloading the photos to a computer, {{there was no way}} to preview them on the camera, nor was there any way to delete individual photos from the camera (though there was a recessed 'trash' button which would delete the entire contents of the camera). It was one of the first digital cameras released targeted to consumers.|$|R
40|$|Wereportonavirtualenvironmentfornaturalimmersiveexplorationofextremelydetailedsurfacemodelsonmulti-projectorlight field displays, {{which give}} multiple, freely moving, naked-eye viewers the {{illusion}} of seeing and manipulating 3 D objects with continuous horizontal parallax. Our specialized 3 D user interface, dubbed FOX (Focus Sliding Surface), allows inexperienced users to inspect 3 D objects at various scales, integrating panning, rotating, and <b>zooming</b> <b>controls</b> into a single low-degree-offreedom operation. At the same time, FOX {{takes into account the}} requirements for comfortable viewing on the light field display hardware, which has a limited field-of-view and a variable spatial resolution. Specialized multi-resolution structures, embedding a fine-grained, per-patch spatial index within a coarse-grained patch-based mesh structure, are exploited for fast batched I/O, GPUaccelerated rendering, and user-interaction-system-related geometric queries. The capabilities of the system are demonstrated by theinteractiveinspectionofagiga-triangledatasetonalarge-scale, 35 MPixellightfielddisplaycontrolledbywiredorvision-based devices. Results of athorough user evaluation, involving quantitative and subjective measurements, are discussed...|$|R
40|$|INTRODUCTION Robotic {{streaming}} video cameras with pan, tilt, and <b>zoom</b> <b>controls</b> are now commercially available {{and are being}} installed in hundreds of locations around the world. Remote viewers can adjust camera parameters via the Internet to observe desired details in the scene. Current methods restrict control to one user at a time; users have to wait in a queue for their turn to operate the camera. In this thesis, we develop ShareCam, a new approach that eliminates the queue and allows many users to access and share control of the robotic camera simultaneously. Since conflicting frame requests are made by users, a primary challenge is computing optimal camera parameters. We formalize the problem using a new metric, Intersection Over Maximum (IOM), to model the degree of satisfaction for each user, and seek to maximize total satisfaction for n users. We develop online algorithms to solve this optimization problem for cases where pan, tilt and zoom values can be either discrete o...|$|R
40|$|We present gravity {{navigation}} (GravNav), {{a family}} of multi-scale navigation techniques that use a gravity-inspired model for assisting navigation in large visual 2 D spaces based on the interest and salience of visual objects in the space. GravNav is an instance of topology-aware navigation, which makes use {{of the structure of}} the visual space to aid navigation. We have performed a controlled study comparing GravNav to standard zoom and pan navigation, with and without variable-rate <b>zoom</b> <b>control.</b> Our results show a significant improvement for GravNav over standard navigation, particularly when coupled with variable-rate zoom. We also report findings on user behavior in multi-scale navigation...|$|E
40|$|Abstract. We {{describe}} {{a method for}} selecting optimal actions affecting the sensors in a probabilistic state estimation framework, with an ap-plication in selecting optimal zoom levels for a motor-controlled camera in an object tracking task. The action is selected to minimize the ex-pected entropy of the state estimate. The contribution {{of this paper is}} the ability to incorporate varying costs into the action selection process by looking multiple steps into the future. The optimal action sequence then minimizes both the expected entropy and the costs it incurs. This method is then tested with an object tracking simulation, show-ing the benefits of multi-step versus single-step action selection in cases where the cameras ’ <b>zoom</b> <b>control</b> motor is insufficiently fast. ...|$|E
40|$|An {{audiovisual}} interface {{equipped with}} a projector, an inclina-tion sensor, and a distance sensor for <b>zoom</b> <b>control</b> has been developed that enables a user to selectively view and listen to specific performers in a video-taped group performance. Dubbed Concert Viewing Headphones, it has both image and sound processing functions. The image processing extracts {{the portion of the}} image indicated by the user and projects it free of distortion on the front and side walls. The sound processing creates imaginary microphones for those performers without one so that the user can hear the sound from any performer. Testing using images and sounds captured using a fisheye-lens camera and 37 lavalier microphones showed that sound locali-zation was fastest when an inverse square function was used for the sound mixing and that the zoom function was useful for locating the desired sound performance...|$|E
50|$|Camera's captures include {{location}} information if the user provides the app {{permission to use}} it. Additional settings included in the app include time delay, <b>zooming,</b> focus <b>control,</b> sensitivity control, white balance control, shutter speed control, brightness control, and a toggle for switching between different cameras. For instance, most Windows phones and tablets have both front- and rear-facing cameras, so Camera’s switch button toggles between the two options.|$|R
30|$|Miyaki, T. and J. Rekimoto, GraspZoom: <b>zooming</b> and {{scrolling}} <b>control</b> {{model for}} single-handed mobile interaction, in Proceedings of the 11 th International Conference on Human-Computer Interaction with Mobile Devices and Services. 2009, ACM: Bonn, Germany.|$|R
40|$|The {{focus of}} spatial {{attention}} can {{be not only}} oriented to a particular location, but also adjusted in its size to select visual information from a narrow (zoom-in) or broad (zoom-out) region of the visual field. Attentional orienting, saccades programming, and visual search {{have been linked to}} the frontal eye fields (FEF) activity. However, the FEF causal role in the frontoparietal network for the attentional focus size modulation remains unclear. Here, we delivered single-pulse transcranial magnetic stimulation (TMS) on FEF while participants performed an attentional zooming task. They were asked to detect a visual target appearing at 3 eccentricities from the fixation. Two cue types modulated the size of the attended region: a small cue was employed to narrow the attentional focus, whereas a large cue induced participants to broaden the attended region. Results showed that TMS delivered on the right FEF, but not on the left FEF, was able to interfere with both zoom-in and zoom-out attentional mechanisms. Our results provide the first evidence of the right FEF casual role in the attentional <b>zooming</b> <b>control</b> and give new insights into the neural mechanisms of dysfunctional spatial attention deployment shown in neurodevelopmental disorders, such as autism and dyslexia...|$|R
