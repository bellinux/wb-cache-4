6|10|Public
25|$|A stereo {{microphone}} integrates two microphones in {{one unit}} {{to produce a}} <b>stereophonic</b> <b>signal.</b> A stereo microphone is often used for broadcast applications or field recording {{where it would be}} impractical to configure two separate condenser microphones in a classic X-Y configuration (see microphone practice) for stereophonic recording. Some such microphones have an adjustable angle of coverage between the two channels.|$|E
5000|$|In 1974, {{auditory}} researchers {{used the}} melody of [...] "Daisy Bell" [...] {{for the first}} demonstration of [...] "pure dichotic" [...] (two-ear only) perception: they encoded the melody in a <b>stereophonic</b> <b>signal</b> {{in such a way}} that it could be perceived when listening with both ears but not with either ear alone.|$|E
50|$|A stereo {{microphone}} integrates two microphones in {{one unit}} {{to produce a}} <b>stereophonic</b> <b>signal.</b> A stereo microphone is often used for broadcast applications or field recording {{where it would be}} impractical to configure two separate condenser microphones in a classic X-Y configuration (see microphone practice) for stereophonic recording. Some such microphones have an adjustable angle of coverage between the two channels.|$|E
40|$|A new {{technique}} is presented for decorrelating the input <b>signals</b> in <b>stereophonic</b> systems {{by means of}} time-varying allpass filters. The controllable and almost linear phase response of higher-order allpass filters is exploited in this context and {{a particular type of}} time variation is used to vary the parameters of such filters. Simulation results are presented based on real <b>stereophonic</b> <b>signals</b> and supported by subjective listening tests. © IEE 1999...|$|R
50|$|KYET (1170 AM) is a {{radio station}} {{broadcasting}} a Classic Country format. Licensed to Golden Valley, Arizona, United States, it serves the Mohave County area. The station is currently owned by Grand Canyon Gateway Broadcasting, LLC. KYET transmits a <b>stereophonic</b> multiplex <b>signal</b> in the C-QUAM standard.|$|R
40|$|A {{method of}} {{encoding}} a multichannel signal, {{such as a}} <b>stereophonic</b> audio <b>signal,</b> including at least first and second signal components includes transforming {{at least the first}} and second signal components by a predetermined transformation into a principal signal including most of the signal energy and at least one residual signal including less energy than the principal signal. The predetermined transformation is parameterized by at least one transformation parameter. The method further includes representing the multichannel signal at least by the principal signal and the transformation parameter...|$|R
50|$|Monaural sound {{has been}} {{replaced}} by stereo sound in most entertainment applications. However, it remains the standard for radiotelephone communications, telephone networks, and audio induction loops for use with hearing aids. A few FM radio stations, particularly talk radio shows, choose to broadcast in monaural, as a monaural signal has a slight advantage in signal strength and bandwidth over a <b>stereophonic</b> <b>signal</b> of the same power.|$|E
40|$|Traditional MDCT-based perceptual audio coding schemes employ mid/side and {{intensity}} stereo techniques to allow efficient joint coding {{of the two}} channels of a <b>stereophonic</b> <b>signal.</b> These techniques, however, provide only little coding gain for critical stereo signals characterized by spectral components with a distinct level or phase difference between the channels. To overcome this deficiency, we propose an extension to the mid/side coding paradigm that utilizes complex-valued inter-channel linear prediction in the MDCT spectral domain. The required imaginary spectrum (MDST) is calculated in a computationally efficient manner without additional algorithmic delay. A formal listening test conducted {{in the course of}} the ISO/MPEG standardization of the unified speech and audio codec USAC illustrates that the proposed stereo prediction approach provides significant improvements in coding efficiency and shows that at 96 kb/s, excellent quality can be obtained even for critical signals...|$|E
40|$|We {{propose a}} new {{approach}} to solo/accompaniment separation from stereophonic music recordings which extends a monophonic algorithm we recently proposed. The solo part is modelled using a source/filter model to which we added two contributions: an explicit smoothing strategy for the filter frequency responses and an unvoicing model to catch the stochastic parts of the solo voice. The accompaniment is modelled as a general instantaneous mixture of several components leading to a Nonnegative Matrix Factorization framework. The <b>stereophonic</b> <b>signal</b> is assumed to be the instantaneous mixture of the solo and accompaniment contributions. Both channels are then jointly used within a Maximum Likelihood framework to estimate all the parameters. Three rounds of parameter estimations are necessary to sequentially estimate the melody, the voiced part and at last the unvoiced part of the solo. Our tests show that there is a clear improvement from a monophonic reference system to the proposed stereophonic system, especially when including the unvoicing model. The smoothness of the filters does not provide the desired improvement in solo/accompaniment separation, but may be useful in future applications such as lyrics recognition. At last, our submissions to the Signal Separation Evaluation Campaign (SiSEC), for the “Professionally Produced Music Recordings ” task, obtained very good results...|$|E
40|$|Binaural room impulse {{responses}} (BRIRs) {{of various}} lengths were convolved with <b>stereophonic</b> audio <b>signals.</b> Listening {{tests were conducted}} to assess how the length of BRIRs affected the perceived externalisation effect and tonal colouration of the audio. The results showed statistically significant correlations between BRIR length and both externalisation and tonal colouration. Conclusions are drawn from this and in addition, reasoning, a critical evaluation and suggested further work are suggested. The experiment provides the basis for further development of an effective and efficient externalisation algorithm...|$|R
40|$|International audienceIn the literature, several {{probabilistic}} models involving latent components {{have been}} proposed for modelling time-frequency (TF) representations of audio signals (such as spectrograms), notably in the nonnegative matrix factorization (NMF) literature. Among them, the recent high resolution (HR) -NMF model is able to take both phases and local correlations in each frequency band into account, and its potential has been illustrated in applications such as source separation and audio inpainting. In this paper, HR-NMF is extended to multichannel signals and to convolutive mixtures. A fast variational expectation-maximization (EM) algorithm is proposed to estimate the enhanced model. This algorithm is applied to a <b>stereophonic</b> piano <b>signal,</b> and proves capable of accurately modelling reverberation and restoring missing observations...|$|R
40|$|Abstract—In many (audio) {{processing}} algorithms, involving {{manipulation of}} discrete-time signals, the performance can vary strongly over the repertoire that is used. This {{may be the}} case when the signals from the various channels are allowed to be strongly positively or negatively correlated. We propose and analyze a general formula for tracking the (time-dependent) correlation between two signals. Some special cases of this formula lead to classical results known from the literature, others are new. This formula is recursive in nature, and uses only the instanta-neous values of the two signals, in a low-cost and low-complexity manner; in particular, {{there is no need to}} take square roots or to carry out divisions. Furthermore, this formula can be modified with respect to the occurrence of the two signals so as to further decrease the complexity, and increase ease of implementation. The latter modification comes at the expense that not the actual correlation is tracked, but, rather, a somewhat deformed version of it. To overcome this problem, we propose, for a number of instances of the tracking formula, a simple warping operation on the deformed correlation. Now we obtain, at least for sinusoidal signals, the correct value of the correlation coefficient. Special attention is paid to the convergence behavior of the algorithm for stationary signals and the dynamic behavior if there is a transition to another stationary state; the latter is considered to be important to study the tracking abilities to nonstationary signals. We illustrate tracking algorithm by using it for stereo music fragments, obtained from a number of digital audio recordings. Index Terms—Audio, cross-correlation coefficient, real-time tracking algorithm, <b>stereophonic</b> <b>signals.</b> I...|$|R
40|$|International audienceWe {{propose a}} projection-based method for the unmixing of {{multi-channel}} audio signals into their different constituent spatial objects. Here, spatial objects are modelled using a unified framework which handles both point sources and diffuse sources. We then propose a novel methodology to estimate {{and take advantage}} of the spatial dependencies of an object. Where previous research has processed the original multichannel mixtures directly and has been principally focused on the use of inter-channel covariance structures, here we instead process projections of the multichannel signal on many different spatial directions. These linear combinations consist of observations where some spatial objects are cancelled or enhanced. We then propose an algorithm which takes these projections as the observations, discarding dependencies between them. Since each one contains global information regarding all channels of the original multichannel mixture, this provides an effective means of learning the parameters of the original audio, while avoiding the need for joint-processing of all the channels. We further show how to recover the separated spatial objects and demonstrate the use of the technique on <b>stereophonic</b> music <b>signals...</b>|$|R
40|$|We {{describe}} a sensory substitution scheme that converts a video stream into an audio stream in real-time. It was initially {{developed as a}} research tool for studying human ability to learn new ways of perceiving the world: the Vibe can give us the ability to learn a kind of ‘vision’ by audition. It converts a video stream into a continuous <b>stereophonic</b> audio <b>signal</b> that conveys information coded from the video stream. The conversion from the video stream to the audio stream uses a kind of retina with receptive fields. Each receptive field controls a sound source and the user listens to a sound that {{is a mixture of}} all these sound sources. Compared to other existing vision-to-audition sensory substitution devices, the Vibe is highly versatile in particular because it uses a set of configurable units working in parallel. In order to demonstrate the validity and interest of this method of vision to audition conversion, we give the results of an experiment involving a pointing task to targets memorised through visual perception or through their auditory conversion by the Vibe. This article is also an opportunity to precisely draw the general specifications of this scheme in order to prepare its implementation on an autonomous/mobile hardware...|$|R
40|$|Abstract—Binaural Cue Coding (BCC) is {{a method}} for {{multichannel}} spatial rendering based on one down-mixed audio channel and BCC side information. The BCC side information has a low data rate and it {{is derived from the}} multichannel encoder input signal. A natural application of BCC is multichannel audio data rate reduction since only a single down-mixed audio channel needs to be transmitted. An alternative BCC scheme for efficient joint transmission of independent source signals supports flexible spatial rendering at the decoder. This paper (Part I) discusses the most relevant binaural perception phenomena exploited by BCC. Based on that, it presents a psychoacoustically motivated approach for designing a BCC analyzer and synthesizer. This leads to a reference implementation for analysis and synthesis of <b>stereophonic</b> audio <b>signals</b> based on a Cochlear Filter Bank. BCC synthesizer implementations based on the FFT are presented as low-complexity alternatives. A subjective audio quality assessment of these implementations shows the robust performance of BCC for critical speech and audio material. Moreover, the results suggest that the performance given by the reference synthesizer is not significantly compromised when using a low-complexity FFT-based synthesizer. The companion paper (Part II) generalizes BCC analysis and synthesis for multichannel audio and proposes complete BCC schemes including quantization and coding. Part II also describes an alternative BCC scheme with flexible rendering capability at the decoder and proposes several applications for both BCC schemes. Index Terms—Audio coding, auditory filter bank, auditory scene synthesis, binaural source localization, coding of binaural spatial cues, spatial rendering. I...|$|R

