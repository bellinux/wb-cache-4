232|165|Public
50|$|Intra-frame {{prediction}} exploits <b>spatial</b> <b>redundancy,</b> i.e. correlation among pixels {{within one}} frame, by calculating prediction values through extrapolation from already coded pixels for effective delta coding. It {{is one of}} the two classes of predictive coding methods in video coding. Its counterpart is inter-frame prediction which exploits temporal redundancy. Temporally independently coded so-called intra frames use only intra coding. The temporally coded predicted frames (e.g. MPEG's P- and B-frames) may use intra- as well as inter-frame prediction.|$|E
50|$|Today, {{nearly all}} {{commonly}} used video compression methods (e.g., those in standards {{approved by the}} ITU-T or ISO) apply a discrete cosine transform (DCT) for <b>spatial</b> <b>redundancy</b> reduction. The DCT that is widely used in this regard was introduced by N. Ahmed, T. Natarajan and K. R. Rao in 1974. Other methods, such as fractal compression, matching pursuit {{and the use of}} a discrete wavelet transform (DWT) {{have been the subject of}} some research, but are typically not used in practical products (except for the use of wavelet coding as still-image coders without motion compensation). Interest in fractal compression seems to be waning, due to recent theoretical analysis showing a comparative lack of effectiveness of such methods.|$|E
50|$|Uncompressed video {{delivers}} maximum quality, {{but with}} a very high data rate.A variety of methods are used to compress video streams, with the most effective ones using a Group Of Pictures (GOP) to reduce spatial and temporal redundancy. Broadly speaking, <b>spatial</b> <b>redundancy</b> is reduced by registering differences between parts of a single frame; this task is known as intraframe compression and {{is closely related to}} image compression. Likewise, temporal redundancy can be reduced by registering differences between frames; this task is known as interframe compression, including motion compensation and other techniques. The most common modern standards are MPEG-2, used for DVD, Blu-ray and satellite television, and MPEG-4, used for AVCHD, Mobile phones (3GP) and Internet.|$|E
40|$|Advanced analog/digital {{electronic}} system for compression of video signals incorporates artificial neural networks. Performs motion-estimation and image-data-compression processing. Effectively eliminates temporal and <b>spatial</b> <b>redundancies</b> of sequences of video images; processes video image data, retaining only nonredundant parts to be transmitted, then transmits resulting data stream {{in form of}} efficient code. Reduces bandwidth and storage requirements for transmission and recording of video signal...|$|R
40|$|International audienceA novel reading {{architecture}} for CMOS {{image sensor}} for low data rate {{has been investigated}} in this paper. The proposed architecture is designed using asynchronous logic and is intended to control and manage the flow of event-driven pixels. This architecture overcomes the standard difficulties encountered when managing simultaneous pixel requests without degrading the image sensor fill factor and resolution. Moreover, this reading architecture does not need an analog-to-digital converter and is capable of suppressing the <b>spatial</b> <b>redundancies.</b> This leads to a reduced image data flow...|$|R
40|$|Multiview video {{compression}} {{is important to}} the imagebased 3 D video applications. In this paper, we proposes a novel neighbor-based multiview {{video compression}} scheme. It is essentially a MPEG 2 -like block-based scheme. In particular, a method to decide the stream encoding order is presented. The resulting stream encoding order can better decorrelate <b>spatial</b> <b>redundancies</b> among multiple video streams than the center approach. Experimental results confirm the superiority of the proposed neighbor approach over the center approach and MPEG 2 for multiview video compression...|$|R
5000|$|The inter-picture {{prediction}} reduces temporal redundancy, with motion vectors used {{to compensate}} for motion. Whilst only integer-valued motion vectors are supported in H.261, a blurring filter {{can be applied to}} the prediction signal - partially mitigating the lack of fractional-sample motion vector precision. Transform coding using an 8×8 discrete cosine transform (DCT) reduces the <b>spatial</b> <b>redundancy.</b> The DCT that is widely used in this regard was introduced by N. Ahmed, T. Natarajan and K. R. Rao in 1974. Scalar quantization is then applied to round the transform coefficients to the appropriate precision determined by a step size control parameter, and the quantized transform coefficients are zig-zag scanned and entropy-coded (using a [...] "run-level" [...] variable-length code) to remove statistical redundancy.|$|E
5000|$|The oversampled {{binary image}} sensor is {{reminiscent}} of photographic film. Each pixel in the sensor has a binary response, giving only a one-bit quantized measurement of the local light intensity. At {{the start of the}} exposure period, all pixels are set to 0. A pixel is then set to 1 if the number of photons reaching it during the exposure is at least equal to a given threshold q. One way to build such binary sensors is to modify standard memory chip technology, where each memory bit cell is designed to be sensitive to visible light. With current CMOS technology, the level of integration of such systems can exceed 109~1010 (i.e., 1 giga to 10 giga) pixels per chip. In this case, the corresponding pixel sizes (around 50~nm [...] ) are far below the diffraction limit of light, and thus the image sensor is oversampling the optical resolution of the light field. Intuitively, one can exploit this <b>spatial</b> <b>redundancy</b> to compensate for the information loss due to one-bit quantizations, as is classic in oversampling delta-sigma conversions.|$|E
50|$|With the {{development}} of very large scale integrated circuit technologies, large workload of computations became achievable which made real-time 3D ultrasound imaging possible. Much research showed that 3D imaging which generates volumetric images, will result in better medical interpretations and more accurate diagnosis than the conventional 2D method. However, the benefits of 3-D ultrasound imaging come with challenges concerning the computational complexities. 3-D imaging requires a 2-D array transducer in which the number of elements can be ten {{times more than the}} linear 1-D array transducer. Moreover, the number of scanlines required in a 3-D volume is at least one order of magnitude higher than in a 2-D image in order to form the volumetric display. To reduce the computational complexities, researchers proposed several methods to support the implementation. To replace the traditional 2-D array, 2-D sparse arrays were put forward to reduce the <b>spatial</b> <b>redundancy.</b> The method of preliminary beamforming for reconfigurable arrays was proposed {{to reduce the number of}} A/D converters and the front-end complexities. To improve the imaging resolution, separable beamforming for SAU 3D imaging was used to decompose the 2D beamforming as a series of 1-D array processing, which was shown to produce images with comparable quality as the non-separable method. A better performance of these methods and devices will be achieved as the technology is improving in terms of higher computation speed and smaller size.|$|E
30|$|GM-LOG [30] {{uses the}} joint {{statistical}} {{relationship between the}} local contrast features of Laplacian of Gaussian (LoG) and gradient magnitude (GM) for BIQA. An adaptive procedure called joint adaptive normalization based on gain control and divisive normalization models on the local neighborhood is used to remove the <b>spatial</b> <b>redundancies</b> of GM and LOG coefficients. The technique follows a two-step approach, i.e., identification of distortion type and quality score prediction. A total of 40 features are extracted, which describe the structural information of the images for assessing the qualityusing SVR.|$|R
40|$|Multiple intra-mode {{prediction}} {{is one of}} the new features introduced in the emerging video standard, H. 264 /AVC. Its function is to further reduce <b>spatial</b> <b>redundancies</b> in an intracoded macroblock prior to conventional transform coding and entropy coding. However, the process is computationally expensive especially when Lagrangian cost evaluation is employed. In this paper, we propose a fast algorithm operating in the frequency domain to accelerate intra-frame mode selection. Extensive simulations verify that the proposed algorithm outperforms several existing methods, providing speed-ups of up to 75 % with insignificant degradation in picture quality. 1...|$|R
40|$|To achieve {{high rates}} of {{compression}}, the MPEG video compression standard provides methods for reducing both <b>spatial</b> and temporal <b>redundancies</b> in video data. The key step in lowering <b>spatial</b> <b>redundancies</b> is {{the application of the}} two-dimensional Discrete Cosine Transformation, while calculation of motion vectors is vital for temporal dependence reduction. Both the DCT coefficients and motion vectors can be used to provide information about the video data itself, without actually viewing or even completely decompressing the original data. After discussing different models for background noise, statistical hypothesis tests are developed to detect either the presence or motion of an object in a small area of one picture in the video sequence using either the DCT or motion vector data. The hypothesis tests are then combined into a unified cumulative sum procedure, based on the p-values of the hypothesis tests, which will signal an alarm at the point in time when an object appears in the video under investigation...|$|R
50|$|An I-frame is a {{compressed}} {{version of}} a single uncompressed (raw) frame. It takes advantage of <b>spatial</b> <b>redundancy</b> and of {{the inability of the}} eye to detect certain changes in the image. Unlike P-frames and B-frames, I-frames do not depend on data in the preceding or the following frames. Briefly, the raw frame is divided into 8 pixel by 8 pixel blocks. The data in each block is transformed by the discrete cosine transform (DCT). The result is an 8 by 8 matrix of coefficients. The transform converts spatial variations into frequency variations, but it does not change the information in the block; the original block can be recreated exactly by applying the inverse cosine transform. The advantage of doing this is that the image can now be simplified by quantizing the coefficients. Many of the coefficients, usually the higher frequency components, will then be zero. The penalty of this step is the loss of some subtle distinctions in brightness and color. If one applies the inverse transform to the matrix after it is quantized, one gets an image that looks very similar to the original image but that is not quite as nuanced. Next, the quantized coefficient matrix is itself compressed. Typically, one corner of the quantized matrix is filled with zeros. By starting in the opposite corner of the matrix, then zigzagging through the matrix to combine the coefficients into a string, then substituting run-length codes for consecutive zeros in that string, and then applying Huffman coding to that result, one reduces the matrix to a smaller array of numbers. It is this array that is broadcast or that is put on DVDs. In the receiver or the player, the whole process is reversed, enabling the receiver to reconstruct, to a close approximation, the original frame.|$|E
30|$|STS[*]=[*] 2) {{to perform}} <b>spatial</b> <b>redundancy</b> processing.|$|E
3000|$|... [*]H 264 /AVC no motion I-([...] GOP - 1)B-I: It {{exploits}} the <b>spatial</b> <b>redundancy</b> {{without any}} motion estimation.|$|E
40|$|We {{describe}} a novel integrated algorithm for real-time enhancement of video acquired under challenging lighting conditions. Such conditions include low lighting, haze, and {{high dynamic range}} situations. The algorithm automatically detects the dominate source of impairment, then depending on whether it is low lighting, haze or others, a corresponding pre-processing {{is applied to the}} input video, followed by the core enhancement algorithm. Temporal and <b>spatial</b> <b>redundancies</b> in the video input are utilized to facilitate real-time processing and to improve temporal and spatial consistency of the output. The proposed algorithm can be used as an independent module, or be integrated in either a video encoder or a video decoder for further optimizations. Comment: 10 pages, 23 figure...|$|R
40|$|Reversible {{compression}} of images {{has been the}} topic of considerable research [l- 31, as it finds applications in many fields in which the deviation of the reproduced image from the original image is intolerable, however small be the deviation. This paper {{is concerned with the}} problem of reducing <b>spatial</b> <b>redundancies</b> in gray scale images, thus providing effective lossless compression, using segmentation information. We will present new edge models that deal effectively with two issues that make such models normally unsuitable for compression applications: local applicability and large number of parameters needed for representation. Segmentation information is provided by a recent transform [6], which we found to possess qualities making it especially suitable for compression. The final residual imag...|$|R
40|$|International audienceWe {{present an}} {{inpainting}} method for images and videos based on nonlocal discrete p-Laplace regularization on weighted graphs. Our work {{has the advantage}} of unifying local geometric methods and nonlocal exemplar-based ones in the same framework. Our image inpainting benefits from local and nonlocal regularities within the image. In addition to that, our video inpainting exploits temporal and <b>spatial</b> <b>redundancies</b> in order to obtain high quality results by considering a video sequence as a volume and not as a sequence of still frames. However, our method does not employ any motion estimation for video inpainting. Experiments demonstrate that our nonlocal method outperforms the local one by completing missing data with finer and more consistent details for textured and non-textured images and videos...|$|R
3000|$|... [*]H 264 /AVC intra I-I-I: Each {{frame is}} encoded {{independently}} from the neighboring frames without exploiting the <b>spatial</b> <b>redundancy.</b>|$|E
40|$|In {{order to}} {{maintain}} the high reliability of a computer system, it is necessary to detect the failure leading to a fault. In general, fault can be detected by exploiting time redundancy or <b>spatial</b> <b>redundancy.</b> However, it negatively affects on either hardware cost or processor performance. To solve the cost-performance issue, in this paper, we propose a concept of cost-effective approach to achieve <b>spatial</b> <b>redundancy</b> for dependable processors. In addition, we perform a primly evaluation for the impact of our method on processor performance. 1...|$|E
40|$|In this paper, {{we present}} a novel {{approach}} to saliency detection. We define a visually salient region in an image with following two properties; global <b>spatial</b> <b>redundancy,</b> i. e., mutual-information, and local saliency, i. e., self-information or simply the region complexity. The former is its probability of occurrence within the image, whereas the latter defines how much information is contained within a region, and it is quantified by the entropy. By combining the global <b>spatial</b> <b>redundancy</b> measure and local entropy, we can achieve a simple, yet robust saliency detector. We evaluate it quantitatively and qualitatively. The comparison to Itti et al. [6], the spectral residual approach by Hou and Zhang [5], Achanta et al. [13] {{as well as to}} Zhai and Shah [14], on publicly available data shows a significant improvement...|$|E
40|$|International audienceWe present nonlocal {{algorithms}} {{for video}} denoising, simplification and inpainting {{based on a}} generic framework of discrete regularization on graphs. We express video denoising, simplification and inpainting problems using the same variational formulation. The main advantage of this framework is the unification of local and nonlocal approaches for these processing procedures. We take advantage of temporal and <b>spatial</b> <b>redundancies</b> {{in order to produce}} high quality results. In this paper, we consider a video sequence as a volume rather than a sequence of frames, and employ algorithms that do not require any motion estimation. For video inpainting, we unify geometric- and texture-synthesis-based approaches. To reduce the computational effort, we propose an optimized method that is faster than the nonlocal approach, while producing equally appealing results...|$|R
30|$|Band {{selection}} and sample covariance matrix estimation {{are presented in}} this article to carry out <b>spatial</b> and spectral <b>redundancy</b> information reduction.|$|R
40|$|In this contribution, a novel spatio-temporal {{prediction}} algorithm {{for video}} coding is introduced. This algorithm exploits temporal {{as well as}} <b>spatial</b> <b>redundancies</b> for effectively predicting the signal to be encoded. To achieve this, the algorithm operates in two stages. Initially, motion compensated prediction is applied on the block being encoded. Afterwards this preliminary temporal prediction is refined by forming a joint model of the initial predictor and the spatially adjacent already transmitted blocks. The novel algorithm is able to outperform earlier refinement algorithms in speed and prediction quality. Compared to pure motion compensated prediction, the mean data rate can be reduced by up to 15 % andupto 1. 16 dB gain in PSNR can be achieved for the considered sequences. Index Terms — Signal extrapolation, Video coding, Prediction 1...|$|R
40|$|Abstract — Location is {{considered}} an important attribute in wireless sensor networks allowing the nodes to send location stamped data. We present a hybrid approach to network localization that exploits the <b>spatial</b> <b>redundancy</b> in the data gathered by such networks and then utilizes the classical localization techniques to refine position estimates. I...|$|E
40|$|Abstract. Shrinking feature {{sizes and}} energy levels coupled with high clock rates and {{decreasing}} node capacitance lead {{us into a}} regime where transient errors in logic cannot be ignored. Consequently, several recent {{studies have focused on}} feed-forward <b>spatial</b> <b>redundancy</b> techniques to combat these high transient fault rates. To complement these studies, we analyze fine-grained rollback techniques and show that they can offer lower <b>spatial</b> <b>redundancy</b> factors with no significant impact on system performance for fault rates up to one fault per device per ten million cycles of operation (Pf = 10 − 7) in systems with 1012 susceptible devices. Further, we concretely demonstrate these claims on nanowire-based Programmable Logic Arrays. Despite expensive rollback buffers and general-purpose, conservative analysis, we show the area overhead factor of our technique is roughly an order of magnitude lower than a gate-level feed-forward redundancy scheme. 1...|$|E
40|$|In this paper, {{we present}} a novel {{approach}} to saliency detection. The method we propose here aims at synthesizing common knowledge of saliency in an image. We define a visually salient region with following two parameters; the <b>spatial</b> <b>redundancy</b> and its local appearance. The former is its probability of occurrence within the image, which is a quantification of the “rarity” of the concerned region, whereas the latter defines how much information is contained within the region, {{and it can be}} quantified using the entropy. By combining the global <b>spatial</b> <b>redundancy</b> measure and local entropy, we can achieve a simple, yet robust measure. We evaluated and compared it to Itti’s and the spectral residual methods, and it has shown a significant improvement of performance. CVIM 2013 年 3 月研究会プログラム 第 186 回, 大阪大学吹田キャンパス産業科学研究所, 3 月 15 日(金...|$|E
40|$|International audienceIn this paper, {{we explain}} and {{validate}} {{the architecture of}} a fully asynchronous reading system dedicated to an event-driven image sensor. The proposed architecture is modelled with a hardware description language. Firstly, the reading method relies on reducing the output data flow of the image sensor. Therefore, the image sensor global activity {{is limited to the}} effective and relevant pixel information. Moreover, this reading technique is capable of suppressing <b>spatial</b> <b>redundancies.</b> Furthermore, the analog to digital converter, which usually is the most consuming component in a standard reading system, has been removed in our architecture. The fully asynchronous Register-Transfer level model of our reading system has been tested and validated using a real picture as a testbench. Finally, the comparison with a standard reading system shows that for a similar PSNR, we are able to drastically reduce the image data flow...|$|R
40|$|An Autostereoscopic 30 viewing. {{system that}} {{operates}} {{on the principles}} of Integral PhotographyIP provides a imique sen. se of depth. full parallax and multi-view functionality. The inherent redundancy of these images results into great amounts of data that should be efficiently coded for transmission or storage operations. In this communication a method for efficient coding of such images is presented, targeting to 30 imaging but video applications. The method is based on common techniques broadly used in image compression and properly adjusted in order {{to take advantage of the}} <b>spatial</b> <b>redundancies</b> of IP images. The generalig and flexibility of the proposed approach along with the stability far a wide range of bit rates constitutes the basic characteristics of the technique. The proposed technique can be easily realized in sofmare or hardn'are for computer based or standalone applications. 1...|$|R
40|$|Standard image {{sequence}} coding methods, such as MPEG, use block-based {{motion compensation}} and transform techniques to reduce temporal and <b>spatial</b> <b>redundancies.</b> Higher compression, however, {{can be achieved}} by object-based image sequence representation. In an object-based representation scheme, the image sequence is decomposed into objects on the basis of motion, texture, structure, etc. By efficiently encoding these arbitrarily shaped objects, significantly lower bit rates can be achieved. We propose an object coding method which separately encodes the intensity and shape of objects. A compact description of the shape is obtained by using a contour encoding algorithm, while efficient transform coding techniques are used to compress the intensity. We describe three such techniques: smooth filling, cosine filling, and region dependent transform coding. We compare these methods with the KLT. Thesis Supervisor: E. H. Adelson Title: Associate Professor, M. I. T. Media Lab Acknowledgements Fi [...] ...|$|R
40|$|Shrinking feature {{sizes and}} energy levels coupled with high clock rates and {{decreasing}} node capacitance lead {{us into a}} regime where transient errors in logic cannot be ignored. Consequently, several recent {{studies have focused on}} feed-forward <b>spatial</b> <b>redundancy</b> techniques to combat these high transient fault rates. To complement these studies, we analyze fine-grained rollback techniques and show that they can offer lower <b>spatial</b> <b>redundancy</b> factors with no significant impact on system performance for fault rates up to one fault per device per ten million cycles of operation (Pƒ = 10 - 7) in systems with 1012 susceptible devices. Further, we concretely demonstrate these claims on nanowire-based programmable logic arrays. Despite expensive rollback buffers and general-purpose, conservative analysis, we show the area overhead factor of our technique is roughly an order of magnitude lower than a gate level feed-forward redundancy scheme...|$|E
30|$|To {{effectively}} remove <b>spatial</b> <b>redundancy</b> between lines, {{a vertical}} interline predictive method is used. The proposed interline prediction method can calculate several directional sum of absolute difference (SAD) between two adjacent lines and then choose the prediction mode to minimize SAD. Four prediction modes {{are determined by}} considering both compression performance and transmission cost.|$|E
40|$|Fault Tolerant Sublithographic Design with Rollback Recovery Shrinking feature {{sizes and}} energy levels coupled with high clock rates and {{decreasing}} node capacitance lead {{us into a}} regime where transient errors in logic cannot be ignored. Consequently, several recent {{studies have focused on}} feed-forward <b>spatial</b> <b>redundancy</b> techniques to combat these high transient fault rates. To complement these studies, we analyze fine-grained rollback techniques and show that they can offer lower <b>spatial</b> <b>redundancy</b> factors with no significant impact on system performance for fault rates up to one fault per device per ten million cycles of operation (Pƒ = 10 - 7) in systems with 1012 susceptible devices. Further, we concretely demonstrate these claims on nanowire-based programmable logic arrays. Despite expensive rollback buffers and general-purpose, conservative analysis, we show the area overhead factor of our technique is roughly an order of magnitude lower than a gate level feed-forward redundancy scheme...|$|E
40|$|We present {{local and}} nonlocal {{algorithms}} for video denoising based on discrete regularization on graphs. The main difference between video and image denoising is the temporal redundancy in video sequences. Recent {{works in the}} literature showed that motion compensation is counter-productive for video denoising. Our algorithms do not require any motion estimation. In this paper, we consider a video sequence as a volume {{and not as a}} sequence of frames. Hence, we combine the contribution of temporal and <b>spatial</b> <b>redundancies</b> in order to obtain high quality results for videos. To enhance the denoising quality, we develop a nonlocal method that benefits from local and nonlocal regularities within the video. Experiments show that the nonlocal method outperforms the local one by preserving finer details at the expense of an increase in the computational effort. We propose an optimized method that is faster than the nonlocal approach, while producing equally attractive results. 1...|$|R
40|$|International Telemetering Conference Proceedings / October 10 - 12, 1972 / International Hotel, Los Angeles, CaliforniaThis paper {{concerns}} an information-preserving data technique {{which is}} applicable to multi-spectral imagery {{such as that}} obtained by earth-resources satellites. The requirements for data compression in such missions is discussed and a rationale is presented {{for the use of}} distortion-free information-preserving compression. The selected compression technique involves the use of the spectralspatial-delta-interleave (SSDI) algorithm, a form of DPCM, to eliminate gross spectral and <b>spatial</b> <b>redundancies.</b> This reduced data is then coded for transmission using either the Huffman or the Rice coding algorithms. The coding algorithms have been simulated using a portion of frame 3698 taken during the Apollo S 065 experiment and, the results are presented. A parametric study presents the compression achieved by the SSDI-Rice algorithm as a function of block size and split-pixel mode used. Implementation considerations are also given...|$|R
40|$|International audienceWe present {{local and}} nonlocal {{algorithms}} for video denoising based on discrete regularization on graphs. The main difference between video and image denoising is the temporal redundancy in video sequences. Recent {{works in the}} literature showed that motion compensation is counter-productive for video denoising. Our algorithms do not require any motion estimation. In this paper, we consider a video sequence as a volume {{and not as a}} sequence of frames. Hence, we combine the contribution of temporal and <b>spatial</b> <b>redundancies</b> in order to obtain high quality results for videos. To enhance the denoising quality, we develop a nonlocal method that benefits from local and nonlocal regularities within the video. Experiments show that the nonlocal method outperforms the local one by preserving finer details at the expense of an increase in the computational effort. We propose an optimized method that is faster than the nonlocal approach, while producing equally attractive results...|$|R
