28|45|Public
5|$|Messages, many in {{commonly}} used ciphers and encryption systems, {{have been found}} hidden in the video and its <b>sound</b> <b>spectrogram,</b> as well as images of tortured and mutilated people. Most of the messages have been decoded by participants in an ongoing Reddit thread, and the images sourced to notable murder investigations such as the Boston Strangler. They have been interpreted as implying a threat of bioterrorism against the United States, although {{it has also been}} speculated that the video is in reality a prank, a viral marketing stunt for an upcoming film or video game, or a student film.|$|E
25|$|The ANS {{synthesizer}} is a photoelectronic {{musical instrument}} created by Russian engineer Evgeny Murzin from 1937 to 1957. The technological {{basis of his}} invention was the method of graphical sound recording used in cinematography (developed in Russia concurrently with USA), which {{made it possible to}} obtain a visible image of a sound wave, as well as to realize the opposite goalsynthesizing a sound from an artificially drawn <b>sound</b> <b>spectrogram.</b>|$|E
500|$|Most {{messages}} had {{a generally}} threatening tone. A <b>sound</b> <b>spectrogram</b> of the DVD's menu yielded {{a picture of}} a skull and more coded messages. The binary title of AETBX's YouTube posting was [...] "Muerte", Spanish for [...] "death", and the description similarly resolved to Spanish text—"Te queda 1 año menos", rendered in English as [...] "you have one less year". The triangle-and-square message {{near the end of the}} video was found to read [...] "Ad oppugnare homines" [...] in Pigpen cipher—Latin for [...] "To attack or target men".|$|E
40|$|Abstract. The {{voice is}} a kind of {{composite}} wave, timbre is different depending on the different overtones. The timbre, namely audio overtones or harmonics components, is determined by the number of the overtone. The pitch {{is one of the most}} important parameters of speech. In the process of collecting phonetic evidence, we always find out the conditions of sampling rate are different, especially when the emotional state of speaker is unstable, we would use comparison of <b>sound</b> <b>spectrograms</b> and statistics to explore what influence has the change of the pitch had on <b>sound</b> <b>spectrograms</b> and the measurement of formants. The final conclusion of the study will have positive effect on the job of phonetic identification...|$|R
5000|$|... #Caption: Above: a sound A waveformBelow: a <b>sound</b> A <b>spectrogram</b> ...|$|R
40|$|Spectrographic {{representations}} of speech, that is, <b>sound</b> <b>spectrograms</b> or simply spectrograms, {{are widely used}} in speech-related fields [1]. A speech signal consists of multiple frequency components dynamically changing in time. Therefore, a speech signal is often represented in the time-frequency plane. In the 1940 ’s, the spectrograph was invented to print spectrograms [1]. Today, we can easil...|$|R
50|$|The ANS {{synthesizer}} is a photoelectronic {{musical instrument}} created by Russian engineer Evgeny Murzin from 1937 to 1957. The technological {{basis of his}} invention was the method of graphical sound recording used in cinematography (developed in Russia concurrently with USA), which {{made it possible to}} obtain a visible image of a sound wave, as well as to realize the opposite goal - synthesizing a sound from an artificially drawn <b>sound</b> <b>spectrogram.</b>|$|E
5000|$|Most {{messages}} had {{a generally}} threatening tone. A <b>sound</b> <b>spectrogram</b> of the DVD's menu yielded {{a picture of}} a skull and more coded messages. The binary title of AETBX's YouTube posting was [...] "Muerte", Spanish for [...] "death", and the description similarly resolved to Spanish text—"Te queda 1 año menos", rendered in English as [...] "you have one less year." [...] The triangle-and-square message {{near the end of the}} video was found to read [...] "Ad oppugnare homines" [...] in Pigpen cipher—Latin for [...] "To attack or target men." ...|$|E
50|$|Messages, many in {{commonly}} used ciphers and encryption systems, {{have been found}} hidden in the video and its <b>sound</b> <b>spectrogram,</b> as well as disturbing images of tortured and mutilated women. Most of the messages have been decoded by participants in an ongoing Reddit thread, and the images sourced to notable murder investigations such as the Boston Strangler. They have been interpreted as implying a threat of bioterrorism against the United States, although {{it has also been}} speculated that the video is in reality a prank, a viral marketing stunt for an upcoming film or video game, or a student film.|$|E
40|$|FIGURE 4. Oscillogram (a), <b>sound</b> <b>spectrograms</b> (b) and {{spectrum}} display (c) for a call of U. micra sp. nov. (WAM R 168042). The bar in (a) and (b) represents 20 ms. In (b) we manipulated contrast, brightness and spectrum window display to maximise the loudest frequency components over pulse structure and side-bands arising from pulse rate (see Gerhardt & Huber, 2002) ...|$|R
40|$|Time-frequency {{representations}} of audio signals often resemble texture images. This paper derives a simple audio classification algorithm based on treating <b>sound</b> <b>spectrograms</b> as texture images. The algorithm {{is inspired by}} an earlier visual classification scheme particularly efficient at classifying textures. While solely based on time-frequency texture features, the algorithm achieves surprisingly good performance in musical instrument classification experiments. Index Terms — Audio classification, visual, time-frequency representation, texture...|$|R
5000|$|From 1972 to 1974 he {{also was}} a {{consultant}} to Scully/Metrotech in Mountain View, California, and to MCI in Ft. Lauderdale, Florida, on audio systems and magnetic recording. In 1973 and 1974 {{he was a member of}} Judge Sirica's [...] "Advisory Panel on White House Tapes" [...] (“The Watergate Tapes”), and in 1977...79 a member of the Committee on Evaluation of <b>Sound</b> <b>Spectrograms</b> of the United States National Academy of Sciences.|$|R
50|$|A single {{echolocation}} call (a call being {{a single}} continuous trace on a <b>sound</b> <b>spectrogram,</b> {{and a series}} of calls comprising a sequence or pass) can last anywhere from 0.2 to 100 milliseconds in duration, depending on the stage of prey-catching behavior that the bat is engaged in. For example, the duration of a call usually decreases when the bat is {{in the final stages of}} prey capture - this enables the bat to call more rapidly without overlap of call and echo. Reducing duration comes at the cost of having less total sound available for reflecting off objects and being heard by the bat.|$|E
5000|$|And {{what about}} the very {{building}} blocks of the language we use to name categories: Are our speech-sounds —/ba/, /da/, /ga/ —innate or learned? The first question we must answer about them is whether they are categorical categories at all, or merely arbitrary points along a continuum. It turns out that if one analyzes the <b>sound</b> <b>spectrogram</b> of ba and pa, for example, both are found to lie along an acoustic continuum called [...] "voice-onset-time." [...] With a technique {{similar to the one}} used in [...] "morphing" [...] visual images continuously into one another, it is possible to [...] "morph" [...] a /ba/ gradually into a /pa/ and beyond by gradually increasing the voicing parameter.|$|E
5000|$|... #Caption: A video {{example of}} an {{electric}} courtship signal in an African weakly electric fish, Paramormyrops sp. recorded in Gabon. This audio recording was made from a male courting a female. The original recording is of the electric signal but here it is converted into sound by the loud speaker. The upper trace shows an oscillogram of the original waveform, the lower figure is a <b>sound</b> <b>spectrogram</b> of the same recording. A photo of the species {{is shown in the}} inset panel. The male produces [...] "rasps", which are bursts of high frequency discharges, as his coursthip calling song. The males electric organ discharge waveform is longer in duration than the female's and thus the sound has a lower pitch. (Data from Hopkins and Bass, 1981) ...|$|E
40|$|FIGURE 4. <b>Sound</b> <b>spectrograms</b> showing {{advertisement}} call {{structure of}} H. litoralis. The same call analyzed by FlatTop (A) and Hamming window (B). Abscissa: time in s. Ordinate: frequency in kHz. The continuous broad band at about 2. 5 kHz is chirps of an insect. The call of several male individuals of H. litoralis was recorded from Ukhia, Cox's Bazar district on 19 June, 2011 just after sunset and their voucher specimen numbers {{are involved in}} those of molecular analyses, but not specified...|$|R
40|$|Two {{types of}} vocalizations of White Carneaux pigeons were {{identified}} in experiments on schedule-induced aggression and were given pictorial representation in <b>sound</b> <b>spectrograms.</b> Characteristics of these vocalizations are examined {{in the context of}} previous descriptions of vocalizations of several varieties of pigeons during aggressive and sexual encounters in naturalistic settings. These earlier descriptions portrayed vocalizations with mnemonic phrases and the symbolds of dictionary pronunciationmdata are presented that indicate that the analysis of social encounters between pigeons during schedule-induced aggression may be aided by employing these and other vocalizations as dependent variables...|$|R
40|$|Abstract—A new {{abstract}} model of computation, the computational manifold, {{provides a framework}} for approaching the problems of consciousness, awareness, cognition and symbolic processing in the central nervous system. Physical properties involving space, time and frequency, such as {{the surface of the}} skin, <b>sound</b> <b>spectrograms,</b> visual images, and the muscle cross-sections are included in the state of manifold automata. The Brodmann areas are modeled as recurrent image associations implemented as neurotransmitter fields. Symbols are defined by state transition behavior near reciprocal-image attractors in dynamical systems. Control masks overlay the images and regulate awareness of the environment and cognitive reflection. Index Terms—natural intelligence, consciousness, perception, cognition, models of computation...|$|R
40|$|We {{have been}} {{developing}} a real time speech-displaying system called “KanNon ” which helps deaf person to understand speaker’s speech contents. We designed the KanNon system {{to display a}} <b>sound</b> <b>spectrogram,</b> pitch frequency and loudness of speech as well as characters by speech-recognition system as real-time scrolling image. For the purpose of displaying formant patterns clearly with high accuracy, we applied Burg method combining with the minimum cross-entropy (Burg-MCE) method, and human auditory characteristics such as an equal loudness preemphasis and mel-scale frequency to the <b>sound</b> <b>spectrogram.</b> Finally, we show more effective display for the spectrogram reading in the KanNon system. 2. THE KANNON SYSTEM We show the interface of the KanNon system in Fig...|$|E
40|$|Figure 6 - Oscillogram, <b>sound</b> <b>spectrogram</b> (first column), {{and power}} {{spectrum}} (second column) of advertisement calls of Tepuihyla shushupe sp. n. (A) and Tepuihyla tuberculosa comb. n. (B). Note differences in call length {{and number of}} notes (first column). Both species have the dominant frequency (arrows) in the first harmonic but in Tepuihyla shushupe sp. n. the second harmonic has similar energy content (asterisk), while in Tepuihyla tuberculosa comb. n. it has much less...|$|E
40|$|FIGURE 8. Comparison of call {{structures}} in Litoria aurifera sp. nov. and Litoria meiriana. A = L. aurifera: 1) advertisement call with following squeak 2) grind with following squeak. B = L. meiriana, advertisement call with following squeak. In all cases the upper trace is an oscillogram {{and the lower}} trace a <b>sound</b> <b>spectrogram.</b> Spectrogram traces have been optimised to illustrate frequency bands and modulation {{and do not necessarily}} reflect analytical settings reported in the text. Bar represents 0. 1 seconds...|$|E
40|$|Abstract- The {{purpose of}} this study is to compare the {{duration}} characteristic of individual words and entire Passage in the speech of adults who stutter (S= 10) recorded near the onset of their stuttering to those of controlled nonstuttering adults (C= 10). Stuttered speech was identified in digital recordings of the clients read speech. The digitized signals were analyzed by means of Cool Edit Pro software. Using visual displays of <b>sound</b> <b>spectrograms,</b> the durations of individual words (including repeated words) in the passage and entire passage duration were analyzed. In this work 80 % of data were used for training and remaining 20 % for testing over all accuracy...|$|R
40|$|Abstract There are {{relatively}} few quantitative descriptive {{studies of the}} vocalisations and vocal behaviour of tropical bird species, {{in spite of the}} tropic’s rich avian biodiversity and the extensive variety of vocalisations produced by tropical birds. This lack of information inhibits our under-standing of tropical animals, including our ability to perform comparative analyses on vocal behaviours from an evolu-tionary perspective. In this study, we present the first quan-titative description of the vocal repertoire and daily vocal activity of White-eared Ground-sparrows (Melozone leuco-tis), using focal and autonomous recordings collected during two consecutive breeding seasons in Costa Rica. We clas-sified vocalisations into categories based on their visual appearance on <b>sound</b> <b>spectrograms</b> to create a library of vocalisations for this species. We found that White-eare...|$|R
40|$|The sharp-nosed reed frog is {{widespread}} in Africa. Although currently {{recognized as one}} species, suggestions have been made {{that more than one}} species might exist. We analysed 237 calls of 69 males from 19 localities in the western to southern parts of Africa. Calls fall into three groups, which we recognize as cryptic species. Of eight published <b>sound</b> <b>spectrograms,</b> all can be assigned to one of the three species. We recognize Hyperolius nasutus, distributed from western Africa to the Okavango Delta in Botswana; Hyperolius viridis, from the central highlands of northwestern Zambia to southern  Tanzania; and Hyperolius acuticeps which occurs from the Ivory Coast to the southeastern coast of South Africa. We assign published names to the synonymies of these three species. No call data are available for populations in the Congo basin. </strong...|$|R
40|$|This paper {{presents}} a new method for sound synthesis. The method consists in mapping the histograms sequence of a cellular automata evolution onto a <b>sound</b> <b>spectrogram.</b> The {{data obtained from}} the histograms are {{in the form of}} sound spectral structures evolving in time in a natural fashion. The main problem of cellular automatas is the difficulty of control due to its unpredictability property. This mapping offers significant controllability characteristics which allow flexible processes for sound and instrument design. The sounds obtained with this mapping present natural behaviour and are capable of simulate acoustic instruments and other real sounds. 1...|$|E
40|$|In a {{prospective}} study {{we tested the}} hypothesis that a cry from an infant {{can be used as}} part of an instrument to measure pain. Ten healthy newly born infants were subjected to painful stimuli on four occasions during their first year of life. The sound of the crying was analysed with regard to duration. With the help of a <b>sound</b> <b>spectrogram,</b> the fundamental frequencies of the first five crying sounds were analysed. The number of crying sounds decreased with age. There was a considerable difference between the 10 children, and also between the different pricking occasions for the respective children. We conclude that if crying is to be used as part of an instrument for measuring pain, the child's age has to be taken into account...|$|E
40|$|Understanding {{the human}} ability to {{reliably}} process and decode speech {{across a wide}} range of acoustic conditions and speaker characteristics is a fundamental challenge for current theories of speech perception. Conventional speech representations such as the <b>sound</b> <b>spectrogram</b> emphasize many spectro-temporal details that are not directly germane to the linguistic information encoded in the speech signal and which consequently do not display the perceptual stability characteristic of human listeners. We propose a new representat?ohM format, the modulation spectrogram, that discards much of the spectro-temporal detail in the speech signal and instead focuses on the underlying, stable structure incorporated in the low-frequency portion of the modulation spectrum distributed across critical-band-like channels. We describe the representation and illustrate its stability with color-mapped displays and with results from automatic speech recognition experiments...|$|E
40|$|Abstract- We present both {{forward and}} {{backward}} adaptive speech coders that operate at 9. 6, 12, and 16 kb/s using inte-ger and fractional rate trees, weighted squared error distortion measures, the (M,L) tree search algorithm, and incremental path map symbol release. We introduce the concept of multitree source codes and illustrate how the multitree structure allows scalar quantizer-based codes and scalar adaptation rules {{to be used for}} fractional rate tree coding. With a frequency weighted distortion measure, the {{forward and backward}} adaptive multitree coders produce near toll quality speech at 16 kb/s, while the back-ward adaptive 9. 6 kb/s multitree coder substantially outperforms adaptive predictive coding and has an encoding delay less than 2 ms. Performance results are presented in terms of Unweighted and weighted signal-to-noise ratio and segmental signal-to-noise ratio, <b>sound</b> <b>spectrograms,</b> and subjective listening tests. I...|$|R
40|$|We {{propose a}} novel method for mapping <b>sound</b> <b>spectrograms</b> onto images and thus {{enabling}} alignment between auditory and visual features for subsequent multimodal processing. We suggest a supervised learning {{approach to this}} audio-visual fusion problem, on the following grounds. Firstly, we use a Gaussian mixture of locally-linear regressions to learn a mapping from image locations to binaural spectrograms. Secondly, we derive a closed-form expression for the condi-tional posterior probability of an image location, given both an observed spectrogram, emitted from an unknown source direction, and the mapping parameters that were previously learnt. Prominently, the proposed method is {{able to deal with}} completely different spectrograms for training and for align-ment. While fixed-length wide-spectrum sounds are used for learning, thus fully and robustly estimating the regres-sion, variable-length sparse-spectrum sounds, e. g., speech, are used for alignment. The proposed method successfully extracts the image location of speech utterances in realistic reverberant-room scenarios. 1...|$|R
40|$|This chapter investigates {{concepts}} {{of space in}} French composer Gérard Grisey’s music. From the 1970 s onward, he used <b>sound</b> <b>spectrograms,</b> introducing the compositional technique of “spectralism,” which can be rooted in Arnold Schoenberg’s concept of Klangfarbe. The cycle Les Espaces acoustiques (1974 – 1985) uses this technique to create a sequence of musical forms that grow from the acoustic seed of a single tone. The cycle {{can be traced back}} to a new role for acoustic space, which emerged in early atonal composition. Grisey confronts the natural order of acoustic space with the human order of producing and perceiving sounds. The dis-symmetry between these two orders of magnitude is further explored in Grisey’s Le Noir de l’Étoile (1990) for six percussionists, magnetic tape, and real-time astrophysical signals. This piece unfolds a triadic constellation of spatial orders where human perception and performance are staged between musical micro-space and cosmic marco-space...|$|R
40|$|We {{developed}} a digital version of Pattern Playback to convert a spectrographic representation of speech {{back into a}} speech signal. Pattern Playback was originally developed by Cooper and his colleagues from Haskins Laboratories in the late 1940 s. We used our Digital Pattern Playback (DPP) for instruction in digital signal processing and speech science. The original DPP used two different algorithms: amplitude modulation and fast Fourier transform. The new DPP uses additive synthesis of sinusoidal harmonics, which is easier for undergraduate college students to understand. We also designed a scientific exhibition with DPP at a science museum for children and adults. DPP is educational {{for a wide variety}} of people, from children to technical students. Index Terms — digital pattern playback, education, speech science, <b>sound</b> <b>spectrogram,</b> voice-print puzzl...|$|E
40|$|In this paper, {{we propose}} a robust {{environmental}} <b>sound</b> <b>spectrogram</b> classification approach. Its purpose is surveillance and security applications {{based on the}} reassignment method and log-Gabor filters. Besides, the reassignment method {{is applied to the}} spectrogram to improve the readability of the time-frequency representation, and to assure a better localization of the signal components. Our approach includes three methods. In the first two methods, the reassigned spectrograms are passed through appropriate log-Gabor filter banks and the outputs are averaged and underwent an optimal feature selection procedure based on a mutual information criterion. The third method uses the same steps but applied only to three patches extracted from each reassigned spectrogram. The proposed approach is tested on a large database consists of 1000 sounds belonging to ten classes. The recognition is based on Multiclass Support Vector Machines...|$|E
40|$|An {{important}} aid in analysis & {{display of}} speech is <b>sound</b> <b>spectrogram.</b> It represents time-frequency-intensity display of short time spectrum. The quality of speech {{can be studied}} by visual inspection of spectrogram. This {{is one of the}} important applications of spectrogram in speech processing especially in speech enhancement. Another application of spectrogram is in isolating voiced and unvoiced regions. But to conclude from visual inspection the clarity of spectrogram is also important. Before plotting the spectrogram the time domain speech signal is converted to frequency domain. The transform domain used plays vital role in resolution of spectrogram. Generally Fast Fourier Transform is used to convert the time domain signal into frequency domain signal. This paper discusses the effect of using different transform for converting the time domain speech signal into frequency domain before plotting spectrogram [...] It is observed that resolution of speech spectrogram is transform dependent...|$|E
40|$|We {{study the}} {{extraction}} of non-linear components from spectrogram data of natural sounds. Such data is processed by the primary auditory cor-tex after preprocessing by the cochlea. The elementary components of log-spectrograms combine according to a point-wise maximum [1]. We therefore apply with Maximal Causes Analysis (MCA) a component extraction algorithm that assumes a maximum non-linearity {{in the place where}} most other algo-rithms assume linear superposition. Conclusion We find that Maximal Causes Analysis successfully extracts the elementary components of <b>sound</b> <b>spectrograms.</b> This suggests that it might be better suited to model component extraction in auditory cortex than linear approaches such as ICA or sparse coding. ICA: Component Extraction for Linearly Combined Data Independent Component Analysis (ICA) is a state-of-the-art technique to decompose data-points into generating causes. ICA assumes that the observed data can be explained by linearly combin-ing generating fields [2]. ICA 0 50 100 150 200 250 Tim e / m s 0. ...|$|R
40|$|A manual {{technique}} for rapidly and accurately scoring <b>sound</b> <b>spectrograms</b> is presented. The method allows for objective phonetic description and evaluation {{without the aid}} of either a highly-trained scorer or complex processing equipment. Empirical evidence using the technique is presented. Investigators in various fields have been attempting to develop fast, efficient, mechanical, and economical means of analysing sounds, particularly speech sounds, for over a century. In her 1929 review, McCarthy described some of the early attempts to record speech by means other than transcription by an observer. The earliest device she described was the phonautograph, invented by Scott in 1859, which recorded the sound wave by means of a stylus attached to a diaphragm which in turn was connected to a recording system. The manometric flame was invented by Koenig in 1862. It utilized a gas flame which sound waves disturbed; the disturbances were then photographically recorded. In 1893 Blondel devised the oscillograph which, with improvements, could give a near-perfect representation of the sound wave. It became apparent, however, that the wave-form, even well-recorded, was too complex to analyse in ways useful t...|$|R
40|$|The {{purpose of}} this study is to compare the {{duration}} characteristic of sound repetitions in the speech of adults who stutter (S= 10) recorded near the onset of their stuttering to those of controlled nonstuttering adults (C= 10). Dysfluent episodes are identified in digital recordings of the clients read speech. The digitized signals are analyzed by means of Cool Edit Pro software. Using visual displays of <b>sound</b> <b>spectrograms,</b> the durations of the spoken repetition units, the silent intervals between the units and the total dysfluency are measured. The stutterers exhibit shorter silent intervals between spoken repetitions units, which is used as one of the parameter for objective assessment of early stuttering. The total duration of the stutterer’s dysfluency is significantly shorter because of their shorter silent intervals when compared to dysfluency of equal repetition units produced by control subjects. Analysis reveal that silent interval duration is capable of differentiating stuttering from control client with 77. 4 - 95 % accuracy. In this work 80 % of data are used for training and remaining 20 % for testing...|$|R
