2|16|Public
5000|$|In 2007 a {{batch of}} Medion laptops sold through the Aldi {{supermarket}} chain appeared to be infected with Angelina. [...] A Medion press release explained that the virus was not really present, rather, it was a <b>spurious</b> <b>warning</b> caused by a bug in the pre-installed antivirus software, Bullguard. A patch was released to fix the error.|$|E
40|$|Adegenet version 2. 0. 1 This version {{contains}} {{a few new}} features, speedups, and bug fixes NEW FEATURES Hs() is now much faster and will scale better for large number of loci Hs. test() allows for testing differences in expected heterozygosity between two groups of individuals strata tutorial is now available via adegenetTutorial(which="strata") repool can now handle objects with no population information, and can return a list of repooled genind (argument 'list' defauling to FALSE) read. fstat() can now handle missing data coded by any number of " 0 " scatter. dapc() function can now label individuals using orditorp() function from vegan package, which takes care of overlapping (See issue # 100) df 2 genind() is now faster at handling missing data (See issue # 114) the summary method has better formatting BUG FIXES Hs() no longer returns NaN if one locus has not been typed; instead, computations are done using all available loci adegenetTutorial() now opens up-to-date tutorials subsetting genlight objects now treats missing data appropriately when given logical, character, or negative subscripts. (See issue # 83) fixed occasional <b>spurious</b> <b>warning</b> about duplicated individual labels in df 2 genind() fixed issues with mis-placed missing data in df 2 genind() when samples were labeled with numbers (See issue # 96) frequencies can now be obtained from P/A genpop objects (See issue # 105) Windows no longer throws an error with as. genlight() (See issue # 109) read. genpop() now returns individual names (See issue # 117...|$|E
40|$|This paper {{describes}} {{the design and}} implementation of ABash, a tool for statically analyzing programs written in the bash scripting language. Although it makes no formal guarantees against missed errors or <b>spurious</b> <b>warnings</b> (largely due to the highly dynamic nature of bash scripts), ABash is useful for detecting certain common program errors {{that may lead to}} security vulnerabilities. In experiments with 49 bash scripts taken from popular Internet repositories, ABash was able to identify 20 of them as containing bugs of varying severity while yielding only a reasonable number of <b>spurious</b> <b>warnings</b> on both these scripts and the generally bug-free initialization scripts of the Ubuntu Linux distribution. ABash works by performing abstract interpretation of a bash script via an abstract semantics that accounts for shell variable expansion. The analysis is also parameterized by a collection of signatures that describe external program interfaces (for Unix commands, etc.), yielding an easily configurable and extensible framework for finding bugs in bash scripts...|$|R
40|$|We {{present an}} {{automatic}} error-detection approach that combines static checking and concrete test-case generation. Our approach consists {{of taking the}} abstract error conditions inferred using theorem proving techniques by a static checker (ESC/Java), deriving specific error conditions using a constraint solver, and producing concrete test cases (with the JCrasher tool) that are executed to determine whether an error truly exists. The combined technique has advantages over both static checking and automatic testing individually. Compared to ESC/Java, we eliminate <b>spurious</b> <b>warnings</b> and improve the ease-of-comprehension of error reports through the production of Java counterexamples. Compared to JCrasher, we eliminate the blind search of the input space, thus reducing the testing time and increasing the test quality. 1...|$|R
40|$|Attempts {{by governments}} to stop bubbles by issuing {{warnings}} seem unsuccessful. This paper examines {{the effects of}} public warnings using a simple model of riding bubbles. We show that public warnings against a bubble can stop it if investors believe that a warning is issued in a definite range of periods commencing around the starting period of the bubble. If a warning involves {{the possibility of being}} issued too early, regardless of the starting period of the bubble, it cannot stop the bubble immediately. Bubble duration can be shortened by a premature public warning, but lengthened if it is late. Our model suggests that governments need to lower the probability of <b>spurious</b> <b>warnings...</b>|$|R
40|$|Abstract. Multithreaded {{programs}} {{are prone to}} errors caused by unintended interference between concurrent threads. This paper focuses on verifying that deterministically-parallel code is free of such thread interference errors. Deterministically-parallel code may create and use new threads, via fork and join, and coordinate their behavior with synchronization primitives, such as barriers and semaphores. Such code does not satisfy the traditional non-interference property of atomicity (or serializability), however, and so existing atomicity tools are inadequate for checking deterministically-parallel code. We introduce a new non-interference specification for deterministically-parallel code, and we present a dynamic analysis to enforce it. We also describe SingleTrack, a prototype implementation of this analysis. SingleTrack’s performance is competitive with prior atomicity checkers, but it produces many fewer <b>spurious</b> <b>warnings</b> because it enforces a more general noninterference property that is applicable to more software. ...|$|R
40|$|Abstract—Mining {{specifications}} {{and using}} them for bug detection is a promising way to reveal bugs in programs. Existing approaches suffer from two problems. First, dynamic specification miners require input that drives a program to generate common usage patterns. Second, existing approaches report false positives, that is, <b>spurious</b> <b>warnings</b> that mislead developers and reduce the practicability of the approach. We present a novel technique for dynamically mining and checking specifications without relying on existing input to drive a program and without reporting false positives. Our technique leverages automatically generated tests in two ways: Passing tests drive the program during specification mining, and failing test executions are checked against the mined specifications. The output are warnings that show with concrete test cases how the program violates commonly accepted specifications. Our implementation reports no false positives and 54 true positives in ten well-tested Java programs. Keywords-Bug detection; Specification mining; False positives I...|$|R
50|$|The IC4 train {{has become}} a {{contentious}} issue in Danish politics, {{mainly because of the}} long delays in AnsaldoBreda’s delivery of the trains. The Danish Minister of Traffic and Energy is routinely required to submit progress updates to parliament, and DSB's choice of a heavily customised train is often criticised as being the major reason for the delays. The expense involved in lengthening the platforms of several stations along the IC4 routes is the source of much additional criticism. In November 2006, it was revealed that the trains appeared to be working properly, but that the formal documentation requirements of the safety authorities were preventing the train from entering service fully. By the end of October 2007, four IC4 units had entered regional service in Jutland. However, service was suspended at the end of February 2008 because of problems with exhaust fumes. There have been many faults that did not really prevent the train from being used, but were still unacceptable, such as <b>spurious</b> <b>warnings</b> and various other computer-related problems.|$|R
40|$|Multithreaded shared-memory {{programs}} {{are susceptible to}} dataraces, bugs that may exhibit themselves only in rare circumstances and can have detrimental effects on program behavior. Dataraces are often difficult to debug because they are difficult to reproduce and can affect program behavior in subtle ways, so tools which aid in detecting and preventing dataraces can be invaluable. Past dynamic datarace detection tools either incurred large overhead, ranging from 3 x to 30 x, or sacrificed precision in reducing overhead, reporting many false errors. This thesis presents a novel approach to efficient and precise datarace detection for multithreaded object-oriented programs. Our runtime datarace detector incurs an overhead ranging from 13 % to 42 % for our test suite, well below the overheads reported in previous work. Furthermore, our precise approach reveals dangerous dataraces in real programs with few <b>spurious</b> <b>warnings.</b> by Manu Sridharan. Thesis (M. Eng.) [...] Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2002. Includes bibliographical references (p. 63 - 66). This electronic version was submitted by the student author. The certified thesis {{is available in the}} Institute Archives and Special Collections...|$|R
2500|$|Teething {{problems}} in service {{have so far}} included three areas: The onboard Maintenance, Repair, Overhaul (MRO) network needed software improvements; Airbus issued service bulletins regarding onboard equipment and removed galley inserts (coffee makers, toaster ovens) because of leaks; and has had to address <b>spurious</b> overheating <b>warnings</b> in the bleed air system by retrofitting an original connector with a gold-plated connector. [...] Airbus targets a 98.5% dependability {{by the end of}} 2016 and to match the mature A330 reliability by early 2019.|$|R
40|$|Abstract. Precise dynamic race {{detectors}} {{report an}} error if {{and only if}} an observed program trace exhibits a data race. They must typically check for races on all memory accesses to ensure that they catch all races and generate no <b>spurious</b> <b>warnings.</b> However, a race check for a particular memory access is guaranteed to be redundant if the accessing thread has already accessed that location within the same release-free span. A release-free span is any sequence of instructions containing no lock releases or other “release-like ” synchronization operations, such as notify or fork. We present a static analysis to identify redundant race checks by reasoning about memory accesses within release-free spans. In contrast to prior whole program analyses for identifying accesses that are always race-free, our redundant check analysis is span-local and can also be made methodlocal without any major loss in effectiveness. RedCard, our prototype implementation for the Java language, enables dynamic race detectors {{to reduce the number of}} run-time checks by close to 40 % with no loss in precision. We also present a complementary shadow proxy analysis for identifying when multiple memory locations can be treated as a single location by a dynamic race detector, again with no loss in precision. Combined, our analyses reduce the number of memory accesses requiring checks by roughly 50 %. ...|$|R
40|$|Abstract:- Mining {{specifications}} {{and using}} them for bug detection is a promising way to reveal bugs in programs. Existing approaches suffer from two problems. First, dynamic specification miners require input that drives a program to generate common usage patterns. Second, existing approaches report false positives, that is, <b>spurious</b> <b>warnings</b> that mislead developers and reduce the practicability of the approach. The time spent in testing is mainly concerned with generating the test cases and testing them. The goal {{of this paper is}} to reduce the time spent in testing by reducing the number of test cases. For this data mining techniques are incorporated to reduce the number of test cases. Data mining finds similar patterns in test cases which helped in finding out redundancy incorporated by automatic generated test cases. The final test suite is tested for coverage which yielded good results. Specification mining not only helps to automate coverage- driven simulation or formal verification, it can also provide useful information for diagnosis. A specification mining-based diagnosis framework is proposed that can be used to simultaneously understand the error and locate it. If not enough tests are available, the resulting specification may be too incomplete to be useful. To solve this problem Code coverage analysis is the process of finding areas of a program not exercised by a set of test cases, creating additional test cases to increase coverage and determining a quantitative measure of code coverage, which is an indirect measure of quality...|$|R
5000|$|Post-crash {{examination}} of the engines found no pre-crash damage or signs of in-flight fire [...] - [...] the fire warnings were false. The fire warning system was almost completely destroyed by the crash and fire {{and the reason for}} the false warnings could not be determined; although there had been nine reported instances of bleed air leaks causing <b>spurious</b> engine fire <b>warnings</b> on Il-62s between 1975 and the date of the crash, this was ruled out as a cause.|$|R
40|$|While {{concurrent}} programming is quickly gaining popularity lately, developing bug-free programs is still challenging. Although developers {{have a wide}} choice of race detection tools available, {{we have found that}} the majority of these techniques do not scale well and developers are often forced to balance precision with speed. Additionally, various practical issues force even precise race detectors to produce <b>spurious</b> <b>warnings,</b> defeating their purpose and burdening their users. We design and implement a novel race detection technique that is both fast and precise, {{even in the face of}} missing program source information. Towards this goal, we have developed two separate tools, TREE and RDIT, that respectively improve performance and precision over existing techniques. TREE, implemented in the RoadRunner framework, acts as a filter and sends through only those events that might add value to race detection while eliminating those events which are deemed redundant for this purpose. All the while, removing these redundant events does not affect its race detection capability. We have evaluated TREE against a whole set of standard benchmarks, including two large real-world applications. We have found that there exists a significant number of redundant events in all these applications and on an average, TREE saves somewhere between 15 - 25 % of analysis time as compared to the state-of-the-art techniques. Meanwhile, our next tool, RDIT, is able to precisely detect races in programs with incomplete source information, generating no false positives. RDIT is also maximal in the sense that it detects a maximal set of true races from the observed incomplete trace. It is underpinned by a sound BarrierPair model that abstracts away the missing events by capturing the invocation data of their enclosing methods. By making the least conservative assumption that a missing method introduces synchronization only when its invocation data overlaps with other missing methods, and by formulating maximal thread causality as a set of logical constraints, RDIT guarantees to precisely detect races with maximal capability. We tested RDIT against seven real-world large concurrent systems and have detected dozens of true races with zero false alarm. Comparatively, existing algorithms such as Happens-Before, Causal-Precede, and Maximal-Causality, which are all known to be precise, were observed reporting hundreds of false alarms due to trace incompleteness...|$|R
40|$|Static {{analysis}} tools report software defects {{that may or}} may not be detected by other verification methods. Two challenges complicat-ing the adoption of these tools are <b>spurious</b> false positive <b>warnings</b> and legitimate warnings that are not acted on. This paper reports automated support to help address these challenges using logistic regression models that predict the foregoing types of warnings from signals in the warnings and implicated code. Because examining many potential signaling factors in large software development set-tings can be expensive, we use a screening methodology to quickly discard factors with low predictive power and cost-effectively build predictive models. Our empirical evaluation indicates that these models can achieve high accuracy in predicting accurate and ac-tionable static analysis warnings, and suggests that the models ar...|$|R
40|$|Static {{analysis}} tools search software looking for defects that may cause an application {{to deviate from}} its intended behavior. These include defects that compute incorrect values, cause runtime exceptions or crashes, expose applications to security vulnerabilities, or lead to performance degradation. In an ideal world, the analysis would precisely identify all possible defects. In reality, {{it is not always}} possible to infer the intent of a software component or code fragment, and static {{analysis tools}} sometimes output <b>spurious</b> <b>warnings</b> or miss important bugs. As a result, tool makers and researchers focus on developing heuristics and techniques to improve speed and accuracy. But, in practice, speed and accuracy are not sufficient to maximize the value received by software makers using static analysis. Software engineering teams need to make static analysis an effective part of their regular process. In this dissertation, I examine the ways static analysis is used in practice by commercial and open source users. I observe that effectiveness is hampered, not only by false warnings, but also by true defects that do not affect software behavior in practice. Indeed, mature production systems are often littered with true defects that do not prevent them from functioning, mostly correctly. To understand why this occurs, observe that developers inadvertently create both important and unimportant defects when they write software, but most quality assurance activities are directed at finding the important ones. By the time the system is mature, there may still be a few consequential defects that can be found by static analysis, but they are drowned out by the many true but low impact defects that were never fixed. An exception to this rule is certain classes of subtle security, performance, or concurrency defects that are hard to detect without static analysis. Software teams can use static analysis to find defects very early in the process, when they are cheapest to fix, and in so doing increase the effectiveness of later quality assurance activities. But this effort comes with costs that must be managed to ensure static analysis is worthwhile. The cost effectiveness of static analysis also depends {{on the nature of the}} defect being sought, the nature of the application, the infrastructure supporting tools, and the policies governing its use. Through this research, I interact with real users through surveys, interviews, lab studies, and community-wide reviews, to discover their perspectives and experiences, and to understand the costs and challenges incurred when adopting static analysis tools. I also analyze the defects found in real systems and make observations about which ones are fixed, why some seemingly serious defects persist, and what considerations static analysis tools and software teams should make to increase effectiveness. Ultimately, my interaction with real users confirms that static analysis is well received and useful in practice, but the right environment is needed to maximize its return on investment...|$|R
40|$|Most drill wear {{monitoring}} {{research found}} in the literature is based on conventional vibration technologies. However, these conventional approaches still have not attracted real interest from manufacturers for multiples of reasons: some of these techniques are not practical and use complicated Tool Condition Monitoring (TCM) systems with less value in industry. In addition, they are also prone to give <b>spurious</b> drill deterioration <b>warnings</b> in industrial environments. Therefore, drills are normally replaced at estimated preset intervals, sometimes long before they are worn or by expertise judgment. Two of the great problems in the implementation of these systems in drilling are: the poor signal-to-noise ratio and the lack of system-made sensors for drilling, as is prevalent in machining operations with straight edge cutters. In order to overcome the noise problems, many researchers recommend advanced and sophisticated signal processing while the work of Rehorn et al. (2005) advises the following possibilities to deal with the lack of commercial system-made sensors:  Some research should be directed towards developing some form of instrumented tool for drill operations.  Since the use of custom-made sensors is being ignored in drilling operations, effort should be focused on intelligent or innovative use of available sensor technology. It is expected that the latter could minimize implementation problems and allows an optimal drill utilization rate by means of modern and smart sensors. In addition to the accelerometer sensor commonly used in conventional methods, this work has considered two other sensor-based methods to monitor the drill wear indirectly. These methods entail the use of an instrumented drill with strain gauges to measure the torque and the use of an encoder to measure the Instantaneous Angular Speed (IAS). The signals from these sensors were analyzed using signal processing techniques such as, statistical parameters, Fast Fourier Transform (FFT), and a ii preliminary Time-Frequency (TF) analysis. A preliminary investigation has revealed that the use of a Regression Analysis (RA) based on a higher order polynomial function can very well follow and give prognosis of the development of the monitored parameters. The experimental investigation has revealed that all the above monitoring systems are sensitive to the deterioration of the drill condition. This work is however particularly concerned with the use of IAS on the spindle of the drill, compared to conventional monitoring systems for drill condition monitoring. This comparison reveals that the IAS approach can generate diagnostic information similar to vibration and torque measurements, without some of the instrumentation complications. This similitude seems to be logical, as {{it is well known that}} the increase of friction between the drill and workpiece due to wear increase the torque and consequently it should reduce or at least affect the spindle rotational speed. However, the use of a drill instrumented with a strain gauge is not practical, because of the inconvenience it causes on production machines. By contrast, the IAS could be measured quite easily by means of an encoder, a tachometer or some other smart rotational speed sensors. Thus, one could take advantage of advanced techniques in digital time interval analysis applied to a carrier signal from a multiple pulse per revolution encoder on the rotating shaft, to improve the analysis of chain pulses. As it will be shown in this dissertation, the encoder resolution does not sensibly affect the analysis. Therefore, one can easily replace encoders by any smart transducers that have become more popular in rotating machinery. Consequently, a non-contact transducer for example could effectively be used in on-line drill condition monitoring such as the use of lasers or time passage encoder-based systems. This work has gained from previous research performed in Tool Condition Monitoring TCM, and presents a sensor that is already available in the arsenal of sensors and could be an open door for a practical and reliable sensor in automated drilling. iii In conclusion, this dissertation strives to answer the following question: Which one of these methods could challenge the need from manufacturers by monitoring and diagnosing drill condition in a practical and reliable manner? Past research has sufficiently proved the weakness of conventional technologies in industry despite good results in the laboratory. In addition, delayed diagnosis due to time-consuming data processing is not beneficial for automated drilling, especially when the drill wears rapidly at the end of its life. No advanced signal processing is required for the proposed technique, as satisfactory results are obtained using common time domain signal processing methods. The recommended monitoring choice will definitely depend on the sensor that is practical and reliable in industry. Dissertation (MEng) [...] University of Pretoria, 2013. gm 2013 Mechanical and Aeronautical EngineeringMEngUnrestricte...|$|R

