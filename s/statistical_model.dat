10000|10000|Public
5|$|In statistics, maximum spacing {{estimation}} (MSE or MSP), or maximum {{product of}} spacing estimation (MPS), {{is a method}} for estimating the parameters of a univariate <b>statistical</b> <b>model.</b> The method requires maximization of the geometric mean of spacings in the data, which are {{the differences between the}} values of the cumulative distribution function at neighbouring data points.|$|E
5|$|In 1924, Einstein {{received}} {{a description of}} a <b>statistical</b> <b>model</b> from Indian physicist Satyendra Nath Bose, based on a counting method that assumed that light could be understood as a gas of indistinguishable particles. Einstein noted that Bose's statistics applied to some atoms {{as well as to the}} proposed light particles, and submitted his translation of Bose's paper to the Zeitschrift für Physik. Einstein also published his own articles describing the model and its implications, among them the Bose–Einstein condensate phenomenon that some particulates should appear at very low temperatures. It was not until 1995 that the first such condensate was produced experimentally by Eric Allin Cornell and Carl Wieman using ultra-cooling equipment built at the NIST–JILA laboratory at the University of Colorado at Boulder. Bose–Einstein statistics are now used to describe the behaviors of any assembly of bosons. Einstein's sketches for this project may be seen in the Einstein Archive in the library of the Leiden University.|$|E
25|$|Researchers {{from the}} Washington University {{developed}} a <b>statistical</b> <b>model</b> to measure systematic {{bias in the}} behavior of Wikipedia's users regarding controversial topics. The authors focused on behavioral changes of the encyclopedia's administrators after assuming the post, writing that systematic bias occurred after the fact.|$|E
50|$|<b>Statistical</b> <b>Modelling</b> is a {{bimonthly}} peer-reviewed scientific journal covering <b>statistical</b> <b>modelling.</b> It {{is published}} by SAGE Publications {{on behalf of the}} <b>Statistical</b> <b>Modelling</b> Society. The editors-in-chief are Brian D. Marx (Louisiana State University), Jeffrey Simonoff (New York University), and Arnošt Komárek (Charles University in Prague).|$|R
40|$|This paper {{presents}} a novel automatic liver segmentation algorithm which combines <b>statistical</b> <b>models</b> with machine learning. In the approach, {{three kinds of}} <b>statistical</b> <b>models</b> are developed, including <b>statistical</b> pose <b>model</b> (SPM), <b>statistical</b> shape <b>model</b> (SSM), and <b>statistical</b> appearance <b>model</b> (SAM). The algorithm contains three major processes, including prior collecting, <b>statistical</b> <b>models</b> building, and shape detecting. In prior collecting, based on benchmark of liver segmentation, the prior information about the liver is collected, including its position, pose, shape, texture, and the statistical intensity distribution of surrounding area. To fully utilise the prior information for segmentation, the <b>statistical</b> <b>models</b> building will build a support vector machine (SVM) classifier and the three <b>statistical</b> <b>models.</b> The shape detecting process will model the segmentation {{as a process of}} model evolution to derive the liver shape. Experiment results of liver segmentation on CT images using the proposed method are presented with performance validation and discussion...|$|R
40|$|Sensor-based <b>statistical</b> <b>models</b> {{promise to}} support a variety of {{advances}} in human-computer interaction, but building applications that use them is currently difficult and potential advances go unexplored. We present Subtle, a toolkit that removes some of the obstacles to developing and deploying applications using sensor-based <b>statistical</b> <b>models</b> of human situations. Subtle provides an appropriate and extensible sensing library, continuous learning of personalized models, fully-automated high-level feature generation, and support for using learned models in deployed applications. By removing obstacles to developing and deploying sensor-based <b>statistical</b> <b>models,</b> Subtle {{makes it easier to}} explore the design space surrounding sensor-based <b>statistical</b> <b>models</b> of human situations. Subtle thus helps to move the focus of human-computer interaction research onto applications and datasets, instead of the difficulties of developing and deploying sensor-based <b>statistical</b> <b>models.</b> Author Keywords Toolkits, Subtle, sensor-based <b>statistical</b> <b>models,</b> machin...|$|R
25|$|The Ising {{model can}} be reinterpreted as a <b>statistical</b> <b>model</b> for the motion of atoms. Since the kinetic energy depends only on {{momentum}} and not on position, while the statistics of the positions only depends on the potential energy, the thermodynamics of the gas only depends on the potential energy for each configuration of atoms.|$|E
25|$|If {{data are}} {{represented}} by a <b>statistical</b> <b>model</b> specifying a particular family of probability distributions, then estimates of the median {{can be obtained by}} fitting that family of probability distributions to the data and calculating the theoretical median of the fitted distribution. Pareto interpolation is an application of this when the population is assumed to have a Pareto distribution.|$|E
25|$|Gene-set {{analysis}} (for example using tools like DAVID and GoSeq) {{has been}} shown to be severely biased when applied to high-throughput methylation data (e.g. genome-wide bisulfite sequencing); it has been suggested that this can be corrected using sample label permutations or using a <b>statistical</b> <b>model</b> to control for differences in the numberes of CpG probes / CpG sites that target each gene.|$|E
50|$|All {{statistical}} hypothesis tests and all statistical estimators {{are derived from}} <b>statistical</b> <b>models.</b> More generally, <b>statistical</b> <b>models</b> {{are part of the}} foundation of statistical inference.|$|R
40|$|Abstract. The {{limitations}} of traditional knowledge representation methods for modeling complex human behaviour {{led to the}} investigation of <b>statistical</b> <b>models.</b> Predictive <b>statistical</b> <b>models</b> enable the anticipation of certain aspects of human behaviour, such as goals, actions and preferences. In this paper, we motivate {{the development of these}} models {{in the context of the}} user modeling enterprise. We then review the two main approaches to predictive <b>statistical</b> <b>modeling,</b> content-based and collaborative, and discuss the main techniques used to develop predictive <b>statistical</b> <b>models.</b> We also consider the evaluation requirements of these models in the user modeling context, and propose topics for future research. 1...|$|R
40|$|Algebraic {{statistics}} is {{a recently}} evolving field, where one would treat <b>statistical</b> <b>models</b> as algebraic objects and thereby use tools from computational commutative algebra and algebraic geometry {{in the analysis}} and computation of <b>statistical</b> <b>models.</b> In this approach, calculation of parameters of <b>statistical</b> <b>models</b> amounts to solving set of polynomial equations in several variables, for which one can use celebrated Grobner bases theory. Owing to {{the important role of}} information theory in statistics, this paper as a first step, explores the possibility of describing maximum and minimum entropy (ME) models in the framework of algebraic statistics. We show that ME-models are toric models (a class of algebraic <b>statistical</b> <b>models)</b> when the constraint functions (that provide the information about the underlying random variable) are integer valued functions, and the set of <b>statistical</b> <b>models</b> that results from ME-methods are indeed an affine variety...|$|R
25|$|In statistics, maximum {{likelihood}} estimation (MLE) {{is a method}} of estimating the parameters of a <b>statistical</b> <b>model</b> given observations, by finding the parameter values that maximize the likelihood of making the observations given the parameters. MLE {{can be seen as}} a special case of the maximum a posteriori estimation (MAP) that assumes a uniform prior distribution of the parameters, or as a variant of the MAP that ignores the prior and which therefore is unregularized.|$|E
25|$|A Bayesian network, Bayes network, belief network, Bayes(ian) {{model or}} {{probabilistic}} directed acyclic graphical {{model is a}} probabilistic graphical model (a type of <b>statistical</b> <b>model)</b> that represents a set of random variables and their conditional dependencies via a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network {{can be used to}} compute the probabilities of the presence of various diseases.|$|E
25|$|Gene-set {{analysis}} (a.k.a. pathway analysis; usually performed {{tools such}} as DAVID, GoSeq or GSEA) {{has been shown to}} be severely biased when applied to high-throughput methylation data (e.g. MeDIP-seq, MeDIP-ChIP, HELP-seq etc.), and a wide range of studies have thus mistakenly reported hyper-methylation of genes related to development and differentiation; it has been suggested that this can be corrected using sample label permutations or using a <b>statistical</b> <b>model</b> to control for differences in the numberes of CpG probes / CpG sites that target each gene.|$|E
40|$|With {{the advent}} of massively {{parallel}} computer systems, scientists {{are now able to}} simulate complex phenomena (e. g., explosions of a stars). Such scientific simulations typically generate large-scale data sets over the spatio-temporal space. Unfortunately, the sheer sizes of the generated data sets make efficient exploration of them impossible. Constructing queriable <b>statistical</b> <b>models</b> is an essential step in helping scientists glean new insight from their computer simulations. We define queriable <b>statistical</b> <b>models</b> to be descriptive statistics that (1) summarize and describe the data within a user-defined modeling error, and (2) are able to answer complex range-based queries over the spatiotemporal dimensions. In this chapter, we describe systems that build queriable <b>statistical</b> <b>models</b> for large-scale scientific simulation data sets. In particular, we present our Ad-hoc Queries for Simulation (AQSim) infrastructure, which reduces the data storage requirements and query access times by (1) creating and storing queriable <b>statistical</b> <b>models</b> of the data at multiple resolutions, and (2) evaluating queries on these models of the data instead of the entire data set. Within AQSim, we focus on three simple but effective <b>statistical</b> <b>modeling</b> techniques. AQSim's first modeling technique (called univariate mean modeler) computes the ''true'' (unbiased) mean of systematic partitions of the data. AQSim's second <b>statistical</b> <b>modeling</b> technique (called univariate goodness-of-fit modeler) uses the Andersen-Darling goodness-of-fit method on systematic partitions of the data. Finally, AQSim's third <b>statistical</b> <b>modeling</b> technique (called multivariate clusterer) utilizes the cosine similarity measure to cluster the data into similar groups. Our experimental evaluations on several scientific simulation data sets illustrate the value of using these <b>statistical</b> <b>models</b> on large-scale simulation data sets...|$|R
40|$|Abstract. Algebraic {{statistics}} is {{a recently}} evolving field, where one would treat <b>statistical</b> <b>models</b> as algebraic objects and thereby use tools from computational commutative algebra and algebraic geometry {{in the analysis}} and computation of <b>statistical</b> <b>models.</b> In this approach, calculation of parameters of <b>statistical</b> <b>models</b> amounts to solving set of polynomial equations in several variables, for which one can use celebrated Gröbner bases theory. Owing to {{the important role of}} information theory in statistics, this paper as a first step, explores the possibility of describing maximum and minimum entropy (ME) models in the framework of algebraic statistics. We show that ME-models are toric models (a class of algebraic <b>statistical</b> <b>models)</b> when the constraint functions (that provide the information about the underlying random variable) are integer valued functions, and the set of <b>statistical</b> <b>models</b> that results from ME-methods are indeed an affine variety. 2 1...|$|R
40|$|Economic {{researches}} using <b>statistical</b> <b>modelling</b> {{methods have}} numerous {{challenges and opportunities}} in the waiting for the twenty-first century, calling for increasing numbers of non-traditional <b>statistical</b> approaches. <b>Statistical</b> <b>modelling</b> {{is one of the}} most widespread methods of research of economic systems. The selection of methods of modelling of the economic systems depends on a great number of conditions (modelling components) of the system being researched. The method of <b>statistical</b> <b>modelling</b> allows developing different scenarios of functioning of the investigated economic systems. <b>Statistical</b> <b>modelling</b> may be used for tackling a wide range of economic problems (design and analysis of industrial systems, stock management, balancing of production capacities, allocation of investment funds, optimization of investment funds, optimization of insurance system etc.). Modelling is frequently associated with the factor of uncertainty (or risk), who’s description goes outside the confines of the traditional <b>statistical</b> <b>modelling,</b> which, in its turn, complicates the modelling proces...|$|R
25|$|In statistics, linear {{least squares}} {{problems}} correspond to a particularly important type of <b>statistical</b> <b>model</b> called linear regression which arises as a {{particular form of}} regression analysis. One basic form of such a model is an ordinary least squares model. The present article concentrates on the mathematical aspects of linear least squares problems, with discussion of the formulation and interpretation of statistical regression models and statistical inferences related to these being {{dealt with in the}} articles just mentioned. See outline of regression analysis for an outline of the topic.|$|E
25|$|Although {{this tool}} for {{evaluating}} models against {{systems that are}} accessible experimentally may be applied in any field, its application to selecting a <b>statistical</b> <b>model</b> via Akaike information criterion are particularly well described in papers and a book by Burnham and Anderson. In a nutshell the Kullback–Leibler divergence of reality from a model may be estimated, to within a constant additive term, by a function (like the squares summed) of the deviations observed between data and the model's predictions. Estimates of such divergence for models that share the same additive term can in turn be used to select among models.|$|E
25|$|In 2011 {{researchers}} from UC Berkeley published a study reporting second-by-second reconstruction of videos watched by the study's subjects, from fMRI data. This {{was achieved by}} creating a <b>statistical</b> <b>model</b> relating visual patterns in videos shown to the subjects, to the brain activity caused by watching the videos. This model was then used to look up the 100 one-second video segments, in a database of 18 million seconds of random YouTube videos, whose visual patterns most closely matched the brain activity recorded when subjects watched a new video. These 100 one-second video extracts were then combined into a mashed-up image that resembled the video being watched.|$|E
40|$|Abstract. We have {{investigated}} {{a combination of}} <b>statistical</b> <b>modelling</b> and expectation maximisation for a texture based approach to the segmentation of mammographic images. Texture modelling {{is based on the}} implicit incorporation of spatial information through the introduction of a set-permutation-occurrence matrix. <b>Statistical</b> <b>modelling</b> is used for data generalisation and noise removal purposes. Expectation maximisation modelling of the spatial information in combination with the <b>statistical</b> <b>modelling</b> is evaluated. The developed segmentation results are used for automatic mammographic risk assessment. ...|$|R
40|$|The {{explosive}} growth of data, {{coupled with the}} emergence of powerful distributed computing platforms, is driving the need for high-performance <b>statistical</b> <b>modeling</b> software. SAS has developed a group of high-performance analytics procedures that perform <b>statistical</b> <b>modeling</b> and model selection by exploiting all the cores available—whether in a single machine or in a distributed computing environment. This paper describes the various execution modes and data access methods for high-performance analytics procedures. It also discusses the design principles for high-performance <b>statistical</b> <b>modeling</b> procedures and offers guidance about how and when these procedures provide performance benefits...|$|R
50|$|Yadrenko {{worked in}} various {{branches}} of applied probability theory, including optimal methods for quality control in mass production, <b>statistical</b> <b>modeling</b> of noises in semiconductors, {{statistical analysis of}} random number generators, statistical problems of reliability theory, and <b>statistical</b> <b>models</b> of distributions with random intensity.|$|R
25|$|Another {{motivation}} for using local alignments {{is that there}} is a reliable <b>statistical</b> <b>model</b> (developed by Karlin and Altschul) for optimal local alignments. The alignment of unrelated sequences tends to produce optimal local alignment scores which follow an extreme value distribution. This property allows programs to produce an expectation value for the optimal local alignment of two sequences, which is a measure of how often two unrelated sequences would produce an optimal local alignment whose score is greater than or equal to the observed score. Very low expectation values indicate that the two sequences in question might be homologous, meaning they might share a common ancestor.|$|E
25|$|The {{microscopic}} {{description in}} statistical mechanics {{is based on}} a model that analyzes a system into its fundamental particles of matter or into a set of classical or quantum-mechanical oscillators and considers the system as a statistical ensemble of microstates. As a collection of classical material particles, temperature {{is a measure of the}} mean energy of motion, called kinetic energy, of the particles, whether in solids, liquids, gases, or plasmas. The kinetic energy, a concept of classical mechanics, is half the mass of a particle times its speed squared. In this mechanical interpretation of thermal motion, the kinetic energies of material particles may reside in the velocity of the particles of their translational or vibrational motion or in the inertia of their rotational modes. In monatomic perfect gases and, approximately, in most gases, temperature is a measure of the mean particle kinetic energy. It also determines the probability distribution function of the energy. In condensed matter, and particularly in solids, this purely mechanical description is often less useful and the oscillator model provides a better description to account for quantum mechanical phenomena. Temperature determines the statistical occupation of the microstates of the ensemble. The microscopic definition of temperature is only meaningful in the thermodynamic limit, meaning for large ensembles of states or particles, to fulfill the requirements of the <b>statistical</b> <b>model.</b>|$|E
500|$|The first {{statistical}} guidance {{used by the}} National Hurricane Center was the Hurricane Analog Technique (HURRAN), {{which was}} available in 1969. [...] It used the newly developed North Atlantic tropical cyclone database to find storms with similar tracks. [...] It then shifted their tracks through the storm's current path, and used location, direction and speed of motion, and the date to find suitable analogs. [...] The method did well with storms south of the 25th parallel which had not yet turned northward, but poorly with systems near or after recurvature. [...] Since 1972, the Climatology and Persistence (CLIPER) <b>statistical</b> <b>model</b> {{has been used to}} help generate tropical cyclone track forecasts. [...] In the era of skillful dynamical forecasts, CLIPER is now being used as the baseline to show model and forecaster skill. [...] The Statistical Hurricane Intensity Forecast (SHIFOR) has been used since 1979 for tropical cyclone intensity forecasting. [...] It uses climatology and persistence to predict future intensity, including the current Julian day, current cyclone intensity, the cyclone's intensity 12 hours ago, the storm's initial latitude and longitude, as well as its zonal (east-west) and meridional (north-south) components of motion.|$|E
40|$|The use of <b>statistical</b> <b>modeling</b> in the {{estimation}} of Origin-Destination (OD) matrix from traffic counts is reviewed. In particular, <b>statistical</b> <b>models</b> that consider explicitly the presence of measurement and sampling errors in the observed link flows are discussed. This paper proposes treating the link choice proportions as random variables. Accordingly, new <b>statistical</b> <b>models</b> are formulated and the corresponding Maximum Likelihood Estimator and Bayesian Estimator of the OD matrix are developed. The accuracies of these estimators are compared with those obtained by previous methods. ...|$|R
40|$|<b>Statistical</b> <b>modeling</b> is {{essential}} to SAR (Synthetic Aperture Radar) image interpretation. It aims to describe SAR images through statistical methods and reveal the characteristics of these images. Moreover, <b>statistical</b> <b>modeling</b> can provide a technical support for a comprehensive understanding of terrain scattering mechanism, which helps to develop algorithms for effective image interpretation and creditable image simulation. Numerous <b>statistical</b> <b>models</b> {{have been developed to}} describe SAR image data, and {{the purpose of this paper}} is to categorize and evaluate these models. We first summarize the development history and the current researching state of <b>statistical</b> <b>modeling,</b> then different SAR image models developed from the product model are mainly discussed in detail. Relevant issues are also discussed. Several promising directions for future research are concluded at last...|$|R
40|$|For a wide {{class of}} <b>statistical</b> <b>models</b> with sigma-finite priors, {{sufficient}} conditions for consistency of Bayes procedures {{for almost all}} values of the parameter are given. For a more restrictive class of invariant <b>statistical</b> <b>models</b> with some sigma-finite priors, we give sufficient conditions for consistency of Bayes procedures for all values of the parameter. Particular cases of these invariant <b>statistical</b> <b>models</b> include the location model on a locally compact Polish group, rotation invariant families on the unit sphere in R-k, and the scale-location model in R-k...|$|R
2500|$|... is {{that the}} Ising model is useful for any model of neural function, because a <b>statistical</b> <b>model</b> for neural {{activity}} should be chosen using the principle of maximum entropy. Given a collection of neurons, a <b>statistical</b> <b>model</b> which can reproduce the average firing rate for each neuron introduces a Lagrange multiplier for each neuron: ...|$|E
2500|$|Statistical justification. [...] Without a <b>statistical</b> <b>model</b> {{underlying}} the method, its estimates {{do not have}} well-defined uncertainties.|$|E
2500|$|Using an algorithm, either {{based on}} a <b>statistical</b> <b>model</b> or some heuristics, to predict the {{likelihood}} of variation at each locus, based on the quality scores and allele counts of the aligned reads at that locus ...|$|E
40|$|The Institute of Education Sciences at the U. S. Department of Education {{contracted}} with Abt Associates {{to develop a}} report that produces empirical evidence on the differences in impact estimates and standard errors resulting from <b>statistical</b> <b>models</b> that use state assessments to measure student achievement, <b>statistical</b> <b>models</b> that use study-administered tests to measure student achievement, and <b>statistical</b> <b>models</b> that {{use a combination of}} these two types of tests. The views expressed in this report are those of the authors, and they do not necessaril...|$|R
40|$|Accurate <b>statistical</b> {{simulation}} and <b>modeling</b> {{are important}} for IC design. Different types of statistical simulation require different types of <b>statistical</b> <b>models.</b> In this paper a unified approach to <b>statistical</b> <b>modeling</b> and characterization is presented. Based on physical process parameters and propagation of variance, it allows modeling of process extremes, distributional modeling for Monte Carlo type simulation, and modeling of mismatch...|$|R
5000|$|Extreme climate events, <b>statistical</b> <b>modelling</b> {{of extreme}} {{precipitation}} ...|$|R
