2|50|Public
40|$|Multi-antenna {{transceivers}} with beam-forming are recently gaining {{interest for}} low GHz frequencies (< 6 GHz) [1 - 4]. In the antenna beam, (phase-shifted) signals from multiple antennas add constructively, improving SNR, while out-ofbeam signals add destructively (i. e. spatial filtering). Usually the <b>summation</b> <b>point</b> is behind some gain blocks, which then {{need to be}} capable of handling strong signals. To improve the input-referred compression point P 1 dB, a fully passive switched-capacitor approach was presented in [4], providing P 1 dB=+ 2 dBm, but at a high noise penalty: NF= 18 dB. Here we propose to sum immediately at the baseband capacitors of passive mixer-first switched-RC downconverters. We show that this can render a direction-dependent RF impedance (spatial filtering) together with RF bandpass frequency filtering at lower noise and higher P 1 dB...|$|E
40|$|This thesis {{deals with}} the timing error problem that appears in high {{frequency}} Digital to Analog Converters. Inequalities among signal paths in different branches and inaccuracies happened during fabrication, result in different time delays in different branches of a Digital to Analog Converter. The consequence of this inequality is having the data for different bits not arriving to the <b>summation</b> <b>point</b> at the same time. This timing error will create some glitches in the output analog signal. A new approach is introduced in this work that measures the timing error among branches of the DAC and corrects them through a calibration process. Being all the error measurement and its correction process done on chip, this approach can correct the errors created by both sources. This idea was implemented and tested in Eldo simulator. A timing error of 8 pS was inserted to the MSB branch of a 10 -bit binary coded DAC. After performing the calibration process on this DAC, the SFDR of the output signal was increased by about 3. 2 dB...|$|E
3000|$|Double slash primes {{over the}} <b>summation</b> symbols <b>point</b> {{to the absence}} of those summands, where n[*]=[*]l[*]=[*]m. Matrix elements, which are {{included}} to this operator, have the following definitions ε [...]...|$|R
5000|$|In his <b>summation,</b> Marsh <b>points</b> {{out that}} [...] "'East-West' {{can be heard}} as part of what sparked the West Coast's rock revolution, in which such song {{structures}} with extended improvisatory passages became commonplace." ...|$|R
5000|$|Note: Total points scored and {{championship}} positions attained {{have been}} adjusted to override <b>points</b> <b>summation</b> {{errors in the}} published results retrieved from www.gtchampionship.com.au ...|$|R
40|$|AbstractRecently, {{measurements}} of a considerable {{portion of the}} phase diagram for the quaternary system water–ethylene glycol–sucrose–NaCl were published (Han et al., 2010). In that article, the data were {{used to evaluate the}} accuracy of two non-ideal multi-solute solution theories: the Elliott et al. form of the multi-solute osmotic virial equation and the Kleinhans and Mazur freezing <b>point</b> <b>summation</b> model. Based on this evaluation, it was concluded that the freezing <b>point</b> <b>summation</b> model provides more accurate predictions for the water–ethylene glycol–sucrose–NaCl system than the multi-solute osmotic virial equation. However, this analysis suffered from a number of issues, notably including the use of inconsistent solute-specific coefficients for the multi-solute osmotic virial equation. Herein, we reanalyse the data using a recently-updated and consistent set of solute-specific coefficients (Zielinski et al., 2014). Our results indicate that the two models have very similar performance, and, in fact, the multi-solute osmotic virial equation can provide more accurate predictions than the freezing <b>point</b> <b>summation</b> model depending on the concentration units used...|$|R
30|$|A new {{discrete}} Hardy-type inequality with kernels and monotone functions is proved for {{the case}} 1 < q< p<∞. This result is discussed in a general framework and some applications related to Hölder’s <b>summation</b> method are <b>pointed</b> out.|$|R
40|$|Gaussian Quadrature is a {{well known}} {{technique}} for numerical integration. Recently Gaussian quadrature with respect to discrete measures corresponding to finite sums have found some new interest. In this paper we apply these ideas to infinite sums in general and give an explicit construction for the weights and abscissae of GAUSSIAN SUMMATION formulas. The abscissae of the Gaussian summation have a very interesting asymptotic distribution function with a (cusp) singularity. We apply the Gaussian summation technique to two problems which have been discussed in the literature. We find that the Gaussian summation has an extremely rapid convergence rate for the Hardy-Littlewood sum for a large range of parameters. For functions which are smooth but have a large scale, a, the error of Gaussian Summation shows exponential convergence {{as a function of}} <b>summation</b> <b>points.</b> The Gaussian <b>summation</b> achieves a given accuracy with a number of points proportional to the sqrt of the large scale whereas other summation schemes require at least a number of function evaluations proportional to the scale. Comment: 14 pages, 4 figures This manuscript is written by a non-expert (me). Any advice on where to publish these results or any other useful comments is appreciate...|$|R
30|$|Since a {{cannot be}} zero, the {{constraint}} becomes ∫ _Mx dx = 0. If we approximate the integral by discrete <b>summations</b> over data <b>points,</b> then Equation 3 becomes the objective function of PCA. The solution {{can be obtained}} by singular value decomposition (SVD).|$|R
50|$|If {{the firm}} is able to sell its {{transfer}} goods in an imperfect market, then it {{need not be a}} price taker. There are two markets each with its own price (Pf and Pt in the next diagram). The aggregate market is constructed from the first two. That is, point C is a horizontal <b>summation</b> of <b>points</b> A and B (and likewise for all other points on the Net Marginal Revenue curve (NMRa)). The total optimum quantity (Q) is the sum of Qf plus Qt.|$|R
2500|$|... {{demonstrates}} {{two different}} ways to write a <b>summation</b> over a <b>point</b> process (see also Campbell's theorem (probability)). More specifically, the integral notation on the left-hand side is interpreting the point process as a random counting measure while the sum {{on the right-hand side}} suggests a random set interpretation.|$|R
40|$|It {{is shown}} that {{standard}} three- and two-dimensional Ewald <b>summation</b> of <b>point</b> charge electrostatics is naturally extended to Gaussian charge distributions. The Gaussian {{nature of the}} charges {{does not affect the}} regularisation of the conditionally convergent sums, which are performed with spherical and cylindrical orderings, respectively. A clear connection is made between the summation of Gaussian charges and the summation of the associated point charge system. The application of these sums to a simple classical model of a metal surface is discussed. Calculations on a conducting sphere highlight the importance of the model parameterisation. © 2010 Elsevier B. V. All rights reserved...|$|R
40|$|The {{essential}} part of numerical solutions of partial differential equations is the representation of gradients and integrals by, respectively, differences between <b>points</b> and <b>summations</b> over <b>points.</b> In order for such numerical representations to be accurate, {{it is necessary that}} these points be more closely spaced in regions of large gradients. The need for accurate representation is particularly acute near body surfaces, since the boundary conditions are generally the most influential part of a partial differential equation solution. This is especially true of viscous solutions at high Reynolds number, where very large gradients occur in the boundary layer...|$|R
40|$|This paper {{provides}} a simpler {{proof of the}} “accurate summation ” algorithm proposed by Demmel and Hida in [1]. It also gives improved bounds in some cases, and examples showing that those new bounds are optimal. This simpler proof {{will be used to}} obtain a computer-checked proof of Demmel-Hida’s algorithm. Key words: Floating <b>point</b> <b>summation,</b> bounded error, formal proof. ...|$|R
40|$|It is {{well known}} that {{different}} ordering of <b>summations</b> in floating <b>point</b> arithmetic can give different sums due to rounding error. This dissertation reviews classic analytic error bounds. A new accurate algorithm is explained thoroughly along with its analytic error bound. These summation algorithms were implemented as dotproducts in an iterative solver to determine which summation ordering is more accurate in practice. Another issue is the relationship between dotproduct accuracy and the convergence of iterative solvers. Analysis and experiments indicate there are two primary sources of errors, and show which summation methods are better for reducing these errors. Results also indicate little correlation between dotproduct accuracy and numbers of iterations required by a solver, within a wide range of accuracies. 1 Statement of problem Floating <b>point</b> <b>summation</b> is a thoroughly studied area in computer science. It {{is well known}} that different summation orders can give greatly differen [...] ...|$|R
40|$|In {{this note}} we study the {{localization}} of Futaki-Morita integrals at isolated degenerate zeros by giving a streamlined exposition {{in the spirit}} of Bott and implement the localization procedure for a holomorphic vector field on CP^n with a maximally degenerate zero, giving an essentially unique formula for the Futaki-Morita integral invariants without using a <b>summation</b> over multiple <b>points.</b> In a coming paper we will apply similar calculations to the Calabi-Futaki invariant of a Kähler blowup...|$|R
40|$|International audienceRecent {{floating}} <b>point</b> <b>summation</b> algorithms {{compute the}} best accurate sum even for arbitrary ill-conditioned problems. So the actual run-time efficiency of these algorithms becomes the criteria {{to decide which}} is the best. Neither the classic flop count nor experimental timings are relevant measures of the actual performance of such core numerical algorithms. We justify these claims and present a reliable performance evaluation of some accurate summation algorithms thanks to an automatic instruction level parallelism analysis...|$|R
50|$|Since November 23, 1997, Wotherspoon {{has held}} the world record for the sprint combination: the <b>point</b> <b>summation</b> of four races (2x500 m and 2x1000 m) skated {{consecutively}} within two or three days, like those calculated for the World Sprint Speed Skating Championships. He improved on his own record five times since. His fastest combination was 135.355 (34.03, 34.14, 1:07.34, 1:07.03), accomplished during World Cup races at the Utah Olympic Oval in November 2007, {{but this was not}} an official world record.|$|R
40|$|We {{prove that}} the {{sequence}} of the characters of the Kirillov-Reshetikhin (KR) modules W_m^(a), m∈Z_m≥ 0 associated to a node a of the Dynkin diagram of a complex simple Lie algebra g satisfies a linear recurrence relation except for some cases in types E_ 7 and E_ 8. To this end we use the Q-system and the existing lattice <b>point</b> <b>summation</b> formula for the decomposition of KR modules, known as domino removal rules when g is of classical type. As an application, we show how to reduce some unproven lattice <b>point</b> <b>summation</b> formulas in exceptional types to finite problems in linear algebra and also give a new proof of them in type G_ 2, {{which is the only}} completely proven case when KR modules have an irreducible summand with multiplicity greater than 1. We also apply the recurrence to {{prove that the}} function W_m^(a) is a quasipolynomial in m and establish its properties. We conjecture that there exists a rational polytope such that its Ehrhart quasipolynomial in m is W_m^(a) and the lattice points of its m-th dilate carry the same crystal structure as the crystal associated with W_m^(a). Comment: 26 pages. v 2 : minor changes, references added. v 3 : Conjecture 3. 6 in v 2 superseded by Proposition 3. 5 in v 3, Section 5 added, references adde...|$|R
40|$|The Spectral Ray Tracking (SRT) {{method is}} an {{original}} ray-based method, where rays represent {{samples of the}} source Plane Wave Spectrum. Arbitrary planar source distributions are admissible, even of limited extension. Backward ray launching and tracking in the spectral domain, allows to represent multiply reflected fields with a computational effort limited to one 2 D discrete <b>summation</b> per observation <b>point.</b> The formulation of the method {{as well as its}} similarities and differences with existing methods (Shooting and Bouncing Rays, Generalized Ray Expansion) are presented. Its concept is validated in cases where exact solutions are known analytically...|$|R
40|$|Demersal {{trawling}} impacts {{extensively on}} the seabed and the extent and frequency of this impact can be assessed using Vessel Monitoring Systems (VMS) data (positional data of fishing vessels). Existing approaches interpolate fishing tracks from consecutive VMS locations (track interpolation) and/or aggregate VMS point data in a spatial grid (<b>point</b> <b>summation).</b> Track interpolation can be quite inaccurate at the current 2 -hour time interval between VMS records, leading to biased estimates. <b>Point</b> <b>summation</b> approaches currently only produce relative estimates of impact and are highly sensitive to the grid size chosen We propose an approach that provides absolute estimates of trawling impact from point data and is not sensitive to an arbitrary choice of grid cell size. The method involves applying a nested grid and estimating the swept area (area covered by fishing gear) for each VMS point. We show that {{the ratio of the}} swept area to the surface area of a cell can be related to the proportion of the seabed that was impacted by the gear a given number of times. We validate the accuracy of this swept-area ratio approach using known vessel tracks and apply the method to international VMS data in the Celtic Sea. Peer- reviewed This is a pre-copy-editing, author-produced PDF of an article accepted for publication in ICES Journal of Marine Science following peer review. The definitive publisher-authenticated version is available online at: doi: 10. 1093 /icesjms/fst 017 Funder: Marine Institut...|$|R
40|$|The D 0 Cryo System is {{monitored}} by a computerized {{process control system}} and an ODH safety system. During steady state operations the cryo system will be unmanned and system experts will depend on communication systems for notification of system problems. The FIRUS system meets the minimum communication requirement and is supplemented with an autodialer which attempts to contact cryo operators by pager or phone. The RD/Safety Department requires the ODH monitor system {{to be connected to}} the labwide FIRUS system. which enables the Communications Center to receive alarms and notify the proper experts of the condition. The ODH system will have two alarm points. One for an ODH alarm and one for a system trouble alarm. The autodialer system has replaced a former cryo operations <b>summation</b> alarm <b>point</b> in the FIRUS system. This has freed space on the FIRUS system and has allowed the cryo experts more flexibility in setting up their own communication link. The FIRUS and the autodialer systems receive alarms and access lists of experts to call for notification of problems. Attempts to contact these experts will continue until the alarm or alarms is acknowledged...|$|R
40|$|Abstract—Reproducibility, i. e. {{getting the}} bitwise {{identical}} floating point results from multiple runs {{of the same}} program, is a property that many users depend on either for debugging or correctness checking in many codes [1]. However, the combination of dynamic scheduling of parallel computing resources, and floating point nonassociativity, make attaining reproducibility a challenge even for simple reduction operations like computing the sum of a vector of numbers in parallel. We propose a technique for floating <b>point</b> <b>summation</b> that is reproducible independent {{of the order of}} summation. Our technique uses Rump’s algorithm for error-free vector transformation [2], and is much more efficient than using (possibly very) high precision arithmetic. Our algorithm trades off efficiency and accuracy: we reproducibly attain reasonably accurate results (with an absolute error boun...|$|R
40|$|The {{emergence}} of high-density reconfigurable hardware devices gives {{scientists and engineers}} an option to accelerating their numerical computing applications on low-cost but powerful “FPGA-enhanced computers”. In this paper, we introduced our efforts towards improving the computational performance of Basic Linear Algebra Subprograms (BLAS) by FPGA-specific algorithms/methods. Our study focus on three BLAS subroutines: floating <b>point</b> <b>summation,</b> matrix-vector multiplication, and matrix-matrix multiplication. They represent all three levels of BLAS functionalities, and their sustained computational performances are either memory bandwidth bounded or computation bounded. By proposing the group-alignment based floating-point summation method and applying this technique to other subroutines, we significantly improved their sustained computational performance and reduced numerical errors with moderate FPGA resources consumed. Comparing with existing FPGA-based implementations, our designs are efficient and compact with improved numerical accuracy and stability. 1...|$|R
40|$|Multipolar {{expansions}} are a foundational {{tool for}} describing basis functions in quantum mechanics, many-body polarization, and other distributions {{on the unit}} sphere. Progress on these topics is often held back by complicated and competing formulas for calculating and using spherical harmonics. We present a complete representation for supersymmetric 3 D tensors that replaces spherical harmonic basis functions by a dramatically simpler set of weights associated to discrete points in 3 D space. This representation is shown to be space optimal. It reduces tensor contraction and the spherical harmonic decomposition of Poisson's operator to pairwise <b>summations</b> over the <b>point</b> set. Moreover, multiplication of spherical harmonic basis functions translates to a direct product in this representation. Comment: 12 pages, 4 figures, presented at Southeast Regional Meeting of the Amer. Chem. Soc., October 201...|$|R
40|$|We {{show that}} a mobile {{observer}} in a natural environment receives systematically co-varying signals in his different sensory modalities. An independent, modality-specific processing- as assumed in classical theories of perception- would hence be sub-optimal. Rather, information theory predicts that the system should use a statistically optimised joint processing strategy. We tested this by measuring the two-dimensional just-noticeable difference (jnd) curves for basic visual-auditory stimulus configurations (a patch of light combined with a 1 kHz tone). The forced-choice task was to detect any change in this configuration, irrespective of modality. The resulting two-dimensional jnd-curve cannot be explained by an independent, modality-specific processing. In particular, the sensitivity increase for the "ecologically relevant " joint auditory-visual increments or decrements {{is much higher than}} the usual probability <b>summation</b> effects. This <b>points</b> to a direct neural integration of visual and auditory information at an early stage...|$|R
40|$|Abstract—Double {{precision}} summation is at {{the core}} of numerous important algorithms such as Newton-Krylov methods and other operations involving inner products, but the effectiveness of summation is limited by the accumulation of rounding errors, which are an increasing problem with the scaling of modern HPC systems and data sets. To reduce the impact of precision loss, researchers have proposed increasedand arbitrary-precision libraries that provide reproducible error or even bounded error accumulation for large sums, but do not guarantee an exact result. Such libraries can also increase computation time significantly. We propose big integer (BigInt) expansions of double precision variables that enable arbitrarily large summations without error and provide exact and reproducible results. This is feasible with performance comparable to that of double-precision floating <b>point</b> <b>summation,</b> by the inclusion of simple and inexpensive logic into modern NICs to accelerate performance on large-scale systems. I...|$|R
50|$|The {{widespread}} {{familiarity of}} fotonovelas in Spanish-language culture makes photo comics an effective vehicle for health promotion and health education. Since the small pamphlets can be traded among individuals, they possess {{an element of}} portability that traditional materials lack. Both health and non-health entities have utilized the fotonovela as informational pamphlets. The fotonovelas produced by these organizations present information {{in a variety of}} illustrated forms but usually contain a <b>summation</b> of key <b>points</b> at the end. Health educators have also utilized the fotonovela because the medium overcomes issues of health literacy, which {{is the degree to which}} individuals can obtain, process and understand basic health information to make appropriate health decisions, in their target audience. Most providers believe that health education materials designed specifically for patients with low health literacy would be helpful: however, written educational materials found in most health settings have been deemed to have serious deficiencies.|$|R
40|$|Fatty acid composition, {{marbling}} {{score and}} melting point data collected between 1994 and 1996 were analysed. The data {{were from the}} adipose tissue of 764 Angus, Belgian Blue, Hereford, Jersey, Limousin, South Devon and Wagyu crossbred cattle slaughtered after lot-feeding at 500 days of age. The aim was to investigate sire-breed differences and to estimate heritability and genetic and phenotypic correlations. Significant breed differences were found: Jersey crosses had the highest marbling score and Belgian Blue crosses had the lowest. Limousin crosses had the highest melting point and Jersey crosses the lowest. South Devon crosses had the highest proportion of stearate and Jersey crosses the lowest. Desaturation indices in C 16 and C 18 fatty acids were highest in Jersey crosses and lowest in Limousin and South Devon crosses. In contrast, there were no breed differences in the proportions of palmitate, oleate, total saturated, total mono-unsaturated fatty acids and elongation index. Heritability estimates of individual fatty acids and their <b>summations,</b> melting <b>point</b> and marbling were low to moderately low (0. 05 – 0. 27). Strong genetic correlations of melting point and desaturation index in C 16 fatty acids (- 0. 93), melting point and stearate (0. 62), marbling and stearate (- 0. 71) and marbling and desaturation index in C 18 fatty acids (0. 62) were observed. Phenotypic correlation were generally low. The results imply that fatty acids in the adipose tissue of lot-fed cattle have a moderately low heritability, hence genetic progress might be slow...|$|R
40|$|According to loop quantum gravity, matter fields must be {{quantized}} in {{a background}} independent manner. For scalar fields, such a background independent quantization is called polymer quantization and is inequivalent {{to the standard}} Schrodinger quantization. It is therefore important to obtain predictions from the polymer quantized scalar field theory and compare with the standard results. As a step towards this, we develop a path integral representation for the polymer quantized scalar field. We notice several crucial differences from the path integral for the schrodinger quantized scalar field. One important difference is the appearance of an extra <b>summation</b> at each <b>point</b> in the path integral for the polymer quantized theory. A second crucial difference {{is the loss of}} manifest Lorentz symmetry for a polymer quantized theory on Minkowski Space. Comment: In this version, the derivation of the path integral is presented starting from the position basis instead of the momentum basis to better highlight the similarities with particle on a circl...|$|R
40|$|The usual {{recursive}} summation {{technique is}} just one of several ways of computing the sum of n floating <b>point</b> numbers. Five <b>summation</b> methods and their variations are analysed here. The accuracy of the methods is compared using rounding error analysis and numerical experiments. Four of the methods are shown to be special cases of a general class of methods, and an error analysis is given for this class. No one method is uniformly more accurate than the others, but some guidelines are given on the choice of method in particular cases. Key words. floating <b>point</b> <b>summation,</b> rounding error analysis, orderings. AMS subject classifications. primary 65 G 05, secondary 65 B 10. 1. Introduction. Sums of floating point numbers are ubiquitous in scientific computing. They occur when evaluating inner products, means, variances, norms, and all kinds of nonlinear functions. Although, at first sight, summation might appear to offer little scope for algorithmic ingenuity, the usual "recursive summation [...] ...|$|R
40|$|A <b>point</b> <b>summation</b> {{technique}} {{has been used}} to analyze systematically the Uhuru data for X-ray emission from the 88 Seyfert galaxies listed by Weedman (1977), plus MCG 8 - 11 - 11 reported by the Ariel 5 group. In addition to measuring the average X-ray intensity for 15 sources reported in the 4 U and 2 A catalogs, three new candidate sources are found. X-ray variability has previously been reported for NGC 4151, 3 C 390. 3, and MCG 8 - 11 - 11; Mrk 279 is now also found to vary. Furthermore, significant flaring activity from NGC 4151 was observed with as much as a factor of 10 increase in intensity on a time possibly as short as 730 seconds. The local X-ray volume emissivity of Seyfert galaxies is measured, and it is found, with standard assumptions, that from 6 % to 25 % of the diffuse 2 - 10 -keV X-ray background can be attributed to emission from Seyfert galaxies. The data show that the luminosity function for X-ray Seyferts is rather steep...|$|R
40|$|This paper {{presents}} novel {{methods for}} spectrophotometric determination of ascorbic acid (AA) in presence of rutin (RU) (coformulated drug) in their combined pharmaceutical formulation. The seven methods are ratio difference (RD), isoabsorptive_RD (Iso_RD), amplitude <b>summation</b> (A_Sum), isoabsorptive <b>point,</b> first derivative of the ratio spectra (1 DD), mean centering (MCN), and ratio subtraction (RS). On the other hand, RU was determined directly {{by measuring the}} absorbance at 358 [*]nm {{in addition to the}} two novel Iso_RD and A_Sum methods. The work introduced in this paper aims to compare these different methods, showing the advantages for each and making a comparison of analysis results. The calibration curve is linear over the concentration range of 4 – 50 [*]μg/mL for AA and RU. The results show the high performance of proposed methods for the analysis of the binary mixture. The optimum assay conditions were established and the proposed methods were successfully applied for the assay of the two drugs in laboratory prepared mixtures and combined pharmaceutical tablets with excellent recoveries. No interference was observed from common pharmaceutical additives...|$|R
40|$|We propose an {{integral}} image based algorithm to extract feature covariance matrices {{of all possible}} rectangular regions within a given image. Covariance is an essential indicator of how much the deviation {{of two or more}} variables match. In our case, these variables correspond to point-wise features, e. g. coordinates, color values, gradients, edge magnitude and orientation, local histograms, filter responses, etc. We significantly improve the speed of the covariance computation by taking advantage of the spatial arrangement of image points using integral images, which are intermediate representations used for calculation of region sums. Each point of the integral image corresponds to the <b>summation</b> of all <b>point</b> values inside the feature image rectangle bounded by the upper left corner and the point of interest. Using this representation, any rectangular region sum can be computed in constant time. We follow a similar idea for fast calculation of region covariance. We construct integral images for all separate features as well as integral images of the multiplication of any two feature combinations. Using these set of integral images and region corner point coordinates, we directly extract the covariance matrix coefficients. We show that the proposed method reduces the computational load to quadratic time...|$|R
40|$|Let G be a fixed {{connected}} multigraph with no loops. A random n-lift of G {{is obtained}} by replacing each vertex of G {{by a set}} of n vertices (where these sets are pairwise disjoint) and replacing each edge by a randomly chosen perfect matching between the n-sets corresponding to the endpoints of the edge. Let X_G be the number of perfect matchings in a random lift of G. We study the distribution of X_G in the limit as n tends to infinity, using the small subgraph conditioning method. We present several results including an asymptotic formula for the expectation of X_G when G is d-regular, d≥ 3. The interaction of perfect matchings with short cycles in random lifts of regular multigraphs is also analysed. Partial calculations are performed for the second moment of X_G, with full details given for two example multigraphs, including the complete graph K_ 4. To assist in our calculations we provide a theorem for estimating a summation over multiple dimensions using Laplace's method. This result is phrased as a <b>summation</b> over lattice <b>points,</b> and may prove useful in future applications. Comment: 29 pages; some minor rewording; change of notation for bilinear form...|$|R
40|$|Time-interleaved {{technique}} {{is widely used}} to increase the sampling rate of analog-to-digital converter (ADC). However, the channel mismatches degrade the performance of time-interleaved ADC (TIADC). Therefore, a statistic-based calibration method for TIADC is proposed in this paper. The average value of sampling points is utilized to calculate offset error, and the <b>summation</b> of sampling <b>points</b> {{is used to calculate}} gain error. After offset and gain error are obtained, they are calibrated by offset and gain adjustment elements in ADC. Timing skew is calibrated by an iterative method. The product of sampling points of two adjacent subchannels is used as a metric for calibration. The proposed method is employed to calibrate mismatches in a four-channel 5 [*]GS/s TIADC system. Simulation results show that the proposed method can estimate mismatches accurately in a wide frequency range. It is also proved that an accurate estimation can be obtained even if the signal noise ratio (SNR) of input signal is 20 [*]dB. Furthermore, the results obtained from a real four-channel 5 [*]GS/s TIADC system demonstrate the effectiveness of the proposed method. We can see that the spectra spurs due to mismatches have been effectively eliminated after calibration...|$|R
