7|13|Public
40|$|By {{studying}} the dynamic value density and urgency of a task, a preemptive scheduling strategy based on dynamic priority assignment is proposed. In the strategy, two param-eters p and q {{are used to}} adjust the weight that the value density and urgency of a task impact on its priority, and a parameter β is used to avoid the possible <b>system</b> <b>thrashing.</b> Finally, the simulations show that our algorithm is prior to the analogous algorithms, such as EDF, HVF and HVDF, on gained-value of the system, deadline miss ratio and pre-emptive number. ...|$|E
40|$|The {{design of}} the buffer manager in a Relational Database Management System can {{significantly}} affect the overall performance of the <b>system.</b> <b>Thrashing</b> is a common phenomenon that occurs in these systems due to {{the combination of a}} regular pattern of accesses made by a process and the competing requests for buffer resources made by concurrently executing processes. In this paper, we present a buffer management algorithm based on a model of database requests. A discussion of problems encountered by traditional methods for buffer management as well as extensions to the algorithm are also presented...|$|E
40|$|Several {{operational}} numerical {{weather prediction}} (NWP) centers will approach a petaflop of peak performance by early 2012 presenting several system operation challenges. An evolution in system utilization strategies along with advanced scheduling technologies are needed to exploit these breakthroughs in computational speed while improving the Quality of Service (QoS) and system utilization rates. The Cray XE 6 ™ supercomputer in conjunction with Altair PBS Professional ® provides a rich scheduling environment designed to support and maximize the specific features of the Cray architecture. Advantages of this model include avoidance of <b>system</b> <b>thrashing,</b> increased predictability in the scheduling model, and reliable and repeatable runtimes benefiting both operational and research users...|$|E
5000|$|The M44/44X {{showed that}} a partial {{approach}} to virtual machines was not good enough, and that thrashing could severely reduce the speed of virtual memory <b>systems.</b> <b>Thrashing</b> is {{a condition in which}} the system runs very slowly because it spends a lot of its time shuffling virtual memory pages between physical memory and disk files.|$|R
50|$|In {{virtual memory}} <b>systems,</b> <b>thrashing</b> {{may be caused}} by {{programs}} or workloads that present insufficient locality of reference: if the working set of a program or a workload cannot be effectively held within physical memory, then constant data swapping, i.e., thrashing, may occur. The term was first used during the tape operating system days to describe the sound the tapes made when data was being rapidly written to and read.An example of this sort of situation occurred on the IBM System/370 series mainframe computer, in which a particular instruction could consist of an execute instruction (which crosses a page boundary) that points to a move instruction (which itself also crosses a page boundary), targeting a move of data from a source that crosses a page boundary, to a target of data that also crosses a page boundary. The total number of pages thus being used by this particular instruction is eight, and all eight pages must be present in memory at the same time. If the operating system allocates fewer than eight pages of actual memory, when it attempts to swap out some part of the instruction or data to bring in the remainder, the instruction will again page fault, and it will thrash on every attempt to restart the failing instruction.|$|R
30|$|The {{paper by}} Carlos Maziero, Douglas dos Santos, and Altair Santin {{contains}} {{an evaluation of}} some popular general-purpose operating <b>systems</b> under memory <b>thrashing</b> conditions, i.e., when system throughput is significantly reduced due to extensive paging activity. Authors describe a portable benchmark tool, identify the performance data about memory management in each system and discuss the different behaviors observed.|$|R
40|$|Operating system {{designers}} {{attempt to}} keep high CPU utilization by maintaining an optimal multiprogramming level (MPL). Although running more processes makes it less likely to leave the CPU idle, too many processes adversely incur serious memory competition, and even introduce thrashing, which eventually lowers CPU utilization. A common practice {{to address the problem}} is to lower the MPL with the aid of process swapping out/in operations. This approach is expensive and is only used when the system begins serious thrashing. The objective of our study is to provide highly responsive and cost-effective thrashing protection by adaptively conducting priority page replacement in a timely manner. We have designed a dynamic <b>system</b> <b>Thrashing</b> Protection Facility (TPF) in the system kernel. Once TPF detects <b>system</b> <b>thrashing,</b> one of the active processes will be identified for protection. The identified process will have a short period of privilege in which it does not contribute its least recently used (LRU) pages for removal so that the process can quickly establish its working set, improving the CPU utilization. With the support of TPF, thrashing can be eliminated in its early stage by adaptive page replacement, so that process swapping will be avoided or delayed until it is truly necessary. We have implemented TPF in a current and representative Linux kernel running on an Intel Pentium machine. Compared with the original Linux page replacement, we show that TPF consistently and significantly reduces page faults and the execution time of each individual job in several groups of interacting SPEC CPU 2000 programs. We also show that TPF introduces little additional overhead to program executions, and its implementation in Linux (or Unix) systems is straightforward...|$|E
40|$|The {{increasing}} demand for high throughputs in transaction processing systems leads to high degrees of transaction concurrency and hence high data contention. The conventional dynamic two-phase locking (2 PL) concurrency control (CC) technique causes <b>system</b> <b>thrashing</b> at high data contention levels, restricting transaction throughput. Optimistic concurrency control (OCC) {{is an alternative}} strategy, but OCC techniques suffer from wasted resources caused by repeated transaction restarts. We propose a new technique, ORDER, that enlists {{the aid of the}} interconnection network in a distributed database system in order to coordinate transactions. The network in an ORDER system provides total ordering of messages at a low cost, enabling efficient CC. We compare the performance of dynamic 2 PL and ORDER, using both an analytical model and a simulation. Unlike previously-proposed models for 2 PL, our analytical model predicts performance accurately even under high data contention. We study the effects of various parameters on performance, and demonstrate that ORDER outperforms dynamic 2 PL {{for a wide range of}} workloads...|$|E
40|$|Most {{computer}} systems use a global page replacement policy {{based on the}} LRU principle to approximately select a Least Recently Used page for a replacement in the entire user memory space. During execution interactions, a memory page can be marked as LRU even when its program is conducting page faults. We define the LRU pages under such a condition as false LRU pages because these LRU pages are not produced by program memory reference delays, which is inconsistent with the LRU principle. False LRU pages can significantly increase page faults, even cause <b>system</b> <b>thrashing.</b> This poses a more serious risk in a large parallel systems with distributed memories because {{of the existence of}} coordination among processes running on individual node. In the case, the process thrashing in a single node or a small number of nodes could severely affect other nodes running coordinating processes, even crash the whole system. In this paper, we focus on how to improve the page replacement algorithm running on one node. After a careful study on characterizing the memory usage and the thrashing behaviors in the multi-programming system using LRU replacement. we propose an LRU replacement alternative, called token-ordered LRU, to eliminate or reduce the unnecessary page faults by effectively ordering and scheduling memory space allocations. Compared with traditional thrashing protection mechanisms such as load control, our policy allows more processes to keep running to support synchronous distributed process computing. We have implemented the token-ordered LRU algorithm in a Linux kernel to show its effectiveness...|$|E
40|$|A {{critical}} {{issue for the}} grain farming industry is the continuous development of new and improved varieties. This often requires field experiments in small plots. Due to {{the small size of}} the plots and the large number of them, harvesting these trials often becomes very inefficient and expensive. Contamination and Occupational Health and Safety issues may also be involved. This paper describes the design and development of a prototype small-plot research harvesting system. The system features a beater type <b>thrashing</b> <b>system</b> which combines the dual functions of removing the grain from the crop and the <b>thrashing</b> <b>system.</b> The material then falls under gravity and is inducted into the conveyance air stream by means of a venturi. The grain and the trash are finally separated in a vertical air aspirator. Due to the self-cleaning nature of this design, the possible contamination between different plots has been avoided. The system is also compact and light. The potential improvements for the commercial production are identified...|$|R
40|$|A metacomputer {{is a set}} of {{machines}} networked together for increased computational performance. To build an efficient metacomputer, one must assign jobs to the various networked machines intelligently. A poor job assignment strategy can result in heavily unbalanced loads and thrashing machines. This cripples the cluster's computational power. A strong job assignment strategy helps a metacomputer complete all of its jobs swiftly. Resource heterogeneity makes job assignment more complex. Placing a job on one machine might risk depleting its small memory. Another machine might have more free memory but a heavily burdened CPU. Bin packing on memory protects the <b>system</b> against <b>thrashing.</b> Load balancing protects the system against high CPU loads. Combining the two approaches, however, gives an ad hoc heuristic algorithm with no clear theoretical merit. The Cost-Benefit Framework, developed in this work, offers a new approach to job assignment on metacomputers. It smoothly handles heterogen [...] ...|$|R
40|$|Collaboration of {{different}} information systems with partially redundant database contents {{may help to}} decrease response times and to increase availability of the system for query activities. For this purpose these query activities {{have to be performed}} redundantly in parallel on databases in different information systems. However, as collaborations are scaling up in the number of participating customers, uncontrolled parallel querying will finally lead to the opposite effect, namely longer and longer waiting time, until the whole collaborating <b>system</b> undergoes <b>thrashing.</b> In this paper we present a novel, natural, analytic pricing scheme for query services which may be used to control the degree of effective redundancy in query activities by way of self-regulation. The pricing scheme is based on the calculation of tailback effects and it binds the prices to actual performance, thus ruling out any deviations of prices from real behavior due to speculative system usage by cus [...] ...|$|R
40|$|Rights to {{individual}} papers {{remain with the}} author or the author's employer. Permission is granted for noncommercial reproduction of the work for educational or research purposes. This copyright notice must {{be included in the}} reproduced paper. USENIX acknowledges all trademarks herein. Adaptive Page Replacement to Protect Thrashing in Linux £ Analyzing the variations of page replacement implementations in recent Linux kernel versions of 2. 0, 2. 2, and 2. 4, we compare their abilities to deal with <b>system</b> <b>thrashing.</b> We show that although the page implementation in Kernel 2. 2 is relatively effective to protect thrashing among the three versions, none of them have adaptive ability, and thus the protection is limited. By running several groups of memory-intensive application programs on Kernel 2. 2, we observe serious thrashing when memory shortage attains a certain level. We propose and implement a thrashing protection patch in Linux kernels, which makes replacement policy responsively resolve excessive memory paging by temporarily helping one of the active processes quickly build up its working set. Consequently, thrashing could be eliminated at the level of page replacement, so that load controls at a higher level, such as process suspensions/swapping can be avoided or delayed until it is truly necessary. Our experiments show that our patch can significantly reduce page faults and the execution time of each individual thrashing process for several groups of interacting programs. We also show that our method introduces little additional overhead to program executions, and its implementation in Linux (or Unix) system is straightforward. ...|$|E
40|$|This paper {{presents}} a user-level runtime system which provides memory malleability to programs running on non-dedicated computational environments. Memory malleability {{is analogous to}} processor malleability in the memory space, i. e. it lets a program shrink and expand its resident set size in response to runtime events, without affecting the correct execution of the program. Malleability becomes relevant {{in the context of}} grid computing, where loosely coupled distributed programs assume to run on busy computational nodes with fluctuating CPU and memory loads. User-level malleable memory is proposed as a portable solution to obtain as much as possible out of the available memory of a computational node, without reverting to more drastic solutions such as job suspension or migration, and without causing the <b>system</b> to <b>thrash.</b> Malleable memory mapping is also a solution to cope with the unpredictable behavior of existing virtual memory management policies under oversized memory loads. The current prototype is simple but leaves plenty of room for application-independent or application-specific optimizations, compiler support and other extensions. Our performance evaluation is a proof of concept that grid programs with malleable memory can improve their performance by an order of magnitude as opposed to grid programs that let their memory being reclaimed and reallocated by the OS. ...|$|R
2500|$|A pbit of 1 {{indicates}} {{the presence of}} the block. [...] In this case, the block can be accessed via the physical address in the descriptor. [...] If the pbit is zero, an interrupt is generated for the MCP (operating system) to make the block present. [...] If the address field is zero, this is the first access to this block, and it is allocated (an init pbit). [...] If the address field is non-zero, it is a disk address of the block, which has previously been rolled out, so the block is fetched from disk and the pbit is set to one and the physical memory address updated to point to the block in memory (another pbit). [...] This makes descriptors equivalent to a page-table entry in an MMU system. System performance can be monitored through the number of pbits. [...] Init pbits indicate initial allocations, but a high level of other pbits indicate that the <b>system</b> may be <b>thrashing.</b>|$|R
50|$|A pbit of 1 {{indicates}} {{the presence of}} the block. In this case, the block can be accessed via the physical address in the descriptor. If the pbit is zero, an interrupt is generated for the MCP (operating system) to make the block present. If the address field is zero, this is the first access to this block, and it is allocated (an init pbit). If the address field is non-zero, it is a disk address of the block, which has previously been rolled out, so the block is fetched from disk and the pbit is set to one and the physical memory address updated to point to the block in memory (another pbit). This makes descriptors equivalent to a page-table entry in an MMU system. System performance can be monitored through the number of pbits. Init pbits indicate initial allocations, but a high level of other pbits indicate that the <b>system</b> may be <b>thrashing.</b>|$|R
40|$|This paper {{investigates the}} issue of {{practical}} load balancing in the highly dynamic environ-ment of local area networks (LANs). Most existing dynamic load balancing techniques dispatch the jobs immediately upon arrival irrespective of the overall loading of the LAN. This may lead to <b>system</b> saturation and <b>thrashing.</b> Moreover, these schemes focus on steady state system throughput without considering the behavior in the transient periods. As a result, they may not deliver satisfactory performance in practice where jobs may arrive in batches and system loading may fluctuate widely. To tackle the above problems, the delay scheduling strategy is proposed to dynamically delay the execution of jobs after the system is fully utilized. The proposed strategy is general and can be augmented on most existing algorithms. Experimental {{results showed that the}} proposed strategy adapts well to load fluctuation, minimizes system loading while producing shorter schedules and improved job fairness compared to some popular schedulers...|$|R
40|$|It {{has been}} {{estimated}} that the cost of black point damage for the Australian wheat industry are up to 50 million dollars each year in down-graded wheat. Similarly in barley, the loss of income for the farmer in the down-graded barley can be in-excess of 50 dollars per tonne. Currently a research project which involves the DPI& F in Toowoomba, Queensland, have discovered that the traditional explanation for black point may not be correct. To achieve this task the DPI& F plant small plots of wheat and barley each year and manage the crop to induce conditions favorable to black point in an aim to try and determine the resistance of each variety to black point. To achieve this task the DPI& F plant small plots of wheat and barley each year and manage the crop to induce conditions favorable to black point. Due to the small size of the plots and the large number of them, harvesting the trials becomes inefficient and expensive. This project is aimed at increasing the efficiency of the harvesting of the trial plots. In doing this the project will aim at mechanizing the cutting and thrashing process into one simple portable device. The system that has been designed features a beater type <b>thrashing</b> <b>system</b> where the grain is thrashed while still on the plant...|$|R
40|$|This article {{suggests}} an integrated view of media entertainment that {{is capable of}} covering more of the dimensional complexity and dynamics of entertain-ment experiences than existing theories do. Based on a description of what is meant by complexity and dynamics, the authors outline a conceptual model that is centered around enjoyment as the core of entertainment, and that ad-dresses prerequisites of enjoyment which have to be met by the individual me-dia user and by the given media product. The theoretical foundation is used to explain why people display strong preferences for being entertained (motiva-tional perspective) and what kind of consequences entertaining media consump-tion may have (effects perspective, e. g., facilitation of learning processes). Strong empirical evidence indicates that the motivational basis of hu-man activity relies on two rather independent systems: a so-called ap-proach system and an avoidance <b>system</b> (Elliot & <b>Thrash,</b> 2002). Acti-vation of the approach system results in pleasure, whereas activation of the avoidance system leads to pain (Berridge, 2003). Research in psy-chology and neuroscience most often uses the term “pleasure ” to de-scribe agreeable reactions to experiences in general. With the exception of Bosshart and Macconi’s (1998) elaboration of the construct enter-tainment as a “reception phenomenon ” (that includes various forms of pleasures), most communication researchers have used the term enjoy-ment to describe and explain such positive reactions toward the medi...|$|R
40|$|Server {{farms are}} popular {{architectures}} for computing infrastructures such as supercomputing centers, data centers and web server farms. As server farms become larger and their workloads more complex, designing efficient policies {{for managing the}} resources in server farms via trial-and error becomes intractable. In this thesis, we employ stochastic modeling and analysis techniques to understand the performance of such complex systems and to guide design of policies to optimize the performance. There is a rich literature on applying stochastic modeling to diverse application areas such as telecommunication networks, inventory management, production systems, and call centers, but there are numerous disconnects between the workloads and architectures of these traditional applications of stochastic modeling and how compute server farms operate, necessitating new analytical tools. To cite a few: (i) Unlike call durations, supercomputing jobs and file sizes have high variance in service requirements and this critically affects the optimality and performance of scheduling policies. (ii) Most existing analysis of server farms focuses on the First-Come- First-Served (FCFS) scheduling discipline, while time sharing servers (e. g., web and database servers) are better modeled by the Processor- Sharing (PS) scheduling discipline. (in) Time sharing <b>systems</b> typically exhibit <b>thrashing</b> (resource contention) which limits the achievable concurrency level, but traditional models of time sharing systems ignore this fundamental phenomenon. (iv) Recently, minimizing energy consumption has become an important metric in managing server farms. State-of-the-art servers come with multiple knobs to control energy consumption, but traditional queueing models don’t take the metric of energy consumption into account. In this thesis we attempt to bridge some of these disconnects by bringing the stochastic modeling and analysis literature closer {{to the realities of}} today’s compute server farms. We introduce new queueing models for computing server farms, develop new stochastic analysis techniques to evaluate and understand these queueing models, and use the analysis to propose resource management algorithms to optimize their performance...|$|R

