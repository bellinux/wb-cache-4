16|10000|Public
50|$|A {{protector}} {{subjected to}} this test method is deemed {{to conform to}} this standard if the average transmitted force of nine tests is less than 35 kN, with no <b>single</b> <b>test</b> <b>result</b> exceeding 50 kN.|$|E
40|$|Many {{contracting}} agencies currently use permeability specifications {{in portland}} cement concrete (PCC) pavements and structures. This project followed {{the implementation of}} the surface resistivity test (TR 233) on a field project in Louisiana. Additionally, a precision statement was developed for TR 233, and a ruggedness study was conducted to determine influencing factors on the results of TR 233 testing. The single operator coefficient of variation of a <b>single</b> <b>test</b> <b>result</b> {{has been found to be}} 2. 2 percent. Therefore, the results of two properly conducted tests by the same operator on concrete samples from the same batch and of the same diameter should not differ by more than 6. 2 percent. The multilaboratory coefficient of variation of a <b>single</b> <b>test</b> <b>result</b> has been found to be 3. 9 percent. Therefore the results of two properly conducted tests in different laboratories on the same material should not differ by more than 11 percent. The collected data only covered the moderate, low, and very low permeability classes; because of this, the precision statement should only be used for values within these ranges. Further testing is recommended to investigate values in the high and negligible permeability classes. The surface resistivity test shows lower variability than rapid chloride permeability test...|$|E
40|$|International audienceThe {{objective}} {{of this paper is}} to understand what characteristics and features of clinical data influence physician's decision about ordering laboratory tests or prescribing medications the most. We conduct our analysis on data and decisions extracted from electronic health records of 4486 post-surgical cardiac patients. The summary statistics for 335 different lab order decisions and 407 medication decisions are reported. We show that in many cases, physician's lab-order and medication decisions are predicted well by simple patterns such as last value of a <b>single</b> <b>test</b> <b>result,</b> time since a certain lab test was ordered or time since certain procedure was executed...|$|E
40|$|Preliminary. The local {{asymptotic}} {{power of}} many popular non-cointegration tests {{has recently been}} shown to depend on a certain nuisance parameter. Depending {{on the value of}} that parameter, different tests perform best. This paper suggests combination procedures with the aim of providing meta tests that maintain high power across the range of the nuisance parameter. The local asymptotic power of the new meta tests is in general almost as high as that of the more powerful of the underlying tests. When the underlying tests have similar power, the meta tests are even more powerful than the best underlying test. At the same time, our new meta tests avoid the arbitrary decision which test to use if <b>single</b> <b>test</b> <b>results</b> conflict. Moreover it avoids the size distortion inherent in separately applying multiple tests for cointegration to the same data set. We apply our test to 161 data sets from published cointegration studies. There, in one third of all cases <b>single</b> <b>tests</b> give conflicting <b>results</b> whereas our meta tests provide an unambiguous test decision...|$|R
40|$|AbstractWe have {{previously}} shown {{that there is}} a complex and dynamic biological interaction between acute mental stress and acute release of inflammatory factors into the blood stream in relation to heart disease. We now hypothesize that the presence of chronic psychosocial stress may modify the weight of <b>single</b> <b>test</b> <b>results</b> for inflammation as a predictor of heart disease. Using a cross-sectional design, 500 participants free from heart disease drawn from the Whitehall II study in UK in 2006 – 2008 were tested for plasma fibrinogen as an inflammatory factor, financial strain as a marker of chronic psychosocial stress, coronary calcification measured using computed tomography, and for plasma high-sensitivity cardiac troponin T (HS-CTnT) as a marker of cardiac risk. Fibrinogen concentration levels above the average were associated with a 5 -fold increase in the odds of HS-CTnT positivity only among individuals with financial strain (N= 208, OR= 4. 73, 95 %CI= 1. 67 to 13. 40, P= 0. 003). Fibrinogen was in fact not associated with HS-CTnT positivity in people without financial strain despite the larger size of that subsample (n= 292, OR= 0. 84, 95 %CI= 0. 42 to 1. 67, P= 0. 622). A test for interaction on the full sample (N= 500) showed a P value of 0. 010 after adjusting for a range of demographics, health behaviours, traditional cardiovascular risk factors, psychosocial stressors, inflammatory cytokines, and coronary calcification. In conclusion, elevated fibrinogen seems to be cardio-toxic only when is combined with financial strain. Chronic psychosocial stress may modify the meaning that we should give to <b>single</b> <b>test</b> <b>results</b> for inflammation. Further research is needed to confirm our results...|$|R
40|$|This is an Open Access Article. It is {{published}} by Elsevier under the Creative Commons Attribution 4. 0 Unported Licenece (CC BY). Full details of this licence are available at [URL] have previously shown {{that there is a}} complex and dynamic biological interaction between acute mental stress and acute release of inflammatory factors into the blood stream in relation to heart disease. We now hypothesize that the presence of chronic psychosocial stress may modify the weight of <b>single</b> <b>test</b> <b>results</b> for inflammation as a predictor of heart disease. Using a cross-sectional design, 500 participants free from heart disease drawn from the Whitehall II study in UK in 2006 – 2008 were tested for plasma fibrinogen as an inflammatory factor, financial strain as a marker of chronic psychosocial stress, coronary calcification measured using computed tomography, and for plasma high-sensitivity cardiac troponin T (HS-CTnT) as a marker of cardiac risk. Fibrinogen concentration levels above the average were associated with a 5 -fold increase in the odds of HS-CTnT positivity only among individuals with financial strain (N = 208, OR = 4. 73, 95 %CI = 1. 67 to 13. 40, P = 0. 003). Fibrinogen was in fact not associated with HS-CTnT positivity in people without financial strain despite the larger size of that subsample (n = 292, OR = 0. 84, 95 %CI = 0. 42 to 1. 67, P = 0. 622). A test for interaction on the full sample (N = 500) showed a P value of 0. 010 after adjusting for a range of demographics, health behaviours, traditional cardiovascular risk factors, psychosocial stressors, inflammatory cytokines, and coronary calcification. In conclusion, elevated fibrinogen seems to be cardio-toxic only when is combined with financial strain. Chronic psychosocial stress may modify the meaning that we should give to <b>single</b> <b>test</b> <b>results</b> for inflammation. Further research is needed to confirm our results...|$|R
40|$|We {{analyzed}} {{pancreatic enzyme}} data from 508 patients with suspected pancreatitis by neural network analysis, by an Expert multirule generation protocol, and by receiveroperator characteristic (ROC) curve {{analysis of a}} <b>single</b> <b>test</b> <b>result.</b> Neural network analysis showed that use of lipase provided the best means for diagnosing pancreatitis. Diagnostic accuracies achieved by using amylase only, lipase only, and amylase and lipase in combination were 76 %, 82 %, and 84 %, respectively. Use of the Expert rule generation protocolprovided a diagnosticaccuracy of 92 % when rules for single and multiple samplings were combined. ROC curve analysis for initialenzyme activities showed the maximal diagnostic accuracy to be 82 % and 85 % for amylase and lipase, respectively; use of peak enzyme activities yielded accuracies of 81 % and 88 %...|$|E
40|$|The paper {{discusses}} several {{knowledge engineering}} techniques {{for the construction}} of Bayesian networks for medical diagnostics when the available numerical probabilistic information is incomplete or partially correct. This situation occurs often when epidemiological studies publish only indirect statistics and when significant unmodeled conditional dependence exists in the problem domain. While nothing can replace precise and complete probabilistic information, still a useful diagnostic system can be built with imperfect data by introducing domain-dependent constraints. We propose {{a solution to the problem}} of determining the combined influences of several diseases on a <b>single</b> <b>test</b> <b>result</b> from specificity and sensitivity data for individual diseases. We also demonstrate two techniques for dealing with unmodeled conditional dependencies in a diagnostic network. These techniques are discussed in the context of an effort to design a portable device for cardiac diagnosis and monitoring from [...] ...|$|E
40|$|Pedestrian {{headform}} {{impact tests}} are generally {{carried out at}} a fixed impact speed, which varies depending on the test protocol in use. Thus, it may be desirable to extrapolate a <b>single</b> <b>test</b> <b>result</b> to higher and lower test speeds. This paper investigates the influence of impact speed on the Head Injury Criterion (HIC) and the peak dynamic displacement. The relationship between impact speed and these test measurements is first considered analytically using a linear spring model, and then empirically using the results of 29 headform tests on seven different locations. The results indicate that power functions {{can be used to}} predict the effect of impact speed, with exponents of approximately 2. 5 for HIC and 0. 8 for peak displacement. These relationships might be used for assessing head impact performance over a wider range of speeds than are presently tested. Daniel J. Searson, Robert W. G. Anderson and T. Paul Hutchinso...|$|E
40|$|As we {{have seen}} in {{previous}} lectures, the technology of DNA chips allows the measurement of mRNA levels simultaneously for thousands of genes. The abundance of readily available data allows us the option to try to extract information, not easily seen while analyzing a <b>single</b> <b>test,</b> from the <b>results</b> of a multitude of <b>tests.</b> The <b>results</b> of DNA chip experiment...|$|R
40|$|This report {{provides}} {{a description of}} the apparatus and the <b>single</b> cell <b>testing</b> <b>results</b> performed at Idaho National Laboratory during January–August 2012. It is an addendum to the Small-Scale Test Report issued in January 2012. The primary program objectives during this time period were associated with design, assembly, and operation of two large experiments: a pressurized test, and a 4 kW test. Consequently, the activities described in this report represent a much smaller effort...|$|R
40|$|Group {{testing has}} been around for over fifty years. Its premise starts in testing blood for {{pathogens}} and abnormalities. Rather than testing individuals, economics motivations suggested testing large collective groups of blood simultaneously. Given its successes in optimizing the number of tests required, group testing methods have been applied to multiple areas of engineering, physical sciences, and social sciences. Group theory requires an understanding of both combinatorial and probabilistic methods and applications. The basis of group testing is to batch together multiple units in a <b>single</b> <b>test,</b> <b>resulting</b> in a minimization on the number of tests required to find all significant units. Obviously, testing everything individually is often inefficient and costly. As such, group theory offers an optimized and less expensive testing method. There are both adaptive algorithms, which alter the units involved in subsequent tests based on the <b>results</b> of previous <b>tests,</b> and static, non-adaptive algorithms. Non-adaptive group testing lends itself to super-imposed coding theory. ...|$|R
40|$|Up to today, {{product safety}} {{evaluation}} in the EU is predominantly based on data/information on their individual ingredients. Consequently, {{the quality and}} reliability of individual ingredient data is of vital interest. In this context, the knowledge about skin sensitization potential is an explicit need for both hazard and risk assessment. Proper skin sensitization data of the individual chemicals is essential, especially when dermal contact is intended, like for cosmetics. In some cases, e. g., {{in the presence of}} irritating chemicals, the combination of individual ingredients may also need to be evaluated to cover possible mixture effects. Today, it seems unlikely or even impossible that skin sensitization in humans can be adequately described by a <b>single</b> <b>test</b> <b>result</b> or even by a simple combination of a few data points (in vivo or in vitro). It is becoming evident that a set of data (including human data and market data) and knowledge about the ingredient’s specific sensitizing potency needs {{to be taken into account}} to enable a reliable assessment of skin sensitization. A more in-depth understanding on mechanistic details of the Adverse-Outcome-Pathway of skin sensitization could contribute key data for a robust conclusion on skin sensitization...|$|E
40|$|A {{reference}} interval {{represents a}} range of values that a physician can use in order to interpret a <b>single</b> <b>test</b> <b>result</b> from a patient. A 95 % reference interval is simply the interval from the 2. 5 th to the 97. 5 th percentiles {{of the distribution of}} the test result. Since such an interval will typically depend on unknown parameters, we can use a random sample to compute an interval that will contain the reference interval with a specified confidence level. The interval so computed is referred to as a central tolerance interval. A central tolerance interval captures, with a given confidence level, a specified percentage of the central part of a univariate population. Reference regions and central tolerance regions are similarly defined for a multivariate population. This thesis considers the problem of deriving central tolerance intervals and regions for some normal populations, including multivariate normal distribution, multivariate linear regression model and the one-way and two-way random models. We also numerically investigate the computation of a simultaneous central tolerance interval for simple linear regression. Simultaneous hypotheses testing of two percentiles is a related problem of interest. The test is usually carried out using two one-sided tests which can be quite conservative. We make an attempt to derive a less conservative test by applying the bootstrap methodology. Numerical examples and real data applications are given to illustrate all of the proposed procedures...|$|E
40|$|Paper {{presented}} at the 56 th Stapp Car Crash Conference, 29 - 31 October 2012, Savannah, GeorgiaCurrent safety testing protocols typically evaluate performance at a single test speed, which may have undesirable side effects if vehicles are optimised to perform at that speed without consideration to performance at other speeds. One way of overcoming this problem is by using an evaluation that incorporates the distribution of speeds that would be encountered in real crashes, the relationship between test speed and test performance, {{and the relationship between}} test performance and injury risk. Such an evaluation is presented in this paper and is applied to pedestrian headform testing. The applicable distribution of pedestrian impact speeds was compiled from in-depth crash data. Values of the Head Injury Criterion across the speed distribution were imputed from a <b>single</b> <b>test</b> <b>result,</b> taking into account the potential for 'bottoming out' on harder structures beneath the hood. Two different risk functions were used: skull fracture risk and fatal head injury risk. Eight example test locations were evaluated; each had an underhood clearance such that it would perform worse at higher speeds than suggested by its original test result. When the effect of bottoming out was included in the evaluation, the calculated average injury risk was generally higher than it was if bottoming out was ignored. The average risk of fatal head injury was more affected by the inclusion of bottoming out than the average skull fracture risk. The methodology presented in this paper may be extended to other forms of impact testing, although the input functions may be more difficult to derive for more complex tests. [URL]...|$|E
50|$|A {{number of}} {{different}} methods are used to evaluate a polymer’s resistance to environmental stress cracking. A common method in the polymer industry is use of the Bergen jig, which subjects the sample to variable strain during a <b>single</b> <b>test.</b> The <b>results</b> of this <b>test</b> indicate the critical strain to cracking, using only one sample. Another widely used test is the Bell Telephone test where bent strips are exposed to fluids of interest under controlled conditions. Current research deals with the application of fracture mechanics {{to the study of}} ESC phenomena.|$|R
40|$|The quasi-modal {{reduction}} {{technique and}} {{finite element model}} (FEM) were used to construct an analytical model for the blade-rotor coupled torsional vibration of a steam turbine generator of the Matuura Power Station. A <b>single</b> rotor <b>test</b> was executed in order to evaluate umbrella vibration characteristics. Based on the <b>single</b> rotor <b>test</b> <b>results</b> and the quasi-modal procedure, the total rotor system was analyzed to predict coupled torsional frequencies. Finally, field measurement of the vibration of the last stage buckets was made, which confirmed that the double synchronous resonance was 124. 2 Hz, meaning that the machine can be safely operated. The measured eigen values {{are very close to}} the predicted value. The <b>single</b> rotor <b>test</b> and this analytical procedure thus proved to be a valid technique to estimate coupled torsional vibration...|$|R
40|$|I. F. 0. 981 The {{concurrence}} of antiphospholipid (aPL) antibodies and thrombosis or {{pregnancy loss}} defines the 'antiphospholipid syndrome' (APS). The Sydney update of the classification criteria for definite APS diagnosis introduced numerous ameliorations {{to the previous}} preliminary consensus statement. Clinical criteria are now better defined as vascular thrombosis must be diagnosed {{on the basis of}} objective criteria. Moreover,additional risk factors for thrombosis or pregnancy loss {{must be taken into account}} before the diagnosis is made and should be described in detail in scientific reports. As far as laboratory criteria are concerned,the lack of standardization and the misinterpretation of results remain major problems often leading to overdiagnosis. A <b>single</b> positive <b>test</b> <b>result</b> out of the possible assays determining aPL antibodies (Lupus Anticoagulant, LAC, anticardiolipin, aCL and anti. beta 2 -Glycoprotein I, beta 2 -GPI, antibodies) is still sufficient,according to the Sydney criteria, to justify a diagnosis of APS. Nevertheless <b>single</b> <b>test</b> positivity may <b>result</b> in overdiagnosis or identification of low risk patients and use of all three tests seems more reasonable. Multiple positivity or (better) triple positivity in our experience allows for the identification of high risk patients for possible recurrence. In the near future, coagulation tests discriminating between a beta 2 -GPI and anti-prothrombin LAC may be useful in identifying high risk patients...|$|R
40|$|The COBAS AMPLICOR CT/NG {{test for}} Neisseria gonorrhoeae cross-reacts with certain strains of nonpathogenic Neisseria species. In some strains, the target {{sequence}} {{is identical to}} that of N. gonorrhoeae, whereas other strains have {{a small number of}} mismatches within the regions recognized by the primers or probe used in the COBAS AMPLICOR NG test. These cross-reactive strains are occasionally present in urogenital specimens, causing false-positive results in the COBAS AMPLICOR NG test. Analysis of the data generated in a large multicenter clinical trial showed that 2. 9 % of the specimens gave signals between A 660 s of 0. 2 and 3. 5 but that one-half of these equivocal specimens did not contain N. gonorrhoeae. Most of these equivocal specimens were correctly classified as true positive or true negative by retesting in duplicate and defining a PCR-positive result as two of three results with an A 660 of ≥ 2. 0. If specimens had been classified as positive or negative based on a <b>single</b> <b>test</b> <b>result</b> using a cutoff of an A 660 of 0. 2, specificity would have ranged from 96. 2 to 98. 9 % depending on specimen type, sex, and presence of symptoms. By employing the equivocal zone-retesting algorithm, specificity increased to 98. 6 to 99. 9 % with little effect (0. 1 to 4. 9 % decrease) on sensitivity in most specimen types, enabling the test to achieve a positive predictive value of at least 90 % in populations with a prevalence of 4 % or higher. In lower-prevalence populations, the test could be used to screen for presumptive infections {{that would have to be}} confirmed by an independent test...|$|E
40|$|Objective: To {{evaluate}} the diagnostic accuracy {{of a single}} CEA (carcinoembryonic antigen) blood test in detecting colorectal cancer recurrence. Background: Patients who have undergone curative resection for primary colorectal cancer are typically followed up with scheduled CEA testing for 5 years. Decisions to investigate further (usually by CT imaging) are based on single test results, reflecting international guidelines. Methods: A secondary analysis was undertaken of data from the FACS trial (two arms included CEA testing). The composite reference standard applied included CT-CAP imaging, clinical assessment and colonoscopy. Accuracy in detecting recurrence was evaluated in terms of sensitivity, specificity, likelihood ratios, predictive values, time-dependent area under the ROC curves, and operational performance when used prospectively in clinical practice are reported. Results: Of 582 patients, 104 (17. 9 %) developed recurrence during the 5 year follow-up period. Applying the recommended threshold of 5 µg/L achieves at best 50. 0 % sensitivity (95 % CI: 40. 1 - 59. 9 %); in prospective use in clinical practice {{it would lead to}} 56 missed recurrences (53. 8 %; 95 % CI: 44. 2 - 64. 4 %) and 89 false alarms (56. 7 % of 157 patients referred for investigation). Applying a lower threshold of 2. 5 µg/L would reduce the number of missed recurrences to 36. 5 % (95 % CI: 26. 5 - 46. 5 %) but would increase the false alarms to 84. 2 % (924 / 1097 referred). Some patients are more prone to false alarms than others – at the 5 µg/L threshold, the 89 episodes of unnecessary investigation were clustered in 29 individuals. Conclusion: Our results demonstrated very low sensitivity for CEA, bringing to question whether it could ever be used as an independent triage test. It is not feasible to improve the diagnostic performance of a <b>single</b> <b>test</b> <b>result</b> by reducing the recommended action threshold because of the workload and false alarms generated. Current national and international guidelines merit re-evaluation and options to improve performance, such as making clinical decisions on the basis of CEA trend, should be further assessed...|$|E
40|$|To {{evaluate}} the diagnostic accuracy {{of a single}} CEA (carcinoembryonic antigen) blood test in detecting colorectal cancer recurrence. Patients who have undergone curative resection for primary colorectal cancer are typically followed up with scheduled CEA testing for 5 years. Decisions to investigate further (usually by CT imaging) are based on single test results, reflecting international guidelines. A secondary analysis was undertaken of data from the FACS trial (two arms included CEA testing). The composite reference standard applied included CT-CAP imaging, clinical assessment and colonoscopy. Accuracy in detecting recurrence was evaluated in terms of sensitivity, specificity, likelihood ratios, predictive values, time-dependent area under the ROC curves, and operational performance when used prospectively in clinical practice are reported. Of 582 patients, 104 (17. 9 %) developed recurrence during the 5 year follow-up period. Applying the recommended threshold of 5 μg/L achieves at best 50. 0 % sensitivity (95 % CI: 40. 1 - 59. 9 %); in prospective use in clinical practice {{it would lead to}} 56 missed recurrences (53. 8 %; 95 % CI: 44. 2 - 64. 4 %) and 89 false alarms (56. 7 % of 157 patients referred for investigation). Applying a lower threshold of 2. 5 μg/L would reduce the number of missed recurrences to 36. 5 % (95 % CI: 26. 5 - 46. 5 %) but would increase the false alarms to 84. 2 % (924 / 1097 referred). Some patients are more prone to false alarms than others-at the 5 μg/L threshold, the 89 episodes of unnecessary investigation were clustered in 29 individuals. Our results demonstrated very low sensitivity for CEA, bringing to question whether it could ever be used as an independent triage test. It is not feasible to improve the diagnostic performance of a <b>single</b> <b>test</b> <b>result</b> by reducing the recommended action threshold because of the workload and false alarms generated. Current national and international guidelines merit re-evaluation and options to improve performance, such as making clinical decisions on the basis of CEA trend, should be further assessed...|$|E
40|$|BACKGROUND: This study aims at obtaining {{unbiased}} {{estimates of}} the sensitivity and specificity of existing screening tests for Trypanosoma cruzi and at simulating the effectiveness of alternative screening strategies at different prevalence rates. STUDY DESIGN AND METHODS: A systematic random sample of 400 was taken from 1200 banked serum samples of donors screened between August 1998 and January 1999 in Santa Cruz, Bolivia. Samples were tested with indirect hemagglutination test (IHA), indirect immunofluorescence assay (IFA), and four enzyme-linked immunosorbent assays (ELISAs). Sensitivity and specificity of tests were estimated through latent class analysis. RESULTS: The sensitivity of individual tests ranged from 96. 5 to 100 percent, and their specificity from 87. 0 to 98. 9 percent. Combinations of two tests used in parallel would, even at 40 percent prevalence, only miss approximately 1 infected unit per 10, 000 screened. At 5 percent prevalence, however, they would yield 75 to 120 false-positive units per 1000 units screened. Parallel testing with IHA plus ELISA or with IHA plus IFA is marginally more cost-effective, compared to <b>single</b> IHA <b>testing,</b> than <b>single</b> ELISA or <b>single</b> IFA <b>testing,</b> regardless of the T. cruzi prevalence. CONCLUSIONS: Routine blood donor screening for T. cruzi with a <b>single</b> <b>test</b> <b>results</b> in unacceptable numbers of false-negative samples in highly endemic areas or in at risk population groups. Adding a second test seems mandatory, but which one to choose depends on local cost components and feasibility...|$|R
40|$|Some of the pogo related {{data from}} STS- 1 are documented. The {{measurements}} and data reduction are described. In {{the data analysis}} reference is made to FRF and <b>single</b> engine <b>test</b> <b>results.</b> The measurements are classified under major project elements {{of the space shuttle}} main engine, the external tank, and the orbiter. The subsystems are structural dynamics and main propulsion. Data were recorded onboard the orbiter with a minimum response rate of 1. 5 to 50 Hz. The wideband, 14 track recorder was used, and the data required demultiplexing before reduction. The flight phase of interest was from liftoff through main engine cutoff...|$|R
40|$|Often {{practical}} {{experiments are}} carried out to analyze production problems. Many of these investigations are conducted to day-to-day production conditions and are not sufficiently coordinated. As a result, these costly experiments often fail or are even wrongly interpreted. A Practise and Process oriented Optimization (PPO) model based on that practical experience is described. The model accommodates a continuous enlargement of processing know-how and supports systematic planning of experiments. Process analysis, process modelling and process oriented Design of Experiments (DoE) are the three basic elements of the model. They {{are connected to the}} existing quality management system. Modelling is used to define the problem of optimization and supports the development of an optimization strategy. Process data and structural interrelations between process steps are determined through process analysis and structured within the process model. The subsequent transfer of interrelations between process parameters to a Design of Experiments results in interlocked DoEs. The outcome of the preceding process steps are communicated to DoEs of connected process steps in the form of experimental factors. In this way <b>single</b> <b>test</b> <b>results</b> can be assessed by looking at the total process. (orig.) Available from TIB Hannover: RR 4682 (18) +a / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekSIGLEDEGerman...|$|R
40|$|Background: Following primary {{surgical}} and {{adjuvant treatment}} for colorectal cancer, many patients are routinely {{followed up with}} blood carcinoembryonic antigen (CEA) testing. Objective: To determine how the CEA test result should be interpreted to inform the decision to undertake further investigation to detect treatable recurrences. Design: Two studies were conducted: (1) a Cochrane review of existing studies describing the diagnostic accuracy of blood CEA testing for detecting colorectal recurrence; and (2) a secondary analysis of data from the two arms of the FACS (Follow-up After Colorectal Surgery) trial in which CEA testing was carried out. Setting and participants: The secondary analysis was {{based on data from}} 582 patients recruited into the FACS trial between 2003 and 2009 from 39 NHS hospitals in England with access to high-volume services offering surgical treatment of metastatic recurrence and followed up for 5 years. CEA testing was undertaken in general practice. Results: In the systematic review we identified 52 studies for meta analysis, including in aggregate 9717 participants (median study sample size 139, interquartile range 72 – 247). Pooled sensitivity at the most commonly recommended threshold in national guidelines of 5 μg/l was 71 % [95 % confidence interval (CI) 64 % to 76 %] and specificity was 88 % (95 % CI 84 % to 92 %). In the secondary analysis of FACS data, the diagnostic accuracy of a single CEA test was less than was suggested by the review [area under the receiver operating characteristic curve (AUC) 0. 74, 95 % CI 0. 68 to 0. 80]. At the commonly recommended threshold of 5 μg/l, sensitivity was estimated as 50. 0 % (95 % CI 40. 1 % to 59. 9 %) and lead time as about 3 months. About four in 10 patients without a recurrence will have at least one false alarm and six out of 10 tests will be false alarms (some patients will have multiple false alarms, particularly smokers). Making decisions to further investigate based on the trend in serial CEA measurements is better (AUC for positive trend 0. 85, 95 % CI 0. 78 to 0. 91), but to maintain approximately 70 % sensitivity with 90 % specificity it is necessary to increase the frequency of testing in year 1 and to apply a reducing threshold for investigation as measurements accrue. Limitations: The reference standards were imperfect and the main analysis was subject to work-up bias and had limited statistical precision and no external validation. Conclusions: The results suggest that (1) CEA testing should not be used alone as a triage test; (2) in year 1, testing frequency should be increased (to monthly for 3 months and then every 2 months); (3) the threshold for investigating a <b>single</b> <b>test</b> <b>result</b> should be raised to 10 μg/l; (4) after the second CEA test, decisions to investigate further should be {{made on the basis of}} the trend in CEA levels; (5) the optimal threshold for investigating the CEA trend falls over time; and (6) continuing smokers should not be monitored with CEA testing. Further research is needed to explore the operational feasibility of monitoring the trend in CEA levels and to externally validate the proposed thresholds for further investigation. Study registration: This study is registered as PROSPERO CRD 42015019327 and Current Controlled Trials ISRCTN 93652154. Funding: The main FACS trial and this substudy were funded by the National Institute for Health Research Health Technology Assessment programme...|$|E
40|$|Ã de conhecimento geral que o desempenho de misturas asfÃlticas com relaÃÃo aos principais defeitos do pavimento (deformaÃÃo permanente, trincamento por fadiga e trincamento tÃrmico) estÃ diretamente ligado Ãs propriedades dos agregados que as constituem. A metodologia tradicional de caracterizaÃÃo de agregados, principalmente em relaÃÃo Ã forma, Ã angularidade e Ã textura, pode se tornar dispendiosa e nÃo fornecer resultados precisos, pois, alÃm de muitas vezes combinar mais de uma propriedade em apenas um ensaio, estes parÃmetros sÃo avaliados a partir de uma mÃdia de valores que pode nÃo representar bem os materiais estudados. Sendo assim, jÃ existem, na literatura, diversas tÃcnicas de Processamento Digital de Imagens (PDI) que podem ser utilizadas com o intuito de caracterizar agregados de forma mais completa e realista. Uma destas tÃcnicas Ã por meio do uso do equipamento Aggregate Image Measurement System (AIMS), desenvolvido para analisar forma, angularidade e textura {{superficial}} de agregados graÃdos e miÃdos, atravÃs de diferentes mÃtodos. Acredita-se que, diferentes pedreiras podem produzir agregados com propriedades distintas entre si devido aos diversos processos de desmonte, de britagem, de controle de qualidade e de armazenamento dos materiais. O objetivo geral do presente trabalho Ã avaliar o potencial do uso do PDI na anÃlise das propriedades de forma, de angularidade e de textura de agregados provenientes de fontes distintas e na anÃlise da estrutura interna de misturas asfÃlticas compostas por esses agregados. Foram obtidos e caracterizados agregados provenientes de trÃs pedreiras distintas, todas localizadas no estado do CearÃ. Os agregados foram analisados no AIMS separando-se os diversos tamanhos de suas partÃculas, e os resultados obtidos para o material proveniente de cada pedreira foram comparados entre si. AlÃm disso, misturas asfÃlticas compostas por esses agregados foram analisadas em relaÃÃo a sua estrutura interna e em relaÃÃo a suas propriedades mecÃnicas. Os principais resultados obtidos levaram Ã conclusÃo de que os agregados estudados sÃo semelhantes no que diz respeito a suas propriedades de forma, de angularidade e de textura. Em relaÃÃo Ãs propriedades mecÃnicas das misturas compostas por esses agregados, os resultados mostram que estas se comportaram de maneira similar, o que provavelmente se deve Ãs caracterÃsticas tambÃm similares de seus agregados. It is {{a general}} consensus that the mechanical performance of asphalt mixtures {{with respect to the}} pavementsâ main distresses (permanent deformation, fatigue cracking and thermal cracking) {{is directly related to the}} aggregatesâ properties used in those mixtures. The traditional methodology of characterizing aggregates, mainly in relation to shape properties, such as form, angularity and texture, can be very time-consuming. It also does not provide precise results, because it combines more than one property in a <b>single</b> <b>test</b> <b>result.</b> Another disadvantage is the fact that these methodologies also provide average values that may not represent well the evaluated materials. There are already in the literature several Digital Image Processing (DIP) techniques that can be used with the purpose of characterizing aggregates in a more complete and realistic way. One of them is the Aggregate Image Measurement System (AIMS), equipment developed to analyze properties, such as: shape, angularity, and superficial texture of coarse and fine aggregates, through different methods. It is believed that different quarries can produce aggregates with very distinct properties, due to the several used rock blasting and crushing processes, quality control activities and material storage methodology. The main objective of this research is to evaluate the potential use of DIP to analyze shape, angularity and texture of aggregates from different sources and to analyze asphalt mixtures internal structure constituted by those aggregates. Aggregates from three different quarries, all located in the State of CearÃ, were obtained and characterized. The aggregates were analyzed using AIMS, for different sizes, and the results were compared. Besides that, the asphalt mixtures constituted with these aggregates were analyzed regarding their internal structure and their mechanical characteristics. The main results obtained in this study led to conclude that the aggregates used in this research are very similar in respect to their form, angularity and texture properties. With respect to the mechanical properties of the mixtures constituted by these aggregates, the results show that their behavior is also very similar, which is probably due to the similar characteristics of their aggregates...|$|E
40|$|Sediment {{toxicity}} tests play {{an important}} role in prospective risk assessment for organic chemicals. This review describes sediment toxicity tests for microorganisms, macrophytes, benthic invertebrates and benthic communities. Current approaches in sediment toxicity testing are fragmentary and diverse. This hampers the translation of <b>single</b> species <b>test</b> <b>results</b> between freshwater, estuarine and marine ecosystems and to the population and community levels. A more representative selection of species and endpoints as well as a unification of dose metrics and exposure assessment methodologies across groups of test species, constitutes a first step towards a balanced strategy for sediment toxicity <b>testing</b> of <b>single</b> organic compounds in the context of prospective risk assessment...|$|R
40|$|SRRC is {{currently}} developing a RF kicker for the lon-gitudinal feedback system. The kicker is a pill-box cavity with nine pieces of striplines. The resonant frequency is {{tuned to the}} designing value 1125 MHz by adjusting {{the length of the}} striplines. Excited by a <b>single</b> stripline, cold <b>test</b> <b>results</b> indicate that the full 3 dB bandwidth exceeds 250 MHz and the shunt impedance deduced from bead-pull measurement is about 80 Ω...|$|R
40|$|The {{development}} of the first multiple material stereolithography (MMSL) system to the author 2 ̆ 7 s knowledge, as a retrofit of a multiple vat carousel assembly into an existing sterolithography (SL) system based on an existing patent application filed by Wicker et al. 2004, is presented along {{with a description of}} the retrofitted MMSL system operation. Use of three different photocrosslinkable epoxy-based resins to demonstrate the MMSL system capabilities, by means of customized multiple material tensile testing samples manufacturing, is also discussed, in an effort to characterize multiple material normal and shear bonding strengths through tensile <b>testing.</b> <b>Single</b> material <b>testing</b> <b>results</b> of customized sample geometries built on different orientations are included as well, providing supportive background on mechanical testing fracture phenomena characterization in both multiple material and single material samples. ...|$|R
40|$|Exhaust {{pollutant}} emissions were measured from single swirl-can combustor modules operating over a pressure range of 69 to 276 N/sq cm (100 to 400 psia), over a fuel-air ratio range of 0. 01 to 0. 04, at an {{inlet air temperature}} of 733 K (860 F), and at a constant reference velocity of 23. 2 m/sec). Many swirl-can module designs were evaluated; the 11 most promising designs exhibited oxides of nitrogen emission levels lower than that from conventional gas-turbine combustors. Although these <b>single</b> module <b>test</b> <b>results</b> are not necessarily indicative of the performance characteristics of a large array of modules, the results are very promixing and offer a number of module designs that should be tested in a full combustor...|$|R
40|$|In modern SoCs, {{embedded}} memories {{occupy the}} largest {{part of the}} chip area and include an even larger amount of active devices. As memories are designed very tightly {{to the limits of}} the technology they are more prone to failures than logic. Thus, memories concentrate the large majority of defects and affect circuit yield dramatically. As a matter Built-In Self-Repair is gaining importance. This work presents optimal reconfigurations functions for memory built-in self-repair on the data-bit level. We also present a dynamic repair scheme that allows reducing the size of the repairable units. The combination of these schemes allows repairing multiple faults affecting both regular and spare units, by means of low hardware cost. The scheme uses a <b>single</b> <b>test</b> pass, <b>resulting</b> on low <b>test</b> and repair time...|$|R
40|$|SUMMARY Two hundred {{patients}} taking varying L-thyroxine replacement doses {{were studied}} using a normal TRH test as {{the index of}} optimal replacement dose. The mean optimal dose was 141 flgfday. Normal serum T a and Ff:J were found in most patients, whatever the TRH response, and they are probably too unspecific. Serum T 4 and F'I' 41 were elevated in many patients with a normal TRH response. A higher range for Ff 41 of 102 - 166, although only 66 · 5 %accurate, gave the best index of optimal L-thyroxine replacement of the <b>single</b> in-vitro <b>tests.</b> <b>Results</b> The patients were divided into three groups according to TRH response TRH (+) 58 patients showed an exaggerated TRH response,> 20 mUll. All but three of this group also showed an elevated baseline TSH...|$|R
40|$|A {{sample of}} 3 dairy herds {{served by the}} same private {{veterinary}} practice was {{selected on the basis}} of similarity of size, calf-rearing practices and owners cooperation. All animals were tested for paratuberculosis 3 or 4 times at 6 -month intervals by faecal culture, (ELISA), agar-gel immunodiffusion (AGID) serology. Faecal-culture results were reported to herd owners through their veterinary practitioner. Production data and somatic-cell counts were collected from 2 of the herds using DHI (Dairy Herd Improvement, Powell, OH) records. Sensitivity and specificity relative to faecal culture of <b>single</b> <b>test</b> <b>results</b> were 71 % and 83 % for ELISA and 38 % and 100 % for AGID, respectively. Repeated tests gave sensitivity and specificity of 73 % and 61 % for ELISA and 33 % and 100 % for AGID when tests were interpreted in parallel testing. A maximum-likelihood procedure was used to generate estimates of sensitivity and specificity of single (91 % and 82 %) and repeated (73 % and 66 %) ELISA testing. There were no significant differences by sign test between faecal culture and ELISA in the time period in which infection was first identified. Faecal culture detected infection earlier than AGID. Mean milk production (controlling for differences in somatic-cell counts) was 3275 lb per lactation or 18. 8 % higher in faecal culture-negative cows than in faecal culture-positive cows. No associations were found between milk production and ELISA results [...] RE: 27 ref.; SC: BE; VE; CA; 0 I; ZA; 0 V; 0 DSource type: Electronic(1) [URL]...|$|R
40|$|International audienceAutism is {{the short}} {{name of a}} complex and {{heterogeneous}} group of disorders (autism spectrum disorders, ASD) with several lead symptoms required for classification, including compromised social interaction, reduced verbal communication and stereotyped repetitive behaviors/restricted interests. The etiology of ASD is still unknown in most cases but monogenic heritable forms exist that have provided insights into ASD pathogenesis and {{have led to the}} notion of autism as a 'synapse disorder'. Among the most frequent monogenic causes of autism are loss-of-function mutations of the NLGN 4 X gene which encodes the synaptic cell adhesion protein neuroligin- 4 X (NLGN 4 X). We previously described autism-like behaviors in male Nlgn 4 null mutant mice, including reduced social interaction and ultrasonic communication. Here, we extend the phenotypical characterization of Nlgn 4 null mutant mice to both genders and add a series of additional autism-relevant behavioral readouts. We now report similar social interaction and ultrasonic communication deficits in females as in males. Furthermore, aggression, nest-building parameters, as well as self-grooming and circling as indicators of repetitive behaviors/stereotypies were explored in both genders. The construction of a gender-specific autism severity composite score for Nlgn 4 mutant mice markedly diminishes population/sample heterogeneity typically obtained for <b>single</b> <b>tests,</b> <b>resulting</b> in p values of 83 % for female mice. Taken together, these data underscore the similarity of phenotypical consequences of Nlgn 4 /NLGN 4 X loss-of-function in mouse and man, and emphasize the high relevance of Nlgn 4 null mutant mice as an ASD model with both construct and face validity...|$|R
40|$|NUMBER OF PAGES: (xliv+ 1199 +x+ 305 suppl.) In modern SoCs, {{embedded}} memories {{occupy the}} largest {{part of the}} chip area and include an even larger amount of active devices. As memories are designed very tightly {{to the limits of}} the technology, they are more prone to failures than logic. Thus, memories concentrate the large majority of defects and affect circuit yield dramatically. Hence, built-in self-repair is gaining importance. This work presents optimal reconfiguration functions for memory built-in self-repair on the data-bit level. We also present a dynamic repair scheme that allows a reduction {{of the size of the}} repairable units. The combination of these schemes allows repairing multiple faults affecting both regular and spare units, by means of low hardware cost. The scheme uses a <b>single</b> <b>test</b> pass, <b>resulting</b> on low <b>test</b> and repair time...|$|R
40|$|Self-Test {{strategies}} for testing embedded processors are increasingly diffused. In this paper, we describe {{a set of}} self-test techniques tackling dual issue embedded processors. The paper details how to produce test programs suitable to detect stuck-at faults in computational modules belonging to dual issue processors. The proposed technique is aimed at extending <b>single</b> issue <b>test</b> programs; <b>results</b> are illustrated for a 32 -bit processor included in an automotive System-on-Chip manufactured by STMicroelectronics and implementing a dual issue strategy with static dispatch of instructions...|$|R
