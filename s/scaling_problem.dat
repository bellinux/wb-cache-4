272|5291|Public
50|$|Earlier P2P networks, such as Napster, used a {{centralized}} database to locate files. This {{does not have}} a <b>scaling</b> <b>problem,</b> but the central server is a single point of failure.|$|E
50|$|Geostatistical {{inversion}} procedures {{detect and}} delineate thin reservoirs otherwise poorly defined. Markov chain Monte Carlo (MCMC) based geostatistical inversion addresses the vertical <b>scaling</b> <b>problem</b> by creating seismic derived rock properties with vertical sampling compatible to geologic models.|$|E
5000|$|An {{alternative}} model {{considers a}} spring-like force for every pair of nodes [...] where the ideal length [...] of each spring {{is proportional to}} the graph-theoretic distance between nodes i and j, without using a separate repulsive force. Minimizing the difference (usually the squared difference) between Euclidean and ideal distances between nodes is then equivalent to a metric multidimensional <b>scaling</b> <b>problem.</b>|$|E
30|$|Also, {{algorithms}} {{with lower}} complexity {{are easier to}} be extended to large <b>scale</b> <b>problems.</b> For small <b>scale</b> <b>problems,</b> algorithms with low or high complexity do not differ much from each other. However as the problem dimension grows, high complexity algorithms can be very very slow for large <b>scale</b> <b>problems</b> because of too much computing time, while algorithms with lower complexity are more preferable.|$|R
40|$|The {{maintenance}} scheduling {{problem has been}} previously tackled by various traditional optimisation techniques. While these methods can give an optimal solution to small <b>scale</b> <b>problems,</b> they are often inefficient when applied to larger <b>scale</b> <b>problems.</b> The memetic algorithm presented here is essentially a genetic algorithm with an element of local search. The effectiveness of the method is tested through its application to real <b>scale</b> <b>problems...</b>|$|R
40|$|Multiple <b>scale</b> {{homogenization}} <b>problems</b> {{are reduced}} to single <b>scale</b> <b>problems</b> in higher dimension. It is shown that sparse tensor product Finite Element Methods (FEM) allow the numerical solution in complexity independent of the dimension and of the length <b>scale.</b> <b>Problems</b> with stochastic input data are reformulated as high dimensional deterministic problems for the statistical moments of the random solution. Sparse tensor product FEM give a deterministic solution algorithm of log-linear complexity for statistical moments...|$|R
50|$|The Virtual Layer-3 Switch {{is useful}} to make a {{large-scale}} site-to-site VPN network. Although the easy {{way to make a}} site-to-site VPN network is to build the layer-2 bridging based VPN, if the number of computers is huge the number of broadcasting packets will increase to load the inter-site links. To prevent that <b>scaling</b> <b>problem,</b> the VPN administrator isolates IP networks by Virtual Layer-3 switch.|$|E
50|$|A noninvasive {{method to}} measure endothelial {{dysfunction}} is % Flow Mediated Dilation (FMD) {{as measured by}} Brachial Artery Ultrasound Imaging (BAUI). Current measurements of endothelial function via FMD vary due to technical and physiological factors. A negative correlation between percent flow mediated dilation and baseline artery size is recognised as a fundamental <b>scaling</b> <b>problem,</b> leading to biased estimates of endothelial function. For research on FMD an ANCOVA approach to adjusting FMD for variation in baseline diameter is more appropriate. Another challenge of FMD is variability across centers and the requirement of highly qualified technicians to perform the procedure.|$|E
40|$|Clustering {{data has}} been of great {{interest}} to many researchers. Hierarchical clustering methods have been preferred because clusters can be visualized as a dendrogram. One of the problems of hierarchical clustering methods, however, is that the resulting dendrogram is not visually pleasing due to the <b>scaling</b> <b>problem.</b> Hence, a series of iterated logarithmic function is proposed so as to mitigate the <b>scaling</b> <b>problem.</b> Theoretical properties of the iterated logarithmic function are presented. I...|$|E
40|$|In Egypt, {{there has}} been a major effort to improve the water supply {{especially}} for groundwater. Applications of numerical models to field <b>scale</b> <b>problems</b> require large grids which are not adequate with actual size of well diameter. Several methods have been used to simulate head at the well scale but these methods have limitations, especially for large <b>scale</b> <b>problems.</b> The paper presents a hierarchical approach for groundwater modeling which enables converting a large, complex problem into a network of smaller models that can be easily solved. The study aimed to provide an accurate representation of head in large <b>scale</b> <b>problems...</b>|$|R
50|$|Parents {{ratings are}} {{converted}} to a total score and sub-scale scores. The cut-offs for the total score, internalizing <b>problems</b> <b>scale,</b> externalizing <b>problems</b> <b>scale,</b> and attention <b>problems</b> <b>scale</b> are 15, 5, 7, and 7, respectively. As on the PSC, a score above the cutoff is not diagnostic; it indicates only that further evaluation is needed.|$|R
50|$|However, such {{experiments}} {{suffer from}} several optical <b>scaling</b> <b>problems</b> as {{the size of}} the experiments are increased.|$|R
40|$|In {{this paper}} first we prove four {{fundamental}} theorems of the alternative, called scaling dualities, characterizing exact and approximate solvability of four significant conic problems in finite dimensional spaces, defined as: homogeneous programming (HP), <b>scaling</b> <b>problem</b> (SP), homogeneous <b>scaling</b> <b>problem</b> (HSP), and algebraic <b>scaling</b> <b>problem</b> (ASP). Let # be a homogeneous function of degree p > 0, K a pointed closed convex cone, W a subspace, and F a #-logarithmically homogeneous barrier for K #. HP tests {{the existence of}} a nontrivial zero of # over W # K. SP, and HSP test the existence of the minimizer of # = # + F, and X = #/ exp(-pF/#) over W # K #, respectively. ASP tests the solvability of the scaling equation (SE), a fundamental equation inherited from properties of homogeneity and those of the operator-cone, T (K) = {D # F ## (d) - 1 / 2 : d # K # }. Each D induces a scaling of # # (d) (or # ## (d)), and SE is solvable if and only if there exists a fixed-point under t [...] ...|$|E
40|$|The test {{we develop}} expresses the null {{hypothesis}} in terms of proximity {{of the distribution of}} a Markov chain (yt) to the subspace of homogeneous Markov chains. The distance we use is the Kullback distance which turns out to be conceptually appropriate. Departure from the point null hypothesis allows us to formulate the question of interest in meaningful terms, but implementing this approach comes up against a <b>scaling</b> <b>problem.</b> In this paper, we propose a new approach in order to solve this <b>scaling</b> <b>problem</b> by formulating the proximity to homogeneity {{as a percentage of the}} maximum distance to. Entropy Homogeneity Kullback-Leibler distance Longitudinal data Proximity Test...|$|E
40|$|International audienceRecently, optimal {{combinatorial}} algorithms {{have been}} presented for the energy minimization multi-processor speed <b>scaling</b> <b>problem</b> with migration [Albers et al., SPAA 2011], [Angel et al., Euro-Par 2012]. These algorithms are based on repeated maximum-flow computations allowing the partition of the set of jobs into subsets {{in which all the}} jobs are executed at the same speed. The optimality of these algorithms is based on a series of technical lemmas showing that this partition and the corresponding speeds lead to the minimization of the energy consumption. In this paper, we show that both the algorithms and their analysis can be greatly simplified. In order to do this, we formulate the problem as a convex cost flow problem in an appropriate flow network. Furthermore, we show that our approach is useful to solve other problems in the dynamic speed scaling setting. As an example, we consider the preemptive openshop speed <b>scaling</b> <b>problem</b> and we propose a polynomial-time algorithm for finding an optimal solution based on the computation of convex cost flows. We also propose a polynomial-time algorithm for minimizing a linear combination of the sum of the completion times of the jobs and the total energy consumption, for the multi-processor speed <b>scaling</b> <b>problem</b> without preemptions. Instead of using convex cost flows, our algorithm is based on the computation of a minimum weighted maximum matching in an appropriate bipartite graph...|$|E
30|$|The average {{running time}} of GA and SA are much shorter than ACS in large <b>scale</b> <b>problems</b> {{according}} to Tables  2 and  3. The running time of LPT, LS and LDF are quite small even in large <b>scale</b> <b>problem.</b> In Table  4, {{the values of}} meta-heuristic algorithms in μ = 0.8 and μ = 0.9 present the average running time merged by μ without “early exit” mechanism and SA is the best.|$|R
40|$|AbstractIn this paper, {{instead of}} using the Hinge loss in {{standard}} support vector machine, we introduce a weighted linear loss function and propose a weighted linear loss support vector machine (WLSVM) for large <b>scale</b> <b>problems.</b> The main characteristics of our WLSVM are: (1) by adding the weights on linear loss, the training points in the different positions are proposed to give different penalties, avoiding over-fitting {{to a certain extent}} and yielding better generalization ability than linear loss. (2) by only computing very simple mathematical expressions to obtain the separating hyperplane, the large <b>scale</b> <b>problems</b> can be easy dealt. All experiments on synthetic and real data sets show that our WLSVM is comparable to SVM and LS-SVM in classification accuracy but with needs computation time, especially for large <b>scale</b> <b>problems...</b>|$|R
3000|$|... [...]. Thus, {{we avoid}} <b>scaling</b> <b>problems</b> that can {{arise from the}} number of frames, the number of {{frequency}} bins or the number of components considered.|$|R
40|$|We {{consider}} convex programming {{problems in}} a canonical homogeneous format, a very general form of Karmarkar’s canonical linear programming problem. More specifically, by homogeneous programming we shall refer {{to the problem of}} testing if a homogeneous convex function has a nontrivial zero over a subspace and its intersection with a pointed convex cone. To this canonical problem, endowed with a normal barrier for the underlying cone, we associate dual problems and prove several matrix scaling dualities. We make use of these scaling dualities to derive new and conceptually simple potential-reduction and path-following algorithms, applicable to self-concordant homogeneous programming, as well as three dual problems defined as: the <b>scaling</b> <b>problem,</b> the homogeneous <b>scaling</b> <b>problem,</b> and the algebraic <b>scaling</b> <b>problem.</b> The simplest of the scaling dualities is the following equivalent of the classic separation theorem of Gordan: a positive semidefinite symmetric matrix Q either has a nontrivial nonnegative zero, or there exists a positive definite diagonal matrix D such that DQDe> 0, where e is the vector of ones. This duality is a key ingredient in the very simple path-following algorithm of Khachiyan and Kalantari for linear programming, as well as for quasi doubly stochastic scaling of Q, i. e. computing D such that DQDe = e. Our general results here give nontrivial extensions of our previous work on the role of matrix scaling in linear o...|$|E
40|$|The {{chemistry}} of geothermal, production, and injection fluids at the Dixie Valley Geothermal Field, Nevada, was characterized to address an ongoing <b>scaling</b> <b>problem</b> and {{to evaluate the}} effects of reinjection into the reservoir. Fluids generally followed mixing-dilution trends. Recharge to the Dixie Valley system apparently originates from local sources. The low-pressure brine and injection waters were saturated with respect to amorphous silica, which correlated with the ongoing <b>scaling</b> <b>problem.</b> Local shallow ground water contains about 15 % geothermal brine mixed with regional recharge. The elevated Ca, Mg, and HCO{sub 3 } content of this water suggests that carbonate precipitation may occur if shallow groundwater is reinjected. Downhole reservoir fluids are close to equilibrium with the latest vein mineral assemblage of wairakite-epidote-quartz-calcite. Reinjection of spent geothermal brine is predicted to affect the region near the wellbore differently than it does the region farther away...|$|E
40|$|In this paper, {{we propose}} {{to use the}} scaling {{ambiguity}} of convolutive blind source separation for shortening the unmixing filters. An often used approach for separating convolutive mixtures is the transformation to the time-frequency domain where an instantaneous ICA algorithm can be applied for each frequency separately. This approach leads to the so called permutation and scaling ambiguity. While different methods for the permutation problem have been widely studied, the solution for the <b>scaling</b> <b>problem</b> is usually based on the minimal distortion principle. We propose an alternative approach that allows the unmixing filters to be as short as possible. Shorter unmixing filters will suffer less from circular-convolution effects that are inherent to unmixing approaches based on binwise ICA followed by permutation and scaling correction. The results for the new algorithm will be shown on a realworld example. Index Terms — Blind source separation, convolutive mixture, frequency-domain ICA, <b>scaling</b> <b>problem...</b>|$|E
30|$|Beside the {{aforementioned}} problems, PSO algorithms {{also have been}} applied to other problems, such as large <b>scale</b> <b>problems</b> [15, 16], dynamic multimodal optimization problems [88], etc.|$|R
40|$|Several {{population}} initialization {{methods for}} evolutionary algorithms (EAs) {{have been proposed}} previously. This paper categorizes the most well-known initialization methods and studies the effect of them on large <b>scale</b> global optimization <b>problems.</b> Experimental {{results indicate that the}} optimization of large <b>scale</b> <b>problems</b> using EAs is more sensitive to the initial population than optimizing lower dimensional problems. Statistical analysis of results show that basic random number generators, which are the most commonly used method for population initialization in EAs, lead to the inferior performance. Furthermore, our study shows, regardless {{of the size of the}} initial population, choosing a proper initialization method is vital for solving large <b>scale</b> <b>problems...</b>|$|R
40|$|Abstract—Applications must scale well to make {{efficient}} use of today’s class of petascale computers, which contain {{hundreds of thousands of}} processor cores. Inefficiencies that do not even appear in modest-scale executions can become major bottlenecks in large-scale executions. Because <b>scaling</b> <b>problems</b> are often difficult to diagnose, there is a critical need for scalable tools that guide scientists to the root causes of <b>scaling</b> <b>problems.</b> Load imbalance {{is one of the most}} common <b>scaling</b> <b>problems.</b> To provide actionable insight into load imbalance, we present post-mortem parallel analysis techniques for pinpointing and quantifying load imbalance in the context of call path profiles of parallel programs. We show how to identify load imbalance in its static and dynamic context by using only low-overhead asyn-chronous call path profiling to locate regions of code responsible for communication wait time in SPMD executions. We describe the implementation of these techniques within HPCTOOLKIT. I...|$|R
40|$|A trainable {{recurrent}} neural network, Simultaneous Recurrent Neural network, {{is proposed}} {{to address the}} <b>scaling</b> <b>problem</b> faced by neural network algorithms in static optimization. The proposed algorithm derives its computational power to address the <b>scaling</b> <b>problem</b> through its ability to &quot;learn &quot; compared to existing recurrent neural algorithms, which are not trainable. Recurrent backpropagation algorithm is employed to train the recurrent, relaxation-based neural network in order to associate fixed points of the network dynamics with locally optimal solutions of the static optimization problems. Performance of the algorithm is tested on the NP-hard Traveling Salesman Problem {{in the range of}} 100 to 600 cities. Simulation results indicate that the proposed algorithm is able to consistently locate high-quality solutions for all problem sizes tested. In other words, the proposed algorithm scales demonstrably well with the problem size with respect to quality of solutions and at the expense of increased computational cost for large problem sizes...|$|E
40|$|An {{enhanced}} solution {{strategy for}} the SIMPLER algorithm is presented for low pressure heat and mass transport calculations with applications in material processing. The accurate solution of highly diffusive flows requires an inflow boundary condition that preserves chemical species mass fluxes. The flux-preserving inflow boundary condition contains a <b>scaling</b> <b>problem</b> that causes the species equations to converge very slowly when using the standard SIMPLER algorithm. A gradient algorithm, coupled to a line-relaxation method, accelerates the convergence of the linear problem. Reformulation of the pressure-correction boundary conditions ensures that continuity is preserved in each finite volume at each iteration. The boundary condition <b>scaling</b> <b>problem</b> is demonstrated with a simple linear model problem. The enhanced solution strategy is implemented in a baseline computer code {{that is used to}} solve the multicomponent Navier-Stokes equations on a generalized, multiple-block grid system. Convergence rate acceleration factors of up to 100 are demonstrated for several material processing example problems...|$|E
40|$|Rendered {{stereoscopic}} scenes {{suffer from}} a <b>scaling</b> <b>problem,</b> which causes major restrictions on their portability to other stereoscopic display mediums. A main solution for this problem is presented, {{as well as other}} complementary solutions. Also, the basics of depth perception are presented, and some insight on the current three-dimensional displays used. This work was supported by identification of grant or project. Copies of this report are available o...|$|E
40|$|Grids - the {{collection}} of heterogeneous computers spread across the globe - present a new paradigm for the large <b>scale</b> <b>problems</b> in variety of fields. We discuss two representative cases {{in the area of}} condensed matter physics outlining the widespread applications of the Grids. Both the problems involve calculations based on commonly used Density Functional Theory and hence can be considered to be of general interest. We demonstrate the suitability of Grids for the problems discussed and provide a general algorithm to implement and manage such large <b>scale</b> <b>problems...</b>|$|R
40|$|In this paper, group {{scheduling}} problem in no-wait flexible flowshop {{is considered by}} considering two stages with group sequence-dependent setup times and random breakdown of the machines. Genetic algorithm and simulated annealing based heuristics have been proposed to solve the problem. The primary objective of scheduling is to minimize the maximum completion time of the jobs for two classes of small and large <b>scale</b> <b>problems.</b> Computational results show that both GA and SA algorithms perform properly, but SA appeared to provide better results for both small and large <b>scale</b> <b>problems...</b>|$|R
40|$|A {{proposed}} {{approach is}} specialized for large <b>scale</b> vehicle routing <b>problems</b> (VRPs) {{and based on}} area segmentation and gradual area integration mechanisms {{so as to avoid}} combinatorial explosion. The purpose of the proposed approach is to deconstruct large <b>scale</b> <b>problem</b> into small size sub-problems and gradually restore these to original state. Firstly, an original large <b>scale</b> <b>problem</b> is divided into some small sub-areas and optimal solutions in each sub-area are derived. When a best incumbent solution remains unchanged for a certain period, sub-areas are gradually integrated and new optimal solutions in a new integrated sub-area are newly searched through use of the obtained solutions in previous sub area. This gradual integration and optimization are iterated until every sub-area are integrated into the one (the original problem), and the optimal solution of original problem can be obtained at this time. The proposed approach aims to deconstruct large <b>scale</b> <b>problem</b> into small size sub-problems and perform more efficient search. Through some typical test problems, it was demonstrated that our approach could derive better results more effectively than conventional approach. ROMBUNNO. SS 15 -...|$|R
40|$|AbstractA {{scaling of}} a nonnegative matrix A is a matrix having the form A′ = UAV where U and V are square {{diagonal}} matrices which have positive diagonal elements. If the matrix A is square and V = U− 1, we call A' a symmetric scaling of A. We consider two problems: the first concerns {{the identification of}} a scaling of a given nonnegative matrix with prescribed row and column products; the second concerns the problem of finding a symmetric scaling of a given nonnegative square matrix whose row products equal the corresponding column products. For each of these two scaling problems we characterize the solutions {{in terms of a}} nonlinear convex optimization problem, we use the characterization to demonstrate that feasibility of either <b>scaling</b> <b>problem</b> is equivalent to the existence of a matrix satisfying the target property and having the same pattern as the given matrix, we establish uniqueness of solutions to either <b>scaling</b> <b>problem</b> whenever it is feasible, and we develop algorithms for computing the desired (unique) scalings in the cases where the problem are feasible...|$|E
40|$|We develop several {{efficient}} algorithms for {{the classical}} Matrix <b>Scaling</b> <b>problem,</b> {{which is used}} in many diverse areas, from preconditioning linear systems to approximation of the permanent. On an input n× n matrix A, this problem asks to find diagonal (scaling) matrices X and Y (if they exist), so that X A Y ε-approximates a doubly stochastic, or more generally a matrix with prescribed row and column sums. We address the general <b>scaling</b> <b>problem</b> {{as well as some}} important special cases. In particular, if A has m nonzero entries, and if there exist X and Y with polynomially large entries such that X A Y is doubly stochastic, then we can solve the problem in total complexity Õ(m + n^ 4 / 3). This greatly improves on the best known previous results, which were either Õ(n^ 4) or O(m n^ 1 / 2 /ε). Our algorithms are based on tailor-made first and second order techniques, combined with other recent advances in continuous optimization, which may be of independent interest for solving similar problems...|$|E
40|$|Klaipeda Geothermal Pilot Plant, {{operating}} since 2000, {{suffers from}} permanent decrease of injectivity of spent geothermal water. Among plenty and various causes of injectivity failures {{the most important}} ones are associated with <b>scaling</b> <b>problem,</b> because geothermal water is supersaturated by many minerals, salts and ions. A geochemical model PHREEQC {{has been used for}} scaling problems simulation, two other models (flow&transport) predicted scale of spread of injected geothermal water in the pumped aquifer...|$|E
40|$|This {{investigation}} was undertaken {{to determine the}} usefulness of interval analysis to numerical integration and matrix inversion techniques and to combine these results to determine the value of interval analysis in bounding computational errors in the two-body problem. Conclusions were that interval analysis may be worthwhile in certain small <b>scale</b> isolated <b>problems,</b> but its usefulness in any large <b>scale</b> <b>problem</b> is doubtful...|$|R
40|$|Investigating {{genotype}} by environment interactions (GxE) {{is generally}} considered challenging due to the scale dependency of the interaction effect. The present paper illustrates {{the problems associated with}} testing for GxEs on summed item scores within the well-known ACE model. That is, it is shown how genuine GxEs may be masked and how spurious interactions can arise from scaling issues in the data. A solution is proposed which explicitly distinguishes between a measurement model for the ordinal item responses and a biometric model in which the GxE effects are investigated. The new approach is studied in a simulation study using both a scenario in which the measurement instrument suffers from mild <b>scaling</b> <b>problems</b> and a scenario in which the measurement instrument suffers from severe <b>scaling</b> <b>problems.</b> Results indicate that the severity of the <b>scale</b> <b>problems</b> affects the power to detect GxE, but it rarely results in false positives. We illustrate the new approach on a real dataset concerning affect. © 2014 Springer Science+Business Media New York...|$|R
40|$|This paper {{shows that}} the {{generalized}} finite element method with global–local enrichment functions (GFEMgl) can be implemented non-intrusively in existing closed-source FEM software as an add-on module. The GFEMgl {{is based on the}} solution of interdependent global (structural) and local (crack) <b>scale</b> <b>problems.</b> In the approach presented here, an initial global <b>scale</b> <b>problem</b> is solved by a commercial finite element analysis software, local problems containing 3 -D fractures are solved by an hp-adaptive GFEM software and an enriched global <b>scale</b> <b>problem</b> is solved by a combination of the FEM and GFEM softwares. The interactions between the solvers are limited to the exchange of load and solution vectors and does not require the introduction of user subroutines to existing FEM software. As a results, the user can benefit from built-in features of available commercial grade FEM software while adding the benefits of the GFEM for this class of problems. Several threedimensional fracture mechanics problems aimed at investigating the applicability and accuracy of the proposed two-solver methodology are presented...|$|R
