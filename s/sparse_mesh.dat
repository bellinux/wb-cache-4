14|19|Public
40|$|Nanoscale CMOS {{technologies}} are posing new network on chip concepts to IC designers. However, {{the electronic network}} on chip design faces many problems like energy consumption, long delay and limited bandwidth. Hence, optical network on chip appears as a good candidate to solve these problems. The advances in nanophotonic technology make it more realistic. A new <b>sparse</b> <b>mesh</b> is proposed for optical network on chip. Two types of non-blocking optical node architecture are also proposed to build up core node and switch node. The new architecture fully utilizes the property of XY routing in 2 D mesh network, thus saving the number of microring resonators used. The comparisons are made with traditional mesh in number of microring resonators, loss and energy. The {{results show that the}} proposed <b>sparse</b> <b>mesh</b> achieves the best in all the aspects. For example, it uses 68 % less number of resonators than the traditional mesh. We simulated 2 D <b>sparse</b> <b>mesh</b> optical network on chip, and showed network performance under different traffic loads and data sizes. The results show <b>sparse</b> <b>mesh</b> achieves lower average delay and higher throughput than the traditional mesh. © 2008 SPIE...|$|E
40|$|Marine controlled-source {{electromagnetic}} (CSEM) {{has been}} in commercial use for offshore hydrocarbon exploration for fifteen years. It is significant for geologists and geophysicists to design and interpret a marine EM survey for hydrocarbon exploration. SeaBed Logging (SBL) and towed streamer EM (TSEM) {{are the most popular}} data acquisition systems. They have their own advantages on acquisition. We compare the two different data acquisition systems by using 1 D sensitivity modelling and 2 D inline synthetic data inversion. In these studies, we test the effects of frequency, range, water depth, target dimensions and reservoir burial depth on the detectable capbility of the two acquisition systems. The relationship between the detectable hydrocarbon reservoir depth and water depth is discussed by anomalous transverse resistance (ATR) ratio, which is used to easily evaluate our inversion results. Moreover, the detectability is described for the two data acquisition systems in different water depth. Apparently, the SBL system has an advantage in deep water environment, but the TSEM has a similar sensitivity to the target with the SBL system in shallow water situation. We develop an irregular <b>sparse</b> <b>mesh</b> to enhance both the speed and resolution of CSEM inversion by introducing structural geological information in the inversion algorithm. This <b>sparse</b> <b>mesh</b> is defined as a coherence-based irregular (IC) <b>sparse</b> <b>mesh,</b> which is based on vertices extracted from available geological information. Synthetic data inversion examples illustrate that the IC <b>sparse</b> <b>mesh</b> has a smaller inversion computational cost compared to the regular dense mesh. Additionally, the IC <b>sparse</b> <b>mesh</b> reduces the computational cost of the matrix operation for model updates. It also has a higher resolution than with a regular <b>sparse</b> <b>mesh</b> for the same number of estimated parameters. Based on these IC sparse meshes, we propose an image-guided smoothing regularization method in the inversion of marine electromagnetic data. In order to enhance the resolution of marine EM inversion, incorporating seismic constraints into EM inversion is an effective approach. Compared to traditional regular dense mesh, a coherence-based irregular <b>sparse</b> <b>mesh</b> reduces computational cost. The image-guided regularization represents an improvement of regularization and also uses the structure taken from a seismic image. In this thesis, we show that this regularization can improve the results of EM inversions with irregular sparse meshes. The image-guided regularized inversion can be applied to marine CSEM data and MT data, especially, it is able to be used for joint inversion of CSEM and MT data. Both synthetic and real data inversion examples presented in this thesis demonstrate that the proposed methods improve the quality of the resistivity image...|$|E
40|$|We {{present a}} new algorithm, Sparse Voronoi Refinement, that {{produces}} a conformal Delaunay mesh in arbitrary dimension with guaranteed mesh size and quality. Our algorithm runs in output-sensitive time O(n log(L/s) + m), with constants depending only on dimension and on prescribed element shape quality bounds. For a large class of inputs, including integer coordinates, this matches the optimal time bound of Θ(n log n + m). Our new technique uses interleaving: we maintain a <b>sparse</b> <b>mesh</b> as we mix the recovery of input features {{with the addition of}} Steiner vertices for quality improvement...|$|E
40|$|An {{efficient}} {{technique to}} visualize {{primary and secondary}} results for combined FEM/BEM models as contours is presented. The technique is based on dividing higher-order surfaces into triangles and on using texture interpolation to produce contour plots. Since results of high accuracy with significant gradients can be obtained using <b>sparse</b> <b>meshes</b> of boundary elements and finite elements, special attention is devoted to element face subdivision. Subdivision density is defined {{on the basis of}} both face edge curvature and ranges of result fields over element faces. Java 3 D API is employed for code development...|$|R
40|$|Model {{reduction}} is often required in optical diffusion tomography (ODT), typically due to limited available computation time or computer memory. In practice, this often {{means that we}} are bound to use <b>sparse</b> <b>meshes</b> in {{the model for the}} forward problem. Conversely, if we are given more and more accurate measurements, we have to employ increasingly accurate forward problem solvers in order to exploit the information in the measurements. In this paper we apply the approximation error theory to ODT. We show that if the approximation errors are estimated and employed, it is possible to use mesh densities that would be unacceptable Nvith a conventional measurement model...|$|R
40|$|Scientists today rely on {{powerful}} {{computers to}} perform simulations critical {{for research and}} development. Modern microprocessors provide high performance by exploiting data locality with carefully designed multi-level caches. Programs can achieve good performance only if they possess data locality, keeping most data in cache and avoiding accesses to memory. Compiler transformations can improve locality and achieve large performance improvements, particularly for linear algebra codes. However, as scientific computations increase in complexity, they employ advanced features such as 3 D arrays, <b>sparse</b> <b>meshes,</b> and pointer-based data structures that {{make it difficult to}} utilize caches well. This proposal aims to develop and evaluate software support for improving locality for advanced scientific applications for both sequential and parallel machines. The basic premise is that both compile-time analyses and sophisticated run-time systems are necessary. Run-time systems are needed because many programs are not analyzable statically. Compiler support is crucial both for inserting interfaces to the run-time system and for directly applying program transformations where possible. Cooperation between the compiler and run-time will be critical for advanced scientific codes. (Also cross-referenced as UMIACS-TR- 2000 - 56...|$|R
40|$|ABSTRACT The authors {{recently}} introduced {{the technique of}} <b>sparse</b> <b>mesh</b> re-finement to produce the first near-optimal sequential time bounds of O(n lg L/s+m) for inputs in any fixed dimension with piecewise-linear constraining (PLC) features. This paper extends that work to the parallel case, refining the same inputs in time O(lg(L/s) lg m) on an EREW PRAM while maintaining the work bound; in practice, this means we expect linear speedup for any practical numberof processors. This is faster than the best previously known parallel Delaunay mesh refinement algorithms in two dimensions. It is thefirst technique with work bounds equal to the sequential case. In higher dimension, {{it is the first}} provably fast parallel technique forany kind of quality mesh refinement with PLC inputs. Furthermore, the algorithm's implementation is straightforward enough that it islikely to be extremely fast in practice...|$|E
40|$|URL : [URL] audienceBeing able {{to produce}} {{realistic}} facial animation is crucial for many speech applications in language learning technologies. For reaching realism, {{it is necessary to}} acquire and to animate dense 3 D models of the face. Recovering dense models is often achieved using stereovision techniques. Unfortunately, reconstruction artifacts are common and are mainly due to the difficulty to match points on untextured areas of the face between images. In this paper, we propose a robust and fully automatic method to produce realistic dense animation. Our input data are a dense 3 D mesh of the talker obtained for one viseme as well as a corpus of stereo sequences of a talker painted with markers that allows the face kinematics to be learned. The main contribution of the paper is to transfer the kinematics learned on a <b>sparse</b> <b>mesh</b> onto the 3 D dense mesh, thus allowing dense facial animation. Examples of face animations are provided which prove the reliability of the proposed method...|$|E
40|$|This {{document}} {{is a product}} of the Delay Tolerant Networking Research Group and has been reviewed by that group. No objections to its publication as an RFC were raised. This document defines PRoPHET, a Probabilistic Routing Protocol using History of Encounters and Transitivity. PRoPHET is a variant of the epidemic routing protocol for intermittently connected networks that operates by pruning the epidemic distribution tree to minimize resource usage while still attempting to achieve the best-case routing capabilities of epidemic routing. It is intended for use in <b>sparse</b> <b>mesh</b> networks where there is no guarantee that a fully connected path between the source and destination exists at any time, rendering traditional routing protocols unable to deliver messages between hosts. These networks are examples of networks where there is a disparity between the latency requirements of applications and the capabilities of the underlying network (networks often referred to as delay and disruption tolerant). The document presents an architectural overview followed by the protocol specification. CN...|$|E
40|$|In {{this paper}} we {{describe}} a methodology developed for quantita-tive {{evaluation of a}} model-based tracking framework targeting 3 D rigid curved objects. In particular, this framework considers the use of <b>sparse</b> polygonal <b>meshes</b> for tracking curved objects in order to solve the trade-off between the computational time and tracking ac-curacy. A simulator was designed considering how to compare ob-jective results from the standard edge based tracking with the new proposed framework. Experimental results using synthetic data are showed...|$|R
40|$|Tests were {{conducted}} in the NAL 0. 9 m wind tunnel to determine the drag on five <b>sparse,</b> <b>meshes</b> of solidity ratios ranging from 0. 054 to 0. 175. These meshes are being considered for the antennae of the Giant Meterwave Radio Telescope (GMRT) to be constructed near Pune. The present series of tests were specially designed for low incidence range upto 15 degree to supplement the results obtained earlier for larger incidences (V. Jayaraman, 1987). The apparatus for the experiment was specially designed so that the meshes were held in tension, the support interference was eliminated, and the incidence was controllable. Load cells were {{used to measure the}} forces acting on the meshes and the data were acquired using HP 9000 / 300 computer. The variation of the drag coefficient Cd with incidence e and solidity ratio is described approximately by the cerrelation 13; Cd a P 1 + b 613; where a and b were found to have values 0. 1 and 13; 0. 02 for freestream speeds of 15 and 20 m/sec, that is 13; for freestream speeds of 15 and 20 m/sec, that is 13; for Reynolds numbers (based on the wire diameter) of 4500 - 6000...|$|R
40|$|In {{this paper}} {{we aim at}} {{refining}} the geometry of 3 D models of real objects by adding surface bumpiness to them. 3 D scanners are usually not accurate enough to measure fine details, such as surface roughness. Photometric stereo is an appropriate technique to recover bumpiness. We use a number of images taken from the same viewpoint under varying illumination and an initial <b>sparse</b> 3 D <b>mesh</b> obtained by a 3 D scanner. We assume the surface to be Lambertian, but the lighting properties are unknown. The novelty of our method is that the initial <b>sparse</b> 3 D <b>mesh</b> is exploited to calibrate light sources and then to recover surface normals. The importance of refining the geometry of a bumpy surface is demonstrated by applying the method to synthetic and real data...|$|R
40|$|Being able {{to produce}} {{realistic}} facial animation is crucial for many speech applications in language learning technologies. For reaching realism, {{it is necessary to}} acquire and to animate dense 3 D models of the face. Recovering dense models is often achieved using stereovision techniques. Unfortunately, reconstruction artifacts are common and are mainly due to the difficulty to match points on untextured areas of the face between images. In this paper, we propose a robust and fully automatic method to produce realistic dense animation. Our input data are a dense 3 D mesh of the talker obtained for one viseme as well as a corpus of stereo sequences of a talker painted with markers that allows the face kinematics to be learned. The main contribution of the paper is to transfer the kinematics learned on a <b>sparse</b> <b>mesh</b> onto the 3 D dense mesh, thus allowing dense facial animation. Examples of face animations are provided which prove the reliability of the proposed method. Index Terms: modeling of facial gesture, face animation. 1...|$|E
40|$|In this paper, {{we develop}} {{a method for}} {{generating}} a high-quality approximation of a noisy set of points sampled from a smooth surface by a sparse triangle mesh. The main idea of the method consists of defining an appropriate set of approximation centers and use them as the vertices of a mesh approximating given scattered data. To choose the approximation centers, a clustering procedure is used. With every point of the input data we associate a local uncertainty measure {{which is used to}} estimate the importance of the point contribution to the reconstructed surface. Then a global uncertainty measure is constructed from local ones. The approximation centers are chosen as the points where the global uncertainty measure attains its local minima. It allows us to achieve a high-quality approximation of uncertain and noisy point data by a <b>sparse</b> <b>mesh.</b> An interesting feature of our approach is that the uncertainty measures take into account the normal directions estimated at the scattered points. In particular it results in accurate reconstruction of high-curvature regions...|$|E
40|$|This article {{concerns}} {{a variation on}} multicasting, called geocasting, for an ad hoc network. The goal of a geocast routing protocol is to deliver packets {{to a group of}} nodes that are within a specified geographical area, i. e., the geocast region. This paper presents a Geocast Adaptive Mesh Environment for Routing (GAMER) which provides geocast communication in an ad hoc network. GAMER adapts to the current network environment by dynamically changing the density of the mesh. Thus, when nodes are highly mobile, a dense mesh is created; when nodes are moving slowly, a <b>sparse</b> <b>mesh</b> is created. We compare the performance of GAMER with non-adaptive mesh-based geocast routing protocols in an ns- 2 simulated ad hoc network. We also compare two versions of GAMER; one version is more active than the other in adapting to the current network environment. We conclude that both versions of GAMER improve the transmission accuracy significantly, without increasing the load on the network significantly, when compared to non-adaptive mesh-based geocast routing approaches...|$|E
40|$|The {{original}} publication {{is available}} at www. springerlink. comInternational audienceBeing able to produce realistic facial animation is crucial for many speech applications in language learning technologies. Reaching realism needs to acquire and to animate dense 3 D models of the face which are often acquired with 3 D scanners. However, acquiring the dy- namics of the speech from 3 D scans is difficult as the acquisition time generally allows only sustained sounds to be recorded. On the contrary, acquiring the speech dynamics on a sparse set of points is easy using a stereovision recording a talker with markers painted on his/her face. In this paper, we propose an approach to animate a very realistic dense talking head which makes use of a reduced set of 3 D dense meshes ac- quired for sustained sounds {{as well as the}} speech dynamics learned on a talker painted with white markers. The contributions of the paper are twofold: We first propose an appropriate principal component anal- ysis (PCA) with missing data techniques in order to compute the basic modes of the speech dynamics despite possible unobservable points in the <b>sparse</b> <b>meshes</b> obtained by the stereovision system. We then propose a method for densifying the modes, that is a method for computing the dense modes for spatial animation from the sparse modes learned by the stereovision system. Examples prove the effectiveness of the approach and the high realism obtained with our method...|$|R
40|$|In {{this paper}} {{we present a}} method for the {{adapting}} a geometrical deformable model (GDM) for reconstruction of real-world objects from multiple range images. Our approach registers the range images simultaneously, carves out an immediate volume and finally generates an accurate, <b>sparse</b> triangle <b>mesh.</b> The proposed GDM scheme refines an initial roughly approximated mesh by deformation and adaptive subtriangulation. Even {{in the case of}} very large data sets our approach presents an efficient method of surface reconstruction due to adaptive improvement to the desired degree of accuracy. Since the root mean square approximation error of each triangle is minimized in an iterative procedure, the mesh quality is higher than that of previous approaches...|$|R
40|$|This paper {{presents}} a global view of measurement-driven traffic engineering, explores {{the interplay between}} traffic matrix estimation and routing optimization and demonstrates how demand uncertainties can be accounted for in the optimization step to guarantee a robust and reliable result. Based on a unique data set of complete measured traffic matrices, we quantify the demand uncertainties in an operational IP network and demonstrate how a number of robust optimization schemes allow to find fixed MPLS configurations that {{are close to the}} performance limits given by time-varying routing under full demand knowledge. We present a novel scheme for computing a <b>sparse</b> MPLS <b>mesh</b> to complement a baseline routing, and explore how the performance depends {{on the size of the}} partial mesh. Corresponding methods for robust OSPF optimization are discussed and a number of challenges are detailed...|$|R
40|$|The {{goal of a}} geocast routing {{protocol}} is to deliver packets {{to a group of}} nodes that are within a specified geographical area, i. e., the geocast region. Previous studies on multicast communication for an ad hoc network have shown that a mesh-based {{routing protocol}} is more robust against frequent topology changes than a tree-based routing protocol. This paper presents a Geocast Adaptive Mesh Environment for Routing (GAMER) which provides geocast communication in an ad hoc network. GAMER adapts to the current network environment by dynamically changing the density of the mesh. Thus, when nodes are highly mobile, a dense mesh is created; when nodes are moving slowly, a <b>sparse</b> <b>mesh</b> is created. We compare the performance of GAMER with non-adaptive mesh-based geocast routing protocols in a NS 2 simulated ad hoc network. We also compare two versions of GAMER; one version is more active than the other in adapting to the current network environment. We conclude that both versions of GAMER improve the transmission accuracy significantly, without increasing the load on the network significantly, when compared to non-adaptive mesh-based geocast routing approaches...|$|E
40|$|In this thesis, {{we first}} discuss the {{differences}} between plain optical switches and wavelength routers, which are the building blocks for static WDM networks and wavelength routing networks respectively. We study the different features of these building blocks, and clarify the different meanings of 'ring/cycle topologies' in those two types of optical WDM networks. Then we propose a heuristic algorithm to find a set of link-covering rings {{to be used in}} WDM network protection/restoration. Our algorithm yields a scalable set of rings that covers most links in a <b>sparse</b> <b>mesh</b> network. To study whether it is beneficial to use P-cycles instead of pure rings in the WDM ring cover approach, we design a generic linear programming model for comparison purposes. By doing simulation studies with the model, we show that the method of P-cycle cover can only achieve very little performance improvement over pure ring cover. Thus from an engineering point of view, P-cycle cover is not practical. Finally we propose a distributed protocol to find rerouting paths by P-cycle decomposition, in wavelength routing networks. The restoration performance of the distributed protocol can be very close to the theoretical upper bound under our traffic distribution assumptions. That justifies P-cycle is a highly efficient topology for network protection/restoration...|$|E
40|$|This thesis {{relates to}} {{research}} and development {{in the field of}} 3 D mesh data for computer graphics. A review of existing storage and manipulation techniques for mesh data is given followed by a framework for mesh editing. The proposed framework combines complex mesh editing techniques, automatic level of detail generation and mesh compression for storage. These methods work coherently due to the underlying data structure. The problem of storing and manipulating data for 3 D models is a highly researched field. Models are usually represented by <b>sparse</b> <b>mesh</b> data which consists of vertex position information, the connectivity information to generate faces from those vertices, surface normal data and texture coordinate information. This sparse data is sent to the graphics hardware for rendering but must be manipulated on the CPU. The proposed framework is based upon geometry images and is designed to store and manipulate the mesh data entirely on the graphics hardware. By utilizing the highly parallel nature of current graphics hardware and new hardware features, new levels of interactivity with large meshes can be gained. Automatic level of detail rendering can be used to allow models upwards of 2 million polygons to be manipulated in real time while viewing a lower level of detail. Through the use of pixels shaders the high detail is preserved in the surface normals while geometric detail is reduced. A compression scheme is then introduced which utilizes the regular structure of the geometry image to compress the floating point data. A number of existing compression schemes are compared as well as custom bit packing. This is a TIF funded project which is partnered with Unlimited Realities, a Palmerston North software development company. The project was to design a system to create, manipulate and store 3 D meshes in a compressed and easy to manipulate manner. The goal is to create the underlying technologies to allow for a 3 D modelling system to become integrated into the Umajin engine, not to create a user interface/stand alone modelling program. The Umajin engine is a 3 D engine created by Unlimited Realities which has a strong focus on multimedia. More information on the Umajin engine can be found at www. umajin. com. In this project we propose a method which gives the user the ability to model with the high level of detail found in packages aimed at creating offline renders but create models which are designed for real time rendering...|$|E
40|$|This paper {{presents}} {{a framework for}} generating smooth-looking transformations between pairs of surfaces that may differ in topology. The user controls the transformation by specifying a <b>sparse</b> control <b>mesh</b> on each surface and by associating each face in one control mesh with a corresponding face in the other. The algorithm builds a transformation from this information in two steps. The first step constructs a series of shapes and meshes (according the theory of topological surgery) that describes how topological changes should occur at critical points during the transformation. This makes possible the second step, which establishes smooth transformations by combining intermediate shapes in this series. Control meshes allow the user precise but intuitive control of the morph, while the 3 D surfaces that result {{can be used for}} rendering or keyframing. R esum e Cet article presente une methode pour engendrer des transformations continues entre deux surfaces dont la topologie peut etre di [...] ...|$|R
40|$|Subdivision {{surfaces}} {{are capable of}} modeling and representing complex shape of ar-bitrary topology. However, methods on how to build the control mesh of a complex surface are not studied much. Currently, most meshes of complicated objects come from triangulation and simplification of raster scanned data points, like the Stanford 3 D Scanning Repository. This approach is costly and leads to very dense meshes. Subdivision surface based one-piece representation means to represent the final object in a design process with only one subdivision surface (i. e. a <b>sparse</b> control <b>mesh),</b> no matter how complicated the object’s topology or shape. No decomposition of the object into simpler components is necessary. Hence the number of parts in the final representation is always the minimum: one. We have developed necessary mathematical theories and geometric algorithms to support subdivision surface based one-piece representation. First an explicit parametriza-tion method is presented for exact evaluation of Catmull-Clark subdivision surfaces. Based on our parametrization techniques, two approaches have been proposed for con-structing a control mesh of a given object with arbitrary topology. The first approac...|$|R
40|$|A typical {{industrial}} design modelling scenario involves defining the overall {{shape of a}} product followed by adding detail features. Procedural features are well-established in computer aided design (CAD) involving regular forms, but are less applicable to free-form modelling involving subdivision surfaces. Current approaches do not generate <b>sparse</b> subdivision control <b>meshes</b> as output, which is why free-form features are manually modelled into subdivision control meshes by domain experts. Domain experts change the local topology of the subdivision control mesh to incorporate features into the surface, without increasing the mesh density unnecessarily and carefully avoiding the appearance of artefacts. In this paper we show how to translate this expert knowledge to grammar rules. The rules may then be invoked in an interactive system to automatically apply features to subdivision surfaces...|$|R
40|$|Parameterized types (generics) {{have been}} {{announced}} for the Java(TM) and C# programming languages. In this paper, we evaluate these extensions {{with respect to}} the realm of scientific computing and compare them with C++ templates. At the heart of our comparison are the set and relation classes from the Janus framework which provides abstraction for the efficient representation of <b>meshes,</b> <b>sparse</b> graphs and associated matrices. As an example, we compare the performance of the Bellman-Ford single source shortest path algorithm when using parameterized Janus data structures implemented in C++, Java(TM), and C#. Our measurements suggest that both Java(TM) and C# generics introduce only little run time overhead when compared with non-parameterized implementations. With respect to scientific application, C# generics have the advantage of allowing value types (including builtin types) as parameters of generic classes and methods...|$|R
40|$|The {{performance}} is measured {{of the components}} of the key interative kernel of a preconditioned Krylov space interative linear system solver. In some sense, these numbers can be regarded as best case timings for these kernels. Sweeps were timed over <b>meshes,</b> <b>sparse</b> triangular solves, and inner products on a large 3 -D model problem over a cube shaped domain discretized with a seven point template. The performance of the CM- 2 is highly dependent on the use of very specialized programs. These programs mapped a regular problem domain onto the processor topology in a careful manner and used the optimized local NEWS communications network. The rather dramatic deterioration in performance was documented when these ideal conditions no longer apply. A synthetic workload generator was developed to produce and solve a parameterized family of increasingly irregular problems...|$|R
40|$|Harmonic phase (HARP) {{analysis}} {{has been used}} in tagged magnetic resonance imaging (MRI) to measure two-dimensional (2 D) in-plane motion and strain, and was recently applied in characterizing the motion of tongue during speech. The 3 D-HARP method extended the HARP method to track three-dimensional (3 D) cardiac motion from short- and longaxis tagged MR images by diffusing 2 D in-plane motion on a <b>sparse</b> 3 D <b>mesh.</b> In this paper, we propose a new 3 D-HARP method to calculate the 3 D tongue motion from three orthogonal tag orientations imaged within sagittal, coronal, and axial image planes. Our method iteratively tracks 2 D in-plane motions on points on a compact mesh using 2 D-HARP followed by thin-plate spline (TPS) interpolation to extend the 2 D motion to the whole mesh. Experiments on real tongue data show that our method is capable of accurate motion tracking in simple utterances. 1...|$|R
40|$|This {{article is}} devoted to a {{selection}} of recent topics in survivable networking. New ideas in capacity design and ring-to-mesh evolution are given, {{as well as a}} systematic comparison of the capacity requirements of several mesh-based schemes showing how they perform over a range of network graph connectivity. The work provides new options and insights to address the following questions. How does one evolve from an existing ring-based network to a future mesh network? If the facilities graph is very <b>sparse,</b> how can <b>mesh</b> efficiency be much better than rings? How do the options for mesh protection or restoration rank in capacity requirements? How much is efficiency increased if we enrich our network connectivity ? We also outline p-cycles, showing this new concept can realize ring-like speed with meshlike efficiency. The scope is limited to conveying basic ideas with an understanding that they could be further adapted for use in IP or DWDM layers with GMPLS-type protocols or a centralized control plane...|$|R
40|$|International audienceThe {{establishment}} of a good correspondence mapping is a key issue in planar animations such as image morphing and deformation. In this paper, we present a novel mapping framework for animation of complex shapes. We firstly let the user extract {{the outlines of the}} interested object and target interested area from the input images and specify some optional feature lines, and then we generate a <b>sparse</b> delaunay triangulation <b>mesh</b> taking the outlines and the feature lines of the source shape as constraints. Then we copy the topology from the source shape to the target shape to construct a valid triangulation in the target shape. After that, each triangle of this triangular mesh is further segmented into a dense mesh patch. Each mesh patch is parameterized onto a unit circle domain. With such parametrization, we can easily construct a correspondence mapping between the source patches and the corresponding target patches. Our framework can work well for various applications such as shape deformation and morphing. Pleasing results generated by our framework show that the framework works well...|$|R
40|$|With the {{increased}} complexity and continual scaling of integrated circuit performance, multi-core chips with dozens, hundreds, {{even thousands of}} parallel computing units require high performance interconnects to maximize data throughput and minimize latency and energy consumption. High core counts render bus based interconnects inefficient and lackluster in performance. Networks-on-Chip were introduced to simplify the interconnect design process and maintain a more scalable interconnection architecture. With the continual scaling of feature sizes for smaller and smaller transistors, the global interconnections of planar integrated circuits are consuming higher energy proportional {{to the rest of}} the chip power dissipation as well as increasing communication delays. Three-dimensional integrated circuits were introduced to shorten global wire lengths and increase chip connectivity. These 3 D ICs bring heat dissipation challenges as the power density increases drastically for each additional chip layer. One of the most popularly researched vertical interconnection technologies is through-silicon vias (TSVs). TSVs require additional manufacturing steps to build but generally have low energy dissipation and good performance. Alternative wireless technologies such as capacitive or inductive coupling do not require additional manufacturing steps and also provide the option of having a liquid cooling layer between planar chips. They are typically much slower and consume more energy than their wired counterparts, however. This work compares the interconnection technologies across several different NoC architectures including a proposed <b>sparse</b> 3 D <b>mesh</b> for inductive coupling that increases vertical throughput per link and reduces chip area compared to the other wireless architectures and technologies...|$|R
40|$|The main aim of {{this thesis}} work is to find/implement various methods that convert Conventional Stereoscopic 3 D Video (CSV) to Multiview video (MVV). The work investigates {{different}} methods that can produce multiple views given a stereoscopic pair from a frame of a particular video sequence and continues {{with the process of}} selecting the best among investigated methods that has optimum quality and speed. In contrast to the existing algorithms, this work disregards the physical depth but instead focus on pixel value correspondence. The intermediate view generation in this work is not considered as a geometrical problem, but a morphing problem. Different morphing algorithms (mesh, field and thin plate spline morphing techniques) are considered for conversion. Performance of each morphing algorithm is in turn compared using different correspondence matching techniqes. The investigated methods aim to produce arbitrary number of novel synthesized camera views from a <b>sparse</b> view set. <b>Mesh</b> morphing algorithm is found to be a better candidate in terms of signal to noise ratio, but requires accurate correspondences at edges of an object in a particular scene and also needs more execution time to generate more number of views. A new approach to field morphing has been introduced in this thesis work, which performs better in terms of execution time and also found to produce intermediate views with reasonable signal to noise ratio. This approach is observed to bring good trade off between speed and accuracy. This conversion has an advantage it {{can be used as a}} decompression mechanism that can produce multiple views required for an Autostereoscopic 3 D display from a stereoscopic left and right pair. This approach also brings the benefit of backward compatibility as present standards for CSV may be used to provide multiview 3 D video to high fidelity Autostereoscopic 3 D displays of the future. This work has applications in free view point television, video conferencing systems etc.,C/O SHASHIDHAR REDDY NACKSTÄVAGAN 16 ’C, LGH 3172, 85352 SUNDSVALL, SWEDEN. Ph: 004673478457...|$|R

