233|465|Public
2500|$|A new {{category}} of mobile search tool that is emerging {{is one in}} which a pre-selected set of possible search content is downloaded in advance by a mobile user and then allows for a final internet <b>search</b> <b>step.</b> [...] An example of such search tools is the Worldport Navigator for the iPhone, which provides users with a push-button experience of selecting from thousands of human-screened and categorized Web selections in three or four seconds, without the need for text entry, search, result review, or page-scrolling.|$|E
50|$|In the <b>search</b> <b>step,</b> the {{accumulator}} cell with {{the maximum number}} of line segments passing through it is found. This is followed by removal of those line segments, and the <b>search</b> <b>step</b> is repeated until this count goes below a certain threshold. As more computing power is now available, points corresponding to two or three mutually orthogonal directions can be found.|$|E
5000|$|SES {{algorithm}} improves upon TSS algorithm as each <b>search</b> <b>step</b> in SES {{is divided}} into two phases: ...|$|E
25|$|First, a maximum-likelihood principle, {{based on}} the idea to {{increase}} the probability of successful candidate solutions and <b>search</b> <b>steps.</b> The mean of the distribution is updated such that the likelihood of previously successful candidate solutions is maximized. The covariance matrix of the distribution is updated (incrementally) such that the likelihood of previously successful <b>search</b> <b>steps</b> is increased. Both updates can be interpreted as a natural gradient descent. Also, in consequence, the CMA conducts an iterated principal components analysis of successful <b>search</b> <b>steps</b> while retaining all principal axes. Estimation of distribution algorithms and the Cross-Entropy Method are based on very similar ideas, but estimate (non-incrementally) the covariance matrix by maximizing the likelihood of successful solution points instead of successful <b>search</b> <b>steps.</b>|$|R
3000|$|... 1. Set {{the current}} local {{minimizer}} z * as the initial point z 0. Set l = 0 {{be the number}} of <b>searching</b> <b>steps.</b>|$|R
40|$|We {{propose a}} simple but {{efficient}} algorithm for searching all occurrences of a pattern or a class of patterns (length m) in a text (length n) with at most k mismatches. This algorithm relies on the Shift-Add algorithm of Baeza-Yates and Gonnet [6], which involves representing by a bit number {{the current state of}} the search and uses the ability of programming languages to handle bit words. State representation should not, therefore, exceeds the word size !, that is, m(dlog 2 (k + 1) e + 1) !. This algorithm consists in a preprocessing <b>step</b> and a <b>searching</b> <b>step.</b> It is linear and performs 3 n operations during the <b>searching</b> <b>step.</b> Notions of shift and character skip found in the Boyer-Moore (BM) [9] approach, are introduced in this algorithm. Provided that the considered alphabet is large enough (compared to the Pattern length), the average number of operations performed by our algorithm during the <b>searching</b> <b>step</b> becomes n(2 + k+ 4 m). 1 Introduction Our purpose is approximate m [...] ...|$|R
50|$|Every {{exchange}} in the <b>search</b> <b>step</b> increases {{the size of}} S by at least 1, and thus can happen at most n times.|$|E
50|$|In the 2000-01 school year, Lakeland Regional High School was {{recognized}} as a New Jersey Best Practice School for its S.C.O.P.E., <b>Search,</b> <b>S.T.E.P.,</b> and SOAR School-to-Career / Workplace Environment programs.|$|E
50|$|Homology {{modelling}}: Homology modelling {{is done by}} the Homodeller program, {{which is}} a part of the PROTEUS2 program. The proteins that are identified during the homology <b>search</b> <b>step</b> are used as the templates in homology modelling.|$|E
40|$|International audienceWe {{propose a}} simple but e cient {{algorithm}} for searching all occurrences of a pattern or a class of patterns (length m) in a text (length n) with at most k mismatches. This algorithm relies on the Shift-Add algorithm of Baeza-Yates and Gonnet [6], which involves representing by a bit number {{the current state of}} the search and uses the ability of programming languages to handle bit words. State representation should not, therefore, exceeds the word size w, that is, m(⌈log 2 (k+ 1) ⌉+ 1) ≤w. This algorithm consists in a preprocessing <b>step</b> and a <b>searching</b> <b>step.</b> It is linear and performs 3 n operations during the <b>searching</b> <b>step.</b> Notions of shift and character skip found in the Boyer-Moore (BM) [9] approach, are introduced in this algorithm. Provided that the considered alphabet is large enough (compared to the Pattern length), the average number of operations performed by our algorithm during the <b>searching</b> <b>step</b> becomes n(2 +(k+ 4) /(m-k)) ...|$|R
3000|$|Feature {{selection}} can {{be described}} as a search into a space of states, and according to the initialization and behavior during the <b>search</b> <b>steps,</b> we can divide the search into three different approaches [...]...|$|R
3000|$|... remain {{unchanged}} in the {{two steps}} of master synchronization, the acquisition time can be further derived by {{the fact of the}} same processes {{in the two}} <b>search</b> <b>steps.</b> The two-step acquisition time is given by (12).|$|R
50|$|At {{the line}} <b>search</b> <b>step</b> (4) the {{algorithm}} might either exactly minimize h, by solving , or loosely, by {{asking for a}} sufficient decrease in h. One example of the former is conjugate gradient method. The latter is called inexact line search and may be performed {{in a number of}} ways, such as a backtracking line search or using the Wolfe conditions.|$|E
50|$|There are {{significantly}} {{large numbers of}} vanishing points present in an image. Therefore, {{the aim is to}} detect the vanishing points that correspond to the principal directions of a scene. This is generally achieved in two steps. The first step, called the accumulation step, as the name suggests, clusters the line segments with the assumption that a cluster will share a common vanishing point. The next step finds the principal clusters present in the scene and therefore it is called the <b>search</b> <b>step.</b>|$|E
50|$|A new {{category}} of mobile search tool that is emerging {{is one in}} which a pre-selected set of possible search content is downloaded in advance by a mobile user and then allows for a final internet <b>search</b> <b>step.</b> An example of such search tools is the Worldport Navigator for the iPhone, which provides users with a push-button experience of selecting from thousands of human-screened and categorized Web selections in three or four seconds, without the need for text entry, search, result review, or page-scrolling.|$|E
30|$|In VNE-ABC algorithm, {{we first}} {{initialize}} k food resources (initial solutions) and bee colony. These bees will {{work together in}} three <b>searching</b> <b>steps</b> to update food resources. After several iterations, {{we can get a}} good solution.|$|R
30|$|In the onlooker bee <b>searching</b> <b>steps,</b> {{roulette}} is a greedy method, {{which will}} cause solutions to be trapped into local optimum. In order {{to solve these}} problems, we introduce pheromone and sensitivity model to replace the roulette method.|$|R
3000|$|... /N 0 will be {{determined}} before searching for the optimal irregular mapping. We set an experience factor δ=(δ 1,δ 2,δ 3),δ 1 >δ 2 >δ 3 > 0 to adjust the initial value and the <b>searching</b> <b>step</b> size of E [...]...|$|R
5000|$|While {{existence}} of (deterministic) polynomial algorithms for graph isomorphism {{is still an}} open problem in the computational complexity theory, in 1977 László Babai reported that with probability at least 1 &minus; exp(&minus;O(n)), a simple vertex classification algorithm after only two refinement steps produces a canonical labeling of a graph chosen uniformly at random from the set of all n-vertex graphs. Small modifications and an added depth-first <b>search</b> <b>step</b> produce canonical labeling of such uniformly-chosen random graphs in linear expected time. This result sheds {{some light on the}} fact why many reported graph isomorphism algorithms behave well in practice. This was an important breakthrough in probabilistic complexity theory which became widely known in its manuscript form and which was still cited as an [...] "unpublished manuscript" [...] long after it was reported at a symposium.|$|E
50|$|Interpolation {{search is}} an {{algorithm}} for {{searching for a}} given key in an indexed array that has been ordered by numerical values assigned to the keys (key values). It parallels how humans search through a telephone book for a particular name, the key value by which the book's entries are ordered. In each <b>search</b> <b>step</b> it calculates where in the remaining search space the sought item might be, based on the key values at the bounds of the search space {{and the value of}} the sought key, usually via a linear interpolation. The key value actually found at this estimated position is then compared to the key value being sought. If it is not equal, then depending on the comparison, the remaining search space is reduced to the part before or after the estimated position. This method will only work if calculations on the size of differences between key values are sensible.|$|E
30|$|According to step (2), {{combined}} with Formula (8)–Formula (10), the <b>search</b> <b>step</b> Hi {{of the current}} iteration is obtained. Individuals search in random directions within the <b>search</b> <b>step</b> range, and the formula is shown in Eq. (13).|$|E
3000|$|Computation {{efforts of}} TDA-EDE are mainly {{determined}} by the eigen-decomposition of Ω 2 (ν) in the 1 -D search operation for CFO estimation, as it will execute in each <b>searching</b> <b>step</b> while other operations execute only once. Although computation of [...]...|$|R
40|$|Abstract — We {{propose the}} design of soft sphere {{detection}} with the search method independently bounded for specific search levels (transmit antennas). The bounds {{are based on the}} distribution of the candidates found in each search level. The area and throughput estimates are compared against two previously implemented soft detectors. It is shown that our approach achieves significantly better performance than the previously implemented modified soft K-best solution mostly because no bound is established for the first search level. The proposed design is smaller and faster than other implemented list sphere detector with 256 <b>search</b> <b>steps.</b> The performance is comparable while the average number of <b>search</b> <b>steps</b> is significantly smaller for our method. I...|$|R
3000|$|..., we {{stop and}} return the current local minimizer. This is because for the problem {{considered}} in this article, the optimal solution {{should be in the}} neighborhood of the continuous solution. Therefore, when the number of <b>searching</b> <b>steps</b> is greater than n [...]...|$|R
40|$|Crossover {{plays an}} {{important}} role in GA-based search. There have been many empirical comparisons of different crossover operators in the literature. However, analytical results are limited. No theory has explained the behaviours of different crossover operators satisfactorily. This paper analyses crossover from quite a different point of view from the classical schema theorem. It explains the behaviours of different crossover operators through the investigation of crossover's search neighbourhood and <b>search</b> <b>step</b> size. It is shown that given the binary chromosome encoding scheme GAs with a large <b>search</b> <b>step</b> size is better than GAs with a small step size for most problems. Since uniform crossover's <b>search</b> <b>step</b> size is larger than that of either one-point or twopoint crossover, uniform crossover is expected to perform better than the other two. Similarly, two-point crossover is expected to perform better than one-point crossover due to its larger <b>search</b> <b>step</b> size. It is also shown in this [...] ...|$|E
30|$|Our {{approach}} is hybrid {{and consists of}} two steps. First, we use the high-level features to guide the search at the coarser level. The resulting documents {{can be used as}} query examples in a second, low-level based step, for purposes such as query refinement or focus change. This approach helps bridging the semantic gap because the set of documents obtained after the first <b>search</b> <b>step</b> can be used as good query examples for the second <b>search</b> <b>step.</b>|$|E
30|$|Iterative {{optimization}} calculations are taken. The individual {{determines the}} <b>search</b> <b>step</b> {{according to the}} previous research method and randomly searches in various directions.|$|E
3000|$|Step 2 : The {{distances}} from the optimal position P_m^t [...] {{to the start}} searching point P_ 0 ^t, and to the end searching point P_n^t [...] in the previous step t (t[*]≥[*] 1) are computed as |P_m^t-P_n^t| 0.5 em and 0.5 em |P_m^t-P_ 0 ^t|, respectively. The maximum of these two distances is computed as D_t= (|P_m^t-P_ 0 ^t|,|P_m^t-P_n^t|) [...]. The next searching area is defined as [P_m^t-D_t/ 2, P_m^t+D_t/ 2], and the <b>searching</b> <b>step</b> size is computed as m_t+ 1 =D_t/D_t- 1 ×m_t [...]. The searching process is {{same as that of}} Step 1. The image definition value is calculated at the <b>searching</b> <b>step,</b> and the optimal position with the maximum definition value is obtained as P_m^t+ 1.|$|R
40|$|In {{this paper}} we {{proposed}} a video signature based on ordinal measure of resampled video frames, which is robust to changing compression formats, compression ratios, frame sizes and frame rates. To effectively localize a short query video clip in a long target video through the proposed video signature, we developed a coarse-to-fine signature comparison scheme. In the coarse <b>searching</b> <b>step,</b> roughly matched positions are determined based on Sequence Shape Similarity, while in the fine <b>searching</b> <b>step,</b> dynamic programming is applied to handle similarity matching {{in the cases of}} losing frames and temporal editing processes are employed on the target video. Experiments showed that the proposed video signature has good robustness and uniqueness, which are the two essential properties of video signatures. 1...|$|R
50|$|Many {{unconstrained}} optimization algorithms {{can be adapted}} to the constrained case, often via the use of a penalty method. However, <b>search</b> <b>steps</b> taken by the unconstrained method may be unacceptable for the constrained problem, leading to a lack of convergence. This {{is referred to as the}} Maratos effect.|$|R
3000|$|... {{increases}} <b>search</b> <b>step</b> {{from one}} hop to two hops in {{the condition of}} a worst case. Herein, the worst case is that a super-peer n [...]...|$|E
3000|$|... {{increases}} the <b>search</b> <b>step</b> conditionally to two hops, a faster convergence {{time at the}} expense of network traffic is achieved by the SPS. Specifically, n [...]...|$|E
30|$|For a 16  ×  16 block, {{the search}} ranges for ρ and θ are [0, 8) and [0, 2 π), respectively. Thus, there are (8 /Δρ) × (2 π/Δθ) {{candidate}} partition lines for a 16  ×  16 block, where Δρ and Δθ are the associated <b>search</b> <b>step</b> sizes of ρ and θ, respectively. Although the small <b>search</b> <b>step</b> size of partitioning parameters leads to good interpolation performance, {{there is a}} huge computational cost in examining a large number of candidate partition lines. Since both PSNR performance and computational cost are reasonably good at Δρ =  1 and Δθ = π/ 8 for various experiments using TWP, these <b>search</b> <b>step</b> sizes are used in this work. Moreover, the search range of ρ is restricted to [0, 5) in the proposed block partitioning method because the larger value of ρ makes one of two wedgelet sub-blocks smaller, and that sub-block may lose its texture characteristic.|$|E
3000|$|Initial conditions: Suppose {{that the}} initial <b>searching</b> <b>step</b> L, which is the search length on the contour, is 1 / 5 of the {{character}} height. The initial searching point O is arbitrary on the contour. The starting and ending pixels of the current segment are A and B, respectively, where [...]...|$|R
40|$|Information {{literacy}} {{programs must}} emphasise {{that the key}} to effective searching lies in four basic processes, planning, acting, recording, and critically reflecting. Importantly this includes reflecting on what has been retrieved and the <b>searching</b> <b>steps</b> undertaken. It is this reflection that is the key to information literacy development and maturity...|$|R
40|$|In this paper, we {{analyze the}} {{statistical}} properties of a randomized binary search algorithm and its variants. The basic discrete {{version of the}} problem is as follows. Suppose there are total m items, numbered 1, 2, [...] ., m, out of which the first k items are marked, where k is unknown. The objective is to choose one of the marked items uniformly at random. In each step of the basic algorithm, a number y is chosen uniformly at random from 1 to x, where x is the number chosen in the previous step and is equal to m in the first step. A query is made about y. If y is marked, the algorithm returns. We will also consider batch versions of this algorithm in which multiple numbers are chosen in each step and multiple queries are made in parallel. We give the mean and variance (exact or asymptotic) for the number of <b>search</b> <b>steps</b> in each version of the algorithm, and when possible, we give its distribution. We also analyze the access or hit pattern to the entire search space. The basic algorithm is fairly efficient {{in terms of the number}} of <b>search</b> <b>steps,</b> and also has small variance. The two batch versions of the algorithm can be used separately or combined to further reduce the number <b>search</b> <b>steps</b> and its variance...|$|R
