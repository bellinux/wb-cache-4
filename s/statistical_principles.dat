272|239|Public
25|$|When {{analysing}} {{field data}} with random variation a proper application of <b>statistical</b> <b>principles</b> like in regression and frequency analysis is necessary.|$|E
25|$|The {{development}} {{of the concept of}} chance throughout history has been very gradual. Historians have wondered why progress in the field of randomness was so slow, given that humans have encountered chance since antiquity. Deborah Bennett suggests that ordinary people face an inherent difficulty in understanding randomness, although the concept is often taken as being obvious and self-evident. She cites studies by Kahneman and Tversky; these concluded that <b>statistical</b> <b>principles</b> are not learned from everyday experience because people do not attend to the detail necessary to gain such knowledge.|$|E
2500|$|Pragmatism {{begins with}} the idea that belief is that on which one is {{prepared}} to act. Peirce's pragmatism is a method of clarification of conceptions of objects. It equates any conception of an object to a conception of that object's effects to a general extent of the effects' conceivable implications for informed practice. It is a method of sorting out conceptual confusions occasioned, for example, by distinctions that make (sometimes needed) formal yet not practical differences. He formulated both pragmatism and <b>statistical</b> <b>principles</b> as aspects of scientific logic, in his [...] "Illustrations of the Logic of Science" [...] series of articles. In the second one, >"", Peirce discussed three grades of clearness of conception: ...|$|E
40|$|This paper {{discusses}} {{exploitation of}} this <b>statistical</b> <b>principle,</b> combined with wavelet image coding methods to extract phase descriptions of incoherent patterns. Demodulation and coarse quantization of the phase information creates decision environments characterized by well-separated clusters, and this {{lends itself to}} rapid and reliable pattern recognitio...|$|R
5000|$|... #Subtitle level 2: Derivation from <b>statistical</b> {{mechanical}} <b>principles</b> ...|$|R
50|$|Multivariate {{analysis}} (MVA) {{is based}} on the <b>statistical</b> <b>principle</b> of multivariate statistics, which involves observation and analysis of more than one statistical outcome variable at a time. In design and analysis, the technique is used to perform trade studies across multiple dimensions while taking into account the effects of all variables on the responses of interest.|$|R
2500|$|... {{are now the}} {{preferred}} method of reconstruction. [...] These algorithms compute {{an estimate of the}} likely distribution of annihilation events that led to the measured data, based on <b>statistical</b> <b>principles.</b> [...] The advantage is a better noise profile and resistance to the streak artifacts common with FBP, but the disadvantage is higher computer resource requirements. A further advantage of statistical image reconstruction techniques is that the physical effects that would need to be pre-corrected for when using an analytical reconstruction algorithm, such as scattered photons, random coincidences, attenuation and detector dead-time, can be incorporated into the likelihood model being used in the reconstruction, allowing for additional noise reduction. Iterative reconstruction has also been shown to result in improvements in the resolution of the reconstructed images, since more sophisticated models of the scanner Physics can be incorporated into the likelihood model than those used by analytical reconstruction methods, allowing for improved quantification of the radioactivity distribution.|$|E
50|$|In {{addition}} to using geometrical facts, stereology applies <b>statistical</b> <b>principles</b> to extrapolate three-dimensional shapes from plane section(s) of a material. The <b>statistical</b> <b>principles</b> {{are the same}} as those of survey sampling (used to draw inferences about a human population from an opinion poll, etc.).Statisticians regard stereology as a form of sampling theory for spatial populations.|$|E
50|$|When {{analysing}} {{field data}} with random variation a proper application of <b>statistical</b> <b>principles</b> like in regression and frequency analysis is necessary.|$|E
40|$|In {{this paper}} we discuss the {{relationship}} between data and representations of data. Data are maped into representations by techniques. Techniques are the objects studied by statisticians. Statistical models which are ideally proposed by the scientists themselves are often used to construct techniques by applying a <b>statistical</b> <b>principle</b> (such as ML or LS). This general framework is applied to visualization of data is particular of categorical data...|$|R
40|$|Importance: One of {{the first}} papers to apply <b>statistical</b> <b>principle</b> of design of {{experiments}} to measurement problems in packet communication networks. Packet networks account for vast majority of global information exchange, so being able to measure them correctly is important. We introduce a novel way of optimally measuring Markov chains for a particular example of a network buffer. 2. A utility based framework for optimal network measuremen...|$|R
40|$|Abstract Background The Central Limit Theorem (CLT) is a <b>statistical</b> <b>principle</b> {{that states}} {{that as the}} number of {{repeated}} samples from any population increase, the variance among sample means will decrease and means will become more normally distributed. It has been conjectured that the CLT has the potential to provide benefits for group living in some animals via greater predictability in food acquisition, if the number of foraging bouts increases with group size. The potential existence of benefits for group living derived from a purely <b>statistical</b> <b>principle</b> is highly intriguing and it has implications for the origins of sociality. Results Here we show that in a social allodapine bee the relationship between cumulative food acquisition (measured as total brood weight) and colony size accords with the CLT. We show that deviations from expected food income decrease with group size, and that brood weights become more normally distributed both over time and with increasing colony size, as predicted by the CLT. Larger colonies are better able to match egg production to expected food intake, and better able to avoid costs associated with producing more brood than can be reared while reducing the risk of under-exploiting the food resources that may be available. Conclusion These benefits to group living derive from a purely <b>statistical</b> <b>principle,</b> rather than from ecological, ergonomic or genetic factors, and could apply {{to a wide variety of}} species. This in turn suggests that the CLT may provide benefits at the early evolutionary stages of sociality and that evolution of group size could result from selection on variances in reproductive fitness. In addition, they may help explain why sociality has evolved in some groups and not others. </p...|$|R
50|$|Winer was the 1967-68 {{president}} of the Psychometric Society. In 1983, he received a Quantitative Methods Teaching Award from the American Psychological Foundation. He authored an influential textbook, <b>Statistical</b> <b>Principles</b> in Experimental Design. The book was reviewed in journals including Educational and Psychological Measurement, Ergonomics and the Journal of the American Statistical Association. In 2002, Winer was ranked fourth {{on a list of}} American psychologists most frequently cited in the professional literature; <b>Statistical</b> <b>Principles</b> in Experimental Design remains his most cited work by far.|$|E
50|$|How Not to Be Wrong: The Power of Mathematical Thinking, {{written by}} Jordan Ellenberg, {{is a book}} that connects various {{economic}} and socialistic philosophies with basic mathematics and <b>statistical</b> <b>principles.</b>|$|E
5000|$|It cooperates {{with public}} and private {{agencies}} in Greece or abroad, such as educational institutions, research centers and non-profit organisations {{for the promotion of}} scientific research for statistical issues, the harmonization of methodology and the implementation of the <b>statistical</b> <b>principles</b> of the Hellenic and European Statistical Systems.|$|E
40|$|The {{efficient}} market hypothesis {{based primarily on}} the <b>statistical</b> <b>principle</b> of Bayesian inference has been proved {{to be only a}} special-case scenario. The generalized financial market, modeled as a binary, stochastic system capable of attaining one of two possible states (High � 1, Low � 0) with finite probabilities, is shown to reach efficient equilibrium with p. M = p if and only if the transition probability matrix M 2 x...|$|R
40|$|Analysts' {{earnings}} forecasts are not perfectly {{correlated with}} actual earnings. One statistical consequence {{is that the}} most optimistic and most pessimistic forecasts are usually too optimistic and too pessimistic. The forecasts' accuracy can be improved by shrinking them towards the mean. Insufficient appreciation of this <b>statistical</b> <b>principle</b> may partly explain the success of contrarian investment strategies, in particular why stocks with the most optimistic earnings forecasts underperform those with the most pessimistic forecasts. ...|$|R
5000|$|In 1881, Newcomb {{discovered}} the <b>statistical</b> <b>principle</b> {{now known as}} Benford's law, when he observed that the earlier pages of logarithm books, used {{at that time to}} carry out logarithmic calculations, were far more worn than the later pages. This led him to formulate the principle that, in any list of numbers taken from an arbitrary set of data, more numbers will tend to begin with [...] "1" [...] than with any other digit.|$|R
5000|$|A risk {{limiting}} post-election audit is one {{of several}} types of election audits. It is based on <b>statistical</b> <b>principles,</b> and is designed to limit the risk that election results are incorrect. Specifically, it enables election managers to limit the risk that computer error or fraud identified the wrong winners, without the need for a full recount.|$|E
5000|$|Furthermore, {{statistical}} regulatory guidance {{is found}} under general topics (e.g. Good Clinical Practice - ICH E6(R2)) and specific ones explicitly related to statistics (e.g. <b>Statistical</b> <b>Principles</b> for Clinical Trials - ICH E9 [...] ) or not explicitly (e.g. Special Populations: Geriatrics E7 [...] or Clinical Trial Endpoints in Oncology FDA). This large volume {{and diversity of}} documents and information sources is subject to regular revisions.|$|E
5000|$|In {{the second}} case, he cites {{an example that}} {{demonstrates}} ignorance of <b>statistical</b> <b>principles</b> in the lay press: Since no such proof is possible genetically modified food, the article in The New York Times was {{what is called a}} [...] "bad rap" [...] against the U.S. Department of Agriculture—a bad rap based on a junk-science belief that it's possible to prove a null hypothesis.|$|E
40|$|Background: The Central Limit Theorem (CLT) is a <b>statistical</b> <b>principle</b> {{that states}} {{that as the}} number of {{repeated}} samples from any population increase, the variance among sample means will decrease and means will become more normally distributed. It has been conjectured that the CLT has the potential to provide benefits for group living in some animals via greater predictability in food acquisition, if the number of foraging bouts increases with group size. The potential existence of benefits for group living derived from a purely <b>statistical</b> <b>principle</b> is highly intriguing and it has implications for the origins of sociality. Results: Here we show that in a social allodapine bee the relationship between cumulative food acquisition (measured as total brood weight) and colony size accords with the CLT. We show that deviations from expected food income decrease with group size, and that brood weights become more normally distributed both over time and with increasing colony size, as predicted by the CLT. Larger colonies are better able to match egg production to expected food intake, and better able to avoid costs associated with producing more brood than can be reared while reducing the risk of under-exploiting the food resources that may be available. Conclusion: These benefits to group living derive from a purely <b>statistical</b> <b>principle,</b> rather than from ecological, ergonomic or genetic factors, and could apply {{to a wide variety of}} species. This in turn suggests that the CLT may provide benefits at the early evolutionary stages of sociality and that evolution of group size could result from selection on variances in reproductive fitness. In addition, they may help explain why sociality has evolved in some groups and not others. Mark I Stevens, Katja Hogendoorn and Michael P Schwar...|$|R
40|$|In Carlen (1991) a {{property}} of the Fisher information called "superadditivity", was proved via analytic means. We show that the superadditivity is a corollary of the following simple <b>statistical</b> <b>principle</b> which is of an independent interest. The Fisher information about a parameter [theta] contained in an observation X = (Y,Z) with a density f(y - [theta],z) is never less than the Fisher information in the first component Y with the equality iff Y is independent of Z. Fisher information Superadditivity...|$|R
5000|$|... {{are now the}} {{preferred}} method of reconstruction. Such algorithms compute estimates of the likely distribution of annihilation {{events that led to}} the measured data, based on <b>statistical</b> <b>principle,</b> often providing better noise profiles and resistance to the streak artifacts common with FBP.Since the density of radioactive tracer is a function in a function space, therefore of extremely high-dimensions, methods whichregularize the maximum-likelihood solution turning it towards penalized or maximum a-posteriori methods can have significant advantages for low counts.Examplessuch as Ulf Grenander's Sieve estimatoror Bayes penalty methods [...] or via I.J. Good's roughness method ...|$|R
50|$|Einstein's earlier {{statistical}} physics papers (from 1902 to 1904) developed {{the foundations of}} a theoretical approach that he applied to concrete problems in 1905 and in subsequent years. His approach combined skepticism about classical mechanics with a firm belief in molecules and a confidence in <b>statistical</b> <b>principles.</b> However, Einstein's PhD thesis does not follow this statistical approach. It {{has been argued that}} Einstein avoided his own theoretical ideas to win the approval of his PhD advisor, Alfred Kleiner.|$|E
5000|$|In 2005 and 2006, {{the site}} partnered with FOXSports.com to cross-publish {{many of the}} Outsiders' regular features, {{including}} power rankings based on a [...] "weighted" [...] version of the DVOA (Defense-adjusted Value Over Average) statistic. In 2007, Football Outsiders content appeared on FOXSports.com (in a reduced capacity) along with AOL Sports and ESPN.com. Since 2008, the site has partnered exclusively with ESPN and provides mostly ESPN Insider content. In 2009, Football Outsiders began analyzing college football using similar <b>statistical</b> <b>principles.</b>|$|E
50|$|Bayesian {{approaches}} to brain function investigate {{the capacity of}} the nervous system to operate in situations of uncertainty in a fashion that is close to the optimal prescribed by Bayesian statistics. This term is used in behavioural sciences and neuroscience and studies associated with this term often strive to explain the brain's cognitive abilities based on <b>statistical</b> <b>principles.</b> It is frequently assumed that the nervous system maintains internal probabilistic models that are updated by neural processing of sensory information using methods approximating those of Bayesian probability.|$|E
2500|$|Another use of Haar {{measure in}} {{statistics}} is in conditional inference, {{in which the}} sampling distribution of a statistic is conditioned on another statistic of the data. In invariant-theoretic conditional inference, the sampling distribution is conditioned on an invariant {{of the group of}} transformations (with respect to which the Haar measure is defined). The result of conditioning sometimes depends on the order in which invariants are used and on the choice of a maximal invariant, so that by itself a [...] <b>statistical</b> <b>principle</b> of invariance fails to select any unique best conditional statistic (if any exist); at least another principle is needed.|$|R
40|$|Atomic {{clock is}} the core {{component}} of navigation satellite payload, playing a decisive role in the realization of positioning function. So the monitoring for anomalies of the satellite atomic clock is very important. In this paper, a complete autonomous monitoring method for the satellite clock is put forward, which is, respectively, based on Phase-Locked Loop (PLL) and <b>statistical</b> <b>principle.</b> Our methods focus on anomalies in satellite clock such as phase and frequency jumping, instantaneous deterioration, stability deterioration, and frequency drift-rate anomaly. Now, method based on PLL has been used successfully in China’s newest BeiDou navigation satellite...|$|R
3000|$|... {{with a high}} {{probability}} of 100 (1 [*]-[*]α)%. On the other hand, when the average criterion ADC of a consecutive subgroup is beyond the limit, it shows {{that there are some}} anomalies. However, it should indicate that the SPC is a <b>statistical</b> <b>principle</b> of hypothesis testing. So, {{there are two types of}} hypothesis testing errors. Usually, the confidence limit can be improved by increasing the sizes p and q. From the anomaly detection procedure, it can be seen that no training data is required to construct a mathematical mode for anomaly detection. That is to say that the proposed algorithm belongs to unsupervised anomaly detection and can achieve online anomaly detection.|$|R
5000|$|Fourth, {{there is}} no ad hoc {{weighting}} of opponents' winning percentage and opponents' opponents' winning percentage, etc., ad infinitum (no random choices of 1/3 of this + 2/3 of that, for example). In this method, very simple <b>statistical</b> <b>principles,</b> with absolutely no fine tuning are used to construct a system of 117 equations with 117 variables, representing each team according only to its wins and losses, (see Ranking Method). The computer simply solves those equations {{to arrive at a}} rating (and ranking) for each team.|$|E
50|$|The {{development}} {{of the concept of}} chance throughout history has been very gradual. Historians have wondered why progress in the field of randomness was so slow, given that humans have encountered chance since antiquity. Deborah Bennett suggests that ordinary people face an inherent difficulty in understanding randomness, although the concept is often taken as being obvious and self-evident. She cites studies by Kahneman and Tversky; these concluded that <b>statistical</b> <b>principles</b> are not learned from everyday experience because people do not attend to the detail necessary to gain such knowledge.|$|E
5000|$|Dutch {{physician}} and anesthesiologist G. M. Woerlee wrote a chapter by chapter examination of Lommel's Consciousness Beyond Life. According to Woerlee {{the book is}} full of [...] "tendentious and suggestive pseudoscientific nonsense", and {{the picture of the}} functioning of the body as proposed by Lommel is not consistent with medical knowledge. Woerlee concluded that the book is a [...] "masterly example of how tendentious and suggestive interpretation of international scientific literature, vague presentation of basic medical facts, together with ignorance of some basic <b>statistical</b> <b>principles</b> leads to incorrect conclusions." ...|$|E
40|$|The <b>statistical</b> <b>principle</b> {{of maximum}} entropy {{is applied to}} the {{analysis}} of dipolar couplings from H- 1 NMR of nonrigid molecules dissolved in liquid-crystalline phases. A distribution function for the orientational and inter-ring angles is so obtained. The most probable internal angle phi is determined for 4, 4 '-dichlorobiphenyl (phi = 34 -degrees) in the nematic phase of I 52, 4 -pentyl- 4 '-cyanobiphenyl (phi = 32 -degrees) and 4 '-Br- 4 -Cl- 2, 6 -difluorobiphenyl (phi = 42 -degrees) in EBBA. The physical reliability of the distributions determined is discussed. The maximum-entropy treatment seems to indicate a limit for the information on the internal motion obtainable from the experimental data...|$|R
40|$|In {{this paper}} errors in {{variables}} methods for fitting straight lines to data are reviewed. In these methods the x and y variables are both {{assumed to be}} subject to measurement error and not, as in simple least squares linear regression, just one of them. The methods are described in a unified context using the <b>statistical</b> <b>principle</b> of the method of moments. Guidance is given on the choice of an appropriate method of estimating the slope and intercept of the fitted line. Formulas for the approximate standard errors of the estimators are provided in a technical appendix. A numerical example from biochemical studies is included to illustrate the methodology...|$|R
50|$|Because you {{are never}} {{sure that your}} meta data (station history) is complete, {{statistical}} homogenization should always be applied as well. The most commonly used <b>statistical</b> <b>principle</b> to detect and remove the effects of artificial changes is relative homogenization, which assumes that nearby stations are exposed to almost the same climate signal and that thus the differences between nearby stations can be utilized to detect inhomogeneities. By looking at the difference time series, the year-to-year variability of the climate is removed, as well as regional climatic trends. In such a difference time series, a clear and persistent jump of, for example 1 °C, can easily be detected and can only be due {{to changes in the}} measurement conditions.|$|R
