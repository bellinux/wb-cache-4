5015|8423|Public
5|$|The {{problem of}} {{induction}} discussed above {{is seen in}} another form in debates over the foundations of statistics. The <b>standard</b> <b>approach</b> to statistical hypothesis testing avoids claims about whether evidence supports a hypothesis or makes it more probable. Instead, the typical test yields a p-value, which is the probability of the evidence being such as it is, {{under the assumption that}} the hypothesis being tested is true. If the p-value is too low, the hypothesis is rejected, in a way analogous to falsification. In contrast, Bayesian inference seeks to assign probabilities to hypotheses. Related topics in philosophy of statistics include probability interpretations, overfitting, and the difference between correlation and causation.|$|E
5|$|Shaw {{appeared}} in regional theatres in Bristol, Liverpool and Hull. In 1925, he performed in London as the Archangel in The Sign of the Sun, and played first Lewis Dodd {{and then the}} Major in separate productions of The Constant Nymph. He received instruction in verse speaking under famed theatre director William Bridges-Adams in the Stratford Festival Company at Stratford-upon-Avon, where he played some of his early Shakespeare roles, including Romeo in Romeo and Juliet, Ferdinand in The Tempest and Prince Hal in Henry IV in 1926. He was criticised for the audacity he displayed in the latter role. When Prince Hal takes on his kingship and rejects the self-indulgent character Falstaff, convention of the day called for Prince Hal to change from a jovial drinking partner to an arrogant snob, but Shaw saw the view as simple-minded and contradictory toward Shakespeare's script. Instead, he displayed inward regrets about leaving Falstaff and accepting the new responsibilities. The interpretation was criticised at the time but, years later, became the <b>standard</b> <b>approach</b> to the character.|$|E
25|$|Metabolic {{networks}} {{typically have}} more reactions than metabolites and this gives an under-determined system of linear equations containing more variables than equations. The <b>standard</b> <b>approach</b> to solve such under-determined systems {{is to apply}} linear programming.|$|E
5000|$|Besides these <b>standard</b> <b>approaches,</b> {{various other}} “alternative” {{asymptotic}} approaches exist: ...|$|R
5000|$|... #Caption: Overview of the {{different}} <b>standard</b> <b>approaches</b> for superlattice transport.|$|R
50|$|Enhance {{the value}} and {{recognition}} of the performance based <b>standards</b> <b>approach.</b>|$|R
25|$|When {{confronted by}} a {{periodically}} varying function, the <b>standard</b> <b>approach</b> is to employ Fourier series, a form of analysis that uses sinusoidal functions as a basis set, having frequencies that are zero, one, two, three, etc. times the frequency of a particular fundamental cycle. These multiples are called harmonics of the fundamental frequency, and the process is termed harmonic analysis. If the basis set of sinusoidal functions suit the behaviour being modelled, relatively few harmonic terms need to be added. Orbital paths are very nearly circular, so sinusoidal variations are suitable for tides.|$|E
25|$|Early expeditions—such as General Charles Bruce's in the 1920s and Hugh Ruttledge's two {{unsuccessful}} {{attempts in}} 1933 and 1936—tried to ascend the mountain from Tibet, via the North Face. Access was closed {{from the north}} to Western expeditions in 1950, after China took control of Tibet. In 1950, Bill Tilman and a small party which included Charles Houston, Oscar Houston, and Betsy Cowles undertook an exploratory expedition to Everest through Nepal along the route which has now become the <b>standard</b> <b>approach</b> to Everest from the south.|$|E
25|$|Some {{research}} groups have recently explored {{the use of}} quantum annealing hardware for training Boltzmann machines and deep neural networks. The <b>standard</b> <b>approach</b> to training Boltzmann machines relies on the computation of certain averages that can be estimated by standard sampling techniques, such as Markov chain Monte Carlo algorithms. Another possibility is {{to rely on a}} physical process, like quantum annealing, that naturally generates samples from a Boltzmann distribution. The objective is to find the optimal control parameters that best represent the empirical distribution of a given dataset.|$|E
40|$|In {{this article}} I present a {{programmatic}} {{outline of a}} new kind of model of language, and offer some criticisms of <b>standard</b> <b>approaches.</b> The discussion begins with issues concerning representation because, so I argue, problems with <b>standard</b> <b>approaches</b> to representation {{are at the heart of}} notions of and problems with language. r 2007 Elsevier Ltd. All rights reserved. 1...|$|R
40|$|Business {{valuation}} {{is dealing}} {{with a series of}} difficulties, legislative and/or normative inaccuracies, as well as problems related to divergent results obtained by <b>standard</b> <b>approaches.</b> This material is synthesizing some of these problems, highlighting possible research directions. Each of the <b>standard</b> <b>approaches</b> is investigated, theoretically and practically analyzed, and the conclusions constitute a basis for developing new theories in the field of valuation...|$|R
50|$|Incorporate <b>standard</b> <b>approaches</b> {{defined in}} these groups when {{designing}} and implementing services on theoretical archives.|$|R
25|$|In the 19th century, infinitesimals were {{replaced}} by the epsilon, delta approach to limits. Limits describe {{the value of a}} function at a certain input in terms of its values at a nearby input. They capture small-scale behavior {{in the context of the}} real number system. In this treatment, calculus is a collection of techniques for manipulating certain limits. Infinitesimals get replaced by very small numbers, and the infinitely small behavior of the function is found by taking the limiting behavior for smaller and smaller numbers. Limits were the first way to provide rigorous foundations for calculus, and for this reason they are the <b>standard</b> <b>approach.</b>|$|E
25|$|The <b>standard</b> <b>approach</b> is {{to test a}} null {{hypothesis}} against an alternative hypothesis. A critical region is the set of values of the estimator that leads to refuting the {{null hypothesis}}. The probability of type I error is therefore {{the probability that the}} estimator belongs to the critical region given that null hypothesis is true (statistical significance) and the probability of type II error is the probability that the estimator doesn't belong to the critical region given that the alternative hypothesis is true. The statistical power of a test is the probability that it correctly rejects the null hypothesis when the null hypothesis is false.|$|E
25|$|Barium X-ray {{examinations}} {{are useful}} tools {{for the study}} of appearance and function of the parts of the gastrointestinal tract. They are used to diagnose and monitor esophageal reflux, dysphagia, hiatus hernia, strictures, diverticula, pyloric stenosis, gastritis, enteritis, volvulus, varices, ulcers, tumors, and gastrointestinal dysmotility, as well as to detect foreign bodies. Historically barium X-ray examinations are the <b>standard</b> <b>approach</b> used to assess and diagnose diseases of the gut, but they are increasingly being replaced by more modern techniques such as computer tomography, magnetic resonance imaging, ultrasound imaging, endoscopy and capsule endoscopy. However, barium contrast imaging remains a common diagnostic test, which has the advantage of lower costs and being more widely available as compared to newer techniques. Newer techniques are also not able to assess superficial mucosal lesions in as much detail as some barium X-ray techniques such as enteroclysis.|$|E
5000|$|One of {{the other}} <b>standard</b> <b>approaches</b> to curvature, through the {{covariant}} derivative , identifies the difference ...|$|R
50|$|Numerous other {{approaches}} exist for IE including hybrid approaches that combine {{some of the}} <b>standard</b> <b>approaches</b> previously listed.|$|R
5000|$|Tools and {{libraries}} distributed with Go suggest <b>standard</b> <b>approaches</b> {{to things like}} API documentation (...) , testing (...) , building (...) , package management (...) , and so on.|$|R
25|$|Developers also {{experimented with}} laserdisc players for {{delivering}} full motion video based games with movie-quality animation. The first laserdisc video game to exploit this technology was 1983's Astron Belt from Sega, soon followed by Dragon's Lair from Cinematronics; {{the latter was}} a sensation when it was released (and, in fact, the laserdisc players in many machines broke due to overuse). While laserdisc games were usually either shooter games with full-motion video backdrops like Astron Belt or interactive movies like Dragon's Lair, Data East's 1983 game Bega's Battle introduced {{a new form of}} video game storytelling: using brief full-motion video cutscenes to develop a story between the game's shooting stages, which would years later become the <b>standard</b> <b>approach</b> to video game storytelling. By the mid-1980s, the genre dwindled in popularity, as laserdiscs were losing out to the VHS format and the laserdisc games themselves were losing their novelty, due to their linearity and, in many cases, depending less on reflexes than on memorizing sequences of moves.|$|E
500|$|The Monte Carlo {{method has}} become a {{ubiquitous}} and <b>standard</b> <b>approach</b> to computation, and the method {{has been applied to}} a vast number of scientific problems. [...] In addition to problems in physics and mathematics, the method has been applied to finance, social science, environmental risk assessment, linguistics, radiation therapy, and sports.|$|E
500|$|Ulam [...] {{considered}} [...] {{the problem}} of nuclear propulsion of rockets, which was pursued by Project Rover, and proposed, {{as an alternative to}} Rover's nuclear thermal rocket, to harness small nuclear explosions for propulsion, which became Project Orion. With Fermi, John Pasta, and Mary Tsingou, Ulam studied the Fermi–Pasta–Ulam–Tsingou problem, which became the inspiration for the field of non-linear science. He is probably best known for realising that electronic computers made it practical to apply statistical methods to functions without known solutions, and as computers have developed, the Monte Carlo method has become a common and <b>standard</b> <b>approach</b> to many problems.|$|E
3000|$|We {{then turn}} to results from Shorrocks (1982) {{decomposition}} of inequality by income source. This {{is one of}} the <b>standard</b> <b>approaches</b> in the literature.|$|R
5000|$|Frameworks—The tool {{will offer}} some <b>standard</b> <b>approaches</b> for ingesting data, {{analyzing}} it and reporting any findings so developers {{can follow the}} same design patterns when possible.|$|R
40|$|<b>Standard</b> equity {{valuation}} <b>approaches</b> (i. e., DDM, RIM, and DCF) {{are based}} on restrictive assumptions regarding the availability and quality of payoff data. Therefore, we provide extensions of the <b>standard</b> <b>approaches</b> being suitable under less than ideal conditions (e. g. dirty surplus accounting and inconsistent steady state growth rates). Empirically, our extended models yield considerably smaller valuation errors, suggesting that the models are worthwhile to implement. Moreover, obtaining identical value estimates across the extended models, our approach provides a benchmark implementation. This allows us to quantify the magnitude of errors resulting from individual violations of ideal conditions in the <b>standard</b> <b>approaches.</b> JEL Classification: G 10, G 12, G 34, M 41...|$|R
500|$|The federal Fair Packaging and Labeling Act (FPLA), {{originally}} {{passed in}} 1964, was amended in 1992 to require consumer goods directly under its jurisdiction to be labelled in both customary and metric units. Some industries {{are engaged in}} efforts to amend this law to allow manufacturers to use only metric labelling. [...] The National Conference on Weights and Measures has developed the Uniform Packaging and Labeling Regulations (UPLR) which provides a <b>standard</b> <b>approach</b> to those sections of packaging law that are under state control. Acceptance of the UPLR varies {{from state to state}} – fourteen states accept it by merely citing it in their legislation.|$|E
500|$|The {{left hand}} follows the <b>standard</b> <b>approach</b> of {{classical}} waltzes, with a bass note {{followed by two}} mid-range chords, and in addition there are some contrapuntal passages where two melodies move independently but complement each other harmonically. For example, the B and C themes are examples of Joplin employing counterpoint in octaves. The B theme in the key of B-flat major {{is closely related to}} the main theme presenting its counterpoint with the bass and the treble melody lines moving in opposite directions (in contrary motion), to each other and then exchanging their melodies (bars 29-30 and 31-32). This pattern repeats itself during the theme. In the opening phrase (bars 77-81) of the [...] "rag-like" [...] C theme in the key of F major, counterpoint is evident with the harmony of the treble moving in contrary motion to the bass line in a similar way to that used in the B section. In the treble, the harmony falls from F to D, while the bass rises from F to G-sharp ...|$|E
500|$|The ratio between {{length and}} {{wingspan}} was practically {{the same as}} that of an Avro Lancaster bomber, which had a wingspan of [...] and a length of , in contrast to modern sport gliders which possess a particularly large wingspan to enhance gliding performance. This was the result of a decision taken by the War Office in early 1940 on how military gliders would be used; the idea was for the glider to be released at a low altitude close to the landing zone and conduct a steep descent to reduce time in the air and exposure to enemy fire. The glider also possessed large flaps which assisted in a steep and rapid descent, and through adjustments of their angle during landing a precise control over descent rate and point of landing could be achieved; they also allowed a slower touchdown speed to be attained. They were operated through a small bottle of compressed air large enough only for a single landing; a small bottle not only saved weight, but gave a smaller chance of it being hit by enemy fire, thereby exploding and damaging the glider. <b>Standard</b> <b>approach</b> speed for the Hamilcar was , although for shorter landings this could be slowed to , and stalling speeds were [...] with flaps up or [...] with flaps deployed. The Hamilcar was fitted with tailwheel landing gear, with oleo-pneumatic shock absorbers that could be deflated to bring the fuselage nose down for loading or unloading purposes. A jettisonable undercarriage was initially designed for the glider, as it was discovered that it travelled for a shorter distance when it landed only on its skids. However, this was eventually replaced with a fixed undercarriage – the same as had been designed for ferrying operations – as pilots found that they preferred to land on wheels because of the extra control it gave them and the ability to avoid other gliders and potential collisions in the landing area. The wheeled undercarriage was not fitted until after the glider had been loaded; two 15-ton jacks were used to lift the aircraft for the fitting.|$|E
40|$|Practitioners are {{generally}} {{well aware of}} the fact that most <b>standard</b> <b>approaches</b> for estimation and inference in panel data regressions are based on assuming that the cross-sectional units are independent of each other, an assumption that is surely mistaken in applications, especially in macroeconomics and finance. Yet, applications involving anything but these <b>standard</b> <b>approaches</b> are very rare. The current paper can be seen as a reaction to this. The purpose is to point to some of the recent advances in the area of factor-augmented panel regressions, and to also provide some guidance regarding their implementation...|$|R
50|$|The third {{adoption}} {{process is}} most {{closely related to}} optimizing behavior and thus <b>standard</b> <b>approaches</b> in economics. The first two processes are, however, the ones focused on by the vast sociological and marketing literature on the subject.|$|R
50|$|Modality is a non-fiction book by the semanticist Paul Portner. The book, first {{published}} by the Oxford University Press in 2009, lays out the basic problems in linguistic modality {{and some of the}} <b>standard</b> <b>approaches</b> to solving them.|$|R
2500|$|When [...] has an {{infinite}} (or very large) {{number of possible}} realizations the <b>standard</b> <b>approach</b> is then to represent this distribution by scenarios. This approach raises three questions, namely: ...|$|E
2500|$|Beutler’s seminal work on G6PD {{deficiency}} led him {{to further}} explore hemolytic anemias caused by various enzyme deficiencies. [...] The systematic methodology that he developed became the <b>standard</b> <b>approach</b> to study of patients with these disorders.|$|E
2500|$|The {{method of}} least squares is a <b>standard</b> <b>approach</b> in {{regression}} analysis to the approximate solution of overdetermined systems, i.e., sets of equations {{in which there}} are more equations than unknowns. [...] "Least squares" [...] means that the overall solution minimizes the sum of the squares of the residuals made in the results of every single equation.|$|E
40|$|Mathematical {{modeling}} of and {{is important for}} many technological applica-tions [1]. Numerical studies of moving interfaces between phases employ both finite difference [2] and finite element [3] <b>approaches.</b> However, these <b>standard</b> <b>approaches</b> have to address the difficult issue of maintaining the accuracy o...|$|R
30|$|In {{electron}} tomography, it {{is generally}} not possible to obtain the exact solution of the inverse problem outlined in Eq. (11), mostly because of the missing wedge problem. The mathematical system is ill-posed, and as a result, many artifacts hamper the reconstructions obtained via <b>standard</b> <b>approaches.</b>|$|R
50|$|Since {{the early}} 1990s {{there have been}} a number of efforts to define <b>standard</b> <b>approaches</b> for {{describing}} and analyzing system architectures. Many of the recent Enterprise Architecture frameworks have some kind of set of views defined, but these sets are not always called view models.|$|R
