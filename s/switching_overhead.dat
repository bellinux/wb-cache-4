84|146|Public
50|$|Shared {{memory and}} message passing {{concurrency}} have different performance characteristics. Typically (although not always), the per-process memory overhead and task <b>switching</b> <b>overhead</b> is lower in a message passing system, but the overhead of message passing {{is greater than}} for a procedure call. These differences are often overwhelmed by other performance factors.|$|E
5000|$|The {{architecture}} of the network interface has been developed to offload the entire task of interprocessor communication from the main processor, and to avoid the overhead of system calls for user process to user process messaging. QsNetII is designed for use within SMP systems [...] - [...] multiple, concurrent processes can utilise the network interface without any task <b>switching</b> <b>overhead.</b> A I/O processor offloads protocol handling from the main CPU. Local memory on the PCI card provides storage for buffers, translation tables and I/O adapter code. All the PCI bandwidth is available to data communication.|$|E
5000|$|Most {{interesting}} {{of all was}} the fast-path, {{which allowed}} for extremely fast invocations [...] - [...] at least when running on SPARC-based platforms. The fast-path used a unique [...] "half-trap" [...] to avoid much of the context <b>switching</b> <b>overhead</b> which plagued Mach systems. Instead of saving out all of the processor state—the normal procedure {{in the case of a}} trap into the kernel—Spring only saved out the top 16 SPARC registers, a number which was defined by specific implementation details of the SPARC architecture. The other portions of the register stack were rendered invisible to the receiver using the SPARC's [...] instruction, providing some level of security. The fast-path strongly resembles a classic procedure call within a single application, which uses register windows on the SPARC, adding some MMU work to move the context from one program to another.|$|E
5000|$|Cache is {{physically}} addressed, solving many cache aliasing problems and reducing context <b>switch</b> <b>overhead.</b>|$|R
30|$|In this section, {{the impact}} of {{processor}} frequency on the context <b>switch</b> <b>overhead</b> for static and dynamic frequency cases is discussed.|$|R
30|$|Irrespective of {{the type}} of the network structure, the system is {{composed}} of <b>switches,</b> <b>overhead</b> lines {{and a variety of other}} devices. Thus, the network composition can be simplified and the directional topology of the distribution network can be obtained.|$|R
50|$|Micro-threading is a {{software-based}} threading {{framework that}} creates small threads inside multi-core or many-core processors. Each core may {{have two or}} more tiny threads that utilize its idle time. It is like hyper-threading invented by Intel or the general multi-threading architecture in modern micro-processors. It enables the existence {{of more than one}} thread running on the same core without performing expensive context switching to system's main memory, even if this core does not have multi-threading hardware logic. Micro-threads mainly hide memory latency inside each core by over lapping computations with memory requests. The main difference between micro-threads and current threading models is that micro-threads context <b>switching</b> <b>overhead</b> is very small. For example, the overhead micro-threads implementation on Cell Broadband Engine is 160 nano seconds; meanwhile, the overhead of context switching of the whole core's (SPE) thread is around 2000 micro-seconds. This low overhead is due to three main factors. First, micro-threads are very small. Each micro-thread runs one or two simple but critical functions. Second, micro-threads context include only the register file of the core currently the micro-thread is executing on. Third, micro-threads are context switched to core's dedicated cache, which makes this process very fast and efficient.|$|E
40|$|We {{study the}} {{scheduling}} polices for asymptotically optimal delay in queueing systems with <b>switching</b> <b>overhead.</b> Such systems {{consist of a}} single server that serves multiple queues, and some capacity is lost whenever the server switches to serve {{a different set of}} queues. The capacity loss due to this <b>switching</b> <b>overhead</b> can be significant in many emerging applications, and needs to be explicitly addressed in the design of scheduling policies. For example, in 60 GHz wireless networks with directional antennas, base stations need to train and reconfigure their beam patterns whenever they switch from one client to another. Considerable <b>switching</b> <b>overhead</b> can also be observed in many other queueing systems such as transportation networks and manufacturing systems. While the celebrated Max-Weight policy achieves asymptotically optimal average delay for systems without <b>switching</b> <b>overhead,</b> it fails to preserve throughput-optimality, let alone delay-optimality, when <b>switching</b> <b>overhead</b> is taken into account. We propose a class of Biased Max-Weight scheduling policies that explicitly takes <b>switching</b> <b>overhead</b> into account. The Biased Max-Weight policy can use either queue length or head-of-line waiting time as an indicator of the system status. We prove that our policies not only are throughput-optimal, but also can be made arbitrarily close to the asymptotic lower bound on average delay. To validate the performance of the proposed policies, we provide extensive simulation with various system topologies and different traffic patterns. We show that the proposed policies indeed achieve much better delay performance than that of the state-of-the-art policy. Comment: 37 page...|$|E
40|$|Abstract — The optical burst {{switching}} (OBS) {{schemes to}} date {{assume that the}} <b>switching</b> <b>overhead</b> at intermediate nodes is either negligible or {{can be considered as}} part of the processing delay of the control packet. In this paper, we will show that the <b>switching</b> <b>overhead</b> can {{have a significant impact on}} the performance of OBS. We have also proposed methods to alleviate the problem. I...|$|E
40|$|Abstract—The {{capacity}} and flexibility of wireless mesh networks (WMNs) can be greatly improved by adopting dynamic channel assignment. However, dynamic channel assignment brings new challenges such as <b>switching</b> <b>overheads</b> and dependency problems. We propose a channel assignment protocol, called the hybrid channel assignment protocol (HCAP) for infrastructure WMNs (I-WMN). To {{find out a}} reasonable tradeoff between flexibility and <b>switching</b> <b>overheads,</b> HCAP adopts static interface assignment strategy for nodes that have the heaviest loads. For other nodes, it adopts a hybrid strategy. HCAP uses a slot-based coordination policy to implement communications between nodes that adopt hybrid strategy. NS 2 -based simulations show that HCAP not only improves {{capacity and}} scalability of I-WMNs, but also enhances per-flow fairness. I...|$|R
40|$|Abstract. High-performance and {{low-power}} VLIW DSP processors {{are increasingly}} being deployed in mobile devices to process video and multimedia applications. The diverse applications of such systems has led to recent research efforts focusing on their resource management and kernel scheduling. In this paper, we address the enhancing {{the performance of the}} microkernel for a VLIW DSP processor, called PAC architectures. In order {{to reduce the number of}} read and write ports in register files of VLIW architectures, so as to reduce both the power consumption and implementation costs, a distributed register file and multibank register architectures are being adopted in PAC architectures. These methods present challenges for microkernel designs in terms of reducing context <b>switch</b> <b>overhead.</b> In our work, we propose a multiset descriptor mechanism with compiler support to reduce the context <b>switch</b> <b>overheads</b> associated with the use of registers. The experiments were done with the microkernel system called pCore which has an efficient and tiny design that prunes its code size down under 11 Kbytes. Experimental results show that our multiset context-switching mechanism may reduce the context <b>switch</b> <b>overhead</b> up to 30 %. ...|$|R
40|$|A gearbox-type {{capacitive}} DC/DC converter {{is presented}} with an enhanced switch- and capacitor-array. The <b>switch</b> <b>overhead</b> is reduced significantly {{with respect to}} a conventional implementation. The presented topologies exhibit a lower swing on the intermediate capacitor nodes so that power loss due to the parasitic capacitances on these nodes is reduced drastically. status: publishe...|$|R
3000|$|Because of the <b>switching</b> <b>overhead,</b> the new demand {{should last}} {{a certain time}} before the benefit of new UL/DL {{configuration}} compensates the switching cost. Given the data transmission time distribution P(t), we can estimate the probability P(T [...]...|$|E
40|$|In this paper, we {{evaluate}} {{two techniques}} for executing shared memory {{programs on the}} EM-X distributed memory multiprocessor: access with no local copy(NL) and access with coherent local copy(CL). For the NL approach, multithreading efficiently hides the latency caused by fine-grain communication, whereas the thread <b>switching</b> <b>overhead</b> still remains. To eliminate the thread <b>switching</b> <b>overhead</b> and exploit locality, we have also implemented the CL mechanism derived from the notion of conventional software distributed shared memory. Performance analyses show that a highly optimized implementation for a frequent shared access program greatly improves the performance, in spite of additional software overhead. Tradeoffs of NL versus CL, thus obtained, {{provide a basis for}} the selection of a technique that is more appropriate for applications on the EM-X. Keywords: Parallel memory systems, Performance evaluation and measurements, Fine-grain communication, Distributed shared memory...|$|E
40|$|In {{this paper}} we develop {{techniques}} for analyzing and optimizing energy management in multi-core servers with speed scaling capabilities. Our framework incorporates the processor's dynamic power, {{and it also}} accounts for other intricate and important power features such as the static (leakage) power and <b>switching</b> <b>overhead</b> between speed levels. Using stochastic fluid models to capture traffic burst dynamics, we propose and study different strategies for adapting the multi-core server speeds based on the observable buffer content, so as to optimize objective functions that balance energy consumption and performance. It is shown that, for a reasonable <b>switching</b> <b>overhead</b> and {{a small number of}} thresholds, a substantial efficiency gain is achieved. In addition, the optimal power consumptions of the different strategies are hardly sensitive to perturbations in the input parameters, so that the performance is robust to misspecifications of the system's input traffic...|$|E
50|$|Multitasking on BareMetal {{is unusual}} for {{operating}} systems {{in this day}} and age. BareMetal uses an internal work queue that all CPU cores poll. A task added to the work queue will be processed by any available CPU core in the system and will execute until completion, which results in no context <b>switch</b> <b>overhead.</b>|$|R
40|$|We {{present the}} motivation, design, implementation, and {{performance}} {{evaluation of a}} UNIX kernel mechanism capable of establishing fast in-kernel data pathways between I/O objects. A new system call, splice() moves data asynchronously and without user-process intervention to and from I/O objects specified by file descriptors. Performance measurements indicate improved I/O throughput and increased CPU availability attributable to data copying and context <b>switch</b> <b>overhead...</b>|$|R
40|$|As one {{optimization}} technique {{used in the}} static analysis of compilers, alias analysis has been developed to detect aliased variables in a code and {{to be used in}} reordering instructions to enhance performance. In high-performance computing, alias analysis is even more useful for instruction-level parallelism and distributed and cluster computing, because the possibility of aliasing adversely affects factors such as context <b>switch</b> <b>overhead,</b> communication delay, and race conditions...|$|R
40|$|Abstract — Cognitive radio {{improves}} {{spectrum efficiency}} by allowing secondary users (SU) to dynamically exploit the idle spectrum owned by primary users (PU). This paper studies optimal bandwidth allocation for SUs. Consider the following tradeoff: a SU increases its instantaneous throughput by accessing larger bands, but channel <b>switching</b> <b>overhead</b> (due to {{the dynamics of}} PU activities) and contention among multiple SUs create higher liability for larger bandwidths. So how much is too much? In this paper, we find the optimal bandwidth allocation in both the single SU and multiple SU cases, accounting {{for the effects of}} channel <b>switching</b> <b>overhead.</b> Our result is validated through both numerical simulation and real channel activity traces. We also study the impact of various factors on SU performance, namely, PU channel idle time and probability, PU channel correlation, and SU sensing and access schemes. The work sheds light on the design of spectrum sharing protocols in cognitive radio networks. I...|$|E
40|$|We {{describe}} how {{the problem of}} effectively servicing a number of users by a multimedia server can be represented as a periodic disk scheduling problem. We give an overview of related work and present {{a number of new}} results, based on periodically servicing users that have different consumption requirements with different periods. In addition, by scheduling sets of disk accesses as batches the worst-case <b>switching</b> <b>overhead</b> is reduce...|$|E
40|$|Abstract—In this paper, {{the author}} extends the {{traditional}} exact schedulability analysis for fixed priority (FP) preemptive scheduling by {{taking into account}} the extra scheduling overhead that may be induced by context switching. Then the author develops a necessary and sufficient scehdulability test for fixed priority scheduling on a single processor by considering the worst-case task response time. In the paper, the existing results on exact response time analysis have been discussed for fixed priority preemption schecduling with task set that is periodic or sporadic periodic respectively. As a generalization of fixed priority preemptive scheduling, the fixed priority preemption-threshold scheduling is also described in this paper. An improvement upon the previous results has been proposed by considering the influence of task response time caused by the context <b>switching</b> <b>overhead</b> and release jitter. Accounting for context <b>switching</b> <b>overhead</b> needs to increase the execution time of each task. The author also extends this analysis to consider the task response time with arbitrary deadlines. Keywords-schedulability analysis; fixed priority; scheduling; response-time; scheduling overhead I...|$|E
40|$|This work {{addresses}} {{the problem of}} reducing context <b>switch</b> <b>overhead</b> on a processor which supports a large register file - a register file much like that {{which is part of}} the Berkeley RISC processors and several other emerging architectures (which are not necessarily reduced instruction set machines in the purest sense). Such a reduction in overhead is particularly desirable in a real-time embedded application, in which task-to-task context <b>switch</b> <b>overhead</b> may result in failure to meet crucial deadlines. A storage management technique by which a context switch may be implemented as cheaply as a procedure call is presented. The essence of this technique is the avoidance of the save/restore of registers on the context switch. This is achieved through analysis of the static source text of an Ada tasking program. Information gained during that analysis directs the optimized storage management strategy for that program at run time. A formal verification of the technique in terms of an operational control model and an evaluation of the technique's performance via simulations driven by synthetic Ada program traces are presented...|$|R
40|$|Photograph of the {{interior}} of the Southern California Gas Company plant, ca. 1955. A series of pipes of various sizes interconnect and plug into a light-colored wall at center. Atop them at left, sit two round meters, connected to the pipe maze. Across from this area, a man stands, turning a knob on a wall dotted with meters and <b>switches.</b> <b>Overhead</b> stream yet another series of pipes, perhaps a ventilation system.; Back of photoprint reads: "application usage"...|$|R
40|$|Fixed-Priority Scheduling with Deferred Preemption (FPDS) is {{a middle}} ground between Fixed-Priority Pre-emptive Scheduling and Fixed-Priority Non-preemptive Scheduling, and offers {{advantages}} with respect to con-text <b>switch</b> <b>overhead</b> and resource access control. In this paper we present our work on extending the real-time operating system RTAI/Linux with support for FPDS. We give an overview of possible alternatives, describe our design choices and implementation, and verify {{through a series of}} measurements that indicate that a FPDS implementation in a real-world RTOS is feasible with minimal overhead. ...|$|R
40|$|Our {{aim is to}} {{investigate}} the suitability of hardware multithreading for real-time event handling in combination with appropriate real-time scheduling techniques. We designed and evaluated a multithreaded microcontroller based on a Java processor core. Java threads are used as Interrupt Service Threads (ISTs) instead of the Interrupt Service Routines (ISRs) of conventional processors. Our proposed Komodo microcontroller supports multiple ISTs with zero-cycle context <b>switching</b> <b>overhead.</b> A so-called priority manager implements several real-time scheduling algorithms in hardware...|$|E
40|$|To date, optical burst {{switching}} (OBS) schemes {{assume the}} switch reconfiguration {{time to be}} negligible, {{a part of the}} processing delay of the control packet, or a fixed value added to the data burst length, regardless of the switch status. In this paper, we show that the <b>switching</b> <b>overhead</b> can {{have a significant impact on}} the performance of some OBS channel scheduling schemes. We have also proposed methods to alleviate the problem. © 2007 IEEE. link_to_subscribed_fulltex...|$|E
30|$|Switching {{interval}} is {{an important}} parameter to consider as it directly impacts the delay jitter and the achievable TCP throughput. Once the transmitting interface R 2 switches to a given channel with non-empty queue, it stays on that channel for a predefined switching interval (awaiting for more packets to arrive to the queue) {{in order to minimize}} <b>switching</b> <b>overhead.</b> A small switching interval leads to smaller jitter and is sensitive to channel mobility of neighbor nodes, but less time on a given channel.|$|E
3000|$|... >ε in a {{transmission}} round, the teleoperator will scan each channel, measure the interference power in each channel, and then select the cleanest channel. Obviously, {{the design of}} the threshold for channel switching is very important. In this paper, we set the threshold according to the link reliability without channel interference. It can be obtained through the wireless link test and evaluation. The channel switching information will be integrated into the ACK packet and shared with the telerobot. The event triggered sensing and switching mechanism can also avoid unnecessary sensing and <b>switching</b> <b>overheads.</b>|$|R
5000|$|Scheduling <b>overheads</b> (<b>switching</b> {{from one}} task to another) are zero.|$|R
40|$|Providing untrusted {{applications}} with {{shared and}} dependable access to modern display hardware is of increasing importance. Our new display system, called Blink, safely multiplexes complex graphical content from multiple untrusted virtual machines onto a single Graphics Processing Unit (GPU). Blink {{does not allow}} clients to program the GPU directly, but instead provides a virtual processor abstraction to which they can program. Blink executes virtual processor programs and controls the GPU {{on behalf of the}} client, in a manner that reduces processing and context <b>switching</b> <b>overheads.</b> To achieve performance and safety, Blink employs just-in-time compilation and simple static analysis. ...|$|R
40|$|We {{propose a}} multithreaded Java {{microcontroller}} [...] called Komodo microcontroller [...] {{with a new}} hardware event handling mechanism that allows handling of simultaneous overlapping events with hard real-time requirements. Real-time Java threads are used as interrupt service threads (ISTs) instead of interrupt service routines (ISRs). Our proposed Komodo microcontroller supports multiple ISTs with zero-cycle context <b>switching</b> <b>overhead.</b> We evaluate the basic architectural attributes using real-time event parameters of an autonomous guided vehicle. When calculating the maximum vehicle speed without violating the real-time constraints, ISTs dominate ISRs by a speed increase of 28 %...|$|E
40|$|We propose {{handling}} of external real-time events through multithreading {{and describe the}} microarchitecture of our multithreaded Java microcontroller, called Komodo microcontroller. Real-time Java threads are used as interrupt service threads (ISTs) instead of interrupt service routines (ISRs). Our proposed Komodo microcontroller supports multiple ISTs with zero-cycle context <b>switching</b> <b>overhead.</b> We evaluate the basic performance and optimize the internal structures of our microcontroller using real-time applications as benchmarks on a software simulator. Our results show a performance gain of about 1. 4 and {{the correlation between the}} fetch bandwidth and the instruction window size...|$|E
40|$|This paper {{introduces}} a multithreaded Java processor kernel which is speci cally designed as core processor of a microcontroller or system-on-a-chip. Handling of external real-time events is performed through multithreading. Real-time Java threads {{are used as}} interrupt service threads (ISTs) instead of interrupt service routines (ISRs). Our proposed Komodo microcontroller supports multiple ISTs with zero-cycle context <b>switching</b> <b>overhead.</b> Simulation results show a performance gain by multithreading of about 1. 4. A VHDL-based hardware design aimed at a Xilinx FPGA yields chip-space requirements of about 55000 gates for a four-threaded processor kernel...|$|E
40|$|Wireless mesh {{networks}} (WMNs) {{have emerged}} as a promising technology for next generation wireless networking. A WMN extends network coverage using wireless mesh routers that {{communicate with each other}} via multi-hop wireless communications. One technique to increase the network capacity of WMNs is to use routers equipped with multiple radios capable of transmitting and receiving on multiple channels. In a Multi-Channel Multi-Radio wireless mesh network (MCMR WMN), nodes are capable of transmitting and receiving data simultaneously through different radios and at least theoretically doubling the average throughput. On the other hand, the use of multi-radio and multi-channel technology in many cases requires routers to switch channels for each transmission and/or reception. Channel switching incurs additional costs and delay. In this thesis, we present a simulation-based study of the impacts of channel <b>switching</b> <b>overheads</b> on the performance of multicast in MCMR WMNs. We study how channel <b>switching</b> <b>overheads</b> affect the performance metrics such as packet delivery ratio, throughput, end-to-end delay, and delay jitter of a multicast session. In particular, we examine: 1. the performance of multicast in MCMR WMNs with three orthogonal channels versus eleven overlapping channels defined in IEEE 802. 11 b. 2. the performance of the Minimum-interference Multi-channel Multi-radio Multicast (M 4) algorithm with and without channel switching. 3. the performance of the Multi-Channel Minimum Number of Transmissions (MCMNT) algorithm (which does not do channel switching) in comparison with the M 4 algorithm (which performs channel switching) ...|$|R
30|$|It {{is noted}} that the context <b>switch</b> energy <b>overhead</b> {{decreases}} {{with the increase of}} the number of context switches. In fact, to switch from one process to another, the state of each process should be saved in a data structure named process control block (PCB). The energy overhead of the creation of the PCB is accounted with the context <b>switch</b> energy <b>overhead</b> and is divided between the context switches so that if the number of context switches increases the average Ecs per context switching decreases. Also, when the scheduling policy used is SCHED_FIFO, the context <b>switch</b> energy <b>overhead</b> {{is more important than the}} energy for the SCHED_RR scheduling policy. In fact, under the round robin scheduling policy, the processor assigns time slices (quantum) to each process. So, before the context switches that we generate, there is another context switches that occurred automatically due to the expiration of the quantum of the process P 1. Consequently, the PCB is created during the automatic context <b>switch.</b> The energy <b>overhead</b> of the PCB creation is not accounted with the energy of the context switch that we generate: Ecs. But, under the FIFO scheduling policy, the processor does not switch automatically from the process P 1 to P 2 only if P 1 terminates its execution so that the energy overhead of the PCB creation is accounted with Ecs.|$|R
40|$|Abstract—In Both multi-interface and dynamicchannel {{adjustment}} are prevailingly used {{to improve}} thecapacity and the flexibility of wireless mesh networks(WMNs). The System over heads that are generated by uncontrolled interface switching adversely decrease the performance of WMNs. To find a reasonable tradeoff between flexibility and <b>switching</b> <b>overheads,</b> we propose ahybrid channelassignment protocol (HCAP) for multiinterface WMNs. The HCAP adopts a static interfaceassignment strategy for nodes that have the heaviest loads to avoid frequent interface switching, whereas it adopts a hybrid interfaceassignment strategy for other nodes to improve the ability of adapting to flow change. In our implementation, we present a slotbased coordination policy. Extensive NS 2 simulations demonstrate that the HCAP improves network capacity, enhances flexibility, and guarantees interflow fairness...|$|R
