5|3|Public
50|$|The Rossmann cluster {{consists}} of HP ProLiant DL165 G7 compute nodes with 64-bit, dual 12-core AMD Opteron 6172 processors (24 cores per node), either 48 gigbytes or 96 GB {{of memory and}} 250 GB of local disk for system software and <b>scratch</b> <b>storage.</b> Nodes with 192 GB of memory and either 1 terabyte or 2 TB of local scratch disk also are available. Rossmann {{consists of}} five logical sub-clusters, each with a different memory and storage configuration. All nodes have 10 Gigabit Ethernet interconnects.|$|E
50|$|The role of {{the cache}} memory, {{independent}} of the main memory, was to hold the top of an evaluation stack for a procedure oriented language. The cache had a two cycle latency after which it could deliver one word per cycle and was divided {{into a number of}} pages each with 512 32-bit words with parity protection. The pages were grouped in pairs with an architectural maximum of 16 pairs. The original machine implemented two pairs. The second member of each pair was typically used as additional fast registers and <b>scratch</b> <b>storage</b> without affecting the stack page. The lower nine bits of the CPU register, which addressed the cache, was implemented with counters and allowed increment and decrement operations (push and pop) as wells as random access.|$|E
40|$|The {{application}} of the parallel programming methodology known as the Force was conducted. Two application issues were addressed. The first involves {{the efficiency of the}} implementation and its completeness in terms of satisfying the needs of other researchers implementing parallel algorithms. Support for, and interaction with, other Computational Structural Mechanics (CSM) researchers using the Force was the main issue, but some independent investigation of the Barrier construct, which is extremely important to overall performance, was also undertaken. Another efficiency issue which was addressed was that of relaxing the strong synchronization condition imposed on the self-scheduled parallel DO loop. The Force was extended by the addition of logical conditions to the cases of a parallel case construct and by the inclusion of a self-scheduled version of this construct. The second issue involved applying the Force to the parallelization of finite element codes such as those found in the NICE/SPAR testbed system. One of the more difficult problems encountered is the determination of what information in COMMON blocks is actually used outside of a subroutine and when a subroutine uses a COMMON block merely as <b>scratch</b> <b>storage</b> for internal temporary results...|$|E
5000|$|DJ-70: {{sampling}} DJ music workstation and synthesizer keyboard {{that featured}} the first <b>scratch</b> wheel pad. <b>Storage</b> on 3½-inch DS/DD Floppy disk drive ...|$|R
40|$|Abstract—Innovative {{scientific}} {{applications and}} emerging dense data sources {{are creating a}} data deluge for highend computing systems. Processing such large input data typicallyinvolves copying(or staging) ontothesupercomputer’s specialized high-speed <b>storage,</b> <b>scratch</b> space, for sustained high I/O throughput. The current practice of conservatively staging data {{as early as possible}} makes the data vulnerable to storage failures, which may entail re-staging and consequently reduced job throughput. To address this, we present a timely staging framework that uses a combination of job startup time predictions, user-specified intermediate nodes, and decentralized data delivery to coincide input data staging with job start-up. By delaying staging to when it is necessary, the exposure to failures and its effects can be reduced. Evaluation using both PlanetLab and simulations based on three years of Jaguar (No. 1 in Top 500) job logs show as much as 85. 9 % reduction in staging times compared to direct transfers, 75. 2 % reduction in wait time on scratch, and 2. 4 % reduction in usage/hour. Keywords-Highperformance datamanagement,data-staging, HPC center serviceability, end-user data delivery I...|$|R
40|$|Abstract—Innovative {{scientific}} {{applications and}} emerging dense data sources {{are creating a}} data deluge for high-end supercomputing systems. Modern applications are often collaborative in nature, with a distributed user base for input and output data sets. Processing such large input data typically involves copying (or staging) the data onto the supercomputer’s specialized high-speed <b>storage,</b> <b>scratch</b> space, for sustained high I/O throughput. This copying is crucial as remotely accessing the data while an application executes results in unnecessary delays and consequently performance degradation. However, the current practice of conservatively staging data {{as early as possible}} makes the data vulnerable to storage failures, which may entail restaging and reduced job throughput. To address this, we present a timely staging framework that uses a combination of job start-up time predictions, user-specified volunteer or cloud-based intermediate storage nodes, and decentralized data delivery to coincide input data staging with job start-up. Evaluation of our approach using both PlanetLab and Azure cloud services, as well as simulations based on three years of Jaguar supercomputer (No. 3 in Top 500) job logs show as much as 91. 0 percent reduction in staging times compared to direct transfers, 75. 2 percent reduction in wait time on scratch, and 2. 4 percent reduction in usage/hour. (An earlier version of this paper appears in [30].) Index Terms—High performance data management, data-staging, HPC center serviceability, end-user data delivery Ç...|$|R
40|$|New {{scientific}} instruments {{are starting to}} generate an unprecedented amount of data. LOFAR, one of the Square Kilometre Array pathfinders, is already producing data on a petabyte scale. The calibration of these data presents a huge challenge for final users: a) extensive storage and computing resources are required; b) the installation and maintenance of the processing software is not trivial; and c) the requirements of (experimental) calibration pipelines are quickly evolving. After encountering some limitations in classical infrastructures, we investigated the viability of cloud infrastructures as a solution. We found that the installation and operation of LOFAR data calibration pipelines is not only possible, but can also be efficient in cloud infrastructures. The main advantages were: (1) ease of software installation and maintenance, {{and the availability of}} standard APIs and tools, widely used in the industry; this reduces the requirement for significant manual intervention, which can have a highly negative impact; (2) flexibility to adapt the infrastructure {{to the needs of the}} problem, especially as those demands change over time; (3) on-demand consumption of (shared) resources. We found no significant impediments associated with the speed of data transfer, the use of external block storage, or the memory available. However, the availability of <b>scratch</b> <b>storage</b> areas of an appropriate size is critical. Finally, we considered the cost-effectiveness of a commercial cloud like Amazon Web Services. While it is more expensive than the operation of a large, fully-utilised cluster completely dedicated to LOFAR data reduction, its costs are competitive if the number of datasets to be analysed is not high, or if the costs of maintaining the dedicated system become high. Coupled with the advantages discussed above, this suggests that a cloud infrastructure may be favourable for many users. Comment: 18 pages, 6 figures, 3 tables, accepted for publication in Astronomy and Computin...|$|E
40|$|New {{scientific}} instruments {{are starting to}} generate an unprecedented amount of data. The Low Frequency Array (LOFAR), one of the Square Kilometre Array (SKA) pathfinders, is already producing data on a petabyte scale. The calibration of these data presents a huge challenge for final users: (a) extensive storage and computing resources are required; (b) the installation and maintenance of the software required for the processing is not trivial; and (c) the requirements of calibration pipelines, which are experimental and under development, are quickly evolving. After encountering some limitations in classical infrastructures like dedicated clusters, we investigated the viability of cloud infrastructures as a solution. We found that the installation and operation of LOFAR data calibration pipelines is not only possible, but can also be efficient in cloud infrastructures. The main advantages were: (1) the ease of software installation and maintenance, {{and the availability of}} standard APIs and tools, widely used in the industry; this reduces the requirement for significant manual intervention, which can have a highly negative impact in some infrastructures; (2) the flexibility to adapt the infrastructure {{to the needs of the}} problem, especially as those demands change over time; (3) the on-demand consumption of (shared) resources. We found that a critical factor (also in other infrastructures) is the availability of <b>scratch</b> <b>storage</b> areas of an appropriate size. We found no significant impediments associated with the speed of data transfer, the use of virtualization, the use of external block storage, or the memory available (provided a minimum threshold is reached). Finally, we considered the cost-effectiveness of a commercial cloud like Amazon Web Services. While a cloud solution is more expensive than the operation of a large, fully-utilized cluster completely dedicated to LOFAR data reduction, we found that its costs are competitive if the number of datasets to be analysed is not high, or if the costs of maintaining a system capable of calibrating LOFAR data become high. Coupled with the advantages discussed above, this suggests that a cloud infrastructure may be favourable for many users. We acknowledge the useful comments of the anonymous referee. We would like to acknowledge the work of all the developers and packagers of the LOFAR software that constitute the core of the processing pipelines (including factor, prefactor, LSMTool, LoSoTo, and the Kern Suite), as well as the useful discussions with the participants in the LOFAR blank fields and direction dependent calibration teleconferences over the years. JS and PNB are grateful for financial support from STFC via grant ST/M 001229 / 1. This work has been also supported by the projects ‘AMIGA 5 : gas in and around galaxies. Scientific and technological preparation for the SKA’ (AYA 2014 - 52013 -C 2 - 1 -R) and ‘AMIGA 6 : gas in and around galaxies. Preparation for SKA science and contribution to the design of the SKA data flow’ (AYA 2015 - 65973 -C 3 - 1 -R) both of which were co-funded by MICINN and FEDER funds and the Junta de Andalucía (Spain) grants P 08 -FQM- 4205 and TIC- 114. We would like to explicitly acknowledge Dr Jose Ruedas – chief of the computer centre and responsible of the computing and communications infrastructures at IAA-CSIC – and Rafael Parra – system administrator of the IAA computing cluster – for their technical assistance. We acknowledge the joint SKA and AWS Astrocompute proposal call that was used to fund all the tests in the AWS infrastructure with the projects “Calibration of LOFAR ELAIS-N 1 data in the Amazon cloud” and “Amazon Cloud Processing of LOFAR Tier- 1 surveys: Opening up a new window on the Universe”. This work made use of the University of Hertfordshire’s high-performance computing facility and the LOFAR-UK computing facility, supported by STFC [grant number ST/P 000096 / 1]. This work benefited from services and resources provided by the fedcloud. egi. eu Virtual Organization, supported by the national resource providers of the EGI Federation. We acknowledge the resources and support provided by the STFC RAL Cloud infrastructure. LOFAR, the Low Frequency Array designed and constructed by ASTRON, has facilities in several countries, that are owned by various parties (each with their own funding sources), and that are collectively operated by the International LOFAR Telescope (ILT) foundation under a joint scientific policy. Peer ReviewedAward-winningPostprint (author's final draft...|$|E

