285|676|Public
25|$|<b>Simple</b> <b>{{hypothesis}}</b> : Any hypothesis which specifies {{the population}} distribution completely.|$|E
60|$|Thus it {{happened}} that, on {{announcement of the}} gradual and perfectly regular decrease observed in the orbit of Enck’s comet, at every successive revolution about our Sun, astronomers were nearly unanimous in the opinion that the cause in question was found—that a principle was discovered sufficient to account, physically, for that final, universal agglomeration which, I repeat, the analogical, symmetrical or poetical instinct of Man had predetermined to understand as {{something more than a}} <b>simple</b> <b>hypothesis.</b>|$|E
60|$|In {{one sense}} {{it must be}} {{admitted}} that we can never prove the existence of things other than ourselves and our experiences. No logical absurdity results from {{the hypothesis that the}} world consists of myself and my thoughts and feelings and sensations, and that everything else is mere fancy. In dreams a very complicated world may seem to be present, and yet on waking we find it was a delusion; that is to say, we find that the sense-data in the dream do not appear to have corresponded with such physical objects as we should naturally infer from our sense-data. (It is true that, when the physical world is assumed, it is possible to find physical causes for the sense-data in dreams: a door banging, for instance, may cause us to dream of a naval engagement. But although, in this case, there is a physical cause for the sense-data, there is not a physical object corresponding to the sense-data {{in the way in which}} an actual naval battle would correspond.) There is no logical impossibility in the supposition that the whole of life is a dream, in which we ourselves create all the objects that come before us. But although this is not logically impossible, there is no reason whatever to suppose that it is true; and it is, in fact, a less <b>simple</b> <b>hypothesis,</b> viewed as a means of accounting for the facts of our own life, than the common-sense hypothesis that there really are objects independent of us, whose action on us causes our sensations.|$|E
40|$|Hypotheses {{about the}} shape of causal reality admit of both theistic and non-theistic interpretations. I argue that, on the <b>simplest</b> <b>hypotheses</b> about the causal shape of reality—infinite regress, {{contingent}} initial boundary, necessary initial boundary—there {{is good reason to}} suppose that non-theism is always either preferable to, or at least the equal of, theism, at least insofar as we restrict our attention merely to the domain of explanation of existence. Moreover, I suggest that it is perfectly proper for naturalists to be undecided between these <b>simple</b> <b>hypotheses</b> about the causal shape of reality: contrary to the proponents of cosmological arguments, there are no decisive objections to any of these <b>simple</b> <b>hypotheses...</b>|$|R
40|$|Inductive {{learning}} searches {{an optimal}} hypothesis that minimizes a given loss function. It is usually {{assumed that the}} <b>simplest</b> <b>hypothesis</b> that fits the data is the best approximate to an optimal hypothesis. Since finding the <b>simplest</b> <b>hypothesis</b> is NP-hard for most representations, we generally employ various heuristics to search its closest match. Computing these heuristics incurs significant cost, making learning inefficient and unscalable for large dataset. In the same time, it is still questionable if the <b>simplest</b> <b>hypothesis</b> is indeed the closest approximate to the optimal model. Recent success of combining multiple models, such as bagging, boosting and meta-learning, has greatly improved {{the accuracy of the}} <b>simplest</b> <b>hypothesis,</b> providing a strong argument against the optimality of the <b>simplest</b> <b>hypothesis.</b> However, computing these combined hypotheses incurs significantly higher cost. In this paper, we first advert {{that as long as the}} error of a hypothesis on each example is within a range dictated by a given loss function, it can still be optimal. Contrary to common beliefs, we propose a completely random decision tree algorithm that achieves much higher accuracy than the single best hypothesis and is comparable to boosted or bagged multiple best hypotheses. The advantage of multiple random tree is its training efficiency as well as minimal memory requirement. 1...|$|R
2500|$|More generally, [...] can be {{some measure}} of the {{complexity}} of the <b>hypothesis</b> , so that <b>simpler</b> <b>hypotheses</b> are preferred.|$|R
5000|$|<b>Simple</b> <b>{{hypothesis}}</b> : Any hypothesis which specifies {{the population}} distribution completely.|$|E
5000|$|<b>Simple</b> <b>{{hypothesis}}</b> : Any hypothesis which specifies {{the population}} distribution completely. For such a hypothesis the sampling distribution of any statistic {{is a function}} of the sample size alone.|$|E
5000|$|In statistics, {{the size}} of a test is the {{probability}} of falsely rejecting the null hypothesis. That is, it is the probability of making a Type I error. It is denoted by the Greek letter α (alpha). For a <b>simple</b> <b>hypothesis,</b> ...|$|E
5000|$|In general, {{regarding}} <b>simple</b> <b>hypotheses</b> on parameter θ ( [...] for example):H0: θ=θ0vs.H1: θ=θ1,the {{likelihood ratio}} test statistic can be referred as: ...|$|R
3000|$|... a <b>simple</b> <b>hypotheses</b> on the {{magnetised}} sources, i.e. a flat, 1 -km-thick layer {{with top}} 5 km below sea level, bearing a ± 10 A/m magnetisation.|$|R
3000|$|... 2. Below we {{will again}} {{make use of}} the {{function}} M of (2) where the <b>simple</b> <b>hypotheses</b> have been replaced by our new joint hypotheses H [...]...|$|R
5000|$|The Feit - Thompson theorem {{states that}} a finite group is always {{solvable}} if its order {{is an odd}} number. This {{is an example of}} odd numbers playing a role in an advanced mathematical theorem where the method of application of the <b>simple</b> <b>hypothesis</b> of [...] "odd order" [...] is far from obvious.|$|E
5000|$|However, it is {{only rarely}} that one needs to test a <b>simple</b> <b>{{hypothesis}},</b> when a fixed [...] as a hypothesis is given. Much more often, one needs to verify parametric hypotheses where the hypothetical , depends on some parameters , which the hypothesis does not specify and which have to be estimated from the sample [...] itself.|$|E
50|$|In this summary, 227 of the 320 {{experts have}} a {{statistical}} accuracy score less than 0.05, {{which is the}} traditional rejection threshold for <b>simple</b> <b>hypothesis</b> testing. Half of the experts score below 0.005, and roughly one third fall into the abysmal range below 0.0001. These numbers challenge {{the assumption that the}} predicate “expert” is a guarantee of quality with regard to uncertainty quantification.|$|E
25|$|Other {{methods for}} {{inferring}} evolutionary relationships use parsimony {{in a more}} traditional way. Likelihood methods for phylogeny use parsimony as they do for all likelihood tests, with hypotheses requiring few differing parameters (i.e., numbers of different rates of character change or different frequencies of character state transitions) being treated as null hypotheses relative to hypotheses requiring many differing parameters. Thus, complex hypotheses must predict data much better than do <b>simple</b> <b>hypotheses</b> before researchers reject the <b>simple</b> <b>hypotheses.</b> Recent advances employ information theory, a close cousin of likelihood, which uses Occam's razor in the same way.|$|R
40|$|This paper {{considers}} a cell population {{model with a}} general maturation rate. This model is described by a nonlinear PDE. We use the theory of operator semigroups to study the problem under <b>simple</b> <b>hypotheses</b> on the growth function and the nonlinear term. By showing that a related operator generates a strongly continuous semigroup, we prove {{the existence of a}} classical solution of the nonlinear problem and its positivity. It is also proved that under <b>simple</b> <b>hypotheses,</b> the problem generates a semiflow. The invariance of the semiflow is studied as well. 1...|$|R
50|$|Other {{methods for}} {{inferring}} evolutionary relationships use parsimony {{in a more}} traditional way. Likelihood methods for phylogeny use parsimony as they do for all likelihood tests, with hypotheses requiring few differing parameters (i.e., numbers of different rates of character change or different frequencies of character state transitions) being treated as null hypotheses relative to hypotheses requiring many differing parameters. Thus, complex hypotheses must predict data much better than do <b>simple</b> <b>hypotheses</b> before researchers reject the <b>simple</b> <b>hypotheses.</b> Recent advances employ information theory, a close cousin of likelihood, which uses Occam's razor in the same way.|$|R
5000|$|A <b>simple</b> <b>hypothesis</b> {{concerning}} {{the distribution of}} many anaphoric elements, of personal pronouns in particular, is that linear order plays a role. In most cases, a pronoun follows its antecedent, and in many cases, the coreferential reading is impossible if the pronoun precedes its antecedent. The following sentences suggest that pure linear can indeed be important for the distribution of pronouns: ...|$|E
50|$|Mainly {{developed}} by the Belgian-Dutch school headed by I. Prigogine, working on a <b>simple</b> <b>hypothesis</b> of local thermodynamic equilibrium, CIT assumes the existence of field laws of the diffusion type. Mathematically, these are parabolic partial differential equations. They entail that a locally applied disturbance propagates at infinite velocity across the body. This contradicts both experimental evidence and the principle of causality. The latter requires that an effect comes after the application of its cause.|$|E
5000|$|The {{inclusion}} of a foreword anonymously written by the Lutheran philosopher Andreas Osiander, stating that the whole work is only a <b>simple</b> <b>hypothesis</b> and intended to facilitate computation, which contradicts the content of Copernicus' work, is a rather controversial feature of the edition by Petreius. Petreius had sent a copy to Hieronymus Schreiber, an astronomer from Nuremberg who died in 1547 in Paris, but left a note in the book about the authorship of Osiander. Via Michael Mästlin, the book came to Johannes Kepler, who uncovered Osiander's deed.|$|E
30|$|In change {{detection}} problem, {{the frame}} of discernment Θ={u,c}, where u represents unchanged class and c represents changed class. In our work, we consider the <b>simple</b> <b>hypotheses</b> and double hypotheses [33].|$|R
40|$|This paper {{sheds light}} on a strong {{connection}} between AdaBoost and several optimization algorithms for data mining. AdaBoost {{has been the subject}} of much interests as an effective methodology for classification task. AdaBoost repeatedly generates one hypothesis in each round, and finally it is able to make a highly accurate prediction by taking a weighted majority vote on the resulting hypotheses. Freund and Schapire have remarked that the use of <b>simple</b> <b>hypotheses</b> such as singletest decision trees instead of huge trees would be promising for achieving high accuracy and avoiding overfitting to the training data. One major drawback of this approach however is that accuracies of <b>simple</b> individual <b>hypotheses</b> may not always be high, hence demanding a way of computing more accurate (or, the most accurate) <b>simple</b> <b>hypotheses</b> efficiently...|$|R
50|$|A {{test for}} homogeneity, {{in the sense}} of exact {{equivalence}} of statistical distributions, can be based on an E-statistic. A location test tests the <b>simpler</b> <b>hypothesis</b> that distributions have the same location parameter.|$|R
5000|$|This {{led to a}} <b>simple</b> <b>hypothesis</b> that VAP {{directly}} binds FFAT motifs, {{which was}} tested by biochemical interaction between purified components, and was later confirmed by structural analysis of VAP-FFAT complexes, both by X-ray crystallography [...] and by NMR. The crystallography study indicated that the parts of FFAT that interact most strongly with VAP were F2 and D4, each binding in pockets on a very electropositive, highly conserved face in the major sperm protein domain of VAP. The NMR study indicated a “fly-casting” process, whereby a weak non-specific electrostatic interaction between VAP and the acidic tract precedes the more specific high affinity interaction with EFFDAxE.|$|E
50|$|Language is a {{cognitive}} skill which develops throughout {{the life of}} an individual. This developmental process has been examined using a number of techniques, and a computational approach is one of them. Human language development does provide some constraints which make it harder to apply a computational method to understanding it. For instance, during language acquisition, human children are largely only exposed to positive evidence. This means that during the linguistic development of an individual, only evidence for what is a correct form is provided, and not evidence for what is not correct. This is insufficient information for a <b>simple</b> <b>hypothesis</b> testing procedure for information as complex as language, and so provides certain boundaries for a computational approach to modeling language development and acquisition in an individual.|$|E
40|$|Bayesian {{inference}} of {{the variance}} of the normal distribution is considered using moving extremes ranked set sampling (MERSS) and is compared with the simple random sampling (SRS) method. Generalized maximum likelihood estimators (GMLE), confidence intervals (CI), and different testing hypotheses are considered using <b>simple</b> <b>hypothesis</b> versus <b>simple</b> <b>hypothesis,</b> <b>simple</b> <b>hypothesis</b> versus composite alternative, and composite hypothesis versus composite alternative based on MERSS and compared with SRS. It is shown that modified inferences using MERSS are more efficient than their counterparts based on SRS...|$|E
5000|$|Minimum {{description}} length: when {{forming a}} hypothesis, attempt {{to minimize the}} length of the description of the hypothesis. The assumption is that <b>simpler</b> <b>hypotheses</b> {{are more likely to be}} true. See Occam's razor.|$|R
40|$|Sequential {{approach}} to hypotheses testing {{is used in}} medical trials, statistical quality control, finance and other applications (see Mukhopadhyay et al. (2004)). Sequential tests are applied efficiently if observed data satisfy the hypothetical model. However, some part of observations does not follow the theoretical models in practice, so the models are distorted by outliers (see Huber (2004) and Hampel et al. (1984)). This results in considerable difference of the performance characteristics of sequential tests from the hypothetical values, and sequential tests lose their optimal properties. In the paper we develop the approach proposed in Kharin (2002) for robustness analysis of sequential testing of <b>simple</b> <b>hypotheses</b> on discrete distributions and the robustification method proposed in Kharin et al. (2002), for robustness analysis of sequential tests of <b>simple</b> <b>hypotheses</b> on the parameter of distributions. We also discuss some robustification techniques, and construct the robust sequential test under outliers. The approximate expressions for error probabilities of sequential tests of <b>simple</b> <b>hypotheses</b> under outliers are obtained, and the accuracy for these approximations is given. The asymptoti...|$|R
40|$|The paper {{moves in}} a {{theoretical}} {{context in which}} the level of economic activity is dependent on aggregate demand in the long {{as well as in the}} short run. The paper shows that given two <b>simple</b> <b>hypotheses</b> the economic system will tend to grow regardless of what happens to the average level of investment over time. The two <b>simple</b> <b>hypotheses</b> are a) that the marginal propensity to consume of the community is lower when the income contracts in the slumps than it is when income increases and b) that investment oscillates over time. These two assumptions are sufficient to identify a source of economic growth which is endogenous to the system...|$|R
40|$|A {{problem is}} {{presented}} that involves <b>simple</b> <b>hypothesis</b> testing {{in regards to}} two different proportions. The Neyman-Pearson Theorem defines a rule in which the best critical region is derived for testing a <b>simple</b> <b>hypothesis.</b> Sequential Analysis builds upon this theorem as another form of sampling within hypothesis tests, in which the number of samples is not predetermined before sampling. Abraham Wald's Sequential Probability Ratio Test, along with standard testing {{will be used to}} make conclusions about our original problem concerning the <b>simple</b> <b>hypothesis</b> test. Contents 1 Introduction 2 1. 1 Statement of The Problem.................... 2 1. 2 Details of The Problem..................... 3 1. 2. 1 Goal............................ 3 1. 2. 2 Distribution........................ 3 1. 2. 3 Error........................... 4 1. 2. 4 Independence.................. [...] . ...|$|E
3000|$|Theorem 2.1 {{shows that}} under <b>simple</b> <b>hypothesis</b> {{conditions}} (1.2), (1.3), and (2.1), the solutions of (1.1) {{will remain in}} the positive cone [...]...|$|E
40|$|We derive the Bartlett {{correction}} for a <b>simple</b> <b>hypothesis</b> on {{the regression}} parameters in a multivariate stationary autoregressive process. Three applications illustrate {{the use of}} the correction: the test for absence of autocorrelation of any order, a <b>simple</b> <b>hypothesis</b> on the autoregressive parameters and two tests for weak exogeneity in the cointegrated VAR model. In the first of these tests, the cointegration space is known, in the second it is not. The Bartlett correction performs well in all simulation studies, except in the one of the last test, that is a test for weak exogeneity in the cointegrated VAR with an unknown cointegration space. ...|$|E
5000|$|Mind change bounds {{are closely}} related to mistake bounds that are studied in {{statistical}} learning theory. [...] Kevin Kelly has suggested that minimizing mind changes is closely related to choosing maximally <b>simple</b> <b>hypotheses</b> in the sense of Occam’s Razor.|$|R
5000|$|In statistics, the Neyman-Pearson lemma, {{named after}} Jerzy Neyman and Egon Pearson, states that when {{performing}} a hypothesis test between two <b>simple</b> <b>hypotheses</b> H0: θ = θ0 and H1: θ = θ1, the likelihood-ratio test which rejects H0 {{in favour of}} H1 when ...|$|R
40|$|The {{asymptotic}} state discrimination {{problem with}} <b>simple</b> <b>hypotheses</b> is {{considered for a}} cubic lattice of bosons. A complete solution is provided for {{the problems of the}} Chernoff and the Hoeffding bounds and Stein’s lemma in the case when both hypotheses are gauge-invariant Gaussian states with translation-invariant quasi-free parts. ...|$|R
