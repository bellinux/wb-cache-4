636|20|Public
25|$|If {{there exists}} a minimal <b>sufficient</b> <b>statistic,</b> and this is usually the case, then every {{complete}} <b>sufficient</b> <b>statistic</b> is necessarily minimal sufficient(note that this statement does not exclude the option of a pathological {{case in which a}} complete sufficient exists while there is no minimal <b>sufficient</b> <b>statistic).</b> While it is hard to find cases in which a minimal <b>sufficient</b> <b>statistic</b> does not exist, it is not so hard to find cases {{in which there is no}} complete statistic.|$|E
25|$|A related {{concept is}} that of linear {{sufficiency}}, which is weaker than sufficiency but can be applied in some cases {{where there is no}} <b>sufficient</b> <b>statistic,</b> although it is restricted to linear estimators. The Kolmogorov structure function deals with individual finite data; the related notion there is the algorithmic <b>sufficient</b> <b>statistic.</b>|$|E
25|$|Intuitively, {{a minimal}} <b>sufficient</b> <b>statistic</b> most {{efficiently}} captures all possible {{information about the}} parameter θ.|$|E
5000|$|... where m is {{the sample}} maximum. This is a scaled and shifted (so unbiased) {{transform}} {{of the sample}} maximum, which is a <b>sufficient</b> and complete <b>statistic.</b> See German tank problem for details.|$|R
2500|$|... {{which results}} in that [...] It is also clear from the {{equation}} above {{that there might be}} a huge difference between [...] and [...] if the condition is not satisfied, as can be demonstrated by toy examples. Crucially, it was shown that sufficiency for [...] or [...] alone, or for both models, does not guarantee sufficiency for ranking the models. However, it was also shown that any <b>sufficient</b> summary <b>statistic</b> for a model [...] in which both [...] and [...] are nested is valid for ranking the nested models.|$|R
40|$|We {{study the}} {{generalized}} linear latent variable models without requiring a distributional {{assumption of the}} latent variable. Using a geometric approach, we derive consistent semiparametric estimators. We demonstrate that these models possess a property {{similar to that of}} a <b>sufficient</b> complete <b>statistic,</b> which enables us to simplify the estimating procedure and explicitly write down the semiparametric estimating equations. We further show that the explicit estimators derived have the usual root-n consistency and asymptotic normality. We explain the computational implementation of our method and illustrate the numerical performance of the estimators in finite sample situation via extensive simulation studies. Advantage of our estimators over the existing likelihood approach is also shown via numerical comparison. We further employ the method to analyze a real data example from economics...|$|R
25|$|<b>Sufficient</b> <b>statistic,</b> a {{function}} of the data through which the MLE (if it exists and is unique) will depend on the data.|$|E
25|$|If X1,....,X'n are {{independent}} {{and have a}} Poisson distribution with parameter λ, then the sum T(X) =X1+...+X'n is a <b>sufficient</b> <b>statistic</b> forλ.|$|E
25|$|This theorem {{shows that}} {{sufficiency}} (or rather, {{the existence of}} a scalar or vector-valued of bounded dimension <b>sufficient</b> <b>statistic)</b> sharply restricts the possible forms of the distribution.|$|E
40|$|We study {{generalized}} linear {{latent variable}} models without requiring a distributional {{assumption of the}} latent variables. Using a geometric approach, we derive consistent semiparametric estimators. We demonstrate that these models have a property {{which is similar to}} that of a <b>sufficient</b> complete <b>statistic,</b> which enables us to simplify the estimating procedure and explicitly to formulate the semiparametric estimating equations. We further show that the explicit estimators have the usual root "n" consistency and asymptotic normality. We explain the computational implementation of our method and illustrate the numerical performance of the estimators in finite sample situations via extensive simulation studies. The advantage of our estimators over the existing likelihood approach is also shown via numerical comparison. We employ the method to analyse a real data example from economics. Copyright (c) 2010 Royal Statistical Society. ...|$|R
40|$|Summary. We study {{generalized}} linear {{latent variable}} models without requiring a distributional {{assumption of the}} latent variables. Using a geometric approach, we derive consistent semiparametric estimators. We demonstrate that these models have a property {{which is similar to}} that of a <b>sufficient</b> complete <b>statistic,</b> which enables us to simplify the estimating procedure and explicitly to formulate the semiparametric estimating equations. We further show that the explicit estimators have the usual root n consistency and asymptotic normality. We explain the computational implementation of our method and illustrate the numerical performance of the estimators in finite sample situations via extensive simulation studies. The advantage of our estimators over the existing likelihood approach is also shown via numerical comparison. We employ the method to analyse a real data example from economics...|$|R
40|$|Summary {{statistics}} {{are widely used}} in population genetics, but they suffer from the drawback that no simple <b>sufficient</b> summary <b>statistic</b> exists, which captures all information required to distinguish different evolutionary hypotheses. Here, we apply boosting, a recent statistical method that combines simple classification rules to maximize their joint predictive performance. We show that our implementation of boosting has a high power to detect selective sweeps. Demographic events, such as bottlenecks, do not result in a large excess of false positives. A comparison to other neutrality tests shows that our boosting implementation performs well compared to other neutrality tests. Furthermore, we evaluated the relative contribution of different summary statistics to the identification of selection and found that for recent sweeps integrated haplotype homozygosity is very informative whereas older sweeps are better detected by Tajima's π. Overall, Watterson's θ was found to contribute the most information for distinguishing between bottlenecks and selection...|$|R
25|$|This can {{be derived}} using the {{exponential}} family formula for the moment generating function of the <b>sufficient</b> <b>statistic,</b> {{because one of the}} sufficient statistics of the gamma distribution is ln(x).|$|E
25|$|In other words, a <b>sufficient</b> <b>statistic</b> T(X) for a {{parameter}} θ is {{a statistic}} {{such that the}} conditional distribution of the data X, given T(X), {{does not depend on}} the parameter θ.|$|E
25|$|Suppose p {{is unknown}} and an {{experiment}} is conducted {{where it is}} decided ahead of time that sampling will continue until r successes are found. A <b>sufficient</b> <b>statistic</b> for the experiment is k, the number of failures.|$|E
40|$|Abstract: We {{examine the}} locally {{efficient}} semiparametric estimator proposed by Tsiatis and Ma (2004) {{in the situation}} when a <b>sufficient</b> and complete <b>statistic</b> exists. We derive a closed form solution and show that when implemented in generalized linear models with normal measurement error, this estimator {{is equivalent to the}} efficient score estimator in Stefanski and Carroll (1987). We also demonstrate how other consistent semiparametric estimators naturally emerge. The method is used in an extension of the usual generalized linear models. Key words and phrases: Measurement error models, semiparametric estimator. 1...|$|R
40|$|The summary {{statistics}} {{are widely used}} in population genetics, but they suffer from the drawback that no simple <b>sufficient</b> summary <b>statistic</b> exists, which captures all information required to distinguish different evolutionary hypotheses. Here, we apply boosting, a recent statistical method that combines simple classification rules to maximize their joint predictive performance. We show that our implementation of boosting has a high power to detect selective sweeps. Demographic events, such as bottlenecks, do not result in a large excess of false positives. A comparison to other neutrality tests shows that our boosting implementation performs well compared to other neutrality tests. Furthermore, we evaluated the relative contribution of different {{summary statistics}} to the identification of selection and found that for recent sweeps integrated haplotype homozygosity is very informative whereas older sweeps are better detected by Tajima&# 39;s pi. Overall, Watterson&# 39;s theta was found to contribute the most information for distinguishing between bottlenecks and selection...|$|R
40|$|Let MΘX = (RX, X, PΘ = {pθ : θ ∈Θ}) be a {{parametrized}} {{statistical model}} and g : Θ→ G be a non-injective function characterizing a parameter of interest. The basic idea of partial sufﬁciency is to ﬁnd a (minimal) <b>statistic</b> <b>sufﬁcient</b> for making inference on g(θ). Following Fraser (1956), Barndorff-Nielsen (1978) has deﬁned {{a concept of}} S-sufﬁciency. Our contribution is ﬁrst to establish the connection between S-sufﬁciency and the identiﬁcation concept. Second, we establish some properties of S-sufﬁciency, in particular we compare the properties of sufﬁciency for the complete parameter with those of S-sufﬁciency...|$|R
25|$|The concept, due to Ronald Fisher, is {{equivalent}} to the statement that, conditional on the value of a <b>sufficient</b> <b>statistic</b> for a parameter, the joint probability distribution of the data does not depend on that parameter. Both the statistic and the underlying parameter can be vectors.|$|E
25|$|The Rao–Blackwell theorem {{states that}} if g(X) is {{any kind of}} {{estimator}} of a parameter θ, then the conditional expectation of g(X) given T(X), where T is a <b>sufficient</b> <b>statistic,</b> is typically a better estimator of θ, and is never worse. Sometimes one can very easily construct a very crude estimator g(X), and then evaluate that conditional expected value to get an estimator that is in various senses optimal.|$|E
25|$|Since each {{observation}} has expectation λ so {{does this}} sample mean. Therefore, the maximum likelihood estimate is an unbiased estimator of λ. It {{is also an}} efficient estimator, i.e. its estimation variance achieves the Cramér–Rao lower bound (CRLB). Hence it is minimum-variance unbiased. Also it can be proved that the sum (and hence the sample mean {{as it is a}} one-to-one function of the sum) is a complete and <b>sufficient</b> <b>statistic</b> for λ.|$|E
40|$|At {{the interim}} {{analyses}} of a clinical trial, it is appealing {{to modify the}} originally planned sample size {{in order to achieve}} an adequate power to detect a meaningful treatment effect. We propose a flexible sequential monitoring scheme through combining the self-designing and classical group sequential methods. The maximum sample size {{does not have to be}} specified in advance and one efficacy interim analysis is conducted for the purpose of possible early termination after the first block of data is observed. At the interim analysis for efficacy, the usual <b>sufficient</b> test <b>statistic</b> is used and the type I error rate is adjusted to maintain the overall nominal level. At the final analysis, the test is constructed from a weighted average of the blockwise test statistics based on the sequentially collected data. The weight function at each stage is determined by the observed data prior to that stage. The futility stopping rule allows the trial to be terminated when there is no beneficial treatment effect. We conduct simulation studies to evaluate the performance of the proposed design. Copyright © Taylor & Francis, Inc. link_to_subscribed_fulltex...|$|R
40|$|We {{describe}} a generalization of extensive-form games that greatly increases representational power while still allowing efficient computation in the zero-sum setting. A principal feature of our generalization {{is that it}} places arbitrary convex optimization problems at decision nodes, {{in place of the}} finite action sets typically considered. The possibly-infinite action sets mean we must “forget ” the exact action taken (feasible solution to the optimization problem), remembering instead only some <b>statistic</b> <b>sufficient</b> for playing the rest of the game optimally. Our new model provides an exponentially smaller representation for some games; in particular, we show how to compactly represent (and solve) extensive-form games with outcome uncertainty and a generalization of Markov decision processes to multi-stage adversarial planning games...|$|R
40|$|Based on {{observations}} of points uniformly distributed over a convex set in ^d, a new estimator for {{the volume of}} the convex set is proposed. The estimator is minimax optimal and also efficient non-asymptotically: it is nearly unbiased with minimal variance among all unbiased oracle-type estimators. Our approach is based on a Poisson point process model and as an ingredient, we prove that the convex hull is a <b>sufficient</b> and complete <b>statistic.</b> No hypotheses on the boundary of the convex set are imposed. In a numerical study, we show that the estimator outperforms earlier estimators for the volume. In addition, an improved set estimator for the convex body itself is proposed. Comment: slightly extended versio...|$|R
25|$|Because a Bayesian {{network is}} a {{complete}} model for the variables and their relationships, {{it can be used}} to answer probabilistic queries about them. For example, the network can be used to find out updated knowledge of the state of a subset of variables when other variables (the evidence variables) are observed. This process of computing the posterior distribution of variables given evidence is called probabilistic inference. The posterior gives a universal <b>sufficient</b> <b>statistic</b> for detection applications, when one wants to choose values for the variable subset which minimize some expected loss function, for instance the probability of decision error. A Bayesian network can thus be considered a mechanism for automatically applying Bayes' theorem to complex problems.|$|E
2500|$|... <b>sufficient</b> <b>statistic</b> by a nonzero {{constant}} and get another <b>sufficient</b> <b>statistic.</b>|$|E
2500|$|... which indicate, respectively, {{that the}} {{conditional}} probability of the parameter θ, given the <b>sufficient</b> <b>statistic</b> t, {{does not depend}} on the data x; and that the conditional probability of the parameter θ given the <b>sufficient</b> <b>statistic</b> t and the conditional probability of the data x given the <b>sufficient</b> <b>statistic</b> t are statistically independent.|$|E
40|$|In this paper, {{we propose}} a {{statistical}} theory on measurement and estimation of Rayleigh fading channels in wireless communications and provide complete {{solutions to the}} fundamental problems: What is the optimum estimator for the statistical parameters associated with the Rayleigh fading channel, and how many measurements are sufficient to estimate these parameters with the prescribed margin of error and confidence level? Our proposed statistical theory suggests that two testing signals of different strength be used. The maximum likelihood (ML) estimator is obtained for estimation of the statistical parameters of the Rayleigh fading channel that is both <b>sufficient</b> and complete <b>statistic.</b> Moreover, the ML estimator is the minimum variance (MV) estimator that in fact achieves the Cramér-Rao lower bound. ...|$|R
40|$|The random {{variable}} X taking values 0, 1, 2,…,x,… with probabilities pλ(x) = e−λλx/x!, where λ∈R 0 + is called a Poisson variable, and its distribution a Poisson distribution, with parameter λ. The Poisson distribution with parameter λ {{can be obtained}} as the limit, as n → ∞ and p → 0 {{in such a way}} that np → λ, of the binomial distribution with exponent n and parameter p. The family of Poisson distributions indexed by λ∈R 0 + is an exponential family, with natural parameter logλ and privileged <b>sufficient</b> and complete <b>statistic</b> X. Poisson distributions are often used in the modeling of count data for “rare events. ” As such, they also play a fundamental role in the so-called Poisson processes...|$|R
40|$|This paper {{examines}} {{a general}} class of inferential problems in semiparametric and nonparametric models defined by conditional moment restrictions. We construct tests for {{the hypothesis that}} at least one element of the identified set satisfies a conjectured (Banach space) "equality" and/or (a Banach lattice) "inequality" constraint. Our procedure is applicable to identified and partially identified models, and is shown to control the level, and under some conditions the size, asymptotically uniformly in an appropriate class of distributions. The critical values are obtained by building a strong approximation to the statistic and then bootstrapping a (conservatively) relaxed form of the <b>statistic.</b> <b>Sufficient</b> conditions are provided, including strong approximations using Koltchinskii's coupling. Leading important special cases encompassed by the framework we study include: (i) Tests of shape restrictions for infinite dimensional parameters; (ii) Confidence regions for functionals that impose shape restrictions on the underlying parameter; (iii) Inference for functionals in semiparametric and nonparametric models defined by conditional moment (in) equalities; and (iv) Uniform inference in possibly nonlinear and severely ill-posed problems...|$|R
2500|$|A <b>{{sufficient}}</b> <b>statistic</b> {{is minimal}} sufficient {{if it can}} be represented as a function of any other <b>sufficient</b> <b>statistic.</b> In other words, S(X) is minimal sufficient if and only if ...|$|E
2500|$|It is {{approached}} {{to within a}} constant distance by the graph of [...] for certain arguments (for instance, for [...] ). [...] For these 's we have [...] and the associated model [...] (witness for [...] ) is called an optimal set for , and its description of [...] bits is therefore an algorithmic <b>sufficient</b> <b>statistic.</b> We write `algorithmic' for `Kolmogorov complexity' by convention. [...] The main properties of an algorithmic <b>sufficient</b> <b>statistic</b> are the following: If [...] is an algorithmic <b>sufficient</b> <b>statistic</b> for , then ...|$|E
2500|$|A case {{in which}} there is no minimal <b>sufficient</b> <b>statistic</b> was shown by Bahadur, 1954. However, under mild {{conditions}}, a minimal <b>sufficient</b> <b>statistic</b> does always exist. In particular, in Euclidean space, these conditions always hold if the random variables (associated with [...] ) are all discrete or are all continuous.|$|E
40|$|This paper {{develops}} a unified framework for estimating {{the volume of}} a set in R^d based on observations of points uniformly distributed over the set. The framework applies to all classes of sets satisfying one simple axiom: a class {{is assumed to be}} intersection stable. No further hypotheses on the boundary of the set are imposed; in particular, the convex sets and the so-called weakly-convex sets are covered by the framework. The approach rests upon a homogeneous Poisson point process model. We introduce the so-called wrapping hull, a generalization of the convex hull, and prove that it is a <b>sufficient</b> and complete <b>statistic.</b> The proposed estimator of the volume is simply the volume of the wrapping hull scaled with an appropriate factor. It is shown to be consistent for all classes of sets satisfying the axiom and mimics an unbiased estimator with uniformly minimal variance. The construction and proofs hinge upon an interplay between probabilistic and geometric arguments. The tractability of the framework is numerically confirmed in a variety of examples. Comment: Condensed version with a new exampl...|$|R
40|$|We {{consider}} a small extent sensor network for event detection, in which nodes periodically take samples and then contend over a random access network to transmit their measurement packets to the fusion center. We consider two procedures at the fusion center for processing the measurements. The Bayesian setting, is assumed, that is, the fusion center has a prior distribution on the change time. In the first procedure, the decision algorithm at the fusion center is network-oblivious {{and makes a}} decision only when a complete vector of measurements taken at a sampling instant is available. In the second procedure, the decision algorithm at the fusion center is network-aware and processes measurements as they arrive, but in a time-causal order. In this case, the decision statistic depends on the network delays, whereas in the network-oblivious case, the decision statistic does not. This yields a Bayesian change-detection problem with a trade-off between the random network delay and the decision delay that is, a higher sampling rate reduces the decision delay but increases the random access delay. Under periodic sampling, in the network-oblivious case, {{the structure of the}} optimal stopping rule is the same as that without the network, and the optimal change detection delay decouples into the network delay and the optimal decision delay without the network. In the network-aware case, the optimal stopping problem is analyzed as a partially observable Markov decision process, in which the states of the queues and delays in the network need to be maintained. A <b>sufficient</b> decision <b>statistic</b> is the network state and the posterior probability of change having occurred, given the measurements received and the state of the network. The optimal regimes are studied using simulation...|$|R
40|$|The {{screening}} mammography programme {{has been}} running succesfully in the Czech republic for 13 years. Due to its ability to uncover earlystages of breast cancer, it helps to fight this serious disease. Although the positive outcome of this programme in the Czech republic has been already proved, there is only approximately 50 % {{of women who have}} been regularly treated with it. One of the major reasons for this fact is a fear of cancer induction caused by the use of X-rays during the mammography. The main goal of this thesis is to calculate {{the risk of breast cancer}} induction caused by the mammography and to compare benefits of the screening programme for a woman who goes for regular checkups and a woman who does not. The measurement of concrete doses was made in one screening centre center located in western bohemia region. The group of observed patients at the age of 45 - 59 was divided into three categories. The object of measurement was an average dose received during the screening mammography, plus an average dose received during an additional mammography, which may sometimes occure during the examination. These data was studied through a statistic investigation to prove that the received dose is not dependent on the patient´s age. In the theoretical part of this work main principals of the screening mammography programme and mammography itself are repeated. The methods of statistic investigation and the methodology for estimating breast cancer risks based on recommendations of ICRP Publication 103. In the practical part was proved, that the average dose received during the classical mammography does not depend on the age of the patient. This could not be proved for an additional mammography dose, because of not <b>sufficient</b> group of <b>statistic</b> data. The main hypothesis, that regular checkups using the mammographical screening programme is benefical for women, was verified...|$|R
