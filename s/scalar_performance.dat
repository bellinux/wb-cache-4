40|36|Public
5000|$|... #Caption: A {{canonical}} five-stage pipelined processor. In {{the best}} case scenario, it takes one clock cycle to complete one instruction {{and thus the}} processor can issue <b>scalar</b> <b>performance</b> (...) [...]|$|E
5000|$|Each J90 {{processor}} {{was composed}} of two chips - one for the scalar portion of the processor, {{and the other for}} the vector portion. The scalar chip was also notable for including a small (128 word) data cache to enhance <b>scalar</b> <b>performance.</b> (Cray machines have always had instruction caching.) ...|$|E
50|$|Cray {{was able}} to look at the failure of the STAR and learn from it. He decided that in {{addition}} to fast vector processing, his design would also require excellent all-around <b>scalar</b> <b>performance.</b> That way when the machine switched modes, it would still provide superior performance. Additionally they noticed that the workloads could be dramatically improved in most cases through the use of registers.|$|E
50|$|Trace zero {{varieties}} {{feature a}} better <b>scalar</b> multiplication <b>performance</b> than elliptic curves. This allows a fast arithmetic in this groups, which can {{speed up the}} calculations with a factor 3 compared with elliptic curves and hence speed up the cryptosystem.|$|R
40|$|Active {{libraries}} can {{be defined}} as libraries which play an active part in the compilation (in particular, the optimisation) of their client code. This paper explores the idea of delaying evaluation of expressions built using library calls, then generating code at runtime for the particular compositions that occur. We explore this idea with a dense linear algebra library for C++. The key optimisations in this context are loop fusion and array contraction. Our library automatically fuses loops, identifies unnecessary intermediate temporaries, and contracts temporary arrays to <b>scalars.</b> <b>Performance</b> is evaluated using a benchmark suite of linear solvers from ITL (the Iterative Template Library), and is compared with MTL (the Matrix Template Library). Excluding runtime compilation overheads (caching means they occur only on the first iteration), for larger matrix sizes, performance matches or exceeds MTL – and in some cases is more than 60 % faster. 1...|$|R
40|$|AbstractActive {{libraries}} can {{be defined}} as libraries which play an active part in the compilation, in particular, the optimisation of their client code. This paper explores the implementation of an active dense linear algebra library by delaying evaluation of expressions built using library calls, then generating code at runtime for the compositions that occur. The key optimisations in this context are loop fusion and array contraction. Our prototype C++ implementation, DESOLA, automatically fuses loops arising from different client calls, identifies unnecessary intermediate temporaries, and contracts temporary arrays to <b>scalars.</b> <b>Performance</b> is evaluated using a benchmark suite of linear solvers from ITL (Iterative Template Library), and is compared with MTL (Matrix Template Library), ATLAS (Automatically Tuned Linear Algebra) and IMKL (Intel Math Kernel Library). Excluding runtime compilation overheads (caching means they occur only on the first iteration), for larger matrix sizes, performance matches or exceeds MTL; when fusion of matrix operations occurs, performance exceeds that of ATLAS and IMKL...|$|R
5000|$|Another {{difference}} is that the main scalar units of the processor ran at half the speed of the vector unit. According to Amdahl's Law computers tend to run at the speed of their slowest unit, and in this case unless the program spent most of its time in the vector units, the slower <b>scalar</b> <b>performance</b> would make it 1/2 the performance of a Cray-1 at the same speed. The reason for this seemingly odd [...] "feature" [...] is unclear.|$|E
5000|$|When {{the machine}} was {{released}} in 1974, it quickly {{became apparent that the}} general performance was nowhere near what people expected. Very few programs can be effectively vectorized into a series of single instructions; nearly all calculations will rely on the results of some earlier instruction, yet the results had to clear the pipelines before they could be fed back in. This forced most programs to hit the high setup cost of the vector units, and generally the ones that did [...] "work" [...] were extreme examples. Making matters worse was that the basic <b>scalar</b> <b>performance</b> was sacrificed in order to improve vector performance. Any time that the program had to run scalar instructions, the overall performance of the machine dropped dramatically. (See Amdahl's Law.) ...|$|E
50|$|CDC's {{approach}} in the STAR used what is today known as a memory-memory architecture. This referred {{to the way the}} machine gathered data. It set up its pipeline to read from and write to memory directly. This allowed the STAR to use vectors of any length, making it highly flexible. Unfortunately, the pipeline had to be very long in order to allow it to have enough instructions in flight {{to make up for the}} slow memory. That meant the machine incurred a high cost when switching from processing vectors to performing operations on individual randomly located operands. Additionally, the low <b>scalar</b> <b>performance</b> of the machine meant that after the switch had taken place and the machine was running scalar instructions, the performance was quite poor. The result was rather disappointing real-world performance, something that could, perhaps, have been forecast by Amdahl's law.|$|E
5000|$|Attempts {{to achieve}} <b>scalar</b> and better <b>performance</b> have {{resulted}} in a variety of design methodologies that cause the CPU to behave less linearly and more in parallel. When referring to parallelism in CPUs, two terms are generally used to classify these design techniques: ...|$|R
40|$|This paper {{deals with}} the {{application}} of the weighted mixed regression estimation of the coefficients in a linear model when some values of some of the regressors are missing. Taking the weight factor as an arbitrary <b>scalar,</b> the <b>performance</b> of weighted mixed regression estimator in relation to the conventional least squares and mixed regression estimators is analyzed and the choice of scalar is discussed. Then taking the weight factor as a specific matrix, a family of estimators is proposed and its performance properties under the criteria of bias vector and mean squared error matrix are analyzed. ...|$|R
40|$|We {{evaluate}} the basic {{performance of the}} Intel iPSC/ 860 computer, which can have up to 128 Intel i 860 -based nodes connected together with a hypercube network topology. After giving {{a brief overview of}} the system, the properties and bottlenecks of the hardware architecture and software environment are discussed. Basic memory, <b>scalar</b> and vector <b>performance</b> of a single node is evaluated, and the communication performance and the overlap of computation and communication are analysed...|$|R
5000|$|This process {{gives rise}} to an {{inherent}} inefficiency in subscalar CPUs. Since only one instruction is executed at a time, the entire CPU must wait for that instruction to complete before proceeding to the next instruction. As a result, the subscalar CPU gets [...] "hung up" [...] on instructions which take more than one clock cycle to complete execution. Even adding a second execution unit (see below) does not improve performance much; rather than one pathway being hung up, now two pathways are hung up {{and the number of}} unused transistors is increased. This design, wherein the CPU's execution resources can operate on only one instruction at a time, can only possibly reach <b>scalar</b> <b>performance</b> (one instruction per clock cycle, [...] ). However, the performance is nearly always subscalar (less than one instruction per clock cycle, [...] ).|$|E
40|$|An {{extremum}} of a <b>scalar</b> <b>performance</b> {{function of}} seven TIPs: Tstack, λfuel, λox, RHfuel, RHox, pfuel, and pox is searched {{for using the}} Nelder-Mead (simplex search) algorithm*. The performance function is evaluated at a constant stack load as a steady-state value corresponding to a given set of the TIPs’ values. The algorithm creates and transforms a simplex, the vertices of which are the sets of TIPs, by replacing the worst vertex with a better one in every iteration...|$|E
40|$|We {{describe}} 3 D operators for extracting anatomical landmarks {{which are}} based on only first-order partial derivatives of an image. To improve the predictability of the extraction results we analyze certain properties of the operators. First, we provide a statistical interpretation in terms of the Cramér—Rao bound representing the minimal localization uncertainty. Second, we show that the operators can be derived on the basis of invariance principles. It turns out that the operators form a complete set of principal invariants. Third, we analyze the detection performance using a certain type of performance visualization and a <b>scalar</b> <b>performance</b> measure. Experimental results are presented for 3 D tomographi...|$|E
40|$|Despite {{years of}} study, branch mispredictions remain as a {{significant}} performance impediment in pipelined superscalar processors. In general, the branch misprediction penalty can be substantially larger than the frontend pipeline length (which is often equated with the misprediction penalty). We identify and quantify five contributors to the branch misprediction penalty: (i) the frontend pipeline length, (ii) the number of instructions since the last miss event (branch misprediction, 1 -cache miss, long D-cache miss) -this {{is related to the}} burstiness of miss events, (iii) the inherent ILP of the program, (iv) the functional unit latencies, and (v) the number of short (L 1) D-cache misses. The characterizations done in this paper are driven by 'interval analysis', an analytical approach that models superb <b>scalar</b> processor <b>performance</b> as a sequence of inter-miss intervals...|$|R
40|$|We {{propose a}} {{deterministic}} characterization of network traffic, based on service curves. The proposed characterization facilitates 1) performance analyses for both average and <b>scalar</b> worst-case <b>performance</b> guarantees, 2) a systematic approach to performance analyses (specifically, {{we show that}} queue-size, output traffic, virtual-delay, aggregate traffic, etc. at various points in a network can easily be characterized {{within the framework of}} the proposed definition), and 3) a systematic approach to measurement-based analyses of probabilistic performance guarantees that are inferred via sample-path computations. We also discuss the notion of burstiness. We indicate that it is the decay-rate of the tail of the queue size distribution that we observe in deciding the degree of burstiness of a flow with respect to another one, after some appropriate normalizations of the flows. The faster the decay-rate is, the less bursty the traffic is, and vice versa...|$|R
40|$|In this paper, {{we present}} a new {{architecture}} for highspeed pseudo vector processor based on a superscalar pipeline. Without using cache memory, the proposed architecture is able to overcome penalty of memory access latency by introducing register windows with register preloading and pipelined memory. One outstanding feature of the proposed architecture {{is that it is}} upward compatible with existing <b>scalar</b> architectures. <b>Performance</b> evaluation of the proposed architecture using the Livermore Loop Kernels shows over 6 times higher performance than a usual superscalar processor and 1. 2 times higher performance than a hypothetical extended model with cache prefetching technique with a memory access latency of 20 CPU clock cycles. List vectors are also effectively handled in a similar architecture. 1. Introduction Motivation Over the past several years, technological and architectural advances have realized dramatic improvements in microprocessor performance with peak performances becoming [...] ...|$|R
40|$|Man's {{nonlinear}} {{characteristics and}} {{his use of}} control with compatible and incompatible multiple inputs, both visual and vestibular were studied. Experiments were also made with pulse and bang-bang controllers {{and the effects of}} sudden changes in control stick mechanical impedance. Closing the loop through the dynamics of the controlled vehicle allowed experiments on the limits of control of unstable vehicles with and without motion cues. The inverted pendulum controlled element, programmed as a self pacing element, was used extensively as a <b>scalar</b> <b>performance</b> index. In addition, the motorbike equations of motion were studied with regard to required human equalization. Abstracts are included for a series of published data on manual control...|$|E
40|$|In {{order to}} meet the {{requirements}} of the NASA Langley Research Center for simulating the increased complexity and higher performance of modern aircraft, a flight simulation computing system with very high <b>scalar</b> <b>performance</b> is needed. The requirements and proposed response, probable areas of difficulty, planned implementation, and current status and plans are reviewed. A solution utilizing centralized minisupercomputers coupled with a proven real-time network technology will provide engineers and research scientists with the tools required for high-performance flight simulation. Subsequent to testing and verification of the initial simulation, general-purpose configuration management software with a nonconfiguration dependent serial highway driver software will be integrated to support any arbitrary combination of simulation sites...|$|E
40|$|In {{a recent}} Letter, Hunt and Ott argued that SHORT-period {{unstable}} periodic orbits (UPOs) {{would be the}} invariant sets associated with a chaotic attractor that {{are most likely to}} optimize the time average of some smooth <b>scalar</b> <b>performance</b> function. In this Comment, we show that their conclusion does not hold generally and that optimal time averages may specifically require long-period UPOs. This situation can arise when long-period UPOs are able to spend substantial amounts of time in a region of phase space that is close to large values of the performance function. Comment: One Page, 1 Figure, Double Column format. Submitted to Physical Review Letter...|$|E
40|$|We {{propose a}} novel {{approach}} for parallel discrete-event network simulation on packet-switched, point-to-point networks. Our algorithm resolves packet conflicts through priority sorting of appropriate integer conflict functions. We implement our method on CM- 5, Cray-T 3 D, and Cray-T 3 E systems using C and MPI, and perform critical optimizations aimed at reducing sorting overhead, minimizing inter-processor communication, and optimizing <b>scalar</b> processing. <b>Performance</b> results for a packet-switched hypercube topology indicate that our parallel simulation approach achieves good scalability and efficiency; our optimized simulator can process ¸ 500 K packet moves in 1 sec, with an efficiency that exceeds ¸ 50 % for a few thousands packets on the Cray-T 3 E with 32 PEs. 1 Introduction Simulation is a general approach for performance evaluation and testing of complex systems. Simulation speed depends on the system complexity and the efficiency of simulation algorithms. In discrete time-driven simulati [...] ...|$|R
40|$|In this paper, {{the cell}} face {{velocities}} in the discretization of the continuity equation, the momentum equation, and the scalar equation of a non-staggered grid system are calculated and discussed. Both the momentum interpolation and the linear interpolation are adopted {{to evaluate the}} coefficients in the discretized momentum and <b>scalar</b> equations. Their <b>performances</b> are compared. When the linear interpolation {{is used to calculate}} the coefficients, the mass residual term in the coefficients must be dropped to maintain the accuracy and convergence rate of the solution. © Shanghai University and Springer-Verlag Berlin Heidelberg 2012...|$|R
40|$|We {{consider}} zero-delay single-user and multi-user {{source coding}} with average distortion constraint and decoder side information. The zerodelay constraint translates into causal (sequential) encoder and decoder pairs {{as well as}} the use of instantaneous codes. For the single-user setting, we show that optimal performance is attained by time sharing at most two scalar encoder-decoder pairs, that use zero-error side information codes. Side information lookahead is shown to useless in this setting. We show that the restriction to causal encoding functions is the one that causes the performance degradation, compared to unrestricted systems, and not the sequential decoders or instantaneous codes. Furthermore, we show that even without delay constraints, if either the encoder or decoder are restricted a-priori to be <b>scalar,</b> the <b>performance</b> loss cannot be compensated by the other component, which can be scalar as well without further loss. Finally, we show that the multi-terminal source coding problem can be solved in the zero-delay regime and the rate-distortion region is given. ...|$|R
40|$|A {{discrete}} adjoint {{is developed}} {{on top of}} a parallel-computing code for the direct numerical simulation of wall turbulence. The architecture of the direct code is designed to obtain good <b>scalar</b> <b>performance</b> and high parallel efficiency using a cluster of dual-CPU Personal Computers. The adjoint code, written in a way that mimicks the basic structure of its direct counterpart, can also benefit of the same computational and parallel efficiency. After describing how we tested the adjoint part of the code against its direct counterpart, we present results concerning the sensitivity of the wall drag to blowing/suction applied at the wall for the case of a turbulent channel flow at Reτ = 180...|$|E
40|$|The typical {{formulation}} of an optimal control or dynamic optimization {{problem is to}} optimize a <b>scalar</b> <b>performance</b> functional; less frequently, also vectors of performance functionals are considered in multiobjective optimization. However, there are many practical problems where the objectives are {{stated in terms of}} desirable trajectories. If the goal would be to approximate the desired trajectory from both sides, then the problem could be equivalently stated as a typical approximation problem. However, in many cases the desired trajectories have the meaning of aspiration levels: if possible, they should be exceeded. The paper presents a mathematical {{formulation of}} a multiobjective trajectory optimization problem, an interpretation as a semi-regularization procedure for ill-posed problems, and examples of actual applications...|$|E
40|$|Much of Chapter III {{is drawn}} from reports by P. Iwanchuk and L. Rudsinski, who {{performed}} the <b>scalar</b> <b>performance</b> evaluation with the able assistance of T. Montoya and J. Miller. J. Moore performed the disk studies and much of Chapter VI is extracted from his reports. The discussion on Computation and 1 / 0 Interference in the sane chapter closely follows a report by J. C. Browne (Visiting Staff Member). R. Tanner edited the manuscript with contributions from many readers and {{with the aid of}} the word processing section of group C- 4. The vector performance studies (Chapter IV) were conducted by A. McKnight. Data reported in Chapter V was collected by N. Nagy and D. Buckner. T. Jordan, K. Witte, an...|$|E
40|$|Speed {{inaccuracy}} {{decreases the}} pump efficiency, reliability, and energy saving. This research {{is devoted to}} the determination of the ways of accurate speed control of the pump drives operated under changeable loads. The impact of speed inaccuracy on the pump performance is studied. Based on the analysis of methods for the static accuracy improvement, the drawbacks of the traditional approaches have been shown with reference to the pumping applications. A new methodology of the slip compensation has been proposed for implementation to improve the <b>scalar</b> drive <b>performance.</b> It notably decreases the speed inaccuracy of the open-ended pumping applications. The enhanced quality of the drive control at different loading conditions has been shown on a laboratory test bench. Also, for the multi-pump systems this approach results in an additional benefit from the viewpoint of the operation around the best operation point providing a safe pump control both to exclude the pump damage and to improve the process quality...|$|R
40|$|Abstract — In {{this paper}} we propose an {{efficient}} region of interest (ROI) coding technique based on multiwavelet transform, set partitioning in hierarchial (SPIHT) algorithm of medical images. This new method reduces the importance of background coefficients in the ROI code block without compromising algorithm complexity. By using this coding method the compressed bit stream are all embedded and suited for progressive transmission. Extensive experimental {{results show that the}} proposed algorithm gives better quality if images using multiwavelets compared to that of the <b>scalar</b> wavelets. The <b>performance</b> of the system has been evaluated based on bits per pixel (bpp), peak signal to noise ratio (PSNR) and mean square error (MSE) ...|$|R
40|$|Wood mice (Apodemus sylvaticus) and OF 1 albino mice (Mus musculus) were {{compared}} over durations ranging from 0. 5 to 7 s, using the differential reinforcement of response duration schedule (DRRD) and a 'platform' response, i. e. staying {{on a small}} platform for a specified criterion duration to be reinforced. Species-related differences were found for mean response durations, efficiency {{and the number of}} trials needed to reach a preset performance criterion. Coefficients of variation of response durations did not differ. Overall, OF 1 mice needed more trials than wood mice to reach a temporal criterion. However, over 3 - 7 s, data from both strains almost fitted the behavioral assumptions of <b>Scalar</b> Timing theory. <b>Performance</b> of mongolian gerbils (Meriones unguiculatus) trained in a similar setting was shown for visual comparison. Peer reviewe...|$|R
40|$|The {{convergence}} {{properties for}} reinforcement learning approaches such as temporal dif-ferences and Q-learning {{have been established}} under moderate assumptions for discrete state and action spaces. In practice, however, many systems have either continuous action spaces or {{a large number of}} discrete elements. This paper presents an approximate dynamic pro-gramming approach to reinforcement learning for continuous action set-point regulator prob-lems which learns near-optimal control policies based on <b>scalar</b> <b>performance</b> measures. The Continuous Action Space (CAS) algorithm uses derivative-free line search methods to obtain the optimal action in the continuous space. The theoretical convergence properties of the algorithm are presented. Several heuristic stopping criteria are investigated and practical ap-plication is illustrated on two example problems{the inverted pendulum balancing problem and the power system stabilization problem. ...|$|E
40|$|The {{issue of}} {{scalability}} {{is key to}} the success of massively parallel processing. Due to their distributed nature, message-passing multicomputers are appropriate for achieving <b>scalar</b> <b>performance.</b> However, the nzessagepassing programming model lacks programmability due to difficulties encountered by the programmers to partition and schedule the computation over the processors and to establish efficient inter-processor communication in the user code. Therefore, this paper presents a compile-time scheduling heuristic, called BLAS, that maps programs onto the processors of a message-passing multicomputer. In contrast to other methods proposed in the literature, BLAS takes a nwre global approach in attempt to balance the tradeoff between exploitation of parallelism and reducing communication overhead. To evaluate the effectiveness of BLAS, detailed simulation studies of scheduling SISAL programs are presented. ...|$|E
40|$|Summary: Receiver {{operating}} characteristic (ROC) analysis is usually applied in bioinformatics {{to evaluate the}} abilities of biological markers to differentiate between {{the presence or absence}} of a disease. It in-cludes the derivation of the useful <b>scalar</b> <b>performance</b> measure area under the ROC curve for binary classification tasks. As real applica-tions often deal with more than two classes, multicategory ROC ana-lysis and the corresponding hypervolume under the manifold (HUM) measure have become a topic of growing interest. To support researchers in carrying out multicategory ROC analysis, we have developed two tools in different programming environments which feature user-friendly, object-oriented and flexible interfaces and enable the user to compute HUM values and plot 2 D- and 3 D-ROC curves. Availability: The software is freely available from our Web sit...|$|E
40|$|Abstract: EPIC {{processor}} {{is one of}} {{the best}} ways to exploit the instruction level parallelism where multiple instructions are issued explicitly by the compiler. VLIW processor is the evolution of EPIC processing paradigm. Very Long Instruction Word (VLIW) is multi-issue processors that try to extract parallelism statically by the compiler. It execute a long instruction that consist of multiple operation. Even if VLIW processor provides higher performance than conventional processor some of application like scientific computing, Digital signal processing, military application may need more processing speed. If instruction set is customize in such a way that its instruction would probably operate on linear array of number (also called as vector) instead of <b>scalar</b> operand, then <b>performance</b> could be increased tremendously. So a VLIW processor can be customize to work as a VLVIW processor which may result in high performance computation...|$|R
40|$|In this paper, {{we present}} a new speech {{enhancement}} ap-proach, {{that is based on}} exploiting the intra-frame depen-dency of discrete cosine transform (DCT) domain coeffi-cients. It can be noted that the existing enhancement tech-niques treat the transform domain coefficients independently. Instead of this traditional approach of independently pro-cessing the scalars, we split the DCT domain noisy speech vector into sub-vectors and each sub-vector is enhanced in-dependently. Through this sub-vector based approach, the higher dimensional enhancement advantage, viz. non-linear dependency, is exploited. In the developed method, each clean speech sub-vector is modeled using a Gaussian mix-ture (GM) density. We show that the proposed Gaussian mixture model (GMM) based DCT domain method, using sub-vector processing approach, provides better performance than the conventional approach of enhancing the transform domain <b>scalar</b> components independently. <b>Performance</b> im-provement over the recently proposed GMM based time do-main approach is also shown. 1...|$|R
40|$|The {{burstiness}} {{of network}} traffic has {{a profound impact}} on the performance of many network protocols. However, a widely accepted definition of burstiness, be it either deterministic or probabilistic, does not exist in the networking community. Deterministic definitions of burstiness provide an initial insight into both traffic characterization and performance analysis. We propose a deterministic definition of burstiness for network traffic characterization, based on service curves. The proposed definition facilitates; (1) performance analyses for both average and <b>scalar</b> worst-case <b>performance</b> guarantees at the same time, (2) a simple systematic approach to performance guarantees, analytically (we show that the queue size, output traffic, virtual-delay, aggregate traffic, etc. at various points in a network can easily be char-acterized {{within the framework of the}} proposed denition), (3) a systematic analytical framework for measurement based analysis of probabilistic performance guarantees that would be inferred via sample-path analyses. We also discuss the notion of burstiness. We point out that one might want to perceive the burstiness of a ow from the perspective of a network element; specifically, based on the queue size behavior that it induces on a network element of interest. We indicate that it is the decay rate of the tail of the queue size distribution that we care about in deciding the degree of burstiness of a flow with respect to another one, after some appropriate normalizations of the flows. The faster the decay rate is the less bursty the traffic is, and vice versa...|$|R
