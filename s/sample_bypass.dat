1|37|Public
40|$|Tests were {{conducted}} to verify that the Generic Effluent Monitoring System (GEMS), as applied to the AP- 40 exhauster stack, meets all applicable regulatory performance criteria for air sampling systems at nuclear facilities. These performance criteria address both the suitability of the air sampling probe location and the transport of the sample to the collection devices. The criteria covering air sampling probe location ensure that the contaminants in the stack are well mixed with the airflow at the probe location such that the extracted sample represents the whole. The sample transport criteria ensure that the sampled contaminants are quantitatively delivered to the collection device. The specific performance criteria are described in detail in the report. The tests demonstrated that the GEMS/AP- 40 system meets all applicable performance criteria. The contaminant mixing tests {{were conducted}} by Pacific Northwest National Laboratory (PNNL) at the wind tunnel facility, 331 -H Building, using a mockup of the actual stack. The particle sample transport tests were conducted by PNNL at the Numatec Hanford Company`s 305 Building. The AP- 40 stack is typical of several 10 -in. diameter stacks that discharge the filtered ventilation air from tank farms at the U. S. Department of Energy`s Hanford Site in Richland, Washington. The GEMS design features a probe with a single shrouded sampling nozzle, a sample delivery line, and sample collection system. The collection system includes a filter holder to collect the sample of record and an in-line detector head and filter for monitoring beta radiation-emitting particles. Unrelated to the performance criteria, {{it was found that}} the record sample filter holder exhibited symptoms of <b>sample</b> <b>bypass</b> around the particle collection filter. This filter holder should either be modified or replaced with a different type. 10 refs., 8 figs., 6 tabs...|$|E
5000|$|The ozone {{molecule}} absorbs ultraviolet radiation, {{and most}} ozone monitors utilized in regulatory applications use ultraviolet absorption to accurately quantify ozone levels. An ozone monitor {{of this type}} operates by pulling an air sample from the atmosphere into the machine with an air pump. [...] During one cycle, the ozone monitor will take one air sample through the air inlet, and scrub the ozone from the air; for the next cycle, an air <b>sample</b> <b>bypasses</b> the scrubber and the ozone value calculated. The solenoid valve is electronically activated to shift the air flow either through the scrubber or to bypass it on a timed sequence. The {{difference between the two}} sampled values determines the actual ozone value at that time. The monitor may also have options to account for air pressure and air temperature to calculate the value of ozone.|$|R
40|$|In {{this work}} a {{systematic}} {{study of the}} potential of chiral solvents in enantioselective crystallization on the example of two chiral systems mandelic acid (compound forming system) and N-Methylephedrine (conglomerate forming system) in (L) -(+) -Diethyl tartrate (chiral solvent) have been investigated. To be able to realize this study a detailed knowledge regarding the underlying solid-liquid equilibria (ternary solubility phase diagram) and metastable zone width measurements is required. Based on these reasons, solubility measurements for mandelic acid and N-Methylephedrine in (L) -Diethyl tartrate at different temperatures were carried out. Also metastable zone width measurements with regard to primary nucleation at various temperatures have been carried out. In all these measurements different analytical technique were used. According to these initial investigations enantioselective (resolution) experiments were designed. The enantioselective crystallization was very successful and with little difficulties associated with very fine crystals blocking the filter holes in the <b>sampling</b> <b>bypass,</b> which prevented liquid phase flow. Further experiments conducted for the enantioselective crystallization in case of racemic-mandelic acid/water, which {{was used as a}} reference to compare with racemic-mandelic acid/(L) -Diethyl tartrate, so as to assess the effect magnitude of chiral solvents...|$|R
40|$|Malicious code (malware) that spreads {{through the}} Internet-such as viruses, worms and trojans-is {{a major threat}} to {{information}} security nowadays and a profitable business for criminals. There are several approaches to analyze malware by monitoring its actions while it is running in a controlled environment, which helps to identify malicious behaviors. In this article we propose a tool to analyze malware behavior in a non-intrusive and effective way, extending the analysis possibilities to cover malware <b>samples</b> that <b>bypass</b> current approaches and also fixes some issues with these approaches. Pages: 8059 - 2...|$|R
40|$|Four-terminal {{measurements}} of impedance spectra {{have long been}} troubled {{by the presence of}} high frequency artifacts that typically indicate unphysically large inductive behavior. We follow up on the observation of Fleig et al., that voltage and current are necessarily measured in different locations of the potentiostat circuit, and that, typically, the electrometer input is a virtual ground. In this case, the capacitance of coaxial cables that connect sample electrodes to the potentiostat provides a high frequency conduction path to ground, so that some of the current that passes through the <b>sample</b> <b>bypasses</b> the electrometer. In four-electrode measurements, this mechanism produces the observed inductive artifacts. We examine a variety of simulated samples, with calculations compared to {{measurements of}} relevant circuits, to quantitatively investigate the nature of the artifacts. Model results agree with measurements when the leakage capacitances are properly included in the circuit analyses. With understanding of the origin of the inductive artifacts, the four-electrode method can be effectively utilized, enabling a combination of two-, three- and four-electrode measurements to be used to best advantage. Using this combination of electrode configurations, temperature dependent measurements of SrTiO 3, Y 2 O 3 -stabilized ZrO 2, and In 2 O 3 films deposited on YSZ substrates are presented. © The Author(s) 2014. Published by ECS. This is an open access article distributed {{under the terms of the}} Creative Common...|$|R
40|$|Sequential {{techniques}} {{can enhance the}} efficiency of the approximate Bayesian computation algorithm, as in Sisson et al. ’s (2007) partial rejection control version. While this method is based upon the theoretical works of Del Moral et al. (2006), the application to approximate Bayesian computation results in a bias in the approximation to the posterior. An alternative version based on genuine importance <b>sampling</b> arguments <b>bypasses</b> this difficulty, in connection with the population Monte Carlo method of Cappé et al. (2004), and it includes an automatic scaling of the forward kernel. When applied to a population genetics example, it compares favourably with two other versions of the approximate algorithm. Some key words: Markov chain Monte Carlo; partial rejection control; importance sampling; sequential Monte Carlo. 1...|$|R
40|$|Adversarial attack {{has cast}} a shadow on the massive success of deep neural networks. Despite being almost {{visually}} identical to the clean data, the adversarial images can fool deep neural networks into wrong predictions with very high confidence. In this paper, however, we show that we can build a simple binary classifier separating the adversarial apart from the clean data with accuracy over 99 %. We also empirically show that the binary classifier is robust to a second-round adversarial attack. In other words, {{it is difficult to}} disguise adversarial <b>samples</b> to <b>bypass</b> the binary classifier. Further more, we empirically investigate the generalization limitation which lingers on all current defensive methods, including the binary classifier approach. And we hypothesize that this is the result of intrinsic property of adversarial crafting algorithms...|$|R
40|$|In {{the winter}} survey, Arcobacter spp. was {{isolated}} from 92 % of the mechanically deboned turkey (MDT) samples with 80 % {{of the samples}} positive for A. butzleri. The summer survey had 83 % of the MDT samples positive for Arcobacter spp. The high rate of positives from the two surveys, clearly indicates that Arcobacter spp. is prevalent in MDT. This may be cause for concern, especially if food testing laboratories are relying on the traditional isolation methods for Campylobacter from meat. Some of the samples could be misinterpretedCampylobacter for Arcobacter and Arcobacter for Campylobacter. This survey uses polymerase chain reaction (PCR) to detect the presence of Arcobacter spp. in the enriched <b>samples,</b> thus <b>bypassing</b> the typical plating and visual identification. Arcobacter butzleri species-specific probes were used to identify the A. butzleri positive samples...|$|R
40|$|Abstract: Consider a {{compound}} Poisson process with jump measure ν supported by finitely many positive integers. We propose {{a method for}} estimating ν from a single, equidistantly sampled trajectory and develop associated statistical procedures. The problem is motivated by the question whether nerve cells in the brain exhibit higher-order interactions in their firing patterns. According to the neuronal assembly hypothesis (Hebb [13]), synchronization of action potentials across neurons of different groups is considered a signature of assembly activity, but it was found notoriously difficult to demonstrate it in recordings of neuronal activity. Our approach based on {{a compound}} Poisson model allows to detect the presence of joint spike events of any order using only population spike count <b>samples,</b> thus <b>bypassing</b> both the “curse of dimensionality ” {{and the need to}} isolate singleneuron spike trains in population signals...|$|R
40|$|Consider a {{compound}} Poisson process with jump measure ν supported by finitely many positive integers. We propose {{a method for}} estimating ν from a single, equidistantly sampled trajectory and develop associated statistical procedures. The problem is motivated by the question whether nerve cells in the brain exhibit higher-order interactions in their firing patterns. According to the neuronal assembly hypothesis (Hebb [13]), synchronization of action potentials across neurons of different groups is considered a signature of assembly activity, but it was found notoriously difficult to demonstrate it in recordings of neuronal activity. Our approach based on {{a compound}} Poisson model allows to detect the presence of joint spike events of any order using only population spike count <b>samples,</b> thus <b>bypassing</b> both the "curse of dimensionality" {{and the need to}} isolate single-neuron spike trains in population signals. Comment: Published in at [URL] the Electronic Journal of Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|R
50|$|In {{the case}} of the D-50, for sound synthesis, the use of PCM <b>samples</b> can be <b>bypassed</b> {{completely}} when using Synth Partials only as the basis for sound creation. This effectively gives 4 DCO subtractive synthesis (resonant filters), which can produce some very analog like sounds that later synths could not due to their sole use of PCM samples, or 'more digital' sounding synthesis methods (such as FM or PM) as the basis for their sound creation. This digital subtractive synthesis method of sound production gave a similar, but colder and more clinical, sound when compared to the analog technology employed by previous Roland synthesizers, e.g. the previous flagship JX-10.|$|R
40|$|Abstract. Ranging from track {{interpolation}} techniques through model atmospheres to the stochastic {{nature of}} the IMF, there are many uncertainties which {{need to be taken}} into account when modelling HR diagrams or performing population synthesis, particularly if comparison with actual data is sought. In this paper, we highlight (1) the problem of discontinuities along evolutionary tracks of massive stars (M> 8 M⊙), showing that inconsistencies appear in the computation of the corresponding isochrones, and (2) the sampling fluctuations produced by the stochastic {{nature of the}} IMF, presenting a statistical formalism to estimate the dispersion in any given observable of a stellar population due to <b>sampling</b> effects which <b>bypasses</b> the need of performing Monte Carlo simulations. 1. Introduction an...|$|R
40|$|Environmental DNA {{surveys are}} used for {{screening}} eukaryotic diversity. However, {{it is unclear how}} quantitative this approach is and to what extent results from environmental DNA studies can be used for ecological studies requiring quantitative data. Mitochondrial cytochrome oxidase (COI) is used for species-level taxonomic studies of testate amoebae and should allow assessing the community composition from environmental <b>samples,</b> thus <b>bypassing</b> biases due to morphological identification. We tested this using a COI clone library approach and focusing on the Nebela collaris complex. Comparisons with direct microscopy counts showed that the COI clone library diversity data matched the morphologically identified taxa, and that community com-position estimates using the two approaches were similar. However, this correlation was improved when microscopy counts were corrected for biovolume. Higher correlation with biovolume-corrected community data suggests that COI clone library data matches the ratio of mitochondria and that within closely-related taxa the density of mitochondria per unit biovolume is approximately constant. Further developments of this metabarcoding approach including quantifying the mitochondrial density among closely-related taxa, experiments on other taxonomic groups and using high throughput sequencing should make if possible to quantitatively estimate community composition of different groups, which would be invaluable for microbial food webs studies...|$|R
40|$|AbstractObjectives. We {{sought to}} examine {{coronary}} arteries {{for the presence}} of viable bacteria of the fastidious species Chlamydia pneumoniae. Background. The respiratory pathogen C. pneumoniaehas been implicated in the pathogenesis of coronary artery disease (CAD). Previous studies have demonstrated an antichlamydial seroresponse to be a cardiovascular risk factor and coronary atheromata to contain chlamydial components in varying proportions. Endovascular demonstration of replicating bacteria is required to provide evidence for an infectious component in CAD and a rationale to discuss antimicrobial therapy. Methods. Myocardial revascularization was performed in 70 patients. Atherosclerotic lesions from 53 coronary endarterectomy and 17 restenotic <b>bypass</b> <b>samples</b> were cultured and subjected to nested polymerase chain reaction (PCR) for C. pneumoniae. Antichlamydial immunoglobulin G (IgG), IgA and IgM was examined by microimmunofluorescence. Results. Viable C. pneumoniaewas recovered from 11 (16 %) of 70 atheromata, and chlamydial deoxyribonucleic acid (DNA) was detected in 21 (30 %) of 70 atheromata; 17 nonatherosclerotic control samples were PCR-negative (p < 0. 01). Fifteen (28 %) of 53 endarterectomy and 6 (35 %) of 17 <b>bypass</b> <b>samples</b> were PCR-positive. DNA sequencing of six different PCR products did not reveal differences between coronary isolates and respiratory reference strains, suggesting that common respiratory strains gain access to the systemic circulation. Serologic results did not correlate with direct detection results and did not identify individual endovascular infection. Conclusions. A significant proportion of atherosclerotic coronary arteries harbor viable C. pneumoniae. This finding supports the hypothesis of a chlamydial contribution to atherogenesis. Whether chlamydiae initiate atherosclerotic injury, facilitate its progression or colonize atheromata is unknown. However, the endovascular presence of viable bacteria justifies a controlled clinical investigation of antimicrobial treatment benefit in the therapy and prevention of CAD...|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedWith digital storage becoming cheaper, bigger, and more prevalent, finding {{evidence from the}} hard drives collected for a case is too difficult and time consuming. Simply reading an entire drive takes hours and it takes even longer to analyze the drive for deleted files and data fragments. Investigations frequently involve multiple drives, and this traditional method of reading entire drives for analysis simply cannot keep up in modern cases. Furthermore, investigators often search drives only for known files, which we call target data, that could help identify a drive holding evidence such as child pornography or malware. Triage is needed to sift through drives to quickly identify drives containing target data. One way is by randomly sampling drive data to find known files or to give a confidence that less than some small amount is present. We determine the optimal <b>sampling</b> strategy <b>bypassing</b> the file system to find even deleted files and fragments in minimum time with maximum confidence. With 15 minutes of sampling we can give a 90 % confidence that less than 10 MiB of target data is present on a 500 GB hard disk drive. By using statistical sampling in combination with sector hashing, our software forms an efficient triage tool for digital forensics. Outstanding ThesisCivilian, Department of the Nav...|$|R
40|$|Treatment of metastatic {{colorectal}} cancer (CRC) has continuously improved {{over the last}} decade. However, disease monitoring remains underdeveloped and mostly dependent on imaging e. g. RECIST 1. 1 criteria. The genetic landscape of individual cancers and subsequently occurring treatment-induced evolution remain neglected in current surveillance strategies. Novel biomarkers demand minimally invasive and repetitive tracking of the cancer mutagenome for therapy stratification and to make prognostic predictions. Carcinoembryonic antigen (CEA), a routinely used tumor marker for CRC, does not meet these goals and thus prevents its use as a reliable monitoring tool. A tumor-derived fraction of circulating cell-free DNA (cfDNA), isolated from blood <b>samples,</b> may <b>bypass</b> the limitations of currently available biomarkers and could be a tool for noninvasive disease monitoring. Here, total cfDNA levels differentiated a cohort of metastatic CRC patients from healthy controls. Furthermore, we correlated cfDNA during chemotherapy of 27 stage IV patients with clinical parameters to establish its prognostic and predictive value. Indeed, cfDNA levels in chemotherapy naive patients correlate with the tumor burden and CEA values at diagnosis and increase upon disease progression during 1 st and 2 nd line treatment. Moreover, we confirm the possibility of cfDNA-based genotyping of KRAS to early detect the emergence of resistance during chemotherapy. These data indicate that repetitive quantitative and mutational analysis of cfDNA might complement current treatment standards but may have also limited value in some patients...|$|R
40|$|Ranging from track {{interpolation}} techniques through model atmospheres to the stochastic {{nature of}} the IMF, there are many uncertainties which {{need to be taken}} into account when modelling HR diagrams or performing population synthesis, particularly if comparison with actual data is sought. In this paper, we highlight (1) the problem of discontinuities along evolutionary tracks of massive stars (M> 8 Mo), showing that inconsistencies appear in the computation of the corresponding isochrones, and (2) the sampling fluctuations produced by the stochastic {{nature of the}} IMF, presenting a statistical formalism to estimate the dispersion in any given observable of a stellar population due to <b>sampling</b> effects which <b>bypasses</b> the need of performing Monte Carlo simulations. Comment: 6 pages, 4 figures, contribution to "Observed HR diagrams and stellar evolution", T. Lejeune and J. Fernandes (eds) with. sty file. More details of the work in [URL]...|$|R
40|$|To {{optimize}} cell separations by centrifugal elutriation, we constructed {{an on-line}} computer-controlled multiparametric light-scatter system. A <b>bypass</b> <b>sample</b> flow, at the outlet of an elutriation rotor, is hydrofocussed and three scatter parameters of each cell are determined up {{to a maximum}} of 15 000 cells/second. The 18 -bit representation of the parameter values are cumulatively stored by means of a direct memory access interface. The histogram memory is continuously displayed to provide information on the number and type of cells that are elutriated. A special purpose operating system, implemented on a stand-alone computer configuration, allows a high data-acquisition rate and ample data processing capacity. In addition, a local network driver was constructed to facilitate off-line detailed analysis of the data. The equipment is well suited to monitor the centrigural elutriation process. The flexibility of the system allows an extension of the monitor to computer-controlled elutriation...|$|R
3000|$|Microbial {{biofouling}} is {{a significant}} problem facing many different systems including industrial (e.g. fuel production, food production, drinking-water, etc.), marine (e.g. ship ballast tanks) and medical (e.g. catheters) (Bixler and Bhushan 2012). Biofilm formation, or the accumulation of water-borne microorganisms and their associated extrapolymeric substances (EPS) on wetted surfaces, is {{a major contributor to}} biofouling and becomes an economic liability when it exceeds some threshold of interference, resulting in material damage, production loss, or elevated health risks (Murthy and Venkatesan 2009). Therefore, rapid sample processing and analysis is necessary for prompt microbial biofouling assessment. Due to limited space, resources, or expertise, samples from the facilities at risk are often shipped to research or commercial laboratories for nucleic acid extraction and analysis, where the efficacy of antifouling approaches, such as biocide treatment or physical biofilm removal (Quarini and Shire 2007), can be rapidly deduced using molecular-based approaches. These approaches include amplification of both the 16 S rRNA gene and functional genes to identify specific microbes and qPCR to quantify target genes (Smith and Osborn 2009). The caveats of shipping samples for extraction in lieu of extracting on-site include: 1) shipping materials that could be considered [...] "hazardous," [...] and 2) the microbial community structure of the sample could shift during the time in transit from facility to lab, leading to erroneous results. Therefore, the ability to extract nucleic acids on-site and within hours of <b>sampling</b> could <b>bypass</b> these two caveats and hasten microbial biofouling assessment and treatment.|$|R
40|$|Many of the {{ecosystem}} services are soil associated with microbes playing a predominant role. Nevertheless, our current knowledge of microbial contribution to ecosystem processes is still limited, partly because in the past centuries research was mostly based on culture-dependent methods, being oblivious of the vast un-cultivable microbial majority as proven during the last decades. Current molecular biology advances provide us {{with the ability to}} screen for microbial identities or functions by targeting marker genes in nucleic acid extracts of environmental <b>samples,</b> therefore partly <b>bypassing</b> previous methodological limitations. Topics addressed here aim at providing an overview of methodologies and concepts related to marker gene screening from environmental samples. Such are the description of marker gene categories, examples of their use in soil environments and the description of marker gene screening state-of-the-art methodologies and specifications. Finally we will exemplify the use of late methodologies for the case of the bacterial small ribosomal subunit screening in soil environments...|$|R
40|$|Abstract- This paper {{proposes a}} {{cognitive}} computational model for video content analysis and classification. Our approach describes {{spatial and temporal}} patterns of videos holistically and computes scene-related features in both the spatial and temporal domains, <b>bypassing</b> <b>sampling</b> uniform patches or explicitly detecting moving objects. Spatially, global and abstract representation of each frame is computed to describe structural and textural attributes, avoiding expensive yet ineffective image segmentation. Inspired from receptive fields in the visual cortex, we apply multi-scale, multiorientation Gabor filters to extract spatial layout and achieve a lowresolution, discriminative representation of video frames. We also propose to extract motion features at scene level. Similar to still features, we extract temporally global, low-level motion features instead of detecting local motions. The baseline model has been evaluated using small databases and shows promising results. The performance of our framework is comparable to state-of-the-art video classification and retrieval systems. Our model {{can be applied to}} autonomous vehicle systems, intelligent robots, digital library management, film archive, post-event analysis, etc...|$|R
40|$|Hypertension is {{a common}} {{disorder}} in general practice and has a widely known association with type 2 diabetes mellitus. Low adhesion to clinical treatment may lead to poor results. Obesity surgery can bring early and relevant resolution rates of both morbidities. To describe hypertension evolution after Roux-en-Y gastric bypass in patients with type 2 diabetes mellitus. Descriptive observational study designed as a historical cohort of 90 subjects with hypertension and diabetes who underwent Roux-en-Y gastric bypass and were evaluated before and after surgery. It was observed a hypertension resolution rate of 85. 6 % along with markedly decrease in anti-hypertensive usage. Mean resolution time was 3. 2 months. Resolution was associated with homeostasis model assessment - insulin resistance, preoperative fasting insulin, anti-hypertensive usage, hypertension time, body mass index and percentage of weight loss. Resolution of hypertension was not statistically associated with diabetes remission within this <b>sample.</b> Roux-en-Y gastric <b>bypass</b> was a safe and effective therapeutic tool to achieve hypertension resolution in patients who also had diabetes mellitus...|$|R
40|$|The {{standard}} method used by high-throughput genome sequencing facilities for detecting mislabelled samples {{is to use}} independently generated high-density SNP data to determine sample identity. However, as it has now become commonplace to have multiple samples sequenced from the same source, such as for analysis of somatic variants using matched tumour and normal samples, we can directly use the genotype information inherent in the sequence data to match <b>samples</b> and thus <b>bypass</b> {{the need for additional}} laboratory testing. Here we present BAM-matcher, a tool that can rapidly determine whether two BAM files represent samples from the same biological source by comparing their genotypes. BAM-matcher is designed to be simple to use, provides easily interpretable results, and is suitable for deployment at early stages of data processing pipelines. BAM-matcher is licensed under the Creative Commons by Attribution license, and is available from: [URL] data are available at Bioinformatics online. paul. wang@sa. gov. au. Paul P. S. Wang, Wendy T. Parker, Susan Branford, Andreas W. Schreibe...|$|R
50|$|Nanopore {{technology}} is significant in analysis of biological macromolecules such as DNA and RNA {{because it can}} detect minute <b>sample</b> quantities and <b>bypasses</b> the need for PCR amplification. PCR amplification and other DNA sequencing methods cannot detect DNA damage alone because their basis relies on the four classical unmodified bases: cytosine, adenine, guanine, and thymine. One {{of the most common}} and prevalent causes of DNA damage is oxidation of guanine residues to 8-oxoguanine brought on through oxidative stresses. 8-oxoguanine causes mismatch pairing with adenine as opposed to cytosine, which can ultimately cause point mutations during DNA replication. In the context of DNA-protein cross linking, 8-oxoguanine is susceptible to forming adducts with amino acids containing reactive groups such as the phenol moiety of tyrosine or terminal amine of lysine., Detection and quantification of 8-oxoguanine content in telomeric sequences is important because content increases with stress since telomeres escape cellular DNA repair mechanisms. Burrows helped to discover specific DNA glycosylases that preferentially repaired oxidative damages at telomeric sites.|$|R
40|$|This paper proposes data envelopment {{analysis}} (DEA) as a quick-and-easy {{tool for}} assessing corporate bankruptcy. DEA is a non-parametric method that measures weight estimates (not parameter estimates) of a classification function for separating default and non-default firms. Using a recent sample of large corporate failures in the United States, we examine the capability of DEA in assessing corporate bankruptcy by comparing it with logistic regression (LR). We find that DEA outperforms LR in evaluating bankruptcy out-of-sample. This feature of DEA is appealing and has practical relevance for investors. Another advantage of DEA over LR {{is that it does}} not have assumptions associated with statistical and econometric methods. Furthermore, DEA does not need a large sample size for bankruptcy evaluation, usually required by such statistical and econometric approaches. The need for such a large sample size is a significant disadvantage to practitioners when investment decisions are made using small <b>samples.</b> DEA can <b>bypass</b> such a difficulty related to a sample size. Thus, DEA is a practically appealing method for bankruptcy assessment. Bankruptcy Data envelopment analysis Logit regression...|$|R
40|$|AbstractObjective: Interleukin 6 is {{a proinflammatory}} {{cytokine}} with a plasma concentration {{that has been}} noted to increase in response to cardiopulmonary bypass. The source of interleukin 6 after cardiopulmonary bypass is unknown. This study examined the myocardium as a potential source of interleukin 6 in this context. Methods: Dogs underwent 90 minutes of hypothermic cardiopulmonary bypass with 60 minutes of cardioplegic arrest. After rewarming, they were reperfused with the chest open for either 3 (n = 4) or 6 (n = 4) hours, {{at the end of}} which myocardial samples were obtained. Four additional animals undergoing open thoracotomy without bypass served as time-matched controls. Northern blot analysis, reverse transcriptase–polymerase chain reaction, and in situ hybridization were used to examine the myocardium for the induction of interleukin 6 and intercellular adhesion molecule- 1. Results: Northern blot analysis and reverse transcriptase–polymerase chain reaction demonstrated a marked increase in myocardial interleukin 6 messenger RNA in 3 of 4 dogs at 3 hours after bypass and 3 of 4 dogs at 6 hours after bypass, which was not present in sham-bypass control animals. Northern blots at 3 hours after cardiopulmonary bypass also demonstrated myocardial intercellular adhesion molecule- 1 induction. In situ hybridization studies confirmed that cardiac myocytes were a principal source of interleukin 6 messenger RNA early after cardiopulmonary bypass. Northern blots of messenger RNA extracted from isolated neutrophils and mononuclear leukocytes obtained from blood <b>samples</b> before <b>bypass,</b> at the end of bypass, and 3 hours after bypass failed to demonstrate interleukin 6 induction. Conclusion: Despite protection with cold cardioplegic arrest, the myocardium was a significant source of interleukin 6 synthesis after cardiopulmonary bypass. Local production of interleukin 6 may play a pivotal role in postoperative myocardial function. (J Thorac Cardiovasc Surg 2000; 120 : 256 - 63...|$|R
40|$|The {{aetiology}} of {{bipolar disorder}} remains unclear and investigation {{to date has}} focussed largely on bipolar patients. Whilst ultimately of huge value, such studies may also be confounded by current mood or experience of repeated illness episodes or current or past medication; using at-risk <b>samples</b> may <b>bypass</b> some of these problems. The current research therefore assessed {{the efficacy of the}} Mood Disorder Questionnaire (MDQ) as a screening tool for vulnerability to bipolar disorder. The MDQ was used with two sets of criteria to identify two sub-groups of medication-naïve young bipolar phenotype subjects who were at risk for bipolar disorder by virtue of experience of mood elevation. Analysis of data from the Student Stress Survey was carried out to characterise the bipolar phenotype. Compared to a control group with no experience of mood elevation, the two bipolar phenotype sub-groups showed a gradient of prevalence of bipolar diagnosis and associated co-morbidity. Behavioural and functional magnetic resonance imaging (fMRI) techniques were employed to investigate emotional processing, decision-making, and sleep and circadian rhythmicity in bipolar phenotype students. Analyses revealed that positive emotional processing biases, disrupted decision-making, and increased activity during sleep were associated with the bipolar phenotype and, therefore, may represent vulnerability markers for bipolar disorder. Finally, a psychopharmacological investigation of quetiapine, which stabilises mood, was carried out in healthy volunteers. One-week quetiapine administration resulted in biases away from both positive and negative emotional stimuli (i. e. a mood-stabilising effect), reduced discrimination between different magnitudes of gains and losses during risky decision-making (consistent with an antidepressant effect), and increased sleep duration. In sum, this research has developed our understanding of vulnerability markers associated with the bipolar phenotype and provided a first step towards uncovering the psychological mechanisms through which quetiapine’s clinical effects may be mediated. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|International audienceThe Sample Analysis at Mars (SAM) {{instrument}} suite {{aboard the}} Curiosity rover catalogued {{a suite of}} organic compounds using pyrolysis gas chromatography-mass spectrometry (Py-GC-MS) and evolved gas analyses (EGA) of aeolian and subsurface drill fines at Gale Crater, Mars. The sources of these organic compounds are also being investigated through laboratory experiments in which Mars analogue mixtures are analyzed by Py-GC-MS and EGA under SAM-like conditions. In this study we examined the hydrocarbon trap, {{a component of the}} SAM Py-GC-MS system, as a potential source of some of the compounds detected on Mars. Analogue mixtures consisting of olivine sand and varying amounts of Cl-containing salts and magnetite were analyzed with Py-GC-MS operated under SAM-like conditions (i. e. trap materials and pyrolysis and GC temperature programs were the same as SAM analyses). Carbon dioxide, benzene and toluene were identified in all experiments and chlorobenzenes (CBs) and HCl were identified in the experiments with Cl salts. As the concentration of Cl salts increased the abundance of CBs, HCl and CO 2 increased whereas the abundance of benzene and toluene remained stable. In order to confirm that these compounds originate from the trap we analyzed the <b>samples</b> while <b>bypassing</b> the trap. Benzene was detected, although at much lower concentrations, and toluene could not be definitively identified. Traces of CB were also detected when Cl salts were analyzed without the trap, again at lower concentrations. Additionally, traces of phthalic acid were detected in the Cl salt samples; this compound readily forms CBs in the presence of HCl (Miller et al., 2013). From this we can conclude that traces of benzene, toluene, and CBs can be degradation products originating from the hydrocarbon trap. Understanding how these compounds form and their relationship to the concentration of different mineral decomposition products helps us interpret the significance of all the organic compounds identified in SAM GC-MS data...|$|R
40|$|Pathogenic {{bacteria}} {{are responsible for}} millions of deaths every year with an estimated mortality of 70 million people by 2010 for Mycobacterium tuberculosis alone. Novel methods for identification of bacterial species in hosts, urban environments, water sources and food stuffs are required to advance diagnosis and preventative medicine. Detection of bacterial species in environmental samples is a complex task since large numbers of {{bacteria are}} present and are resistant to culturing. Therefore, the genetic content of the entire sample has to be analysed simultaneously and this constitutes a metagenomic sample. Commonly-used methods of bacterial identification focus on detection of specific genomic regions to determine species. Currently only one percent of a metagenomic sample {{can be used for}} identification employing phylogenetic markers. This method is highly inefficient. The search for more widespread markers within each genome is essential to improve detection methods. Also, modern sequencing technologies used in these environments have short read lengths which prove difficult to assemble e. g. repeats can lead to incorrect assembly. The use of overrepresented oligonucleotides provides a solution to both of these difficulties. Overrepresented oligonucleotides (8 - 14 bp in length) are utilised to differentiate between species based on observed frequency of occurrence rather than presence or absence. They occur throughout the genome thereby increasing genomic coverage. Furthermore, overrepresented oligonucleotides can be easily identified in a raw metagenomic <b>sample,</b> <b>bypassing</b> the need for sequence assembly. Raw oligonucleotide data was filtered, analysed and imported into a structured database. A program, Oligosignatures, allowed for creation of species and phylogenetic lineage specific oligonucleotide markers dependent on the selection of species specified by the user. For the purposes of this study, the context of bacterial identification in an unknown environment was selected. A similarity trial was then executed to determine if strains of the same species can be separated from each other using overrepresented oligonucleotides. Outcomes of this test provided a guideline for the creation of species and lineage specific oligonucleotide markers. Each species and lineage was therefore described by a marker profile which consisted of representative oligonucleotide markers. These marker profiles were then tested against artificial and experimental data to determine their effectivity. Two approaches were used for testing, namely Oligonucleotide frequency analysis and Sequence read analysis. Oligonucleotide frequency analysis focused on the identification of species dependent on the global frequencies of marker oligonucleotides within each marker profile. Sequence read analysis attempted to assign metagenomic reads to a specific species dependent on the number of marker oligonucleotides present within the read. The final database contained 439 bacterial genomes from 22 different phylogenetic lineages. Interpretation of the results obtained after strain similarity testing showed that strains of the same species had highly similar markers and were not separable using this approach. All strains of a species that conformed to this premise were reduced to a single representative member. Similarly, species marker profiles demonstrated that closely related species remained difficult to separate. Twenty-one of the 22 lineages showed sufficient lineage specific markers for use in testing. This provides support for the abundance of overrepresented oligonucleotides and their potential for use as a detection method. In general, metagenomic testing of marker profiles showed that species specific determination was prone to interference, specifically, in closely related species. However, more distantly related species could be separated using both methods. Lineage discrimination generated more reliable results proving that lineage determination was possible in both artificial and experimental datasets. Oligonucleotide frequency analysis, the most sensitive approach, showed the best results for lineage determination but poorer results for species identification. Sequence read analysis provided a more effective method of determining confidence using different thresholds for read classification. In conclusion, the use of overrepresented oligonucleotides holds promise as a novel method for bacterial identification in a metagenomic context. Although several obstacles still prevent optimal utilization of these oligonucleotides, with further research the classification and identification of species and phylogenetic lineages from metagenomic samples can become a reality. CopyrightDissertation (MSc) [...] University of Pretoria, 2009. Biochemistryunrestricte...|$|R
40|$|With the {{exception}} of a brief period of slightly elevated chloride level in the secondary blowdown, water-chemistry conditions during the period were satisfactory. During the period, the reactor was shut down for end-of-core-life testing and rearrangement. A set of specifications covering all electronic and electromechanical mechanisms required to control the SM- 1 reactor through the rod- drive motors and clutches was prepared and issued. Installation of instrumentation for plant response and system performance was virtually completed. Work on the interpretation of long-lived radiochemical data obtained at the SM- 1 during core lifetime was continued. Analysis of all fissionproduct data collected during Core I life has started. Thirty-eight stationary and seven control subassemblies from SM- 1 Core II were checked for alpha contamination by a gas-flow proportional-counting technique. The work on the final design of a waste-disposal system for SM-lA was stopped and an investigation of an interim system containing a <b>bypass</b> <b>sampling</b> system was undertaken. Work continued on tests 202, 203, and 204 in the activitybuildup phase of Test Series 200. Core- physics measurements were taken at end of Core I life to complete the series of measurements made throughout the lifetime of the core. (W. L. H. ...|$|R
40|$|With one {{operational}} amplifier (op-amp) in negative feedback, the traditional zero potential circuit could access one {{element in the}} two-dimensional (2 -D) resistive sensor array with the shared row-column fashion but it suffered from the crosstalk problem for the non-scanned elements’ bypass currents, which were injected into array’s non-scanned electrodes from zero potential. Firstly, for suppressing the crosstalk problem, we designed a novel improved zero potential circuit with one more op-amp in negative feedback to <b>sample</b> the total <b>bypass</b> current and calculate the precision resistance of the element being tested (EBT) with it. The improved setting non-scanned-electrode zero potential circuit (S-NSE-ZPC) was given as an example for analyzing and verifying {{the performance of the}} improved zero potential circuit. Secondly, in the S-NSE-ZPC and the improved S-NSE-ZPC, the effects of different parameters of the resistive sensor arrays and their readout circuits on the EBT’s measurement accuracy were simulated with the NI Multisim 12. Thirdly, part features of the improved circuit were verified with the experiments of a prototype circuit. Followed, the results were discussed and the conclusions were given. The experiment results show that the improved circuit, though it requires one more op-amp, one more resistor and one more sampling channel, can access the EBT in the 2 -D resistive sensor array more accurately...|$|R
40|$|Objective - In {{patients}} with coronary artery disease, the concomitant presence of renal function impairment {{is associated with}} decreased survival. We aimed to assess whether in coronary artery diseased patients renal function impairment is associated with systemic vascular function, functional parameters of the renin-angiotensin system, or inflammation as potential mediators for cardiovascular risk. Methods and Results - We studied 125 patients, 87 % male, {{with a mean age}} of 62. 2 +/- 8. 2 years; 72 % had 3 -vessel disease, and mean renal function was 74 +/- 13 mL/min per 1. 73 m(2). Internal thoracic artery rings were <b>sampled</b> during coronary <b>bypass</b> surgery and used for in vitro vascular measurements. We could not establish an association between endothelium-dependent vasorelaxation (response to methacholine) and renal function. In addition, vascular response to potassium chloride, phenylephrine, and angiotensin II were not associated with renal function. Finally, serum angiotensin-converting enzyme (ACE) activity, usage of ACE inhibitors, C-reactive protein, and soluble intercellular adhesion molecule 1 were not related to renal function. Conclusions - In coronary artery disease patients, mild renal function impairment is not associated with systemic vasomotor responsiveness, inflammation, or functional systemic parameters of the renin-angiotensin system. The relation between systemic endothelial dysfunction and mild renal insufficiency might be more complicated than previously thought...|$|R
40|$|Past work on {{fertilization}} in echinoids {{and other}} egg-broadcasting, free-spawning invertebrates {{suggests that these}} organisms might be extremely sperm limited in the field unless individuals spawn in close proximity and under nearly ideal flow conditions. However, virtually all previous experiments have used one or more techniques (surrogates for males and females, and short <b>sampling</b> duration) that <b>bypassed</b> two potentially important aspects of echinoid reproductive biology: the release of gametes in viscous fluids that cling to tests and spines, and extended longevity of eggs and undiluted sperm. We hypothesized that these attributes might interact with some flow regimes to facilitate time-integrated fertilization. Consequently, we explored fertilization processes in sea urchins induced to spawn in a benthic boundary layer in a flow-through flume, with a male 0. 5 m upstream of a female. Our observations and data suggest that at free–stream flow velocities of 2. 5 and 8. 5 cm s� 1, gametes were slowly and continually advected from the aboral surfaces of spawning animals. Eggs {{on the surface of}} the aboral mass were often fertilized before they ablated from the surface; many advected eggs were fertilized after being trapped in the vortex downstream of the female. Gamete advection and fertilization continued for several hours, with the actual time course depending on flow velocity. Fertilization levels declined only slightly with increasing flow velocity. These results suggest that fertilization in echinoderms and other free-spawners with viscous, long-lived gamete...|$|R
40|$|The {{institution}} of cardiopulmonary bypass generates many pro-inflammatory cytokines and several clinical variables, including temperature, {{have been shown}} to influence cytokine release during and after cardiopulmonary bypass. The release of tumour necrosis factor and interleukin- 6 are the best predictors of post-cardiopulmonary bypass related morbidity. Their release during normothermic and hypothermic cardiopulmonary bypass and the correlation with clinical parameters of organ injury was studied. This prospective study was carried out in 52 adult patients, scheduled for cardiac surgery, exposed to normothermic and 27 to hypothermic cardiopulmonary <b>bypass.</b> <b>Samples</b> for estimation of tumour necrosis factor and interleukin- 6 were collected preoperatively, 1 hour and 24 hours post cardiopulmonary bypass and analysed by ELISA. Haemodynamic parameters and respiratory parameters were noted and lung injury scores calculated. Interleukin- 6 levels were raised in both the groups at 1 hour and 24 hours post cardiopulmonary bypass and the response was higher in the normothermic group. Tumour necrosis factor response was, however, similar in both the groups, with a rise at 1 hour returning back to baseline by 24 hours post cardiopulmonary bypass. The normothermic group had a better respiratory index in the postoperative period, early extubation was possible, had better clinical haemodynamics, a shorter cardiopulmonary bypass time and had reduced requirement of defibrillation after the release of aortic cross clamp. We conclude that the release of interleukin- 6 was thermo-dependant but did not correlate with the clinical signs of organ injury. Tumour necrosis factor levels were significantly raised after the cardiopulmonary bypass but the rise was not thermo-dependant...|$|R
40|$|The {{incidence}} of {{severe hearing loss}} after {{coronary artery bypass graft}} surgery has been as rare as 1 / 1000 surgery (Plasse et al., 1981). To evaluate possible changes in the auditory status, audiograms were taken before and after this operation. Our objectives were to: 1) evaluate hearing losses that were not necessarily severe and; 2) examine the factors associated with any auditory insult. we selected patients among a group without any history of hearing problems and the intake of any ototoxic drugs. All patients underwent careful examination and hearing evaluation the day prior to surgery. 6 days after surgery they were examined again and asked about any hearing problems or experience of tinnitus. Various operative details were taken and the post-operative drug record examined. Patients who were too ill for retesting, or who had been given sedative or ototoxic drugs or opiate analgesia, were dropped from the study. The whole procedure was repeated on a control group of 20 patients who were having thorocotomy but without being placed on a cardiopulmonary bypass. From the <b>bypass</b> <b>sample</b> of 40 ears, five of them (four individuals) had an individually statistically significant high-frequency hearing loss. The results showed a statistically significant difference between the bypass group and the control group, with the bypass patients having worse threshold shifts following the operation. Discriminant analyses have shown that those patients suffering hearing impairment may be discriminated principally with four variables: the patient's age, minimum temperature and minimum blood pressure during the operation, and the time on bypass...|$|R
40|$|Hierarchical {{models have}} emerged as a {{promising}} tool {{for the analysis of}} repeated binary data. However, the computational complexity in these models have limited their applications in practice. Several approaches have been proposed in the literature to overcome the computational difficulties including maximum likelihood estimation from a frequentist perspective (e. g., J. Amer. Statist. Assoc. 89 (1994) 330 - 335) and Markov chain Monte Carlo (MCMC) methods from a Bayesian perspective (e. g., Generalized Linear Models: A Bayesian Perspective, Marcel Dekker, New York, pp. 113 - 131). Although MCMC methods provide the whole posterior of the parameter of interest, the convergence diagnostics problem of the Markov chain and the slow convergence problem owing to the introduction of too many Gaussian latent variables are still unresolved. Recently, Tan et al. (Statist. Sinica 13 (2003) 625 - 639) proposed a noniterative sampling approach, the inverse Bayes formulae (IBF) sampler, for computing posteriors in the structure of EM algorithm. This article develops the IBF sampler in the structure of Monte Carlo EM (MCEM) for the hierarchical model with repeated binary data for which current methods encounter difficulty. An efficient IBF sampler is implemented by utilizing the estimated posterior modes obtained via MCEM algorithm. The proposed method generates independent and identically distributed (iid) samples approximately from the observed posterior distribution and thus alleviates the convergence problem associated with the MCMC methods. In addition, the slow convergence problem in Gibbs <b>sampler</b> can be <b>bypassed</b> in the noniterative IBF sampler via running some fast EM-type algorithm. Real datasets from six cities children's wheeze study and children's ear fluid study illustrate the proposed methods. © 2004 Elsevier B. V. All rights reserved. link_to_subscribed_fulltex...|$|R
