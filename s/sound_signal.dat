577|1256|Public
25|$|Underwater {{loudspeaker}} {{and public}} address (PA) systems produce either a pre-programmed or microphone input <b>sound</b> <b>signal.</b>|$|E
25|$|All stops are request stops, {{made only}} on {{passenger}}s' requests. In order {{to leave a}} tram at a given stop, a passenger must push a special button in the tram. This invokes a short <b>sound</b> <b>signal,</b> and a special red indicator lamp is lit in the passenger space, along with a signal lamp on the driver's control panel.|$|E
25|$|Computational {{modeling}} {{has also}} been used to simulate how speech may be processed by the brain to produce behaviors that are observed. Computer models {{have been used to}} address several questions in speech perception, including how the <b>sound</b> <b>signal</b> itself is processed to extract the acoustic cues used in speech, and how speech information is used for higher-level processes, such as word recognition.|$|E
40|$|In {{a method}} for {{canceling}} unwanted signals from at least one external sound source, such as a loudspeaker, by means of headphones provided with microphones, at least known <b>sound</b> <b>signals</b> from {{the at least one}} external sound source are compensated by anti-phase <b>sound</b> <b>signals.</b> These <b>sound</b> <b>signals</b> simulate the at least known <b>sound</b> <b>signals</b> from said at least one external sound source in anti-phase. Said anti-phase <b>sound</b> <b>signals</b> are generated in the headphones in response to signals derived from audio input signals of the at least one external sound source in a filter device which is controlled by the resulting microphone signals...|$|R
40|$|Abstract. Objective: We do it {{to realize}} the {{wireless}} collection and transmission for heart <b>sound</b> <b>signals,</b> from the shape to simplify the equipment and to reduce the interference of the tested patients. Method: We use state-of-the-art 2. 4 G transmission technology to transport the heart <b>sound</b> <b>signals</b> collected by the front-end to the terminal of analysis. Results: We have successfully exploited the transmission equipment to transmit heart <b>sound</b> <b>signals</b> with good performance. Conclusion: By using the method can we realize the wireless collection and transmission for heart <b>sound</b> <b>signals...</b>|$|R
40|$|Auditory system detects <b>sound</b> <b>signals</b> {{and uses}} the temporal-frequency {{information}} of the <b>sound</b> <b>signals</b> to conduct <b>sound</b> identification, sound localization, and sound source separation. Thanks to the past studies, {{we know that the}} hair cells at cochlea show frequency-dependent responses against input <b>sound</b> <b>signals.</b> But, little is known about how the sound information is conducted to and processed within the auditory cortex. Recently, B. A. Pearlmutter and A. M. Zador proposed an algorithm for monaural source separation under the assumption that the precise head-related transfer function (HRTF) and all the sound dictionary elements are known (ref. [1]). To apply this algorithm to the real <b>sound</b> <b>signals,</b> here I propose that non-negative matrix factorization (NMF) applied to the spectrograms of <b>sound</b> <b>signals</b> would successfully give a set of sound dictionary elements. When NMF was applied to solo-music signals with an appropriate value for the rank of factorization, it could extract instrument-specific patterns of basis spectrograms, each of which has a peak frequency for different notes. Interestingly, the <b>sound</b> <b>signals</b> converted back from the obtained basis spectrograms sounded more or less like the corresponding instrument, which suggests that the obtained basis spectrograms, or basis sound elements, would be a good candidate for the sound dictionary. When NMF was applied to <b>sound</b> <b>signals</b> played with several different instruments, the obtained basis sound elements can be categorized into each instrument-specific pattern by hand. In addition, using the categorized elements, <b>sound</b> <b>signals</b> can be reconstructed corresponding to each instrument part of the original music. The fact that source separation can be done using the basis sound elements obtained by NMF suggests that NMF would be a possible way to learn sound dictionaries from <b>sound</b> <b>signals.</b> ...|$|R
25|$|The {{process of}} perceiving speech {{begins at the}} level of the <b>sound</b> <b>signal</b> and the process of audition. (For a {{complete}} description of the process of audition see Hearing.) After processing the initial auditory signal, speech sounds are further processed to extract acoustic cues and phonetic information. This speech information can then be used for higher-level language processes, such as word recognition.|$|E
25|$|Other {{potential}} sources of the sounds normally associated with tinnitus should be ruled out. For instance, two recognized sources of high-pitched sounds might be electromagnetic fields common in modern wiring and various <b>sound</b> <b>signal</b> transmissions. A common and often misdiagnosed condition that mimics tinnitus is radio frequency (RF) hearing, in which subjects have been tested and found to hear high-pitched transmission frequencies that sound similar to tinnitus.|$|E
25|$|Electrostatic drivers {{consist of}} a thin, {{electrically}} charged diaphragm, typically a coated PET film membrane, suspended between two perforated metal plates (electrodes). The electrical <b>sound</b> <b>signal</b> {{is applied to the}} electrodes creating an electrical field; depending on the polarity of this field, the diaphragm is drawn towards one of the plates. Air is forced through the perforations; combined with a continuously changing electrical signal driving the membrane, a sound wave is generated. Electrostatic headphones are usually more expensive than moving-coil ones, and are comparatively uncommon. In addition, a special amplifier is required to amplify the signal to deflect the membrane, which often requires electrical potentials in the range of 100 to 1000volts.|$|E
40|$|In {{a method}} and a media system of/for {{generation}} {{of at least}} one output signal (HPL,HPR) from at least one input signal belonging to a second set of <b>sound</b> <b>signals</b> (M) having a related second set of Head Related Transfer Functions, in which the media system can be a TV, a CD player, a DVD player, a Radio, a display, an amplifier, a headphone or a VCR, the method includes the steps of determining, for each signal in the second set of <b>sound</b> <b>signals,</b> a weighted relation (14) including at least one signal belonging to a third set of intermediate <b>sound</b> <b>signals</b> (CHI 1, CHI 2) and at least one weight value (Weights); determining a first set of Head Related Transfer Functions (HRTFs) based on the second set of <b>sound</b> <b>signals,</b> the second set of Head Related Transfer Functions and the weighted relation; and transferring at least one signal belonging to the third set of intermediate <b>sound</b> <b>signals</b> by means {{of at least one}} HRTF belonging to said first set of Head Related Transfer Functions in order to generate at least one output signal belonging to said first set of <b>sound</b> <b>signals.</b> Hereby, in the end, fewer HRTFs are determined for a subsequent transfer of input signal(s) to output signal(s). Accordingly, few convolutions are require...|$|R
50|$|SemanticitySpecific <b>sound</b> <b>signals</b> are {{directly}} tied to certain meanings.|$|R
5000|$|Two DACs (stereo) convert {{digital data}} to analog <b>sound</b> <b>signals</b> ...|$|R
25|$|Digital {{sampling}} is used {{to convert}} an audio wave to a sequence of binary numbers that {{can be stored in}} a digital format, such as MP3. Common features of all MP3 players are a memory storage device, such as flash memory or a miniature hard disk drive, an embedded processor, and an audio codec microchip to convert the compressed file into an analogue <b>sound</b> <b>signal.</b> During playback, audio files are read from storage into a RAM based memory buffer, and then streamed through an audio codec to produce decoded PCM audio. Typically audio formats decode at double to more than 20 times real speed on portable electronic processors, requiring that the codec output be stored for a time until the DAC can play it. To save power, portable devices may spend much or nearly all of their time in a low power idle state while waiting for the DAC to deplete the output PCM buffer before briefly powering up to decode additional audio.|$|E
2500|$|The speech <b>sound</b> <b>signal</b> {{contains}} {{a number of}} acoustic cues {{that are used in}} speech perception. The cues differentiate speech sounds belonging to different phonetic categories. For example, one of the most studied cues in speech is voice onset time or VOT. VOT is a primary cue signaling the difference between voiced and voiceless plosives, such as [...] "b" [...] and [...] "p". Other cues differentiate sounds that are produced at different places of articulation or manners of articulation. The speech system must also combine these cues to determine the category of a specific speech sound. This is often thought of in terms of abstract representations of phonemes. These representations can then be combined for use in word recognition and other language processes.|$|E
2500|$|The {{electric}} signal is transmitted through cables into an electronic or digital drum module ("brain" [...] {{as it is}} sometimes called), synthesizer or other device, which then produces a sound associated with, and triggered by, the struck pad. The <b>sound</b> <b>signal</b> from the drum module can be plugged into a keyboard amp or PA system {{for use in a}} live band performance or listened to with headphones for silent practice. Since digital drums have become more popular, companies have started selling digital electronic drum files, referred to as drum kits. While electronic drum kits are typically used to trigger drum and percussion sounds, a MIDI-equipped electronic drum kit can be used to trigger any types of MIDI sounds, such as synthesized or sampled piano, guitar, or any other instrument.|$|E
50|$|Auditory masking is {{exploited}} {{to perform}} data compression for <b>sound</b> <b>signals</b> (MP3).|$|R
5000|$|Balancing, {{mixing and}} {{adjusting}} <b>sound</b> <b>signals</b> from multitrack recording devices using a mixing console ...|$|R
40|$|Gear drives {{are one of}} {{the most}} widely used {{transmission}} system in many machinery. <b>Sound</b> <b>signals</b> of a rotating machine contain the dynamic information about its health conditions. Not much information available in the literature reporting suitability of <b>sound</b> <b>signals</b> for fault diagnosis applications. Maximum numbers of literature are based on FFT (Fast Fourier Transform) analysis and have its own limitations with non-stationary signals like the ones from gears. In this paper, attempt has been made in using <b>sound</b> <b>signals</b> acquired from gears in good and simulated faulty conditions for the purpose of fault diagnosis through a machine learning approach. The descriptive statistical features were extracted from the acquired <b>sound</b> <b>signals</b> and the predominant features were selected using J 48 decision tree technique. The selected features were then used for classification using Large Margin K-nearest neighbor approach. The paper also discusses the effect of various parameters on classification accuracy. Comment: Accepted for publicatio...|$|R
2500|$|The {{sounds that}} are heard in {{everyday}} life are not characterized by a single frequency. Instead, they consist of a sum of pure sine frequencies, each one at a different amplitude. When humans hear these frequencies simultaneously, we can recognize the sound. This is true for both [...] "non-musical" [...] sounds (e.g. water splashing, leaves rustling, etc) and for [...] "musical sounds" [...] (e.g. a piano note, a bird's tweet, etc). This set of parameters (frequencies, their relative amplitudes, and how the relative amplitudes change over time) are encapsulated by the timbre of the sound. Fourier analysis is the technique {{that is used to}} determine these exact timbre parameters from an overall sound signal; conversely, the resulting set of frequencies and amplitudes is called the Fourier series of the original <b>sound</b> <b>signal.</b>|$|E
2500|$|Igor Kurdin, {{a former}} [...] {{commander}} {{and the current}} head of the St. Petersburg Submariners Club, attributed the high casualty count {{to the presence of}} a large number of civilian specialists on the submarine at the time of the accident. 17 of 20 people who perished in the accident were civilians. He noted that civilian observers would be untrained in the proper response to the release of the boat's firefighting gas, which would be preceded by a specific light and <b>sound</b> <b>signal,</b> after which all on board are supposed to put on oxygen masks to allow them to survive during the 30-minute period required to ventilate the compartments before they can be reopened. In addition, because the accident occurred at 8:30 PM many of the civilian specialists and crewmen would likely have been asleep, and unable to don their oxygen masks in time. The accident was the country's worst since 2000, when a torpedo exploded and sank another Russian nuclear submarine, [...] Kursk, killing 118 people.|$|E
5000|$|Sound. Since a {{sound is}} a {{vibration}} of a medium (such as air), a <b>sound</b> <b>signal</b> associates a pressure value to every value {{of time and}} three space coordinates. A <b>sound</b> <b>signal</b> is converted to an electrical signal by a microphone, generating a voltage signal as an analog of the <b>sound</b> <b>signal,</b> making the <b>sound</b> <b>signal</b> available for further signal processing. Sound signals can be sampled at a discrete set of time points; for example, compact discs (CDs) contain discrete signals representing sound, recorded at 44,100 samples per second; each sample contains data for a left and right channel, which may {{be considered to be}} a 2-vector signal (since CDs are recorded in stereo). The CD encoding is converted to an electrical signal by reading the information with a laser, converting the <b>sound</b> <b>signal</b> to an optical signal.|$|E
40|$|In this paper, {{we propose}} the method for {{automatically}} estimating sound source positions using {{the information on}} <b>sound</b> <b>signals</b> and generalized harmonic analysis and cluster analysis. <b>Sound</b> <b>signals,</b> which are measured with closely located four-point microphone method, are analyzed using generalized harmonic analysis and we get the distribution of frequency components and find the frequency component clusters from these results. Finally {{we can get the}} movement of sound sources and separated <b>sound</b> source <b>signals.</b> The quality of separated sound was same as that of the original sound. 1...|$|R
40|$|Abstract: A heart <b>sounds</b> <b>signal</b> {{generator}} in {{the heart}} sound analysis instrument based on the LabVIEW is devised. The instrument is developed in PC. Heart <b>sounds</b> <b>signal</b> generator can according to need to produce a synthetic heart <b>sounds</b> <b>signal</b> for users to learn and use. The parameters setting are also discussed {{to find out the}} best for the each part. All the parameters can be set by user and the best ones are default values so that the instrument can fit other environment. The running test of this instrument proves it can generate and play heart sound precisely，and can be used as an assistance to show, play, and analyze heart sound. 1...|$|R
50|$|Alternative ways to characterise a <b>sound</b> <b>signal's</b> {{distribution}} of energy vs. frequency include spectral rolloff, spectral centroid.|$|R
5000|$|Monhegan Island Light - Manana Island <b>Sound</b> <b>Signal</b> Station, Maine, April 4, 1870 ...|$|E
5000|$|The raising (hoisting) or {{removing}} of {{a visual}} signal {{is accompanied by}} the emission of a <b>sound</b> <b>signal</b> {{to draw attention to}} the new signal. The type of the <b>sound</b> <b>signal</b> (one short sound, two short sounds, one long sound, etc.) is described by the rule according to the type of signal.The usual meanings of these flags are as follows: ...|$|E
5000|$|Underwater {{loudspeaker}} {{and public}} address (PA) systems produce either a pre-programmed or microphone input <b>sound</b> <b>signal.</b>|$|E
5000|$|Sound signal: a {{foreground}} sound; e.g. a dog, {{an alarm}} clock; messages/meaning is usually carried through <b>sound</b> <b>signals.</b>|$|R
5000|$|<b>Sound</b> <b>signals</b> {{passing through}} lengthy signal chains {{involving}} processes {{at different times}} and places, involving a variety of skills ...|$|R
2500|$|... a {{transmitter}} {{that sends}} {{power and the}} processed <b>sound</b> <b>signals</b> across the skin to the internal device by electromagnetic induction, ...|$|R
5000|$|The optimal {{reconstruction}} of the <b>sound</b> <b>signal</b> from STFT after amplitude modifications has been proposed by Griffin and Lim in 1984. This algorithm does not consider the problem of producing a coherent STFT, but it does allow finding the <b>sound</b> <b>signal</b> that has an STFT that {{is as close as}} possible to the modified STFT even if the modified STFT is not coherent (does not represent any signal).|$|E
50|$|Pitch/frequency effects modify pitch by {{altering}} {{the frequency of}} a sound wave or <b>sound</b> <b>signal</b> or adding new harmonies.|$|E
5000|$|... 2000: Lighthouse {{is open to}} {{the public}} and managed by the Buffalo Lighthouse association, Inc. There are no {{existing}} keepers quarters on existing <b>sound</b> <b>signal</b> building.|$|E
40|$|Abstract-In this study, {{we develop}} a new {{automated}} pattern recognition system for interpretation of heart sound based on wavelet decomposition of signals and classification using neural network. Inputs to the system are the heart <b>sound</b> <b>signals</b> acquired by a stethoscope in a noiseless environment. We generate features for the objective concise representation of heart <b>sound</b> <b>signals</b> by means of wavelet decomposition. Classification of the features is performed using a back propagation neural network with adaptive learning rate. With two hundred record windows obtained from young humans are studied. One hundred of the record windows in database are selected for use as training phase for neural network. In the test result of the intelligent pattern recognition system with ten different types heart <b>sound</b> <b>signals</b> are acquired a high success...|$|R
5000|$|Alligator - a pattern-based gate effect which splits <b>sound</b> <b>signals</b> {{into three}} channels, {{allowing}} for differing multiple effects on each separate channel.|$|R
40|$|Auscultation {{has been}} widely {{regarded}} as one of the most important noninvasive diagnostic tools for clinical diagnosis of the respiratory tract. The purpose of this thesis was to develop a software application capable of extracting the key features of respiratory <b>sound</b> <b>signals</b> from the lungs and trachea of healthy persons. The efficacy of the program was evaluated by the verification of the important features of the <b>sound</b> <b>signals</b> from the left and right lungs and the trachea such as 1) right and left lung symmetry and 2) dissimilarity between the trachea and both lungs. The program was developed in LabView and was designed to capture the respiratory <b>sound</b> <b>signals</b> from the lungs and the trachea in real-time and process them in the time and frequency domains for further analysis. The features compared were 1) signal amplitude in the time domain and 2) power spectra in the frequency domain. Results of the study had shown that the program had been able to verify that 1) the key features of the breath <b>sound</b> <b>signals</b> from the left and right lungs were similar and 2) the features of the signals from the trachea and both lungs were different...|$|R
