0|22|Public
40|$|Statistical {{distribution}} of Journal Impact Factor (JIF) is characteristically asymmetric and non-mesokurtic. Even the {{distribution of}} log 10 (JIF) exhibits conspicuous skewness and non-mesokurticity. In this paper we estimate the parameters of Johnson <b>SU</b> <b>distribution</b> fitting to the log 10 (JIF) data for 8 years, 2001 through 2008, and study the temporal variations in those estimated parameters. We also study ‘over-the-samples stability’ in the estimated parameters for each year by the method of re-sampling close to bootstrapping. It {{has been found that}} log 10 (JIF) is Pearson-IV distributed. Johnson <b>SU</b> <b>distribution</b> fits very well to the data and yields parameters stable over the samples. We conclude that Johnson <b>SU</b> <b>distribution</b> is the best choice to fit to the log 10 (JIF) data. We have also found that over the years the log 10 (JIF) distribution is becoming more skewed and leptokurtic, possibly suggesting the Mathew effect in operation, which means that more cited journals are cited ever more over time. ...|$|R
40|$|This thesis {{deals with}} design and {{implementation}} of an investment model, which applies methods of Post-modern portfolio theory. Particle swarm optimization (PSO) metaheuristic was used for portfolio optimization and the parameters were analyzed with several experiments. Johnsons <b>SU</b> <b>distribution</b> was used for estimation of future returns as {{it proved to be}} the best of analyzed distributions. The result is software application written in Python, which is tested for stability and performance of model in extreme situations...|$|R
30|$|Numerical {{results will}} show that the SU network (SN) {{throughput}} depends on the additional separation radius which is used for protection of PUs. The SN consists of <b>SB</b> and <b>SU</b> terminals. For example, without the additional separation radius, the transmit power margins are significantly increased. This fact implies an existence of optimal additional separation radius, and this will also be confirmed by the numerical results.|$|R
40|$|It {{is widely}} {{accepted}} {{that some of}} the most accurate Value-at-Risk (VaR) estimates are based on an appropriately specified GARCH process. But when the forecast horizon is greater than the frequency of the GARCH model, such predictions have typically required time-consuming simulations of the aggregated returns distributions. This paper shows that fast, quasi-analytic GARCH VaR calculations can be based on new formulae for the first four moments of aggregated GARCH returns. Our extensive empirical study compares the Cornish–Fisher expansion with the Johnson <b>SU</b> <b>distribution</b> for fitting distributions to analytic moments of normal and Student t, symmetric and asymmetric (GJR) GARCH processes to returns data on different financial assets, for the purpose of deriving accurate GARCH VaR forecasts over multiple horizons and significance levels...|$|R
5000|$|As an alternative, Edgeworth {{binomial}} trees {{allow for}} an analyst-specified skew and kurtosis in spot price returns; see Edgeworth series. This approach is useful when the underlying's behavior departs (markedly) from normality. A related use is to calibrate the tree to the volatility smile (or surface), by a [...] "judicious choice" [...] of parameter values—priced here, options with differing strikes will return differing implied volatilities. For pricing American options, an Edgeworth-generated ending distribution {{may be combined}} with an R-IBT. Note that this approach is limited as to the set of skewness and kurtosis pairs for which valid distributions are available. One recent proposal, Johnson binomial trees, is to use N. L. Johnson's system of distributions, as this is capable of accommodating all possible pairs; see Johnson <b>SU</b> <b>distribution.</b>|$|R
40|$|It {{is widely}} {{accepted}} {{that some of}} the most accurate predictions of aggregated asset returns are based on an appropriately specified GARCH process. As the forecast horizon is greater than the frequency of the GARCH model, such predictions either require time-consuming simulations or they can be approximated using a recent development in the GARCH literature, viz. analytic conditional moment formulae for GARCH aggregated returns. We demonstrate that this methodology yields robust and rapid calculations of the Value-at-Risk (VaR) generated by a GARCH process. Our extensive empirical study applies Edgeworth and Cornish-Fisher expansions and Johnson <b>SU</b> <b>distributions,</b> combined with normal and Student t, symmetric and asymmetric (GJR) GARCH processes to returns data on different financial assets; it validates the accuracy of the analytic approximations to GARCH aggregated returns and derives GARCH VaR estimates that are shown to be highly accurate over multiple horizons and significance levels. ...|$|R
40|$|Univariate {{continuous}} distributions with unbounded {{range of}} variation {{have not been}} so widely used in simulation as those that are bounded (usually to the left). However situations do occur when they are needed, particularly in operations research and financial applications. Two distributions that have such unbounded range are the Pearson Type IV and Johnson <b>SU</b> <b>distributions.</b> Though both are well known in statistics, {{there is still a}} lack of methods in the literature for fitting these distributions to data which are both efficient and comprehensively reliable. Indeed the Pearson Type IV has the reputation of being difficult to fit. In this paper we identify the pitfalls and propose a fitting method that avoids them. We also show how to test the goodness of fit of estimated distributions. All the procedures described are included as VBA code in an accompanying Excel workbook available online. Two numerical examples are described in detail. ...|$|R
40|$|The {{assumption}} that equity returns follow the normal distribution, most commonly made in financial economics theory and applications, is strongly rejected by empirical evidence {{presented in this}} paper. As it was found in many other studies, we confirm that stock returns follow a leptokurtic distribution and skewness, which {{in most of the}} Southeast European (SEE) markets is negative. This paper investigates further whether there is any distribution that may be considered an optimal fit for stock returns in the SEE region. Using daily, weekly and monthly data samples for a period of five years from ten Southeast European emerging countries, we applied the Anderson-Darling test of Goodness-of-fit. We strongly rejected the aforementioned assumption of normality for all considered data samples and found that the daily stock returns are best fitted by the Johnson <b>SU</b> <b>distribution</b> whereas for the weekly and monthly stock returns there was not one predominant, but many distributions that can be considered a best fit...|$|R
40|$|FIGURE 1. Pseudoceros astrorum n. sp. A, dorsal view of type specimen; B, {{detail of}} dorsal {{anterior}} margin; C, diagram of colour pattern; D, diagram of ventral view; E, ventral view of fixed specimen. Abbreviations to the figures: fg, female gonopore; i, intestine; m, mouth; mg, male gonopore; ph, pharynx; <b>sb,</b> spermiducal bulbs; <b>su,</b> sucker; u, uterus. Scale bars: A, 10 mm; B, 3 mm...|$|R
40|$|We {{present an}} {{extension}} of the traditional Sharpe ratio to allow for the evaluation of non-normal return distributions. Combining earlier work in this area with stochastic simulation, we develop a procedure that allows {{for the construction of a}} benchmark for the evaluation of the performance of funds with a non-normal return distribution, while maintaining the operational ease of the Sharpe ratio. Similar to the latter, our procedure only requires the risk-free rate of interest rate, the distribution of the market index and an assumption about the type of return distributions to be evaluated. Unlike the Sharpe ratio, however, we are not restricted to normality but are able to handle any reasonable type of distribution. Since our benchmarking procedure is based on the no-arbitrage assumption, it also provides insight into the conditional arbitrage-free value of one distributional parameter in terms of another. We show that in case of the Johnson <b>Su</b> <b>distribution</b> the relationship between skewness and mean return is more or less flat. Skewness and median return on the other hand exhibit a strong negative relationship. ...|$|R
40|$|ABSTRACT. The {{non-tidal}} LOD {{data are}} analysed (data span 01. 01. 1962 - 09. 01. 2008) {{in order to}} provide the probabilistic characteristics of the Earth rotation rate fluctuations. The skewness of the LOD probability distribution is of- 0. 31 indicating that the probability distribution is asymmetrical. Moreover, the residual non-tidal LOD data is considered (after removal of the semiannual, annual, 9. 3 -years, and 18. 6 -years oscillations, and linear trend). The skewness of this residual data equals to- 0. 64 and indicates an increased asymmetry in the distribution. For the non-transformed LOD data the kurtosis is of 2. 31 and it shows that the distribution is flattened. The kurtosis for the residuals is 5. 64 indicating that the distribution is more peaked than a normal distribution. For the non-transformed LOD data, the Jonhson SB distribution provides the best fit. For the residual LOD data, the Johnson <b>SU</b> <b>distribution</b> is found to be the most appropriate model. Both the LOD and it’s residual time series are appropriately modeled by probability laws that are different from a normal distribution. 1...|$|R
40|$|The {{aim of this}} diploma {{thesis is}} to analyze ways of Value at Risk calculation. Its core {{is to get a}} {{suitable}} model that could most appropriately reflect the probability distribution of returns of the Czech stock portfolio that we have generated. In this thesis we find out that the returns follow unbounded distribution which was first described by Johnson (1949). Since we detect that returns are correlated we have to apply appropriate autoregressive process that removes this dependency. In the empirical part we discover an inability of models based on assumptions of normality, to correctly predict the Value at Risk. Historical simulation methods, which have promising backtesting results, are rejected because of the slow adaptation to the recent changes in the market. However, we find a way how to implement Johnson <b>SU</b> <b>distribution</b> into the GARCH model. This model, which passes all the tests, is thus able to predict Value at Risks of the portfolio most accurately. JEL Classification: C 16, C 22, G 11 Keywords: Market risk, Value at Risk, Risk management, Johnson SU distributio...|$|R
40|$|The data on JIFs {{provided}} by Thomson Scientific {{can only be}} considered as a sample since they do not cover the entire universe of those documents that cite an intellectual output (paper, article, etc) or are cited by others. Then, questions arise if the empirical distribution (best fit to the JIF data for any particular year) really represents the true or universal distribution, are its estimated parameters stable over the samples and do they have some scientific interpretation? It may be noted that if the estimated parameters do not exhibit stability over the samples (while the sample size is large enough), they cannot be scientifically meaningful, since science is necessarily related with a considerable degree of regularity and predictability. Stability of parameters is also a precondition to other statistical properties such as consistency. If the estimated parameters lack in stability and scientific meaning, then the empirical distribution, howsoever fit to data, has little significance. This study finds that although Burr- 4 p, Dagum- 4 p and Johnson <b>SU</b> <b>distributions</b> fit extremely well to the sub-samples, the parameters of the first two distributions do not have stability over the subsamples. The Johnson SU parameters have this property. ...|$|R
30|$|Every SU can {{determine}} the probability of each channel being available on its entire transmission time and determine the channel availability vector E. This probabilities are determined using the PU and SU arrival rates over each channel received from its BS. Since the arrival pattern of PU and <b>SU</b> follows Poisson <b>distribution,</b> probability that no SU or PU will appear over the data transmission time can be generated, which {{will lead to the}} overall probability of a channel being idle.|$|R
40|$|Abstract Simplified dentin-bonding {{systems are}} {{clinically}} employed for most adhesive procedures, {{and they are}} prone to hydrolytic degradation. Objective This study aimed to investigate the effect of laser diode irradiation {{on the degree of}} conversion (DC), water sorption (WS), and water solubility (WSB) of these bonding systems in an attempt to improve their physico-mechanical resistance. Material and Methods Two bonding agents were tested: a two-step total-etch system [Adper™ Single Bond 2, 3 M ESPE (SB) ] and a universal system [Adper™ Single Bond Universal, 3 M ESPE (SU) ]. Square-shaped specimens were prepared and assigned into 4 groups (n= 5) : <b>SB</b> and <b>SU</b> (control groups – no laser irradiation) and SB-L and SU-L [SB and SU laser (L) – irradiated groups]. DC was assessed using Fourier transform infrared spectroscopy with attenuated total reflectance. Additional uncured resin samples (≈ 3. 0 µL, n= 5) of each adhesive were also scanned for final DC calculation. For WS/WSB tests, similar specimens (n= 10) were prepared and measured by monitoring the mass changes after dehydration/water storage cycles. For both tests, adhesive fluids were dropped into standardized Teflon molds (6. 0 × 6. 0 × 1. 0 mm), irradiated with a 970 -nm laser diode, and then polymerized with an LED-curing unit (1 W/cm 2). Results Laser irradiation immediately before photopolymerization increased the DC (%) of the tested adhesives: SB-L>SB>SU-L>SU. For WS/WSB (μg/mm 3), only the dentin bonding system (DBS) was a significant factor (p SU. Conclusion Irradiation with a laser diode improved the degree of conversion of all tested simplified dentin bonding systems, with no impact on water sorption and solubility...|$|R
40|$|Skewness–kurtosis (β 3 −β 4) and L-skewness–L-kurtosis (τ 3 −τ 4) {{planes are}} {{proposed}} here as diagnostic tools {{to guide the}} identification of drop size distributions (DSDs) of rainfall at the ground. Firstly, we have determined β 3 −β 4 and τ 3 −τ 4 domains of 13 distribution families, namely normal, exponential, gamma, truncated gamma, log-normal, truncated log-normal, Weibull, hyperbolic, generalized hyperbolic, log-logistic, skewed Laplace, Johnson <b>SB</b> and Johnson <b>SU.</b> These include those most used to represent DSDs and, in general, particle size distributions (PSDs). Secondly, we have considered 1 min and 2 min disdrometric data, collected at six sites in the United States, and reported the empirical couples (β 3,β 4) and (τ 3,τ 4) in the moment diagrams. The location uncertainty of the empirical couples in the diagrams, mostly due to the occurrence of sampling errors, has been thoroughly investigated. The variability of the empirical DSD couples (β 3,β 4) and (τ 3,τ 4) is well described by truncated gamma, truncated log-normal and Johnson SB over the other considered distributions. However, a Monte Carlo analysis {{has shown that the}} Johnson SB is the most adequate distribution in describing the drop size variability, being characterized by the lowest level of uncertainty...|$|R
30|$|We {{assume that}} there are N {{licensed}} channels in a primary system, {{and each of them}} has identical bandwidth. Among these N channels, R channels are dedicated for PUs, and N - R channels are shared by PUs and SUs. A SU can sense the shared channels by spectrum sensing and access the channel if it is not occupied by a PU. The PU and the SU arrival processes follow Poisson process with arrival rates λp and λs, respectively. The service in the CRN is a single-slot first come first served transmission. The service time of the PU follows exponential distribution with mean 1 /μp and that of the <b>SU</b> follows exponential <b>distribution</b> with mean 1 /μs. As the number of spectrum holes varies with PUs traffic dramatically, we assume the traffic of SUs has much shorter average service time compared to the traffic of PUs. A first in first out buffer of size Q is allocated for the secondary system.|$|R
40|$|Abstract—In this paper, we {{consider}} a cognitive radio system with N secondary user (SU) pairs sharing spectrum {{with a pair}} of primary users (PU). The SU power allocation problem is formulated as a capacity maximisation problem under PU and SU quality of service and SU peak power constraints. We show our problem formulation is a geometric program and can be solved with convex optimisation techniques. We examine the effect of PU transmissions in our formulations. Solutions for both low-and high- signal-to-interference-and-noise ratio (SINR) scenarios are provided. We show that including the PU capacity in the optimisation problem in some circumstances leads to increased PU performance while not significantly degrading SU capacity. In a practical wireless communications system, accurate channel state information (CSI) is not often available hence we formulate power allocation problems with both perfect and imperfect CSI and analyse the performance loss incurred due to imperfect CSI. Furthermore, we present a novel method of detecting and removing infeasible SU quality of service constraints from the SU power allocation problem that results in considerably improved <b>SU</b> performance. Cumulative <b>distribution</b> functions of capacity for various Rayleigh fading channels are presented. I...|$|R
30|$|Let us now {{describe}} the related {{work in the}} context of CCRNs. Authors in [18] investigated the performance of a CRN in which the primary network performs spectrum leasing considering the QoS requirements of both primary and secondary users. Additionally, in [18], a dynamic pricing scheme for spectrum leasing and utility-based resource allocation is introduced, and the performance of dynamic leasing is compared against that of static leasing and approaches without leasing. Authors in [22] proposed a mechanism that incentivizes PUs to release resources for cooperation with SUs by means of a reward function. Other related work is [23], in which a CCRN model that comprises two primary networks and one secondary network is proposed. The two primary networks lease resources to a secondary network on different time intervals. To maximize the throughput of the three networks, a lease scheduler is implemented to guarantee the QoS needed for each network. In [24], a leasing mechanism based on resource reservation from a primary network is proposed and analyzed. Authors in [25] proposed a game theory solution for resource leasing through the usage of long-term contracts with PUs and short-term contracts with <b>SUs</b> for the <b>distribution</b> of the leased channels. Finally, in our early work [5], we studied the Erlang capacity 1 in CCRNs with delay-sensitive traffic. However, in [5], the impact of the mean service time of SUs on system performance is not addressed. Opposite to [18, 22 – 25], in this paper, a third sub-network is considered as the one that leases the channels from the main CCRN and the system performance in terms of both spectrum leasing cost and system Erlang capacity is evaluated. This allows us to find relevant performance issues not previously reported in the existing literature. Moreover, in the present work, the maximum Erlang capacity is obtained and the factors that impact system Erlang capacity are investigated.|$|R
30|$|Numerous {{studies have}} been {{conducted}} on this subject. Analytical expressions to characterize a spontaneous cocurrent imbibition process of the wetting fluid into gas-saturated porous media were proposed based on the fractal characteristics of porous media (Cai et al. 2010 a, b). Cai et al. (2010 a, b) presented a fractal capillary model to analyze the depth of extraneous fluid invasion and concluded that the tortuosity of the capillaries and capillary pressure should be considered in analyzing extraneous fluid invasion in low-porosity porous media. Cai et al. (2012) presented a complete analytical model to characterize the vertical spontaneous imbibition of wetting liquid into gas-saturated porous media based on the fractal characteristics of pores in porous media; they also analyzed the influencing factors of the imbibition process. Cai et al. (2014) presented a generalized model that could describe the time evolution of spontaneous imbibition for many wetting liquids in natural and artificial porous media based on the modified H − P and L − Y equations. Tan et al. (2015 c, d) presented a model for transient flow in porous media based on the fractal properties of tree-shaped capillaries and generalized Darcy’s law. They believed that the fractal characteristics of the tree-shaped fractal networks should be considered in analyzing transient flow in heterogeneous porous media. Zhang et al. (2015) presented a new semi-analytical model for vertical wells with simulated reservoir volume (SRV). A model that could approximately reproduce the global trend of the variation in the time exponent with changing porosity was derived for capillary imbibition in porous media based on the tortuous capillary model and fractal geometry (Cai and Yu 2011). Tan et al. (2015 c, d) presented a permeability model for porous media that considered the stress sensitivity based on the mechanics of materials and the fractal characteristics of solid cluster size <b>distribution.</b> <b>Su</b> et al. (2015) presented an improved second-order finite element mixed model for fluid flow in constricted reservoirs. Sensitivity analyses were performed {{to determine the effects of}} the threshold pressure gradient and the permeability of various media. The result shows that increasing the permeability of artificial fractures is a suitable strategy to raise the actual bottom hole pressure, but only in low-permeability matrices. Tan et al. (2015 a, b) presented novel predictive models for the permeability and porosity of porous media. These models, which considered stress sensitivity, were based on fractal theory and the mechanics of materials.|$|R
40|$| particolare la telefonia mobile, è cresciuta in maniera impressionante nell'ultima decade e fa oramai parte della vita quotidiana di una grande porzione della società odierna. La crescente diffusione di apparecchiature di comunicazione mobile, attualmente rinforzata dal passaggio dalla tecnologia di seconda generazione 1 a quella di terza generazione 2, fa sorgere questioni riguardanti la produzione, l'utilizzo, e il trattamento o lo smaltimento rispettosi dell'ambiente. Per poter determinare le possibili ottimizzazioni dal lato ecologico di grandi sistemi tecnici, viene vieppiù utilizzato il metodo dei bilanci ecologici 3. Finora questo metodo è stato impiegato soprattutto per studiare le ripercussioni sull'ambiente delle fasi di produzione e di utilizzo dei sistemi di telefonia mobile. LCA e' stato utilizzato solo marginalmente per studiare le conseguenze ambientali del trattamento di rifiuti elettronici prodotti dalla telefonia mobile. Pertanto una valutazione affidabile e completa delle conseguenze sull'ambiente richiede un'analisi di tutti i tre cicli di vita. Oggetto di questa tesi di dottorato è l'analisi delle ripercussioni sull'ambiente dovute al trattamento di rifiuti elettronici della telefonia mobile, sia dell'attuale generazione (2 G) che della futura (3 G). Lo scopo e' quello di ricavare conoscenze profonde riguardo alle conseguenze ambientali del trattamento, e di paragonarle con quelle delle fasi di produzione e di utilizzo. Inoltre si vogliono mettere a disposizione di studi futuri dati ambientali affidabili. Mentre il Capitolo 1 contiene una breve introduzione, il Capitolo 2 (LCA method {{applied to}} mobile phone technology) si dedica al metodo sviluppato per applicare LCA all'analisi ecologica di reti di telefonia mobile. Per poter studiare separatamente le ripercussioni sull'ambiente delle singole componenti delle reti di telefonia mobile durante le fasi di produzione, di utilizzo e di smaltimento, viene proposta una suddivisione dell'infrastruttura della rete nella quale le componenti vengono {{ordinate}} gerarchicamente nelle classi A-D. Contemporaneamente conoscenze tecniche di base servono a modellare una rete di telefonia mobile. Per ottenere un modello e un'analisi degli aspetti ecologici del trattamento di rifiuti elettronici il più possibilmente vicini alla realtà, pure la fase di trattamento viene suddivisa. In seguito, nel Capitolo 3 (Technical characterisation of mobile phone technology) vengono descritti in dettaglio l'infrastruttura e le tecniche di comunicazione della telefonia mobile 2 G e 3 G. Basandosi sull'approccio presentato nel Capitolo 2, le singole componenti di una rete di telefonia mobile vengono presentate in dettaglio. Aspetti tecnici vengono brevemente descritti. Utilizzando il medesimo approccio vengono spiegate le fasi di trattamento di rifiuti e le sue subfasi. Nei Capitoli 4 e 5 vengono presentati i risultati di studi LCA applicati a una singola componente di rete rispettivamente a una rete intera, entrambi facenti parte dello standard Global System for Mobile communication 4. Lo studio (Screening LCA of an antenna station rack) si basa su un'inventarizzazione completa dei consumi di materia prima e delle emissioni di un armadio di distribuzione. Pure in maniera estesa viene inventarizzata l'infrastruttura utilizzata nella fase di trattamento dei rifiuti. Sei scenari differenti di trattamento di rifiuti vengono sviluppati per poter determinare le ripercussioni sull'ambiente di diverse strategie di trattamento dei rifiuti e per poter definire l'alternativa ottimale. Un allargamento del sistema tramite inclusione della fase di produzione viene elaborato per ciascun scenario. La produzione di materia prima per sostituire materiale deposto o non riciclato, in particolare la produzione di palladio (che contribuisce per il 40 % al carico ambientale qualitativo del sistema ambientale), domina il carico totale sull'ambiente di un armadio di distribuzione. Emissione di metalli pesanti provenienti da componenti deposti di armadi di distribuzione e di altri prodotti di smaltimento influiscono in particolare sulla salute dell'uomo e sulla qualità del sistema ecologico. Il deposito di parti di armadi di distribuzione contribuisce per il 70 % a effetti non cancerogeni e i rifiuti sottoforma di polveri depositati provenienti dalla produzione di acciaio contribuiscono per il 11 %. I risultati mettono in risalto il fatto che possibilmente tutti i rifiuti elettronici dovrebbero essere trattati adeguatamente in impianti di riciclaggio di metallo. Non è necessario disfare completamente un armadio di distribuzione prima che venga riciclato. Con una separazione completa delle materie prime durante la fase di trattamento dei rifiuti, per esempio in materiali ferrosi, non ferrosi e plastici, non si ottiene un'ulteriore riduzione del carico ambientale. Per evitare che si sprigionino metalli pesanti volatili, si dovrebbe evitare di incenerire rifiuti elettronici durante il loro smaltimento. Una standardizzazione di componenti elettroniche potrebbe contribuire al riutilizzo di queste componenti o di interi armadi di distribuzione. Lo studio (LCA of a GSM Network) comprende il bilancio ecologico di una rete di telefonia mobile GSM, basandosi su una completa inventarizzazione dei consumi di materia prima, delle emissioni e delle fasi di trattamento delle singole componenti di una rete di telefonia mobile GSM. L'importanza ecologica delle tre fasi: produzione, utilizzo e trattamento, è stata studiata con IMPACT 2002 +. I sei scenari di trattamento dei rifiuti, menzionati sopra, sono stati nuovamente analizzati per determinare la strategia di trattamento ottimale. I risultati mostrano che la fase di utilizzo di una rete è la predominante tra le tre fasi di vita di una rete di telefonia mobile in quanto a carico ambientale. Nella fase di produzione il carico proviene dalla produzione di schede madri, che richiede molta energia. La fase di trattamento domina negli effetti sulla qualità del sistema ecologico, in particolare le emissioni a lungo termine di metalli pesanti. Un'analisi dettagliata della fase di trattamento mostra che il riciclaggio di materiali porta a una riduzione del carico ambientale totale di una rete di telefonia mobile: nella fase di trattamento (minore e nessuna emissione a lungo termine da deponie) e tramite diminuzione della produzione di materia prima nella fase di produzione. I risultati sperimentali del trattamento termico di schede madri 5 vengono presentati nel Capitolo 6 (Heavy metal partitioning from electronic scrap during thermal End-of-Life treatment), in particolare concentrandosi sulla volatilità dei metalli pesanti As, Cd, Ni, Ga, Pb, Sb e Zn. Campioni di schede madri identiche furono bruciati in un forno a quarzo per analizzare la volatilità di alcuni metalli pesanti. In preparazione sono stati eseguiti esperimenti di evaporazione con un termogravimetro, unito a un TG-ICP. Gli esperimenti nel forno a quarzo sono stati realizzati in condizioni riducenti e ossidanti a 550 e 880 °C. Le misurazioni ottenute sono state paragonate con i risultati di calcolo dell'equilibrio termodinamico. Né As, né Cd e neppure Ga sono stati trovati nei residui. Ni rimane quasi stabile sotto qualsiasi condizione. Zn tende a una volatilità maggiore a temperature più elevate e a dipendenza del contenuto di ossigeno. Per contro Sb è volatile quasi indipendentemente dalla temperatura e dal contenuto in ossigeno. I risultati mostrano che nell'incenerimento di rifiuti elettronici bisogna porre particolare attenzione <b>su</b> <b>Sb,</b> As e Ga. Questi metalli vengono utilizzati sempre più nelle componenti più recenti, in particolare nelle componenti di reti della telefonia mobile di terza generazione. I risultati di uno studio comparativo sulle ripercussioni ambientali di una rete GSM paragonata a una rete Universal Mobile Telecommunication System 6 sono contenuti nel Capitolo 7 (LCA of of Second Generation (2 G) and Third Generation (3 G) Mobile Phone Networks). Inoltre vengono riportate possibili ripercussioni ambientali di reti UMTS future. Lo studio si basa su un'inventarizzazione completa dei consumi di materia prima e delle emissioni di ogni singola componente di rete di telefonia mobile UMTS. Per ottenere risultati vicini alla realtà vien dato peso a un modello fedele agli standard di telefonia mobile. Le ripercussioni sull'ambiente sono state nuovamente analizzate con IMPACT 2002 +. La credibiltà dei risultati è stata confortata da risultati ottenuti con altri metodi. I risultati mostrano che un'operazione parallela di reti GSM e UMTS è sfavorevole all'ambiente. La fase di transizione tra i due standard dovrebbe essere la più breve possibile. La fase di utilizzo delle componenti di rete contribuiscono maggiormente al carico totale della rete. In particolare dovrebbe essere ridotto il consumo di energia delle stazioni di antenne. Se paragonate tra di loro, la tecnologia UMTS (utilizzata in maniera sensata) è più ecologica per unità di dati trasmessa. Gli studi mostrano che il riciclaggio di rifiuti elettronici è ecologicamente sensato. Nelle condizioni attuali il riciclaggio può compensare il carico ambientale della fase di produzione fino al 50 %. Il Capitolo 8 infine ricapitola i risultati ottenuti, basandosi sui quali vengono mostrati i limiti tematici e le possibili sfide future nell'ambito della telefonia mobile. I risultati documentati nella presente tesi di dottorato vengono completati nelle appendici (AD). [...] 1 2 G. 2 3 G. 3 Life Cycle Assessment (LCA). 4 GSM. 5 Printed Wired Board Assembly (PWBA). 6 UMTS. Mobilkommunikation, insbesondere Mobiltelephonie, stellt eine Dienstleistung dar, deren Inexistenz heutzutage undenkbar ist. Die beständig zunehmende Anzahl an Mobiltelephonen und Netzwerkinfrastruktur, derzeit noch verstärkt bedingt durch den Übergang zwischen Mobiltelephontechnologien der zweiten Generation 1 und der dritten Generation 2, wirft zunehmend die Frage nach umweltschonender Produktion, Verwendung und Verwertung auf. Um die ökologischen Optimierungspotentiale großer technischer Systeme, wie Mobiltelephonnetzwerke, zu bestimmen, wird zunehmend die Ökobilanzierungsmethode, als dem Stand der Technik entsprechend, eingesetzt. Bis heute wurde diese Methode hauptsächlich verwendet um die Umweltauswirkungen der Produktions- und der Verwendungsphase zu bestimmen. Nur marginal hingegen, wurde die Bedeutung der Verwertungsphase von Elektronikschrott aus Mobiltelephonnetzwerken untersucht. Eine verlässliche und umfassende Bewertung des ökologischen Aspektes der Mobiltelephonie bedarf jedoch einer tiefgreifenden Untersuchung aller drei Lebensphasen eines entsprechenden Netzwerkes. Der Schwerpunkt der vorliegenden Dissertation liegt auf der Bewertung der Umweltauswirkungen hervorgerufen durch der Verwertung von Elektronikschrott aus Mobiltelephonnetzwerken, die sowohl den derzeit aktuellen (2 G) als auch den zukünftigen (3 G) Mobiltelephoniestandards entsprechen. Ziel ist es, profunde Kenntnisse über die Effekte auf die Umwelt zu erlangen und verlässliche Daten für nachfolgende Studien zur Verfügung zu stellen. Nach einer kurzen Einführung in Kapitel 1, wird in Kapitel 2 die Verwendung der Ökobilanzmethode zur Bestimmung der Umweltauswirkungen von Mobiltelephonnetzwerken vorgestellt (LCA method applied to mobile phone technology). Um die ökologischen Auswirkungen der einzelnen Netzwerkkomponenten während der Herstellungs-, der Benutzungsund während der Verwertungsphase getrennt darstellen zu können, wird eine Untergliederung der Netzwerkinfrastruktur vorgeschlagen. Dabei sollen die einzelnen Komponenten hierarchisch in Klassen A bis D eingeteilt werden. Gleichzeitig angeeignetes technisches Grundlagenwissen wird verwendet um Mobiltelephonnetzwerke modellhaft zusammenzustellen. Um eine möglichst realitätsnahe Modellierung u...|$|R

