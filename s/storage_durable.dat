0|41|Public
40|$|End-to-end Data Integrity Feature For NFSv 4 draft-cel-nfsv 4 -end 2 end-data-protection- 00 End-to-end data {{integrity}} protection provides a strong guarantee that data an application reads from <b>durable</b> <b>storage</b> {{is exactly the}} same data it wrote previously to <b>durable</b> <b>storage.</b> This document specifies possible additions to the NFSv 4 protocol enabling it to convey endto-end {{data integrity}} information between client and server...|$|R
40|$|Unity {{provides}} {{secure and}} <b>durable</b> <b>storage</b> for personal data {{that does not}} depend on the security or availability of a cen-tral service. Instead, Unity exploits the trend towards users having more personal computing devices and the increasing amounts of storage available on those devices. This moti-vates the design of Unity, which does not store data on the cloud provider at all, but instead leverages the availability of the cloud provider to mount a coordination service that enables a user’s devices to provide <b>durable</b> <b>storage</b> for the user’s data themselves. Categories and Subject Descriptor...|$|R
40|$|Decentralized storage systems {{aggregate}} {{the available}} disk space of participating computers {{to provide a}} large storage facility. These systems rely on data redundancy to ensure <b>durable</b> <b>storage</b> despite of node failures. However, existing systems either assume independent node failures, or they rely on introspection to carefully place redundant data on nodes with low expected failure correlation. Unfortunately, node failures are not independent in practice and constructing an accurate failure model is difficult in large-scale systems. At the same time, malicious worms that propagate through the Internet pose a real threat of large-scale correlated failures. Such rare but potentially catastrophic failures must be considered when attempting to provide highly <b>durable</b> <b>storage...</b>|$|R
40|$|TDRI Training course: The Tropical Development and Research Institute near London {{offers a}} 4 - 6 month {{course in the}} <b>storage</b> of <b>durable</b> {{agricultural}} products in the tropics. The course starting in August each yea;, aims to provide {{an understanding of the}} characteristics of crops after harvesting, the potential causes of deterioration and the methods of prevention and control. Candidates should hold a degree or equivalent in biology agriculture or engineering, or should have some years of experience in the <b>storage</b> of <b>durable</b> agricultural products The first part of the course consists of an introductory four week series of lectures case studies and discussions aimed at providing an overall understanding of tropical storage problems. This is followed by 3 to 5 months of specialized training according to the needs of participants. Applications should be submitted by June 1 st. There are 20 places available. Candidates should indicate where their specific needs lie for {{the second part of the}} course and provide details of present job responsibilities and future prospects. For applications and further enquiries, write to: Training Officer Tropical Development and Research Institute 127 Clerkenwell Road London EC 1 R 5 D B UK[training course...|$|R
40|$|Peer-to-peer {{systems are}} {{positioned}} {{to take advantage}} of gains in network bandwidth, storage capacity, and computational resources to provide long-term <b>durable</b> <b>storage</b> infrastructures. In this paper, we contribute a naming technique to allow an erasure encoded document to be self-verified by the client or any other component in the system...|$|R
40|$|The Natural Resources Institute (NRI) has {{conducted}} annual training courses entitled <b>Storage</b> of <b>durable</b> agricultural {{products in the}} tropics. These courses have trained 371 students from 61 countries who, on completion, were awarded the NRI certificate. In June 1994 the course was validated by the University of Greenwich as a postgraduate Diploma in Grain Storage Management {{in its own right}} and as a pre-qualifier for an NRI supervised programme leading to an MSc in Grain Storage Management. David Walker Course Director NRI, Central Avenue. Chatham Maritime, Kent ME 4 4 TB, UK The University of East Anglia's School of Development Studies (DEV) has joined together with two other institutes - Biological Sciences (BIO) and the John Innes Centre of Plant Science Research (JIC) to bring together expertise to offer a MSc in Plant Breeding for Agricultural Development. The programme comprises biology and biotechnology courses run by BIO; farming systems and experimentation courses run by DEV, and a specialist plant breeding module run by the JIC. Dr Stephen Morse, School of Development Studies, University of East Anglia Norwich NR 4 7 TJ UKThe Natural Resources Institute (NRI) {{has conducted}} annual training courses entitled <b>Storage</b> of <b>durable</b> agricultural products in the tropics. These courses have trained 371 students from 61 countries who, on completion, were awarded the NRI [...] ...|$|R
5000|$|Transactions {{provide an}} [...] "all-or-nothing" [...] proposition, stating that each work-unit {{performed}} in a database must either complete in its entirety or have no effect whatsoever. Further, the system must isolate each transaction from other transactions, results must conform to existing constraints in the database, and transactions that complete successfully must get written to <b>durable</b> <b>storage.</b>|$|R
40|$|Decentralized storage systems {{aggregate}} {{the available}} disk space of participating computers {{to provide a}} large storage facility. These systems rely on data redundancy to ensure <b>durable</b> <b>storage</b> despite of node failures. However, existing systems either assume independent node failures, or they rely on introspection to carefully place redundant data on nodes with low expected failure correlation. Unfortunately, node failures are not independent in practice and constructing an accurate failure model is difficult in large-scale systems. At the same time, malicious worms that propagate through the Internet pose a real threat of large-scale correlated failures. Such rare but potentially catastrophic failures must be considered when attempting to provide highly <b>durable</b> <b>storage.</b> In this paper, we describe Glacier, a distributed storage system that relies on massive redundancy to mask the effect of large-scale correlated failures. Glacier is designed to aggressively minimize {{the cost of this}} redundancy in space and time: Erasure coding and garbage collection reduces the storage cost; aggregation of small objects and a loosely coupled maintenance protocol for redundant fragments minimizes the messaging cost. In one configuration, for instance, our system can provide six-nines <b>durable</b> <b>storage</b> despite correlated failures of up to 60 % of the storage nodes, at the cost of an elevenfold storage overhead and an average messaging overhead of only 4 messages per node and minute during normal operation. Glacier is used as the storage layer for an experimental serverless email system. ...|$|R
5000|$|Cart {{bags are}} {{generally}} {{designed to be}} harnessed to a two-wheeled pull cart or a motorized golf cart during play of a round. They often have only a rudimentary carry strap or handle for loading and transporting the bag, and no stand legs, but may feature extra <b>storage</b> or more <b>durable</b> construction, as weight of the loaded bag is a lesser concern.|$|R
40|$|Transaction {{performance}} {{is dominated by}} logging throughput. It is proposed to split up logs into a fast bounded part and a conventional unlimited part. The bounded log consists of volatile main memory which is saved to <b>durable</b> <b>storage</b> when a power failure occurs. A plug-in power failure detection device of a negligible costs is presented which significantly improves transaction throughput on customary workstations. (orig.) Available from TIB Hannover: RO 5634 (1994, 3) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekSIGLEDEGerman...|$|R
40|$|Abstract—This paper {{explores the}} {{feasibility}} of a storage architecture that offers the reliability and access performance characteristics of a high-end system, yet is cost-efficient. We propose ThriftStore, a storage architecture that integrates two types of components: volatile, aggregated storage and dedicated, yet low-bandwidth <b>durable</b> <b>storage.</b> On the one hand, the <b>durable</b> <b>storage</b> forms a back end that enables the system to restore the data the volatile nodes may lose. On the other hand, the volatile nodes provide a high-throughput frontend. Although integrating these components {{has the potential to}} offer a unique combination of high throughput and durability at a low cost, a number of concerns need to be addressed to architect and correctly provision the system. To this end, we develop analytical and simulation-based tools to evaluate the impact of system characteristics (e. g., bandwidth limitations on the durable and the volatile nodes) and design choices (e. g., the replica placement scheme) on data availability and the associated system costs (e. g., maintenance traffic). Moreover, to demonstrate the high-throughput properties of the proposed architecture, we prototype a GridFTP server based on ThriftStore. Our evaluation demonstrates an impressive, up to 800 Mbps transfer throughput for the new GridFTP service. Index Terms—Distributed storage, modeling storage reliability trade-offs, low-cost storage. ...|$|R
50|$|EOS' Drip Irrigation System is a low-cost, low-tech {{irrigation}} method that uses less water than traditional methods focused on fighting food security and increasing incomes. Through {{the use of}} plastic tubing with small holes at consistent intervals (commonly known as drip tape), water is distributed directly {{to the root of}} a plant. A drip irrigation system requires a water source and typically consists of: a storage device, a piping system, a filter, and drip tape. Systems may include the use of rivers, wells, and portable water for the primary source as well as low-cost methods such as harvesting rainwater to allow farmers in the driest, most water scarce areas to increase yields and grow vegetables during the dry season. EOS' system uses a metal barrel as a <b>storage</b> device, <b>durable</b> plastic hose for the piping system, and plastic connectors that connect drip tape to the plastic hose. All materials are purchased in Nicaragua.|$|R
40|$|Concentrating {{solar power}} (CSP) systems use solar {{absorbers}} {{to convert the}} heat from sunlight to electric power. Increased operating temperatures are necessary to {{lower the cost of}} solar-generated electricity by improving efficiencies and reducing thermal energy <b>storage</b> costs. <b>Durable</b> new materials are needed to cope with operating temperatures > 600 C. The current coating technology (Pyromark High Temperature paint) has a solar absorptance in excess of 0. 95 but a thermal emittance greater than 0. 8, which results in large thermal losses at high temperatures. In addition, because solar receivers operate in air, these coatings have long term stability issues that add to the operating costs of CSP facilities. Ideal absorbers must have high solar absorptance (> 0. 95) and low thermal emittance (< 0. 05) in the IR region, be stable in air, and be low-cost and readily manufacturable. We propose to utilize solution-based synthesis techniques to prepare intrinsic absorbers for use in central receiver applications...|$|R
40|$|Peer-to-peer {{systems are}} {{positioned}} {{to take advantage}} of gains in network bandwidth, storage capacity, and computational resources to provide long-term <b>durable</b> <b>storage</b> infrastructures. In this paper, we quantitatively compare building a distributed storage infrastructure that is self-repairing and resilient to faults using either a replicated system or an erasure-resilient system. We show that systems employing erasure codes have mean time to failures many orders of magnitude higher than replicated systems with similar storage and bandwidth requirements. More importantly, erasure-resilient systems use an order of magnitude less bandwidth and storage to provide similar system durability as replicated systems...|$|R
40|$|Digital storage {{demand is}} growing with the {{increasing}} use of digital artifacts from media files to business docu-ments. Regulatory frameworks ask for unaltered, <b>durable</b> <b>storage</b> of business communications. In this paper we consider the problem of getting reliable evidence of the integrity and existence of some data from a storage service even if the data is not available for reference anymore and even if the partner may be untrustworthy or malicious. Several solutions are presented, among them the first cryptographically secure approach providing evidence of the integrity of remotely stored data, without owning or transferring the complete original data. ...|$|R
40|$|The rise of {{ubiquitous}} computing {{has created a}} need for wide-area <b>durable</b> <b>storage.</b> We propose a model and interface for such an archival system, that stores data in a durable, verifiable, available, and self-maintainable manner. We argue that such a system can be created by using novel techniques of erasure codes, secure hashing, and decentralized wide-area location infrastructures to distribute fragments across the wide-area on an arbitrary set of servers. This model allows files to remain available even as servers fail. Finally, we implement Silverback, a prototype archival system using the model that we developed, and measure its performance. ...|$|R
40|$|This paper {{explores the}} {{feasibility}} of a cost-efficient storage architecture that offers the reliability and access performance characteristics of a high-end system. This architecture exploits two opportunities: First, scavenging idle storage from LAN-connected desktops not only offers a low-cost storage space, but also high I/O throughput by aggregating the I/O channels of the participating nodes. Second, the two components of data reliability – durability and availability – can be decoupled to control overall system cost. To capitalize on these opportunities, we integrate two types of components: volatile, scavenged storage and dedicated, yet low-bandwidth <b>durable</b> <b>storage.</b> On the one hand, the <b>durable</b> <b>storage</b> forms a low-cost back-end that enables the system to restore the data the volatile nodes may lose. On the other hand, the volatile nodes provide a high-throughput front-end. While integrating these components {{has the potential to}} offer a unique combination of high throughput, low cost, and durability, a number of concerns need to be addressed to architect and correctly provision the system. To this end, we develop analytical- and simulation-based tools to evaluate the impact of system characteristics (e. g., bandwidth limitations on the durable and the volatile nodes) and design choices (e. g., replica placement scheme) on data availability and the associated system costs (e. g., maintenance traffic). Further, we implement and evaluate a prototype of the proposed architecture: namely a GridFTP server that aggregates volatile resources. Our evaluation demonstrates an impressive, up to 800 MBps transfer throughput for the new GridFTP service...|$|R
5000|$|The most {{well-known}} and notable of Craiks’ research analyzed how memory is encoded and {{various levels of}} depths of processing.Craik and Lockhart postulated that during {{the first stage of}} memory where information is acquired, the encoding stage, there is a series of processing hierarchies. During the initial phase of encoding, an individual experiences [...] "shallow" [...] processing and may reach into the deepest level. Memory traces form {{as a result of these}} processes, containing coding characteristics and persistence in memory.A deeper depth of processing implies that a greater amount of semantic or cognitive analysis must be conducted. Therefore, a stimulus that has undergone a deep level of processing will have a longer, more <b>durable</b> <b>storage</b> and retention.|$|R
40|$|Crop {{protection}} {{has been}} conducted and documented in Kenya {{since the turn of}} the century, but much of it has not been readily accessible. Every Kenyan research station since at least 1907 has published annual reports. The purpose of this review is to document these reports, as well as papers in refereed journals on crop protection topics of concern to smallholder agriculture. The review covers all crop protection topics concerned with pest and disease incidence and the emphasis is on in-country research relevant to smallholder plantings of vegetable and other food crops in Kenya. The document is arranged in three sections: pre-harvest (including pesticide analysis and residues), post-harvest (mainly <b>storage</b> of <b>durable</b> products) and a short chapter on socio-economic aspects. The bibliography contains over 900 references. A review of crop protection research in Kenya edited by G. Farrell G. Kibata and J Sutherland 1995 165 pp ISBN 9966 9604 0 6 KARl National Agricultural Research Laboratories PO Box 14733, Nairobi, KENYAA review of crop protection research in Kenya edited by G. Farrell G. Kibata and J Sutherland 1995 165 pp ISBN 9966 9604 0 6 KARl National Agricultural Research Laboratories PO Box 14733, Nairobi, KENY...|$|R
30|$|Therefore, after a {{thorough}} {{assessment of the}} slopes and materials of the tailing dumps, the authors recommended a number of precautions that could be beneficial {{in the design of}} tailing dumps for safe and <b>durable</b> <b>storage.</b> Effort should be made to avoid flooding within the mine perimeter, thereby reducing water level to as low as possible around and within the dump slope. Slope height and angle optimization must be a common cautious exercise in the design of tailing dump storage. Slopes with higher height and angle than the optimal and/or high groundwater level must undergo stabilization, using any affordable method such as the benching method. The method helps reduce high slope height and angle and also reduces sliding mass in the event of possible slope failure.|$|R
40|$|Molecular {{dynamics}} simulations yield {{large amounts}} of trajectory data. For their <b>durable</b> <b>storage</b> and accessibility an efficient compression algorithm is paramount. State of the art domain-specific algorithms combine quantization, Huffman encoding and occasionally domain knowledge. We propose the high resolution trajectory compression scheme (HRTC) that relies on piecewise linear functions to approximate quantized trajectories. By splitting the error budget between quantization and approximation, our approach beats {{the current state of}} the art by several orders of magnitude given the same error tolerance. It allows storing samples at far less than one bit per sample. It is simple and fast enough to be integrated into the inner simulation loop, store every time step, and become the primary representation of trajectory data...|$|R
40|$|The {{majority}} of processing, memory, and <b>durable</b> <b>storage</b> resources of modern-day computers are left unused, {{leading to a}} tremendous waste of potential computing power. Previous work attempted to address this inefficiency by putting otherwise unused transient resources to work completing processing tasks distributed across a network of machines. The Spawn model adopted a peer-to-peer approach wherein any node can create and distribute a job onto the network. Our model differs in that we allow migration of uncompleted jobs from one node to another. As jobs grow in size, their cost of migration as well, leading to a congealing effect whereby jobs drift towards underutilized nodes until {{it is no longer}} productive to do so. We present a simulation that highlights the important concepts of our model. ...|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimited. Recently developed optical disc technology offers economical, convenient, and <b>durable</b> <b>storage</b> of {{large quantities of}} data. Advanced data retrieval software allows rapid search optical discs. A number of commercial and government publishers produce optical disc databases offering enhanced direct access to data, text or large reference indexes. In library settings direct patron access to optical disc databases has proven popular. This paper examines issues a librarian or information manager should consider before and during implementation of public access optical disc databases. It documents results of a trial optical disc implementation at the Naval Postgraduate School's Dudley Knox Library and a product survey identifying candidate optical disc database products library use. Computer hardware options for distribution of information from optical discs are discussed. A strategy is presented for further implementation of optical disc databases in the Dudley Knox Library. Commander, United States Nav...|$|R
40|$|SIR – In {{looking for}} {{cost-efficient}} CCS, please {{step up and}} walk to your window, where you may see a tree. The evolution of woody plants has solved the problem of capture (photosynthesis) and <b>storage</b> (formation of <b>durable</b> cells) at minimal cost. After what is called “forest transition”, woody resources of a country cease to shrink and start to expand. Forest transition implies a shift of the landscape from a carbon source to a carbon sink, thus marking the onset of organic, cheap CCS. Alexander Mather of the University of Aberdeen predicted in 1992 that forest transition is the likely future of tropical countries, too. Since then, however, biofuel clearings and other pressures have created new concerns. Organic CCS will again become an issue as climate negotiators reconvene to consider a post-Kyoto treaty in Copenhagen in December this year. Pekka Kauppi Professor of environmental science and policy University of Helsinki Helsink...|$|R
40|$|Windows Azure Storage (WAS) is a {{cloud storage}} system that {{provides}} customers {{the ability to}} store seemingly limitless amounts of data for any duration of time. WAS customers have access to their data from anywhere, at any time, and only pay for what they use and store. To provide durability for that data {{and to keep the}} cost of storage low, WAS uses erasure coding. In this paper we introduce a new set of codes for erasure coding called Local Reconstruction Codes (LRC). LRC reduces the number of erasure coding fragments that need to be read when reconstructing data fragments that are offline, while still keeping the storage overhead low. The important benefits of LRC are that it reduces the bandwidth and I/Os required for repair reads over prior codes, while still allowing a significant reduction in storage overhead. We describe how LRC is used in WAS to provide low overhead <b>durable</b> <b>storage</b> with consistently low read latencies. ...|$|R
40|$|International audienceThis paper surveys {{previous}} distributed storage {{systems and}} related data redundancy and fault-tolerance schemes which {{are introduced to}} overcome the impact of host churn on data reliability. Furthermore, a hybrid storage system model is proposed which offers a reliable data storage service by integrating idle storage contributed by volatile peer nodes and stable and <b>durable</b> <b>storage</b> utilities. In order to ensure high availability and durability for this hybrid storage system, we explore four reliability improvement strategies, including File Replica Strategy, File Encoding Strategy, Replica Repair Strategy, and Stable-Volatile Strategy, {{as well as the}} combination of these four strategies. Extensive simulations based on real traces are performed, in which data availability, data durability, and storage overhead are evaluated. Simulation results show that compared with previous peer-to-peer storage systems, the proposed hybrid storage system could achieve a higher availability and durability with less storage consumption, due to proposed new strategies. Finally, taking into account storage and traffic cost, the tradeoffs between storage efficiency and reliability are discussed...|$|R
40|$|This paper {{presents}} SafeStore, a distributed {{storage system}} designed to maintain long-term data durability despite conventional hardware and software faults, environmental disruptions, and administrative failures caused by human error or malice. The architecture of SafeStore is based on fault isolation, which Safe-Store applies aggressively along administrative, physical, and temporal dimensions by spreading data across autonomous storage service providers (SSPs). However, current storage interfaces provided by SSPs are not designed for high end-to-end durability. In this paper, we propose a new storage system architecture that (1) spreads data efficiently across autonomous SSPs using informed hierarchical erasure coding that, for a given replication cost, provides several additional 9 ’s of durability over what can be achieved with existing black-box SSP interfaces, (2) performs an efficient end-to-end audit of SSPs to detect data loss that, for a 20 % cost increase, improves data durability by two 9 ’s by reducing MTTR, and (3) offers <b>durable</b> <b>storage</b> with cost, performance, and availability competitive with traditional storage systems. We instantiate and evaluate these ideas by building a SafeStore-based file system with an NFSlike interface. ...|$|R
40|$|Abstract. Peer-to-peer {{systems are}} {{positioned}} {{to take advantage}} of gains in net-work bandwidth, storage capacity, and computational resources to provide longterm <b>durable</b> <b>storage</b> infrastructures. In this paper, we quantitatively comparebuilding a distributed storage infrastructure that is self-repairing and resilient to faults using either a replicated system or an erasure-resilient system. We showthat systems employing erasure codes have mean time to failures many orders of magnitude higher than replicated systems with similar storage and bandwidthrequirements. More importantly, erasure-resilient systems use an order of magnitude less bandwidth and storage to provide similar system durability as replicatedsystems. 1 Introduction Today's exponential growth in network bandwidth, storage capacity, and com-putational resources has inspired a whole new class of distributed, peer-to-peer storage infrastructures. Systems such as Farsite[2], Freenet[4], Intermemory[3],OceanStore[8], CFS[5], and PAST[7] seek to capitalize on the rapid growth of resources to provide inexpensive, highly-available storage without centralizedservers. The designers of these systems propose to achieve high availability and long-term durability, in the face of individual component failures, through repli-cation and coding techniques...|$|R
40|$|Traditional {{approaches}} to replication require client requests to be ordered before making them durable by copying them to replicas. As a result, clients must wait for two round-trip times (RTTs) before updates complete. In this paper, {{we show that}} this entanglement of ordering and durability is unnecessary for strong consistency. Consistent Unordered Replication Protocol (CURP) allows clients to replicate requests {{that have not yet}} been ordered, {{as long as they are}} commutative. This strategy allows most operations to complete in 1 RTT (the same as an unreplicated system). We implemented CURP in the Redis and RAMCloud storage systems. In RAMCloud, CURP improved write latency by ~ 2 x (13. 8 us -> 7. 3 us) and write throughput by 4 x. Compared to unreplicated RAMCloud, CURP's latency overhead for 3 -way replication is just 0. 4 us (6. 9 us vs 7. 3 us). CURP transformed a non-durable Redis cache into a consistent and <b>durable</b> <b>storage</b> system with only a small performance overhead. Comment: 16 pages, 13 figure...|$|R
40|$|File {{storage system}} is an {{interesting}} topic in distributed system research. Researchers are finding ways to guarantee a persistent available system. OceanStore is a global persistent data store designed to scale to billions of users. It provides a consistent, highly-available, and <b>durable</b> <b>storage</b> utility atop an infrastructure comprised of untrusted servers. [1] Instead of focusing {{on a large scale}} system, we starts with relatively small one that assumes no heavy request pressure at a very short time period. There’re two main principles of our design: Reed-Solomon Algorithm for fault tolerance and reservation time before retrieval for alleviating server’s burden. Design It is well-known that Reed-Solomon codes may be used to provide error correction for multiple failures in RAID like systems. [2] In our system, each submitted file is first divided into segments of the same size. And then each segment contains several data blocks of the same size. Certain checksum blocks are calculated from these data blocks. All the i th blocks (including both data and checksum blocks) of different segments in a single file are assigned to the same node. Assume that in each segment, the number of data blocks i...|$|R
40|$|Expression of {{cultural}} heritage looking from the informatics angle falls into text, images, video and sound categories. ICT {{can be used}} to conserve all these heritage items like; the text information consisting of palm leaf manuscripts, stone tablets, handwritten paper documents, old printed records, books, microfilms, fiche etc, images including paintings, drawings, photographs and the like, sound items which includes musical concerts, poetry recitations, chanting of mantras, talks of important persons etc, and video items like archival films historical importance. To retrieve required information from such a large mass of materials in different formats and to transmit them across space and time, there are several limitations. Digital technology allows hitherto unavailable facilities for <b>durable</b> <b>storage</b> and speedy and efficient transmission / retrieval of information contained in all the above formats. Hypertext and hypermedia features of digital media enable integrating text with graphics, sound, video and animation. This paper discusses the international and national efforts for digitizing heritage items, digital archiving solutions available, the possibilities of the media, and the need to follow standards prescribed by organizations like UNESCO to enable easy exchange and pooling of information and documents generated in digital archiving systems at national and international level. The need to develop language technology for local scripts for organizing and preserving our cultural heritage is also stressed...|$|R
40|$|In {{the present}} study, I {{examined}} how the {{temporal and spatial}} relationship between two visual targets (T 1 and T 2) affects the recall of both targets when they are embedded in rapidly displayed distractors. Presented on a trial were two synchronized streams of characters, one {{to the left and}} the other {{to the right of the}} fixation. Independent of their spatial relationship, a U-shaped curve described the recall of the second target (T 2) as a function of stimulus onset asynchrony (SOA) between T 1 and T 2. It indicated the presence of the attentional blink with a T 2 deficit sparing up to about 150 - to 200 -msec SOA. However, T 2 deficit was greater at short SOAs (up to about 250 msec) when T 1 and T 2 occurred at different locations than when they occurred at a common location. When SOA was short (100 msec or so), recall of T 1 was impaired when T 1 and T 2 occurred at a common location, but not when they were at different locations. The present findings can be reconciled with existing models (e. g., the interference model and the two-stage model) by distinguishing automatic and controlled attention gating processes at the transfer of perceptual representations to a more <b>durable</b> <b>storage</b> (e. g., visual short-term memory) ...|$|R
40|$|The {{project on}} File {{recovery}} and backup involves using {{of the emerging}} cloud based technology Amazon Simple Storage Service (S 3). This project caters the purpose of safe and secure storage of the data as well as backup of that data for an Educational Institute such as a school or a college. The process in this project involves registering the client using Facebook or Google Accounts or simple sign up form. Then client gets his own manageable storage space for storing the data on the Amazon servers. The project involves creating server side webpage which eases the Signup process. For Backup and recovery of the customer data, project uses the java platform to create client side software. The software involves the simple UI for uploading the data onto the Amazon servers. A person can share the particular data by making that data public, data is stored safely on the cloud as they use different encryption methods. And it involves a feature known as time URL which makes selected data on the cloud available to person or group of persons for a specified interval of the time. The project uses Amazon S 3 Technology because Amazon servers provide Easy robust and <b>durable</b> <b>storage</b> of the data and they provide complete (99. 9999 percent) availability. And also they are inexpensive and reliable than the physical storage options...|$|R
40|$|Fluxes {{of carbon}} dioxide, methane, nitrous oxide, and carbon stocks were {{measured}} at selected Hungarian forests {{and at a}} Croatian stand 120 km far from the Hungarian border. Annual carbon balance for Hungarian forests was also determined. Carbon stock of dendromass in selected Hungarian beech, hornbeam-pedunculate oak, and Turkey oak forests was between 191 and 292 t C ha- 1 in 2003 - 2005. The total carbon stock (dendromass and soil) of Hungarian forests {{is estimated to be}} 377 MtC (1991 - 2000), and the annual carbon uptake of dendromass is 6. 9 MtC. According to the measurements and estimations, the gross primary production (GPP) in 2008 - 2009 at Jastrebarsko pendunculate oak forest was 1, 428 and 1, 633 g C m- 2 year- 1, while ecosystem respiration of CO 2 was 1, 044 and 1, 049 gC m- 2 year- 1, yielding a net ecosystem exchange (NEE) of- 384 and- 584 g C m- 2 year- 1, respectively. Net primary production (NPP) of Hungarian forests (1991 - 2000) was 377 g C m- 2 year- 1 (246 g C m- 2 year- 1 excluding leaves) part of which is released back to the atmosphere by heterotrophic respiration and harvest. Comparison of CO 2 sinks using forest inventory approach (and not considering carbon <b>storage</b> in <b>durable</b> wood products) of Hungary's forest (2. 26 t CO 2 ha- 1 year- 1) and pedunculate oak forests in Croatia (4. 57 t CO 2 ha- 1 year- 1) gives a good agreement considering differences between forests and management practice. However, flux measurements from Jastrebarsko indicate that forests are probably much stronger sink. The share of the other greenhouse gases (CH 4 and N 2 O) to the greenhouse balance between atmosphere and forest ecosystem (expressed in CO 2 equivalent) is negligible in lowland forests of Bodrogköz region (average uptake of CH 4 of 0. 03 t CO 2 -eq ha- 1 year- 1 and emission of N 2 O of 0. 05 t CO 2 -eq ha- 1 year- 1). However, for the forests in Mátra mountain region, emissions of N 2 O are not negligible with 0. 61 and 0. 83 t CO 2 -eq ha- 1 year - 1 for spruce plantation and sessile oak-hornbeam forest, respectively. Due to the lack of long-term observations, these results have to be regarded as first approximations. © 2011 Springer Science+Business Media B. V...|$|R
40|$|Cassia alata L. leaf {{contains}} flavonoids {{which have}} anti-inflammatory, anti-allergy, antioxidants and antifungal effects. The traditional application of it requires long preparation time {{so we need}} a formulation with more practical and <b>durable</b> <b>storage</b> is needed. Gel formulation was chosen because {{it is easy to}} dry, forming an easy to wash film layer and give a sense of cold on the skin. Gel components affect the stability of the gel. Physical stability is analyzed to ensure that the formulated gel’s quality, safety and benefits meet the specifications and survive during storage. This study aimed to create a gel formula and analysed its physical stability test of Cassia alata L. leaf extract gel. Research design adopted in this study was an experimental laboratory. Optimum gel formula determined by variations in the concentration of sodium carboxymethyl celulose (CMC-Na). Gel that meet the criteria of homogeneity, consistency, pH and spreadibility was set as the optimum formula. Physical stability of optimum formula was analyzed by organoleptic, homogeneity   test, pH test, viscosity test and spredability test. Gel that meets the acceptance criteria are Cassia alata L. leaf extract gel with CMC-Na concentration of 3 % those determined as the optimum formula. Stability analysis of optimum formula didn’t show any changes in pH, color, smell and taste, although it changes of the shape, viscosity and spreadibility were found. The optimum  formula  gel obtained by the concentration of CMC-Na 3 %. results were less stable during the 8 weeks of storage at a temperature of 40 °C. </em...|$|R
40|$|Fault {{tolerance}} for cluster operating systems (OSs) {{can be achieved}} by relying on redundant hardware modules to compensate for malfunctioning devices. A well-known software approach is to store periodically distributed consistent snapshots of single applications or the entire cluster on <b>durable</b> <b>storage.</b> If a critical error occurs, a node is restarted from a former consistent snapshot without losing all its computation work before this checkpoint. These backward error recovery techniques require on the one hand saving a consistent distributed snapshot including application and operating system state information {{and on the other hand}} the ability to restart a cluster from such a snapshot. Checkpointing an application in traditional OSs like Microsoft Windows or Linux is a non-trivial task [1] [2]. Solely saving the user context of the target is not sufficient, as processes depend on additional information stored inside the kernel space, e. g. the process id or file descriptors. When the application is restarted, the entire process context must be restored, which includes the user context as well as the affected kernel space structures. In a distributed cluster system each participating node must also store its relevant local application and system states in a coordinated way to guarantee a cluster-wide consistent snaphot. We have implemented smart restartable snapshots within our cluster operating system named "Plurix". A key feature of Plurix is the distributed transactional memory (DTM) storing applications, the kernel, and device drivers [3]. All nodes execute speculative transactions operating on the DTM. Write operations are bundled within transactions transformin...|$|R
