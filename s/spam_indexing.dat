0|16|Public
40|$|Spammers are {{constantly}} creating sophisticated new weapons {{in their arms}} race with anti-spam technology, the latest of which is image-based spam. The newest image-based spam uses simple image processing technologies to vary the content of individual messages, e. g. by changing foreground colors, backgrounds, font types, or even rotating and adding artifacts to the images. Thus, they pose great challenges to conventional spam filters. In this paper, we propose a system using a probabilistic boosting tree to determine whether an incoming image is a spam or not based on global image features, i. e. color and gradient orientation histograms. The system identifies spam {{without the need for}} OCR and is robust {{in the face of the}} kinds of variation found in current spam images. Evaluation results show the system correctly classifies 90 % of spam images while mislabeling only 0. 86 % of non-spam images as <b>spam.</b> <b>Index</b> Terms — Image spam, probabilistic boosting tree 1...|$|R
40|$|Abstract—This paper {{addresses}} {{the problem of}} rumor source detection with multiple observations, from a statistical {{point of view of}} a spreading over a network, based on the susceptible-infectious model. For tree networks, multiple independent obser-vations can dramatically improve the detection probability. For the case of a single rumor source, we propose a unified inference framework based on the joint rumor centrality, and provide explicit detection performance for degree-regular tree networks. Surprisingly, even with merely two observations, the detection probability at least doubles that of a single observation, and further approaches one, i. e., reliable detection, with increasing degree. This indicates that a richer diversity enhances detectabil-ity. Furthermore, we consider the case of multiple connected sources and investigate the effect of diversity. For general graphs, a detection algorithm using a breadth-first search strategy is also proposed and evaluated. Besides rumor source detection, our results can be used in network forensics to combat recurring epidemic-like information spreading such as online anomaly and fraudulent email <b>spams.</b> <b>Index</b> Terms—Graph networks, inference algorithms, maxi-mum likelihood detection, multiple observations, rumor spread-ing I...|$|R
50|$|Doorway {{pages are}} web pages that are created for spamdexing. This is for <b>spamming</b> the <b>index</b> of {{a search engine}} by {{inserting}} results for particular phrases {{with the purpose of}} sending visitors to a different page. They are also known as bridge pages, portal pages, jump pages, gateway pages, entry pages and by other names. Doorway pages that redirect visitors without their knowledge use some form of cloaking. This usually falls under Black Hat SEO.|$|R
40|$|Abstract—New {{paradigms}} for the Future Internet {{are receiving}} an increased attention {{by the research}} community. The publish/subscribe paradigm {{is one of these}} and of particular interest, as it turns the Internet into an information-centric rather than endpoint-centric place of communication. While significant work has been undertaken to secure publish/subscribe systems, little attention has been given to prevent spam. In this paper we propose a light-weight solution for fighting spam, based on information items ranking. We compare our solution to a usersranking based solution and we show that our solution is more effective in terms of publication <b>spam</b> isolation. <b>Index</b> Terms—Information-Centric Architectures, Publish/Subscribe, Spa...|$|R
5000|$|On the World Wide Web, a {{link farm}} is {{any group of}} web sites that all {{hyperlink}} to every other site in the group. In graph theoretic terms, a link farm is a [...] clique. Although some link farms can be created by hand, most are created through automated programs and services. A link farm {{is a form of}} <b>spamming</b> the <b>index</b> of a web search engine (sometimes called spamdexing). Other link exchange systems are designed to allow individual websites to selectively exchange links with other relevant websites and are not considered a form of spamdexing.|$|R
2500|$|Spamdexing (a {{portmanteau}} of <b>spamming</b> and <b>indexing)</b> {{refers to}} a practice on the World Wide Web of modifying HTML pages to increase their chances of high placement on search engine relevancy lists. These sites use [...] "black-hat" [...] search engine optimization techniques to deliberately manipulate their rank in search engines. Many modern search engines modified their search algorithms to try to exclude web pages utilizing spamdexing tactics. For example, the search bots will detect repeated keywords as spamming by using a grammar analysis. If a website owner is found to have spammed the webpage to falsely increase its page rank, the website may be penalized by search engines.|$|R
40|$|Spam {{has long}} been {{identified}} as a problem that web search engines are required to deal with. Large collection sizes are also an increasing issue for institutions that do not have the necessary resources to process them in their entirety. In this paper we investigate the effect that withholding documents identified as spam has on the resources required to process large collections. We also investigate the resulting search effectiveness and efficiency when different amounts of spam are withheld. We find that by removing <b>spam</b> at <b>indexing</b> time we are able to decrease the index size without affecting the indexing throughput, and are able to improve search precision for some thresholds...|$|R
5000|$|Spamdexing (a {{portmanteau}} of <b>spamming</b> and <b>indexing)</b> {{refers to}} a practice on the World Wide Web of modifying HTML pages to increase their chances of high placement on search engine relevancy lists. These sites use [...] "black-hat" [...] search engine optimization techniques to deliberately manipulate their rank in search engines. Many modern search engines modified their search algorithms to try to exclude web pages utilizing spamdexing tactics. For example, the search bots will detect repeated keywords as spamming by using a grammar analysis. If a website owner is found to have spammed the webpage to falsely increase its page rank, the website may be penalized by search engines.|$|R
40|$|Abstract—Spamming is any {{deliberate}} action solely {{in order to}} boost a web page’s position in search engine results, incommensurate with page’s real value. Web Spam is the Web pages that {{are the result of}} spamming. Web spam is the deliberate manipulation of search engine indexes. It is one of the search engine optimization methods. Implementing web spam on a search engine reduces the redundant and non-desirable results. In our paper we discuss the features which are responsible for web page ranking. We also discuss the results of the different classification techniques on our dataset which we process from the WEBSPAM-UK 2006 Dataset. We are also proposing a feature which will help in the web <b>spam</b> detection. <b>Index</b> Terms—web spam; feature selection; classification technique; N gram algorithm; I...|$|R
40|$|Abstract—In {{adversarial}} classification tasks like spam filtering, {{intrusion detection}} in computer networks, and biometric identity verification, malicious adversaries can design attacks which exploit vulnerabilities of {{machine learning algorithms}} to evade detection, or to force a classification system to generate many false alarms, making it useless. Several works have addressed the problem of designing robust classifiers against these threats, although mainly focusing on specific applications and kinds of attacks. In this work, we propose a model of data distribution for adversarial classification tasks, and exploit it to devise a general method for designing robust classifiers, focusing on generative classifiers. Our method is then evaluated on two case studies concerning biometric identity verification and <b>spam</b> filtering. <b>Index</b> Terms—Pattern classification, adversarial classification, robust classifiers. I...|$|R
40|$|Abstract Web spam {{potentially}} causes three deleterious effects: unnecessary {{work for}} crawlers and search engines; {{diversion of traffic}} away from legitimate businesses; and annoyance to search engine users through poorer results. Past research on web spam has focused on spamming techniques, spam suppression techniques, and methods for classifying web content as spam or non-spam. Here {{we focus on the}} deterioration of search result quality caused by the presence of spam in a countryscale web. We present a framework for measuring the degradation in quality of search results caused by the presence of web <b>spam.</b> We <b>index</b> the 80 million page UK 2006 web spam collection on one machine. We trial the proposed framework in an experiment with the UK 2006 collection and demonstrate that simple removal of spam pages from result sets can increase result quality. We conclude that the framework is a reasonable vehicle for research in this area and outline changes necessary for planned future experiments...|$|R
50|$|The {{purpose of}} a splog can be to {{increase}} the PageRank or backlink portfolio of affiliate websites, to artificially inflate paid ad impressions from visitors (see made for AdSense or MFA-blogs), and/or use the blog as a link outlet to sell links or get new sites <b>indexed.</b> <b>Spam</b> blogs are usually a type of scraper site, where content is often either inauthentic text or merely stolen (see blog scraping) from other websites. These blogs usually contain {{a high number of}} links to sites associated with the splog creator which are often disreputable or otherwise useless websites.|$|R
50|$|BTDigg was {{the first}} BitTorrent DHT search engine. It participated in the BitTorrent DHT network, {{supporting}} the network and making correspondence between magnet links and a few torrent attributes (name, size, list of files) which are indexed and inserted into a database. For end users, BTDigg provides a full text database search via Web interface. The web part of its search system retrieved proper information by a user's text query. The Web search supported queries in European and Asian languages. The project name was an acronym of BitTorrent Digger (in this context digger means a treasure-hunter). It went offline in June 2016, reportedly due to <b>index</b> <b>spam.</b> The site returned later in 2016 at a dot-com domain, but has since shut down again.|$|R
40|$|The {{presence}} of spam in a document ranking {{is a major}} issue for Web search engines. Common approaches that cope with spam remove from the document rankings those pages {{that are likely to}} contain spam. These approaches are implemented as post-retrieval processes, that filter out spam pages only after documents have been retrieved with respect to a user’s query. In this paper we suggest to remove <b>spam</b> pages at <b>indexing</b> time, therefore obtaining a pruned index that is virtually “spam-free”. We investigate the benefits of this approach from three points of view: indexing time, index size, and retrieval performances. Not surprisingly, we found that the strategy decreases both the time required by the indexing process and the space required for storing the index. Surprisingly instead, we found that by considering a spam-pruned version of a collection’s index, no difference in retrieval performance is found when compared to that obtained by traditional post-retrieval spam filtering approaches...|$|R
40|$|In {{this age}} of {{information}} overload, one experiences a rapidly growing over-abundance of written text. To assist with handling this bounty, this plethora of texts is now widely used to develop and optimize statistical natural language processing (NLP) systems. Surprisingly, the use of more fragments of text to train these statistical NLP systems may not necessarily lead to improved performance. We hypothesize that those fragments that help the most with training are those that contain the desired information. Therefore, determining informativeness in text has become a central issue in our view of NLP. Recent developments in this field have spawned a number of solutions to identify informativeness in text. Nevertheless, a shortfall of most of these solutions is their dependency on the genre and domain of the text. In addition, {{most of them are}} not efficient regardless of the natural language processing problem areas. Therefore, we attempt to provide a more general solution to this NLP problem. This thesis takes a different approach to this problem by considering the underlying theme of a linguistic theory known as the Code Quantity Principle. This theory suggests that humans codify information in text so that readers can retrieve this information more efficiently. During the codification process, humans usually change elements of their writing ranging from characters to sentences. Examples of such elements are the use of simple words, complex words, function words, content words, syllables, and so on. This theory suggests that these elements have reasonable discriminating strength and can {{play a key role in}} distinguishing informativeness in natural language text. In another vein, Stylometry is a modern method to analyze literary style and deals largely with the aforementioned elements of writing. With this as background, we model text using a set of stylometric attributes to characterize variations in writing style present in it. We explore their effectiveness to determine informativeness in text. To the best of our knowledge, this is the first use of stylometric attributes to determine informativeness in statistical NLP. In doing so, we use texts of different genres, viz., scientific papers, technical reports, emails and newspaper articles, that are selected from assorted domains like agriculture, physics, and biomedical science. The variety of NLP systems that have benefitted from incorporating these stylometric attributes somewhere in their computational realm dealing with this set of multifarious texts suggests that these attributes can be regarded as an effective solution to identify informativeness in text. In addition to the variety of text genres and domains, the potential of stylometric attributes is also explored in some NLP application areas [...] -including biomedical relation mining, automatic keyphrase <b>indexing,</b> <b>spam</b> classification, and text summarization [...] -where performance improvement is both important and challenging. The success of the attributes in all these areas further highlights their usefulness...|$|R
40|$|THESIS Submitted in {{fulfilment}} of {{the requirements}} for the degree MAGISTER TECHNOLOGIAE in INFORMATION TECHNOLOGY in the FACULTY OF BUSINESS INFORMATICS at the CAPE PENINSULA UNIVERSITY OF TECHNOLOGY 2005 The aim {{of this research was}} to determine how search engine exclusion policies and <b>spam</b> affect the <b>indexing</b> of e-Commerce websites. The Internet has brought along new ways of doing business. The unexpected growth of the World Wide Web made it essential for firms to adopt e-commerce as a means of obtaining a competitive edge. The introduction of e-commerce in turn facilitated the breaking down of physical barriers that were evident in traditional business operations. It is important for e-commerce websites to attract visitors, otherwise the website content is irrelevant. Websites can be accessed through the use of search engines, and it is estimated that 88 % of users start with search engines when completing tasks on the web. This has resulted in web designers aiming to have their websites appear in the top ten search engine result list, as a high placement of websites in search engines is one of the strongest contributors to a commercial website’s success. To achieve such high rankings, web designers often adopt Search Engine Optimization (SEO) practices. Some of these practices invariably culminate in undeserving websites achieving top rankings. It is not clear how these SEO practices are viewed by search engines, as some practices that are deemed unacceptable by certain search engines are accepted by others. Furthermore, there are no clear standards for assessing what is considered good or bad SEO practices. This confuses web designers in determining what is spam, resulting in the amount of search engine spam having increased over time, impacting adversely on search engine results. From the literature reviewed in this thesis, as well as the policies of five top search engines (Google, Yahoo!, AskJeeves, AltaVista, and Ananzi), this author was able to compile a list of what is generally considered as spam. Furthermore, 47 e-commerce websites were analysed to determine if they contain any form of spam. The five major search engines indexed some of these websites. This enabled the author to determine to what extent search engines adhere to their policies. This analysis returned two major findings. A small amount of websites contained spam, and from the pre-compiled list of spam tactics, only two were identified in the websites, namely keyword stuffing and page redirects. Of the total number of websites analysed, it was found that 21. 3 % of the websites contained spam. From these findings, the research contained in this thesis concluded that search engines adhere to their own policies, but lack stringent controls for the majority of websites that contained spam, and were still listed by search engines. In this study, the author only analysed e-commerce websites, and cannot therefore generalise the results to other websites outside ecommerce...|$|R

