407|2237|Public
5000|$|To see why {{the initial}} {{observation}} assumption stated by Prais-Winsten (1954) is reasonable, considering the mechanics of generalized least <b>square</b> <b>estimation</b> procedure sketched above is helpful. The inverse of [...] can be decomposed as [...] with ...|$|E
50|$|Athanasios Papoulis {{specialized}} in engineering mathematics, his work covers probability, statistics, and estimation {{in the application}} of these fields to modern engineering problems. Papoulis also taught and developed subjects such as stochastic simulation, mean <b>square</b> <b>estimation,</b> likelihood tests, maximum entropy methods, Monte Carlo method, spectral representations and estimation, sampling theory, bispectrum and system identification, cyclostationary processes, deterministic signals in noise (part of deterministic systems and dynamical system studies), wave optics and the Wiener and Kalman filters.|$|E
50|$|Quantitative {{research}} mainly {{deals with}} application of bi-variate and multivariate statistics to marketing research. IBM SPSS is mostly widely used tool for data analysis though some companies {{would prefer to}} stay with excel and WinCross. Industrial application of statistics is limited to cross tabulations using Chi <b>square</b> <b>estimation,</b> but some companies do use linear and logistic regression analysis. From last few years Conjoint analysis is one the new techniques that has become popular for its application on product bundling and pricing.|$|E
40|$|Abstract:Through {{theoretical}} derivation, some {{properties of}} the total least <b>squares</b> <b>estimation</b> are found. The total least <b>squares</b> <b>estimation</b> is the linear transformation of the least <b>squares</b> <b>estimation,</b> and the total least <b>squares</b> <b>estimation</b> is unbiased. The condition number of the total least <b>squares</b> <b>estimation</b> {{is greater than the}} least <b>squares</b> <b>estimation,</b> so the total least <b>squares</b> <b>estimation</b> is easier to be affected by the data error than the least <b>squares</b> <b>estimation.</b> Then through the further derivation, the relationships of solutions, residuals and unit weight variance estimations between the total least squares and the least squares are given...|$|R
3000|$|Block Least <b>Squares</b> <b>Estimation</b> (Block-LSE): least <b>squares</b> <b>estimation</b> {{based on}} present but not past pilot data, also often called 2 D-Wiener {{filtering}} [29, 30]; [...]...|$|R
40|$|Conference PaperVolterra filters {{have been}} applied to many {{nonlinear}} system identification problems. However, obtaining good filter estimates from short and/or noisy data records is a difficult task. We propose a penalized least <b>squares</b> <b>estimation</b> algorithm and derive appropriate penalizing functionals for Volterra filters. An example demonstrates that penalized least <b>squares</b> <b>estimation</b> can provide much more accurate filter estimates than ordinary least <b>squares</b> <b>estimation...</b>|$|R
5000|$|Penalization {{has several}} effects on inference, {{relative}} to a regular GLM. For one thing the estimates are subject to some smoothing bias, which is the price that must be paid for limiting estimator variance by penalization. However if smoothing parameters are selected appropriately the (squared) smoothing bias introduced by penalization should be less than the reduction in variance that it produces, so that the net effect is a reduction in mean <b>square</b> <b>estimation</b> error, relative to not penalizing. A related effect of penalization is {{that the notion of}} degrees of freedom of a model has to be modified to account for the penalties' action in reducing the coefficients' freedom to vary. For example if [...] is the diagonal matrix of IRLS weights at convergence, and [...] is the GAM model matrix, then the model effective degrees of freedom is given by [...] where ...|$|E
40|$|This paper made a discuss on the {{relative}} efficiency of the generalized conditional root <b>square</b> <b>estimation</b> and the specific conditional root <b>square</b> <b>estimation</b> in paper [1, 2] in inhomogeneous equality restricted linear model. It is shown that the generalized conditional root squares estimation has not smaller {{the relative}} efficiency than the specific conditional root <b>square</b> <b>estimation,</b> by a constraint condition in root squares parameter, we compare bounds of them, thus, choose appropriate squares parameter, the generalized conditional root <b>square</b> <b>estimation</b> has the good performance on mean squares error...|$|E
40|$|We {{study the}} lowest {{achievable}} mean <b>square</b> <b>estimation</b> error in two limiting optimal linear filtering problems. First, when {{the intensity of}} the process noise tends to zero, the lowest achievable mean <b>square</b> <b>estimation</b> error {{is a function of the}} unstable poles of the system. Second, when {{the intensity of the}} measurement noise tends to zero, the lowest achievable mean <b>square</b> <b>estimation</b> error is a function of the nonminimum phase zeros of the system. We link these results with Bode integral characterizations of performance limitations in linear filtering...|$|E
40|$|A {{function}} based nonlinear least <b>squares</b> <b>estimation</b> (FNLSE) {{method is}} proposed and investigated in parameter estimation of Jelinski-Moranda software reliability model. FNLSE extends the potential fitting functions of traditional least <b>squares</b> <b>estimation</b> (LSE), {{and takes the}} logarithm transformed nonlinear least <b>squares</b> <b>estimation</b> (LogLSE) as a special case. A novel power transformation function based nonlinear least <b>squares</b> <b>estimation</b> (powLSE) is proposed and applied to the parameter estimation of Jelinski-Moranda model. Solved with Newton-Raphson method, Both LogLSE and powLSE of Jelinski-Moranda models are applied to the {{mean time between failures}} (MTBF) predications on six standard software failure time data sets. The experimental results demonstrate the effectiveness of powLSE with optimal power index compared to the classical least [...] <b>squares</b> <b>estimation</b> (LSE), maximum likelihood estimation (MLE) and LogLSE in terms of recursively relative error (RE) index and Braun statistic index...|$|R
50|$|Then {{the usual}} least <b>squares</b> <b>estimation</b> is done.|$|R
40|$|Contract {{generated}} publications are compiled which {{describe the}} research {{activities for the}} reporting period. Study topics include: equivalent configurations of systolic arrays; least <b>squares</b> <b>estimation</b> algorithms with systolic array architectures; modeling and equilization of nonlinear bandlimited satellite channels; and least <b>squares</b> <b>estimation</b> and Kalman filtering by systolic arrays...|$|R
40|$|Abstract. TDOA is a {{predominant}} localization algorithm. In this paper, {{we propose}} a TDOA localization algorithm called NLLS, using the nonlinear least <b>square</b> <b>estimation.</b> We first get the initial location utilizing the LCLS algorithm, which could achieve the global optimal {{solution to the}} ordinary constraint linear least <b>square</b> <b>estimation,</b> {{and based on the}} initial location, we make use of nonlinear least <b>square</b> <b>estimation</b> to improve the localization precision. Simulation results show that compared with the CTLS algorithm, its performance is superior especially when the measurement noise is larger...|$|E
40|$|Power {{estimation}} is {{an important}} issue in digital VLSI circuit design. The estimation of average power dissipation of a circuit through exhaustive simulation is impractical due to the large number of primary inputs and their combinations. In this paper, two algorithms based on least <b>square</b> <b>estimation</b> are proposed for determining the average power dissipation in CMOS circuits. Least <b>square</b> <b>estimation</b> converges faster by attempting to minimize the mean square error value during each iteration. Two approaches namely, the sequential least <b>square</b> <b>estimation</b> and the recursive least <b>square</b> <b>estimation,</b> are investigated. The proposed methods are distribution independent in terms of the input samples, unbiased and point estimation based. Experimental results for the MCNC ' 91 and the ISCAS ' 89 benchmark circuits are presented. While the sequential least square algorithm performs comparable with the Monte-Carlo method, the recursive least square method converges up to 12 times faster than the Monte-Carlo technique...|$|E
40|$|Acoustic {{array sensor}} along with Root-MUSIC {{algorithm}} {{is used to}} estimate the direction 13; of arrival of the acoustic signal emitted by an acoustic target. Three architectures are used to 13; track the target in Cartesian coordinates: (i) digital filter with least <b>square</b> <b>estimation,</b> (ii) linear 13; Kalman filter with least <b>square</b> <b>estimation,</b> and (iii) extended Kalman filter. A comparative evaluation 13; of the three architectures in terms of performance metrics is presented...|$|E
5000|$|... #Subtitle level 2: Implications for {{ordinary}} least <b>squares</b> <b>estimation</b> ...|$|R
5000|$|The vector of {{estimated}} polynomial {{regression coefficients}} (using ordinary least <b>squares</b> <b>estimation)</b> is ...|$|R
30|$|A note on three-stage least <b>squares</b> <b>estimation,</b> Journal of Econometrics, 4 (4), 325 â€“ 330, 1976.|$|R
40|$|Abstract. The sensor {{needs to}} {{maneuver}} {{to get better}} observability in Bearings-Only passive target tracking with single sensor which makes the observation time longer. Multisensor Bearings-Only passive target tracking can solve the problem using exchange data. So the constrained Least Square Estimation(CLSE) algorithm is proposed for Multisensor Bearings-Only passive target tracking. The constrained condition is introduced to the Least <b>Square</b> <b>Estimation</b> algorithm firstly. Then the eigenvector corresponding to the least eigenvalue of the matrix is used to overcome the shortcoming of Extend Kalman Filter algorithm which needs the initial value. Also the bias problem of Least <b>Square</b> <b>Estimation</b> is conquered. The simulation {{results show that the}} CLSE can gradually approach the Cramer-Rao Lower Bound and its precision is better than the Least <b>Square</b> <b>Estimation</b> algorithm. Finally the CLSE is proved to be a gradually, stable and almost unbiased estimation algorithm...|$|E
3000|$|... â€²-axis {{orthogonal}} to the plane. The least <b>square</b> <b>estimation</b> {{of the circle}} enables us to get a first estimate of the sphere radius, R [...]...|$|E
40|$|We {{study the}} lowest {{achievable}} mean <b>square</b> <b>estimation</b> error in two limiting optimal linear filtering problems. First, when {{the intensity of}} the process noise tends to zero, the lowest achievable mean <b>square</b> <b>estimation</b> error {{is a function of the}} unstable poles of the system. Second, when {{the intensity of the}} measurement noise tends to zero, the lowest achievable mean <b>square</b> <b>estimation</b> error is a function of the nonminimum phase zeros of the system. We link these results with Bode integral characterizations of performance limitations in linear filtering. Keywords: Optimal Linear Filtering, Performance Limits, Nonminimum Phase Zeros, Unstable Poles, Bode Integrals, Singular Perturbations. 1 Introduction 1 1 Introduction The analysis of feedback limitations using analytic function theory was initiated by Bode in the 1940 's and is of continuing interest (Bode 1945, Francis & Zames 1984, Freudenberg & Looze 1985, Middleton 1991, Chen 1995). In recent studies of performance limitations in [...] ...|$|E
40|$|Vertical {{deflection}} estimation independent linear least <b>squares</b> <b>estimation</b> 20. ABSTRACT (Cenvtllu do revers sh If newovmy end Identify by block number) The paper addresses {{vertical deflection}} estimation methods developed by Jordan a d by White and Goldstein, presents {{the development of}} two independent * linear least <b>squares</b> <b>estimation</b> methods with one having opitnal characteristics as to accuracy, economy, and versatility, and arrives at significant conclusion...|$|R
3000|$|To {{estimate}} the regression coefficients and constant term, we conducted an ordinary least <b>squares</b> <b>estimation,</b> {{as well as}} a fixed-effects estimation considering the unique characteristics of each country and a random-effects estimation. The ordinary least <b>squares</b> <b>estimation</b> method assumes that all countries in the analysis have the same slope and constant term, and the method does not depend on the unique characteristic of each country. On the other hand, the fixed-effects estimation considering the unique characteristics of each country assumes the same slope for all countries, but with different constant terms for each. The random-effects estimation assumes that the unique characteristics are probability variables rather than constant terms. As a result, in the ordinary least <b>squares</b> <b>estimation,</b> c [...]...|$|R
40|$|In {{a linear}} model Y = X[beta] + Z a linear {{functional}} [beta] [...] > [gamma]'[beta] {{is to be}} estimated under squared error loss. It is well known that, provided Y is normally distributed, the ordinary least <b>squares</b> <b>estimation</b> function minimizes the risk uniformly {{in the class of}} all equivariant estimation functions and is admissible in the class of all unbiased estimation functions. For the design matrix X of a polynomial regression set up it is shown for almost all estimation problems that the ordinary least <b>squares</b> <b>estimation</b> function is uniformly best in and also admissible in only if Y is normally distributed. normal distribution polynomial regression least <b>squares</b> <b>estimation</b> function equivariant estimation functions admissible estimation functions...|$|R
40|$|In {{this paper}} {{we present a}} robust and discriminative {{segmental}} trajectory modeling for vowel recognition. We proposed two new approaches. One is using weighted least <b>square</b> <b>estimation</b> for the parametric trajectory parameter, which gives a much more robust performance over traditional least <b>square</b> <b>estimation</b> approach. The other is a specifically designed transformation matrix proposed to reduce the possible mismatch between the Gaussian modeling assumption and the trajectory featureâ€™s nature. Our experiments on the vowel classification using the mobile phone data of SpeechDAT(II) MDB showed significant improvement over both standard HMM and traditional segmental modeling. 1...|$|E
40|$|Experimental {{design and}} Taguchi's {{parameter}} design are widely employed by industry {{to optimize the}} process/product. However, censored data are often observed in product lifetime testing during the experiments. After implementing a repetitious experiment with type II censored data, the censored data are usually estimated by establishing a complex statistical model. However, using the incomplete data to fit a model may not accurately estimates the censored data. Moreover, the model fitting process is complicated for a practitioner who has only limited statistical training. This study proposes a less complex approach to analyze censored data, using the least <b>square</b> <b>estimation</b> method and Torres's analysis of unreplicated factorials with possible abnormalities. This study also presents an effective method to analyze the censored data from Taguchi's parameter design using least <b>square</b> <b>estimation</b> method. Finally, examples are given to illustrate {{the effectiveness of the}} proposed methods. Type II censored data, least <b>square</b> <b>estimation,</b> Torres's method, experimental design, Taguchi's parameter design,...|$|E
40|$|For spectrally {{efficient}} transmission over time-varying channels, {{the use of}} Adaptive Coding and Modulation (AMC) in wireless OFDM systems {{requires the}} estimation of radio channel at the receiver. This paper focuses {{on the use of}} time domain channel statistics, mainly concentrating on two schemes: Linear Minimum Mean <b>Square</b> <b>Estimation</b> (LMMSE) and Least <b>Square</b> <b>Estimation</b> (LSE) and their variants. LMMSE performs better than LSE but at the cost of computational complexity. The performance of LSE can be improved by increasing CIR samples and channel taps. To avoid the matrix inversion lemma, the channel matrix can be downsampled or regularized. Theoretical analysis and computer simulations are used for performance and complexity comparisons...|$|E
30|$|In this subsection, we go {{over the}} {{preliminaries}} of centralized and distributed weighted non-linear least <b>squares</b> <b>estimation.</b>|$|R
5000|$|One {{can then}} solve for the {{coefficient}} matrix B (e.g. using an ordinary least <b>squares</b> <b>estimation</b> of [...] ).|$|R
40|$|We {{consider}} a cointegrated system generated by processes {{that may be}} fractionally integrated, and by additive polynomial and generalized polynomial trends. In view of the consequent competition between stochastic and deterministic trends, we consider various estimates of the cointegrating vector and develop relevant asymptotic theory, including the situation where fractional orders of integration are unknown. Fractional cointegration, deterministic trends, ordinary least <b>squares</b> <b>estimation,</b> generalized least <b>squares</b> <b>estimation,</b> Wald tests. ...|$|R
30|$|The above fuzzy model {{obtained}} by grid partitioning was trained by updating the consequent {{parameters of the}} rules by least <b>square</b> <b>estimation</b> algorithm and the premise parameters by the backpropagation gradient descent algorithm. This {{is also known as}} the hybrid learning algorithm.|$|E
40|$|Block {{designs for}} {{observations}} correlated in one dimension are investigated. Nearest neighbour least balanced designs turnout to be optimal for auto-regressive models when generalized least <b>square</b> <b>estimation</b> is used. Performance of these designs for moving average and ARMA correlation model {{is shown to}} be quite satisfactory...|$|E
30|$|The {{convergence}} {{properties of}} the different receiver combinations are presented, {{both in terms of}} BER, mean <b>square</b> <b>estimation</b> error, and through the use of extrinsic information transfer (EXIT) charts [16]. The EXIT charts visualize the exchange of extrinsic information between the outer code {{and the rest of the}} receiver incorporating channel estimation and MUD.|$|E
3000|$|The {{solution}} of equation (3.2) is termed the fuzzy least <b>squares</b> <b>estimation</b> (FLSE) of Î± and denoted by [...]...|$|R
5000|$|An example {{which is}} only {{slightly}} less simple is that of least <b>squares</b> <b>estimation</b> of a and b in the model ...|$|R
50|$|In {{this case}} the Fisher {{information}} matrix may be identified with the coefficient matrix of the normal equations of least <b>squares</b> <b>estimation</b> theory.|$|R
