0|10000|Public
40|$|AbstractThis paper {{introduces}} a pedagogical {{proposal for the}} development of writing skills based on the analysis of multi-genre structures (AMS). The objective of this AMS model is to help teachers design genres that may be used in the language classroom. This genre analysis can be performed with <b>software</b> tools <b>for</b> <b>corpus</b> <b>analysis.</b> This study is based on a previous model of genre analysis designed to determine the forms of the macrostructure, microstructure, and the format of the target genre (Casañ-Pitarch, 2015) and focuses on ESP students at tertiary education. The AMS model is expected to help students improve their writing skills and gain specific knowledge on professional and academic genres...|$|R
40|$|This paper {{describes}} the design, analysis and utilization of a simultaneous interpretation corpus. The corpus has been con-structed at the Center for Integrated Acoustic Information Re-search (CIAIR) of Nagoya University {{in order to}} promote the realization of the multi-lingual communication supporting envi-ronment. The size of transcribed data is about 1 million words, and the corpus would deserve to be called the simultaneous in-terpretation corpus of the largest-in-the-world class. The dis-course tag and the utterance time tag were given to the corpus, and some <b>software</b> tools <b>for</b> <b>corpus</b> <b>analysis</b> in order to support the practical use of the corpus have been developed. Therefore, the corpus is expected to be useful not only for the development of simultaneous interpreting systems but also for the construc-tion of an interpreting theory. 1...|$|R
40|$|As {{the number}} of Arabic corpora is {{constantly}} increasing, there is an obvious and growing need <b>for</b> concordancing <b>software</b> <b>for</b> <b>corpus</b> search and <b>analysis</b> that supports as many features as possible of the Arabic language, and provides users with {{a greater number of}} functions. This paper evaluates six existing <b>corpus</b> search and <b>analysis</b> tools based on eight criteria which seem to be the most essential for searching and analysing Arabic corpora, such as displaying Arabic text in its right-to-left direction, normalising diacritics and Hamza, and providing an Arabic user interface. The results of the evaluation revealed that three tools: Khawas, Sketch Engine, and aConCorde, have met most of the evaluation criteria and achieved the highest benchmark scores. The paper concluded that developers’ conscious consideration of the linguistic features of Arabic when designing these three tools was the most significant factor behind their superiority...|$|R
40|$|Despite {{the central}} role of the {{computer}} in corpus research, programming is generally not seen as a core skill within corpus linguistics. As a consequence, limitations in <b>software</b> <b>for</b> text and <b>corpus</b> <b>analysis</b> slow down the progress of research while analysts often have to rely on third party software or even manual data analysis if no suitable software is available. Apart from software itself, data formats are also of great importance for text processing. But again, many practitioners are not very aware of the options available to them, and thus idiosyncratic text formats often make sharing of resources difficult if not impossible. &# 13; This article discusses some issues relating to both data and processing which should aid researchers to become more aware of the choices available to them when it comes to using computers in linguistic research. It also describes an easy way towards automating some common text processing tasks that can easily be acquired without knowledge of actual computer programming...|$|R
40|$|Development of task-oriented and problem-based {{learning}} and teaching; use of global simulations; use of case studies in language teaching; development of language level descriptors {{and methods of}} language testing, use of ICT <b>for</b> <b>corpus</b> <b>analysis</b> and concordancing {{and the use of}} ICT for language testing...|$|R
50|$|A corpus manager (corpus browser or corpus query system) {{is a tool}} <b>for</b> {{multilingual}} <b>corpus</b> <b>analysis,</b> {{which allows}} effective searching in corpora.|$|R
40|$|This article aims to {{introduce}} new resources and methods used in Estonian corpus lexicography to create monolingual Estonian dictionaries. Corpora {{can be used}} in many ways: headwords list development, grammatical and frequency labels, word sense division, identifying collocations, good dictionary examples, translation equivalents (Kilgarriff 2013). The paper focuses on features offered by Sketch Engine (Kilgarriff et al. 2004), a state-of-the-art lexicographic tool <b>for</b> <b>corpus</b> <b>analysis.</b> <b>For</b> Estonian, Sketch Engine contains different types of corpora, including the recently created 260 million-word web corpus etTenTen 13 and the 463 million-word Esto- nian National Corpus...|$|R
40|$|The chapter {{analyzes}} {{the contribution of}} CL {{to the study of}} dialogue and language as dialogue, with examples from English linguistics and academic discourse studies. CL has extended the possibilities of dialogue analysis and added to its theoretical-methodological perspectives. The chapter provides some background on CL, with special emphasis on spoken <b>corpora</b> and tools <b>for</b> <b>corpus</b> <b>analysis.</b> It then focuses on the complexity of language in use, the interactional features of language in speech and writing and the importance of CL for studies on language variation. The conclusions also mention the impact of CL in teaching language in a dialogic perspective...|$|R
40|$|Log-likelihood and Chi-square {{tests are}} {{probably}} the most popular statistical tests used in corpus linguistics, especially when the research is aiming to describe the lexical variations between corpora. However, because this specific use of the Chi-square test is not valid, it produces far too many significant results. This paper explains the source of the problem (i. e., the non-independence of the observations), the reasons for which the usual solutions are not acceptable and which kinds of statistical test should be used instead. A <b>corpus</b> <b>analysis</b> conducted on the lexical differences between American and British English is then reported, in order to demonstrate the problem and to confirm the adequacy of the proposed solution. The last section presents the commands that can be used with WordSmith Tools, a very popular <b>software</b> <b>for</b> <b>corpus</b> processing, to obtain the necessary data for the adequate tests, as well as a very easy-to-use procedure in R, a free and easy to install statistical software, that performs these tests...|$|R
40|$|CLaRK is an XML-based <b>software</b> system <b>for</b> <b>corpora</b> development. It {{incorporates}} several technologies: XML technology; Un i code; Regular Cascaded Grammars; Constraints over XML Documents. The basic {{components of}} the system are: a tagger, a concordancer, an extractor, a grammar processor, a constraint engine. ...|$|R
40|$|The article {{investigates the}} metaphors that three {{financial}} newspapers employ {{to describe the}} current Euro crisis, namely: The Economist in English, Der Spiegel in German, e the Italian Il Sole 24 ORE. With {{the help of the}} computer program <b>for</b> <b>corpus</b> <b>analysis</b> Wordsmith Tools 6. 0 the most frequently recurring conceptual domains and the metaphors that realise them are analysed; a contrastive approach describes how the newspapers describe to their readers the so-called Euro crisis. The results will be contrasted with those already analysed in Cesiri & Colaci (2011) with the aim of understanding whether the metaphors investigated are those already in use to present the global crisis or whether new ones are created purposefully to describe the new aspect of the crisis in Europe...|$|R
50|$|The corpus {{is marked}} up {{following}} {{the recommendations of}} the Text Encoding Initiative (TEI) and includes full linguistic annotation and contextual information. The licence for the CLAWS4 part-of-speech tagger may be purchased to use the tagger. Alternatively, a tagging service is offered at Lancaster University. The BNC itself may be ordered with either a personal or institutional license. The edition available is the BNC XML edition and it comes with the Xaira search engine software. Ordering may be carried out via the BNC website. An online corpus manager, BNCweb, has been developed for the BNC XML edition. The interface is designed to be easy to use, and the program offers query features and functions <b>for</b> <b>corpus</b> <b>analysis.</b> Users can retrieve results and data from searches and analyses.|$|R
40|$|This paper {{describes}} {{a set of}} computer programs <b>for</b> Chinese <b>corpus</b> <b>analysis.</b> These programs include (1) extraction of diflrent characters, bigrams and words; (2) word segmentation based on bigrain, maximal-matching and the combined teclmique; (3) identification of special terms; (4) Chinese concordancing; (5) compiling collocatiou statistics and (6) evaluation utilities. These programs run on the IBMPC and batch programs co-ordinate {{the use of these}} programs...|$|R
40|$|This paper {{focuses on}} three axes. The first axis gives {{a survey of}} the {{importance}} of corpora in language studies e. g. lexicography, grammar, semantics, Natural Language Processing and other areas. The second axis demonstrates how the Arabic language lacks textual resources, such as <b>corpora</b> and tools <b>for</b> <b>corpus</b> <b>analysis</b> and the effected of this lack on the quality of Arabic language applications. There are rarely successful trials in compiling Arabic corpora, therefore, the third axis presents the technical design of the International Corpus of Arabic (ICA), a newly established representative corpus of Arabic that is intended to cover the Arabic language as being used all over the Arab world. The corpus is planned to support various Arabic studies that depends on authentic data, in addition to building Arabic Natural Language Processing Applications. ...|$|R
40|$|A lemmatiser-tagger {{must not}} only lemmatise word-forms {{consisting}} of a single lexical element but {{it must also be}} able to detect complex units. In this paper we try to delimit linguistically which complex words deserve to be lemmatised as a unit. Then, we propose a formal description for MultiWord Lexical Units (MWLU) in Basque [...] -resulting of a conscientious analysis of their syntactic and morphological behaviour. Based on that formal description we propose a simple logical formalism to represent those MWLUs {{so that they can be}} automatically processed. 1. Introduction A lemmatiser-tagger is a computational tool used for assigning the correct lemma and grammatical category to each token of a corpus. It is a basic device <b>for</b> <b>corpus</b> <b>analysis,</b> automatic indexation, syntactic and semantic analyses etc. For example, the lemmatiser-tagger for Basque (EUSLEM) (Aduriz et al., 96 a) is essential for the second phase of the Systematic Compilation of Modern Basque 1 (EEBS) project (Urkia et a [...] ...|$|R
40|$|The {{contents}} of English-language online-news over 5 {{years have been}} analyzed to explore {{the impact of the}} Fukushima disaster on the media coverage of nuclear power. This big data study, based on millions of news articles, involves the extraction of narrative networks, association networks, and sentiment time series. The key finding is that media attitude towards nuclear power has significantly changed {{in the wake of the}} Fukushima disaster, in terms of sentiment and in terms of framing, showing a long lasting effect that does not appear to recover before the end of the period covered by this study. In particular, we find that the media discourse has shifted from one of public debate about nuclear power as a viable option for energy supply needs to a re-emergence of the public views of nuclear power and the risks associated with it. The methodology used presents an opportunity to leverage big data <b>for</b> <b>corpus</b> <b>analysis</b> and opens up new possibilities in social scientific research. Peer-reviewedPost-printIEE...|$|R
40|$|Recently, many Natural Language Processing (NLP) {{applications}} {{have improved}} {{the quality of}} their output by using various machine learning techniques to mine Information Extraction (IE) patterns for capturing information from the input text. Currently, to mine IE patterns one should know in advance the type of the information that should be captured by these patterns. In this work we propose a novel methodology <b>for</b> <b>corpus</b> <b>analysis</b> based on cross-examination of several document collections representing different instances of the same domain. We show that this methodology can be used for automatic domain template creation. As the problem of automatic domain template creation is rather new, there is no well-defined procedure for the evaluation of the domain template quality. Thus, we propose a methodology for identifying what information should be present in the template. Using this information we evaluate the automatically created domain templates through the text snippets retrieved according to the created templates. ...|$|R
40|$|International audienceThis {{research}} {{describes a}} protocol <b>for</b> specialized <b>corpus</b> <b>analysis</b> using {{natural language processing}} (NLP) tools to define semantic hierarchies of verbs in the specialized domain of Volcanology. The experimental analysis was carried out with a domain-specific corpus in English of approximately 500, 000 tokens composed of dissertations and scientific articles {{in the domain of}} Volcanology. The combination of semantic and syntactic analysis results in the verb macro structure that illustrates the evolution of the meaning from more general to more specific verbs...|$|R
40|$|Abstract. Text {{categorization}} {{technology can}} be used to streamline the process of content <b>analysis</b> of <b>corpus</b> data. However, while recent results <b>for</b> automatic <b>corpus</b> <b>analysis</b> show great promise, tools that are currently being used for HCI research and practice do not make use of it. Here, we empirically evaluate trade-offs between semi automatic and hand labeling of data in terms of speed, validity, and reliability of coding in order to assess the usefulness of incorporating this technology into HCI tools...|$|R
40|$|This paper {{presents}} a method <b>for</b> large <b>corpus</b> <b>analysis</b> to semantically classify an entire clause. In particular, we use cooccurrence statistics among similar clauses {{to determine the}} aspectual class of an input clause. The process examines linguistic features of clauses {{that are relevant to}} aspectual classification. A genetic algorithm determines what combinations of linguistic features to use for this task. Comment: postscript, 9 pages, Proceedings of the Second International Conference on New Methods in Language Processing, Oflazer and Somers ed...|$|R
40|$|In recent years, great {{progress}} has been made in Chinese corpus processing. A fifty-million-word Chinese National Corpus project has been put into effect, and many automatic corpus processing programs have also been developed. In this paper, we will briefly introduce our work on constructing a large scale annotated <b>corpus</b> <b>for</b> Chinese grammatical research and developing a Chinese Corpus Multilevel Processing system [...] CCMP. First, we give our annotation scheme. Then, we discuss some basic methodologies <b>for</b> Chinese <b>corpus</b> <b>analysis,</b> and propose a man-machine mutually dependent corpus processing model. Finally, we introduce the survey of our CCMP. We hope our work will give impetus to the further research in Chinese corpus linguistics...|$|R
40|$|Many {{translation}} {{scholars have}} proposed {{the use of}} corpora to allow professional translators to produce high quality texts which read like originals. Yet, the diffusion of this methodology has been modest, one reason being the fact that <b>software</b> <b>for</b> <b>corpora</b> analyses have been developed with the linguist in mind, {{which means that they}} are generally complex and cumbersome, offering many advanced features, but lacking the level of usability and the specific features that meet translators’ needs. To overcome this shortcoming, we have developed TranslatorBank, a free <b>corpus</b> creation and <b>analysis</b> tool designed for translation tasks. TranslatorBank supports the creation of specialized monolingual corpora from the web; it includes a concordancer with a query system similar to a search engine; it uses basic statistical measures to indicate the reliability of results; it accesses the original documents directly for more contextual information; it includes a statistical and linguistic terminology extraction utility to extract the relevant terminology of the domain and the typical collocations of a given term. Designed to be easy and intuitive to use, the tool may help translation students as well as professionals to increase their translation quality by adhering to the specific linguistic variety of the target text corpus. ...|$|R
40|$|With {{the aim of}} {{establishing}} a guideline for how to teach successful job interview communication in a multi-cultural Business as a Lingua Franca (BELF) setting, this thesis examines authentic job interview communications in the world maritime industry, compares overall features of successful and unsuccessful communications, and discusses pedagogical implications for ESP language teaching. For this purpose, authentic job interview communications conducted in four different countries between non-native speakers of English (both English as a Second Language and English as a Foreign Language speaker) including India, the Philippines, Sri Lanka, and Vietnam were collected. The data from 40 job interviews in total was transcribed <b>for</b> <b>corpus</b> <b>analysis,</b> and finally a Corpus of ELF Job Interviews in a Multicultural Business World (hereinafter CELF-JOIN) has been compiled for this research. Based on the analysis, {{a wide range of}} BELF job interview features were investigated in terms of contextual and schematic structures, interactional pragmatic features and lexico-grammatical characteristics. From the findings, pedagogical implications were drawn as ways to enhance learners’ schematic structural awareness, utilise diversified narrative strategies, increase interactional and presentational competency and finally to raise their multi-cultural awareness for successful business communicative outcomes in the future cross-cultural BELF job interview communicative setting...|$|R
40|$|Sources for {{the planned}} PhD project: I use an IT LSP corpus and a GeWiss-based corpus of spoken {{academic}} discourse (two genres: student presentations and oral exams). Both corpora are ≈ 400. 000 T each. The IT LSP corpus consists of forum & blog entries about Apple and Microsoft hard- and <b>software.</b> <b>For</b> both <b>corpora,</b> {{the process of}} data collection has been completed and they are being analysed. GeWiss Project German acronym, which stands for gesprochene Wissenschaftssprache = spoken academic discourse. Extract from the project description from the websit...|$|R
40|$|Objective To characterise {{empirical}} {{instances of}} Unified Medical Language System (UMLS) Metathesaurus term strings {{in a large}} clinical corpus, and to illustrate what types of term characteristics are generalisable across data sources. Design Based on the occurrences of UMLS terms in a 51 million document corpus of Mayo Clinic clinical notes, this study computes statistics about the terms’ string attributes, source terminologies, semantic types and syntactic categories. Term occurrences in 2010 i 2 b 2 /VA text were also mapped; eight example filters were designed from the Mayo-based statistics and applied to i 2 b 2 /VA data. Results <b>For</b> the <b>corpus</b> <b>analysis,</b> negligible numbers o...|$|R
40|$|Color poster with text, diagrams, and map. Preliminary {{research}} {{reveals that}} current {{work in the}} machine translation of Chinese-to-English focuses entirely on a statistical approach. This approach {{does not allow for}} a culturally-sensitive translation with regards to the source language, as meaning is often lost through metaphor or folklore references. This project explored the option of compiling a database to allow <b>for</b> the <b>corpora</b> <b>analysis</b> of creative works previously translated and employing an example-based approach to translation in conjunction with the currently popular statistical approach. University of Wisconsin [...] Eau Claire Office of Research and Sponsored Programs; University of Wisconsin [...] Eau Claire Center for Faculty and Undergraduate Student Research Collaboratio...|$|R
40|$|The Prague Dependency Treebank 2. 0 (PDT 2. 0) {{contains}} {{a large amount}} of Czech texts with complex and interlinked morphological (two million words), syntactic (1. 5 MW) and complex semantic annotation (0. 8 MW); in addition, certain properties of sentence information structure and coreference relations are annotated at the semantic level. PDT 2. 0 is based on the long-standing Praguian linguistic tradition, adapted for the current Computational Linguistics research needs. The corpus itself uses the latest annotation technology. <b>Software</b> tools <b>for</b> <b>corpus</b> search, annotation and language analysis are included. Extensive documentation (in English) is provided as well...|$|R
40|$|This paper {{presents}} {{a new approach}} and a <b>software</b> <b>for</b> collecting specialized <b>corpora</b> on the Web. This approach takes advantage of a very popular XML-based norm used on the Web for sharing content among websites: RSS (Really Simple Syndication). After a brief introduction to RSS, we explain the interest {{of this type of}} data sources in the framework of corpus development. Finally, we present Corporator, an Open Source software which was designed <b>for</b> collecting <b>corpus</b> from RSS feed...|$|R
40|$|Corpus Workbench (CWB) is a widely-used {{architecture}} <b>for</b> <b>corpus</b> <b>analysis,</b> {{originally designed}} at the IMS, University of Stuttgart (Christ 1994). It {{consists of a}} set of tools for indexing, managing and querying very large corpora with multiple layers of word-level annotation. CWB’s central component is the Corpus Query Processor (CQP), an extremely powerful and efficient concordance system implementing a flexible two-level search language that allows complex query patterns to be specified both at the level of an individual word or annotation, and at the level of a fully- or partially-specified pattern of tokens. CWB and CQP are commonly used as the back-end <b>for</b> web-based <b>corpus</b> interfaces, <b>for</b> example, in the popular BNCweb interface to the British National Corpus (Hoffmann et al. 2008). CWB has influenced other tools, such as the Manatee software used in SketchEngine, which implements the same query language (Kilgarriff et al. 2004). This paper details recent work to update CWB for the new century. Perhaps the most significant development is that CWB version 3 is now an open source project, licensed under the GNU General Public Licence. This change has substantially enlarged the community of developers and users and has enabled us to leverage existing open-source libraries in extending CWB’s capabilities. As a result, several key improvements were made to the CWB core: (i) support for multiple character sets, most especially Unicode (in the form of UTF- 8) ...|$|R
30|$|The data {{analysed}} in {{this research}} were generated from two sources. To address research aim 1, we used samples of the written text of the EYLF as our <b>corpus</b> <b>for</b> <b>analysis.</b> To address research aim 2, the first author interviewed 6 early childhood teachers currently working in settings catering for children aged from 6  weeks to 2  years.|$|R
40|$|A small {{subset of}} PDT 2. 0 made {{available}} under a permissive license. Prague Dependency Treebank 2. 0 (PDT 2. 0) contains {{a large amount}} of Czech texts with complex and interlinked morphological (2 million words), syntactic (1. 5 MW) and complex semantic annotation (0. 8 MW); in addition, certain properties of sentence information structure and coreference relations are annotated at the semantic level. PDT 2. 0 is based on the long-standing Praguian linguistic tradition, adapted for the current Computational Linguistics research needs. The corpus itself uses the latest annotation technology. <b>Software</b> tools <b>for</b> <b>corpus</b> search, annotation and language analysis are included. Extensive documentation (in English) is provided as well...|$|R
40|$|Procs of Computer Supported Collaborative Learning (CSCL 2009), Rhodos, Greece - PosterWe {{point out}} the need for CSCL {{community}} to reach large scale validation for its results by addressing the lack of sharing of interaction indicators and data. The main goal of the Mulce project is a definition for teaching and learning <b>corpora</b> (especially <b>for</b> interaction tracks), a technical format to organize data and a platform <b>for</b> <b>corpus</b> sharing, providing <b>analysis</b> and visualization tools...|$|R
30|$|The CQP {{edition of}} BNCweb was {{applied in the}} present study to assist the {{students}} in the experimental group to redraft essays for the following advantages. Firstly, it’s fast {{due to the fact that}} it just takes seconds to obtain a collocation analysis of more than 20 thousand instances in the BNC. Secondly, it’s simple to use and flexible. Students can not only perform simple queries just by typing in a single word or a sequence of lexical items, but also conduct more complex searches by using CQP syntax (Hoffmann et al. 2008). This means students can easily get the high-frequent adjective or adverb collocates of target words, which to some extent can solve students’ problems of under-using adjectives or adverbs and improve the lexical richness of their writing. Furthermore, the advanced CQP query syntax can assist students to get some special sentence structure, which provides guidance for redrafting sentences. Thirdly, it provides a whole range of features <b>for</b> <b>corpus</b> <b>analysis,</b> such as concordance display, sort, collocations, distribution analysis, etc. (Hoffmann and Evert 2006). Fourthly, it is freely accessible to everyone with an Internet connection. Fifthly, it can be simultaneously used by thousands of learners. In this study, BNC was mainly used for two types of revision tasks, including error correction and rewritten work, to make their essays more accurate, fluent and complex.|$|R
40|$|International audienceIn this article, we have {{attempted}} to trace {{the history of the}} statistical analysis of textual data, focusing on the influence of Benzécri's work and school, and to make explicit their theoretical positions, clearly opposed to AI and to Chomskyan linguistics. After a presentation of the intellectual project, as an inductive approach to language based on the exploration of corpora, we present the principles of correspondence analysis, which is the main method developed in the Data <b>Analysis</b> School, used <b>for</b> <b>corpus</b> <b>analysis</b> but also <b>for</b> many other types of datasets. Then, we will focus on textual data analysis. Based on the fact that software programmes have {{played a major role in}} the use of these statistical techniques, we shall examine a selection of these, display their specificities and their underlying theoretical bases. Résumé Cet article revient sur une des deux branches à l'origine de la statistique textuelle, l'école d'analyse des données « à la française », dont Jean-Paul Benzécri peut être considéré comme l'initiateur. Après avoir explicité les orientations théoriques de l'analyse des données, et le rôle joué par une approche inductive du langage, nous présentons rapidement les principes de l'analyse des correspondances. Ensuite, nous explorerons l'application de l'analyse des données aux corpus de textes en montrant le rôle joué par les logiciels dans la diffusion de cette approche...|$|R
40|$|SpeechRecorder is a {{platform}} independent audio recording <b>software</b> <b>for</b> speech <b>corpus</b> recordings. It is implemented in Java {{in a clean}} object-oriented design and adheres to established technology standards and document interchange formats. SpeechRecorder allows Unicode text and multimedia prompts, it supports audio recordings via more than two channels, and it features multiple configurable screens. Recording sessions are defined by recording scripts written in XML. The recording scripts can be executed manually by the experimenter, or automatically for unsupervised recordings; progress through the script can be sequential or randomized. SpeechRecorder is based on URLs to access local and network resources and thus allows recordings via the WWW. 1...|$|R
40|$|From a {{research}} conducted with 20 journals specialized in education, published between 1971 and 2000, the present text analyses the appropriation forms from Pierre Bourdieus {{work in the}} Brazilian educational field. 355 articles published in those periodics making references to the sociologist constitute the basic <b>corpus</b> <b>for</b> the <b>analysis</b> of the peculiarities of the brazilian interpretation of the author...|$|R
