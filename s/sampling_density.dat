764|2168|Public
5000|$|Suppose the <b>sampling</b> <b>density</b> is {{a multivariate}} normal {{distribution}} ...|$|E
5000|$|... where [...] is {{a random}} sample from [...] For {{positive}} , the theoretically optimal importance <b>sampling</b> <b>density</b> (pdf) is given by ...|$|E
5000|$|One of {{the objects}} of {{interest}} in designing a sampling scheme for wavenumber-limited fields is to identify the configuration of points {{that leads to the}} minimum <b>sampling</b> <b>density,</b> i.e., the density of sampling points per unit spatial volume in [...] Typically the cost for taking and storing the measurements is proportional to the <b>sampling</b> <b>density</b> employed. Often in practice, the natural approach to sample two-dimensional fields is to sample it at points on a rectangular lattice. However, this is not always the ideal choice in terms of the <b>sampling</b> <b>density.</b> The theorem of Petersen and Middleton can be used to identify the optimal lattice for sampling fields that are wavenumber-limited to a given set [...] For example, it can be shown that the lattice in [...] with minimum spatial density of points that admits perfect reconstructions of fields wavenumber-limited to a circular disc in [...] is the hexagonal lattice. As a consequence, hexagonal lattices are preferred for sampling isotropic fields in [...]|$|E
30|$|It {{is clear}} that ED method is the fastest (with low quality) and GPR is the most {{computationally}} expensive method (with good quality), {{and the result is}} consistent with the existing literatures. For example, at <b>sample</b> <b>density</b> of 3 % for image Lena, ED representation takes only 0.026 s, while GPR method takes 248 s. For image peppers at <b>sample</b> <b>density</b> of 3 %, ED method takes 0.027 s while GPR takes 318 s. The smaller <b>sample</b> <b>density,</b> the longer time GPR needs because more points need to be removed before reaching the desired <b>sample</b> <b>density.</b> For all other methods, the computational cost is lower for smaller <b>sample</b> <b>density.</b>|$|R
40|$|The {{production}} of geochemical {{maps of the}} world as proposed by the International Geochemical Mapping project (IGM) will require sampling of new areas at low densities and amalgamation of new data with existing geochemical data to ensure map production within reasonable time scales and budgets. Assuming compatibility in sampling and analytical techniques, two important considerations regarding <b>sample</b> <b>density</b> are: (i) how far can <b>sample</b> <b>densities</b> be reduced before meaningful geochemical patterns are lost, and (ii) how can datasets of different <b>sample</b> <b>density</b> be amalgamated? These considerations are examined in the present study by applying two methods of computational data reduction simulating low <b>density</b> <b>sampling</b> to the British Geological Survey's high precision, high resolution (1 sample per 1. 5 km 2) stream sediment datasets for Northern Britain, exemplified by Ni and B. A series of grids with grid squares of 25 km 2, 100 km 2, 500 km 2 and 160000 km 2 corresponding to <b>sample</b> <b>densities</b> recommended elsewhere for IGM (Table 1) are superimposed on the data. For each grid square the data are reduced by (i) selecting a single sample at random, and (ii) calculating the median value. Results are presented as a suite of image processed maps contoured with similar percentile levels enabling comparisons of element distributions to be made. The maps demonstrate that geochemical patterns become distorted at <b>sample</b> <b>densities</b> lower than 1 per 25 km 2 using the random selection method. Random sub-sampling of existing datasets with high <b>sample</b> <b>densities</b> is therefore unlikely to be successful. Employing the median value method, geochemical patterns are maintained with a reasonable degree of accuracy to densities as low as 1 sample per 500 km 2. The optimum reduced <b>sample</b> <b>density</b> for the Northern Britain datasets for Ni and B is 1 per 25 km 2. The size of the geological feature (s) and the magnitude of the geochemical variation are the principle factors controlling the resolution of geochemical patterns at low <b>sample</b> <b>densities.</b> Hence it is unrealistic to recommend an optimum <b>sample</b> <b>density</b> suitable for geochemical mapping throughout the world. Additional factors which influence the choice of <b>sample</b> <b>density</b> include the objectives of the survey (eg. regional reconnaissance, mineral reconnaissance, environmental monitoring), logistical controls on sampling (access, vegetation, climate etc.) and funding constraints. A <b>sample</b> <b>density</b> structure based on grid sizes of 25600 km...|$|R
5000|$|... <b>sample</b> <b>density</b> {{distribution}} {{of differences between}} adjacent NN intervals; ...|$|R
50|$|Each {{of these}} classes of {{confocal}} microscope have particular advantages and disadvantages. Most systems are either optimized for recording speed (i.e. video capture) or high spatial resolution. Confocal laser scanning microscopes {{can have a}} programmable <b>sampling</b> <b>density</b> and very high resolutions while Nipkow and PAM use a fixed <b>sampling</b> <b>density</b> defined by the camera's resolution. Imaging frame rates are typically slower for single point laser scanning systems than spinning-disk or PAM systems. Commercial spinning-disk confocal microscopes achieve frame rates of over 50 per second - a desirable feature for dynamic observations such as live cell imaging.|$|E
50|$|The {{minimum number}} of samples per unit area {{required}} to completely recover the continuous time signal is termed as optimal <b>sampling</b> <b>density.</b> In applications where memory or processing time are limited, emphasis {{must be given to}} minimizing the number of samples required to represent the signal completely.|$|E
50|$|A lowpass pyramid {{is made by}} {{smoothing}} the image with an appropriate smoothing filter and then subsampling the smoothed image, usually {{by a factor of}} 2 along each coordinate direction. The resulting image is then subjected to the same procedure, and the cycle is repeated multiple times. Each cycle of this process results in a smaller image with increased smoothing, but with decreased spatial <b>sampling</b> <b>density</b> (that is, decreased image resolution). If illustrated graphically, the entire multi-scale representation will look like a pyramid, with the original image on the bottom and each cycle's resulting smaller image stacked one atop the other.|$|E
30|$|Step 1 : Generate {{an initial}} mesh {{based on the}} desired <b>sample</b> <b>density.</b>|$|R
40|$|Gamma Spectrometry Counting System {{requires}} similar counting geometries for the calibration source, {{reference material}} and samples. The objectives {{of this study}} were to find out the effects of the <b>sample</b> <b>density</b> on 137 Cs activities measurement and propose reasonable corrections. Studies found that the activity of the samples is decreasing when the <b>density</b> of <b>samples</b> increased. Therefore, {{in order to have a}} more accurate estimation of <b>samples</b> activities; <b>density</b> corrections should be done either by performs mathematical corrections using equation or by increasing the expanded uncertainty when <b>sample</b> <b>densities</b> deviated from calibration source. ABSTRA...|$|R
40|$|In {{this work}} the DC {{resistivity}} of sintered nickel manganite NiMn 2 O 4 (NTC thermistor material) was studied {{as a function}} of additional powder activation time in a planetary ball mill (0, 5, 15, 30, 45 and 60 min). The activated powders and non-activated powder were sintered at different temperatures (900, 1050 and 12000 C) for an hour. Structural changes were analyzed using XRD. <b>Sample</b> <b>density,</b> porosity and DC resistivity were measured on the same sintered samples. Correlations between <b>sample</b> <b>density,</b> porosity, and intrinsic DC resistivity vs. additional powder activation time and the sintering temperature were made. It was noticed that the resistivity falls with the increase of <b>sample</b> <b>density</b> (or increase of the sintering temperature) ...|$|R
5000|$|In the {{simplest}} implementations, the finite point set is stored as an unstructured list of {{points in the}} medium. In the Lagrangian approach the points move with the medium, and points may be added or deleted {{in order to maintain}} a prescribed <b>sampling</b> <b>density.</b> The point density is usually prescribed by a smoothing length defined locally. In the Eulerian approach the points are fixed in space, but new points may be added where there is need for increased accuracy. So, in both approaches the nearest neighbors of a point are not fixed, and are determined again at each time step.|$|E
50|$|From U, the {{periodicity}} matrix, we {{can calculate}} the optimal <b>sampling</b> <b>density</b> {{for both the}} rectangular and hexagonal schemes. It is found {{that in order to}} completely recover the circularly band-limited signal, the hexagonal sampling scheme requires 13.4% fewer samples than the rectangular sampling scheme. The reduction may appear to be of little significance for a 2-dimensional signal. But as the dimensionality of the signal increases, the efficiency of the hexagonal sampling scheme will become far more evident. For instance, the reduction achieved for an 8-dimensional signal is 93.8%. To highlight the importance of the obtained result http://www.springerreference.com/docs/html/chapterdbid/318221.html, try and visualize an image as a collection of infinite number of samples. The primary entity responsible for vision, i.e. the photoreceptors (rods and cones) are present on the retina of all mammals. These cells are not arranged in rows and columns. By adapting a hexagonal sampling scheme, our eyes are able to process images much more efficiently. The importance of hexagonal sampling {{lies in the fact that}} the photoreceptors of the human vision system lie on a hexagonal sampling lattice and, thus, perform hexagonal sampling http://hyperphysics.phy-astr.gsu.edu/hbase/vision/rodcone.html. In fact, it can be shown that the hexagonal sampling scheme is the optimal sampling scheme for a circularly band-limited signal.|$|E
40|$|An {{adaptive}} importance {{sampling methodology}} is proposed {{to compute the}} multidimensional integrals encountered in reliability analysis. In the proposed methodology, samples are simulated as the states of a Markov chain and are distributed asymptotically according to the optimal importance <b>sampling</b> <b>density.</b> A kernel <b>sampling</b> <b>density</b> is then constructed from these samples which is used as the <b>sampling</b> <b>density</b> in an importance sampling simulation. The Markov chain samples populate the region of higher probability density in the failure domain and so the kernel <b>sampling</b> <b>density</b> approximates the optimal importance <b>sampling</b> <b>density</b> for a large variety of shapes of the failure domain. This adaptive feature is insensitive to the probability level to be estimated. A numerical example demonstrates the accuracy, efficiency and robustness of the method...|$|E
40|$|We {{study the}} spatiotemporal {{sampling}} of a diffusion field generated by K point sources, aiming to fully reconstruct the unknown initial field distribution {{from the sample}} measurements. The sampling operator in our problem can be described by a matrix derived from the diffusion model. We analyze the important properties of the sampling matrices, leading to precise bounds on the spatial and temporal <b>sampling</b> <b>densities</b> under which perfect field reconstruction is feasible. Moreover, our analysis indicates {{that it is possible}} to compensate linearly for insufficient spatial <b>sampling</b> <b>densities</b> by oversampling in time. Numerical simulations on initial field reconstruction under different spatiotemporal <b>sampling</b> <b>densities</b> confirm our theoretical results. Index Terms — Diffusion equation, initial inverse problems, spatiotemporal sampling, point sources localization, compressed sensing 1...|$|R
25|$|In other words, slow-attack {{compressors}} decrease {{both physical}} and perceptual levels, decrease macro-dynamics, but have no influence on crest factor and clipped <b>sample</b> <b>density.</b>|$|R
30|$|GPRAMA(γ): GPR {{starting}} from an AMA representation of γ {{times of the}} desired <b>sample</b> <b>density</b> and utilizing ear clipping for mesh patching, proposed in Section 4.|$|R
40|$|It {{is known}} that {{in the absence of}} distortion, the {{necessary}} <b>sampling</b> <b>density</b> for a multiband signal is given by its spectral occupancy. However, in general, the samples have to he acquired nonuniformly. There exist sampling patterns such that reconstruction is feasible even if the actual spectral support of the multiband signal is not known. If the samples are distorted, an increased <b>sampling</b> <b>density</b> may lead to a superior performance. In this paper, we consider the case of small distortion due to fine quantization of the samples, and we derive a necessary condition on the optimal <b>sampling</b> <b>density...</b>|$|E
3000|$|... i+k− 1 {{because of}} the photo {{collection}} data <b>sampling</b> <b>density</b> decrease towards the periphery of landmark models.|$|E
40|$|In the thesis, a <b>sampling</b> <b>density</b> {{based on}} the {{auxiliary}} particle filter <b>sampling</b> <b>density</b> has been proposed. The calculatation of the primary weights comes from complete utilization {{of information about the}} system given by the transient probability density function and the measurement probability density function. The estimate quality of the particle filter with so functionally designed <b>sampling</b> <b>density</b> is higher with respect to the auxiliary particle filter or particle filter with the prior <b>sampling</b> <b>density.</b> Specification of the sample size is often missed out. In the thesis a setting sample size of the particle filter with respect to the Cramer-Rao bound has been proposed. The Cramer-Rao bound represents the lower bound for estimation of the state mean. The procedure consists of comparison of the mean square error matrices of particle filter estimates for various sample sizes and the Cramer-Rao bound. The sample size adaptation technique has been proposed. The technique is based on measuring of generated samples quality with respect to the current measurement. The samples quality is required to be time-invariant. The sample size adaptation technique enables the estimate quality to be independent from <b>sampling</b> <b>density</b> of the particle filter. The sequential Monte Carlo method has been confronted with the point-mass method as a representative of numerical methods for solution of the state estimation problem. The comparison is realized from theoretical point of view and in some numerical examples as well. Available from STL Prague, CZ / NTK - National Technical LibrarySIGLECZCzech Republi...|$|E
40|$|The {{influence}} of the bulk density of YBa 2 Cu 3 -xFexOy (0 < x < 0. 01) ceramics on the intergranular superconducting (SC) properties was studied using the temperature dependence of AC magnetic susceptibility measurements. It {{was found that the}} simultaneous variation of the <b>sample's</b> <b>density</b> and the iron impurity concentration does not influence effectively the onset temperature of the superconducting state Tc(on). While only increasing of the <b>sample's</b> <b>density</b> shifts the intergranular hysteresis losses peak temperature Tm(J) to the lower values which connects with the decreasing of the Josephson magnetic vortices pinning role. It was established that the shielding capability and Tm(J) display a plateau with X in the 0. 003 < X < 0 [...] 007 region which is due to the monotonous decrease of the <b>sample's</b> <b>density.</b> It was shown that the shielding capability at the T= 78 K for the sample with 3. 8 g/cm 3 is two times higher than that for the <b>sample</b> with the <b>density</b> of 5. 0 g/cm 3. The possible interpretations of the observed results are discussed. Comment: 6 pages, LaTex, 4 figure...|$|R
40|$|A nonlocal variational {{formulation}} for interpolating a sparsel sampled {{image is}} introduced in this paper. The proposed variational formulation, originally motivated by image inpainting problems, encourages/nthe transfer of information between similar image patches, following {{the paradigm of}} exemplar-based methods. Contrary to the classical inpainting/nproblem, no complete patches {{are available from the}} sparse image/nsamples, and the patch similarity criterion has to be redefined as here proposed. Initial experimental results with the proposed framework, at very low <b>sampling</b> <b>densities,</b> are very encouraging. We also explore some/ndepartures from the variational setting, showing a remarkable ability to recover textures at low <b>sampling</b> <b>densities...</b>|$|R
30|$|The {{calculation}} flow of {{the peak}} density algorithm is shown in Fig.  1. The basic principle is to measure <b>sample</b> <b>density</b> {{based on the number}} of similar samples. Select the maximum <b>density</b> <b>sample</b> of the local area as the clustering center, and ensure that the distance between the sample and other samples with a larger density is large enough.|$|R
40|$|This {{research}} {{considers the}} Fourier transform calculations of multidimensional signals. The calculations {{are based on}} random sampling, where the sampling points are nonuniformly distributed according to strategically selected probability functions, to provide new opportunities that are unavailable in the uniform sampling environment. The latter imposes the <b>sampling</b> <b>density</b> of at least the Nyquist density. Otherwise, alias frequencies occur in the processed bandwidth {{which can lead to}} irresolvable processing problems. Random sampling can mitigate Nyquist limit that classical uniform-sampling-based approaches endure, for the purpose of performing direct (with no prefiltering or downconverting) Fourier analysis of (high-frequency) signals with unknown spectrum support using low <b>sampling</b> <b>density.</b> Lowering the <b>sampling</b> <b>density</b> while achieving the same signal processing objective could be an efficient, if not essential, way of exploiting the system resources in terms of power, hardware complexity and the acquisition-processing time. In this research we investigate and devise novel random sampling estimation schemes for multidimensional Fourier transform. The main focus of the investigation and development is on the aspect of the quality of estimated Fourier transform in terms of the <b>sampling</b> <b>density.</b> The former aspect is crucial as it serves towards the heart objective of random sampling of lowering the <b>sampling</b> <b>density.</b> This research was motivated by the applicability of the random-sampling-based approaches in determining the Fourier transform in multidimensional Nuclear Magnetic Resonance (NMR) spectroscopy to resolve the critical issue of its long experimental time...|$|E
40|$|The {{problem of}} numerically {{integrating}} spiky functions over high dimensional domains arises in computational statistics, particle physics, computer graphics and computational finance. We propose a Monte Carlo method based on adaptive importance sampling. Adaptive importance sampling methods alternate between importance sampling from a density constructed {{to suit the}} integrand, and updating the <b>sampling</b> <b>density</b> with the newly sampled data. We present a method in which the <b>sampling</b> <b>density</b> {{is a mixture of}} products of beta distributions. ...|$|E
40|$|It is {{well known}} that the Gabor {{expansions}} converge to identity operator in weak* sense on the Wiener amalgam spaces as <b>sampling</b> <b>density</b> tends to infinity. In this paper we prove the convergence of Gabor expansions to identity operator in the operator norm as well as weak* sense on W(L^p, ℓ^q) as the <b>sampling</b> <b>density</b> tends to infinity. Also we show the validity of the Janssen's representation and the Wexler-Raz biorthogonality condition for Gabor frame operator on W(L^p, ℓ^q). Comment: 16 page...|$|E
40|$|A {{series of}} {{polycrystalline}} SmFeAs 1 -xOx bulks {{was prepared to}} systematically investigate the influence of <b>sample</b> <b>density</b> on flux pinning properties. Different <b>sample</b> <b>densities</b> were achieved by controlling the pelletizing pressure. The superconducting volume fraction, the critical current densities Jcm and the flux pinning force densities Fp were estimated from the magnetization measurements. Experimental results manifest that: (1) the superconducting volume fraction decreases with the decreasing of <b>sample</b> <b>density.</b> (2) The Jcm values have the similar trend except for the sample with very high density may due to different connectivity and pinning mechanism. Moreover, The Jcm(B) curve develops a peak effect at approximately the same field at which the high-density sample shows a kink. (3) The Fp(B) curve of the high-density sample shows a low-field peak and a high-field peak at several temperatures, which {{can be explained by}} improved intergranular current, while only one peak can be observed in Fp(B) of the low-density samples. Based on the scaling behaviour of flux pinning force densities, the main intragranular pinning is normal point pinning. Comment: 23 pages, 8 figures, 1 tabl...|$|R
40|$|Abstract. A nonlocal variational {{formulation}} for interpolating a sparsely sampled {{image is}} introduced in this paper. The proposed variational for-mulation, originally motivated by image inpainting problems, encourages {{the transfer of}} information between similar image patches, following the paradigm of exemplar-based methods. Contrary to the classical inpaint-ing problem, no complete patches {{are available from the}} sparse image samples, and the patch similarity criterion has to be redefined as here proposed. Initial experimental results with the proposed framework, at very low <b>sampling</b> <b>densities,</b> are very encouraging. We also explore some departures from the variational setting, showing a remarkable ability to recover textures at low <b>sampling</b> <b>densities.</b> ...|$|R
3000|$|For {{representation}} methods without greedy-point removal technique, M_H, 3 and M_aniso, 3 {{provide better}} quality (see Table 1) but have higher computational cost than ED method (see Table 6). For example, at <b>sample</b> <b>density</b> of 3 [...]...|$|R
30|$|All three spiral scans {{we tested}} have {{successfully}} eliminated the flyback delay common in conventional STEM. Both Archimedean and Fermat scans yield STEM images with a quality comparable with conventional scan paths, but both {{have problems with}} <b>sampling</b> <b>density</b> (dose distribution). The constant linear velocity scan solves the sampling problem but introduces significant distortion in the center. For ease of use, the Fermat scan {{seems to be the}} best choice due to its relatively uniform <b>sampling</b> <b>density</b> and easy interpretation of the reconstructed image.|$|E
40|$|Yield mapping {{represents}} the spatial variability concerning {{the features of}} a productive area and allows intervening on the next year production, for example, on a site-specific input application. The trial aimed at verifying {{the influence of a}} <b>sampling</b> <b>density</b> and the type of interpolator on yield mapping precision to be produced by a manual sampling of grains. This solution is usually adopted when a combine with yield monitor can not be used. An yield map was developed using data obtained from a combine equipped with yield monitor during corn harvesting. From this map, 84 sample grids were established and through three interpolators: inverse of square distance, inverse of distance and ordinary kriging, 252 yield maps were created. Then they were compared with the original one using the coefficient of relative deviation (CRD) and the kappa index. The loss regarding yield mapping information increased as the <b>sampling</b> <b>density</b> decreased. Besides, it was also dependent on the interpolation method used. A multiple regression model was adjusted to the variable CRD, according to the following variables: spatial variability index and <b>sampling</b> <b>density.</b> This model aimed at aiding the farmer to define the <b>sampling</b> <b>density,</b> thus, allowing to obtain the manual yield mapping, during eventual problems in the yield monitor...|$|E
40|$|We {{study the}} spatial-temporal {{sampling}} of a linear diffusion field, {{and show that}} it is possible to compensate for insufficient spatial sampling densities by oversampling in time. Our work is motivated by the following issue often encountered in sensor network sampling, namely increasing the temporal <b>sampling</b> <b>density</b> is often easier and less expensive than increasing the spatial <b>sampling</b> <b>density</b> of the network. For the case of sampling a diffusion field, we show that, to achieve trade-off between spatial and temporal sampling, the spatial arrangement of the sensors must satisfy certain conditions. We provide in this paper the precise relationships between the achievable reduction of spatial <b>sampling</b> <b>density,</b> the required temporal oversampling rate, the spatial arrangement of the sensors, and the bound for the condition numbers of the resulting sampling and reconstruction procedures. Index Terms — Sampling, sensor network, diffusion process, spatial-temporal sampling, super-resolution. 1...|$|E
50|$|In early surveys, {{measurements}} were recorded individually and plotted by hand. Although useful results were sometimes obtained, practical applications were {{limited by the}} enormous amount of labor required. Data processing was minimal and <b>sample</b> <b>densities</b> were necessarily low.|$|R
40|$|Digital {{elevation}} models (DEMs) are {{the framework}} for the modeling of numerous coastal processes including tsunami propagation and inundation, storm-surge, and sea-level-rise. The National Oceanic and Atmospheric Administration (NOAA) National Geophysical Data Center (NGDC) develops integrated bathymetric-topographic DEMs across coastal zones to support tsunami propagation and inundation modeling efforts. The development of integrated bathymetric-topographic DEMs requires extreme interpolation across large distances between sparse bathymetric measurements {{in order for the}} model to retain the resolution of dense coastal topographic data, particularly lidar. This study examines the accuracy of three common interpolation methods used to develop bathymetric DEMs of Kachemak Bay, Alaska: inverse distance weighting (IDW), spline, and triangular irregular network (TIN). The goal of the study is {{to examine the relationship between}} interpolation deviations from measured depths and <b>sample</b> <b>density,</b> distance to the nearest depth measurement, and terrain characteristics. A split-sample method was used to determine that the accuracy of the three evaluated interpolation methods decreases in areas of high surface curvature, at greater distances from the nearest measurement, and at smaller <b>sampling</b> <b>densities.</b> Furthermore, spline is the most accurate interpolation method at all <b>sampling</b> <b>densities.</b> Predictive equations of interpolation uncertainty derived from the quantification of interpolation deviations in relationship to <b>sample</b> <b>density</b> and distance to the nearest depth measurement were developed. These predictive equations of the uncertainty in DEMs introduced by interpolation methods can aid mitigation efforts for coastal communities prone to tsunamis, storm-surge, and other coastal hazards, by improving the understanding of the propagation of uncertainty into the modeling of such coastal processes that rely on integrated bathymetric-topographic DEMs...|$|R
30|$|Note that in step 4, we {{can choose}} {{different}} orders of finite element interpolation methods. However, in this paper, we only consider linear finite element interpolation for triangular elements {{and the effects}} of higher order interpolation on representation quality is currently under investigation. In fact, quadratical interpolation provides higher representation quality; however, the <b>sample</b> <b>density</b> is also higher since it uses the midpoints on the edges of the triangles. There is no need to sample the midpoints because their coordinates can be computed; however, the function values at the midpoints need to be assigned in Step 2. A fair comparison is needed between higher order interpolation and linear interpolation with the same <b>sample</b> <b>density.</b>|$|R
