10|59|Public
40|$|Dependent pattern {{matching}} {{is a fundamental}} technique for writing Agda code, but by default it implies the K axiom, making it incompatible with homotopy type theory. The —without-K flag imposes a <b>syntactic</b> <b>check</b> for definitions that rely on K, however so far it lacked a formal correctness proof. In this talk, I propose a new specification for —without-K that works by limiting the unification algorithm used for case splitting. It is strictly more liberal than the current specification, particularly when {{pattern matching}} on parametrized datatypes, {{and at the same}} time it solves a currently open problem with the <b>syntactic</b> <b>check.</b> More importantly, it allows a translation of definitions by pattern matching to eliminators in the style of Goguen et al. (2006) without relying on the K axiom, thus proving its correctness. So with it, we can finally stop worrying when using pattern matching in HoTT. status: publishe...|$|E
40|$|This paper {{describes}} a scheme for defining partial higher-order functions as the least fixed points of monotone functionals. The scheme {{can be used}} to define both single functions by recursion and systems of functions by mutual recursion. The scheme is implemented in the IMPS Interactive Mathematical Proof System. The IMPS implementation includes an automatic <b>syntactic</b> <b>check</b> for monotonicity that succeeds for many common recursive definitions. ...|$|E
40|$|This paper {{presents}} a syntactic method to check so-called assignable clauses of annotated Java programs. Assignable clauses describe which variables may be assigned by a method. Their correctness {{is crucial for}} reasoning about class specifications. The method that we propose is incomplete, as it only makes a <b>syntactic</b> <b>check</b> {{and it does not}} take aliasing or expression evaluation into account, but it provides efficient means to find the most common errors in assignable clauses. This is [...] ...|$|E
40|$|Abstract. Hume is a {{proposed}} new environment for constructing safety critical systems. Here, possible constraints on recursion in the Hume expression language, to aid termination determinacy, are discussed and structural operational semantics for static <b>syntactic</b> <b>checks</b> for simple and nested recursion are presented. ...|$|R
40|$|We {{present an}} {{automated}} verification method for security of Diffie–Hellman–based key exchange protocols. The method includes a Hoare-style logic and <b>syntactic</b> <b>checking.</b> The method {{is applied to}} protocols in a simplified version of the Bellare–Rogaway–Pointcheval model (2000). The security of the protocol in the complete model can be established automatically by a modular proof technique of Kudla and Paterson (2005) ...|$|R
50|$|Anyone who {{can read}} English can {{immediately}} read ClearTalk, {{and the people}} who write ClearTalk learn to write it while using it. The ClearTalk system itself does most of the training through use: the restrictions are shown by menus and templates and are enforced by immediate <b>syntactic</b> <b>checks.</b> By consistently using ClearTalk for its output, a system reinforces the acceptable syntactic forms.|$|R
40|$|AbstractThe {{integration}} of the constraint solving paradigm in programming languages raises {{a number of new}} issues. Foremost is the need for a useful canonical form for the representation of constraints. In the context of an extended class of linear arithmetic constraints we develop a natural canonical representation and we design polynomial time algorithms for deciding solvability and generating the canonical form. Important issues encountered include negative constraints, the elimination of redundancy and parallelism. The canonical form allows us to decide by means of a simple <b>syntactic</b> <b>check</b> the equivalence of two sets of constraints and provides the starting point for a symbolic computation system. It has, moreover, other applications and we show in particular that it yields a completeness theorem for constraint propagation and is an appropriate tool to be used in connection with constraint based programming languages...|$|E
40|$|Part 1 : Full PapersInternational audienceInteroperability, {{as one of}} the key {{competition}} {{factors for}} modern enterprises, describes the ability to establish partnership activities in an environment of unstable market. In some terms, interoperability determines the future of enterprises; so, improving enterprises’ interoperability turns to be a research focus. “Sharing data among heterogeneous partners” {{is one of the most}} basic common interoperability problems, which requires a general methodology to serve. Model transformation, which plays a key role in model-driven engineering, provides a possible solution to data sharing problem. A general model transformation methodology, which could shield traditional model transformation practices’ weaknesses: low reusability, contains repetitive tasks, involves huge manual effort, etc., is an ideal solution to data sharing problem. This paper presents a general model transformation methodology “combining semantic check measurement and <b>syntactic</b> <b>check</b> measurement into refined model transformation processes” and the mechanism of using it to serve interoperability’s data sharing issue...|$|E
40|$|Digital signal–processing (DSP) {{development}} {{tools such as}} Ptolemy, LabView and iConnect allow application developers to assemble reactive systems by connecting predefined components in generalised dataflow graphs and by hierarchically building new components by encapsulating sub–graphs. We follow the literature in calling this approach dataflow-oriented development. Previous work has shown how a novel process calculus, CaSE, can provide a model for this form of software, and how this {{can be used as}} the basis for a system of behavioural types. Well-typedness in this system implies reactivity (non-termination) in terms of a generalisation to the dataflow principle of consistency, which was previously unable to handle, in general, statefulness and non-determinism. In the previous presentation the typing rules were parameterised in a semantic behavioural equivalence, temporal observation congruence, which specialises CCS’s notion of weak bisimulation to this setting. In this work, we show how a complete axiom system for CaSE allows these equivalences to be reduced to a <b>syntactic</b> <b>check,</b> which is more fitting to a type system...|$|E
40|$|International audienceWith {{enterprise}} collaboration {{becoming increasingly}} frequent, {{the ability of}} an enterprise to cooperate with others {{has become one of}} the core factors in gaining competitive advantage. This trend has led to an urgent requirement to improve cooperation ability. To this end, model-based systems engineering is being adapted so that it can be used to represent and simulate the working processes of enterprises. Model-to-model mappings and transformations, as important aspects in model-based systems engineering, have become two of the key factors in improving the cooperation capabilities of enterprises. However, the foundations for achieving automatic model-to-model transformation have not yet been built. Normally, model transformation rules are built on the basis of model mappings, and model mappings concern semantic or syntactic representations. One of the difficulties in achieving model-to-model mappings and transformations lies in detecting the semantics and semantic relations that are conveyed in different models. This paper presents an automatic model-to-model mapping and transformation methodology, which applies semantic and <b>syntactic</b> <b>checking</b> measurements to detect the meanings and relations between different models automatically. Both of the semantic and <b>syntactic</b> <b>checking</b> measurements are combined into a refined meta-model based model transformation process. To evaluate the performance of this methodology, we demonstrate its applicability with a realistic example...|$|R
40|$|International audienceBusiness process {{modeling}} {{is an important}} concern in enterprise. Formal analysis techniques are crucial to detect semantic issues in the corresponding models, or to help with their refactoring and evolution. However, business process development frameworks often fall short {{when it comes to}} go beyond simulation or <b>syntactic</b> <b>checking</b> of the models. In this paper, we present our VBPMN verification framework. It features several techniques for the automatic analysis of business processes modeled using BPMN, the de facto standard for business {{process modeling}}. As such, it supports a more robust development of business processes...|$|R
40|$|Variant Process Algebra is {{designed}} for the formal behavioral modeling of software variation, as arises, for instance, in software product line engineering. Process terms are labelled with the sets of variants, i. e., specific products, where they are enabled. A multi-modal operational semantics enables two compositional forms of reasoning. The first one is concerned with relating {{the behavior of a}} variant to the whole family. The second notion relates variants between each other, for instance to be able to formally capture the intuitive idea that a variant is a conservative extension of another, {{in the sense that it}} adds more behavior without breaking any existing one. Sufficient conditions are given to establish such a relation statically, by means of <b>syntactic</b> <b>checks</b> on process terms...|$|R
40|$|Many notations and {{supporting}} tools {{are available to}} be used for describing business processes, but most lack the ability to do more than a <b>syntactic</b> <b>check</b> on the form of the model. Checking that they correctly depict the semantics of the situation is left to inspection. In this paper we examine the uses that the analysable language Alloy can play in Process modelling. We explore its application to descriptions of the organisation and its rules, to the description of processes and in the meta-modelling of process models. From the experiment with modelling a somewhat complex organisational situation, we conclude that Alloy has potential to increase the value of process modelling. Keywords: Alloy, process modelling, data modelling, state machine, UML, OCL, model analysis 1 Organisational Modelling The task of understanding organisations, their structures and processes, sufficiently well to engage in organisational design or redesign with confidence is daunting. Analysts must draw on many perspectives, their underlying philosophies and their enabling techniques. For example, qualitative understanding might be articulated with Soft Systems Methodology [5]. Precision may be added with the use of causal models, and feedback loops explored with systems dynamics [4]. Questions of capacity and timing may require discrete event simulation or critical path method (CPM). Interaction between actors might be described with UML Activity models [9] or RAD(Role Activity Diagrams) [18]. Structural relationships might be represented with UML Class Diagrams. Even this sample from the armamentarium of techniques shows the complexity of the task facing organisational analysts and designers, ranging as they do over the spectrum from qualitative to quantitative, from essentially social to ess [...] ...|$|E
40|$|Zeno-timelocks {{constitute}} {{a challenge for}} the formal verification of timed automata: they are difficult to detect, and the verification of most properties (e. g., safety) is only correct for timelock-free models. Some time ago, Tripakis proposed a <b>syntactic</b> <b>check</b> {{on the structure of}} timed automata: If a certain condition (called strong non-zenoness) is met by all the loops in a given automaton, then zeno-timelocks are guaranteed not to occur. Checking for strong non-zenoness is efficient, and compositional (if all components in a network of automata are strongly non-zeno, then the network is free from zeno-timelocks). Strong non-zenoness, however, is sufficient-only: There exist non-zeno specifications which are not strongly non-zeno. A TCTL formula is known that represents a sufficient-and-necessary condition for non-zenoness; unfortunately, this formula requires a demanding model-checking algorithm, and not all model-checkers are able to express it. In addition, this algorithm provides only limited diagnostic information. Here we propose a number of alternative solutions. First, we show that the compositional application of strong non-zenoness can be weakened: Some networks can be guaranteed to be free from Zeno-timelocks, even if not every component is strongly non-zeno. Secondly, we present new syntactic, sufficient-only conditions that complement strong non-zenoness. Finally, we describe a sufficient-and-necessary condition that only requires a simple form of reachability analysis. Furthermore, our conditions identify the cause of zeno-timelocks directly on the model, in the form of unsafe loops. We also comment on a tool that we have developed, which implements the syntactic checks on Uppaal models. The tool is also able to derive, from those unsafe loops in a given automaton (in general, an Uppaal model representing a product automaton of a given network), the reachability formulas that characterise the occurrence of zeno-timelocks. A modified version of the CSMA/CD protocol is used as a case-study...|$|E
40|$|AbstractIn Bounded Model Checking (BMC), {{the search}} for counterexamples of {{increasing}} lengths is translated into a sequence of satisfiability (SAT) checks. It is natural to try to exploit the similarity of these SAT instances by forwarding clauses learned during conflict analysis from one instance to the next. The methods proposed to identify clauses that remain valid fall into two categories: Those that are oblivious to the mechanism that generates the sequence of SAT instances and those that rely on it. In {{the case of a}} BMC run, it was observed by Strichman [O. Shtrichman. Pruning techniques for the SAT-based bounded model checking problem. In Correct Hardware Design and Verification Methods (CHARME 2001), pages 58 – 70, Livingston, Scotland, Sept. 2001. Springer. LNCS 2144] that those clauses learned during one SAT check that only depend on the structure of the model remain valid when checking for longer counterexamples. Eén and Sörensson [N. Eén and N. Sörensson. Temporal induction by incremental SAT solving. Electronic Notes in Theoretical Computer Science, 89 (4), 2003. First International Workshop on Bounded Model Checking. [URL] pointed out that all learned clauses can be forwarded if the translation into SAT obeys commonly followed rules. Many clauses that are forwarded this way, however, are of little usefulness and may degrade performance. This paper presents an extension to Strichman's approach {{in the form of a}} more general criterion to filter conflict clauses that can be profitably forwarded to successive instances. This criterion, in particular, is still syntactic and quite efficient, but accounts for the presence of both primary and auxiliary objectives in the SAT instance. This paper also introduces a technique to distill clauses to be forwarded even though they fail the <b>syntactic</b> <b>check.</b> Distillation is a semantic approach that can be applied in general to incremental SAT, and often produces clauses that are independent of the primary objective, and hence remain valid for the remainder of the sequence of instances. In addition, distillation often improves the quality of the clauses, that is, their ability to prevent the examination of large regions of the search space. Experimental results obtained with the CirCUs SAT solver confirm the efficacy of the proposed techniques, especially for large, hard problems...|$|E
50|$|A {{theory is}} said to be {{consistent}} if falsehood is not provable (from no assumptions) and is complete if every theorem is provable using the inference rules of the logic. These are statements about the entire logic, and are usually tied to some notion of a model. However, there are local notions of consistency and completeness that are purely <b>syntactic</b> <b>checks</b> on the inference rules, and require no appeals to models. The first of these is local consistency, also known as local reducibility, which says that any derivation containing an introduction of a connective followed immediately by its elimination can be turned into an equivalent derivation without this detour. It is a check on the strength of elimination rules: they must not be so strong that they include knowledge not already contained in its premises. As an example, consider conjunctions.|$|R
40|$|DrScheme {{provides}} a {{graphical user interface}} for editing and interactively evaluating Scheme programs on all major graphical platforms (Windows 95 /nt, MacOs, Unix/X). The environment is especially well-suited to beginning programmers because it supports a tower of Scheme subsets. Each level corresponds to a particular stage in a typical introductory Scheme course and implements a stringent set of <b>syntactic</b> <b>checks.</b> The environment also pinpoints run-time exceptions in a graphical manner and implements a mostly functional readeval -print loop. DrScheme's most advanced component is a powerful static debugger. It permits programmers to inspect programs for potential safety violations before running them. If the debugger discovers a potential problem, it explains the problem by drawing a value-flow graph over the program text. The value-flow graphs shows how an inappropriate value may reach a program operation and trigger a run-time check. The development of DrScheme in Scheme validated the [...] ...|$|R
40|$|Abstract. Parameterized runtime {{monitoring}} formalisms allow predicates to bind free {{variables to}} values during the program’s execution. Some runtime monitoring tools, like J-LO, increase the formalism’s expressiveness by allowing predicates to query variables already during the matching process. This is problematic because, if no special care is taken, the predicate’s evaluation {{may need to}} query a variable {{that has not yet}} been bound, rendering the entire formula meaningless. In this paper we present a <b>syntactic</b> <b>checking</b> algorithm that recognizes meaningless formulas in future-time linear temporal logic. The algorithm assures that a predicate accesses a potentially unbound variable only when the truth value of this predicate cannot possibly impact the truth value of the entire formula at the time the predicate is being evaluated. Our approach allows users to specify a wide range of meaningful parameterized logic formulas, {{while at the same time}} forbidding such formulas that would otherwise have an unclear semantics due to insu cient bindings. We have implemented the checking algorithm in the J-LO runtime verification tool. ...|$|R
40|$|Formal {{languages}} with semantics {{based on}} ordinary differential equations (ODEs) {{have emerged as}} a useful tool to reason about large-scale distributed systems. We present differential bisimulation, a behavioral equivalence developed as the ODE counterpart of bisimulations for languages with probabilistic or stochastic semantics. We study {{it in the context}} of a Markovian process algebra. Similarly to Markovian bisimulations yielding an aggregated Markov process in the sense of the theory of lumpability, differential bisimulation yields a partition of the ODEs underlying a process algebra term, whereby the sum of the ODE solutions of the same partition block is equal to the solution of a single (lumped) ODE. Differential bisimulation is defined in terms of two symmetries that can be verified only using <b>syntactic</b> <b>checks.</b> This enables the adaptation to a continuous-state semantics of proof techniques and algorithms for finite, discrete-state, labeled transition systems. For instance, we readily obtain a result of compositionality, and provide an efficient partition-refinement algorithm to compute the coarsest ODE aggregation of a model according to differential bisimulation...|$|R
40|$|This book {{deals with}} four {{theoretical}} and methodological approaches to syntax: Polish semantic syntax, two <b>Check</b> <b>syntactic</b> models, a dictionary of syntactic and semantic units called syntaxemes, and {{a review of}} generative grammar history. In the conclusion, the monograph highlights the application of these theories and methods on Macedonian sentence analysis...|$|R
40|$|IEEE 11 th World Congress on Services, New York, NY, JUN 27 -JUL 02, 2015 International audienceWeb service composition, {{as one of}} the key aspects in {{web service}} domain, has {{attracted}} more and more research attentions. Generally, in order to provide a powerful function to a specific problematic, several web services should combine and work together. Such a collaboration of web services is regarded as web service composition. There are two main difficulties in web service composition: selecting web services as partners and making interactions among these web services. A web service works as a functional black box; it takes in inputs and generates outputs. For a specific web services, both the inputs and the outputs are in specific formats. In order to make interactions among web services, it is necessary to be synergistic among their inputs and outputs. To generate specific inputs for a particular web service, the outputs from one or several other web services should transform the formats and combine together. This paper presents an automatic model transformation methodology, which focuses on transforming and combining outputs to generate inputs for web services. This automatic model transformation methodology regards all web services' inputs and outputs as models. In order to do the transformation and combination process efficiently and effectively, <b>syntactic</b> <b>checking</b> and semantic checking measurements have been combined into a refined model transformation process...|$|R
40|$|Since the World Wide Web (WWW) is {{currently}} the principal infrastructure used by the general public, web site creators are required to build universally accessible sites. Accessibility guidelines such as the Web Content Accessibility Guidelines (WCAG) show the creators how to make accessible web pages. Also, tools which automatically verify {{whether or not a}} given web page complies with the guidelines are provided. Although those tools are useful, applying them to any document format other than HTML or any guidelines other than WCAG is difficult, since these tools assume a fixed set of guidelines (e. g., WCAG) and a fixed document format (e. g., HTML). That is, changing the built-in guidelines (e. g., verifying whether a given document complies with the guidelines defined by their organization) requires modifying the tool. This paper proposes a simple and clear language for specifying guidelines, and discusses a verification tool which has the following characteristics. • A guideline specification is separated from the tool and can be easily modified. • An arbitrary XML document can be verified. • The tool can be used not only for accessibility guideline verification but also for complex <b>syntactic</b> <b>checking.</b> We also present the verification results conducted on about 3, 000 web pages of forty major organizations in the U. S. A. and Japan using our verification tool...|$|R
50|$|Even though over 90% of Cape Verdean Creole {{words are}} derived from Portuguese, the grammar is very different, which makes it {{extremely}} difficult for an untrained Portuguese native speaker even to understand a basic conversation. On the other hand, the grammar shows a lot of similarities with other creoles, Portuguese-based or not (<b>check</b> <b>syntactic</b> similarities of creoles).|$|R
40|$|Total {{functional}} programming offers the beguiling vision that, just {{by virtue of}} the compiler accepting a program, we are guaranteed that it will always terminate. In the case of programs that are not in- tended to terminate, e. g., servers, we are guaranteed that programs will always be productive. Productivity means that, even if a pro- gram generates an infinite amount of data, each piece will be gen- erated in finite time. The theoretical underpinning for productive programming with infinite output is provided by the category theo- retic notion of final coalgebras. Hence, we speak of coprogramming with non-well-founded codata, as a dual to programming with well- founded data like finite lists and trees. Systems that offer facilities for productive coprogramming, such as the proof assistants Coq and Agda, currently do so through syntactic guardedness checkers, which ensure that all self-recursive calls are guarded by a use of a constructor. Such a check ensures productivity. Unfortunately, these <b>syntactic</b> <b>checks</b> are not compo- sitional, and severely complicate coprogramming. Guarded recursion, originally due to Nakano, is tantalising as a basis for a flexible and compositional type-based approach to co- programming. However, as we show, guarded recursion by itself is not suitable for coprogramming {{due to the fact that}} there is no way to make finite observations on pieces of infinite data. In this paper, we introduce the concept of clock variables that index Nakano’s guarded recursion. Clock variables allow us to “close over” the generation of infinite codata, and to make finite observations, some- thing that is not possible with guarded recursion alone...|$|R
40|$|Abstract. Recent {{research}} has explored using Datalog-based languages {{to express a}} distributed system {{as a set of}} logical invariants. Two properties of distributed systems proved difficult to model in Datalog. First, the state of any such system evolves with its execution. Second, deductions in these systems may be arbitrarily delayed, dropped, or reordered by the unreliable network links they must traverse. Previous efforts addressed the former by extending Datalog to include updates, key constraints, persistence and events, and the latter by assuming ordered and reliable delivery while ignoring delay. These details have a semantics outside Datalog, which increases the complexity of the language and its interpretation, and forces programmers to think operationally. We argue that the missing component from these previous languages is a notion of time. In this paper we present Dedalus, a foundation language for programming and reasoning about distributed systems. Dedalus reduces to a subset of Datalog with negation, aggregate functions, successor and choice, and adds an explicit notion of logical time to the language. We show that Dedalus provides a declarative foundation for the two signature features of distributed systems: mutable state, and asynchronous processing and communication. Given these two features, we address two important properties of programs in a domain-specific manner: a notion of safety appropriate to non-terminating computations, and stratified monotonic reasoning with negation over time. We also provide conservative <b>syntactic</b> <b>checks</b> for our temporal notions of safety and stratification. Our experience implementing full-featured systems in variants of Datalog suggests that Dedalus is well-suited to the specification of rich distributed services and protocols, and provides both cleaner semantics and richer tests of correctness...|$|R
40|$|DrScheme {{provides}} a {{graphical user interface}} for editing and interactively evaluating Scheme programs on all major graphical platforms (Windows 95 /nt, MacOs, Unix/X). The environment is especially well-suited to beginning programmers because it supports a tower of Scheme subsets. Each level corresponds to a particular stage in a typical introductory Scheme course and implements a stringent set of <b>syntactic</b> <b>checks.</b> The environment also pinpoints run-time exceptions in a graphical manner and implements a mostly functional readeval-print loop. DrScheme's most advanced component isapowerful static debugger. It permits programmers to inspect programs for potential safety violations before running them. If the debugger discovers a potential problem, it explains the problem bydrawing a value- ow graph over the program text. The value- ow graphs shows how an inappropriate value may reach a program operation and trigger a run-time check. The development of DrScheme in Scheme validated the strengths of Scheme, but also revealed several weaknesses. To overcome the latter, the underlying Scheme implementationwas extended with a class-based object system, a language of program units, and a sophisticated GUI engine. All of these extensions are available to the programmer, who can thus interactively create fully portable, graphical applications. 1 Origins and Goals Over the past ten years, Scheme [1] {{has become the most}} widely used functional programming language in introductory courses in the United States [11, 12]. When Rice University implemented an introductory course using Scheme, the instructors noticed three signi cant problems with its popular implementations. First, although Scheme's parenthesized pre x notation is extremely simple, beginning students often encounter surprising syntactic and run-time errors due to the transi...|$|R
40|$|The {{topic of}} this thesis is the {{development}} of knowledge based statistical software. The shortcomings of conventional statistical packages are discussed to illustrate the need to develop software which is able to exhibit a greater degree of statistical expertise, thereby reducing the misuse of statistical methods by those not well versed in the art of statistical analysis. Some of the issues involved in the development of knowledge based software are presented and a review is given of some of the systems that have been developed so far. The majority of these have moved away from conventional architectures by adopting what can be termed an expert systems approach. The thesis then proposes an approach which is based upon the concept of semantic modelling. By representing some of the semantic meaning of data, it is conceived that a system could examine a request to apply a statistical technique and check if the use of the chosen technique was semantically sound, i. e. will the results obtained be meaningful. Current systems, in contrast, can only perform what can be considered as <b>syntactic</b> <b>checks.</b> The prototype system that has been implemented to explore the feasibility of such an approach is presented, the system has been designed as an enhanced variant of a conventional style statistical package. This involved developing a semantic data model to represent some of the statistically relevant knowledge about data and identifying sets of requirements that should be met for the application of the statistical techniques to be valid. Those areas of statistics covered in the prototype are measures of association and tests of location...|$|R
40|$|Thesis (Master) [...] İzmir Institute of Technology, Computer Engineering, İzmir, 2010 Includes bibliographical {{references}} (leaves: 57 - 60) Text in English Abstract: Turkish and Englishix, 77 leavesThe {{scope of}} this thesis is to enhance a static analysis tool {{in order to find}} security limitations in java applications. This will contribute to the removal of some of the existing limitations related with the lack of java source codes. The generally used tools for a static analysis are FindBugs, Jlint, PMD, ESC/Java 2, Checkstyle. In this study, it is aimed to utilize PMD static analysis tool which already has been developed to find defects Possible bugs (empty try/catch/finally/switch statements), Dead code (unused local variables, parameters and private methods), Suboptimal code (wasteful String/StringBuffer usage), Overcomplicated expressions (unnecessary if statements for loops that could be while loops), Duplicate code (copied/pasted code means copied/pasted bugs). On the other hand, faults possible unexpected exception, length may be less than zero, division by zero, stream not closed on all paths and should be a static inner class cases were not implemented by PMD static analysis tool. PMD performs <b>syntactic</b> <b>checks</b> and dataflow analysis on program source code. In addition to some detection of clearly erroneous code, many of the. bugs. PMD looks for are stylistic conventions whose violation might be suspicious under some circumstances. For example, having a try statement with an empty catch block might indicate that the caught error is incorrectly discarded. Because PMD includes many detectors for bugs that depend on programming style, PMD includes support for selecting which detectors or groups of detectors should be run. While PMD. s main structure was conserved, boundary overflow vulnerability rules have been implemented to PMD...|$|R
40|$|National audienceIn {{this paper}} we give an {{overview}} of a novel tool which learns structured constraint models from flat, positive examples of solutions. It is based on previous work on a Constraint Seeker, which finds constraints in the global constraint catalog satisfying positive and negative examples. In the current tool we extend this system to find structured conjunctions of constraints on regular subsets of variables in the given solutions. Two main elements of the approach are a bi-criteria optimization problem which finds conjunctions of constraints which are both regular and relevant, and a <b>syntactic</b> dominance <b>check</b> between conjunctions, which removes implied constraints without requiring a full theorem prover, using meta-data in the constraint catalog. Some initial experiments on a proof-of-concept implementation show promising results...|$|R
40|$|We {{present a}} generic {{preprocessor}} for combined static/dynamic validation and debugging of constraint logic programs. Passing programs through the preprocessor prior to execution allows detecting many bugs automatically. This {{is achieved by}} performing a repertoire of tests which range from simple <b>syntactic</b> <b>checks</b> to much more advanced checks based on static analysis of the program. Together with the program, the user may provide a series of assertions which trigger further automatic checking of the program. Such assertions are written using the assertion language presented in Chapter 2, which allows expressing {{a wide variety of}} properties. These properties extend beyond the predefined set which may be understandable by the available static analyzers and include properties defined by means of user programs. In addition to user-provided assertions, in each particular CLP system assertions may be available for predefined system predicates. Checking of both user-provided assertions and assertions for system predicates is attempted first at compile-time by comparing them with the results of static analysis. This may allow statically proving that the assertions hold (Le., they are validated) or that they are violated (and thus bugs detected). User-provided assertions (or parts of assertions) which cannot be statically proved ñor disproved are optionally translated into run-time tests. The implementation of the preprocessor is generic in that it can be easily customized to different CLP systems and dialects and in that it is designed to allow the integration of additional analyses in a simple way. We also report on two tools which are instances of the generic preprocessor: CiaoPP (for the Ciao Prolog system) and CHIPRE (for the CHIP CLP(FL>) system). The currently existing analyses include types, modes, non-failure, determinacy, and computational cost, and can treat modules separately, performing incremental analysis...|$|R
40|$|Recent {{research}} has explored using Datalog-based languages to ex-press a distributed {{system as a}} set of logical invariants [2, 19]. Two properties of distributed systems proved difficult to model in Data-log. First, the state of any such system evolves with its execution. Second, deductions in these systems may be arbitrarily delayed, dropped, or reordered by the unreliable network links they must traverse. Previous efforts addressed the former by extending Datalog to include updates, key constraints, persistence and events, and the latter by assuming ordered and reliable delivery while ignoring delay. These details have a semantics outside Datalog, which increases the complexity of the language or its interpretation, and forces program-mers to think operationally. We argue that the missing component from these previous languages is a notion of time. In this paper we present D, a foundation language for programming and reasoning about distributed systems. D re-duces to a subset of Datalog [30] with negation, aggregate functions, successor and choice, and admits an explicit representation of time into the logic language. We show that D provides a declara-tive foundation for the two signature features of distributed systems: mutable state, and asynchronous processing and communication. Given these two features, we address three important properties of programs in a domain-specific manner: a notion of safety appropri-ate to non-terminating computations, stratified monotonic reasoning with negation over time, and efficient evaluation over time via a simple execution strategy. We also provide conservative <b>syntactic</b> <b>checks</b> for our temporal notions of safety and stratification. Our experience implementing full-featured systems in variants of Dat-alog suggests that D is well-suited to the specification of rich distributed services and protocols, and provides both cleaner semantics and richer tests of correctness. 1...|$|R
40|$|Formal {{languages}} are {{a powerful and}} convenient way of communicating difficult problems to automatic problem solvers. However complex problems tend to require complex descriptions and this can make problem specifications dense, hard to read and write, inefficient to execute or solve, and prone to errors. Further, a necessary trade-off between expressiveness and efficiency means language designers must often choose to sacrifice performance for a nicely expressive language, or cripple expressiveness to achieve performance targets. This compromise between expressiveness and efficiency {{is the subject of}} this thesis, which we explore with case studies from the Game Description Language, Situation Calculus, and Agent Logic Programs. In games, we derive primitive structures from raw logical game descriptions and compose these patterns into higher-order features suitable for synthesising automatic visualisations or constructing evaluation functions for game play. We then provide a formal theory of decomposition and methods for mapping extended imperfect-information games into a format suitable for existing complete-information players. In the Situation Calculus, we show how the syntactic structure of action theories affects the complexity of solutions. Finally, we map Agent Logic Programs to the oClingo tool, simultaneously providing an implementation to the former and satisfying a difficult opaque structural safety requirement of the latter. Importantly, the structure of both Agent Logic Programs and oClingo's input language are exploited so that simple <b>syntactic</b> <b>checks</b> confer strong semantic guarantees. By targeting the languages at the heart of these fields we make systems that run faster and more efficiently, better understand author intent, avoid traps, and make smarter decisions. Best of all, these improvements can be made without disrupting existing users or mandating large incompatible rewrites to existing domains. By advocating the intelligent use of structure over enforced structural changes we achieve the best of both worlds...|$|R
40|$|SOL is {{computer}} language geared to solution of design problems. Includes mathematical modeling and logical capabilities of {{computer language}} like FORTRAN; also includes additional power of nonlinear mathematical programming methods at language level. SOL compiler takes SOL-language statements and generates equivalent FORTRAN code and system calls. Provides <b>syntactic</b> and semantic <b>checking</b> for recovery from errors and provides detailed reports containing cross-references to show where each variable used. Implemented on VAX/VMS computer systems. Requires VAX FORTRAN compiler to produce executable program...|$|R
40|$|In this {{contribution}} {{we consider}} a light weight approach to support software architecture design over the web. We outline the architecture description language we use {{for this purpose}} {{which is based on}} the concept of a self con-tained software component. Based on this con-cept a loosely coupled approach to combine remote repositories is supported. A number of analysis tools are sketched which are available as back end services within this approach. These tools not only allow to <b>check</b> <b>syntactic</b> and semantic properties but also allow to assess a given architecture wrt. quantitative proper-ties like performance...|$|R
40|$|The {{traditional}} role of integrity constraints {{is to protect}} the integrity of data. But integrity constraints can and do play other roles in databases; for example, they can be used for query optimization. In this role, they do not need to model the domain; it is sufficient that they describe regularities that are true about the data currently stored in a database. In this paper we describe two algorithms for finding such regularities (in the <b>syntactic</b> form of <b>check</b> constraints) and discuss some of their applications in DB 2...|$|R
40|$|Abstract. In {{this paper}} we give an {{overview}} of an early prototype which learns structured constraint models from flat, positive examples of solutions. It is based on previous work on a Constraint Seeker, which finds constraints in the global constraint catalog satisfying positive and negative examples. In the current tool we extend this system to find structured conjunctions of constraints on regular subsets of variables in the given solutions. Two main elements of the approach are a bi-criteria optimzation problem which finds conjunctions of constraints which are both regular and relevant, and a <b>syntactic</b> dominance <b>check</b> between conjunctions, which removes implied constraints without requiring a full theorem prover, using meta-data in the constraint catalog. Some initial experiments on a proof-ofconcept implementation show promising results. 1 Scope and Assumptions Global constraints were initially introduced [3] in order to more efficiently handle the filtering associated with some recurring structured constraint networks [1]. An inherent disadvantage of the approach is that the introduction of global constraints does no...|$|R
