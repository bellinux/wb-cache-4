3|2|Public
40|$|In this paper, {{we develop}} simple and numerically stable family of {{algorithms}} for computing geodesic paths on meshes. The exact {{version of the}} algorithm {{is based on the}} interval propagation idea introduced by Mitchell, Mount, and Papadimitriou, and has the same O(n log n) worst case time complexity. The fastest approximate version works in roughly O(n log n) time and still guarantees computing exact geodesics on any subdivision of a plane. The desired tradeoff between the time complexity and the error of the approximation can be achieved by setting the interval <b>simplification</b> <b>threshold.</b> The algorithms were evaluated on meshes with up to 100, 000 vertices...|$|E
40|$|Line {{simplification}} is {{an important}} method {{in the context of}} cartographic generalization, which is helpful for improving the visualization of digital vector maps. The evaluation method for the simplification algorithms is still an open issue when facing applications of vector data, including progressive transmission, web mapping, and so on. This paper proposes a novel evaluation approach for line simplification algorithms based on several factors towards vector map visualization, including the features of displays, map scales, and the ability of the human eye to distinguish pixels. In order to ensure the evaluation of the line simplification algorithms is conducted under the consistent strength of simplification, a measurement approach for the difference between an original line and its simplified one is proposed in this study, and the method of solving the appropriate <b>simplification</b> <b>threshold</b> is presented. With this method, four simplification algorithms are evaluated at five map scales using three evaluation indicators: standard deviation, compression ratio, and simplification time. The experiment and results show the evaluation approach in this study is feasible, and represents a good means in which to facilitate the application of line simplification towards progressive transmission and visualization of vector maps...|$|E
40|$|In this thesis, {{we discuss}} {{data on the}} total ascent along 250 {{recorded}} GPS tracks, situated in western Slovenia. GPS tracks were recorded with a simple single-frequency code GPS recorder Qstarz BT-Q 1000 P with distance criteria of 10 m from the previously recorded point. From all recorded GPS tracks, we calculated the total ascent in different ways, namely directly from the recorded ellipsoidal heights, using different thresholds of height profile simplification, {{with the help of}} Google Earth in view mode of the recorded GPS track “Absolute” or “Clamped to ground”, meaning on the SRTM 90 m, and by laying the GPS tracks on DMV 1 m data from a recent recording of Slovenia with ALS, which we used as a reference. We evaluated the suitability of a particular method of determining the total ascent and empirically determined correction factors to ensure comparability of data with reference values and their mutual comparability. Based on the aggregated processing of 250 recorded GPS tracks, we found out that the total ascent, calculated from originally recorded, uncorrected GPS tracks, has to be reduced for 20 % on average, to get a value that is comparable to the reference. Total ascents, obtained with Google Earth in view mode “Clamped to ground” or calculated by the <b>simplification</b> <b>threshold</b> of 15 m, are on average too small due to more generalized terrain model or height profile, therefore it is necessary to increase them for 20 % on average to ensure the comparison with the reference values. It turns out that the sum of deviations, calculated by comparing each total ascent to its reference value, is on average the smallest when total ascent is determined using 5 m threshold of height profile simplification...|$|E
40|$|Vector line feature {{matching}} {{is one of}} the hot {{research in}} the field of spatial data matching. This paper proposes a method of improving the accuracy of line feature matching based on dynamic simplification. Firstly, use the square root law to respectively determine the numeric ranges of the <b>simplification</b> <b>thresholds</b> for the two matching line features; then, let the threshold change by a certain step within this range, and simplify the line features separately. After every simplification is completed, the matching similarity is recalculated and replaced by a higher value so as to attain the maximum similarity. Finally, compare the maximum similarity with the matching threshold to judge if the two line features match. This algorithm is essentially a process which adopts the dynamic simplification to drive the dynamic matching. The dynamic simplification can retain line's main morphological feature, in this way the impact of local details on matching algorithm decreases, accordingly the matching accuracy is improved. The validity and universality of the presented method is proved through experiments and comparative analysis...|$|R
40|$|Dewhurst, R., Nyström, M., Jarodzka, H., & Holmqvist, K. (2011, August). Scanpath {{similarity}} {{depends on}} how you look at it: Evaluating a ‘MultiMatch’ comparison algorithm. Presentation at ECEM, Marseille, France. Visual scanpaths represent a paradox; they are incredibly revealing yet inherently difficult to compare. Here we evaluate our new method for comparing scanpaths (Jarodzka et al., 2010, Proceedings of the 2010 Symposium on Eye-Tracking Research & Applications, 211 - 218). Instead of representing fixation-and-saccade sequences by way of discreet Regions of Interest (ROIs) and comparing strings of corresponding letters for similarity (cf. Levenshtein string-edit distance), or representing scanpaths as Gaussian-based attention map functions which retain no information about the order of fixations, our ‘MultiMatch’ method treats scanpaths as geometric vectors and performs pair-wise comparisons based on a number of dimensions: shape, position, length, direction and duration. This requires a <b>simplification</b> step using <b>thresholding</b> to align scanpaths, and enables us to quantify characteristics which other comparison principles find problematic. With data from two experiments we assess how this algorithm copes with within- and between-subject similarity calculations, how task difficulty affects scanpath similarity, and how the similarity metric produced is dependent on scanpath length. Scanpaths can be similar in a number of intuitive ways, and the capability of our method to use multiple dimensions allows these qualitative distinctions to be tested empirically...|$|R

