1|6|Public
40|$|EUROSPEECH 2003 : 8 th European Conference on Speech Communication and Technology, September 1 - 4, 2003, Geneva, Switzerland. In real {{acoustic}} environments, humans {{communicate with}} each other through speech by focusing on the target speech among environmental sounds. We can easily identify the target sound from other environmental sounds. For hands-free speech recognition, the identification of the target speech from environmental sounds is imperative. This mechanism may also be important for a <b>self-moving</b> <b>robot</b> to sense the acoustic environments and communicate with humans. Therefore, this paper first proposes Hidden Markov Model (HMM) -based environmental sound source identification. Environmental sounds are modeled by three states of HMMs and evaluated using 92 kinds of environmental sounds. The identification accuracy was 95. 4 %. This paper also proposes a new HMM composition method that composes speech HMMs and an HMM of categorized environmental sounds for robust environmental sound-added speech recognition. As a result of the evaluation experiments, we confirmed that the proposed HMM composition outperforms the conventional HMM composition with speech HMMs and a noise (environmental sound) HMM trained using noise periods prior to the target speech in a captured signal...|$|E
40|$|HSC 2001 : IEEE International Workshop on Hands-Free Speech Communication, April 9 - 11, 2001, Kyoto, Japan. Distant-talking speech {{recognition}} {{is very important}} in providing a natural interface for machines like <b>self-moving</b> <b>robots.</b> Distant-talking {{speech recognition}} systems must deal with noises and acoustic reverberations in real environments. A microphone array signal processing and model adaptation method are proposed for distant-talking speech recognition in noisy reverberant environments. However, speech sounds captured by a microphone aπay are distorted by the directional gain patterns of the microphone array and reverberations in the room. Furthermore, model adaptation would give better performance with high SNR. This paper proposes a method to combine microphone aπay signal processing with model adaptation methods. A speech recognition experiment in a real room showed that the proposed method provides better performance than conventional methods...|$|R
40|$|ICSLP 2002 : the 7 th International Conference on Spoken Language Processing, September 16 - 20, 2002, Denver, Colorado, USA. Recognition of distant-talking {{speech is}} {{indispensable}} for <b>self-moving</b> <b>robots</b> or teleconference systems. However, background noise and room reverberations seriously degrade the sound capture quality in real acoustic environments. A microphone array {{is an ideal}} candidate as an effective method for capturing distant-talking speech. AMNOR (Adaptive Microphone-array for NOise Reduction) was proposed an adaptive beamformer for capturing the desired distant signals in noisy environments by Kaneda et al. Although AMNOR has proven itself effective, it could be further improved if we knew the spectrum characteristics of desired distant signals in advance. Therefore, {{in this paper we}} regard speech as a desired distant signal and design AMNOR based on the average speech spectrum for distant-talking speech capture and recognition. As a result of evaluation experiments in real acoustic environments, we could confirm that the ASR (Automatic Speech Recognition) performance was improved 5 ~ 10 % by AMNOR based on average speech spectrum in noisy environments...|$|R
40|$|Distant-talking speech {{recognition}} in noisy environments is indis-pensable for <b>self-moving</b> <b>robots</b> or tele-conference systems. How-ever, background noise and room reverberations seriously degrade the sound-capture quality in real acoustic environments. A micro-phone array {{is an ideal}} candidate as an effective method for captur-ing distant-talking speech. AMNOR (Adaptive Microphone-array for NOise Reduction) was proposed as an adaptive beamformer for capturing the desired distant signals in noisy environments by Kaneda et al. Although the AMNOR has been proven effective, it can be further improved if we know the spectrum characteristics of the desired distant signals in advance. Therefore, we regarded speech as a desired distant signal and designed an AMNOR based on the average speech spectrum. In this paper, we particularly fo-cused {{on the performance of}} AMNOR based on the average speech spectrum for distant-talking speech capture and recognition. As a result of evaluation experiments in real acoustic environments, we confirmed that the ASR (Automatic Speech Recognition) perfor-mance was improved 5 – 10 % by using an AMNOR based on the average speech spectrum in noisy environments. In addition, the proposed AMNOR provides better noise reduction performance than that of conventional AMNOR. 1...|$|R
40|$|Insects can {{estimate}} distance or time-to-contact {{of surrounding}} objects from locomotion-induced {{changes in their}} retinal position and/or size. Freely walking fruit flies (Drosophila melanogaster) use the received mixture of different distance cues to select the nearest objects for subsequent visits. Conventional methods of behavioral analysis fail to elucidate the underlying data extraction. Here we demonstrate first comprehensive solutions of this problem by substituting virtual for real objects; a tracker-controlled 360 ° panorama converts a fruit fly's changing coordinates into object illusions that require the perception of specific cues to appear at preselected distances up to infinity. An application reveals the following: (1) en-route sampling of retinal-image changes accounts for distance discrimination within a surprising range of at least 8 - 80 body lengths (20 - 200 mm). Stereopsis and peering are not involved. (2) Distance from image translation in the expected direction (motion parallax) outweighs distance from image expansion, which accounts for impact-avoiding flight reactions to looming objects. (3) The ability to discriminate distances is robust to artificially delayed updating of image translation. Fruit flies appear to interrelate self-motion and its visual feedback within a surprisingly long time window of about 2 s. The comparative distance inspection practiced in the small fruit fly deserves utilization in <b>self-moving</b> <b>robots...</b>|$|R
40|$|AbstractInsects can {{estimate}} distance or time-to-contact {{of surrounding}} objects from locomotion-induced {{changes in their}} retinal position and/or size [1 – 8]. Freely walking fruit flies (Drosophila melanogaster) use the received mixture of different distance cues to select the nearest objects for subsequent visits [9, 10]. Conventional methods of behavioral analysis fail to elucidate the underlying data extraction. Here we demonstrate first comprehensive solutions of this problem by substituting virtual for real objects; a tracker-controlled 360 ° panorama converts a fruit fly's changing coordinates into object illusions that require the perception of specific cues to appear at preselected distances up to infinity. An application reveals the following: (1) en-route sampling of retinal-image changes accounts for distance discrimination within a surprising range of at least 8 – 80 body lengths (20 – 200 mm). Stereopsis and peering are not involved. (2) Distance from image translation in the expected direction (motion parallax) outweighs distance from image expansion, which accounts for impact-avoiding flight reactions to looming objects. (3) The ability to discriminate distances is robust to artificially delayed updating of image translation. Fruit flies appear to interrelate self-motion and its visual feedback within a surprisingly long time window of about 2 s. The comparative distance inspection practiced in the small fruit fly deserves utilization in <b>self-moving</b> <b>robots...</b>|$|R
40|$|Abstract: We aim for realizing {{a robotic}} system that hands over {{necessary}} objects to a user {{as soon as}} he/she attempts to reach out for it. In this study, we adopt <b>self-moving</b> trays as <b>robots.</b> In order to hand the objects to the user, the system has to predict the user’s hand movements and adapt to them. In this paper, we propose a method to predict durations and final positions of reaching movements of the user's hand. We apply the minimum jerk model to the prediction and estimate parameters of the model by using Levenberg-Marquardt method. A description of the experimental results demonstrates {{the usefulness of the}} method proposed here...|$|R

