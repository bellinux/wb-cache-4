11|19|Public
5000|$|... recclient: {{recognition}} client: it is {{the interface}} between the IVR <b>speech</b> <b>path</b> and the speech recognizing software, the recserver. The recclient can be developed into the IVR software.|$|E
50|$|There {{are some}} of the {{traditional}} telephone equipment manufacturers and smaller enterprises that offer IP-DECT systems, both for residential (single base station/access points), as well as for enterprise usage (multiple base stations/access points) where it is important to cover large areas with a maintained <b>speech</b> <b>path.</b>|$|E
50|$|Hybrids and cancellers are {{sometimes}} combined with echo suppressors. These {{work on the}} assumption that usually only one of the two parties to a conversation is speaking at a given time. The suppressor switches a loss into the inactive <b>speech</b> <b>path,</b> thus enhancing the echo-cancelling effect of the hybrid at the expense of simultaneous two-way conversation.|$|E
30|$|Hence, {{the direct}} <b>path</b> <b>speech</b> {{components}} in the microphones are aligned, but additionally the microphone signals are weighted with {{the magnitude of the}} ATFs similar to the P-MWF approach.|$|R
40|$|This study {{examines}} whether vocal cues {{can be used}} to reliablyinferspeakerhappiness. Two-hundredspeakerswere asked to perform a simple referential communication task and to rate their current emotional state. Arange of vocal cues was traced through the <b>speech</b> chain using <b>path</b> analysis. The results indicate that reported happiness of the speakers and perceived happiness of the listeners were not related. The only vocal cue that mediated between reported and perceived happiness was F 1, and, for the female speakers,pitchrange. Thefindingssuggestthatlistenerstend toover-interpretvocalcuesasmostofthemarenotlikelyto berelatedtospeakers’internalstates...|$|R
5000|$|BU Law's first {{homes were}} 36 Bromfield Street, 18-20 Beacon Street and 10 Ashburton Place. In 1895, the University Trustees {{acquired}} 11 Ashburton Place, which was refurbished and named Isaac Rich Hall {{in honor of}} the third founder of Boston University. The dedication speaker was Oliver Wendell Holmes, Jr. whose historic <b>speech</b> [...] "The <b>Path</b> of the Law" [...] was delivered in 1897. Former United States President William Howard Taft lectured on legal ethics from 1918 until his appointment as Chief Justice of the Supreme Court in 1921.|$|R
50|$|SS5 {{and earlier}} systems used in-band signaling, {{in which the}} call-setup {{information}} was sent by playing special multi-frequency tones into the telephone lines, known as bearer channels. As the bearer channel was directly accessible by users, it was exploited with devices such as the blue box, which played the tones required for call control and routing. As a remedy, SS6 and SS7 implemented out-of-band signaling, carried in a separate signaling channel, thus keeping the <b>speech</b> <b>path</b> separate. SS6 and SS7 {{are referred to as}} common-channel signaling (CCS) protocols, or Common Channel Interoffice Signalling (CCIS) systems.|$|E
50|$|The {{telephone}} exchanges may be connected via E1 or T1 trunks which transport the speech from the calls. These trunks {{are divided into}} 64 kbit/s timeslots, and one timeslot can carry exactly one call. Regardless of what facilities are used to interconnect switches, each circuit between two switches is uniquely identified by a circuit identification code (CIC) that {{is included in the}} ISUP messages. The exchange uses this information along with the received signaling information (especially the called party number) to determine which inbound and outbound circuits should be connected together to provide an end to end <b>speech</b> <b>path.</b>|$|E
50|$|When {{the call}} {{arrives at the}} Visited MSC, the MSRN is used to {{determine}} which of the phones {{in this area is}} being called, that is the MSRN maps back to the IMSI of the original phone number dialled. The MSC pages all the mobile phone masts in the area that the IMSI is registered in order to inform the phone that there is an incoming call for it. If the subscriber answers, a <b>speech</b> <b>path</b> is created through the Visiting MSC and Gateway MSC back to the network of the person making the call, and a normal telephone call follows.|$|E
40|$|With the {{outbreak}} of the economic crisis, which began 5 years ago, libraries have faced the need to reconsider their role and services. The strong reduction of their budgets, particularly evident (but not only) in the countries of the Mediterranean area, caused a significant decline in terms of performance and available resources for users. This paradigm forced the librarians to redesign their activity in order to find new strategies and solutions that ensure an acceptable quality level of services offered by the library. The <b>speech</b> analyzes the <b>path</b> of the Istituto Superiore di Sanità’s Library (the Italian National Health Institute) for obtaining a rationalization of journals subscriptions, strengthening of cooperation strategies and implementation of new projects and services. ...|$|R
40|$|Abstract—In {{the field}} of human robot {{interaction}} (HRI), providing robot with emotions and psychology like human can be useful to achieve natural interaction. Previous HRI research focused {{on issues such as}} gesture recognition, <b>speech</b> recognition and <b>path</b> planning etc. Using the affective information obtained from multimodal interaction can improve the robot’s perception to interaction object and environment, and then can improve the harmoniousness of HRI. The paper puts forward a method of decision-making on multimodal interaction information fusion of service robot based on artificial psychology. The experimental results demonstrate that the proposed method can be efficient and effective in HRI. Index Terms—Artificial psychology, behavior decision-making, multimodal interaction information fusion, service robot. I...|$|R
40|$|The paper {{explains}} {{the effectiveness of}} Active Noise Control (ANC) System based on adaptive filter forreducing noise from a noisy speech. ANC generally reduces low frequency noise and creates a quiet zone. In this paper a feedforward ANC is used for reducing low frequency noise from a noisy speech signal for improving <b>speech</b> quality. Secondary <b>path</b> transfer function has been compensated using FXNLMS (Filtered-X NLMS) algorithm. The proposed system is analyzed and simulated on a digital computer. Theperformance {{of the system is}} evaluated by measuring noise power reduction in the noisy speech by retaining the intelligibility of the speech signal. Simulation results reveal that the method can reduce noise power to a level of - 11. 6 dB...|$|R
50|$|Whereas VoIP {{calls from}} mobile devices are {{controlled}} by IP infrastructure, according to the VCC specifications, calls to and from a cellular phone in the circuit switched domain are also anchored in an IP domain, for example the IP Multimedia Subsystem (IMS). As the handset becomes attached and detached from wireless access points such as WiFi hotspots, a client application in the device provides notifications of the radio conditions to a VCC platform in the network. This allows circuit switched and IP call legs to be originated and terminated such that the <b>speech</b> <b>path</b> is transferred between domains, transparently to the end user.|$|E
40|$|This paper explicates ontological and {{philosophical}} underpinnings of Tibetan Buddhism and its relevancy to human communication and interactions. Taking from the author's personal journey {{and practice of}} Tibetan Buddhism, and her research interests in intercultural communication, this paper provides examples that integrate essential Tibetan Buddhism teachings, cosmology and communication concepts, specifically with regard to symbolism, cultural values, spirituality, and (post) modernity. Essential Tibetan Buddhist teachings such as mind, body and <b>speech,</b> <b>path</b> to enlightenment, rituals, non-dualistic reality and impermanence are depicted to illustrate its relevancy to communication and everyday life in this (post) modern world...|$|E
40|$|Many occupations {{of today}} {{requires}} {{the usage of}} personal preservative equipment such as a mask to protect the employee from dangerous substances or the usage {{of a pair of}} ear-muffs to damp high sound pressure levels. The goal of this Master thesis is to investigate the possibility of placing a microphone for communication purposes inside such a preservative mask as well as the possibility of placing the microphone inside a persons auditory meatus and perform a digital channel equalization on the <b>speech</b> <b>path</b> in question in order to enhance the speech intelligibility. Subjective listening tests indicates that the speech quality and intelligibility can be considerably improved using some of the methods described in this thesis...|$|E
40|$|Whitening {{processing}} methods are {{proposed to improve}} the effectiveness of blind separation of speech sources based on ADF. The proposed methods include preempha-sis, prewhitening, and joint linear prediction of common component of speech sources. The effect of ADF filter lengths on source separation performance was also inves-tigated. Experimental data were generated by convolv-ing TIMIT <b>speech</b> with acoustic <b>path</b> impulse responses measured in real acoustic environment, where microphone-source distances were approximately 2 m and initial target-to-interference ratio was 0 dB. The proposed methods sig-nificantly speeded up convergence rate, increased target-to-interference ratio in separated speech, and improved accu-racy of automatic phone recognition on target speech. The preemphasis and prewhitening methods alone produced large impact on system performance, and combined pre-emphasis with joint prediction yielded the highest phone recognition accuracy. 1...|$|R
40|$|Recent psycholinguistic {{experiments}} {{show that}} acoustic and syntactic aspects of online speech processing {{are influenced by}} visual context through cross-modal influences. During interpretation of speech, visual context seems to steer speech processing and vice versa. We present a real-time multimodal system motivated by these findings that performs early integration of visual contextual information to recognize the most likely word sequences in spoken language utterances. The system first acquires a grammar and a visually grounded lexicon from a "show-and-tell" procedure where the training input consists of camera images consisting of sets of objects paired with verbal object descriptions. Given a new scene, the system generates a dynamic visually-grounded language model and drives a dynamic model of visual attention to steer <b>speech</b> recognition search <b>paths</b> towards more likely word sequences...|$|R
5000|$|The {{prediction}} {{theory of}} law {{was a key}} component of the Oliver Wendell Holmes' jurisprudential philosophy. At its most basic, the theory is an attempted refutation of most previous definitions of the law. Holmes believed that the law should be defined as a prediction, most specifically, a prediction of how the courts behave. His rationale was based on an argument regarding the opinion of a [...] "bad man." [...] Bad men, Holmes argued in his <b>speech</b> [...] "The <b>Path</b> of the Law", care little for ethics or lofty conceptions of natural law; instead they care simply about staying out of jail and avoiding the payment of damages. In Holmes's mind, therefore, it was most useful to define [...] "the law" [...] as a prediction of what will bring punishment or other consequences from a court.|$|R
40|$|OCB 283 means Organ De CommandeB 2 Version 8300 Microprocessor. It is {{the latest}} {{electronics}} digital ISDN type switching technology being imported in INDIA, OCB 283 is a digital switching system which supports a variety of communication needs like basic telephony, ISDN, interface to mobile communication, data communication etc, it is a Digital Switching System (DSS) with single ‘T’ stage switch. A maximum of 2048 PCM’s can be connected. It supports both analog and digital subscriber. Subscriber connected units (CSN) are so designed {{that they can be}} equipped with either analog subscriber or digital subscriber. Or both the cards for analog subscriber and digital are different, but can be equipped in any slot of the shelf. These provide facility to connect <b>speech</b> <b>path</b> from a subscriber’s loop or circuits from an external PCM’s and transfers these speech samples on to selected time slots called voice channels on a LR link (internal PCM). This provides access for Man Machine dialogues for the human operators to interact and command the working of exchange equipments...|$|E
40|$|Approximation of F 0 {{contours}} in Cantonese {{speech is}} investigated. Multiple approximations are examined and evaluated. The modified speech utterances that carry the approximated contours at syllable, word and sentence levels are perceptually examined {{with reference to}} natural speech. It is found that linear approximation can adequately describe all perception-sensitive F 0 variations in Cantonese speech. Each tone contour can be represented by one or two linear movements, and the transition between co-articulated tones can be represented by one linear movement. F 0 contours measured from human speech (observed contours) generally vary to a considerable extent. This research attempts to investigate perception-critical variations in these highly varying contours. In particular, F 0 contours in Cantonese speech are concerned. Cantonese is a major Chinese dialect that is known of being rich in tones. Psychoacoustic findings suggest that human perception has limitations in perceiving pitch movements. This means {{that not all of}} the variations in the observed contours are perceivable. A major problem addressed in this study is to find the simplest acoustic representation of an observed F 0 contour that is adequate to attain comparable perception with the natural speech. F 0 variation in speech is known to carry abundant information, both linguistic and paralinguistic. Its impact on speech communication is thus widely concerned. F 0 variation in speech, being a major super-segmental acoustic feature, has received a lot of attention, particularly from the perspectives of production-acoustics and perception-acoustics. However, it is noted that perception-acoustic knowledge of F 0 variation in association with speech naturalness is quite limited. This is especially the case in the studies of tonal languages, in which most efforts are made on acoustic cues related to tone identification. The feasibility of using linear approximation greatly simplifies the way to understand and interpret F 0 variations in speech processing, by the means of learning the properties of linear movements. Three steps of analysis are carried out on the generated linear approximations. The first one examines the movement slopes in the approximated F 0 contours of isolated syllables, in comparison with the perceptual thresholds found in the psychoacoustic studies. The second analysis is performed over a set of linearly approximated F 0 contours of polysyllabic Cantonese words. The determining attributes of these linear movements, i. e., movement slopes, movement heights and time locations of turning points are analyzed statistically. The last analysis concerns the evaluation of modified F 0 contours. Objective evaluations are compared with perceptual evaluation. These analyses provide knowledge which can improve our understanding on how F 0 variations are processed in <b>speech</b> <b>path.</b> To explore the potentials oflinear approximation in research of speech prosody, a perception-oriented framework of automatic approximation is developed, so as to replace the manual process in the feasibility study. The framework aims to make the process of deriving approximations standardized, consistent and efficient. It is formulated based on the experiences from manual approximations and is also implemented with other perceptual findings. The initial test on polysyllabic words gives promising results. Li, Yujia. Adviser: Tan Lee. Source: Dissertation Abstracts International, Volume: 73 - 04, Section: B, page:. Thesis (Ph. D.) [...] Chinese University of Hong Kong, 2011. Includes bibliographical references (leaves 161 - 170). Electronic reproduction. Hong Kong : Chinese University of Hong Kong, [2012] System requirements: Adobe Acrobat Reader. Available via World Wide Web. Electronic reproduction. [Ann Arbor, MI] : ProQuest Information and Learning, [201 -] System requirements: Adobe Acrobat Reader. Available via World Wide Web. Electronic reproduction. Ann Arbor, MI : ProQuest Information and Learning Company, [200 -] System requirements: Adobe Acrobat Reader. Available via World Wide Web. Abstract also in Chinese; includes Chinese characters...|$|E
5000|$|John Gumperz {{described}} how dialectologists had taken {{issue with the}} dominant approach in historical linguistics that saw linguistic communities as homogeneous and localized entities {{in a way that}} allowed for drawing neat tree diagrams based on the principle of 'descent with modification' and shared innovations. Dialectologists rather realized that dialect traits spread through diffusion and that social factors were decisive in how this happened. They also realized that traits spread as waves from centers and that often several competing varieties would exist in some communities. This insight prompted Gumperz to problematize the notion of the linguistic community as the community that carries a single speech variant, and instead to seek a definition that could encompass heterogeneity. This could be done by focusing on the interactive aspect of language, because interaction in <b>speech</b> is the <b>path</b> along which diffused linguistic traits travel. Gumperz defined the community of speech: ...|$|R
40|$|Abstract—In this study, {{we propose}} a novel {{technique}} for acoustic echo suppression (AES) during speech recognition under barge-in conditions. Conventional AES methods based on spectral subtraction apply fixed weights to the estimated echo path transfer function (EPTF) {{at the current}} signal segment and to the EPTF estimated until the previous time interval. However, the effects of echo path changes should be considered for eliminating the undesired echoes. We describe a new approach that adaptively updates weight parameters in response to abrupt changes in the acoustic environment due to background noises or double-talk. Furthermore, we devised a voice activity detector and an initial time-delay estimator for barge-in speech recognition in communication networks. The initial time delay is estimated using log-spectral distance measure, as well as cross-correlation coefficients. The experimental {{results show that the}} developed techniques can be successfully applied in barge-in speech recognition systems. Keywords—Acoustic echo suppression, barge-in, <b>speech</b> recognition, echo <b>path</b> transfer function, initial delay estimator, voice activity detector. I...|$|R
40|$|In this paper, {{we present}} a novel method for data {{collection}} which produces aligned real-time <b>speech</b> recordings of <b>path</b> descriptions and the corresponding GPS track of that path in the real world. We give a preliminary report on the pilot corpus we have gathered using this method and our initial annotation plans for references to location, orientation and movement within the speech. Using the GPS track of the real path and a GIS database, we plan to annotate spatial references in the corpus with the “ground truth ” of objects in the GIS database, or lat/lon coordinates corresponding to referrant object or location. This annotated data will provide a set of natural language descriptions of paths, locations, orientation and movement {{which can be used}} for training/testing algorithms for understanding spatial language situated in the real world and aided by a GIS database. We also describe an initial annotation tool we are building that uses Google Earth for visualizing and annotating the corpus. 1...|$|R
40|$|When {{asked to}} explain their {{solutions}} to a problem, both adults and children gesture as they talk. These gestures at times convey information that is not conveyed in speech and thus reveal thoughts that are distinct from those revealed in speech. In this study, we use the classic Tower of Hanoi puzzle to validate the claim that gesture and speech taken together can reflect the activation of two cognitive strategies within a single response. The Tower of Hanoi is a well-studied puzzle, known to be most efficiently solved by activating subroutines at theoretically defined choice points. When {{asked to explain}} how they solved the Tower of Hanoi puzzle, both adults and children produced significantly more gesture–speech mismatches—explanations in which <b>speech</b> conveyed one <b>path</b> and gesture another— at these theoretically defined choice points than they produced at non-choice points. Even when the participants did {{not solve the problem}} efficiently, gesture could be used to indicate where the participants were deciding between alternative paths. Gesture can, thus, serve as a useful adjunct to speech when attempting to discover cognitive processes in problem-solving...|$|R
40|$|In {{this first}} year of the project, our work was focused on the problem of {{identifying}} and separating specific sound sources in mixtures. The core of our approach is to use prior knowledge about the sounds in the world, encapsulated in some kind of model, to provide the constraints needed to solve the blind separation problem which is otherwise ill-posed. We have looked at using this approach in a reverberant multi-microphone case. In collaboration with Bhiksha Raj of MERL in Cambridge, we looked at setting the parameters of a filter-and-sum beamformer by doing gradient descent on the match between the separated signals and the constrained speech approximation resulting from the model means corresponding to the states of the best-match path found by a speech recognizer [9]. Beam-former parameters and <b>speech</b> recognizer state <b>path</b> parameters can be alternately re-estimated; we found this process to converge successfully after a few cycles. When just a single voice is present, this process amounts to blind estimation of a dereverberation filter. But we were more interested in the problem of multiple overlapping voices, which requires tw...|$|R
40|$|Recent {{experimental}} {{data suggest that}} spike-timing and membrane dynamics of biological neurons may encode information in a way not achievable using artificial neural networks (ANNs) or traditional machine learning algorithms. Practical applications of spike-coded neural networks include flexible and robust artificial intelligence that simultaneously utilize multiple sensory modalities (as do humans) in situation requiring pattern recognition, <b>speech</b> comprehension, and <b>path</b> planning. In particular, continuous speech recognition systems based on hidden Markov models or ANNs have not demonstrated human-like performance levels. To explore the computational capacity and scalability of multimodal neocortex, we have initiated a series of large-scale computer simulations of integrated auditory (spoken sentences) and visual (corresponding lip reading) perception and learning. We distributed more than 25, 000 excitatory and inhibitory neurons in layered minicolumns, representing primary sensory and association cortex. Cell compartments included realistic mixtures of voltage-sensitive and calcium-dependent potassium channels. Three types of sentences (modified from the TIMIT corpus) were repetitively presented to the network, and layer-specific Hebbian learning was observed using partial depolarization {{to serve as a}} &quot;reward &quot; in association layers integrating auditory and visual spike-coded signals. Preliminary results suggest that computational models of this scope can produce realistic spike encoding o f human speech. ...|$|R
40|$|International audienceBased on the {{theoretical}} background combining {{the hypothesis of}} linguistic relativity (Whorf, 1956), the verb-framed vs. satellite-framed language typology (Talmy, 1985) and the gesture-speech tandem conception (McNeill, 1992), we studied the language and age impact on the way French and Czech natives speak and gesture when describing motion. French and Czech 5 and 10 year old children and adults told just watched animated cartoons containing voluntary motion events (Hickmann, 2006; Gullberg et al., 2008; Özyürek et al., 2008). The analysis of their speech and gesture brought five main results: (1) Children adopt adult language-specific descriptive patterns very early. French children verbalise motion path (up/down) rather than manner (walking/running) while Czech children express both path and manner. (2) Semantic density of motion descriptions grows with age but in language-specific ways. In French, {{an increasing number of}} clauses was observed: manner information appears in a second clause in addition to a path clause. In Czech, a decreasing number of motion expressions was found: path and manner information are conflated into one verb rather than split into two separate expressions. (3) Children produce co-speech iconic gesture less frequently than adults. (4) Concerning the gesture-speech relation, we observed cases of semantic redundancy (path&manner in speech - path&manner in gesture) {{as well as those of}} non-redundancy (path&manner in <b>speech</b> - only <b>path</b> in gesture). (5) Gesture provides more appropriate representation of originally viewed manner than speech. When verbally conveyed manner is less appropriate, gesture completely excludes manner information or makes corrections but almost never displays incorrect manner...|$|R
40|$|A {{study was}} {{designed}} to explore the content and process of marital history storytelling in later life couples and the relationship of storytelling content and process to marital satisfaction. Fifty-six Caucasian couples ranging in age from 47 [...] 83 years were recruited for participation in the study. Each spouse completed the Satisfaction subscale of the Dyadic Adjustment Scale (DAS; Spanier 2 ̆ 6 Cole, 1974) and couples participated in an Oral History Interview (Buehlman, Gottman, 2 ̆ 6 Katz, 1992) designed to elicit their marital history story. Research team members viewed each interview and obtained consensus scores for each couple using the Global Assessment of Relational Functioning (GARF; APA, 1994). All interviews were transcribed and data was segmented into thought units and <b>speeches.</b> The Relationship <b>Path</b> Coding System (RPCS) was designed to code thought units in marital stories according to content and process variables. Content variables included amount of detail, positive storytelling, and chronological reference. Process variables included detailed and general communication sequences, and speech and thought unit differences between spouses for each couple. Investigators hypothesized that couples who utilized greater amounts of detail, positive storytelling and chronological reference in their marital history stories would report higher marital satisfaction on the DAS and would receive higher GARF ratings from observers. It was also hypothesized that couples who utilized more detailed communication sequences and spoke equally in terms of thought units and speeches would report higher marital satisfaction and would receive higher GARF ratings. Regression analyses were used to predict DAS and GARF scores. Results for content variables demonstrated that the greater use of detail in marital stories predicted DAS scores of husbands and GARF ratings but no content variables predicted DAS scores of wives. Results for process variables demonstrated that use of detailed sequences predicted greater marital satisfaction of husbands and wives as well as higher GARF ratings. GARF rating was predicted by equality in the number of speeches from husbands and wives. Implications include increasing the storytelling literature the use of stories as assessment in marital therapy...|$|R
40|$|This work {{presents}} the philosophical {{analysis of the}} three lectures of Mr. Heidegger that make up his topic “The essence of speech”. It becomes the possibility of making an experience out of speech, in the context, the field of speech not about or in reference to speech. This document shares all previous experience by following Jaime Hoyos, SJ, in several seminars about the author. In the first lecture Philology is chosen as the path to access the region of speech; we find that words open, they break and thus make {{possible for us to}} see-listen what is at the bottom. Words are necessary in order for things to exist. But the technical analysis is not enough. Things do not just exist. By existing, they are immersed in the name of their name. Neither is it enough to recognize man as custodian of the image of the being and the word. Therefore, in the second lecture it is introduced a path and a way of walking along that path not from the scientific method but from the philosophical thought. In the vicinity speech-poetry it is the poets who make the experience when they reject the arrogance of owner and provider of the word; when they give themselves unrestrictedly to sing songs, sayings of their conscience that summon them. It is wondering on the regions of <b>speech</b> when the <b>path</b> becomes the inaugural word of speech. The third lecture takes as starting point the fact of having placed us in the path so that we can reach what our essence rightly demands. The vicinity Poetry-Thought summons the essence of speech that is the speech of essence. The proximity, the closeness of all instances of the universe that must be said in order to appear in donation and discovering so that it is woven the universal fabric of the relationships among all regions of the world. The “being” happens where the word breaks...|$|R
40|$|In {{this thesis}} we develop {{algorithms}} for estimating broadband source signals from a mixture using only two sensors. This {{is motivated by}} what is known in the literature as cocktail party effect, the ability of human beings {{to listen to the}} desired source from a mixture of sources with at most two ears. Such a study lets us, achieve {{a better understanding of the}} auditory pathway in the brain and confirmation of the results from physiology and psychoacoustics, have a clue to search for an equivalent structure in the brain which corresponds to the modification which improves the algorithm, come up with a benchmark system to automate the evaluation of the systems like 'surround sound', perform speech recognition in noisy environments. Moreover, it is possible that, what we learn about the replication of the functional units in the brain may help us in replacing those using signal processing units for patients suffering due to the defects in these units. There are two parts to the thesis. In the first part we assume the source signals to be broadband and having strong spectral overlap. Channel is assumed to have a few strong multipaths. We propose an algorithm to estimate all the strong multi-paths from each source to the sensors for more than two sources with measurement from two sensors. Because the channel matrix is not invertible when the number of sources is more than the number of sensors, we make use of the estimates of the multi-path delays for each source to improve the SIR of the sources. In the second part we look at a specific scenario of colored signals and channel being one with a prominent direct <b>path.</b> <b>Speech</b> signals as the sources in a weakly reverberant room and a pair of microphones as the sensors satisfy these conditions. We consider the case with and without a head like structure between the microphones. The head like structure we used was a cubical block of wood. We propose an algorithm for separating sources under such a scenario. We identify the features of speech and the channel which makes it possible for the human auditory system to solve the cocktail party problem. These properties are the same as that satisfied by our model. The algorithm works well in a partly acoustically treated room, (with three persons speaking and two microphones and data acquired using standard PC setup) and not so well in a heavily reverberant scenario. We see that there are similarities in the processing steps involved in the algorithm and what we know of the way our auditory system works, especially so in the regions before the auditory cortex in the auditory pathway. Based on the above experiments we give reasons to support the hypothesis about why all the known organisms need to have only two ears and not more but may have more than two eyes to their advantage. Our results also indicate that part of pitch estimation for individual sources might be occurring in the brain after separating the individual source components. This might solve the dilemma of having to do multi-pitch estimation. Recent works suggest that there are parallel pathways in the brain up to the primary auditory cortex which deal with temporal cue based processing and spatial cue based processing. Our model seem to mimic the pathway which makes use of the spatial cues...|$|R

