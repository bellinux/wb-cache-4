28|87|Public
25|$|There is {{support for}} several methods of {{sharpening}} and blurring images, including the blur and sharpen tool. The unsharp mask tool {{is used to}} sharpen an image selectively— it only sharpens areas of an image that are sufficiently detailed. The Unsharp Mask tool is considered to give more targeted results for photographs than a normal <b>sharpening</b> <b>filter.</b> The Selective Gaussian Blur tool works in a similar way, except it blurs areas of an image with little detail.|$|E
5000|$|For image editing purposes, the {{gradient}} {{is obtained}} from an existing image and modified. Various operators, such as finite difference or Sobel, {{can be used}} to find the gradient of a given image. This gradient can then be manipulated directly to produce a number of different effects when the resulting image is solved for. For example, if the gradient is scaled by a uniform constant it results in a simple <b>sharpening</b> <b>filter.</b> A better <b>sharpening</b> <b>filter</b> can be made by only scaling the gradient in areas deemed important.Other uses include: ...|$|E
50|$|In the <b>Sharpening</b> <b>filter,</b> sliders {{allow for}} the {{sharpening}} of both small details and edges. In expert preview mode, an equalizer {{can be used to}} sharpen specific colors and leave others unchanged. White halos can also be suppressed when sharpening.|$|E
30|$|Previous to SVM training, it {{is crucial}} to preprocess each image where this {{procedure}} involves histogram equalization, filtering using a median filter, followed by the <b>sharpen</b> <b>filter.</b> The median filter is used to reduce image noise, and the <b>sharpen</b> <b>filter</b> enhances the borders.|$|R
50|$|Coarse grain or noise can, like <b>sharpening</b> <b>filters,</b> {{increase}} acutance, hence {{increasing the}} perception of sharpness, even though they degrade the signal-to-noise ratio.|$|R
40|$|In various {{spectrum}} of image processing, images are acquired with low {{variations in the}} intensity level and thus they possess small gradient values. In these cases, it is convenient to apply watershed segmentation on the gradient image, rather than the original image. The most common output of these segmented images is over segmentation and it implies the presence of numerous watershed ridges that do not correspond to the object boundaries of interest. Under this intermingled problematic scenario, {{the role of the}} spatial edge <b>sharpening</b> <b>filters</b> should not be ignored. This research paper deals with the role of various edge <b>sharpening</b> <b>filters</b> and to find the ultimate effect of them on the output image using watershed algorithm is presented...|$|R
50|$|Typically H.263 {{optimized}} is now recommended as {{the default}} quantization matrix with DivX encoding. In simple terms {{this can be}} described as a softening matrix, better suited to lower bit rates. In comparison, the MPEG-2 matrix, can be likened to a <b>sharpening</b> <b>filter,</b> better suited to higher bit rates.|$|E
50|$|There is {{support for}} several methods of {{sharpening}} and blurring images, including the blur and sharpen tool. The unsharp mask tool {{is used to}} sharpen an image selectively — it only sharpens areas of an image that are sufficiently detailed. The Unsharp Mask tool is considered to give more targeted results for photographs than a normal <b>sharpening</b> <b>filter.</b> The Selective Gaussian Blur tool works in a similar way, except it blurs areas of an image with little detail.|$|E
3000|$|The {{simplest}} <b>sharpening</b> <b>filter</b> {{based on}} Gaussian filter with sharpening parameter α> 0 and its generalization include [...]...|$|E
50|$|As {{a feature}} {{enhancement}} algorithm, {{the difference of}} Gaussians can be utilized to increase the visibility of edges and other detail present in a digital image. A wide variety of alternative edge <b>sharpening</b> <b>filters</b> operate by enhancing high frequency detail, but because random noise also has a high spatial frequency, many of these <b>sharpening</b> <b>filters</b> tend to enhance noise, which can be an undesirable artifact. The difference of Gaussians algorithm removes high frequency detail that often includes random noise, rendering this approach {{one of the most}} suitable for processing images {{with a high degree of}} noise. A major drawback to application of the algorithm is an inherent reduction in overall image contrast produced by the operation.|$|R
50|$|Properly, {{perceived}} sharpness is the steepness of transitions (slope), {{which is}} change in output value divided by change in position - hence it is maximized for large changes in output value (as in <b>sharpening</b> <b>filters)</b> and {{small changes in}} position (high resolution).|$|R
40|$|Image {{deblurring}} {{refers to}} procedures {{that attempt to}} reduce the blur amount in a blurry image and grant the degraded image an overall sharpened appearance to obtain a clearer image. The point spread function (PSF) {{is one of the}} essential factors that needed to be calculated, since it will be employed with different types of deblurring algorithms. In this paper, the authors studied various fast deblurring techniques like Richardson – Lucy and its optimized version, Van Cittert and its enhanced version, Landweber, Poisson Map, and Laplacian <b>sharpening</b> <b>filters.</b> Furthermore, altered optimized versions of Landweber and Poisson Map algorithms have been presented. The usage of the PSF in the deblurring algorithm is explained and a comparison between the optimized, the enhanced algorithms and Laplacian <b>sharpening</b> <b>filters</b> {{in terms of the number}} of mathematical operations, number of iterations employed, computation time, deblurring in case of noise existence, and the accuracy measurement using peak signal to noise ratio (PSNR) for each technique is conducted...|$|R
30|$|The {{following}} pseudo-code {{shows how}} SR CPUs work. It implements a 3 × 3 <b>sharpening</b> <b>filter</b> using only one processor. It {{will have a}} poor performance, but it illustrates how the programming is done and how these platforms speed up the development.|$|E
30|$|Current {{research}} in morphological image processing has demonstrated various processing techniques, many which have advantages over equivalent typical image processing techniques. Mahmoud and Marshall applied an edge-guided morphological filter to medical images [23], demonstrating a superior response over other <b>sharpening</b> <b>filter</b> methods {{with respect to}} noise removal, edge sharpening and restoring fine image detail [24].|$|E
30|$|The {{preprocessing}} step aims {{to enhance}} image features along {{a set of}} chosen directions. First, image is grey-scaled and filtered with a <b>sharpening</b> <b>filter</b> (we subtract from the image its local-mean filtered version), thus eliminating the DC component. Tests showed that this solution makes our approach independent {{of the color of}} the scratch, and it helps the next steps in detecting its direction.|$|E
30|$|We have to {{introduce}} <b>sharpening</b> <b>filters</b> {{that will be}} used for studying relationship between SNR and entropy changes in image enhancement. Our interest [15] is focused only on linear infinite impulse response (IIR) filters [17] with radial symmetry in frequency domain whose response can be easily calculated by the Discrete Fourier Transform [18] (DFT). Their advantage is in the side effect suppression of a rectangular grid.|$|R
40|$|Abstract — A scaling {{algorithm}} is proposed {{for the implementation of}} image scaling. The method consists of a bilinear interpolation, a clamp <b>filter,</b> and a <b>sharpening</b> spatial <b>filter.</b> The bilinear interpolation is selected due to its low complexity and high quality. An adaptive technology is used to enhance the effects of clamp and <b>sharpening</b> spatial <b>filters.</b> The clamp and <b>sharpening</b> spatial <b>filters</b> are added as pre-filters to solve the blurring and aliasing effects produced by bilinear interpolation. To reduce memory buffers and computing resources for the very large scale integration (VLSI) implementation, the clamp <b>filter</b> and <b>sharpening</b> spatial <b>filters</b> both convoluted by a 3 × 3 matrix coefficient kernel are combined into a 5 × 5 combined convolution filter, Compared with previous techniques, this paper not only reduces gate counts by more than 46. 6 % and power consumptions by 24. 2 %, but also improves average quality by over 0. 42 dB [...] The bilinear interpolation is simplified by the co-operation and hardware sharing technique to reduce computing resource and hardware costs. The PSNR values for the scaled image are used to signify the overall quality of the scaling Algorithm...|$|R
40|$|ISBN: 978 - 146732533 - 2 International audienceWe present here {{discrete}} {{and continuous}} partial differential equation (PDE) -based methods for image enhancement/ sharpening. Using more robust and spatially adaptive PDEs, multiscale morphological operators that account image features are introduced, and then, {{used to provide}} a discrete enhancement operator, thanks to the former Kramer and Bruckner filter. A novel PDE associated to the introduced enhancement operator is established in 2 D. Both the discrete and PDE-based <b>sharpening</b> <b>filters</b> are illustrated on synthetic, binary and real images...|$|R
40|$|Image scaling {{is widely}} used in many fields. A high quality image scaling is need of the hour. The {{proposed}} scaling algorithm consists of a <b>sharpening</b> <b>filter,</b> clamp filter and bilinear interpolation, to reduce the blurring and the aliasing effect and prefilters {{are added to the}} design. The proposed filter reduces the complexity and the hardware cost. Keyword: sharpening spatial filter, clamp filter, image zooming, reconfigurable calculation unit. 1...|$|E
40|$|Most {{digital cameras}} capture imagery with a color filter array (CFA), {{sampling}} only one color value for each pixel, afterwards interpolating other two missing color values. This interpolation process {{is known as}} demosaicing. In this paper, pipelining concept is introduced to increase the processing speed of the color interpolation algorithm. The proposed algorithm consists of pipelining stages along with an edge detector, an anisotropic weighting model, and a filter based compensator. The edge detector is used to discover the edge information in the images; an anisotropic weighting model is designed to catch more information from the horizontal direction than vertical direction. The filter based compensator includes laplacian and spatial <b>sharpening</b> <b>filter</b> which are used to reduce the blurring effect and improve the edge information. The hardware cost is reduced by using hardware sharing and re-configurable design techniques. When compared with previous low complexity techniques, this paper performs good performance, high speed, low memory requirements, low cost and better quality. Keywords- Color filter array, Color interpolation, edge detector, demosaicing, Laplacian <b>sharpening</b> <b>filter</b> [...] I...|$|E
30|$|For detail enhancement, an LTI {{based on}} the {{difference}} of Gaussian [27] is used. The Gaussian mask sizes in the LTI are 3 [*]×[*] 3 and 5 [*]×[*] 5, which are with pre-calculated coefficients. Since CTI rarely affects image quality, a simple 1 [*]×[*] 3 Laplacian <b>sharpening</b> <b>filter</b> is configured for the CTI. The separable filters are implemented for LTI and the filter size is adjusted. As the filter that is used here is also a vertical filter, a vertical data loading technique was applied.|$|E
5000|$|By {{combining}} these operators one {{can obtain}} algorithms for many image processing tasks, such as feature detection, image segmentation, image <b>sharpening,</b> image <b>filtering,</b> and classification.Along this line one should also look into Continuous Morphology ...|$|R
40|$|Background The first {{commercial}} system for digital radiography {{was introduced in}} 1987, and it has evolved a great deal since then. Currently, {{it is possible to}} enhance images in digital radiography. Objectives The aim {{of this study is to}} evaluate the diagnostic accuracy of image enhancement in direct digital radiography as it relates to interproximal carries assessment. Materials and Methods Following extraction, 50 human teeth were kept in acidic gel (methyl cellulose + acetate buffer PH = 4. 8) for 42 days at 37 °C to cause caries before mounting. Direct digital radiography was then taken. Two <b>filters</b> were used: <b>sharpen</b> and emboss. Three radiologists evaluated the images with two weeks interval. The histologic assessments were gold standard. Additionally, SPSS 20 was used to draw an ROC curve and calculate AUC. Cohen’s kappa and interclass correlation coefficient (ICC) were used to measure intra- and inter-observer reliability. Results For the emboss filter, sensitivity was 95 %, specificity was 100 %, and accuracy was 96 %. For the <b>sharpen</b> <b>filter,</b> sensitivity was 88 %, specificity was 100 %, and accuracy was 90 %. Also, the AUC for the emboss filter was 0. 97, and it was 0. 94 for the <b>sharpen</b> <b>filter.</b> Cohen’s simple kappa was in the range of excellent. Conclusions Using these filters in intra-oral direct digital radiography (especially the emboss filter) can help some clinicians to increase diagnostic accuracy in the assessment of inter proximal caries of posterior teeth...|$|R
40|$|Objectives: The aim of {{this study}} was to assess the {{performance}} of photostimulable storage phosphor (PSP) radiographs with or without using the <b>sharpen</b> <b>filter</b> and cone beam CT (CBCT) for detecting enamel subsurface demineralization. Methods: Enamel subsurface demineralization was induced on one of the approximal surfaces of 120 sound human teeth. Standardized images of all teeth were acquired after the demineralization phase using the Digora (R) Optime (Orion Corp. /Soredex, Helsinki, Finland) (PSP) and the i-CAT (TM) (Imaging Sciences International, Hatfield, PA) (CBCT) systems. Three calibrated observers interpreted the images using a five-point scale (1, demineralization definitely absent; 2, demineralization probably absent; 3, unsure; 4, demineralization probably present; and 5, demineralization definitely present). Diagnoses were validated by cross-sectional microhardness profiling in the test areas of the approximal surfaces. Interobserver agreement was analysed using kappa statistics. Accuracy was estimated by the areas under the receiver operating characteristic curves (4,), which were compared using the Kruskal Wallis test (alpha = 5 %). Results: Interobserver agreement was higher for CBCT (kappa = 0. 7 - 0. 8), followed by sharpen-filtered (kappa = 0. 6 - 0. 7) and original (kappa = 0. 5 - 0. 6) images. CBCT presented the highest accuracy value (A(z) = 0. 897) compared with the original (A(z) = 0. 792) and sharpen-filtered (A(z) = 0. 712) images. However, no statistical differences were observed between the imaging modalities (p = 0. 0794). Conclusions: It can be concluded that PSP radiographs with or without using the <b>sharpen</b> <b>filter</b> and the CBCT images may be useful adjuncts for detecting subtle approximal enamel demineralization...|$|R
40|$|The {{primary focus}} of the {{application}} of image processing to radiography {{is the problem of}} segmentation. The general segmentation problem has been attacked on a broad front [1, 2], and thresholding, in particular, is a popular method [1, 3 - 6]. Unfortunately, geometric unsharpness destroys the crisp edges needed for unambiguous decisions, and this difficulty can be considered a problem in filtering in which the object is to devise a high-pass (<b>sharpening)</b> <b>filter.</b> This approach has been studied for more than 20 years [7 - 13]...|$|E
40|$|A hybrid Super Resolution (SR) {{algorithm}} is proposed {{to deal with the}} Low Resolution (LR) images degraded by Mixed (Gaussian + Impulse) noise. The algorithm adaptively estimates and removes the impulse noise from the input LR images based on edge, geometrical & size characteristics. The fuzzy based impulse noise removal {{algorithm is}} along with adaptive <b>sharpening</b> <b>filter</b> based SR using steering kernel regression are used to obtain a HR image. The experimental results confirm the efficacy of the algorithm for different types of images at various noise densities...|$|E
40|$|A unique space {{oriented}} filer {{is presented}} in order to detect and isolate the cell of a nucleus for applications in cytopathology. A classification method for nuclei is then considered based on {{the application of a}} set of features which includes certain fractal parameters. Segmentation algorithms are considered in which a self-adjustable <b>sharpening</b> <b>filter</b> is designed to enhance object location. Although the methods discussed and the algorithms developed have a range of applications, in this work we focus the engineering of a system for automating a Papanicolaou screening test using standard optical image...|$|E
40|$|Enhancement {{filters are}} {{potentially}} supposed {{to improve the}} diagnostic performance of digital images. Thus, {{the aim of this}} study was to compare the performance of digital radiography with and without enhancement filters for the detection of induced proximal caries lesions. The total sample consisted of 120 sound human teeth (40 premolars, 80 molars). Enamel subsurface demineralization was induced in one of the proximal surfaces of 60 teeth. Standardized radiographs of all teeth were acquired after the demineralization phase using the Digora-Optime (R) system. Four radiologists examined the digital radiographs and applied the following filters provided by the Digora (R) for Windows 2. 6 package: Negative, Sharpen and both (Negative plus Sharpen). Validation of radiographic diagnosis was carried out by Knoop cross-sectional micro-hardness profiling on the proximal surfaces. Intraobserver agreement was estimated using Kappa statistics (k). Sensitivity, specificity and overall accuracy were compared using ANOVA/Tukey test (alpha = 5 %). Intraobserver agreement ranged from good to very good/optimal (k: 0. 65 - 0. 83). Although not statistically significant, the highest sensitivity (0. 68 +/- 0. 22) and accuracy (0. 76 +/- 0. 16) values were observed using the <b>Sharpen</b> <b>filter</b> as opposed to the Negative filter, which presented the lowest performance indices (0. 57 +/- 0. 13 and 0. 70 +/- 0. 10, respectively). Specificity ranged from 0. 84 to 0. 85, considering all imaging modalities (p > 0. 05). Insofar as the <b>Sharpen</b> <b>filter</b> had the highest performance indices, it may be considered a useful adjunct for detecting subtle proximal caries lesions...|$|R
40|$|This paper {{presents}} an automatic {{region of interest}} (ROI) segmentation method for application of watermarking in medical images. The advantage of using this scheme is that the proposed method is robust against different attacks such as median, Wiener, Gaussian, and <b>sharpening</b> <b>filters.</b> In other words, this technique can produce the same result for the ROI before and after these attacks. The proposed algorithm consists of three main parts; suggesting an automatic ROI detection system, evaluating the robustness of the proposed system against numerous attacks, and finally recommending an enhancement part to increase {{the strength of the}} composed system against different attacks. Results obtained from the proposed method demonstrated the promising performance of the method...|$|R
40|$|This paper {{presents}} a double <b>sharpened</b> CIC decimation <b>filter,</b> {{which consists of}} generalized comb filter as first stage, <b>sharpened</b> comb <b>filter</b> as second and third stage. The comb decimation filter at the first stage operates at the input sampling rate, sharpened second stage operates at lower sampling rate as compared to first stage and sharpened third stage operates at lower than {{the first and second}} stages. This reduces the sampling at every stage of the three stage CIC decimation <b>filter.</b> The <b>sharpened</b> second stage produces the narrow passband droop and better stop band alias rejection. This narrow passband droop will be compensated with the help of third stage which is <b>sharpened</b> section. This <b>filter</b> structure is designed in MATLAB Simulink environment and implemented with help of Virtex-V XC 5 VLX 110 T- 3 ff 1136. Device utilization and simulation results are tabulated...|$|R
40|$|Nuclear {{medicine}} (NM) images inherently {{suffer from}} {{large amounts of}} noise and blur. The purpose {{of this research is}} to reduce the noise and blur while maintaining image integrity for improved diagnosis. The proposed solution is to increase image quality after the standard pre- and post-processing undertaken by a gamma camera system. Mean Field Annealing (MFA) is the image processing technique used in this research. It is a computational iterative technique that makes use of the Point Spread Function (PSF) and the noise associated with the NM image. MFA is applied to NM images with the objective of reducing noise while not compromising edge integrity. Using a <b>sharpening</b> <b>filter</b> as a post-processing technique (after MFA) yields image enhancement of planar NM images. Comment: 4 page...|$|E
40|$|There {{are several}} {{inherent}} {{difficulties in the}} existing firearm identification algorithms, include requiring the physical interpretation and time consuming. Therefore, {{the aim of this}} study is to propose a robust algorithm for a firearm identification based on extracting a set of informative features from the segmented region of interest (ROI) using the simulated noisy center-firing pin impression images. The proposed algorithm comprises Laplacian <b>sharpening</b> <b>filter,</b> clustering-based threshold selection, unweighted least square estimator, and segment a square ROI from the noisy images. A total of 250 simulated noisy images collected from five different pistols of the same make, model and caliber are used to evaluate the robustness of the proposed algorithm. This study found that the proposed algorithm is able to perform the identical task on the noisy images with noise levels as high as 70...|$|E
40|$|The paper {{presents}} FPGA {{implementation of}} a spectral sharpening process suitable for speech enhancement and noise reduction algorithms for digital hearing aids. Booth and Booth Wallace multiplier is used for implementing digital signal processing algorithms in hearing aids. VHDL simulation results confirm that Booth Wallace multiplier is hardware efficient and performs faster than Booth’s multiplier. Booth Wallace multiplier consumes 40 % less power compared to Booth multiplier. A novel digital hearing aid using spectral <b>sharpening</b> <b>filter</b> employing booth Wallace multiplier is proposed. The results reveal that the hardware requirement for implementing hearing aid using Booth Wallace multiplier is less when {{compared with that of}} a booth multiplier. Furthermore it is also demonstrated that digital hearing aid using Booth Wallace multiplier consumes less power and performs better in terms of speed...|$|E
40|$|Abstract. <b>Sharpening</b> <b>filters</b> {{increase}} {{the depth of}} digital images by adding a fraction of their gradient. This portion is tuned by a coefficient which is usually selected according to rules of thumb or subjective evaluation. This paper proposes statistical measures for designing such parameter in a nearly automatic way, avoiding subjective evaluations. The proposed measures {{are based on the}} distance between sharpened and equalized images, which serve as an early reference, and test statistics of uniformity of the luminance histogram. More complex measures, based on the trade-off between skewness and kurtosis, and variance and autocovariance of the sharpened image, are also studied. Numerical applications to various kinds of digital images show that the proposed measures provide similar and acceptable solutions...|$|R
40|$|This paper {{presents}} a novel technique for embedding a binary logo watermark into video frames. The proposed scheme is an imperceptible and a robust hybrid video watermarking scheme. PCA {{is applied to}} each block of the two bands (LL – HH) which result from Discrete Wavelet transform of every video frame. The watermark is embedded into the principal components of the LL blocks and HH blocks in different ways. Combining the two transforms improved {{the performance of the}} watermark algorithm. The scheme is tested by applying various attacks. Experimental results show no visible difference between the watermarked frames and the original frames and show the robustness against a wide range of attacks such as MPEG coding, JPEG coding, Gaussian noise addition, histogram equalization, gamma correction, contrast adjustment, <b>sharpen</b> <b>filter,</b> cropping, resizing, and rotation. Key words...|$|R
30|$|There {{are several}} {{strategies}} in nonlinear filtering for improving results related to edges and details preserving {{in the case}} of environments with high percentage of impulse noise. A particular interesting strategy is described in [7] with <b>sharpening</b> <b>filters.</b> However, these filters are applied only to the part of available pixels in attempt to preserve edges and similar details. For high-density salt-and-pepper noise, it is difficult to have large number of uncorrupted pixels within the local neighborhood to perform such operations. An alternative solution for this problem is described in [6, 11], with variants of the total variation filters used in order to stabilize output of the filtering scheme preserving edges and details. Again, these strategies are not robust enough to a high-density salt-and-pepper noise. In addition, they require elaborate algorithm for implementation meaning they are relatively inefficient.|$|R
