22|6|Public
50|$|The {{proof of}} the theorem is {{commonly}} illustrated for the setup of Bell tests in which two observers Alice and Bob perform local observations on a common bipartite system, and uses the <b>statistical</b> <b>machinery</b> of quantum mechanics, namely density states and quantum operations.|$|E
5000|$|Punjab {{is among}} very few States of Indian Union {{which has its}} <b>statistical</b> <b>machinery</b> at the sub {{district}} i.e. block level. One statistical personnel known as [...] "investigator" [...] is posted at the block level in the State who collect the basic information on over 250 items {{from each of the}} village. This data is used for grass root level planning and other decision in the State. As on 31 March 2016 there are 146 blocks and over 12154 villages.|$|E
50|$|From {{a modern}} perspective, Elo's {{simplifying}} assumptions are not necessary because computing power is inexpensive and widely available. Moreover, {{even within the}} simplified model, more efficient estimation techniques are well known. Several people, most notably Mark Glickman, have proposed using more sophisticated <b>statistical</b> <b>machinery</b> to estimate the same variables. On the other hand, the computational simplicity of the Elo system {{has proven to be}} one of its greatest assets. With the aid of a pocket calculator, an informed chess competitor can calculate to within one point what his next officially published rating will be, which helps promote a perception that the ratings are fair.|$|E
25|$|As such, the {{traditional}} <b>statistical</b> inference <b>machinery</b> {{will not work}} with these more complicated models, and in this case, {{it is common to}} instead use a forward simulation-based approach.|$|R
50|$|As {{discussed}} above, it {{is possible}} to directly infer parameters of simple compartmental epidemiological models, such as SIR models, from sequence data by looking at genealogical patterns.Additionally, general patterns of geographic movement can be inferred from sequence data, but these inferences do not involve an explicit model of transmission dynamics between infected individuals.For more complicated epidemiological models, such as those involving cross-immunity, age structure of host contact rates, seasonality, or multiple host populations with different life history traits, it is often impossible to analytically predict genealogical patterns from epidemiological parameters.As such, the traditional <b>statistical</b> inference <b>machinery</b> will not work with these more complicated models, and in this case, it is common to instead use a forward simulation-based approach.|$|R
40|$|Imitative {{learning}} has recently piqued {{the interest of}} various fields including neuroscience, cognitive science and robotics. In computational behavior modeling and development, it promises an accessible framework for rapidly forming behavior models without tedious supervision or reinforcement. Given the availability of lowcost wearable sensors, the robustness of real-time perception algorithms and the feasibility of archiving large amounts of audio-visual data, {{it is possible to}} unobtrusively archive the daily activities of a human teacher and his responses to external stimuli. We combine this data acquisition/representation process with <b>statistical</b> learning <b>machinery</b> (hidden Markov models) as well as discriminative estimation algorithms to form a behavioral model of a human teacher directly from the data set. The resulting system learns audio-visual interactive behavior from the human and his environment to produce an interactive autonomous agent. The agent subsequently exhibits simple audio-visual behaviors that appear coupled to real-world test stimuli...|$|R
40|$|In {{this article}} I’m {{going to argue}} two points. First, content {{analysis}} should {{not be seen as}} fundamentally different to any other type of quantitative method available to political scientists. And second, that political methodologists need to integrate content analysis into their current <b>statistical</b> <b>machinery...</b>|$|E
40|$|Abstract—Association {{discovery}} {{is a fundamental}} data mining task. The primary statistical approach to association discovery between variables is log-linear analysis. Classical approaches to log-linear analysis do not scale beyond about ten variables. We develop an efficient approach to log-linear analysis that scales to hundreds of variables by melding the classical <b>statistical</b> <b>machinery</b> of log-linear analysis with advanced data mining techniques from association discovery and graphical modeling...|$|E
30|$|This {{paper has}} {{investigated}} the asymptotic {{behavior of the}} MLE for gfBm. However, we just estimated these parameters separately. What {{should be done in}} the near future is to estimate all the unknown parameters (including the Hurst index) simultaneously. Thus, the problem of joint parameter estimation for stochastic models driven by fBm is interesting and attractive. We also expect the need for these methods and for improvements in the <b>statistical</b> <b>machinery</b> that is available to practitioners to grow further as the financial industry continues to expand and data sets become richer. The field is therefore of growing importance for both theorists and practitioners.|$|E
40|$|This {{bachelor}} {{thesis is}} focused on constructional design of workshop handling platform which is driven by linear hydraulic motor. First the types of gripping of linear hydraulic motor are described. Next part contains drafted solutions of handling platform construction. Important part of thesis is composed of <b>statistical</b> computing of <b>machinery</b> parts and their strenght computations within the frame of designed proportions. Afterwards the bearings are chosen according to thickness of shaft and the hydraulic motor and the diagram of hydraulic circuit are designed. At last controlling computations are done and assembly and weldment drawings are created...|$|R
40|$|Statistical {{prediction}} {{models can}} be an effective technique to identify vulnerable components in large software projects. Two aspects of vulnerability prediction models have a profound impact on their performance: 1) the features (i. e., the characteristics of the software) that are used as predictors and 2) the way those features are used in the setup of the <b>statistical</b> learning <b>machinery.</b> In a previous work, we compared models based on two different types of features: software metrics and term frequencies (text mining features). In this paper, we broaden the set of models we compare by investigating an array of techniques for the manipulation of said features. These techniques fall under the umbrella of dimensionality reduction and have the potential to improve the ability of a prediction model to localize vulnerabilities. We explore the role of dimensionality reduction through a series of cross-validation and cross-project prediction experiments. Our results show {{that in the case of}} software metrics, a dimensionality reduction technique based on confirmatory factor analysis provided an advantage when performing cross-project prediction, yielding the best F-measure for the predictions in five out of six cases. In the case of text mining, feature selection can make the prediction computationally faster, but no dimensionality reduction technique provided any other notable advantage...|$|R
40|$|Large scale {{clinical}} trials and population based research studies collect {{huge amounts of}} neuroimaging data. Machine learning classifiers can potentially use these data to train models that diagnose brain related diseases from individual brain scans. In this dissertation we address two distinct challenges that beset a wider adoption of these tools for diagnostic purposes. ^ The first challenge that besets the neuroimaging based disease classification {{is the lack of}} a <b>statistical</b> inference <b>machinery</b> for highlighting brain regions that contribute significantly to the classifier decisions. In this dissertation, we address this challenge by developing an analytic framework for interpreting support vector machine (SVM) models used for neuroimaging based diagnosis of psychiatric disease. To do this we first note that permutation testing using SVM model components provides a reliable inference mechanism for model interpretation. Then we derive our analysis framework by showing that under certain assumptions, the permutation based null distributions associated with SVM model components can be approximated analytically using the data themselves. Inference based on these analytic null distributions is validated on real and simulated data. p-Values computed from our analysis can accurately identify anatomical features that differentiate groups used for classifier training. Since the majority of clinical and research communities are trained in understanding statistical p-values rather than machine learning techniques like the SVM, we hope that this work will lead to a better understanding SVM classifiers and motivate a wider adoption of SVM models for image based diagnosis of psychiatric disease. ^ A second deficiency of learning based neuroimaging diagnostics is that they implicitly assume that, 2 ̆ 7 a single homogeneous pattern of brain changes drives population wide phenotypic differences 2 ̆ 7. In reality {{it is more likely that}} multiple patterns of brain deficits drive the complexities observed in the clinical presentation of most diseases. Understanding this heterogeneity may allow us to build better classifiers for identifying such diseases from individual brain scans. However, analytic tools to explore this heterogeneity are missing. With this in view, we present in this dissertation, a framework for exploring disease heterogeneity using population neuroimaging data. The approach we present first computes difference images by comparing matched cases and controls and then clusters these differences. The cluster centers define a set of deficit patterns that differentiates the two groups. By allowing for more than one pattern of difference between two populations, our framework makes a radical departure from traditional tools used for neuroimaging group analyses. We hope that this leads to a better understanding of the processes that lead to disease and also that it ultimately leads to improved image based disease classifiers. ...|$|R
40|$|Abstract Background Assessing {{agreement}} in method comparison studies depends on two fundamentally important components; validity (the between method agreement) and reproducibility (the within method agreement). The Bland-Altman limits of agreement technique {{is one of}} the favoured approaches in medical literature for assessing between method validity. However, few researchers have adopted this approach for the assessment of both validity and reproducibility. This may be partly {{due to a lack of}} a flexible, easily implemented and readily available <b>statistical</b> <b>machinery</b> to analyse repeated measurement method comparison data. Methods Adopting the Bland-Altman framework, but using Bayesian methods, we present this <b>statistical</b> <b>machinery.</b> Two multivariate hierarchical Bayesian models are advocated, one which assumes that the underlying values for subjects remain static (exchangeable replicates) and one which assumes that the underlying values can change between repeated measurements (non-exchangeable replicates). Results We illustrate the salient advantages of these models using two separate datasets that have been previously analysed and presented; (i) assuming static underlying values analysed using both multivariate hierarchical Bayesian models, and (ii) assuming each subject's underlying value is continually changing quantity and analysed using the non-exchangeable replicate multivariate hierarchical Bayesian model. Conclusion These easily implemented models allow for full parameter uncertainty, simultaneous method comparison, handle unbalanced or missing data, and provide estimates and credible regions for all the parameters of interest. Computer code for the analyses in also presented, provided in the freely available and currently cost free software package WinBUGS. </p...|$|E
40|$|A {{critical}} objective {{for many}} empirical studies is a thorough evaluation of both substantive importance and statistical significance. Feminist economists have critiqued neoclassical economics studies for an excessive focus on <b>statistical</b> <b>machinery</b> {{at the expense}} of substantive issues. Drawing from the ongoing debate about the rhetoric of economic inquiry and significance tests, this paper examines approaches for presenting empirical results effectively to ensure that the analysis is accurate, meaningful, and relevant for the conceptual and empirical context. To that end, it demonstrates several measurement issues that affect the interpretation of economic significance and are commonly overlooked in empirical studies. This paper provides guidelines for clearly communicating two distinct aspects of “significance ” in empirical research, using prose, tables, and charts based on OLS, logit, and probit regression results. These guidelines are illustrated with samples of ineffective writing annotated to show weaknesses, followed by concrete examples and explanations of improved presentation...|$|E
40|$|The {{multiscale}} autoregressive (MAR) framework {{was introduced}} to support the development of optimal multiscale statistical signal processing. Its power resides in the fast and flexible algorithms to which it leads. While the MAR framework was originally motivated by wavelets, the link between these two worlds has been previously established only in the simple case of the Haar wavelet. The first contribution {{of this paper is}} to provide a unification of the MAR framework and all compactly supported wavelets as well as a new view of the multiscale stochastic realization problem. The second contribution {{of this paper is to}} develop wavelet-based approximate internal MAR models for stochastic processes. This will be done by incorporating a powerful synthesis algorithm for the detail coefficients which complements the usual wavelet reconstruction algorithm for the scaling coefficients. Taking advantage of the <b>statistical</b> <b>machinery</b> provided by the MAR framework, we will illustrate the applicati [...] ...|$|E
40|$|Lattice QCD {{can be used}} {{to study}} the QCD phase diagram at finite {{temperature}} and zero density. Non-zero density is implemented on the lattice using the chemical potential however this gives a possibility of a complex probability which means the <b>statistical</b> <b>machinery</b> normally used breaks down. Switching the gauge group from three colour to two colour avoids this problem. Simulating two colour QCD allows for investigation into the thermodynamic phase transitions of a theory which is similar to three colour QCD. Confinement and asymptotic freedom are observed, however the definitions for hadrons differ. To study the thermodynamics we use the derivative method which requires the determination of the Karsch coefficients. Previous studies determined these coefficients perturbatively however this led to negative values for pressure. Non-perturbative studies have proved difficult due to the computational cost and accuracy required. We attempt to determine these coefficients non-perturbatively and review possible improvements in setup and calculation. We finish with thermodynamic results for the pressure, energy density and trace anomaly...|$|E
40|$|The {{challenges}} of modeling students ’ performance in computer based interactive assessments include accounting for multiple aspects {{of knowledge and}} skill that arise in different situations and the conditional dependencies among multiple aspects of performance. This paper describes a Bayesian approach to modeling and estimating cognitive models in such situations, {{both in terms of}} <b>statistical</b> <b>machinery</b> and actual instrument development. The method taps the knowledge of experts to provide initial estimates for the probabilistic relationships among the variables in a multivariate latent variable model and refines these estimates using Markov chain Monte Carlo procedures. This process is illustrated in the context of NetPASS, a computer based interactive assessment in the domain of computer networking. We describe a parameterization of the relationships in NetPASS via an ordered polytomous item response model and detail the updating of the model with observed data via Bayesian statistical procedures ultimately being provided by Markov chain Monte Carlo estimation...|$|E
40|$|Abstract: 2 ̆ 2 The most {{commonly}} used algorithms for spatial data searches such as k-nearest-neighbor and spherical range queries {{are based on a}} class of data structures we call space-partitioning trees, which have remained the pragmatic method of choice due to their ability to often empirically provide sub-linear efficiency in reported dimensionalities in the tens and occasionally beyond, in contrast to methods designed for worst-case optimality. Despite long-standing practical interest in a more realistic runtime analysis of such methods, particularly in the high-dimensional case demanded by many modern applications, little further progress has been made since the seminal expected-time analysis of 1977. One fundamental {{reason for this is that}} algorithm analysis has not, to date, provided examples of analyses which link algorithmic runtime to probabilistic properties of the input distribution. This paper introduces some basic <b>statistical</b> <b>machinery</b> for making this link, and thereby presents initial steps toward providing a statistically principled framework for distribution-dependent runtime analysis of space-partitioning-based algorithms, with an emphasis on providing explanations for their observed behavior in high-dimensional spaces. 2 ̆...|$|E
40|$|Article dans revue scientifique avec comité de lecture. The {{multiscale}} autoregressive (MAR) framework {{was introduced}} to support the development of optimal multiscale statistical signal processing. Its power resides in the fast and flexible algorithms to which it leads. While the MAR framework was originally motivated by wavelets, the link between these two worlds has been previously established only in the simple case of the Haar wavelet. The first contribution {{of this paper is}} to provide a complete unification of the MAR framework and all compactly supported wavelets as well as a new view of the multiscale stochastic realization problem. The second contribution {{of this paper is to}} develop wavelet-based approximate MAR models for stochastic processes. This will be done by incorporating a powerful synthesis algorithm for the detail coefficients which compliments the usual wavelet synthesis algorithm for the scaling coefficients. Taking advantage of the <b>statistical</b> <b>machinery</b> provided by the MAR framework, we will illustrate the application of our models to sample-path generation and estimation from noisy, irregular, and sparse measurements...|$|E
40|$|Empirical {{research}} in the field of electronic commerce has been growing fast due to the availability of rich, high-quality data. Ecommerce data originates from many different behavioral, social, or economic processes and interactions online which have not been observable and measurable in the offline world. This data-rich environment allows for the questioning of existing theories and the uncovering of new phenomena. However, eCommerce data, and the new research questions associated with this data, are often not supported by classic <b>statistical</b> <b>machinery.</b> New dependency structures arise due to factors such as online competition and user interaction. In this paper, we discuss three key aspects of eCommerce data: eCommerce process dynamics, competition between processes, and user networks. Each data-structure raises new challenges for data representation, visualization, and modeling, and we describe each of them in detail. We also present three case studies that showcase the various statistical challenges and present some solutions. Key words and phrases: Online auctions; electronic commerce; dynamics; competition; networks; loyalty; functional data analysis; social network analysis; nonparametric methods; smoothing; forecasting. 1...|$|E
40|$|A {{single user}} activity, such as {{planning}} a conference trip, typically involves multiple actions. Although these actions may involve several applications, the central point of coordination for any particular activity is usually email. Previous work on email activity management {{has focused on}} clustering emails by activity. Dredze et al. [3] accomplished this by combining supervised classifiers based on document similarity, authors and recipients, and thread information. In this paper, we take a different approach and present an unsupervised framework for email activity clustering. We use the same information sources as Dredze et al. —namely, document similarity, message recipients and authors, and thread information—but combine them to form an unsupervised, non-parametric Bayesian user model. This approach enables email activities to be inferred without any user input. Inferring activities from a user’s mailbox adapts the model to that user. We next describe the <b>statistical</b> <b>machinery</b> that forms {{the basis of our}} user model, and explain how several email properties may be incorporated into the model. We evaluate this approach using the same data as Dredze et al., showing that our model does well at clustering emails by activity...|$|E
40|$|The {{search for}} general {{mechanisms}} of community assembly {{is a major}} focus of community ecology. The common practice so far has been to examine alternative assembly theories using dichotomist approaches of the form neutrality versus niche, or compensatory dynamics versus environmental forcing. In reality, all these mechanisms will be operating, albeit with different strengths. While there have been different approaches to community structure and dynamics, including neutrality and niche differentiation, less work has gone into separating out the temporal variation in species abundances into relative contributions from different components. Here we use a refined <b>statistical</b> <b>machinery</b> to decompose temporal fluctuations in species abundances into contributions from environmental stochasticity and inter-/intraspecific interactions, to see which ones dominate. We apply the methodology to community data from a range of taxa. Our results show that communities are largely driven by environmental fluctuations, and that member populations are, to different extents, regulated through intraspecific interactions, the effects of interspecific interactions remaining broadly minor. By decomposing the temporal variation in this way, {{we have been able to}} show directly what has been previously inferred indirectly: compensatory dynamics are in fact largely outweighed by environmental forcing, and the latter tends to synchronize the population dynamics...|$|E
40|$|There is an ever-expanding body of {{biological}} data, growing {{in size and}} complexity, out- stripping the capabilities of standard database tools or traditional analysis techniques. Such examples include molecular dynamics simulations, drug-target interactions, gene regulatory networks, and high-throughput imaging. Large-scale acquisition and curation biological data has already yielded results {{in the form of}} lower costs for genome sequencing and greater cov- erage in databases such as GenBank, and is viewed as the future of biocuration. The “big data” philosophy and its associated paradigms and frameworks have the potential to uncover solutions to problems otherwise intractable with more traditional investigative techniques. Here, we focus on two biological systems whose data form large, undirected graphs. First, we develop a quantitative model of ciliary motion phenotypes, using spectral graph methods for unsupervised latent pattern discovery. Second, we apply similar techniques to identify a mapping between physiochemical structure and odor percept in human olfaction. In both cases, we experienced computational bottlenecks in our <b>statistical</b> <b>machinery,</b> necessitating {{the creation of a new}} analysis framework. At the core of this framework is a distributed hierarchical eigensolver, which we compare directly to other popular solvers. We demon- strate its essential role in enabling the discovery of novel ciliary motion phenotypes and in identifying physiochemical-perceptual associations...|$|E
40|$|The {{study of}} {{collective}} or group-level movement patterns can provide insight regarding the socio-ecological interface, {{the evolution of}} self-organization and mechanisms of inter-individual information exchange. The suite of drivers influencing coordinated movement trajectories occur across scales, resulting from regular annual, seasonal and circadian stimuli and irregular intra- or interspecific interactions and environmental encounters acting on individuals. Here, we promote a conceptual framework with an associated <b>statistical</b> <b>machinery</b> to quantify the type and degree of synchrony, spanning absence to complete, in pairwise movements. The application of this framework offers a foundation for detailed understanding of collective movement patterns and causes. We emphasize the use of Fourier and wavelet approaches of measuring pairwise movement properties and illustrate them with simulations that contain different types of complexity in individual movement, correlation in movement stochasticity, and transience in movement relatedness. Application of this framework to movements of free-ranging African elephants (Loxodonta africana) provides unique insight on the separate roles of sociality and ecology in the fission–fusion society of these animals, quantitatively characterizing the types of bonding that occur {{at different levels of}} social relatedness in a movement context. We conclude with a discussion about expanding this framework to the context of larger (greater than three) groups towards understanding broader population and interspecific collective movement patterns and their mechanisms...|$|E
40|$|In {{this short}} note I {{speculate}} about the various {{ways in which the}} study of neurological aspects of decision making could be fruitful for economic modelling. In the numerous discussions on neuroeconomics, commentators react not only to the substance of neuroeconomics but also to the hype that surrounds it. The hype reflects a genuine excitement. Neuroscience is a frontier field which is at least as interesting as economics. Moreover, it has the scientific and high-tech cachet that economists find irresistible. Still, it is a hype and as such it tends to trigger critical responses. I agree with many of the criticisms. Like Harrison (2008), I am taken aback by the small samples that characterize neuroeconomics studies, the methodology of pooling subjects, and the heavy <b>statistical</b> <b>machinery</b> required to make raw fMRI data amenable to analysis. Like Rubinstein (2006), I observe that so far, neuroeconomics has consisted of attempts to find neural correlates to existing behavioural concepts (for instance, Sanfey et al. (2003) correlate subjects ’ reaction to insolently low offers in the Ultimatum Game with activity in a brain area which researchers have learned to associate with a sense of disgust), and as such may be great news for neuroscience but is hardly newsworthy for economics. And like Gul and Pesendorfer (2005), I am not convinced that the mere fact that decision making takes place in the brain implies that neuroscience is necessarily relevant for economic analysis. For example, Roth (2006) emphasizes the role of repugnance in limiting market transactions. Clearly, repugnance has its brain-level correlate. But as Roth points out, th...|$|E
40|$|For the {{improvement}} of genetic material suitable for on farm use under low-input conditions, participatory and formal plant breeding strategies are frequently presented as competing options. A common frame of reference to phrase mechanisms and purposes related to breeding strategies will facilitate clearer descriptions of {{similarities and differences between}} participatory plant breeding and formal plant breeding. In this paper an attempt is made to develop such a common framework by means of a statistically inspired language that acknowledges the importance of both on farm trials and research centre trials as sources of information for on farm genetic improvement. Key concepts are the genetic correlation between environments, and the heterogeneity of phenotypic and genetic variance over environments. Classic selection response theory is taken as the starting point for the comparison of selection trials (on farm and research centre) with respect to the expected genetic improvement in a target environment (low-input farms). The variance-covariance parameters that form the input for selection response comparisons traditionally come from a mixed model fit to multi-environment trial data. In this paper we propose a recently developed class of mixed models, namely multiplicative mixed models, also called factor-analytic models, for modelling genetic variances and covariances (correlations). Mixed multiplicative models allow genetic variances and covariances to be dependent on quantitative descriptors of the environment, and confer a high flexibility in the choice of variance-covariance structure, without requiring the estimation of a prohibitively high number of parameters. As a result detailed considerations regarding selection response comparisons are facilitated. ne <b>statistical</b> <b>machinery</b> involved is illustrated on an example data set consisting of barley trials from the International Center for Agricultural Research in the Dry Areas (ICARDA). Analysis of the example data showed that participatory plant breeding and formal plant breeding are better interpreted as providing complementary rather than competing information...|$|E
40|$|Indiana University-Purdue University Indianapolis (IUPUI) Clinical Decision Support {{is one of}} {{the only}} aspects of health {{information}} technology that has demonstrated decreased costs and increased quality in healthcare delivery, yet it is extremely expensive and time-consuming to create, maintain, and localize. Consequently, a majority of health care systems do not utilize it, and even when it is available it is frequently incorrect. Therefore it is important to look beyond traditional guideline-based decision support to more readily available resources in order to bring this technology into widespread use. This study proposes that the wisdom of physicians within a practice is a rich, untapped knowledge source that can be harnessed for this purpose. I hypothesize and demonstrate that this wisdom is reflected by order entry data well enough to partially reconstruct the knowledge behind treatment decisions. Automated reconstruction of such knowledge is used to produce dynamic, situation-specific treatment suggestions, in a similar vein to Amazon. com shopping recommendations. This approach is appealing because: it is local (so it reflects local standards); it fits into workflow more readily than the traditional local-wisdom approach (viz. the curbside consult); and, it is free (the data are already being captured). This work develops several new machine-learning algorithms and novel applications of existing algorithms, focusing on an approach called Bayesian network structure learning. I develop: an approach to produce dynamic, rank-ordered situation-specific treatment menus from treatment data; <b>statistical</b> <b>machinery</b> to evaluate their accuracy using retrospective simulation; a novel algorithm which is an order of magnitude faster than existing algorithms; a principled approach to choosing smaller, more optimal, domain-specific subsystems; and a new method to discover temporal relationships in the data. The result is a comprehensive approach for extracting knowledge from order-entry data to produce situation-specific treatment menus, which is applied to order-entry data at Wishard Hospital in Indianapolis. Retrospective simulations find that, in a large variety of clinical situations, a short menu will contain the clinicians' desired next actions. A prospective survey additionally finds that such menus aid physicians in writing order sets (in completeness and speed). This study demonstrates that clinical knowledge can be successfully extracted from treatment data for decision support...|$|E
40|$|Markov chains provide {{excellent}} {{statistical models}} for studying many natural phenomena that evolve with time. One particular class of continuous-time Markov chain, called birth–death processes, {{can be used}} for modelling population dynamics in fields such as ecology and microbiology. The challenge for the practitioner when fitting these models is to take measurements of a population size over time in order to estimate the model parameters, such as per capita birth and death rates. In many biological contexts, it is impractical to follow the fate of each individual in a population continuously in time, so the researcher is often limited to a fixed number of measurements of population size over the duration of the study. We show that, for a simple birth–death process, with positive Malthusian growth rate, subject to common practical constraints, there is an optimal schedule for measuring the population size that minimises the expected confidence region of the parameter estimates. Throughout our exposition of the optimal experimental design, we compare it to a simpler equidistant design, where the population is sampled at regular intervals. This is an experimental design worthy of comparison since it can represent a much simpler design to implement in practice. In order to find optimal experimental designs for our population model, we make use of a combination of useful <b>statistical</b> <b>machinery.</b> Firstly, we use a Gaussian diffusion approximation of the underlying discrete-state Markov process, which allows us to obtain analytical expressions for Fisher’s information matrix (FIM), which is crucial to optimising the experimental design. We also make use of the cross-entropy method of stochastic optimisation for the purpose of maximising the determinant of FIM to obtain the optimal experimental designs. Our results show that the optimal schedule devised by others for a simple model of population growth without death can be extended, for large populations, to the two-parameter model that incorporates both birth and death. For the simple birth–death process, we find that the likelihood surface is also problematic and poses serious problems for point estimation and easily defining confidence regions. A Bayesian approach to inference is proposed as a way in which these problems could be circumvented...|$|E

