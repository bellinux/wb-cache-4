16|792|Public
50|$|<b>Sampling</b> <b>risk</b> {{represents}} {{the possibility that}} an auditor's conclusion based on a sample {{is different from that}} reached if the entire population were subject to audit procedure. The auditor may conclude that material misstatements exist, when in fact they do not; or material misstatements do not exist but in fact they do exist. Auditors can lower the <b>sampling</b> <b>risk</b> by increasing the sampling size.|$|E
5000|$|In addition, {{the auditor}} is {{concerned}} with <b>sampling</b> <b>risk</b> and its relationship with controls. Two types of sample risk/control risks are: ...|$|E
5000|$|Auditors {{must often}} make {{professional}} judgments in assessing <b>sampling</b> <b>risk.</b> When testing samples the auditor is {{primarily concerned with}} two aspects of sampling risk: ...|$|E
3000|$|... 2 [*]×[*]K {{significantly}} negative. Second, with 31 small-and-medium-sized {{commercial banks}} as <b>sample,</b> <b>RISK</b> [...]...|$|R
50|$|Any {{risks that}} will affect the testing process must be listed along with the mitigation. By documenting a risk, its {{occurrence}} can be anticipated well ahead of time. Proactive action may be taken {{to prevent it from}} occurring, or to mitigate its damage. <b>Sample</b> <b>risks</b> are dependency of completion of coding done by sub-contractors, or capability of testing tools.|$|R
40|$|We compare various {{approaches}} for {{the determination of}} finite <b>sample</b> <b>risks</b> of one-dimensional location M-estimators on convex-contamination- and total-variation-type neighborhoods. As risks we consider MSE and certain over- / undershooting probabilities like in Huber (1968) and Rieder (1980). These methods include (numerically) exact formulae (via FFT, Kohl et al. (2005)), Edgeworth expansions, saddlepoint approximations, an approach by Fraiman et al. (2001), first-, second- and third order asymptotics as well as simulations...|$|R
50|$|Although {{there are}} many types of risks {{associated}} with the audit process, each type primarily {{has an effect on}} the overall audit engagement. The effects produced by <b>sampling</b> <b>risk</b> generally can increase audit risk, the risk that an entity's financial statements will contain a material misstatement, though given an unqualified ('clean') audit report. <b>Sampling</b> <b>risk</b> can also increase detection risk which suggests the possibility that an auditor will not find material misstatements relating to the financial statements through substantive tests and analysis.|$|E
5000|$|<b>Sampling</b> <b>risk</b> {{is one of}} {{the many}} types of risks an auditor may face when {{performing}} the necessary procedure of audit sampling. Audit sampling exists because of the impractical and costly effects of examining all or 100% of a client's records or books. As a result, a [...] "sample" [...] of a client's accounts are examined.Due to the negative effects produced by <b>sampling</b> <b>risk,</b> an auditor may have to perform additional procedures which in turn can impact the overall efficiency of the audit.|$|E
40|$|Based on {{traditional}} archival appraisal theories and existing appraisal/selection policies in libraries, archives, museums, social science and science data centers, {{this paper presents}} a generic appraisal/selection framework for digital curation and discusses {{how it can be}} implemented. The framework includes three selection methods: statistical <b>sampling,</b> <b>risk</b> analysis and appraisal. Details about the appraisal method are illustrated, including the objects of appraisal, appraisal criteria and appraisal decisions...|$|E
40|$|This article {{evaluates the}} {{relative}} significance of research published in 16 risk, insurance, and actuarial journals {{by examining the}} frequency of citations in these risk, insurance, and actuarial journals and 16 of the leading finance journals during the years 1996 through 2000. First, the article provides {{the frequency with which}} each <b>sample</b> <b>risk,</b> insurance, and actuarial journal cites itself and the other sample journals so as to communicate the degree to which each journal's published research has had an influence on the other sample journals. Then the article divides the 16 journals into two groups: (1) the risk and insurance journal group, and (2) the actuarial journal group, and ranks them within their group based on their total number of citations, including and excluding self-citations. A ranking within each group is based on the journals' influence on a per article published basis. Finally, this study observes and reports on the most frequently cited articles from the <b>sample</b> <b>risk,</b> insurance, and actuarial journals. Copyright 2003 The Journal of Risk and Insurance. ...|$|R
40|$|The {{application}} of websites to offer free samples {{of products and}} services to consumers is widespread. Online approaches, {{including the use of}} “freebie” sites and social media, offer consumers free product samples by completing a request form. However, this study demonstrates that a significant number of requests are ignored or unfulfilled. Companies who fail to provide the requested <b>samples,</b> <b>risk</b> undermining the potential benefits of offering freebies online by creating customer dissatisfaction...|$|R
40|$|The authors {{previously}} reported that genetic {{variation in the}} gene coding for nicastrin (NCSTN) modified risk for familial early-onset Alzheimer disease (AD) in a Dutch population-based <b>sample.</b> <b>Risk</b> was highest in patients without an APOE epsilon 4 allele. Here, they evaluated if NCSTN polymorphisms increased risk of AD in the eastern Finnish population. A significant difference in one haplotype was observed in AD patients without the APOE epsilon 4 allele. status: publishe...|$|R
40|$|Labor market {{intermediaries}} play {{an important}} role in turnover in many labor markets. This paper analyzes one class of such inter- mediaries, namely, search firms. We first model the hiring decision of the firm in both succession and replacement planning. We show that employers will, in equilibrium, use search firms to find new hires even where the search firms have no technological advantage in search. This can be interpreted as being due to the search firms' ability to diversify away <b>sampling</b> <b>risk...</b>|$|E
40|$|Labor-market {{intermediaries}} play {{an important}} role in turnover in many labor markets. This paper analyzes one class of such intermediaries, namely, search firms. The authors first model the hiring decision of the firm in both succession and replacement planning. They show that employers will, in equilibrium, use search firms to find new hires even where the search firms have no technological advantage in search. This can be interpreted as being due to the search firms' ability to diversify away <b>sampling</b> <b>risk.</b> Copyright 1987 by University of Chicago Press. ...|$|E
40|$|This paper {{intends to}} look into the {{application}} of statistical sampling techniques to auditing. As voluminous data extensive testing, the conventional techniques may not be adequate and competence to the statistical method. The user of die data especially the financial statements require more stringent and concrete evidence to evaluate the status of their investment. The objectivity and calculated <b>sampling</b> <b>risk</b> of the statistical method assure a higher degree of confidence in auditor’s opinion and a more defensible results. Somehow on the contrary, the Bayesian approach which suggests the auditor's subjective estimate to the population be involved in the evaluation is discussed...|$|E
30|$|Following these bioassays with {{drinking}} water-relevant single substances and these biological tests with real water <b>samples,</b> <b>risk</b> assessment criteria will be derived for the HRIV concept {{in the fourth}} and final phase of the project. Thus, criteria {{will be based on}} hierarchical test strategies for hazard-based exposure scenarios of anthropogenic trace substances in drinking water. In addition to the resulting guidance document, further information material and specific training programs will be developed and provided for public use.|$|R
40|$|International audienceIn this paper, {{we develop}} a novel {{approach}} {{to the problem of}} learning sparse representations in the context of fused sparsity and unknown noise level. We propose an algorithm, termed Scaled Fused Dantzig Selector (SFDS), that accomplishes the aforementioned learning task by means of a second-order cone program. A special emphasize is put on the particular instance of fused sparsity corresponding to the learning in presence of outliers. We establish finite <b>sample</b> <b>risk</b> bounds and carry out an experimental evaluation on both synthetic and real data...|$|R
40|$|In {{this paper}} we {{investigate}} the finite <b>sample</b> <b>risk</b> performance of feasible generalised least squares estimators applied in models with serially correlated error terms. The risk {{functions of the}} ordinary least squares, generalised least squares and feasible generalised least squares estimators are derived under the asymmetric Linear-Exponential loss function. A numerical evaluation using simulation is {{used to compare the}} risk functions. Our numerical results show that the relative risk gains of the feasible generalised least squares estimators over the ordinary least squares estimator increases with higher loss asymmetry, particularly for larger serial correlation coefficients. [URL]...|$|R
40|$|This article {{approaches}} the general issue of diminishing the evidence investigation space in audit activities, {{by means of}} sampling techniques, given that in the instance of a significant data volume an exhaustive examination of the assessed popula¬tion is not possible and/or effective. The general perspective of the presentation involves dealing with <b>sampling</b> <b>risk,</b> in essence, the risk that a selected sample may not be representative for the overall population, in correlation with the audit risk model and with the component parts of this model (inherent risk, control risk and non detection risk) and highlights the inter-conditionings between these two models...|$|E
40|$|Because {{relevant}} {{historical data}} for farms are inevitably sparse, most risk programming studies rely on few observations of uncertain crop and livestock returns. We show {{the instability of}} model solutions with few observations and discuss how to use available information to derive an appropriate multivariate distribution function that can be sampled for a more complete representation of the possible risks in risk-based models. For the particular example of a Norwegian mixed livestock and crop farm, the solution is shown to be unstable with few states of nature producing a risky solution that may be appreciably sub-optimal. However, the risk of picking a sub-optimal plan declines with increases in number of states of nature generated by Latin hypercube <b>sampling.</b> <b>Risk</b> programming States of nature Sparse data Kernel smoothing Latin hypercube sampling...|$|E
40|$|When hedging {{longevity}} risk with standardized contracts, the hedger needs to calibrate the hedge carefully {{so that it}} can effectively reduce the risk. In this article, we present a calibration method that is based on matching mortality rate sensitivities. Specifically, we introduce a measure called key q-duration, which allows us to estimate the price sensitivity of a life-contingent liability to each portion of the underlying mortality curve. Given this measure, one can easily construct a longevity hedge with a handful of mortality forwards. Our empirical results indicate that using key q-durations, a hedge effectiveness of more than 97 % can be attained with only five mortaltiy forwards. We also investigate other important issues that are related to standardized longevity hedges, including the adaptation needed for hedging multiple birth cohorts, and the quantification of <b>sampling</b> <b>risk</b> and basis risk...|$|E
40|$|The {{stability}} {{of risk factors}} in the UK stock market is examined over time and across stock <b>samples.</b> <b>Risk</b> factors were identified by principal components analysis (PCA) on 22 small samples of stocks, over short time horizons. Stability across samples was investigated by a second-stage PCA, to identify commonalities (referred to as 'superfactors') among the estimated principal components. Stability over time was examined by estimating the predictability of superfactor loadings and superfactor scores over 20 years. Only one stable market-wide risk factor emerged. Other components seemed to be sample-specific and unstable across time. ...|$|R
40|$|Objective: Identifying {{risk factors}} for the {{occurrence}} of falls in 					hospitalized adult patients. Method: Integrative review carried out 					in the databases of LILACS, SciELO, MEDLINE and Web of Science, including 					articles published between 1989 and 2012. Results: Seventy-one 					articles {{were included in the}} final <b>sample.</b> <b>Risk</b> factors for falls presented in 					this review were related to patients (intrinsic), the hospital setting and the 					working process of health professionals, especially in nursing (extrinsic). 						Conclusion: The systematic screening of {{risk factors for}} falls 					was identified as a contributing factor to the reduction of this injury, helping 					the non-occurrence of this event that, despite being preventable, can have 					serious consequences including death. &# 8233...|$|R
40|$|Background As {{climate change}} {{increases}} {{the frequency and}} intensity of extreme heat events research-ers and public health officials must work towards understanding the causes and outcomes of heat-related morbidity and mortality. While {{there have been many}} studies on both heat-related illness (HRI), there are fewer on heat-related morbidity than on heat-related mortality. Objective To identify individual and environmental risk factors for hospitalizations and document pat-terns of household cooling. Methods We performed a pooled cross-sectional analysis of secondary U. S. data, the Nationwide In-patient <b>Sample.</b> <b>Risk</b> ratios were calculated from multivariable models to identify risk factors for hospitalizations. Hierarchical modeling was also employed to identify relationships be-tween individual and hospital level predictors of hospitalizations. Patterns of air conditionin...|$|R
40|$|In {{this paper}} we discuss {{statistical}} properties and convergence of the Stochastic Dual Dynamic Programming (SDDP) method applied to multistage linear stochastic programming problems. We assume that the underline data process is stagewise independent and consider the framework where at first a random sample from the original (true) distribution is generated and consequently the SDDP algorithm {{is applied to the}} constructed Sample Average Approximation (SAA) problem. Then we proceed to analysis of the SDDP solutions of the SAA problem and their relations to solutions of the "true" problem. Finally we discuss an extension of the SDDP method to a risk averse formulation of multistage stochastic programs. We argue that the computational complexity of the corresponding SDDP algorithm is almost the same as in the risk neutral case. Stochastic programming Stochastic Dual Dynamic Programming algorithm Sample Average Approximation method Monte Carlo <b>sampling</b> <b>Risk</b> averse optimization...|$|E
40|$|The {{bacterium}} Staphylococcus aureus {{has been}} an important human ailment for centuries, and with the overuse of antibiotics, methicillin resistant Staphylococcus aureus (MRSA) has emerged as a deadly, costly pathogen worldwide. Healthy carriers can become sick or can spread MRSA without symptoms. The amount of asymptomatic colonization among healthy college students and risk factors for colonization by MRSA are not well understood. According to the epidemiologic triangle model, the host (students who take antibiotics or have a history of skin infections), the infectious agent (MRSA) and the environment (direct contact with people, animals, or objects that may harbor MRSA) all {{play an important role in}} this disease. This study explored MRSA colonization rates among healthy students at a community college and explored the possibility that students exposed to sources of MRSA might have a higher colonization rate. Using a cross-sectional quantitative design with stratified <b>sampling,</b> <b>risk</b> factors to include student 2 ̆ 7 s discipline, gender, race, work, and leisure exposure were surveyed. In tandem, Mannitol Salt Agar and MRSA Select Agar were inoculated from nasal swabs to identify students colonized by MRSA. The data were analyzed using contingency tables and Chi Squares. Significant risk factors identified included students who had a major that involved touching shared equipment and/or those who were in majors such as nursing, students who had close contact with animals, and students who had a skin infection. The implication for positive social change include improved awareness of MRSA colonization and risk factors which can lead to better prevention strategies and increased awareness among the student population...|$|E
40|$|Introduction. ST {{waveform}} {{analysis was}} introduced to reduce metabolic acidosis at birth and avoid unnecessary operative deliveries relative to conventional cardiotocography. Our objective was to quantify the efficacy of ST waveform analysis vs. cardiotocography and assess {{the quality of the}} evidence using the Grading of Recommendations Assessment, Development and Evaluation tool. Material and methods. We identified randomized controlled trials through systematic literature searches and assessed included studies for risk of bias. Meta-analyses were performed, calculating pooled risk ratio or peto odds ratio. We performed post hoc trial sequential analyses for selected outcomes to assess the risk of false-positive results and the need for additional studies. Results. Six randomized controlled trials were included in the meta-analysis. ST waveform analysis was not associated with a reduction in operative deliveries due to fetal distress, but we observed a significantly lower rate of metabolic acidosis (peto odds ratio 0. 64; 95 % confidence interval 0. 46 – 0. 88). Accordingly, 401 women need to be monitored with ST waveform analysis to prevent one case of metabolic acidosis. No statistically significant effects were observed in other fetal or neonatal outcomes, except from fetal blood <b>sampling</b> (<b>risk</b> ratio 0. 59; 95 % confidence interval 0. 45 – 0. 79) and a minor {{reduction in the number of}} operative vaginal deliveries for all indications (risk ratio 0. 92; 95 % confidence interval 0. 86 – 0. 99). The quality of the evidence was high to moderate. Conclusions. Absolute effects of ST waveform analysis were minor, and the clinical significance of the observed reduction in metabolic acidosis is questioned. There is not enough evidence to justify the use of ST waveform analysis in contemporary obstetrics. Abbreviations: CTG, cardiotocography; GRADE, The Grading of Recommendations Assessment, Development and Evaluation; MeSH, Medical Subject Headings; NICU, neonatal intensive care unit; OR, odds ratio; RCT, randomized controlled trial; RR, risk ratio; STAN, ST waveform analysis; TSA, trial sequential analyses...|$|E
40|$|This paper {{proposes a}} new discriminative {{training}} method, called minimum <b>sample</b> <b>risk</b> (MSR), of estimating parameters of language models for text input. While most existing discriminative training methods use a loss function {{that can be}} optimized easily but approaches only approximately to the objective of minimum error rate, MSR minimizes the training error directly using a heuristic training procedure. Evaluations {{on the task of}} Japanese text input show that MSR can handle a large number of features and training samples; it significantly outperforms a regular trigram model trained using maximum likelihood estimation, and it also outperforms the two widely applied discriminative methods, the boosting and the perceptron algorithms, by a small but statistically significant margin. ...|$|R
40|$|The bachelor's {{thesis is}} aimed to {{influences}} of <b>sampled</b> <b>risk</b> faktors in seniors falls. At first I focused on various definitions and kinds of falls. I {{also sought to}} clarify the most frequent and less common, often neglected risk factors of falls. I placed emphases on negative influences of seniors malnutrition, methods of risk eldery people identification and ways of prevention against falls in our country and abroad. Empirical research is based on observational study which was carried out to measure risk factors in 100 women that were older than 70 years. I also carried out the falls screening of these women. Patients were hospitalized to the General university hospital in Prague...|$|R
40|$|Objective: The {{accurate}} {{prediction of}} {{type 1 diabetes}} (T 1 D) is essential for appropriately identifying prevention trial participants. Moreover, improved prediction accuracy might ultimately result in an earlier diagnosis. Thus, we have developed a risk score for the prediction of T 1 D. Research Design and Methods: Diabetes Prevention Trial-Type 1 (DPT- 1) participants, islet-cell autoantibody (ICA) positive relatives of T 1 D patients (n= 670), were randomly divided into development and validation <b>samples.</b> <b>Risk</b> score values were calculated for the validation sample from development sample model coefficients obtained through forward stepwise proportional hazards regression. Results: A risk score based on a model including log-BMI, age, log-fasting C-peptide, and post-challenge glucose and C-peptide sums from 2 -hr oral glucose tolerance tests (OGTTs) {{was derived from the}} development <b>sample.</b> The baseline <b>risk</b> score strongly predicted T 1 D in the validation sample (chi-square= 82. 3, p< 0. 001). Its strength of prediction was almost the same as (chi-square= 83. 3) a risk score additionally dependent on a decreased first-phase insulin respons...|$|R
40|$|The Murray Smelter Site (2 ̆ 2 the Site 2 ̆ 2) {{is located}} in the city of Murray, Utah, in Salt Lake County as {{illustrated}} on Figure 1. The Site includes the former operational areas of the Murray Smelter and adjacent Germania Smelter which are referred to as the 2 ̆ 2 on-facility 2 ̆ 2 area., as well as surrounding residential and commercial areas where airborne emissions from the smelters impacted the environment or where contamination in shallow ground water may be transported in the future. These surrounding areas are referred to as the 2 ̆ 2 off-facility 2 ̆ 2 area. The on-facility area is approximately 142 acres. Its boundaries are 5300 South Street to the south, State Street to the east, Little Cottonwood Creek to the north, and the west set of Union Pacific railroad tracks to the west. The off-facility area is approximately 30 acres {{to the west of the}} on-facility area, approximately 106 acres south and southeast of the on-facility area., an: a small area between 5200 South Street and Little Cottonwood Creek to the east of the on-facility area. The west portion of the off-facility area is bounded by Little Cottonwood Creek to the north, 300 West Street to the west, 5300 South Street to the south, and the on-facility boundary to the east. The south/southwest portion is bounded by 5300 South Street to the north and Wilson Avenue to the south. The off-facility boundaries were determined by EPA based on the results of air dispersion modeling performed in November, 1994. The purpose of the modeling was to identify the area that potentially would have received the greatest amount of deposition resulting from lead and arsenic emissions from the Murray Smelter during its operating period. For environmental <b>Sampling,</b> <b>risk</b> assessment, and risk management purposes. the Site was divided into smaller areas to represent realistic areas of humar and ecological exposure. The 142 acre on-facility area was divided into eleven 2 ̆ 2 exposure units 2 ̆ 2 (EU 2 ̆ 7 s) and the 136 acre off-facility area was divided into eight 2 ̆ 2 initial study zones 2 ̆ 2 (ISZ 2 ̆ 7 s). The riparian area along Little Cottonwood Creek was delineated as the ecological study area...|$|E
40|$|The aim of {{this paper}} is to provide an {{application}} of the Shapley Value to decompose financial portfolio <b>risk.</b> Decomposing the <b>sample</b> covariance <b>risk</b> measure yields relative measures, which enable securities of a portfolio to be classified according to risk scales. Decomposition, Risk, Shapley, Volatility...|$|R
40|$|This paper {{considers}} {{the estimation of}} the error variance after a pre-test of an interval restriction on the coefficients. We derive the exact finite <b>sample</b> <b>risks</b> of the interval restricted and pre-test estimators of the error variance, and examine the risk properties of the estimators to model misspecification through the omission of relevant regressors. It is found that the pre-test estimator performs better than the interval restricted estimator {{in terms of the}} risk properties in a large region of the parameter space; moreover, its risk performance is more robust with respect to the degrees of model misspecification. Furthermore, we propose a bootstrap procedure for estimating the risks of the estimators, to overcome the difficulty of computing the exact risks. Error variance Interval restriction Pre-test Model misspecification...|$|R
40|$|Standard use of Cox's {{regression}} model and other relative risk {{regression model}}s for censored survival data requires collection of covariate information on all individuals under study even when {{only a small}} fraction of them die or get diseased. For such situations <b>risk</b> set <b>sampling</b> designs offer useful alternatives. For cohort data, methods based on martingale residuals are useful for assessing the fit of a model. Here we introduce grouped martingale residual processes for <b>sampled</b> <b>risk</b> set data, and show that plots of these processes provide a useful tool for checking model-fit. Further we study the large sample properties of the grouped martingale residual processes, and use these to derive a formal goodness-of-fit test to go along with the plots. The methods are illustrated using data on lung cancer deaths in a cohort of uranium miners...|$|R
40|$|This {{paper is}} an {{empirical}} {{study on the}} performance of different discriminative approaches to reranking the N-best hypotheses output from a large vocabulary continuous speech recognizer (LVCSR). Four algorithms, namely perceptron, boosting, ranking support vector machine (SVM) and minimum <b>sample</b> <b>risk</b> (MSR), are compared in terms of domain adaptation, generalization and time efficiency. In our experiments on Mandarin dictation speech, we found that for domain adaptation, perceptron performs the best; for generalization, boosting performs the best. The best result on a domain-specific test set is achieved by the perceptron algorithm. A relative character error rate (CER) reduction of 11 % over the baseline was obtained. The best result on a general test set is 3. 4 % CER reduction over the baseline, achieved by the boosting algorithm. 1...|$|R
50|$|PED also {{enables the}} use of {{electron}} diffraction to investigate beam-sensitive organic materials. Because PED can reproduce symmetric zone axis diffraction patterns even when the zone axis is not perfectly aligned, it enables information to be extracted from sensitive <b>samples</b> without <b>risking</b> overexposure during a time-intensive orientation of the sample.|$|R
