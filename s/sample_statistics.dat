265|747|Public
25|$|L-estimators {{can also}} be used as {{statistics}} in their own right – for example, the median is a measure of location, and the IQR is a measure of dispersion. In these cases, the <b>sample</b> <b>statistics</b> can act as estimators of their own expected value; for example, the sample median is an estimator of the population median.|$|E
25|$|Sampling {{theory is}} part of the {{mathematical}} discipline of probability theory. Probability is used in mathematical statistics to study the sampling distributions of <b>sample</b> <b>statistics</b> and, more generally, the properties of statistical procedures. The use of any statistical method is valid when the system or population under consideration satisfies the assumptions of the method.|$|E
2500|$|Rather {{than using}} <b>sample</b> <b>statistics</b> as estimators of {{population}} parameters and applying confidence intervals to these estimates, one considers [...] "the next sample" [...] as itself a statistic, and computes its sampling distribution.|$|E
40|$|A Berry-Esseen theorem for {{the rate}} of {{convergence}} of general nonlinear multivariate <b>sampling</b> <b>statistics</b> with normal limit distribution is derived via a multivariate extension of Stein's method. The result generalizes in particular previous results of Bolthausen for one-dimensional linear rank statistics, one-dimensional results of van Zwet and Friedrich for general functions of independent random elements and provides convergence bounds for general multivariate <b>sampling</b> <b>statistics</b> without restrictions on the sampling proportions...|$|R
25|$|When full {{census data}} cannot be collected, statisticians collect sample data by {{developing}} specific experiment designs and survey <b>samples.</b> <b>Statistics</b> itself also provides tools for prediction and forecasting through statistical models.|$|R
50|$|DOE results, when {{discovered}} accurately with DOE software, {{strengthen the}} capability to discern truths about sample populations being tested: see <b>Sampling</b> (<b>statistics).</b> Statisticians describe stronger multifactorial DOE methods as being more “robust”: see Experimental design.|$|R
2500|$|Note that in {{the formula}} for the {{predictive}} confidence interval no mention is made of the unobservable parameters μ and σ of population {{mean and standard deviation}} – the observed <b>sample</b> <b>statistics</b> [...] and [...] of sample mean and standard deviation are used, and what is estimated is the outcome of future samples.|$|E
5000|$|Some {{commonly}} used symbols for <b>sample</b> <b>statistics</b> are given below: ...|$|E
50|$|Asymptotic theory ("asymptotics") is used {{in several}} {{mathematical}} sciences. In statistics, asymptotic theory provides limiting approximations of the probability distribution of <b>sample</b> <b>statistics,</b> such as the likelihood ratio statistic and the expected value of the deviance. Asymptotic theory {{does not provide a}} method of evaluating the finite-sample distributions of <b>sample</b> <b>statistics,</b> however. Non-asymptotic bounds are provided by methods of approximation theory.|$|E
50|$|The {{probabilistic}} approach is considered {{as the most}} rigorous approach to uncertainty {{analysis in engineering design}} due to its consistency with the theory of decision analysis. Its cornerstone is the calculation of probability density functions for <b>sampling</b> <b>statistics.</b> This can be performed rigorously for random variables that are obtainable as transformations of Gaussian variables, leading to exact confidence intervals.|$|R
30|$|As {{the use of}} MVA with ToF-SIMS data increases, it is {{important}} that users understand the methods used and assumptions made when processing their data. Proper experimental design and <b>sampling</b> <b>statistics</b> are required to ensure the results acquired are statistically significant. The choice of MVA method should also be considered according to the goals of the analysis and the results obtained should be verified by going back to the raw data.|$|R
40|$|Natural gas {{production}} and consumption data have been drifting apart. Production should equal consumption plus increases or decreases in storage, but sampling and estimation errors typically result in slight discrepancies. Seeing these gaps rise, the Energy Information Administration (EIA) implemented a new methodology {{with the release of}} February's production data that should ensure greater accuracy. Estimates for the prior 12 months were revised as well. Natural gas; <b>Sampling</b> (<b>Statistics)</b> ...|$|R
5000|$|... the {{jackknife}} procedure, used {{to estimate}} biases of <b>sample</b> <b>statistics</b> and to estimate variances, and ...|$|E
50|$|Given the {{difficulty}} in specifying exact distributions of <b>sample</b> <b>statistics,</b> many methods {{have been developed for}} approximating these.|$|E
50|$|Using the {{scientific}} method of falsification, the probability value that the sample statistic is sufficiently different from the null-model than {{can be explained by}} chance alone is given prior to the test. Most statisticians set the prior probability value at 0.05 or 0.1, which means if the <b>sample</b> <b>statistics</b> diverge from the parametric model more than 5 (or 10) times out of 100, then the discrepancy is unlikely to be explained by chance alone and the null-hypothesis is rejected. Statistical models provide exact outcomes of the parametric and estimates of the <b>sample</b> <b>statistics.</b> Hence, the burden of proof rests in the <b>sample</b> <b>statistics</b> that provide estimates of a statistical model. Statistical models contain the mathematical proof of the parametric values and their probability distributions.|$|E
40|$|Data {{augmentation}} and Gibbs sampling are two closely related, sampling-based {{approaches to}} the calculation of posterior moments. The fact that each produces a sample whose constituents are neither independent nor identically distributed complicates the assessment of convergence and numerical accuracy of the approximations to the expected value of functions of interest under the posterior. In this paper methods for spectral analysis are used to evaluate numerical accuracy formally and construct diagnostics for convergence. These methods are illustrated in the normal linear model with informative priors, and in the Tobit-censored regression model. <b>Sampling</b> (<b>Statistics)</b> ...|$|R
40|$|UNLABELLED: GeneRecon is a {{tool for}} fine-scale {{association}} mapping using a coalescence model. GeneRecon takes as input case-control data from phased or unphased SNP and microsatellite genotypes. The posterior distribution of disease locus position is obtained by Metropolis-Hastings sampling in the state space of genealogies. Input format, search strategy and the <b>sampled</b> <b>statistics</b> can be configured through the Guile Scheme programming language embedded in GeneRecon, making GeneRecon highly configurable. AVAILABILITY: The source code for GeneRecon, written in C++ and Scheme, is available under the GNU General Public License (GPL) at [URL] CONTACT: mailund@birc. au. dk...|$|R
40|$|Packet {{sampling}} {{has become}} a practical and indispensable means to measure flow statistics. Recent {{studies have demonstrated that}} analyzing traffic patterns is crucial in detecting network anomalies. We {{may not be able to}} infer the original traffic patterns correctly from the <b>sampled</b> flow <b>statistics</b> because <b>sampling</b> process wipes out a lot of information about small flows, which play a vital role in determining the characteristics of traffic patterns. In this paper, we first show an example of how the sampling process wipes out the original statistics using measured data. Then, we show empirical examples indicating that the original traffic pattern cannot be inferred correctly even if we use a statistical inference method for incomplete data, i. e., the EM algorithm, for <b>sampled</b> flow <b>statistics.</b> Finally, we show that additional information about the original flow statistics, the number of unsampled flows, is helpful in tracking the change in original traffic patterns using <b>sampled</b> flow <b>statistics.</b> 1...|$|R
5000|$|... (4) Polynomial {{least squares}} {{processing}} produces deterministic moments (analogous to mechanical moments), {{which may be}} considered as moments of <b>sample</b> <b>statistics,</b> but not of statistical moments.|$|E
5000|$|Estimating the {{precision}} of <b>sample</b> <b>statistics</b> (medians, variances, percentiles) by using subsets of available data (jackknifing) or drawing randomly with replacement from a set of data points (bootstrapping) ...|$|E
5000|$|Rather {{than using}} <b>sample</b> <b>statistics</b> as estimators of {{population}} parameters and applying confidence intervals to these estimates, one considers [...] "the next sample" [...] as itself a statistic, and computes its sampling distribution.|$|E
40|$|High <b>statistics</b> <b>sample</b> of kaon decays {{collected}} by NA 48 -CERN experiment allowed two independent {{measurements of the}} pion scattering length. Methods and Results are discussedHigh <b>statistics</b> <b>sample</b> of kaon decays {{collected by}} NA 48 -CERN experiment allowed two independent measurements of the pion scattering length. Methods and Results are discusse...|$|R
5000|$|... #Subtitle level 4: Order <b>statistics</b> <b>sampled</b> from {{a uniform}} {{distribution}} ...|$|R
40|$|By {{incorporating}} the Tropical Rain Measuring Mission (TRMM) satellite orbital information into the geodesic {{version of the}} Colorado State University General Circulation Model (CSU GCM), {{we are able to}} fly a satellite in the GCM, and sample the simulated atmosphere {{in the same way as}} the TRMM sensors sample the real atmosphere. The TRMM <b>sampling</b> <b>statistics</b> of precipitation and radiative fluxes at annual, intraseasonal, monthly-mean and composited diurnal time scales are evaluated by comparing the satellite-sampled against fully-sampled simulated atmospheres. This information provides a valuable guidance for efficient usage of TRMM data and future satellite mission planning...|$|R
50|$|In the {{practical}} setting the population values are typically not known {{and must be}} estimated from <b>sample</b> <b>statistics.</b> The several versions of effect sizes based on means differ with respect to which statistics are used.|$|E
50|$|The {{exponential}} {{family of}} distributions provides a general framework for selecting a possible alternative parameterisation of the distribution, {{in terms of}} natural parameters, and for defining useful <b>sample</b> <b>statistics,</b> called the natural sufficient statistics of the family.|$|E
5000|$|Note that in {{the formula}} for the {{predictive}} confidence interval no mention is made of the unobservable parameters μ and σ of population {{mean and standard deviation}} - the observed <b>sample</b> <b>statistics</b> [...] and [...] of sample mean and standard deviation are used, and what is estimated is the outcome of future samples.|$|E
40|$|Hierarchical {{structure}} from stellar clusters, to subgroups, to {{associations and}} star complexes {{is discussed in}} the context of the Orion stellar grouping and its origin. The analogous structure in gas clouds is also reviewed, with an emphasis on general fractal properties and mass distribution functions. The distinction between the cloud, cluster, and stellar mass functions is discussed in terms of different <b>sampling</b> <b>statistics</b> for hierarchically structured clumps in clouds with an interclump medium. Comment: 34 pages, 12 figures; to be published in "The Orion Complex Revisited," ed. M. J. McCaughrean & A. Burkert, ASP Conference Series, 199...|$|R
40|$|The {{existence}} of 'sampling bias' in individual-realization laser velocimeter measurements is experimentally verified and {{shown to be}} independent of sample rate. The experiments were performed in a simple two-stream mixing shear flow with the standard for comparison being laser-velocimeter results obtained under continuous-wave conditions. It is also demonstrated that the errors resulting from sampling bias can be removed by a proper interpretation of the <b>sampling</b> <b>statistics.</b> In addition, data obtained in a shock-induced separated flow and in the near-wake of airfoils are presented, both bias-corrected and uncorrected, to illustrate the effects of sampling bias in the extreme...|$|R
40|$|Rating-dependent {{financial}} regulators {{assume that}} the same letter ratings from different agencies imply the same levels of default risk. Most "third" agencies, however, assign significantly higher ratings on average than Moody's and Standard & Poor's. We show that, contrary to the claims of some rating industry professionals, sample selection bias can account for at most half of the observed average difference in ratings. We also investigate the economic rationale for using multiple rating agencies. Among the many variables considered, only size and bond-issuance history are consistently related to the probability of an issuer seeking third ratings. The probability ties to improve their standing under rating-dependent regulations. Credit; Corporate bonds; <b>Sampling</b> (<b>Statistics)</b> ...|$|R
50|$|L-estimators {{can also}} be used as {{statistics}} in their own right - for example, the median is a measure of location, and the IQR is a measure of dispersion. In these cases, the <b>sample</b> <b>statistics</b> can act as estimators of their own expected value; for example, the sample median is an estimator of the population median.|$|E
50|$|MSM is {{a special}} case of Indirect Inference. While Indirect Inference allows the {{researcher}} to use any {{of the features of}} <b>sample</b> <b>statistics</b> as a basis for comparison of moments and data, the name MSM applies only when those statistics are moments of the data, i.e. averages, across the sample of functions defined for a single sample element.|$|E
5000|$|He {{was among}} the first to apply the {{bootstrap}} in his 1975 analyses of 2&times;2 designs with a missing cell. [...] His chief contributions to statistics are in the area of small <b>sample</b> <b>statistics,</b> including a uniformly most powerful unbiased (UMPU) permutation test for Type I censored data, an exact test for comparing variances, and an exact test for cross-over designs.|$|E
40|$|As {{statistical}} agencies try to “move as far {{as possible}} toward the use of a small number of standardized disclosure limitation methods whose effectiveness has been demonstrated ” (Working Paper 22), many practical difficulties arise in applying these standard sensitivity measures to particular data sets. This paper discusses possible ways of handling several common complications which arise when applying the p-percent rule and, more generally, the pq-rule. This includes: analysis of increases and decreases in disclosure risk due to imputation, suggestions for use of weights as protection, and discussion of handling final weights less than one which generally arise from controlling <b>sampled</b> <b>statistics</b> to independent universe values...|$|R
40|$|Abstract. Failure {{analysis}} {{is important for}} the safety design of the folding umbrella. The failure point is found on certain location after large amount of <b>samples</b> <b>statistics.</b> In order to verify this problem, apply SolidWorks to finish the umbrella framework model, import the model into ADAMS. Then the experiment is conducted, the push force on the umbrella piston which adds to the mechanical model is measured through Gravity Sensor. The dynamic simulation of umbrella framework is discussed by using ADAMS. The simulation result shows the failure point we found is accurate. Since the destructive force on umbrella is diversity and uncertainty, further studies are necessary for this subject...|$|R
50|$|Research on {{the immune}} system repertoire, may develop methods of <b>sampling</b> and <b>statistics,</b> to {{directly}} assess diversity in ecology. It may be that this too, is best approached, through genetic markers and high throughput sequencing—rather than more traditional taxonomy.|$|R
