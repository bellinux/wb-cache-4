348|134|Public
25|$|The {{validity}} (<b>statistical</b> <b>validity</b> {{and test}} validity) of the MBTI as a psychometric instrument {{has been the}} subject of much criticism.|$|E
2500|$|For Pearson, {{the theory}} of {{evolution}} was not intended to identify a biological mechanism that explained patterns of inheritance, whereas the Mendelian's postulated the gene as the mechanism for inheritance. Pearson criticized Bateson and other biologists for their failure to adopt biometrical techniques in their study of evolution. Pearson criticized biologists who did not focus on the <b>statistical</b> <b>validity</b> of their theories, stating that [...] "before we can accept [...] as a factor we must have not only shown its plausibility but if possible have demonstrated its quantitative ability" [...] Biologists had succumb to [...] "almost metaphysical speculation as to the causes of heredity," [...] which had replaced the process of experimental data collection that actually might allow scientists to narrow down potential theories.|$|E
2500|$|Self-experimentation {{has value}} in rapidly obtaining the first results. [...] In some cases, such as with Forssmann's {{experiments}} done {{in defiance of}} official permission, results may be obtained that would never otherwise have come to light. [...] However, self-experiment lacks the <b>statistical</b> <b>validity</b> of a larger experiment. [...] It {{is not possible to}} generalise from an experiment on a single person. [...] For instance, a single successful blood transfusion does not indicate, as we now know from the work of Karl Landsteiner, that all such transfusions between any two random people will also be successful. [...] Likewise, a single failure does not absolutely prove that a procedure is worthless. [...] Psychological issues such as confirmation bias and the placebo effect are unavoidable in a single-person self-experiment where {{it is not possible to}} put scientific controls in place.|$|E
5000|$|The {{most common}} threats to <b>statistical</b> {{conclusion}} <b>validity</b> are: ...|$|R
5000|$|... {{the choice}} of {{statistical}} methods (see: <b>Statistical</b> conclusion <b>validity).</b>|$|R
50|$|<b>Statistical</b> {{conclusion}} <b>validity</b> involves {{ensuring the}} use of adequate sampling procedures, appropriate statistical tests, and reliable measurement procedures. As this type of validity is concerned solely with the relationship that is found among variables, the relationship may be solely a correlation.|$|R
2500|$|In 2007, Gordon H. Guyatt et al. {{conducted}} a meta-analysis, or systematic review, of all studies that compared health outcomes for similar conditions in Canada and the U.S., in Open Medicine, an open-access peer-reviewed Canadian medical journal. They concluded, [...] "Available {{studies suggest that}} health outcomes may be superior in patients cared for in Canada versus the United States, but differences are not consistent." [...] Guyatt identified 38 studies addressing conditions including cancer, coronary artery disease, chronic medical illnesses and surgical procedures. Of 10 studies with the strongest <b>statistical</b> <b>validity,</b> 5 favoured Canada, 2 favoured the United States, and 3 were equivalent or mixed. Of 28 weaker studies, 9 favoured Canada, 3 favoured the United States, and 16 were equivalent or mixed. Overall, results for mortality favoured Canada with a 5% advantage, but the results were weak and varied. The only consistent pattern was that Canadian patients fared better in kidney failure.|$|E
2500|$|Psychometricians {{generally}} regard IQ {{tests as}} having high statistical reliability. A high reliability implies that – although test-takers may have varying scores when taking the same test on differing occasions, {{and although they}} may have varying scores when taking different IQ tests at the same age – the scores generally agree {{with one another and}} across time. Like all statistical quantities, any particular estimate of IQ has an associated standard error that measures uncertainty about the estimate. For modern tests, the standard error of measurement is about three points. Clinical psychologists generally regard IQ scores as having sufficient <b>statistical</b> <b>validity</b> for many clinical purposes. In a survey of 661 randomly sampled psychologists and educational researchers, published in 1988, Mark Snyderman and Stanley Rothman reported a general consensus supporting the validity of IQ testing. [...] "On the whole, scholars with any expertise in the area of intelligence and intelligence testing (defined very broadly) share a common view of the most important components of intelligence, and are convinced that it can be measured with some degree of accuracy." [...] Almost all respondents picked out abstract reasoning, ability to solve problems and ability to acquire knowledge as the most important elements.|$|E
50|$|From the {{perspective}} of <b>statistical</b> <b>validity,</b> psychometrics and positivism, criticisms of projective tests, and depth psychology tests, usually include the well-known discrepancy between <b>statistical</b> <b>validity</b> and clinical validity.|$|E
5000|$|<b>Statistical</b> {{conclusion}} <b>validity</b> is {{the degree}} to which conclusions about the relationship among variables based on the data are correct or ‘reasonable’. This began as being solely about whether the statistical conclusion about the relationship of the variables was correct, but now there is a movement towards moving to ‘reasonable’ conclusions that use: quantitative, statistical, and qualitative data.Fundamentally, two types of errors can occur: type I (finding a difference or correlation when none exists) and type II (finding no difference when one exists). <b>Statistical</b> conclusion <b>validity</b> concerns the qualities of the study that make these types of errors more likely.Statistical conclusion validity involves ensuring the use of adequate sampling procedures, appropriate statistical tests, and reliable measurement procedures.|$|R
40|$|A {{framework}} for understanding methodological practices {{from the perspectives of}} internal <b>validity,</b> external <b>validity,</b> <b>statistical</b> control <b>validity,</b> and construct validity is presented. One hundred doctoral dissertations completed between 1980 and 1988 at a single urban public university were analyzed for various methodological practices and types of statistical techniques used. Techniques were further coded into categories of univariate or multivariate designs and subcategories within these categories. In all, 201 techniques were discovered across papers, which were generated by the Departments of Educational Leadership and Foundation, Curriculum and Instruction, and Special Education of the university. Demographics, size, and other infemation about sampling techniques were also assessed. Randomization and convenience were the most typical sample selection techniques. Means of establishin...|$|R
50|$|<b>Statistical</b> {{conclusion}} <b>validity</b> is {{the degree}} to which conclusions about the relationship among variables based on the data are correct or ‘reasonable’. This began as being solely about whether the statistical conclusion about the relationship of the variables was correct, but now there is a movement towards moving to ‘reasonable’ conclusions that use: quantitative, statistical, and qualitative data.|$|R
50|$|It {{must have}} <b>statistical</b> <b>validity.</b>|$|E
50|$|Applying SMOG {{to other}} {{languages}} lacks <b>statistical</b> <b>validity.</b>|$|E
50|$|The {{validity}} (<b>statistical</b> <b>validity</b> {{and test}} validity) of the MBTI as a psychometric instrument {{has been the}} subject of much criticism.|$|E
30|$|However, {{there is}} a {{possible}} threat to <b>statistical</b> conclusion <b>validity.</b> A major finding {{of the study was}} that the use of inclusive pronouns did not seem to explain the difference between successful and unsuccessful projects. However, while the p values for peripheral members are not significant, they are low, so it could be that this negative finding is due to an under-powered statistical test, specifically, the non-parametric test used due to concerns about the skewed distribution of the data.|$|R
40|$|The {{traditional}} {{procedure for}} obtaining judged ratings, to ascertain if treatment-related change has occurred, involves the randomization {{of the materials}} to be rated. An alternative approach (linked judgments) is investigated as a potential solution to certain instrumentation- related threats to <b>statistical</b> conclusion <b>validity</b> of the incumbent rating procedure. Data from a weight reduction study are presented which suggest that linked raters’ judgments provide both a more powerful and a more valid index of treatment effectiveness than the traditional procedure...|$|R
40|$|Single-case {{experimental}} designs {{have received}} little {{attention in the}} program evaluation literature. This article {{provides an overview of}} single-case methodology, with a specific focus on the A-B-A and multiple baseline designs. The potential utility of these designs for evaluating social programs is discussed, and several examples are provided. Internal <b>validity,</b> <b>statistical</b> conclusion <b>validity,</b> external validity, construct validity, and cost-effec-tiveness are considered. Single-case designs offer essentially equivalent protection against threats to experimental validity compared to traditional randomized group designs. It appears that these designs offer, at a minimum, viable methodological alternatives in situations where traditional randomized group designs are infeasible. / he experimental evaluation of social intervention programs is,even under the best of circumstances, extraordinarily complex. Economical, logistical, and ethical constraints often preclude the direct application of traditional experimental methodologies in field settings. Because the preferred traditional experimental design is simply not applicable in certain evaluation settings, a number of quasi-experimen-tal designs have been proposed as methodological alternatives (Camp...|$|R
5000|$|By {{selecting}} {{samples of}} varying sizes dictated by <b>statistical</b> <b>validity,</b> risk-limiting audits {{eliminate the need}} to count all the ballots to obtain rapid confirmation of the outcome (that, is, who won?), while providing a comparable level of valid statistical confidence.|$|E
50|$|The {{large number}} of {{participants}} using RIQAS ensures an extensive database for many analytical methods directly increasing <b>statistical</b> <b>validity</b> as a result. RIQAS helps maintain and improve analytical quality; improve inter-laboratory agreement; detect reagent and equipment problems; and compares different analytical methods. RIQAS is also ISO 13485 and UKAS accredited.|$|E
5000|$|Employing PsycINFO {{index terms}} the 451 {{articles}} could be classified {{into the following}} groups: Peabody Picture Vocabulary Test (325), Intelligence Measures (122), Test Validity (87), Intellectual Development Disorder (74), Elementary School Students (70), Wechsler Intelligence Scale for Children (65), Test Reliability (47), Blacks (46), Test Scores (43), <b>Statistical</b> <b>Validity</b> (41), Vocabulary (37).|$|E
40|$|The {{concepts}} and tools developed by Donald Campbell {{and his colleagues}} have quite properly dominated the eld of evaluation research. Internal validity, external validity, and later, construct <b>validity</b> and <b>statistical</b> conclusion <b>validity,</b> for instance, are central concepts on which a variety of evaluation research methods depend (Rossi and Freeman, 1993). However, these methods have been developed to understand phenomena that may be empirically observed. In this chapter, we broaden their applicability by considering evaluations undertaken in the virtual worlds manufactured by computer simulations...|$|R
40|$|Based on an {{analysis}} of empirical articles published in the academic literature between 1999 and 2003, we examined {{the current state of}} the small business and entrepreneurship research in terms of its validity. We collected 275 relevant publications in order to explore the small business and entrepreneurship field with respect to internal validity, external validity, construct <b>validity,</b> and <b>statistical</b> conclusion <b>validity.</b> Our aim was to gain insight into the dominant methodological and statistical practices that currently shape the field, shed light on possible gaps, and compare these observations with the findings in the management literature. ...|$|R
40|$|This paper {{reviews the}} {{four types of}} {{validity}} that make up Cook and Campbell’s traditional approach for social science research in general and psychological research in particular: internal <b>validity,</b> <b>statistical</b> conclusion <b>validity,</b> external validity, and construct validity. The most important generalizability threat to the validity of jury research is not likely a selection main effect (i. e., the effect of relying solely on undergraduate mock jurors) but is more likely the interaction of sample with construct validity factors. Researchers who try to capture the trial process with experimental paradigms may find that undergraduate mock jurors react differently to those efforts than do more representative community samples. We illustrate these issues with the seven papers that make up this volume, and conclude by endorsing Diamond’s call for a two-stage research process in which findings with samples of convenience gradually add more realistic trial processes and representative samples to confirm the initial findings and increase the research program’s credibility. Copyright © 2011 John Wiley & Sons, Ltd. It has been over 30 years since Thomas Cook and Donald Campbell (1979) publishe...|$|R
50|$|David Roodman and Jonathan Morduch {{question}} the <b>statistical</b> <b>validity</b> {{of studies of}} microcredit's effects on poverty, noting {{the complexity of the}} situations involved. Yoolim Lee and Ruth David discuss how microfinance and the Grameen model in South India have in recent years been distorted by venture capitalism and profit-makers. In some cases, poor rural families have suffered debt spirals, harassment by microfinance debt collectors, and in some cases suicide.|$|E
50|$|Cytel {{specializes in}} {{adaptive}} trials - {{a new type}} of randomized clinical trial that allows modifications of ongoing trials while aiming to preserve the <b>statistical</b> <b>validity</b> and integrity of the study. Based on either frequentist or Bayesian statistics, adaptive trial designs are now widely accepted by government regulatory agencies including the United States Food and Drug Administration (FDA), European Medicines Agency (EMA), and Medicines and Healthcare products Regulatory Agency (MHRA) in early and later stage clinical studies.|$|E
50|$|In 2006, Reinagel {{introduced}} the IF Ratings, {{a system that}} attempts to predict the inflammatory or anti-inflammatory potential of foods and mixed meals based on their nutrient composition. Reinagel cites peer-reviewed published research on the associations between various nutrients, food components, and dietary patterns on inflammatory markers such as C-reactive protein {{as the basis for}} the unpublished formula used to produce the ratings. No analyses of the <b>statistical</b> <b>validity</b> of the IF Ratings or results of any controlled interventions have been published.|$|E
40|$|The author {{looks at}} the effects of online searching on his day-to-day work in an {{industrial}} patent department. This new technology has revolutionised patent family searching and name searching but has had less effect on subject matter searches, especially those used to investigate infringement and <b>validity.</b> <b>Statistical</b> analysis of patent holdings is reviewed. ...|$|R
30|$|To {{assessment}} of discrimination, VISA-P-Tr scores compared all groups using the Mann–Whitney-U test. SPSS ver 11.5 (SPSS Inc., Chicago, IL, USA) {{was used for}} <b>statistical</b> analysis. Convergent <b>validity</b> was studied of questionnaire with using Pearson correlation test. Factorial validity was studied of questionnaire with using Confirmative Factor Analysis. Goodness of fit indexes was also given.|$|R
40|$|<b>Statistical</b> {{tests of}} <b>validity</b> of farming systems models may be {{inappropriate}} {{for a number}} of reasons. A specific example is the F test for zero intercept and unit slope; this test has intuitive appeal, but bias in parameter estimates can lead to rejection of a valid model. It is suggested that descriptive statistics and subjective tests be used to build up confidence in a model as it proceeds through a number of prototypes. © 1990...|$|R
5000|$|Richards commends Skocpol for {{choosing}} to focus her book on three revolutions rather than bringing {{to light a}} large number of successful and unsuccessful revolutions “in hopes of achieving <b>statistical</b> <b>validity</b> or in the expectation of being able to explain a number of related phenomena.” [...] Like Peter Manicas, Richards also finds Skocpol’s refusal to create a general theory for all revolutions commendable. According to Richards, Skocpol “recognizes the limitations of comparative study and the dangers of fitting events into relatively inflexible categories.” ...|$|E
50|$|There {{is nothing}} magic about the assumed time-periods - {{three years for}} system {{development}} and one month for the walk-forward interval. Picking these two time parameters is a trade-off between optimization time and <b>statistical</b> <b>validity</b> of the results. In practice, I have found that using about 20% of the optimization period for the walk- forward window works fairly well. Which window sizes work best is also affected by the given system, for different systems the optimal training and out-of-sample window size will be different.|$|E
50|$|A {{piece of}} {{evidence}} that he often cites with respect to this hypothesis is the 1964 star map drawn by alleged alien abductee Betty Hill during a hypnosis session, which she said was shown to her during her abduction. Astronomer Marjorie Fish constructed a three-dimensional map of nearby sun-like stars and claimed a good match from the perspective of Zeta Reticuli, about 39 light years distant. The fit of the Hill/Fish star maps was hotly debated in the December 1974 edition of Astronomy Magazine, with Friedman and others defending the <b>statistical</b> <b>validity</b> of the match.|$|E
30|$|The {{validity}} framework offers {{important considerations}} {{that the research}} community can use to further minimize errors when quasi-experimental designs are used with ILSA data. Reminding ourselves that error exists throughout the entire research process and reasoning helps clarify that statistical techniques alone do not establish causal claims. In other words, sound <b>statistical</b> conclusion <b>validity</b> {{does not lead to}} an acceptable causal claim unless it is supported by a compelling causal mechanism that has been clearly articulated and taken into account {{in the design of the}} study. When designing a study that approximates a randomized experiment using ILSA data issues of construct, internal, and external validity are of critical importance, as we have illustrated in the context of the two studies we discussed earlier. As we have noted, there are a number of ways to examine the validity of findings in relation to experimental and quasi-experimental studies. In this paper we depended largely on the framework that was first developed by Campbell and Stanley (1963) and later refined by Cook and Campbell (1979) and Shadish et al. (2002). This framework allowed us to productively examine the validity of claims made by two well cited studies in education. As such, we recommend that all quasi-experimental studies that use ILSA data: (1) choose an established validity framework to work from; and (2) clearly explain threats to the validity of their claims. For example, if the validity framework outlined in this paper is chosen the study should include a discussion of: construct validity, internal validity, external <b>validity,</b> and <b>statistical</b> conclusion <b>validity.</b> Shadish et al. (2002) provide a detailed description of possible threats to validity and are a useful resource for both researchers and the reviewers of these studies.|$|R
40|$|For {{want of a}} nail, {{the shoe}} was lost. For want of the shoe, the horse was lost. For want of the horse, the rider was lost. For want of the rider, the battle was lost. For want of the battle, the kingdom was lost. And all for the want of a nail. In some respects, this nursery rhyme {{summarizes}} the point of my brief essay on what I believe {{is one of the}} major problems in the manuscripts I review. Only instead of a missing nail, the problem is poor construct conceptualization. I chose this issue because it is one that has serious consequences for the validity of research in the discipline, but it is something that authors tend to overlook. As noted by Cook and Campbell (1979), there are four main questions that should be considered when evaluating the validity of a study’s findings. First, what are the particular cause-and-effect constructs involved? This is a question of construct validity. Second, is there a relationship between the presumed cause and effect? This question relates to <b>statistical</b> conclusion <b>validity.</b> Third, is the relationship causal? This question addresses the internal validity of the research and concerns whether the conditions for establishing causal priority of the causal variable over the effect variable have been established and whether the design of the study renders any rival explanations of the observed effect implausible. Fourth, how generalizable is this relationship across persons, settings, and times? Here the focus is on external validity. Even though all four types of validity are important, and indeed essential, authors often give far more attention to some of these questions than to others. Most authors give a great deal of consideration to questions of internal and external validity (as they should), but far less to issues related to construct and <b>statistical</b> conclusion <b>validity...</b>|$|R
40|$|Conventional wisdom {{suggests}} that the Teaching-Family Model (TFM) approach to treating youthful offenders is not effective in reducing post-treatment recidivism. This article reviews two major studies referenced {{in support of this}} widespread perception. Data presented in one widely referenced study are treated with a Cochran-Mantel-Haensel test, which, the author argues, is appropriate for data originally presented in two separate 2 X 2 tables (one for boys and one for girls). The construct and <b>statistical</b> conclusion <b>validity</b> of a major evaluation study presented to the NIMH is critically evaluated and discussed. A revised view of the leading TFM evaluations has implications for public policy regarding juvenile justice. The author {{suggests that}} a belief in the lack of post-treatment efficacy associated with community-based residential treatment has resulted in harsher treatment of juveniles and a higher incarceration rate...|$|R
