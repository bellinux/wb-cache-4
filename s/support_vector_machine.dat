10000|10000|Public
25|$|Yan, C., Dobbs, D., and Honavar, V. Identifying Protein-Protein Interaction Sites from Surface Residues  A <b>Support</b> <b>Vector</b> <b>Machine</b> Approach. Neural Computing Applications. Vol. 13. pp.123–129, 2004.|$|E
25|$|Once the stacked auto encoder is trained, its output {{can be used}} as {{the input}} to a {{supervised}} learning algorithm such as <b>support</b> <b>vector</b> <b>machine</b> classifier or a multi-class logistic regression.|$|E
25|$|In 2014, Padma et al. used {{combined}} wavelet statistical texture {{features to}} segment and classify AD benign and malignant tumor slices. Zhang et al. found kernel <b>support</b> <b>vector</b> <b>machine</b> decision tree had 80% classification accuracy, {{with an average}} computation time of 0.022s for each image classification.|$|E
40|$|<b>Support</b> <b>vector</b> <b>machines</b> {{based on}} {{positive}} feedback are put {{forward with the}} analysis of both {{advantages and disadvantages of}} current <b>support</b> <b>vector</b> <b>machines</b> based on Gaussian kernel function(including <b>support</b> <b>vector</b> <b>machines</b> based on K-nearest neighbors) and the essence why <b>support</b> <b>vector</b> <b>machines</b> is capable of describing data sets'distribution characteristics, thus the rigor constraint is overcome that maintains "corresponding parameters ofkernel function <b>support</b> <b>vectors</b> should be equal". The learning algorithm of <b>support</b> <b>vector</b> <b>machines</b> based on positive feedback is given. Simulation experiments of artificialand real data proves that <b>support</b> <b>vector</b> <b>machines</b> based on positive feedback is be obviously superior to current ones in its generation capabilities. Though, only the <b>support</b> <b>vector</b> <b>machines</b> based on positive feedback for pattern recognition is discussed, the idea included in <b>support</b> <b>vector</b> <b>machines</b> based on positive feedback is using kernel functions with different corresponding parameters to construct <b>support</b> <b>vector</b> <b>machines,</b> and is adaptive to other types <b>support</b> <b>vector</b> <b>machines...</b>|$|R
30|$|<b>Support</b> <b>vector</b> <b>machines,</b> {{one of the}} {{non-parametric}} controlled classifiers, is a two-class {{classification method}} introduced {{in the context of}} statistical learning theory and structural risk minimization. <b>Support</b> <b>vector</b> <b>machines</b> are basically divided into two groups as linear <b>support</b> <b>vector</b> <b>machines</b> and nonlinear <b>support</b> <b>vector</b> <b>machines.</b> Nonlinear <b>support</b> <b>vector</b> <b>machines</b> are designed to make classifications by creating a plane in a space by mapping data to that higher dimensional input space. This method basically involves solving a quadratic programming problem. In this study, the <b>support</b> <b>vector</b> <b>machines,</b> which have an increasing rate of use in pattern recognition area, are used in the quality control of DNA sequencing data. Consequently, the classification of quality of all the DNA sequencing data will automatically be made as ‘high quality/low quality’.|$|R
40|$|<b>Support</b> <b>vector</b> <b>machines</b> have {{attracted}} much attention in theo-retical and in applied statistics. Main topics of recent interest are consistency, learning rates and robustness. In this article, it is shown that <b>support</b> <b>vector</b> <b>machines</b> are qualitatively robust. Since <b>support</b> <b>vector</b> <b>machines</b> can be represented by a functional {{on the set of}} all probability measures, qualitative robustness is proven by showing that this functional is continuous with respect to the topology generated by weak convergence of probability measures. Combined with the exis-tence and uniqueness of <b>support</b> <b>vector</b> <b>machines,</b> our results show that <b>support</b> <b>vector</b> <b>machines</b> are the solutions of a well-posed mathematical problem in Hadamard’s sense...|$|R
25|$|One newer {{method for}} model {{assessment}} relies on machine learning {{techniques such as}} neural nets, which may be trained to assess the structure directly or to form a consensus among multiple statistical and energy-based methods. Results using <b>support</b> <b>vector</b> <b>machine</b> regression on a jury of more traditional assessment methods outperformed common statistical, energy-based, and machine learning methods.|$|E
25|$|The soft-margin <b>support</b> <b>vector</b> <b>machine</b> {{described}} above {{is an example}} of an empirical risk minimization (ERM) algorithm for the hinge loss. Seen this way, support vector machines belong to a natural class of algorithms for statistical inference, and many of its unique features are due to the behavior of the hinge loss. This perspective can provide further insight into how and why SVMs work, and allow us to better analyze their statistical properties.|$|E
25|$|More formally, a <b>support</b> <b>vector</b> <b>machine</b> {{constructs}} a hyperplane {{or set of}} hyperplanes in a high- or infinite-dimensional space, {{which can}} be used for classification, regression, or other tasks like outliers detection. Intuitively, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data point of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier.|$|E
40|$|Abstract. In {{this paper}} {{we are going}} to {{construct}} a new kernel function, which is based on Walsh functions, for <b>support</b> <b>vector</b> <b>machines.</b> We prove some theoretical results about the VC-dimension of the <b>support</b> <b>vector</b> <b>machines</b> which are built in the space of the Walsh functions. This information is very important, because the learning capability of the <b>support</b> <b>vector</b> <b>machines</b> depends on the VC-dimension of the kernel function used. The paper also proposes the application of majority voting on the output of several <b>support</b> <b>vector</b> <b>machines</b> in order to select the most suitable learning machine for frontal face detection. The paper also reports the rst experimental results related to application of <b>support</b> <b>vector</b> <b>machines</b> with Walsh kernel and majority voting technique to face detection. ...|$|R
40|$|Stochastic {{gradient}} descent algorithm {{has been successfully}} applied on <b>support</b> <b>vector</b> <b>machines</b> (called PEGASOS) for many classification problems. In this paper, stochastic {{gradient descent}} algorithm is investigated to twin <b>support</b> <b>vector</b> <b>machines</b> for classification. Compared with PEGASOS, the proposed stochastic gradient twin <b>support</b> <b>vector</b> <b>machines</b> (SGTSVM) is insensitive on stochastic sampling for stochastic gradient descent algorithm. In theory, we prove the convergence of SGTSVM instead of almost sure convergence of PEGASOS. For uniformly sampling, the approximation between SGTSVM and twin <b>support</b> <b>vector</b> <b>machines</b> is also given, while PEGASOS only {{has an opportunity to}} obtain an approximation of <b>support</b> <b>vector</b> <b>machines.</b> In addition, the nonlinear SGTSVM is derived directly from its linear case. Experimental results on both artificial datasets and large scale problems show the stable performance of SGTSVM with a fast learning speed. Comment: 31 pages, 31 figure...|$|R
50|$|Scikit-learn {{is largely}} written in Python, with some core {{algorithms}} written in Cython to achieve performance. <b>Support</b> <b>vector</b> <b>machines</b> are implemented by a Cython wrapper around LIBSVM; logistic regression and linear <b>support</b> <b>vector</b> <b>machines</b> by a similar wrapper around LIBLINEAR.|$|R
25|$|A {{version of}} SVM for {{regression}} was proposed in 1996 by Vladimir N. Vapnik, Harris Drucker, Christopher J. C. Burges, Linda Kaufman and Alexander J. Smola. This method is called support vector regression (SVR). The model produced by support vector classification (as described above) depends {{only on a}} subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by SVR depends only on {{a subset of the}} training data, because the cost function for building the model ignores any training data close to the model prediction. Another SVM version known as least squares <b>support</b> <b>vector</b> <b>machine</b> (LS-SVM) has been proposed by Suykens and Vandewalle.|$|E
2500|$|... of test {{examples}} to be classified. Formally, a transductive <b>support</b> <b>vector</b> <b>machine</b> {{is defined by}} the following primal optimization problem: ...|$|E
2500|$|Caragea, C., Sinapov, J., Silvescu, A., Dobbs, D. And Honavar, V. (2007). Glycosylation Site Prediction Using Ensembles of <b>Support</b> <b>Vector</b> <b>Machine</b> Classifiers. BMC Bioinformatics [...]|$|E
40|$|The {{method of}} <b>support</b> <b>vector</b> <b>machines</b> has been {{developed}} for solving classication and static function approximation problems. In this paper we introduce <b>support</b> <b>vector</b> <b>machines</b> {{within the context of}} recurrent neural networks. Instead of Vapnik's epsilon insensitive loss function, we consider a least squares version related to a cost function with equality constraints for a recurrent network. Essential features of <b>support</b> <b>vector</b> <b>machines</b> remain such as Mercer's condition {{and the fact that the}} output weights are a Lagrange multiplier weighted sum of the data points. The solution to recurrent least squares <b>support</b> <b>vector</b> <b>machines</b> is characterized by a set of nonlinear equations. Due to its high computational complexity, we focus on a limit case of assigning the squared error an innitely large penalty factor with early stopping as a form of regularization. The eectiveness of the approach is demonstrated on trajectory learning of the double scroll attractor in Chua's circuit. Keywords. Recurrent neural networks, <b>Support</b> <b>vector</b> <b>machines,</b> Radial basis functions, Double scroll. ...|$|R
40|$|We propose an {{algorithm}} {{for exploring}} the entire regularization path of asymmetric-cost linear <b>support</b> <b>vector</b> <b>machines.</b> Empirical evidence suggests the predictive power of <b>support</b> <b>vector</b> <b>machines</b> depends on the regularization parameters of the training algorithms. The algorithms exploring the entire regularization paths have been proposed for single-cost <b>support</b> <b>vector</b> <b>machines</b> thereby providing the complete knowledge {{on the behavior of}} the trained model over the hyperparameter space. Considering the problem in two-dimensional hyperparameter space though enables our algorithm to maintain greater flexibility in dealing with special cases and sheds light on problems encountered by algorithms building the paths in one-dimensional spaces. We demonstrate two-dimensional regularization paths for linear <b>support</b> <b>vector</b> <b>machines</b> that we train on synthetic and real data. Comment: 8 pages, 2 figure...|$|R
40|$|<b>Support</b> <b>Vector</b> <b>Machines</b> (SVMs) {{have become}} a popular tool for machine {{learning}} with large amounts of high dimensional data. In this paper an approach for incremental learning with <b>Support</b> <b>Vector</b> <b>Machines</b> is presented, that improves the existing approach of [3]. Also, {{some insight into the}} interpretability of <b>support</b> <b>vectors</b> is given...|$|R
2500|$|The SVM {{algorithm}} {{has been}} widely applied in the biological and other sciences. [...] They {{have been used to}} classify proteins with up to 90% of the compounds classified correctly. Permutation tests based on SVM weights have been suggested as a mechanism for interpretation of SVM models. <b>Support</b> <b>vector</b> <b>machine</b> weights have also been used to interpret SVM models in the past. Posthoc interpretation of <b>support</b> <b>vector</b> <b>machine</b> models in order to identify features used by the model to make predictions is a relatively new area of research with special significance in the biological sciences.|$|E
2500|$|Using a {{different}} annealing technology based on {{nuclear magnetic resonance}} (NMR), a quantum Hopfield network was implemented in 2009 that mapped the input data and memorized data to Hamiltonians, allowing the use of adiabatic quantum computation. NMR technology also enables universal quantum computing, and it {{was used for the}} first experimental implementation of a quantum <b>support</b> <b>vector</b> <b>machine</b> to distinguish hand written number ‘6’ and ‘9’ on a liquid-state [...] quantum computer in 2015. The training data involved the pre-processing of the image which maps them to normalized 2-dimensional vectors to represent the images as the states of a qubit. The two entries of the vector are the vertical and horizontal ratio of the pixel intensity of the image. Once the vectors are defined on the feature space, the quantum <b>support</b> <b>vector</b> <b>machine</b> was implemented to classify the unknown input vector. The readout avoids costly quantum tomography by reading out the final state in terms of direction (up/down) of the NMR signal.|$|E
2500|$|<b>Support</b> <b>vector</b> <b>machine</b> {{is by far}} {{the most}} {{frequently}} used classifier in vessel segmentation, up to 90% of cases. SVM is a supervised learning model that belongs to the broader category of pattern recognition technique. The algorithm works by creating a largest gap between distinct samples in the data. The goal is to create the largest gap between these components that minimize the potential error in classification. In order to successfully segregate blood vessel information from the rest of the eye image, SVM algorithm creates support vectors that separate the blood vessel pixel from the rest of the image through a supervised environment. Detecting blood vessel from new images can be done through similar manner using support vectors. Combination with other pre-processing technique, such as green channel filtering, greatly improves the accuracy of detection of blood vessel abnormalities. Some beneficial properties of SVM include ...|$|E
40|$|In this paper, {{we propose}} to use <b>support</b> <b>vector</b> <b>machines</b> for {{classification}} of bacterial growth and non growth database and modeling the probability= of growth. Unlike artificial neural networks paradigms, <b>support</b> <b>vector</b> <b>machines</b> use the kernel functions and <b>support</b> <b>vectors</b> with maximum margin, which allows a better performance. As a practical {{application of the}} new approach, <b>support</b> <b>vector</b> <b>machines</b> were investigated for their quality and accuracy in classifi-cation of growth/no-growth state of a pathogenic Escherichia coli R 31 in response to temperature and water activity. A comparison with the most common used statistics, machine learning, and data mining schemes was carried out. The results shows that <b>support</b> <b>vector</b> <b>machines</b> classifier based on the Gaussian RBF Kernel was found {{to do better than}} most of logistic regression, K-nearest neighbor, probabilistic networks, and multilayer perceptron classifiers...|$|R
40|$|MIT’s Lincoln Labs {{to study}} {{intrusion}} detection systems, {{the performance of}} robust <b>support</b> <b>vector</b> <b>machines</b> (RVSMs) was {{compared with that of}} conventional <b>support</b> <b>vector</b> <b>machines</b> and nearest neighbor classifiers in separating normal usage profiles from intrusive profiles of computer programs. The results indicate the superiority of RSVMs {{not only in terms of}} high intrusion detection accuracy and low false positives but also in terms of their generalization ability in the presence of noise and running time. Keywords—Intrusion detection, computer security, robust <b>support</b> <b>vector</b> <b>machines,</b> noisy data. I...|$|R
5000|$|Manifold regularization {{can extend}} {{a variety of}} {{algorithms}} that can be expressed using Tikhonov regularization, by choosing an appropriate loss function [...] and hypothesis space [...] Two commonly used examples are the families of <b>support</b> <b>vector</b> <b>machines</b> and regularized least squares algorithms. (Regularized least squares includes the ridge regression algorithm; the related algorithms of LASSO and elastic net regularization can be expressed as <b>support</b> <b>vector</b> <b>machines.)</b> The extended versions of these algorithms are called Laplacian Regularized Least Squares (abbreviated LapRLS) and Laplacian <b>Support</b> <b>Vector</b> <b>Machines</b> (LapSVM), respectively.|$|R
5000|$|... #Subtitle level 2: From <b>support</b> <b>vector</b> <b>machine</b> to {{least squares}} <b>support</b> <b>vector</b> <b>machine</b> ...|$|E
50|$|The {{structured}} <b>support</b> <b>vector</b> <b>machine</b> is {{a machine}} learning algorithm that generalizes the <b>Support</b> <b>Vector</b> <b>Machine</b> (SVM) classifier. Whereas the SVM classifier supports binary classification, multiclass classification and regression, the structured SVM allows training of a classifier for general structured output labels.|$|E
50|$|The quantum {{algorithm}} for linear {{systems of}} equations {{has been applied}} to a <b>support</b> <b>vector</b> <b>machine,</b> which is an optimized linear or non-linear binary classifier. A <b>support</b> <b>vector</b> <b>machine</b> can be used for supervised machine learning, in which training set of already classified data is available, or unsupervised machine learning, in which all data given to the system is unclassified. Rebentrost et al. show that a quantum <b>support</b> <b>vector</b> <b>machine</b> can be used for big data classification and achieve an exponential speedup over classical computers.|$|E
5000|$|... #Article: Regularization {{perspectives}} on <b>support</b> <b>vector</b> <b>machines</b> ...|$|R
5000|$|... #Subtitle level 3: Laplacian <b>Support</b> <b>Vector</b> <b>Machines</b> (LapSVM) ...|$|R
40|$|In this paper, {{a method}} for {{secondary}} structure with <b>support</b> <b>vector</b> <b>machines</b> is presented. The system used two layers of <b>support</b> <b>vector</b> <b>machines,</b> with a weighted cost function to balance the uneven class memberships. Using this method, prediction accuracy reaches 71. 5 %, comparable to the best techniques avaliable...|$|R
5000|$|For {{contributions}} to <b>support</b> <b>vector</b> <b>machine</b> algorithms and software.|$|E
5000|$|... #Subtitle level 2: <b>Support</b> <b>vector</b> <b>machine</b> {{definition}} of margin ...|$|E
5000|$|The linear {{classifier}} {{for this}} <b>support</b> <b>vector</b> <b>machine</b> classifier is, ...|$|E
40|$|This {{document}} is intended as {{an introduction to}} structural extensions of <b>support</b> <b>vector</b> <b>machines</b> {{for those who are}} familiar with logistic regression (binary and multinomial) and discrete-state probabilistic graphical models (in particular, conditional random fields). No prior knowledge about <b>support</b> <b>vector</b> <b>machines</b> is assumed. The outline is as follow...|$|R
40|$|Abstract. This paper reports rst {{experimental}} results {{in order to}} access {{the properties of the}} <b>support</b> <b>vector</b> <b>machines</b> in the Walsh Transform Domain for face detection. We prove theoretical results about the VC-dimension of the <b>support</b> <b>vector</b> <b>machines</b> which are built in the space of the two-dimensional (2 -D) Walsh functions. Morever, we demonstrate by experiments that <b>support</b> <b>vector</b> <b>machines</b> in the Walsh Transform Domain can separate more eciently face patterns from non-face ones {{in the sense that the}} margin between the two classes of patterns is increased. ...|$|R
40|$|In {{this paper}} a new method for edge {{detection}} {{that is based}} into the use of <b>support</b> <b>vector</b> <b>machines</b> is presented. This new method shows how the <b>support</b> <b>vector</b> <b>machines</b> may be used into image processing in a simple way and by using a few training examples. The results presented are comparable to those from classical edge detection methods. Besides we show how the training parameters affect the edge detection and how they {{may be used to}} improve the overall performance of the <b>support</b> <b>vector</b> <b>machines</b> in this task...|$|R
