391|1762|Public
25|$|IIT Madras is {{designing}} a <b>spiking</b> <b>neuron</b> accelerator for new RISC-V systems, aimed at big-data analytics in servers.|$|E
50|$|The <b>spiking</b> <b>neuron</b> model {{assumes that}} {{frequency}} (inverse {{of the rate}} at which spikes are generated) of spiking train starts at 0 and increases with the stimulus current. There is another hypothetical model that formulates the firing to happen at the threshold, but there is a quantum jump in frequency in contrast to smooth rise in frequency as in the <b>spiking</b> <b>neuron</b> model. This model is called the rate model. Gerstner, W., & Kistler, W. (2002), and Sterratt, D., Graham, B., Gillies, A., & Willshaw, D. (2011) are good sources for a detailed treatment of <b>spiking</b> <b>neuron</b> models and rate neuron models.|$|E
5000|$|Wulfram Gerstner and Werner M. Kistler, <b>Spiking</b> <b>Neuron</b> Models, Single Neurons, Populations, Plasticity, ...|$|E
40|$|We {{investigate}} through theoretical {{analysis and}} computer simulations {{the consequences of}} unreliable synapses for fast analog computations in networks of <b>spiking</b> <b>neurons,</b> with analog variables encoded by the current firing activities of pools of <b>spiking</b> <b>neurons.</b> Our results suggest a possible functional role for the well-established unreliability of synaptic transmission on the network level. We also investigate computations on time series and Hebbian learning in this context of space-rate coding in networks of <b>spiking</b> <b>neurons</b> with unreliable synapses. ...|$|R
40|$|We {{investigate}} through theoretical {{analysis and}} computer simulations {{the consequences of}} unreliable synapses for fast analog computations in networks of <b>spiking</b> <b>neurons,</b> with analog variables encoded by the ring activities of pools of <b>spiking</b> <b>neurons.</b> Our {{results suggest that the}} known unreliability of synaptic transmission may be viewed as a useful tool for analog computing, rather than as a "bug" in neuronal hardware. We also investigate computations on analog time series encoded by the ring activities of pools of <b>spiking</b> <b>neurons...</b>|$|R
40|$|We {{exhibit a}} novel way of {{simulating}} sigmoidal neural nets by networks of noisy <b>spiking</b> <b>neurons</b> in temporal coding. Furthermore it is shown that networks of noisy <b>spiking</b> <b>neurons</b> with temporal coding have a strictly larger computational power than sigmoidal neural nets {{with the same}} number of units...|$|R
5000|$|IIT Madras are {{designing}} a <b>spiking</b> <b>neuron</b> accelerator for new RISC-V systems, aimed at big-data analytics in servers.|$|E
5000|$|The <b>spiking</b> <b>neuron</b> {{model by}} Nossenson & Messer [...] {{produces}} {{the probability of}} the neuron to fire a spike {{as a function of}} either an external or pharmacological stimulus. The model consists of a cascade of a receptor layer model and a <b>spiking</b> <b>neuron</b> model, as shown in Fig 4. The connection between the external stimulus to the spiking probability is made in two steps: First, a receptor cell model translates the raw external stimulus to neurotransmitter concentration, then, a <b>spiking</b> <b>neuron</b> model connects between neurotransmitter concentration to the firing rate (spiking probability). Thus, the <b>spiking</b> <b>neuron</b> model by itself depends on neurotransmitter concentration at the input stage.An important feature of this model is the prediction for neurons firing rate pattern which captures, using a low number of free parameters, the characteristic edge emphasized response of neurons to a stimulus pulse, as shown in Fig. 5. The firing rate is identified both as a normalized probability for neural spike firing, and as a quantity proportional to the current of neurotransmitters released by the cell. The expression for the firing rate takes the following form: ...|$|E
50|$|A {{biological}} neuron model, {{also known}} as a <b>spiking</b> <b>neuron</b> model, is a mathematical description of the properties of certain cells in the nervous system that generate sharp electrical potentials across their cell membrane, roughly one millisecond in duration, as shown in Fig. 1. Spiking neurons are known to be a major signaling unit of the nervous system, and for this reason characterizing their operation is of great importance. It is worth noting that not all the cells of the nervous system produce the type of spike that define the scope of the <b>spiking</b> <b>neuron</b> models. For example, cochlear hair cells, retinal receptor cells, and retinal bipolar cells do not spike. Furthermore, many cells in the nervous system are not classified as neurons but instead are classified as glia.|$|E
5000|$|... #Subtitle level 3: Conjecture 2: Loops of <b>spiking</b> <b>neurons</b> for {{decision}} making ...|$|R
5000|$|TrueNorth {{a similar}} device {{simulating}} <b>spiking</b> <b>neurons</b> instead of low precision tensors.|$|R
40|$|This paper {{gives an}} {{introduction}} to spiking neural networks, some biological background, and will present two models of <b>spiking</b> <b>neurons</b> that employ pulse coding. Networks of <b>spiking</b> <b>neurons</b> are more powerful than their non-spiking predecessors as they can encode temporal information in their signals, but therefore do also need dilferent and biologically more plausible rules for synaptic plasticit...|$|R
50|$|From the {{information}} theory point of view, {{the problem is}} to propose a model that explains how information is encoded and decoded {{by a series of}} trains of pulses, i.e. action potentials. Thus, one of the fundamental questions of neuroscience is to determine if neurons communicate by a rate or temporal code. Temporal coding suggests that a single <b>spiking</b> <b>neuron</b> can replace hundreds of hidden units on a sigmoidal neural net.|$|E
50|$|The {{speed of}} signal {{transmission}} at 200 Hz, the most conserved bandwidth of signal transmission for non-spiking neurons, was approximately 2500 bits/second {{in which there}} was a 10-15% decrease in speed as the signal propagated down the axon. A <b>spiking</b> <b>neuron</b> compares at 200bits/ second, but reconstruction is greater and there is less influence by noise. There are other non-spiking neurons that exhibit conserved signal transmission at other bandwidths.|$|E
50|$|The first {{scientific}} {{model of}} a <b>spiking</b> <b>neuron</b> was proposed by Alan Lloyd Hodgkin and Andrew Huxley in 1952. This model describes how action potentials are initiated and propagated. Spikes, however, are not generally transmitted directly between neurons. Communication requires the exchange of chemical substances in the synaptic gap, called neurotransmitters. The complexity and variability of biological models have resulted in various neuron models, such as the integrate-and-fire (1907), FitzHugh-Nagumo model (1961-1962) and Hindmarsh-Rose model (1984).|$|E
40|$|We {{investigate}} the computational {{power of a}} formal model for networks of <b>spiking</b> <b>neurons.</b> It is shown that simple operations on phasedifferences between spike-trains provide a very powerful computational tool that can in principle be used to carry out highly complex computations on a small network of <b>spiking</b> <b>neurons.</b> We construct networks of <b>spiking</b> <b>neurons</b> that simulate arbitrary threshold circuits, Turing machines, and {{a certain type of}} random access machines with real valued inputs. We also show that relatively weak basic assumptions about the response- and threshold-functions of the <b>spiking</b> <b>neurons</b> are sufficient in order to employ them for such computations. 1 Introduction and Basic Definitions There exists substantial evidence that timing phenomena such as temporal differences between spikes and frequencies of oscillating subsystems are integral parts of various information processing mechanisms in biological neural systems (for a survey and references see e. g. Kandel et al., [...] ...|$|R
40|$|Abstract. We propose an {{event-driven}} framework {{dedicated to}} the design and the simulation of networks of <b>spiking</b> <b>neurons.</b> It consists of an abstract model of <b>spiking</b> <b>neurons</b> and an efficient event-driven simulation engine so as to achieve good performance in the simulation phase while maintaining {{a high level of}} flexibility and programmability in the modelling phase. Our model of neurons encompasses a large class of <b>spiking</b> <b>neurons</b> ranging from usual leaky integrate-and-fire neurons to more abstract neurons, e. g. defined as complex finite state machines. As a result, the proposed framework allows the simulation of large networks that can be composed of unique or different types of neurons. ...|$|R
40|$|An {{important}} open {{problem of}} computational neuroscience is the generic organization of computations in networks of neurons in the brain. We show here through rigorous theoretical analysis that inherent stochastic features of <b>spiking</b> <b>neurons,</b> {{in combination with}} simple nonlinear computational operations in specific network motifs and dendritic arbors, enable networks of <b>spiking</b> <b>neurons</b> to carry out probabilistic inference through sampling in general graphical models. In particular, it enables them to carry out probabilistic inference in Bayesian networks with converging arrows ("explaining away") and with undirected loops, that occur in many real-world tasks. Ubiquitous stochastic features of networks of <b>spiking</b> <b>neurons,</b> such as trial-to-trial variability and spontaneous activity, are necessary ingredients of the underlying computational organization. We demonstrate through computer simulations that this approach can be scaled up to neural emulations of probabilistic inference in fairly large graphical models, yielding {{some of the most}} complex computations that have been carried out so far in networks of <b>spiking</b> <b>neurons...</b>|$|R
50|$|Many of the nonspiking neurons {{are found}} near neuromuscular {{junctions}} and exist as long fibers {{that help to}} innervate certain motor nerves such as the thoracic-coxalmuscle receptor organ (TCMRO) of a crab. They function in a modulatory role by helping to establish posture and directional behavior. This was intensely modeled in the crustacean and in insects showing how appendages are oriented via these nonspiking neural pathways. Amacrine cells are another major type of non-spiking neuron and their lifetime involves the conversion to a non-spiking neuron from a <b>spiking</b> <b>neuron</b> once the retina obtains maturity. They {{are one of the}} first cells to differentiate during prenatal development. Upon the opening of the eyes, these cells begin to shed their sodium ion channels and become non-spiking neurons. It was hypothesized that the reason for its establishment as a <b>spiking</b> <b>neuron</b> was to help with the maturation of the retina by the usage of action potentials themselves, and not necessarily the information the action potential carried. This was supported with the occurrence of synchronous firing by the starburst amacrine cells during the initial stages of development. This study used a rabbit model.|$|E
5000|$|The FitzHugh - Nagumo {{model is}} a {{simplified}} version of the Hodgkin - Huxley model which models in a detailed manner activation and deactivation dynamics of a <b>spiking</b> <b>neuron.</b> In the original papers of FitzHugh, this model was called Bonhoeffer - van der Pol oscillator (named after Karl Friedrich Bonhoeffer and Balthasar van der Pol) because it contains the van der Pol oscillator as a special case for [...] The equivalent circuit was suggested by Jin-ichi Nagumo, Suguru Arimoto, and Shuji Yoshizawa.http://www.siam.org/news/news.php?id=647 ...|$|E
50|$|The {{generation}} of the action potential is called the “firing.” The firing neuron described above is called a <b>spiking</b> <b>neuron.</b> We will model the electrical circuit of the neuron in Section 3.6. There {{are two types of}} spiking neurons. If the stimulus remains above the threshold level and the output is a spike train, it is called the Integrate-and-Fire (IF) neuron model. If output is modeled as dependent on the impulse response of the circuit, then it is called the Spike Response Model (SRM) (Gestner, W. (1995)).|$|E
40|$|We {{characterize}} {{the class of}} functions with real-valued input and out-put which can be computed by networks of <b>spiking</b> <b>neurons</b> with piecewise linear response- and threshold-functions and unlimited timing precision. We show that this class coincides with the class of functions computable by recurrent analog neural nets with piecewise linear activation functions, and with the class of functions computable on {{a certain type of}} random access machine (N-RAM) which we introduce in this article. This result is proven via constructive real-time simulations. Hence it provides in par-ticular a convenient method for constructing networks of <b>spiking</b> <b>neurons</b> that compute a given real-valued function f: it now suces to write a program for computing f on an N-RAM; that program can be -ically " transformed into an equivalent network of <b>spiking</b> <b>neurons</b> (by our simulation result). Finally, one learns from the results of this paper that certain very sim-ple piecewise linear response- and threshold-functions for <b>spiking</b> <b>neurons</b> are universal, in the sense that neurons with these particular response-and threshold-functions can simulate networks of <b>spiking</b> <b>neurons</b> with ar-bitrary piecewise linear response- and threshold-functions. The results of this paper also show that certain very simple piecewise linear activation functions are in a corresponding sense universal for recurrent analog neural nets...|$|R
50|$|Brian is an {{open source}} Python package for {{developing}} simulations of networks of <b>spiking</b> <b>neurons.</b>|$|R
40|$|Bayesian <b>spiking</b> <b>neurons</b> (BSNs) {{provide a}} probablisitic and {{intuitive}} interpretation of how <b>spiking</b> <b>neurons</b> could work {{and have been}} shown to be equivalent to leaky integrate-and-fire neurons under certain condi-tions [1]. The study of BSNs has been restricted mainly to small networks because online learning, which cur-rently involves a maximum-likelihood-expectation-maxi-misation (ML-EM) approach [2, 3], is quite slow. Here a new approach to estimating the parameters of Bayesian <b>spiking</b> <b>neurons,</b> referred to as fast learning (FL), is pre-sented and compared to online ML-EM learning. Learning in a BSN is local to the neuron and involves estimation of the transition rate and observation rate parameters of an underlying implicit hidden Markov model (HMM), the hidden state of which the BSN outpu...|$|R
40|$|In {{this paper}} we review {{some of our}} recent results on discrete-state <b>spiking</b> <b>neuron</b> models. The discrete-state <b>spiking</b> <b>neuron</b> model is a wired system of shift {{registers}} and can generate various spike-trains by adjusting {{the pattern of the}} wirings. In this paper we show basic relations between the wiring pattern and characteristics of the spike-train. We also show a learning algorithm which utilizes successive changes of the wiring pattern. It is shown that the learning algorithm enables the neuron to approximate various spike-trains generated by a chaotic analog <b>spiking</b> <b>neuron...</b>|$|E
40|$|In a {{previous}} work (Mohemmed et al., Method for training a <b>spiking</b> <b>neuron</b> to associate input–output spike trains) [1] we {{have proposed a}} supervised learning algorithm based on temporal coding to train a <b>spiking</b> <b>neuron</b> to associate input spatiotemporal spike patterns to desired output spike patterns. The algorithm {{is based on the}} conversion of spike trains into analogue signals and the application of the Widrow–Hoff learning rule. In this paper we present a mathematical formulation of the proposed learning rule. Furthermore, we extend the application of the algorithm to train a SNN consisting of multiple spiking neurons to perform spatiotemporal pattern classification and we show that the accuracy of classification is improved significantly over a single <b>spiking</b> <b>neuron.</b> We also investigate a number of possibilities to map the temporal output of the trained <b>spiking</b> <b>neuron</b> into a class label. Potential applications for motor control in neuro-rehabilitation and neuro-prosthetics are discussed as a future work...|$|E
40|$|International audienceIn {{the present}} overview, our wish is to demystify {{some aspects of}} coding with spike-timing, through a simple review of well-understood {{technical}} facts regarding spike coding. Our goal is {{a better understanding of}} the extent to which computing and modeling with <b>spiking</b> <b>neuron</b> networks might be biologically plausible and computationally efficient. We intentionally restrict ourselves to a deterministic implementation of <b>spiking</b> <b>neuron</b> networks and we consider that the dynamics of a network is defined by a non-stochastic mapping. By staying in this rather simple framework, we are able to propose results, formula and concrete numerical values, on several topics: (i) general time constraints, (ii) links between continuous signals and spike trains, (iii) <b>spiking</b> <b>neuron</b> networks parameter adjustment. Beside an argued review of several facts and issues about neural coding by spikes, we propose new results, such as a numerical evaluation of the most critical temporal variables that schedule the progress of realistic spike trains. When implementing <b>spiking</b> <b>neuron</b> networks, for biological simulation or computational purpose, it is important to take into account the indisputable facts here unfolded. This precaution could prevent one from implementing mechanisms that would be meaningless relative to obvious time constraints, or from artificially introducing spikes when continuous calculations would be sufficient and more simple. It is also pointed out that implementing a large-scale <b>spiking</b> <b>neuron</b> network is finally a simple task...|$|E
40|$|Colloque avec actes et comité de lecture. internationale. International audienceWe propose an {{event-driven}} framework {{dedicated to}} the design and the simulation of networks of <b>spiking</b> <b>neurons.</b> It consists of an abstract model of <b>spiking</b> <b>neurons</b> and an efficient event-driven simulation engine so as to achieve good performance in the simulation phase while maintaining {{a high level of}} flexibility and programmability in the modelling phase. Our model of neurons encompasses a large class of <b>spiking</b> <b>neurons</b> ranging from usual leaky integrate-and-fire neurons to more abstract neurons, e. g. defined as complex finite state machines. As a result, the proposed framework allows the simulation of large networks that can be composed of unique or different types of neurons...|$|R
40|$|We {{consider}} a statistical framework for learning {{in a class}} of net-works of <b>spiking</b> <b>neurons.</b> Our aim is to show how optimal local learning rules can be readily derived once the neural dynamics and desired functionality of the neural assembly have been speci¯ed, in contrast to other models which assume (sub-optimal) learning rules. Within this framework we derive local rules for learning tem-poral sequences in a model of <b>spiking</b> <b>neurons</b> and demonstrate its superior performance to correlation (Hebbian) based approaches. We further show how to include mechanisms such as synaptic de-pression and outline how the framework is readily extensible to learning in networks of highly complex <b>spiking</b> <b>neurons.</b> A stochas-tic quantal vesicle release mechanism is considered and implications on the complexity of learning discussed. ...|$|R
40|$|Abstract. We propose an {{implementation}} of covert attention mecha-nisms with <b>spiking</b> <b>neurons.</b> <b>Spiking</b> neural models describe {{the activity of}} a neuron with precise spike-timing rather than firing rate. We inves-tigate the interests offered by such a temporal code for low-level vision and early attentional process. This paper describes a spiking neural net-work which achieves saliency extraction and stable attentional focus of a moving stimulus. Experimental results obtained using real visual scene illustrate the robustness and the quickness of this approach. Key words: <b>spiking</b> <b>neurons,</b> precise spike-timing, covert attention, saliency...|$|R
40|$|We {{investigate}} the computational {{power of a}} model for a <b>spiking</b> <b>neuron</b> in the Boolean domain by comparing it with traditional neuron models such as threshold gates (or McCulloch-Pitts neurons) and sigma-pi units (or polynomial threshold gates). In particular, we estimate the number of gates required to simulate a <b>spiking</b> <b>neuron</b> by a disjunction of threshold gates and we establish tight bounds for this threshold number. Furthermore, we analyze the degree of the polynomials that a sigma-pi unit must use for the simulation of a <b>spiking</b> <b>neuron.</b> We show that this degree cannot be bounded by any fixed value. Our results give evidence that the use of continuous time as a computational resource endows single-cell models with substantially larger computational capabilities...|$|E
40|$|The {{high level}} of realism of <b>spiking</b> <b>neuron</b> {{networks}} and their complexity require a considerable computational resources limiting {{the size of the}} realized networks. Consequently, the main challenge in building complex and biologically accurate <b>spiking</b> <b>neuron</b> network is largely set by the high computational and data transfer demands. In this thesis, I implement several efficient models of the <b>spiking</b> <b>neuron</b> with characteristics such as axon conduction delays and spike timing-dependent plasticity in a real-time data-flow learning network. With the performance analysis, the trade-offs between the biophysical accuracy and computation complexity are defined for the different models. The experimental results indicate that the proposed real-time data-flow learning network architecture allows the capacity of over 1, 188 (max. 6, 300, depending on the model complexity) biophysically accurate neurons in a single FPGA device. Circuits and System...|$|E
40|$|Abstract- A novel {{algorithm}} named Spike-LMS {{is described}} that adapts the synaptic weights of an artificial <b>spiking</b> <b>neuron</b> {{to produce a}} desired response. The derivation of Spike-LMS follows from the derivation of the Least-Mean Squares (LMS) algorithm used in adaptive filter theory. Spike-LMS works directly {{in the domain of}} spike trains, and therefore makes no assumptions about any particular neural encoding method. This algorithm is able to identify the synaptic weights of a <b>spiking</b> <b>neuron</b> given the pre-synaptic and post-synaptic spike trains. ...|$|E
40|$|We use a two-layered {{bio-inspired}} {{neural network}} to segregate sound sources, i. e. double-vowels or intruding noises in speech. The {{architecture of the}} network consists of <b>spiking</b> <b>neurons.</b> The <b>spiking</b> <b>neurons</b> in both layers are modelized by relaxation oscillators. The first layer of the network is locally connected, while the second layer is a fully connected network. Our auditory image {{is based on the}} reassigned spectrum technique. No prior estimation or knowledge of pitch is necessary for the segregation...|$|R
40|$|Abstract. Current digital, {{directly}} mapped implementations of spiking {{neural networks}} use serial processing and parallel arithmetic. On a standard CPU, {{this might be the}} good choice, but when using a Field Programmable Gate Array (FPGA), other implementation architectures are possible. This work present a hardware implementation of a broad class of integrate and fire <b>spiking</b> <b>neurons</b> with synapse models using parallel processing and serial arithmetic. This results in very fast and compact implementations of <b>spiking</b> <b>neurons</b> on FPGA. ...|$|R
5000|$|Spaun (Semantic Pointer Architecture Unified Network) - by Chris Eliasmith at the Centre for Theoretical Neuroscience at the University of Waterloo - Spaun is {{a network}} of 2,500,000 {{artificial}} <b>spiking</b> <b>neurons,</b> which uses groups of these neurons to complete cognitive tasks via flexibile coordination. Components of the model communicate using <b>spiking</b> <b>neurons</b> that implement neural representations called [...] "semantic pointers" [...] using various firing patterns. Semantic pointers {{can be understood as}} being elements of a compressed neural vector space.|$|R
