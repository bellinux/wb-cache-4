1|26|Public
40|$|The {{experiment}} had {{the objective}} of fitting regression models to data of {{the height of the}} bedding plants cultivated in three multicellular Styrofoam trays with three different cell volumes. We proposed two types of models in the current experiment. First, we fit a model with normal errors and next a model with a skew-normal distribution of errors. The skew-normal regression was suitable for modelling both cases. First, when the model included the time covariate and next when the cell size covariate was part of the model. However, the value of the parameter l for the multivariate model was very high, which is an indication that the skew-normal model is also not the best. Thus, we suggest further fitting using the <b>skew</b> <b>regression</b> model of t-Student. ...|$|E
40|$|Background The {{most common}} {{chromosomal}} abnormality due to non-obstructive azoospermia (NOA) is Klinefelter syndrome (KS) which occurs in 1 - 1. 72 out of 500 - 1000 male infants. The probability of retrieving sperm as the outcome could be asymmetrically different between patients {{with and without}} KS, therefore logistic regression analysis is not a well-qualified test {{for this type of}} data. This study has been designed to evaluate <b>skewed</b> <b>regression</b> model analysis for data collected from microsurgical testicular sperm extraction (micro-TESE) among azoospermic patients with and without non-mosaic KS syndrome. Materials and Methods This cohort study compared the micro-TESE outcome between 134 men with classic KS and 537 men with NOA and normal karyotype who were referred to Royan Institute between 2009 and 2011. In addition to our main outcome, which was sperm retrieval, we also used logistic and <b>skewed</b> <b>regression</b> analyses to compare the following demographic and hormonal factors: age, level of follicle stimulating hormone (FSH), luteinizing hormone (LH), and testosterone between the two groups. Results A comparison of the micro-TESE between the KS and control groups showed a success rate of 28. 4...|$|R
40|$|In this paper, {{we discuss}} a novel class of skewed multivariate {{distributions}} and, more generally, {{a method of}} building such a class {{on the basis of}} univariate skewed distributions. The method is based on a general linear transformation of a multidimensional random variable with independent components, each with a skewed distribution. The proposed class of multivariate skewed distributions has a simple form for the pdf, and moment existence only depends on that of the underlying symmetric univariate distributions. In addition, we can freely allow for any mean and covariance structure in combination with any magnitude and direction of skewness. In order to deal with both skewness and fat tails, we introduce multivariate <b>skewed</b> <b>regression</b> models with fat tails based on Student distributions. We present two main classes of such distributions, one of which is novel even under symmetry. Under standard non-informative priors on both regression and scale parameters, we derive conditions for propriety of the posterior and for existence of posterior moments. We describe MCMC samplers for Bayesian inference and analyse an application to biomedical data. ...|$|R
40|$|This paper {{introduces}} a <b>skewed</b> log-Birnbaum-Saunders <b>regression</b> model {{based on the}} skewed sinh-normal distribution proposed by Leiva et al. [A skewed sinh-normal distribution and its properties and application to air pollution, Comm. Statist. Theory Methods 39 (2010), pp. 426 - 443]. Some influence methods, such as the local influence and generalized leverage, are presented. Additionally, we derived the normal curvatures of local influence under some perturbation schemes. An empirical application to a real data set is presented in order to illustrate {{the usefulness of the}} proposed model. FAPESP (Brazil) FAPESP (Brazil...|$|R
40|$|In this paper, we {{introduce}} a novel class of skewed multivariate distributions and, more generally, {{a method of}} building such a class {{on the basis of}} univariate skewed distributions. The method is based on a general linear transformation of a multidimensional random variable with independent components, each with a skewed distribution. Our proposed class of multivariate skewed distributions has a simple, intuitive form for the pdf, moment existence only depends on the existence of the moments of the underlying symmetric univariate distributions, and we avoid any conditioning on unobserved variables. In addition, we can freely allow for any mean and covariance structure in combination with any magnitude and direction of skewness. In order to deal with both skewness and fat tails, we introduce multivariate <b>skewed</b> <b>regression</b> models with fat tails, based on Student distributions. We present two main classes of such distributions, one of which is novel even under symmetry. Under standard non-informative priors on both regression and scale parameters, we derive conditions for propriety of the posterior and for existence of posterior moments. We describe MCMC samplers for conducting Bayesian inference and analyse two applications, one concerning the distribution of various measures of firm size and another on a set of biomedical data. Asymmetric distributions; Heavy tails; Linear regression model; Mardia's measure of skewness; Orthogonal matrices; Posterior propriety. ...|$|R
40|$|The goal of our {{research}} was to apply SVMs (Support Vector Machines) {{to the problem of}} parse selection. More specifically, to the parse trees produced by Alpino, and to compare its performance with the current Alpino disambiguation component, which is based on Maximum Entropy. There are two basic problems to be dealt with when applying a machine learning technique to parse selection. The first is how to compare different parse trees to each other. We addressed this problem {{in the same way that}} it had been already addressed by Alpino, which allowed us to turn to (structured) trees into vectors in Nn. The second issue is whether to consider the problem as a classification or as a regression problem. Many view parse selection as a classification problem, in a one-against-all manner, but it is actually a <b>skewed</b> <b>regression</b> problem. While the data modeled takes values in the interval [0, 1], the evaluation of success is effectively measured by how well the upper end of the interval (the higher scores) were modeled. That holds as long as the lower scores are actually kept low somehow. Despite the skewness of the evaluation, we believe that parse selection is inherently a regression problem, and have chosen to use SV regression to perform parse selection, instead of the more popular SV classification. ...|$|R
40|$|The {{paper by}} Leiva et al. (2010) {{introduced}} a skewed {{version of the}} sinh-normal distribution, discussed some of its properties and characterized {{an extension of the}} Birnbaum-Saunders distribution associated with this distribution. In this paper, we introduce a <b>skewed</b> log-Birnbaum-Saunders <b>regression</b> model based on the skewed sinh-normal distribution. Some influence methods, such as the local influence and generalized leverage are presented. Additionally, we derived the normal curvatures of local influence under some perturbation schemes. An empirical application to a real data set is presented in order to illustrate the usefulness of the proposed model. Comment: Submitted for publicatio...|$|R
40|$|This paper {{presents}} a comprehensive comparison of well-known partially adaptive estimators (PAEs) {{in terms of}} efficiency in estimating regression parameters. The aim is to identify the best estimators of regression parameters when error terms follow from normal, Laplace, Student's t, normal mixture, lognormal and gamma distribution via the Monte Carlo simulation. In {{the results of the}} simulation, efficient PAEs are determined in the case of symmetric leptokurtic and <b>skewed</b> leptokurtic <b>regression</b> error data. Additionally, these estimators are also compared in terms of regression applications. Regarding these applications, using certain standard error estimators, it is shown that PAEs can reduce the standard error of the slope parameter estimate relative to ordinary least squares. ...|$|R
40|$|We {{present a}} simple {{regression}} technique, called Versatile Regression, where the error distribution {{is described by}} the Generalized Lambda Distribution. The flexibility of this distribution allows the error distribution to be heavy-tailed, skewed or approximately normal. Versatile Regression was found to perform well on heavy-tailed and <b>skewed</b> data. Versatile <b>Regression</b> also provided a reasonable approximation to Normal-Error Regression. Simulation studies found that Versatile Regression produced accurate parameter estimates...|$|R
40|$|Estimates of flood skew are {{inaccurate}} and the inaccuracy influences {{flood discharge}} estimates using the log Pearson Type III distribution. The skew map {{is commonly used}} {{despite the fact that}} it's inaccurate, lacks a conceptual basis, and does not reflect watershed processes. Attempts at regionalizing station <b>skew</b> using <b>regression</b> analysis have only provided marginal improvements in accuracy, possibly because the predictor variables are not good indicators of the physical characteristics that influence the variation in skewness. Therefore, a new approach is needed to improve skew estimates. This research explored the potential of using a distributed model that includes predictor variables that better represent watershed storage. The results showed that watershed storage is the main factor that affects flood skew, and that increases in watershed storage causes flood skews to be algebraically more negative...|$|R
40|$|See the Editorial Commentary by Craig {{on pages}} 1084 – 5.) Background. Morbidity and {{mortality}} for critically ill patients with infections remains a global healthcare prob-lem. We aimed {{to determine whether}} β-lactam antibiotic dosing in critically ill patients achieves concentrations as-sociated with maximal activity and whether antibiotic concentrations affect patient outcome. Methods. This was a prospective, multinational pharmacokinetic point-prevalence study including 8 β-lactam antibiotics. Two blood samples were taken from each patient during a single dosing interval. The primary pharma-cokinetic/pharmacodynamic targets were free antibiotic concentrations above the minimum inhibitory concentra-tion (MIC) of the pathogen at both 50 % (50 % f T>MIC) and 100 % (100 % f T>MIC) of the dosing interval. We used <b>skewed</b> logistic <b>regression</b> to describe the effect of antibiotic exposure on patient outcome. Results. We included 384 patients (361 evaluable patients) across 68 hospitals. The median age was 61 (inter...|$|R
40|$|The {{mixed model}} {{approach}} to semiparametric regression is considered for stochastic frontier models, with focus on clustered data. Standard {{assumptions about the}} model component representing the inefficiency effect lead to a closed skew normal distribution for the response. Model parameters are estimated by a generalization of restricted maximum likelihood, and random effects are estimated by an orthodox best linear unbiased prediction procedure. The method is assessed by means of Monte Carlo studies, and illustrated by an empirical application on hospital productivity. Clustered data Efficiency evaluation Flexible frontier Random effect Semiparametric <b>regression</b> <b>Skew</b> normality...|$|R
40|$|A {{critical}} issue in modelling binary response data {{is the choice}} of the links. We introduce a new link based on the generalized t-distribution. There are two parameters in the generalized t-link: one parameter purely controls the heaviness of the tails of the link and the second parameter controls the scale of the link. Two major advantages are offered by the generalized t-links. First, a symmetric generalized t-link with an unknown shape parameter is much more identifiable than a Student t-link with unknown degrees of freedom and a known scale parameter. Secondly, skewed generalized t-links with both unknown shape and scale parameters provide much more flexible and improved <b>skewed</b> link <b>regression</b> models than the existing skewed links. Various theoretical properties and attractive features of the proposed links are examined and explored in detail. An efficient Markov chain Monte Carlo algorithm is developed for sampling from the posterior distribution. The deviance information criterion measure is used for guiding the choice of links. The proposed methodology is motivated and illustrated by prostate cancer data. Copyright 2008, Oxford University Press. ...|$|R
40|$|We {{present a}} Distributionally Robust Optimization (DRO) {{approach}} to outlier detection in a linear regression setting, where {{the closeness of}} probability distributions is measured using the Wasserstein metric. Training samples contaminated with outliers <b>skew</b> the <b>regression</b> plane computed by least squares and thus impede outlier detection. Classical approaches, such as robust regression, remedy this problem by downweighting the contribution of atypical data points. In contrast, our Wasserstein DRO approach hedges against a family of distributions that {{are close to the}} empirical distribution. We show that the resulting formulation encompasses a class of models, which include the regularized Least Absolute Deviation (LAD) as a special case. We provide new insights into the regularization term and give guidance on the selection of the regularization coefficient from the standpoint of a confidence region. We establish two types of performance guarantees for the solution to our formulation under mild conditions. One is related to its out-of-sample behavior, and the other concerns the discrepancy between the estimated and true regression planes. Extensive numerical results demonstrate the superiority of our approach to both robust regression and the regularized LAD in terms of estimation accuracy and outlier detection rates...|$|R
40|$|In the {{information}} system research, {{a question of}} particular interest is to interpret and to predict {{the probability of a}} firm to adopt a new technology such that market promotions are targeted to only those firms that were more likely to adopt the technology. Typically, there exists significant difference between the observed number of ``adopters'' and ``nonadopters,'' which is usually coded as binary response. A critical issue involved in modeling such binary response data is the appropriate choice of link functions in a regression model. In this paper we introduce a new flexible skewed link function for modeling binary response data based on the generalized extreme value (GEV) distribution. We show how the proposed GEV links provide more flexible and improved <b>skewed</b> link <b>regression</b> models than the existing skewed links, especially when dealing with imbalance between the observed number of 0 's and 1 's in a data. The flexibility of the proposed model is illustrated through simulated data sets and a billing data set of the electronic payments system adoption from a Fortune 100 company in 2005. Comment: Published in at [URL] the Annals of Applied Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|R
40|$|Recently, {{variable}} selection by penalized likelihood has attracted much research interest. In this paper, we propose adaptive Lasso quantile regression (BALQR) from a Bayesian perspective. The method extends the Bayesian Lasso quantile regression by allowing different penalization parameters for different regression coefficients. Inverse gamma prior distributions {{are placed on}} the penalty parameters. We treat the hyperparameters of the inverse gamma prior as unknowns and estimate them {{along with the other}} parameters. A Gibbs sampler is developed to simulate the parameters from the posterior distributions. Through simulation studies and analysis of a prostate cancer data set, we compare the performance of the BALQR method proposed with six existing Bayesian and non-Bayesian methods. The simulation studies and the prostate cancer data analysis indicate that the BALQR method performs well in comparision to the other approaches. Gibbs sampler, Lasso, Quantile <b>regression,</b> <b>Skewed</b> Laplace distribution. ...|$|R
30|$|Given the {{dependence}} between successive attempts {{of the same}} volunteer, a mixed-effect logistic regression model was used [20]. As operator’s selection {{was based on a}} block design, sex was treated as a random block effect. Because observed times appeared <b>skewed,</b> a mixed-effect <b>regression</b> model with auto-correlated errors was used with time being lognormal. Point and interval estimates were then retransformed using the exponential function to present the results in conventional units. Since no failure was observed in all operator groups at several occasions, the p value for differences between attempts using AWS was derived by comparing the results of each attempt with respect to the first one, the one with the lowest number of successes for this device, and then correcting for multiple testing using Holm’s procedure. In case of undefined logits, the sign test was carried out for local testing and simple logistic regression was used in all other cases.|$|R
40|$|Problem statement: Several {{studies have}} been carried out on the {{modeling}} of claim severity data in actuarial literature as well as in insurance practice. Since it is well established that the claim cost distributions generally have positive support and are positively <b>skewed,</b> the <b>regression</b> models of Gamma and Lognormal have been used by practitioners for modeling claim severities. However, the fitting of claim severities via regression models assumes that the claim types are independent. Approach: In this study, independent assumption between claim types will be investigated as we will consider three types of Malaysian motor insurance claims namely Third Party Body Injury (TPBI), Third Party Property Damage (TPPD) and Own Damage (OD) and applied the normal, t, Frank and Clayton copulas for modeling dependence structures between these claim types. Results: The AIC and BIC indicated that the Clayton is the best copula for modeling dependence between TPBI and OD claims and between TPPD and OD claims, whereas the t-copula is the best copula for modeling dependence between TPBI and TPPD claims. Conclusion: This study modeled the dependence between insurance claim types using copulas on the Malaysian motor insurance claim severity data. The main advantage of using copula is that each marginal distribution can be specified independently based on the distribution of individual variable and then joined by the copula which takes into account the dependence between these variables. Based on the results, the estimated of copula parameter for claim severities indicate that the dependence between claim types is significant...|$|R
40|$|Background.  Morbidity and {{mortality}} for critically ill patients with infections remains a global healthcare problem. We aimed {{to determine whether}} β-lactam antibiotic dosing in critically ill patients achieves concentrations associated with maximal activity and whether antibiotic concentrations affect patient outcome. Methods.  This was a prospective, multinational pharmacokinetic point-prevalence study including 8 β-lactam antibiotics. Two blood samples were taken from each patient during a single dosing interval. The primary pharmacokinetic/pharmacodynamic targets were free antibiotic concentrations above the minimum inhibitory concentration (MIC) of the pathogen at both 50 % (50 % f T>MIC) and 100 % (100 % f T>MIC) of the dosing interval. We used <b>skewed</b> logistic <b>regression</b> to describe the effect of antibiotic exposure on patient outcome. Results.  We included 384 patients (361 evaluable patients) across 68 hospitals. The median age was 61 (interquartile range [IQR], 48 - 73) years, the median Acute Physiology and Chronic Health Evaluation II score was 18 (IQR, 14 - 24), and 65 % of patients were male. Of the 248 patients treated for infection, 16 % did not achieve 50 % f T>MIC and these patients were 32 % {{less likely to have}} a positive clinical outcome (odds ratio [OR], 0. 68; P =. 009). Positive clinical outcome was associated with increasing 50 % f T>MIC and 100 % f T>MIC ratios (OR, 1. 02 and 1. 56, respectively; P <. 03), with significant interaction with sickness severity status. Conclusions.  Infected critically ill patients may have adverse outcomes as a result of inadeqaute antibiotic exposure; a paradigm change to more personalized antibiotic dosing may be necessary to improve outcomes for these most seriously ill patients. status: publishe...|$|R
40|$|Parameter {{estimates}} from commonly used multivariable parametric survival regression models do not directly quantify differences in {{years of life}} expectancy. Gaussian linear regression models give results in terms of absolute mean differences, but are not appropriate in modeling life expectancy, because in many situations time to death has a negative <b>skewed</b> distribution. A <b>regression</b> approach using a skew-normal distribution would be an alternative to parametric survival models in the modeling of life expectancy, because parameter estimates can be interpreted in terms of survival time differences while allowing for skewness of the distribution. In this paper we show {{how to use the}} skew-normal regression so that censored and left-truncated observations are accounted for. With this we model differences in life expectancy using data from the Swiss National Cohort Study and from official life expectancy estimates and compare the results with those derived from commonly used survival regression models. We conclude that a censored skew-normal survival regression approach for left-truncated observations can be used to model differences in life expectancy across covariates of interest...|$|R
40|$|BACKGROUND. Length of stay (LOS) is an {{important}} measure of hospital activity and health care utilization, but its empirical distribution is often positively skewed. OBJECTIVE. This study reviews the mean and median regression approaches for analyzing LOS, which have implications for service planning, resource allocation, and bed utilization. METHODS. The two approaches are applied to analyze hospital discharge data on cesarean delivery. Both models adjust for patient and health-related characteristics, and for the dependency of LOS outcomes nested within hospitals. The estimation methods are also compared in a simulation study. RESULTS. For the empirical application, the mean regression results are somewhat sensilive to the magnitude of trimming chosen. The identified factors from median regression, namely number of diagnoses, number of procedures, and payment classification, are robust to high-LOS outliers. The simulation experiment shows that median regression can outperform mean regression even when the response variable is moderately positively <b>skewed.</b> CONCLUSION. Median <b>regression</b> {{appears to be a}} suitable alternative to analyze the clustered and positively skewed LOS, without transforming and trimming the data arbitrarily. © 2003 Lippincott Williams & Wilkins, Inc. link_to_subscribed_fulltex...|$|R
40|$|Abstract Background Health-related {{quality of}} life (HRQL) has become an {{increasingly}} important outcome parameter in clinical trials and epidemiological research. HRQL scores are typically bounded {{at both ends of}} the scale and often highly <b>skewed.</b> Several <b>regression</b> techniques have been proposed to model such data in cross-sectional studies, however, methods applicable in longitudinal research are less well researched. This study examined the use of beta regression models for analyzing longitudinal HRQL data using two empirical examples with distributional features typically encountered in practice. Methods We used SF- 6 D utility data from a German older age cohort study and stroke-specific HRQL data from a randomized controlled trial. We described the conceptual differences between mixed and marginal beta regression models and compared both models to the commonly used linear mixed model in terms of overall fit and predictive accuracy. Results At any measurement time, the beta distribution fitted the SF- 6 D utility data and stroke-specific HRQL data better than the normal distribution. The mixed beta model showed better likelihood-based fit statistics than the linear mixed model and respected the boundedness of the outcome variable. However, it tended to underestimate the true mean at the upper part of the distribution. Adjusted group means from marginal beta model and linear mixed model were nearly identical but differences could be observed with respect to standard errors. Conclusions Understanding the conceptual differences between mixed and marginal beta regression models is important for their proper use in the analysis of longitudinal HRQL data. Beta regression fits the typical distribution of HRQL data better than linear mixed models, however, if focus is on estimating group mean scores rather than making individual predictions, the two methods might not differ substantially. </p...|$|R
40|$|Background: Hepatocellular {{carcinoma}} (HCC) is endemic {{in parts}} of Asia and Africa and most patients are not suitable for treatment with a curative approach. Little {{is known about the}} cost of palliative care for HCC. Objective: To determine: (i) patient-specific costs of palliative care of HCC; and (ii) individual factors that drive patient-specific costs and to develop a model of cost per case under alternative circumstances. Methods: 204 patients with inoperable HCC were prospectively tracked from first hospitalisation until death for health service utilisation. A societal perspective of cost was taken, including costs of formal and informal services incurred by payers, caregivers and patients. Observational data from a large Hong Kong cancer care programme were used. A regression analysis was performed using formal costs only, with the cost per observed day as the dependent variable. Results: The median survival was 95 days and the mean observation period was 153 days. The mean value per person for formal healthcare cost was 30 983 Hong Kong dollars [$HK] ($US 3872, 1998 values). The distribution of cost values were positively <b>skewed.</b> The <b>regression</b> analysis showed that age, days of observation and survival were negatively related to cost per observed day, and the Child-Pugh grading of severity of liver cirrhosis was positively related to cost per observed day. A sensitivity analysis based on the regression equation indicated that nonsurvivorship doubles the cost per case, increased severity as measured by the Child-Pugh Index adds about 50 % to the cost, and chemotherapy increases cost 2 -fold. Conclusions: The relatively modest average cost per patient with HCC in Hong Kong reflects the short median survival and subsequently the limited use of inpatient care and chemotherapy. Antineoplastics, Cost analysis, Herbal medicines, Liver cancer, Pharmacoeconomics...|$|R
40|$|Background. Morbidity and {{mortality}} for critically ill patients with infections remains a global healthcare problem. We aimed {{to determine whether}} ?-lactam antibiotic dosing in critically ill patients achieves concentrations associated with maximal activity and whether antibiotic concentrations affect patient outcome. Methods. This was a prospective, multinational pharmacokinetic point-prevalence study including 8 ?-lactam antibiotics. Two blood samples were taken from each patient during a single dosing interval. The primary pharmacokinetic/pharmacodynamic targets were free antibiotic concentrations above the minimum inhibitory concentration (MIC) of the pathogen at both 50 % (50 % f TMIC) and 100 % (100 % f T MIC) of the dosing interval. We used <b>skewed</b> logistic <b>regression</b> to describe the effect of antibiotic exposure on patient outcome. Results. We included 384 patients (361 evaluable patients) across 68 hospitals. The median age was 61 (interquartile range [IQR], 48 - 73) years, the median Acute Physiology and Chronic Health Evaluation II score was 18 (IQR, 14 - 24), and 65 % of patients were male. Of the 248 patients treated for infection, 16 % did not achieve 50 % f TMIC and these patients were 32 % {{less likely to have}} a positive clinical outcome (odds ratio [OR], 0. 68; P =. 009). Positive clinical outcome was associated with increasing 50 % f TMIC and 100 % f TMIC ratios (OR, 1. 02 and 1. 56, respectively; P <. 03), with significant interaction with sickness severity status. Conclusions. Infected critically ill patients may have adverse outcomes as a result of inadeqaute antibiotic exposure; a paradigm change to more personalized antibiotic dosing may be necessary to improve outcomes for these most seriously ill patients. 2014 The Author 2014. Published by Oxford University Press on behalf of the Infectious Diseases Society of America. All rights reserved. SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|R
40|$|This thesis {{consists}} of four empirical essays on option-implied information and asset pricing in the US market. The first essay examines the predictive ability of option-implied volatility measures proposed by previous studies by using firm-level option and stock data. This essay documents significant non-zero returns on long-short portfolios formed on call-put implied volatility spread, and implied volatility <b>skew.</b> Cross-sectional <b>regressions</b> show that the call-put implied volatility spread {{is the most important}} factor in predicting one-month ahead stock returns. For two-month and three-month ahead stock returns, “out-minus-at” of calls has stronger predictive ability. The second essay constructs pricing factors by using at-the-money option-implied volatilities and their first differences, and tests whether these pricing factors have significant risk premiums. However, results about significant risk premiums are limited. The third essay focuses on the relationship between an asset’s return and its sensitivity to aggregate volatility risk. First, to separate different market conditions, this study focuses on how VIX spot, VIX futures, and their basis perform different roles in asset pricing. Secondly, this essay decomposes the VIX index into two parts: volatility calculated from out-of-the-money call options and volatility calculated from out-of-the-money put options. The analysis shows that out-of-the-money put options capture more useful information in predicting future stock returns. The fourth essay concentrates on systematic standard deviation (i. e., beta) and skewness (i. e., gamma) by incorporating option-implied information. Portfolio level analysis shows that option-implied gamma performs better than historical gamma in explaining portfolio returns at longer horizons (five-month or longer). In addition, firm size plays an important role in explaining returns on constituents of the S&P 500 index. Finally, cross-sectional regression results confirm the existence of risk premiums on option-implied components for systematic standard deviation and systematic skewness calculation...|$|R
40|$|The {{capability}} to acquire accurate and reliable {{measurements of the}} wind is essential for wind turbine validation and wind resource assessment. These measurements are typically done with tall meteorological masts (met-masts). However, as of 2004 the concept of remote sensing has started to become specialised in wind energy. A key requirement before full implementation can be accepted, is a fully traceable uncertainty chain. The aim of this thesis is closing the uncertainty chain by providing an innovative best practise guideline {{in order to establish}} a quantifiable and repeatable classification uncertainty. After the closure the viability of remote sensing to replace met-masts has been assessed. The classification uncertainty comprises several steps, of which the assessment of interdependency of atmospheric variables was left open by the regulations. This thesis provides a tested automated procedure, hence contributes to both the wind industry and body of science. Based on the results of this thesis it was observed that the usage of thresholds, imposed by the regulations, led to erratic behaviour in the uncertainties. The acquired inconsistencies are significant {{to the extent that the}} author recommends not to use the classification uncertainties, and therefore the total uncertainties. As a direct result the author does not recommend a full replacement of met-masts. To improve the current uncertainty model, it is recommended to implement weighing functions to create a smoother output. Furthermore, additional requirements to the acceptance of binned data are recommended in order to avoid <b>skewing</b> of the <b>regression</b> due to biased data-sets, hereby increasing the robustness of the procedure. After these recommendations are implemented it is advised to do a second iteration of establishing the thresholds. Aerospace EngineeringAerodynamic...|$|R

