16|484|Public
50|$|A {{smooth curve}} through {{a set of}} data points {{obtained}} with this statistical technique is called a Loess Curve, particularly when each <b>smoothed</b> <b>value</b> is given by a weighted quadratic least squares regression over the span of values of the y-axis scattergram criterion variable. When each <b>smoothed</b> <b>value</b> is given by a weighted linear least squares regression over the span, this is known as a Lowess curve; however, some authorities treat Lowess and Loess as synonyms.|$|E
5000|$|... {{by which}} was meant [...] "the nth <b>smoothed</b> <b>value</b> {{of the time}} {{derivative}} of a radius R". Since the smoothing function indicated by the bar was {{left out of the}} specification for the program, the implementation treated normal minor variations of velocity as if they were serious, causing spurious corrections that sent the rocket off course. [...] It was then destroyed by the Range Safety Officer.|$|E
5000|$|... {st} {{represents}} the <b>smoothed</b> <b>value</b> {{of the constant}} part for time t. {bt} {{represents the}} sequence of best estimates of the linear trend that are superimposed on the seasonal changes. {ct} is the sequence of seasonal correction factors. ct is the expected proportion of the predicted trend at any time t mod L in the cycle that the observations take on. As a rule of thumb, {{a minimum of two}} full seasons (or 2L periods) of historical data is needed to initialize a set of seasonal factors.|$|E
2500|$|<b>Smooth</b> <b>values</b> found: 25952 by sieving directly, 24462 by {{combining}} numbers with large primes ...|$|R
50|$|The {{forward-backward}} {{algorithm is}} an efficient method for computing the <b>smoothed</b> <b>values</b> for all hidden state variables.|$|R
40|$|The {{method of}} {{smoothing}} observational data {{in its original}} form [2 – 3] did not allow the uncertainties of either the individual <b>smoothed</b> <b>values</b> or their function to be estimated. The present paper, using the same original system of linear equations but a different method for solving them (Cholesky decomposition instead of Gauss elimination), describes an algorithm to compute the covariances of the smoothed data in any selected band. These enable the uncertainties of any function of the <b>smoothed</b> <b>values</b> to be calculated...|$|R
50|$|The Jacobian has 10 columns, one {{for each}} of the {{parameters}} a00 − a03, and 35 rows, {{one for each}} pair of v and w values. Each row has the formThe convolution coefficients are calculated asThe first row of C contains 35 convolution coefficients, which can be multiplied with the 35 data values, respectively, to obtain the polynomial coefficient , which is the <b>smoothed</b> <b>value</b> at the central node of the kernel (i.e. at the 18th node of the above table). Similarly, other rows of C can be multiplied with the 35 values to obtain other polynomial coefficients, which, in turn, can be used to obtain smoothed values and different smoothed partial derivatives at different nodes.|$|E
30|$|On {{the other}} hand, for {{estimating}} response-time, we use single exponential smoothing {{because for the}} oscillatory response-time, {{we do not need}} to predict the trend but a <b>smoothed</b> <b>value.</b>|$|E
40|$|Various asset {{valuation}} methods {{are used in}} the context of funding valuations. The motivation for such methods and their properties are briefly described. Some <b>smoothed</b> <b>value</b> or market-related methods based on arithmetic averaging and exponential smoothing are considered and their effect on funding is discussed. Suggestions for further research are also made...|$|E
40|$|An {{exponential}} smoothing procedure {{applied to a}} homogeneous Markovian observation sequence generates an inhomogeneous Markov process as sequence of <b>smoothed</b> <b>values.</b> If the underlying observation sequence is moreover ergodic then for two classes of smoothing functions the strong ergodicity of the sequence of <b>smoothed</b> <b>values</b> is proved. As a consequence a central limit theorem and a law of large numbers hold true for the <b>smoothed</b> <b>values.</b> The proof uses general results for so-called convergent inhomogeneous Markov processes. In the literature {{a lot of time}} series are discussed to which the smoothing procedures are applicable. Smoothing of non-linear time series; Generalized {{exponential smoothing}}; Convergent Inhomogeneous Markov processes AMS 1991 Subject Classification: Primary 60 J 20 Secondary 62 M 10 1 Postal address: Fachbereich 11 Mathematik, Gerhard-Mercator-Universitat [...] - GH Duisburg, Postfach 10 15 03, D- 47048 Duisburg 1. Introduction Exponential smoothing procedures are well-kno [...] ...|$|R
30|$|Weighted mean of fr, fr +[*] 1, and fr +[*] 2 {{that are}} {{calculated}} by three successive windows is adopted {{as a substitute}} of fr, which can provide a more <b>smooth</b> <b>value</b> for evaluation [25].|$|R
50|$|This {{spectral}} framework can be {{used for}} value function approximation(VFA). Given the fixed policy, the edge weights are determined by corresponding states' transition probability. To get <b>smooth</b> <b>value</b> approximation, diffusion wavelets are used.|$|R
40|$|We {{propose a}} new method for {{smoothing}} 2 -D (or 3 -D) images which preserves edge elements. Pixel data contained within a moving square (or cube), centered at each point under consideration, is tested {{to determine if}} it is homogeneous, or if a region boundary is present. If the area is homogeneous, the mean value of the whole area gives the <b>smoothed</b> <b>value</b> for the central point. If a region boundary is detected, then the data are classified into two clusters, and the <b>smoothed</b> <b>value</b> of the central point results from the mean value of the cluster including this point, if a context-based consistency criterion is respected. Two cases are considered: Gaussian noise and noise whose distribution is unknown. Results are given on a synthetic image corrupted by additive Gaussian noise, illustrating the efficiency of our method in relation with the signal to noise ratio. Results on a 2 -D MR image are also shown...|$|E
30|$|The {{methods of}} “locally {{weighted}} scatterplot smooth,” known as “LOWESS” and “LOESS” employ locally weighted linear regressions to data smoothing. The {{difference between them}} is that LOWESS uses a first-degree polynomial to fit, whereas in the LOESS the second-degree polynomial is used. These methods, similar to the SMA and S–G filters, use neighboring data points to estimate <b>smoothed</b> <b>value,</b> and hence, they are local smoothing methods. In addition, a regression weight function is defined for each point inside the span, so the methods are weighted (Cleveland 1979, 1981; Cleveland and Devlin 1988; Cleveland and Loader 1996; Fan and Gijbels 1996; Hardle 1990).|$|E
40|$|Using the dynamo theory {{method to}} predict solar activity, a {{value for the}} {{smoothed}} sunspot number of 109 + or - 20 is obtained for solar cycle 22. The predicted cycle is expected to peak near December, 1990 + or - 1 year. Concommitantly, F(10. 7) radio flux {{is expected to reach}} a <b>smoothed</b> <b>value</b> of 158 + or - 18 flux units. Global mean exospheric temperature is expected to reach 1060 + or - 50 K and global total average total thermospheric density at 400 km is expected to reach 4. 3 x 10 to the - 15 th gm/cu cm + or - 25 percent...|$|E
40|$|Comput Methods Programs Biomed. Author manuscript; {{available}} in PMC 2012 Jun 7 GGOPT is a derivative-free non-linear optimizer for smooth functions with added noise. If the function values arise from observations or from extensive computations, these errors can be considerable. GGOPT uses an adjustable mesh together with linear least squares to find <b>smoothed</b> <b>values</b> of the function, gradient and Hessian {{at the center}} of the mesh. These values drive a descent method that estimates optimal parameters. The <b>smoothed</b> <b>values</b> usually result in increased accuracy. grant RR 01243 and EB 08407 from the National Institutes of Healthgrant RR 01243 and EB 08407 from the National Institutes of Healt...|$|R
5000|$|... #Caption: Animation showing {{smoothing}} being applied, {{passing through}} the data from left to right. The red line represents the local polynomial being used to fit a sub-set of the data. The <b>smoothed</b> <b>values</b> are shown as circles.|$|R
30|$|Due to {{that some}} medical images {{do not have}} {{cancerous}} tissues, {{there will be the}} phenomenon of empty map, and the <b>smoothing</b> <b>value</b> k is introduced to correct the function. In this paper, we use k[*]=[*] 5 e- 4.|$|R
40|$|The Stepwise Relaxed Value Picking (SRVP) regularization technique, {{proposed}} earlier for the iterative {{reconstruction of}} piecewise (quasi-) homogeneous objects, is a non-spatial technique, whereby the reconstruction unknowns are clustered around {{a limited number}} of-a-priori unknown-reference values. Artifacts have been observed in some 2 -D and 3 D complex permittivity reconstructions. This paper therefore combines the non-spatial SRVP technique with a spatial smoothing technique, whereby the reference values provided by the former-in each iteration-are employed by the latter to define separate smoothing regions. This way edges are preserved, since the spatial smoothing constraints in the cost function are active within but not across the region boundaries. This combined technique, denoted as Stepwise Relaxed Piecewise <b>Smoothed</b> <b>Value</b> Picking (SRPSVP) regularization, is formulated for the 2. 5 D microwave inverse scattering problem and is illustrated with reconstructions from the Institut Fresnel 2 -D scattering database...|$|E
40|$|The Research Group for Proton Magnetometer {{designed}} and constructed a nuclear precession magnetometer for station-use {{as has been}} already reported in this bulletin. Since April, 1959, the magnetometer has been installed at Oshima Geophysical Observatory {{in the hope of}} detecting changes in the earth's magnetic field related to volcanic activities. It is approximately proved that averages of the observations made there four times every 20 minutes may represent the hourly <b>smoothed</b> <b>value.</b> By comparing them with those at Kakioka Magnetic Observatory, influences of magnetic storms, daily variations, general secular variations and such like may be eliminated. The activity of the Research Group is still continuing. A sea-borne magnetometer that has been mainly used by the Japanese Antarctic Research Expedition, a proton magnetometer with a counter and a transistorized proton magnetometer were constructed by the Research Group and will be here briefly described...|$|E
40|$|The impulse {{response}} of the SAR system is not a delta function and the spectra represent {{the product of the}} underlying image spectrum with the transform of the {{impulse response}} which must be removed. A digitally computed spectrum of SEASAT imagery of the Atlantic Ocean east of Cape Hatteras was smoothed with a 5 x 5 convolution filter and the trend was sampled in a direction normal to the predominant wave direction. This yielded a transform of a noise-like process. The <b>smoothed</b> <b>value</b> of this trend is the transform of the impulse response. This trend is fit with either a second- or fourth-order polynomial which is then used to correct the entire spectrum. A 16 x 16 smoothing of the spectrum shows the presence of two distinct swells. Correction of the effects of speckle is effected by the subtraction of a bias from the spectrum...|$|E
50|$|In {{the case}} that the <b>smoothed</b> <b>values</b> can be written as a linear {{transformation}} of the observed <b>values,</b> the <b>smoothing</b> operation is known as a linear smoother; the matrix representing the transformation is known as a smoother matrix or hat matrix.|$|R
5000|$|Lattice sieving is a {{technique}} for finding <b>smooth</b> <b>values</b> of a bivariate polynomial [...] over a large region. It is almost exclusively {{used in conjunction with}} the number field sieve. The original idea of the lattice sieve came from John Pollard.|$|R
5000|$|Another {{proposed}} tracking {{signal was}} developed by Trigg (1964). In this model, et is the observed error in period t and |et| is the absolute value of the observed error. The <b>smoothed</b> <b>values</b> of the error and the absolute error are given by: ...|$|R
40|$|The {{introduction}} of a relative cane payment system initiated {{the need for a}} method of predicting the average season pol % cane for a mill. A system of forecasting was developed, consisting of the following 2 main steps: (a) Development of a pre-season estimate before commencement of crushing, based on the weighted sum of the double exponentially <b>smoothed</b> <b>value</b> of previous seasons ' averages. (b) Development of a within-season prediction once the current season is under way, based on the weighted sum of: The pre-season estimate determined in (a), the to-date average pol % cane value and the differelice between the latest month's pol % cane and the to-date average pol % cane values. The relative values of these weightings change from month to month {{during the course of the}} season. A computer was used both in the development of the method and for routine forecasting...|$|E
40|$|Using the 'dynamo theory' {{method to}} predict solar activity, an {{accurate}} prediction {{was made for}} solar cycle 21 by Schatten et al. (1978). Using the same dynamo technique for solar cycle 22, a value for the smoothed sunspot number of 170 + or - 25 is obtained. This large sunspot number is expected to peak in 1990 + or - 1 year. The F(10. 7) radio flux {{is expected to reach}} a <b>smoothed</b> <b>value</b> of 210 + or - 25 flux units. Since this value is larger than values obtained with prediction schemes based upon 'statistical' and 'periodicity' methods, it provides a useful test for the current methodology, based upon the strength of the sun's polar field near solar minimum. The predicted degree of solar activity is expected to enhance the density and temperature of the earth's thermosphere to values somewhat larger than those found in solar cycle 21. This {{will have an impact on}} the orbital lifetime of low altitude satellites...|$|E
40|$|Summary. Surface-ship and {{satellite}} derived data have been compiled in new free-air gravity anomaly, bathymetry and geoid anomaly {{maps of the}} Pacific Ocean basin and its margin. The maps are based on smoothed values of the gravity anomaly, bathymetry and geoid interpolated on to a 90 x 90 km grid. Each <b>smoothed</b> <b>value</b> was obtained by Gaussian filtering measurements along individual ship and subsatellite tracks. The resulting maps resolve features in the gravity, bathymetry and geoid with wavelengths that range from a few hundred to a few thousand kilometres. The smoothed values of bathymetry and geoid anomaly have been corrected for age. The resulting maps show the Pacific ocean basin {{is associated with a}} number of ENE-WSW-trending geoid anomaly highs with amplitudes of about k 5 m and wavelengths of about 3000 km. The most prominent of these highs correlate with the Magellan seamounts-Marshall Gilbert Islands-Magellan rise and the Hess rise-Hawaiian ridge regions. The correlation between geoid anomaly and bathymetry cannot be explained by models of static compensation, but is consistent with a model in which the geoid anomaly and bathymetry are supported by some form of dynamic compensation. We suggest that the dynamic compensation, which characterizes oceanic lithosphere older than 80 Myr, is the result of mantle convection on scales that are smaller than the lithospheric plates themselves...|$|E
3000|$|KL, and PC {{fixed in}} the course of coordinated events were {{smoothed}} {{with the use of the}} boxcar average of the 15 -min width, and then they were separated into different groups according to delay value ΔT. Afterwards, the time evolutions of the <b>smoothed</b> <b>values</b> V [...]...|$|R
40|$|This paper proposes an {{effective}} preprocessing procedure for current manifold learning algorithms, such as LLE and ISOMAP, {{in order to}} make the reconstruction more robust to noise and outliers. Given a set of noisy data sampled from an underlying manifold, we first detect outliers by histogram analysis of the neighborhood distances of data points. The linear error-in-variables (EIV) model is then applied in each region to compute the locally <b>smoothed</b> <b>values</b> of data. Finally a number of locally <b>smoothed</b> <b>values</b> of each sample are combined together to obtain the global estimate of its noise-free coordinates. The fusion process is weighted by the fitness of EIV model in each region to account for the variation of curvatures of the manifold. Experimental results demonstrate that our preprocessing procedure enables the current manifold learning algorithms to achieve more robust and accurate reconstruction of nonlinear manifolds. 1...|$|R
40|$|Although {{we expect}} to find many smooth numbers (i. e., numbers with no large prime factors) among the values taken by a {{polynomial}} with integer coefficients, {{it is unclear what}} the asymptotic number of such <b>smooth</b> <b>values</b> should be; this is in contrast to the related problem of counting the number of prime values of a polynomial, for which Bateman and Horn published a conjectured asymptotic formula that is widely believed to be true. We discuss how to employ the Bateman-Horn conjecture to derive an asymptotic formula for the number of <b>smooth</b> <b>values</b> of a polynomial, with the smoothness parameter in a non-trivial range. This conditional result provides a believable heuristic for the number of smooth integers among all values {F(n) }, and also among the values {F(p) } on prime arguments only. Comment: 57 pages. Revised version - an appendix has been added and some other material rewritten slightl...|$|R
40|$|Crop {{coefficients}} (K(c)) are {{a useful}} means of predicting {{how much water}} is needed for irrigating a crop. The crop water stress index (CWSI), on the other hand, {{is a means of}} knowing when to irrigate. Two field experiments were conducted during the summers of 1990 and 1991 at Maricopa Agricultural Center and Marana Agricultural Center, respectively, to evaluate water use (evapotranspiration, ET) of different cotton varieties, to develop crop coefficients for cotton grown in the state of Arizona, and to evaluate empirical and theoretical crop water stress indices under field conditions. For the 1990 experiment, ET from the cotton variety DPL 77 was obtained using soil water balance (SWB) and steady state heat balance (SSHB) techniques. For the 1991 experiment, ET from two cotton varieties (DPL 20 and Pima S- 6) was estimated using the Bowen ratio energy balance (BREB) method and the steady state heat balance method. Reference evapotranspiration (ETᵣ) was obtained from weather stations located close to the experimental plots. Average daily ET from the SSHB measurements ranged from 8. 24 to 15. 13 mm and from 10. 34 to 12. 12 mm for the 1990 and 1991 experiments, respectively. Total ET from the SWB was approximately 19 % less than the total ET estimated by the SSHB. Total ET from individual plants was well correlated with average stem area over the evaluation periods. Daily ET from the two cotton varieties (DPL 20 and Pima S- 6) was approximately similar when irrigation conditions were the same, but differed later by as much as 48. 4 % as irrigation continued for the variety Pima S- 6 only. Daily ET from the BREB measurements and ETᵣ were used to develop a crop coefficient curve for cotton grown at Marana, Arizona, which had a maximum <b>smoothed</b> <b>value</b> of 1. 21. A critical value of CWSI equal to 0. 3 was obtained by observing the pattern of the CWSI values over well-watered and drier conditions, and from previous research. Using the developed crop coefficient curve and the CWSI should provide a useful means of scheduling irrigation for cotton grown under climatic conditions similar to those at Marana, Arizona...|$|E
40|$|Fluid {{properties}} of refrigerant- 21 were investigated at temperatures from the freezing point to 423 Kelvin and at pressures to 1. 38 x 10 to the 8 th power N/sq m (20, 000 psia). The fluid properties included were: density, vapor pressure, viscosity, specific heat, thermal conductivity, thermal expansion coefficient, freezing point and bulk modulus. Tables of <b>smooth</b> <b>values</b> are reported...|$|R
40|$|Abstract [...] A smooth {{function}} is measured at equally spaced abscissae and the measurements contain random errors. We {{address the problem}} of making the least sum of squares change to the data by requiring nonnegative differences of order r for the <b>smoothed</b> <b>values.</b> The problem is a strictly convex quadratic programming calculation, where each of the constraint functions depends on r+ 1 adjacent components of the <b>smoothed</b> <b>values,</b> which are the binomial coefficients with alternating signs that arise in the expansion of (1 1). r We take account of this structure and describe a special active set method that is much faster than general quadratic programming algorithms. We present two examples that illustrate our approach and that although have a common development they follow different solution paths. The first of them starts from the point that satisfies all the constraints as equalities and the second starts from the unconstrained minimum of the problem. Index Terms [...] data smoothing, divided difference, least squares fitting, r-convexity, quadratic programmin...|$|R
3000|$|The main {{difference}} between the Wiener–Kolmogorov filtered values, [...] x_t|∞^K, and the Kalman filter <b>smoothed</b> <b>values,</b> x_t|T^K, results from the dependence of the former on a double infinite sequence of observations (but see Levinson 1947). As shown by Fiorentini (1995) and Gómez (1999), though, they can be made numerically identical by replacing both pre- and post-sample observations by their least squares projections onto the linear span of the sample observations.|$|R
40|$|The MSFC/J 70 Orbital Atmospheric Density Model, a {{modified}} version of the Smithsonian Astrophysical Observatory Jacchia 1970 model is explained. The algorithms describing the MSFC/J 70 model are included as well as listing of the computer program. The 13 -month <b>smoothed</b> <b>values</b> of solar flux (F sub 10. 7) and geomagnetic index (S sub p), which are required as inputs for the MSFC/J 70 model, are also included and discussed...|$|R
40|$|Surface-ship and {{satellite}} derived data have been compiled in new free-air gravity anomaly, bathymetry and geoid anomaly {{maps of the}} Pacific Ocean basin and its margin. The maps are based on <b>smoothed</b> <b>values</b> of the gravity anomaly, bathymetry and geoid interpolated on to a 90 X 90 km grid. The <b>smoothed</b> <b>values</b> of bathymetry and geoid anomaly have been corrected for age. The resulting maps show the Pacific ocean basin {{is associated with a}} number of ENE-WSW trending geoid anomaly highs with amplitudes of approx + or - 5 m and wavelengths of approx 3000 km. The most prominent of these highs correlate with the Magellan seamounts-Marshall Gilbert Islands-Magellan rise and the Hess rise-Hawaiian ridge regions. The correlation between geoid anomaly and bathymetry cannot be explained by models of static compensation, but is consistent with a model in which the geoid anomaly and bathymetry are supported by some form of dynamic compensation. Suggests that the dynamic compensation, which characterizes ocean lithosphere older than 80 Ma is the result of mantle convection on scales that are smaller than the lithospheric plates themselves. -from Author...|$|R
