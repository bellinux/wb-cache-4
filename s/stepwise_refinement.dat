382|62|Public
50|$|Data reification (<b>stepwise</b> <b>refinement)</b> {{involves}} {{finding a}} more concrete {{representation of the}} abstract data types used in a formal specification.|$|E
50|$|The {{language}} design focuses strongly on structured programming, and has {{a special}} construction for <b>stepwise</b> <b>refinement,</b> allowing students to focus on top-down design, and bottom-up coding.|$|E
5000|$|Another {{consequence}} of JSP's focus on data streams {{is that it}} creates program designs with a very different structure to the kind created by the <b>stepwise</b> <b>refinement</b> methods of Wirth and Dijkstra. One typical feature {{of the structure of}} JSP programs is that they have several input operations distributed throughout the code in contrast to programs designed using <b>stepwise</b> <b>refinement,</b> which tend to have only one input operation. Jackson illustrates this difference in Chapter 3 of Principles of Program Design. [...] He presents two versions of a program, one designed using JSP, the other using [...] "traditional" [...] methods.|$|E
40|$|Abstract. This paper {{presents}} some {{techniques that}} reduce {{the complexity of}} the verification of Prolog implementations. Two key techniques are <b>stepwise</b> <b>refinements</b> and bisimulation. The method of <b>stepwise</b> <b>refinements</b> captures various optimization techniques employed in the implementation process. The correctness of each refinement is justified using a notion called bisimulation. Our definition of bisimulation makes use of the notion of essential states in a machine. The notion of essential states further reduces {{the complexity of the}} proof. We illustrate our method by presenting two equivalent machines: a nondeterministic interpreter for Prolog and its refinement that executes compiled codes...|$|R
40|$|AbstractWe {{develop an}} {{algebraic}} theory for the top-down design of communicating systems in which levels of abstraction {{are represented by}} algebras, and their <b>stepwise</b> <b>refinements</b> are represented by homomorphisms. Particular {{attention is paid to}} the equational specification of these levels of abstraction. A number of examples are included for illustration, most notably a top-down design for a communication protocol...|$|R
40|$|AbstractIf {{the firing}} of a {{transition}} in a Petri net is considered non instantaneous, it becomes possible to replace a transition in a net P by another net P′ This allows to proceed the description and the analysis of a control structure by <b>stepwise</b> <b>refinements.</b> The necessary and sufficient conditions on P and P′ for the resulting net to be bounded and live, are given...|$|R
5000|$|In formal methods, program {{refinement}} is the verifiable {{transformation of}} an abstract (high-level) formal specification into a concrete (low-level) executable program. <b>Stepwise</b> <b>refinement</b> allows this process {{to be done}} in stages. Logically, refinement normally involves implication, but there can be additional complications.|$|E
5000|$|The {{refinement}} calculus is a formalized {{approach to}} <b>stepwise</b> <b>refinement</b> for program construction. The required behaviour {{of the final}} executable program is specified as an abstract and perhaps non-executable [...] "program", which is then refined {{by a series of}} correctness-preserving transformations into an efficiently executable program.|$|E
50|$|Back {{originated}} the refinement calculus, {{an important}} {{approach to the}} formal development of programs using <b>stepwise</b> <b>refinement,</b> in his 1978 PhD thesis at the University of Helsinki, On the Correctness of Refinement Steps in Program Development. He has undertaken much subsequent research in this area. He has held positions at CWI Amsterdam, the Academy of Finland and the University of Tampere.|$|E
40|$|The Gurevich's Abstract State Machine {{formalism}} is used {{to specify}} the well known Kerberos Authentication System based on the Needham-Schroeder authentication protocol. A complete model {{of the system is}} reached through <b>stepwise</b> <b>refinements</b> of ASMs, and is used as a basis both to discover the minimum assumptions to guarantee the correctness of the system and to analyse its security weaknesses. Each refined model comes together with a correctness refinement theorem...|$|R
40|$|Component based {{design and}} {{development}} of software {{is one of the}} most challenging issues in software engineering. In this paper, we adopt a somewhat simplified view of software components and discuss how they can be conveniently modeled in a framework that provides a modular approach to formal software development by means of <b>stepwise</b> <b>refinements.</b> In particular we take into account an observational interpretation of requirements specifications and study its impact on the definition of the semantics of specifications of (parametrized) components. Our study is carried out in the context of Casl architectural specifications...|$|R
40|$|This paper {{describes}} a design flow {{used in an}} on-going implementation of a fixed-point DSP processor with variable word length. Based on the top-down design approach described in [1], we discuss {{the approach based on}} a sequence of models and <b>stepwise</b> <b>refinements.</b> The pros and cons of selecting different models are investigated as they are compared with each other. For the purpose of validation, a flexible framework has been developed. It allows regression testing and dynamic changing of the test data set and model to be tested. Using the tools, the output is then automatically compared with the expected result as provided by the golden model. ...|$|R
5000|$|Data reification (<b>stepwise</b> <b>refinement)</b> {{involves}} {{finding a}} more concrete {{representation of the}} abstract data types used in a specification. There may be several steps before an implementation is reached. Each reification step for an abstract data representation [...] involves proposing a new representation [...] In order {{to show that the}} new representation is accurate, a retrieve function is defined that relates [...] to , i.e[...] The correctness of a data reification depends on proving adequacy, i.e.|$|E
50|$|His article Program Development by <b>Stepwise</b> <b>Refinement,</b> {{about the}} {{teaching}} of programming, {{is considered to be}} a classic text in software engineering.In 1975 he wrote the book Algorithms + Data Structures = Programs, which gained wide recognition. Major revisions of this book with the new title Algorithms + Data Structures were published in 1985 and 2004. The examples in the first edition were written in Pascal. These were replaced in the later editions with examples written in Modula-2 and Oberon respectively.|$|E
50|$|Anderson {{designed}} a symbol and submitted three variations {{of it to}} the competition. The alternatives actually represented Anderson’s <b>stepwise</b> <b>refinement</b> of a basic idea involving three arrows - from a more elaborate design utilizing different tones and the word “recycle”, to a simple black and white line drawing with no wording. The arrows were planar, suggesting strips of paper, but they curved and bent back upon themselves as though captured {{in the midst of an}} industrial manufacturing process, and the three arrows taken together as a continuous band formed the topological figure known as a Möbius strip.|$|E
40|$|The paper {{discusses}} the tracking control of uncertain nonlinear systems. The specific system studied is an XY-table. The control system {{consists of an}} 2 ̆ 2 exact 2 ̆ 2 linearizing state feedback and a linear robustifying controller in an outer loop. For the outer loop a controller is synthesized using the mu -methodology. Iterative improvements in the robustness of the controlled system are obtained by <b>stepwise</b> <b>refinements</b> {{in the structure of}} the mu -synthesis problem and in the choice of performance weights and uncertainty weights, at the cost of decreased performance. This uncovers the need for preciseness in the choice of design weight...|$|R
40|$|AbstractThis paper {{deals with}} control system design and {{implementation}} problems encountered in multiple robot systems. A systematic method of constructing hierarchical net models is described for their direct implementation such that the net is translated into the detailed net by <b>stepwise</b> <b>refinements</b> from the highest task specification level to the lowest machine control level. Basic Petri nets are extended as a prototyping tool for expressing real-time control of robotic systems based on command response concept. The hierarchical and distributed coordinators are introduced to perform the synchronization and coordination of the robots and other machines in the system. The proposed method allows a direct coding of the inter-task cooperation from the net specification and can be implemented using off–the-shelf real-time executives...|$|R
40|$|This paper {{illustrates}} {{a method for}} the design of parallel programs that is based on the separation of computation and coordination. We use Gamma programs to specify computations and show how to prove their corr ectness using a UNITYlike programming logic. A separate coordinationlanguage is used to determine behavioural aspects of the program. We illustr ate how behaviour can be structured through a process of suc cessive <b>stepwise</b> <b>refinements.</b> R easoning ab out refinement is supported formally by two types of simulation. As an illustration of our method we study the problem of solving triangular systems of linear equations. F or this problem we derive a number of known algorithms and show how they are relate d by our refinement ordering...|$|R
50|$|Extended ML is a wide-spectrum {{language}} covering both specification {{and implementation}} {{and based on}} the ML programming language. It extends the syntax of ML to include axioms, which need not be executable but can rigorously specify the behavior of the program. With this addition the language can be used for <b>stepwise</b> <b>refinement,</b> proceeding gradually from an initial formal specification to eventually yield an executable Standard ML program. Correctness of the final executable SML program with respect to the original specification can then be established by proving the correctness of each of the refinement steps. Extended ML is used for research into and teaching of formal program development and specification, and research into automatic program verification.|$|E
50|$|The main {{article on}} compilers honours PL/0 for {{introducing}} several influential concepts (<b>stepwise</b> <b>refinement,</b> recursive descent parsing, EBNF, P-code, T-diagrams) {{to the field}} by educating students to use these concepts. Over the last 3 decades, most university courses on compiler construction that used PL/0 have followed Wirth strictly in employing these techniques (see references below). Some years ago university courses dared to deviate from the course set by Wirth with {{the replacement of the}} classical recursive descent parsing technique by a (nonetheless classical) Unix-like approach of employing lex and yacc. Only recently an implementation (PL/0 Language Tools) along this way has also combined modern concepts like object-orientation and design patterns with a modern scripting language (Python), allowing students to consume the source text of the implementation in a contemporary programming style.|$|E
50|$|The first S-algol {{implementation}} {{was on a}} PDP-11/40 computer {{running the}} Unix operating system. Due to the small 64 kilobyte address space available on the PDP-11, an interpreted bytecode implementation was chosen. A single-pass, recursive descent compiler written in S-algol translated S-algol source into S-code, a bytecode for a stack-based abstract machine tailored for S-algol. The S-code was then executed by an interpreter. The S-algol implementation had many similarities with work on earlier Pascal compilers. The technique of using a recursive descent compiler to produce code for an abstract machine was well known, with the Pascal P compiler being a famous example from the early 1970s. The S-algol compiler was written using the <b>stepwise</b> <b>refinement</b> process described by Urs Amman {{for the development of}} a Pascal compiler and championed by the inventor of Pascal, Niklaus Wirth.|$|E
40|$|This paper {{advocates a}} novel {{approach}} {{to the construction of}} secure software: controlling information flow and maintaining integrity via monadic encapsulation of effects. This approach is constructive, relying on properties of monads and monad transformers to build, verify, and extend secure software systems. We illustrate this approach by construction of abstract operating systems called separation kernels. Starting from a mathematical model of shared-state concurrency based on monads of resumptions and state, we outline the development by <b>stepwise</b> <b>refinements</b> of separation kernels supporting Unix-like system calls, interdomain communication, and a formally verified security policy (domain separation). Because monads may be easily and safely represented within any pure, higher-order, typed functional language, the resulting system models may be directly realized within a language such as Haskell...|$|R
40|$|Methods used {{to specify}} {{real-time}} control software should enable {{the expression of}} functional, control and real-time requirements. They should enable multi-disciplinary system development and promote reuse of specifications. This paper describes a specification of a real-time control software developed using the DisCo method. DisCo is an object-oriented action-based method with precise semantics in logic. The specification is layered and partly reusable. It consists of functional, control and real-time parts. The real-time part includes layers which specify generic periodic and aperiodic events. The control part specifies the control algorithms, and the functional part {{the rest of the}} system. The three parts are specified using <b>stepwise</b> <b>refinements</b> and combined in a simple way. Although the specification presented is quite small, the techniques used are applicable when specifying larger systems with complex real-time behavior...|$|R
40|$|This paper {{describes}} {{a series of}} <b>stepwise</b> <b>refinements</b> of a biological model resulting in a high-performance simulation system for individual-based models of the coevolutionary dynamics associated with spatially explicit epidemic processes. Our model includes two competing host species, a macroparasite capable of serving as a vector, and the vector-borne microparasite. Genetic algorithms are used to simulate genetic change; we are {{particularly interested in the}} evolution of pathogen virulence. The simulation system employs cellular automata to track individual organisms distributed over a two-dimensional lattice. Our models are able to iden- tiy each individual's parentage, and to account for both biotic and abiotic spatial heterogeneity. Using the developed system we conducted a series of experiments to demonstrate how individual-based modeling and explicit representation of space, although computationally expensive, can produce qualitatively new biological re- sults...|$|R
5000|$|Reification is {{the process}} by which an {{abstract}} idea about a computer program is turned into an explicit data model or other object created in a programming language. A computable/addressable object — a resource — is created in a system as a proxy for a non computable/addressable object. By means of reification, something that was previously implicit, unexpressed, and possibly inexpressible is explicitly formulated and made available to conceptual (logical or computational) manipulation. Informally, reification {{is often referred to as}} [...] "making something a first-class citizen" [...] within the scope of a particular system. Some aspect of a system can be reified at language design time, which is related to reflection in programming languages. It can be applied as a <b>stepwise</b> <b>refinement</b> at system design time. Reification is one of the most frequently used techniques of conceptual analysis and knowledge representation.|$|E
50|$|Top-down {{design was}} {{promoted}} in the 1970s by IBM researchers Harlan Mills and Niklaus Wirth. Mills developed structured programming concepts for practical use and tested {{them in a}} 1969 project to automate the New York Times morgue index. The engineering and management success of this project led {{to the spread of}} the top-down approach through IBM {{and the rest of the}} computer industry. Among other achievements, Niklaus Wirth, the developer of Pascal programming language, wrote the influential paper Program Development by <b>Stepwise</b> <b>Refinement.</b> Since Niklaus Wirth went on to develop languages such as Modula and Oberon (where one could define a module before knowing about the entire program specification), one can infer that top-down programming was not strictly what he promoted. Top-down methods were favored in software engineering until the late 1980s, and object-oriented programming assisted in demonstrating the idea that both aspects of top-down and bottom-up programming could be utilized.|$|E
40|$|Formal {{development}} by <b>stepwise</b> <b>refinement</b> {{is one of}} the most prominent approaches in formal program development. <b>Stepwise</b> <b>refinement</b> comes with a bunch of methodological claims and pragmatical guidelines. The purpose of this paper is twofold: first, we present a transformational program development for a mediumsize example, a (core of a) lexical scanner generator. The concepts of the algorithmic solution are developed by deriving them "on the fly", not via "inventing and post-verifying" them. Second, we use this case study to review some arguments in the methodological and pragmatical debate...|$|E
40|$|Many {{interface}} designs {{have been}} developed for the exploration of multi-dimensional data sets which are based on finding subsets by filtering attribute values. Systems such as dynamic queries use a collection of independent filters to interactively query by restricting attribute values. However, for large data sets {{there is a need for}} an alternative style of filtering that better supports <b>stepwise</b> query <b>refinement.</b> This article introduces a new filter coordination which supports both <b>stepwise</b> query <b>refinement</b> and independent filters. Our filter visualization also supports the visualization of attribute value hierarchies enabling multi-level data distribution overviews to be given. Our coordination design is implemented in our SGViewer query tool which we demonstrate with a multi-dimensional web log data set. An evaluation of SGViewer showed that after a short learning period users were able to use it to read trends and proportions and make drill-down queries...|$|R
40|$|International audienceSelf-Organizing Multi-Agent Systems (SO-MAS) {{are defined}} {{as a set of}} {{autonomous}} entities called agents interacting together in order to achieve a given task. Generally, the development process of these systems is based on the bottom-up approach which focuses on the design of the entities individual behavior. The main question arising when developing SO-MAS is how to insure that the designed entities, when interacting together, will give rise to the desired behavior? Our proposition to deal with this question is to use formal methods. We propose a correct by construction method for systematic design of SO-MAS based on the use of design patterns and formal <b>stepwise</b> <b>refinements.</b> Our work gives guidelines to assist the designer when developing the individual behavior of the entities and prove its correctness at {{the early stages of the}} design process. The method is illustrated with the foraging ants’ case study...|$|R
40|$|Abstract. In the CAD/CAS field, the {{increasing}} domination of splinebased graphic objects has driven a great attention to methods {{focusing on the}} management of free-form curves. These methods range from basic manipulation of control points to more sophisticated direct manipulation techniques by using tangency or curvature constraints. However they are not suitable for the earlier conceptual stage where quick brainstorming illustrations and <b>stepwise</b> <b>refinements</b> are required. In this paper we present a method, which automatically reconstructs the designer’s free-form 3 D curve through recognizing his design “intention”. This algorithm automatically extracts the relevant control points through dynamic sampling mechanism. Furthermore, we apply these approximated curves to create freeform surfaces. We also introduce a touch-enabled global and local modification method which is capable of effectively obtain quality and accuracy deformation results. The method has been tested with various types of sketches which are rendered in 3 D scene environment. ...|$|R
40|$|In this paper, {{rigorous}} {{application of}} <b>stepwise</b> <b>refinement</b> is explored. The steps of definition, decomposition, and completion are described, where completion is a newly introduced step. This combination of steps extends {{the use of}} <b>stepwise</b> <b>refinement</b> to larger systems. The notions of range, active objects, and backlog interface are intro-duced. Verification of incomplete programs via interactive testing is described. The paradigm is demonstrated in an example. The relationship between the paradigm and the current programming languages is considered. It is argued that the WHILE-DO loop is a harmful construct {{from this point of}} view. 1...|$|E
40|$|In {{this paper}} we discuss how mutual {{influences}} (e. g. conflicts) of different stakeholder concerns can be detected and reasoned about through composition and <b>stepwise</b> <b>refinement.</b> Some concepts from the Aspect-Oriented Software Development paradigm are used to support the composition-centric approach. 1...|$|E
40|$|AbstractA simple {{methodology}} {{for the design}} and the verification of finite-state concurrent programs is proposed and illustrated by a short example. In most cases, this methodology {{is likely to be}} more systematic than the technique of <b>stepwise</b> <b>refinement,</b> and more efficient than the fixpoint-based method...|$|E
40|$|Graph {{transformation}} {{systems are}} formal models of computational systems, specified by rules that describe the atomic {{steps of the}} system. A refinement of a graph transformation system is given by associating with each of its rules a composition of rules of a refining system, that has the same visible effect as the original rule. The basic composition operations on graph transformation rules are sequential and parallel composition, corresponding to temporal and spatial refinements respectively. Syntactically refinements are represented by rule expressions that describe how the refining rules shall be composed. 1 Introduction Refinements are the basic steps {{in the development of}} system specifications. Starting from an abstract description of the system's behaviour <b>stepwise</b> <b>refinements</b> yield more and more concrete specifications, that should finally be directly implementable on a machine. In this paper we consider refinements of graph transformation systems, that are formal models of sys [...] ...|$|R
40|$|The {{necessity}} of using formal methods in software engineering is now widely recognized [6, 2], {{in particular in}} the framework of safety-critical systems. There are mainly two ways of exploiting formal methods: the verification a posteriori of the correctness of already implemented programs, or the development of "correct by construction" software, starting from a very abstract specification and applying correctness preserving transformations until the final implementation is obtained. Our work is related to the latter approach, more precisely {{in the framework of}} the B method [1], [4]. This method covers the entire cycle of software development, from the early customer requirements to the nal code generation. Starting from an initial abstract machine that gives a very high-level expression of the user requirements (state variables, invariants, high-level specification of the operations), <b>stepwise</b> <b>refinements</b> can be built to get eventually a concrete machine, called the i...|$|R
40|$|The {{connection}} between some modularity properties and interpolation is revisited and restated {{in a general}} "logicindependent " framework. The presence of uniform interpolants is shown to assist in certain proof obligations, which suffice to establish the composition of refinements. The absence of the desirable interpolation properties from many logics {{that have been used}} in refinement, motivates a thorough investigation of methods to expand a specification formalism orthogonally, so that the critical uniform interpolants become available. A potential breakthrough is outlined in this paper. 1. A refinement paradigm Let us consider program development by means of <b>stepwise</b> <b>refinements.</b> One postulates some abstract data typelike specification 1 (ADT), suitable for the problem at hand, which has to be implemented on the available system. The end product consists of (the text of) an abstract program manipulating the postulated ADT, together with a suite of (texts of) modules implementin [...] ...|$|R
