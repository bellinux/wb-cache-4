2450|2669|Public
25|$|Both {{iterative}} {{and direct}} methods exist for <b>sparse</b> <b>matrix</b> solving.|$|E
25|$|This is the {{traditional}} format for specifying a <b>sparse</b> <b>matrix</b> in MATLAB (via the sparse function).|$|E
25|$|Eigen3 is a C++ library that {{contains}} several <b>sparse</b> <b>matrix</b> solvers. However, {{none of them}} are parallelized.|$|E
40|$|SparseM {{provides}} some basic R functionality for linear algebra with <b>sparse</b> <b>matrices.</b> Use {{of the package}} is illustrated by a family of linear model fitting functions that implement least squares methods for problems with <b>sparse</b> design <b>matrices.</b> Significant performance improvements in memory utilization and computational speed are possible for applications involving large <b>sparse</b> <b>matrices.</b> ...|$|R
50|$|When storing and {{manipulating}} <b>sparse</b> <b>matrices</b> on a computer, it {{is beneficial}} and often necessary to use specialized algorithms and data structures that {{take advantage of}} the sparse structure of the matrix. Operations using standard dense-matrix structures and algorithms are slow and inefficient when applied to large <b>sparse</b> <b>matrices</b> as processing and memory are wasted on the zeroes. Sparse data is by nature more easily compressed and thus require significantly less storage. Some very large <b>sparse</b> <b>matrices</b> are infeasible to manipulate using standard dense-matrix algorithms.|$|R
5000|$|Datastructures for {{unstructured}} <b>sparse</b> <b>matrices</b> {{in these}} formats: ...|$|R
25|$|The term <b>sparse</b> <b>matrix</b> was {{possibly}} coined by Harry Markowitz who triggered some pioneering work but {{then left the}} field.|$|E
25|$|A {{symmetric}} <b>sparse</b> <b>matrix</b> arises as the {{adjacency matrix}} of an undirected graph; {{it can be}} stored efficiently as an adjacency list.|$|E
25|$|In large linear-programming {{problems}} A {{is typically}} a <b>sparse</b> <b>matrix</b> and, when the resulting sparsity of B is exploited when maintaining its invertible representation, the revised simplex algorithm {{is much more}} efficient than the standard simplex method. Commercial simplex solvers {{are based on the}} revised simplex algorithm.|$|E
40|$|LINSOL is an {{iterative}} linear solver {{package for}} <b>sparse</b> <b>matrices</b> that contains presently eight different iterative methods and two polyalgorithms of generalized Conjugate Gradient (CG) type. The data {{structures of the}} <b>sparse</b> <b>matrices</b> that support pipelining, cache reuse and parallelization are discussed. Measurements on different supercomputers are presented...|$|R
2500|$|When storing and {{manipulating}} <b>sparse</b> <b>matrices</b> on a computer, it {{is beneficial}} and often necessary to use specialized algorithms and data structures that {{take advantage of}} the sparse structure of the matrix. [...] Operations using standard dense-matrix structures and algorithms are slow and inefficient when applied to large <b>sparse</b> <b>matrices</b> as processing and memory are wasted on the zeroes. [...] Sparse data is by nature more easily compressed and thus require significantly less storage. [...] Some very large <b>sparse</b> <b>matrices</b> are infeasible to manipulate using standard dense-matrix algorithms.|$|R
40|$|Abstract: – Many of the {{elements}} of <b>sparse</b> <b>matrices</b> have zero value. Known examples of these types of matrices are lower and upper triangular, which arise frequently in the solution of linear equation systems. Because many of {{the elements}} of these matrices are zero, it is advisable to store them – whether on disk or in memory – in a way that saves space. In this paper <b>sparse</b> <b>matrices</b> called central triangular are analyzed and mechanisms for their efficient storage by means of 1 D arrays are proposed. Key-Words: <b>sparse</b> <b>matrices,</b> addressing formulas, efficient storage...|$|R
2500|$|... sparsersb - Interface to the librsb package {{implementing}} the RSB <b>sparse</b> <b>matrix</b> format for fast shared-memory <b>sparse</b> <b>matrix</b> computations ...|$|E
2500|$|... at the University of Florida, {{containing}} the UF <b>sparse</b> <b>matrix</b> collection.|$|E
2500|$|Several {{software}} libraries support sparse matrices, {{and provide}} solvers for <b>sparse</b> <b>matrix</b> equations. The following are open-source: ...|$|E
5000|$|Basic {{linear algebra}} {{operations}} for dense and <b>sparse</b> <b>matrices</b> ...|$|R
5|$|In many {{practical}} situations {{additional information}} about the matrices involved is known. An important case are <b>sparse</b> <b>matrices,</b> that is, matrices most of whose entries are zero. There are specifically adapted algorithms for, say, solving linear systems Ax = b for <b>sparse</b> <b>matrices</b> A, such as the conjugate gradient method.|$|R
5000|$|CSR format, a {{technique}} for storing <b>sparse</b> <b>matrices</b> in memory ...|$|R
2500|$|Calculating degree {{centrality}} {{for all the}} nodes in a graph takes [...] in a dense {{adjacency matrix}} representation of the graph, and for edges takes [...] in a <b>sparse</b> <b>matrix</b> representation.|$|E
2500|$|T. Bouwmans, N. Aybat, and E. Zahzah. Handbook on Robust Low-Rank and <b>Sparse</b> <b>Matrix</b> Decomposition: Applications in Image and Video Processing, CRC Press, Taylor and Francis Group, May 2016. (more information: http://www.crcpress.com/product/isbn/9781498724623) ...|$|E
2500|$|The (old and new) Yale <b>sparse</b> <b>matrix</b> formats are {{instances}} of the CSR scheme. The old Yale format works exactly as described above, with three arrays; the new format achieves a further compression by combining [...] and [...] {{into a single}} array.|$|E
5000|$|Iterative solvers for {{unstructured}} <b>sparse</b> <b>matrices</b> {{from the}} Templates project: ...|$|R
40|$|The aim of {{this paper}} is to prove the achievability of several coding {{problems}} by using <b>sparse</b> <b>matrices</b> and maximal-likelihood (ML) coding. These problems are the Slepian-Wolf problem, the Gel’fand-Pinsker problem, the Wyner-Ziv problem, and the problem of source coding with partial side information at the decoder. To this end, the notion of a hash property for an ensemble of functions is introduced and it is proved that an ensemble of q-ary <b>sparse</b> <b>matrices</b> satisfies the hash property. Based on this property, it is proved that the rate of codes using <b>sparse</b> <b>matrices</b> and maximal-likelihood (ML) coding can achieve the optimal rate...|$|R
5000|$|Datastructures for {{dense and}} {{structured}} <b>sparse</b> <b>matrices</b> {{in the following}} formats: ...|$|R
2500|$|Sparse dictionaries. This method {{focuses on}} not only {{providing}} a sparse representation but also constructing a sparse dictionary which is {{enforced by the}} expression [...] where [...] is some pre-defined analytical dictionary with desirable properties such as fast computation and [...] is a <b>sparse</b> <b>matrix.</b> Such formulation allows to directly combine the fast implementation of analytical dictionaries with the flexibility of sparse approaches.|$|E
2500|$|DOK {{consists}} of a dictionary that maps -pairs {{to the value of}} the elements. [...] Elements that are missing from the dictionary are taken to be zero. [...] The format is good for incrementally constructing a <b>sparse</b> <b>matrix</b> in random order, but poor for iterating over non-zero values in lexicographical order. [...] One typically constructs a matrix in this format and then converts to another more efficient format for processing.] ...|$|E
2500|$|In {{the case}} of a <b>sparse</b> <b>matrix,</b> {{substantial}} memory requirement reductions can be realized by storing only the non-zero entries. Depending on the number and distribution of the non-zero entries, different data structures can be used and yield huge savings in memory when compared to the basic approach. [...] The trade-off is that accessing the individual elements becomes more complex and additional structures are needed to be able to recover the original matrix unambiguously.|$|E
5000|$|... #Subtitle level 3: Robust PCA via {{decomposition}} in low-rank and <b>sparse</b> <b>matrices</b> ...|$|R
25|$|Large <b>sparse</b> <b>matrices</b> {{often appear}} in {{scientific}} or engineering applications when solving partial differential equations.|$|R
5000|$|Linear algebra {{classes with}} support for <b>sparse</b> <b>matrices</b> and vectors (with a F# {{friendly}} interface).|$|R
2500|$|In {{numerical}} analysis and computer science, a <b>sparse</b> <b>matrix</b> or sparse array is a matrix {{in which most}} of the elements are zero. [...] By contrast, if most of the elements are nonzero, then the matrix is considered dense. [...] The number of zero-valued elements divided {{by the total number of}} elements (e.g., m × n for an m × n matrix) is called the sparsity of the matrix (which is equal to 1 minus the density of the matrix).|$|E
2500|$|An {{important}} special type of sparse matrices is band matrix, {{defined as}} follows. The lower bandwidth of a matrix [...] {{is the smallest}} number [...] such that the entry [...] vanishes whenever [...] Similarly, the upper bandwidth is the smallest number [...] such that [...] whenever [...] For example, a tridiagonal matrix has lower bandwidth [...] and upper bandwidth [...] As another example, the following <b>sparse</b> <b>matrix</b> has lower and upper bandwidth both equal to 3. Notice that zeros are represented with dots for clarity.|$|E
5000|$|... sparsersb - Interface to the librsb package {{implementing}} the RSB <b>sparse</b> <b>matrix</b> format for fast shared-memory <b>sparse</b> <b>matrix</b> computations ...|$|E
5000|$|Real {{and complex}} linear algebra types and solvers with support for <b>sparse</b> <b>matrices</b> and vectors.|$|R
40|$|Standard preconditioners, like {{incomplete}} factorizations, {{perform well}} when the coefficient matrix is diagonally dominant, but often fail on general <b>sparse</b> <b>matrices.</b> We experiment with nonsymmetric permutationsand scalingsaimed at placing large entrieson the diagonal {{in the context}} of preconditioning for general <b>sparse</b> <b>matrices.</b> The permutations and scalings are those developed by Olschowka and Neumaier [Linear Algebra Appl., 240 (1996), pp. 131 – 151] and by Duff an...|$|R
40|$|The {{efficient}} {{solution of}} large sparse systems of linear equations {{is one of}} the key tasks in many computationally intensive scientific applications. The parallelization and efficiency of codes for solving these systems heavily depends on the sparsity structure of the matrices. As the variety of different structured <b>sparse</b> <b>matrices</b> is large, there is no uniform method which is well suited for the whole range of <b>sparse</b> <b>matrices...</b>|$|R
