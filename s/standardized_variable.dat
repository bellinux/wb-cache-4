21|216|Public
5000|$|Using the <b>standardized</b> <b>variable</b> , where [...] is the {{location}} parameter and [...] is the scale parameter, the {{cumulative distribution function}} of the GEV distribution is ...|$|E
50|$|A very {{interesting}} device available in GeoDa to explore global patterns of autocorrelation in space is Anselin's Moran scatterplot. This graph depicts a <b>standardized</b> <b>variable</b> in the x-axis versus the spatial lag of that <b>standardized</b> <b>variable.</b> The spatial lag {{is nothing but}} a summary {{of the effects of the}} neighboring spatial units. That summary is obtained by means of a spatial weights matrix, which can take various forms, but a very commonly used is the contiguity matrix. The contiguity matrix is an array that has a value of one in the position (i, j) whenever the spatial unit j is contiguous to the unit i. For convenience that matrix is standardized in such a way that the rows sum to one by dividing each value by the row sum of the original matrix.|$|E
50|$|Factor loadings: Commonality is {{the square}} of {{standardized}} outer loading of an item. Analogous to Pearson's r, the squared factor loading is the percent of variance in that indicator variable explained by the factor. To get the percent of variance in all the variables accounted for by each factor, add {{the sum of the}} squared factor loadings for that factor (column) and divide by the number of variables. (Note the number of variables equals the sum of their variances as the variance of a <b>standardized</b> <b>variable</b> is 1.) This is the same as dividing the factor's eigenvalue by the number of variables.|$|E
40|$|This paper {{studies the}} optimal risky {{investment}} problem with fewer restrictions on utilities, and more structure on risks, {{than does the}} current literature. It uses discrete random variables defined on a common domain, hereafter called <b>standardized</b> <b>variables,</b> to obtain new results without important loss of generality. The optimal amount of investment in a single risky asset does not always decrease as risk increases in the Rothschild-Stiglitz ([1970, 1971]; hereafter RS) sense. However, by using <b>standardized</b> <b>variables</b> to define wealth dependent measures of risk and return, the paper finds necessary and sufficient conditions on risks such that an increase in risk does cause decreasing optimal risky investment. The paper thus complements the RS results. For investment in two risky assets, the paper uses <b>standardized</b> <b>variables</b> to find conditions on risks such that the riskier asset's demand to decrease (increase) as the Arrow-Pratt absolute risk aversion index increases (decreases), and thereby complements Ross' [1981] results. ...|$|R
40|$|The {{stability}} of the correlation matrices is noteworthy. Usually to testing {{stability of}} coorelation matrices used to statistics M Box, Jennrich and G. Its statistics However, M Box and G statistics as computation of matrix determinant and J statistic involves matrix inversion. The former needs the condition that all sample correlation matrices are positive definite which is not always satisfied in practice. This condition is not apt for high dimension data sets because its computational efficiency becomes low. To handle this obstacles, we proposed a new statistical test {{based on what we}} call vector variance of <b>standardized</b> <b>variables</b> (VVSV). The proposed test is constucted based on vector variance (VV). This is evidenced by several papers describing the correlation matrix, Vector variance of <b>standardized</b> <b>variables</b> sample used a statistical formula variance vector. In practice there are difficulties in the calculation to determine variance of Vector Variance of Sample Variance of <b>Standardized</b> <b>Variables.</b> In this paper, by utilizing the vec operator and properties of the matrix will be investigated alternative formulation of asymptotic variance of Vector Variance of Sample Variance o...|$|R
50|$|Standard {{scores are}} also called z-values, z-scores, normal scores, and <b>standardized</b> <b>variables.</b> They are most {{frequently}} used to compare an observation to a standard normal deviate, though they can be defined without assumptions of normality.|$|R
30|$|To {{define a}} precise meaning for {{input and output}} datapoints far beyond <b>standardized</b> <b>variable</b> types, {{semantic}} types are introduced. They are assigned to datapoints and used to create semantically correct connections between datapoints and to analyse the interoperability between profiles. Semantic types are predefined in the semantics ontology of Layer 2 of the ontology layer architecture (cf. Figure 1) and used via object referencing in the ODDs.|$|E
30|$|Beside a {{comprehensive}} specification of the hardware of building automation devices, especially specific semantic knowledge about their profiles {{is required for}} an automated design. This includes knowledge about the specific functions implemented by each profile (e.g., constant light control, automatic light control, and occupancy control), how profiles should be parameterized, what purpose their input and output datapoints have (more precisely than <b>standardized</b> <b>variable</b> types allow for), {{and how they can}} be connected appropriately.|$|E
40|$|For the Lego {{discrepancy}} with M bins, {{which is}} equivalent with a chi^ 2 -statistic with M bins, {{we present a}} procedure to calculate the moment generating function of the probability distribution perturbatively if M and N, the number of uniformly and randomly distributed data points, become large. Furthermore, we present a phase diagram for various limits of the probability distribution {{in terms of the}} <b>standardized</b> <b>variable</b> if M and N become infinite. Comment: 16 page...|$|E
50|$|A {{regression}} {{carried out}} on original (unstandardized) variables produces unstandardized coefficients. A regression {{carried out on}} <b>standardized</b> <b>variables</b> produces <b>standardized</b> coefficients. Values for standardized and unstandardized coefficients can also be derived subsequent to either type of analysis.|$|R
50|$|Before {{solving a}} {{multiple}} regression problem, all variables (independent and dependent) can be <b>standardized.</b> Each <b>variable</b> can be <b>standardized</b> by subtracting its mean {{from each of}} its values and then dividing these new values by the standard deviation of the <b>variable.</b> <b>Standardizing</b> all <b>variables</b> in a multiple regression yields standardized regression coefficients that show {{the change in the}} dependent variable measured in standard deviations.|$|R
2500|$|This {{formula is}} used in the Spearman–Brown {{prediction}} formula of classical test theory. This converges to ρ if n goes to infinity, provided that the average correlation remains constant or converges too. So for the variance of the mean of <b>standardized</b> <b>variables</b> with equal correlations or converging average correlation we have ...|$|R
30|$|Our network {{variable}} has {{a standard}} deviation of 0.051. Thus, an increase by one standard deviation of migration prevalence in a district raises the probability to emigrate by 6 percentage points (1.19 ∗ 0.051, based on IV results of column 6). By contrast, a one-standard deviation increase in wealth raises this probability by 1.8 percentage points (wealth being a <b>standardized</b> <b>variable),</b> i.e., a less than three times smaller impact. This indicates the important magnitude of the effect of networks in favoring migration.|$|E
40|$|The article {{deals with}} the {{evaluation}} of the firm's position. Recognition of market position among competitors but also among customers is important {{for the development of the}} company. A comparison can be made by using multicriterial methods. The ranking of companies is based on different methods (method of simple order, weighted order sum method, point methods, <b>standardized</b> <b>variable</b> method). Compliance order will be assessed by the rank correlation coefficients. Acquired assessment of market state allows us to design a strategic...|$|E
3000|$|... where TCD il [...] {{total cost}} of {{controlling}} finished product i at distribution center l,MD 3 il [...] mean demand of product i at distribution center l,ML 3 il [...] expected demand of product i during lead time at distribution center l, LI 3 il [...] = I N ((u d [...]) il [...]) loss integral representing the expected number of units out of stock during an order cycle of product i at distribution center l, and (u d [...]) il [...] <b>standardized</b> <b>variable</b> at distribution center echelon.|$|E
25|$|Therefore, the {{variance}} of the mean {{of a large number}} of <b>standardized</b> <b>variables</b> is approximately equal to their average correlation. This makes clear that the sample mean of correlated variables does not generally converge to the population mean, even though the law of large numbers states that the sample mean will converge for independent variables.|$|R
40|$|ABSTRACTThe aim of {{this study}} was i) to compare the results {{obtained}} with the Z index (sum of <b>standardized</b> <b>variables)</b> in relation to the classical (Smith, 1936; HAZEL, 1943) and rank sumindexes (Mulamba and MOCK, 1978) by means of the growth and wood technology traits and ii) to verify the progeny x environmentinteraction in simultaneous multiple trait selection, based on the Z index. Thus, we used a full sib progenytest ofEucalyptus urophyllaandEucalyptus grandisevaluated in two locations in the municipality of Ipaba,Minas Gerais state. At three years of age, the following traits were evaluated: average yearly increase ofwood, wood density, refined pulp yield and effective alkali. There was good agreement in the progenyselection by the indexes. Thus, use of the Z index of <b>standardized</b> <b>variables</b> is a good option in simultaneousselection of multiple traits in the forestry sector through its ease of application and, above all, interpretationof results. The mentioned index revealed efficient {{in the study of the}} genotypes x environments interaction...|$|R
30|$|For the SVM algorithm, data {{standardization}} is {{also carried}} out {{in order to make}} data dimensionless. After standardization, all knowledge of the scale and the location of the original data may be lost. It is essential to <b>standardize</b> <b>variables</b> in cases where the difference measure, such as the Euclidean distance, is sensitive to the changes in the magnitudes or scales of the input variables [18].|$|R
3000|$|... where TCS nk [...] {{total cost}} of {{controlling}} raw material n required at plant k, MDI nk [...] mean demand of raw material n required at plant k, ML 1 nk [...] expected demand of raw material n during lead time at plant k, LI 1 nk [...] = I N ((u s [...]) nk [...]), loss integral representing the expected number of units out of stock during an order cycle of raw material n at plant k, and (u s [...]) nk [...] <b>standardized</b> <b>variable</b> at the supplier echelon.|$|E
3000|$|... where TC ik [...] {{total cost}} of {{controlling}} finished product i at plant k, TCP ik [...] cost of production of product i at plant k, TCF ik [...] {{total cost of}} controlling finished product stockpile, MD 2 ik [...] mean demand of product i at plant k,ML 2 ik [...] expected demand of product i during leadtime at plant k,LI 2 ik [...] = I N (u p [...]) ik, loss integral representing the expected number of units out of stock during an order cycle of product i at plant k, and (u p [...]) ik [...] <b>standardized</b> <b>variable</b> of product i at plant echelon.|$|E
40|$|Background: The VDW TUMOR {{database}} {{is constructed}} using nationally <b>standardized</b> <b>variable</b> definitions. The North American Association of Central Cancer Registries (NAACCR) {{is a professional}} agency for establishing data standards. The database is NAACCR compatible with respect to variable definitions and record layout. Many of the CRN sites are also Surveillance, Epidemiology, and End Results (SEER) Program sites. SEER collects data from specific geographic regions representing 26 % of US population and is demographically diverse. Access to SEER data provides a mature data source and familiar variable list for use in cancer research. Non SEER site can access comparable data using either hospital based or State based data collection systems...|$|E
5000|$|... 1) [...] where: [...] is the <b>standardized</b> {{observed}} <b>variable</b> {{measured with}} the ith trait and jth method. [...] is the reliability coefficient, which is equal to: [...] is the <b>standardized</b> true score <b>variable</b> [...] is the random error, which is equal to: [...] Consequently: [...] where: [...] is the reliability ...|$|R
40|$|Velleman & Wilkinson (1993) {{give a lot}} of {{examples}} to show that the hierarchy of scalers introduced by Stevens is misleading. We will show that these examples gain are misleading if we accept that most real data sets stem come derived measurement. For derived measurement the absolute scale is very important. Count data, ranks, percent data, <b>standardized</b> <b>variables</b> all have an absolute scale. [...] Scale type,permissible transformations,data transformation...|$|R
40|$|The {{asymptotic}} {{correlations among}} maximum likelihood (ML) and various least squares (LS) estimators in factor analysis are derived. The LS estimators include the unweighted (ULS) and weighted estimators for unstandardized {{variables and the}} ULS estimators for <b>standardized</b> <b>variables.</b> The derived formulas cover the cases with restrictions on parameters. Numerical examples with simulations are provided to confirm {{the accuracy of the}} formulas and the influence of scales on the asymptotic correlations...|$|R
40|$|This bachelor's {{thesis is}} focused on {{financial}} analysis of company SONBERK, a. s. The aim of this thesis is to determinate the financial health of this company from {{a point of view}} of a potential investor. Financial analysis is based on annual reports issued by the company in the period of years 2008 [...] 2012. The structure is divided into introduction, theoretical part, application and conclusion. The company is analyzed by horizontal and vertical analysis, balance principles, ratio analysis, analysis of a net working capital, analysis of an economic value added and bankruptcy and credibility models. For intercompany comparison is used <b>standardized</b> <b>variable</b> method...|$|E
40|$|The {{aim of this}} diploma {{thesis is}} to {{evaluate}} the age and aging of the European population. At present, the aging process is affecting most countries in the Word. It occurs mainly in advanced countries, which includes most of the European countries. Population aging {{is linked to the}} process of the second demographic transition, which results in a change of the shares of the child´s and the old population. The main objective is to evaluate the age of individual regions (states) of Europe, according to various indicators, and thereafter to identify sub-regions in Europe according to the age of the population based on multidimensional statistical methods (point method, <b>standardized</b> <b>variable</b> method, factor and cluster analysis) ...|$|E
40|$|OBJECTIVE: To examine {{response}} rate information from mailed physician questionnaires reported in published articles. DATA SOURCES/STUDY SETTING: Citations for articles published between 1985 and 1995 were obtained using a key word {{search of the}} Medline, PsychLit, and Sociofile databases. STUDY DESIGN: A 5 percent random sample of relevant citations was selected from each year. DATA COLLECTION/EXTRACTION METHODS: Citations found to be other than physician surveys were discarded and replaced with the next randomly assigned article. Selected articles were abstracted using a <b>standardized</b> <b>variable</b> list. PRINCIPAL FINDINGS: The average {{response rate}} for mailed physician questionnaires was 61 percent. The average response rate for large sample surveys (> 1, 000 observations) was 52 percent. In addition, only 44 percent of the abstracted articles reported a discussion of response bias, and only 54 percent reported any type of follow-up. CONCLUSIONS: (1) Response rates have remained somewhat constant over time, and (2) researchers need to document the efforts used to increase response rates to mailed physician questionnaires...|$|E
5000|$|Equivalently, the {{correlation}} matrix {{can be seen}} as the covariance matrix of the <b>standardized</b> random <b>variables</b> [...] for [...]|$|R
3000|$|Overall, {{they found}} that Years of Education was a {{stronger}} predictor of wage outcomes than Numeracy. For example, for the dichotomous outcome indicating whether the respondent’s income is in the top quartile of the income distribution, the IV-estimated coefficient of the (<b>standardized)</b> education <b>variable</b> {{is more than twice}} as large as the IV-estimated coefficient of the (<b>standardized)</b> Numeracy <b>variable.</b> 9 As we shall see, these findings, as well as those from the other studies cited, are broadly consistent with ours. 10 [...]...|$|R
50|$|Disadvantages: Critics voice {{concerns}} {{that such a}} standardization can be misleading. Since <b>standardizing</b> a <b>variable</b> removes the unit of measurement from its value, a standardized coefficient for a given relationship only represents its strength relative to the variation in the distributions. This invites bias due to sampling error when one <b>standardizes</b> <b>variables</b> using {{means and standard deviations}} based on small samples. Furthermore, a change of one standard deviation in one variable is only equivalent to a change of one standard deviation in another predictor insofar as the shapes of the two variables' distributions resemble one another. The meaning of a standard deviation may vary markedly between non-normal distributions (e.g., when skewed or otherwise asymmetrical). This underscores the importance of normality assumptions in parametric statistics, and poses an additional problem when interpreting standardized coefficient estimates that even nonparametric regression does not solve when dealing with non-normal distributions.|$|R
40|$|The paper “Possibilities of {{regional}} disparities´ measurement – a new viewpoint” {{is devoted to}} the description and analysis of utilization of statistical-mathematic methods with whose help it is possible to expertly evaluate trends and development trends in the field {{of regional}} divergences. Regarding to the fact that this paper is a certain consummation of previous works dealing with the problems of regional disparities´ measurement, not all the analyzed methods are its part, but only those at which the author came to a conclusion that their utilization seems to be as the most suitable from the viewpoint of the difficulty level of their practical use {{as well as from the}} viewpoint of the quantification of necessary supporting data. Within this paper you will thus subsequently acquaint with a semaphore method, spot method and <b>standardized</b> <b>variable</b> method while you will simultaneously see the practical possibility of utilization of these methods while evaluating of regional disparities. ...|$|E
40|$|The {{bachelor}} thesis {{deals with}} evaluation of peripherality of municipalities {{in the model}} area of Rožmitálsko by means of spatial and so-called "aspatial" characteristics (features). The aim of the thesis is a comparison of municipalities in the area under study {{by way of the}} <b>standardized</b> <b>variable</b> method and through identification of their development possibilities. The introductory part is devoted to theoretical definitions of basic terms related to peripheral areas and to discussion of relevant sources of literature. Attention is paid also to discussion of studies concerning research of peripheral areas in Czechia. The next part of the work is dedicated to determination of methods for the analysis; the characterisation of the model area follows. In the main empirical part of the work, the chosen methodological basis is applied on the model area of Rožmitálsko. The evaluation of peripherality of the municipalities in the model area is implemented using primary and available secondary data. The conclusion summarizes the obtained findings and formulates suggestions in terms of further development of the municipalities of the model area...|$|E
40|$|This paper {{attempts}} {{to explain how}} the problem of multicollinearity can be reduced in polynomial regression analysis. A simple standardizat ion technique is il lustrated to deal wi th curvi 1 ineari ty and mul ticoll ineari ty problems, Suppose we have a cubic relationship to solve, and that the independent variable. 1 ’ has large values. (1) Y = co + clx + @X 2 + QX 3 Then, it is very likely that the problem of multicell variables X, A’: and X 3 in equation (1) are collinear inearity arises s inte the independent wi th each other. In case of severe multicollinearity, this regression equation cannot be solved sinte the rank of the matrix is less than its order, If the tolerante is very small, but still larger than the specified tolerante level, unstable beta weights(standardized regression coefficents) and calculations can be expected. In this case, multicollinearity can also be detected by size of the standard errors o f beta, The problem of multicollinearity can be removed or reduced substantially by standardizing the linear, quadratic, and cubic terms in the polynomial regression equation. First, it is suggested that the independent variable is transformed {{in such a way that}} the resul ting mean is zero and the resul ting standard deviation is one. For example, the independent variable J’can be standardized by using the following linear transformation: (2) Standard i zed Vari ab 1 e (Z) = ~%,T’) /Sx Then, new quadratic and cubic terms are created by taking squared and tubed values of this <b>standardized</b> <b>variable,</b> (3) Squared Variable(ZZ) = (ciu_. ~~/S~) ~ (4) Cube...|$|E
50|$|Individual {{competence}} {{values are}} used to weight the responses and estimate the culturally correct answers. In the formal model, a confidence level (Bayesian adjusted probabilities) is obtained for each answer from the pattern of responses and the individual competence scores. In the informal model, responses are also weighted, using a linear model. When factoring a correlation matrix, the estimated answers appear as {{the first set of}} factor scores. Also, note that factor scores are usually provided as <b>standardized</b> <b>variables</b> (mean of zero), but may be transformed back to your original data collection units.|$|R
50|$|Pradhan was {{successful}} in elucidating the mechanisms of neurophysiological F‑response generation in healthy and diseased bodies and discovered a phenomenon, F-response multiplicity, {{a symptom of the}} lower motor neuron disorder. Pradhan asserted, by way of his findings, loss of early components and scattering of late components are responsible for the different F-response parameters in the lower neuron disorders. He has also demonstrated <b>standardized</b> <b>variables</b> of contraction enhanced H‑reflex called R‑1 response and its utility in nerve root lesions where H-reflex is not electable. This was ratified by many researchers.|$|R
30|$|The <b>standardized</b> random <b>variable</b> z=(y−μ)/σ {{clearly has}} density and {{reliability}} functions f 0 (z) and R 0 (z) respectively, and Equation (1) with μ= 0 and σ= 1 {{is called the}} standard form of the distribution.|$|R
