745|3096|Public
25|$|Performance of cross {{interleaver}} : As {{shown in}} the above interleaver figure, the output is nothing but the diagonal symbols generated {{at the end of}} each delay line. In this case, when the input multiplexer switch completes around half switching, we can read first row at the receiver. Thus, we need to store maximum of around half message at receiver in order to read first row. This drastically brings down the <b>storage</b> <b>requirement</b> by half. Since just half message is now required to read first row, the latency is also reduced by half which is good improvement over the block interleaver. Thus, the total interleaver memory is split between transmitter and receiver.|$|E
5000|$|Note {{that each}} {{choice of the}} bit-subset {{selector}} imposes a <b>storage</b> <b>requirement</b> (C) that is exponential in the cardinality of the set of chosen bits.|$|E
5000|$|A Range Query Tree with an {{underlying}} array of size n (padded to {{a power of}} two) has n leaves {{and a total of}} [...] nodes which requires O(n) <b>storage</b> <b>requirement.</b>|$|E
40|$|The <b>storage</b> <b>requirements</b> of {{conventional}} enumerative schemes {{can be reduced}} by using floating point arithmetical operations instead of the conventional fixed point operations. The new enumeration scheme incurs a small coding loss. A simple relationship between <b>storage</b> <b>requirements</b> and coding loss is derived...|$|R
5|$|There is {{no limit}} {{in the law}} on number of owned guns. The law {{specifies}} safe <b>storage</b> <b>requirements</b> for those owning more than two weapons or more than 500 rounds of ammunition. The safe <b>storage</b> <b>requirements</b> are further exacerbated for those owning more than 10 and more than 20 firearms.|$|R
30|$|We {{have reduced}} <b>storage</b> <b>requirements</b> {{by a factor}} of n.|$|R
5000|$|Storage - The {{amount of}} memory {{required}} for the processing of the lock mechanism. The <b>storage</b> <b>requirement</b> scales {{with the number of}} threads due to the increase {{in the size of the}} array can_serve.|$|E
50|$|Because of {{the expense}} and effort {{involved}} in chemical synthesis, the chemicals must be correctly stored and banked away {{for later use}} to prevent early degradation. Each chemical has a particular shelf life and <b>storage</b> <b>requirement</b> and in a good-sized chemical library, there is a timetable by which library chemicals are disposed of and replaced on a regular basis. Some chemicals are fairly unstable, radioactive, volatile or flammable and must be stored under careful conditions in accordance with safety standards such as OSHA.|$|E
50|$|Although Hough {{transform}} (HT) {{has been}} widely used in curve detection, it has two major drawbacks: First, for each nonzero pixel in the image, the parameters for the existing curve and redundant ones are both accumulated during the voting procedure. Second, the accumulator array (or Hough space) is predefined in a heuristic way. The more accuracy needed, the higher parameter resolution should be defined. These two needs usually result in a large <b>storage</b> <b>requirement</b> and low speed for real applications. Therefore, RHT was brought up to tackle this problem.|$|E
40|$|In {{this paper}} we {{investigate}} the additional storage overhead {{needed for a}} parallel implementation of finite element applications. In particular, we compare the <b>storage</b> <b>requirements</b> for the factorization of the sparse matrices that would occur on parallel processor versus a uniprocessor. This variation in storage results from the factorization fill-in. We {{address the question of}} whether the storage overhead is so large for parallel implementations that it imposes severe limitations on the problem size in contrast to the problems executed sequentially on a uniprocessor. The <b>storage</b> <b>requirements</b> for the parallel implementation is based upon a new ordering scheme, the Combination Mesh-Based scheme. This scheme uses a domain decomposition method which attempts to balance the processors' loads and decrease the interprocessor communication. The <b>storage</b> <b>requirements</b> for the sequential implementation is based upon the Minimum Degree algorithm. The difference between the two <b>storage</b> <b>requirements</b> [...] ...|$|R
5000|$|Allocating system {{storage and}} {{planning}} future <b>storage</b> <b>requirements</b> for the database system ...|$|R
5000|$|ISO 11799:2003 Information and documentation—Document <b>storage</b> <b>requirements</b> for archive {{and library}} {{materials}} ...|$|R
5000|$|Hence, {{the memory}} <b>storage</b> <b>requirement</b> in a DNS grows very fast with the Reynolds number. In addition, given {{the very large}} memory necessary, the {{integration}} of the solution in time must be done by an explicit method. This means that in order to be accurate, the integration, for most discretization methods, must be done with a time step, , small enough such that a fluid particle moves {{only a fraction of the}} mesh spacing [...] in each step. That is,( [...] is here the Courant number). The total time interval simulated is generally proportional to the turbulence time scale [...] given by ...|$|E
50|$|Performance of cross {{interleaver}} : As {{shown in}} the above interleaver figure, the output is nothing but the diagonal symbols generated {{at the end of}} each delay line. In this case, when the input multiplexer switch completes around half switching, we can read first row at the receiver. Thus, we need to store maximum of around half message at receiver in order to read first row. This drastically brings down the <b>storage</b> <b>requirement</b> by half. Since just half message is now required to read first row, the latency is also reduced by half which is good improvement over the block interleaver. Thus, the total interleaver memory is split between transmitter and receiver.|$|E
5000|$|In the point-source concept {{the major}} problem {{that has to be}} {{circumvented}} is the trade-off between data storage capacity and computational speed. In particular, algorithms that raise the computational speed usually need very high data storage capabilities,while on the other hand algorithms that lower the data <b>storage</b> <b>requirement</b> lead to high computational complexity, though some optimizations could be achieved.Another concept which leads to Point Source CGHs is the Ray tracing method. Ray tracing is perhaps the simplest method of computer generated holography to visualize. Essentially, the path length difference between the distance a virtual [...] "reference beam" [...] and a virtual [...] "object beam" [...] have to travel is calculated; this will give the relative phase of the scattered object beam.|$|E
5|$|The European Nucleotide Archive handles {{large volumes}} of data which pose a {{significant}} storage challenge. As of 2012, the ENA's <b>storage</b> <b>requirements</b> continue to grow exponentially, with a doubling time of approximately 10 months. To manage this increase, the ENA selectively discards less-valuable sequencing platform data and implements advanced compression strategies. The CRAM reference-based compression toolkit was developed to help reduce ENA <b>storage</b> <b>requirements.</b>|$|R
50|$|CIRT also {{maintains}} a computer centre, {{catering to the}} communication and data <b>storage</b> <b>requirements</b> of the Institute.|$|R
5000|$|... #Caption: The law {{specifies}} safe <b>storage</b> <b>requirements</b> {{for those}} owning more than 2, 10 and 20 firearms.|$|R
5000|$|Sweden {{has also}} made it {{possible}} in 2013 to certify safe and sanitized blackwater (urine and human excreta) from blackwater systems and for further use as a recognized fertilizer. Such blackwater systems could be vacuum toilets or septic tanks. The criteria for the certification have been developed by the Swedish Institute for Agricultural and Environmental Engineering and may {{pave the way for}} farmers to use human waste for agricultural production. The Federation of Swedish Farmers have been active in this development. Furthermore, the Swedish EPA in their last proposal in 2014 has downgraded the hygiene risk associated with urine. Previously the normal <b>storage</b> <b>requirement</b> for hygienic quality for large scale use of urine was 6 months. Now they propose decreasing this to one month.|$|E
50|$|The {{developers}} {{have a set}} of minimum requirements that must be met for the emulator to function properly. As of March 3, 2017, users are required to be running a 64-bit version of either Windows 7 or later, or a modern distro of Linux. A modern processor in the AMD FX series or Intel Sandy Bridge series or later is required; the processor needs to support SSE3. Any modern GPU that supports OpenGL 4.3 or greater can be used, Vulkan and DirectX 12 are also supported. 8GB of RAM or higher. In order to actually run the emulator, the Microsoft Visual C++ 2015 redistributable, the PlayStation 3's firmware dump, and games or applications are required. As games and applications can be installed onto the emulated PS3, the <b>storage</b> <b>requirement</b> depends on what is installed.|$|E
5000|$|The P-Dimensional inverse Discrete Cosine Transform {{is given}} by: [...] The DCT finds {{its use in}} data {{compression}} applications such as the JPEG image format. The DCT has high degree of spectral compaction at a qualitative level, which makes it very suitable for various compression applications. A signals DCT representation tends {{to have more of}} its energy concentrated in a small number of coefficients when compared to other transforms like the DFT. Thus you can reduce your data <b>storage</b> <b>requirement</b> by only storing the DCT outputs that contain significant amounts of energy. The computational complexity of P-D DCT goes by O(......). Since the number of operations required to compute the DCT is less than that required to compute the DFT without the use of FFT, the DCTs are also called as Fast Cosine Transforms(FCT).|$|E
5000|$|The {{data rates}} and <b>storage</b> <b>requirements</b> for the widely used YCbCr 4:2:2 chroma {{subsampling}} format are listed below: ...|$|R
40|$|While low-density parity-check (LDPC) {{convolutional}} codes tend {{to significantly}} outperform LDPC block codes {{with the same}} processor complexity, large <b>storage</b> <b>requirements</b> and a long initial decoding delay are two issues related to their continuous pipeline decoding architecture [A. Jimenez Feltstrom et al., (1999) ]. In this paper, we propose reduced complexity decoding strategies to lessen the <b>storage</b> <b>requirements</b> and the initial decoding delay without significant loss in performance...|$|R
40|$|Server {{assisted}} {{one time}} signature scheme was recently {{presented as a}} nonrepudiation service for mobile and constrained devices. However, the scheme suffered with high <b>storage</b> <b>requirements</b> for the virtual server and high memory requirements for the mobile client. We improve the scheme by significantly reducing virtual server <b>storage</b> <b>requirements</b> as well as mobile client memory requirements. More precisely, the virtual server <b>storage</b> <b>requirements</b> in our scheme are reduced {{by a factor of}} more than 80 compared to the original scheme. Further, memory requirements for the mobile client are reduced by a factor of more than 130. This is done by generating various quantities pseudorandomly and storing just their cryptographic hash (instead of storing them fully) wherever possible, while still being able to perform dispute resolution. ...|$|R
50|$|In bioinformatics, {{inverted}} indexes {{are very}} important in the sequence assembly of short fragments of sequenced DNA. One way to find {{the source of a}} fragment is to search for it against a reference DNA sequence. A small number of mismatches (due to differences between the sequenced DNA and reference DNA, or errors) can be accounted for by dividing the fragment into smaller fragments—at least one subfragment is likely to match the reference DNA sequence. The matching requires constructing an inverted index of all substrings of a certain length from the reference DNA sequence. Since the human DNA contains more than 3 billion base pairs, and we need to store a DNA substring for every index and a 32-bit integer for index itself, the <b>storage</b> <b>requirement</b> for such an inverted index would probably be in the tens of gigabytes.|$|E
5000|$|Hough {{transforms}} are {{techniques for}} object detection, a critical step in many implementations of computer vision, or data mining from images. Specifically, the Randomized Hough transform is a probabilistic variant {{to the classical}} Hough transform, and is commonly used to detect curves (straight line, circle, ellipse, etc.) The basic idea of Hough transform (HT) is to implement a voting procedure for all potential curves in the image, and at the termination of the algorithm, curves that do exist in the image will have relatively high voting scores. Randomized Hough transform (RHT) is different from HT in that it tries to avoid conducting the computationally expensive voting process for every nonzero pixel in the image {{by taking advantage of}} the geometric properties of analytical curves, and thus improve the time efficiency and reduce the <b>storage</b> <b>requirement</b> of the original algorithm.|$|E
50|$|Wood-pellet {{heating and}} other types of wood heating systems have {{achieved}} their greatest success in heating premises that are off the gas grid, typically being previously heated using heating oil or coal. Solid wood fuel requires a large amount of dedicated storage space, and the specialized heating systems can be expensive (though grant schemes are available in many European countries to offset this capital cost.) Low fuel costs mean that wood fuelled heating in Europe is frequently able to achieve a payback period of less than 3 to 5 years. Because of the large fuel <b>storage</b> <b>requirement</b> wood fuel can be less attractive in urban residential scenarios, or for premises connected to the gas grid (though rising gas prices and uncertainty of supply mean that wood fuel is becoming more competitive.) There is also growing concern over the air pollution from wood heating versus oil or gas heat, especially the fine particulates.|$|E
5000|$|It has {{substantial}} computational and <b>storage</b> <b>requirements</b> which become acute when {{object orientation}} and scale {{have to be}} considered.|$|R
30|$|The aim of {{blocking}} the matrix G {{is to reduce}} the computational efforts and <b>storage</b> <b>requirements</b> when implementing the final algorithm.|$|R
50|$|The main {{advantage}} of the algorithm is that its <b>storage</b> <b>requirements</b> remain constant and are independent {{of the number of}} processors.|$|R
5000|$|In {{scientific}} computing, skyline matrix storage, or SKS, or {{a variable}} band matrix storage, or envelope storage scheme {{is a form}} of a sparse matrix storage format matrix that reduces the <b>storage</b> <b>requirement</b> of a matrix more than banded storage. In banded storage, all entries within a fixed distance from the diagonal (called half-bandwidth) are stored. In column-oriented skyline storage, only the entries from the first nonzero entry to the last nonzero entry in each column are stored. There is also row oriented skyline storage, and, for symmetric matrices, only one triangle is usually stored. Skyline storage has become very popular in the finite element codes for structural mechanics, because the skyline is preserved by Cholesky decomposition (a method of solving systems of linear equations with a symmetric, positive-definite matrix; all fill-in falls within the skyline), and systems of equations from finite elements have a relatively small skyline. In addition, the effort of coding skyline Cholesky is about same as for Cholesky for banded matrices (available for banded matrices, e.g. in LAPACK; for a prototype skyline code, see [...] ).|$|E
50|$|Power-to-gas {{systems may}} be {{deployed}} as adjuncts to wind parks or solar-electric generation. The excess power or off-peak power generated by wind generators or solar arrays may {{then be used}} {{at a later time}} for load balancing in the energy grid. Before switching to natural gas, the German gas networks were operated using towngas, which for 50-60 % consisted of hydrogen. The storage capacity of the German natural gas network is more than 200,000 GWh which is enough for several months of energy requirement. By comparison, the capacity of all German pumped storage power plants amounts to only about 40 GWh. The <b>storage</b> <b>requirement</b> in Germany is estimated at 16GW in 2023, 80GW in 2033 and 130GW in 2050. The transport of energy through a gas network is done with much less loss (<0.1%) than in a power network (8%). The storage costs per kilowatt hour are estimated at €0.10 for hydrogen and €0.15 for methane. The use of the existing natural gas pipelines for hydrogen was studied by the EU NaturalHy project and US DOE. The blending technology is also used in HCNG.|$|E
50|$|On other systems, the {{compiler}} might {{build its}} symbol {{table in a}} similar manner, but eventually the storage requirements would be collated and the machine code would be written to use flat memory addresses of 16-bits or 32-bits or even 64-bits. These addresses might contain anything so that a write to the wrong address could damage anything. Instead, the two-part address scheme was implemented by the hardware. At each lexical level, variables were placed at displacements up {{from the base of}} the level's stack, typically occupying one word - double precision or complex variables would occupy two. Arrays were not stored in this area, only a one word descriptor for the array. Thus, at each lexical level the total <b>storage</b> <b>requirement</b> was not great: dozens, hundreds or a few thousand in extreme cases, certainly not a count requiring 32-bits or more. And indeed, this was reflected in the form of the VALC instruction (value call) that loaded an operand onto the stack. This op-code was two bits long and the rest of the byte's bits were concatenated with the following byte to give a fourteen-bit addressing field. The code being executed would be at some lexical level, say six: this meant that only lexical levels zero to six were valid, and so just three bits were needed to specify the lexical level desired. The address part of the VALC operation thus reserved just three bits for that purpose, with the remainder being available for referring to entities at that and lower levels. A deeply nested procedure (thus at a high lexical level) would have fewer bits available to identify entities, and so for level sixteen upwards their number was restricted. At the deepest nesting five bits would be needed to specify the choice of levels 0-31 thus leaving nine bits to identify 512 entities - not a severe constraint. This is much more compact than addressing entities by their literal memory address in a 32-bit addressing space. Further, only the VALC opcode loaded data: opcodes for ADD, MULT and so forth did no addressing, working entirely on the top elements of the stack.|$|E
5000|$|Network optimizations: Mirage leverages {{a global}} single index that enables {{significant}} deduplication across all users, reducing network traffic and <b>storage</b> <b>requirements.</b>|$|R
50|$|Y.Station is {{designed}} specifically to meet large-scale energy <b>storage</b> <b>requirements.</b> The pre-designed, pre-engineered solution allows for efficient project planning and faster deployment.|$|R
40|$|The paper {{considers}} {{the use of}} numerically stable square-root information filter (SRIF) algorithms to reduce the computation and <b>storage</b> <b>requirements</b> of a certain class of large-scale linear interconnected systems (multistation satellite tracking is examined as an example). The reductions are in comparison with conventional sequential covariance type formulations. To illustrate the SRIF algorithm: a 40 subsystem, 10 state problem, for example, has its <b>storage</b> <b>requirements</b> reduced by a full order of magnitude (from 84255 to 8100) ...|$|R
