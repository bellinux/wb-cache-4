196|448|Public
50|$|The bipartite {{selection}} (Fig 1 (C)) {{method was}} proposed by Isalan et al., 2001 as {{a compromise between}} the parallel and <b>sequential</b> <b>selection</b> strategies. The first and last 5 bp of the 9 bp target site are selected in parallel and combined to produce a library from which the final ZFP is chosen.|$|E
5000|$|By the Medieval era the Jewish {{community}} had a standardized schedule of scripture readings {{both from the}} Torah and the prophets to be read in the synagogue. A <b>sequential</b> <b>selection</b> was read from the Torah, followed by the [...] "haftarah" [...] - a selection from the prophetic books or historical narratives (e.g. [...] "Judges," [...] "Kings," [...] etc.) {{closely linked to the}} selection from the Torah. Jesus may have read a providentially [...] "random" [...] reading when he read from , as recorded in , when he inaugurated his public ministry. The early Christians adopted the Jewish custom of reading extracts from the Old Testament on the Sabbath. They soon added extracts from the writings of the Apostles and Evangelists.|$|E
5000|$|Herbert Ellis Robbins (January 12, 1915 - February 12, 2001) was an American {{mathematician}} and statistician. He did {{research in}} topology, measure theory, statistics, {{and a variety}} of other fields. [...] He was the co-author, with Richard Courant, of What is Mathematics?, a popularization that is still (...) in print. The Robbins lemma, used in empirical Bayes methods, is named after him. Robbins algebras are named after him because of a conjecture (since proved) that he posed concerning Boolean algebras. The Robbins theorem, in graph theory, is also named after him, as is the Whitney-Robbins synthesis, a tool he introduced to prove this theorem. The well-known unsolved problem of minimizing in <b>sequential</b> <b>selection</b> the expected rank of the selected item under full information, sometimes referred to as the fourth secretary problem, also bears his name: Robbins' problem (of optimal stopping).|$|E
3000|$|... [34], {{sequential}} search such as <b>sequential</b> forward <b>selection,</b> <b>sequential</b> backward elimination, bidirectional selection [...]...|$|R
40|$|Background: In {{this paper}} we compare a highly {{accurate}} supervised to an unsupervised technique that uses breast thermal images {{with the aim}} of assisting physicians in early detection of breast cancer. Methods: First, we segmented the images and determined the region of interest. Then, 23 features that included statistical, morphological, frequency domain, histogram and gray-level co-occurrence matrix based features were extracted from the segmented right and left breasts. To achieve the best features, feature selection methods such as minimum redundancy and maximum relevance, <b>sequential</b> forward <b>selection,</b> <b>sequential</b> backward <b>selection,</b> <b>sequential</b> floating forward <b>selection,</b> <b>sequential</b> floating backward <b>selection,</b> and genetic algorithm were used. Contrast, energy, Euler number, and kurtosis were marked as effective features. Results: The selected features were evaluated by fuzzy C-means clustering as the unsupervised method and compared with the AdaBoost supervised classifier which has been previously studied. As reported, fuzzy C-means clustering with a mean accuracy of 75...|$|R
30|$|<b>Sequential</b> forward <b>selection</b> (Jain and Zongker 1997; Sun and Yao 2006).|$|R
50|$|Methods of item {{selection}} {{fall into}} two categories: cutscore-based and estimate-based. Cutscore-based methods (also known as <b>sequential</b> <b>selection)</b> maximize the {{information provided by the}} item at the cutscore, or cutscores if there are more than one, regardless of the ability of the examinee. Estimate-based methods (also known as adaptive selection) maximize information at the current estimate of examinee ability, regardless of the location of the cutscore. Both work efficiently, but the efficiency depends in part on the termination criterion employed. Because the sequential probability ratio test only evaluates probabilities near the cutscore, cutscore-based item selection is more appropriate. Because the confidence interval termination criterion is centered around the examinees ability estimate, estimate-based item selection is more appropriate. This is because the test will make a classification when the confidence interval is small enough to be completely above or below the cutscore (see below). The confidence interval will be smaller when the standard error of measurement is smaller, and the standard error of measurement will be smaller when there is more information at the theta level of the examinee.|$|E
50|$|<b>Sequential</b> <b>selection</b> (Fig. 1 (B)), {{put forward}} by the Pabo group in 1997 embraces the {{cooperative}} binding between zinc fingers to produce DNA-binding domains of great affinity and specificity. As suggested by the name, each finger is selected from a randomised library {{in the context of}} the previously selected finger. The techniques used in selection are similar to those described below except that the target oligonucleotide used in selection contains the entire target sequence. As shown in Fig. 1, a library is created in which finger three contains the randomised alpha-helix. The domain with the best binding characteristics is selected and then included in another library in which the finger-one anchor is removed and another randomised finger is added to the opposite end. This continues and results in a DNA-binding domain in which all fingers were selected {{in the context of the}} neighbouring finger and since each round of selection is applied to the same final target sequence, target site overlap still occurs but is an asset rather than a hindrance.|$|E
50|$|Coffman is {{best known}} for his seminal {{research}} together with his international collaborations, measured in part by some 150 co-authors in his collection of publications. His work can be found in over 180 articles in technical journals devoted to original research contributions. He published 4 graduate-level text books, and papers in the proceedings of some 250 conferences and workshops, most of these being preliminary versions of journal articles. In his research, Coffman has been a generalist following many parallel paths in engineering and applied mathematics. The directions he has taken have drawn on the tools of combinatorial optimization and the theory of algorithms, along with those of applied probability and stochastic processes. The processes studied include those in the theories of scheduling, bin packing, <b>sequential</b> <b>selection,</b> graphs, and dynamic allocation, along with those in queueing, polling, reservation, moving-server, networking, and distributed local-rule systems (e.g. cellular automata). His contributions have been divided between mathematical foundations and the design and analysis of approximation algorithms providing the basis for engineering solutions to NP-hard problems. Computer and network engineering applications have been broad in scope; a partial list includes research addressing problems in the scheduling and storage allocation functions of computer operating systems, storage architectures, data structures, computer timing problems such as deadlocks and synchronization, Internet congestion, peer-to-peer file sharing networks, stream merging, self-assembly processes of molecular computing, minimalist algorithms in sensor networks, optical burst switching, and dynamic spectrum management in cognitive networks. The list expands greatly when including the myriad applications in industrial engineering and operations research of Coffman's research in scheduling and bin-packing theory in one and two dimensions. As of November 11, 2015, his works have been cited 13,597 times, and he has an h-index of 49.|$|E
5000|$|Gearbox: Six-speed {{longitudinal}} <b>sequential.</b> Gear <b>selection</b> via a Magneti Marelli paddle {{shift system}} ...|$|R
40|$|This paper {{presents}} a novel method {{to control the}} number of crossvalidation repetitions in <b>sequential</b> forward feature <b>selection</b> algorithms. The criterion for selecting a feature is the probability of correct classification achieved by the Bayes classifier when the class feature probability density function is modeled by a single multivariate Gaussian density. Let the probability of correct classification achieved by the Bayes classifier be a random variable. We demonstrate by experiments that the probability density function of the latter random variable can be modeled by a Gaussian density. Based on this observation, a method for reducing the computational burden in <b>sequential</b> forward <b>selection</b> algorithms is proposed. The method predicts the number of crossvalidation repetitions by employing a t-test to guarantee that a statistically significant improvement in the probability of correct classification is obtained by {{increasing the number of}} selected features. The proposed method is twice feaster than the <b>sequential</b> forward <b>selection</b> algorithm that uses a fixed number of crossvalidation repetitions and it maintains the performance of the <b>sequential</b> floating forward <b>selection</b> algorithm. 1...|$|R
40|$|SUMMARY <b>Sequential</b> <b>selections</b> {{are to be}} {{made from}} two {{stochastic}} processes, or "arms", each yielding Bernoulli responses. A t each stage the arm selected depends on previous observations. The objectfve fs to maximize the expected number of successes In the first n selections. The probability of success for a given selection depends on a covariate through a logistic transformation, For one arm, this transformation is completely known; for the other, it depends on an unknown parameter, Optimal strategies are developed In terms of a break-even value for the cavariate: it is optimal to observe the arm w-l t...|$|R
40|$|This paper studies {{sequential}} forward {{feature selection}} {{that uses the}} scatter-matrix-based class separability measure. We find that by adding a scale factor to each iteration of the conventional <b>sequential</b> <b>selection,</b> a <b>sequential</b> <b>selection</b> that guarantees the global optimum can be attained. We give a thorough theoretical proof of its optimality via a novel geometric interpretation, and {{this leads to a}} unified framework including the optimal <b>sequential</b> <b>selection,</b> the conventional <b>sequential</b> <b>selection</b> and the best-individual-N selection. In addition, we show that with our formulation, feature selection can be treated as a linear fractional maximization problem, and it can be efficiently solved by algorithms well developed in the literature. This gives a non-sequential globally optimal feature selection algorithm. Both theoretical and experimental study demonstrate their efficiency. Lei Wang, Chunhua Shen and Richard Hartle...|$|E
40|$|This paper {{reveals the}} {{surprising}} result that a single-parent non-elitist evolution strategy (ES) can be locally {{faster than the}} (1 + 1) -ES. The result is brought by mirrored sampling and <b>sequential</b> <b>selection.</b> With mirrored sampling, two offspring are generated symmetrically or mirrored {{with respect to their}} parent. In <b>sequential</b> <b>selection,</b> the offspring are evaluated sequentially and the iteration is concluded as soon as one offspring is better than the current parent. Both concepts complement each other well. We derive exact convergence rates of the (1,λ) -ES with mirrored sampling and/or <b>sequential</b> <b>selection</b> on the sphere model. The log-linear convergence of the ES is preserved. Both methods lead to an improvement and in combination they can sometimes even double the convergence rate. Naively implemented into the CMA-ES with recombination, mirrored sampling leads to a bias on the step-size. However, the (1, 4) -CMA-ES with mirrored sampling and <b>sequential</b> <b>selection</b> is unbiased and appears to be faster, more robust, and as local as the (1 + 1) -CMA-ES...|$|E
3000|$|STEP 2 : {{sequential}} {{determination of}} each cluster is transformed to the <b>sequential</b> <b>selection</b> of small cells [...]...|$|E
3000|$|Failho et al. decided upon two {{possible}} methods for feature selection: <b>Sequential</b> Forward <b>Selection</b> (SFS), or bottom-up approach, and Sequential Backwards Elimination (SBE), or top-down feature selection where both {{are discussed in}} [...]...|$|R
40|$|Classifying {{cork stopper}} into group {{required}} large set of visual features. Selecting an optimal feature subset from large input feature set speeds up classification task {{and improve the}} classifier accuracy. Traditional feature selection methods, such as <b>sequential</b> forward <b>selection,</b> <b>sequential</b> backward <b>selection,</b> and <b>sequential</b> forward floating search are costly to implement. This {{paper we propose a}} feature selection method known as principal feature analysis that exploits the structure of the principal components of a feature set to find a subset of the original features information and support vector machines (SVMs) for classification. The experimental result show that the proposed method for SVM based classifier is lot faster than PCA and ICA based methods. It is also leads to better performance when the same number of principal/independent components is used and consistently picks the best subset of features in terms of sum-squared-error compared to competing methods...|$|R
50|$|Weitzman, R. A. (1982a). <b>Sequential</b> {{testing for}} <b>selection.</b> Applied Psychological Measurement, 6, 337-351.|$|R
40|$|International audienceThis paper {{reveals the}} {{surprising}} result that a single-parent non-elitist evolution strategy (ES) can be locally {{faster than the}} (1 + 1) -ES. The result is brought about by mirrored sampling and <b>sequential</b> <b>selection.</b> With mirrored sampling, two offspring are generated symmetrically or mirrored {{with respect to their}} parent. In <b>sequential</b> <b>selection,</b> the offspring are evaluated sequentially and the it- eration is concluded as soon as one offspring is better than the current parent. Both concepts complement each other well. We derive exact convergence rates of the (1, λ) -ES with mirrored sampling and/or <b>sequential</b> <b>selection</b> on the sphere model. The log-linear convergence of the ES is preserved. Both methods lead to an improvement and in combination the (1, 4) -ES becomes about 10 % faster than the (1 + 1) -ES. Naively implemented into the CMA-ES with recombination, mirrored sampling leads to a bias on the step-size. However, the (1, 4) -CMA-ES with mirrored sampling and <b>sequential</b> <b>selection</b> is unbiased and appears to be faster, more robust, and as local as the (1 + 1) -CMA-ES...|$|E
40|$|<b>Sequential</b> <b>selection</b> was {{introduced}} for Evolution Strategies (ESs) {{with the aim}} of accelerating their convergence— performing the evaluations of the different offspring sequentially and concluding an iteration immediately if one offspring is better than the parent. This paper investigates the impact of the application of <b>sequential</b> <b>selection</b> to the (1, 2) -CMA-ES on the BBOB- 2010 noisy benchmark testbed. The performance of the (1, 2 s) -CMA-ES, where <b>sequential</b> <b>selection</b> is implemented, is compared to the baseline algorithm (1, 2) -CMA-ES. Independent restarts for the two algorithms are conducted up to a maximum number of 10 4 D function evaluations, where D is the dimension of the search space. The results show a slight improvement of the (1, 2 s) -CMA-ES over the baseline (1, 2) -CMA-ES on the sphere function with Cauchy noise and a stronger decline on the sphere function with moderate uniform noise. Overall, the (1, 2 s) -CMA-ES seems slighly less reliable and we conclude that for the (1, 2) -CMA-ES, <b>sequential</b> <b>selection</b> is no improvement on noisy functions...|$|E
40|$|International audienceThis paper {{investigates the}} impact of <b>sequential</b> <b>selection,</b> a concept {{recently}} introduced for Evolution Strategies (ESs), that consists in performing the evaluations of the different candidate solutions sequentially, concluding the iteration immediately if one offspring {{is better than the}} parent. The performance of the (1, 2) -Covariance-Matrix-Adaptation Evolution-Strategy (CMA-ES) is compared to the performance of the (1, 2 $^s$) -CMA-ES where <b>sequential</b> <b>selection</b> is implemented on the BBOB- 2010 noiseless benchmark testbed. For each strategy, an independent restart mechanism is implemented. A total budget of $ 10 ^{ 4 } D$ function evaluations per trial has been used, where $D$ is the dimension of the search space. The experiments do not allow a general statement regarding a statistically significant difference between the two algorithms and we conclude that the <b>sequential</b> <b>selection</b> has no impact on the performance of the (1, 2) -CMA-ES on the noiseless BBOB- 2009 testbed...|$|E
40|$|Earlier {{literature}} has {{pointed to the}} effectiveness of residual income-type measures based on particular accrual accounting rules such as the relative benefit allocation rule. These performance metrics have been shown to generate desirable managerial incentives when investment decisions are delegated. This paper further attests to the robustness of these measures by extending the result to a <b>sequential</b> adverse <b>selection</b> model with an inherent real option (an option to abandon). In other words, as long as the residual income measures are judiciously constructed, neither private information nor the requirement to selectively exercise an option derails their use in this setting. sequential capital budgeting, residual income, accounting adjustments, <b>sequential</b> adverse <b>selection</b> problem...|$|R
3000|$|... [...]. Also, {{it is more}} {{computationally}} {{expensive than}} SBS or SFS. An optimal decision is made by using the branch and bound method [31]; however, this method's complexity increases exponentially with the required number of features to be selected. The <b>Sequential</b> Forward Floating <b>Selection</b> (SFFS) and <b>Sequential</b> Backward Floating <b>Selection</b> (SBFS) [32] are similar to Plus-l-Minus-r in some sense but instead of being tied up by [...]...|$|R
3000|$|Haralick et al. {{described}} 14 different texture features {{based on}} GCM {{at a different}} angle, which are used for quantitative description of texture features [17]. Based on the theoretical analysis and the feature selection procedure based on <b>Sequential</b> Backward <b>Selection</b> (SBS), four statistical parameters [...]...|$|R
40|$|This paper {{investigates the}} impact of <b>sequential</b> <b>selection,</b> a concept {{recently}} introduced for Evolution Strategies (ESs), that consists in performing the evaluations of the differ-ent candidate solutions sequentially, concluding the iteration immediately if one offspring {{is better than the}} parent. The performance of the (1, 2) -Covariance-Matrix-Adaptation Evo-lution-Strategy (CMA-ES) is compared to the performance of the (1, 2 s) -CMA-ES where <b>sequential</b> <b>selection</b> is imple-mented on the BBOB- 2010 noiseless benchmark testbed. For each strategy, an independent restart mechanism is im-plemented. A total budget of 104 D function evaluations per trial has been used, where D is the dimension of the search space. The experiments do not allow a general statement regard-ing a statistically significant difference between the two al-gorithms and we conclude that the <b>sequential</b> <b>selection</b> has no impact on the performance of the (1, 2) -CMA-ES on the noiseless BBOB- 2009 testbed...|$|E
40|$|This paper {{investigates the}} impact of <b>sequential</b> <b>selection,</b> a concept {{recently}} introduced for Evolution Strategies (ESs). <b>Sequential</b> <b>selection</b> performs the evaluations of the different candidate solutions sequentially and concludes the iteration immediately if one offspring {{is better than the}} parent. In this paper, the (1, 4 s) -CMA-ES, where <b>sequential</b> <b>selection</b> is implemented, is compared on the BBOB- 2010 noiseless testbed to the (1, 4) -CMA-ES. For each strategy, an inde-pendent restart mechanism is implemented. A total budget of 104 D function evaluations per trial has been used, where D is the dimension of the search space. The experiments show for the (1, 4 s) -CMA-ES a statisti-cally significant worsening compared to the (1, 4) -CMA-ES only on the attractive sector function but a significant im-provement by about 20 % on 5 out of the 24 BBOB- 2010 functions (sphere, separable and rotated ellipsoid, discus, and sum of different powers) ...|$|E
40|$|Abstract. We nd a two term {{asymptotic}} expansion for the optimal expected {{value of a}} sequentially selected monotone subsequence from a random permu-tation of length n. A striking feature of this expansion is that {{tells us that the}} expected value of optimal selection from a random permutation is quanti-ably larger than optimal <b>sequential</b> <b>selection</b> from an independent sequences of uniformly distributed random variables; specically, it is larger by at least (1 = 6) logn+O(1). Key Words. Monotone subsequence problem, <b>sequential</b> <b>selection,</b> online selection, Markov decision problem, nonlinear recursion, asymptotic...|$|E
40|$|When {{selecting}} features {{with the}} <b>sequential</b> forward floating <b>selection</b> (SFFS), the “nesting effect ” is avoided, {{which is a}} common phenomenon if the computationally less expensive <b>sequential</b> forward <b>selection</b> (SFS) is used instead. In this paper, we answer the key question, if the more complex and sophisticated SFFS {{should be used in}} on-line HMMbased recognition of handwritten whiteboard notes. In addition, an efficient method of displaying the selected feature set, the “feature map”, is introduced. In an experimental section, both selection approaches are evaluated, the derived feature sets are compared, and a discussion on the selected features is given. 1...|$|R
40|$|We {{propose a}} fully {{automated}} algorithm that {{is able to}} select a discriminative feature set from a training database via <b>sequential</b> forward <b>selection</b> (SFS), <b>sequential</b> backward <b>selection</b> (SBS), and F-score methods. We applied this scheme to microcalcifications cluster (MCC) detection in digital mammograms for early breast cancer detection. The system was able to select features fully automatically, regardless of the input training mammograms used. We tested the proposed scheme using a database of 111 clinical mammograms containing 1, 050 microcalcifications (MCs). The accuracy of the system was examined via a free response receiver operating characteristic (fROC) curve of the test dataset. The system performance for MC identifications was Az = 0. 9897, the sensitivity was 92 %, and 0. 65 false positives (FPs) were generated per image for MCC detection...|$|R
30|$|In the wrapper {{approach}} [37, 38], {{the seminal}} work of Keirn and Aunon [4] has used a combination of forward <b>sequential</b> feature <b>selection</b> and an exhaustive search to obtain a subset of relevant and non-redundant features for the mental task classification. However, wrapper approach is not suitable for high-dimensional data as it is computationally expensive.|$|R
40|$|Abstract. The feature {{selection}} {{problem in the}} field of classification consists of obtaining a subset of variables to optimally realize the task without taking into account the remainder variables. This work presents how the search for this subset is performed using the Scatter Search metaheuristic and is compared with two traditional strategies in the literature: the Forward <b>Sequential</b> <b>Selection</b> (FSS) and the Backward <b>Sequential</b> <b>Selection</b> (BSS). Promising results were obtained. We use the lazy learning strategy together with the nearest neighbour methodology (NN) also known as Instance-Based Learning Algorithm 1 (IB 1). ...|$|E
40|$|In this paper, a <b>sequential</b> <b>selection</b> {{process in}} group {{decision}} making under linguistic assessments is presented, where {{a set of}} linguistic preference relations represents individuals preferences. A collective linguistic preference is obtained {{by means of a}} defined linguistic ordered weighted averaging operator whose weights are chosen according to the concept of fuzzy majority, specified by a fuzzy linguistic quantifier. Then we define the concepts of linguistic nondora nance, linguistic dominance, and strict dominance degrees as parts of the <b>sequential</b> <b>selection</b> process. The solution alternative(s) is obtained by applying these concepts...|$|E
40|$|International audienceThis paper {{investigates the}} impact of <b>sequential</b> <b>selection,</b> a concept {{recently}} introduced for Evolution Strategies (ESs). <b>Sequential</b> <b>selection</b> performs the evaluations of the different candidate solutions sequentially and concludes the iteration immediately if one offspring {{is better than the}} parent. In this paper, the (1, 4 $^s$) -CMA-ES, where <b>sequential</b> <b>selection</b> is implemented, is compared on the BBOB- 2010 noiseless testbed to the (1, 4) -CMA-ES. For each strategy, an independent restart mechanism is implemented. A total budget of $ 10 ^{ 4 } D$ function evaluations per trial has been used, where $D$ is the dimension of the search space. The experiments show for the (1, 4 $^s$) -CMA-ES a statistically significant worsening compared to the (1, 4) -CMA-ES only on the attractive sector function but a significant improvement by about 20 % on 5 out of the 24 BBOB- 2010 functions (sphere, separable and rotated ellipsoid, discus, and sum of different powers) ...|$|E
40|$|Protein SUMO {{modification}} is {{an important}} post-translational modification and the optimization of prediction methods remains a challenge. Here, by using Support Vector Machines algorithm (SVM), a novel computational method was developed for SUMO modification site prediction based on <b>Sequential</b> Forward <b>Selection</b> (SFS) of hundreds of amino acid properties, which are collected by Amino Acid Index databas...|$|R
40|$|Within the Bayesian framework, we utilize Gaussian {{processes}} for parametric studies of long running computer codes. Since the simulations are expensive, {{it is necessary}} to exploit the computational budget in the best possible manner. Employing the sum over variances —being indicators for the quality of the fit—as the utility function, we establish an optimized and automated <b>sequential</b> parameter <b>selection</b> procedure. However, it is also often desirable to utilize the parallel running capabilities of present computer technology and abandon the <b>sequential</b> parameter <b>selection</b> for a faster overall turn-around time (wall-clock time). This paper proposes to achieve this by marginalizing over the expected outcomes at optimized test points in order to set up a pool of starting values for batch execution. For a one-dimensional test case, the numerical results are validated with the analytical solution. Eventually, a systematic convergence study demonstrates the advantage of the optimized approach over randomly chosen parameter settings...|$|R
40|$|Abstract—The P 300 -based speller is a {{well-established}} brain–computer interface for communication. It displays {{a matrix of}} objects on the computer screen, flashes each object in sequence, and looks for a P 300 response induced by flashing the desired object. Most existing P 300 spellers uses a fixed set of flash objects. We demonstrate that performance can be significantly improved by <b>sequential</b> <b>selections</b> from a hierarchy of flash sets containing variable number of objects. Theoretically, the optimal hierarchy of flash sets—with respect to a given statistical language model—can be found by solving a stochastic control problem of low computational complexity. Experimentally, statistical analysis demonstrates that the average time per output character at 85 % accuracy is reduced by over 50 % using our variable-flash-set approach as compared to traditional fixed-flash-set spellers. Index Terms—Brain–computer interfaces (BCIs), hierarchical flash sets, stochastic dynamic programming, P 300. I...|$|R
