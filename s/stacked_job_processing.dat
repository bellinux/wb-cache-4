1|862|Public
50|$|The Model 20 {{offered a}} {{simplified}} and rarely used tape-based system called TPS (Tape Processing System), and DPS (Disk Processing System) that provided {{support for the}} 2311 disk drive. TPS could run on a machine with 8 KB of memory; DPS required 12 KB, which was pretty hefty for a Model 20. Many customers ran quite happily with 4 KB and CPS (Card Processing System). With TPS and DPS, the card reader was used to read the Job Control Language cards that defined the stack of jobs to run and to read in transaction data such as customer payments. The operating system was held on tape or disk, and results could also be stored on the tapes or hard drives. <b>Stacked</b> <b>job</b> <b>processing</b> became an exciting possibility for the small but adventurous computer user.|$|E
50|$|Distributed <b>job</b> <b>processing</b> in JobServer {{is enabled}} using an agent model where remote nodes {{communicate}} {{with a central}} pair (primary/secondary) of master nodes. The master nodes {{are responsible for the}} job scheduling and distribute the <b>job</b> <b>processing</b> across a cluster of agent nodes.|$|R
40|$|We study {{a problem}} of {{scheduling}} n jobs on a single machine in batches. A batch {{is a set of}} jobs processed contiguously and completed together when the <b>processing</b> of all <b>jobs</b> in the batch is finished. Processing of a batch requires a machine setup time dependent on the position of this batch in the sequence. Setup times and <b>job</b> <b>processing</b> times are continuously controllable, that is, they are real-valued variables within their lower and upper bounds. A deviation of a setup time or <b>job</b> <b>processing</b> time from its upper bound is called a compression. The problem is to find a job sequence, its partition into batches, and the values for setup times and <b>job</b> <b>processing</b> times such that (a) total job completion time is minimized, subject to an upper bound on total weighted setup time and <b>job</b> <b>processing</b> time compression, or (b) a linear combination of total job completion time, total setup time compression, and total <b>job</b> <b>processing</b> time compression is minimized. Properties of optimal solutions are established. If the lower and upper bounds on <b>job</b> <b>processing</b> times can be similarly ordered or the job sequence is fixed, then O(n 3 log n) and O(n 5) time algorithms are developed to solve cases (a) and (b), respectively. If all <b>job</b> <b>processing</b> times are fixed or all setup times are fixed, then more efficient algorithms can be devised to solve the problems. Department of Logistics and Maritime Studie...|$|R
5000|$|<b>Job</b> <b>Processing</b> Cycle - for {{detailed}} description of batch processing in the mainframe field ...|$|R
40|$|ISCO 2010 - International Symposium on Combinatorial OptimizationInternational audienceThe {{problem of}} {{scheduling}} n jobs on an unbounded batching machine {{to minimize the}} total completion time is studied. The machine can process any number of jobs simultaneously in a batch, subject to an additional constraint that, in the same batch, the <b>job</b> <b>processing</b> times are compatible. There are given normal job pro- cessing times. An actual <b>job</b> <b>processing</b> time can exceed its normal value {{up to a certain}} percent. This percent is the same for all jobs. Thus, there are processing time intervals for the <b>jobs.</b> The <b>job</b> <b>processing</b> times are compatible if the corresponding processing time intervals intersect. The processing time of a batch is given by the longest processing time of the tasks in the batch and it correspnds to the left endpoint of the intersection of the <b>job</b> <b>processing</b> time intervals in this batch. For the total completion time a dynamic programming algorithm is provided...|$|R
5000|$|It {{shifts the}} time of <b>job</b> <b>processing</b> to when the {{computing}} resources are less busy ...|$|R
50|$|It {{can shift}} {{the time of}} <b>job</b> <b>processing</b> to when the {{computing}} resources are less busy.|$|R
50|$|The terms Remote Batch, Remote Job System and Remote <b>Job</b> <b>Processing</b> {{are also}} used for RJE facilities.|$|R
40|$|<b>Jobs</b> with {{different}} <b>processing</b> complexity {{may have different}} effects on human learning, which in turn will affect the <b>job</b> <b>processing</b> times. So, the learning effect on <b>job</b> <b>processing</b> times in scheduling is not only sum-of-processing-time-based and job-position-based, but also depends on the processing complexity of the jobs already processed. We introduce in this paper a general learning effect model for scheduling. We provide some properties that are helpful for finding the optimal solutions for some single-machine and flowshop scheduling problems under the general effect model. Department of Logistics and Maritime Studie...|$|R
40|$|We {{consider}} two single-machine group scheduling {{problems with}} deteriorating group setup and <b>job</b> <b>processing</b> times. That is, the <b>job</b> <b>processing</b> times and group setup times are linearly increasing (or decreasing) functions of their starting times. Jobs {{in each group}} have the same deteriorating rate. The objective of scheduling problems is to minimize the sum of completion times. We show that the sum of completion times minimization problems remains polynomially solvable under the agreeable conditions...|$|R
5000|$|BatchPipes is a batch <b>job</b> <b>processing</b> utility {{designed}} for the MVS/ESA operating system, and all later incarnations - OS/390 and z/OS.|$|R
40|$|<b>Job</b> <b>processing</b> {{and data}} {{transfer}} {{are the main}} computing activities on the WLCG infrastructure. Reliable monitoring of the <b>job</b> <b>processing</b> on the WLCG scope is a complicated task due {{to the complexity of}} the infrastructure itself and the diversity of the currently used job submission methods. The paper will describe current status and the new strategy for the job monitoring on the WLCG scope, covering primary information sources, job status changes publishing, transport mechanism and visualization...|$|R
40|$|In {{this study}} we {{consider}} the single machine scheduling problems with sum-of-logarithmprocessing-times based deterioration, i. e., the actual <b>job</b> <b>processing</b> time {{is a function of}} the sum of the logarithm of the processing times of the jobs already processed. We show that even with the introduction of the sum-of-logarithm-processing-times based deterioration to <b>job</b> <b>processing</b> times, single machine makespan minimization problem remain polynomially solvable. But for the total completion time minimization problem, we show that the optimal schedule is not always V-shaped with respect to <b>job</b> normal <b>processing</b> times. Heuristic algorithms and computational results are presented for the total completion time minimization problem. Department of Industrial and Systems Engineerin...|$|R
50|$|On IBM mainframes, BatchPipes is a batch <b>job</b> <b>processing</b> utility {{which runs}} under the MVS/ESA {{operating}} system and later versions - OS/390 and z/OS.|$|R
40|$|We {{consider}} a single-machine scheduling problem in which <b>job</b> <b>processing</b> times are controllable through {{the allocation of}} a limited resource. The amount of resource consumption of a job {{is assumed to be}} linearly related to the <b>job</b> <b>processing</b> time. The performance criteria are the total resource consumption and the number of tardy jobs. Our objective is to construct the trade-off curve between the total amount of resource consumed and the number of tardy jobs. An NP-hardness proof is presented for the problem of minimizing the total amount of allocated resource subject to a limited number of tardy jobs. A pseudo-polynomial-time dynamic programming algorithm is proposed for constructing the trade-off curve. This dynamic program can be generalized to the case where the <b>job</b> <b>processing</b> time is a decreasing function of the amount of allocated resource. Department of Logistics and Maritime Studie...|$|R
3000|$|... its {{completion}} time. The problem is, given <b>job</b> <b>processing</b> time p and setup time S, {{to find the}} number of batches k, and batch sizes [...]...|$|R
40|$|This paper {{deals with}} a single machine {{scheduling}} problem with start time dependent <b>job</b> <b>processing</b> times. The <b>job</b> <b>processing</b> times are characterized by decreasing linear functions dependent on their start times. The problem {{is to find a}} schedule for which the total weighted completion time is minimized. It is proved that the problem is NP-hard. Some properties of special cases of the general problem are also given. Based on these results, two heuristic algorithms are constructed and their performance is compared. Department of Logistics and Maritime Studie...|$|R
40|$|We {{present a}} unified {{analysis}} for single-machine scheduling problems {{in which the}} actual <b>job</b> <b>processing</b> times are controlled by either a linear or a convex resource allocation function and also vary concurrently depending on either the job's position in the sequence and/or on the total processing time of the already processed jobs. We show {{that the problem is}} solvable in O(nlogn) time by using a weight-matching approach when a convex resource allocation function is in effect. In the case of a linear resource allocation function, we show that the problem can be solved in O(n 3) time by using an assignment formulation. Our approach generalizes the solution approach for the corresponding problems with controllable <b>job</b> <b>processing</b> times to incorporate the variability of the <b>job</b> <b>processing</b> times stemming from either the job's position in the sequence and/or the total processing time of the already processed jobs. Scheduling Single-machine Variable processing times...|$|R
40|$|ABSTRACT: We {{consider}} the two-machine flowshop scheduling problem where <b>jobs</b> have random <b>processing</b> times which are bounded within certain intervals. The {{objective is to}} minimize total completion time of all jobs. The decision of finding a solution for the problem {{has to be made}} based on the lower and upper bounds on <b>job</b> <b>processing</b> times since this is the only information available. The problem is NP-hard since the special case when the lower and upper bounds are equal, i. e., the deterministic case, is known to be NP-hard. Therefore, a reasonable approach is to come up with well performing heuristics. We propose eleven heuristics which utilize the lower and upper bounds on <b>job</b> <b>processing</b> times based on the Shortest Processing Time (SPT) rule. The proposed heuristics are compared through randomly generated data. The computational analysis has shown that the heuristics using the information on the bounds of <b>job</b> <b>processing</b> times on both machines perform much better than those using the information on one of the two machines. It has also shown that one of the proposed heuristics performs as the best for different distributions with an overall average percentage error of less than one...|$|R
40|$|International audienceThe {{problem of}} {{scheduling}} N jobs on M uniform parallel machines is studied. The {{objective is to}} minimize the mean tardiness or the weighted sum of tardiness with weights based on jobs, on periods or both. For the mean tardiness criteria in the preemptive case, this problem is NP-hard but good solutions can be calculated with a transportation problem algorithm. In the nonpreemptive case the problem is therefore NP-hard, except for the cases with equal <b>job</b> <b>processing</b> times or with job due dates equal to <b>job</b> <b>processing</b> times. No dominant heuristic is known in the general nonpreemptive case. The author has developed a heuristic to solve the nonpreemptive scheduling problem with unrelated <b>job</b> <b>processing</b> times. Initially, the algorithm calculates a basic solution. Next, it considers the interchanges of job subsets to equal processing time sum interchanging resources (i. e. a machine for a given period). This paper models the scheduling problem. It presents the heuristic and its result quality, solving 576 problems for 18 problem sizes. An application of school timetable scheduling illustrates {{the use of this}} heuristic...|$|R
40|$|AbstractIn practice, {{processing}} {{times can}} be more accurately represented as intervals with the most probable completion time somewhere near {{the middle of the}} interval. A fuzzy number which is essentially a generalized interval can represent this processing time interval exactly and naturally. In this work, triangular and trapezoidal fuzzy numbers are used to represent those vague <b>job</b> <b>processing</b> times in <b>job</b> shop production systems. The job sequencing algorithms of Johnson and Ignall and Schrage are modified to accept fuzzy <b>job</b> <b>processing</b> times. Fuzzy makespans and fuzzy mean flow times are then calculated for greater decision-making information. Numerous examples are used to illustrate the approach...|$|R
40|$|In {{this paper}} {{the size of}} the set of V-shaped {{schedules}} is investigated when a point is identified to be the change point from nonincreasing to nondecreasing order of <b>job</b> <b>processing</b> times. More specifically the significance of identifying such a point in reducing {{the size of the}} set of V-shaped schedules is investigated. It is found that the resulting subset of the V-shaped schedules is significantly less than the set of V-shaped schedules. The reduction is more significant with the increase in the variation in <b>job</b> <b>processing</b> times. This result encourages researchers to identify the changing point for problems known to have V-shaped optimal schedules...|$|R
40|$|In a {{scheduling}} problem with controllable <b>processing</b> times the <b>job</b> <b>processing</b> {{time can be}} compressed through incurring an additional cost. We consider the identical parallel machines max flow time minimization problem with controllable processing times. We address the preemptive and non-preemptive version of the problem. For the preemptive case, a linear programming formulation is presented which solves the problem optimally in polynomial time. For the non-preemptive problem it is shown that the First In First Out (FIFO) heuristic has a tight worst-case performance of 3 − 2 /m, when <b>jobs</b> <b>processing</b> times and costs are set as in some optimal preemptive schedule. AMS Subject Classification:????,?????,???...|$|R
40|$|We {{consider}} a single-machine scheduling {{model in which}} the <b>job</b> <b>processing</b> times are controllable variables with linear costs. The objective is to minimize {{the sum of the}} cost incurred in compressing <b>job</b> <b>processing</b> times and the cost associated with the number of late jobs. The problem is shown to be NP-hard even when the due dates of all jobs are identical. We present a dynamic programming solution algorithm and a fully polynomial approximation scheme for the problem. Several efficient heuristics are proposed for solving the problem. Computational experiments demonstrate that the heuristics are capable of producing near-optimal solutions quickly. Department of Logistics and Maritime Studie...|$|R
3000|$|The Relief {{results show}} that the {{performance}} measures <b>job</b> <b>processing</b> time and <b>job</b> turnaround, have the highest quality scores (W [...]) and also have the potential to be distinguishing features between the two classes. In this case the performance measure ‘hard disk bytes written’ is also selected by means of the same approach as in the means and variance analysis: in other words, this has in terms of their use to stand out {{from the rest of the}} measures and give more certainty to the analysis of relationships. Thus, the measures <b>job</b> <b>processing</b> time, <b>job</b> turnaround and hard disk bytes written are also selected as candidates to represent the performance of BDA in the Hadoop system.|$|R
40|$|Abstract: We {{consider}} {{the problem of}} scheduling jobs with given release times and due dates on a single machine to minimize the maximal job lateness. This problem is NP-hard, and its version when the <b>job</b> <b>processing</b> times are restricted to p, 2 p, 3 p, 4 p, [...] ., for an integer p, is also NP-hard. We {{consider the}} case when the maximal <b>job</b> <b>processing</b> time is kp, for any constant k, and propose its polynomial-time solution. We easily establish that the version of this problem with unrestricted k is NP-hard. Moreover, it is strongly NP-hard if p has no exponential-time dependence on the maximal job due date. From a practical point of view, this is a realistic assumption...|$|R
40|$|In this paper, we {{consider}} the single-machine scheduling problems {{with the effects of}} learning and deterioration. By the effects of learning and deterioration, we mean that <b>job</b> <b>processing</b> times are defined by functions of their starting times and positions in the sequence. It is shown that even with the introduction of learning effect and deteriorating <b>jobs</b> to <b>job</b> <b>processing</b> times, single-machine makespan and sum of completion times (square) minimization problems remain polynomially solvable, respectively. But for the following objective functions: the weighted sum of completion times and the maximum lateness, this paper proves that the WSPT rule and the EDD rule can construct the optimal sequence under some special cases, respectively. Scheduling Single machine Learning effect Deteriorating jobs...|$|R
50|$|OS/360 {{provided}} limited interactive {{facilities in}} Conversational Remote Job Entry (CRJE), Graphic <b>Job</b> <b>Processing</b> (GJP), Interactive Terminal Facility (ITF) and Satellite Graphic <b>Job</b> <b>Processing</b> (SGJP) {{prior to the}} Time Sharing Option (TSO), but IBM did not carry those forward to SVS. TSO continues to provide equivalent facilities, except {{that it does not}} support use of a 2250 as a terminal. Use of a 2250 from a batch job using Graphics Access Method (GAM) and Graphics Subroutine Package (GSP) remains supported. OS/360 included a batch debugging facility named TESTRAN; it was clumsier than the equivalent facility in IBSYS/IBJOB, and was not used much. With the advent of TSO TESTRAN became even less relevant, and SVS did not include it.|$|R
40|$|Monitoring of the {{large-scale}} data processing of the ATLAS experiment includes monitoring {{of production and}} user analysis jobs. The Experiment Dashboard provides a common job monitoring solution, which is shared by ATLAS and CMS experiments. This includes an accounting portal as well as real-time monitoring. Dashboard job monitoring for ATLAS combines information from PanDA <b>job</b> <b>processing</b> database, Production system database and monitoring information from jobs submitted through GANGA to Workload Management System (WMS) or local batch systems. Usage of Dashboard-based job monitoring applications will decrease load on the PanDA database and overcome scale limitations in PanDA monitoring caused by the short job rotation cycle in the PanDA database. Aggregation of the task/job metrics from different sources provides complete view of <b>job</b> <b>processing</b> activity in ATLAS scope...|$|R
40|$|Abstract. The {{paper is}} a survey devoted to job {{scheduling}} problems with resource allocation. We {{present the results}} available in the scientific literature for commonly used models of <b>job</b> <b>processing</b> times and <b>job</b> release dates, i. e., the models in which the <b>job</b> <b>processing</b> time or the job release date is given as a linear or convex function dependent {{on the amount of}} the additional resource allotted to the job. The scheduling models with resource dependent processing times or resource dependent release dates extend the classical scheduling models to reflect more precisely scheduling problems that appear in real life. Thus, in this paper we present the computational complexity results and solution algorithms that have been developed for this kind of problems...|$|R
40|$|The {{problem of}} {{unconstrained}} minimization of a piecewise linear function of one variable {{is shown to}} be NP-hard given an oracle representation of the function. This result {{can be applied to}} establish the NP-hardness of the scheduling problem with controllable <b>job</b> <b>processing</b> times given an oracle representation of the scheduling cost. The computational complexity of this scheduling problem has remained unknown for more than 20 years. We consider the problem of unconstrained optimization from the perspective of classifying its computational complexity, and show that it is NP-hard. This result enables us to establish the NP-hardness of the scheduling problem with controllable <b>job</b> <b>processing</b> times, whose computational complexity status has remained unknown for a long time. Department of Logistics and Maritime Studie...|$|R
40|$|ABSTRACT: The two-machine flowshop {{scheduling}} problem of minimizing makespan is addressed where <b>jobs</b> have random <b>processing</b> times which are bounded within certain intervals. The probability distributions of <b>job</b> <b>processing</b> times within intervals are not known. The only known information about <b>job</b> <b>processing</b> times is the lower and upper bounds. The decision about {{a solution of}} the problem, i. e., finding a sequence, {{has to be made}} based on these bounds. Different heuristics using the bounds are proposed, and the proposed heuristics are compared based on randomly generated data. The computational analysis has shown that three of the proposed heuristics perform well with an overall average error of less than one percent. Moreover, it has also shown that one of the heuristics, which applies Johnson's algorithm to the average of the lower and upper bounds, perform as the best with an overall average percentage error of 0. 71. The obtained results are also shown to be consistent with the recent results obtained in the literature...|$|R
50|$|JobServer {{supports}} some connectivity with Hadoop {{and can be}} used as a way {{of launching}} and monitoring Hadoop <b>job</b> <b>processing</b> activity. JobServer also includes support for the open source community distribution of Mule which can allow jobs and tasks to work with ESB and SOA platforms such as Mule.|$|R
40|$|Recently, {{learning}} scheduling {{problems have}} received increasing attention. However, {{the majority of}} the research assume that the actual <b>job</b> <b>processing</b> time is a function of its position. This paper deals with the single-machine scheduling problem with a sum-of-processing-time-based learning effect. By the effect of sum-of-processing-time-based learning, we mean that the processing time of a job is defined by total normal <b>processing</b> time of <b>jobs</b> in front of it in the sequence. We show that the single-machine makespan problem remains polynomially solvable under the proposed model. We show that the total completion time minimization problem for a≥ 1 remains polynomially solvable under the proposed model. For the case of 0 <a< 1, we show that an optimal schedule of the total completion time minimization problem is V-shaped with respect to normal <b>job</b> <b>processing</b> times...|$|R
40|$|Recently, {{the topic}} of {{scheduling}} with learning effects has kept growing attention, and a survey is further provided to classify the learning models in scheduling into two types, namely position-based learning and sum-of- processingtimes-based learning. However, the actual processing time of a given job fast drops to zero {{as the number of}} jobs increases in the first model and when the normal <b>job</b> <b>processing</b> times are large in the second model. Motivated by this observation, this paper extends both models to a more general learning model where the actual <b>job</b> <b>processing</b> time {{is a function of the}} sum of the logarithm of the processing times of the jobs already processed and general position-based learning. Under the proposed learning model, this paper shows that some single-machine scheduling problems can be solved in polynomial time...|$|R
40|$|This paper {{describes}} an advanced Grid architecture efficient for homology searching in bioinformatics. An integrated {{architecture of the}} computing-grid and datagrid through P 2 P communication attains efficient <b>job</b> <b>processing.</b> Field trials, in which broadband users and bioinformatics researchers participated to find new DNAs, show its effectiveness in reducing processing time...|$|R
