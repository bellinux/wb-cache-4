2906|6599|Public
25|$|Another {{class of}} methods for <b>sampling</b> <b>points</b> in a volume is to {{simulate}} random walks over it (Markov chain Monte Carlo). Such methods include the Metropolis-Hastings algorithm, Gibbs sampling, Wang and Landau algorithm, and interacting type MCMC methodologies {{such as the}} sequential Monte Carlo samplers.|$|E
25|$|All {{international}} going ships {{under the}} Convention must implement a ‘Ballast water management plan’ {{that enables the}} ship to manage their ballast water and sediment discharge to a certain standard. The plan is {{designed to meet the}} requirements for compliance with the Convention and the G4 Guidelines produced by the IMO. It includes standard operational guidance, planning and management, as well as additional details including <b>sampling</b> <b>points</b> and systems. Additionally all ships over 400GT must also carry a ballast water record book that details such requirements as the filling and discharge of each tank, according to time, date, location and the treatment applied to the water.|$|E
25|$|According to Klaas Schwartz, DWD is {{expected}} to monitor the quality of drinking water provided by NWSC. In practice, however, NWSC monitors its drinking water quality internally without any complementary external monitoring. NWSC's internal Quality Control Department examines whether the supplied water complies with {{the national standards for}} drinking water, which in turn follow the World Health Organization guidelines. There are a central laboratory in Kampala and satellite laboratories in the other NWSC operation areas. At several <b>sampling</b> <b>points,</b> water is controlled for pH, color, turbidity, residue chlorine, and E. coli. The results are available at the official NWSC website and mostly comply with the national standards. Where NWSC does not provide the service, districts are responsible for water quality monitoring. According to the MWE, this is done insufficiently, and data are scarce.|$|E
50|$|In random <b>sampling</b> new <b>sample</b> <b>points</b> are {{generated}} without {{taking into account}} the previously generated <b>sample</b> <b>points.</b> One does not necessarily need to know beforehand how many <b>sample</b> <b>points</b> are needed.|$|R
50|$|In Latin Hypercube {{sampling}} {{one must}} first decide how many <b>sample</b> <b>points</b> {{to use and}} for each <b>sample</b> <b>point</b> remember in which row and column the <b>sample</b> <b>point</b> was taken. Note that such configuration is similar to having N rooks on a chess board without threatening each other.|$|R
30|$|In addition, {{in order}} {{to make up for the}} {{instability}} of the feature points without affine invariance, the gradient amplitude of each <b>sampling</b> <b>point</b> should be weighted for each gradient histogram, and the weight of each <b>sampling</b> <b>point</b> is finally determined by the gradient modulus value of the <b>sampling</b> <b>point</b> and the Gaussian weight.|$|R
500|$|Recent {{studies of}} the 18O and 13C {{isotopes}} found in stalagmites in Belize show that tropical cyclone events can leave markers that can be separated out on a week-by-week basis. [...] The error rate {{of this type of}} microanalysis was 1 error in 1,200 <b>sampling</b> <b>points.</b>|$|E
2500|$|On {{the left}} are values of [...] at the <b>sampling</b> <b>points.</b> The {{integral}} {{on the right}} will be recognized as essentially So Shannon's coefficients are [...] which agrees with [...] the nth coefficient in a Fourier-series expansion of the function [...] taking the interval –B to B as a fundamental period. This means that {{the values of the}} samples [...] determine the Fourier coefficients in the series expansion of [...] Thus they determine [...] since [...] is zero for frequencies greater than B, and for lower frequencies [...] is determined if its Fourier coefficients are determined. But [...] determines the original function [...] completely, since a function is determined if its spectrum is known. Therefore the original samples determine the function [...] completely.|$|E
5000|$|... and {{the matrix}} [...] is also depends {{only on the}} choice of those <b>sampling</b> <b>points.</b> However, even if those <b>sampling</b> <b>points</b> are distinct, [...] could still be singular. No rules for {{determining}} whether the matrix is nonsingular or not have been found. Therefore, for all implementation of 2-D NDFT, we just check [...] for {{a specific set of}} <b>sampling</b> <b>points.</b> In general, the complexity of 2-D NDFT [...]|$|E
3000|$|The {{average number}} of <b>sample</b> <b>points</b> in the safe region when the process is in out-of-control state and current <b>sample</b> <b>point</b> belongs to safe region [...]...|$|R
3000|$|... of the tensor field Q in the {{direction}} v around a the <b>sampling</b> <b>point</b> P - the resulting surface is intersecting the <b>sampling</b> <b>point</b> P whenever [...]...|$|R
40|$|In {{supervised}} learning, {{the selection}} of <b>sample</b> <b>points</b> and models is crucial for acquiring {{a higher level of}} generalization capability. So far, the problems of active learning and model selection have been independently studied. If <b>sample</b> <b>points</b> and models are simultaneously optimized, then a higher level of generalization capability is expected. We call this problem active learning with model selection. However, this problem can not be generally solved by simply combining existing active learning and model selection techniques because of the active learning / model selection dilemma: the model should be fixed for selecting <b>sample</b> <b>points</b> and conversely the <b>sample</b> <b>points</b> should be fixed for selecting models. In spite of the dilemma, the problem of active learning with model selection can be straightforwardly solved if there is a set of <b>sample</b> <b>points</b> being optimal for all models in consideration. In this paper, we show that such an optimal set of <b>sample</b> <b>points</b> actually exists, and give [...] ...|$|R
5000|$|... #Caption: FIR filter in {{frequency}} domain with d=2; n1=n2=5 and has 61 <b>sampling</b> <b>points</b> ...|$|E
50|$|It is {{noticeable}} {{that the}} number of <b>sampling</b> <b>points</b> decreases after we apply the time-frequency distribution.|$|E
5000|$|Sampling. Along {{the part}} of the ray of sight that lies within the volume, {{equidistant}} <b>sampling</b> <b>points</b> or samples are selected. In general, the volume is not aligned with the ray of sight, and <b>sampling</b> <b>points</b> will usually be located in between voxels. Because of that, it is necessary to interpolate the values of the samples from its surrounding voxels (commonly using trilinear interpolation).|$|E
30|$|Here, k is {{the active}} {{biological}} response scale of <b>sample</b> <b>points,</b> u is embedded devices activated <b>sample</b> <b>points,</b> and v is the biological response data buffer factor for embedded devices.|$|R
3000|$|Equation (17) is the 2 -rank {{expansion}} model, and {{the roots}} (0, √(3), −√(3) of the 3 -rank Hermite polynomials can be selected {{with a total}} of nine <b>sample</b> <b>points.</b> If the number of random variables is more than 3, the number of <b>sample</b> <b>points</b> is two times larger than that of the unknown factor, and thus large amount of calculation is required. However, the selected <b>sample</b> <b>points</b> are in the standard normal distribution space, and therefore, it is necessary to convert them to the original space. The transformation of the original space <b>sample</b> <b>points</b> corresponds to the real response value, and the unknown coefficients a [...]...|$|R
40|$|Recently, a precomputed shadow fields {{method was}} {{proposed}} to achieve fast rendering of dynamic scenes under environment illumination and local light sources. This method can render shadows fast by precomputing the occlusion information at many <b>sample</b> <b>points</b> arranged on concentric shells around each object and combining multiple precomputed occlusion information {{rapidly in the}} rendering step. However, this method uses {{the same number of}} <b>sample</b> <b>points</b> on all shells, and cannot achieve real-time rendering due to the rendering computation rely on CPU rather than graphics hardware. In this paper, we propose an algorithm for decreasing the data size of shadow fields by reducing the amount of <b>sample</b> <b>points</b> without degrading the image quality. We reduce the number of <b>sample</b> <b>points</b> adaptively by considering the differences of the occlusion information between adjacent <b>sample</b> <b>points.</b> Additionally, we also achieve fast rendering under low-frequency illuminations by implementing shadow fields on graphics hardware. ...|$|R
50|$|Note that NDFT reduces to DFT {{when the}} <b>sampling</b> <b>points</b> {{are located on}} the unit circle at equally spaced angles.|$|E
50|$|The green {{cells are}} <b>sampling</b> <b>points.</b> All except the first are {{synchronized}} by the 1->0 {{edge in the}} transmission fragment shown.|$|E
5000|$|... where [...] are uniformly {{obtained}} {{from all the}} phase space (PS) and N {{is the number of}} <b>sampling</b> <b>points</b> (or function evaluations).|$|E
3000|$|Based on {{calculations}} {{under the}} same number of iterations (i) and amount of original <b>sample</b> <b>points</b> of a coastline, the total number of <b>sample</b> <b>points</b> used by a random Koch curve algorithm in the simulation process is 2 [...]...|$|R
40|$|Abstract—In {{inspection}} and workpiece localization, <b>sampling</b> <b>point</b> data {{is an important}} issue. Since the devices for sampling only <b>sample</b> discrete <b>points,</b> not the completely surface, sampling size and location of the points will be taken into consideration. In this paper a method is presented for determining the <b>sampled</b> <b>points</b> size and location for achieving efficient sampling. Firstly, uncertainty analysis of the localization parameters is investigated. A localization uncertainty model is developed to predict {{the uncertainty of the}} localization process. Using this model the minimum size of the <b>sampled</b> <b>points</b> is predicted. Secondly, based on the algebra theory an eigenvalue-optimal optimization is proposed. Then a freeform surface is used in the simulation. The proposed optimization is implemented. The simulation result shows its effectivity. Keywords—eigenvalue-optimal optimization, freeform surface inspection, sampling size and location, <b>sampled</b> <b>points.</b> I...|$|R
30|$|The {{location}} of <b>sample</b> <b>points</b> {{was found with}} the help of co-ordinates of points. The co-ordinates of the <b>sample</b> <b>points</b> were taken {{with the help of}} Global Positioning System receiver (GPS Receiver). For this purpose, Explorist 210 GPS receiver was used.|$|R
50|$|In the {{mathematical}} topic of wavelet theory, the cascade algorithm is a numerical method for calculating function {{values of the}} basic scaling and wavelet functions of a discrete wavelet transform using an iterative algorithm. It starts from values on a coarse sequence of <b>sampling</b> <b>points</b> and produces values for successively more densely spaced sequences of <b>sampling</b> <b>points.</b> Because it applies the same operation {{over and over to}} the output of the previous application, it is known as the cascade algorithm.|$|E
5000|$|... "A {{generalized}} sampling theorem {{with the}} inverse of an arbitrary square summable sequence as <b>sampling</b> <b>points,</b> The Journal of Fourier Analysis and Applications, Vol. 2 (1996), pp. 303-314.|$|E
5000|$|Similarly, the {{configuration}} of uniformly spaced <b>sampling</b> <b>points</b> in one-dimension can be generalized to a lattice in higher dimensions. A lattice {{is a collection of}} points [...] of the form ...|$|E
5000|$|The simplicial {{depth of}} a point [...] in -dimensional Euclidean space, {{with respect to a}} set of <b>sample</b> <b>points</b> in that space, is the number of -dimensional {{simplices}} (the convex hulls of sets of [...] <b>sample</b> <b>points)</b> that contain [...]The same notion can be generalized to any probability distribution on points of the plane, not just the empirical distribution given by a set of <b>sample</b> <b>points,</b> by defining the depth to be the probability that a randomly chosen -tuple of points has a convex hull that contains [...] This probability can be calculated, from the number of simplices that contain , by dividing by [...] where [...] is the number of <b>sample</b> <b>points.</b>|$|R
40|$|With China’s rapid {{economic}} development, {{the reduction in}} arable land has {{emerged as one of}} the most prominent problems in the nation. The long-term dynamic monitoring of arable land quality is important for protecting arable land resources. An efficient practice is to select optimal <b>sample</b> <b>points</b> while obtaining accurate predictions. To this end, the selection of effective points from a dense set of soil <b>sample</b> <b>points</b> is an urgent problem. In this study, data were collected from Donghai County, Jiangsu Province, China. The number and layout of soil <b>sample</b> <b>points</b> are optimized by considering the spatial variations in soil properties and by using an improved simulated annealing (SA) algorithm. The conclusions are as follows: (1) Optimization results in the retention of more <b>sample</b> <b>points</b> in the moderate- and high-variation partitions of the study area; (2) The number of optimal <b>sample</b> <b>points</b> obtained with the improved SA algorithm is markedly reduced, while the accuracy of the predicted soil properties is improved by approximately 5 % compared with the raw data; (3) With regard to the monitoring of arable land quality, a dense distribution of <b>sample</b> <b>points</b> is needed to monitor the granularity...|$|R
40|$|A Monte Carlo {{study was}} {{conducted}} to assess the errors present in digital elevation model (DEM) estimation when the number of <b>sample</b> <b>points</b> increases. The influence of spatial location and the correlation of these <b>sample</b> <b>points</b> on the DEM errors was also investigated. The errors were quantified by computing the root mean squared error and mean absolute deviation measures, both calculated global and locally. The results showed that for an area of 100 m x 100 m with DEM resolution of 1. 25 m the increasing of <b>sample</b> <b>points</b> tend to decrease the DEM errors. The effect of increasing the <b>sample</b> <b>points</b> on the errors seems to be higher over surfaces with correlated points. The spatial locations of the <b>sample</b> <b>points</b> are highly related to the triangles formed on the triangular irregular net grid and consequently have great influence on the DEM errors, thus grids having more acute triangles tend to present greater errors than those having more equilateral triangles. Pages: 3579 - 358...|$|R
50|$|The Puniu is 6th dirtiest out of 13 <b>sampling</b> <b>points</b> in the Waipa catchment, with {{unsatisfactory}} nitrogen, phosphorus and turbidity levels. Puniu is {{the only}} river in Waikato with worsening ammonia levels.|$|E
50|$|The {{stratified}} sampling algorithm concentrates the <b>sampling</b> <b>points</b> in {{the regions}} where the variance of the function is largest thus reducing the grand variance and making the sampling more effective, as shown on the illustration.|$|E
50|$|This {{includes}} {{for example}} the planning of the sampling method, marking and recording of <b>sampling</b> <b>points,</b> excavation of sampling pits {{as well as the}} taking of samples and analysing them in a specialised (MCERTS / UKAS (UK only) certified laboratory.|$|E
40|$|Abstract Recently, a precomputed shadow fields {{method was}} {{proposed}} to achieve fast rendering of dynamic scenes under environment illumination and local light sources. This method can render shadows fast by precomputing the occlusion information at many <b>sample</b> <b>points</b> arranged on concentric shells around each object and combining multiple precomputed occlusion information {{rapidly in the}} rendering step. However, this method uses {{the same number of}} <b>sample</b> <b>points</b> on all shells, and cannot achieve real-time rendering due to the rendering computation rely on CPU rather than graphics hardware. In this paper, we propose an algorithm for decreasing the data size of shadow fields by reducing the amount of <b>sample</b> <b>points</b> without degrading the image quality. We reduce the number of <b>sample</b> <b>points</b> adaptively by considering the differences of the occlusion information between adjacent <b>sample</b> <b>points.</b> Additionally, we also achieve fast rendering under low-frequency illuminations by implementing shadow fields on graphics hardware. Keywords photo-realistic rendering · real-time rendering · precomputed shadow fields...|$|R
40|$|Abstract: A non-local {{denoising}} (NLD) algorithm for point-sampled surfaces (PSSs) {{is presented}} based on similarities, including geometry intensity and features of <b>sample</b> <b>points.</b> By using the trilateral filtering operator, the differential signal of each <b>sample</b> <b>point</b> is determined and called “geometry intensity”. Based on covariance analysis, a regular grid of geometry intensity of a <b>sample</b> <b>point</b> is constructed, and the geometry-intensity similarity of two points is measured {{according to their}} grids. Based on mean shift clustering, the PSSs are clustered {{in terms of the}} local geometry-features similarity. The smoothed geometry intensity, i. e., offset distance, of the <b>sample</b> <b>point</b> is estimated according to the two similarities. Using the resulting intensity, the noise component from PSSs is finally removed by adjusting the position of each <b>sample</b> <b>point</b> along its own normal direction. Ex-perimental results demonstrate that the algorithm is robust and can produce a more accurate denoising result while having better feature preservation...|$|R
50|$|In Orthogonal sampling, {{the sample}} space {{is divided into}} equally {{probable}} subspaces. All <b>sample</b> <b>points</b> are then chosen simultaneously {{making sure that the}} total ensemble of <b>sample</b> <b>points</b> is a Latin Hypercube sample and that each subspace is sampled with the same density.|$|R
