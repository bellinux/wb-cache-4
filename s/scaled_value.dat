18|3196|Public
50|$|We {{discovered}} that the precision of these fractions {{is going to be}} a problem. The following box illustrates this showing the original data, its scaled decimal values, and the binary equivalent of the <b>scaled</b> <b>value.</b>|$|E
5000|$|The Fangraphs {{formula for}} {{position}} players involves offense, defense, and base running. These are measured using weighted Runs Above Average, Ultimate zone rating (UZR), and Ultimate base running (UBR), respectively. These values are adjusted using park factors, and a positional adjustment is applied, {{resulting in a}} player's [...] "value added above league average". To this is added a <b>scaled</b> <b>value</b> to reflect the player's value compared to a replacement-level player, which {{is assumed to be}} 20 runs below average per 600 plate appearances. All four values are measured in runs.|$|E
50|$|Other AAT tables (or AAT-specific {{extensions}} {{to standard}} TrueType tables) allow for context-sensitive kerning, justification, and ligature splitting. AAT also supports variation fonts, {{in which a}} font's shape can vary depending on a <b>scaled</b> <b>value</b> supplied by the user. Variation fonts are similar to Adobe's defunct Multiple master fonts, where the endpoints are defined and any medial value is valid. With this, the user can then drag sliders in the user interface to make glyphs taller or shorter, to make them fatter or thinner, to increase or decrease {{the size of the}} serifs, and the like, all independently of one another. Glyphs may even have their fundamental shapes radically altered. Before OpenType introduced Font Variation in September 2016, there is nothing like this in OpenType.|$|E
5000|$|... "For {{an object}} with {{multiple}} potential collisions {{on a set}} of dates, a Torino <b>Scale</b> <b>value</b> should be determined for each date. It may be convenient to summarize such an object by the greatest Torino <b>Scale</b> <b>value</b> within the set." ...|$|R
40|$|The {{aim of this}} {{research}} is to investigate the effect of vertical <b>scale</b> <b>value</b> of fractal interpolation to the quality of magnified image resulted. In {{this research}}, three groups of rgb image are used, they are of bird, tree, and batik images. The results of the research show that vertical <b>scale</b> <b>value</b> of zero gives the best quality magnified image. The farther the vertical <b>scale</b> <b>value</b> from zero, the worse the quality of the magnified imag...|$|R
40|$|A {{class of}} unidimensional choice models is described. Thurstone’s paired {{comparisons}} model, Case 5, and the Bradley-Terry-Luce model both {{fall into the}} class. A simple nonmetric method is presented for estimating <b>scale</b> <b>values</b> from choice data which satisfy any model in the class. In two examples, nonmetric <b>scale</b> <b>values</b> are compared to Thurstone estimates. The scaling method is extended to permit estimation of <b>scale</b> <b>values</b> {{in a class of}} unidimensional ordered-category models, a class which includes the law of categorical judgment...|$|R
3000|$|Assume that a cyber {{system has}} l {{identified}} vulnerabilities: r 1, r 2 ···r [...] l [...]. DERFEM is employed to assign each vulnerability a <b>scaled</b> <b>value</b> within [0, 1] which quantitatively characterizes the vulnerable level. The larger the <b>scaled</b> <b>value</b> is, {{the higher the}} vulnerable level will be.|$|E
3000|$|... [...]. Moreover, Equation 17 has {{the feature}} that the <b>scaled</b> <b>value</b> is always bounded between 0 and 1. The {{normalized}} distance matrix D is denoted by [...]...|$|E
40|$|In this paper, we {{analyze the}} error {{recovery}} performance of variable length codes (VLCs) transmitted over binary symmetric channel (BSC). Simple expressions for the exact mean symbol error rate (MSER) and the exact variance of symbol error rate (VSER) for any crossover probability p, are presented. We also {{prove that the}} mean error propagation length (MEPL) derived for single bit inversion error case is a <b>scaled</b> <b>value</b> of MSER when p, tends to zero. Comparisons with simulations demonstrate {{the accuracy of the}} MSER and VSER expressions...|$|E
50|$|Rasch model scaling - {{respondents}} {{interact with}} items and comparisons are inferred between {{items from the}} responses to obtain <b>scale</b> <b>values.</b> Respondents are subsequently also scaled based on their responses to items given the item <b>scale</b> <b>values.</b> The Rasch model has a close relation to the BTL model.|$|R
40|$|Using several {{lines of}} {{evidence}} {{we show that}} the <b>scale</b> <b>values</b> of the geomagnetic variometers operating in Helsinki in the 19 th century were not constant throughout the years of operation 1844 – 1897. Specifically, the adopted <b>scale</b> <b>value</b> of the horizontal force variometer appears to be too low by 30...|$|R
30|$|Combination of {{attribute}} <b>scale</b> <b>values</b> {{to predict}} overall image quality.|$|R
40|$|The error {{recovery}} capability of variable length code (VLC) {{has been considered}} as an important performance and design criterion {{in addition to its}} coding efficiency. However, almost all of the existing methods for evaluating the {{error recovery}} capability of VLC assume that the transmission fault is a random single bit inversion. In this paper, we consider a more generalized problem of precisely evaluating the error recovery capability of VLC in the case that the encoded bit stream is transmitted over a BSC with arbitrary crossover probability. By making use of the Perron-Frobenius Theorem, we derive a very simple expression for the exact mean error propagation rate (MEPR), and show that the variance of error propagation rate (VEPR) is zero. We also prove that in the regime of very low crossover probability, the mean error propagation length (MEPL) derived for single inversion error case approaches a <b>scaled</b> <b>value</b> of the MEPR. Furthermore, we briefly discuss the problem of evaluating the error detection capability of non-exhaustive code over BSC...|$|E
40|$|Abstract- For Bamboo {{to become}} a {{mainstream}} material of the building industry, as a material of substance; an up <b>scaled</b> <b>value</b> of its utility and its deliverance is where research energies have to be spent. Resolution of primary technical issues make it even more compelling to focus on why bamboo construction is not popular despite possessing qualities needed for structural members in tension. Prima Facie; more than the structural vulnerability, appearance, fire and insect attack proneness affecting durability; it is {{the resolution of the}} issue of finance, the legal status and the detailed implementation strategy, that appear to be of a greater urgency. To hand over a dwelling unit to its owner, an extra cost amounting to 28 % is incurred in urban areas. The conventional system of a “financed house ” becomes financially burdensome and cannot be a solution for the poor. An appropriate Deliverance system for mass housing for the Urban Poor in India, using Bamboo as an Alternative Building Material is therefore an urgent requirement...|$|E
30|$|The {{system was}} {{prepared}} {{based on the}} settings of neural network classifier stated in the last section. The status of the eye (normal or high IOP) came from the activation function of the neural network implementation. The implementation dictates the type of normalization functions {{that can be used}} to bring the activation values in the range between 0 and 1. These computations are done in a fashion that sums up all the percentages to 1. For example, higher values of the pupil/iris ratio could relate to having a higher value in the range close to 1. It is important to note that, however, the system does not count on one feature to make the final decision and rather depends on five features altogether along with a neural network machine learning model to provide the final decision. The value 0.5 from the output range is used as a cutoff to differentiate between normal and high IOP. As an example, when the pupil/iris ratio was equal to 0.7, the resulted <b>scaled</b> <b>value</b> was high and close to 1. This indicates that if the other features of the same eye image also result in a high value from the range [0 – 1], the eye status is likely to be classified as high IOP.|$|E
50|$|Although it {{will not}} strike for at least 800 years and thus has no Torino scale rating, (29075) 1950 DA {{was added to the}} list in 2002 {{because it was the first}} object with a Palermo <b>scale</b> <b>value</b> greater than zero. After further measurements, its impact risk was downgraded, but as of December 2016, its Palermo <b>scale</b> <b>value</b> of -1.42 is still the highest for all objects on the list. As of December 2016, only two more objects, both with {{potential}} strikes in the 22nd century only (101955 Bennu and (410777) 2009 FD), have cumulative Palermo <b>scale</b> <b>values</b> above -2.|$|R
40|$|Multidimensional Scaling (MDS) {{has been}} used as a growth mixture {{modeling}} technique in psychological and education research in recent years. This note focuses on a detailed explanation of interpreting the <b>scale</b> <b>values</b> in MDS growth analysis. Since <b>scale</b> <b>values</b> from MDS growth analysis are based on the Euclidean metric, we attempt to offer some guidance on interpretation of the <b>scale</b> <b>values</b> in terms of percentage of change in growth between each time interval. This approach is illustrated with a hypothetical example, and it can be used in actual research settings.   DOI: 10. 2458 /azu_jmmss. v 2 i 2. 15988 </p...|$|R
30|$|Determination of {{relationships}} between attribute <b>scale</b> <b>values</b> and objective, image based measures.|$|R
30|$|Assuming that {{harvested}} biomass would {{substitute for}} fossil-intensive products increased {{the value of}} the carbon mitigation metric in stands WS 4 and WS 5. This increase was small in the case of ethanol: only 0.38 units of fossil carbon are avoided for every 1 unit of carbon in ethanol substituting for petroleum fuels (Lippke et al. 2012). In this case, the carbon benefits of product substitution were insufficient to compensate for the loss of carbon during harvest and WS 4 and WS 5 still incurred a “carbon debt”. The magnitude of this debt was less than in the case where no substitution was assumed, however. Comparatively, the use of LLWP in place of fossil-intensive substitutes (steel, concrete, etc.) results in much larger quantities of avoided carbon: 2.1 units for every 1 unit in the biomass itself (Lippke et al. 2012). Because the carbon displacement ratio in this case is greater than 1 : 1 (i.e. the loss of carbon due to harvesting biomass is less than the benefits gained by producing and using LLWP), the carbon mitigation metric actually increased after harvest. In WS 5, the <b>scaled</b> <b>value</b> for this metric increased from 0.31 to 0.96 in the harvest year. In WS 4, where less than half as much biomass was removed, the value increased from 0.21 in the year before harvest to 0.43 after the third harvest was completed. In both stands, aboveground carbon continued to increase after harvest as biomass regenerated and stand carbon recovered.|$|E
40|$|Among {{the members}} of the series R 2 PdSi 3 (R= Rare-earths), the {{magnetic}} behavior of the Nd compound is interesting in some respects. This compound is considered to order ferromagnetically (below 16 K), unlike other members of this series which order antiferromagnetically. In addition, magnetic ordering temperature (To) is significantly enhanced with respect to de Gennes <b>scaled</b> <b>value.</b> In order to understand the magnetism of this compound better, we have investigated the magnetic behavior in detail (under external pressure as well) and also of its solid solutions based on substitutions at Nd and at Si sites, viz., on the series, Nd(2 -x) (Y,La) (x) PdSi(3 -y) Ge(y) (x, y= 0. 2, 0. 4, 0. 8, and 1. 2) by bulk measurements. The results overall establish that Nd 2 PdSi 3 orders ferromagnetically below 16 K, but antiferromagnetic component seems to set in at very low temperatures. Notably, there is a significant suppression of To for Y and Ge substitutions, compared to La substitution, for a given magnitude of unit-cell volume change, however qualitatively correlating with the separation between the layers of Nd and Pd-Si(Ge). On the basis of this observation, we conclude that 4 f(Nd) hybridization plays a major role on the magnetism of the former solid solutions. To our knowledge, this work serves as a rare demonstration of 4 f-hybridization effects on the magnetism of a Nd-based intermetallic compound. Comment: PRB November 2011 (accepted...|$|E
40|$|The {{issues of}} {{electronic}} polarizability in molecular dynamics simulations are discussed. We {{argue that the}} charges of ionized groups in proteins, and charges of ions in conventional non-polarizable force fields such as CHARMM, AMBER, GROMOS, etc should be scaled by a factor about 0. 7. Our model explains why a neglect of electronic solvation energy, which typically amounts to about a half of total solvation energy, in non-polarizable simulations with un-scaled charges can produce a correct result; however, the correct solvation energy of ions does not guarantee the correctness of ion-ion pair interactions in many non-polarizable simulations. The inclusion of electronic screening for charged moieties is shown to result in significant changes in protein dynamics and can give rise to new qualitative results compared with the traditional non-polarizable force field simulations. The model also explains the striking difference between the value of water dipole μ 3 D reported in recent ab initio and experimental studies with the value μ_eff 2. 3 D typically used in the empirical potentials, such as TIP 3 P or SPC/E. It is shown that the effective dipole of water {{can be understood as}} a <b>scaled</b> <b>value</b> μ_eff=μ/√(ϵ_el), where ϵ_el= 1. 78 is the electronic (high-frequency) dielectric constant of water. This simple theoretical framework provides important insights into the nature of the effective parameters, which is crucial when the computational models of liquid water are used for simulations in different environments, such as proteins, or for interaction with solutes. Comment: 53 pages, 6 figures, 3 table...|$|E
5000|$|For {{interval}} data , where v and v' [...] are interval <b>scale</b> <b>values.</b>|$|R
50|$|The window scale {{option is}} used {{only during the}} TCP 3-way handshake. The window <b>scale</b> <b>value</b> {{represents}} the number of bits to left-shift the 16-bit window size field. The window <b>scale</b> <b>value</b> {{can be set from}} 0 (no shift) to 14 for each direction independently. Both sides must send the option in their SYN segments to enable window scaling in either direction.|$|R
5000|$|A {{follow-up}} of each patient {{with regard to}} the goals and <b>scale</b> <b>values</b> chosen at intake ...|$|R
40|$|This {{research}} {{has been carried out}} in the Rift Valley Lakes Basin (RVLB), {{which is one of the}} twelve major river basins in Ethiopia. The RVLB has been considered in this research due to its high priority that comes from the significant ecological and environmental interest from different sectors. The {{research has}} tried to compare the relative environmental impact of Bedene Alemtena, Eballa, Argeda and Gedemso irrigation projects. Impact assessment at the community level has been collected on a base of key informant interviews and ad- hock technique. For the study, a summary of two sets of structured questioners are also used. Check lists, matrices, and rule based analysis are used to aggregate a <b>scaled</b> <b>value</b> of the individual parameters collected through the interviews and physical observations at the four sites. Deforestation, overgrazing, poor watershed management, soil salinity, soil acidity, communicable and non communicable diseases, and water logging are the major problems of all schemes. Specifically, about 34 % of respondents have encountered soil fertility deterioration in Argeda, Gedemso and Bedene Alemtena irrigation projects with high significant variation (X 2, 97. 7). Land degradation scenario after the implementation of the projects is also reported in Argeda (19. 9 %), Gedemso (10. 4 %), Ebala (23. 8 %), Bedene Alemtena (33. 3 %) (X 2, 86. 3). About 76. 2 % of farmers in Argeda irrigation project have  perceived that soil erosion in their plot is significantly more severe than other schemes (X 2, 198. 3). The comparison based on aggregated values shows that the Argeda 01 and Gedemso 01 irrigation projects have environmentally performed better than the Eballa and Bedene Alemtena irrigation projects. DOI: [URL] </p...|$|E
40|$|A set of {{equations}} governing oxygen diffusion {{and consumption}} in soils {{has been developed}} to include microbial and plant-root sinks. The dependent variable is the transformed oxygen concentration, which {{is the difference between}} the gaseous concentration and a <b>scaled</b> <b>value</b> of the aqueous oxygen concentration at the root-soil interface. The results show how, as the air-filled porosity decreases, the reduced oxygen flux causes the depth of extinction to decrease. The results also show how the depth of extinction at a particular value of soil water content decreases with increasing temperature, due to increased microbial respiration. The critical value of water content at which the oxygen concentration goes to extinction at a finite depth was compared with alternative calculations with only a microbial sink. By ignoring the feedback of oxygen concentration on root uptake, the alternative calculations yielded substantially higher critical values of water content at all temperatures. Two soil oxygen diffusion coefficient functions from the literature were compared and shown to give significantly different critical values of water content for fine-textured soils, one more realistic than the other. A single relationship between the extinction depth and the ratio of the water content to the critical value was shown to apply for all temperatures and soil textures. The oxygen profiles were used along with a function relating redox potential to oxygen concentration to generate redox potential profiles. This application of the model could be useful in explaining soil biochemical processes in soils. For one such process, denitrification, the depth at which a critical oxygen concentration is reached was calculated {{as a function of the}} air-filled porosity and temperature of the soil. The implications of the critical value of soil water content in terms of water-filled pore space and matric potential are discussed in relation to the diffusion coefficient functions and recent literature. Griffith Sciences, Griffith School of EnvironmentNo Full Tex...|$|E
30|$|Biometric systems {{require a}} {{transformation}} of biometric template to ensure privacy, security, and revocability of biometric data. The technique which can meet this requirement is called cancelable or revocable biometric. This privacy enhancement problem is identified, and conceptual frameworks of biometric templates are presented in [21, 22]. Ratha et al. [23] formally defined the problem of cancelable biometric. Uludag et al. [6] provides a comprehensive review on privacy and revocability of biometrics with some corrective measures. Recently, Ratha et al. [19] proposed three practical solutions to cancelable biometrics and generate cancelable fingerprint templates. These three template transformation approaches are Cartesian, polar, and functional transformations on feature domain. In Cartesian transformation approach, the minutiae space is divided into rectangular cells which are numbered with sequence. A user-specific transformation key (i.e., matrix) is used to shift the cells to a new location, and the minutiae points are relocated to the new cells. In polar transformation, coordinate space is divided into polar sectors that are numbered in sequence. The sector position is changed {{with the help of}} translation key, and it changes the minutiae location also. In functional transformation, Ratha et al. [19] model the translation using a vector-valued function F(x,y) which is an electric potential field parameterized by a random distribution of charges. The phase angle of the resulting vector decides the direction of translation and the magnitude |F| of this vector function parameterizes the extent of movement. In an alternate formulation, Ratha et al. [19] use the gradient of a mixture of Gaussian kernels to determine the direction of movement and the extent of movement is determined by the <b>scaled</b> <b>value</b> of the mixture. Some researchers proposed shuffling-based transformation to generate cancelable templates using a user-specified random key [24, 25]. In these works, the iris code is divided into blocks and then the blocks are shuffled with a user-specified random shuffling key to generate cancelable iris template.|$|E
50|$|He {{published}} a manuscript in 1475 containing the {{first use of}} the terms bymillion and trimillion, which {{gave rise to the}} modern terms billion and trillion. His usage referred to the long <b>scale</b> <b>values</b> of 1012 and 1018 respectively. These terms have subsequently been revalued in English to the short <b>scale</b> <b>values</b> 109 and 1012 respectively, although the original values remain in long scale countries.|$|R
40|$|The {{theory of}} {{conjoint}} measurement described by Krantz et al. (1971) {{is shown to}} indicate how a descriptive model of human processing of probabilistic information built around Bayes' rule is to be tested {{and how it is}} to be used to obtain subjective <b>scale</b> <b>values.</b> Specific relationships concerning these <b>scale</b> <b>values</b> are shown to emerge, and the theoretical prospects resulting from this development are discussed...|$|R
30|$|Spectral {{centroid}} (closely {{related to}} the pitch, and will be expressed both in Hz and note <b>scale</b> <b>value).</b>|$|R
40|$|Data {{with time}} {{intervals}} is prominently present in finance, accounting, medicine {{and many other}} application domains. When querying such data, {{it is important to}} perform operations on aligned intervals, i. e., data is processed together only for the common interval where it is valid in the real world. For instance, an employee contributed to a project only for the time period where both the project was running and the employee was employed by the company, i. e., the employee contributed to the project only over their aligned time interval. A temporal join is thus only evaluated over the aligned interval of an employee and a project. The problem of performing temporal operations, such as temporal aggregation or temporal joins, on data with time intervals using relational database systems {{can be attributed to the}} lack of primitives for the alignment of intervals. Even more challenges arise, when the data includes attribute values that are interval-dependent, such as project budgets or cumulative costs, and need to be scaled along with the alignment of intervals during processing. The goal of this thesis is to provide systematic and built-in support for querying data with intervals in relational database systems. The solution we propose uses two temporal primitives a temporal normalizer and a temporal aligner for the alignment of intervals. Temporal operators on interval data are defined by reduction rules that map a temporal operator to an operation with a temporal primitive followed by the corresponding traditional non-temporal operator that uses equality on aligned intervals. A key feature of our approach is that operators can access the original time intervals in predicates and functions, such as join conditions and aggregation functions, using timestamp propagation. Our approach, through timestamp propagation, supports the scaling of attribute values that are interval-dependent. When intervals are aligned during query processing, scaling can be performed at query time with the help of user-defined functions. This allows users to choose whether and how attribute values should be scaled. This is necessary since they may be interested in the total value in one query and the <b>scaled</b> <b>value</b> according to days or even working days in another query. We integrated our solution into the kernel of the open source database system PostgreSQL, which allows to leverage existing query optimization techniques and algorithms...|$|E
40|$|Over {{the last}} few years there has been an {{increasing}} interest in the area of type- 2 fuzzy logic sets and systems in academic and industrial circles. Within robotic research the majority of type- 2 fuzzy logic investigations has been centred on large autonomous mobile robots, where resource availability (memory and computing power) is not an issue. These large robots usually have a variation of a Unix operating system on board. This allows the implementation of complex fuzzy logic systems to control the motors. Specifically the implementation of interval and geometric type- 2 fuzzy logic controllers is of interest as they are shown to outperform type- 1 fuzzy logic controllers in uncertain environments. However when it comes to using micro robots it is not practical to use type- 1 and type- 2 fuzzy logic controllers, {{due to the lack of}} memory and the processor time needed to calculate a control output value. The choice of motor controller is usually either fixed pre-set values, a variable <b>scaled</b> <b>value</b> or a PID controller to generate wheel velocities. In this research novel ways of implementing type- 1 and interval type- 2 fuzzy logic controllers on micro robots with limited resources are investigated. The solution thatis being proposed is the use of pre-calculated 3 D surfaces generated by an off-line Fuzzy Logic System covering the expected ranges of the input and output variables. The surfaces are then loaded into the memory of the micro robots and can be accessed by the motor controller. The aim of the research is to test if there is an advantage of using type- 2 fuzzy logic controllers implemented as surfaces over type- 1 and PID controllers on a micro robot with limited resources. Control surfaces were generated for both type- 1 and average interval type- 2 fuzzy logic controllers. Each control surface was then accessed using bilinear interpolation to provide the crisp output value that was used to control the motor. Previously when this method has been used a single surface was employed to hold the information. This thesis presents the novel approach of the dual surface type- 2 fuzzy logic controller on micro robots. The lower and upper values that are averaged for the classic interval type- 2 controller are generated as surfaces and installed on the micro robots. The advantage is that nuances and features of both the lower and upper surfaces are available to be exploited, rather than being lost due to the averaging process. Having conducted the experiments it is concluded that the best approach to controlling micro robots is to use fuzzy logic controllers over the classical PID controllers where ever possible. When fuzzy controllers are used then type- 2 fuzzy controllers (dual or single surface) should be used over type- 1 fuzzy controllers when applied as surfaces on micro robots. When a type- 2 fuzzy controller is used then the novel dual surface type- 2 fuzzy logic controller should be used over the classic average surface. The novel dual surface controller offers a dynamic, weighted, adaptive and superior response over all the other fuzzy controllers examined. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|E
40|$|This {{dissertation}} {{is concerned}} with the design and analysis of algorithms that address two related issues in communication networks, namely erasures and broadcast. Erasures are an appropriate model for communication channels from a network layer perspective. A class of efficient and flexible codes known as fountain codes, is available to deal with erasures for the basic erasure channel. However, in the network applications that we consider, it remains a challenging problem to design efficient and scalable codes. For an erasure code, the efficiency of encoding and decoding algorithms is distinct from the efficiency of reconstructing erased code symbols from other code symbols, which is of importance in storage applications. In our work, we propose new codes together with algorithms to efficiently repair lost code symbols, simultaneously with low encoding and decoding complexities. Our work on codes for storage also leads us to systematic fountain codes with improved complexity. We also study the design and analysis of degree distributions for fountain codes when the receivers have side information, and we provide upper and lower bounds on the overhead. In a network with multiple hops from the source, we construct a code to import the one-hop traits of LT codes end-to-end using an idea based on online encoding, which {{is also one of the}} components of the repair algorithm for storage codes that we propose. We then consider wireless erasure networks, where local broadcast is another property which influences the role of coding beyond that of merely dealing with erasures. We show that feedback signaling is a critical factor that defines the role of coding in this situation, in the sense that it is one way to avoid the extensive feedback signaling that is necessary for routing policies. To characterize this more precisely, we consider a formal notion of restricted feedback signaling and derive the throughput of routing policies with restricted feedback on a two-hop network. This allows us to obtain a lower bound on the throughput when the losses are independent, and also to show that it is possible to have arbitrary degradation of throughput with dependent losses. Finally, we consider optimization problems involving the control of a queue whose server is defined by the broadcast property, where each service satisfies all the customers simultaneously. Customers in the queue incur holding costs. We consider two constraints on the server and derive the associated optimal controls. For the first constraint, a constant non-negative cost is charged per service whereas in the second type, we consider an online running constraint on the ability to operate the broadcast server. To address this, we solve a more general problem called the online knapsack problem where one needs to choose a sequence of actions over time, with each action incurring a stochastic cost and also consuming a resource, which is replenished stochastically over time with a given rate. The objective is to minimize the total cost subject to the constraint of not exhausting the resource at any point. We derive a limiting characterization of the optimal policy by showing the convergence of the <b>scaled</b> <b>value</b> function to that of a continuous time problem when the discount factor vanishes. In other words, this provides a method to approximate such problems when the discount factor is effective over a long duration and consequently, the magnitude of transactions in each time slot vanishes in comparison to the long-term utility...|$|E
40|$|In {{order to}} clarify the {{significant}} acoustical parameters affecting the subjective judgment of the balance between a singer and instruments inside opera theatres acoustical measurements, virtual reproductions and listening tests were accomplished and analysed. With impulse responses measured inside several theatres of different design and with an anechoic source consisting of a soprano singer and a piano keyboard, several virtual sound fields with particular objective parameters were reproduced under controlled conditions. The sound level difference between the stage and the pit (or orchestra) sources was fixed in the virtual sound fields and paired-comparison tests were conducted to obtain <b>scale</b> <b>values</b> of the balance. A relationship between the <b>scale</b> <b>value</b> of the balance {{and a group of}} parameters used in the acoustical qualification of rooms is then achieved by using multiple regression analysis. Relevant finding is that the early decay time (EDT) for the stage source has a major effect on the <b>scale</b> <b>values</b> of balance. The contribution of the stage parameters to the <b>scale</b> <b>value</b> is greater than that of the pit/orchestra parameters...|$|R
40|$|This {{study was}} {{conducted}} to provide a point of view about scaling methods using in psychology and social sciences and to determine the consistency between <b>scaling</b> <b>values</b> obtained by these two scaling methods. In this study, an attitude scale about BYM was used to collect data. Research was carried out on 2100 high school students and 84 experts. A series scaling procedure was conducted to obtain a <b>scale</b> <b>value.</b> In order to determine the relationship between <b>scaling</b> <b>values</b> obtained from two different scaling procedures, Spearman rho correlation coefficient was calculated. At the same time, in order to provide validity concerning the two different approaches, ANOVA and correlation analysis was conducted. The study revealed that there was a medium level (rs= 0, 43) relationship between <b>scale</b> <b>values</b> obtained from the different scaling approaches. When the number of items was taken as a criterion (K= 20) for developing a scale, 13 items were selected common to the final scale. Although there were differences between scale items selected for different scaling approaches, these two scaling procedures produced similar results with respect to validity...|$|R
40|$|Triangular designs {{analyzed}} by contemporary nonmetric scaling procedures can yield estimated scales that are nonlinearly {{related to the}} "true " <b>scale</b> <b>values.</b> In nonmetric unidimensional scaling, observers rate the "difference, " "similarity, " or "dissimilarity" of stimulus pairs, and a subtractive model is used to estimate <b>scale</b> <b>values</b> that will reproduce the rank order of the judgments. For example, subjects could be asked to listen to two tones and judge the "dissimilarity" in loudness. If the subjects report that the "dissimilarity " of A to B exceeds the "dissimilarity" of C to D, then the absolute difference in <b>scale</b> <b>values</b> between A and B is assumed to exceed the difference in sensation between C and D. dij = M (I si- sj I), (2...|$|R
