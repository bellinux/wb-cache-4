29|915|Public
40|$|This paper {{deals with}} the more recent results {{obtained}} by {{the application of the}} CSLU Toolkit frame-based hybrid HMM/ANN architecture on the connected digit recognition task for the Italian language. The hybrid architecture for <b>speaker</b> <b>independent</b> <b>recognition</b> is described and the last obtained results are introduced in detail. 1...|$|E
40|$|Objective of {{the work}} is <b>speaker</b> <b>independent</b> <b>recognition</b> of vowels of British English. Back {{propagation}} is one of the simplest and most widely used methods for supervised training of multi layer neural networks. In this paper we use parallel implementation of Backpropagation (BP) on Master – Slave architecture to recognize speaker independent eleven steady state vowels of British English. We perform the recognition task on both sequential and parallel implementation. The performance parameters speed-up, optimal number of processors and processing time are evaluated for both implementations...|$|E
40|$|Abstract: In this paper, {{we address}} the <b>speaker</b> <b>independent</b> <b>recognition</b> of Chinese number speeches 0 ~ 9 based on HMM. Our former results of {{inside and outside}} testing {{achieved}} 92. 5 % and 76. 79 % respectively. To improve further the performance, two important features of speech; MFCC and cluster number of vector quantification, are unified together and evaluated on various values. The best performance achieve 96. 2 % and 83. 1 % on MFCC Number = 20 and VQ clustering number = 64...|$|E
5000|$|<b>Speaker</b> <b>independent</b> speech <b>recognition</b> with voice digit dialing ...|$|R
5000|$|The card {{deployed}} <b>speaker</b> <b>independent</b> speech <b>recognition</b> {{for multiple}} languages ...|$|R
5000|$|The <b>speaker</b> <b>independent</b> speech <b>recognition</b> {{vocabulary}} database could {{created by}} the hosting telephone company as necessary based on the call scenario ...|$|R
40|$|The {{basis for}} all methods {{described}} in this paper is the application of an adaptive transition bias to the sequences of phoneme models that represent spoken utterances. This offers significantly improved accuracy in phoneme based <b>speaker</b> <b>independent</b> <b>recognition,</b> while adding very little overhead to the overall system complexity. The algorithms were tested using the low complexity hybrid recognizer denoted Hidden Neural Networks (HNN) on US English and Japanese speaker independent name dialing tasks. Experimental results show that our approach provides a relative error rate reduction of up to 47 % over the baseline system...|$|E
40|$|We {{describe}} a speechreading (lipreading) system purely based on visual features extracted from {{grey level image}} sequences of the speaker's lips. Active shape models are used to track the lip contours while visual speech information is extracted from {{the shape of the}} contours. The distribution and temporal dependencies of the shape features are modelled by continuous density Hidden Markov Models. Experiments are reported for <b>speaker</b> <b>independent</b> <b>recognition</b> tests of isolated digits. The analysis of individual feature components suggests that speech relevant information is embedded in a low dimensional space and fairly robust to inter- and intraspeaker variability...|$|E
40|$|This {{document}} presents MATIS {{along the}} lines defined for the description of common exemplars: overview, reference material available, hardware and software platforms, usage, and future plans. MATIS allows an end-user to obtain information about flight schedules using speech, mouse, keyboard, or a synergistic combination of these techniques. At any time, the user can switch freely between these techniques: there is no prevailing modality. In addition, MATIS supports interleaving at the command level. This system runs under NeXTStep on a NeXT machine. Speech input is processed by Sphynx, a continuous, <b>speaker</b> <b>independent</b> <b>recognition</b> engine. Table of content...|$|E
40|$|Speech {{recognition}} {{is important for}} successful development of speech recognizers in most real world applications. While speaker dependent speech recognizers have achieved close to 100 % accuracy, the <b>speaker</b> <b>independent</b> speech <b>recognition</b> systems have poor accuracy not exceeding 75 %. In this paper we describe a two-module <b>speaker</b> <b>independent</b> speech <b>recognition</b> system for all-British English speech. The first module performs phoneme recognition using two-level neural networks. The second module executes word recognition from the string of phonemes employing Hidden Markov Model. The system was trained by British English speech consisting of 5000 words uttered by 100 speakers. The test samples comprised 2000 words spoken by {{a different set of}} 50 speakers. The recognition accuracy is found to be 98 % which is well above the previous results. Keywords: <b>speaker</b> <b>independent</b> speech <b>recognition,</b> Neural Network...|$|R
5000|$|The <b>speaker</b> <b>independent</b> speech <b>recognition</b> {{vocabulary}} database {{could be}} efficiently changed by downloading an update over the switch time-slot network {{at any time}} ...|$|R
40|$|We {{describe}} a telephone bandwidth <b>speaker</b> <b>independent</b> name <b>recognition</b> system for Saudi speakers. The acoustic models {{of this system}} are implemented using the Hidden Markov model toolkit (HTK) and developed based on the Saudi accented Arabic voice bank (SAAVB) database. The SAAVB database was designed for the construction and evaluation of <b>speaker</b> <b>independent</b> telephone speech <b>recognition.</b> It consists of 1033 speakers. Each speaker spoke 59 utterances. Our system has shown a word recognition rate of 79 % and a sentence recognition rat...|$|R
40|$|Abstract. Most {{of current}} speech {{recognition}} systems {{are based on}} Hidden Markov Models assuming that speech features are sequence of stationary stochastic processes. However, there are certain speech attributes, such as background noise type or speaker voice color, {{that do not have}} stochastic character. This fact is often ignored, by designers of robust <b>speaker</b> <b>independent</b> <b>recognition</b> system. In this work, we investigate how the performance of a noisy speech recognition can be improved provided that we have prior knowledge about type and level of noise. Next, recognizer that is using separate models, each trained on a particular type and level of noise, is proposed for more appropriate modeling of speech. ...|$|E
40|$|This paper {{describes}} {{a novel approach}} for visual speech recognition. The shape of the mouth is modelled by an Active Shape Model which {{is derived from the}} statistics of a training set and used to locate, track and parameterise the speaker's lip movements. The extracted parameters representing the lip shape are modelled as continuous probability distributions and their temporal dependencies are modelled by Hidden Markov Models. We present recognition tests performed on a database of a broad variety of speakers and illumination conditions. The system achieved an accuracy of 85. 42 % for a <b>speaker</b> <b>independent</b> <b>recognition</b> task of the first four digits using lip shape information only...|$|E
40|$|We {{describe}} a speechreading system that uses both, shape {{information from the}} lip contours and intensity information from the mouth area. Shape information is obtained by tracking and parameterising {{the inner and outer}} lip boundary in an image sequence. Intensity information is extracted from a grey level model, based on principal component analysis. In comparison to other approaches, the intensity area deforms with the shape model to ensure that similar object features are represented after non-rigid deformation of the lips. We describe <b>speaker</b> <b>independent</b> <b>recognition</b> experiments based on these features and Hidden Markov Models. Preliminary results suggest that similar performance can be achieved by using either shape or intensity information and slightly higher performance by their combined use. 1...|$|E
40|$|Speech {{processing}} {{is developed}} {{as one of}} the paramount requisition region of digital signal processing. Different fields for research in speech processing are speech recognition, speaker identification, speech bland, speech coding etc. The objective of <b>Speaker</b> <b>Independent</b> Speech <b>Recognition</b> is to concentrate, describe and distinguish information about speech signal and methodology towards creating the speaker free speech recognition system. Extracted information will be valuable for the directing and working different electronic contraptions and hardware through the human voice proficiently. Feature extraction is the first venture for speech recognition. Numerous algorithms are recommended / created by the scientists for feature extraction. In this work, the cubic-log compression in Mel-Frequency Cepstrum Coefficient (MFCC) feature extraction system is utilized to concentrate the characteristics from speech sign for outlining a <b>speaker</b> <b>independent</b> <b>speaker</b> <b>recognition</b> system. Extracted features are used to train and test this system with the help of Vector Quantization approach...|$|R
40|$|Conventional <b>speaker</b> <b>independent</b> speech <b>recognition</b> {{systems are}} trained {{using data from}} many {{different}} speakers. Inter-speaker variability {{is a major problem}} because parametric representations of speech are highly speaker dependent. This paper describes a technique which allows speaker dependent parameters to be considered when building a <b>speaker</b> <b>independent</b> speech <b>recognition</b> system. The technique is based on utterance clustering, where subsets of the training data are formed and the variability within each subset minimized. Cluster dependent connectionist models are then used to estimate phone probabilities as part of a hybrid connectionist hidden Markov model based large vocabulary talker <b>independent</b> speech <b>recognition</b> system. The system has been evaluated on the ARPA Wall Street Journal continuous speech recognition task. 1. INTRODUCTION Speaker dependent speech recognition systems are generated using training utterances from a single speaker, resulting in a system tuned to a spec [...] ...|$|R
40|$|Phoneme {{recognition}} {{is important for}} successful development of speech recognizers in most real world applications. While speaker dependent phoneme recognizers have achieved close to 100 % accuracy, the <b>speaker</b> <b>independent</b> phoneme <b>recognition</b> systems have poor accuracy not exceeding 75 %. In this paper we describe a two-module <b>speaker</b> <b>independent</b> phoneme <b>recognition</b> system for all-Indian English speech. The first module performs classification of phonemes recognition using Probabilistic neural networks. The second module executes the recognized phonemes from the classified phonemes employing Recurrent Neural Networks. The system was trained by Indian English speech consisting of 1000 words uttered by 50 speakers. The test samples comprised 500 words spoken by {{a different set of}} 30 speakers. The recognition accuracy is found to be 98 % which is well above the previous results...|$|R
40|$|A new {{framework}} for the context and <b>speaker</b> <b>independent</b> <b>recognition</b> of emotions from voice, based on a richer and more natural representation of the speech signal, is proposed. The utterance is viewed as consisting {{of a series of}} voiced segments and not as a single object. The voiced segments are first identified and then described using statistical measures of spectral shape, intensity, and pitch contours, calculated at both the segment and the utterance level. Utterance classification is performed by combining the segment classification decisions using a fixed combination scheme. The performance of two learning algorithms, Support Vector Machines and K Nearest Neighbors, is compared. The proposed approach yields an overall classification accuracy of 87 % for 5 emotions, outperforming previous results on a similar database. 1...|$|E
40|$|We {{introduce}} a new Bayesian predictive classification (BPC) approach to robust speech recognition and apply the BPC framework to Gaussian mixture continuous density hidden Markov model based speech recognition. We propose and focus {{on one of the}} approximate BPC approaches called quasi-Bayesian predictive classification (QBPC). In comparison with the standard plug-in maximum a posteriori decoding, when the QBPC method is applied to <b>speaker</b> <b>independent</b> <b>recognition</b> of a confusable vocabulary namely 26 English letters, where a broad range of mismatches between training and testing conditions exist, the QBPC achieves around 14 % relative recognition error rate reduction. While the QBPC method is applied to cross-gender testing on a less confusable vocabulary, namely 20 English digits and commands, the QBPC method achieves around 24 % relative recognition error rate reduction. published_or_final_versio...|$|E
40|$|The {{presence}} of background noise {{and the frequency}} response of a transmission line like in telephone applications have {{a major influence on}} the performance of speech recognition systems. An approach is presented in this paper to cope with both effects. It is based on an estimation of the stationary noise spectrum and an estimation of the mismatch between the frequency responses present during training and during recognition. These estimations are used in combination with the PMC scheme [1] to adapt the whole word HMMs for a <b>speaker</b> <b>independent</b> <b>recognition</b> of connected words. A considerable improvement can be achieved on recognizing distorted speech data. The technique is also used as part of a complete speech dialogue system over the telephone network where it could also proof its beneficial usability...|$|E
40|$|Two <b>speaker</b> <b>independent</b> speech <b>recognition</b> experiments, {{regarding}} the automatic discrimination of the Italian alphabet I-set and E-set, two very difficult Italian phonetic classes, will be described. The speech signal is analyzed by a recently developed joint synchrony/mean-rate auditory processing scheme and a fully-connected feed-forward recurrent BP network {{was used for}} the classification stage. The achieved <b>speaker</b> <b>independent</b> mean <b>recognition</b> rate was 65 %, for the Iset and 88 % for the E-set showing rather satisfactory results given the difficulty of both tasks. 1. INTRODUCTION Both static and dynamic networks have been proposed in speech technology, and especially in speech recognition, as opposed to more classic statistical approaches. Experimental results show that neural networks represent an effective alternative to classical pattern recognition methods in several applications. The Multi-Layered Neural Networks (MLN) trained with Back-Propagation (BP) are probably the most u [...] ...|$|R
40|$|A bimodal {{automatic}} {{speech recognition}} system, in which the speech signal is synchronously analyzed by an audio channel producing spectral-like parameters every 2 ms and by a visual channel computing lip and jaw kinematic parameters, is described and some results are given for various <b>speaker</b> <b>independent</b> phonetic <b>recognition</b> experiments regarding the Italian plosive class in different noisy conditions...|$|R
50|$|Automated Computer Telephone Interviewing (ACTI) is a {{technique}} by which a computer with <b>speaker</b> <b>independent</b> voice <b>recognition</b> capabilities asks respondents a series of questions, recognizes then stores the answers, {{and is able to}} follow scripted logic and branch intelligently according to the flow of the questionnaire based on the answers provided, as well as information known about the participant.|$|R
40|$|Automatic spoken {{language}} identification (LID) {{plays an important}} part in routing foreign callers to operators who speak the caller's language, or as a front-end to a multi-lingual translation system to route the call to the appropriate translation system. A common approach to {{spoken language}} ID is adopted from current <b>speaker</b> <b>independent</b> <b>recognition</b> techniques. These generally involve the development of a phonetic recognisor for each language and then combining the acoustic likelihood scores to determine the highest scoring language. The models are trained using hidden Markov modelling (HMM) or neural networks (NN). This paper proposes a novel approach to spoken language identification by the use of inductive inference "decision trees". To develop the production rules, the classification models are generated inductively by examining a large speech database and then generalising the pattern from the specific examples. This approach has already been successfully used for isolated digit [...] ...|$|E
40|$|Frequency warping {{approaches}} to speaker normalization {{have been proposed}} and evaluated on various speech recognition tasks [1, 2, 3]. These techniques {{have been found to}} significantly improve performance even for <b>speaker</b> <b>independent</b> <b>recognition</b> from short utterances over the telephone network. In maximum likelihood (ML) based model adaptation a linear transformation is estimated and applied to the model parameters in order to increase the likelihood of the input utterance. The {{purpose of this paper is}} to demonstrate that significant advantage can be gained by performing frequency warping and ML speaker adaptation in a unified framework. A procedure is described which compensates utterances by simultaneously scaling the frequency axis and reshaping the spectral energy contour. This procedure is shown to reduce the error rate in a telephone based connected digit recognition task by 30 - 40 %. 1. INTRODUCTION A major hurdle in building successful automatic speech recognition applications is [...] ...|$|E
40|$|We compare a {{standard}} HMM based and a neural network based approach to speech recognition. The application is the <b>speaker</b> <b>independent</b> <b>recognition</b> {{of a small}} vocabulary over the telephone. While the recognition results are comparable, {{it is argued that}} the neural network system is a better choice for implementation. 1. Introduction Hidden Markov model (HMM) based speech recognition systems are widely accepted as the method of choice for speaker-independent speech recognition[1]. For reasons described ins Section 3, we believe that neural network technology will eventually be preferable for large scale (thousands of ports), small vocabulary (on the order of 100 phrases), speaker independent, telephone applications. It is, however, crucial to evaluate this less mature technology against the HMMs to see how they compare from both the perspectives of classification accuracy and cost of implementation. In this paper we discuss a specific application: voice access to networked telephone ser [...] ...|$|E
40|$|We {{have already}} {{proposed}} a new feature extraction method based on higher-order local auto-correlation and Fisher weight map (FWM) at Interspeech 2006. This paper shows ef-fectiveness {{of the proposed}} FWM in speaker dependent and <b>speaker</b> <b>independent</b> phoneme <b>recognition.</b> Widely used MFCC features lack temporal dynamics. To solve this problem, local auto-correlation features are computed and accumulated by weighting high scores on the discriminative areas. This score map is called Fisher weight map. From the speaker de-pendent phoneme recognition, the proposed FWM showed 79. 5 % recognition rate, by 5. 0 points higher than the result by MFCC. Furhermore by combing FWM with MFCC and ∆MFCC, the recognition rate improved to 88. 3 %. In the <b>speaker</b> <b>independent</b> phoneme <b>recognition,</b> it showed 84. 2 % recognition rate, by 11. 0 points higher than the result by MFCC. By combining FWM with MFCC and ∆MFCC, the reecognition rate improved to 89. 0 %. ...|$|R
40|$|In {{this paper}} is given a {{proposal}} for a <b>speaker</b> <b>independent</b> speech <b>recognition</b> interface of isolated words with application in Interactive Voice Telephone systems. The overall structure of the recognition engine is based on the Dynamic Time Warping (DTW) paradigm for com-putational efficiency. The system is implemented and evaluated in simulated conditions and the results show that reasonable performance can be achieved by these methods...|$|R
40|$|Discriminative {{training}} {{techniques for}} Hidden-Markov Models were recently proposed and successfully applied for automatic speech recognition. In this paper {{a discussion of}} the Minimum Classification Error and the Maximum Mutual Information objective is presented. An extended reestimation formula is used for the HMM parameter update for both objective functions. The discriminative training methods were utilized in <b>speaker</b> <b>independent</b> phoneme <b>recognition</b> experiments and improved the phoneme recognition rates for both discriminative training techniques...|$|R
40|$|Speaker accent is an {{important}} issue in the formulation of robust <b>speaker</b> <b>independent</b> <b>recognition</b> systems. Knowledge gained from a reliable accent classification approach could improve overall recognition performance. In this paper, a new algorithm is proposed for foreign accent classification of American English. A series of experimental studies are considered which focus on establishing how speech production is varied to convey accent. The proposed method uses a source generator framework, recently proposed for analysis and recognition of speech under stress[5]. An accent sensitive database is established using speakers of American English with foreign language accents. An initial version of the classification algorithm classified speaker accent from among four different accents with an accuracy of 81. 5 % in the case of unknown text, and 88. 9 % assuming known text. Finally, it is shown that as accent sensitive word count increases, the ability to correctly classify accent also increase [...] ...|$|E
40|$|Speech {{recognition}} systems (SRS) {{designed for}} applications in low cost products like telephones or in systems with energetic constraints like autonomous vehicles {{are faced with}} the demand for solutions with low complexity. A small vocabulary consisting of a few command words and the digits is sufficient for most of the applications but has to be recognized robustly. Here we report about investigations concerning the application of Recurrent Neural Networks (RNN) for <b>speaker</b> <b>independent</b> <b>recognition</b> of speech signals with telephone bandwidth. A RNN-SRS with low complexity is developed which recognizes isolated words as well as connected digits in adverse conditions. To enable an efficient hardware implementation of the SRS we introduce Locally Recurrent Neural Networks (LRNN). LRNN are layered networks which have recurrent connections only between the neurons of a hidden layer and their n-nearest neighbours. The neurons of the input and the output layer have unidirectional and sparse c [...] ...|$|E
40|$|Speech {{recognition}} solely {{based on}} visual {{information such as}} the lip shape and its movement {{is referred to as}} lipreading. This paper presents an automatic lipreading technique for speaker dependent (SD) and speaker independent (SI) speech recognition tasks. Since the visual features are derived according to the frame rate of the video sequence, spline representation is then employed to translate the discrete-time sampled visual features into continuous domain. The spline coefficients in the same word class are constrained to have similar expression and can be estimated from the training data by the EM algorithm. In addition, an adaptive multi-model approach is proposed to overcome the variation caused by different speaking style in speaker-independent recognition task. The experiments are carried out to recognize the ten English digits and an accuracy of 96 % for speaker dependent recognition and 88 % for <b>speaker</b> <b>independent</b> <b>recognition</b> have been achieved, which shows the superiority of our approach compared with other classifiers investigated. 1...|$|E
40|$|This paper {{describes}} an improved input coding method for a text-to-phoneme (TTP) {{neural network model}} for <b>speaker</b> <b>independent</b> speech <b>recognition</b> systems. The code-book is self-organizing and is jointly optimized with the TTP model ensuring that the coding is optimal in terms of overall performance. The code-book {{is based on a}} set of single layer neural networks with shared weights. Experiments show that performance is increased com-pared to the NETTalk and NETSpeak models. 1...|$|R
50|$|To provide {{even higher}} levels of automation, CCI started a very {{aggressive}} program in the early 1980s to develop a PCM digital telephone switching system targeted for automated, user defined call scenarios. Initial installations handled intercept and calling card calls by capturing Multi-Frequency and DTMF audio band signaling via the DSP based Multi-Frequency Receiver board. Later systems added <b>speaker</b> <b>independent</b> speech <b>recognition</b> via the Quad Digital Audio Processor board to initially automate collect calls.|$|R
40|$|This paper {{presents}} ongoing {{work on a}} <b>speaker</b> <b>independent</b> visual speech <b>recognition</b> system. The work {{presented here}} builds on previous research efforts {{in this area and}} explores the potential use of simple hidden Markov models for limited vocabulary, <b>speaker</b> <b>independent</b> visual speech <b>recognition.</b> The task at hand is recognition of the first four English digits, a task with possible applications in car-phone dialing. The images were modeled as mixtures of independent Gaussian distributions, and the temporal dependencies were captured with standard left-to-right hidden Markov models. The results indicate that simple hidden Markov models may be used to successfully recognize relatively unprocessed image sequences. The system achieved performance levels equivalent to untrained humans when asked to recognize the fIrst four English digits. ...|$|R
