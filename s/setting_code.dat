7|2579|Public
50|$|Trains {{with some}} {{specific}} requirements, such as out-of-gauge loads or the Royal Train, {{run with the}} letter X, and special trains not in the regular train service (e.g. charters, railtours, emergency trains or as-required locomotive moves) have Z. Automatic Route <b>Setting</b> <b>code</b> prevents the automatic routing of trains with the letter X in their headcode and signallers must route these trains manually. The former White Rose service from London King's Cross to Leeds ran with a prefix of 1X due to the train's unusually long length.|$|E
50|$|The idea {{of putting}} all the video mode <b>setting</b> <b>code</b> in one place inside the kernel had been {{acknowledged}} for years, but the graphics card manufacturers had argued {{that the only way}} to do the mode-setting was to use the routines provided by themselves and contained in the Video BIOS of each graphics card. Such code had to be executed using x86 real mode, which prevented it to be invoked from a kernel running in protected mode. The situation changed when Luc Verhaegen and other developers found a way to do the mode-setting natively instead of BIOS-based, showing that it was possible to do it using normal kernel code and laying the groundwork for what would become the Kernel Mode Setting. In May 2007 Jesse Barnes (Intel) published the first proposal for a drm-modesetting API and a working native implementation of mode-setting for Intel GPUs within the i915 DRM driver. In December 2007 Jerome Glisse started to add the native mode-setting code for ATI cards to the radeon DRM driver. Work on both the API and drivers continued during 2008, but got delayed by the necessity of a memory manager also in kernel space to handle the framebuffers.|$|E
40|$|This paper {{presents}} a status of non-CFD aeroacoustic codes at NASA Langley Research Center for {{the prediction of}} helicopter harmonic and Blade-Vortex Interaction (BVI) noise. The prediction approach incorporates three primary components: CAMRAD. Mod 1 - a substantially {{modified version of the}} performance/trim/wake code CAMRAD; HIRES - a high resolution blade loads post-processor; and WOPWOP - an acoustic code. The functional capabilities and physical modeling in CAMRAD. Mod 1 /HIRES will be summarized and illustrated. A new multi-core roll-up wake modeling approach is introduced and validated. Predictions of rotor wake and radiated noise are compared with to the results of the HART program, a model BO- 105 windtunnel test at the DNW in Europe. Additional comparisons are made to results from a DNW test of a contemporary design four-bladed rotor, as well as from a Langley test of a single proprotor (tiltrotor) three-bladed model configuration. Because the method is shown to help eliminate the necessity of guesswork in <b>setting</b> <b>code</b> parameters between different rotor configurations, it should prove useful as a rotor noise design tool...|$|E
50|$|<b>Setting</b> <b>codes</b> {{of ethics}} and {{standards}} of accounting and auditing.|$|R
5000|$|... symlink target {{names of}} {{unlimited}} length and character <b>set</b> <b>coding</b> ...|$|R
50|$|All {{cards are}} marked {{in the lower}} left with an {{expansion}} <b>code</b> or <b>set</b> <b>code,</b> a three-letter <b>code</b> identifying the <b>set</b> in which the card was published. The first letter of the <b>set</b> <b>code</b> indicates the <b>setting</b> (D for DC Comics and M for Marvel Comics), except for the Essential Collection set, which starts with the letter E.|$|R
40|$|In 1974 Kolmogorov {{proposed}} a non-probabilistic approach to statistics, an individual combinatorial {{relation between the}} data and its model, expressed by the so-called "structure function" of the data. We show that the structure function determines all stochastic properties of the data {{in the sense of}} determining the best- tting model at every model-complexity level. A consequence is this: minimizing the data-to-model code length (finding the ML estimator or MDL estimator), in a class of contemplated models of prescribed maximal (Kolmogorov) complexity, always results in a model of best fit, irrespective of whether the source producing the data is in the model class considered. In this <b>setting,</b> <b>code</b> minimization always separates optimal model information from the remaining accidental information, and not only with high probability. The function that maps the maximal allowed model complexity to the goodness-of-fit (expressed as minimal "randomness deficiency") of the best model cannot itself be monotonically approximated. However, the shortest one-part or two-part code above can [...] implicitly optimizing this elusive goodness-of-fit. We show that [...] within the obvious constraints [...] every graph is realized by the structure function of some data. We determine the (un) computability properties of the various functions contemplated and of the "algorithmic minimal sufficient statistic. "...|$|E
40|$|The role of good {{governance}} in local development is worth mentioning. Local governments that achieved better transparency, accountability and responsiveness {{are likely to}} bring development than their counterparts. This study deals {{with the performance of}} {{good governance}} in Naeder Adet woreda of Tigiray Regional State. It assesses the performance of good governance in land administration mainly after the GTP period. Specifically, the study assessed the performance of good governance from the perspective of transparency, accountability and responsiveness. Besides, the study figured out factors that inhibit the performance of good governance in the land administration. The study was conducted by using 182 household heads that were selected via convenience sampling. Furthermore, focused group discussion, interview, secondary data was used to gather relevant data. The study finding indicates that the performance of the woreda land administration pertaining to transparency is at its infancy stage. With regard to accountability, the woreda land administration has installed both formal and informal accountability mechanisms where administrative accountability could be ensured. In spite of that, the practicability of these accountability mechanisms and tools in the land administration is in its early stage. There is the dearth of downward accountability. Furthermore, the performance of responsiveness was also found dissatisfactory. Finally, despite the agenda of good governance in the woreda seems getting a due emphasis, the overall performance was found low, which makes it difficult to conclude that there is significant change contrary to the expectation hoped to be realized after GTP period. In this regard, the prime factors that inhibit the performance of good governance in the land administration of the woreda were found, among others,corruption, weak public education, weak monitoring and evaluation system, low implementation capacity, low participation and low coordination among stake holders, low incentives. Thus, if good land governance is to be ensured the woreda government should work on tackling the above bottlenecks by setting clear guidelines and service standards, empowering civic engagement on monitoring and evaluating service delivery process, providing adequate trainings and incentives to land committees and local councils and <b>setting</b> <b>code</b> of conduct to the land administrators. Key words: Good governance, transparency, accountability, responsiveness, land administratio...|$|E
40|$|Objective of {{this study}} is to assess ethical {{behavior}} among professionals at procurement and after tendering process with its impacts and drivers in Nepalese Construction Industry. Different literatures were reviewed to assess ethical practices along with its cause and effect inside Nepalese Construction Industry. Pilot study was conducted for the validity of the questionnaire. One key informant from each selected organization was interviewed. The questionnaire contains shortcomings of ethical behavior at procurement and after tendering phase impact of shortcomings of ethical practices and factors leading to these ethical practices based on the objectives of the research. Five ranking Likert Scale were used. The collected data were analyzed based on relative importance index RII in three different categories as Investigating Offices 3 numbers Professional Associations 4 numbers and Government Departments 4 numbers with total of 11 organizations. All together 240 respondents were targeted out of which 170 response were collected with response rate of 70. 83. The research shows that for commitment of professionals The overall level of unethical conduct in construction industry is placed at first rank with agreement level of 72. 7. For Professionals shortcomings of ethical behavior at procurement phase Individuals or organizations undertaking work without adequate qualification experience training is placed at first rank with agreement level of 68. 00. For Professionals shortcomings of ethical behavior after awarding the Tender Contractors professional dont dispose waste in suitable and safe ways which is friendly with the environment is placed at first rank with agreement level of 67. 50. For factors lead to shortcomings of ethical behavior Personal culture or personal behavior is placed at first rank with agreement level of 78. 20. From the research it is clear that shortcomings of ethical behaviors have negative impact firstly on cost as it affects the profitability of the organization and causes loss for these organizations every year. Secondly it affects the projects quality as it is noticed that construction projects quality in Nepal ranges from moderate to very low. The dissemination of ethical awareness heavier penalties compulsory training and <b>setting</b> <b>code</b> of ethics are considered the best ways to monitor these shortcomings of ethical behaviors occurred in construction industry. The codes of practice are the most feasible way to attempt change behavior. Characteristic and responsibility that professionals should have is important in order to perform their work. With a good character and full set of responsibilities in hand professional will always know what to do when facing problem and will try their best to avoid any shortcomings of ethical behavior. A self-building training and motivation to comprehend the professional about the responsibility and character as an ethical professional should be conducted from time to time...|$|E
5000|$|<b>Set.</b> <b>Code</b> Geass: Suzaku of the Counterattack ("Code Geass - O Contra-ataque de Suzaku" [...] in Brazil) ...|$|R
2500|$|... 53 Alstom Metropolis eight car <b>sets</b> (<b>Coded</b> as AC06) – Line 1, (Coded as AC08) – Line 2 ...|$|R
50|$|In 2006, Diana Department Store was {{exit from}} SET, company sold under {{backdoor}} of DragonOne company. The old <b>SET</b> <b>code</b> was DIANA.|$|R
40|$|Trustworthiness {{is one of}} {{the main}} aspects that {{contribute}} to the adoption/rejection of a software product. This is actually true for any product in general, but it is especially true for Open Source Software (OSS), whose trustworthiness is sometimes still regarded as not as guaranteed as that of closed source products. Only recently, several industrial software organizations have started investigating the potential of OSS products as users or even producers. As they are now getting more and more involved in the OSS world, these software organizations are clearly interested in ways to assess the trustworthiness of OSS products, so as to choose OSS products that are adequate for their goals and needs. Trustworthiness is a major issue when people and organizations are faced with the selection and the adoption of new software. Although some ad-hoc methods have been proposed, there is not yet general agreement about which software characteristics contribute to trustworthiness. Such methods –like the OpenBQR [30] and other similar approaches [58][59]– assess the trustworthiness of a software product by means of a weighted sum of specific quality evaluations. None of the existing methods based on weighted sums has been widely adopted. In fact, these methods are limited in that they typically leave the user with two hard problems, which are common to models built by means of weighted sums: identify the factors that should be taken into account, and assign to each of these factors the “correct” weight to adequately quantify its relative importance. Therefore, this work focuses on defining an adequate notion of trustworthiness of Open Source products and artifacts and identifying a number of factors that influence it to help and guide both developers and users when deciding whether a given program (or library or other piece of software) is “good enough” and can be trusted in order to be used in an industrial or professional context. The result of this work is a set of estimation models for the perceived trustworthiness of OSS. This work has been carried out in the context of the IST project QualiPSo ([URL] funded by the EU in the 6 th FP (IST- 034763). The first step focuses on defining an adequate notion of trustworthiness of software products and artifacts and identifying a number of factors that influence it. The definition of the trustworthiness factors is driven by specific business goals for each organization. So, we carried out a survey to elicit these goals and factors directly from industrial players, trying to derive the factors from the real user needs instead of deriving them from our own personal beliefs and/or only by reading the available literature. The questions in the questionnaire were mainly classified in three different categories: 1) Organization, project, and role. 2) Actual problems, actual trustworthiness evaluation processes, and factors. 3) Wishes. These questions are needed to understand what information should be available but is not, and what indicators should be provided for an OSS product to help its adoption. To test the applicability of the trustworthiness factors identified by means of the questionnaires, we selected a set of OSS projects, widely adopted and generally considered trustable, to be used as references. Afterwards, a first quick analysis was carried out, to check which factors were readily available on each project’s web site. The idea was to emulate the search for information carried out by a potential user, who browses the project’s web sites, but is not willing to spend too much effort and time in carrying out a complete analysis. By analyzing the results of this investigation, we discovered that most of the trustworthiness factors are not generally available with information that is enough to make an objective assessment, although some factors have been ranked as very important by the respondents of our survey. To fill this gap, we defined a set of different proxy-measures to use whenever a factor cannot be directly assessed on the basis of readily available information. Moreover, some factors are not measurable if developers do not explicitly provide essential information. For instance, this happens for all factors that refer to countable data (e. g., the number of downloads cannot be evaluated in a reliable way if the development community does not publish it). Then, by taking into account the trustworthiness factors and the experience gained through the project analysis, we defined a Goal/Question/Metric (GQM[29]) model for trustworthiness, to identify the qualities and metrics that determine the perception of trustworthiness by users. In order to measure the metrics identified in the GQM model, we identified a set of tools. When possible, tools were obtained by adapting, extending, and integrating existing tools. Considering that most of metrics were not available via the selected tools, we developed MacXim, a static code analysis tool. The selected tools integrate a number of OSS tools that support the creation of a measurement plan, starting from the main actors’ and stakeholders’ objectives and goals (developer community, user community, business needs, specific users, etc.), down to the specific static and dynamic metrics that will need to be collected to fulfill the goals. To validate the GQM model and build quantitative models of perceived trustworthiness and reliability, we collected both subjective evaluations and objective measures on a sample of 22 Java and 22 C/C++ OSS products. Objective measures were collected by means of MacXim and the other identified tools while subjective evaluations were collected by means of more than 500 questionnaires. Specifically, the subjective evaluations concerned how users evaluate the trustworthiness, reliability and other qualities of OSS; objective measures concerned software attributes like size, complexity, modularity, and cohesion. Finally, we correlated the objective code measures to users’ and developers’ evaluations of OSS products. The result is a set of quantitative models that account for the dependence of the perceivable qualities of OSS on objectively observable qualities of the code. Unlike the models based on weighted sums usually available in the literature, we have obtained estimation models [87], so the relevant factors and their specific weights are identified via statistical analysis, and not in a somewhat more subjective way, as usually happens. Qualitatively, our results may not be totally surprising. For instance, it may be generally expected that bigger and more complex products are less trustworthy than smaller and simpler products; likewise, it is expected that well modularized products are more reliable. For instance, our analyses indicate that the OSS products are most likely to be trustworthy if: • Their size is not greater than 100, 000 effective LOC; • The number of java packages is lower than 228. These models derived in our work can be used by end-users and developers that would like to evaluate the level of trustworthiness and reliability of existing OSS products and components they would like to use or reuse, based on measurable OSS code characteristics. These models can also be used by the developers of OSS products themselves, when <b>setting</b> <b>code</b> quality targets based on the level of trustworthiness and reliability they want to achieve. So, the information obtained via our models can be used as an additional piece of information that can be used when making informed decisions. Thus, unlike several discussions that are based on –sometimes interested– opinions about the quality of OSS, this study aims at deriving statistically significant models that are based on repeatable measures and user evaluations provided by a reasonably large sample of OSS users. The detailed results are reported in the next sections as follows: •Chapter 1 reports the introduction to this work •Chapter 2 reports the related literature review •Chapter 3 reports the identified trustworthiness factors •Chapter 4 describe how we built the trustworthiness model •Chapter 5 shows the tools we developed for this activity •Chapter 6 reports on the experimentation phase •Chapter 7 shows the results of the experimentation •Chapter 8 draws conclusions and highlights future works •Chapter 9 lists the publication made during the Ph...|$|E
30|$|Figure  5 {{compares the}} BERs between system {{adopting}} OZCZ <b>code</b> <b>set</b> and system adopting ZCC <b>code</b> <b>set.</b> The results are calculated for time delays within ZCZ length Z[*]=[*] 3. For ZCC <b>code</b> <b>set,</b> only in-phase cross-correlation is zero. For OZCZ <b>code</b> <b>set,</b> {{all of the}} cross-correlations within the ZCZ are zeros. Therefore, the system adopting OZCZ <b>code</b> <b>set</b> performs better than that adopting ZCC <b>code</b> <b>set,</b> which verifies the theoretical analysis of the performance, presented in Section 4.|$|R
5000|$|Some type of {{blinking}} cursor {{that can be}} positioned (with arrow keys and/or [...] "home" [...] and other direct cursor address <b>setting</b> <b>codes).</b>|$|R
3000|$|... 2) in dB. Considering {{conditions}} (12) and (13) of an orthogonal <b>code</b> <b>set,</b> it {{is clear}} that the smaller the values of ASPs and CPs of a nearly orthogonal <b>code</b> <b>set</b> are, the closer the <b>code</b> <b>set</b> is to an orthogonal <b>code</b> <b>set.</b>|$|R
50|$|The DOS filenames {{are in the}} OEM {{character}} <b>set.</b> <b>Code</b> 0xE5 as {{the first}} byte (see below) makes troubles when extra-ASCII characters are used.|$|R
50|$|Hewlett-Packard uses {{a similar}} concept in its HP-UX {{operating}} {{system and its}} Printer Command Language (PCL) protocol for printers (either for HP printers or not). The terminology, however, is different: What others call a character set, HP calls a symbol set, and what IBM or Microsoft call a code page, HP calls a symbol <b>set</b> <b>code.</b> HP developed a series of symbol sets, each with an associated symbol <b>set</b> <b>code,</b> to encode both its own character sets and other vendors’ character sets.|$|R
5000|$|Despite its name, Code 128 {{does not}} have 128 {{distinct}} symbols, so it cannot represent 128 code points directly. To represent all 128 ASCII values, it shifts among three <b>code</b> <b>sets</b> (A, B, C). Together, <b>code</b> <b>sets</b> A and B cover all 128 ASCII characters. <b>Code</b> <b>set</b> C is used to efficiently encode digit strings. The initial subset is selected by using the appropriate start symbol. Within each <b>code</b> <b>set,</b> some of the 103 data code points are reserved for shifting {{to one of the}} other two <b>code</b> <b>sets.</b> The shifts are done using code points 98 and 99 in <b>code</b> <b>sets</b> A and B, 100 in <b>code</b> <b>sets</b> A and C and 101 in <b>code</b> <b>sets</b> B and C to switch between them): ...|$|R
30|$|It is {{impossible}} to design an orthogonal <b>code</b> <b>set</b> that perfectly satisfies both (12) and (13). Hence, an alternative is to design a <b>code</b> <b>set</b> that satisfies the conditions as closely as possible. In this paper, we call such a <b>code</b> <b>set</b> a nearly orthogonal <b>code</b> <b>set.</b>|$|R
5000|$|Triangle Fraternity {{was founded}} on high ethical and moral ideals, and expects {{the men of the}} {{fraternity}} to follow a <b>set</b> <b>Code</b> of Ethics, which is as follows: ...|$|R
40|$|In this paper, {{the authors}} propose {{the design of}} a new {{orthogonal}} small <b>set</b> Kasami <b>code</b> sequence generated using combination of non-orthogonal m-sequence and small <b>set</b> Kasami <b>code</b> sequence. The authors demonstrate that the proposed code sequence has comparable auto-correlation function (ACF), cross- correlation function (CCF), peak cross-correlation values with that of the existing orthogonal small <b>set</b> Kasami <b>code</b> sequence. Though the proposed code sequence has less <b>code</b> sequence <b>sets</b> than that of the existing orthogonal small <b>set</b> Kasami <b>code</b> sequence, the proposed code sequence possesses one more numbers of members in each <b>code</b> sequence <b>set.</b> The members of the same <b>code</b> <b>set</b> of the proposed code sequence are orthogonal to each other...|$|R
40|$|AbstractReproducibility of {{experiments}} is {{considered as one}} of the main principles of the scientiﬁc method. Recent developments in data and computation intensive science, i. e. e-Science, and state of the art in Cloud computing provide the necessary components to preserve data <b>sets</b> and re-run <b>code</b> and software that create research data. The Executable Paper (EP) concept uses state of the art technology to include data <b>sets,</b> <b>code,</b> and software in the electronic publication such that readers can validate the presented results. In this paper we present how to advance current state of the art to preserve, data <b>sets,</b> <b>code,</b> and software that create research data, the basic components of an execution platform to preserve long term compatibility of EP, and we identify a number of issues and challenges in the realization of EP...|$|R
30|$|It is {{proved that}} the new <b>code</b> <b>set</b> is still an OZCZ <b>code</b> <b>set.</b>|$|R
40|$|It {{is shown}} that cyclic {{difference}} <b>set</b> <b>codes</b> and one-step majority-logic codes may be constructed from idempotents {{based upon the}} cyclomic cosets. The construction method produces the well known perfect difference <b>set</b> <b>codes</b> of lengths 21, 73, 273 and 1057 plus some new codes. The code design produces the dual code idempotent and following F. J. MacWilliams [1][2] [3], it is shown {{that this may be}} used directly to define the parity check matrix of the code. A feature of the cyclotomic idempotent codes is the incremental approach to the sparseness of the parity check matrix and the property of parity check bit orthogonality which is known to be useful in iterative decoding using belief propagation. It is shown that the difference enumerator polynomial is composed of some, or all of the cyclotomic idempotents which allows a <b>set</b> of <b>code</b> design conditions to be defined...|$|R
40|$|In this paper, a new large {{spreading}} <b>code</b> <b>set</b> with {{a uniform}} low cross-correlation is proposed. The proposed <b>code</b> <b>set</b> {{is capable of}} (1) {{increasing the number of}} assigned user (capacity) in a multicarrier code division multiple access (MC-CDMA) system and (2) reducing the peak-to-average power ratio (PAPR) of an orthogonal frequency division multiplexing (OFDM) system. In this paper, we derive a new <b>code</b> <b>set</b> and present an example to demonstrate performance improvements of OFDM and MC-CDMA systems. Our proposed <b>code</b> <b>set</b> with <b>code</b> length of N has K= 2 N+ 1 number of codes for supporting up to (2 N+ 1) users and exhibits lower cross correlation properties compared to the existing spreading <b>code</b> <b>sets.</b> Our results with subcarrier N= 16 confirm that the proposed <b>code</b> <b>set</b> outperforms the current pseudo-orthogonal carrier interferometry (POCI) <b>code</b> <b>set</b> with gain of 5 dB at bit-error-rate (BER) level of 10 - 4 in the additive white Gaussian noise (AWGN) channel and gain of more than 3. 6 dB in a multipath fading channel...|$|R
50|$|Apple's {{developer}} documentation {{states that}} applications {{should continue to}} <b>set</b> type <b>codes</b> and optionally <b>set</b> creator <b>codes.</b> If either already exists, applications should preserve them. Furthermore, creator codes are used in document binding prior to the file extension alone.|$|R
5000|$|The Challenge uses a {{board set}} {{known as the}} POWERpath-2 board <b>set,</b> <b>code</b> named [...] "Everest". The boards that make up this board set are the IP19, IP21, IP25 CPU boards, the MC3 memory board and the IO4 POWERchannel-2 {{interface}} board.|$|R
40|$|Reproducibility of {{experiments}} is {{considered as one}} of the main principles of the scientific method. Recent developments in data and computation intensive science, i. e. e-Science, and state of the art in Cloud computing provide the necessary components to preserve data <b>sets</b> and re-run <b>code</b> and software that create research data. The Executable Paper (EP) concept uses state of the art technology to include data <b>sets,</b> <b>code,</b> and software in the electronic publication such that readers can validate the presented results. In this paper we present how to advance current state of the art to preserve, data <b>sets,</b> <b>code,</b> and software that create research data, the basic components of an execution platform to preserve long term compatibility of EP, and we identify a number of issues and challenges in the realization of EP. © 2011 Published by Elsevier Ltd...|$|R
5000|$|Finally, {{there were}} {{exclusive}} sets, consisting of promotional cards {{associated with a}} particular expansion set released through venues such as conventions and tournaments. The exclusives sets are listed here {{rather than in the}} table below. The 5 exclusive <b>sets</b> (with <b>set</b> <b>codes</b> in parentheses) were: ...|$|R
40|$|Gaussian {{processes}} are powerful, yet analytically tractable models for supervised learning. A Gaussian process {{is characterized by}} a mean function and a covariance function (kernel), which are determined by a model selection criterion. The functions to be compared do not just differ in their parametrization but in their fundamental structure. It is often not clear which function structure to choose, for instance to decide between a squared exponential and a rational quadratic kernel. Based on the principle of approximation <b>set</b> <b>coding,</b> we develop a framework for model selection to rank kernels for Gaussian process regression. In our experiments approximation <b>set</b> <b>coding</b> shows promise to become a model selection criterion competitive with maximum evidence (also called marginal likelihood) and leave-one-out cross-validation...|$|R
5000|$|<b>Code</b> <b>Set</b> Manager: [...] Helps {{drive the}} {{organization}} of user defined reference data and <b>Code</b> <b>Sets</b> across an enterprise.|$|R
40|$|We examine <b>sets</b> of <b>codes</b> {{such that}} certain {{properties}} are invariant under {{the choice of}} oracle {{from a range of}} possible oracles and establish a connection between such codes and Medvedev reductions. In examing the complexity of such <b>sets</b> of universal <b>codes,</b> we prove completeness results at various levels of the arithmetic hierarchy as well as two general theorems for obtaining Π_ 1 ^ 1 -completeness for <b>sets</b> of universal <b>codes.</b> Among other corollaries, we show that the <b>set</b> of <b>codes</b> for Medvedev reductions of bi-immune sets to DNC functions is Π_ 1 ^ 1 -complete. Comment: 13 page...|$|R
5000|$|Although <b>code</b> <b>set</b> C uses one {{code symbol}} to {{represent}} two digits, {{it does not}} always produce a more compact <b>code</b> than <b>code</b> <b>sets</b> A or B. Using <b>code</b> <b>set</b> C saves one symbol per two digits, but costs a mode-shift symbol to enter and exit the set. Thus, it only worth using if there are enough consecutive digits. For example, encoding the string [...] "X00Y" [...] with <b>code</b> <b>set</b> A or B requires 7 code symbols (...) , while using <b>code</b> <b>set</b> C for the [...] "00" [...] {{would result in a}} code 8 symbols long (...) [...]|$|R
40|$|A primary {{security}} {{challenge of}} the mobile agent paradigm is that of protecting the data carried by a mobile agent. With {{the growing popularity of}} e-commerce applications that use software agents, the protection of mobile agent data has become imperative. To that end, we evaluate the performance of four methods that protect the data integrity of mobile agents. While some techniques have been proposed in the literature, there has previously been no experimental study comparing the various alternatives. The set of integrity mechanisms that we investigate includes existing approaches known as the Partial Result Authentication Codes (PRACs), Hash Chaining, and <b>Set</b> Authentication <b>Code</b> methods, as well as a technique of our own design, which we refer to as the Modified <b>Set</b> Authentication <b>Code</b> method. The Modified <b>Set</b> Authentication <b>Code</b> method addresses several limitations of the <b>Set</b> Authentication <b>Code</b> method. The performance experiments were run using the DADS mobile agent system, for which we designed a Data Integrity Module. The experimental results showed that our Modified <b>Set</b> Authentication <b>Code</b> technique performed comparably to the <b>Set</b> Authentication <b>Code</b> method, showing some improvement in the key generation times and agent size. In this paper, we compare the trade-offs between security features and performance for all four methods and identify the niche that each technique could fill...|$|R
40|$|International audienceIn this paper, {{we present}} an {{efficient}} Mgorithm of maximals superpositions of two digital <b>sets</b> <b>coded</b> in grids. It represents {{the basis of}} an application which controls the printing quality of industrial products labels, in which we consider a grid as the representation of a character...|$|R
