68|34|Public
50|$|KMouth - <b>Speech</b> <b>synthesiser.</b>|$|E
5000|$|... #Caption: [...] Computer and <b>speech</b> <b>synthesiser</b> housing used by Stephen Hawking in 1999 ...|$|E
50|$|Like its {{successor}} UoSAT-2 {{it carried}} a CCD {{camera and a}} Digitalker <b>speech</b> <b>synthesiser,</b> and transmitted telemetry data on a 145.826 MHz beacon at 1200 baud using asynchronous AFSK.|$|E
50|$|One of {{the most}} {{attractive}} features of structured documents {{is that they can}} be reused in many contexts and presented in various ways on mobile phones, TV screens, <b>speech</b> <b>synthesisers,</b> and any other device which can be programmed to process them.|$|R
40|$|The author {{presents}} an experiment which critically examines whether {{current state of}} the art text to <b>speech</b> <b>synthesisers</b> and web based VRML style graphics can be utilised to provide a multimodal persona presence in less than ideal user conditions. Audio visual anthropomorphised representations of online agents are becoming more common and their use in commercial systems has to be examined in order to ascertain whether multimodal processes such as the McGurk effect may limit their use. An experiment is outlined that is based around a web tele-shopping service with the users placed under adverse stress through information overload. The ability of the participants to successfully complete their task is objectively measured and conclusions are drawn on the suitability of current day <b>speech</b> <b>synthesisers</b> and VRML modelling for persona based human computer interface output. ...|$|R
5000|$|Florian Schneider - album concept, cover, electronics, mixing, production, recording, <b>speech</b> synthesis, <b>synthesiser,</b> vocoder ...|$|R
50|$|Ghoti {{is used to}} test speech synthesizers. The Speech! allophone-based <b>speech</b> <b>synthesiser</b> {{software}} for the BBC Micro was tweaked to pronounce ghoti as fish. Examination of the code reveals the string GHOTI used to identify the special case.|$|E
5000|$|Weebl and Bob. In their clip Balance, Stephen Hawking flies {{across the}} screen in his buggy and the various {{characters}} play around with his <b>speech</b> <b>synthesiser</b> against his will, making it say strange things, such as [...] "Ive wet my pants".|$|E
50|$|Lightwriters are text-to-speech devices - {{the person}} who cannot speak types {{a message on the}} keyboard, and this message is {{displayed}} on two displays, one facing the user and a second outfacing display for the communication partner(s). A <b>speech</b> <b>synthesiser</b> is also used to provide speech output.|$|E
40|$|This paper {{introduces}} {{a new concept}} in <b>speech</b> <b>synthesisers</b> by constructing devices and methods by which they can be operated in real-time. Further development of this concept may lead to an improvement in the conversational capabilities of people with ‘speech communicators’. This paper outlines the current limitations of such systems and then describes the methods used to give the user real-time control of the vocal synthesis device. 1...|$|R
40|$|The {{issue of}} {{representing}} speech rhythm is understood {{in this paper}} as the search for relevant primary parameters {{that will allow the}} formalisation of speech rhythm. Current <b>speech</b> <b>synthesisers</b> show that phonological models are not satisfactory with respect to the modelling of speech rhythmicity. Our analysis indicates that this may be in part related to the formalisation of rhythmic representation. Based on the observation of other communicative systems facing the problem of representing rhythm, parameters are described for representing speech rhythmic structures...|$|R
50|$|The kits were {{a perfect}} vehicle to sell Maplin's {{components}} and {{the articles in}} Electronics published full circuit diagrams, parts lists and technical articles with full instructions for building and setting up the projects. All this happened in the early 1980s, and the sudden boom in home computer ownership spawned by manufacturers such as Sinclair, Commodore International and Atari created opportunities for Maplin to create home build project kits to plug into these computers such as <b>speech</b> <b>synthesisers,</b> memory expansion cards, extension keyboards, cables, and connectors.|$|R
5000|$|The [...] "Sinistar" [...] arcade game {{featured}} {{the sound of}} it laughing and saying [...] "I am complete" [...] after all the pieces were assembled, but sample playback was beyond the abilities of the standard BBC Micro and Electron sound hardware. The BBC Micro had a <b>speech</b> <b>synthesiser</b> chip available as an official add-on, featuring the voice of Kenneth Kendall, a well-known BBC newsreader, but it had a very limited vocabulary.|$|E
5000|$|From the QL, the OPD {{borrowed}} the 68008 CPU, ZX8301/8302 ULAs, 128 KB of RAM and dual Microdrives (re-engineered by ICL for greater reliability) {{but not the}} 8049 Intelligent Peripheral Controller. Unique to the OPD was a [...] "telephony module" [...] incorporating an Intel 8051 microcontroller (which also controlled the keyboard), two PSTN lines and a V.21/V.23 modem, plus a built-in telephone handset and a TI TMS5220 <b>speech</b> <b>synthesiser</b> (for automatic answering of incoming calls).|$|E
5000|$|Amebis from Kamnik {{is a major}} {{company in}} Slovenia {{in the field of}} {{language}} technologies. [...] Its current manager is Miro Romih. The company has published a number of machine-readable dictionaries and encyclopedic dictionaries (e.g. ASP(32) dictionaries), and developed spell checkers, grammar checkers, hyphenators and lemmatizers for Slovene, Serbian and Albanian languages. In co-operation with the Jožef Stefan Institute they have developed a <b>speech</b> <b>synthesiser</b> and screen reader Govorec (Speaker). They have also provided technical support for the largest text corpus of Slovene language, called FidaPLUS.|$|E
40|$|The {{quality of}} unit {{selection}} <b>speech</b> <b>synthesisers</b> depends significantly {{on the content}} of the speech database being used. In this paper a technique is introduced that can highlight mispronunciations and abnormal units in the speech synthesis voice database through the use of articulatory acoustic feature extraction to obtain an additional layer of annotation. A set of articulatory acoustic feature classifiers help minimise the selection of inappropriate units in the speech database and are shown to significantly improve the word error rate of a diphone <b>synthesiser.</b> Index Terms: <b>speech</b> synthesis, unit selection, articulatory acoustic feature extractio...|$|R
50|$|NVDA uses eSpeak as its {{integrated}} speech synthesizer. It {{also supports}} the Microsoft <b>Speech</b> platform <b>synthesiser,</b> ETI Eloquence and also supports SAPI synthesizers. Output to braille displays is supported officially from Version 0.6p3 onwards.|$|R
40|$|In {{many modern}} concatenative <b>speech</b> <b>synthesisers</b> the unit {{sequence}} used to synthesise each sentence is determined at runtime by a search algorithm seeking to optimise a multidimensional cost function. One of these costs is usually {{some form of}} spectral continuity cost, computed {{between the end of}} one segment and the start of the following segment, intended to ensure that the synthetic speech does not contain any unpleasant spectral discontinuities. This paper presents the results of listening tests conducted to evaluate the performance of several possible continuity measures. It also describes a new continuity measure developed at IBM which substantially out-performs all other measures tested. 1...|$|R
50|$|The {{satellite}} {{carries a}} Digitalker <b>speech</b> <b>synthesiser,</b> magnetometers, a CCD camera, a Geiger-Müller tube, and a microphone {{to detect the}} vibrations of micrometeoroid impacts. Like UoSAT-1 it transmits telemetry data on the VHF beacon at 1200 baud, using asynchronous AFSK, though now all analogue telemetry channels have failed; on an FM receiver the audio signal resembles the cassette data format of the contemporary BBC Micro computer. Actually it is a BASICODE signal, but no citation. Slight modulation had also been observed on the S band beacon.|$|E
50|$|Arpabet is a phonetic {{transcription}} code {{developed by}} Advanced Research Projects Agency (ARPA) {{as a part}} of their Speech Understanding Project (1971-1976). It represents each phoneme of General American English with a distinct sequence of ASCII characters. Arpabet has been used in several speech synthesizers, including Computalker for the S-100 (Altair) system, SAM for the Commodore 64, SAY for the Amiga and TextAssist for the PC and Speakeasy from Intelligent Artefacts (see ST_Robotics) which used the Votrax SC01 <b>speech</b> <b>synthesiser</b> IC. It is also used in the CMU Pronouncing Dictionary.|$|E
5000|$|... "Star" [...] is New Zealand band Stellar's ninth single, {{and their}} third single from their second album Magic Line. This single {{is the last}} of the band's to have a {{physical}} release. The single, though charting for five weeks would only reach a position of #40. The single featured two B-sides, the Sub Mariner Remix of Taken, as well as the new B-side We Go Out. This song would later be featured on the soundtrack to the New Zealand horror film The Locals in 2003. Furthermore. after five minutes of silence the single would feature some bonus material. The first bonus sound clip was of the song We Go Out being read by a <b>speech</b> <b>synthesiser</b> and the other being a short James Bond take-of, where Andrew Maclaren played the role of James Bond and Kurt Shanks the speaking role of Miss Moneypenny.|$|E
40|$|The {{importance}} of phase {{information in the}} perceptual quality of the speech signals is studied in this paper. Many <b>speech</b> <b>synthesisers</b> do not use the original phase information of the signals assuming their contribution is almost inaudible. The Relative Phase Shift (RPS) representation of the phase allows straightforward phase structure analysis, manipulation and resynthesis, and we use these features to do a comparative evaluation of some phase modifications usually found in speech models. The final intention {{of this study is}} to get an answer to the question of whether phases deserve elaborate models to get high quality synthetic speech, or their subtle effects justify overlooking them. Index Terms: phase perception, RPS, speech synthesis. 1...|$|R
40|$|Most <b>speech</b> <b>synthesisers</b> and recognisers for English {{currently}} use pronunciation lexicons {{in standard}} British or American accents, but as use of speech technology grows {{there will be}} more demand for the incorporation of regional accents. This paper describes the use of rules to transform existing lexicons of standard British and American pronunciations to a set of regional British and American accents. The paper briefly discusses some features describes of the regional accents in the project, and the framework used for generating pronunciations. Certain theoretical and practical problems are highlighted; for some of these, solutions are suggested, but it is shown that some difficulties cannot be resolved by automatic rules. However, although the method described cannot produce phonetic transcriptions with 100...|$|R
40|$|Abstract?Unit {{selection}} is a data-driven approach to speech synthesis that concatenates pieces of recorded speech {{from a large}} database {{in order to create}} novel sentences. Many corpora are available in the English language, including the Arctic database [1], which allows a user to create small, reliable <b>speech</b> <b>synthesisers</b> using only a small set of recorded sentences. Such resources for minority languages are scarce however, despite their increasing importance for the survival of such languages. This paper describes the current research in creating efficient Irish language corpora for speech synthesis. Corpus design techniques are discussed, in particular, two methods of data reduction that are applied to an aligned spoken corpus of Irish in order to create smaller, more efficient speech corpora...|$|R
5000|$|MC Hawking. The {{imaginary}} alter-ego for the [...] "theoretical physicist turned gangster-rapper", MC Hawking's songs parody Hawking's distinctive <b>speech</b> <b>synthesiser.</b> Song titles include [...] "E=MC Hawking" [...] ("I explode like a bomb/no one is spared/my {{power is}} my mass times {{the speed of}} light squared"), [...] "Fuck the Creationists" [...] ("Fuck the damn creationists I say it with authority/because kicking their punk asses be my paramount priority") and [...] "Entropy" [...] ("You down with entropy?") The success of the MC Hawking amongst internet users eventually led to a 'greatest hits' compilation CD entitled A Brief History of Rhyme (a play on Hawking's A Brief History of Time book title), featuring album artwork done by comic artist Tony Moore. Hawking himself is reported to have said that he is [...] "flattered, as it's a modern day equivalent to Spitting Image".|$|E
50|$|Meanwhile, PC Lesley May, {{who is on}} {{indefinite}} {{sick leave}} after suffered a magical attack that resulted in catastrophic facial injuries (see Rivers of London) returns to The Folly after her latest round of reconstructive surgery still wearing a therapeutic mask but now able to speak without using a <b>speech</b> <b>synthesiser.</b> Peter and Lesley receive instruction from Nightingale {{in the art of}} magical staff-making: splitting open his walking stick (dubbed a 'cad walloper' by the revenant Mister Punch). Using a hammer and chisel to split the wooden casing Nightingale reveals a pattern-welded iron core that he explains can, correctly forged using magic, acts as a magical reservoir that enables a practitioner to wield magical power at otherwise dangerous levels while avoiding 'thaumaturgical degradation' (inevitable severe and irreversible brain damage resulting from over-use of magic, a condition Peter has irreverently dubbed cauliflower brain syndrome - after cauliflower ear).|$|E
5000|$|The Organiser II {{also had}} an {{external}} device slot, into which various plug-in modules could be fitted, including a device that provided an RS232 port (called [...] "CommsLink"), thus enabling it to communicate with other devices or computers. This [...] "top slot" [...] also supported various other hardware additions, such as telephone dialers, a <b>speech</b> <b>synthesiser,</b> barcode reader and even a dedicated thermal printer. This latter was used by several banks as a counter-top exchange-rate calculator for some years. As {{it was easy to}} get hardware specifications, numerous bespoke devices were developed by small companies such as A/D converters and even an interface to the entire range of Mitutoyo measuring equipment, allowing it to be used in quality control for various car manufacturers. Later models in the Organiser II range offered other hardware improvements, with 4-line displays, and also models were introduced with 32, 64 and 96 KiB RAM.|$|E
40|$|In {{this paper}} we revisit some basic {{configuration}} choices of HMM based speech synthesis, such as waveform sampling rate, auditory frequency warping {{scale and the}} logarithmic scaling of F 0, {{with the aim of}} improving speaker similarity which is an acknowledged weakness of current HMM-based <b>speech</b> <b>synthesisers.</b> All of the techniques investigated are simple but, as we demonstrate using perceptual tests, can make substantial differences {{to the quality of the}} synthetic speech. Contrary to common practice in automatic speech recognition, higher waveform sampling rates can offer enhanced feature extraction and improved speaker similarity for speech synthesis. In addition, a generalized logarithmic transform of F 0 results in larger intra-utterance variance of F 0 trajectories and hence more dynamic and natural-sounding prosody...|$|R
40|$|Speech {{synthesis}} {{is increasingly}} developing as the computers {{are getting more}} powerful. In this work, the area of speech synthesis is introduced through a diphone and unit selection based synthesis methods. The basic architecture of the synthesizers, text analysis {{and the use of}} algorithms for digital signal processing are described. DTW algorithm was used as a tool for labelling phonemes from an audio signal using MFCC coefficients and LPC coding of the speech signal. Festival Speech Synthesis System with Edinburgh Speech Tools and Festvox documentation are used for building new languages. The characteristics of the Slovenian language are described and their usage in the Festival. We have successfully built two <b>speech</b> <b>synthesisers</b> and the results are evaluated using a MOS scale. ...|$|R
40|$|A novel articulatory speech {{production}} system which is stochastically trained from a pre-specified initialisation state is presented. The target positions {{for a set}} of pseudo-articulators and the mapping from these to output speech spectral vectors are jointly optimised using linearised Kalman filtering and an assembly of neural networks. The techniques used to initialise and train the system are described, and preliminary results when synthesising speech are demonstrated. INTRODUCTION Articulatory <b>speech</b> <b>synthesisers</b> model human <b>speech</b> dynamics and hence theoretically can produce very high quality speech waveforms with explicit timedomain modelling of co-articulation [8, 12, 15]. Two major problems confronting such systems are: ffl Specification of the sequence of articulator positions or vocal tract area functions corresponding to a given text. ffl Provision of an accurate model of the human vocal tract. The former is frequently achieved using an "inverse" model to map parametris [...] ...|$|R
5000|$|For his communication, Hawking {{initially}} {{raised his}} eyebrows to choose letters on a spelling card. But in 1986 he received a computer program called the [...] "Equalizer" [...] from Walter Woltosz, CEO of Words Plus, who had developed {{an earlier version of}} the software to help his mother-in-law, who also suffered from ALS and had lost her ability to speak and write. In a method he uses to this day, Hawking could now simply press a switch to select phrases, words or letters from a bank of about 2,500-3,000 that are scanned. The program was originally run on a desktop computer. However, Elaine Mason's husband, David, a computer engineer, adapted a small computer and attached it to his wheelchair. Released from the need to use somebody to interpret his speech, Hawking commented that [...] "I can communicate better now than before I lost my voice." [...] The voice he uses has an American accent and is no longer produced. Despite the availability of other voices, Hawking has retained this original voice, saying that he prefers it and identifies with it. At this point, Hawking activated a switch using his hand and could produce up to 15 words a minute. Lectures were prepared in advance and were sent to the <b>speech</b> <b>synthesiser</b> in short sections to be delivered.|$|E
5000|$|Gnuspeech was {{originally}} commercial software {{produced by the}} now-defunct Trillium Sound Research for the NeXT computer as various grades of [...] "TextToSpeech" [...] kit. Trillium Sound Research was a technology transfer spin-off company formed at the University of Calgary, Alberta, Canada, based on long-standing research in the computer science department on computer-human interaction using speech, where papers and manuals relevant to the system are maintained. The initial version in 1992 used a formant-based <b>speech</b> <b>synthesiser.</b> When NeXT ceased manufacturing hardware, the synthesizer software was completely re-written and also ported to NSFIP (NextStep For Intel Processors) using the waveguide approach to acoustic tube modeling based on the research at the Center for Computer Research in Music and Acoustics (CCRMA) at Stanford University, especially the Music Kit. The synthesis approach is explained in more detail in a paper presented to the American Voice I/O Society in 1995. [...] The system used the onboard 56001 Digital Signal Processor (DSP) on the NeXT computer and a Turtle Beach add-on board with the same DSP on the NSFIP version to run the waveguide (also known as the tube model). Speed limitations meant that the shortest vocal tract length {{that could be used}} for speech in real time (that is, generated at the same or faster rate than it was [...] "spoken") was around 15 centimeters, because the sample rate for the waveguide computations increases with decreasing vocal tract length. Faster processor speeds are progressively removing this restriction, an important advance for producing children's speech in real time.|$|E
40|$|THESIS 10297 The {{primary focus}} of this thesis was the {{optimisation}} of the cost function of a unit selection <b>speech</b> <b>synthesiser.</b> This was achieved by: [...] 1. investigating the signal processing techniques used to calculate numerical represen?tations of speech signals, [...] 2. developing an objective method of determining which numerical representation is suitable for a given signal, and [...] 3. modifying the cost function of a unit selection <b>speech</b> <b>synthesiser</b> by using a suitable numerical representation for different types of speech signals...|$|E
40|$|We {{discuss the}} use of an accent-independent keyword lexicon to {{synthesise}} speakers with different regional accents. The paper describes the system architecture and the transcription system used in the lexicon, and then focuses on the construction of word-lists for recording speakers. We illustrate by mentioning some of the features of Scottish and Irish English, which we are currently synthesising, and describe how these are captured by keyword synthesis. Keywords: lexicon, accents, regional pronunciation, synthesis 1. INTRODUCTION Different accents of English can have different pronunciations for the same word, for example 'bother' is ?#Ec#'#? in RP but ?#E$#'#S? in General American. <b>Speech</b> <b>synthesisers</b> that store their lexicons in the form of phonetic transcriptions need separate lexicons for different accents. Rather than using phonetic symbols, our lexicon contains pronunciations transcribed in terms of keywords based on [1]. Abstracting away from the phoneti [...] ...|$|R
5000|$|Benny {{is taken}} to prison, and Molly, unable to cope, sends Billy {{to live with his}} uncle David in Los Angeles, California, United States. Since Billy can hear radio waves in his head ("Radio Waves" [...] - track 1), he begins to explore the {{cordless}} phone, recognising its similarity to a radio. He experiments with the phone and is able to access computers and <b>speech</b> <b>synthesisers,</b> and learns to speak through them. He calls a radio station in L.A. named Radio KAOS and tells them of his life story about his brother being in jail ("Me or Him" [...] - track 3), about his sister-in-law not being able to cope and sending him to L.A. to live with his uncle Dave ("Sunset Strip" [...] - track 5), and about the closures of the mines ("Powers That Be" [...] - track 4).|$|R
40|$|We {{present a}} review of some {{recently}} developed techniques {{in the field of}} natural language processing. This area has witnessed a confluence of approaches which are inspired by theories from linguistics and those which are inspired by theories from information theory: statistical language models are becoming more linguistically sophisticated and the models of language used by linguists are incorporating stochastic techniques to help resolve ambiguities. We include a discussion about the underlying similarities between some of these systems and mention two approaches to the evaluation of statistical language processing systems. 1 Introduction Within the last decade, {{a great deal of attention}} has been paid to techniques for processing large natural language copora. The purpose of much of this activity has been to refine computational models of language so that the performance of various technical applications can be improved (e. g. speech recognisers [67], <b>speech</b> <b>synthesisers</b> [32], optica [...] ...|$|R
