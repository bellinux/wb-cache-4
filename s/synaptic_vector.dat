5|15|Public
40|$|A {{geometrical}} {{interpretation of}} the elementary constituents which make up perceptual patterns is proposed: if {{a number of different}} pattern-vectors lie approximately within the same plane in the pattern-vector space, those patterns can be interpreted as sharing a common constituent. Individual constituents are associated with individual planes of patterns: a pattern lying within an intersection of several such planes corresponds to a combination of several constituents. This interpretation can model patterns as hierarchical combinations of constituents that are themselves combinations of yet more elementary constituents. A neuron can develop transformation-invariances in its recognition-response by aligning its <b>synaptic</b> <b>vector</b> with one of the plane-normals: a pattern-vector's projection along the <b>synaptic</b> <b>vector</b> is then an invariant of all the patterns on the plane. In this way, discriminating detectors for individual constituents can self-organise through Hebbian adaptatio [...] ...|$|E
40|$|We {{developed}} a parallel strategy for learning optimally specific realizable rules by perceptrons, {{in an online}} learning scenario. Our result is a generalization of the Caticha–Kinouchi (CK) algorithm developed for learning a perceptron with a <b>synaptic</b> <b>vector</b> drawn from a uniform distribution over the N-dimensional sphere, so called the typical case. Our method outperforms the CK algorithm in almost all possible situations, failing only in a denumerable set of cases. The algorithm is optimal {{in the sense that}} it saturates Bayesian bounds when it succeeds...|$|E
40|$|Abstract. We {{developed}} a parallel strategy for learning optimally specific realizable rules by perceptrons, in an on-line learning scenario. Our {{result is a}} generalisation of the Caticha-Kinouchi (CK) algorithm developed for learning a perceptron with a <b>synaptic</b> <b>vector</b> drawn from a uniform distribution over the N-dimensional sphere, so called the typical case. Our method outperforms the CK algorithm in almost all possible situations, failing only in a denumerable set of cases. The algorithm is optimal {{in the sense that}} it saturates Bayesian bounds when it succeeds. PACS numbers: 89. 70. Eg, 84. 35. +i, 87. 23. Kg Submitted to: J. Phys. A: Math. Gen. Parallel strategy for optimal learning in perceptrons...|$|E
3000|$|There are N {{clusters}} in the grouping process, {{and hence}} N neurons {{required in the}} SOM model. For any neuron, the <b>synaptic</b> weight <b>vector</b> is denoted by: [...]...|$|R
5000|$|The {{most basic}} {{model of a}} neuron {{consists}} of an input with some <b>synaptic</b> weight <b>vector</b> and an activation function or transfer function inside the neuron determining output. This is the basic structure used in artificial neurons, which in a neural network often looks like ...|$|R
3000|$|When we add the {{dynamics}} of the <b>synaptic</b> conductances, the <b>vector</b> Θ of model parameters also includes τ [...]...|$|R
40|$|The {{dynamics}} of {{learning from examples}} in the K = 3 nonoverlapping committee machine with single presentation of examples is studied. The optimal algorithm, {{in the sense of}} mean generalization, is obtained from a variational analysis of the differential equations which describe the dynamics. The agreement of the theoretical predictions and the results of numerical simulations is excellent. The optimized dynamics has the extra advantage with respect to the nonoptimized cases in that it uncouples the differential equations which describe the evolution of the relevant parameters, i. e. the student-teacher overlap and the norm of the student <b>synaptic</b> <b>vector.</b> This in turn translates into the possibility of constructing useful practical optimized on-line algorithms which work optimally {{even in the absence of}} knowledge of the probability distribution of examples. For the optimal algorithm the generalization error decays as ¸ 0 : 88 ff Γ 1, the same nominal error as for the simple perceptr [...] ...|$|E
40|$|The {{domain of}} {{artificial}} neural networks has evolved rapidly {{during the last}} decade, and many research groups are presently working on new neuronal algorithms and investigating their potential for technological applications. The idea to use biologically inspired models to implement intelligent systems is issue {{from the fact that}} animals, through their adaptation to the environment, have evolved towards robust and reliable structures, well adapted to the imperfections or even the destruction of some of their cells. In addition, these structures are particularly well adapted to perception tasks. These properties arise from the large redundancy inherent to their massive parallelism. Many models have been validated with computers, but the sequential operation of the latter leads to prohibitive computing time. The implementation of new architectures, leading to hardware that is better suited to the parallelism of the models, is slowed both by the complexity of some digital operators, and by the huge number of interconnexions between cells in the analogue and digital domains. The goal of this thesis is the implementation of a neural network, using analogue integrated technologies, to evaluate the potential and the weaknesses of such implementations. The Kohonen network has been chosen as a basis for this exploratory work because of its relative simplicity. As a matter of fact, this is a non-supervised network, which greatly simplifies the interfaces with the outside world. Furthermore, methods for limiting the number of interconnexions were known, thus overcoming the inherent limitations of intrinsically two-dimensional VLSI technologies. The study begins with a brief recall of the Kohonen algorithm, followed by the description of an architecture adapted to the integration of the network by means of standard CMOS analogue VLSI technologies. Before looking at the design of circuits needed to implement the network, the effects of some inaccuracies inherent to analogue circuits on the behaviour of the algorithm are analysed qualitatively by means of simulations. This analysis is needed to set up the specifications of the circuits, which may sometimes be quite different from the specifications that are encountered in more classical domains of analogue electronics. Then, the various circuits used in the implementation of the network are described. A nonlinear network, made of transistors connecting the nearest neighbour cells, defies the topology of the network and is used to generate the learning neighbourhood. A Winner-Take-All circuit is used to select the neuron whose <b>synaptic</b> <b>vector</b> is closest to the input vector. The most important element is certainly the synapse. The latter memorizes and updates, according to the learning rule, the elementary information called synaptic weight. Long term storage of an analogue value requires special technologies (EEPROM) and the update of this value is slow and badly controlled. To overcome this drawback, a medium term memory has been developed that has a leakage corresponding to 0. 1 % of full scale per second. This retention time is sufficient to operate the network under continuous learning, and also sufficient to periodically read the synaptic weights. All the proposed circuits are analysed with respect to the requirements of the network, and most of them have been integrated and measured. In particular, measurements of the synapse, made on several chips, are in good accordance with the analytical previsions. Finally, an evaluation chip, including four neurons with three synapses each, has been integrated. This chip can be used to build a complete network containing up to a hundred or so neurons. Measurements of single chips demonstrate the feasibility of the system, despite some tactical errors that can be easily corrected for an eventual redesign of the chip...|$|E
40|$|We {{show that}} it is {{possible}} to relate the Support Vector Machine formalism to Hebbian Learning in the context of olfactory learning in the insect brain. Since neurons cannot have negative firing rates, two neurons and synaptic inhibition are required to encode a binary classification problem in a biologically realistic way. We show that the two neuron system with plausible Hebbian learning rules can be mapped to a large margin classifier. Two formalisms are analyzed: regular SVMs and the so-called inhibitory SVMs. The regularization term in regular SVMs brings the <b>synaptic</b> <b>vectors</b> of the two neurons close to each other, while the inhibitory SVM can bring them to 0 resembling the memory loss process in Hebbian learning. Based on the analogy to large margin classifiers we also predict the existence of a negative Hebbian leaning rule for negative reinforcement signals...|$|R
40|$|A fully-differential, CMOS {{implementation}} of a self-organizing, dual-synapse neuron with on-chip learning for real-time facial feature extraction is presented. The adaptation of the network follows Oja's learning rule and the <b>synaptic</b> weight <b>vector</b> is shown {{to adapt to the}} principal component vector of the set of two-dimensional input vectors. I. INTRODUCTION Principal component analysis (PCA) is a powerful tool for extracting features for face recognition. Mathematically, PCA is a method of calculating the eigenvector of an autocorrelation matrix with the largest eigenvalu...|$|R
40|$|This paper {{presents}} a novel hardware architecture for principal component analysis. The architecture {{is based on}} the Generalized Hebbian Algorithm (GHA) because of its simplicity and effectiveness. The architecture is separated into three portions: the weight vector updating unit, the principal computation unit and the memory unit. In the weight vector updating unit, the computation of different <b>synaptic</b> weight <b>vectors</b> shares the same circuit for reducing the area costs. To show the effectiveness of the circuit, a texture classification system based on the proposed architecture is physically implemented by Field Programmable Gate Array (FPGA). It is embedded in a System-On-Programmable-Chip (SOPC) platform for performance measurement. Experimental results show that the proposed architecture is an efficient design for attaining both high speed performance andlow area costs...|$|R
40|$|This report {{investigates the}} {{classification}} of power system states using an artificial neural network model, Kohonen's self-organizing feature map. The ultimate goal of this classification is to assess power system static security in real-time. Kohonen's self-organizing feature map is an unsupervised neural network which maps N-dimensional input vectors to an array of M neurons. After learning, the <b>synaptic</b> weight <b>vectors</b> exhibit a topological organization which represents {{the relationship between the}} vectors of the training set. This learning is unsupervised, which means that the number and size of the classes are not specified beforehand. In the application developed in this report, the input vectors used as the training set are generated by off-line load-flow simulations. The learning algorithm and the results of the organization are discussed...|$|R
40|$|It was {{reported}} (Kabashima and Shinomoto 1992) that estimators of a binary decision boundary show asymptotically strange behaviors when the probability model is ill-posed. We give a rigorous {{analysis of this}} phenomenon in a stochastic perceptron by using the estimating function method. A stochastic perceptron consists of a neuron which is excited depending on the weighted sum of inputs but its probability distribution form is unknown here. It is shown that there exists no p n-consistent estimator of the threshold value h, that is, no estimator h which converges to h {{in the order of}} 1 = p n as the number n of observations increases. Therefore, the accuracy of estimation is much worse in this semiparametric case with an unspecified probability function than in the ordinary case. On the other hand, it is shown that there is a p n-consistent estimator w of the <b>synaptic</b> weight <b>vector.</b> These results elucidate strange behaviors of learning curves in a semiparametric statistical model [...] . ...|$|R
40|$|In {{this paper}} {{we present a}} {{necessary}} and sufficient condition for global optimality of unsupervised Learning Vector Quantization (LVQ) in kernel space. In particular, we generalize the results presented for expansive and competitive learning for vector quantization in Euclidean space, to the general case of a kernel-based distance metric. Based on this result, we present a novel kernel LVQ algorithm with an update rule consisting of two terms: the former regulates the force of attraction between the <b>synaptic</b> weight <b>vectors</b> and the inputs: the latter, regulates the repulsion between the weights {{and the center of}} gravity of the dataset. We show how this algorithm pursues global optimality of the quantization error by means of the repulsion mechanism. Simulation results are provided to show the performance of the model on common image quantization tasks: in particular, the algorithm is shown to have a superior performance with respect to recently published quantization models such as Enhanced LBG and Adaptive Incremental LB...|$|R
40|$|The spike count {{distribution}} observed when recording from {{a variety}} of neurons in many different conditions has a fairly stereotypical shape, with a single mode at zero or close to a low average count, and a long quasi-exponential tail to high counts. Such a distribution has been suggested to be the direct result of three simple facts: the firing frequency of a typical cortical neuron is close to linear in the summed input current entering the soma, above a threshold; the input current varies on several time scales, both faster and slower than the window used to count spikes; and the input distribution at any time scale can be taken to be approximately normal. The third assumption is violated, for example, by associative learning, which generates correlations between the <b>synaptic</b> weight <b>vector</b> on the dendritic tree of a neuron, and the input activity vectors it is repeteadly subject to. We show analytically that, for a simple feedforward model, the normal distribution of [...] ...|$|R
40|$|Spike {{patterns}} in vivo are often incomplete or corrupted with noise that makes inputs to neuronal networks appear to vary although they may, in fact, be samples {{of a single}} underlying pattern or repeated presentation. Here we present a recurrent spiking neural network (SNN) model that learns noisy pattern sequences {{through the use of}} homeostasis and spike-timing dependent plasticity (STDP). We find that the changes in the <b>synaptic</b> weight <b>vector</b> during learning of patterns of random ensembles are approximately orthogonal in a reduced dimension space when the patterns are constructed to minimize overlap in representations. Using this model, representations of sparse patterns maybe associated through co-activated firing and integrated into ensemble representations. While the model is tolerant to noise, prospective activity and pattern completion differ in their ability to adapt in the presence of noise. One version of the model is able to demonstrate the recently discovered phenomena of preplay and replay reminiscent of hippocampal like behaviors...|$|R
40|$|It is {{believed}} that energy efficiency is an important constraint in brain evolution. As synaptic transmission dominates energy consumption, energy can be saved by ensuring {{that only a few}} synapses are active. It is therefore likely that the formation of sparse codes and sparse connectivity are fundamental objectives of synaptic plasticity. In this work we study how sparse connectivity can result from a synaptic learning rule of excitatory synapses. Information is maximised when potentiation and depression are balanced according to the mean presynaptic activity level and the resulting fraction of zero-weight synapses is around 50 %. However, an imbalance towards depression increases the fraction of zero-weight synapses without significantly affecting performance. We show that imbalanced plasticity corresponds to imposing a regularising constraint on the L 1 -norm of the <b>synaptic</b> weight <b>vector,</b> a procedure that is well-known to induce sparseness. Imbalanced plasticity is biophysically plausible and leads to more efficient synaptic configurations than a previously suggested approach that prunes synapses after learning. Our framework gives a novel interpretation to the high fraction of silent synapses found in brain regions like the cerebellum...|$|R
40|$|In this paper, the {{adaptive}} competitive learning (ACL) neural network algorithm is proposed. This neural network not only groups similar input feature vectors together but also determines the appropriate {{number of groups}} of these vectors. This algorithm uses a new proposed criterion {{referred to as the}} ACL criterion. This criterion evaluates different clustering structures produced by the ACL neural network for an input data set. Then, it selects the best clustering structure and the corresponding network architecture for this data set. The selected structure is composed of the minimum number of clusters that are compact and balanced in their sizes. The selected network architecture is efficient, in terms of its complexity, as it contains the minimum number of neurons. <b>Synaptic</b> weight <b>vectors</b> of these neurons represent well-separated, compact and balanced clusters in the input data set. The performance of the ACL algorithm is evaluated and compared with the performance of a recently proposed algorithm in the literature in clustering an input data set and determining its number of clusters. Results show that the ACL algorithm is more accurate and robust in both determining the number of clusters and allocating input feature vectors into these clusters than the other algorithm especially with data sets that are sparsely distributed...|$|R
40|$|This paper {{presents}} a proactive maintenance scheme {{for the detection}} and diagnosis of faults in electrical valves. In our case study, these actuators are used for controlling the flow in an oil distribution network. An embedded system implements self-organizing maps for the detection and classification of faults that lead to deviations either on torque, or on the valve opening position. For fault detection, the map is trained using a mathematical model devised for the electrical valve. For fault classification, training is performed by fault injection based on parameter deviations over this same mathematical model. In both cases, the maps store the energies of the torque and the opening position that are computed using the wavelet packet transform. Once the maps are trained, the embedded system is ready for on-line monitoring the actuator. During the on-line testing phase, the embedded system computes the best matching between an acquired input vector (current torque and position energies) and the <b>synaptic</b> weight <b>vector</b> of the trained map. This matching is quantified by computing the Euclidean distance between these vectors and guide the fault detection and classification steps. The complete scheme was prototyped using FPGAs. The results obtained for area, performance and memory requirements point out to a low cost, promising solution for embedding maintenance in electrical actuators...|$|R
40|$|The {{nervous system}} can be the target for various {{bacterial}} and viral infectious agents. Certain bacterial toxins {{have been found to}} impair specific proteins, such as proteins involved in transmitter release and in the regulation of the cytoskeleton. Viral infections can also cause specific disturbances in neuronal function, but less is known about their actions on the cellular level. The aim {{of the present study was}} to obtain further knowledge about the cellular actions of two types of neurotropic agents, clostridial toxins and enveloped RNA viruses, on central nervous system (CNS) neurons. As an experimental model, hippocampal neurons in primary culture were used. The neurons were subjected to toxin treatment or viral infection, and changes in cellular protein content were monitored by immunolabelling. The actions on synaptic transmission, or on ionic currents, were eximined by patch clamp whole-cell recordings, and relative changes of intracellular Ca 2 + concentration with Ca 2 + imaging. Certain presynaptic and viral proteins were overexpressed using the Semliki forest virus (SFV) vector. The following conclusions can be drawn from the present study: 1. Incubation of cultured neurons with tetanus toxin (TeTx), which cleaves the SNARE protein synaptobrevin, was found to block synaptic transmission completely. Botulinum neurotoxin A (BoNT/A) which cleaves the C- terminal of SNAP- 25, on the other hand, caused only a partial inhibition of the synaptic response even at high doses. This type of synaptic block could be overcome by high frequency stimulation. When full-length SNAP- 25 was overexpressed using the SFV <b>vector,</b> <b>synaptic</b> transmission was inhibited in a similar manner as after TeTx treatment, i. e. the inhibition could not be overcome by repetitive stimulation. Thus, it seems that the C-terminal of SNAP- 25 plays a specific role in setting the level of transmitter release. 2. Incubation with large Clostridial Cytotoxins (LCTs), which inactivate GTPases of the Ras (Rap, Rai, R- Ras, Ras) and Rho families (Rho, Rac, Cdc 42), inhibited synaptic transmission and modified the activity- dependent modulation. To examine the possible involvement of Rai, a dominant negative mutant of this GTPase, was overexpressed using the SFV vector. The Ral-overexpressing neurons exhibited an abnormal activitydependent facilitation of the synaptic response. These results provide evidence for an involvement of Ras- related non-Rab GTPases, including Rai, in presynaptic regulation. 3. Infections of cultures with mumps virus (RW) or a neuroadapted strain of influenza A virus (WSN/ 33), were found to affect the neuronal Ca 2 + homeostasis. Mumps virus reduced voltage-dependent Ca 2 + currents to the same degree in infected and non-infected neurons, probably due to a disturbed interaction between glial cells and neurons. Influenza A virus reduced Ca 2 + currents in infected neurons at an early time-point, presumably due to a direct effect on Ca 2 + channels. Later during infection Ca 2 + Currents were also reduced in non- infected neurons. At this late time-point, a substantial fraction of the cells in culture had died due to infection, and the surviving neurons also showed an increased cytosolic Ca 2 + concentration. 4. The influenza nucleoprotein (NP) is known to interact with actin. When NP was overexpressed with the SFV vector, it was targeted to dendritic spines, where it colocalized with actinin. This targeting to the postsynaptic element did not affect synaptic transmission. In summary, the study has provided further insight into the actions of clostridial toxins on presynaptic function, and shown that infection with enveloped RNA viruses affect somatic Ca 2 + homeostasis by direct and indirect mechanisms. It has also shown that the SFV vector is a useful tool for analysis of synaptic function...|$|R

