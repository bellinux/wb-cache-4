3|26|Public
40|$|The paper {{presents}} a laboratory setup {{based on a}} SIMOVERT MASTER- DRIVES MC Inverter from SIEMENS used for open and closed loop speed control of an induction machine. The platform allows four quadrant operation of the machine using both the classical V/Hz <b>scalar</b> <b>principle</b> and field oriented vector control principle. The rectifier unit consists of a classical three phase diode bridge and two high voltage capacitors {{in order to obtain}} a voltage source behavior of the converter. In order to obtain the four quadrant operation of the drive, several methods are investigated out of which one is proposed and implemented. The theoretical aspects of V/Hz <b>scalar</b> <b>principle</b> and field oriented vector control principle are better explained using the proposed experimental platform...|$|E
40|$|Let S be {{the sphere}} of {{dimension}} n- 1, n≥ 4. Let (π_λ) _λ∈ C be the <b>scalar</b> <b>principle</b> series of representations of the conformal group SO_ 0 (1,n), realized on C^∞(S). For λ = (λ_ 1,λ_ 2,λ_ 3) ∈ C^ 3, let Tri(λ) be the space of continuous trilinear forms on C^∞(S) × C^∞(S) × C^∞(S) which are invariant under π_λ_ 1 ⊗π_λ_ 2 ⊗π_λ_ 3. For each value of λ, the dimension of Tri(λ) is computed and a basis of Tri(λ) is described...|$|E
40|$|The study {{examined}} {{the extent to which}} digital revolution has affected the organizational structure of Nigerian banks. Twenty-five banks were selected for the study in south-western Nigeria. Interview was conducted for middle and top level managers and questionnaire was developed and administered to the other staff using a five-point Likert scale to determine the attitudes and opinions of the staff on the effects of digital revolution on the organizational structure of the banks. The mean was used as an indicator of central tendency for quantitative variables that have frequency distributions in the study. The study found that standard operating procedures, politics, culture, surrounding environment and management decisions were all affected by digital revolution. It affected the organizational balance of rights, privileges, obligations, responsibilities, and feelings that have been established {{over a long period of}} time. The revolution brought structural changes in the line and unit of command, the principles of span of control, unity of command, and <b>scalar</b> <b>principle</b> of graded chain of superiors in the studied banks. It encouraged flat organizations as decision making became more decentralized. It also altered the required skill and increased the perceived advantage of workers with computer engineering background. Authority relied on knowledge and competence and not on mere forma...|$|E
50|$|Scalar Chain (line of {{authority}} with peer level communication): The <b>scalar</b> chain <b>principle</b> contends that communication {{within the organization}} should only be one uninterrupted vertical flow of communication and {{any other type of}} communication should only occur in times of emergencies and when approved by a manager.|$|R
40|$|In this paper, {{we first}} give a vector-valued version of Brézis and Browder's <b>scalar</b> general <b>{{principle}}.</b> We then apply the vector-valued general principle {{to study a}} vector Ekeland's variational principle in a F-type topological space, which unifies and improves the corresponding vector-valued Ekeland's variational results in complete metric space. Department of Applied Mathematic...|$|R
40|$|A popular argument, {{generally}} {{attributed to}} Amdahl [1], is that vector and parallel architectures {{should not be}} carried to extremes because the scalar or serial portion of the code will eventually dominate. Since pipeline stages and extra processors obviously add hardware cost, a corollary to this argument is that the most cost-effective computer is one based on uniprocessor, <b>scalar</b> <b>principles.</b> For architectures that are both parallel and vector, the argument is compounded, making it appear that near-optimal performance on such architectures is a near-impossibility. A new argument is presented {{that is based on}} the assumption that program execution time, not problem size, is constant for various amounts of vectorization and parallelism. This has a dramatic effect on Amdahl’s argument, revealing that one can be much more optimistic about achieving high speedups on massively parallel and highly vectorized machines. The revised argument is supported by recent results of over 1000 times speedup on 1024 processors on several practical scientific applications [2]. 1...|$|R
40|$|We {{look at the}} {{fundamental}} equations of analytical dynamics from a different per-spective. We discuss an additional way of evaluating the approaches for deriving the equations of motion, and we show that all of {{the fundamental}} equations can be viewed as projections of the force and moment balances onto directions affected by the velocity variables. We re-classify the existing approaches into two parts: Those based on vector variational principles, and those based on <b>scalar</b> variational <b>principles.</b> We discuss the relative merits and disadvantages of these approaches. ...|$|R
40|$|The variational {{method is}} {{proposed}} {{to analyze the}} influence of the fabrication parameters on the performance of buried K+-Na+ ion-exchanged Er 3 +-Yb 3 + ions co-doped glass waveguide. The unknown parameters of the Hermite-Gaussian functions as the trial field distribution are determined based on the <b>scalar</b> variational <b>principle.</b> It is demonstrated that the results calculated in this paper agree with those measured in the experiment. The mode dimensions, the effective refractive index, and the overlap factor as the functions of the fabrication parameters are investigated. These results of the variational analysis are useful for the design and optimization of Er 3 +-Yb 3 + ions co-doped waveguides...|$|R
40|$|The {{equivalence}} of inertial and gravitational masses is {{a defining}} feature of general relativity. Here, we clarify {{the status of}} the equivalence principle for interactions mediated by a universally coupled scalar, motivated partly by recent attempts to modify gravity at cosmological distances. Although a universal scalar-matter coupling is not mandatory, once postulated, it is stable against classical and quantum renormalizations in the matter sector. The coupling strength itself is subject to renormalization of course. The <b>scalar</b> equivalence <b>principle</b> is violated only for objects for which either the graviton self-interaction or the scalar self-interaction is important [...] -the first applies to black holes, while the second type of violation is avoided if the scalar is Galilean-symmetric. Comment: 4 pages, 1 figur...|$|R
40|$|A {{underlying}} dynamical {{structure for}} both relativity and quantum theory— “superrelativity ” {{has been proposed}} on order to overcome the well known incompatibility between these theories. The relationship between curvature of spacetime (gravity) and curvature of the projective Hilbert space of pure quantum states is established as well. Key words: variational principle, projective Hilbert space, <b>scalar</b> field, equivalence <b>principle,</b> curvature of the space of pure quantum states, tangent fiber bundle, gauge field 1 Introduction. Abou...|$|R
40|$|AbstractThe equational {{properties}} of the iteration operation in Lawvere theories are captured by the notion of iteration theories axiomatized by the Conway identities together with a complicated equation scheme, the “commutative identity”. The first result of the paper shows that the commutative identity is implied by the Conway identities and the Scott induction principle formulated to involve only equations. Since the Scott induction principle holds in free iteration theories, we obtain a relatively simple first order axiomatization of the equational {{properties of}} iteration theories. We show, {{by means of an}} example that a simplified version of the Scott induction principle does not suffice for this purpose: There exists a Conway theory satisfying the <b>scalar</b> Scott induction <b>principle</b> which is not an iteration theory. A second example shows that there exists an iteration theory satisfying the scalar version of the Scott induction principle in which the general form fails. Finally, an example is included to verify the expected fact that there exists an iteration theory violating the <b>scalar</b> Scott induction <b>principle.</b> Interestingly, two of these examples are ordered theories in which the iteration operation is defined via least pre-fixed points...|$|R
40|$|At {{the start}} of the Internet era, a web page was mainly made up of written texts containing, every now and then, some hot words (i. e. hyperlinks) that took you to a related web page. So, you started reading {{from the top of the}} page and ended at the bottom of the same page. But with the {{development}} of increasingly visually-oriented programs (e. g. Adobe Flash), in the last 15 years, web pages have evolved greatly: they have become even more complex. Sophisticated animations, short videos, and interactive objects have often taken the place of written text. The real trick for readers and web analysts is in finding ways of keeping track of the complexity of web pages, which is where multimodal web page analysis comes in handy. With specific reference to the issue of climate change, one of the major social issues of the contemporary age, this paper reports on part of the research into websites and web film analysis and annotation carried out within the Living Knowledge Project (Baldry, 2010, 2011 a, 2011 b; Baldry, Coccetta, in press). In particular, Sections 2, 3, and 4 explore a multimodal model of web analysis inspired by <b>scalar</b> <b>principles</b> (Baldry, Thibault, 2006 a, 2006 b; Coccetta, 2011) and shows the kind of information researchers can gather when applying a scalar model to film clips and web pages. Section 5 briefly describes the climate change film corpus that the author has compiled for research purposes while Section 6 explores the concept of thematic system (Baldry, 2010; Baldry, O’Halloran, 2010; Baldry, Thibault, 2006 a) in relation to this corpus and, in particular, provides some examples of multimodal intertextual thematic formations (Baldry, Thibault, 2006 a: 55). Finally, Section 7 examines websites and web film annotation in relation to the McaWeb tools ([URL] developed as part of the Living Knowledge Project and reports on the benefits they bring to web genre analysis...|$|R
40|$|The <b>scalar</b> Huygens-Fresnel <b>Principle</b> {{describing}} {{the propagation of}} light is reformulated {{to take into account}} the vector nature of light and the associated directed electric and magnetic fields. A vector Huygens secondary source is developed in terms of the fundamental radiating units of electromagnetism: the electric and magnetic dipoles. The vector Huygens wavelets are incorporated into a computer model that calculates the resulting vector fields after light passes through a diffracting system by a wavefront reconstruction process similar to that originally proposed by Huygens himself in 1687. Fresnel and Fraunhofer diffraction patterns are computed for common apertures such as rectangles and circles where theoretical results are available for comparison and validation of the model. However, irregular apertures not easily described in closed mathematical form are studied as well. Both completely absorbing and infinitely conducting screens are considered as well as plane wave and spherical illumination...|$|R
40|$|The <b>scalar</b> Huygens–Fresnel <b>principle</b> is reformulated to {{take into}} account the vector nature of light and its {{associated}} directed electric and magnetic fields. Based on Maxwell’s equations, a vector Huygens secondary source is developed in terms of the fundamental radiating units of electromagnetism: the electric and magnetic dipoles. The formulation is in terms of the vector potential from which the fields are derived uniquely. Vector wave propagation and diffraction formulated in this way are entirely consistent with Huygens’s principle. The theory is applicable to apertures larger than a wavelength situated in dark, perfectly absorbing screens and for points of observation in the right half-space at distances greater than a wavelength beyond the aperture. Alternatively, a formulation in terms of the fields is also developed; it is referred to as a vector Huygens–Fresnel theory. The proposed method permits the determination of the diffracted electromagnetic fields along with the detected irradiance. © 2001 Optical Society of America OCIS codes: 260. 1960, 260. 2110, 260. 5430. 1...|$|R
40|$|The field {{equations}} {{associated with}} the Born-Infeld-Einstein action including matter are derived using a Palatini variational <b>principle.</b> <b>Scalar,</b> electromagnetic, and Dirac fields are considered. It is shown that an action can be chosen for the scalar field that produces field equations identical to the usual Einstein field equations minimally coupled to a scalar field. In the electromagnetic and Dirac cases the field equations reproduce the standard equations only to lowest order. The spherically symmetric electrovac equations are studied in detail. It is shown that the resulting Einstein equations correspond to gravity coupled to a modified Born-Infeld theory. It is also shown that point charges are not allowed. All particles must have a finite size. Mass terms for the fields are also considered. ...|$|R
40|$|The {{original}} Abelian U(1) Higgs {{model in}} flat spacetime is enlarged {{by the addition}} of one real scalar with a particular potential. It is then shown that, while maintaining the original masses of the vector boson and Higgs scalar, there exists a time-dependent homogeneous solution of the classical field equations, which corresponds to dynamical breaking of Lorentz invariance (DBLI). The same DBLI mechanism holds for the standard model enlarged {{by the addition of}} a real isosinglet scalar with an appropriate potential. The resulting DBLI with an assumed TeV energy scale would manifest itself primarily in the interactions of the two <b>scalar</b> particles. In <b>principle,</b> this DBLI could feed into the neutrino sector and give rise to a superluminal maximum velocity. Comment: 16 pages, v 5 : improved title, added subsection on nonstandard Higgs physic...|$|R
40|$|The {{theory of}} scalar gravity {{proposed}} by Nordström, and refined by Einstein and Fokker, provides a striking analogy to general relativity. In its modern form, scalar gravity appears as the low-energy {{effective field theory}} of the spontaneous breaking of conformal symmetry within a CFT, and is AdS/CFT dual to the original Randall-Sundrum I model, but without a UV brane. Scalar gravity faithfully exhibits several qualitative features of the cosmological constant problem of standard gravity coupled to quantum matter, and the Weinberg no-go theorem can be extended to this case as well. Remarkably, {{a solution to the}} scalar gravity cosmological constant problem has been proposed, where the key is a very small violation of the <b>scalar</b> equivalence <b>principle,</b> which can be elegantly formulated as a particular type of deformation of the CFT. In the dual AdS picture this involves implementing Goldberger-Wise radion stabilization where the Goldberger-Wise field is a pseudo-Nambu Goldstone boson. In quantum gravity however, global symmetries protecting pNGBs are not expected to be fundamental. We provide a natural six-dimensional gauge theory origin for this global symmetry and show that the violation of the equivalence principle {{and the size of the}} vacuum energy seen by scalar gravity can naturally be exponentially small. Our solution may be of interest for study of non-supersymmetric CFTs in the spontaneously broken phase. Comment: 22 page...|$|R
40|$|We {{try to use}} scale-invariance and the 1 /N {{expansion}} {{to construct}} a non-trivial 4 d O(N) scalar field model with controlled UV behavior and naturally light <b>scalar</b> excitations. The <b>principle</b> is to fix interactions at each order in 1 /N by requiring the effective action for arbitrary background fields to be scale-invariant. We find a line of non-trivial UV fixed-points in the large-N limit, parameterized by a dimensionless coupling. Nether action nor measure is scale invariant, but the effective action is. Scale invariance makes it natural to set a mass deformation to zero. The model has phases where O(N) invariance is unbroken or spontaneously broken. Masses of the lightest excitations above the unbroken vacuum are found. Slowly varying quantum fluctuations are incorporated at order 1 /N. We find the 1 /N correction to the potential, beta function of mass and anomalous dimensions of fields that preserve a line of fixed points for constant backgrounds. Comment: 32 page...|$|R
40|$|It is {{the main}} {{function}} of quantitative NDE to detect and to evaluate defects. Some {{of the most dangerous}} defects are cracks, especially cracks on or near surfaces. These cracks can be found by scattering ultrasonic waves from them (either bulk waves or surface waves), but up to now there is no theory (at least in the most interesting low-to-intermediate frequency region) which has been implemented to compute scattering from surface or near-surface cracks in 3 d. The purpose of the present report is to explain, via a simple <b>scalar</b> example, the <b>principles</b> of a general boundary-integral-representation method which has been used 1 to calculate scattering of waves of all polarizations by a 2 d surface or subsurface crack. The method is developed for bulk defects and cracks in a slab as well as in a half-space, and is straightforwardly applicable to 3 -dimensional problems as well as to 2 d ones...|$|R
40|$|We {{try to use}} scale-invariance and the large-N {{limit to}} find a non-trivial 4 d O(N) scalar field model with {{controlled}} UV behavior and naturally light <b>scalar</b> excitations. The <b>principle</b> is to fix interactions by requiring the effective action for space-time dependent background fields to be finite and scale-invariant when regulators are removed. We find a line of non-trivial UV fixed-points in the large-N limit, parameterized by a dimensionless coupling. They reduce to classical la phi^ 4 theory when hbar -> 0. For hbar non-zero, neither action nor measure is scale-invariant, but the effective action is. Scale invariance makes it natural to set a mass deformation to zero. The model has phases where O(N) invariance is unbroken or spontaneously broken. Masses of the lightest excitations above the unbroken vacuum are found. We derive a non-linear equation for oscillations about the broken vacuum. The interaction potential is shown to have a locality property at large-N. In 3 d, our construction reduces {{to the line of}} large-N fixed-points in |phi|^ 6 theory. Comment: 23 page...|$|R
40|$|The {{asymptotic}} method of post-Newtonian (PN) expansion for weak gravitational fields, recently developed, is {{compared with the}} standard method of PN expansion, in the particular case of a massive test particle moving along a geodesic line of a weak Schwarzschild field. First, {{the expression of the}} active mass in Schwarzschild's solution is given for a barotropic perfect fluid, both for general relativity (GR) and for an alternative, <b>scalar</b> theory. The <b>principle</b> of the {{asymptotic method}} is then recalled and the PN expansion of the active mass is derived. The PN correction to the active mass is made of the Newtonian elastic energy, augmented, for the scalar theory, by a term due to the self-reinforcement of the gravitational field. Third, two equations, both correct to first order, are derived for the geodesic motion of a mass particle: a " standard " one and an " asymptotic " one. Finally, the difference between the solutions of these two equations is numerically investigated in the case of Mercury. It is found that the asymptotic solution deviates from the standard one like the square of the time elapsed since the initial time. This may be cured by reinitializing the problem...|$|R
40|$|A {{general theory}} for {{predicting}} {{the distribution of}} scalar gradients (or concentration differences) in heterogeneous flows is proposed. The evolution of scalar fields is quantified from {{the analysis of the}} evolution of elementary lamellar structures, which naturally form under the stretching action of the flows. Spatial correlations in scalar fields, and concentration gradients, hence develop through diffusive aggregation of stretched lamellae. Concentration levels at neighbouring spatial locations result from a history of lamella aggregation, which is partly common to the two locations. Concentration differences eliminate this common part, and thus depend only on lamellae that have aggregated independently. Using this principle, we propose a theory which envisions concentration increments {{as the result of a}} deconstruction of the basic lamella assemblage. This framework provides analytical expressions for concentration increment probability density functions (PDFs) over any spatial increments for a range of flow systems, including turbulent flows and low-Reynolds-number porous media flows, for confined and dispersing mixtures. Through this deconstruction <b>principle,</b> <b>scalar</b> increment distributions reveal the elementary stretching and aggregation mechanisms building scalar fields. © 2017 Cambridge University Press. TLB acknowledges the support of the European Research Council (ERC) through the project ReactiveFronts (648377) and of the Agence Nationale de la Recherche (ANR) through the project Subsurface mixing and reaction (ANR- 14 -CE 04 - 0003). MD acknowledges the support of the European Research Council (ERC) through the project MHetScale (617511). EV acknowledges the Agence Nationale de la Recherche (ANR) for funding of the ANR-DFG grant TurbMix (ANR- 14 -CE 35 - 0031 - 01). Peer reviewe...|$|R
40|$|ResumenEl objetivo de este trabajo es contribuir al conocimiento del concepto de escala para la comprensión geográfica e {{integral}} de los problemas de socio-ambientales en México. Este objetivo se lleva a cabo mediante la comparación de las dimensiones escalares de las políticas territoriales de adaptación al cambio climático y del cambio de uso de suelo, dos procesos socio-ambientales que representan dos vertientes diferentes de análisis geográfico. El trabajo presenta los principales elementos que se han debatido en los últimos años en la literatura anglosajona sobre el concepto de escala, así como los diferentes elementos y dimensiones que lo componen: la extensión, la resolución, el nivel, la jerarquía, el problema de la unidad de área modificable y las falacias espaciales. Al aplicar dichos principios a la comparación entre dos problemas geográficos de naturaleza epistemológica diferente, se pone de manifiesto la importancia que tiene este concepto para el pensamiento geográfico y la necesidad de generar reflexiones sistemáticas {{en este sentido}} para la geografía que se produce en lengua española. Para la política de cambio climático, los resultados sugieren que la falta de integración conceptual y programática entre las políticas de los diferentes niveles, así como la relación concurrente entre ellas, genera un problema para producir resultados efectivos de adaptación. En relación con el cambio de uso del suelo, la visión escalar revela que las directas (próximas) e indirectas (subyacentes) operan en múltiples jerarquías; asimismo, sus consecuencias biofísicas, sociales y económicas se manifiestan en diferentes escalas de espacio y tiempo. AbstractThe aim of {{this paper}} is to contribute to the knowledge of the concept of scale for an integrated geographical understanding of socio-environmental problems in Mexico. This objective is accomplished by comparing the same scalar dimensions of two different problems: the first one is adaptation to climate change policies in Mexico, and the second is deforestation. This paper presents the main elements that have been discussed in recent years in the Anglo literature on the concept of scale, particularly the extent, resolution, level, hierarchy, the problem of modifiable area unit and spatial fallacies. The properties of geographical concepts emerge and can be observed according to the combination of scalar elements. The most fundamental <b>scalar</b> <b>principles</b> discussed in the field refer to identify the combination of elements in which it is possible to observe each geographical phenomena's variability, characteristics and properties. One of the most relevant problems of the scalar thinking refers to the Modifiable Areal Unit Problem (MAUP), which stems from the data aggregation; the relevant values represented for each spatial unit relate to each other in different ways according to how they organized within a hierarchical structure. Data aggregation affects how the variability and heterogeneity of a phenomenon can be observed, given that the change of scale may show or hide specific properties of the dataset in a change of resolution. This problem is relevant for the inferences that stem from the data, given that individualistic or ecological fallacies may emerge if the dataset characteristics are not correctly interpreted in terms of representation, similarity or heterogeneity. Applying these principles to the comparison between two different geographical problems of different epistemological nature, we show the importance of this concept for the geographical thought; the comparison highlights the need to generate systematic reflections in this regard for the Geography produced in Spanish language. Regarding the socio-environmental problems addressed, the climate change adaptation policies in Mexico show a lack of conceptual and programmatic integration in different levels; the lack of an adequate concurrent relationship between them creates a problem to generate effective results for adaptation. We identify three policy levels (global, national and local), in which we briefly examine the relevant policy instrument and actor(s) that negotiate, design and/ or implement it in each level. For the international level we briefly present the role of the Mexican government on the negotiation and adoption of the Kyoto Protocol principles and goals; for the national level, we examine the approach and jurisdiction of the Special Program of Climate Change and the related juridical field; for the local level, we discuss the Municipal Climate Action Programmes, their design and scope, as well as the lack density that has prevented these instrument to influence other policy levels. This section discusses the hierarchies between these levels, the extension (jurisdiction) under which each of them are relevant and the importance of each scalar level for visualizing the main characteristics of the different adaptation policies. Regarding deforestation and land use change, the scale analysis reveals that might be direct (proximate) and indirect (underlying) and operate on multiple levels; also, its consequences are manifested in different scales of space and time. The spatial heterogeneity of land use change reveals the combination of biophysical and social, economic and political conditions, so the deforestation rate and causes change with observation scale. The implication of scale is that important land use processes could remain undetected, thus not monitored by traditional tools and aggregated land use categories typically applied. On the other hand, the choice of the time scale could undetect economic or social processes that change from year to year, which obscure the underlying causes of deforestation. It is therefore necessary to see change in land use with a view hierarchies...|$|R
40|$|PostScript, 12 pages, 3 {{figures in}} 2 {{additional}} PS files. Accepted {{for publication in}} Nuovo Cimento B. V 3 : a few typos in V 2, plus one sentence (p. 10), corrected. V 2 : the cure outlined in V 1, to remedy a numerical shortcoming of the asymptotic method, has been implemented. Result: in the investigated case of a test particle in a weak Schwarzschild field, the standard and asymptotic methods of PN expansion are definitely equivalent, also numericallyThe asymptotic method of post-Newtonian (PN) expansion for weak gravitational fields, recently developed, is compared with the standard method of PN expansion, in the particular case of a massive test particle moving along a geodesic line of a weak Schwarzschild field. First, {{the expression of the}} active mass in Schwarzschildś solution is given for a barotropic perfect fluid, both for general relativity (GR) and for an alternative, <b>scalar</b> theory. The <b>principle</b> of the asymptotic method is then recalled and the PN expansion of the active mass is derived. The PN correction to the active mass is made of the Newtonian elastic energy, augmented, for the scalar theory, by a term due to the self-reinforcement of the gravitational field. Third, two equations, both correct to first order, are derived for the geodesic motion of a mass particle: a s̈tandardöne and an äsymptoticöne. Finally, the difference between the solutions of these two equations is numerically investigated in the case of Mercury. The asymptotic solution deviates from the standard one like the square of the time elapsed since the initial time. This is due to a practical shortcoming of the asymptotic method, which is shown to disappear if one reinitializes the asymptotic problem often enough. Thus, both methods are equivalent in the case investigated. In a general case, the asymptotic method seems more natural...|$|R
40|$|An {{intriguing}} {{feature of}} scalar-tensor theories is {{the emergence of}} different metrics, e. g. when matter is minimally coupled to a metric non-trivially related to the Einstein metric g[mu,nu] used to construct the Ricci <b>scalar.</b> Strong equivalence <b>principle</b> constraints then typically force permissible “many-metric” scenarios to reduce to a bimetric picture. In this thesis we first aim to construct the most general bimetric relation, where the two metrics are related by a single scalar degree of freedom [phi] and its derivatives. This results in the disformal metric relation and a natural extension which we present. In the context of primordial structure formation, disformal bimetric theories give rise to “general single field inflation” models of the P(X, [phi]) type. We investigate the perturbative properties of such disformally motivated models. The focus is on non-Gaussian phenomenology and we establish non-Gaussian fingerprints for inflationary single field models and non-inflationary bimetric setups, also going beyond the slow-roll approximation. Furthermore we show that various dualities exist between disformally motivated P(X, [phi]) theories and higher-form models. As an explicit example we use the dual picture to compute non-Gaussian signals for three-form theories. In the context of dark energy/modified gravity, we show that the conformal subgroup of the general disformal relation can be used to construct a generalized “derivative” Chameleon setup. We present and investigate this setup and study its phenomenology. Finally we show that a natural extension of the disformal relation can generate Galileon solutions from a single geometrical invariant - the first Lovelock term - in four dimensions. As such the over-arching theme of this thesis is to show that the disformal bimetric picture and its extensions present us with a geometrical understanding of scalar-tensor/single field models. That they provide a unified description of large classes of scenarios linked to accelerated space-time expansion and also point us towards new physically motivated setups. Imperial Users onl...|$|R
40|$|The {{asymptotic}} method of post-Newtonian (PN) expansion for weak gravitational fields, recently developed, is {{compared with the}} standard method of PN expansion, in the particular case of a massive test particle moving along a geodesic line of a weak Schwarzschild field. First, {{the expression of the}} active mass in Schwarzschild's solution is given for a barotropic perfect fluid, both for general relativity (GR) and for an alternative, <b>scalar</b> theory. The <b>principle</b> of the {{asymptotic method}} is then recalled and the PN expansion of the active mass is derived. The PN correction to the active mass is made of the Newtonian elastic energy, augmented, for the scalar theory, by a term due to the self-reinforcement of the gravitational field. Third, two equations, both correct to first order, are derived for the geodesic motion of a mass particle: a "standard" one and an "asymptotic" one. Finally, the difference between the solutions of these two equations is numerically investigated in the case of Mercury. The asymptotic solution deviates from the standard one like the square of the time elapsed since the initial time. This is due to a practical shortcoming of the asymptotic method, which is shown to disappear if one reinitializes the asymptotic problem often enough. Thus, both methods are equivalent in the case investigated. In a general case, the asymptotic method seems more natural. Comment: PostScript, 12 pages, 3 figures in 2 additional PS files. Accepted for publication in Nuovo Cimento B. V 3 : a few typos in V 2, plus one sentence (p. 10), corrected. V 2 : the cure outlined in V 1, to remedy a numerical shortcoming of the asymptotic method, has been implemented. Result: in the investigated case of a test particle in a weak Schwarzschild field, the standard and asymptotic methods of PN expansion are definitely equivalent, also numericall...|$|R
40|$|We {{show the}} {{computational}} procedure of the renormalization of the electroweak chiral Lagrangian (the 4 D Higgsless standard model), and provide one simplified {{version of its}} one-loop renormalization group equations, which we demonstrate its simplicity and reliability. By analyzing the solutions of the one-loop renormalization group equations of the electroweak chiral Lagrangian, we study the parameter space of the precision test parameters at ultraviolet cutoff with the current low energy experimental constraints. We find that {{the region of the}} permitted parameter space can be greatly amplified (1 to 2 order) by the radiative corrections of those undetermined anomalous couplings. 05. 10. Cc, 11. 10. Hi, 12. 15. Lk The effective Lagrangian method is a bottom-to-up and model-independent approach to understand experimental data [1 – 3]. The electroweak chiral Lagrangian (EWCL) method (it was also called as the non-linear gauged sigma model or 4 D Higgsless standard model in some references) [4 – 6] is an effective field method which is expected to describe the electroweak physics without a Higgs in the standard model, since till now there is no direct evidence for the existence of such a <b>scalar</b> field. In <b>principle,</b> the EWCL with low energy fermion can explain all the experiment data [7] below the energy scale µ = 200 GeV. We can also use this theoretical framework to understand physics beyond µ = 200 GeV before we will find new particles and new resonances in the future experiments. However, {{from the fact that the}} unitarity of the scattering amplitude of the longitudinal vector bosons (according to the equivalent theorem [8], this corresponds to the Goldstone scatterings) will be violated, we can deduce that new physics must be below 4 π v, a few TeV [9]. In principle, before a new resonance is found at experiments, with its anomalous couplings as parameters the EWCL can describe all possible effects of the new physics to the electroweak bosonic sector, either the strong couplings and weak couplings origi...|$|R
40|$|We {{show the}} {{computational}} procedure of the renormalization of the electroweak chiral Lagrangian (the 4 D Higgsless standard model), and provide one simplified {{version of its}} one-loop renormalization group equations, which we demonstrate its simplicity and reliability. By analyzing the solutions of the one-loop renormalization group equations of the electroweak chiral Lagrangian, we study the parameter space of the precision test parameters at ultraviolet cutoff with the current low energy experimental constraints. We find that {{the region of the}} permitted parameter space can be greatly amplified (1 to 2 order) by the radiative corrections of those undetermined anomalous couplings. The effective Lagrangian method is a bottom-to-up and model-independent approach to understand experimental data [1 – 3]. The electroweak chiral Lagrangian (EWCL) method (it was also called as the non-linear gauged sigma model or 4 D Higgsless standard model in some references) [4 – 6] is an effective field method which is expected to describe the electroweak physics without a Higgs in the standard model, since till now there is no direct evidence for the existence of such a <b>scalar</b> field. In <b>principle,</b> the EWCL with low energy fermion can explain all the experiment data [7] below the energy scale µ = 200 GeV. We can also use this theoretical framework to understand physics beyond µ = 200 GeV before we will find new particles and new resonances in the future experiments. However, {{from the fact that the}} unitarity of the scattering amplitude of the longitudinal vector bosons (according to the equivalent theorem [8], this corresponds to the Goldstone scatterings) will be violated, we can deduce that new physics must be below 4 π v, a few TeV [9]. In principle, before a new resonance is found at experiments, with its anomalous couplings as parameters the EWCL can describe all possible effects of the new physics to the electroweak bosonic sector, either the strong couplings and weak couplings origi...|$|R
40|$|Cet article présente le fonctionnement des rapports d'autorité entre des cadres subalternes et leurs supérieurs dans la section psychiatrique d'un hôpital montréalais. In {{light of}} the {{evolution}} of administrative theories, the author analyses how the authority System works at the lower levels as well as the effect on this System of judicial decisions in a hospital of the Montréal area. Decisions are {{explained by the fact that}} the lower level managers asked for unionization. The Québec Labour Court was favorable to their demand, but on appeal by management the Superior Court was unfavorable. In this hospital, authority relationships between lower level managers and superiors do not work very well: managers have either the right of delegated authority, or are accountable only to their superior — that is, they can give orders and expect obedience, but not enforce obedience. Observed variation in the authority structure, presumable, rests on an arbitrary basis, putting lower level managers in a situation of role conflicts with regard either to their common superior or to themselves. According to a theorist of administrative sciences, it is sometimes better for a lower level manager to be accountable rather than have the right of delegated authority. The latter form of authority when given to a lower level manager, may lead to misuse or abuses. On the other hand, the other form of authority, accountability, can be a source of greater equity for subordinates who acquire by the very fact an appeal System. Administrative theories admit that authority or influence can be distributed throughout the organization according to different patterns, because accountability is not equivalent to delegated authority. The Law, however, continues to assume that authority is unitary and homogeneous. In the present case, the two forms of authority exist at the same hierarchical level and may be interpreted as a sign of evolution of the authority system. At the same time, nevertheless, this can be a source of confusion. That may explain why Judge Brière of the Québec Labour Court maintained that lower level managers did not shore any delegation of authority and therefore could be unionized, while Judge Benoît of the Superior Court argued that since the criteria of delegation was not writ-ten in the Law, he was required to annul that judgement and consequently refuse unionization. The decision of the Superior Court will in fact confirm the existing System of authority that is hybrid and arbitrary. According to existing law as interpreted by the Superior Court, however, an employee has only to be a member of the direction of an organization to be excluded from any bargaining unit. More precisely, a lower level manager who is accountable to his superior is part of direction as much as another lower level manager who has the right of delegated authority. Thus, authority relationships take on a legalistic colouring that can only favour the confusion observed in the actual authority system, and reconfirm the traditional conception of authority. Another serious deficiency should also be underlined: the article reveals that, as a result of this Superior Court decision, the <b>scalar</b> administrative <b>principle</b> prevails over the group work method. It is, nevertheless, very important to use the group work method in the kind of institution considered here. By the same token, is it not reasonable to assume that the actual goal of this institution is partially com-promised?In the context studied, the development of the administrative authority system follows in the wake of jurisprudence. From an historical point of view, though, the distinction between delegated authority and accountability was in part a union gain...|$|R

