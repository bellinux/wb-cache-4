37|2647|Public
50|$|Exact maximum {{likelihood}} learning {{in this model}} is intractable, but approximate learning of DBMs {{can be carried out}} by using a variational approach, where mean-field inference is used to estimate data-dependent expectations and an MCMC based <b>stochastic</b> <b>approximation</b> <b>procedure</b> is used to approximate the model’s expected sufficient statistics.|$|E
40|$|The <b>stochastic</b> <b>approximation</b> <b>procedure</b> with {{series of}} delayed {{observations}} is investigated. The procedure is formed by modifying the Robbins–Monro <b>stochastic</b> <b>approximation</b> <b>procedure</b> {{to be applicable}} {{in the presence of}} series of delayed observations. The modified procedure depends on a new base concerning the relation between service time of the series and service times of its components. Two loss systems are introduced for application to the proposed procedure. This new situation can be applied to increase the production of items in many fields such as biological, medical, life time experiments, and some industrial projects, where items are realized after random time delays. The efficiency of the procedure is computed. Our proposal is general and we expect that it can be applied to any other <b>stochastic</b> <b>approximation</b> <b>procedure...</b>|$|E
40|$|AbstractConsistent, {{asymptotically}} {{efficient and}} asymptotically normal stepwise estimators are given for a subclass of the uniparametric and multiparametric exponential families. The estimators are derived {{by using the}} Robbins-Monro <b>stochastic</b> <b>approximation</b> <b>procedure</b> with certain families of random variables arising from the normalized log-likelihood. Considered in detail are three multivariate normal examples where the maximum likelihood estimators are not tractable...|$|E
40|$|In {{this paper}} we propose a wide class of {{truncated}} <b>stochastic</b> <b>approximation</b> <b>procedures</b> with moving random bounds. While we believe that the proposed class of procedures will find its way to a wider range of applications, the main motivation is to accommodate applications to parametric statistical estimation theory. Our class of <b>stochastic</b> <b>approximation</b> <b>procedures</b> has three main characteristics: truncations with random moving bounds, a matrix valued random step-size sequence, and dynamically changing random regression function. We establish convergence and consider several examples to illustrate the results...|$|R
40|$|We study {{asymptotic}} behaviour of <b>stochastic</b> <b>approximation</b> <b>procedures</b> with three main characteristics: truncations with random moving bounds, a matrix valued random step-size sequence, and a dynamically changing random regression function. In particular, {{we show that}} under quite mild conditions, <b>stochastic</b> <b>approximation</b> <b>procedures</b> are asymptotically linear in the statistical sense, that is, they can be represented as weighted sums of random variables. Therefore, a suitable form of the central limit theorem {{can be applied to}} derive asymptotic distribution of the corresponding processes. The theory is illustrated by various examples and special cases. Comment: Portions of this text were previously a part of arXiv: 1508. 01902 v 1 which has been divided into two papers for publication {{at the request of the}} journal. The first part is now arXiv: 1508. 01902 v...|$|R
40|$|The {{analysis}} of asymptotic behaviour of <b>stochastic</b> <b>approximation</b> <b>procedures</b> rests {{heavily on the}} use of martingale limit theory, although explicit recognition of this situation is notable for its absence in the literature. This point is emphasized and in illustration a martingale iterated logarithm result is used to obtain strong convergence results of iterated logarithm type for the basic Robbins-Monro and Kiefer-Wolfowitz <b>procedures.</b> <b>stochastic</b> <b>approximation</b> martingales iterated logarithm law Robbins-Monro procedure Kiefer-Wolfowitz procedure...|$|R
40|$|Consistent, {{asymptotically}} {{efficient and}} asymptotically normal stepwise estimators are given for a subclass of the uniparametric and multiparametric exponential families. The estimators are derived {{by using the}} Robbins-Monro <b>stochastic</b> <b>approximation</b> <b>procedure</b> with certain families of random variables arising from the normalized log-likelihood. Considered in detail are three multivariate normal examples where the maximum likelihood estimators are not tractable. Estimation Exponential Family Stochastic Approximation...|$|E
40|$|Abstract. The semimartingale <b>stochastic</b> <b>approximation</b> <b>procedure,</b> namely, the Robbins–Monro type SDE is {{introduced}} which naturally includes both generalized stochastic approximation algorithms with martingale noises and recursive parameter estimation procedures for statistical models associated with semimartingales. General results concerning the asymptotic behaviour {{of the solution}} are presented. In particular, the conditions ensuring the convergence, rate of convergence and asymptotic expansion are established. The results concerning the Polyak weighted averaging procedure are also presented. Contents 0. Introduction [...] ...|$|E
40|$|The {{modified}} <b>stochastic</b> <b>approximation</b> <b>procedure</b> xi+ 1 = xi + αig(xi + ξi+ 1), x 0 = ζ (1) is considered. Here {αj} is {{the sequence}} of positive numbers, {ξn} is a sequence of martingale-differences, function g is twice differentiable, ug(u) ≤ 0 for u = 0, ζ is the initial value. Results on the almost-sure boundedness and the exponen-tial stability of procedure (1) are obtained. The theorem of the convergence of nonnegative semimartingale has been applied. ...|$|E
40|$|AbstractThe {{analysis}} of asymptotic behaviour of <b>stochastic</b> <b>approximation</b> <b>procedures</b> rests {{heavily on the}} use of martingale limit theory, although explicit recognition of this situation is notable for its absence in the literature. This point is emphasized and in illustration a martingale iterated logarithm result is used to obtain strong convergence results of iterated logarithm type for the basic Robbins–Monro and Kiefer–Wolfowitz procedures...|$|R
40|$|A {{family of}} {{one-dimensional}} linear <b>stochastic</b> <b>approximation</b> <b>procedures</b> in continuous time where processes of errors are Gaussian martingales is considered. Under some general assumptions the asymptotic behaviour of these procedures is studied concerning strong consistency, {{rate of convergence}} and limiting law of involved estimates and costs. At first some asymptotic results for Gaussian martingales, associated quadratic functionals and functions with finite variation are discussed. asymptotic normality estimate Gaussian martingale rate of convergence <b>stochastic</b> <b>approximation</b> strong consistency Wiener process...|$|R
40|$|AbstractA {{family of}} {{one-dimensional}} linear <b>stochastic</b> <b>approximation</b> <b>procedures</b> in continuous time where processes of errors are Gaussian martingales is considered. Under some general assumptions the asymptotic behaviour of these procedures is studied concerning strong consistency, {{rate of convergence}} and limiting law of involved estimates and costs. At first some asymptotic results for Gaussian martingales, associated quadratic functionals and functions with finite variation are discussed...|$|R
40|$|The {{main goal}} {{of this paper is}} to develop {{accuracy}} estimates for stochastic programming problems by employing stochastic approximation (SA) type algorithms. To this end we show that while running a Mirror Descent <b>Stochastic</b> <b>Approximation</b> <b>procedure</b> one can compute, with a small additional effort, lower and upper statistical bounds for the optimal objective value. We demonstrate that for a certain class of convex stochastic programs these bounds are comparable in quality with similar bounds computed by the sample average approximation method, while their computational cost is considerably smaller...|$|E
40|$|AbstractA {{generalization}} of Robbins-Monro stochastic approximation {{is presented in}} the paper. It is shown that, if disturbances satisfy a sort of generalized law of large numbers then an appropriate <b>stochastic</b> <b>approximation</b> <b>procedure</b> converges almost surely or only in probability, depending on what kind of law of large numbers (strong or weak) is satisfied by disturbances. In that sense theorems presented in the paper generalize Robbins-Monro stochastic approximation schemes, because the law of large numbers can be satisfied, as is well-known, by sequences of dependent random variables. On the other hand, as theorem of Gladishev (a generalized version of Robbins-Monro theorem) can be obtained from the results presented in the paper (see Theorem 10), one can consider this paper as the one providing new proofs for different versions of stochastic approximation. The proofs of the theorems of the paper are different than usual proofs of <b>stochastic</b> <b>approximation</b> <b>procedure.</b> In particular, they are not based on the Martingale convergence theorem. Roughly speaking the proofs exploit the analogy between the stochastic approximation procedures of Robbins-Monro versions and deterministic numerical iterative procedures seeking zeros of the system of nonliner equations. As the results of the paper were thought to be applied to estimation of parameters of discrete stochastic processes (so called identification) special notation has been introduced. This notation is believed to be useful for the above purpose...|$|E
40|$|Let Yn be a {{sequence}} of dependent random variables and Phi_n (cdot, cdot) be {{a sequence}} of Borel functions. Let θ_n be a solution of the equation M_n(x) = 0 for each n geqq 1, where M_n(x) = mathrmE Phi_n(x, Y_n). A Robbins-Monro type <b>stochastic</b> <b>approximation</b> <b>procedure</b> X_n+ 1 = X_n - a_n Phi_n(X_n, Y_n) is considered for estimating θ_n for n sufficiently large. Under some assumptions about a_n,θ_n,Y_n and Phi_n(cdot, cdot) which may not include the fundamental condition mathrmE[Phi_n(X_n, Y_n) mid X_ 1, cdots, X_n] = M_n(X_n) a. s., the a. s. convergence and in mean-square convergence of mid X_n - θ_n mid to zero are studied...|$|E
40|$|This paper {{deals with}} the {{application}} of "large deviation" theory {{to the analysis of}} <b>stochastic</b> <b>approximation</b> <b>procedures.</b> The approach allows to get new results in the asymptotical behaviour of stochastic procedures under very mild assumption about the "noise". The paper contains a short but illuminative survey of these results together with some new author's findings. For applications the last section seems to be interesting presenting some new ideas in multiobjective optimization...|$|R
40|$|The {{paper is}} {{concerned}} with <b>stochastic</b> <b>approximation</b> <b>procedures</b> having three main characteristics: truncations with random moving bounds, a matrix valued random step-size sequence, and a dynamically changing random regression function. We study convergence and rate of convergence. Main results are supplemented with corollaries to establish various sets of sufficient conditions, with the main emphases on the parametric statistical estimation. The theory is illustrated by examples and special cases. Comment: 30 page...|$|R
40|$|We {{study the}} {{recently}} introduced Cross-Entropy (CE) method for optimization, an iterative random sampling {{approach that is}} based on sampling and updating an underlying distribution function over the set of feasible solutions. In particular, we propose a systematic approach to investigate the convergence and asymptotic convergence rate for the CE method through a novel connection with the well-known <b>stochastic</b> <b>approximation</b> <b>procedures.</b> Extensions of the approach to stochastic optimization will also be discussed. ...|$|R
40|$|Abstract. This work {{develops}} {{a class of}} stochastic optimization algorithms. It aims to provide numerical procedures for solving thresholdtype optimal control problems. The main motivation stems from applications involving optimal or suboptimal hedging policies, for example, production planning of manufacturing systems including random demand and stochastic machine capacity. The proposed algorithm is a constrained <b>stochastic</b> <b>approximation</b> <b>procedure</b> that uses randomdirection finite-difference gradient estimates. Under fairly general conditions, the convergence of the algorithm is established {{and the rate of}} convergence is also derived. A numerical example is reported to demonstrate the performance of the algorithm. Key Words. Gradient estimates, random directions, stochastic approximations, manufacturing. 1...|$|E
40|$|Abstract. Stochastic {{approximation}} is {{a framework}} unifying many random iterative algorithms occurring in a {{diverse range of}} applications. The stability of the process {{is often difficult to}} verify in practical applications and the process may even be unstable without additional stabilisation techniques. We study a <b>stochastic</b> <b>approximation</b> <b>procedure</b> with expanding projections similar to Andradóttir [Oper. Res. 43 (2010) 1037 – 1048]. We focus on Markovian noise and show the stability and convergence under general conditions. Our framework also incorporates the possibility to use a random step size sequence, which allows us to consider settings with a non-smooth family of Markov kernels. We apply the theory to stochastic approximation expectation maximisation with particle independent Metropolis-Hastings sampling. 1...|$|E
40|$|In this paper, we are {{interested}} in the almost sure convergence of randomly truncated stochastic algorithms. In their pioneering work Chen and Zhu [Chen, H., Zhu, Y., 1986. <b>Stochastic</b> <b>Approximation</b> <b>Procedure</b> with Randomly Varying Truncations. In: Scientia Sinica Series. ] required that the family of the noise terms is summable to ensure the convergence. In our paper, we present a new convergence theorem which extends the already known results by making vanish this condition on the noise terms [...] a condition which is quite hard to check in practice. The aim of this work is to prove an almost sure convergence result for randomly truncated stochastic algorithms under assumptions expressed independently of the algorithm paths so that the conditions can easily be verified in practical applications. ...|$|E
40|$|The {{adaptive}} {{processes of}} growth modeled by a generalized urn scheme {{have proved to}} be an efficient tool for the analysis of complex phenomena in economics, biology and physical chemistry. They demonstrate non-ergodic limit behavior with multiple limit states. There are two major sources of complex feedbacks governing these processes: nonlinearity (even local, which is caused by nondifferentiability of the functions driving them) and multiplicity of limit states stipulated by the nonlinearity. We suggest an analytical approach for studying some of the patterns of complex limit behavior. The approach is based on conditional limit theorems. The corresponding limits are, in general, not infinitely divisible. We show that convergence rates could be different for different limit states. The rates depend upon the smoothness (in neighborhoods of the limit states) of the functions governing the processes. Since the mathematical machinery allows us to treat a quite general class of recursive stochastic discrete-time processes, we also derive corresponding limit theorems for <b>stochastic</b> <b>approximation</b> <b>procedures.</b> The theorems yield new insight into the limit behavior of <b>stochastic</b> <b>approximation</b> <b>procedures</b> in the case of nondifferentiable regression functions with multiple root...|$|R
40|$|Abstract-Many <b>stochastic</b> <b>approximation</b> <b>procedures</b> {{result in}} a sto-chastic {{algorithm}} of the form 1 hk+l = h k +- (bk- A k h k), k for all IC = 1, 2, 3 [...] . Here, { b k, IC = 1, 2, 3, [...] . } is a Rd-valued process, { Ak, IC = 1, 2. 3 [...] } is a symmetric, positive semidefinite Redxd-valued process, and { h k k = 1, 2, 3, [...] } is a sequence of stochastic estimates which hopefully con-verges t...|$|R
40|$|We study a {{recursive}} algorithm {{which includes the}} multidimensional Robbins-Monro and Kiefer-Wolfowitz processes. The assumptions on the disturbances are weaker than the usual assumption that they be a martingale difference sequence. It is shown that the algorithm can be represented as a weighted average of the disturbances. This representation {{can be used to}} prove asymptotic results for <b>stochastic</b> <b>approximation</b> <b>procedures.</b> As an example, we approximate the one dimensional Kiefer-Wolfowitz process almost surely by Brownian motion, and as a byproduct obtain a law of the iterated logarithm...|$|R
40|$|The {{problem of}} {{optimization}} with noisy measurements {{is common in}} many areas of engineering. The only available information is the noise-corrupted value of the objective function at any chosen point in the parameter space. One well-known method for solving this problem is the <b>stochastic</b> <b>approximation</b> <b>procedure.</b> In this paper we consider an adaptive random search procedure, based on the reinforcement-learning paradigm. The learning model presented here generalizes the traditional model of a learning automaton [Narendra and Thathachar, Learning Automata: An Introduction, Prentice Hall, Englewood Cliffs, 1989]. This procedure requires a lesser number of function evaluations at each step compared to the stochastic approximation. The convergence properties of the algorithm are theoretically investigated. Simulation results are presented to show the efficacy of the learning method...|$|E
40|$|We prove {{convergence}} with probability one of {{a multivariate}} Markov <b>stochastic</b> <b>approximation</b> <b>procedure</b> of the Robbins-Monro type with several roots. The argument exploits convergence of the corresponding system of ordinary differential equations to its stationary points. If the points are either linearly stable or linearly unstable, we prove convergence with probability 1 of the procedure to a random vector whose distribution concentrates {{on the set of}} stable stationary points. This generalizes for procedures with several roots the approach suggested by L. Ljung for processes with a single root. Along with stochastic approximation processes as such, the result can be applied to generalized urn schemes and stochastic models of technological and economic dynamics based on them, in particular, evolutionary games with incomplete information...|$|E
40|$|A limit theorem for the Robbins-Monro <b>stochastic</b> <b>approximation</b> <b>procedure</b> is proved in {{the case}} of a non-smooth {{regression}} function. Using this result a conditional limit theorem is given for the case when the regression function has several stable roots. The first result shows that the rate of convergence for the stochastic approximation-type procedures (including Monte-Carlo optimization algorithms and adaptive processes of growth being modelled by the generalized urn scheme) decreases as the smoothness increases. The second result demonstrates that {{in the case}} of several stable roots, there is no convergence rate for the procedure as whole, but for each of stable roots there exists its specific rate of convergence. The latter allows to derive several conceptual results for applied problems in biology, physical chemistry and economics which can be described by the generalized urn scheme...|$|E
40|$|In {{the general}} area of {{optimization}} under uncertainty, {{there are a}} large number of applications which require finding the `best' values for a set of control variables or parameters and for which the only data available consist of measurements prone to random errors. <b>Stochastic</b> <b>approximation</b> provides a method of handling such noise or randomness in data; it has been widely studied in the literature and used in several applications. In this paper, we examine a new class of <b>stochastic</b> <b>approximation</b> <b>procedures</b> which are based on carefully controlling the number of observations or measurements taken before each computational iteration. This method, which we refer to as Sampling-controlled <b>Stochastic</b> <b>Approximation,</b> has advantages over standard <b>stochastic</b> <b>approximation</b> such as requiring less computation and the ability to handle bias in estimation. We address the growth rate required of the number of samples and prove a general convergence theorem for this new <b>stochastic</b> <b>approximation</b> method [...] . ...|$|R
30|$|The {{issue of}} {{controlling}} that data processing {{in an experiment}} results not affected {{by the presence of}} outliers is relevant for statistical control and learning studies. Learning schemes should thus be tested for their capacity of handling outliers in the observed training set so to achieve reliable estimates with respect to the crucial bias and variance aspects. We describe possible ways of endowing neural networks with statistically robust properties by defining feasible error criteria. It is convenient to cast neural nets in state space representations and apply both Kalman filter and <b>stochastic</b> <b>approximation</b> <b>procedures</b> in order to suggest statistically robustified solutions for on-line learning.|$|R
40|$|This work is {{our first}} attempt in {{establishing}} the connections between evolutionary computation algorithms and <b>stochastic</b> <b>approximation</b> <b>procedures.</b> By treating evolutionary algorithms as recursive stochastic procedures, we study both constant gain and decreasing step size algorithms. We formulate the problem in a rather general form, supply the sufficient conditions for convergence (both with probability one, and in the weak sense). Among other things, our approach reveals the natural connection of the discrete iterations and the continuous dynamics (ordinary differential equations, and/or stochastic differential equations). We hope that this attempt will open up a new domain for further research and lead to in depth understanding of the underlying algorithms...|$|R
40|$|Stock exchanges are {{modelled}} as nonlinear feedback systems {{where the}} plant dynamics {{is defined by}} known stock market regulations but the actions of agents are unknown. It is assumed though that each agent submits transaction requests according to his/her beliefs on the price dynamics and his/her behavior. The latter may be loss-aversive, rational or risk seeking. The action of the agents may contain a random element, thus we get a non-linear stochastic feedback system. The market is in equilibrium when {{the actions of the}} agents reinforce their beliefs on the price dynamics. Assuming that an AR(k) predictor is used for prediction of the price process, a <b>stochastic</b> <b>approximation</b> <b>procedure</b> for finding market equilibrium is described. The proposed procedure is analyzed using the theory of Benveniste, Métivier and Priouret, and simulation results are presented...|$|E
40|$|We {{investigate}} the asymptotic behavior of {{one version of}} the so-called two-armed bandit algorithm. It {{is an example of}} <b>stochastic</b> <b>approximation</b> <b>procedure</b> whose associated ODE has both a repulsive and an attractive equilibrium, at which the procedure is noiseless. We show that if the gain parameter is constant or goes to 0 not too fast, the algorithm does fall in the noiseless repulsive equilibrium with positive probability, whereas it always converges to its natural attractive target when the gain parameter goes to zero at some appropriate rates depending on the parameters of the model. We also elucidate the behavior of the constant step algorithm when the step goes to 0. Finally, we highlight the connection between the algorithm and the Polya urn. An application to asset allocation is briefly described. © Institute of Mathematical Statistics, 2004...|$|E
40|$|We {{propose a}} novel {{adaptive}} MCMC algorithm named AMOR (Adaptive Metropolis with Online Relabeling) for efficiently simulating from permutation-invariant targets occurring in, for example, Bayesian analysis of mixture models. An {{important feature of}} the algorithm is to tie the adaptation of the proposal distribution to {{the choice of a}} particular restriction of the target to a domain where label switching cannot occur. The algorithm relies on a <b>stochastic</b> <b>approximation</b> <b>procedure</b> for which we exhibit a Lyapunov function that formally defines the criterion used for selecting the relabeling rule. This criterion reveals an interesting connection with the problem of optimal quantifier design in vector quantization which was only implicit in previous works on the label switching problem. In benchmark examples, the algorithm turns out to be fastconverging and efficient at selecting meaningful non-trivial relabeling rules to allow accurate parameter inference. ...|$|E
40|$|The {{issue of}} {{controlling}} that data processing {{in an experiment}} results not affected {{by the presence of}} outliers is relevant for statistical control and learning studies. Learning schemes should thus be tested for their capacity of handling outliers in the observed training set so to achieve reliable estimates with respect to the crucial bias and variance aspects. We describe possible ways of endowing neural networks with statistically robust properties by defining feasible error criteria. It is convenient to cast neural nets in state space representations and apply both Kalman filter and <b>stochastic</b> <b>approximation</b> <b>procedures</b> in order to suggest statistically robustified solutions for on-line learning. </p...|$|R
40|$|The Robbins-Monro {{procedure}} is investigated for various choices of step sizes, {{including those of}} the form c / nlog n, c> 0. Rates of convergence using this and other choices are compared to rates obtained using cn[beta], for. Of all choices considered, it is seen that a procedure of Fabian's (1968) yields the best rate of convergence. <b>stochastic</b> <b>approximation</b> Robbins-Monro <b>procedure</b> step size...|$|R
40|$|Abstract: Accuracy for {{main class}} of Simultaneous Perturbation <b>Stochastic</b> <b>Approximation</b> (SPSA) <b>procedures</b> is being researched. The model of {{observation}} {{is considered to}} be one of the most general among SPSA research. The power of moments of expectation for which the estimates of the procedure do converge is lowerized from 2 to 1 (not including lower bound). The conditions for the convergence are presented, with additional generalisations made about noise and trial perturbation properties...|$|R
