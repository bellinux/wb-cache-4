1461|809|Public
5|$|Firewatch {{runs on the}} Unity game engine. Ng {{disapproved of}} the tools for {{creating}} trees, and therefore hand-modeled the 23 kinds of trees that would be placed within the game 4,600 times. A custom <b>shader</b> was also employed to produce more stylized and simplified foliage. The in-game fire lookout towers were built in accordance with government specifications, utilizing standard lumber size, after Ng's first attempt was unsatisfactory.|$|E
5|$|Age of Empires III {{builds on}} and {{introduces}} new features to the Age of Mythology engine, called Bang! Engine. One new {{feature is the}} inclusion of the Havok physics simulation middleware engine on the Windows version and the similar PhysX engine on Mac OS X. This means that many events such as building destruction and tree falls will not be pre-created animations, but will be calculated according to the physics engine, in an innovation for the series. Other graphical features of the game include bloom lighting and support for pixel <b>shader</b> 3.0.|$|E
5|$|In 2003 the United Kingdom {{was a major}} {{contributor}} to the invasion of Iraq, sending a force of over 46,000 military personnel. The British Army controlled southern Iraq, maintained a peace-keeping presence in Basra. All British troops were withdrawn from Iraq by 30 April 2009, after the Iraqi government refused to extend their mandate. One hundred seventy-nine British military personnel died in Iraqi operations. The British Armed Forces returned to Iraq in 2014 as part of Operation <b>Shader</b> to counter the Islamic State (ISIL).|$|E
50|$|Shadows, texture mapping, surface <b>shaders,</b> light <b>shaders,</b> volume <b>shaders,</b> {{displacement}} <b>shaders,</b> all pixel filters, generate {{image to}} file (RGB & RGBA), delayed Read Archive.|$|R
5000|$|Pixel <b>shaders</b> + Vertex <b>shaders.</b> Since Kepler, Unified <b>shaders</b> are used.|$|R
50|$|Similar to Radeon X1000 series, {{texturing}} {{units and}} raster operators {{are separated from}} pixel <b>shaders.</b> Chrome 20 has 4 vertex <b>shaders,</b> 8 pixel <b>shaders,</b> 4 texturing units, 4 raster operators.|$|R
5|$|A PlayStation 3 {{version of}} Oblivion was {{released}} on March 20, 2007 in North America, and April 27, 2007 in Europe, following delays similar to those for the Xbox 360 release. The PlayStation 3 release was touted for its improvement over the graphics of the PC and Xbox 360 versions, {{although some of the}} improved <b>shader</b> routines optimized for the PlayStation 3 release were set to be ported over to the other releases through patches. A plan to distribute content through downloads paid by micropayment was initially met with criticism by customers due to its alleged low value, but later releases—at a reduced price, and with more content—proved more popular.|$|E
5|$|Developers {{working on}} Oblivion focused on {{providing}} a tighter storyline, with fewer filler quests and more developed characters. The developers {{sought to make}} information in the game world more accessible to players, making the game easier {{to pick up and}} play. Oblivion features improved AI (courtesy of Bethesda's proprietary Radiant AI), improved physics (courtesy of the Havok physics engine), and impressive graphics, taking advantage of advanced lighting and <b>shader</b> routines like high dynamic range rendering (HDR) and specular mapping. Bethesda developed and implemented procedural content creation tools in the creation of Oblivions terrain, leading to landscapes that are more complex and realistic than those of past titles, with less of a drain on Bethesda's staff.|$|E
5|$|During Oblivions development, Bethesda {{concentrated}} on creating {{a system with}} a more realistic storyline, believable characters, and meaningful quests than had {{been done in the}} past. In comparison with previous titles in the series, the game features improved artificial intelligence thanks to the use of Bethesda proprietary Radiant A.I. software, and enhanced physics facilitated by the Havok physics engine. The graphics take advantage of advanced lighting and <b>shader</b> routines such as high dynamic range rendering (HDR) and specular mapping. Bethesda developed and implemented procedural content creation tools in the building of Oblivions terrain, leading to the expedited creation of landscapes that are more complex and realistic than in past titles.|$|E
50|$|AGAL (Adobe Graphics Assembly Language) - A {{language}} for writing <b>Shaders</b> (programs {{that run on}} the GPU) such {{that they can be}} converted to OpenGL GLSL <b>Shaders</b> and DirectX HLSL <b>Shaders</b> by Flash Player.|$|R
5000|$|... 1Pixel <b>shaders</b> : Vertex <b>shaders</b> : Texture mapping units : Render output units ...|$|R
50|$|In Q1 2017, AMD CodeXL Analyzer was {{replaced}} by Radeon GPU Analyzer (RGA), maintaining backward compatibility. Radeon GPU Analyzer CLI is an offline compiler and a performance analysis tool for DirectX <b>shaders,</b> OpenGL <b>shaders,</b> Vulkan <b>shaders</b> and OpenCL kernels.|$|R
5|$|The film used Technicolor's Oz process during post-production. This is {{a partial}} silver {{retention}} on the interpositive, similar to bleach bypass, {{which will be}} used to lend to the sense of detachment from the modern world McG was looking for. Industrial Light & Magic developed <b>shader</b> programs to make the desaturated lighting of the CGI realistic and well-integrated to the on-set footage. The filmmakers consulted with many scientists about the effects of an abandoned world and nuclear winter. McG cited , the original Star Wars trilogy and Children of Men, as well as the novel The Road, as his visual influences. He instructed his cast to read the latter as well as Do Androids Dream of Electric Sheep? Like Children of Men, McG would storyboard scenes so that it would be edited together to resemble a seamless, continuous shot. It took two weeks to film a two-minute shot of Connor getting caught up in a bombing on the Skynet base where he discovers plans for the T-800.|$|E
5|$|Ninety {{percent of}} the animation-related work was done in Hyderabad; the {{remaining}} ten percent was completed in the United States. In an Indo-Asian News Service interview, Draper said he collaborated with thirteen experts and a large team of animators to design the fly. Because the film's fly's eyes comprise 80 percent of its face, Rajamouli felt they could make it expressive; he used the 1986 Pixar American short film Luxo Jr. for inspiration. The output of the first team of animators, using the reference material prepared, was unsatisfactory and Rajamouli reworked the fly's detailing. Using a powerful lens, the film team conducted an arduous photographic shoot of unconscious flies in a bottle stored in a refrigerator. After enlarging the details, Rajamouli made cosmetic changes to the fly's face {{to make it look}} appealing onscreen. A new team including Draper, three concept artists, three modellers, two <b>shader</b> designers, two hair and fur designers, three riggers and several animators, designed the animated fly in two months. Its head and fur were designed after shaping its body and wings. The fly was refined daily using clay models to expedite the process. The animators found the sequences between Sudeep and the fly much more difficult to execute because the latter had to express emotions only though its slender arms rather than its face.|$|E
25|$|In 2002, Microsoft {{released}} DirectX 9 {{with support}} {{for the use of}} much longer <b>shader</b> programs than before with pixel and vertex <b>shader</b> version 2.0. Microsoft has continued to update the DirectX suite since then, introducing <b>Shader</b> Model 3.0 in DirectX 9.0c, released in August 2004.|$|E
5000|$|Batman has two unlockable <b>shaders</b> {{based on}} Batman of Zur-En-Arrh in Injustice 2 (2017). The <b>shaders</b> are named [...] "Zur-En-Arrh" [...] and [...] "Zur-En-Arrh (Alternative)".|$|R
50|$|In computing, a compute kernel is {{a routine}} {{compiled}} for high throughput accelerators (such as GPUs), DSPs or FPGAs, separate from (but used by) a main program. They are sometimes called compute <b>shaders,</b> sharing execution units with vertex <b>shaders</b> and pixel <b>shaders</b> on GPUs, {{but are not}} limited to execution on one class of device, or graphics APIs.|$|R
40|$|The gaming {{industry}} plays {{a pivotal}} role in creating real-time advanced <b>shaders</b> nowadays. With better and more affordable computer hardware, <b>shaders</b> are beginning to be used in other non- gaming softwares. The virtual reality visualization tools used by the architectural designers can benefit from this. This paper investigates the impact of real-time <b>shaders</b> on the performance of architectural virtual reality visualization of 3 D models and provides a guide for architectural users to decide the optimal number of <b>shaders</b> to use based {{on the size of the}} model...|$|R
25|$|<b>Shader</b> Model 3.0 {{altered the}} specification, {{increasing}} full precision requirements {{to a minimum}} of FP32 support in the fragment pipeline. ATI's <b>Shader</b> Model 3.0 compliant R5xx generation (Radeon X1000 series) supports just FP32 throughout the pipeline while Nvidia's NV4x and G7x series continued to support both FP32 full precision and FP16 partial precisions. Although not stipulated by <b>Shader</b> Model 3.0, both ATI and Nvidia's <b>Shader</b> Model 3.0 GPUs introduced support for blendable FP16 render targets, more easily facilitating the support for High Dynamic Range Rendering.|$|E
25|$|The Direct3D 10 API {{introduces}} unified vertex <b>shader</b> and pixel <b>shader.</b> In addition, it {{also supports}} geometry shaders, which operate on entire geometric primitives (points, lines, and triangles), and can allow calculations based on adjacent primitives as well. The {{output of the}} geometry <b>shader</b> can be passed directly onwards to the rasterizer for interpolation and pixel shading, or written to a vertex buffer (known as 'stream out') to be fed back into {{the beginning of the}} pipeline.|$|E
25|$|For highly {{parallel}} {{floating point}} math workloads, the cards can speed up large computations {{by more than}} 10 times; Folding@Home, the earliest {{and one of the}} most visible users of the GPGPU, obtained 20-40 times the CPU performance. Each pixel and vertex <b>shader,</b> or unified <b>shader</b> in later models, can perform arbitrary floating-point calculations.|$|E
5000|$|The set of APIs used to compile, link, {{and pass}} {{parameters}} to GLSL programs are specified in three OpenGL extensions, and {{became part of}} core OpenGL as of OpenGL Version 2.0. The API was expanded with geometry <b>shaders</b> in OpenGL 3.2, tessellation <b>shaders</b> in OpenGL 4.0 and compute <b>shaders</b> in OpenGL 4.3. These OpenGL APIs {{are found in the}} extensions: ...|$|R
5000|$|LightWave {{comes with}} a nodal texture editor that {{comes with a}} {{collection}} of special-purpose material <b>shaders.</b> Some of the types of surface for which these <b>shaders</b> have been optimized include: ...|$|R
50|$|<b>Shaders</b> {{replace a}} section of the {{graphics}} hardware typically called the Fixed Function Pipeline (FFP), so-called because it performs lighting and texture mapping in a hard-coded manner. <b>Shaders</b> provide a programmable alternative to this hard-coded approach.|$|R
25|$|Computer video {{cards are}} {{produced}} by various vendors, such as Nvidia, and AMD and ATI. Cards from such vendors differ on implementing data-format support, such as integer and floating-point formats (32-bit and 64-bit). Microsoft introduced a <b>Shader</b> Model standard, to help rank the various features of graphic cards into a simple <b>Shader</b> Model version number (1.0, 2.0, 3.0, etc.).|$|E
25|$|Direct3D 10.1 is an {{incremental}} update of Direct3D 10.0 which shipped with, and required, Windows Vista Service Pack 1. This release mainly sets {{a few more}} image quality standards for graphics vendors, while giving developers more control over image quality. It also adds support for cube map arrays, separate blend modes per-MRT, coverage mask export from a pixel <b>shader,</b> ability to run pixel <b>shader</b> per sample, access to multi-sampled depth buffers and requires that the video card supports <b>Shader</b> Model 4.1 or higher and 32-bit floating-point operations. Direct3D 10.1 still fully supports Direct3D 10 hardware, {{but in order to}} utilize all of the new features, updated hardware is required.|$|E
25|$|The first {{screenshots}} of Unreal Engine 3 {{were presented}} in 2004, {{at which point the}} engine had already been in development for 18 months. Unlike Unreal Engine 2, which still supported fixed-function pipeline, Unreal Engine 3 was designed to take advantage of fully programmable <b>shader</b> hardware (in DirectX 9 terms, it required <b>shader</b> model 3.0). All lighting calculations were done per-pixel, instead of per-vertex. On the rendering side, Unreal Engine 3 provided support for a gamma-correct high-dynamic range renderer.|$|E
50|$|Direct3D was not {{considered}} to be user friendly, but as of DirectX version 8.1, many usability problems were resolved. Direct3D 8 contained many powerful 3D graphics features, such as vertex <b>shaders,</b> pixel <b>shaders,</b> fog, bump mapping and texture mapping.|$|R
50|$|<b>Shaders</b> {{that are}} {{designed}} to be executed directly on the GPU became useful for high throughput general processing because of their stream programming model; this {{led to the development of}} compute <b>shaders</b> running on similar hardware (see also: GPGPU).|$|R
5000|$|September 6, 2014: Version 3-0.1 - Support for Python 3.4 and Blender 2.70, vertex <b>shaders</b> and pixel <b>shaders,</b> {{improved}} performance (the rendering {{process has been}} entirely rewritten using vertex buffer object), fullscreen anti-aliasing, per-pixel lighting and cellshading, GPL v3 license ...|$|R
25|$|Any {{language}} {{that allows the}} code running on the to poll a GPU <b>shader</b> for return values, can create a GPGPU framework.|$|E
25|$|OpenGL ES 3.1 API and <b>shader</b> {{compatibility}} – {{to enable}} the easy development and execution of the latest OpenGL ES applications on desktop systems.|$|E
25|$|<b>Shader</b> storage buffer objects, {{allowing}} shaders to {{read and}} write buffer objects like image load/store from 4.2, but through the language rather than function calls.|$|E
50|$|It was {{noticed that}} an {{unexpected}} number of <b>shaders</b> are not hand-written but generated. This means these <b>shaders</b> were originally written in HLSL and then translated into GLSL by some translator program, such as e.g. HLSL2GLSL. The problem is, that the generated code is often {{far from being}} optimal. Matt Turner said it was much easier to fix this in the translator program than having to make Mesa's compiler carry the burden of dealing with such bloated <b>shaders.</b>|$|R
50|$|Displacement <b>shaders</b> can be stacked.|$|R
50|$|A {{programming}} {{model with}} <b>shaders</b> {{is similar to}} a higher order function for rendering, taking the <b>shaders</b> as arguments, and providing a specific dataflow between intermediate results, enabling both data parallelism (across pixels, vertices etc.) and pipeline parallelism (between stages). (see also map reduce).|$|R
