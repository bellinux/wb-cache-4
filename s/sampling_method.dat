10000|10000|Public
5|$|A 2004 {{study by}} Kathryn Thomas and Sandra Carlson used teeth {{from the upper}} jaw of three {{individuals}} interpreted as a juvenile, a subadult, and an adult, recovered from a bone bed in the Hell Creek Formation of Corson County, South Dakota. In this study, successive teeth in columns in the edmontosaurs' dental batteries were sampled from multiple locations along each tooth using a microdrilling system. This <b>sampling</b> <b>method</b> {{takes advantage of the}} organization of hadrosaurid dental batteries to find variation in tooth isotopes over a period of time. From their work, it appears that edmontosaur teeth took less than about 0.65years to form, slightly faster in younger edmontosaurs. The teeth of all three individuals appeared to show variation in oxygen isotope ratios that could correspond to warm/dry and cool/wet periods; Thomas and Carlson considered the possibility that the animals were migrating instead, but favored local seasonal variations because migration would have more likely led to ratio homogenization, as many animals migrate to stay within specific temperature ranges or near particular food sources.|$|E
25|$|Cornelius Lanczos did {{pioneering}} {{work on the}} FFT and FFS (Fast Fourier <b>Sampling</b> <b>method)</b> with G. C. Danielson (1940).|$|E
25|$|The best-known {{importance}} <b>sampling</b> <b>method,</b> the Metropolis algorithm, can be generalized, {{and this}} gives {{a method that}} allows analysis of (possibly highly nonlinear) inverse problems with complex a priori information and data with an arbitrary noise distribution.|$|E
40|$|The goal of {{this paper}} is to improve the {{prediction}} performance of fault-prone module prediction models (fault-proneness models) by employing over/under <b>sampling</b> <b>methods,</b> which are preprocessing procedures for a fit dataset. The <b>sampling</b> <b>methods</b> are expected to improve prediction performance when the fit dataset is imbalanced, i. e. there exists a large difference between the number of fault-prone modules and not-fault-prone modules. So far, there has been no research reporting the effects of applying <b>sampling</b> <b>methods</b> to fault-proneness models. In this paper, we experimentally evaluated the effects of four <b>sampling</b> <b>methods</b> (random over <b>sampling,</b> synthetic minority over sampling, random under sampling and one-sided selection) applied to four fault-proneness models (linear discriminant analysis, logistic regression analysis, neural network and classification tree) by using two module sets of industry legacy software. All four <b>sampling</b> <b>methods</b> improved the prediction performance of the linear and logistic models, while neural network and classification tree models did not benefit from the <b>sampling</b> <b>methods.</b> The improvements of F 1 -values in linear and logistic models were 0. 078 at minimum, 0. 224 at maximum and 0. 121 at the mean...|$|R
40|$|Many <b>sampling</b> <b>methods</b> {{have been}} {{suggested}} for estimating the population median. In the situation when the sampling units in a study can be easily ranked than quantified, the ranked set <b>sampling</b> <b>methods</b> {{are found to be}} more efficient and cost effective as compared to the simple random sampling. In this paper, the superiority of several ranked set <b>sampling</b> <b>methods</b> over the simple random sampling are illustrated through some simulation study. In addition, some new research topics under ranked set sampling are suggested...|$|R
5000|$|... <b>sampling</b> <b>methods</b> {{suitable}} for parallelization and GPU calculations; ...|$|R
25|$|The Box–Muller {{transform}} {{was developed}} as a more computationally efficient alternative to the inverse transform <b>sampling</b> <b>method.</b> The Ziggurat algorithm gives an even more efficient method. Furthermore, the Box–Muller transform can be employed for drawing from truncated bivariate Gaussian densities.|$|E
25|$|The uniform {{distribution}} {{is useful for}} sampling from arbitrary distributions. A general method is the inverse transform <b>sampling</b> <b>method,</b> which uses the cumulative distribution function (CDF) of the target random variable. This method is very useful in theoretical work. Since simulations using this method require inverting the CDF of the target variable, alternative methods have been devised for the cases where the cdf is not known in closed form. One such method is rejection sampling.|$|E
25|$|Second, {{utilizing}} a stratified <b>sampling</b> <b>method</b> {{can lead to}} more efficient statistical estimates (provided that strata are selected based upon relevance to the criterion in question, instead of availability of the samples). Even if a stratified sampling approach {{does not lead to}} increased statistical efficiency, such a tactic will not result in less efficiency than would simple random sampling, provided that each stratum is proportional to the group's size in the population.|$|E
40|$|ABSTRACT This study compares <b>sampling</b> <b>methods</b> {{based on}} plots of fixed area {{and based on}} a fixed number of trees. The study was {{conducted}} in a Eucalyptus forest surveyed using three plot types: rectangular with fixed area, circular with fixed area and fixed number of trees. The estimation accuracies were evaluated for the average diameter per plot and for the number of stems, the basal area and the volume per plot. The null hypothesis of equality between the <b>sampling</b> <b>methods</b> was assessed by t-test. No {{significant differences were found between}} the three <b>sampling</b> <b>methods...</b>|$|R
2500|$|... on typical {{drilling}} and <b>sampling</b> <b>methods</b> in geotechnical engineering.|$|R
50|$|Ligand <b>sampling</b> <b>methods</b> used by {{the program}} DOCK include.|$|R
25|$|The Box–Muller transform, by George Edward Pelham Box and Mervin Edgar Muller, is a {{pseudo-random}} number <b>sampling</b> <b>method</b> for generating pairs of independent, standard, normally distributed (zero expectation, unit variance) random numbers, given {{a source of}} uniformly distributed random numbers. The method was in fact first mentioned by Raymond E. A. C. Paley and Norbert Wiener in 1934, and {{it is more likely}} than not that this source was well known to Box and Muller, which, however, failed to cite it in their article of 1958.|$|E
25|$|Polls {{conducted}} for The Age found Victorians favoured {{construction of the}} Metro rail project before the East West Link. A May 2013 poll (which did not reveal the poll sample, <b>sampling</b> <b>method,</b> questions or date of the survey) found 47 percent of voters supported the rail plan, while 43 percent supported the road project. A similar poll of 1000 voters in February 2014 found the Metro rail project was viewed as the infrastructure project with the highest priority (42 percent), followed by removing level crossings (27 percent) and the East West Link (24 percent). A November 2013 Age/Neilsen poll of 1000 voters found that 74 percent believed improving public transport was a bigger priority than building the East West Link tunnel. Some 23 percent considered the tunnel a higher priority. A Herald Sun/Seven News poll of more than 2500 Victorians in November 2014 found the East West Link was considered a higher priority than removing level crossings, an airport rail link and second underground rail loop. The Herald Sun provided no details on the poll's <b>sampling</b> <b>method,</b> questions or actual results. A further Age poll of 1000 respondents in November 2014 reported 59 percent in favour {{of the project and}} 29 percent opposed.|$|E
25|$|For {{the normal}} distribution, {{the lack of}} an {{analytical}} expression for the corresponding quantile function means that other methods (e.g. the Box–Muller transform) may be preferred computationally. It is often the case that, even for simple distributions, the inverse transform <b>sampling</b> <b>method</b> can be improved on: see, for example, the ziggurat algorithm and rejection sampling. On the other hand, it is possible to approximate the quantile function of the normal distribution extremely accurately using moderate-degree polynomials, and in fact the method of doing this is fast enough that inversion sampling is now the default method for sampling from a normal distribution in the statistical package R.|$|E
40|$|Clever <b>sampling</b> <b>methods</b> {{can be used}} {{to improve}} the {{handling}} of big data and increase its usefulness. The subject of this study is remote sensing, specifically airborne laser scanning point clouds representing different classes of ground cover. The aim is to derive a supervised learning model for the classification using CARTs. In order to measure the effect of different <b>sampling</b> <b>methods</b> on the classification accuracy, various experiments with varying types of <b>sampling</b> <b>methods,</b> <b>sample</b> sizes, and accuracy metrics have been designed. Numerical results for a subset of a large surveying project covering the lower Rhine area in Germany are shown. General conclusions regarding sampling design are drawn and presented. ...|$|R
30|$|Perennial grass, 1 -hr surface fuel, total surface fuel, bare ground, {{and rock}} were all {{strongly}} correlated between ground cover <b>sampling</b> <b>methods</b> (P < 0.05; Table 2). Two fuel classes, 10 -hr and 100 -hr ground cover, displayed a weak correlation between <b>sampling</b> <b>methods,</b> and 1000 -hr fuels were poorly correlated.|$|R
40|$|Currently {{available}} and recently developed <b>sampling</b> <b>methods</b> for slurry and solid manure were tested for bias and reproducibility in {{the determination of}} total phosphorus and nitrogen content of <b>samples.</b> <b>Sampling</b> <b>methods</b> were based on techniques in which samples were taken either during loading from the hose or from the transport vehicle after loading. Most methods were unbiased. New <b>sampling</b> <b>methods</b> for slurry from the hose were substantially more reproducible than existing methods. For practical reasons, the mechanization of sampling is desirable, and to minimize the influence of human activity on sample quality, the automation of sampling is advisable...|$|R
25|$|Nonprobability {{sampling}} is any <b>sampling</b> <b>method</b> {{where some}} elements of the population have no chance of selection (these are {{sometimes referred to as}} 'out of coverage'/'undercovered'), or where the probability of selection can't be accurately determined. It involves the selection of elements based on assumptions regarding the population of interest, which forms the criteria for selection. Hence, because the selection of elements is nonrandom, nonprobability sampling does not allow the estimation of sampling errors. These conditions give rise to exclusion bias, placing limits on how much information a sample can provide about the population. Information about the relationship between sample and population is limited, making it difficult to extrapolate from the sample to the population.|$|E
25|$|Overall, {{on a first}} date, women’s {{goals are}} related more to {{establishing}} friendships and having fun. Mongeau, in his 2004 study found that 60 percent of women go on first dates mainly to have fun and 59 percent go on dates to reduce uncertainty. (Note, however, {{that there are some}} errors in the mathematical calculations done in the study, and the study is statistically invalid as it uses the convenience <b>sampling</b> <b>method.)</b> Women's goals tend to be more relationship-oriented than men’s goals. Also, unlike men, women are usually more reserved {{when it comes to the}} sexual aspect and having sex. Women are more likely to express companionship, friendship, and romantic relationship goals than men and are also more likely to consider the first date in terms of their relational implications than men.|$|E
500|$|This result {{showed that}} {{scintillation}} counters {{can not only}} determine of the arrival times of shower disks at many detectors spread over a large area, but also to estimate the number of particles striking each detector. [...] These capabilities combine the [...] "fast-timing" [...] method of determining shower arrival directions with the density <b>sampling</b> <b>method</b> of determining their size {{and the location of}} their axes.|$|E
5000|$|Development of <b>sampling</b> <b>methods</b> for hard-bottom {{habitats}} in the Great Lakes ...|$|R
5000|$|... {{and local}} {{elevation}} umbrella <b>sampling</b> <b>methods)</b> to obtain more accurate free energies.|$|R
40|$|Background <b>Sampling</b> <b>methods</b> {{have proven}} to be a very {{efficient}} and intuitive method to understand properties of complicated spaces that cannot easily be computed using deterministic meth-ods. Therefore, <b>sampling</b> <b>methods</b> became a popular tool in the applied sciences. Results Here, we show that <b>sampling</b> <b>methods</b> are not an appropriate tool to analyze qualitative properties of complicated spaces unless RP = NP. We illustrate these results on the exam-ple of the thermodynamically feasible flux space of genome-scale metabolic networks and show that with artificial centering hit and run (ACHR) not all reactions that can have variable flux rates are sampled with variables flux rates. In particular a uniform sample of the flux space would not sample the flux variabilities completely. Conclusion We conclude that unless theoretical convergence results exist, qualitative results obtained from <b>sampling</b> <b>methods</b> should be considered with caution and if possible double checked using a deterministic method...|$|R
2500|$|The {{problem that}} the inverse {{transform}} <b>sampling</b> <b>method</b> solves is as follows: ...|$|E
2500|$|Specifying a <b>sampling</b> <b>method</b> for {{selecting}} items or events {{from the frame}} ...|$|E
2500|$|If X has a {{standard}} uniform distribution, {{then by the}} inverse transform <b>sampling</b> <b>method,</b> Y = − λ−1 ln(X) [...] has an exponential distribution with (rate) parameter λ.|$|E
5000|$|A {{sampling}} plan to outline <b>sampling</b> <b>methods</b> both during and between production batches ...|$|R
40|$|This paper {{represents}} 1 of 6 {{papers in}} the special series "Passive <b>Sampling</b> <b>Methods</b> for Contaminated Sediments," which was generated from the SETAC Technical Workshop "Guidance on Passive <b>Sampling</b> <b>Methods</b> to Improve Management of Contaminated Sediments," held November 2012 in Costa Mesa, California, USA. Recent advances in passive <b>sampling</b> <b>methods</b> (PSMs) offer an improvement in risk-based decision making, since bioavailability of sediment contaminants can be directly quantified. Forty-five experts, representing PSM developers, users, and decision makers from academia, government, and industry, convened to review the state of science to gain consensus on PSM applications in assessing and supporting management actions on contaminated sediments. Full Tex...|$|R
40|$|Combinatorial {{estimation}} is a {{new area}} of application for sequential Monte Carlo methods. We use ideas from sampling theory to introduce new without-replacement <b>sampling</b> <b>methods</b> in such discrete settings. These without-replacement <b>sampling</b> <b>methods</b> allow the addition of merging steps, which can significantly improve the resulting estimators. We give examples showing {{the use of the}} proposed methods in combinatorial rare-event probability estimation and in discrete state-space models...|$|R
2500|$|The {{voluntary}} <b>sampling</b> <b>method</b> {{is a type}} of non-probability sampling. A voluntary {{sample is}} made up of people who self-select into the survey. Often, these subjects have a strong interest in the main topic of the survey.|$|E
2500|$|The formula above for {{the margin}} of error assume that there is an {{infinitely}} large population and thus do not depend {{on the size of the}} population of interest. [...] According to sampling theory, this assumption is reasonable when the sampling fraction is small. The margin of error for a particular <b>sampling</b> <b>method</b> is essentially the same regardless of whether the population of interest is the size of a school, city, state, or country, as long as the sampling fraction is less than 5%.|$|E
2500|$|The {{simulation}} can {{be performed}} either by a solution of kinetic equations for density functions [...] (A. G. [...] Khachaturyan, S.V. Semenovskaya, B.K.Vainstein in 1979 and A. Khachaturyan, S. Semenovsakaya, B. Vainshtein in 1981) or by using the stochastic <b>sampling</b> <b>method</b> which was independently described by Scott Kirkpatrick, C. Daniel Gelatt and Mario P. Vecchi in 1983, by Vlado Černý in 1985 and by Svetlana V. Semenovskaya, Karen A. Khachaturyan and Armen G. Khachaturyan in 1985. The method is an adaptation of the Metropolis–Hastings algorithm, a Monte Carlo method to generate sample states of a thermodynamic system, invented by M.N. Rosenbluth and published by N. Metropolis et al. in 1953.|$|E
5000|$|HEEDS {{allows the}} user to predict sensitivities using Design of Experiments. The {{following}} <b>sampling</b> <b>methods</b> are available: ...|$|R
40|$|We {{consider}} the inverse elastic scattering of incident plane compressional and shear waves from {{the knowledge of}} the far field patterns. Specifically, three direct <b>sampling</b> <b>methods</b> for location and shape reconstruction are proposed using the different component of the far field patterns. Only inner products are involved in the computation, thus the novel <b>sampling</b> <b>methods</b> are very simple and fast to be implemented. With the help of the factorization of the far field operator, we give a lower bound of the proposed indicator functionals for sampling points inside the scatterers. While for the sampling points outside the scatterers, we show that the indicator functionals decay like the Bessel functions as the sampling point goes away from the boundary of the scatterers. We also show that the proposed indicator functionals continuously dependent on the far field patterns, which further implies that the novel <b>sampling</b> <b>methods</b> are extremely stable with respect to data error. For the case when the observation directions are restricted into the limited aperture, we firstly introduce some data retrieval techniques to obtain those data that can not be measured directly and then use the proposed direct <b>sampling</b> <b>methods</b> for location and shape reconstructions. Finally, some numerical simulations in two dimensions are conducted with noisy data, and the results further verify the effectiveness and robustness of the proposed <b>sampling</b> <b>methods,</b> even for multiple multiscale cases and limited-aperture problems...|$|R
50|$|In other cases, the {{normalization}} constant {{can usually}} be ignored, as most <b>sampling</b> <b>methods</b> do not require it.|$|R
