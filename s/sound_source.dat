3165|2453|Public
5|$|In {{some animals}} with mobile pinnae (like the horse), each pinna can be aimed {{independently}} to better receive the sound. For these animals, the pinnae help localise {{the direction of}} the <b>sound</b> <b>source.</b>|$|E
5|$|The Leslie is {{specifically}} designed, via {{reproduction of the}} Doppler effect, to alter or modify sound. As the <b>sound</b> <b>source</b> is rotated around a specific pivot point, it produces tremolo (the modulation of amplitude) and a variation in pitch. This produces a sequence of frequency modulated sidebands. To stop a Leslie's rotor, a special brake circuit {{was added to the}} Leslie motor controls, that incorporated an electronic relay by producing a half-wave of direct current.|$|E
5|$|Kikuta {{released}} Lost Files, {{his first}} album of original music, in 2006. The album includes the demo tapes Kikuta submitted when first applying {{for the job}} of game composer at Square, using the <b>sound</b> <b>source</b> of the Nintendo Entertainment System. It was followed in August 2007 by his second original album, Alphabet Planet. He has also composed three other albums and two singles in conjunction with other singers or performers; these albums have been released though his Norstrilia label and are the only works he has been credited as composing for since Concerto Gate was released in 2007. His next album, due to be released in spring 2010, is another album of original works, entitled Tiara.|$|E
5000|$|Loudness: Distant <b>sound</b> <b>sources</b> {{have a lower}} {{loudness}} than close ones. This aspect can {{be evaluated}} especially for well-known <b>sound</b> <b>sources.</b>|$|R
40|$|A longstanding {{philosophical}} tradition {{holds that}} the primary objects of hearing are sounds rather than <b>sound</b> <b>sources.</b> In this case, we hear <b>sound</b> <b>sources</b> by—or in virtue of—hearing their sounds. This paper argues that, on the contrary, we {{have good reason to}} believe that the primary objects of hearing are <b>sound</b> <b>sources,</b> and that the relationship between a <b>sound</b> and its <b>source</b> is much like the relationship between a color and its bearer. Just as we see objects in seeing their colors, so we hear <b>sound</b> <b>sources</b> in hearing their sounds...|$|R
5000|$|Movement: Similar to {{the visual}} system {{there is also the}} {{phenomenon}} of motion parallax in acoustical perception. For a moving listener nearby <b>sound</b> <b>sources</b> are passing faster than distant <b>sound</b> <b>sources.</b>|$|R
25|$|Advanced audio capabilities: Audio {{processing}} capabilities include sophisticated {{acoustic noise}} suppression and echo cancellation, beam formation {{to identify the}} current <b>sound</b> <b>source,</b> and integration with Windows speech recognition API.|$|E
25|$|Spill is also {{avoided by}} using a 3:1 {{distance}} rule of thumb, which states that for each unit of distance between a <b>sound</b> <b>source</b> and its microphone, other microphones should be placed {{at least three times}} as far.|$|E
25|$|Headphones {{are also}} useful for video games that use 3D {{positional}} audio processing algorithms, as they allow players to better judge {{the position of}} an off-screen <b>sound</b> <b>source</b> (such as the footsteps of an opponent or their gun fire).|$|E
30|$|As {{mentioned}} in Section 1, the proposed system {{assumes that the}} <b>sound</b> <b>sources</b> are from speech <b>sound</b> <b>sources.</b> This is taken advantage of, as detailed in the following section, {{for the purpose of}} multi-DOA estimation.|$|R
40|$|Abstract—The present work {{faces the}} problem of {{automatic}} enumeration and recognition of an unknown and time-varying number of environmental <b>sound</b> <b>sources</b> while using a single microphone. The assumption that is made is that the sound recorded is a realization of <b>sound</b> <b>sources</b> belonging {{to a group of}} audio classes which is known a-priori. We describe two variations of the same principle which is to calculate the distance between the current unknown audio frame and all possible combinations of the classes that are assumed to span the soundscene. We concentrate on categorizing environmental <b>sound</b> <b>sources,</b> such as birds, insects etc. in the task of monitoring the biodiversity of a specific habitat. Keywords — automatic recognition of multiple <b>sound</b> <b>sources...</b>|$|R
50|$|An {{acoustic}} {{camera is}} an imaging device used to locate <b>sound</b> <b>sources</b> and to characterize them. It {{consists of a}} group of microphones — also called microphone array — that are simultaneously acquired to form a representation of the location of the <b>sound</b> <b>sources.</b>|$|R
25|$|The Strouhal {{relationship}} {{suggests that}} the jet Mach number and the ratio of effective pipe length to the edge distance are important in a first approximation. Normal pipe operation would be a monopole <b>sound</b> <b>source</b> in Stage I with Class III feedback.|$|E
25|$|Wilson, et al., {{in their}} study of human {{whistling}} (see below), pointed out the importance of including the symmetry or asymmetry of the unstable flow in addition to the feedback classes listed below. Because of the close relationship of flow symmetry to the sound field generated, their concept was included here as part of the <b>sound</b> <b>source</b> description (monopole -symmetric and dipole - asymmetric).|$|E
25|$|A {{licensed}} interactive fiction {{video game}} {{based on the}} script was being developed for Infocom by Bob Bates, but was cancelled when Infocom was shut down by its then-parent company Activision. <b>Sound</b> <b>Source</b> Interactive later created an action video game entitled The Abyss: Incident at Europa. The game takes place {{a few years after}} the film, where the player must find a cure for a deadly virus.|$|E
40|$|In this paper, {{we present}} a novel {{approach}} for detection and localization of both impulsive and non-impulsive <b>sound</b> <b>sources.</b> At first, theoretical basics of the used algorithms are presented. Subsequently, we describe a standard SRP-PHAT based localization method and discuss occurring complications, especially for impulsive <b>sound</b> <b>sources.</b> Therefore, a modified approach {{is presented as a}} solution. It distinguishes between impulsive and non impulsive <b>sound</b> <b>sources,</b> and additionally aligns the detection window to the event. The preclassification and alignment are done {{with the help of an}} energy detector. 1...|$|R
50|$|Vector {{synthesis}} provides {{movement in}} a sound by providing dynamic cross-fading between (usually) four <b>sound</b> <b>sources.</b> The four <b>sound</b> <b>sources</b> are conceptually arranged as the extreme points of X and Y axes, and typically labelled A, B, C and D. A given {{mix of the}} four <b>sound</b> <b>sources</b> can be represented by a single point in this 'vector plane'. Movement of the point provides sonic interest and {{is the power of}} this technique. Mixing is frequently done using a joystick, although the point can be controlled using envelope generators or LFOs.|$|R
40|$|Introduction This sketch {{describes}} the Mixed Reality application ASR (Augmented Sound Reality) which uses the overlay of virtual {{images on the}} real world to support the placement of three dimensional <b>sound</b> <b>sources.</b> Our system allows to place <b>sound</b> <b>sources</b> in a virtual or real room with the advantage of feeling, seeing and hearing them. This implies a more intuitive and better feeling of space and 3 D sound. Our main goal was to demonstrate how Augmented Reality {{can be used for}} placing 3 D <b>sound</b> <b>sources</b> to enhance the impression and feeling of 3 D sound. The ASR application shows how this could work in the future. Another objective of our work was to develop an AR interface (pen with marker), which allows the user to interact with the virtual objects in a very simple way. The users control the <b>sound</b> <b>sources</b> with the pen. The different <b>sound</b> <b>sources</b> are represented as 3 D objects on the HMD and they can be freely positioned by the user. Moreover, the user can select a sound-type and plac...|$|R
25|$|Every {{electronic}} {{hearing aid}} has at minimum a microphone, a loudspeaker (commonly called a receiver), a battery, and electronic circuitry. The electronic circuitry varies among devices, {{even if they}} are the same style. The circuitry falls into three categories based on the type of audio processing (analog or digital) and the type of control circuitry (adjustable or programmable). Hearing aid devices generally do not contain processors strong enough to process complex signal algorithms for <b>sound</b> <b>source</b> localization.|$|E
25|$|In {{some ways}} {{similar to the}} laser {{turntable}} is the IRENE scanning machine for disc records, which images with microphotography in two dimensions, invented {{by a team of}} physicists at Lawrence Berkeley Laboratories. IRENE will retrieve the information from a laterally modulated monaural grooved <b>sound</b> <b>source</b> without touching the medium itself, but cannot read vertically modulated information. This excludes grooved recordings such as cylinders and some radio transcriptions that feature a hill-and-dale format of recording, and stereophonic or quadraphonic grooved recordings, which utilize {{a combination of the two}} as well as supersonic encoding for quadraphonic.|$|E
25|$|Spatial {{location}} (see: Sound localization) {{represents the}} cognitive placement {{of a sound}} in an environmental context; including the placement of a sound on both the horizontal and vertical plane, {{the distance from the}} <b>sound</b> <b>source</b> and the characteristics of the sonic environment. In a thick texture, it is possible to identify multiple sound sources using a combination of spatial location and timbre identification. It is the main reason why we can pick the sound of an oboe in an orchestra and the words of a single person at a cocktail party.|$|E
5000|$|... #Subtitle level 2: Preservation {{of vintage}} optical <b>sound</b> <b>sources</b> ...|$|R
50|$|A more {{technical}} definition: an audio engineer in sound recording, audio editing and sound systems who balances the relative volume and frequency {{content of a}} number of <b>sound</b> <b>sources.</b> Typically, these <b>sound</b> <b>sources</b> are the different musical instruments in a band or vocalists, the sections of an orchestra and so on.|$|R
5000|$|Flood for the Eternal's Origins (1970) {{for global}} <b>sound</b> <b>sources</b> ...|$|R
25|$|DJ mixers {{are small}} audio mixing {{consoles}} specialized for DJing. Most DJ mixers have far fewer channels than a mixer {{used by a}} record producer or audio engineer; whereas standard live sound mixers in small venues have 12 to 24 channels, and standard recording studio mixers have even more (as many as 72 on large boards), basic DJ mixers may have only two channels. While DJ mixers have {{many of the same}} features found on larger mixers (faders, equalization knobs, gain knobs, effects units, etc), DJ mixers have a feature that is usually only found on DJ mixers: the crossfader. The crossfader is a type of fader that is mounted horizontally. DJs used the crossfader to mix two or more sound sources. The midpoint of the crossfader's travel is a 50/50 mix of the two channels (on a two channel mixer). The far left side of the crossfader provides only the channel A <b>sound</b> <b>source.</b> The far right side provides only the channel B <b>sound</b> <b>source</b> (e.g., record player number 2). Positions in between the two extremes provide different mixes of the two channels. Some DJs use a computer with DJ software and a DJ controller instead of an analog DJ mixer to mix music, although DJ software can be used in conjunction with a hardware DJ mixer.|$|E
25|$|Bending wave {{transducers}} use {{a diaphragm}} that is intentionally flexible. The rigidity {{of the material}} increases from the center to the outside. Short wavelengths radiate primarily from the inner area, while longer waves reach {{the edge of the}} speaker. To prevent reflections from the outside back into the center, long waves are absorbed by a surrounding damper. Such transducers can cover a wide frequency range (80Hz to 35,000Hz) and have been promoted as being close to an ideal point <b>sound</b> <b>source.</b> This uncommon approach is being taken by only a very few manufacturers, in very different arrangements.|$|E
25|$|The {{bracketed}} term includes two feedback loop speeds; the downstream speed is {{the speed of}} the vortices u and the upstream speed is that of sound c0. The various modes are described by an integer n with an empirical delay constant β (near 0.25). The integer n is closely related to the number of vortices en route to the edge. It is clear from shadowgraphs that the fluctuating force near the downstream edge is the <b>sound</b> <b>source.</b> Since the Mach number of the flow can be appreciable, refraction makes it difficult to determine the major axis of the dipole-like sound field. The preferred frequencies in shallow cavities are different from those for the edge tone.|$|E
5000|$|Support for {{multi-channel}} <b>sound</b> <b>sources</b> (including assets encoded using Ambisonics).|$|R
30|$|In {{everyday}} life, {{quality of}} sound {{depends on the}} environment. Scientists and engineers working on room acoustics (see, e.g., [11]) have studied this crucial issue intensively. The influence of the environment is a complex problem, and modelling sounds taking architectural specificities into account are not {{the scope of this}} study. In particular, the effects of reverberation {{can be explained by the}} physical laws of sound propagation, which impose that distant <b>sound</b> <b>sources</b> lead to more highly reverberated signals than nearby <b>sound</b> <b>sources</b> because with distant <b>sound</b> <b>sources,</b> both the direct and reflected sound paths are of similar orders of magnitude, whereas with nearby <b>sources,</b> the direct <b>sound</b> is of greater magnitude than the reflected <b>sounds.</b> Moving <b>sound</b> <b>sources,</b> therefore, involve a time-dependent direct-reverberated ratio, the value of which depends on the distance between source and listener.|$|R
5000|$|... {{working with}} finite <b>sound</b> <b>sources</b> {{reinforces}} concepts of liberation through limitation ...|$|R
25|$|Sound {{localization}} is {{the process}} of determining the location of a <b>sound</b> <b>source.</b> The brain utilizes subtle differences in loudness, tone and timing between the two ears to allow us to localize sound sources. Localization can be described in terms of three-dimensional position: the azimuth or horizontal angle, the zenith or vertical angle, and the distance (for static sounds) or velocity (for moving sounds). Humans, as most four-legged animals, are adept at detecting direction in the horizontal, but less so in the vertical due to the ears being placed symmetrically. Some species of owls have their ears placed asymmetrically, and can detect sound in all three planes, an adaption to hunt small mammals in the dark.|$|E
25|$|Active noise-cancelling {{headphones}} use a microphone, amplifier, {{and speaker}} to pick up, amplify, and play ambient noise in phase-reversed form; this {{to some extent}} cancels out unwanted noise from the environment without affecting the desired <b>sound</b> <b>source,</b> which is not picked up and reversed by the microphone. They require a power source, usually a battery, to drive their circuitry. Active noise cancelling headphones can attenuate ambient noise by 20dB or more, but the active circuitry is mainly effective on constant sounds and at lower frequencies, rather than sharp sounds and voices. Some noise cancelling headphones are designed mainly to reduce low-frequency engine and travel noise in aircraft, trains, and automobiles, and are less effective in environments {{with other types of}} noise.|$|E
25|$|After several {{releases}} on the Bugge Wesseltoft label Jazzland, {{he released}} the album Dream Logic (2012) {{on the label}} ECM, where he collaborated closely with Jan Bang and Erik Honoré on the production and timbral design of melodies, sound sculptures and soundscapes. Aarset is a pioneering guitarist with {{a great sense of}} electronics sound and expressiveness, and often the guitar plays a role as manipulable <b>sound</b> <b>source</b> and tool for layer-by-layer routing more than a traditional melody. His style is often associated with that of nu jazz, and his performance, improvising and albums feature, a strong 21'st century electronic influence. He is considered {{to be one of the}} unique re-interpretations of what the role and sound of the electric guitarist can be.|$|E
5000|$|Musique Athlétique (2014), for 2 break (core) dancers) {{and mobile}} <b>sound</b> <b>sources</b> ...|$|R
5000|$|Musical Design : The {{exploration}} into the sonic possibilities of various materials in various formats, {{the design and}} construction of <b>sound</b> <b>sources</b> {{based on these findings}} and suitable for performance, and the re-{{exploration into}} the sonic worlds of the <b>sound</b> <b>sources</b> during performance through exquisite improvisation (from seeking, not providing), is a compositional process called musical design.|$|R
5000|$|Aria (2007) Unlimited {{duration}} 21 <b>sound</b> <b>sources</b> (63 loudspeakers) in {{an exhibition}} hall ...|$|R
