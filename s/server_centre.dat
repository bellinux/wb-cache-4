2|26|Public
5000|$|Global <b>Server</b> <b>Centre,</b> Suite 570 - 167 Lombard Avenue, Winnipeg, Manitoba ...|$|E
40|$|Green Public Procurement (GPP) {{can play}} a {{significant}} a role in efforts to influence the private sector towards more sustainable products and services. Following {{an account of the}} environmental impact of Nordic public procurement, the project provides an assessment of the potential of strengthening GPP within three key product groups in Nordic public procurement: Taxi and coach services, Computers and related services, and Cleaning products. The analyses show that compared to traditional regulatory instruments GPP tends to be more soft and dynamic - but also slower and less comprehensive. A decision model for GPP is developed. Two cases illustrate the potential benefits of GPP: Procurement of a new <b>server</b> <b>centre</b> in the City of Copenhagen, and procurement of transportation for disabled and elderly people in the City of Stockholm. Conclusions and recommendations on how to strengthen the benefits of GPP are provided. The study has been initiated and supervised by The Working Group on Environment and Economics under the Nordic Council of Ministries...|$|E
50|$|Soitec's {{semiconductor}} {{materials are}} used to manufacture chips which equip smartphones, tablets, computers, IT <b>servers,</b> and data <b>centres.</b> Soitec's products are also found in electronic components used in cars, connected objects (Internet of Things), as well as industrial and medical equipment.|$|R
50|$|ICCP {{is based}} on client / server principles. Data {{transfers}} result from a request from a control centre (client) to another control <b>centre</b> (<b>server).</b> Control <b>centres</b> may be both clients and servers. ICCP operates at the application layer in the OSI model. As such any physical interfaces, transport and network services that fit this model are supported. TCP/IP over Ethernet (802.3) {{seems to be the}} most common. ICCP may operate over a single point-to-point link between two control centres; however, the more general case is for many control centres and a routed wide area network. The logical connections or “associations” between control centres are completely general. A client may establish associations with more than one server and a client may establish more than one association with the same server. Multiple associations with same server can be established at different levels of quality of service so that high priority real time data is not delayed by lower priority or non real time data transfers.|$|R
40|$|Abstract. Thispaper proposes aswarm-inspireddata centre self-organizingand {{consolidation}} technique whichaimsatreducing {{the power}} demand in data centres while ensuring the workload execution within the established performance parameters. Each data <b>centre</b> <b>server</b> is managed by an intelligent agent that implements a bird’s migration-inspired behaviour {{to decide on}} the appropriate server consolidation actions. The selected actions are executed to achieve an optimal utilization of server computing resources thus lowering power demand. The data <b>centre</b> <b>servers</b> self-organize in logical clusters according to the birds V-formation self-organizing migration model. The results are promising showing that by using the proposed swarm-inspired solution, the data centre Deployed Hardware Utilization Ratio Green Indicator increases compared to the widely used Fit First consolidation algorithm. The average power saving of the proposed technique isaround 40 % ofthe power demanded by the data centre computing resources and about 16 % of its total power demand including the IT facility, when comparing to OpenNebula Fit First consolidation technique. This paper is an extended version of the one published in WIMS’ 12 proceedings showing more details about the swarm-inspired consolidation technique and the defined algorithms...|$|R
40|$|Carbon Capture and Storage (CCS), {{also known}} as Carbon Capture and Sequestration, {{includes}} geological storage CO 2. Safe, long-term geological storage (sequestration) of CO 2 also requires a continuous monitoring system to detect CO 2 leakage from reservior. This paper gives details about a remote carbon dioxide (CO 2) concentration monitoring system developed, based on the technologies of wireless sensor networks, in allusion to the gas leakage monitoring requirement for CO 2 capture and storage. The remote online CO 2 monitoring system consists of monitoring equipment and a data <b>centre</b> <b>server.</b> The monitoring equipment is composed of a central processing unit (CPU), air environment sensors array, secure digital memory card (SD) storage module, liquid crystal display (LCD) module, and general packet radio service (GPRS) wireless transmission module. The sensors array of CO 2, temperature, humidity and light intensity are used to collect data. The CPU automatically stores the collected data in the SD card data storage module and displays them on the LCD display module in real-time. Afterwards, the GPRS module continuously wirelessly transmits the collected information to the data <b>centre</b> <b>server...</b>|$|R
40|$|Recovery {{of value}} and {{materials}} present in waste {{electrical and electronic equipment}} (WEEE) {{is critical to}} prevent environmental damage, prevent depletion of resources and use energy efficiently. In returning the materials back to a forward supply chain, often a closed-loop supply chain is preferred over an open-loop supply chain. In a closed-loop supply chain new production is substituted with recovery of old equipment, recovering maximum economic and environmental value. For data <b>centre</b> <b>servers</b> however, this is not possible as servers have very short lifecycles and become technologically obsolete quickly. This thesis investigates how to improve on the current open-loop reverse supply chain for data <b>centre</b> <b>servers,</b> to come to a more environmentally friendly, resource efficient and economically viable reverse supply chain for data <b>centre</b> <b>servers.</b> To perform the study, literature on business to business (B 2 B) WEEE, open-loop supply chains and green IT was used to create a conceptual framework. The supply chain from production, use, through to disposal was considered. A qualitative case study was performed, collecting qualitative data by doing eighteen different interviews with actors in the Amsterdam Metropolitan Area representing all relevant actors. Data was coded and analysed using the theoretical concepts to create categories for qualitative content analysis. In the disposing of data <b>centre</b> <b>servers</b> four relevant scenarios can be identified. Dismantling of servers is preferred over shredding of servers, before materials are sorted and brought to shredding actors. In the reuse spectrum the crux is whether the servers are reused regionally or exported for reuse. With exporting for reuse valuable materials leave the region, while it cannot be made sure these resources can be recovered {{at the end of their}} next lifecycle. Regional reuse on the other hand is more environmentally friendly, but is very difficult to accomplish, as servers can be freely traded. It is common that multiple brokers have had ownership over the server before it ends up at the next end-user, making it impossible to retain control over the materials. By informing actors on the possibilities of data destruction without losing value, servers users could be motivated through the financial incentive to dismantle servers instead of shredding them. For reusing servers, a new business model is needed which guarantees servers stay in the region and are recycled at the end of their second life. To be able to assess whether a server should be recycled or reused, more research is needed...|$|R
5000|$|The project Wiki {{contains}} {{lists of}} digital Classics projects, software tools {{that have been}} made available for classicists, and a FAQ that solicits collaborative community advice on a range of topics from simple questions about, e.g., Greek fonts and Unicode, word-processing and printing issues, to more advanced Humanities Computing questions and project management advice. The Wiki is hosted on the <b>servers</b> of the <b>Centre</b> for Computing in the Humanities at King's College London ...|$|R
50|$|India {{has also}} {{approved}} {{the construction of}} a HDVSL-compliant rich media data centre with an initial capacity of 28,000 video <b>servers.</b> The data <b>centre</b> is being built by the Interactivity consortium and would connect to the San Francisco to London medianet submarine cable. The Medianet submarine cable is likely to have full function landing points in Singapore Media City and Dubai Media City. Once fully operational - it will facilitate a place shift of television channels making terminal based place shift devices like slingbox obsolete.|$|R
50|$|The Gateway Mobile Location Centre {{contains}} functionality {{required to}} support LBS (Location-based Service). In one PLMN (Public Land Mobile Network), {{there may be}} more than one GMLC. The GMLC is the first node an external LBS client accesses in a GSM, UMTS or LTE network. The GMLC may request routing information from the HLR (Home Location register) or HSS (Home Subscriber Server). After performing registration authorization, it sends positioning requests to either the VMSC (Visited Mobile Switching Centre), SGSN (Serving GPRS Support Node) or MSC (Mobile Switching <b>Centre)</b> <b>Server</b> and receives final location estimates from the corresponding entity.|$|R
50|$|These allow gains to be {{made through}} {{optimisation}} of servers. This is typically done by doing diagnostic tests on individual servers and developing {{a model for a}} data center’s energy demand using these measurements. By analysing every server in a data <b>centre,</b> <b>server</b> power management software can identify servers that can be removed. It also enables servers to be virtualized, processes to be consolidated to a smaller number of servers, and servers with a predictable cyclical power demand to be fully powered down when not in use. Active power management features are also included which put remaining servers into their lowest power state that allows instant wake-up on demand when required.|$|R
40|$|The paper Works {{describes}} {{concept of}} cloud computing, his main characteristics. Cloud computing is new trend in IT. The company doesn't {{have your own}} <b>servers</b> and data <b>centres,</b> but everything is situated in cloud [...] in set of pc and server. The owners of these are suppliers of these services. The aim of the thesis is defined Cloud Computing concepts and evaluate by criterion if cloud computing application can compete to classic application. Introduction of the Works describe the gist of cloud computing, the models of cloud and history. The {{main part of the}} Works is about the suppliers. The last chapter is dedicated to evaluation of applications of cloud computing by comparison with other application...|$|R
40|$|This project {{report is}} about a Customer Service System with java {{programming}} language. It {{is built based on}} TCP / IP with threads and Binary Tree. This project consists of three main programs. It consists of one server and two client programs. <b>Server</b> is the <b>centre</b> of the data transfer. The Clients are Customer Service and Customers. Server runs as the background process and responsible in managing clients and their data transfers. While these two clients are connected to the server, server will connect them and forwards the data in accordance to the intended client. This program has no limitation on the number of clients connected to the server. When there are too many clients connected to the server, it can slow down the server performance...|$|R
40|$|This paper {{gives an}} {{overview}} of the functions of the proxy servers in accessing multimedia objects over the Internet. The proxy servers use the cache replacement policies to select the cold objects to be removed from cache to release space. They use the object partitioning methods to divide each multimedia object into cacheable and non-cacheable parts. When the client bandwidth is insufficient to receive the original object, the proxy servers convert multimedia objects into objects of lower resolution in order to adapt to the client characteristics. When multiple proxy servers are present, they may work together in a cooperative manner to further enhance the cache efficiency or in a distributed manner to evenly spread the load. In summary, proxy <b>servers</b> become the <b>centre</b> of management with respect to the delivery of multimedia objects from web servers to clients over the Internet...|$|R
40|$|The Open Compute Project. OCP ([URL] was {{launched}} by Facebook in 2011 {{with the objective}} of building efficient computing infrastructures at the lowest possible cost. The technologies are released as open hardware. with the goal to develop <b>servers</b> and data <b>centres</b> following the model traditionally associated with open source software projects. In 2013 CERN acquired a few OCP servers in order to compare performance and power consumption with standard hardware. The conclusions were that there are sufficient savings to motivate an attempt to procure a large scale installation. One objective is to evaluate if the OCP market is sufficiently mature and broad enough to meet the constraints of a public procurement. This paper summarizes this procurement. which started in September 2014 and involved the Request for information (RFI) to qualify bidders and Request for Tender (RFT) ...|$|R
40|$|This report {{summarizes}} the technology I {{used to provide}} a cheap lab console <b>server</b> for Swinburne's <b>Centre</b> for Advanced Internet Architectures (CAIA). The idea is lifted directly from Gregory Bond's FreeBSD. org article on building console servers. My only contribution here is to document a specific configuration used at CAIA - a VIA EPIA ESP 5000 "Mainboard" (motherboard), a Stallion Technologies EasyIO 4 port serial card (PCI), and the conserver software package. This report is {{for the benefit of}} anyone curious enough to try and replicate our scheme. In a nutshell: We'd like remote console access to a bunch of machines in our server room, Commercial console/terminal servers are rather expensive, FreeBSD supports multi-port serial cards, Purchase a multi-port serial card, Install the free 'conserver' software package: Logs the console outputs of devices attached to our serial ports, Allows interactive access to these device, Voila, we have a console server...|$|R
40|$|Temperature {{monitoring}} plays a {{major role}} in controlling it according to its varied conditions. Thisprocess is common in all critical areas like data <b>centre,</b> <b>server</b> rooms, grid rooms and other datacommunication equipped rooms. This is mandatory for each organization/industry to impart suchprocess, as most of the critical data would be in data centre along with their network infrastructure whichhaving various electronic, electrical and mechanical devices are involved for data transmissions. Thesedevices are very much depend on the environmental factors such as temperature, moisture, humidity etc.,and also emit heat in the form of thermal energy when they are in functional. To overcome these heats,the server/data centre room(s) would be engaged with multiple (distributed) air-conditioning (ac) systemsto provide cooling environment and maintain the temperature level of the room. The proposed paper isthe study of automization of monitoring and controlling temperature as per desired requirements withwsn networ...|$|R
40|$|Canadian {{retail banking}} {{customers}} expect more information, wider choices and greater convenience when they bank. At {{the same time}} banks are trying to reduce their costs in the traditional bank branch network as well as add many new products (stock sales, investments, new types of lending etc.). To fulfil these requirements, a new electronic retail banking service delivery mechanism has been introduced, the video banking kiosk. This is a stand-alone unit where retail customers, at remote locations, can interact with personal bankers (<b>servers)</b> at call <b>centres</b> via video conferencing and telecomputing capabilities. The results of our research present a framework for measuring with his/her workstation. Application of this framework can provide bank management with additional insight to plan all the requirements for server training. The framework contains suggestions for methodology to determine customer profiles, establish time standards, determine service quality delivered by the servers and evaluate workstation design. This framework {{was applied to the}} Royal Bank of Canada's 'Video Banking Services' pilot. ...|$|R
40|$|In {{order to}} {{minimise}} their energy use, data centre operators are constantly exploring {{new ways to}} construct computing infrastructures. As low power CPUs, exemplified by ARM-based devices, are becoming increasingly popular, {{there is a growing}} trend for the large scale deployment of low power <b>servers</b> in data <b>centres.</b> For example, recent research has shown promising results on constructing small scale data centres using Raspberry Pi (RPi) single-board computers as their building blocks. To enable larger scale experimentation and feasibility studies, cloud simulators could be utilised. Unfortunately, state-of-the-art simulators often need significant modification to include such low power devices as core data centre components. In this paper, we introduce models and extensions to estimate the behaviour of these new components in the DISSECT-CF cloud computing simulator. We show that how a RPi based cloud could be simulated {{with the use of the}} new models. We evaluate the precision and behaviour of the implemented models using a Hadoop-based application scenario executed both in real life and simulated clouds...|$|R
5000|$|Previously any client {{with good}} bandwidth, no {{restrictions}} due to firewall or {{network address translation}} (NAT), and adequate processing power could become a supernode. This placed an extra burden on those who connected to the Internet without NAT, as Skype used their computers and Internet connections as third parties for UDP hole punching (to directly connect two clients both behind NAT) or to completely relay other users' calls. In 2012, Microsoft altered {{the design of the}} network, and brought all supernodes under their control as hosted <b>servers</b> in data <b>centres.</b> [...] Microsoft at the time defended the move, saying they [...] "believe this approach has immediate performance, scalability and availability benefits for {{the hundreds of millions of}} users that make up the Skype community." [...] At the time there was some concern regarding the privacy implications of the change, which appear to have been proven true with the revelation of the PRISM surveillance program in June 2013.|$|R
40|$|Paper {{depicts the}} {{practical}} experience in creating digital repository of theses and dissertations {{available in the}} holdings of the Mysore University Library. Issues and challenges faced are discussed in obtaining access to soft copy of the doctoral theses and splitting them chapter-wise and finally converting into PDF. Various issues regarding the problems faced while up-loading on to the Shodhganga <b>Server</b> of UGC-INFLIBNET <b>Centre</b> {{on one hand and}} providing display so as to access and download the theses and dissertations by the researchers and academicians across the country on the other are highlighted. Attempt is made while studying various aspects pertaining to selection of dissertations submitted to the University as part fulfillment of various Masters’ Degree programmes including the project works of M. Phil courses. Authors have also discussed various constraints faced in up-dating and maintaining the digital repositories of Shodhganga, Vidyanidhi, Indian Institute of Science and such other popular repositories. Paper also deals with the issues which affect the user communities while attempting to do comprehensive survey of the research work done in the concerned subject discipline...|$|R
40|$|Abstract: This paper proposes and {{implements}} a {{low cost}} object tracking system using GPS and GPRS and GIS. The system allows a user {{to view the}} present and the past positions recorded of a target object on Google Map through the internet. The key features of the system are an open-source GIS platform, HTTP protocol, A web application is developed using PHP, JavaScript, Ajax and MySQL with the Google Map embedded and a communications server, a web-server, a database server, and a map <b>server.</b> The Monitoring <b>Centre</b> displays the above information on Google Map by means of Internet and sends commands to all the subsystems. The real time availability of all exact locations and speeds of the vehicles enables the system to encompass very clear traffic information Identity (IMEI) number as its own identity to the server. The data is checked for validity and the valid data is saved into the database. When a user wants to track the device, she/he logs into the service provider’s website and gets the live position of the device on Google Map...|$|R
40|$|Computing {{applications}} and data are growing {{so rapidly that}} increasingly larger <b>servers</b> and data <b>centre</b> are needed for fast processing within the required time. A fundamental shift in the way Information Technology (IT) and computing services are being delivered and purchased results {{in the development of}} cloud computing. The out of control cost of power in terms of electricity generation, personnel hardware and limited spaces in data centers have encouraged a significant number of enterprises to move more infrastructures into a third party provided Cloud. However, Cloud computing requires that organizations trust that a service provider’s platforms are secured and provide a sufficient level of integrity for the client’s data. Elliptical curve cryptography (ECC) is a public key encryption technique based on elliptic curve theory {{that can be used to}} create faster, smaller, and more efficient cryptographic keys. An important factor is the key strength, i. e. the difficulty in breaking the key and retrieving the plain text. In this paper, we proposed Elliptic Curve Cryptography scheme as a secure tool to model a Secured platform for th...|$|R
40|$|This paper {{tells the}} story of a {{collaborative}} trial project between the University of Southern Queensland (USQ) and Queensland Corrective Services, from its inception to the present stage of near completion. The project involved the use of internet-independent ICT for prisoner education. A major aim was to enable prisoners to greatly enhance their employment and further education prospects by developing their e-literacy/learning skills. The project involved the development of an internet-independent form of a USQ course Moodle site that could be placed on a correctional <b>centre</b> <b>server</b> intra-netted to computer labs for educational use by prisoners. Additionally, participating prisoners were individually supplied with internet-independent e-readers containing the course study materials. The trial commenced at the start of semester 2, 2012. Student support in the use of the Moodle site and the e-readers was provided by correctional centre staff and through regular visits by USQ Tertiary Preparation Program (TPP) teachers. The evaluation plan for the trial included gathering weekly feedback from the students via an evaluation instrument in the Moodle site, and from the correctional centre staff. The paper provides an account of the numerous challenges encountered and overcome by the project team, and a summary evaluation of the trial project. ...|$|R
40|$|Nowadays, the {{operation}} of tuition centre system poses difficulty to users which include teachers, parents and students. For example, the use of paper to keep student attendance can pose a difficulty to the teacher to record and retrieve student's attendance. The student may skip the tuition class and their parents do not know this thing happen. Furthermore, the student may forget their homework given by the tuition centre, hence not finishing the homework after back from tuition class. From the problems mentioned above, a survey has been conducted to get all this information from the parents, student and tuition centre staff. In order to solve these problems, this project proposed a new paradigm of web site leveraging on web services named Tuition Centre Interaction System (TCIS), which overcomes all these problems. Besides that, the TCIS will provide both the teacher and parents with up-to-date information about their student/children in term of attendance and homework in the tuition centre. On the other hand, parents able to check their children attendance and homework in the tuition class with the TCIS. In relation with that, student performance can be improved. TCIS will be implemented within a centralized <b>server</b> whereby tuition <b>centre</b> can just subscribe to the services that they want. This may reduce the high maintenance cost and development cost...|$|R
40|$|In a data <b>centre,</b> <b>server</b> {{clusters}} {{are typically}} {{used to provide}} the required processing capacity to provide acceptable response time performance to interactive applications. The workload of each application may be time-varying. Static allocation to meet peak demand is not an efficient usage of resources. Dynamic resource allocation, on the other hand, can result in efficient resource utilization while meeting the performance goals of individual applications. In this thesis, we develop a new interactive system model where the number of logon users changes over time. Our objective is to obtain results {{that can be used}} to guide dynamic resource allocation decisions. We obtain approximate analytic results for the response time distribution at steady state for our model. Using numerical examples, we show that these results are acceptable in terms of estimating the steady state probabilities of the number of logon users. We also show by comparison with simulation that our results are acceptable in estimating the response time distribution under a variety of dynamic resource allocation scenarios. More importantly, we show that our results are accurate in terms of predicting the minimum number of processor nodes required to meet the performance goal of an interaction application. Such information is valuable to resource provisioning and we discuss how our results can be used to guide dynamic resource allocation decisions...|$|R
30|$|The article {{considers}} {{the model and}} method of converged computing and storage to create SCADA systems based on wireless networks for the energy industry. Computing power of modern wireless sensor network nodes allow the transfer to them some operations sensor data mining and offload the dispatching data <b>centre</b> <b>servers.</b> This fog computing model {{is used for the}} aggregation of primary data, forecast trends controlled variables as well as to warn about abnormal and emergency situations on distributed SCADA systems objects. Large arrays of sensor data, integral indicators and heterogeneous information from other sources (e.g., weather stations, security and fire alarm systems, video surveillance systems, etc.) is more appropriate to process via GRID computing model. GRID computing model has three-tier architecture, which includes the main server at the first level, a cluster of servers at the second level, and a lot of GPU video card with support for Compute Unified Device Architecture at the third level. The model of cloud computing and cloud storage today is the basis for the accumulation of the results of data mining and knowledge discovery. Means of communication and remote access can solve the problem of intellectual processing and visualization of information with elements of augmented reality and geo-information technologies within the framework of mobile computing model. The implementation of these four computing models for the operation of components of SCADA system is the convergent approach to distributed sensor data processing, which is discussed in the article.|$|R
40|$|Since {{industrial}} devices create {{power dissipation}} {{in the form}} of heat created as a by-product, which can {{have a negative effect on}} their performance, certain temperature limit constraints are required for almost all these applications to work within suitable conditions. That is, these engineering devices might fail in some way if these limitations are surpassed by overheating. In all the related industries, inexorable increases in power densities are driving innovation in heat exchange techniques. Furthermore, electronic devices are becoming smaller at the same time as their thermal power generation increases. Thus, heat sinks can be applied for cooling critical components in many important applications ranging from aero-engines and nuclear reactors to computers, data <b>centre</b> <b>server</b> racks and other microelectronic devices. The most common cooling technique for heat dissipation for thermal control of electronics is air cooling. Reduced cost, simplicity of design, the easy availability of air, and increased reliability are the main benefits of this cooling method. Heat sinks with a fan/blower are commonly used for air-cooled devices as a forced convection heat transfer. An amount of heat is dissipated from the heat source to environmental air utilising a heat sink as a heat exchanger, which is a vital practice employed in air-cooling systems. This transfer mechanism is easy, simple and leads to reduced cost and increased reliability, and pinned heat sinks are more beneficial than plate fin heat sinks. The main interest of this study is to investigate the benefits of using perforated, slotted, and notched pinned heat sinks with different configurations to reduce CPU temperature and fan power consumption to overcome the pressure drop and maximise a heat transfer rate through the heat sink. An experimental heat sink with multiple perforations is designed and fabricated, and parameter studies of the effect of this perforated pin fin design on heat transfer and pressure drops across the heat sinks are undertaken, to compare it to solid pinned heat sinks without perforations. Experimental data is found to agree well with predictions from a CFD model for the conjugate heat transfer and turbulent airflow model into the cooling air stream. The validated CFD model is used to carry out a parametric study of the influence of the number and positioning of circular perforations, and slotted/notched pinned heat sinks. Then, the multi-objective optimum pinned heat sink designs are tested to obtain CPU temperature and fan power consumption as lowest as possible through the heat sink. In addition, the limitations in application of pinned heat sinks based on the pin density and applied heat flux are reported for active air-cooling electronic systems. An overview of the findings indicates that the CPU temperature, the fan power consumption, and the heat transfer rate in terms of Nusselt number are enhanced with the number of pin perforations and slotted/notched pinned heat sinks, while the locations of the pin perforations are much less influential. These benefits arise due to not only the increased surface area but also to the heat transfer enhancement near the perforations through the formation of localised air jets. Finally, the perforated heat sinks will be lighter in weight compared with solid pinned heat sinks. ...|$|R

