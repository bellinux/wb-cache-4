0|473|Public
40|$|This study investigates {{innovative}} <b>sound</b> <b>sequences</b> in Japanese. A {{relatively large}} number of phonological changes {{have occurred in the}} short period of time since WWII, mainly due to an influx of loanwords from English. However, innovative <b>sound</b> <b>sequences</b> have not been accepted in Japanese uniformly. This fact raises two questions. Why are some innovative <b>sound</b> <b>sequences</b> fully accepted in Japanese while others are still foreignisms? Why are certain <b>sound</b> <b>sequences</b> acceptable in one situation, but not so in others?Previous studies on innovative <b>sound</b> <b>sequences</b> in modern standard Japanese have tried to solve these problems by establishing innovative lexical strata, such as "Assimilated Foreign" and "Unassimilated Foreign. " However, this study found that the distribution of innovative <b>sound</b> <b>sequences</b> is much more complex than previously believed. Furthermore, in many cases, the acceptance of innovative <b>sound</b> <b>sequences</b> is word-by-word or speaker-by-speaker. This suggests that the cause of the distribution of innovative <b>sound</b> <b>sequences</b> in Japanese is better described as an intricate interaction among various extra-grammatical factors, such as processes of borrowing, speakers' socioeconomic status, influence of English education, acoustic and articulatory phonetics, the writing system, and historical linguistic factors...|$|R
50|$|The phonetic-phonological {{properties}} of an idiolect system are {{to a large}} degree determined by the way <b>sound</b> <b>sequences</b> combine to form more complex ones, and the way phonetic <b>sound</b> <b>sequences</b> are related to phonological ones. There is a 'connection function' on the phonological level that takes pairs of structured <b>sound</b> <b>sequences</b> and assigns to each pair another such sequence, and a 'connection function' on the phonetic level that takes such pairs and assigns to each pair a set of structured <b>sound</b> <b>sequences.</b> Both levels are connected through a 'variant relation' relating structured phonetic <b>sound</b> <b>sequences</b> to structured phonological <b>sound</b> <b>sequences.</b> While the two connection functions jointly represent the 'phonotactics' of the idiolect system, the variant relation is only partly analogous to the 'allophone' relation in structuralist phonology and avoids its problems (treatment of diphthongs, affricates etc.) by connecting structured phonetic with structured phonological <b>sound</b> <b>sequences</b> instead of connecting individual sounds. Phonetic variants of a complex structured phonological <b>sound</b> <b>sequence</b> may be determined not only phonetically but also phonologically, by underlying sequences on the phonological level. The variant relation is postulated as a third component of the sound system of an idiolect system, in addition to its phonetic and phonological parts.|$|R
40|$|One {{essential}} part of a native speaker’s knowledge is the characterization of what logically possible <b>sound</b> <b>sequences</b> constitute legitimate possible words in the speaker’s language, or phonotactics. Although formal phonotactic models were originally categorical—classifying every <b>sound</b> <b>sequence</b> dichotomously as eithe...|$|R
50|$|Phonological {{words and}} morphs (on the phonological level) {{as well as}} phonetic words (on the phonetic level) are {{construed}} as 'structured sound sequences,' that is, ordered pairs consisting of (1) a phonetic or phonological <b>sound</b> <b>sequence</b> (called the 'basis' of the morph or word) and (2) a phonetic or phonological structure of the <b>sound</b> <b>sequence</b> - itself a pair of (2a) a constituent structure and (2b) an intonation structure of the <b>sound</b> <b>sequence.</b> ('Sequence' is understood in the Integrational Theory of Language in a specific, set-theoretical sense that allows for the limiting cases of empty and one-member or 'unit sequences.') The constituent structure relates parts of the <b>sound</b> <b>sequence</b> {{to one of the}} sound categories Vocalic-in-S, Consonantal-in-S, and VocalicGroup-in-S, thereby determining a syllable sequence (possibly empty) for the <b>sound</b> <b>sequence.</b> The <b>sound</b> categories (simultaneously belonging to the phonetic and the phonological level) are uniformly construed as sets not of individual sounds but of <b>sound</b> <b>sequences</b> of the idiolect system, allowing a treatment of affricates and long consonants (elements of Consonantal-in-S), diphthongs and long vowels (elements of Vocalic-in-S) and the like alongside simple vowels and consonants. The intonation structure assigns sets of 'auditory values' (pitches, degrees of loudness, phonation modes etc.) to the syllables of a (syllabic) <b>sound</b> <b>sequence</b> identified by the constituent structure. Prosodic phenomena in both accent languages and tone languages are then treated in a unified way: differences of tone or stress are represented through sets of auditory values directly within a specific component of a phonological word, namely, the phonological intonation structure, which is properly linked to the (syntactic) intonation structures of syntactic units in which the phonological word occurs; and tone languages differ from accent languages mainly in the way phonological intonation structures are 'processed' in syntactic intonation structures. The constituents of a structured <b>sound</b> <b>sequence</b> are connected through phonological relations (p-nucleus, p-complement, p-modifier).|$|R
50|$|Films {{from this}} period often combine silent and <b>sound</b> <b>sequences.</b>|$|R
40|$|Beat and meter {{induction}} {{are considered}} important structuring mechanisms underlying {{the perception of}} rhythm. Meter comprises two or more levels of hierarchically ordered regular beats with different periodicities. When listening to music, adult listeners weight events within a measure in a hierarchical manner. We tested if listeners without advanced music training form such hierarchical representations for a rhythmical <b>sound</b> <b>sequence</b> under different attention conditions (Attend, Unattend, and Passive). Participants detected occasional weakly and strongly syncopated rhythmic patterns {{within the context of}} a strictly metrical rhythmical <b>sound</b> <b>sequence.</b> Detection performance was better and faster when syncopation occurred in a metrically strong as compared to a metrically weaker position. Compatible electrophysiological differences (earlier and higher-amplitude MMN responses) were obtained when participants did not attend the rhythmical <b>sound</b> <b>sequences.</b> These data indicate that hierarchical representations for rhythmical <b>sound</b> <b>sequences</b> are formed preattentively in the human auditory system...|$|R
40|$|We have {{previously}} reported that <b>sound</b> <b>sequence</b> discrimination learning requires cholinergic inputs to the auditory cortex (AC) in rats. In that study, reward {{was used for}} motivating discrimination behavior in rats. Therefore, dopaminergic inputs mediating reward signals may {{have an important role}} in the learning. We tested the possibility in the present study. Rats were trained to discriminate <b>sequences</b> of two <b>sound</b> components, and licking behavior in response to one of the two sequences was rewarded with water. To identify the dopaminergic inputs responsible for the learning, dopaminergic afferents to the AC were lesioned with local injection of 6 -hydroxydopamine (6 -OHDA). The injection attenuated <b>sound</b> <b>sequence</b> discrimination learning, while it had no effect on discrimination between the sound components of the sequence stimuli. Local injection of 6 -OHDA into the nucleus accumbens attenuated sound discrimination learning. However, not only discrimination learning of <b>sound</b> <b>sequence</b> but also that of the sound components were impaired. SCH 23390 (0. 2 mg/kg, i. p.), a D 1 receptor antagonist, had no effect on <b>sound</b> <b>sequence</b> discrimination learning, while it attenuated the licking behavior to unfamiliar stimuli. Haloperidol (0. 5 mg/kg, i. p.), a D 2 family antagonist, attenuated <b>sound</b> <b>sequence</b> discrimination learning, while it had no clear suppressive effect on discrimination of two different sound components and licking. These results suggest that D 2 family receptors activated by dopaminergic inputs to the AC are required for <b>sound</b> <b>sequence</b> discrimination learning...|$|R
5000|$|A bi-annual Slide <b>Sound</b> <b>sequences</b> weekend run in {{conjunction}} with the Royal Photographic Society: ...|$|R
25|$|Brosch M, Schulz A, Scheich H (1999) Processing of <b>sound</b> <b>sequences</b> in macaque {{auditory}} cortex: response enhancement. J Neurophysiol 82:1542-1559.|$|R
25|$|Children produce mostly adult-like segments. Their {{ability to}} produce complex <b>sound</b> <b>sequences</b> and multisyllabic words {{continues}} to improve throughout middle childhood.|$|R
5000|$|If {{this process}} {{would lead to}} the <b>sound</b> <b>sequence</b> , the epenthetic vowel is an [...] in many dialects. Example; dearg [...]|$|R
40|$|This study {{explored}} the statistical patterns of English and Mandarin Chinese <b>sound</b> <b>sequences,</b> by comparing their learning {{in a simple}} recurrent network. Experiment 1 showed that vivid syllable structure emerged from the <b>sound</b> <b>sequence</b> of Mandarin Chinese. Experiment 2 further demonstrated that the emerged syllable structure of Mandarin Chinese is considerably more salient than that of English. We claim that the more salient syllable structure in Mandarin Chinese inputs {{is one reason why}} syllable units are particularly emphasized in its processing in comparison to English...|$|R
40|$|This paper {{presents}} {{a new approach}} to sound composition for soundtrack composers and sound designers. We propose a tool for usable sound manipulation and composition that targets sound variety and expressive rendering of the composition. We first automatically segment audio recordings into atomic grains which are displayed on our navigation tool according to signal properties. To perform the synthesis, the user selects one recording as model for rhythmic pattern and timbre evolution, and a set of audio grains. Our synthesis system then processes the chosen sound material to create new <b>sound</b> <b>sequences</b> based on onset detection on the recording model and similarity measurements between the model and the selected grains. With our method, we can create a large variety of sound events such as those encountered in virtual environments or other training simulations, but also <b>sound</b> <b>sequences</b> that can be integrated in a music composition. We present a usability-minded interface that allows to manipulate and tune <b>sound</b> <b>sequences</b> in an appropriate way for sound design...|$|R
40|$|Two {{experiments}} {{examined the}} effects of phonotactic probability and neighborhood density on word learning by 3 -, 4 -, and 5 -year-old children. Nonwords orthogonally varying in probability and density were taught with learning and retention measured via picture naming. Experiment 1 used a within-story probability/across-story density exposure context. Experiment 2 used an across-story probability/within-story density exposure context. Results showed that probability and density interacted to create optimal learning conditions. Specifically, rare/sparse <b>sound</b> <b>sequences</b> appeared to facilitate triggering of word learning. In contrast, the optimal convergence for lexical configuration and engagement was dependent on exposure context. In particular, common <b>sound</b> <b>sequences</b> and dense neighborhoods were optimal when density was manipulated across stories, whereas rare <b>sound</b> <b>sequences</b> and sparse neighborhoods were optimal when density was manipulated within a story. Taken together, children’s phonological and lexical representations were hypothesized to be interdependent on one another resulting in a convergence of form characteristics for optimal word learning...|$|R
40|$|Temporally {{unpredictable}} stimuli influence murine {{and human}} behaviour, as previously demonstrated for <b>sequences</b> of simple <b>sounds</b> with regular or irregular onset. It is unknown whether this influence is mediated by {{an evaluation of}} the unpredictable <b>sound</b> <b>sequences</b> themselves, or by an interaction with task context. Here, we find that humans evaluate unrelated neutral pictures as more negative when these are presented together with a temporally unpredictable <b>sound</b> <b>sequence,</b> compared to a predictable sequence. The same is observed for evaluation of neutral, angry and fearful face photographs. Control experiments suggest this effect is specific to interspersed presentation of negative and neutral visual stimuli. Unpredictable sounds presented on their own were evaluated as more activating, but not more aversive, and were preferred over predictable sounds. When presented alone, these <b>sound</b> <b>sequences</b> also did not elicit tonic autonomic arousal or negative mood change. We discuss how these findings might account for previous data on the effects of unpredictable sounds, in humans and rodents...|$|R
40|$|SummaryNeurons in {{auditory}} cortex {{are sensitive}} to the probability of stimuli: responses to rare stimuli tend to be stronger than responses to common ones. Here, intra- and extracellular recordings from the auditory cortex of halothane-anesthetized rats revealed the existence of a finer sensitivity to the structure of <b>sound</b> <b>sequences.</b> Using oddball sequences in which the order of stimulus presentations is periodic, we found that tones in periodic sequences evoked smaller responses than the same tones in random sequences. Significant reduction in the responses to the common tones in periodic relative to random sequences occurred even when these tones consisted of 95 % of the stimuli in the sequence. The reduction in responses paralleled the complexity of the <b>sound</b> <b>sequences</b> and could not be explained by short-term effects of clusters of deviants on succeeding standards. We conclude that neurons in auditory cortex {{are sensitive to}} the detailed structure of <b>sound</b> <b>sequences</b> over timescales of minutes...|$|R
5000|$|Nonword reading: reading {{lists of}} pronounceable {{nonsense}} words out loud. The difficulty is increased by using longer words, {{and also by}} using words with more complex spelling or <b>sound</b> <b>sequences.</b>|$|R
5000|$|The Latin letter Ë (E-umlaut) has no {{relation}} to the Cyrillic letter Ё (Yo). The Latin letter Ë represents the <b>sound</b> <b>sequence</b> [...] and thus corresponds to the Cyrillic letter Є in Ukrainian or Е in Russian.|$|R
50|$|White Cargo is a 1929 British drama film {{directed}} by J.B. Williams and starring Leslie Faber, John F. Hamilton and Maurice Evans. Originally made at Twickenham Studios as a silent film, it had <b>sound</b> <b>sequences</b> added at Elstree Studios.|$|R
5000|$|In 1961, he {{starred in}} The Twilight Zone episode [...] "Once Upon a Time", which {{included}} both silent and <b>sound</b> <b>sequences.</b> Keaton played time-traveler Mulligan, who traveled from 1890 to 1960, then back, {{by means of}} a special helmet.|$|R
50|$|In the {{beginning}} of the song you can hear a signal, that is taken from the film Alien. It is the <b>sound</b> <b>sequence</b> when the S.O.S. signal appears on the screens of the spaceship Nostromo {{at the start of the}} film.|$|R
50|$|This table {{describes}} the main developments of Middle English diphthongs, {{starting with the}} Old English <b>sound</b> <b>sequences</b> that produced them (sequences of vowels and g, h or w) and ending with their Modern English equivalents. Many special cases have been ignored.|$|R
40|$|Mass-interaction {{physical}} modeling {{is one of}} the few formalisms {{that can}} unify the work on music composition and sound synthesis. It allows generating <b>sound</b> <b>sequences</b> that exhibit, for example, some of the qualities of instrumental performance. This article introduces a method for building mass-interaction models whose physical structure changes during the simulation. Structural evolution is implemented in a physically consistent manner, by using nonlinear interactions that set temporary viscoelastic links between simulated objects. We present in details a model built with this method. It produces a wide range of complex <b>sound</b> <b>sequences,</b> the user having a control over global aspects of its behavior. This example shows that evolving models are particularly useful for the generation of macrotemporal musical forms. ...|$|R
40|$|Vehicle {{accidents}} are complicated {{events and the}} resulting <b>sound</b> <b>sequence</b> created by that event is equally complex. Understanding how accident sounds are created is important for two main reasons. One reason is {{to better understand the}} events in an accident sequence that have not left visible physical evidence. There are often witnesses to vehicle accidents, and their observations include what they saw as well as what they heard. This information is useful when reconstructing the accident. Another reason {{is to be able to}} create more accurate simulated sound composites of an accident for use in forensic visualization, a visual/auditory tool that helps one understand a dynamic accident sequence that they were not able to see in person. While the composite accident sound is complicated when analyzed as an entire <b>sound</b> <b>sequence</b> it is still derived from discreet parts, and by analyzing the discreet parts individually one is able to better understand the contri-bution each individual sound makes to the entire accident <b>sound</b> <b>sequence.</b> This paper looks at some of the discreet sounds in a vehicle accident, and evaluates how changes in the parameters of the accident sequence affect the resulting sounds...|$|R
30|$|The {{character}} ‘|’ (آ) {{is pronounced}} as {{a sequence of}} a consonant (glottal stop) and a long vowel. Each member of the sequence has its own HMM in Phonetic methods, whereas the Grapheme methods utilize a single new HMM for the <b>sound</b> <b>sequence.</b>|$|R
50|$|The Vitaphone sound-on-disc {{system was}} {{employed}} for <b>sound</b> <b>sequences.</b> Discs 6 and 8 {{are in the}} UCLA Film and Television Archive. Other sound discs to this film were donated by Arthur Lennig to the George Eastman House Motion Picture Collection in Rochester, New York.|$|R
5000|$|The words origin, Florida, horrible, quarrel, warren, {{as well as}} tomorrow, sorry, sorrow,, etc. are all {{generally}} use the <b>sound</b> <b>sequence</b> [...] (as in gory), {{rather than}} [...] (as in starry) or [...] The latter set of words often distinguishes Canadian pronunciation from U.S. pronunciation.|$|R
40|$|International audienceMass-interaction {{physical}} modeling {{is one of}} the few formalisms {{that can}} unify the work on music composition and sound synthesis. It allows generating <b>sound</b> <b>sequences</b> that exhibit, for example, some of the qualities of instrumental performance. This article introduces a method for building mass-interaction models whose physical structure changes during the simulation. Structural evolution is implemented in a physically consistent manner, by using nonlinear interactions that set temporary viscoelastic links between simulated objects. We present in details a model built with this method. It produces a wide range of complex <b>sound</b> <b>sequences,</b> the user having a control over global aspects of its behavior. This example shows that evolving models are particularly useful for the generation of macrotemporal musical forms...|$|R
50|$|Complex {{perceptions}} like {{voice recognition}} or discrimination of similar <b>sound</b> <b>sequences</b> need experience and can generally be trained.Complex perceptions are often multi-sensory perceptions.Example: To distinguish between a real apple and fake the subject needs to touch or to smell it, because visual inspection may be insufficient.|$|R
5000|$|Although {{the episode}} was wiped by the BBC and no copy {{is known to}} exist, three of Delia Derbyshire's <b>sound</b> <b>sequences</b> were {{published}} on a BBC record of sound effects Out of This World, renamed as [...] "Heat Haze", [...] "Frozen Waste" [...] and Icy Peak".|$|R
5000|$|Otto Lederer (April 17, 1886 [...] - [...] September 3, 1965) was an Austria-Hungary-born American film actor. He {{appeared}} in 120 films between 1912 and 1933, most notably The Jazz Singer, the first full-length film to have <b>sound</b> <b>sequences,</b> and the Laurel and Hardy short You're Darn Tootin'.|$|R
40|$|In {{this paper}} we {{present the results}} of a {{comparative}} study that explores the potential benefits of using embodied concepts related to musical sounds. Forty children learned to create musical <b>sound</b> <b>sequences</b> using an interactive sound making environment. Half the children used a version of the system that instantiated a body-based metaphor in the mapping layer connecting body movements to output sounds. The remaining children used a version of the same environment that did not instantiate a metaphor in the mapping layer. In general, children were able to more accurately demonstrate <b>sound</b> <b>sequences</b> in the embodied metaphor based system version. However, we observed that children often resorted to spatial rather than body-based metaphors and that the mapping must be easily discoverable as well as metaphorical to provide benefit. Author Keywords Embodied interaction, embodied schema, metaphor...|$|R
3000|$|In practice, three {{different}} events could be distinguished: a single muon reaching {{a pair of}} detectors (by successively hitting a detector placed above the ceiling, then one located under the floor), a [...] "small bunch," [...] where more than one, but less than four pairs of detectors are hit simultaneously, and a [...] "large bunch," [...] when at least four pairs are hit. The three cases corresponded to different <b>sound</b> <b>sequences</b> (<b>sound</b> examples can be found at: [URL] [...]...|$|R
50|$|To make an {{international}} version, the studio would simply insert (on the soundtrack) music over any dialogue {{in the film}} and splice in intertitles (which would be replaced with the appropriate language of the country). Singing sequences were left intact {{as well as any}} <b>sound</b> <b>sequences</b> that did not involve speaking.|$|R
5000|$|Other {{than a few}} foreign words, morpheme-initial [...] doesnt occur (even its phonemic {{state is}} highly debated), {{therefore}} {{it is hard to}} find a real example when it induces voicing (even alapdzadzíki is forced and not used colloquially). However, the regressive voice assimilation before [...] does occur even in nonsense <b>sound</b> <b>sequences.</b>|$|R
40|$|This {{dissertation}} introduces {{methods for}} extracting synthesis {{information from the}} <b>sound</b> <b>sequence</b> of musical instruments where a 2 ̆ 2 <b>sound</b> <b>sequence</b> 2 ̆ 2 is the digital representation of a one-dimensional acoustical pressure function. These methods are based on modern digital signal processing modeling techniques such as auto-regressive modeling. Particular attention {{is placed on the}} implementation of the McIntyre-Schumacher-Woodhouse model (J. Acoust. Soc. Am. 74, 1325 - 1345, 1983) where a non-linear controller and a linear element interact to produce sound. It is shown that the specific characteristics of a musical instrument waveform may be preserved in the synthesized version of the waveform. It is also shown that reflection functions are obtainable from estimates of the impulse response when the estimation is performed on the release transient of certain classes of musical instruments (most notably, air-reed). ...|$|R
