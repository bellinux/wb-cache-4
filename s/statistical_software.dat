4677|1052|Public
5|$|JMP (<b>statistical</b> <b>software),</b> {{also from}} SAS Institute Inc.|$|E
25|$|Use of {{the sample}} {{standard}} deviation implies that these 14 fulmars are a sample from a larger population of fulmars. If these 14 fulmars comprised the entire population (perhaps the last 14 surviving fulmars), then instead {{of the sample}} standard deviation, the calculation would use the population standard deviation. In the population standard deviation formula, the denominator is N instead of N-1. It is rare that measurements can be taken for an entire population, so, by default, <b>statistical</b> <b>software</b> packages calculate the sample standard deviation. Similarly, journal articles report the sample standard deviation unless otherwise specified.|$|E
25|$|Data was {{gathered}} primarily {{by means of}} subjective report interviews, conducted according to a structured questionnaire memorized by the experimenters (but not marked on the response sheet in any way). The response sheets were encoded in this way to maintain the confidentiality of the respondents, being entered on a blank grid using response symbols defined in advance. The data were later computerized for processing. All of this material, including the original researchers' notes, remains available from the Kinsey Institute to qualified researchers who demonstrate a need to view such materials. The institute also allows researchers to use <b>statistical</b> <b>software</b> (such as PSPP or SPSS) in order to analyze the data.|$|E
30|$|All {{statistical}} computations {{were calculated}} using the R <b>statistical</b> package, <b>software</b> version 3.3. 3.|$|R
30|$|The {{statistical}} analysis of this study has been performed using JMP <b>statistical</b> discovery <b>software</b> (2010).|$|R
30|$|Analysis was {{completed}} using both the <b>Statistical</b> Analysis <b>Software</b> version 9.3 (SAS Institute, Cary, NC) and R Statistical Library (R Core Team, 2013).|$|R
25|$|With the {{introduction}} of <b>statistical</b> <b>software</b> that can process large datasets easily, a number of state- or country-wide studies {{have been carried out}} to investigate whether birthdays have any effect on mortality. The first large-scale study used the records of 2,745,149 Californians who died between 1969 and 1990. After correcting for confounding factors such as seasonality in deaths, elective surgery, and people born on February 29, there was a significant increase in deaths in the week before the individual's birthday for men, and in the week after the birthday for women â€“ in both cases, mortality did not peak on the birthday, but close to it. This effect was consistent across age and race cohorts.|$|E
25|$|Thus {{computing}} a p-value {{requires a}} null hypothesis, a test statistic (together with deciding whether the researcher is performing a one-tailed test or a two-tailed test), and data. Even though computing the test statistic on given data may be easy, computing the sampling distribution under the null hypothesis, and then computing its {{cumulative distribution function}} (CDF) is often a difficult problem. Today, this computation is done using <b>statistical</b> <b>software,</b> often via numeric methods (rather than exact formulae), {{but in the early}} and mid 20th century, this was instead done via tables of values, and one interpolated or extrapolated p-values from these discrete values. Rather than using a table of p-values, Fisher instead inverted the CDF, publishing a list of values of the test statistic for given fixed p-values; this corresponds to computing the quantile function (inverse CDF).|$|E
2500|$|One can {{estimate}} a normal-ogive latent trait {{model by}} factor-analyzing {{a matrix of}} tetrachoric correlations between items. [...] This means it is technically possible to estimate a simple IRT model using general-purpose <b>statistical</b> <b>software.</b>|$|E
30|$|<b>Statistical</b> {{analysis}} <b>software</b> ([SAS 1985]) {{was used}} to test variations between treatments, and the least significant difference (LSD) {{was used to}} determine differences between treatment means.|$|R
50|$|MaxStat is a <b>statistical</b> {{analysis}} <b>software</b> platform {{specifically designed}} for students and researchers with little background in statistics. It was developed in Germany by MaxStat Software.|$|R
50|$|NumXL's <b>statistical</b> {{analysis}} <b>software</b> {{is compatible}} with all Excel versions from version 97 to version 2013 (Office 365), and with Windows versions 9x to Windows 8 (32- and 64-bits).|$|R
2500|$|Preston's {{theory has}} an {{interesting}} application: if a community is truly lognormal yet under-sampled, the lognormal distribution {{can be used}} to estimate the true species richness of a community. Assuming the shape of the total distribution can be confidently predicted from the collected data, the normal curve can be fit via <b>statistical</b> <b>software</b> or by completing the Gaussian formula: ...|$|E
2500|$|Increased {{computing}} power has {{also led to}} {{the growing popularity of}} computationally intensive methods based on resampling, such as permutation tests and the bootstrap, while techniques such as Gibbs sampling have made use of Bayesian models more feasible. The computer revolution has implications for the future of statistics with new emphasis on [...] "experimental" [...] and [...] "empirical" [...] statistics. A large number of both general and special purpose <b>statistical</b> <b>software</b> are now available.|$|E
2500|$|The plots {{are often}} made on {{logarithmic}} scales. Because different fluorescent dyes' emission spectra overlap, signals at the detectors {{have to be}} compensated electronically as well as computationally. Data accumulated using the flow cytometer can be analyzed using software, e.g., JMP (<b>statistical</b> <b>software),</b> WinMDI, Flowing Software, and web-based Cytobank (all [...] ), Cellcion, , FlowJo, FACSDiva, CytoPaint (aka Paint-A-Gate), VenturiOne, CellQuest Pro, Infinicyt or Cytospec. Once the data is collected, {{there is no need}} to stay connected to the flow cytometer and analysis is most often performed on a separate computer. This is especially necessary in core facilities where usage of these machines is in high demand.|$|E
5000|$|Synoptic {{signature}} analysis, <b>statistical</b> analysis, and <b>software</b> visualization tools ...|$|R
30|$|In Patient {{material}} VI, conditional {{logistic regression}} {{analysis was used to}} estimate odds ratios (OR:s) and confidence intervals (CI:s), using the proportional hazards regression procedure in <b>statistical</b> analysis <b>software</b> (SAS).|$|R
5000|$|Working with Time Series data is a {{relatively}} common use for <b>statistical</b> analysis <b>software.</b> As a result of this, there are many offerings both commercial and open source. Some examples include: ...|$|R
2500|$|Markov Chain Monte Carlo (MCMC) is a {{flexible}} procedure {{designed to fit}} a variety of Bayesian models. It is the underlying method used in computational software such as the LaplacesDemon R Package and WinBUGS. [...] The advancements and developments {{of these types of}} <b>statistical</b> <b>software</b> have allowed for the growth of Bayes by offering ease of calculation. This is achieved by the generation of samples from the posterior distributions, which are then used to produce a range of options or strategies which are allocated numerical weights. MCMC obtains these samples and produces summary and diagnostic statistics while also saving the posterior samples in the output. The decision maker can then assess the results from the output data set and choose the best option to proceed.|$|E
2500|$|Revolution Analytics (formerly REvolution Computing) is a <b>statistical</b> <b>software</b> company {{focused on}} {{developing}} open source and [...] "open-core" [...] {{versions of the}} free and open source software R for enterprise, academic and analytics customers. Revolution Analytics was founded in 2007 as REvolution Computing providing support and services for R in a model similar to Red Hat's approach with Linux in the 1990s as well as bolt-on additions for parallel processing. In 2009 the company received nine million in venture capital from Intel along with a private equity firm and named Norman H. Nie as their new CEO. In 2010 the company announced the name change {{as well as a}} change in focus. Their core product, Revolution R, would be offered free to academic users and their commercial software would focus on big data, large scale multiprocessor (or [...] "high performance") computing, and multi-core functionality.|$|E
2500|$|Functionalist systems theorists such as Niklas Luhmann {{remained}} dominant {{forces in}} sociology {{up to the}} end of the century. In 1994, Robert K. Merton won the National Medal of Science for his contributions to the sociology of science. The positivist tradition is popular to this day, particularly in the United States. The discipline's two most widely cited American journals, the American Journal of Sociology and the American Sociological Review, primarily publish research in the positivist tradition, with ASR exhibiting greater diversity (the British Journal of Sociology, on the other hand, publishes primarily non-positivist articles). [...] The twentieth century saw improvements to the quantitative methodologies employed in sociology. [...] The development of longitudinal studies that follow the same population over the course of years or decades enabled researchers to study long-term phenomena and increased the researchers' ability to infer causality. The increase in the size of data sets produced by the new survey methods was followed by the invention of new statistical techniques for analyzing this data. Analysis of this sort is usually performed with <b>statistical</b> <b>software</b> packages such as SAS, Stata, or SPSS.|$|E
30|$|The {{data were}} {{collected}} in summer 2014 and analyzed in autumn 2014 to spring 2015. All statistical analyses were conducted with the <b>statistical</b> analysis <b>software</b> package SPSS Statistics (version 22) from IBM.|$|R
40|$|AbstractIn {{order to}} solve {{problems}} existing in household travel survey, such as tedious designing work of survey forms, single survey method, single function, poor pertinence of existing data <b>statistical</b> analysis <b>software,</b> and the difficulty to examine data validation, a survey data <b>statistical</b> analysis <b>software</b> named OD Star had been developed for engineering. The OD Star sets a series functions of independent designing and generating of the questionnaire, data entry, data validation, statistical analysis, graphic charts generation and engineering management. The main improved points of the software lie in highly integrated function, key technologies of statistical analysis, examination of data validation and so on...|$|R
30|$|Statistical {{analyses}} were performed with the SPSS <b>statistical</b> analysis <b>software</b> (SPSS, Chicago, IL, USA). A difference {{was considered to be}} statistically significant when the P value was < 0.05 (using a two-tailed test).|$|R
5000|$|The Journal of <b>Statistical</b> <b>Software</b> is a peer-reviewed {{open access}} {{scientific}} journal that publishes papers related to <b>statistical</b> <b>software.</b> The Journal of <b>Statistical</b> <b>Software</b> {{was founded in}} 1996 by Jan de Leeuw of the Department of Statistics at the University of California, Los Angeles. Its current Editors-in-Chief are Achim Zeileis, Bettina GrÃ¼n, Edzer Pebesma, and Torsten Hothorn. It is published by the Foundation for Open Access Statistics. The journal charges no author fees or subscription fees.|$|E
5000|$|NCSS (<b>statistical</b> <b>software)</b> {{includes}} hierarchical cluster analysis.|$|E
50|$|Most <b>statistical</b> <b>software</b> can do binary {{logistic}} regression.|$|E
30|$|Questionnaire {{data was}} {{analysed}} using <b>statistical</b> analysis <b>software</b> R (R Foundation for Statistical Computing, Austria). Initially, playersâ€™ perceptions were {{considered as a}} whole followed by more in-depth interrogation of the data by surface experience.|$|R
50|$|In the 1970s, Poliakov co-authored {{the second}} {{mathematical}} statistics software suite. The first <b>statistical</b> analysis <b>software</b> suite {{was developed in}} the USSR at the Arctic and Antarctic Research Institute and described in a book Algorithms and Programs for Computer-Aided Statistical Data Analysis by E.P. Borisenkov and M.A. Romanov Although the <b>statistical</b> analysis <b>software</b> suite developed by the Uralmash Research Institute was completed in 1968, the information about it became available only in 1970 in a book Algorithms of Computer-Aided Statistical Data Analysis by F.M. Karlinskaya, Yu.D. Makarov, B.N. Poliakov. in the USSR, which, alongside system analysis, became widely used in algorithm engineering for optimal automatic operation of reversible rolling mills.|$|R
30|$|The data then {{analyzed}} through statistical {{control charts}} (X-bar and S charts) and Cp and Cpk values calculated through <b>statistical</b> analysis <b>software.</b> Based {{on the same}} the present sigma level was worked as 3.24. FigureÂ  4 illustrates the same.|$|R
5000|$|Stata <b>statistical</b> <b>software</b> {{includes}} multilevel mixed-effects models analysis.|$|E
5000|$|NCSS (<b>statistical</b> <b>software)</b> {{includes}} longitudinal mixed models analysis.|$|E
5000|$|The journal publishes peer-reviewed {{articles}} about <b>statistical</b> <b>software,</b> {{together with the}} source code.It also publishes reviews of <b>statistical</b> <b>software</b> and books (by invitation only). Articles are licensed under the Creative Commons Attribution License, while the source codes distributed with articles are licensed under the GNU General Public License.|$|E
30|$|Normality of {{population}} distribution was evaluated {{before and after}} outlier exclusions using JMPÂ® <b>Statistical</b> Discovery <b>Software,</b> version 10.0 (SAS Institute Inc., Cary, NC) for screening analysis. Assay cut point determination is described above. The individual samples from the treatment-naive ALL diseased human serum population (n[*]=[*] 50) were each analyzed at a minimum of 3 independent runs for screening for â‰¥[*] 2 Â days with â‰¥[*] 2 analysts. A cut point factor was calculated by dividing the plate cut point by the mean response of NC for each plate and averaging each plate cut point factor. Normality {{of population}} distribution was evaluated before and after outlier exclusions using JMP <b>Statistical</b> Discovery <b>Software,</b> version 10.0 (SAS Institute Inc.) for screening analysis.|$|R
30|$|The {{correlation}} {{of the two}} groups was analyzed using the Ï‡ 2 test or Fisherâ€™s exact test as appropriate. Statistical analyses were performed using a <b>statistical</b> analysis <b>software</b> package (Version 21; IBM, Armonk, NY), and p values < 0.05 were considered significant.|$|R
30|$|Significant {{outliers}} {{were identified}} by Grubb tests and removed from the raw data set. Subsequently, data were analysed using the GraphPad Prism 5.04 (GraphPad Software Inc., San Diego, CA, USA) <b>statistical</b> package <b>software.</b> Significance was set at P[*]â‰¤[*] 0.05 for all statistical analyses.|$|R
