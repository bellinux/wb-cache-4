7|558|Public
50|$|Good storage {{practices}} {{help ensure}} the long-life of an artwork {{and should be}} incorporated into a museum's conservation <b>strategy.</b> <b>Storage</b> practices vary by media, so a museum will need to employ a range of storage options to best care for their time-based media art collections. A variety of physical media, such as film, tape, and disks must be stored properly to prevent physical loss. Prevention is the best measure of protection against loss. How media is stored will be dependent {{on the type of}} media it is, film has different considerations than disks or videotape. Generally, time-based media storage will require cool temperatures and low humidity.|$|E
40|$|This paper {{addresses}} {{the problem of}} storage assignment in a warehouse characterized by multi-command picking and served by milkrun logistics. In such a logistic system, vehicles circulate between the warehouse and the production facilities of the plant according to a pre-defined schedule, often with multiple cycles (routes) serving different departments. We assume that a request probability can be assigned to each item and each cycle, {{which leads to a}} special case of the correlated storage assignment problem. A MIP model is proposed for finding a class-based storage policy that minimizes the order cycle time, the average picking effort, or a linear combination of these two criteria. Computational experiments show that our approach can achieve an up to 36 - 38 % improvement in either criterion compared to the classical COI-based <b>strategy.</b> <b>Storage</b> assignment Correlated Warehousing Milkrun Mixed-integer programming...|$|E
40|$|We {{formulate}} a robust optimal control {{problem for a}} general nonlinear system with finitely many admissible control settings and with costs assigned to switching of controls. We formulate the problem both in an L 2 -gain/dissipative system framework and in a game-theoretic framework. We show that, under appropriate assumptions, a continuous switching-storage function is characterized as a viscosity supersolution of the appropriate system of quasivariational inequalities (the appropriate generalization of the Hamilton-Jacobi-Bellman-Isaacs equation for this context), and that the minimal such switching-storage function {{is equal to the}} continuous switching lower-value function for the game. Finally we show how a prototypical example with one-dimensional state space can be solved by a direct geometric construction. Key Words. running cost, switching cost, worst-case disturbance attenuation, di#erential game, state-feedback control, nonanticipating <b>strategy,</b> <b>storage</b> function, lower value function, system of quasivariational inequalities, viscosity solution AMS Classification. Primary: 49 J 35; Secondary: 49 L 20, 49 L 25, 49 J 35, 93 B 36, 93 B 52 Abbreviated title. Robust optimal switching control. ...|$|E
40|$|Abstract. The {{inventory}} management problem with uncertain demands is considered in this paper. the possibility theory and credibility theory are employed {{to solve the}} problem regarding uncertain demands, and different order <b>strategies</b> and <b>storage</b> <b>strategies</b> according to the different customer’s demand are designed for rational {{inventory management}}. Finally, a rule is designed for choosing the <b>storage</b> <b>strategy...</b>|$|R
40|$|Abstract—When {{wireless}} sensor {{networks are}} employed for event monitoring, such as fire detection and enemy movement monitoring, the observers {{are more interested in}} the monitored events rather than the readings from sensors. To answer event queries such as “Where was the fire detected during 2 - 6 pm?”, an energy-efficient query processing technique is required. This paper presents a data-centric <b>storage</b> <b>strategy,</b> called CM-DCS, and also proposes two distributed event query processing algorithms. Furthermore, the energy consumptions for query processing methods based on three kinds of <b>storage</b> <b>strategies</b> namely external <b>storage,</b> local storage and CM-DCS are analyzed and compared, so that users can have a guideline of choosing a correct <b>storage</b> <b>strategy</b> for different applications. Theoretical analysis and simulation results show that the event query processing algorithm based on CM-DCS can save more energy than those algorithms based on the external <b>storage</b> <b>strategy</b> and the local <b>storage</b> <b>strategy</b> in most cases. Keywords-event formulation; event query processing; data centric storage; {{wireless sensor networks}} I...|$|R
40|$|In {{this work}} an optimum <b>storage</b> <b>strategy</b> is {{proposed}} for fresh water and waste water minimization in multipurpose batch plants. Three intermediate <b>storage</b> <b>strategies</b> considered for cost comparison. The graphical approach of representation of water composite curves {{and the concept}} of water surplus which is similar to the grand composite curve in heat pinch analysis is applied here for targeting fresh water and wastewater minimization. Analysis of results reveals that intermediate <b>storage</b> <b>strategy</b> helps in reuse of low COD wash water for further washings. This strategy reduces fresh water intake by approximately 40 % for the industrial case study under consideration...|$|R
40|$|New carioca bean cultivars {{are being}} {{introduced}} into the market necessitating their evaluation under trade conditions, which often require storage under ambient conditions. We therefore evaluated the darkening and hardening processes of six carioca bean genotypes each representing regular and slow darkening trait during storage under ambient conditions for five months to elucidate their relationship as a breeding <b>strategy.</b> <b>Storage</b> time adversely affected color characteristics (L*, a*, b*, C* and ?E) depending on bean genotype, whereas hardness and resistance to cooking increased during storage independent of the lignification process. Bean darkening and hardening occurred during storage at different intensities in each genotype and were not always correlated. BRSMG-Madrepérola, a slow darkening genotype, was unaffected (resistant to storage conditions), whereas BRS-Pontal with regular tegument darkening, was highly susceptible to storage conditions reflected in extended cooking time and darkening (low L* values). Principal component and cluster analyses on 8 constituents analyzed in this study demonstrate the difference in color characteristics, cooking time and hardness as major factors in segregating the bean genotypes. Seed coat color is an important but inappropriate single parameter for predicting the resistance to cooking or hardness induced by storage of carioca beans under ambient conditions. Development of carioca bean genotypes resistant to storage conditions is essential in reducing food losses during postharvest. 201...|$|E
40|$|Abstract—This paper {{presents}} a performance modeling and optimization analysis tool to predict and optimize {{the performance of}} sparse matrix-vector multiplication (SpMV) on GPUs. We make the following contributions: (1) We present an integrated analytical and profile-based performance modeling to accurately predict the kernel execution times of CSR, ELL, COO, and HYB SpMV kernels. Our proposed approach is general, and neither limited by GPU programming languages nor restricted to specific GPU architectures. In this paper, we use CUDA-based SpMV kernels and NVIDIA Tesla C 2050 for our performance modeling and experiments. According to our experiments, for 77 out of 82 test cases, the performance differences between the predicted and measured execution times are less than 9 %; for the rest 5 test cases, the differences are between 9 % and 10 %. For CSR, ELL, COO, and HYB SpMV CUDA kernels, the average differences are 6. 3 %, 4. 4 %, 2. 2 %, and 4. 7 %, respectively. (2) Based on the performance modeling, we design a dynamic-programming based SpMV optimal solution auto-selection algorithm to automatically report an optimal solution (i. e., optimal storage <b>strategy,</b> <b>storage</b> format(s), and execution time) for a target sparse matrix. In our experiments, the average performance improvements of the optimal solutions are 41. 1 %, 49. 8 %, and 37. 9 %, compared to NVIDIA’s CSR, COO, and HYB CUDA kernels, respectively...|$|E
40|$|Abstract Background Staphylococcus aureus is a {{commensal}} {{of human}} skin and nares. It {{is also one}} of the leading nosocomial pathogens in both developed and developing countries and is responsible for a wide range of life threatening infections, especially in patients who are immunocompromised, post-surgery, undergoing haemodialysis and those who are treated with catheters and ventilators. Over the past two decades, the incidence of nosocomial staphylococcal infections has increased dramatically. Currently there are at least seven vaccine and immunotherapy candidates against S. aureus in the developmental phase targeting both active and passive immunization. Methods We used a modified CHNRI methodology for setting priorities in health research investments. This was done in two stages. In Stage I, we systematically reviewed the literature related to emerging vaccines against Staphylococcus aureus relevant to several criteria of interest: answerability; cost of development, production and implementation; efficacy and effectiveness; deliverability, affordability and sustainability; maximum potential impact on disease burden reduction; acceptability to the end users and health workers; and effect on equity. In Stage II, we conducted an expert opinion exercise by inviting 20 experts (leading basic scientists, international public health researchers, international policy makers and representatives of pharmaceutical companies) to participate. The policy makers and industry representatives accepted our invitation on the condition of anonymity, due to sensitive nature of their involvement in such exercises. They answered questions from CHNRI framework and their “collective optimism” towards each criterion was documented on a scale from 0 to 100 %. Results The panel of experts expressed low levels of optimism (score around or below 50 %) on the criteria of answerability, efficacy, maximum disease burden reduction potential, low cost of production, low cost of implementation and affordability; moderate levels of optimism (scores around 60 to 80 %) that these vaccines could be developed at a low cost, and thus on the deliverability, sustainability and impact on equity; and high levels of optimism (scores above 80 %) regarding acceptable of such a product to both the end-users and health workers. While assessing the candidates for passive immunization against S. aureus, the experts were poorly optimistic regarding low production cost, low implementation cost, efficacy, deliverability, sustainability, affordability and equity; moderately optimistic regarding answerability and acceptability to health workers and end-users. They were of the opinion that these interventions would have only a modest impact (3 to 5 %) on the burden of childhood pneumonia.. Conclusion In order to provide an effective vaccine against S. aureus, a number of unresolved issues in vaccine development relating to optimal antigenic target identification, criteria for acceptable efficacy, identification of target population, commercial development limitations, optimal timing of immunization <b>strategy,</b> <b>storage,</b> cold chain requirements and cost need to be addressed properly. There is still a great deal unknown about the complex interaction between S. aureus and the human host. However, given the nature of S. aureus and the lessons learned from the recent failure of two emerging vaccines, it is clear that a multi-component vaccine is essential. Combating only one virulence factor is not sufficient in the human host but finding the right combination of factors will be very challenging. </p...|$|E
30|$|At this time, {{rationing}} {{seems to}} be the only way of managing water shortage in Tlemcen. The rationed households seem to have adapted and developed <b>strategies</b> of <b>storage,</b> which depend on the frequency of water supply.|$|R
40|$|AbstractEnsuring the {{availability}} of tools in the dynamic production environment requires the optimization of decisions regarding tool provision. In industrial reality, choosing the storage area and the tool state {{as part of an}} optimal tool <b>storage</b> <b>strategy</b> is rather based on experience than on a systematic approach. Thus, the first {{purpose of this paper is}} to identify relevant factors and their possible specifications influencing the choice for the tool <b>storage</b> <b>strategy.</b> This is followed by an analysis of the influence on the choice for the storage area. This analysis is the first step of the systematic approach for deriving an optimal tool <b>storage</b> <b>strategy...</b>|$|R
40|$|Dynamically typed {{language}} implementations often use more memory and execute slower than their statically typed cousins, {{in part because}} operations on collections of elements are unoptimised. This paper describes <b>storage</b> <b>strategies,</b> which dynamically optimise collections whose elements are instances of the same primitive type. We implement <b>storage</b> <b>strategies</b> in the PyPy virtual machine, giving a performance increase of 18 % on wide-ranging benchmarks of real Python programs. We show that <b>storage</b> <b>strategies</b> are simple to imple-ment, needing only 1500 LoC in PyPy, and have applicability {{to a wide range}} of virtual machines. Categories and Subject Descriptors D. 3. 4 [Programming Languages]: Processors—run-time environments, code gen-eration, incremental compilers, interpreter...|$|R
50|$|Information {{lifecycle}} management (ILM) {{refers to}} <b>strategies</b> for administering <b>storage</b> systems on computing devices.|$|R
5000|$|Personal Geodatabase - Esri's closed, {{integrated}} {{vector data}} <b>storage</b> <b>strategy</b> using Microsoft's Access MDB format.|$|R
5000|$|Personal Geodatabase - closed, {{integrated}} {{vector data}} <b>storage</b> <b>strategy</b> using Microsoft's Access MDB format (by ESRI) ...|$|R
40|$|Appropriate <b>storage</b> <b>strategies</b> were {{helpful in}} keeping the {{integrity}} and activity {{as well as in}} promoting the engineering application of granular sludge. The particle size distribution, composition of extracellular polymeric substances (EPSs), and activity of shortcut nitrification aerobic granular sludge (SNAGS) of the different <b>storage</b> <b>strategies</b> (15 °C with intermittent aeration, 4 °C, − 20 °C, and − 80 °C) were investigated in this study. The results showed that the <b>storage</b> <b>strategies</b> influenced particle size distribution, EPSs, protein (PN), polysaccharide (PS), PN/PS, and the oxidation ability of ammonium nitrogen of SNAGS. However, <b>storage</b> <b>strategies</b> had little effect on nitrite accumulation. The particle size, EPSs, PN, and PS of the SNAGS decreased after storage. The change of EPSs, PN, and PS of SNGS was smaller under the storage condition of − 20 °C. The ammonium nitrogen oxidation and denitrification abilities of SNAGS were highest under the storage condition of − 80 °C and − 20 °C. This work is licensed under a Creative Commons Attribution 4. 0 International License...|$|R
3000|$|This paper studies {{an optimal}} {{configuration}} <b>strategy</b> of energy <b>storage</b> in grid-connected microgrid and detail work is as follows: [...]...|$|R
40|$|International audienceWe {{consider}} a stochastic process Q(t) satisfying a linear differential equation, {{driven by a}} jump process which is modulated by a binary sequence. We prove multimodular properties related to the process Q(t) and apply those results to optimal <b>strategies</b> in <b>storage</b> systems models and in ruin problems...|$|R
5000|$|Coverage - Esri's closed, {{hybrid vector}} data <b>storage</b> <b>strategy.</b> Legacy ArcGIS Workstation / ArcInfo format with reduced support in ArcGIS Desktop lineup.|$|R
40|$|This paper {{considers}} replication <b>strategies</b> for <b>storage</b> {{systems that}} aggregate the disks of many nodes {{spread over the}} Internet. Maintaining replication in such systems can be prohibitively expensive, since every transient network or host failure could potentially lead to copying a server's worth of data over the Internet to maintain replication levels. Th...|$|R
40|$|In recent decades, {{around the}} world, {{a huge amount}} of daytime peak power has been shifted to the {{off-peak}} hours by using different types of thermal energy storage systems. However, the contribution of these systems in Malaysia is still minor in comparison with their potential. Therefore, the feasibility and potentiality of employing ice thermal storage (ITS) systems for office building cooling applications is studied to investigate their economical and environmental benefits. The air conditioning systems in Malaysia are considered as the major energy consumers in office buildings with around 57 share. The economical analysis of the cost benefits is carried out for a system including chiller and storage system. The installation costs are mainly dominated by the total system capacity; hence the study was conducted for a range of 100 - 2000 tons of refrigeration (TR) (352 - 7034 kW) for two <b>storage</b> <b>strategy</b> of full <b>storage</b> and load levelling <b>storage</b> <b>strategy.</b> The results indicate that considering the special off-peak tariff rate of 0. 06 /kWh for the total system capacities of 500 and 1500 TR (1758 kW and 5275 kW), the annual cost saving varies from 230, 000 to 700, 000 and from 65, 000 to 190, 000 for full storage and load levelling <b>storage</b> <b>strategy,</b> respectively. The overall results show that the full <b>storage</b> <b>strategy</b> can reduce the annual costs of the air conditioning system by up to 35 while this reduction is limited to around 8 for a load levelling strategy. The comparison study reveals that for the full <b>storage</b> <b>strategy</b> the payback period varies between 3 and 6 years while the payback period for the load levelling strategy varies between 1 and 3 years. It was concluded that the ITS system can {{play a vital role in}} consuming the natural resources in a more efficient, economical and environmentally benign way by changing the electricity consumption pattern to overcome the disparity between energy generation and energy demand...|$|R
40|$|Many {{scientific}} workflows are data intensive: {{large volumes}} of intermediate datasets are generated during their execution. Some valuable intermediate datasets need to be stored for sharing or reuse. Traditionally, they are selectively stored according to the system storage capacity, determined manually. As doing science on clouds has become popular nowadays, more intermediate datasets in scientific cloud workflows can be stored by different <b>storage</b> <b>strategies</b> based on a pay-as-you-go model. In this paper, we build an intermediate data dependency graph (IDG) from the data provenances in scientific workflows. With the IDG, deleted intermediate datasets can be regenerated, and as such we develop a novel algorithm that can find a minimum cost <b>storage</b> <b>strategy</b> for the intermediate datasets in scientific cloud workflow systems. The strategy achieves the best trade-off of computation cost and storage cost by automatically storing the most appropriate intermediate datasets in the cloud <b>storage.</b> This <b>strategy</b> can be utilised on demand as a minimum cost benchmark for all other intermediate dataset <b>storage</b> <b>strategies</b> in the cloud. We utilise Amazon clouds' cost model and apply the algorithm to general random as well as specific astrophysics pulsar searching scientific workflows for evaluation. The results show that benchmarking effectively demonstrates the cost effectiveness over other representative <b>storage</b> <b>strategies...</b>|$|R
25|$|Nintendo {{designed}} the 64DD as an enabling {{technology for the}} development of new genres of games, which was principally accomplished by its three main design features: its dual <b>storage</b> <b>strategy</b> of cartridges and disks; its new real-time clock (RTC); and its Internet connectivity. The dual <b>storage</b> <b>strategy</b> of the Nintendo 64 plus the 64DD combines the traditional high speed cartridges, which are low-capacity, non-writable, and expensive but very fast along with the introduction of proprietary mass storage disks, which are large-capacity, rewritable, and cheap but only moderately fast.|$|R
30|$|As {{has been}} highlighted, the {{challenge}} of achieving SLA-awareness in cloud environments is a multifaceted research issue {{with a number of}} dimensions stemming from it such as determining the levels of resource availability, service continuity rates and scalability trends of performance that will be able to satisfy QoS constraints. As a starting point towards addressing the vital issues pertinent to QoS maintenance in storage cloud infrastructures, this paper focuses on developing characterisations of cache performance trends, an aspect which we consider to have potential for serving as an important source of guidance for informed decisions on the provisioning of scalable data storage services in enterprise IT environments. Our approach, thus aims to support QoS readiness in resource allocation management <b>strategies</b> for <b>storage</b> clouds through accurate modelling of content availability levels at individual cache entities in the infrastructure, and the modelled scalability trends can serve as a feed into the management <b>strategies</b> for <b>storage</b> space provisioning.|$|R
40|$|Massive {{computation}} {{power and}} storage capacity of cloud computing systems enable users to either store large generated scientific datasets {{in the cloud}} or delete and then regenerate them whenever reused. Due to the pay-as-you-go model, the more datasets we store, the more storage cost we need to pay, alternatively, we can delete some generated datasets to save the storage cost but more computation cost is incurred for regeneration whenever the datasets are reused. Hence, there should exist a trade-off between computation and storage in the cloud, where different <b>storage</b> <b>strategies</b> lead to different total costs. The minimum cost, which reflects the best trade-off, is an important benchmark for evaluating the cost-effectiveness of different <b>storage</b> <b>strategies.</b> However, the current benchmarking approach is neither efficient nor practical to be applied on the fly at runtime. In this paper, we propose a novel Partitioned Solution Space based approach with efficient algorithms for dynamic yet practical on-the-fly minimum cost benchmarking of storing generated datasets in the cloud. In this approach, we pre-calculate all the possible minimum cost <b>storage</b> <b>strategies</b> and save them in different partitioned solution spaces. The minimum cost <b>storage</b> <b>strategy</b> represents the minimum cost benchmark, and whenever the datasets storage cost changes at runtime in the cloud (e. g. new datasets are generated and/or existing datasets 2 ̆ 7 usage frequencies are changed), our algorithms can efficiently retrieve the current minimum cost <b>storage</b> <b>strategy</b> from the partitioned solution space and update the benchmark. By dynamically keeping the benchmark updated, our approach can be practically utilised on the fly at runtime in the cloud, based on which the minimum cost benchmark can be either proactively reported or instantly responded upon request. Case studies and experimental results based on Amazon cloud show the efficiency, scalability and practicality of our approach...|$|R
40|$|To better {{understand}} co-product utilization, inclusion rates, pricing and <b>storage</b> <b>strategies,</b> Nebraska cattle producers were surveyed regarding their co-product feeding and pricing practices. Although nearly 91 % of cattle on feed in Nebraska were being fed ethanol co-products in 2007, {{many types of}} co-products were being utilized from both ethanol plants in Nebraska and surrounding states. As illustrated by the price data collected, especially those for wet distillers grains plus solubles, opportunities existed for pricing and <b>storage</b> <b>strategies,</b> although more price variation was present in the data collected from the survey {{as compared to the}} prices reported by the Agricultural Marketing Service...|$|R
5000|$|A {{descriptive}} schema-driven <b>storage</b> <b>strategy</b> {{is developed}} {{which consists of}} clustering nodes of an XML document according to their positions in the descriptive schema of the document. In contrast to a prescriptive schema that is known in advance and is usually specified in DTD or XML Schema, the descriptive schema is generated from data dynamically (and is maintained incrementally) and represents a concise and an accurate structure summary for data. Using the descriptive schema instead of the prescriptive one makes the <b>storage</b> <b>strategy</b> applicable to any XML document, even a one that comes with no prescriptive schema.|$|R
40|$|Information in a data {{warehouse}} {{is stored in}} materialized views, which must be kept consistent with respect to changes made in the sources. This problem has been extensively studied in the relational model. The process {{is referred to as}} view maintenance. XML is emerging as the de facto standard for data representation and data exchange of semistructured data. Most discussions involving XML assume the XML data is stored in plain text files. However, {{there are a number of}} different approaches for storing XML data, which can be categorized according to the underlying system used. Views and materialized views can also be specified in XML. This dissertation investigates how view maintenance in an XML context is influenced by the utilized approach for storage. We survey existing <b>storage</b> <b>strategies</b> using a relational database as the underlying system for <b>storage,</b> and <b>storage</b> <b>strategies</b> using plain text files. Further, we survey approaches for maintenance in the context of XML. We investigate three selected <b>storage</b> <b>strategies</b> in detail. We conclude with some insights gained during the investigation...|$|R
40|$|As {{radioactive}} spent {{nuclear fuel}} accumulates at nuclear power reactors around the country, it becomes increasingly important to develop a national interim <b>storage</b> <b>strategy</b> to manage spent fuel while permanent disposal facilities are being developed. This report examines {{the advantages and disadvantages}} of using centralized interim storage, and determines how centralized interim storage fits into a national interim <b>storage</b> <b>strategy.</b> This report also considers the federal government’s interest in centralized storage and its role in creating an optimized interim storage system for future nuclear reactors. It offers several recommendations concerning the implementation of an optimized interim storage system and outlines the major policy issues that need to be addressed. ...|$|R
25|$|As {{an example}} of {{variable}} <b>storage</b> <b>strategies,</b> Nintendo determined {{that the development of}} The Legend of Zelda: Ocarina of Time would be retargeted from 64DD disk format alone, to the much faster cartridge format, for performance reasons.|$|R
40|$|The Open Systems SnapVault (OSSV) {{client has}} {{extended}} {{the reach of}} Network Appliance™ SnapVault technology to the host. OSSV facilitates blocklevel incremental transfers from the open system platform directly to the secondary storage system. In Oracle environments, this approach can {{reduce the amount of}} data that must be transferred and perform dramatically more quickly than traditional file-based backup software. Network Appliance Inc. TECHNICAL REPORT data management <b>strategies.</b> advanced <b>storage</b> solutions and globa...|$|R
50|$|Sedna is an {{open source}} {{database}} management system that provides native storage for XML data.The distinctive design decisions employed in Sedna are (i) schema-based clustering <b>storage</b> <b>strategy</b> for XML data and (ii) memory management based on layered address space.|$|R
40|$|Novel <b>strategies</b> forinformation <b>storage</b> {{technology}} {{rely upon}} multistable systems {{that can be}} controllably switched between different configurations of comparable free energy. Multistability is present in many molecular and supramolecular systems {{through a variety of}} properties (conformations, redox and spin states, shape and dimensionality) that can be influenced by external stimuli. In the solid state, however, it is difficult to amplify the effects of molecular switching to higher length scales in order to relay the response to the outside world...|$|R
40|$|Resumen del trabajo presentado al LII Congreso Anual de la Sociedad Española de Cerámica y Vidrio celebrado en Burgos del 3 al 6 de octubre de 2012. Scientific {{analyses}} of historical glass samples (both archaeological glasses and glasses from stained glass windows) {{are crucial to}} complement historical knowledge, optimize restoration and conservation procedures, and improve preservation <b>strategies</b> (<b>storage</b> and exposure conditions). XAS techniques (X‐Ray absorption spectroscopy) have demonstrated to be useful in the structural characterization {{of different types of}} materials (coordination numbers, chemical nature of bondings, symmetries, etc), including historical ones, as metals and ceramics in a non‐invasive way. In the present study we have used synchrotron radiation to characterize the conservation state of some original historical glasses from different provenances and periods. Experiments carried out at the European Synchrotron Radiation Facility (BM 25 Branch A Beamline) enabled the identification of the oxidation state of some glass chromophores (Fe, Mn and Cu ions); the characterization of their near environment at atomic scale and, additionally, with some ions it was able to establish a relationship between the molecular environment of the glass chromophore and the superficial degradation state of the historical glass. Peer reviewe...|$|R
30|$|In this section, {{we present}} {{three sets of}} results for RBBD: the default HDFS data block placement, the {{comparison}} of the network transfer volume of Map execution under the default HDFS <b>storage</b> <b>strategy,</b> and the RBBD storage strategy; the overhead of performing RBBD.|$|R
50|$|First of {{all this}} step regards {{processes}} which take place on company level. These processes are mapped within the Enterprise-Resource-Planning-System (ERP). This mainly concerns demand and resource planning, batch size development, dispatching planning, replenishment <b>strategy</b> along <b>storage</b> levels within the push or pull approach, inventory planning and general software architecture. On this stage a quantification of all processes is also necessary to reach a most ideal solution. For process planning simulation usage is also an adequate tool to reach decisions.|$|R
