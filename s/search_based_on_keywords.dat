7|10000|Public
40|$|Summary: The {{variability}} in normalization and labeling of data in NCBI GEO and other publicly available databases is {{a barrier to}} large-scale data mining of microarray data. Because platform-wide summary tools are not available, researchers must download and curate data without {{an overview of the}} available samples. GPLBrowse is an interactive web tool for investigating the character-istics of samples and series for microarray platforms in NCBI GEO. The first release of GPLBrowse provides statistical overviews of all samples and series in the top 18 oligonucleotide microarray plat-forms of GEO. GPLBrowse allows users to select and save sam-ples, <b>search</b> <b>based</b> <b>on</b> <b>keywords,</b> and view additional metadata or link to GEO. GPLBrowse uses an AJAX-based split-client-server architecture to achieve highly responsive user interaction...|$|E
40|$|AbstractIntellectual Properties (IP's) are {{attracting}} progressively {{growing popularity}} for corporate houses and the academia {{in the current}} years. Patent system {{is one of them}} which generate high economical values of the IP rights. This in turn calls for the increased work responsibility of patent prior art search to generate effective patent search reports for the innovator (s). In the field of patent innovations, prior knowledge of innovative steps of the technologies developed so far must be known to innovator (s). In the present research work, technology/ patent <b>search</b> <b>based</b> <b>on</b> <b>keywords</b> has been investigated to arrive at the usefulness of the methodology particularly for the case of patent documents. The present paper helps to figure out the limitations and the scope of the methodology for patent prior art search based on extent of the keywords...|$|E
40|$|Abstract — Because of {{explosive}} growth of {{resources in the}} internet, the information retrieval technology has become particularly important. However the current retrieval methods are essentially based on the full text matching of keywords approach lacking of semantic information and can’t understand the user's query intent very well. These methods return {{a large number of}} irrelevant information, and are unable to meet the user's request. Systems have been established so far failed to overcome fully the limitations of <b>search</b> <b>based</b> <b>on</b> <b>keywords.</b> Such systems are built from variations of classic models that represent information by keywords. Using Semantic Web is a way to increase the precision of information retrieval systems. In this paper, we propose the semantic information retrieval approach to extract the information from the web documents in certain domain (jaundice diseases) by collecting the domain relevant documents using focused crawler based on domain ontology, and using similar semantic content that is matched with a given user’s query. Semantic retrieval approach aims to discover semantically similar terms in documents and query terms using WordNet...|$|E
3000|$|To {{implement}} in a geoportal a <b>search</b> experience <b>based</b> <b>on</b> <b>keywords,</b> {{as well as}} {{temporal and}} spatial faceting [...]...|$|R
30|$|Thirty {{publications}} {{were selected}} from the <b>searches</b> <b>based</b> <b>on</b> the <b>keyword</b> “mobile devices” and Twenty from the keyword “smart devices”.|$|R
5000|$|... {{a monthly}} {{electronic}} literature <b>search</b> <b>based</b> <b>on</b> all relevant <b>keywords</b> ...|$|R
40|$|Web {{services}} allow {{communication between}} heterogeneous systems in a distributed environment. Their enormous success and their increased use {{led to the}} fact that thousands of Web services are present on the Internet. This significant number of Web services which not cease to increase has led to problems of the difficulty in locating and classifying web services, these problems are encountered mainly during the operations of web services discovery and substitution. Traditional ways of <b>search</b> <b>based</b> <b>on</b> <b>keywords</b> are not successful in this context, their results do not support the structure of Web services and they consider in their search only the identifiers of the web service description language (WSDL) interface elements. The methods based on semantics (WSDLS, OWLS, SAWSDL [...] .) which increase the WSDL description of a Web service with a semantic description allow raising partially this problem, but their complexity and difficulty delays their adoption in real cases. Measuring the similarity between the web services interfaces is the most suitable solution for this kind of problems, it will classify available web services so as to know those that best match the searched profile and those that do not match. Thus, the main goal of this work is to study the degree of similarity between any two web services by offering a new method that is more effective than existing works. Comment: 7 pages, 4 figures, 8 tables, International Journal of Advanced Computer Science and Applications (IJACSA),Vol. 5, No. 8, 201...|$|E
40|$|Abstract—Web {{services}} allow {{communication between}} heterogeneous systems in a distributed environment. Their enormous success and their increased use {{led to the}} fact that thousands of Web services are present on the Internet. This significant number of Web services which not cease to increase has led to problems of the difficulty in locating and classifying web services, these problems are encountered mainly during the operations of web services discovery and substitution. Traditional ways of <b>search</b> <b>based</b> <b>on</b> <b>keywords</b> are not successful in this context, their results do not support the structure of Web services and they consider in their search only the identifiers of the web service description language (WSDL) interface elements. The methods based on semantics (WSDLS, OWLS, SAWSDL…) which increase the WSDL description of a Web service with a semantic description allow raising partially this problem, but their complexity and difficulty delays their adoption in real cases. Measuring the similarity between the web services interfaces is the most suitable solution for this kind of problems, it will classify available web services so as to know those that best match the searched profile and those that do not match. Thus, the main goal of this work is to study the degree of similarity between any two web services by offering a new method that is more effective than existing works. Keywords—web service; semantic similarity; syntactic similarity; WordNet; word sense disambiguation; Hausdorff distance I...|$|E
40|$|We {{assume that}} in order to {{properly}} capture opinion and sentiment expressed in a text or dialog any system needs a deep text processing approach. In particular, the idea that the task may be solved by the use of Information Retrieval tools like Bag of Words Approaches (BOWs) is totally flawed. BOWs approaches are sometimes also camouflaged by a keyword based Ontology matching and Concept search, based on such lexica as SentiWordNet, by simply stemming a text and using content words to match its entries and produce some result. Any <b>search</b> <b>based</b> <b>on</b> <b>keywords</b> and BOWs is fatally flawed by the impossibility to cope with such fundamental issues as the following ones: • presence of negation at different levels of syntactic constituency; • presence of lexicalized negation in the verb or in adverbs; • presence of conditional, counterfactual subordinators; • double negations with copulative verbs; • presence of modals and other modality operators. In order to cope with these linguistic elements we propose to build a Flat Logical Form (FLF) directly from a Dependency Structure representation augmented by indices and where anaphora resolution has operated pronoun-antecedent substitutions. We implemented these additions our the system called venses that we will show. The output of the system is an xml representation where each sentence of a text or dialog is a list of attribute-value pairs, like polarity, attitute and factuality. In order to produce this output, the system makes use of FLF and a vector of semantic attributes associated to the verb at propositional level and then memorized. Important notions required by the computation of opinion and sentiment are also the distinction of the semantic content of each proposition into two separate categories: • Objective vs Subjective This distinction is obtained by searching for factivity markers again at propositional level. In particular we take into account: • tense; • voice; • mood; • modality operators; • modifiers and attributes adjuncts at sentence level; • lexical type of the verb (in Levin’s classes and also using WordNet classification) ...|$|E
40|$|We {{consider}} {{the problem of}} using untrusted components to build correlation-resistant survivable storage systems that protect file replica locations, while allowing nodes to continuously re-distribute files throughout the network. The principal contribution is a chosen-ciphertext secure, searchable public key encryption scheme which allows for dynamic re-encryption of ciphertexts, and provides for node-targeted <b>searches</b> <b>based</b> <b>on</b> <b>keywords</b> or other identifiers. The scheme is provably secure under the SXDH assumption which holds in certain subgroups of elliptic curves, and a closely related assumption that we introduce...|$|R
40|$|Abstract: The goal of {{this paper}} is {{representing}} a suitable approach to content based document image retrieval. in proposed algorithm a feature vector is extracted with wavelet transform for sub-words. then <b>based</b> <b>on</b> this features, sub-words are clustered with support vector clustering (SVC) algorithm, then this approach is used for <b>searching</b> <b>based</b> <b>on</b> <b>keyword</b> in content <b>based</b> document retrieval problem. The retrieval algorithm achieved a precision of 81. 6 % at a recall rate of 74. 2 % on a database of 20 pages. Key word: Document image retrieval, Document feature vector, Keywords spotting, wavelet transform, support vector clusterin...|$|R
40|$|Firstly, {{this paper}} expounded that the {{traditional}} <b>searching</b> <b>based</b> <b>on</b> <b>keywords</b> has a severe deficiency on Recall and Precision because the computer can’t understand what information mean efficiently. In Succession, through extending from ordinary information to Geo-spatial information, on the support of geo-ontology theory and semantic web services technology, this paper discussed the principle, methods and application of ontology-driven geo-spatial information retrieving mechanism in semantic web. In other word, How to structure the framework for running this mechanism, how to build the ontology information models and how to establish the ontology-driven services models are discussed in detail in context. A design of systems for implementing the mechanism and experiment are given {{at the end of}} the text 1...|$|R
40|$|Publication <b>searching</b> <b>based</b> <b>on</b> <b>keywords</b> {{provided}} by users is traditional in digital libraries. While useful in many circumstances, {{the success of}} locating related publications via keyword-based searching paradigm is influenced by how users choose their keywords. Example-based searching, where user provides an example publication to locate similar publications, is also becoming commonplace in digital libraries. Existing publication similarity measures, needed for example-based searching, fall into two classes, namely, text-based similarity measures from Information Retrieval, and citation-based similarity measures <b>based</b> <b>on</b> bibliographic coupling and/or co-citation. In this paper, we list a number of publication similarity measures, and extend and evaluate {{them in terms of}} their accuracy, separability, and independence. For evaluation, we use the ACM SIGMOD Anthology, a digital library of about 15, 000 publications. ...|$|R
40|$|Web mining in <b>searching</b> <b>based</b> <b>on</b> <b>keywords</b> by {{automatic}} clustering is {{a document}} searching method by classifying documents <b>based</b> <b>on</b> its <b>keyword.</b> Following is the clustering by centroid linkage hierarchical method (CLHM) {{to the number}} of keywords from each document. In clustering, initialization is commonly required for the number of cluster to be formed first, however, in some clustering cases, the user cannot determine how many clusters can be built. Therefore, on this paper, the Valley tracing method is applied as a constraint which identifies variants movement from each cluster formation step and also analyzes its pattern to form automatic clustering. Document data used are from text mining process <b>on</b> documents. <b>Based</b> <b>on</b> 424 documents, this research shows that clustering method using CLHM algorithm can be generally used to classifying documents with exact number automatically...|$|R
40|$|Chinese Academy of Sciences; National Basic Research Program of China; IEEE Computer Society; Information Society Technologies; ICTThe wide {{adoption}} of web services raises the challenging problem of service discovery. UDDI has {{the limitation of}} lack of semantic inference support, and the <b>search</b> mechanism <b>based</b> <b>on</b> <b>keyword</b> and categorization information leads to a limited performance...|$|R
30|$|The {{annotation}} {{information of}} amino acid sequences from A. luchuensis genome (accession nos: BCWF 01000001 –BCWF 01000044, Yamada et al. 2016) was searched using BLASTP against the Swiss-Prot and TrEMBL protein databases from Uniprot (e-value cut-off 1 e- 5). Candidate genes related to vanillin production were <b>searched</b> <b>based</b> <b>on</b> the <b>keywords,</b> such as vanillin, enoyl (feruloyl), glucosidase and glucosyl transferase.|$|R
40|$|Social Media {{have emerged}} as an {{additional}} interaction channel for companies with their partners, employees and customers. However, the pace of interaction in this channel {{is as high as}} the data volume that may be relevant for businesses. Therefore, mechanisms other than manually monitoring and analyzing this data are needed. Basic approaches have emerged in the areas of Social Search and Social Media Monitoring, but <b>searches</b> <b>based</b> <b>on</b> <b>keywords</b> and simple grammar are limited in their results and deliver only first indications. The main challenge with more sophisticated approaches in the domain of ontology engineering is that they require considerable investments to establish domain-specific ontologies which has often prevented their use in many cases for Social Media Analysis. This research presents work in progress and suggests an approach which aims at increasing the efficiency of defining ontologies by automatically extracting knowledge from existing enterprise application systems. For thi...|$|R
40|$|With the {{availability}} of numerous curated databases, researchers {{are now able to}} efficiently use the multitude of biological data by integrating these resources via hyperlinks and cross-references. A large proportion of bioinformatics research tasks, however, may include labor-intensive tasks such as fetching, parsing, and merging datasets and functional annotations from distributed multi-domain databases. This data integration issue {{is one of the key}} challenges in bioinformatics. We aim to provide an identifier conversion and data aggregation system as a part of solution to solve this problem with a service named G-Links, 1) by gathering resource URI information from 130 databases and 30 web services in a gene-centric manner so that users can retrieve all available links about a given gene, 2) by providing RESTful API for easy retrieval of links including facet <b>searching</b> <b>based</b> <b>on</b> <b>keywords</b> and/or predicate types, and 3) by producing a variety of outputs as visual HTML page, tab-delimited text, and in Semantic Web formats such as Notation 3 and RDF. G-Links as well as other relevant documentation are available a...|$|R
40|$|Up to 15 years ago, bibliographic <b>searches</b> <b>based</b> <b>on</b> <b>keywords</b> such as {{photoreceptor}} degeneration, inner retina or photoreceptor degeneration, second order neurons returned only {{a handful}} of papers, as the field was dominated by the general assumption that retinal degeneration had direct effects on the sole populations of rods and cones. Since then, {{a number of studies have}} been dedicated to understanding the process of gradual morphological, molecular and functional changes arising among cells located in the inner retina (comprising neurons, glia and blood vessels), that is to say beyond photoreceptors. General aspects of this progression of biological rearrangements, now referred to as remodeling, were revealed and demonstrated to accompany consistently photoreceptor loss, independently from the underlying cause of degeneration. Recurrent features of remodeling are summarized here, to provide a general frame for to the various analytical descriptions and reviews provided by the articles in the issue (among others, see Stasheff; Goo et al., Puthussery et al.; Fernández-Sánchez et al.; Euler and Schubert; Jones et al.; this issue) ...|$|R
30|$|To make {{matching}} feasible, it {{is required}} {{to find out which}} images may see the same scene without doing full expensive matching. Often, expensive image matching is replaced by much more efficient <b>search</b> <b>based</b> <b>on</b> meta-information, e.g., <b>keywords</b> or GPS location, or by visual search [10].|$|R
50|$|An image {{search is}} {{a search engine}} that is {{designed}} to find an image. The <b>search</b> can be <b>based</b> <b>on</b> <b>keywords,</b> a picture, or a web link to a picture. The results depend on the search criterion, such as metadata, distribution of color, shape, etc., and the search technique which the browser uses.|$|R
5000|$|On June 16, 2008, {{the company}} laid off 68 of its {{approximately}} 200 employees; Executive Vice President of Corporate Development York Baur, company co-founder Chief Technology Officer Ken Smith, and company co-founder Chief Information Officer Doug Hanhart also left. Zango {{said it was}} narrowing its focus to concentrate on its new product Platrium, a [...] "casual gaming experience" [...] that showed targeted ads, shopping comparisons and <b>search</b> suggestions <b>based</b> <b>on</b> <b>keywords</b> from the user's Internet browsing. On December 15, 2008, Zango closed their Tel Aviv office, which had been the Hotbar headquarters before Hotbar and 180solutions merged, thereby laying off another 50 employees.|$|R
40|$|Within the {{emergent}} Semantic Web framework, {{the use of}} traditional web <b>search</b> engines <b>based</b> <b>on</b> <b>keywords</b> provided by the users is not adequate anymore. Instead, new methods <b>based</b> <b>on</b> the semantics of user keywords must be defined to search in the vast Web space without incurring in an undesirable loss of information. In this {{paper we propose a}} system that takes as input a list of plain keywords provided by the user and outputs equivalent semantic queries expressed in a knowledge representation language, {{that could be used to}} retrieve relevant data. For the translation task, specialized agents manage a third-party thesaurus and a pool of pre-existing ontologie...|$|R
50|$|The {{application}} supports {{drag and}} drop as well as cut and paste from the Nautilus (GNOME file manager) and also other applications. It can use files from a network when the protocol is handled via GVFS. It can search for files using Tracker, allowing a <b>search</b> that is <b>based</b> <b>on</b> <b>keywords</b> or <b>on</b> file type. Brasero can also display a playlist and its contents. Playlists are automatically searched using Tracker.|$|R
40|$|AbstractThis paper {{introduces}} a patent searching framework {{to assess the}} state of the art of a product or a technology and to support technology transfer activities. It combines several dimensions (IPC, Object & Behaviour/Function keywords) together with an integrated abstraction methodology <b>based</b> <b>on</b> WordNet/Multi-screen in order to systemize and facilitate FBOS (Function/Behaviour–Oriented Search). The core of the method is the abstraction of behaviour (<b>based</b> <b>on</b> a semantic approach) resulting in keywords at different abstraction levels, and a patent <b>search</b> <b>based</b> <b>on</b> these <b>keywords</b> and <b>on</b> a preassembled classification of Physical Effects. Key patents and space opportunities are mapped in a suitable graph, <b>based</b> <b>on</b> a revision of the classical Gero's FBS theory. An exemplary application in the lens sterilization domain shows the functioning of the patent-pending software...|$|R
40|$|As more {{information}} becomes {{available on the}} World Wide Web, {{it has become an}} acute problem to provide effective search tools for information access. Previous generations of search engines are mainly keyword-based and cannot satisfy many informational needs of their users. <b>Search</b> <b>based</b> <b>on</b> simple <b>keywords</b> returns many irrelevant documents that can easily swamp the user. In this paper, we describe the system architecture of a next-generation search engine that we have built with a goal to provide accurate search result on frequently asked concepts. Our key differentiating factors from other search engines are natural language user interface, accurate search results, and interactive user interface and multimedia content retrieval. We describe the architecture, design goals and experience in developing the search engin...|$|R
40|$|This {{resource}} {{consists of}} freely available images contributed by academics, researchers, Learned Societies, industry and individuals with rights cleared for educational purposes. Users {{are able to}} <b>search</b> for images <b>based</b> <b>on</b> <b>keywords</b> or browse within {{a wide range of}} bioscience subject areas. Images are then 'downloadable' along with informative descriptive text provided by the contributor. All images undergo a validation process by Centre for Bioscience staff with good subject knowledge. ImageBank also offers reviews of, and links to existing bioscience image databases. ...|$|R
40|$|Given {{a set of}} {{documents}} and an input query that is expressed in a natural language, the problem of document search is retrieving the most relevant documents. Unlike most existing systems that perform document <b>search</b> <b>based</b> <b>on</b> <b>keyword</b> matching, we propose a method that considers {{the meaning of the}} words in the queries and documents. As a result, our algorithm can return documents that have no words in common with the input query as long as the documents are relevant. For example, a document that contains the words 2 ̆ 2 Ford 2 ̆ 2, 2 ̆ 2 Chrysler 2 ̆ 2 and 2 ̆ 2 General Motors 2 ̆ 2 multiple times is surely relevant for the query 2 ̆ 2 car 2 ̆ 2 even if the word 2 ̆ 2 car 2 ̆ 2 never appears in the document. Our information retrieval algorithm is <b>based</b> <b>on</b> a similarity graph that contains the degree of semantic closeness between terms, where a term can be a word or a phrase. Since the algorithms that constructs the similarity graph takes as input a myriad of parameters, in this paper we fine-tune the part of the algorithm that constructs the Wikipedia part of the graph. Specifically, we experimentally fine-tune the algorithm on the Miller and Charles study benchmark that contains 30 pairs of terms and their similarity score as determined by human users. We then evaluate the performance of the fine-tuned algorithm on the Cranfield benchmark that contains 1400 documents and 225 natural language queries. The benchmark also contains the relevant documents for every query as determined by human judgment. The results show that the fine-tuned algorithm produces higher mean average precision (MAP) score than traditional keyword-based search algorithms because our algorithm considers not only the words and phrases in the query and documents, but also their meaning...|$|R
40|$|The {{tasks of}} solving {{question}} or clarifying doubts are determined primarily {{by a good}} analysis of the question {{in order to identify}} the subject target of the response. This article presents a question-answer system that uses ontologies and information retrieval techniques in the question analysis and thus improves the extraction of the response. Question-answer systems seek to provide the same facilities that occur in dialogues among people, where questions are answered promptly. They go beyond the more familiar <b>search</b> engines <b>based</b> <b>on</b> <b>keywords,</b> in an attempt to recognize what the question expresses and submit a correct answer. This feature makes them very favorable to learners. When embedded in collaborative learning environments it is possible to set or examine the content of a domain by means of questions and answers. A prototype was developed and tested and results are presented...|$|R
40|$|This paper {{describes}} {{the implementation of}} a semantic web search engine on conversation styled transcripts. Our choice of data is Hansard, a publicly available conversation style transcript of parliamentary debates. The current search engine implementation on Hansard is limited to running <b>search</b> queries <b>based</b> <b>on</b> <b>keywords</b> or phrases hence lacks the ability to make semantic inferences from user queries. By making use of knowledge such as the relationship between members of parliament, constituencies, terms of office, as well as topics of debates the search results can be improved in terms of both relevance and coverage. Our contribution is not algorithmic instead we describe how we exploit a collection of external data sources, ontologies, semantic web vocabularies and named entity extraction in the analysis of underlying semantics of user queries as well as the semantic enrichment of the search index thereby improving the quality of results...|$|R
40|$|In {{the fast}} moving world, {{the use of}} web is been {{increasing}} day by day so that the requirement of users relative to web search are also increasing. The content search over the web {{is one of the}} important research area comes under the web content mining. According to a traditional search engine, the <b>search</b> is <b>based</b> <b>on</b> the content <b>based</b> matching. But when some site is optimized under the SEO tools, such kind of search is not effective in all ways. The aim of this research is to design a user assisted, reliable, <b>search</b> <b>based</b> <b>on</b> the <b>keyword</b> <b>based</b> analysis,to provide the user assisted ranked results so that user can select the priority links,discard the spam links over the web and efficient search optimization model over the open web. The main objective of the work is to implement the work in user friendly environment and analysis of work under different parameters...|$|R
40|$|Physical {{therapy is}} a medical {{specialty}} where the professionals help restore movement and function when someone is affected by injury, illness or disability. This paper wishes to establish the connection between ethics, physiotherapy and bioengineering. The research method was achieved using academic database <b>searches</b> <b>based</b> <b>on</b> specific <b>keywords.</b> A SWOT analysis of the physiotherapy devices utilization and design was made, for extracting ethical considerations. The main results suggest that physiotherapy devices are able to generate ethical dilemmas, classified in 4 main items: (1) Bioengineering in physical therapy, ethical and clinical standards for manufacturers; (2) Social impact of physical therapy devices and ethical issues; (3) Inter-professional lack of communication and ethical concerns; (4) Bioengineering ethical research and education. As conclusions, for the physical therapy or electrotherapy research equipment development, a multidisciplinary team is needed. The equipment used in rehabilitation must fulfil specific technical and scientific requirements drafted by the professionals...|$|R
40|$|Design {{guidelines}} are {{an essential part}} of human computer interac- tion, for they provide an outlines of the requirements and needs of a user on how best to interact with technology devices. Our research project focuses on understanding how the elderly interact with cur- rent technologies (mobile devices and digital games) and how they can benefit from mobile digital games. The goal is to come up with interface design guidelines to be used by developers when designing mobile games for the elderly. The identification of the guidelines is carried out through a literature survey that includes a literature <b>search</b> <b>based</b> <b>on</b> outlined <b>keywords,</b> and a literature review of the selected research. The evaluation process of the guidelines involves user testing of a prototype <b>based</b> <b>on</b> some of the identified guidelines. Even with minor setbacks, the evaluation process indicates that the {{guidelines are}} beneficial for the development of mobile games for the elderly...|$|R
40|$|A bibliographic {{database}} will {{be constructed}} {{with the purpose}} to be a general tool for searching references for galaxy clusters. The structure of the database will be completely different from the available now databases as NED, SIMBAD, LEDA. <b>Search</b> <b>based</b> <b>on</b> hierarchical <b>keyword</b> system will be performed through web interfaces from numerous bibliographic sources [...] journal articles, preprints, unpublished results and papers, theses, scientific reports. Data {{from the very beginning}} of the extragalactic research will be included as well. References for galaxy clusters (RGC) is continuation of a previous project for collecting all published information for galaxy clusters. There are neither previous attempts nor projects for compiling such database for galaxy clusters and our effort will be to include into the database all of the available bibliography through our system of keywords until the end of 1999. Now over 3000 entries are included into the preliminary version of the database. Comment: Presented at ADASSVIII conference, 1 - 5 Nov 1998, Urbana-Champaign, I...|$|R
40|$|The {{traditional}} approach to video retrieval is to #rst annotate the video by textual information #titles and key words# {{and then the}} queries will be <b>searched</b> <b>based</b> <b>on</b> this <b>keyword</b> set. Since automatic annotation {{has not yet been}} available, this work needs great amount of labor and has been proved to be unrealistic in applications. Another approach, which seems to be at the other extreme, is to utilize the low-level video content, such as color, texture, shape, motion features and so on, in an attempt {{to get rid of the}} need of key words annotation. We hold the view in this paper that a user preferable query form should include both the keywords and video contents. In this paper, we will explore the semantic aspect <b>based</b> <b>on</b> video TOC structuring # 1 #. Closecaptioning is used to extract a basic keywords set. WordNet, an electronic lexical system, is used to provide semantic association. The approach has been applied in Web-MARS VIR and the running result has shown that the retrieval perf [...] ...|$|R
30|$|Currently, {{there are}} some {{practical}} works addressed {{on the construction of}} metadata layer on the whole Web and create the user interface to users for querying the Semantic Web by indexing all available schemas on Internet. For example, the Swoogle [12] is a search engine for semi-structured knowledge information. The knowledge can be expressed by either RDF or Web Ontology Language (OWL) [13]. The search engine periodically acquires the knowledge files available on the Web. Users can perform search operations to query the knowledge repository managed by Swoogle. On the other hand, Sindice [14] is another project for Semantic search on the Web. It maintains the index to Semantic Web pages available on the Web. Users can perform semantic <b>search</b> <b>based</b> <b>on</b> either <b>keywords</b> or SPARQL [15]. However, the main problem of evolution of current semantic search engine is the insufficiency of Semantic Web resources on the Web. Nowadays, only a few portions of Web resources are created or maintained following the Semantic Web standard. It is not easy for users to acquire enough results that can be utilized for problem solving purposes.|$|R
