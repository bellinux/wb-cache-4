4|39|Public
40|$|Software project <b>scope</b> <b>verification</b> {{is a very}} {{important}} process in project scope management {{and it needs to be}} performed properly and thoroughly so as to avoid project rework and scope creep. Moreover, software <b>scope</b> <b>verification</b> is crucial in the process of delivering exactly what the customer requested and minimizing project scope changes. Well defined software scope eases the process of <b>scope</b> <b>verification</b> and contributes to project success. Furthermore, a deliverable-oriented WBS provides a road map to a well defined software scope of work. It is on the basis of this that this paper extends the use of deliverable-oriented WBS to that of <b>scope</b> <b>verification</b> process. This paper argues that a deliverable-oriented WBS is a tool for software <b>scope</b> <b>verification...</b>|$|E
40|$|The paper {{deals with}} {{conscientious}} objection in health care, addressing {{the problems of}} <b>scope,</b> <b>verification</b> and limitation of such refusal, paying attention to ideological agendas hidden behind the right of conscience where the claimed refusal can cause harm or where such a claim {{is an attempt to}} impose certain moral values on society or an excuse for not providing health care. The nature of conscientious objection will be investigated and an ethical analysis of conscientious objection will be conducted. Finally some suggestions for health care policy will be proposed...|$|E
40|$|Research on {{the impact}} of social media in the {{workplace}} has focused on identifying ways in which the latest modes of shared communications has influenced productivity in project development, testing, and implementation. Traditionally, a written formal project management communication plan has been key in supporting the integration management that coordinates the project from beginning-to-end, to ensure a greater level of collaboration when managing project scope, schedule, budget and quality, and also to provide a greater sense of confidence in completing the project successfully. However, how beneficial are social media networks in improving the project team’s dynamics, which are important for successfully managing <b>scope</b> <b>verification,</b> budget adherence, and schedule compliance? How will these tools play a role in ensuring greater user involvement, senior management commitment, and user/system requirements? The purpose of this research is firstly to recognize what types of social media networks have been used and accepted as potential models for improving project management, and secondly; to promote a discussion to advance research paradigms for investigating the impact of implementing social media networks in project management. info:eu-repo/semantics/publishedVersio...|$|E
5000|$|Furthermore, {{implementation}} artefacts {{will likely}} be {{in the form of}} source files, links to which can be established in various ways at various <b>scopes.</b> <b>Verification</b> artefacts such as those generated by internal tests or formal verification tools (e.g., LDRA Testbed suite, Parasoft Concerto, and SCADE) ...|$|R
50|$|Thus, in {{the dispute}} over charismata, the {{questions}} regarding sola scriptura give rise to further {{issues such as the}} authority, <b>scope</b> and <b>verification</b> of modern prophecies, covered in the next sections.|$|R
40|$|The {{greenhouse}} {{theory of}} climate change has reached the crucial stage of verification. Surface warming as large as that predicted by models would be unprecedented during an interglacial period such as the present. The theory, its <b>scope</b> for <b>verification,</b> and the emerging complexities of the climate feedback mechanisms are discussed in this paper. The evidence for change is described and competing nonclimatic forcings are discussed...|$|R
40|$|A Comprehensive Test Ban Treaty (CTBT) {{had been}} on the arms control agenda since 1954, the subject of {{intermittent}} bilateral or trilateral talks that achieved only partial measures. The end of the Cold War provided renewed public pressure and political impetus for banning nuclear explosions. This thesis analyses the context and processes of the multilateral test ban negotiations that opened in the Conference on Disarmament in 1994. Combining participant-observation and contemporaneous notes with extensive use of documentary sources, unpublished materials and interviews, the study explores the dynamics of the CTBT negotiations in light both of regime theory and post cold war concepts of multilateralism, highlighting the role of civil society actors as well as states. Providing historical background and rich detail on the negotiating process from 1994 - 1996, the thesis examines the causal factors, strengths and weaknesses of the outcomes in four key areas: prenegotiations, <b>scope,</b> <b>verification</b> and entry into force. Focusing on the strategies and mechanisms by which actors with competing expectations and interests reached agreement, two types of convergence are explored: distributive, encompassing both imposed and managed divisions of gains and losses; and integrative, in which expectations of what would constitute an acceptable agreement are expanded or changed through cognitive strategies and the shaping of norms and interests. The thesis shows that whilst sharing a general objective of a CTBT, governments had significantly different views on what a test ban should encompass and accomplish, particularly with respect to broader concepts of nonproliferation and disarmament. While nuclear interests {{played a major role in}} determining a state's expectations and negotiating posture, other factors were important in reaching convergence. These included: knowledge and ideas; civil society engagement; norms and regime values; partnerships and alliances; internal policy cohesion or division; and the level of domestic and international political attention and support. By choosing to incorporate transnational civil society as a principal unit of analysis, along with states, the thesis contributes to a fuller understanding of how governments' calculations of what constitutes self-interest and security can be influenced and shaped, opening up alternative solutions for agreement than might have been initially envisaged. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|E
50|$|However, {{it is also}} {{possible}} to perform internal static tests to find out if it meets the requirements specification but that falls into the <b>scope</b> of static <b>verification</b> because the software is not running.|$|R
40|$|In {{the classic}} {{approach}} to logic model checking, software verification requires a manually constructed artifact (the model) {{to be written}} in the language that is accepted by the model checker. The construction of such a model typically requires good knowledge of both the application being verified and of {{the capabilities of the}} model checker that is used for the verification. Inadequate knowledge of the model checker can limit the <b>scope</b> of <b>verification</b> that can be performed; inadequate knowledge of the application can undermine the validity of the verification experiment itself...|$|R
40|$|We {{propose a}} tool-supported {{methodology}} for design-space exploration for embedded systems. It provides means to define high-level models of applications and multi-processor architectures {{and evaluate the}} performance of different deployment (mapping, scheduling) strategies while taking un-certainty into account. We argue that this extension of the <b>scope</b> of formal <b>verification</b> {{is important for the}} viability of the domain. ...|$|R
40|$|AbstractSymbolic Model Checking {{extends the}} <b>scope</b> of <b>verification</b> {{algorithms}} {{that can be}} handled automatically, by using symbolic representations rather than explicitly searching the entire state space of the model. However even the most sophisticated symbolic methods cannot be directly applied to many of today's large designs because of the state explosion problem. Approximate symbolic model checking {{is an attempt to}} trade off accuracy with the capacity to deal with bigger designs. This paper explores the idea of using overlapping projections as the underlying approximation scheme. The idea is evaluated by applying it to several modules from the I/O unit in the Stanford FLASH Multiprocessor, and some larger circuits in ISCAS 89 benchmark suite...|$|R
40|$|Through an experiment, t {{his study}} investigates {{the effects that}} {{verification}} has on honest traders. This paper demonstrates that by reducing the <b>scope</b> for trust <b>verification</b> can {{have a negative effect}} on the behaviour of honest individuals. Specifically, the analysis shows that trustworthy agents are willing to punish or seek compensation from those that deprive them of trust opportunities through the use of verification. Regulatory implications are discussed...|$|R
40|$|Abstract—For a speed-up of analog design cycles {{to keep up}} {{with the}} {{continuously}} decreasing time to market, iterative design refinement and redesigns are more than ever regarded as showstoppers. To deal with this issue, referred to as design and verification gap, the development of a continuous and consistent verification is mandatory. In digital design, formal verification methods are considered as a key technology for efficient design flows. However, industrial availability of formal methods for analog circuit verification is still negligible despite a growing need. In recent years, research institutions have made considerable advances in the area of formal verification of analog circuits. This paper presents a selection of four recent approaches in analog verification that cover a broad <b>scope</b> of <b>verification</b> philosophies. I...|$|R
30|$|The {{composite}} nodes, besides better structuring an application, {{define the}} perspective of the elements inside it. So, the links in a composition are only capable of referencing elements inside the same composition. The perspective notion can also be understood as a nested hierarchy where an element is found. Due to this notion, we say that NCL has a perspective-based <b>scope.</b> Furthermore, the <b>verification</b> of NCL code must be contextual, that is, it must take the contexts perspective into consideration.|$|R
40|$|We {{point out}} {{deficiencies}} of previous treatments of liveness. We define a new liveness condition in two forms: {{one based on}} finite trace theory, {{and the other on}} automata. We prove the equivalence of these two definitions. We also introduce a safety condition and provide modular and hierarchical verification theorems for both safety and liveness. Finally, we present a verification algorithm for liveness. Index terms: Concurrent systems, deadlock, fairness, finite automata, liveness, safety, trace structures, verification. 1 Introduction Motivation and <b>scope</b> Formal <b>verification,</b> especially if it can be automated, gains importance as designed systems become more and more complex. Formal verification is particularly important for concurrent systems because non-deterministic interleavings of events can generate considerable complexity. The subject of this paper is the definition, analysis, and automatic verification of a liveness condition for (possibly asynchronous) digital circuits [...] ...|$|R
40|$|In {{the problem}} area of {{evaluating}} complex software systems, {{there are two}} distinguished areas of research, development, and application identified by the two buzzwords validation and verification, respectively. From the perspective adopted by the authors (cf. (O'Keefe & O'Leary 1993), e. g.), verification is usually more formally based and, thus, can be supported by formal reasoning tools like theorem provers, for instance. The <b>scope</b> of <b>verification</b> approaches {{is limited by the}} difficulty of finding a sufficiently complete formalization to built upon. In paramount realistic problem domains, validation seems to be more appropriate, although it is less stringent in character and, therefore, validation results are often less definite. The aim {{of this paper is to}} exemplify a validation approach based on a clear and thoroughly formal theory. In this way, validation and verification should be brought closer to each other, for the benefit of a concerted action towards depend [...] ...|$|R
40|$|Symbolic Model Checking {{extends the}} <b>scope</b> of <b>verification</b> {{algorithms}} {{that can be}} handled automatically, by using symbolic representations rather than explicitly searching the entire state space of the model. However even the most sophisticated symbolic methods cannot be directly applied to many of today's large designs because of the state explosion problem. Approximate symbolic model checking {{is an attempt to}} trade off accuracy with the capacity to deal with bigger designs. This paper explores the idea of using overlapping projections as the underlying approximation scheme. The idea is evaluated by applying it to several modules from the I/O unit in the Stanford FLASH Multiprocessor, and some larger circuits in ISCAS 89 benchmark suite. 1 Introduction The ability to enumerate the set of states reachable from a certain state, and the ability to enumerate the set of states that can reach a certain state are essential to many model checking algorithms. Binary Decision Diagrams (BDDs) [2 [...] ...|$|R
30|$|Most of {{existing}} approaches do not select landmarks except when building the initial landmark set, {{which is often}} considered out of the <b>scope</b> of location <b>verification</b> approaches. However, initial landmark set selection is of prime importance, as the quality of results directly depends on measurements provided by landmarks. One should not choose landmarks with too much randomness {{in the quality of}} network connection, otherwise measurements would experience high variance, resulting in a wider location zone. An exciting challenge for next generation location verification solutions would be the proposal of machine learning based approaches, which dynamically adapt landmark set according to the required granularity of data location and to observed network traffic conditions.|$|R
40|$|Verification {{techniques}} {{that rely on}} state enumeration (such as model checking) face two important challenges: 1) State-space explosion: exponential increase in the state space with {{the increasing number of}} components. 2) Environment generation: modeling components that are either not available for analysis, or that are outside the <b>scope</b> of the <b>verification</b> tool at hand. We propose a semi-automated approach for attacking these two problems. In our approach, interfaces for the components that are outside the scope of the current verification effort are specified using an interface specification language based on grammars. Specifically, an interface grammar for a component specifies the sequences of method invocations that are allowed by that component. Using interface grammars, one can specify nested call sequences that cannot be specified using interface specification formalisms that rely on finite state machines. Moreover, ou...|$|R
40|$|In {{the problem}} area of {{evaluating}} complex software systems, {{there are two}} distinguished areas of research, development, and application identified by the two busswords valida*ion and verification, respectively. From the perspective adopted by the authors (cf. (O’Keefe & O’Leary 1993), e. g.), verification usually more formally based and, thus, can be sup-ported by formal reasoning tools like theorem provers, for instance. The <b>scope</b> of <b>verification</b> approaches {{is limited by the}} difllculty of finding a sufficiently complete formalisa-tion to built upon. In p~amount realistic problem domains, validation seems to be more appropriate, al-though it is less stringent in character and, therefore, validation results are often less definite. The aim {{of this paper is to}} exemplify a validation ap-proach based on a clear and thoroughly formal the-ory. In this way, validation and verification should be brought closer to each other, for the benefit of a con-cexted action towards dependable software systems. To allow for precise and sufficiently ciear results, the authors have selected the application domain of al-goritluns and systems for learning formal languages. By means of the validation toolkit TIC, some series of validation experiments have been performed. The results are presented for the sake of illustrating the underlying formal concepts in use. Comparing the validity of one learning approach to the invalidity of another one can be seen as an interesting result in its own right...|$|R
30|$|Pre-training {{landmark}} selection: first, {{a set of}} nodes {{that may}} be used as active or passive landmarks is considered. This set may be static or provided by specific function (e.g., all academic websites in a country). It is worth noticing that authors focusing on location verification process do not take care of this set arguing it is out of the <b>scope</b> of location <b>verification.</b> However, a good choice of the initial set is a key for success and accuracy of the verification process. Then either all nodes in the initial set participate in the training step or only a subset is selected. Indeed, in order to minimize communication cost of training and optimize accuracy of zone estimate, a selection of the closest nodes is performed and only those nodes participate in training, as proposed in [23].|$|R
40|$|In {{the recent}} trend of network and technology, “Cryptography” and “Steganography” have emerged {{out as the}} {{essential}} elements of providing network security. Although Cryptography {{plays a major role}} in the fabrication and modification of the secret message into an encrypted version yet it has certain drawbacks. Steganography is the art that meets one of the basic limitations of Cryptography. In this paper, a new algorithm has been proposed based on both Symmetric Key Cryptography and Audio Steganography. The combination of a randomly generated Symmetric Key along with LSB technique of Audio Steganography sends a secret message unrecognizable through an insecure medium. The Stego File generated is almost lossless giving a 100 percent recovery of the original message. This paper also presents a detailed experimental analysis of the algorithm with a brief comparison with other existing algorithms and a future <b>scope.</b> The experimental <b>verification</b> and security issues are promising...|$|R
40|$|Functional {{verification}} consumes {{more than}} 70 % {{of the labor}} invested in today’s SoC designs. Yet, even with such a large investment in verification, there’s more risk of functional failure at tapeout than ever before. The primary {{reason is that the}} design team does not know where they are, in terms of functional correctness, relative to the tapeout goal. They lack a functional verification map for reference that employs coverage as its primary element. Coverage, in the broadest sense, is responsible for measuring verification progress across a plethora of metrics and for helping engineers assess their location relative to design completion. [1] The map to be referenced must be created by the design team upfront, so they know not only where they are starting from-specification but no implementationbut also where they are going: fully functional first silicon. The metrics of the map must be chosen for their utility: RTL written, software written, features, properties, assertion count, simulation count, failure rate, and coverage closure rate. The map is the verification plan, an executable natural language document [2],[3] that defines the <b>scope</b> of the <b>verification</b> problem and its solution. The scope of the problem is defined by implicit and explicit coverage models. [1] The solution to the verification problem is described by the methodology employed to achieve full coverage: dynamic and static verification. Simulation (dynamic) contributes to coverage closure through RTL execution. Formal analysis (static) contributes to coverage closure through proven properties. By annotating the verification plan with these (and other) progress metrics, it becomes a live, executable document that directs the design team to their goal. Most verification planning today lacks the rigor required to recognize the full <b>scope</b> of the <b>verification</b> problem faced by the design team. The {{reason for this is that}} substantial effort is required to write a thorough verification plan. If that plan is obsolete as soon as it is written, the effort is not justified. However, by transforming the verification plan into an active specification that controls the verification process, the planning effort is more than justified. This article illustrates the application of an executable verification plan to a processor-based SoC...|$|R
40|$|We {{describe}} the formal specification and verification of the VGI parallel DSP chip [1], which contains 64 compute processors with ∼ 30 K gates in each processor. Our effort coincided {{in time with}} the “informal ” verification stage of the chip. By interacting with the designers, we produced an abstract but executable spec-ification of the design which embodies the programmer’s view of the system. Given the size of the design, an automatic check that even one of the 64 processors satisfies its specification is well beyond the <b>scope</b> of current <b>verification</b> tools. However, the check can be decomposed using assume-guarantee reason-ing. For VGI, the implementation and specification operate at different time scales: several steps of the implementation corre-spond to a single step in the specification. We generalized both the assume-guarantee method and our model checker MOCHA to allow compositional verification for such applications. We used our proof rule to decompose the verification problem of the VGI chip into smaller proof obligations that were discharged auto-matically by MOCHA. Using our formal approach, we uncovered and fixed subtle bugs that were unknown to the designers. ...|$|R
40|$|Users {{increasingly}} rely on mobile {{applications for}} computational needs. Google Android {{is a popular}} mobile platform, hence the reliability of Android applications is becoming increasingly important. Many Android correctness issues, however, fall outside the <b>scope</b> of traditional <b>verification</b> techniques, as they are due to the novelty of the platform and its GUI-oriented application construction paradigm. In this paper we present an approach for automating the testing process for Android applications, {{with a focus on}} GUI bugs. We first conduct a bug mining study to understand the nature and frequency of bugs affecting Android applications; our study finds that GUI bugs are quite numerous. Next, we present techniques for detecting GUI bugs by automatic generation of test cases, feeding the application random events, instrumenting the VM, producing log/trace files and analyzing them post-run. We show how these techniques helped to re-discover existing bugs and find new bugs, and how they could be used to prevent certain bug categories. We believe our study and techniques have the potential to help developers increase the quality of Android applications...|$|R
40|$|Relaxed {{notions of}} decidability widen the <b>scope</b> of {{automatic}} <b>verification</b> of hybrid systems. In quasi-decidability and δ-decidability, the fundamental compromise {{is that if}} we are willing to accept a slight error in the algorithm 2 ̆ 7 s answer, or a slight restriction on the class of problems we verify, then it is possible to obtain practically useful answers. This paper explores the connections between relaxed decidability and the robust semantics of Metric Temporal Logic formulas. It establishes a formal equivalence between the robustness degree of MTL specifications, and the imprecision parameter δ used in δ-decidability when it is used to verify MTL properties. We present an application of this result {{in the form of an}} algorithm that generates new constraints to the δ-decision procedure from falsification runs, which speeds up the verification run. We then establish new conditions under which robust testing, based on the robust semantics of MTL, is in fact a quasi-semidecision procedure. These results allow us to delimit what is possible with fast, robustness-based methods, accelerate (near-) exhaustive verification, and further bridge the gap between verification and simulation...|$|R
40|$|Abstract—Differential {{privacy is}} a rigorous, {{worst-case}} notion of privacy-preserving computation. Informally, a probabilistic program is differentially private if {{the participation of}} a single individual in the input database has a limited effect on the program’s distribution on outputs. More technically, differential privacy is a quantitative 2 -safety property that bounds {{the distance between the}} output distributions of a probabilistic program on adjacent inputs. Like many 2 -safety properties, dif-ferential privacy lies outside the <b>scope</b> of traditional <b>verification</b> techniques. Existing approaches to enforce privacy are based on intricate, non-conventional type systems, or customized relational logics. These approaches are difficult to implement and often cumbersome to use. We present an alternative approach that verifies differen-tial privacy by standard, non-relational reasoning on non-probabilistic programs. Our approach transforms a probabilistic program into a non-probabilistic program which simulates two executions of the original program. We prove that if the target program is correct with respect to a Hoare specification, then the original probabilistic program is differentially private. We provide a variety of examples from the differential privacy literature to demonstrate the utility of our approach. Finally, we compare our approach with existing verification techniques for privacy. I...|$|R
40|$|Formal {{verification}} {{techniques have}} been playing {{an important role in}} pre-silicon validation processes. One of the most important points considered in performing formal verification is to define good verification scopes; we should define clearly what to be verified formally upon designs under tests. We considered the following three practical requirements when we defined the <b>scope</b> of formal <b>verification.</b> They are (a) hard to verify (b) small to handle, and (c) easy to understand. Our novel approach is to break down generic properties for system into stereotype properties in block level and to define requirements for Verifiable RTL. Consequently, each designer instead of verification experts can describe properties of the design easily, and formal model checking can be applied systematically and thoroughly to all the leaf modules. During the development of a component chip for server platforms, we focused on RAS (Reliability, Availability, and Serviceability) features and described more than 2000 properties in PSL. As a result of the formal verification, we found several critical logic bugs in a short time with limited resources, and successfully verified all of them. This paper presents a study of the functional verification methodology. 1...|$|R
40|$|Submitted {{on behalf}} of EDAA ([URL] audienceFormal {{verification}} techniques have been playing {{an important role in}} pre-silicon validation processes. One of the most important points considered in performing formal verification is to define good verification scopes; we should define clearly what to be verified formally upon designs under tests. We considered the following three practical requirements when we defined the <b>scope</b> of formal <b>verification.</b> They are (a) hard to verify (b) small to handle, and (c) easy to understand. Our novel approach is to break down generic properties for system into stereotype properties in block level and to define requirements for Verifiable RTL. Consequently, each designer instead of verification experts can describe properties of the design easily, and formal model checking can be applied systematically and thoroughly to all the leaf modules. During the development of a component chip for server platforms, we focused on RAS (Reliability, Availability, and Serviceability) features and described more than 2000 properties in PSL. As a result of the formal verification, we found several critical logic bugs in a short time with limited resources, and successfully verified all of them. This paper presents a study of the functional verification methodology...|$|R
40|$|OBJECTIVE—Necropsy {{confirmed}} clinical diagnostic accuracy for Alzheimer's {{disease is}} claimed to exceed 90 %. This figure contains two fallacies; it includes {{cases in which}} Alzheimer's disease exists with other diseases affecting cognition and the studies that report these figures excluded cases without necropsy (verification bias). The effect of these errors is estimated.  METHODS—Data {{were taken from the}} University of Western Ontario Dementia Study, a registry of dementia cases with clinical and psychometric follow up to necropsy based in a university memory disorders clinic with secondary and tertiary referrals. Data were available on 307 patients; 200 (65 %) had clinically diagnosed Alzheimer's disease, 12 (4 %) vascular dementia, 47 (15 %) mixed dementia, and 48 (16 %) had other diagnoses. One hundred and ninety two of 307 cases (63 %) died and 122 of 192 fatalities (64 %) had necropsies. The pathological material was interpreted in two ways, allowing and disallowing coexistent disease in making a diagnosis of Alzheimer's disease. In cases without necropsy, progressive cognitive loss was used as a marker for degenerative dementia. The outcome measures of interest were the positive predictive value of a clinical diagnosis of Alzheimer's disease allowing and disallowing coexistent diseases and with and without correction for cases that were not necropsied.  RESULTS—The clinical diagnoses differed significantly between the population who died and those who did not. In cases without necropsy, 22 % had no dementia on follow up, concentrated in early cases and men, showing considerable <b>scope</b> for <b>verification</b> bias. The positive predictive value of a diagnosis of Alzheimer's disease was 81 % including coexistent diseases, falling to 44 % when limited to pure cases. Combined, these factors reduce the positive predictive value to 38 % for pure Alzheimer's disease.  CONCLUSIONS—Correction for dual pathology and verification bias halves the positive predictive value of the clinical diagnosis of Alzheimer's disease. Data derived from necropsy studies cannot be extrapolated to the whole population. This has important implications including uncertainty about diagnosis and prognosis and a dilution effect in therapeutic trials in Alzheimer's disease. ...|$|R
40|$|Differential {{privacy is}} a rigorous, {{worst-case}} notion of privacy-preserving computation. Informally, a probabilistic program is differentially private if {{the participation of}} a single individual in the input database has a limited effect on the program's distribution on outputs. More technically, differential privacy is a quantitative 2 -safety property that bounds {{the distance between the}} output distributions of a probabilistic program on adjacent inputs. Like many 2 -safety properties, differential privacy lies outside the <b>scope</b> of traditional <b>verification</b> techniques. Existing approaches to enforce privacy are based on intricate, non-conventional type systems, or customized relational logics. These approaches are difficult to implement and often cumbersome to use. We present an alternative approach that verifies differential privacy by standard, non-relational reasoning on non-probabilistic programs. Our approach transforms a probabilistic program into a non-probabilistic program which simulates two executions of the original program. We prove that if the target program is correct with respect to a Hoare specification, then the original probabilistic program is differentially private. We provide a variety of examples from the differential privacy literature to demonstrate the utility of our approach. Finally, we compare our approach with existing verification techniques for privacy. Comment: Published at the Computer Security Foundations Symposium (CSF), 201...|$|R
40|$|Due {{to their}} safety-critical nature, cyber-physical systems (CPS) demand the most {{rigorous}} verification techniques. However, {{the complexity of}} the domain puts many cyber-physical systems outside the <b>scope</b> of automated <b>verification</b> tools. Formal deductive proofs hold the potential to verify virtually any property of any system, but proofs for practical cyber-physical systems often require an impractical amount of manual effort. This proof burden can be mitigated by capturing common reasoning patterns in powerful higher-order proof rules. Existing work has focused on proof rules applicable to arbitrary hybrid systems (a formal model for CPS), but many systems actually fall into more constrained classes. One such class of systems are called sampled-data systems, in which a discrete controller runs periodically. In this dissertation, we complement general hybrid system proof rules with a series of rules that leverage the particular structure of sampled-data systems. We demonstrate the applicability of these rules on the double integrator, an important model in robotic and vehicle systems. All work is formalized in the Coq proof assistant, whose expressive logic is crucial to maintaining soundness while applying domain-specific proof rules for sampled-data systems. Finally, we experimentally evaluate our results by implementing verified controllers on a quadcopter and conducting flight tests...|$|R
40|$|Reviewing the {{institutional}} processes {{and problems of}} a Korea-Japan nuclear weapon free zone, this paper considers its feasibility, appropriate legal forms, compliance mechanisms, UN role and portential benefits. Michael Hamel-Green of Victoria University examines "the feasibility of the proposal {{in the light of}} precedents from previous Nuclear Weapon Free Zone (NWFZ) establishment; appropriate legal forms; negotiation forums and phases; governance; <b>scope</b> and domain; <b>verification</b> and compliance mechanisms and arrangements; and a UN role in negotiations and implementation". Hamel-Green argues that such a NWFZ could "(a) build on the experience of previous NWFZs in other regions in achieving longer term denuclearization outcomes; (b) offer immediate confidence-building benefits in achieving ways through the present impasse with North Korea; and (c) provide longer term security benefits in reducing or even preventing potential nuclear rivalry between Japan and the two Koreas. " This would play, he concludes, "a very significant regional role in acting as a circuit-breaker in the current downward spiral of mistrust. It would serve to confirm and guarantee in a rigorously verified and transparent way the current non-nuclear-weapon status of Japan and South Korea, while acting as an important confidence-building step that would enable North Korea to join such a zone at a later date", where "the negative security guarantee offered under such a zone would be a powerful inducement for North Korea to join. ...|$|R
40|$|This article asserts {{the thesis}} that {{customary}} international law (CIL), {{even in the absence}} of any new treaty, already provides a legal regime constraining the testing and use in combat of anti-satellite (ASAT) weapons. This argument, if validated, is important for both legal and public policy considerations: the world (especially, but not only, the United States) has grown increasingly dependent upon satellites for the performance of a wide array of commercial and military functions. At the same time, because of this growing reliance (and hence vulnerability), interest has surged in developing novel systems for attacking a potential enemy’s satellites – ASAT technology has been tested by the United States, Russia, and China, and other countries may soon shoot up that same dangerous trajectory. Oddly, the United States has consistently opposed international efforts to negotiate an arms control solution to this problem. Any comprehensive treaty would certainly be difficult to reach (there are numerous complicated issues of definitions, <b>scope,</b> and <b>verification</b> to surmount) but the American stance (not only during the Bush Administration) has been that we should not even try, because 2 ̆ 2 there is currently no arms race occurring in outer space. 2 ̆ 2 This article turns that resistance on its head, by asking whether customary international law, even without any formal treaty on point, already imposes meaningful constraints upon ASAT activities. To develop the argument, it analyzes three strands of CIL: first, “general” customary international law, which has long been recognized as an authoritative, albeit complex, source of binding rules; second, the specialized legal regime incorporated into the law of armed conflict, which imposes its own strictures, fully applicable to conventional warfare, but not yet applied extra-terrestrially; and finally, another realm of specialized CIL, the emerging jurisprudence governing international environmental law. My conclusion is that there is already a meaningful ASAT-control regime, created by CIL even without codification in a new treaty. This is not, to be sure, a fully comprehensive legal web, and there would still be plenty of additional scope for a new treaty to enhance and extend the legal regime. But the conclusion is that treaty negotiators would not be drafting on a complete tabula rasa; an incipient legal order is already in place...|$|R
40|$|Formal {{verification}} {{techniques are}} used routinely in finite-state digital circuits. Theorem proving {{is also used}} successfully for infinite-state discrete systems. But many safetycritical computers are actually embedded in physical systems. Hybrid systems [1] model complex physical systems as dynamical systems with interacting discrete transitions and continuous evolutions along differential equations. They arise frequently in many application domains, including aviation, automotive, railway, and robotics. There is a wellunderstood theory for proving programs. But what about complex physical systems? How can we prove that a hybrid system works as expected, e. g., an aircraft does not crash into another one? This talk illustrates the complexities and pitfalls of hybrid systems verification. It describes a theoretical and practical foundation for deductive verification of hybrid systems called differential dynamic logic (dL). The proof calculus for this logic is interesting from a theoretical perspective, {{because it is a}} complete axiomatization of hybrid systems relative to differential equations. The approach is of considerable practical interest too. Its implementation in the theorem prover KeYmaera 1 [7] has been used successfully to verify collision avoidance properties in the European Train Control System [8] and air traffic control systems [6]. The number of dimensions and nonlinearities in they hybrid dynamics of these systems is surprisingly tricky such that they are still out of <b>scope</b> for other <b>verification</b> tools. This talk is based on recent work [2, 3, 4]. More comprehensive details {{can be found in a}} corresponding book [5]...|$|R
40|$|In this thesis, I aim {{to build}} an {{accurate}} fine-grained retail product recognition system for improving customer in-store shopping experience. To achieve high accuracy, I developed a two-phase visual recognition scheme to identify the viewed retail product by verifying different types of visual features. The proposed scheme is robust enough to distinguish visually similar products in the tests. However, the computation cost of this scheme increases as the database scale becomes larger since it needs to verify all the products in the database. To improve the computation efficiency, my system integrates RFID as a second data source. By attaching an RFID tag to each product, the RFID reader is able to capture the identity information of surrounding products. The detection results can help reduce the <b>verification</b> <b>scope</b> from the whole database to the detected products only. Hence computation cost is saved. In the experiments, I first tested the recognition accuracy of my visual recognition scheme on a database containing visually similar products for different viewing angles, and my scheme achieved over 97. 92 % recognition accuracy for horizontal viewpoint variations of less than 30 degree. I then experimentally measured the computation cost of both the original system and the RFID-enhanced system. The computation cost is the processing time to recognize a target product. The RFID-enhanced system speeds up system performance dramatically when the scale of detected surrounding products is small. by Yongbin Sun. Thesis: S. M., Massachusetts Institute of Technology, Department of Mechanical Engineering, 2016. Cataloged from PDF version of thesis. Includes bibliographical references (pages 63 - 67) ...|$|R
