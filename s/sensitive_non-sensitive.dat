2|69|Public
30|$|QoS: {{refers to}} the ability to manage the network traffic in order to satisfy the user requirement. In other words, it is the ability to {{introduce}} a different QoS for a various services (<b>sensitive,</b> <b>non-sensitive</b> application) in the mobility protocols.|$|E
40|$|Wireless Sensor Networks (WSNs) {{consist of}} {{numerous}} sensor nodes {{which can be}} used in many new emerging appli cations like healthcare. One of the major challenges in healthcare environments is to manage congestion, because in applications, such as medical emergencies or patients remote monitoring, transmitted data is important and critical. So it is essential in the first place to avoid congestion as much as possible and in cases when congestion avoidance is not possible, to control the congestion. In this paper, a class based congestion management protocol has been proposed for healthcare applications. We distinguish between <b>sensitive,</b> <b>non-sensitive</b> and control traffics, and service the input traf fics based on their priority and quality of service requirements (QoS). The proposed protocol which is called COCM avoids congestion in the first step using multipath routing. The proposed AQM algorithm uses separate virtual queue's condition on a single physical queue to accept or drop the incoming packets. In cases where input traffic rate increases and congestion cannot be avoided, it mitigates congestion by using an optimized congestion control algorithm. This paper deals with parameters like end to end delay, packet loss, energy consumption, lifetime and fairness which are important in healthcare applications. The performance of COCM was evaluated using the OPNET simulator. Simulation results indicated that COCM achieves its goals. </p...|$|E
30|$|Towards the future, the {{employed}} confidentiality {{model can}} be refined. The algorithm now assumes that an attribute or policy is labeled <b>sensitive</b> or <b>non-sensitive.</b> In a more extensive case, a sensitivity policy could express more complex rules, for example, limiting attribute release to some parties based on their identity or defining a certain combination of multiple attributes as confidential.|$|R
40|$|This study aims {{to obtain}} the extent of {{disclosure}} of companies’ CSR activity based on type of disclosure (monetary, quantitative, and narrative) and {{whether there is a}} difference by such types of disclosure across industries (<b>sensitive</b> and <b>non-sensitive</b> industry) or not. To measure the extent of CSR disclosure, this study used GRI Index 3. 1. The population of this study was all companies listed in Indonesia Stock Exchange (IDX) in 2012. Sample consists of companies which included in top 100 companies based on market capitalization and disclose annual report and or sustainability report in 2012. Data analysis was performed with the statistics descriptive and independent sample t-test. Statistic program in this study used SPSS 16. The results {{of this study indicate that}} the extent of narrative type is high and it becomes the most used type among Indonesia companies in 2012 to disclose their CSR activity and there is a difference of disclosure in forms between type of industry (<b>sensitive</b> and <b>non-sensitive</b> industry...|$|R
40|$|This paper aims to {{investigate}} the structure and role of mathematics and its objects in Aristotle’s philosophy, by analyzing his writings about mathematics – more specifically the books M and N of Metaphysics – {{as well as the}} philosophical discussions with his teacher, Plato. An important and preliminary step was to analyze the essential concepts of mathematics, such as the notions of substance (οὐσία), <b>sensitive</b> and <b>non-sensitive</b> objects, and the problems that arise from them when attempting to situate mathematical objects in some domain...|$|R
40|$|The use of {{the life}} history {{calendar}} (LHC) or the event history calendar as tools for collecting retrospective data has received increasing attention in many fields of social science and medicine. However, little research has examined {{the use of this}} method with web-based surveys. In this study, we adapted this method to an on-line setting to collect information about young adults' life histories, sexual behaviors, and substance use. We hypothesized that the LHC method would help respondents to date <b>sensitive</b> and <b>non-sensitive</b> events more precisely than when using a conventional questionnaire. We conducted an experimental design study comparing university students' responses to an on-line LHC and a conventional on-line question list. A test-retest design in which the respondents completed the survey again two weeks later was also applied to test the precision and reliability of the participants' dating of events. The results showed that whereas the numbers of <b>sensitive</b> and <b>non-sensitive</b> events were generally similar for the two on-line questionnaires, the responses obtained with the LHC were more consistent across the two administrations. Analyses of the respondents' on-line behavior while completing the LHC confirmed that respondents used the LHC's graphic interface to correct and reedit previous answers, thus decreasing data errors. (C) 2015 Elsevier Ltd. All rights reserved...|$|R
40|$|In today‟s era {{acquiring}} {{information about}} others {{is not difficult}} task but securing this data form interlopers is a big deal. K-anonymity model used to protect released data. Released data which is available for public used may contain <b>sensitive</b> and <b>non-sensitive</b> data. But K-anonymity model faces changes when set of sensitive attributes {{are present in the}} data set. To achieve K-anonymous table with diversity may causes distortion of data in some extent. This paper proposed a new concept to minimize this data distortion without using tuple suppression for M-SA K-anonymity Model...|$|R
40|$|Abstract:- The 790 - 862 MHz {{frequency}} band is allocated for IMT-Advanced system on a co-primary basis operation alongside existing Digital Video Broadcasting- Terrestrial (DVB-T), At WRC- 07. As {{a result of}} frequency spectrum scarcity and emerging various wireless applications, coexistence and sharing between wireless systems become a recently considerable issue. Therefore, coexistence and sharing requirements must be achieved {{in terms of both}} cochannel and adjacent channel frequencies. Coexistence situation is analyzed by using <b>sensitive</b> and <b>non-sensitive</b> spectral emission mask (SEM) of DVB-T. Possible intersystem interference mitigation techniques are suggested. Key-Words:- DVB-T, IMT-Advanced, intersystem interference, SEM, coexistence...|$|R
40|$|Cloud Computing (CC) {{is one of}} {{the fast}} growing {{computer}} network technologies and many companies offer their services through cloud network. Cloud Computing has many properties with respect to the existing traditional service provisions like scalability, availability, fault tolerance, capability and so on which are supported by many IT companies like Google, Amazon, Salesforce. com. These IT companies have more chances to adapt their services into a new environment, known as Cloud computing systems. There are many cloud computing services which are being provided by many IT companies. The purpose of this thesis is to investigate which cloud environment (public, private and hybrid) and services (Infrastructure as a Service, Software as a Service, and Platform as a Service) are suitable for Swedish Armed Forces (SWAF) with respect to performance, security, cost, flexibility and functionality. SWAF is using private (internal) cloud for communications where both <b>sensitive</b> and <b>non-sensitive</b> information are located in the internal cloud. There are problems like maintenance of hardware, cost issues and secure communication while maintaining the private cloud. In order to overcome those problems we have suggested a hybrid and community cloud environment and SaaS, IaaS, PaaS services for SWAF. For suggesting these cloud environments and cloud services we have performed a literature study and two empirical studies (survey and interviews) with different organizations. A new cloud model is designed based on the suggested cloud environment, separate storage spaces for <b>sensitive</b> and <b>non-sensitive</b> information, suitable services and an effective infrastructure for sharing the internal information for SWAF...|$|R
40|$|Conversational agents (CAs) are {{becoming}} an increasingly common component in many information systems. The ubiquity of CAs in cell phones, entertainment systems, and messaging applications {{has led to}} a growing need to understand how design choices made when developing CAs influence user interactions. In this study, we explore the use case of CAs that gather potentially sensitive information from people-”for example, in a medical interview. Using a laboratory experiment, we examine the influence of CA responsiveness and embodiment on the answers people give in response to <b>sensitive</b> and <b>non-sensitive</b> questions. The results show that for sensitive questions, the responsiveness of the CA increased the social desirability of the responses given by participants...|$|R
40|$|We {{experimentally}} {{examined the}} effects of negative integral affect on preferences among the double in-group (ii), crossed (io and oi), and double out-group (oo) targets of the crossed categorization paradigm. We used insults from members of politically <b>sensitive</b> vs. <b>non-sensitive</b> out-group categories of a crossed target (Oi) to induce affect. Dependent measures included self-reports and a psycho-physiological measure of affect (facial electromyography, EMG). Under no insult, participants conformed to social desirability pressure and favorably evaluated targets with a politically sensitive out-group membership, whereas facial EMG measures indicated greater negativity toward those same targets. Negativity of self-report and facial EMG measures converged, however, when members of a politically sensitive out-group category had provided hostility-justifying insults...|$|R
40|$|AbstractWe {{consider}} constrained variational {{problems in}} mixed formulation. Denoting by V and M the energy {{space and the}} space of the Lagrange multipliers, the resolvent is continuous from V′×M′ into V×M. In applications to PDE, it is said that the problem is Lagrange multiplier sensitive when M′ does not contain the space D of test functions of distributions. This amounts to some kind of instability as very small and smooth perturbations of the data imply that the solution goes out of V×M. We give a criterion of sensitivity and relations with penalty perturbation problems. We apply the abstract theory to thin elastic shells (in the case when the middle surface S is not geometrically rigid) and give several examples of <b>sensitive</b> and <b>non-sensitive</b> shells...|$|R
40|$|Large scale {{rating data}} usually {{contains}} both ratings of <b>sensitive</b> and <b>non-sensitive</b> issues, and {{the ratings of}} sensitive issues belong to personal privacy. Even when survey participants do not reveal any of their ratings, their survey records are potentially identifiable by using information from other public sources. In {{order to protect the}} privacy in the large-scale rating data, it is important to propose new privacy principles which consider the properties of the rating data. Moreover, given the privacy principle, how to efficiently determine whether the rating data satisfied the required privacy principle is crucial as well. Furthermore, if the privacy principle is not satisfied, an efficient method is needed to securely publish the large-scale rating data. In this paper, all these problem will be addressed...|$|R
40|$|Purpose - The {{purpose of}} this paper is to measure the extent of {{corporate}} social and environmental disclosure (CSED) made by Indonesian listed companies in the Indonesia Stock Exchange (IDX) on their corporate web sites, and to investigate the relationship between the company 2 ̆ 7 s environmental sensitivity and the extent of the corporate social and environmental web site disclosure. Design/methodology/approach - The corporate social and environmental web site disclosure examination in this study was conducted at the company level, with the target sample of Indonesian listed companies that provided their profiles on the IDX web site. The sample consisted of both <b>sensitive</b> and <b>non-sensitive</b> companies. Content analysis was used to analyse the data. Findings - The results of this study suggest that the extent of CSED made by Indonesian listed companies on their corporate web sites is low and the nature of disclosure is mostly descriptive, without any specific time frame. The most disclosed information is 2 ̆ 2 community 2 ̆ 2, followed by 2 ̆ 2 human resources 2 ̆ 2. It is found that there is no significant difference between the extent of CSED in both <b>sensitive</b> and <b>non-sensitive</b> industries. Overall, the results indicate that the practice of CSED in Indonesia is still at an early stage. It seems that most of the companies in this study still have a lack of understanding about CSED, and the main reason for their disclosure is to gain societal recognition of the adequacy of their social behavior. Originality/value - In the lack of studies which explore the practice of CSED within the developing country context, and in media other than annual reports, this paper provides some insight about the practice of CSED made by Indonesian listed companies on their corporate web sites...|$|R
40|$|We {{explore the}} {{feasibility}} of automatically finding accounts that publish sensitive content on Twitter. One natural approach to this problem is to first create a list of sensitive keywords, and then identify Twitter accounts that use these words in their tweets. But such an approach may overlook sensitive accounts that {{are not covered by}} the subjective choice of keywords. In this paper, we instead explore finding sensitive accounts by examining the percentage of anonymous and identifiable followers the accounts have. This approach is motivated by an earlier study showing that sensitive accounts typically have a large percentage of anonymous followers and a small percentage of identifiable followers. To this end, we first considered the problem of automatically determining if a Twitter account is anonymous or identifiable. We find that simple techniques, such as checking for name-list membership, perform poorly. We designed a machine learning classifier that classifies accounts as anonymous or identifiable. We then classified an account as sensitive based on the percentages of anonymous and identifiable followers the account has. We applied our approach to approximately 100, 000 accounts with 404 million active followers. The approach uncovered accounts that were sensitive for a diverse number of reasons. These accounts span across varied themes, including those that are not commonly proposed as sensitive or those that relate to socially stigmatized topics. To validate our approach, we applied Latent Dirichlet Allocation (LDA) topic analysis to the tweets in the detected <b>sensitive</b> and <b>non-sensitive</b> accounts. LDA showed that the <b>sensitive</b> and <b>non-sensitive</b> accounts obtained from the methodology are tweeting about distinctly different topics. Our results show that it is indeed possible to objectively identify sensitive accounts at the scale of Twitter. Comment: A shorter 4 -page version of this work has been published as a poster in the International AAAI Conference on Web and Social Media (ICWSM), 201...|$|R
40|$|Data anonymization {{is gaining}} much {{attention}} these days as {{it provides the}} fundamental requirements to safely outsource datasets containing identifying information. While some techniques add noise to protect privacy others use generalization to hide the link between <b>sensitive</b> and <b>non-sensitive</b> information or separate the dataset into clusters to gain more utility. In the latter, {{often referred to as}} bucketization, data values are kept intact, only the link is hidden to maximize the utility. In this paper, we showcase the limits of disassociation, a bucketization technique that divides a set-valued dataset into $k^m$-anonymous clusters. We demonstrate that a privacy breach might occur if the disassociated dataset is subject to a cover problem. We finally evaluate the privacy breach using the quantitative privacy breach detection algorithm on real disassociated datasets. Comment: Accepted to Secrypt 201...|$|R
40|$|This paper {{proposes a}} new model of user-centric, global, {{probabilistic}} privacy, geared for today’s challenges of helping users to manage their privacy-sensitive information across a wide variety of social networks, online communities, QA fo-rums, and search histories. Our approach anticipates an adversary that harnesses global background knowledge and rich statistics in order to make educated guesses, that is, probabilistic inferences at sensitive data. We aim for a tool that simulates such a powerful adversary, predicts pri-vacy risks, and guides the user. In this paper, our frame-work is specialized for the case of Internet search histo-ries. We present preliminary experiments that demonstrate how estimators of global correlations among <b>sensitive</b> and <b>non-sensitive</b> key-value items can be fed into a probabilistic graphical model in order to compute meaningful measures of privacy risk...|$|R
30|$|Excluding {{irregular}} touches (e.g., fumbling phones) and multi-touch behaviour (e.g., zooming gestures), {{our test}} {{is focused on}} the most common case that a user types characters on a qwerty soft keyboard with his or her single-touch behaviour. The user app used for test is SMS. When a user types text in SMS, the translation results of keystrokes will be analyzed by IM-Visor to decide whether the keystrokes are sensitive. The conclusions can be classified into two types: <b>Sensitive</b> keystrokes and <b>non-sensitive</b> keystrokes.|$|R
40|$|The {{antibacterial}} activity of 68 nitrofuran derivatives and 18 thiophene derivatives {{was investigated in}} vitro. The effects of the nitrofuran derivatives on the bacterial growth were studied by the turbidimetric method, {{and the fate of}} the 5 -position nitro radical, especially the reduction of nitro radical, caused by the bacterial metabolism or multiplication was demonstrat-ed by means of spectrophotometry and polarography, using the <b>sensitive,</b> resistant or <b>non-sensitive</b> organisms. Finally the development of bacterial resistance to nitrofuran derivatives was studied by several procedures...|$|R
50|$|The primary {{application}} for LIS {{is to provide}} a place-focused evidence base that is easily accessible {{to a wide range of}} users including data experts, managers, policy makers, front-line staff and citizens. They provide a wide range of statistics and reports allowing users to review the current evidence base and build a picture of localities and neighbourhoods for their area of interest. LIS are commonly used by partnerships where they need to come together to provide joined-up services for a common area. The ability to have a common evidence base and a platform to share <b>sensitive</b> and <b>non-sensitive</b> data is critical in this situation. LIS enables partners to publish a wide range of indicators in the form of defined outputs which combine locally and nationally available data into more meaningful intelligence aimed at specific user groups.|$|R
40|$|Aspartame is a {{commonly}} used intense artificial sweetener, being approximately 200 times sweeter than sucrose. There have been concerns over aspartame since approval in the 1980 s including a large anecdotal database reporting severe symptoms. The {{objective of this}} study was to compare the acute symptom effects of aspartame to a control preparation. This was a double-blind randomized cross over study conducted in a clinical research unit in United Kingdom. Forty-eight individual who has self reported sensitivity to aspartame were compared to 48 age and gender matched aspartame non-sensitive individuals. They were given aspartame (100 mg) -containing or control snack bars randomly at least 7 days apart. The main outcome measures were acute effects of aspartame measured using repeated ratings of 14 symptoms, biochemistry and metabonomics. Aspartame <b>sensitive</b> and <b>non-sensitive</b> participants differed psychologically at baseline in handling feelings and perceived stress. Sensitive participants had higher triglycerides (2. 05 ± 1. 44 vs. 1. 26 ± 0. 84 mmol/L; p value 0. 008) and lower HDL-C (1. 16 ± 0. 34 vs. 1. 35 ± 0. 54 mmol/L; p value 0. 04), reflected in 1 H NMR serum analysis that showed differences in the baseline lipid content between the two groups. Urine metabonomic studies showed no significant differences. None of the rated symptoms differed between aspartame and control bars, or between sensitive and control participants. However, aspartame sensitive participants rated more symptoms particularly in the first test session, whether this was placebo or control. Aspartame and control bars affected GLP- 1, GIP, tyrosine and phenylalanine levels equally in both aspartame <b>sensitive</b> and <b>non-sensitive</b> subjects. Using a comprehensive battery of psychological tests, biochemistry and state of the art metabonomics there was no evidence of any acute adverse responses to aspartame. This independent study gives reassurance to both regulatory bodies and the public that acute ingestion of aspartame does not have any detectable psychological or metabolic effects in humans. ISRCTN Registry ISRCTN 39650237...|$|R
30|$|The {{authors of}} [18] {{introduced}} an online 3 G budget algorithm that decides which sensory data should be uploaded via 3 G communication while {{others will be}} uploaded or downloaded later when Wi-Fi access point is available. Their optimization scheme ensures efficient 3 G budget utilization but the algorithm has significant computational overheads. Therefore, the approach is both computational resource and energy hungry. Also, they proposed a heuristic algorithm which splits the 3 G budget in each time cycle into two pieces, namely, reserved budget and flexible budget. Sensitive applications use reserved budget and non-sensitive applications use flexible budget. If the reserved budget runs out, then sensitive applications use the allocation from the flexible budget. But this two-state application classification (<b>sensitive</b> and <b>non-sensitive)</b> decreases the dynamicity and flexibility of bandwidth allocation. In addition, budget allocation strategies for heterogenous applications based on urgency/priority are not been explicitly discussed or analyzed.|$|R
30|$|Most {{state-of-the-art}} research such as [18 – 21] do {{not consider}} dynamic budget allocation for mobile applications. The solution in [19], for example, seeks to satisfy the bandwidth requirement for constant and variable bit rate connections, while minimizing blocking probability. In [18], a heuristic solution for allocating budget to <b>sensitive</b> and <b>non-sensitive</b> applications was developed. However, it did not handle over- and under-provisioning issues while allocating portion of data budgets. In other words, the research did not allow the allocation of different resources to different applications. In our earlier work [22], an optimization framework for Internet data budget management was developed. However, the framework was designed for small systems, {{but it is not}} suited for larger system deployment due to the high computational complexity associated with an increased number of user applications. This is the gap we seek to contribute to in this paper.|$|R
40|$|Mix {{design in}} {{production}} cementitious materials is of importance where selection {{the value of}} each parameter has a critical effect on final properties of the material. In the present work, a new method has been developed {{to determine the effect}} of each considered mix design factor on the output properties. A specific property can be related linearly to the factors of mix design through normalized nonlinear weight functions. The proposed procedure was applied on two different mix designs available in the literature. The first analysis was conducted on ordinary Portland cement based concrete specimens to analyse the importance of each factor on their compressive strength. The second one was conducted on a geopolymeric system to analyse the compressive strength. For both systems, the factors were divided into <b>sensitive</b> and <b>non-sensitive</b> where <b>sensitive</b> factors were suggested to be considered with more attention in mix design procedure...|$|R
40|$|A {{statistical}} database (SDB) is {{a database}} {{that contains a}} large number of individual sensitive records, but is intended to supply only statistical summary information to its users. A SDB suffers from the inference problem, a way to infer or derive <b>sensitive</b> data from <b>non-sensitive</b> data. In this study, two security techniques of SDBs, Query-Set Size and Fixed-Data Perturbation are selected to review and compare each other. As a result, no one is a perfect solution for the inference problem. The selection of technique depends on some factors mentioned in this paper...|$|R
40|$|Telemetric {{techniques}} for the monitoring of physiological parameters during housing, handling and transport may help producers to reduce mortality and improve meat quality. In order {{to know the}} reliability of electrocardiogram parameters as stress indicators, piglets (12 to 23 kg) being different {{with respect to the}} halothane gene (homozygous halothane <b>sensitive</b> (nn) and <b>non-sensitive</b> (NN), heterozygotes (nN)), and thus different with respect to stress susceptibility, were monitored with an ambulatory ECG device during housing, handling and transport. Skeletal muscularity of all animals was measured with an ultrasound device. status: publishe...|$|R
40|$|Abstract We {{study the}} {{challenges}} of protecting privacy of individuals in the large public survey rating data in this paper. Recent study shows that personal information in supposedly anonymous movie rating records are de-identified. The survey rating data usually contains both ratings of <b>sensitive</b> and <b>non-sensitive</b> issues. The ratings of sensitive issues involve personal privacy. Even though the survey participants do not reveal any of their ratings, their survey records are potentially identifiable by using information from other public sources. None of the existing anonymisation principles (e. g., k-anonymity, l-diversity, etc.) can effectively prevent such breaches in large survey rating data sets. We tackle the problem by defining a principle called (k,ɛ) -anonymity model to protect privacy. Intuitively, the principle requires that, for each transaction t in the given survey rating data T, at least (k − 1) other transactions in T must have ratings similar to t, where the similarity is controlled by ɛ. Th...|$|R
40|$|Careless {{development}} of web-based applications results in vulnerable code being deployed and {{made available to}} the whole Internet, creating easily-exploitable entry points for the compromise of entire networks. To ameliorate this situ-ation, we propose an approach that composes a web-based anomaly detection system with a reverse HTTP proxy. The approach {{is based on the assumption}} that a web site's con-tent can be split into security <b>sensitive</b> and <b>non-sensitive</b> parts, which are distributed to dierent servers. The anomaly score of a web request is then used to route suspicious re-quests to copies of the web site that do not hold sensitive content. By doing this, it is possible to serve anomalous but benign requests that do not require access to sensitive information, sensibly reducing the impact of false positives. We developed a prototype of our approach and evaluated its applicability with respect to several existing web-based applications, showing that our approach is both feasible and eective...|$|R
40|$|In {{the present}} {{experiment}} we examined reality monitoring for socially <b>sensitive</b> and <b>non-sensitive</b> {{information relating to}} prejudice (negative and positive beliefs about an ethnic minority). At study, participants either imagined or perceived negative or positive attributes in connection with photographs of target individuals that clearly varied in ethnicity (non-western or western). At test, the task {{was to determine if}} they had imagined or perceived an attribute in connection with each target person from the study phase. We predicted that participants high in motivation to control prejudice would avoid responding that they had generated (imagined) socially sensitive information (i. e. negative attributes in connection with non-western targets), and instead respond that such information was presented. Results confirmed this prediction: Motivation to control prejudice was connected with a bias towards responding "perceived" (instead of "imagined") when source-decisions were made in regard to the socially sensitive combination non-western + negative. This pattern appeared for both perceived and imagined combinations...|$|R
30|$|Industry type {{refers to}} {{environmentally}} <b>sensitive</b> versus <b>non-sensitive</b> industries. Where environmentally sensitive industries refer to industries whose activities affect the environment directly. Mahmood (1999) suggests that disclosure levels reflect {{the type of}} industry, whilst Reverte (2009, p. 355) cite “mining, oil, and chemical industries as emphasising information regarding environmental, health, and safety issues” as opposed to finance and insurance companies. This makes such companies more environmentally sensitive. These disclosures are more aligned to companies whose activities affect the environment significantly (Brammer and Pavelin 2006, Brammer and Pavelin 2008; Campbell et al. 2003; Cho and Patten 2007; Deegan and Gordon 1996; Hackston and Milne 1996; Roberts 1992; Zeng et al. 2012). First of all, firms in sensitive industries comply with strict environmental regulations due to the emission effect of their activities and therefore should disclose their environmental concerns, otherwise stakeholders and especially investors may assume the worst (Cormier and Magnan 2003; Clarkson et al. 2008; Cho and Patten 2007; Hackston and Milne 1996; da Silva Monteiro and Aibar-Guzmán 2010).|$|R
30|$|Challenges. To {{leverage}} {{the above}} three key ideas, {{we are facing}} three main challenges. First, in the existing modern mobile devices, an IME app is the first service to receive (<b>sensitive</b> or <b>non-sensitive)</b> keystrokes from Android event subsystem, and translates them to text. Distinct from a post-IME design which does a rollback after the IMEs translating keystrokes, in a pre-IME design, how can we intercept and isolate sensitive keystrokes ahead of IME translation? This is called the “Isolation ahead of IME translation issue” (Challenge 1). Second, after we succeeding in intercepting and isolating those sensitive keystrokes, how can we build a trusted path for user apps to access these sensitive keystrokes? We call it the “Trusted path issue” (Challenge 2). Finally, recalling the reason why users got incentives to use IMEs in the first paragraph, an IME app does provide convenience and extra benefits. In a pre-IME design, how can we retain the value added feature for user apps? We call it the “Benefits retaining issue” (Challenge 3).|$|R
40|$|Abstract. Businesses, governments, and {{individuals}} leak confidential information, both accidentally and maliciously, at tremendous cost in money, privacy, national security, and reputation. Several security software vendors now offer “data loss prevention ” (DLP) solutions that use simple algorithms, such as keyword lists and hashing, which are too coarse {{to capture the}} features what makes sensitive documents secret. In this paper, we present automatic text classification algorithms for classifying enterprise documents as either <b>sensitive</b> or <b>non-sensitive.</b> We also introduce a novel training strategy, supplement and adjust, to create a classifier that has a low false discovery rate, even when presented with documents unrelated to the enterprise. We evaluated our algorithm on several corpora that we assembled from confidential documents published on WikiLeaks and other archives. Our classifier had a false negative rate of less than 3. 0 % and a false discovery rate of less than 1. 0 % on all our tests (i. e, in a real deployment, the classifier can identify more than 97 % of information leaks while raising at most 1 false alarm every 100 th time). ...|$|R
40|$|In this paper, {{we study}} {{a problem of}} privacy {{protection}} in large survey rating data. The rating data usually contains both ratings of <b>sensitive</b> and <b>non-sensitive</b> issues, and the ratings of sensitive issues include personal information. Even when survey participants do not reveal any of their ratings, their survey records are potentially identifiable by using information from other public sources. We propose a new (k,, l) - anonymity model, in which each record is required to be similar with at least k−l others based on the non-sensitive ratings, where the similarity is controlled by, and the standard deviation of sensitive ratings is at least l. We study an interesting yet nontrivial satisfaction problem of the (k,, l) -anonymity, which is to decide whether a survey rating data set satisfies the privacy requirements given by users. We develop a slice technique for the satisfaction problem and the experimental {{results show that the}} slicing technique is fast, scalable and much more efficient in terms of execution time than the heuristic pairwise method...|$|R
40|$|In this paper, a novel {{two-stage}} noise removal algorithm to {{deal with}} salt-pepper impulse noise is proposed. In the first stage, the decision-based recursive adaptive noise-exclusive median filter is applied to remove the noise cleanly {{and to keep the}} uncorrupted information as well as possible. In the second stage, the fuzzy decision rules inspired by human visual system (HVS) are proposed to classify image pixels into human perception <b>sensitive</b> class and <b>non-sensitive</b> class. A neural network is proposed to compensate the sensitive regions for image quality enhancement. According to the experimental results, the proposed method is superior to conventional methods in perceptual image quality as well as the clarity and the smoothness in edge regions of the resultant images...|$|R
40|$|One of the {{obstacles}} in using data mining techniques such as association rules is the risk of leakage of sensitive data after the data is released to the public. Therefore, a trade-off between the data privacy and data mining {{is of a great}} importance and must be managed carefully. In this study an efficient algorithm is introduced for preserving the privacy of association rules according to distortion-based method, in which the sensitive association rules are hidden through deletion and reinsertion of items in the database. In this algorithm, {{in order to reduce the}} side effects on non-sensitive rules, the item correlation between <b>sensitive</b> and <b>non-sensitive</b> rules is calculated and the item with the minimum influence in non-sensitive rules is selected as the victim item. To reduce the distortion degree on data and preservation of data quality, transactions with highest number of sensitive items are selected for modification. The results show that the proposed algorithm has a better performance in the non-dense real database having less side effects and less data loss compared to its performance in dense real database. Further the results are far better in synthetic databases in compared to real databases...|$|R
40|$|Abstract In {{this paper}} we study {{the problem of}} pro-tecting privacy in the {{publication}} of set-valued data. Consider a collection of supermarket transactions that contains detailed information about items bought to-gether by individuals. Even after removing all personal characteristics of the buyer, which can serve as links to his identity, the publication of such data is still subject to privacy attacks from adversaries who have partial knowledge about the set. Unlike most previous works, we do not distinguish data as <b>sensitive</b> and <b>non-sensitive,</b> but we consider them both as potential quasi-identifiers and potential sensitive data, depending {{on the point of}} view of the adversary. We define a new version of the k-anonymity guarantee, the km-anonymity, to limit the effects of the data dimensionality and we propose effi-cient algorithms to transform the database. Our anony-mization model relies on generalization instead of sup-pression, which is the most common practice in related works on such data. We develop an algorithm which finds the optimal solution, however, at a high cost which makes it inapplicable for large, realistic problems. Then, we propose a greedy heuristic, which performs general...|$|R
