0|34|Public
5000|$|... 5. How {{pharmaceutical}} companies could use open <b>source</b> molecular <b>descriptors</b> and algorithms which would facilitate computational model sharing with {{the academic and}} neglected disease community ...|$|R
40|$|This work {{investigates the}} {{edge-to-edge}} performance of an Optical Burst Switched network employing a reser-vation mechanism known as JET, Just Enough Time. Two main components are studied and analyzed, the edge router with the burst assembly function and {{the core network}} with routing and forwarding. We assess the performance, in terms of burst blocking probability and delay, experienced by three service classes on a proposed European network. Multiple traffic <b>source</b> <b>descriptors</b> are considered. Different traffic settings, resource management and routing options are investigated with the aim to meet diverse QoS requirements...|$|R
40|$|Nowadays {{there are}} several direct or {{indirect}} measurement methods for {{the determination of the}} surface velocity in vibrating structures, but two of them seem to be the most promising and interesting, in particular for vibro-acoustic problems: laser Doppler vibrometry (LDV) and near-field acoustic holography (NAH). While LDV is a direct laser-based vibration measurement technique, NAH allows the determination of the particle surface velocity starting from simultaneous microphone measurements performed on a plane array positioned near the vibrating object, although with some limitations. In this work the two structural and acoustic techniques are compared on a simple laboratory case, specifically a plate, in order to carefully and quantitatively assess the measurement uncertainty in the indirect NAH method used to estimate the vibration velocity. Advantages and disadvantages of the two methods are discussed briefly. This study was conducted within the European Growth Project ‘‘ACES’’ (Optimal Acoustic Equivalent <b>Source</b> <b>Descriptors</b> for Automotive Noise Problems) GRD 1 - 1999 - 11202...|$|R
40|$|In {{this paper}} we {{address the problem of}} {{determining}} <b>source</b> traffic <b>descriptors</b> for virtual path connections formed by a number of delay-sensitive virtual channel connections. In order to obtain a virtual path connection which allows an efficient use of the network resources, traffic shaping should be performed at the origin of the connection to reduce its burstiness. However, a conventional shaping process in conjunction with FIFO multiplexing introduces delays that may be too long, especially for delay-sensitive connections offering a low cell rate, such as phone calls or video telephony. To solve this trade-off, we propose a shaping algorithm called dual cell spacing, which allows to obtain small shaping delays and an efficient use of the network resources at the same time. Furthermore, we give simple dimensioning rules for the <b>source</b> traffic <b>descriptors</b> of the shaped virtual path. Such a dimensioning is not straightforward if traffic shaping is omitted. Numerical examples are presen [...] ...|$|R
40|$|In {{this paper}} we {{address the problem of}} the {{determination}} of <b>Source</b> Traffic <b>Descriptors</b> for Virtual Path Connections formed by a number of delay-sensitive Virtual Channel Connections. In order to obtain a Virtual Path Connection which allows an efficient use of the network resources, some traffic shaping should be performed at the origin of the connection to reduce its burstiness. However, a shaping process introduces a delay which may be too large in some cases, especially for delay-sensitive connections offering a low cell rate, like e. g. phone calls. To solve this trade-off, we propose in this paper a shaping algorithm called Dual Cell Spacing, which allows to obtain small shaping delays and an efficient use of the network resources at the same time. Furthermore, we give simple dimensioning rules for the <b>Source</b> Traffic <b>Descriptors</b> of the shaped Virtual Path. Such a dimensioning is not straightforward if traffic shaping is omitted. Numerical examples illustrate the feasibility of the proposed approach...|$|R
40|$|The {{information}} available on {{factors that influence}} emissions from the principal societal sources of ammonia to the atmosphere, namely combustion processes, volatilization of farm animal wastes, and volatilization of fertilizers, is reviewed. Emission factors are established for each major source of atmospheric ammonia. The factors are then multiplied by appropriate <b>source</b> characterization <b>descriptors</b> to obtain calculated fluxes of ammonia to the atmosphere on a state-by-state basis for the United States...|$|R
40|$|International audienceIn video coding, it is {{commonly}} accepted that the encoding paramaters such as the quantization step-size have an influence on the perceived quality. When dealing with Scalable Video Coding (SVC), the parameters used to encode each layer logically have an influence on the overall perceived quality. It is also commonly accepted that using given encoding parameters, the perceived quality does not change significantly according to the encoded source content. In this paper, we evaluate the impact of both SVC coding artifacts and source contents on the quality perceived by human observers. We exploit the outcomes of two subjective experiments designed and conducted under standard conditions {{in order to provide}} reliable results. The two experiments are aligned on a common scale using a set of shared processed video sequences, resulting in a database containing the subjective scores for 60 different sources combined with 20 SVC scenarios. We analyse the performance of several <b>source</b> <b>descriptors</b> in modeling the relative behaviour of a given source content when compared to the average of other source contents...|$|R
2500|$|The {{neoplasms}} currently {{referred to}} as meningiomas were referred to {{with a wide range}} of names in older medical literature, depending on the <b>source.</b> Various <b>descriptors</b> included [...] "fungoid tumors", [...] "fungus of the dura mater", [...] "epithelioma", [...] "psammoma", [...] "dural sarcoma", [...] "dural endothelioma", [...] "fibrosarcoma", [...] "angioendothelioma", [...] "arachnoidal fibroboastoma", [...] "endotheliosis of the meninges", [...] "meningeal fibroblastoma", [...] "meningoblastoma", [...] "mestothelioma of the meninges", [...] "sarcoma of the dura", and others.|$|R
40|$|The Nix {{software}} deployment {{system is}} based on the paradigm of transparent source/binary deployment: distributors deploy descriptors that build components from source, while client machines can transparently optimise such source builds by downloading pre-built binaries from remote repositories. This model combines the simplicity and flexibility of source deployment with the efficiency of binary deployment. A desirable property is sharing of components: if multiple users install from the same <b>source</b> <b>descriptors,</b> ideally only one remotely built binary should be installed. The problem is that users must trust that remotely downloaded binaries were built from the sources they are claimed to have been built from, while users in general do not have a trust relation with each other or with the same remote repositories. This paper presents three models that enable sharing: the extensional model that requires that all users on a system have the same remote trust relations, the intensional model that does not have this requirement but may be suboptimal in terms of space use, and the mixed model that merges the best properties of both. The latter two models are achieved through a novel technique of hash rewriting in content-addressable component stores, and were implemented {{in the context of the}} Nix system. 1...|$|R
40|$|During 1995 - 2002, 109 {{articles}} and 2277 references published in 10 volumen of the “Revista de Investigaciones Agropecuarias” (RIA) were investigated, except 1998 - 1999 that RIA wasn´t edited. In order to determinate authors´s behavior patttern: use of documental <b>source,</b> <b>descriptors</b> and languages´s prevalence, coauthors, colaboration´s groups, annual obsolescence and productivy, different index and bibliometric techniques were applied. Minimum references (in absolute terms) were during 1997 and 2001 years, {{but it was}} 1997 (144 references) considering relatives amount by volume. In respect to quotations, 67 was the maximum and 3 the minimum, {{with an average of}} 20, 89 quotations by article; it was an coauthor´s index of 2, 37, and 3, 36 considering the 2277 references and the 109 papers, respectively. Spanish is the main language in works (91, 74 %) followed by english (8, 26 %). There was a prevalence of english as more consulted language (65, 17 %), followed by spanish (31, 18 %), and in minor proportion portuguese, french, germany and italian language (3, 65 %). Besides, the periodical publications were the most consulted documental source (62, 41 %), following monographs (28, 15 %) and finally papers presented at Congress (9, 44 %). Regarding consulted literature during the last 3 years, major frecuency ocurred during the 80 ´ and 90 ´ decades. Cattle, forages, dairy cattle, insecticides and direct drilling were the main subject, fitted with INTA´s objectives, editor of RIA. Applying Bradford´s law, core will contain authors with up to 4 works: Saini (7), Paccapelo (6), Kloster and Santini (5), Benintende, Covas and Rearte (4). There was coincidence between the subject of these authors and the descriptors core...|$|R
40|$|We {{present a}} dynamic call {{admission}} control method {{based on the}} diffusion approximation method, which is traditionally used for calculating the effective bandwidth {{for a number of}} connections having a certain cell loss requirement. The proposed method has a low implementation overhead since it performs measurements on the aggregate traffic stream at a particular ATM switch as opposed to other methods that need to perform per-source measurements. We provide a study of the performance of the method for specific traffic types using simulation and compare with the performance of the static diffusion approximation effective bandwidth method. We also study the influence of variation in traffic parameters such as the <b>source</b> traffic <b>descriptors,</b> buffer size, and the required cell loss probability on the performance of the method. 1...|$|R
30|$|To {{reduce the}} role of {{subjectivity}} in the task categorization step, an alternative approach to grouping occupations manually is to append a set of standardized job descriptors to each occupation. These descriptors can in turn be used to develop task measures. In the U.S., two major data sources, the Dictionary of Occupational Titles (DOT) and its successor, the Occupational Information Network (O*NET), offer <b>sources</b> for job <b>descriptors,</b> and both have been used frequently in empirical work on job tasks.|$|R
5000|$|We {{introduce}} the atomic orbitals , which are eigenfunctions of the Hamiltonian [...] {{of a single}} isolated atom. When the atom is placed in a crystal, this atomic wave function overlaps adjacent atomic sites, and so are not true eigenfunctions of the crystal Hamiltonian. The overlap is less when electrons are tightly bound, which is the <b>source</b> of the <b>descriptor</b> [...] "tight-binding". Any corrections to the atomic potential [...] required to obtain the true Hamiltonian [...] of the system, are assumed small: ...|$|R
40|$|This paper {{takes an}} overall view of {{findings}} from the Positive Soundscape Project, a large inter-disciplinary soundscapes study. Qualitative fieldwork (soundwalks and focus groups) have found that soundscape perception is influenced by cognitive effects such as {{the meaning of a}} soundscape and its components, and how information is conveyed by a soundscape, for example on the behaviour of people within the soundscape. Three significant clusters were found in the language people use to describe soundscapes: sound <b>sources,</b> sound <b>descriptors</b> and soundscape descriptors. Results from listening tests and soundwalks have been integrated to show that the two principal dimensions of soundscape emotional response seem to be calmness and vibrancy. Further, vibrancy seems to have two aspects: organisation of sounds and changes over time. The possible application of the results to soundscape assessment and design are briefly discussed...|$|R
40|$|The present paper {{continues}} with the bibliometric analysis of Revista de Investigaciones Agropecuarias. For that purpose, it is analysed {{a total of}} 90 contributions, appeared in 14 numbers and their 1336 references, published in the period comprehended between 1964 - 1979, {{with an average of}} 14. 85 cites per work, and a rank between 87 and 3 respectively. The mean productivity of the magazine is 6. 43 numbers/year with a rank comprehended between 1 and 21. In order to determinate authors´s behavior patttern: use of documental <b>source,</b> <b>descriptors</b> and languages´s prevalence, coauthors, colaboration´s groups, annual obsolescence and productivy, different index and bibliometric techniques were applied. Among the papers it predominated English (the most consulted language) with 75. 97 %, the Spanish with a 18. 64 % and in less proportion French, German and others with a 5. 39. Periodicals was the documentary source most used with a 74. 70 %, second was monographies with a 18. 94 %, and finally contributions to congresses with a 6. 36 %. With respect to citations, the highest frequency corresponds to seventies (813 citations) and the sixties (523 citations). The most treated subjects are: Bovines (21. 47 %), Feeding (8. 90), Pastures (7. 85 %), Rodents (6. 81 %) and Pigs (4. 71), all of which matches INTA’s goals. All this subjects sum up 49. 74 % of the distribution. The index of coauthorship reaches 2. 76 per article. The mean life is 8. 39 years, the utility 7 %, and the index of Price 0. 78356 (26. 77 %.). The core of periodicals comprises 7 titles: Journal of Animal Science, RIA, Journal of Nutrition, Journal of Dairy Science., Journal of Agricultural Science, Australian Journal of Agricultural Research, and Journal of Biological Chemistry. The most productive authors are: Massoia, E. (8. 75), Giraudo, C. G. (5. 08), Marchi, A. (5. 08), Cercos, A. P. (5), Cairnie, A. G. (3, 5) y García, Pilar T. (3, 08) ...|$|R
40|$|The {{control of}} cell {{processes}} plays {{a central role}} in ATM network management and strongly influences the overall system overload behavior and the quality of service. One of the crucial control functions is the usage parameter control according to the user-network contract for a connection. Due to the slotted time of the ATM cell process discrete-time models are suited for performance analysis. In this paper we first present queueing models operating in discrete-time for the cell rate monitoring, where arbitrary renewal cell input processes are taken into account. Subsequently cell processes of ON/OFF-type are considered, where the lengths of ON- and OFF-phases can be arbitrarily distributed. Numerical results are discussed aiming the proper choice of the <b>source</b> traffic <b>descriptor</b> parameter values like cell delay variation tolerance, sustainable cell rate and burst tolerance for different cell process characteristics to be enforced. Keywords Asynchronous Transfer Mode, Consecutive Cell [...] ...|$|R
40|$|This study {{presented}} a {{content analysis of}} the use of confidential sources by journalists from Time and Newsweek in 2006. The results of the content analysis were then compared to studies conducted by Hugh M. Culbertson in 1976 and Tim K. Wulfemeyer in 1985 of the same newsmagazines. The content analysis looked at the types of confidential sources being used, what stories contained confidential <b>sources</b> and the <b>descriptors</b> used to describe the sources to readers. The study shows evidence that use of confidential sources by journalists has decreased over timeDepartment of JournalismThesis (M. A. ...|$|R
40|$|In this work, multivariate {{characterization}} data such as infrared (IR) spectroscopy {{was used}} as a <b>source</b> of <b>descriptor</b> data involving information on molecular architecture for designing structured molecules with tailored properties. Application of multivariate statistical techniques such as principal component analysis (PCA) allowed capturing important features of the molecular architecture from complex data to build appropriate latent variable models. Combining the property clustering techniques and group contribution methods (GCM) based on characterization data in a reverse problem formulation enabled identifying candidate components by combining or mixing molecular fragments until the resulting properties match the targets. The developed methodology is demonstrated using molecular design of biodiesel additive which when mixed with off-spec biodiesel produces biodiesel that meets the desired fuel specifications. The contribution of this work is that the complex structures and orientations of the molecule can be included in the design, thereby allowing enumeration of all feasible candidate molecules that matched the identified target but were not part of original training set of molecules...|$|R
40|$|The ATM {{transport}} {{concept of}} B-ISDN allows for an efficient and flexible use of network resources for {{variable bit rate}} services with high transmission rates. One major traffic source which will make use of this capability will be video and multimedia applications. The changes in the cell rate of such sources are caused by the compression of the digitized video frames. The MPEG (ISO Moving Picture Expert Group) coding scheme {{is expected to be}} the major compression algorithm for the first ATM video applications. Thus there is a need to find <b>source</b> traffic <b>descriptors</b> of MPEG video sources which can be used efficiently for connection admission control and usage parameter control for these sources. Besides the peak cell rate, the sustainable cell rate might be used for these purposes. Both the ITU-T and the ATM Forum propose the Generic Cell Rate Algorithm (GCRA) for policing these two cell rates. In this paper we develop a novel model for the cell stream of an MPEG video coder output and [...] ...|$|R
40|$|The RANK command has {{now been}} added to Dialog DataStarWeb {{so that you can}} find {{statistical}} trends in search results. In addition, greater Alerts functionality—the ability to set up multifile Alerts, edit Alerts and use a tool to track Alert deliveries—is available. The RANK Option To help you pinpoint essential information relevant to your search, Dialog DataStarWeb offers a new data-mining tool called RANK. The RANK command analyzes statistical trends found in your search results by counting the occurrences of unique terms within a specific field. Depending on the database selected, specific document fields are conveniently listed in a dropdown box from approximately 122 of the Dialog DataStar databases. Some of the fields that can be ranked are: descriptor terms, author, journal, company names, CAS ® registry numbers and country. RANK SAMPLES You can answer a number of important questions by RANKing authors, <b>sources</b> or <b>descriptors.</b> Some examples include: f Who was the most published author in a given field in a given country in the last three years...|$|R
40|$|Among {{computational}} chemistry methods, {{quantum mechanics}} calculates geometries and electronic structures with accuracy especially for systems with electronic delocalization. The {{use of a}} multiconfigurational approach is able to treat highly degenerated states such as those occurring at the transition state in some chemical reactions. Moreover, an accurate description of potential energy surfaces can be obtained with {{the evaluation of the}} dynamic electron correlation effects by this approach. Molecular properties range from simple dipole moments, vibrational frequencies or IR intensities to frequency dependent hyperpolarizabilities. Quantum chemical calculations are thus an attractive <b>source</b> of molecular <b>descriptors</b> which can be used in QSAR/QSPR studies and which can express all electronic and geometric properties of molecules. A survey and a comparison of the performance of free e-resources for semi-empirical and ab initio calculations is provided...|$|R
40|$|In {{this paper}} we apply {{nonlinear}} signal analysis to a music information retrieval task. More concretely, we apply {{the concept of}} recurrence plots and recurrence histograms to extract information from music audio frames. We {{evaluate the effectiveness of}} this approach with a typical genre classification framework and compare it against a baseline obtained from standard spectrum-based descriptors. The accuracy reached by the histogram-based descriptors alone does not surpass the one achieved by the spectral-based descriptors. However, we show that the combination of both <b>descriptor</b> <b>sources</b> results in consistent improvements up to 5 absolute percent points. This highlights the potential of nonlinear signal analysis for quantitative music description. In particular, it suggests that the information resulting from this approach is complementary to the information obtained through the commonly used spectral representation...|$|R
40|$|Abstract—Compounds from {{discovery}} {{are often}} poor candidates for lead optimization or preclinical testing because screening efforts focus on target affinity, while paying limited attention to ADME/Tox properties. So here the ADME/Tox properties of certain drugs {{have been studied}} and a mathematical model has been developed using machine learning algorithms. This model will predict whether the given molecule is a proteomic drug or not which will be a preliminary step in drug designing. 630 proteomic drugs and equal number of non-drugs were obtained from database <b>sources.</b> Around 1103 <b>descriptors</b> for both drugs and non-drugs were generated. The obtained datasets were manually validated. Descriptor load was reduced using PCA. Statistical machine learning techniques like ANN and SVM were used to explore the data and study drug-likeness. SVM {{was found to be}} the best classifier providing a classification accuracy o...|$|R
40|$|We {{present a}} {{technique}} for efficiently synthesizing images of atmospheric clouds {{using a combination}} of Monte Carlo integration and neural networks. The intricacies of Lorenz-Mie scattering and the high albedo of cloud-forming aerosols make rendering of clouds [...] -e. g. the characteristic silverlining and the "whiteness" of the inner body [...] -challenging for methods based solely on Monte Carlo integration or diffusion theory. We approach the problem differently. Instead of simulating all light transport during rendering, we pre-learn the spatial and directional distribution of radiant flux from tens of cloud exemplars. To render a new scene, we sample visible points of the cloud and, for each, extract a hierarchical 3 D descriptor of the cloud geometry with respect to the shading location and the light <b>source.</b> The <b>descriptor</b> is input to a deep neural network that predicts the radiance function for each shading configuration. We make the key observation that progressively feeding the hierarchical descriptor into the network enhances the network's ability to learn faster and predict with high accuracy while using few coefficients. We also employ a block design with residual connections to further improve performance. A GPU implementation of our method synthesizes images of clouds that are nearly indistinguishable from the reference solution within seconds interactively. Our method thus represents a viable solution for applications such as cloud design and, thanks to its temporal stability, also for high-quality production of animated content. Comment: ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia 2017...|$|R
40|$|Abstract Background Adenosine- 5 ′-triphosphate (ATP) {{is one of}} {{multifunctional}} nucleotides {{and plays}} an important role in cell biology as a coenzyme interacting with proteins. Revealing the binding sites between protein and ATP is significantly important to understand the functionality of the proteins and the mechanisms of protein-ATP complex. Results In this paper, we propose a novel framework for predicting the proteins’ functional residues, through which they can bind with ATP molecules. The new prediction protocol is achieved by combination of sequence evolutional information and bi-profile sampling of multi-view sequential features and the sequence derived structural features. The hypothesis for this strategy is single-view feature can only represent partial target’s knowledge and multiple <b>sources</b> of <b>descriptors</b> can be complementary. Conclusions Prediction performances evaluated by both 5 -fold and leave-one-out jackknife cross-validation tests on two benchmark datasets consisting of 168 and 227 non-homologous ATP binding proteins respectively demonstrate the efficacy of the proposed protocol. Our experimental results also reveal that the residue structural characteristics of real protein-ATP binding sites are significant different from those normal ones, for example the binding residues do not show high solvent accessibility propensities, and the bindings prefer to occur at the conjoint points between different secondary structure segments. Furthermore, results also show that performance is affected by the imbalanced training datasets by testing multiple ratios between positive and negative samples in the experiments. Increasing the dataset scale is also demonstrated useful for improving the prediction performances. </p...|$|R
5000|$|FooDB (The Food Database) is a freely available, open-access {{database}} containing chemical (micronutrient and macronutrient) {{composition data}} on common, unprocessed foods. It also contains extensive data on {{flavour and aroma}} constituents, food additives as well as positive and negative health effects associated with food constituents. The database contains information on more than 28,000 chemicals found in more than 1000 raw or unprocessed food products. The data in FooDB was collected from many sources including textbooks, scientific journals, on-line food composition or nutrient databases, flavour and aroma databases and various on-line metabolomic databases. [...] This literature-derived information has been combined with experimentally derived data measured on thousands of compounds from more than 40 very common food products through the Alberta Food Metabolome Project which is led by Dr. David Wishart of the University of Alberta. Users are able to browse through the FooDB data by food <b>source,</b> name, <b>descriptors</b> or function. Chemical structures and molecular weights for compounds in FooDB may be searched via a specialized chemical structure search utility. Users are able to view the content of FooDB using two different “Viewing” options: FoodView, which lists foods by their chemical compounds, or ChemView, which lists chemicals by their food sources. Knowledge about the precise chemical composition of foods {{can be used to}} guide public health policies, assist food companies with improved food labelling, help dieticians prepare better dietary plans, support nutraceutical companies with their submissions of health claims and guide consumer choices with regard to food purchases.|$|R
40|$|Connectionless traffic {{originating}} from LANs is carried as Available Bit Rate (ABR) {{traffic in the}} ATM network and is subjected to ATM level flow control, either rate-based or credit-based. In addition, transport layer flow control (e. g. TCP window control) is applied at the end points. In this paper, we study the interaction between TCP and ATM flow controls. We use simulation to compare the "goodput" of different ABR flow control schemes and to evaluate the sensitivity to different parameter selections (e. g., TCP Maximum Segment Size (MSS), round trip delay, etc). The study uncovers an unfairness situation caused {{by the use of}} non uniform MSS values. 1 Introduction ATM networks must be able to support connectionless traffic. This {{turns out to be a}} challenge, since ATM requires that <b>sources</b> provide traffic <b>descriptors</b> at call set up time for bandwidth negotiation and assignment. However, data sources cannot predict their traffic behavior. The ATM network will thus carry connectionle [...] ...|$|R
40|$|In {{support of}} {{applications}} involving multiview sources in distributed object recognition using lightweight cameras, we propose a new method for the distributed coding of sparse <b>sources</b> as visual <b>descriptor</b> histograms extracted from multiview images. The problem is challenging {{due to the}} computational and energy constraints at each camera {{as well as the}} limitations regarding inter-camera communication. Our approach addresses these challenges by exploiting the sparsity of the visual descriptor histograms as well as their intra- and inter-camera correlations. Our method couples distributed source coding of the sparse sources with a new joint recovery algorithm that incorporates multiple side information signals, where prior knowledge (low quality) of all the sparse sources is initially sent to exploit their correlations. Experimental evaluation using the histograms of shift-invariant feature transform (SIFT) descriptors extracted from multiview images shows that our method leads to bit-rate saving of up to 43 % compared to the state-of-the-art distributed compressed sensing method with independent encoding of the sources...|$|R
30|$|To sum up, {{the present}} study serves two purposes: (1) to {{investigate}} the suitability of CEFR descriptors, among other <b>descriptor</b> <b>sources,</b> in profiling the writing proficiency of TEM test takers, who are university students, and (2) to explore how to construct a writing ability scale in the local context. Our findings show that some CEFR-related descriptors are generally suitable for describing the writing proficiency of the target test population, though their new levels on the constructed scale have differed, in many cases, from their original levels. In scaling the descriptors obtained from the third stage of the study, we adopted one of the methods proposed by North (2000), p. 293 by looking for “natural gaps” or “clusters of data” in the FACETS measurement scale, and then we compared the gaps with the requirements stipulated in the teaching syllabus (2000). On {{the basis of the}} comparison and the results of the following interviews, we decided on two cut-off points that divided the descriptors into three levels. The scaled descriptors generally correspond to what the teaching syllabus has stipulated.|$|R
40|$|Traditional {{collaborative}} filtering generates {{recommendations for the}} active user based solely on ratings of items by other users. However, most businesses today have item ontologies that provide a useful <b>source</b> of content <b>descriptors</b> {{that can be used}} to enhance the quality of recommendations generated. In this article, we present a novel approach to integrating user rating vectors with an item ontology to generate recommendations. The approach is novel in measuring similarity between users in that it first derives factors, referred to as impacts, driving the observed user behavior and then uses these factors within the similarity computation. In doing so, a more comprehensive user model is learned that is sensitive to the context of the user visit. An evaluation of our recommendation algorithm was carried out using data from an online retailer of movies with over 94, 000 movies, 44, 000 actors, and 10, 000 directors within the item knowledge base. The evaluation showed a statistically significant improvement in the prediction accuracy over traditional {{collaborative filtering}}. Additionally, the algorithm was shown to generate recommendations for visitors that belong to sparse sections of the user space, areas where traditional collaborative filtering would generally fail to generate accurate recommendations...|$|R
40|$|Objective To {{establish}} {{the range of}} medicine information sources used by consumers and {{their perception of the}} reliability of these, using the repertory grid technique. Method Consumers visiting three community pharmacies in Brisbane, Australia, were interviewed using the repertory grid technique. During the interview, consumers were asked to name up to three medicine information sources that they used for a supermarket medicine, an over-the-counter medicine and a prescription medicine. They were then presented with their named information sources in groups of three and asked to discriminate between these in terms of their perceived reliability of the information <b>source.</b> The <b>descriptors</b> used by the consumer to discriminate between the information sources are known as constructs and these were recorded. The consumer was then asked to rate each of their information sources against each generated construct. Main outcome measure The range of information sources generated was determined along with the perceived reliability of these from the calculated median score of each information source when rated on each generated construct. Results A total of 110 consumers were interviewed and identified 648 information sources that they would use. The most frequent information sources cited by the 110 consumers were their doctor (83 %), written information (90 %) and the pharmacist (78 %). There were a total of 299 constructs generated by 88 of the consumers and these were themed into 16 discrete categories. The most common generated constructs themes were “good knowledge” (15 %), “training” (14 %) and “trustworthiness” (13 %). The consumer perception of their information sources were that the doctor and pharmacist have good knowledge (median score 1) and are trained (median score 1) and were perceived to be trustworthy (median score 3 and 2, respectively). Conclusion The repertory grid technique was successful in identifying the information sources consumers accessed to find out about their medicines and in identifying the perception of these sources in terms of their reliability. The repertory grid technique offers a novel method for future research into consumer preferences for different treatment options...|$|R
40|$|A {{series of}} {{structural}} {{features of the}} three main humic fractions (humic acid, fulvic acid and humin) from different depths of a peat bog deposit in Mazago´n (Huelva, Southern Spain) were isolated and analysed by flash pyrolysis–gas chromatography–mass spectrometry, solid-state 13 C-nuclear magnetic resonance and Fourier transformed infrared spectroscopy. Such techniques demonstrate that the various humic fractions were very different {{not only in terms}} of molecular weight but also in their composition and structural characteristics, showing well differentiated patterns in the relative distribution of alkyl, O-alkyl and aromatic moieties in each humic fraction. To a large extent, selectively preserved lignin accumulates in the humic acid (HA) fraction, whereas the fulvic acid (FA) consists of a colloidal carbohydrate with a substantial peptidic moiety and the humin includes the noteworthy concentration of insoluble, macromolecular polyalkyl structures. A diagnostic lignin signature in the resolution-enhanced infrared spectra of the HA points to processes of selective preservation of macromolecular substances derived from vascular plants. In general, the humic fractions were not extensive <b>sources</b> of chemotaxonomic <b>descriptors</b> of previously documented biodiversity changes along the period of the peat deposit formation. This is in harmony with a substantial homogenising effect of diagenetic transformations throughout the whole sedimentary record with peat stratigraphy recording mostly the authigenic processes of bog formation and development. Anthropogenic perturbations may be also responsible for the loss of information in the humic proxy resulting in the lack of an apparent relationship between any particular input and the composition of the humic fractions from the peat bog. Peer reviewe...|$|R
40|$|There is {{currently}} {{a great amount of}} interest in the use of salts in the pharmaceutical industry because the physiochemical properties of the solid forms can be modified without altering the biochemical properties of the drug. Much effort has been expended in screening to select the best salt form and a number of empirical rules have been proposed such as the ‘rule of three’, which states that a successful salt formation generally requires a difference of three pKa units between the conjugate acid and the conjugate base. However, this rule does not always hold and the reasons are often unclear. The idea of this project is to perform a detailed systematic study of organic salt formation through a series of designed experiments, in order to obtain a broader and better understanding of the chemical descriptors, or factors that might be involved. A set of descriptors that describe molecular properties relevant to salt formation have been identified. For the initial experiments, a collection of salt forming acids has been assembled from the Cambridge Structural Database [1], and other <b>sources,</b> and their <b>descriptor</b> values calculated. These acids define a chemical space from which the compounds for the experiments can be chosen. The experiments aim to explore this chemical space whilst building statistical models that will allow an understanding of how the descriptors affect salt formation. Findings from the chemical space investigation coupled with experimental results from this study will be presented. This work is part of the Combechem E-science project at the University of Southampton. [1]	F. H. Allen and O. Kennard (1993). Chem. Des. Autom. News, 8, 31 - 37...|$|R

