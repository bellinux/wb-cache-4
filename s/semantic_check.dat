6|136|Public
40|$|The Expert Systems Validation Associate (EVA), a {{validation}} {{system under}} {{development at the}} Lockheed Artificial Intelligence Center {{for more than a}} year, provides a wide range of validation tools to check the correctness, consistency and completeness of a knowledge-based system. A declarative meta-language (higher-order language), is used to create a generic version of EVA to validate applications written in arbitrary expert system shells. The architecture and functionality of EVA are presented. The functionality includes Structure Check, Logic Check, Extended Structure Check (using semantic information), Extended Logic Check, <b>Semantic</b> <b>Check,</b> Omission Check, Rule Refinement, Control Check, Test Case Generation, Error Localization, and Behavior Verification...|$|E
40|$|Abstract. In {{this paper}} we {{introduce}} a novel framework to compute jointly syntactic parses and semantic representations of a written sentence. To achieve this goal, we couple a syntactico-semantic grammar and a knowledge base. The knowledge base is implemented in Description Logics, in a polynomial variant. The grammar is a Range Concatenation Grammar, which combines expressive power and polynomial parsing time, and allows external predicate calls. These external calls {{are sent to}} the knowledge base, which is able either to answer these calls or to learn new information, this process taking place during parsing. Thus, only semantically acceptable parses are built, avoiding the costly a posteriori <b>semantic</b> <b>check</b> of all syntactically correct parses. 1 Introduction an...|$|E
40|$|We {{present an}} {{approach}} to database interoperation that exploits the semantic information provided by integrity constraints defined on the component databases. We identify two roles of integrity constraints in database interoperation. First, a set of integrity constraints describing valid states of the integrated view {{can be derived from}} the constraints defined on the underlying databases. Moreover, local integrity constraints {{can be used as a}} <b>semantic</b> <b>check</b> on the validity of the specification of the integrated view. We illustrate our ideas in the context of an instance-based database interoperation paradigm, where objects rather than classes are the unit of integration. We introduce the notions of objectivity and subjectivity as an indication of whether a constraint is valid beyond the context of a specific database, and demonstrate the impact of these notions. 1 Introduction Although the interoperation of autonomous legacy databases has been an important research subject for the la [...] ...|$|E
5000|$|Metadata {{storage in}} a {{relational}} database management system, significantly reducing {{the time to}} perform <b>semantic</b> <b>checks</b> during query execution.|$|R
40|$|Abstract:- UPnP {{devices are}} now widely {{available}} in the SOHO market, making device conformance and reliability important. UPnP conformance is supported by automated test suites from the UPnP Implementor's Corporation (UIC). While the UIC test suite is helpful, it covers normal case tests only and performs no <b>semantic</b> <b>checks</b> on the device. We present the UPnP Device Tester (UDT), a test framework for UPnP devices. Both normal and exceptional tests are supported and <b>semantic</b> <b>checks</b> are performed. UDT {{has been used to}} test two commercial Internet Gateway Devices, revealing serious errors...|$|R
40|$|Abstract. The {{collective}} communication {{operations of}} MPI, {{and in general}} MPI operations with non-local semantics, require the processes participating in the calls to provide consistent parameters, eg. a unique root process, matching type signatures and amounts for data to be exchanged, or same operator. Exhaustive consistency checks are typically too expensive to perform under normal use of MPI and would compromise optimizations for high performance in the collective routines, but confusing and hard-to-find errors (deadlocks, wrong results, or program crash) can happen by inconsistent calls to collective operations. We suggest to use the MPI profiling interface to provide for more extensive <b>semantic</b> <b>checking</b> of calls to MPI routines with collective (non-local) semantics. With this, exhaustive <b>semantic</b> <b>checks</b> can be enabled during application development, and disabled for production runs. We discuss what can reasonably be checked by such an interface, and mention some inherent limitations of MPI to making a fully portable interface for <b>semantic</b> <b>checking.</b> The proposed collective semantics verification interface for the full MPI- 2 standard has been implemented for the NEC proprietary MPI/SX and MPI/EX implementations. ...|$|R
40|$|Part 1 : Full PapersInternational audienceInteroperability, {{as one of}} the key {{competition}} {{factors for}} modern enterprises, describes the ability to establish partnership activities in an environment of unstable market. In some terms, interoperability determines the future of enterprises; so, improving enterprises’ interoperability turns to be a research focus. “Sharing data among heterogeneous partners” {{is one of the most}} basic common interoperability problems, which requires a general methodology to serve. Model transformation, which plays a key role in model-driven engineering, provides a possible solution to data sharing problem. A general model transformation methodology, which could shield traditional model transformation practices’ weaknesses: low reusability, contains repetitive tasks, involves huge manual effort, etc., is an ideal solution to data sharing problem. This paper presents a general model transformation methodology “combining <b>semantic</b> <b>check</b> measurement and syntactic check measurement into refined model transformation processes” and the mechanism of using it to serve interoperability’s data sharing issue...|$|E
40|$|Abstract. Static <b>semantic</b> <b>check</b> {{remains an}} active {{research}} {{topic in the}} construction of compiler front-ends. The main reasons lie into the ever-increasing set of semantic properties that have to be checked in modern languages and the diverse requirements in the timing of checks during the compilation process. Challenging single-pass compilers, impose the development of appropriate parse-driven attribute evaluation mechanisms. LR parsing is amenable to L-attributed definitions, where, in every syntax tree, the attributes may be evaluated in only one left to right depth-first-order traversal. In such definitions, for every production of the grammar, the inherited attributes on the right side depend only on attributes to the left of themselves. When the LR-parser begins to analyze a word for a non-terminal, the values of its inherited attributes must be made available. This outstanding difficulty constitutes the major concern of the existing attribute evaluation mechanisms, found in the literature. We review them and then we introduce the evaluation mechanism, which we successfully utilized in YAPL, a complete programming language build exclusively for educational purposes. Our approach takes advantage of an effective attributecollection management scheme, between a parse-driven symbol node stack and the symbol table, for delivering synthesized and inherited attribute values. Finally, we demonstrate the applicability of the suggested approach to representative problems of enforcing language rules for declaration and scope related properties and in type checking of assignments. ...|$|E
40|$|OUN (Oslo University Notation) is {{developed}} at the University of Oslo. It is a formal notation intended to be easier to understand and use. It is adjusted to support the development of distributed systems. In the rst part of this thesis, we have found the principles of translation from OUN to Java. In OUN all objects are active, they execute their own methods, even if other objects calls them. To solve that, all objects in the generated Java code runs in its own thread. These objects have a queue of calls, where calls are taken and methods are executed. Return values are put back in a queue of return values in the caller's object. When the queue of calls is empty, the object will fall asleep. We save processing time, when not having to poll on the queue. When an object wants a method in another object executed, it puts a call into the queue of calls, and wakes the object. If the method has return values, the caller {{will have to wait}} for them. It falls asleep, and is wakened by the object running the method after the return values is placed in the queue of return values. We have considered several ways of implementing this. Even though OUN and Java are quite similar, they have important differences. It has not been possible to find a perfect solution. We have prioritized to choose solutions similar to the principles of OUN. In {{the second part of the}} thesis, we have built a compiler for the translation. We have continued to build on the syntax checker already built in JavaCC. That is a tool for automatic generation of syntax checkers. We ran the grammar file through JJTree to build up a syntax tree. It generates a class for every non-terminal in the language. The nodes are linked together to form a tree. In every node we added a method to translate the code. After the syntax is checked, we now can translate the OUN code by calling the added method in the root of the tree. Code is generated by traversing down the tree from the root. <b>Semantic</b> <b>check</b> of OUN is not yet added, and we have no symbol table implemented. In this version of the compiler, we therefore had to put some limitations on the functionality of OUN...|$|E
5000|$|SpecBox: from Adelard {{provides}} syntax <b>checking,</b> {{some simple}} <b>semantic</b> <b>checking,</b> and generation of a LaTeX file enabling specifications to be printed in mathematical notation. This tool is freely available {{but it is}} not being further maintained.|$|R
40|$|The {{financial}} reporting world has recently faced {{a number of}} changes due {{to the impact of}} the Internet. Today, the revolution in business communication is accelerating and more data is being shared by a large number of participant users, aside from the company’s internal management, including: clients, business partners, financial market analysts, investors and government regulators. These changes have {{led to the development of}} eXtensible Business Reporting Language (XBRL), which is an opensource Internet-based {{financial reporting}} language. XBRL is an extension of eXtensible Markup Language (XML) that provides machinereadable tags for each individual data element in each financial statement. XBRL is likely to be used as a platform that offers universal standards for defining business information. XBRL can ease the preparation, analysis, and exchange of business information along each part of financial reporting supply chain and across companies around the world. It can also increase the efficiency for all related users of business data. This study has analysed the accuracy of XBRL outputs by conducting a literature review and by checking the accuracy of the real company XBRL filing submissions that are published publicly. This study has found that there were many errors in these public XBRL documents that were caused either through a few basic common errors or from mistakes in related financial information. Therefore, this study has aimed to discover any possible causes of errors in XBRL filings. It has also aimed to find a way to detect those errors. Consequently, this study conducted a <b>semantic</b> <b>checking</b> system that aimed to detect XBRL errors and so enhance the accuracy of financial statements. To develop the <b>semantic</b> <b>checking</b> system, the results of an error finding analysis were combined, filtered, and classified into each category of errors, including the integration of accounting, business, and technology knowledge to fulfil the system. A process flow for the <b>semantic</b> <b>checking</b> system was created to help understand both the method and the rule. The rules were then set up to determine the different aspect of errors, which had a different method to manage and reduce errors. The <b>semantic</b> <b>checking</b> system was created in terms of the information specification of the XBRL filings. The system was designed to be more practical for the users by presenting the relationship between the real data and accounting practice. Moreover, a prototype was produced and the case study method was applied to prove the development of the system. This step was able to ensure the accuracy of the <b>semantic</b> <b>checking</b> system. Finally, this <b>semantic</b> <b>checking</b> system has been shown to improve the accuracy of XBRL filings. It also emphasises the importance of employing XBRL preparers who are aware of all of the possible issues that may arise while preparing an XBRL-based filing submission. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|Coco/R 1 is a {{compiler}} generator, {{which takes}} an attributed grammar of a source language and generates a scanner and a parser for this language. The scanner {{works as a}} deterministic finite automaton. The parser uses recursive descent. LL(1) conflicts can be resolved by a multi-symbol lookahead or by <b>semantic</b> <b>checks.</b> Thus the class of accepted grammars is LL(k) for an arbitrary k...|$|R
40|$|The article {{discusses}} various {{rules about}} use of distributed arrays in HPJava programs. These rules are peculiar to the HPspmd programming model. They can be enforced {{by a combination}} of static <b>semantic</b> <b>checks,</b> compile-time analysis and compiler-generated run-time checks. We argue that the cost of any necessary run-time checks should be acceptable, because, by design, the associated computations can be lifted out of inner loops...|$|R
40|$|In Thai language, {{the word}} {{boundary}} is not explicitly clear, therefore, word segmentation {{is needed to}} determine word boundary in Thai sentences. Many applications of Thai Language Processing require the word segmentation. Several approaches of Thai word segmentation such as maximal matching, longest matching and n-gram model do not take semantics into consideration. This paper presents a Thai word segmentation system using semantic corpus which is composed of four steps: generating all possible candidates, proper noun consideration, semantic tagging and <b>semantic</b> <b>checking.</b> The first three steps are conducted using a dictionary. <b>Semantic</b> <b>checking</b> is carried out on the basis of corpus-based approach. Finally, we assign the semantic scores to segmented words and select the ones that contain maximum semantic scores. In order to assign semantic scores, we use a Thai proper noun database and the semantic corpus derived from ORCHID corpus. This approach is more reliable than other approaches that do not take the meaning into consideration and performs the level of accuracy at 96 - 99 % depending on the characteristic of input and the dictionary used in the segmentation. ...|$|R
40|$|Java Layers {{extends the}} Java {{programming}} language by implementing a software component model based on layer composition. Each layer implements a single design feature and may contain code that crosscuts multiple classes. Layer composition enables large software applications to be constructed {{in a more}} modular way, and with {{a higher level of}} <b>semantic</b> <b>checking,</b> than is typically achieved using current programming techniques such as object-oriented frameworks. This paper describes the Java Layers language extension. ...|$|R
50|$|In some approaches, {{synthesized}} attributes {{are used}} to pass semantic information up the parse tree, while inherited attributes help pass semantic information down and across it. For instance, when constructing a language translation tool, such as a compiler, it {{may be used to}} assign semantic values to syntax constructions. Also, it is possible to validate <b>semantic</b> <b>checks</b> associated with a grammar, representing the rules of a language not explicitly imparted by the syntax definition.|$|R
50|$|The {{fundamental}} {{problem is that}} HLLCAs only simplify the code generation step of compilers, which is typically a relatively small part of compilation, and a questionable use of computing power (transistors and microcode). At the minimum tokenization is required, and typically syntactic analysis and basic <b>semantic</b> <b>checks</b> (unbound variables) will still be performed - {{so there is no}} benefit to the front end - and optimization requires ahead-of-time analysis - so there is no benefit to the middle end.|$|R
40|$|SOL is {{computer}} language geared to solution of design problems. Includes mathematical modeling and logical capabilities of {{computer language}} like FORTRAN; also includes additional power of nonlinear mathematical programming methods at language level. SOL compiler takes SOL-language statements and generates equivalent FORTRAN code and system calls. Provides syntactic and <b>semantic</b> <b>checking</b> for recovery from errors and provides detailed reports containing cross-references to show where each variable used. Implemented on VAX/VMS computer systems. Requires VAX FORTRAN compiler to produce executable program...|$|R
40|$|A {{case study}} in formal {{verification}} of concurrent/distributed software is presented. The study concerns the modular specification and verification of a remote task protocol. The verification methodology used is based on <b>semantic</b> equivalence <b>checking</b> and is applicable to systems with hierarchical architectures. To support the methodology, we extended the verification tool Spin {{with the ability to}} check a particular class of semantic relations, and the language Promela upon which Spin is based with a simple mechanism to specify external operations. The foundations of <b>semantic</b> equivalence <b>checking</b> are also discussed briefly...|$|R
40|$|Abstract. Implementing web {{services}} that participate in long-running, multi-lateral conversations {{is difficult because}} mainstream programming languages are poor {{when it comes to}} manipulating XML data and handling concurrent and interrelated interactions. We have designed a programming language to deliberately address these problems. In this paper we describe how to use this language to consume a popular web service, and discuss the compiler and runtime system. We demonstrate the compiler, including the kinds of <b>semantic</b> <b>checks</b> it performs, the running program, and the SOAP messages produced at runtime. The compiler and sample program are available a...|$|R
40|$|This paper {{reviews the}} design issues {{that arise in}} the {{construction}} of effective language-based editors for the preparation of syntactically and static semantically correct language sentences, typically computer programs. The need for such editors to support a pluralistic view of program structure is identified, together with the need to observe the constraints on performance and storage consumption if such editors are to be accepted by professional programmers. From these basic needs, more specific requirements for the display, parsing and <b>semantic</b> <b>checking</b> components of such an editor are derive...|$|R
5000|$|The parser uses {{recursive}} descent; LL(1) conflicts can {{be resolved}} by either a multi-symbol lookahead or by <b>semantic</b> <b>checks.</b> Thus the class of accepted grammars is LL(k) for an arbitrary k. Fuzzy parsing is supported by so-called ANY symbols that match complementary sets of tokens. Semantic actions are written in the same language as the generated scanner and parser. The parser's error handling can be tuned by specifying synchronization points and [...] "weak symbols" [...] in the grammar. Coco/R checks the grammar for completeness, consistency, non-redundancy {{as well as for}} LL(1) conflicts.|$|R
5000|$|Luca Cardelli and Martín Abadi {{wrote the}} book A Theory of Objects in 1997 laying out formal calculi for the {{semantics}} of object-oriented programming languages. Baby Modula-3 influenced this work according to Luca Cardelli, and guided a calculus {{of the type of}} self in Types for object and the type of self.It has open the way for work on Modula-3 formal <b>semantic</b> <b>checking</b> systems, for object oriented type system programming languages that have been used to model the formal semantics of programming languages such as Ada (programming language) and C (programming language) [...]|$|R
40|$|To {{solve the}} close {{relationship}} between the data query and interface design of management information system, the paper implements a SQL parser based on ANTLR. The SQL parser is composed of lexical analysis, syntax analysis and <b>semantic</b> <b>checking.</b> The SQL parser is successfully integrated with management information system and the query is implemented by the SQL parser to search the data from database. It solves the problem {{that the relationship between}} the data query and interface design of management information system is very close, and improves the reliability and reusability of the software. © 2010 IEEE...|$|R
40|$|As {{more and}} more {{architectural}} design and construction data is represented using the Resource Description Framework (RDF) data model, {{it makes sense to}} take advantage of the logical basis of RDF and implement a <b>semantic</b> rule <b>checking</b> process as it is currently not available in the architectural design and construction industry. The argument for such a <b>semantic</b> rule <b>checking</b> process has been made a number of times by now. However, there are a number of strategies and approaches that can be followed regarding the realization of such a rule checking process, even when limiting to the use of semantic web technologies. In this article, we compare three reference rule checking approaches that have been reported earlier for <b>semantic</b> rule <b>checking</b> in the domain of architecture, engineering and construction (AEC). Each of these approaches has its advantages and disadvantages. A criterion that is tremendously important to allow adoption and uptake of such <b>semantic</b> rule <b>checking</b> approaches, is performance. Hence, this article provides an overview of our collaborative test results in order to obtain a performance benchmark for these approaches. In addition to the benchmark, a documentation of the actual rule checking approaches is discussed. Furthermore, we give an indication of the main features and decisions that impact performance for each of these three approaches, so that system developers in the construction industry can make an informed choice when deciding for one of the documented rule checking approaches...|$|R
40|$|Nowadays, it is {{increasing}} {{the interest of}} working with systems designed under the object-based paradigm. However, most organizations possess their data stored in relational or pre-relational databases, data that they need for the every day work. In order to permit them to continue dealing with the existing databases while {{taking advantage of the}} features provided by the new systems, different tools are being designed. In this paper we present an interface that allows generating mapping information between relational databases and schemata defined using systems designed under the objectbased paradigm. This interface provides a user friendly environment and syntactic and <b>semantic</b> <b>checking</b> to help the user in the connexion process...|$|R
40|$|Although OCR {{technology}} is now commonplace, character recognition errors {{are still a}} problem, in particular, in automated systems for information extraction from printed documents. This paper proposes a method for the automatic detection and correction of OCR errors in an information extraction system. Our algorithm uses domain-knowledge about possible misrecognition of characters to propose corrections; then it exploits knowledge about the type of the extracted information to perform syntactic and <b>semantic</b> <b>checks</b> in order to validate the proposed corrections. We assess our proposal on a real-world, highly challenging dataset composed of nearly 800 values extracted from approximately 100 commercial invoices and we obtained very good results...|$|R
40|$|Business Process Model and Notation (BPMN) 2. 0 process {{models are}} used more and more, both in {{practice}} as in academia. Although academic research mainly focuses on sophisticated <b>semantic</b> <b>checks</b> and extensions there still exist {{problems in the}} basic usage of BPMN. This paper investigates issues in BPMN model serializations which arise {{as a result of}} the complexity and inconsistency of the standard document. We present a set of serialization constraints as a starting point for sophisticated compliance checks on serialized BPMN models. Furthermore, these constraints are used to perform an evaluation of current modeling tools. This evaluation reveals that the creation of standard compliant models is still a non-trivial endeavor...|$|R
40|$|Fitnesse and FIT [5] allow systems {{tests to}} be written by non-programmers using a Wiki or HTML style of input. However, there is little support for {{syntactic}} and <b>semantic</b> <b>checks</b> as the tests are being designed. This paper describes a support tool for designing table-based test cases that gives deep semantic analysis about a set of test cases. It uses a variety of strategies such as pairwise analysis, boundary value analysis and test case subsumption to suggest missing test cases and to generalise concrete tests into more abstract tests. The goal is to interactively {{improve the quality of}} test suites during the test design phase...|$|R
40|$|This short paper gives a {{model for}} and a proof of {{completeness}} of the NRB verification logic for deterministic imperative programs, the logic having {{been used in the}} past as the basis for automated <b>semantic</b> <b>checks</b> of large, fast-changing, open source C code archives, such as that of the Linux kernel source. The model is a colored state transitions model that approximates from above the set of transitions possible for a program. Correspondingly, the logic catches all traces that may trigger a particular defect at a given point in the program, but may also flag false positives. Comment: To appear in OpenCert 2013 Workshop, Sept 23, Madrid, 15...|$|R
50|$|Passive data {{structures}} are appropriate {{when there is}} a part of a system where it should be clearly indicated that the detailed logic for data manipulation and integrity are elsewhere. PDSs are often found at the boundaries of a system, where information is being moved to and from other systems or persistent storage and the problem domain logic that is found {{in other parts of the}} system is irrelevant. For example, PDS would be convenient for representing the field values of objects that are being constructed from external data, in a part of the system where the <b>semantic</b> <b>checks</b> and interpretations needed for valid objects are not applied yet.|$|R
5000|$|Semantic {{analysis}} is the phase {{in which the}} compiler adds semantic information to the parse tree and builds the symbol table. This phase performs <b>semantic</b> <b>checks</b> such as type checking (checking for type errors), or object binding (associating variable and function references with their definitions), or definite assignment (requiring all local variables to be initialized before use), rejecting incorrect programs or issuing warnings. Semantic analysis usually requires a complete parse tree, meaning that this phase logically follows the parsing phase, and logically precedes the code generation phase, though it is often possible to fold multiple phases into one pass over the code in a compiler implementation.|$|R
40|$|Abstract: Business Process Model and Notation (BPMN) 2. 0 process {{models are}} used more and more, both in {{practice}} as in academia. Although academic research mainly focuses on sophisticated <b>semantic</b> <b>checks</b> and extensions there still exist {{problems in the}} basic usage of BPMN. This paper investigates issues in BPMN model serializations which arise {{as a result of}} the complexity and inconsistency of the standard document. We present a set of serialization constraints as a starting point for sophisticated compliance checks on serialized BPMN models. Furthermore, these constraints are used to perform an evaluation of current modeling tools. This evaluation reveals that the creation of standard compliant models is still a non-trivial endeavor. ...|$|R
40|$|This article {{illustrates}} Conceptual Graph networks {{representing the}} content of courses to help students understand, relate, compare, memorize and retrieve many of their concepts. It shows that the ontology of WebKB- 2 and its FL notation could be exploited by lecturers to create normalized representations in a scalable way and relatively quick way. They also permit the students to complement these representations, thus providing lecturers with ways to test the students' understanding and analytical skills. Very strong mechanisms supporting <b>semantic</b> <b>checking,</b> cooperation support and normalization need to be implemented for the approach to be successful. Current semantic wikis and knowledge servers (WebKB- 2 included) are far from fulfilling such constraints. Griffith Sciences, School of Information and Communication TechnologyFull Tex...|$|R
40|$|Formal {{specification}} and verification {{techniques can}} {{improve the quality of}} object-oriented software by enabling <b>semantic</b> <b>checks</b> and certification of properties. To be applicable to object-oriented programs, they have to cope with subtyping, aliasing via object references, as well as abstract and recursive methods. For mastering the resulting complexity, mechanical aid is needed. The article outlines the specific technical requirements for the specification and verification of object-oriented programs. Based on these requirements, it argues that verification of OO-programs should be done interactively and develops an modular architecture for this task. In particular, it shows how to integrate interactive program verification with existing universal interactive theorem provers, and explains the new developed parts of the architecture. To underline the general approach, we describe interesting features of our prototype implementation...|$|R
40|$|This thesis {{describes}} {{a set of}} tools and abstractions that facilitate {{the development of an}} Integrated Development Environment (IDE) that runs in the browser. We present a plugin for ACE, the Ajax. org Cloud 9 Editor, that adds support for languages defined with SDF. The plugin supports vital editor support such as syntax highlighting and validation. Language syntax can be declaritively defined with no concern for the underlying implementation. This approach is realized by compiling a Java library to JavaScript using the Google Web Toolkit. Besides syntactic support, <b>semantic</b> <b>checks</b> are supported by means of the Stratego language. A compiler backend for the Stratego compiler is developed targetting the JavaScript language. Software Engineering Research GroupSoftware TechnologyElectrical Engineering, Mathematics and Computer Scienc...|$|R
