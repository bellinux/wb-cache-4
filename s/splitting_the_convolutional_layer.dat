0|10000|Public
30|$|In <b>the</b> first <b>convolutional</b> <b>layer,</b> <b>the</b> {{input image}} patch is {{continuously}} filtered with 48 feature maps of 3 × 11 × 11 and 3 × 7 × 7 kernels with a stride of 2. <b>The</b> second <b>convolutional</b> <b>layer</b> continuously filters <b>the</b> output of <b>the</b> first <b>convolutional</b> <b>layer</b> with 128 feature maps of 3 × 9 × 9 and 3 × 5 × 5 kernels. <b>The</b> third <b>convolutional</b> <b>layer</b> filters <b>the</b> output of <b>the</b> second <b>convolutional</b> <b>layer</b> with 128 feature maps of 3 × 7 × 7 and 3 × 3 × 3 kernels, sequentially. The following fully connected layer has 512 neurons.|$|R
3000|$|... e {{shows the}} results for {{different}} numbers of layers. The most accurate result is provided when <b>the</b> <b>convolutional</b> <b>layer</b> Ψ [...]...|$|R
3000|$|... where m∗n is {{the size}} of the {{convolution}} matrix, w and x are the inputs from the previous layer, and y is the output of <b>the</b> <b>convolutional</b> <b>layer.</b>|$|R
40|$|We {{present a}} new deep network <b>layer</b> called “Dynamic <b>Convolutional</b> <b>Layer</b> ” {{which is a}} {{generalization}} of the con-volutional <b>layer.</b> <b>The</b> conventional <b>convolutional</b> <b>layer</b> uses filters that are learned during training and are held constant during testing. In contrast, <b>the</b> dynamic <b>convolutional</b> <b>layer</b> uses filters that will vary from input to input during testing. This is achieved by learning a function that maps the input to the filters. We apply <b>the</b> dynamic <b>convolutional</b> <b>layer</b> to <b>the</b> application of short range weather prediction and show performance improvements compared to other baselines. 1...|$|R
30|$|In [7], D. Zeiler {{presented}} a view, <b>the</b> <b>convolutional</b> <b>layers</b> in <b>the</b> CNN played the analogous role of traditional feature descriptors that extracted feature map from images. And <b>the</b> <b>convolutional</b> <b>layers</b> are changeable compared with traditional feature descriptors. We {{just need to}} train the CNN with different image datasets. When we want to obtain a special feature map to distinguish similar categories, we only need to train the CNN with a small image dataset that constituted by the similar categories. This characteristic of the CNN makes it possible that we obtain special feature map of all similar categories.|$|R
30|$|Actually, it is {{interesting}} to investigate the proper way to combine <b>the</b> <b>convolutional</b> <b>layers.</b> In <b>the</b> experimental section, we will show how the change on its combination, by summing and concatenating them, will affect the entire network results.|$|R
30|$|Convolutional {{neural network}} (CNN) {{is a typical}} deep {{learning}} model which consists of <b>the</b> input <b>layer,</b> <b>the</b> <b>convolutional</b> <b>layers,</b> <b>the</b> pooling layers, the fully connected layers, and <b>the</b> output <b>layer.</b> <b>Convolutional</b> <b>layers</b> are used for extracting multi-level features of data according to convolution kernels of different sizes. Pooling layers aim at reducing the dimensions of feature representation and making the feature invariant from the location through a pooling function. Fully connected layers combine the outputs of all the previous layers into high-level features. Generally, CNN can automatically learn high-level features and has proved its powerful classification capability.|$|R
30|$|<b>The</b> <b>convolutional</b> <b>layer</b> aims {{to learn}} the {{parameters}} of filters that activate on some type of visual feature of the inputs. It comprises a bank of filters that perform a 2 D filtering (convolution) on the input image data and produce a 2 D feature map.|$|R
30|$|Our {{contributions}} are twofold. First, we introduce a coalesced style of <b>the</b> <b>convolutional</b> <b>layers</b> with <b>the</b> residual-flavored network {{to build an}} efficient model for the semantic road segmentation. Subsequently, we exhibit an asymmetric encoder-decoder network for reducing the model size even more, unlike the conventional symmetric approach used by the previous methods, e.g., SegNet [8].|$|R
40|$|Deep {{convolutional}} {{neural networks}} are {{generally regarded as}} robust function approximators. So far, this intuition is based on perturbations to external stimuli such as the images to be classified. Here we explore <b>the</b> robustness of <b>convolutional</b> neural networks to perturbations to the internal weights and architecture of the network itself. We show that convolutional networks are surprisingly robust {{to a number of}} internal perturbations in <b>the</b> higher <b>convolutional</b> <b>layers</b> but <b>the</b> bottom <b>convolutional</b> <b>layers</b> are much more fragile. For instance, Alexnet shows less than a 30 % decrease in classification performance when randomly removing over 70 % of weight connections in <b>the</b> top <b>convolutional</b> or dense <b>layers</b> but performance is almost at chance with the same perturbation in <b>the</b> first <b>convolutional</b> <b>layer.</b> Finally, we suggest further investigations which could continue to inform <b>the</b> robustness of <b>convolutional</b> networks to internal perturbations. Comment: under review at ICML 201...|$|R
3000|$|... is m×n {{for each}} face image, and PCANet {{is assumed to}} have a filter size of k 1 ×k 2 at any phase of <b>the</b> <b>convolutional</b> <b>layer.</b> <b>The</b> ICA {{algorithm}} is used to obtain the ICA filter by unsupervised learning in the training sample. So, the two stages of convolution layer are introduced in the following.|$|R
30|$|We use a multi-scale {{network for}} the {{deblurring}} problem {{as shown in}} Fig.  2. This network has two scales which share weight between each level. The basic model for each scale begins with three convolutional layers; we add six ResBlocks and three <b>convolutional</b> <b>layers</b> after <b>the</b> beginning unit. Especially, except <b>the</b> first <b>convolutional</b> <b>layer</b> sets <b>the</b> kernel size to 11 × 11 to increase the receptive field, all <b>the</b> <b>convolutional</b> <b>layers</b> have <b>the</b> kernel size of 5 × 5 with 64 channels. The input of each scale contains six channels, that is to say, we combine the blurred image and deblurred result of lower scale as the input of the upper scale. Here, note that we copy the blurred image (i.e., six channels) as the input of the first scale. In this multi-scale model, we utilize a deconvolutional layer to achieve upsampling. For this multi-task framework, except <b>the</b> last three <b>convolutional</b> <b>layers</b> which utilized to perform reconstruction, the basic deblurring network shares the weight between two tasks (i.e., structure deblurring sub-network and image deblurring sub-network).|$|R
30|$|<b>Convolutional</b> <b>layer</b> 1. There are 48 kernels (size 17 × 17, stride of 2) in <b>the</b> first <b>convolutional</b> <b>layer,</b> {{which is}} {{combined}} with one maxout operator and one max pooling layer.|$|R
30|$|Speeding up the {{computation}} in <b>the</b> <b>convolutional</b> <b>layer</b> is {{a common}} method to accelerate the deep neural network [30 – 33, 74]. For example, Lebedev et al. propose a two-step framework to speed up convolution layers based on tensor decomposition and discriminative fine-tuning [32]. The tensor decomposition uses non-linear squares to compute a low-rank CP-decomposition to decompose the full kernel tensor. Then <b>the</b> original <b>convolutional</b> <b>layers</b> are replaced by four <b>convolutional</b> <b>layers</b> with small kernels. After that, the new network will be fine-tuned on the training dataset again. The evaluations show that the new network achieves a 8.5 × CPU speedup of the whole network with only very little accurate drop. Moreover, Zhang et al. try to accelerate <b>the</b> very deep <b>convolutional</b> networks with <b>the</b> nonlinear asymmetric reconstruction, which achieves a 4 × speedup with merely a 0.3 percent increase of top- 5 center-view error [33].|$|R
30|$|We {{propose a}} novel CNN {{architecture}} with triple input and <b>the</b> doubly <b>convolutional</b> <b>layer</b> which both are {{adapted to the}} characteristics of refrigerator front-view images.|$|R
30|$|Figure  7 {{shows that}} in <b>the</b> first <b>convolutional</b> <b>layer</b> (<b>the</b> first column in Fig.  7), the global {{features}} including shape and edge of the refrigerator are extracted. In <b>the</b> two following <b>convolutional</b> <b>layers</b> (<b>the</b> second and third columns in Fig.  7), local features are extracted hierarchically. Notable that these features, which {{are different from the}} handcrafted features exploited in [5 – 8], are automatically extracted through our proposed CNN.|$|R
30|$|The CNN {{has become}} a {{research}} focus {{in the field of}} image understanding. Its similar weight-sharing network structure with the biological neural networks makes it possible to reduce the complexity of network model as well as the number of weights. This advantage is more obvious when the input of the network is a multi-dimensional image. The image can be directly used as the input of the network, avoiding the complicated feature extraction and data reconstruction processes in the traditional recognition algorithms. <b>The</b> <b>convolutional</b> network is a multi-layer perceptron which specially designs to recognize two-dimensional shapes with its certain invariance to translation, scaling, and other forms of deformation. In a typical CNN structure, the first few layers are usually alternating between <b>the</b> <b>convolutional</b> <b>layer</b> and <b>the</b> downsampling layer, and the last few layers of the network near the output layer are usually fully connected networks. The focus of the training processes of the CNN is to learn the parameters, such as <b>the</b> <b>convolutional</b> kernel parameters of <b>the</b> <b>convolutional</b> <b>layer</b> and <b>the</b> network parameters of the inter-layer connection weight. The prediction process is mainly based on the input image and network parameters to calculate the category label. The keys of the CNN are the network structure (including <b>convolutional</b> <b>layer,</b> downsampling layer, fully connected layer, etc.) and the back propagation algorithms.|$|R
30|$|The {{number of}} filters is an {{important}} factor for the recognition performance in <b>the</b> <b>convolutional</b> <b>layer</b> on ICANet, because there are more binary images encoded and the amount of information increased when the number of filters in the first stage gradually increased. However, when the number is bigger than 8, redundant information will be increased. Therefore, the face recognition system is a slightly fluctuated.|$|R
30|$|FV-CNN {{framework}} extract descriptors from <b>the</b> last <b>convolutional</b> <b>layer</b> of a CNN {{to replace}} hand-crafted features, {{and the rest}} steps are similar to classical BOW model.|$|R
30|$|As we {{have noted}} in the {{previous}} section, it is intriguing to examine different ways of coalescing <b>the</b> <b>convolutional</b> <b>layers.</b> Both summing and concatenated <b>convolutional</b> <b>layers</b> of <b>the</b> RCC-Net surpass the other methods. Nevertheless, the concatenated version of RCC-Net has advantages over the summing one. One interesting result is the pedestrian segmentation of the summing version of the RCC-Net achieves the highest accuracy (70.6 %). This fact {{may lead to a}} promising application in the future research, e.g., to determine the salient regions for the pedestrian detection.|$|R
40|$|In this paper, {{we propose}} a novel {{unsupervised}} deep learning model, called PCA-based <b>Convolutional</b> Network (PCN). <b>The</b> architecture of PCN {{is composed of}} several feature extraction stages and a nonlinear output stage. Particularly, each feature extraction stage includes two layers: a <b>convolutional</b> <b>layer</b> and a feature pooling <b>layer.</b> In <b>the</b> <b>convolutional</b> <b>layer,</b> <b>the</b> filter banks are simply learned by PCA. In the nonlinear output stage, binary hashing is applied. For <b>the</b> higher <b>convolutional</b> <b>layers,</b> <b>the</b> filter banks are learned from the feature maps that were obtained in the previous stage. To test PCN, we conducted extensive experiments on some challenging tasks, including handwritten digits recognition, face recognition and texture classification. The results show that PCN performs competitive with or even better than state-of-the-art deep learning models. More importantly, {{since there is no}} back propagation for supervised finetuning, PCN is much more efficient than existing deep networks. Comment: 8 pages, 5 figure...|$|R
30|$|These steps {{expand the}} input into {{a set of}} simple local features. We denote H_k =Ψ ^C_k(H_k- 1) as the output of a <b>convolutional</b> <b>layer</b> for k= 2, 3,…,|C|+ 1. More details of <b>the</b> <b>convolutional</b> <b>layer</b> can be {{referred}} to [17]. We interpret these convolutional steps as an adaptive pre-processing step. The purpose of these convolutional steps is to extract low-level features, like simple edges and textures. Notice that the sub-sampling layers make the output of convolution networks more robust to local translations and small registrational errors, which is important in facial recognition problem.|$|R
30|$|From the existed works, we {{can find}} that {{speeding}} up <b>the</b> <b>convolutional</b> <b>layer</b> and reducing <b>the</b> weights in full layers are two common methods to accelerate the deep neural network on the mobile devices. Although the networks continue to become deeper, we believe that {{with the development of}} hardware on the mobile devices and improvement of speed-up technologies, the deep learning hashing can be commonly used on the mobile devices.|$|R
40|$|The {{two main}} {{bottlenecks}} using deep neural networks are data dependency and training time. This thesis proposes a novel method for weight initialization of <b>the</b> <b>convolutional</b> <b>layers</b> in a <b>convolutional</b> neural network. This thesis introduces {{the usage of}} sparse dictionaries. A sparse dictionary optimized on domain specific data {{can be seen as}} a set of intelligent feature extracting filters. This thesis investigates the effect of using such filters as kernels in <b>the</b> <b>convolutional</b> <b>layers</b> in <b>the</b> neural network. How do they affect the training time and final performance? The dataset used here is the Cityscapes-dataset which is a library of 25000 labeled road scene images. The sparse dictionary was acquired using the K-SVD method. The filters were added to two different networks whose performance was tested individually. One of the architectures is much deeper than the other. The results have been presented for both networks. The results show that filter initialization is an important aspect which should be taken into consideration while training the deep networks for semantic segmentation...|$|R
30|$|Convolutional neural {{networks}} consist of sparsely connected <b>convolutional</b> <b>layers</b> followed by fully connected dense layers (these dense layers are {{equivalent to a}} multilayer perceptron neural network). <b>Convolutional</b> <b>layers</b> are sparsely connected. Neurons in these layers are connected to a small region of the previous layer known as the receptive field, instead of the entire previous layer as is found in a dense layer. The most common receptor field sizes are small, such as 3 × 3 or 5 × 5 neurons. By being sparsely connected CNNs view and learn local correlations. <b>The</b> <b>convolutional</b> <b>layers</b> in <b>the</b> network are followed by several densely connected layers with the last layer containing one neuron for each possible classification outcome.|$|R
40|$|In this paper, a {{new method}} is {{proposed}} for crowd density estimation. An improved {{convolutional neural network}} is combined with traditional texture feature. The data calculated by <b>the</b> <b>convolutional</b> <b>layer</b> can {{be treated as a}} new kind of features. So more useful information of images can be extracted by different features. In the meantime, the size of image has little effect on <b>the</b> result of <b>convolutional</b> neural network. Experimental results indicate that our scheme has adequate performance to allow for its use in real world applications...|$|R
40|$|Convolutional neural {{networks}} (CNNs) have attracted increasing {{attention in the}} remote sensing community. Most CNNs only take the last fully-connected layers as features for the classification of remotely sensed images, discarding <b>the</b> other <b>convolutional</b> <b>layer</b> features which may also be helpful for classification purposes. In this paper, we propose a new adaptive deep pyramid matching (ADPM) model that {{takes advantage of the}} features from all of <b>the</b> <b>convolutional</b> <b>layers</b> for remote sensing image classification. To this end, the optimal fusing weights for different <b>convolutional</b> <b>layers</b> are learned from the data itself. In remotely sensed scenes, the objects of interest exhibit different scales in distinct scenes, and even a single scene may contain objects with different sizes. To address this issue, we select the CNN with spatial pyramid pooling (SPP-net) as the basic deep network, and further construct a multi-scale ADPM model to learn complementary information from multi-scale images. Our experiments have been conducted using two widely used remote sensing image databases, and the results show that the proposed method significantly improves the performance when compared to other state-of-the-art methods...|$|R
40|$|Relatively small {{data sets}} {{available}} for expression recognition research make {{the training of}} deep networks for expression recognition very challenging. Although fine-tuning can partially alleviate the issue, the performance is still below acceptable levels as the deep features probably contain redun- dant information from the pre-trained domain. In this paper, we present FaceNet 2 ExpNet, a novel idea to train an expression recognition network based on static images. We first propose a new distribution function to model the high-level neurons of the expression network. Based on this, a two-stage training algorithm is carefully designed. In the pre-training stage, we train <b>the</b> <b>convolutional</b> <b>layers</b> of <b>the</b> expression net, regularized by the face net; In the refining stage, we append fully- connected <b>layers</b> to <b>the</b> pre-trained <b>convolutional</b> <b>layers</b> and train <b>the</b> whole network jointly. Visualization shows that the model trained with our method captures improved high-level expression semantics. Evaluations on four public expression databases, CK+, Oulu-CASIA, TFD, and SFEW demonstrate that our method achieves better results than state-of-the-art...|$|R
30|$|Convolutional neural {{networks}} offer certain advantages {{that make them}} desirable to address these problems. First, each neuron in the first hidden layer, instead of connecting to all input neurons, is only connected to a small region of them. This reduction in connection complexity works to also reduce potential computational problems. Second, using the same weights {{for each of the}} hidden neurons provides the opportunity to detect the same feature in different locations in the input text. At the end of the network, a pooling layer simplifies the information from <b>the</b> <b>convolutional</b> <b>layers</b> to <b>the</b> output [97]. <b>The</b> <b>convolutional</b> neural network is one of the methods that can be used effectively for Big Data analysis. <b>The</b> <b>convolutional</b> neural network {{which is one of the}} powerful models in Deep Learning, use <b>convolutional</b> <b>layers</b> to filter inputs for useful information.|$|R
30|$|Firstly, our method densely {{extracts}} {{the feature}} descriptors and their locations from the input image. In {{the same spirit}} of [53, 54], we input images in a modern CNN architecture [38, 55, 56] and use <b>the</b> <b>convolutional</b> <b>layers</b> as densely detected keypoints on a regular grid, i.e., cropping out the fully connected and softmax layers. In the following, we chose VGG- 16 [38] as the base network architecture {{and focus on the}} description tailored to it, but this can be replaced with other networks with marginal modification.|$|R
40|$|Convolutional Neural Network (CNN) {{recognition}} rates drop in {{the presence}} of noise. We demonstrate a novel method of counteracting this drop in recognition rate by adjusting the biases of the neurons in <b>the</b> <b>convolutional</b> <b>layers</b> according to <b>the</b> noise conditions encountered at runtime. We compare our technique to training one network for all possible noise levels, dehazing via preprocessing a signal with a denoising autoencoder, and training a network specifically for each noise level. Our system compares favorably in terms of robustness, computational complexity and recognition rate...|$|R
30|$|Having {{discussed}} <b>the</b> {{operation of}} <b>convolutional</b> neurons, let us describe {{the structure of}} the whole network. There are two strategies for combining the information got from the neurons assigned to different spectral bands. Abdel-Hamid et al. argue that the spectral phenomena occurring in different spectral regions are different, so each band should have a separate set of convolutional neurons. This scenario is known as the limited weight sharing (LWS) strategy [7]. In the full weight sharing scheme (FWS), all neurons are applied across all spectral regions, so the neurons encounter a more elaborate learning task. However, Sainath et al. argue that with a large enough number of hidden units, FWS can attain the same performance as LWS, while it is technically simpler and allows <b>the</b> stacking of <b>convolutional</b> <b>layers</b> [12]. In this study, we applied limited weight sharing, which is shown in Fig. 1 by the division of <b>the</b> <b>convolutional</b> <b>layer</b> into smaller blocks.|$|R
40|$|The {{goal of a}} Knowledge Base–supported Question Answering (KB-supported QA) {{system is}} to answer a query natural {{language}} by obtaining the answer from a knowledge database, which stores knowledge {{in the form of}} (entity, relation, value) triples. QA systems understand questions by extracting entity and relation pairs. This thesis aims at recognizing the relation candidates inside a question. We define a multi-label classification problem for this challenging task. Based on the word 2 vec representation of words, we propose two convolutional neural networks (CNNs) to solve the multi-label classification problem, namely Parallel CNN and Deep CNN. The Parallel CNN contains four parallel <b>convolutional</b> <b>layers</b> while Deep CNN contains two serial <b>convolutional</b> <b>layers.</b> <b>The</b> <b>convolutional</b> <b>layers</b> of both <b>the</b> models capture local semantic features. A max over time pooling layer is placed on the top of <b>the</b> last <b>convolutional</b> <b>layer</b> to select global semantic features. Fully connected layers with dropout are used to summarize the features. Our experiments show that these two models outperform the traditional Support Vector Classification (SVC) –based method by a large margin. Furthermore, we observe that Deep CNN has better performance than Parallel CNN, indicating that the deep structure enables much stronger semantic learning capacity than the wide but shallow network...|$|R
30|$|Using {{additional}} <b>convolutional</b> <b>layers</b> {{should improve}} classifier performance as higher-level features will be extracted. As we are {{curious about the}} benefits of using padded layers to build deeper networks on data where one dimension of input volume is small, comparing a six layer network against a three layer network is unfair. Thus, we designed two networks, one using padding and one with no padding, that are otherwise equal. Each network consists of three 3 × 3 convolutonal layers, 128 filters on each layer, followed by a 2 × 2 max pooling layer, then two fully connected layers with 1024 neurons and a final output layer of two neurons. Parameters for <b>the</b> <b>convolutional</b> <b>layers</b> are provided in Table 1.|$|R
30|$|In {{order to}} verify the {{effectiveness}} of the 3 DCRBM algorithm, this paper constructs a DBN deep network structure composed of 3 DCRBM, CNN, and BP and applies it to human behavior recognition experiments. The network structure firstly uses 3 DCRBM for unsupervised training and extracts spatio-temporal features from RGB-D images. Then, the link weights of <b>the</b> <b>convolutional</b> <b>layer</b> and <b>the</b> pooling layer of the 3 DCRBM are assigned to the CNN, and on the basis of supervised training again, there is a CNN and BP network, and the behavior is identified.|$|R
40|$|This article {{demonstrates}} that convolutional operation {{can be converted}} to matrix multiplication, which has the same calculation way with fully connected layer. The article is helpful for the beginners of the neural network to understand how fully connected <b>layer</b> and <b>the</b> <b>convolutional</b> <b>layer</b> work in <b>the</b> backend. To be concise and to make the article more readable, we only consider the linear case. It can be extended to the non-linear case easily through plugging in a non-linear encapsulation to the values like this σ(x) denoted as x^'. Comment: 9 page...|$|R
