270|0|Public
2500|$|Inter-process {{communication}} works via a <b>shared-nothing</b> asynchronous {{message passing}} system: every process has a [...] "mailbox", a queue of messages {{that have been}} sent by other processes and not yet consumed. A process uses the receive primitive to retrieve messages that match desired patterns. A message-handling routine tests messages in turn against each pattern, until one of them matches. When the message is consumed and removed from the mailbox the process resumes execution. A message may comprise any Erlang structure, including primitives (integers, floats, characters, atoms), tuples, lists, and functions.|$|E
5000|$|<b>Shared-Nothing</b> databases-an {{architecture}} {{in which all}} data is segregated to internally managed partitions with clear, well-defined data location boundaries. <b>Shared-nothing</b> databases require manual partition management.|$|E
5000|$|Competitive {{products}} offering <b>shared-nothing</b> architectures include: ...|$|E
5000|$|Reia—uses {{asynchronous}} {{message passing}} between <b>shared-nothing</b> objects ...|$|E
5000|$|<b>Shared-nothing</b> {{concurrent}} programming via message passing (Actor model) ...|$|E
50|$|VoltDB uses a <b>shared-nothing</b> {{architecture}} {{to achieve}} database parallelism. Data and the processing {{associated with it}} are distributed among all the CPU cores within the servers composing a single VoltDB cluster. By extending its <b>shared-nothing</b> foundation to the per-core level, VoltDB scales with the increasing core-per-CPU counts on modern commodity servers.|$|E
50|$|Clusterpoint {{database}} has multi-master <b>shared-nothing,</b> distributed, document-oriented database architecture storing XML and JSON data types.|$|E
50|$|CUBRID High Availability {{provides}} load-balanced, fault-tolerant {{and continuous}} service availability through its <b>shared-nothing</b> clustering, fail-over and fail-back automated mechanisms.|$|E
5000|$|<b>Shared-nothing</b> architecture, {{which reduces}} system {{contention}} for shared resources and allows gradual degradation of {{performance in the}} face of hardware failure.|$|E
50|$|<b>Shared-nothing</b> and shared-everything {{architectures}} {{each have}} advantages over the other. DBMS vendors and industry analysts regularly debate the matter; for example, Microsoft touts {{a comparison of}} its SQL Server 2005 with Oracle 10g RAC.|$|E
5000|$|Database-partitioning feature: A <b>shared-nothing</b> {{approach}} to clustering, with data hashed across multiple partitions {{on the same}} server or different processors. With the right database design, this approach allows near-linear scaling. This form of clustering is generally employed for large data warehouses rather than OLTP workloads.|$|E
50|$|The Clustrix {{database}} {{operates on}} a distributed cluster of <b>shared-nothing</b> nodes using a query to data approach. Here nodes typically own {{a subset of}} the data. SQL queries are split into query fragments and sent to the nodes that own the data. This enables Clustrix to scale horizontally (scale out) as additional nodes are added.|$|E
5000|$|MySQL Cluster is a {{technology}} providing <b>shared-nothing</b> clustering and auto-sharding for the MySQL database management system. It {{is designed to}} provide high availability and high throughput with low latency, while allowing for near linear scalability. MySQL Cluster is implemented through the NDB or NDBCLUSTER storage engine for MySQL ("NDB" [...] stands for Network Database).|$|E
50|$|In the C-Store project, {{started in}} 2005, Stonebraker, along with colleagues from Brandeis, Brown, MIT, and University of Massachusetts Boston, {{developed}} a parallel, <b>shared-nothing</b> column-oriented DBMS for data warehousing. By dividing and storing data in columns, C-Store {{is able to}} perform less I/O and get better compression ratios than conventional database systems that store data in rows.|$|E
50|$|Building on {{the success}} of Version 7, Informix split its core {{database}} development investment into two efforts. One effort, first known as XMP (for eXtended Multi-Processing), became the Version 8 product line, also known as XPS (for eXtended Parallel Server). This effort focused on enhancements in data warehousing and parallelism in high-end platforms, including <b>shared-nothing</b> platforms such as IBM's RS-6000/SP.|$|E
5000|$|Google App Engine's {{integrated}} Google Cloud Datastore database has a SQL-like syntax called [...] "GQL". GQL {{does not}} support the Join statement. Instead, one-to-many and many-to-many relationships can be accomplished using ReferenceProperty (...) [...] [...] This <b>shared-nothing</b> approach allows disks to fail without the system failing. Switching from a relational database to Cloud Datastore requires a paradigm shift for developers when modeling their data.|$|E
50|$|The first type of NewSQL {{systems are}} {{completely}} new database platforms. These {{are designed to}} operate in a distributed cluster of <b>shared-nothing</b> nodes, in which each node owns {{a subset of the}} data. These databases are often written from scratch with a distributed architecture in mind, and include components such as distributed concurrency control, flow control, and distributed query processing. Example systems in this category are LeanXcale, Google Spanner, Citus Data, CockroachDB, Clustrix, VoltDB, MemSQL, NuoDB and Trafodion.|$|E
50|$|UCIPT is {{developing}} a new open-source platform for ingesting, storing, indexing, querying, and analyzing vast quantities of data. Projects combine ideas from three areas (semi-structured data, parallel databases, and data-intensive computing) {{in order to create}} an open-source platform that scales by running on large, <b>shared-nothing</b> commodity computing clusters. An example of work in this area is AsterixDB, which grew out of a collaborative grant awarded by the National Science Foundation to UC Irvine professor and UCIPT member Michael Carey.|$|E
50|$|The General Parallel File System (GPFS) is a {{high-performance}} clustered file system developed by IBM. It can be deployed in shared-disk or <b>shared-nothing</b> distributed parallel modes. It {{is used by}} many of the world's largest commercial companies, {{as well as some of}} the supercomputers on the Top 500 List. For example, GPFS was the filesystem of the ASC Purple Supercomputer which was composed of more than 12,000 processors and has 2 petabytes of total disk storage spanning more than 11,000 disks.|$|E
50|$|The ParAccel Analytic Database was a {{parallel}} relational database system using a <b>shared-nothing</b> architecture with a columnar orientation, adaptive compression, memory-centric design.ParAccel's DBMS engine is built for analytics, initially based on PostgreSQL. ParAccel began phasing {{in a new}} optimizer (Omne) in release 2.0 and made significant changes to Omne in subsequent releases (3.1 released in June 2011).ParAccel implements compiled queries, and a proprietary interconnect protocol for inter-node communications. It integrated with storage area network technologies such as those from EMC Corporation.|$|E
5000|$|The eXtremeDB high {{availability}} edition supports both synchronous (2-safe) and asynchronous (1-safe) database replication, with automatic failover. [...] eXtremeDB Cluster edition provides for <b>shared-nothing</b> database clustering. eXtremeDB also supports distributed query processing, {{in which the}} database is partitioned horizontally and the DBMS distributes query processing across multiple servers, CPUs and/or CPU cores. eXtremeDB supports heterogeneous client platforms (e.g. a mix of Windows, Linux and RTOSs) with its clustering and {{high availability}} features. A single partitioned database can include shards running on a mix of hardware and OS platforms ...|$|E
5000|$|Inter-process {{communication}} works via a <b>shared-nothing</b> asynchronous {{message passing}} system: every process has a [...] "mailbox", a queue of messages {{that have been}} sent by other processes and not yet consumed. A process uses the [...] primitive to retrieve messages that match desired patterns. A message-handling routine tests messages in turn against each pattern, until one of them matches. When the message is consumed and removed from the mailbox the process resumes execution. A message may comprise any Erlang structure, including primitives (integers, floats, characters, atoms), tuples, lists, and functions.|$|E
5000|$|SAP IQ has a massively {{parallel}} processing (MPP) framework {{based on a}} shared-everything environment that supports distributed query processing. Most other products capable of MPP tend {{to be based on}} <b>shared-nothing</b> environments. The benefit of shared-everything is that it's more flexible in terms of the variety of queries that can be optimized—especially for balancing the needs of many concurrent users. The downside is that in extreme cases, competition among processors to access a shared pool of storage (usually a storage-area network), can lead to I/O contention, which affects query performance.12 ...|$|E
50|$|Couchbase Server, {{originally}} {{known as}} Membase, is an open-source, distributed (<b>shared-nothing</b> architecture) multi-model NoSQL document-oriented database software package that is optimized for interactive applications. These applications may serve many concurrent users by creating, storing, retrieving, aggregating, manipulating and presenting data. In support {{of these kinds}} of application needs, Couchbase Server is designed to provide easy-to-scale key-value or JSON document access with low latency and high sustained throughput. It is designed to be clustered from a single machine to very large-scale deployments spanning many machines.A version originally called Couchbase Lite was later marketed as Couchbase Mobile combined with other software.|$|E
50|$|H-Store is able {{to execute}} {{transaction}} processing with high throughput by forgoing much of legacy architecture of System R-like systems. For example, H-Store {{was designed as a}} parallel, row-storage relational DBMS that runs on a cluster of <b>shared-nothing,</b> main memory executor nodes.The database is partitioned into disjoint subsets that are assigned to a single-threaded execution engine assigned to one and only one core on a node. Each engine has exclusive access to all of the data at its partition. Because it is single-threaded, only one transaction at a time {{is able to}} access the data stored at its partition. Thus, there are no physical locks or latches in the system, and no transaction will stall waiting for another transaction once it is started.|$|E
5000|$|David DeWitt and Michael Stonebraker, {{computer}} scientists specializing in parallel databases and <b>shared-nothing</b> architectures, {{have been critical}} of the breadth of problems that MapReduce can be used for. They called its interface too low-level and questioned whether it really represents the paradigm shift its proponents have claimed it is. They challenged the MapReduce proponents' claims of novelty, citing Teradata as an example of prior art that has existed for over two decades. They also compared MapReduce programmers to CODASYL programmers, noting both are [...] "writing in a low-level language performing low-level record manipulation." [...] MapReduce's use of input files and lack of schema support prevents the performance improvements enabled by common database system features such as B-trees and hash partitioning, though projects such as Pig (or PigLatin), Sawzall, Apache Hive, YSmart, HBase and BigTable are addressing some of these problems.|$|E
50|$|Oracle NoSQL Database is a client-server, sharded, <b>shared-nothing</b> system. The data in each shard are {{replicated}} {{on each of}} the nodes which {{comprise the}} shard. It provides a simple key-value paradigm to the application developer. The major key for a record is hashed to identify the shard that the record belongs to.Oracle NoSQL Database is designed to support changing the number of shards dynamically in response to availability of additional hardware. If the number of shards changes, key-value pairs are redistributed across the new set of shards dynamically, without requiring a system shutdown and restart. A shard is made up of a single electable master node which can serve read and write requests,and several replicas (usually two or more) which can serve read requests. Replicas are kept up to date using streaming replication. Each change on the master node is committed locally to disk and also propagated to the replicas.|$|E
50|$|Clients use {{client-side}} libraries {{to contact}} the servers which, by default, expose their service at port 11211. Each client knows all servers; the servers do not communicate with each other. If a client wishes to set or read the value corresponding to a certain key, the client's library first computes a hash of the key to determine which server to use. This gives a simple form of sharding and scalable <b>shared-nothing</b> architecture across the servers. The server computes a second hash of the key to determine where to store or read the corresponding value. The servers keep the values in RAM; if a server runs out of RAM, it discards the oldest values. Therefore, clients must treat Memcached as a transitory cache; they cannot assume that data stored in Memcached is still there when they need it. Other databases, such as MemcacheDB, Couchbase Server, provide persistent storage while maintaining Memcached protocol compatibility.|$|E
50|$|While NetWare 3.x was current, Novell {{introduced}} its first high-availability clustering system, named NetWare SFT-III, which allowed a logical server {{to be completely}} mirrored to a separate physical machine. Implemented as a <b>shared-nothing</b> cluster, under SFT-III the OS was logically split into an interrupt-driven I/O engine and the event-driven OS core. The I/O engines serialized their interrupts (disk, network etc.) into a combined event stream that was fed to two identical copies of the system engine through a fast (typically 100 Mbit/s) inter-server link. Because of its non-preemptive nature, the OS core, stripped of non-deterministic I/O, behaves deterministically, like a large finite state machine. The outputs of the two system engines were compared to ensure proper operation, and two copies fed back to the I/O engines. Using the existing SFT-II software RAID functionality present in the core, disks could be mirrored between the two machines without special hardware. The two machines could be separated {{as far as the}} server-to-server link would permit. In case of a server or disk failure, the surviving server could take over client sessions transparently after a short pause since it had full state information. SFT-III was the first NetWare version able to make use of SMP hardware - the I/O engine could optionally be run on its own CPU. NetWare SFT-III, ahead of its time in several ways, was a mixed success.|$|E
40|$|Two typical {{architectures}} {{of parallel}} database systems are the shared-everything and <b>shared-nothing</b> architectures. Shared-everything architecture provides better performance than the <b>shared-nothing</b> architecture {{but it is}} not scalable to large system sizes. On the other hand, <b>shared-nothing</b> architecture provides good system scalability but is sensitive to data skew. Hierarchical architectures have been proposed to incorporate the best features of these two architectures (i. e., the <b>shared-nothing</b> and the shared-everything). In this paper, we present a detailed simulation study comparing the performance of the two-level hierarchical architecture with that of the <b>shared-nothing</b> and shared-everything architectures. The results from the simulation experiments presented here show that a properly designed hierarchical system can provide performance very close to that of the shared-everything while providing system scalability similar to that provided by the <b>shared-nothing</b> architecture. The res [...] ...|$|E
40|$|Abstract- The {{performance}} of high-volume transaction pro-cessing systems for business applications {{is determined by}} the degree of contention for hardware resources as well as for data. Hardware resource requirements may be met cost-effectively with a data-partitioned or <b>shared-nothing</b> architecture. However, the two-phase locking (2 PL) concurrency control method may restrict the {{performance of}} a <b>shared-nothing</b> system more se-verely than that of a centralized system due to increased lock holding times. Deadlock detection and resolution are an added complicating factor in <b>shared-nothing</b> systems. In this paper, we describe distributed Wait-Depth Limited (WDL) concurrency control (CC), a locking-based distributed CC method that limits the wait-depth of blocked transactions to one, thus preventing the occurrence of deadlocks. Several implementations of distributed WDL which vary in the number of messages and the amoun...|$|E
40|$|In this paper, we {{show that}} shared virtual memory, in a <b>shared-nothing</b> multiprocessor, {{facilitates}} the design and implementation of parallel join processing algorithms that perform significantly better {{in the presence of}} skew than previously proposed parallel join processing algorithms. We propose two variants of an algorithm for parallel join processing using shared virtual memory, and perform a detailed simulation to investigate their performance. The algorithm is unique in that it employs both the shared virtual memory paradigm and the message-passing paradigm used by current sharednothing parallel database systems. The implementation of the algorithm requires few modifications to existing <b>shared-nothing</b> parallel database systems. 1 Introduction The next generation of <b>shared-nothing</b> multiprocessors are expected to be equipped with shared virtual memory (henceforth called SVM) providing a globally shared address space (e. g. the Intel Paragon product literature states that it will pr [...] ...|$|E
40|$|LOGFLOW is a {{distributed}} Prolog system {{running on}} multi-transputer machines and workstation clusters. It {{is based on}} the dataflow principle. The most recent database management systems prefer <b>shared-nothing</b> architectures and dataflow execution mechanism because of their promising features. In this paper LOGFLOW is examined from the database point of view. Both relational and deductive DBMSs can be parallelized by the use of the parallel engine of LOGFLOW. 1. Introduction LOGFLOW is a distributed Prolog system based on the dataflow principle and runs on <b>shared-nothing</b> architectures. The execution of programs written in Prolog is very similar to the execution of programs written in declarative database handling languages, such as SQL or Datalog. The parallel engine of LOGFLOW has the same principles as the ones of parallel database engines. Therefore, this Prolog engine seems to be a good basis for implementing a parallel database system on a <b>shared-nothing</b> architecture. The s [...] ...|$|E
30|$|Cassandra is an {{open-source}} <b>shared-nothing</b> NoSQL column-store database {{developed and}} used in Facebook [10, 52, 56]. It {{is based on the}} ideas behind Google BigTable [3] and Amazon Dynamo [14].|$|E
30|$|In this paper, Kavosh, {{which is}} a method for {{association}} rule mining, is proposed. This method is designed for a <b>shared-nothing</b> architecture and can properly solve the association rule mining problem in Map-Reduce.|$|E
40|$|Abstract. Data {{placement}} in <b>shared-nothing</b> database systems {{has been studied}} extensively {{in the past and}} various placement algorithms have been proposed. However, there is no consensus on the most efficient data placement algorithm and placement is still performed manually by a database administrator with periodic reorganization to correct mistakes. This paper presents the first comprehensive simulation study of data placement issues in a <b>shared-nothing</b> system. The results show that current hardware technology trends have significantly changed the performance tradeoffs considered in past studies. A simplistic data placement strategy based on the new results is developed and shown to perform well for a variety of workloads...|$|E
