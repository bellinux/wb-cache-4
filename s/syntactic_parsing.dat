489|278|Public
50|$|LXR {{is written}} in Perl, handy choice for CGI scripts, but not really fit for lexical or <b>syntactic</b> <b>parsing.</b>|$|E
50|$|This architecture, {{with a few}} improvements, {{has been}} used for {{successfully}} parsing natural scenes and for <b>syntactic</b> <b>parsing</b> of natural language sentences.|$|E
5000|$|Since the lexical {{scanning}} and <b>syntactic</b> <b>parsing</b> processing is combined, {{the resulting}} parser {{tends to be}} harder to understand and debug for more complex languages ...|$|E
40|$|This paper {{proposes a}} new {{approach}} to dynamically determine the tree span for tree kernel-based semantic relation extraction. It exploits constituent dependencies to keep the nodes and their head children along the path connecting the two entities, while removing the noisy information from the <b>syntactic</b> <b>parse</b> tree, eventually leading to a dynamic <b>syntactic</b> <b>parse</b> tree. This paper also explores entity features and their combined features in a unified parse and semantic tree, which integrates both structured <b>syntactic</b> <b>parse</b> information and entity-related semantic information. Evaluation on the ACE RDC 2004 corpus shows that our dynamic <b>syntactic</b> <b>parse</b> tree outperforms all previous tree spans, and the composite kernel combining this tree kernel with a linear state-of-the-art feature-based kernel, achieves the so far best performance. ...|$|R
40|$|This paper {{proposes a}} dependency-driven scheme to {{dynamically}} determine the <b>syntactic</b> <b>parse</b> tree structure for tree kernel-based anaphoricity determination in coreference resolution. Given a full <b>syntactic</b> <b>parse</b> tree, {{it keeps the}} nodes and the paths related with current mention based on constituent dependencies from both syntactic and semantic perspectives, while removing the noisy information, eventually leading to a dependency-driven dynamic <b>syntactic</b> <b>parse</b> tree (D-DSPT). Evaluation on the ACE 2003 corpus shows that the D-DSPT outperforms all previous parse tree structures on anaphoricity determination, and that applying our anaphoricity determination module in coreference resolution achieves the so far best performance. ...|$|R
5000|$|... #Caption: A <b>syntactic</b> <b>parse</b> of [...] "Alfred spoke" [...] {{under the}} {{dependency}} formalism ...|$|R
50|$|In recent years, {{perceptron}} {{training has}} become {{popular in the}} field of natural language processing for such tasks as part-of-speech tagging and <b>syntactic</b> <b>parsing</b> (Collins, 2002).|$|E
50|$|Word and phrase embeddings, {{when used}} as the {{underlying}} input representation, {{have been shown to}} boost the performance in NLP tasks such as <b>syntactic</b> <b>parsing</b> and sentiment analysis.|$|E
5000|$|Although some text {{analytics}} systems apply exclusively advanced statistical methods, {{many others}} apply more extensive natural language processing, such {{as part of}} speech tagging, <b>syntactic</b> <b>parsing,</b> {{and other types of}} linguistic analysis.|$|E
40|$|Scope {{detection}} {{is a key}} task {{in information}} ex-traction. This paper proposes a new approach for tree kernel-based scope detection by using the structured <b>syntactic</b> <b>parse</b> information. In addi-tion, we have explored the way of selecting compatible features for different part-of-speech cues. Experiments on the BioScope corpus show that both constituent and dependency structured <b>syntactic</b> <b>parse</b> features have the advantage in capturing the potential relationships between cues and their scopes. Compared {{with the state of}} the art scope detection systems, our system achieves substantial improvement. *...|$|R
5000|$|Operation of {{generalization}} commutes {{with the}} operation of transition from <b>syntactic</b> (<b>parse</b> trees) to semantic (symbolic expressions) level. The latter can then be subject to conventional anti-unification.|$|R
5000|$|ACE {{processor}} (Answer Constraint Engine): {{an efficient}} system to process DELPH-IN grammars that provide HPSG <b>syntactic</b> <b>parses</b> with MRS outputs. The {{latest version of}} ACE is able to generate natural language sentences.|$|R
5000|$|A {{number of}} {{researchers}} have been using ACT-R to model several aspects of natural language understanding and production. They include models of <b>syntactic</b> <b>parsing,</b> language understanding, language acquisition [...] and metaphor comprehension.|$|E
50|$|Frazier's {{work has}} {{examined}} how listeners approach {{the task of}} processing the incoming language stream. She has proposed and refined <b>syntactic</b> <b>parsing</b> models, including a two-tier parsing system, the garden path model, and the Active Filler Hypothesis. Her recent work has focused on how listeners parse ellipsis.|$|E
5000|$|The {{advantages}} of the CCM declarative formulation {{and the availability of}} off-the-shelf solvers have led to a large variety of natural language processing tasks being formulated within the framework, including semantic role labeling, <b>syntactic</b> <b>parsing,</b> coreference resolution, summarization, transliteration, natural language generation [...] and joint information extraction.|$|E
5000|$|Sentence {{generalization}} and generalization diagrams can {{be defined}} as a special sort of conceptual graphs which can be constructed automatically from <b>syntactic</b> <b>parse</b> trees and support semantic classification task [...] Similarity measure between <b>syntactic</b> <b>parse</b> trees can be done as a generalization operation on the lists of sub-trees of these trees. The diagrams are representation of mapping between the syntax generalization level and semantics generalization level (anti-unification of logic forms). Generalization diagrams are intended to be more accurate semantic representation than conventional conceptual graphs for individual sentences because only syntactic commonalities are represented at semantic level.|$|R
40|$|This paper {{proposes a}} Unified Dynamic Relation Tree (DRT) span for tree kernel-based {{semantic}} relation extraction between entity names. The basic {{idea is to}} apply a variety of linguistics-driven rules to dynamically prune out noisy information from a <b>syntactic</b> <b>parse</b> tree and include necessary contextual information. In addition, different kinds of entity-related semantic information are unified into the <b>syntactic</b> <b>parse</b> tree. Evaluation on the ACE RDC 2004 corpus shows that the Unified DRT span outperforms other widely-used tree spans, and our system achieves comparable performance with the state-of-the-art kernel-based ones. This indicates that our method can not only well model the structured syntactic information but also effectively capture entity-related semantic information...|$|R
40|$|This paper {{describes}} {{the use of}} two machine learning tech-niques, naive Bayes and decision trees, to address the task of assigning function tags to nodes in a <b>syntactic</b> <b>parse</b> tree. Function tags are extra functional information, such as log-ical subject or predicate, that {{can be added to}} certain nodes in <b>syntactic</b> <b>parse</b> trees. We model the function tags assign-ment problem as a classification problem. Each function tag is regarded as a class and the task is to find what class/tag a given node in a parse tree belongs to from a set of predefined classes/tags. The paper offers the first systematic comparison of the two techniques, naive Bayes and decision trees, for the task of function tags assignment. The comparison is based on a standardized data set...|$|R
5000|$|The base form {{reduction}} {{is used to}} prepare lists of words and a text for automatic retrieval of terms from a term bank. On the other hand, <b>syntactic</b> <b>parsing</b> {{may be used to}} extract multi-word terms or phraseology from a source text. So parsing is used to normalise word order variation of phraseology, this is which words can form a phrase.|$|E
50|$|In the 1970s and 1980s Professor Cook {{introduced}} various {{research methods}} in second language acquisition research (elicited imitation, short-term memory measures, response times and micro artificial languages). He favoured an experimental approach to second language research and conducted various experiments. He wrote {{a very successful}} textbook about Noam Chomsky's theories (Chomsky's Universal Grammar: An Introduction), and related Universal Grammar to second language acquisition and teaching. He also published papers on second language teaching, and developed Computer-Assisted Language Learning programs for learning English as a Foreign Language, including adventure games and <b>syntactic</b> <b>parsing</b> programs.|$|E
50|$|Eugene Charniak is a Computer Science and Cognitive Science {{professor}} at Brown University. He has an A.B. in Physics from The University of Chicago and a Ph.D. from M.I.T. in Computer Science. His research {{has always been}} {{in the area of}} language understanding or technologies which relate to it, such as knowledge representation, reasoning under uncertainty, and learning. Since the early 1990s he has been interested in statistical techniques for language understanding. His research in this area has included work in the subareas of part-of-speech tagging, probabilistic context-free grammar induction, and, more recently, syntactic disambiguation through word statistics, efficient <b>syntactic</b> <b>parsing,</b> and lexical resource acquisition through statistical means.|$|E
40|$|Parsers {{have been}} shown to be helpful in {{information}} retrieval tasks because they are able to model long-span word dependencies efficiently. While previous work focused on using traditional <b>syntactic</b> <b>parse</b> trees, this paper proposes a new approach where, unlike previous work, the parser parameters are discriminatively trained to directly optimize a non-convex and non-smooth IR measure. The relevance between a document and a query is then modeled by the weighted tree edit distance between their parses. We evaluate our method on a large scale web search task consisting of a real world query set. Results show that the new parser is more effective for document retrieval than using traditional <b>syntactic</b> <b>parse</b> trees. It gives significant improvement, especially for long queries where proper modeling of long-span dependencies is crucial. Index Terms — information retrieval, parsing model, end-to-end optimization, tree edit distance 1...|$|R
40|$|Discourse Representation Theory (DRT) was {{developed}} by Hans Kamp 1981 in order to combine “a definition of truth with a systematic account of semantic representations (277) ”. The semantic representations produced are to provide a bridge between <b>syntactic</b> <b>parses</b> and model theoretic semantics such that the representations {{can be used to}} determine the trut...|$|R
40|$|Prosodic {{patterns}} {{provide important}} cues for resolving syntactic ambiguity, {{and might be}} used to improve the accuracy of automatic speech understanding. With this goal, we propose a method of scoring <b>syntactic</b> <b>parses</b> in terms of observed prosodic cues, which can be used in ranking sentence hypotheses and associated parses. Specifically, the score is the probability of acoustic features of a hypothesized word sequence given an associated <b>syntactic</b> <b>parse,</b> based on acoustic and "language" (prosody/syntax) models that represent probabilities in terms of abstract prosodic labels. This work reports initial efforts aimed at extending the algorithm to spontaneous speech, specifically the ATIS task, where the prosody/parse score is shown to improve the average rank of the correct sentence hypothesis. 1. INTRODUCTION Human listeners bring several sources of information to bear in interpreting an utterance, including syntax, semantics, discourse, pragmatics and prosodic cues. Prosody, in partic [...] ...|$|R
50|$|TeLQAS {{includes}} three main subsystems: an online subsystem, an offline subsystem, and an ontology. The online subsystem answers questions submitted by users in real time. During the online process, TeLQAS processes the question using a {{natural language processing}} component that implements part-of-speech tagging and simple <b>syntactic</b> <b>parsing.</b> The online subsystem also utilizes an inference engine in order to carry out necessary inference on small elements of knowledge. The offline subsystem automatically indexes documents collected by a focused web crawler from the web. An ontology server along with its API is used for knowledge representation. The main concepts and classes of the ontology are created by domain experts. Some of these classes, however, can be instantiated automatically by the offline components.|$|E
50|$|What she and C.L.R.U. {{were trying}} to do was far ahead of its time. Efforts were made to tackle {{fundamental}} problems with the computers of the day that had the capacity of a modern digital wrist watch. Despite every kind of problem, the Unit produced numerous publications on language and related subjects, including information retrieval and automatic classification. For over ten years the Unit's presence was strongly felt in the field, always with an emphasis on basic semantic problems of language understanding. Margaret had no time for those who felt that all that needed doing was <b>syntactic</b> <b>parsing,</b> or that complete parsing was necessary before you did anything else. Now that the semantics of language are regarded as a basic part of its understanding by machine, the ideas of C.L.R.U. seem curiously modern.|$|E
50|$|ABBYY Compreno is {{a natural}} {{language}} processing technology that provides morphology, syntactic and semantic analysis of unstructured content. At the beginning of 2011, ABBYY received a grant of 475 million Russian roubles (about US$15 million at that time) from the Skolkovo innovation center {{for the development of}} Compreno. In February 2016, the company announced two products based on this technology, ABBYY InfoExtractor SDK and ABBYY Smart Classifier. The technology is based on USH (Universal Semantic Hierarchy). <b>Syntactic</b> <b>Parsing</b> Technology is used to complement the USH. The approach will allow for both the in-depth syntax analysis of the source text and the differentiation of subtle details of meaning based on world- and subject-knowledge. It can be used for intellectual information search based on abstractly defined content and expressed ideas / involved subjects (regardless of specific terminology and vocabulary used), as opposed to currently widely used keyword searching.|$|E
40|$|We {{extend the}} {{mechanism}} of logical generalization toward <b>syntactic</b> <b>parse</b> trees and attempt to detect semantic signals unobservable {{in the level of}} keywords. Generalization from a <b>syntactic</b> <b>parse</b> tree as a measure of syntactic similarity is defined by the obtained set of maximum common sub-trees and is performed at the level of paragraphs, sentences, phrases and individual words. We analyze the semantic features of this similarity measure and compare it with the semantics of traditional anti-unification of terms. Nearest-Neighbor machine learning is then applied to relate the sentence to a semantic class. By using a <b>syntactic</b> <b>parse</b> tree-based similarity measure instead of the bag-of-words and keyword frequency approaches, we expect to detect a subtle difference between semantic classes that is otherwise unobservable. The proposed approach is evaluated in three distinct domains in which a lack of semantic information makes the classification of sentences rather difficult. We conclude that implicit indications of semantic classes can be extracted from syntactic structuresWe are grateful to our colleagues SO Kuznetsov, B Kovalerchuk and others for valuable discussions and to our anonymous reviewers for their suggestions. This research is partially funded by the EU Project No. 238887, a unique European Citizens' attention service (iSAC 6 +) IST-PSP. This research is also funded by the Spanish MICINN (Ministerio de Ciencia e Innovacion) IPT- 430000 - 2010 - 13 project Social powered Agents for Knowledge search Engine (SAKE), TIN 2010 - 17903 Comparative approaches to the implementation of intelligent agents in digital preservation from a perspective of the automation of social networks, and the AGAUR 2011 Fl_B 00927 research grant awarded to Gabor Dobrocsi and the grup de recerca consolidat CSI-ref. 2009 SGR- 120...|$|R
5000|$|A Parse Thicket is a graph that {{represents}} the syntactic {{structure of a paragraph}} of text in natural language processing. A Parse Thicket includes Parse tree for each sentence for this paragraph plus some arcs for other relations between words other than <b>syntactic.</b> <b>Parse</b> thickets can be constructed for both constituency parse trees and dependency parse trees. The relations which link parse trees within a Parse Thicket are: ...|$|R
40|$|We {{present a}} {{reformulation}} {{of the word}} pair features typically used for the task of disambiguating implicit relations in the Penn Discourse Treebank. Our word pair features achieve significantly higher performance than the previous formulation when evaluated without additional features. In addition, we present results for a full system using additional features which achieves close to {{state of the art}} performance without resorting to gold <b>syntactic</b> <b>parses</b> or to context outside the relation. ...|$|R
5000|$|From 1993, as a K.U.Leuven Research Fellow in Linguistics, Beeken {{developed}} {{a number of}} software tools and packages for Dutch: CSE (computer-supported education), <b>syntactic</b> <b>parsing</b> (Comenius, Butler, Cogito)for grammar and writing instruction/computational linguistics (CONST) [...] and for spelling (DT-manie). During this period of research Beeken also worked {{on the development of}} electronic dictionaries: Beknopte ABN-gids (Concise guide to Standard Dutch) with Dirk Geeraerts and Dirk Speelman.From 1993-1996 Beeken continued her research with the NFWO project, 'De lexicale thesaurus als begin- en eindpunt van de interactie tussen lexicologische theorie en lexicografische toepassing' (The lexical thesaurus as starting and finishing point of the interaction between lexicological theory and lexicographic application)From 1997 -1999 Beeken was employed by the Dutch Language Union (Taalunie) to work on the revision of the Groene Boekje 1995 (Green Booklet 1995) towards the creation of the 2005 Groene Boekje. She also was the project manager of the Dutch HLT Agency / TST-Centrale from 2004 to 2007. From 2007 to 2014 Jeannine Beeken was director of the Institute for Dutch Lexicology.|$|E
5000|$|Dating from 1987, a {{year after}} the {{publication}} of A Linguistic Atlas of Late Medieval English (LALME), LAEME's parent project, LAEME builds on medieval dialect methodologies developed for LALME, but parts ways with the latter by employing corpus linguistics methods. In its present form, such methods include the lexico-grammatical tagging of a select but comprehensive Early Middle English corpus, to which an ongoing project will add <b>syntactic</b> <b>parsing</b> (see Parsed Linguistic Atlas of Early Middle English P-LAEME). [...] Public access to a fully tagged, syntactically annotated corpus should provide unprecedented scope for phonological, lexico-grammatical, semantic, pragmatic as well as dialectal inquiry into a period marked by rapid linguistic change but also by a paucity of surviving texts. At the lexical level, further scope is added by A Corpus of Narrative Etymologies (CoNE, Version 1.1, 2013). [...] CoNE derives a Corpus of Changes (CC) from LAEME's Corpus of Tagged Texts (CTT), giving relative chronologies of lexical forms annotated by Special Codes. CoNE's narrative etymologies are not based on semantic backtracking through cognates, but on the processual narrative of word forms through time.|$|E
5000|$|Keyword {{extraction}} is {{the first}} step for identifying the input question type. In some cases, there are clear words that indicate the question type directly. i.e. [...] "Who", [...] "Where" [...] or [...] "How many", these words tell the system that the answers should be of type [...] "Person", [...] "Location", [...] "Number" [...] respectively. In the example above, the word [...] "When" [...] indicates that the answer should be of type [...] "Date". POS (Part of Speech) tagging and <b>syntactic</b> <b>parsing</b> techniques {{can also be used to}} determine the answer type. In this case, the subject is [...] "Chinese National Day", the predicate is [...] "is" [...] and the adverbial modifier is [...] "when", therefore the answer type is [...] "Date". Unfortunately, some interrogative words like [...] "Which", [...] "What" [...] or [...] "How" [...] do not give clear answer types. Each of these words can represent more than one type. In situations like this, other words in the question need to be considered. First thing to do is to find the words that can indicate the meaning of the question. A lexical dictionary such as WordNet can then be used for understanding the context.|$|E
40|$|In this paper, {{we present}} the KIT systems {{participating}} in the Shared Translation Task translating between English↔German and English↔French. All translations are generated using phrase-based translation systems, using different kinds of word-based, part-of-speech-based and cluster-based language models trained on the provided data. Additional models include bilingual lan-guage models, reordering models based on part-of-speech tags and <b>syntactic</b> <b>parse</b> trees, {{as well as a}} lexicalized reordering model. In order to make use of nois...|$|R
40|$|We {{investigate}} {{the problem of}} determining a compact underspecified semantical representation for sentences that may be highly ambiguous. Due to combinatorial explosion, the naive method of building semantics for the different syntactic readings independently is prohibitive. We present a method that takes as input a <b>syntactic</b> <b>parse</b> forest with associated constraintbased semantic construction rules and directly builds a packed semantic structure. The algorithm is fully implemented and runs in O(n 4 log(n)) in sentence length, if the grammar meets some reasonable ‘normality’ restrictions. ...|$|R
40|$|Tree-based {{approaches}} to alignment model translation as {{a sequence of}} probabilistic op-erations transforming the <b>syntactic</b> <b>parse</b> tree of a sentence in one language into that of the other. The trees may be learned directly from parallel corpora (Wu, 1997), or provided by a parser trained on hand-annotated treebanks (Ya-mada and Knight, 2001). In this paper, we compare these approaches on Chinese-English and French-English datasets, and find that au-tomatically derived trees result in better agree-ment with human-annotated word-level align-ments for unseen test data. ...|$|R
