0|2730|Public
40|$|Keywords:Document {{similarity}} search; graph representation; Text mining; Information retrieval Abstract. Document similarity {{search is}} {{to retrieve a}} ranked list of <b>similar</b> <b>documents</b> and find <b>documents</b> <b>similar</b> to a query document in a text corpus or a web page on the web. But most of the previous researches regarding <b>searching</b> for <b>similar</b> <b>documents</b> are focused on classifying documents based on the contents of documents. To solve this problem, we propose a novel retrieval approach based on undirected graphs to represent each document in corpus. In addition, this study also considers unified graph in conjunction with multiple graphs {{to improve the quality}} of <b>searching</b> for <b>similar</b> <b>documents.</b> Experimental results on the Reuters- 21578 data demonstrate that the proposed system has better performance and success than the traditional approach...|$|R
40|$|Most {{existing}} Peer-to-Peer (P 2 P) {{systems support}} only title-based searches and {{are limited in}} functionality when compared to today 2 ̆ 7 s search engines. In this paper, we present {{the design of a}} distributed P 2 P information sharing system that supports semantic-based content searches of relevant documents. First, we propose a general and extensible framework for <b>searching</b> <b>similar</b> <b>documents</b> in P 2 P network. The framework is based on the novel concept of Hierarchial Summary Structure. Second, based on the framework, we develop our efficient document searching system by effectively summarizing and maintaining all documents within the network with different granularity. Finally, an experimental study is conducted on a real P 2 P prototype, and a large-scale network is further simulated. The results show the effectiveness, efficiency, and scalability of the proposed system...|$|R
40|$|Abstract — Most {{existing}} Peer-to-Peer (P 2 P) {{systems support}} only title-based searches and {{are limited in}} functionality when compared to today’s search engines. In this paper, we present {{the design of a}} distributed P 2 P information sharing system that supports semantic-based content searches of relevant documents. First, we propose a general and extensible framework for <b>searching</b> <b>similar</b> <b>documents</b> in P 2 P network. The framework is based on the novel concept of Hierarchical Summary Structure. Second, based on the framework, we develop our efficient document searching system, by effectively summarizing and maintaining all documents within the network with different granularity. Finally, an experimental study is conducted on a real P 2 P prototype, and a large-scale network is further simulated. The results show the effectiveness, efficiency and scalability of the proposed system. Index Terms — content-based, similarity search, peer-to-peer, hierarchical summary, indexin...|$|R
40|$|Repozitar. cz is a {{new system}} for {{detection}} of plagiarism that will join the systems like Theses. cz and Odevzdej. cz. The system will provide {{the services of a}} scientific digital library. This project is an effort of 15 universities which with the technical solution form the necessary organizational, social and legal environment. This contribution introduces this system whose aim is, among others, the presentation of works according to Open Access idea, <b>searching</b> for <b>similar</b> <b>documents</b> or transfer of publication meta-data to the RIV...|$|R
40|$|Figure: The {{recursive}} {{search strategy}} uses search result {{to find more}} results and then combines them. SimSeer [2] is a similarity search engine that incorporates the recursive similarity search algorithm. It currently supports three similarity functions, though {{it is possible to}} add more. They are: •Key phrase similarity •Shingle similarity [3] based on sequences of words in a document. •Simhash similarity [4] based on all of the words in a document. The need to find <b>similar</b> <b>documents</b> arises in many situations: •Plagiarism detection •Near duplicate detection •Research paper recommendation Traditionally, queries are constructed by users and submitted to search engines; however, it may not be obvious to users how they should construct such queries or overly complex to do so. Thus querying with a whole document and allowing a system to automatically construct queries can be useful. This research investigates methods for automatically <b>searching</b> for <b>similar</b> <b>documents</b> using a sample document while dealing with issues such as scalability and multiple notions of similarity. <b>Searching</b> for <b>similar</b> <b>documents</b> with a sample document may not return all <b>similar</b> <b>documents</b> since it’s possible for <b>similar</b> <b>documents</b> to not have any features in common. Swanson’s ABC model [1] provides inspiration for a recursive search algorithm to overcome this shortcoming. The search algorithm based on this model is: 1. Search with initial query document 2. Take the top k results and use them to search for more documents 3. Repeat step 2 a set number of times 4. Combine all results 5. Rank each result either by similarity to the initial query (target ranking) document submitted by the user or by the similarity to the query document that found it (local ranking...|$|R
40|$|Abstract—Nearest Neighbor <b>Search</b> for <b>similar</b> <b>document</b> {{retrieval}} {{suffers from}} an efficiency problem when scaled {{to a large}} dataset. In this paper, we introduce an unsupervised approach based on Locality Sensitive Hashing to alleviate its search complexity problem. The advantage of our proposed approach {{is that it does}} not need to scan all the documents for retrieving top-K Nearest Neighbors, instead, a number of hash table lookup operations are conducted to retrieve the top-K candidates. Experiments on two massive news and tweets datasets demonstrate that our approach is able to achieve over an order of speedup compared with the traditional Information Retrieval method and maintain reasonable precision...|$|R
40|$|The {{need to find}} <b>similar</b> <b>documents</b> {{occurs in}} many settings, such as in {{plagiarism}} detection or research paper recommen-dation. Manually constructing queries to find similar doc-uments may be overly complex, thus motivating the use of whole documents as queries. This paper introduces Sim-SeerX, a <b>search</b> engine for <b>similar</b> <b>document</b> retrieval that receives whole documents as queries and returns a ranked list of <b>similar</b> <b>documents.</b> Key {{to the design of}} SimSeerX is that is able to work with multiple similarity functions and document collections. We present the architecture and in-terface of SimSeerX, show its applicability with 3 different similarity functions and demonstrate its scalability on a col-lection of 3. 5 million academic documents...|$|R
40|$|Clustering {{methods are}} {{frequently}} used in data analysis to find {{groups in the}} data such that objects in the same group are similar to each other. Applied to document collections, clustering methods {{can be used to}} structure the collection based on the similarities of the contained documents and thus support a user in <b>searching</b> for <b>similar</b> <b>documents.</b> Furthermore, the discovered clusters can be automatically indexed by keywords. Therefore the user does not depend on manually defined index terms or a fixed hierarchy, which often did not reflect recent changes in the underlying document collections. In this article we present an approach that clusters a document collection using a growing selforganizing map. The presented method was implemented in a software tool, which combines keyword search methods with a visualization of the document collection...|$|R
50|$|SQLite version 3.7.4 {{first saw}} the {{addition}} of the FTS4(full text search) module, which features enhancements over the older FTS3 module. FTS4 allows users to perform full text <b>searches</b> on <b>documents</b> <b>similar</b> to how <b>search</b> engines search webpages. Version 3.8.2 added support for creating tables without rowid, which may provide space and performance improvements. Common table expressions support was added to SQLite in version 3.8.3.|$|R
40|$|Abstract: In this paper, we {{show how}} to {{represent}} to our formal reasoning and to model social context as knowledge using network models to aggregate heterogeneous information. We show how social context can be efficiently used for well understood tasks in {{natural language processing}} (such as context-dependent automated, large scale semantic annotation, term disambiguation, <b>search</b> of <b>similar</b> <b>documents),</b> {{as well as for}} novel applications such as social recommender systems which aim to alleviate information overload for social media users by presenting the most attractive and relevant content. We present the algorithms and the architecture of a hybrid recommender system in the activity centric environment Nepomuk-Simple (EU 6 th Framework Project NEPOMUK) : recommendations are computed on the fly by network flow methods performing in the unified multidimensional network of concepts from the personal information management ontology augmented with concepts extracted from the documents pertaining to the activity in question...|$|R
40|$|The current {{availability}} of large collections of full-text documents in electronic form {{emphasizes the need}} for intelligent information retrieval techniques. Especially in the rapidly growing World Wide Web {{it is important to}} have methods for exploring miscellaneous document collections automatically. In the report, we introduce the WEBSOM method for this task. Self-Organizing Maps (SOMs) are used to position encoded documents onto a map that provides a general view into the text collection. The general view visualizes similarity relations between the documents on a map display, which can be utilized in exploring the material rather than having to rely on traditional <b>search</b> expressions. <b>Similar</b> <b>documents</b> become mapped close to each other. The potential of the WEBSOM method is demonstrated in a case study where articles from the Usenet newsgroup "comp. ai. neural-nets" are organized. The map is available for exploration at the WWW address [URL]...|$|R
40|$|Search {{effectiveness}} is investigated when a corpus {{is created by}} using only “Title, ” “Abstract, ” and “Claims, ” which are expected to briefly express the invention, instead of using the entire document in the <b>search</b> for <b>documents</b> <b>similar</b> to a patent that expresses the invention is {{used to make a}} comparison with the search effectiveness of “Title + Abstract ” for the patent application, and the merits of each are discussed. 1...|$|R
40|$|We propose an {{associative}} {{document retrieval}} method, {{in which a}} document {{is used as a}} query to <b>search</b> for other <b>similar</b> <b>documents.</b> Because a long document usually includes more than one topic, we first analyze a query document to extract multiple subtopics. For each subtopic element, a sub-query is produced and <b>similar</b> <b>documents</b> are retrieved with a relevance score. The relevance scores are weighted by the importance of each subtopic element and are integrated to determine the final relevant documents. In the calculation of the subtopic importance, the specificity of a query term is evaluated using entropy, which is the deviation degree of the appearances of the term in each subtopic element. We apply this method to an invalidity patent search. By exploiting certain unique features of Japanese patent claims, we use features distinguishing the preamble and the essential portion in a query patent claim. To demonstrate the effectiveness of our method, we experimentally evaluated our associative document retrieval method on five years of patent documents...|$|R
40|$|Clustering {{techniques}} {{have been used}} by manyintelligent software agents in order to retrieve, #lter, and categorize documents available on the World Wide Web. Clustering is also useful in extracting salient features of related web documents to automatically formulate queries and <b>search</b> for other <b>similar</b> <b>documents</b> on the Web. Traditional clustering algorithms either use a priori knowledge of document structures to de#ne a distance or similarity among these documents, or use probabilistic techniques such as Bayesian classi#cation. Many of these traditional algorithms, however, falter when the dimensionalityofthe feature space becomes high relative {{to the size of the}} document space. In this paper, we introduce two new clustering algorithms that can e#ectively cluster documents, even in the presence of a very high dimensional feature space. These clustering techniques. which are based on generalizations of graph partitioning, do not require pre-speci#ed ad hoc distance functions [...] ...|$|R
40|$|Abstract: Clustering {{techniques}} {{have been used}} by many intelligent software agents in order tretrieve, filter and categorize document available on the World Wide Web. Clustering isalso useful in extracting salient features of related web documents to automatic ally formulate queries and <b>search</b> for other <b>similar</b> <b>documents</b> on the Web. In this paper, we introduce two new clustering algorithm withs K-Means Clustering in GeneLinker™ that can effectively cluster documents, even {{in the presence of a}} veryhighdimensional feature space. These clusteringtechniques, which are based on generalizations of graphpartitioning, do not require prespecifiedad hoc distance functions, and are capable of automatically discovering document similarities or associations. We conduct several experiment son real Web data using various feature selection and find out the no off clusters in the data documenting this paper also discuss about the real example. In this example we are find out the no. Of clusters...|$|R
40|$|User-generated {{content on}} the Internet has been explosively {{growing in the}} current Web 2. 0 era. This has been {{facilitated}} through widespread user access to the web through mobile devices, {{the rapid growth of}} social media applications, and review-based provider websites. The majority of this data {{is in the form of}} free text, as in social posts. Storing and querying this massive unstructured textual data is a challenging task that has been studied extensively recently. Current search solutions, such as Google, Bing and Amazon’s internal search, are effective in allowing users to find relevant documents in large collections. Those solutions rely on several content and reputation-based factors including document relevance to the user query. However, capturing and exploiting user intent particularly, in a domain-specific setting, remains an open problem with a variety of research challenges. In this thesis, we study several such settings where existing search techniques are inadequate. In particular, we studied the following subproblems where we are showcasing the benefit of leveraging domain-specific knowledge and user-generated content: 1) We argue for more effective item ranking for crowd-sourced review platforms and provide efficient algorithms to support it. 2) We provide a practical high-quality solution to build domain-specific ontologies from unstructured text documents. We describe our approach and provide fast and simple algorithms to use the generated ontology in extracting domain-specific features from the textual data. In particular, we describe our approach using a real-estate agency case study where domain agents are interested in evaluating the textual property descriptions. 3) We study how to <b>search</b> for <b>similar</b> <b>documents,</b> given a set of input documents, when the data source can only be accessed through a query interface (such as Google search). We propose a ranking model to extract effective query keywords from the input <b>documents</b> to retrieve <b>similar</b> <b>documents</b> through keyword-based <b>search</b> APIs. 4) We use data mining techniques to classify user-generated content on online forums in terms of its characteristics, such as bullying behavior. In particular, we crawl Yik Yak, an anonymous social media, to detect potentially harmful behaviors...|$|R
40|$|Latent Semantic Analysis (LSA) {{is widely}} used for finding the {{documents}} whose semantic {{is similar to the}} query of keywords. Although LSA yield promising similar results, the existing LSA algorithms involve lots of unnecessary operations in similarity computation and candidate check during on-line query processing, which is expensive in terms of time cost and cannot efficiently response the query request especially when the dataset becomes large. In this paper, we study the efficiency problem of on-line query processing for LSA towards efficiently <b>searching</b> the <b>similar</b> <b>documents</b> to a given query. We rewrite the similarity equation of LSA combined with an intermediate value called partial similarity that is stored in a designed index called partial index. For reducing the searching space, we give an approximate form of similarity equation, and then develop an efficient algorithm for building partial index, which skips the partial similarities lower than a given threshold θ. Based on partial index, we develop an efficient algorithm called ILSA for supporting fast on-line query processing. The given query is transformed into a pseudo document vector, and the similarities between query and candidate documents are computed by accumulating the partial similarities obtained from the index nodes corresponds to non-zero entries in the pseudo document vector. Compared to the LSA algorithm, ILSA reduces the time cost of on-line query processing by pruning the candidate documents that are not promising and skipping the operations that make little contribution to similarity scores. Extensive experiments through comparison with LSA have been done, which demonstrate the efficiency and effectiveness of our proposed algorithm...|$|R
40|$|Abstract We study a new {{research}} problem, where an implicit information retrieval query is inferred from eye movements measured when the user is reading, {{and used to}} retrieve new documents. In the training phase, the user’s interest is known, and we learn a mapping from how the user looks at a term {{to the role of}} the term in the implicit query. Assuming the mapping is universal, that is, the same for all queries in a given domain, we can use it to construct queries even for new topics for which no learning data is available. We constructed a controlled experimental setting to show that when the system has no prior information as to what the user is searching, the eye movements help significantly in the search. This is the case in a proactive search, for instance, where the system monitors the reading behaviour of the user in a new topic. In contrast, during a search or reading session where the set of inspected documents is biased towards being relevant, a stronger strategy is to <b>search</b> for content-wise <b>similar</b> <b>documents</b> than to use the eye movements...|$|R
40|$|Most topic {{modeling}} algorithms {{that address}} the evolution of documents over time use {{the same number of}} topics at all times. This obscures the common occurrence in the data where new subjects arise and old ones diminish or disappear entirely. We propose an algorithm to model the birth and death of topics within an LDA-like framework. The user selects an initial number of topics, after which new topics are created and retired without further supervision. Our approach also accommodates many of the acceleration and parallelization schemes developed in recent years for standard LDA. In recent years, topic modeling algorithms such as latent semantic analysis (LSA) [17], latent Dirichlet allocation (LDA) [10] and their descendants have offered a powerful way to explore and interrogate corpora far too large for any human to grasp without assistance. Using such algorithms we are able to <b>search</b> for <b>similar</b> <b>documents,</b> model and track the volume of topics over time, search for correlated topics or model them with a hierarchy. Most of these algorithms are intended for use with static corpora where the number of documents {{and the size of the}} vocabulary are known in advance. Moreover, almost all current topic modeling algorithms fix the number of topics as one of the input parameters and keep it fixed across the entire corpus. While this is appropriate for static corpora, it becomes a serious handicap when analyzing time-varying data sets where topics come and go as a matter of course. This is doubly true for online algorithms that may not have the option of revising earlier results in light of new data. To be sure, these algorithms will account for changing data one way or another, but without the ability to adapt to structural changes such as entirely new topics they may do so in counterintuitive ways...|$|R
50|$|An optimal {{index term}} {{is one that}} can {{distinguish}} two different documents {{from each other and}} relate two <b>similar</b> <b>documents.</b> On the other hand, a sub-optimal index term can not distinguish two different <b>document</b> from two <b>similar</b> <b>documents.</b>|$|R
30|$|Step 4. <b>Search</b> <b>similar</b> {{symmetrical}} exemplar in arbitrary directions.|$|R
5000|$|... the {{completion}} of an advance care directive or <b>similar</b> <b>document.</b>|$|R
30|$|Careful {{examination}} of <b>similar</b> <b>documents</b> in different instances showed that two documents {{with the highest}} similarity degree use almost identical term sets. In other words, the more terms two documents have in common, the more similar they are. In such cases, some of the well-known measures cannot recognize the most <b>similar</b> <b>documents.</b> The following example clarifies the case. Suppose three documents d 1, d 2, and d 3 with four terms and tf as term weighting scheme. Further, the terms are not shared by all the documents of the collection. Our goal {{is to find the}} most <b>similar</b> <b>document</b> to d 1.|$|R
40|$|Finding <b>similar</b> <b>documents</b> {{in natural}} {{language}} document collections {{is a difficult}} task that requires general and domain-specific world knowledge, deep analysis of the documents, and inference. However, {{a large portion of}} the pairs of <b>similar</b> <b>documents</b> can be identified by simpler, purely word-based methods. We show the use of Probabilistic Latent Semantic Analysis for finding <b>similar</b> <b>documents.</b> We evaluate our system on a collection of photocopier repair tips. Among the 100 top-ranked pairs, 88 are true positives. A manual analysis of the 12 false positives suggests the use of more semantic information in the retrieval model. 1...|$|R
5000|$|... #Subtitle level 3: Detecting and {{indexing}} <b>similar</b> <b>documents</b> {{in large}} corpora ...|$|R
50|$|Amsterdam Declaration 2002, a <b>similar</b> <b>document</b> {{from the}} International Humanist and Ethical Union.|$|R
40|$|A basic {{topic in}} mining of massive dataset is finding similar items. As an example, finding <b>similar</b> <b>documents</b> can be recommended. In this case many methods are existed. For example, Shingling method and length based {{filtering}} {{are one of}} them. In Shingling method, from each document, substrings have been selected with symbol name and, they are placed on one set. For finding <b>similar</b> <b>documents,</b> the similarities of sets that related with them have been calculated. In Length based filtering just documents which close these lengths have been compared. These methods don't consider repetition of symbols. With considering the repetition can calculate length of documents with more accurately. In this paper we suggested a method for finding <b>similar</b> <b>documents</b> with considering the repetition of symbols. This method separated documents to better form. The main goal {{of this paper is}} presentation a method for finding <b>similar</b> <b>documents</b> with take fewer comparisons and time indeed...|$|R
40|$|In this paper, {{we present}} a {{methodology}} for finding the n most <b>similar</b> <b>documents</b> across multiple text databases for any given query and for any positive integer n. This methodology consists of two steps. First, databases are ranked in a certain order. Next, documents are retrieved from the databases according to the order and in a particular way. If the databases containing the n most <b>similar</b> <b>documents</b> for a given query can be ranked ahead of other databases, the methodology will guarantee the retrieval of the n most <b>similar</b> <b>documents</b> for the query. A statistical method is provided to identify databases, {{each of which is}} estimated to contain {{at least one of the}} n most <b>similar</b> <b>documents.</b> Then, a number of strategies is presented to retrieve documents from the identified databases. Experimental results are given to illustrate the relative performance of different strategies. 1 Introduction The Internet has become a vast information source in recent years and can be considered as the w [...] ...|$|R
5000|$|One {{example of}} LSH is MinHash {{algorithm}} used for finding <b>similar</b> <b>documents</b> (such as web-pages): ...|$|R
40|$|<b>Searching</b> for <b>similar</b> {{objects in}} metric-space {{databases}} can be efficiently solved by using index data structures. A number of alternative sequential indexes {{have been proposed}} in the literature. This paper proposes the parallelization of a recent pivot-based index data structure which can efficiently accommodate on-line updates and reduces the number of object-to-object comparisons during searches. We assume a large collection of objects evenly distributed on the secondary memory {{of a set of}} processors and consider the parallel processing of a constant stream of queries {{as in the case of}} search engines. We present algorithms for index construction and query processing. Applications of metric-space indexes are in multimedia databases and text databases in cases such as detection of <b>similar</b> <b>documents...</b>|$|R
5000|$|Find <b>similar</b> <b>documents</b> across languages, after {{analyzing}} a {{base set}} of translated documents (cross language retrieval).|$|R
5000|$|I - {{certificate}} or <b>similar</b> <b>document</b> {{issued by}} {{authority of the}} national civil registry, or foreign equivalent; ...|$|R
5000|$|Golden section <b>search</b> (<b>similar</b> to ternary <b>search,</b> useful if {{evaluating}} f takes most of {{the time}} per iteration) ...|$|R
40|$|Abstract – Similarity is {{the most}} {{important}} feature of document clustering as the amount of web documents and the need of integrating documents from the huge multiple repositories, one of the challenging issues is to perform clustering of <b>similar</b> <b>documents</b> efficiently [...] A measure of the similarity between two patterns drawn from the same feature space is essential to most clustering procedures. From huge repositories, <b>similar</b> <b>document</b> identification for clustering is costly both in terms of space and time duration, and specially when finding near documents where documents could be added or deleted. In this paper, we try to find the effectiveness of Simhash based similarity measurement technique for detecting the <b>similar</b> <b>documents</b> which are used to perform clustering of documents using novel based K-means clustering method...|$|R
5000|$|Immunity Resource Foundation {{hosts the}} {{complete}} library of Continuum magazine among an internet database of 120,000 <b>similar</b> <b>documents</b> [...]|$|R
50|$|In addition, {{there is}} a <b>similar</b> <b>document</b> {{entitled}} A Secular Humanist Declaration published in 1980 by the Council for Secular Humanism.|$|R
