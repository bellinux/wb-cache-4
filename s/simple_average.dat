439|1095|Public
2500|$|If all {{outcomes}} [...] are equiprobable (that is, [...] ), {{then the}} weighted average {{turns into the}} <b>simple</b> <b>average.</b> This is intuitive: the expected value of a random variable is the average of all values it can take; thus the expected value is what one expects to happen on average. If the outcomes [...] are not equiprobable, then the <b>simple</b> <b>average</b> must be replaced with the weighted average, which {{takes into account the}} fact that some outcomes are more likely than the others. The intuition however remains the same: the expected value of [...] is what one expects to happen on average.|$|E
50|$|If the variances of the {{measurements}} are all equal, then the inverse-variance weighted average becomes the <b>simple</b> <b>average.</b>|$|E
5000|$|The <b>simple</b> <b>average</b> {{convection}} coefficient formulation {{can be replaced}} with a formula incorporating the power law relationship : ...|$|E
2500|$|For example, {{consider}} a non-orthogonal matrix {{for which the}} <b>simple</b> <b>averaging</b> algorithm takes seven steps ...|$|R
40|$|We give a {{lower bound}} on the {{spectral}} gap {{for a class}} of binary collision processes. In 2008, Caputo showed that, for a class of binary collision processes given by <b>simple</b> <b>averages</b> on the complete graph, {{the analysis of the}} spectral gap of an $N$-component system is reduced to that of the same system for N= 3 In this paper, we give a comparison technique to reduce the analysis of the spectral gap of binary collision processes given by <b>simple</b> <b>averages</b> on $d$-dimensional lattice to that on the complete graph. We also give a comparison technique to reduce the analysis of the spectral gap of binary collision processes which are not given by <b>simple</b> <b>averages</b> to that given by <b>simple</b> <b>averages.</b> Combining them with Caputo's result, we give a new and elementary method to obtain spectral gap estimates. The method applies to a number of binary collision processes on the complete graph and also on d-dimensional lattice, including a class of energy exchange models which was recently introduced in by Grigo et al., and zero-range processes. Comment: 17 page...|$|R
3000|$|Obtain rapid approximations of {{the missing}} {{pairwise}} distances using <b>simple</b> <b>averaging</b> for available measurements corresponding to the pair of nodes, yielding [...]...|$|R
50|$|In fluid dynamics, stream thrust {{averaging}} is {{a process}} used to convert three-dimensional flow through a duct into one-dimensional uniform flow. It makes the assumptions that the flow is mixed adiabatically and without friction. However, due to the mixing process, there is a net increase in the entropy of the system. Although there {{is an increase in}} entropy, the stream thrust averaged values are more representative of the flow than a <b>simple</b> <b>average</b> as a <b>simple</b> <b>average</b> would violate the second Law of Thermodynamics.|$|E
5000|$|If all {{outcomes}} [...] are equiprobable (that is, [...] ), {{then the}} weighted average {{turns into the}} <b>simple</b> <b>average.</b> This is intuitive: the expected value of a random variable is the average of all values it can take; thus the expected value is what one expects to happen on average. If the outcomes [...] are not equally probable, then the <b>simple</b> <b>average</b> must be replaced with the weighted average, which {{takes into account the}} fact that some outcomes are more likely than the others. The intuition however remains the same: the expected value of [...] is what one expects to happen on average.|$|E
50|$|Diffusion index -- {{calculated}} for each item with the overall CMI being a <b>simple</b> <b>average</b> of the 10 categories. Survey responses for each item capture the change—higher, lower or the same. The results are compared contrasting the current period/month to the previous one.|$|E
30|$|Fuzzy c-means {{clustering}} {{has been}} preferred over <b>simple</b> <b>averaging</b> of the ranks {{obtained from the}} experts. This is because FCM accommodates the fuzziness in the data collected from the experts. Moreover, <b>simple</b> <b>averaging</b> can result in same prioritized rank {{for more than one}} issue which can be avoided using FCM which does fulfill the objective of prioritization of the issues which affect or influence transboundary water sharing. Since all the experts have different backgrounds, they all view the same problem in a different way. So <b>simple</b> <b>averaging</b> cannot be the procedure that can be employed in case of prioritization of issues as it cannot deal with the fuzziness caused due to the variety in experts. As the main objective is prioritization of the issues, the theory and mathematics of FCM have not been dealt in detail. The utility of FCM has been used in prioritizing in the present study.|$|R
3000|$|... create {{analyses}} of data (including climate and health data) ranging from <b>simple</b> <b>averaging</b> to more advanced Empirical Orthogonal Function (EOF) analyses using the Ingrid programming language; [...]...|$|R
40|$|We {{compare the}} {{performance}} of averaged regularized estimators. We show that the improvement in performance which {{can be achieved by}} averaging depends critically on the degree of regularization which is used in training the individual estimators. We compare four different <b>averaging</b> approaches: <b>simple</b> <b>averaging,</b> bagging, variance-based weighting and variance-based bagging. In any of the averaging methods the greatest degree of improvement —if compared to the individual estimators — is achieved if no or only a small degree of regularization is used. Here, variance-based weighting and variance-based bagging are superior to <b>simple</b> <b>averaging</b> or bagging. Our experiments indicate that better performance for both individual estimators and for averaging is achieved in combination with regularization. With increasing degrees of regularization, the two bagging-based approaches (bagging, variance-based bagging) outperform the individual estimators, <b>simple</b> <b>averaging,</b> as well as variance-based weighting. Bagging and variance-based bagging seem to be the overall best combining methods over a wide range of degrees of regularization...|$|R
50|$|One {{possible}} unreal {{solution for}} the stream thrust averaged velocity yields a negative entropy. Another method of determining the proper solution {{is to take a}} <b>simple</b> <b>average</b> of the velocity and determining which value is closer to the stream thrust averaged velocity.|$|E
5000|$|The {{winner of}} each {{competition}} {{is determined by}} a <b>simple</b> <b>average</b> of the average speeds over the three distances. This average is not an average speed, since the distances {{are not used to}} weight the average. The table below shows thissimple average as ``average speed``.|$|E
50|$|The UUCW {{is one of}} {{the factors}} that {{contribute}} {{to the size of the}} software being developed. It is calculated based on the number and complexity of the use cases for the system. To find the UUCW for a system, each of the use cases must be identified and classified as <b>Simple,</b> <b>Average</b> or Complex based on the number of transactions the use case contains. Each classification has a predefined weight assigned. Once all use cases have been classified as <b>simple,</b> <b>average</b> or complex, the total weight (UUCW) is determined by summing the corresponding weights for each use case. The following chart shows the different classifications of use cases based on the number of transactions and the weight value assigned for each use case within the classification.|$|E
40|$|We {{explore the}} {{benefits}} of forecast combinations based on forecast- encompassing tests compared to <b>simple</b> <b>averages</b> and to Bates-Granger combinations. We also consider a new combination method that fuses test-based and Bates-Granger weighting. For a realistic simulation design, we generate multivariate time-series samples from a macroe- conomic DSGE-VAR model. Results generally support Bates-Granger over uniform weighting, whereas benefits of test-based weights depend on the sample size and on the prediction horizon. In a corresponding application to real-world data, <b>simple</b> <b>averaging</b> performs best. Uni- form averages may be the weighting scheme that is most robust to empirically observed irregularities...|$|R
40|$|A 3 -D hybrid ice-sheet {{model is}} applied to the last deglacial retreat of the West Antarctic Ice Sheet over the last [*]∼[*]  20   000  yr. A large {{ensemble}} of 625  model runs is used to calibrate the model to modern and geologic data, including reconstructed grounding lines, relative sea-level records, elevation–age data and uplift rates, with an aggregate score computed for each run that measures overall model–data misfit. Two types of statistical methods are used to analyze the large-ensemble results: <b>simple</b> <b>averaging</b> weighted by the aggregate score, and more advanced Bayesian techniques involving Gaussian process-based emulation and calibration, and Markov chain Monte Carlo. The analyses provide sea-level-rise envelopes with well-defined parametric uncertainty bounds, but the <b>simple</b> <b>averaging</b> method only provides robust results with full-factorial parameter sampling in the large ensemble. Results for best-fit parameter ranges and envelopes of equivalent sea-level rise with the <b>simple</b> <b>averaging</b> method agree well with the more advanced techniques. Best-fit parameter ranges confirm earlier values expected from prior model tuning, including large basal sliding coefficients on modern ocean beds...|$|R
30|$|As can be seen, all the adopted {{weighted}} averaging methods give {{better results}} {{compared to the}} <b>simple</b> <b>averaging</b> (Sum) rule. Also, among the weighted averaging methods, a better performance is achieved using the LDA method.|$|R
50|$|The {{concept of}} the bulk {{temperature}} is that adiabatic mixing of the fluid from a given {{cross section of the}} duct will result in some equilibrium temperature that accurately reflects the average temperature of the moving fluid, more so than a <b>simple</b> <b>average</b> like the film temperature.|$|E
50|$|The mktor {{scale is}} a 15-item, 7-point Likert-type scale, with all points speciﬁed. In this measure, market {{orientation}} is conceptualised as a one-dimensional construct, with three components, namely: customer orientation, competitor orientation, and interfunctional coordination. The <b>simple</b> <b>average</b> {{of the scores}} of the three components is the market orientation score.|$|E
50|$|In general, super-sampling is a {{technique}} of collecting data points at a greater resolution (usually by a power of two) than the final data resolution. These data points are then combined (down-sampled) to the desired resolution, often just by a <b>simple</b> <b>average.</b> The combined data points have less visible aliasing artifacts (or moiré patterns).|$|E
50|$|Location of {{residing}} objects gets reported moving, {{as soon as}} {{the measures}} taken are biased by secondary path reflections with increasing weight over time. Such effect is caused by <b>simple</b> <b>averaging</b> and the effect indicates insufficient discrimination of first echoes.|$|R
40|$|A merged land–air–sea surface {{temperature}} reconstruction analysis is developed for monthly anomalies. The reconstruction is global and spatially complete. Reconstructed anomalies damp toward zero in regions with insufficient sampling. Error estimates {{account for the}} damping associated with sparse sampling, and also for bias uncertainty in both the land and sea observations. Averages of the reconstruction are similar to <b>simple</b> <b>averages</b> of the unanalyzed data {{for most of the}} analysis period. For the nineteenth century, when sampling is most sparse and the error estimates are largest, the differences between the averaged recon-struction and the <b>simple</b> <b>averages</b> are largest. Sampling is always sparse poleward of 60 ° latitude, and historic reconstructions for the polar regions should be used with caution. 1...|$|R
40|$|This {{report is}} related to the {{analysis}} of the HART II (Higher Harmonic Control [=HHC] Aeroacous-tic Rotor Test II) microphone data with respect to the conditional averaging method. Up to now only <b>simple</b> <b>averaging</b> of the instantaneous time histories was used, but without con-sideration of differences caused by fluctuations in rotational speed and varying vortex locations and blade vortex interactions. This so called time jitter has an important influence on the aver-aged time history since by only <b>simple</b> <b>averaging</b> the peak-to-peak magnitude is reduced and the azimuthal difference between minimum and maximum peaks is falsified. This happens even in the case of identical individual time histories in each revolution, just with a scatter in its azi-muthal location caused by fluctuations...|$|R
50|$|The GII is {{computed}} {{by taking}} a <b>simple</b> <b>average</b> of the scores in two sub-indices, the Innovation Input Index and Innovation Output Index, which are composed of five and two pillars respectively. Each of these pillars describe an attribute of innovation, and comprise up to five indicators, and their score is calculated by the weighted average method.|$|E
5000|$|... #Caption: The {{better the}} linear {{regression}} (on the right) fits {{the data in}} comparison to the <b>simple</b> <b>average</b> (on the left graph), the closer the value of [...] is to 1. The areas of the blue squares represent the squared residuals with respect to the linear regression. The areas of the red squares represent the squared residuals with respect to the average value.|$|E
5000|$|The Knowledge Index or KI is an {{economic}} indicator prepared by the World Bank Institute to measure a country s ability to generate, adopt and diffuse knowledge. Methodologically, the KI is the <b>simple</b> <b>average</b> of the normalized performance scores of {{a country or region}} on the key variables in three Knowledge Economy pillars - education and human resources, the innovation system and information and communication technology (ICT) ...|$|E
40|$|We {{investigate}} model uncertainty {{associated with}} predictive regressions employed in asset return forecasting research. We use simple combination and Bayesian model averaging (BMA) techniques {{to compare the}} performance of these forecasting approaches in short-vs. long-run horizons of S&P 500 monthly excess returns. <b>Simple</b> <b>averaging</b> involves an equally-weighted averaging of the forecasts from alternative combinations of factors used in the predictive regressions, whereas BMA involves computing the predictive probability that each model is the true model and uses these predictive probabilities as weights in combing the forecasts from different models. From a given set of multiple factors, we evaluate all possible pricing models to the extent, which they describe the data as dictated by the posterior model probabilities. We find that, while <b>simple</b> <b>averaging</b> compares quite favorably to forecasts derived from a random walk model with drift (using a 10 -year out-of-sample iterative period), BMA outperforms <b>simple</b> <b>averaging</b> in longer compared to shorter forecast horizons. Moreover, we find {{further evidence of the}} latter when the predictive Bayesian model includes shorter, rather than longer lags of the predictive factors. An interesting outcome of this study tends to illustrate the power of BMA in suppressing model uncertainty through model as well as parameter shrinkage, especially when applied to longer predictive horizons. ...|$|R
40|$|At {{precision}} agriculture {{data analysis}} workshops {{there are often}} diverging views {{on the kind of}} statistics that is appropriate for sensor based or remotely sensed data such as yield monitor data and imagery. One view is that yield monitor data represent a sample. In that case, some sort of sampling theory applies and well-known tools from spatial statistics and geostatistics should be utilized. The concurrent view is that precision agriculture data on yields actually represent the population; the use of sample statistics is therefore not pertinent and <b>simple</b> <b>averages</b> or proportions referring to this one state-of-the-world can be calculated without having to evoke statistical inference. At first sight, it may seem natural to believe that taking <b>simple</b> <b>averages</b> from treatment blocks and comparing these to other averages provides useful decision making information. Many advocates of the idea that precision agriculture data are population data, farmers and researchers alike, argue that <b>simple</b> <b>averages</b> of yield monitor data are sufficient for whole-farm decision making purposes. Unfortunately this is not the case. Standard farm-level software therefore often includes tools that summarize yield or other site-specific data into classes according to soils or other predefined management zones. Although there are many degrees of freedom with site-specific yield monitor data, this doe...|$|R
40|$|Aeronautical {{aerodynamics}} {{is characterized}} by compressible ows at high Reynolds numbers, so that the turbulence modeling is essential for predicting the ow eld in agreement with experi-ments. A standard approach for this purpose {{is based on the}} so-called Reynolds-averaged Navier-Stokes (RANS) equations, where for compressible ow <b>simple</b> <b>averages</b> and mass weighte...|$|R
50|$|The {{geometric}} mean decreases {{the level of}} substitutability between dimensions compared {{and at the same}} time ensures that a 1 percent decline in say life expectancy at birth has the same impact on the HDI as a 1 percent decline in education or income. Thus, as a basis for comparisons of achievements, this method is also more respectful of the intrinsic differences across the dimensions than a <b>simple</b> <b>average.</b>|$|E
50|$|To measure {{systemic}} stability, {{a number}} of studies attempt to aggregate firm-level stability measures (z-score and distance to default) into a system-wide evaluation of stability, either by taking a <b>simple</b> <b>average</b> or weighing each measure by the institution’s relative size. However, these aggregate measures fail to account for correlated risks among financial institutions. In other words, the model fails to consider the inter-connectedness between institutions, and that one institution’s failure can lead to a contagion.|$|E
5000|$|India Becoming, by the Harvard University trained social anthropologist, Akash Kapur, is an {{in-depth}} study of <b>simple</b> <b>average</b> {{people from a}} nondescript town in South India, their mundane life and their struggle to survive the vicissitudes of the burgeoning society that modernization has thrown their way. In this debut attempt, he chose to go to such great lengths to depict the real-life stories and experiences of people that it took him five years to finish the book.|$|E
30|$|It is a {{price index}} (<b>simple</b> {{arithmetic}} <b>average</b> of courses).|$|R
50|$|Note {{that the}} {{unweighted}} term indicates that all distances contribute equally to each average that is computed {{and does not}} refer to the math by which it is achieved. Thus the <b>simple</b> <b>averaging</b> in WPGMA produces a weighted result and the proportional averaging in UPGMA produces an unweighted result (see the working example).|$|R
40|$|A <b>Simple</b> <b>Averaging</b> Mode (SAM) of {{impression}} formation {{was developed}} as an institutional demonstration. One-digit demographic or descriptive traits and an affective reaction are input for each perceived encounter. The model uses a weighted averaging procedure {{similar to that}} suggested by Anderson (1965), to combine these trait perceptions into an affective response. Traits are weighted according to (a) the frequency of past experience relative to present encounters, (b) {{the degree to which}} experiences observed can be matched to existing traits of the perceived person, and (c) as internal program state or "mood. " EDUCATIONAL AND PSYCHOLOGICAL MEASUREMENT 1976, 36, 215 - 216. THIS paper describes the <b>Simple</b> <b>Averaging</b> Model (SAM) which was developed to demonstrate impression-formation computer mod-eling with less complex and less expensive procedures than are re-quired by more established programs such as ALDOUS (Loehlin...|$|R
