127|267|Public
2500|$|Roots, in particular, display {{observable}} swarm behavior, {{growing in}} patterns that exceed the <b>statistical</b> <b>threshold</b> for random probability, and {{indicate the presence}} of communication between individual root apexes. The primary function of plant roots is the uptake of soil nutrients, {{and it is this}} purpose which drives swarm behavior. Plants growing in close proximity have adapted their growth to assure optimal nutrient availability. This is accomplished by growing in a direction that optimizes the distance between nearby roots, thereby increasing their chance of exploiting untapped nutrient reserves. The action of this behavior takes two forms: maximization of distance from, and repulsion by, neighboring root apexes. The transition zone of a root tip is largely responsible for monitoring for the presence of soil-borne hormones, signaling responsive growth patterns as appropriate. Plant responses are often complex, integrating multiple inputs to inform an autonomous response. Additional inputs that inform swarm growth includes light and gravity, both of which are also monitored in the transition zone of a root's apex. These forces act to inform any number of growing [...] "main" [...] roots, which exhibit their own independent releases of inhibitory chemicals to establish appropriate spacing, thereby contributing to a swarm behavior pattern. Horizontal growth of roots, whether in response to high mineral content in soil or due to stolon growth, produces branched growth that establish to also form their own, independent root swarms.|$|E
3000|$|In children, {{experience}} with SPM in epilepsy is more limited. Only one study formally assessed {{the role of}} <b>statistical</b> <b>threshold</b> on sensitivity and specificity [...]...|$|E
30|$|If p in Equation  1 {{is smaller}} than a given, predetermined, <b>statistical</b> <b>threshold</b> θ the link is {{detected}} as statistically {{not consistent with the}} null hypothesis of a uniform distribution and therefore is preserved in the filtered network, otherwise it is deleted. Differently than in [10] we fix the univariate <b>statistical</b> <b>threshold</b> θ first and then perform a multiple hypothesis test correction. In fact, due to the large number of tests needed to investigate the entire network, a multiple hypothesis test correction is needed in order to minimize the number of false positive. In the present study we set our threshold to the value θ= 0.01.|$|E
50|$|These {{arbitrary}} <b>statistical</b> <b>thresholds</b> {{create a}} discontinuity, causing unnecessary confusion and artificial controversy.|$|R
5000|$|Nolan, H., Whelan, R., & Reilly, R.B. [...] "FASTER: Fully Automated <b>Statistical</b> <b>Thresholding</b> for EEG {{artifact}} Rejection". Journal of Neuroscience Methods 192 (1): 152-162 (2010) ...|$|R
50|$|Consider the {{scenario}} in finance {{where there is}} a need to understand historic price volatility to determine <b>statistical</b> <b>thresholds</b> of future price movements. This is helpful for both trade models and transaction cost analysis.|$|R
30|$|Values were {{expressed}} as mean ± SD. Comparisons {{were made by}} using one-factor ANOVA and multiple comparisons between ventilators by using Tukey test. Significant <b>statistical</b> <b>threshold</b> was set to P < 0.001 {{to take into account}} the number of statistical tests performed.|$|E
40|$|Reconstruction of gene {{regulatory}} networks {{based on}} experimental data usually relies on statistical evidence, necessitating {{the choice of}} a <b>statistical</b> <b>threshold</b> which defines a significant biological effect. Approaches to this problem found in the literature range from rigorous multiple testing procedures to ad hoc P -value cut-off points. However, when the data implies graphical structure, {{it should be possible to}} exploit this feature in the threshold selection process. In this article we propose a procedure based on this principle. Using coding theory we devise a measure of graphical structure, for example, highly connected nodes or chain structure. The measure for a particular graph can be compared to that of a random graph and structure inferred on that basis. By varying the <b>statistical</b> <b>threshold</b> the maximum deviation from random structure can be estimated, and the threshold is then chosen on that basis. A global test for graph structure follows naturally. </p...|$|E
40|$|We {{investigate}} the dopant model employed in drift-diffusion device simulations {{for the study}} of <b>statistical</b> <b>threshold</b> voltage variations associated with discrete random dopants. It is pointed out that the conventional dopant model, when extended to the extreme "atomistic" regime, becomes physically inconsistent with the length-scale presumed in drift-diffusion simulations. Splitting the Coulomb potential of localized dopants between the long-range and short-range parts, we propose a dopant model appropriate for three-dimensional drift-diffusion simulations...|$|E
40|$|With {{the extreme}} {{dimensionality}} of functional neuroimaging data comes extreme risk for false positives. Across the 130, 000 voxels {{in a typical}} fMRI volume the probability {{of at least one}} false positive is almost certain. Proper correction for multiple comparisons should be completed during the analysis of these datasets, but is often ignored by investigators. To highlight the danger of this practice we completed an fMRI scanning session with a post-mortem Atlantic Salmon as the subject. The salmon was shown the same social perspectivetaking task that was later administered to a group of human subjects. Statistics that were uncorrected for multiple comparisons showed active voxel clusters in the salmon’s brain cavity and spinal column. Statistics controlling for the familywise error rate (FWER) and false discovery rate (FDR) both indicated that no active voxels were present, even at relaxed <b>statistical</b> <b>thresholds.</b> We argue that relying on standard <b>statistical</b> <b>thresholds</b> (p 8) is an ineffective control for multiple comparisons. We further argue {{that the vast majority of}} fMRI studies should be utilizing proper multiple comparisons correction as standard practice when thresholding their data...|$|R
40|$|Abstract- We {{consider}} one of {{the problems}} of detecting the amount of bone reposition in treating peri apical lesions in dentistry. A new edge detection algorithm which uses multi scale and multi directional analysis along with a <b>statistical</b> <b>thresholding</b> is used in this paper for perfect localization and detection of lesion boundary. The results obtained from the proposed method are compared with the most popular Canny edge detector and are found to be more appreciable...|$|R
40|$|Abstract:Speech {{recognition}} {{systems have}} been applied in real world applications for several decades，where {{there should be an}} unsatisfactory recognition performance under various noise conditions，particularly in lower signal-to-noise ratio (SNR) circumstances ． In this paper，we propose a <b>statistical</b> <b>thresholding</b> method for mean and variance normalization technique，further reducing the mismatch between training and testing environments，which makes an automatic speech rec-ognition system more robust to environmental changes ． Mel-frequency cepstrum coefficient(MFCC) features are extracted as acoustic features，and they are further normalized with the mean and variance normalization method to get the cepstral mean and variance normalization (CMVN) features ． The proposed <b>statistical</b> <b>thresholding</b> method is then applied ． The viability of the proposed approach was verified in various experiments with different types of background noises at different SNR levels． In an isolated word recognition task，the experimental results show that the proposed approach reduced the error rate by over 40 % in some cases compared with the baseline MFCC front-end，and under lower SNR conditions the proposed method also outperforms other robust features such as cepstral mean subtraction (CMS) and CMVN． Key words:robust;feature extraction;mean subtraction;mean and variant normalization;Mel-frequency cepstrum coeffi-cient(MFCC);statistical thresholding;speech recognitio...|$|R
30|$|Many {{groups have}} {{proposed}} solutions {{to verify the}} presence of ERPs in individual subjects, and even single epochs, by applying techniques such as wavelet analysis [9 – 11], independent component analysis [12], integrated waveforms [13], and nonparametric analyses [14]. In some very specific clinical situations, the approach of repeating stimulation until a <b>statistical</b> <b>threshold</b> is reached [15], or using basic t-scores to evaluate the presence or {{the absence of a}} particular component [16, 17], has been examined.|$|E
40|$|Using 3 D {{simulations}} of statistical ensembles of unprecedented size, we have studied <b>statistical</b> <b>threshold</b> voltage variations {{induced by the}} combined effects of random dopants and line edge roughness {{in a state of}} the art 35 nm MOSFET. Statistical samples of 10 5 microscopically different transistors have been simulated. Based on careful statistical analysis of the simulation results we have developed statistical enhancement techniques, which deliver a high degree of statistical accuracy at a greatly reduced computational cost...|$|E
3000|$|In our {{previous}} work (Kafaf and Kim 2017), we presented a two-phase approach {{based on the}} traditional kNN algorithm for making a self-adaptive decision using SOAP for web services. The work in this paper improves the previous work by using a three-phase approach based on B-kNN (with <b>statistical</b> <b>threshold</b> detection) using REST for web services to improve precision and efficiency. The overhead introduced by the additional phase is outweighed by the efficiency gained by B-kNN and REST as discussed in [...] "Validation" [...] section.|$|E
40|$|A {{new method}} {{to improve the}} {{signal-to-noise}} ratio of single evoked potentials (EP) measurements is presented, in which, contrary to previous methods, no a priori assumptions on the signal are necessary. This method {{is based on the}} wavelets decomposition of the individual signals. A <b>statistical</b> <b>thresholding</b> is applied on the coefficients of the decomposition: we estimate whether the mean value of the coefficients across trials and for each time point is significantly different from a random estimate. The performance of the method is evaluated against similar ones with simulations and the method is applied to real dat...|$|R
40|$|ISBN : 978 - 2 - 9532965 - 0 - 1 A {{new method}} {{to improve the}} {{signal-to-noise}} ratio of single evoked potentials (EP) measurements is presented, in which, contrary to previous methods, no a priori assumptions on the signal are necessary. This method {{is based on the}} wavelets decomposition of the individual signals. A <b>statistical</b> <b>thresholding</b> is applied on the coefficients of the decomposition: we estimate whether the mean value of the coefficients across trials and for each time point is significantly different from a random estimate. The performance of the method is evaluated with simulation and the method is applied to real dat...|$|R
40|$|Abstract- Tracking {{the motion}} of cells in culture is a task, which often still is {{undertaken}} manually, and for which automated methods are strongly desirable. Researchers visually perform cell motion analysis, observe cell move-ments and cell shape changes for hours to discover when, where and how fast it moves, splits or dies. Hematopoietic Stem Cells (HSCs) proliferate and differentiate to different blood cell types continuously during their lifetime, and are of substantial interest in gene therapy, cancer, and stem-cell research. In this paper a statistical method is introduced to track HSCs over time. A <b>statistical</b> <b>thresholding</b> method is combined with joint probabilistic data association in the proposed HSC tracker...|$|R
40|$|BACKGROUND AND PURPOSE: Functional MR imaging (fMRI) {{is used to}} {{determine}} preoperatively the laterality of cortical language representation along with the relationship of language areas to adjacent brain tumors. The {{purpose of this study}} was {{to determine}} whether changing the <b>statistical</b> <b>threshold</b> for different language tasks influences the language laterality index (LI) for a group of controls, patients with tumor without prior surgery, and patients with tumor and prior surgery. MATERIALS AND METHODS: Seven controls, 9 patients with tumor without prior surgery, and 4 patients with tumor and prior surgery performed verb-generation, phonemic fluency, and semantic fluency language tasks during fMRI. Interhemispheric activation differences between the left and right Broca regions of interest were determined by calculating language LIs. LIs were compared within each group, between groups, and between language tasks. Intraoperative electrocortical mapping or the presence of aphasia during postoperative neurology examinations or both were used as ground truth. RESULTS: The language LI varied as a result of statistical thresholding, presence of tumor, prior surgery, and language task. Although patients and controls followed a similar shape in the LI curve, there was no optimal P value for determining the LI. Three patients demonstrated a shift in the LI between hemispheres as a function of <b>statistical</b> <b>threshold.</b> Verb generation was the least variable tas...|$|E
40|$|We study, in detail, <b>statistical</b> <b>threshold</b> voltage {{variability}} {{in a state}} of the art n-channel MOSFET introduced by line edge roughness. A large sample of 35, 000 transistors with microscopically different LER patterns was simulated using the Glasgow 3 D `atomistic' device simulator. Such large-scale simulation has been enabled by advanced grid computing technology. The results show that the statistical distribution of threshold voltage due to LER is asymmetric. Detailed analysis of the simulation results provide in depth understanding of the physical mechanisms governing LER induced statistical variability...|$|E
40|$|International audienceGraph {{theory is}} a {{powerful}} mathematical tool recently introduced in neuroscience field for quantitatively describing the main properties of investigated connectivity networks. Despite the technical advancements provided {{in the last few}} years, further investigations are needed for overcoming actual limitations in the field. In fact, the absence of a common procedure currently applied for the extraction of the adjacency matrix from a connectivity pattern has been leading to low consistency and reliability of ghaph indexes among the investigated population. In this paper we proposed a new approach for adjacency matrix extraction based on a <b>statistical</b> <b>threshold</b> as valid alternative to empirical approaches, extensively used in Neuroscience field (i. e. fixing the edge density). In particular we performed a simulation study for investigating the effects of the two different extraction approaches on the topological properties of the investigated networks. In particular, the comparison was performed on two different datasets, one composed by uncorrelated random signals (null-model) and the other one by signals acquired on a mannequin head used as a phantom (EEG null-model). The results highlighted the importance to use a <b>statistical</b> <b>threshold</b> for the adjacency matrix extraction in order to describe the real existing topological properties of the investigated networks. The use of an empirical threshold led to an erroneous definition of small-world properties for the considered connectivity patterns...|$|E
50|$|Dedicated CGH {{software}} is commercially {{available for the}} image processing step, and is required to subtract background noise, remove and segment materials not of chromosomal origin, normalize the fluorescence ratio, carry out interactive karyotyping and chromosome scaling to standard length. A “relative copy number karyotype” which presents chromosomal areas of deletions or amplifications is generated by averaging the ratios {{of a number of}} high quality metaphases and plotting them along an ideogram, a diagram identifying chromosomes based on banding patterns. Interpretation of the ratio profiles is conducted either using fixed or <b>statistical</b> <b>thresholds</b> (confidence intervals). When using confidence intervals, gains or losses are identified when 95% of the fluorescence ratio does not contain 1.0.|$|R
40|$|Oedema is fluid {{retention}} within the myocardial tissue due to damage tissue causing swelling {{in the affected}} area after myocardial infarction (MI). Quantification of oedema area after an MI {{is an important step}} in medical prognosis to differentiate between viable and death myocardial tissue. In this paper a novel technique of Hybrid Thresholding Oedema Sizing Algorithm (HTOSA) is presented. To quantify the oedema a hybrid technique based on combination of morphological operation combined with <b>statistical</b> <b>thresholding</b> is used. The performance of the method was tested on real T 2 weighted MRI data. The quantitative result of the automatic method compare to manual segmentation by a skill clinician is very encouraging with correlation score of 81. 1 %. 1...|$|R
40|$|Recent {{advances}} in functional neuroimaging techniques have prompted {{an increase in}} the number of studies investigating lateralization of language functions. One of the problems in relating findings of various studies to one another is the diversity of reported results. This may be due to differences in the tasks that are used to stimulate language processing regions and in the control tasks, as well as differences in the way imaging data are analyzed, in particular the threshold for significance of signal change. We present a simple method to assess language lateralization that allows for some variation of tasks and <b>statistical</b> <b>thresholding,</b> but at the same time yields reliable and reproducible results. Images acquired during a set of word-comprehensio...|$|R
40|$|In this paper, a Euclidean {{distance}} based minutia {{matching algorithm}} is proposed to improve the matching accuracy in fingerprint verification system. This algorithm extracts matched minutia pairs from input and template fingerprints by using the smallest minimum sum of closest Euclidean distance (SMSCED), corresponding rotation angle and empirically chosen <b>statistical</b> <b>threshold</b> values. Instead of using the minutia type and orientation angle, which are widely employed in existing algorithms, the proposed algorithm uses only the minutia location, to reduce the effect of non-linear distortion. Experimental {{results show that the}} proposed method has higher accuracy with improved verification rate and rejection rate...|$|E
40|$|This paper {{describes}} a background-subtraction system with light change-detection which {{works on a}} luminance QCIF-size video signal for surveillance applications. The new proposed pixel background model is controlled by a <b>statistical</b> <b>threshold</b> and is robust for cluttered background and small object motions. Moreover, (or light-change detection, we introduce temporal prediction of pixel values to estimate trends while quickly adapting to scene changes to facilitate a very sensitive detection of moving targets. Experiments show that a local contrast enhancement applied prior to down-sampling improves detection sensitivity, arid combined with the shifted sealed difference and me Wronskian determinant operators provides the best background/foreground detectio...|$|E
40|$|This work {{investigates the}} very low {{frequency}} (VLF) modulation of QRS slopes and heart rate variability (HRV). Electrocardiogram (ECG) and respiratory flow signal were acquired from patients with dilated cardiomyopathy and ischemic cardiomyopathy. HRV {{as well as the}} upward QRS slope (IUS) and downward QRS slope (IDS) were extracted from the ECG. The relation between HRV and QRS slopes in the VLF band was measured using ordinary coherence in 5 -minute segments. Partial coherence was then used to remove the influence that respiration simultaneously exerts on HRV and QRS slopes. A <b>statistical</b> <b>threshold</b> was determined, below which coherence values were considered not to represent a linear relation. Postprint (published version...|$|E
40|$|A fully {{automated}} and online artifact removal method for the electroencephalogram (EEG) is developed {{for use in}} brain-computer interfacing. The method (FORCe) is based upon a novel combination of wavelet decomposition, independent component analysis, and thresholding. FORCe is able to operate on a small channel set during online EEG acquisition and does not require additional signals (e. g. electrooculogram signals). Evaluation of FORCe is performed offline on EEG recorded from 13 BCI particpants with cerebral palsy (CP) and online with three healthy participants. The method outperforms the state-of the-art automated artifact removal methods Lagged auto-mutual information clustering (LAMIC) and Fully automated <b>statistical</b> <b>thresholding</b> (FASTER), {{and is able to}} remove a wide range of artifact types including blink, electromyogram (EMG), and electrooculogram (EOG) artifacts...|$|R
40|$|Radio {{frequency}} interference {{detection and}} mitigation are becoming {{of paramount importance}} due to {{the increasing number of}} services and applications based on the position obtained by means of Global Navigation Satellite Systems. A way to cope with such threats is the implementation in the receiver of advanced signal processing algorithm able to raise proper warning or improve the receiver performance. In this paper, we propose a method based on the Wavelet Transform able to split the useful signal from the interfering component in a transformed domain. The wavelet packet decomposition and proper <b>statistical</b> <b>thresholds</b> allow the algorithm to show very good performance in case of multiple pulse interference {{as well as in the}} case of narrowband interference, two scenarios in which traditional countermeasures might not be effectiv...|$|R
40|$|Traditional <b>statistical</b> <b>thresholding</b> methods, {{directly}} {{constructing the}} optimal threshold criterion using the class variance, have certain versatility but lack {{the specificity of}} practical application in some cases. To select the optimal threshold for infrared image thresholding, a simple and efficient method based on cloud model is proposed. The method firstly generates the cloud models corresponding to image background and object, respectively, and defines a novel threshold dependence criterion related with the hyper-entropy of these cloud models and then determines the optimal grayscale threshold by the minimization of this criterion. It is indicated by the experiments that, compared with selected methods, using both image thresholding and target detection, the proposed method is suitable for infrared image thresholding since it performs good results and is reasonable and effective...|$|R
40|$|We used {{high-resolution}} fMRI {{to investigate}} claims that {{learning to read}} r !sults in greater left occipito-temporal (OT) activation for written words relative to pictures of objects. In tl e first experiment, 9 / 16 subjects performing a one-back task showed activation in >= 1 left OT voxel for word: relative to pictures (P = 1 left OT voxel for words relative to pictures. However, at this low <b>statistical</b> <b>threshold</b> false positives need to be excluded. The semantic decision paradigm was therefore repeated, withii subject, in two different scanners (1. 5 and 3 T). Both scanners consistently localised left OT activation for -, -ords relative to fixation and pictures relative to words, {{but there were no}} consistent effects for words relativ,, to pictures. Finally, in a third experiment, we minimised the voxel size (1. 5 X 1. 5 X 1. 5 mm 3) and demonst -ated a striking concordance between the voxels activated for words and pictures, irrespective of task (narni g vs. one-back) or script (English vs. Hebrew). In summary, although we detected differential activation for v ords relative to pictures, these effects: (i) do not withstand statistical rigour; (ii) do not replicate within or bi tween subjects; and (iii) are observed in voxels that also respond to pictures of objects. Our findings have in plications for the role of left OT activation during reading. More generally, they show that studies using lov, statistical thresholds in single subject analyses should correct the <b>statistical</b> <b>threshold</b> for the number of comparisons made or replicate effects within subjec...|$|E
40|$|This {{paper is}} in closed access until 22 nd Feb 2018. Extreme value {{analysis}} {{is an important}} tool for studying coastal flood risk, but requires the estimation of a threshold to define an ‘extreme’, which is traditionally undertaken visually. Such subjective judgement is not accurately reproducible, so recently a number of quantitative approaches have been proposed. This paper therefore reviews existing methods, illustrated with coastal tide-gauge data and the Generalized Pareto Distribution, and proposes a new automated method that mimics the enduringly popular visual inspection method. In total five different types of <b>statistical</b> <b>threshold</b> selection and their variants are evaluated by comparison to manually derived thresholds, demonstrating that the new method is a useful, complementary tool...|$|E
40|$|Using {{full-scale}} 3 -D {{simulations of}} 100 000 s of devices, enabled by "push-button" cluster technology, we study in detail <b>statistical</b> <b>threshold</b> voltage variability in a state-of-the-art n-channel MOSFET {{introduced by the}} combined effect of random discrete dopants (RDDs) and line edge roughness (LER) and demonstrate that the resulting distribution is non-normal. Based on careful statistical analysis of the results, we show how the resulting distribution of V T can be constructed from the distributions arising from the individual simulation of RDD and LER variability. In accomplishing this task, we have deployed computationally efficient statistical enhancement techniques that drastically reduce the computational effort needed to accurately characterize threshold voltage variability under the combined influence of RDD and LER...|$|E
30|$|Statistical {{analyses}} were performed using SAS (version 9.3). Variables were expressed as mean with standard deviation for numerical variables and as frequencies and percentages for categorical variables. Groups were compared using Wilcoxon, Chi-square, or Fisher’s exact tests, as appropriate, with a <b>statistical</b> significance <b>threshold</b> of 0.05.|$|R
40|$|Viability {{assessment}} of heart muscle after a myocardial infarction {{is an important}} step for diagnosis and therapy planning. It is important to quantify the area of edema because it can differentiate between viable and death myocardial tissues. In this paper an automatic method to quantify cardiac edema is presented. The method is based on a combination of morphological operations and <b>statistical</b> <b>thresholding.</b> Using real MRI data it is demonstrated that the proposed method can delineate edema region comparable to manual segmentation with a linear correlation coefficient r= 0. 76 and the mean difference is around 9. 95 %. The quantification result is also used to generate 3 D visualisation model showing normal myocardial wall and edema region, which will enhance clinician diagnosis capability with real pattern of edema distribution and quantitative description...|$|R
40|$|The inverse energy cascade,which {{is one of}} the {{important}} phenomen α to enhance the large scale flow instability in bubbly flow,is investigated by measuring a local two phase flow structure driven by buoyant bubbles using Particle Imaging Velocimetry (PIV). In the PIV,flow field of liquid phase is measured by separating an original image to respective two phase images using a <b>statistical</b> <b>thresholding</b> method for image parameters of bubble and particle. The present results obtained in case that bubble Reynolds number and average void fraction are less than 30 and 1. 5 % respectively,confilm the presence of a large energy-decaying with slope index of sharper than - 5 / 3 in log-log diagram of energy spectrum at high wavenumber region. Also an important relationship between the energy spectrum and bubble-bubble interval distance is detected...|$|R
