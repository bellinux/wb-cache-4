22|368|Public
50|$|In {{numerical}} analysis, multi-time-step integration, {{also referred}} to as multiple-step or asynchronous time integration, is a numerical time-integration method that uses different time-steps or time-integrators for different parts of the problem. There are different approaches to multi-time-step integration. They are based on domain decompostition and can be classified into strong (monolithic) or weak (staggered) schemes. Using different time-steps or time-integrators {{in the context of a}} weak algorithm is rather straightforward, because the numerical solvers operate independently. However, this is not the case in a <b>strong</b> <b>algorithm.</b> In the past few years a number of research articles have addressed the development of strong multi-time-step algorithms. In either case, strong or weak, the numerical accuracy and stability needs to be carefully studied. Other approaches to multi-time-step integration in the context of operator splitting methods have also been developed; i.e., multi-rate GARK method and multi-step methods for molecular dynamics simulations.|$|E
40|$|Today’s {{internet}} {{world is}} very competitive and {{to survive in}} such a competitive world {{there must be a}} secure environment to communicate. Internet and network applications are growing fast day by day. For this purpose there is a requirement of an efficient and <b>strong</b> <b>algorithm</b> which will provide strong encryption. This paper presents a detailed study of various secret key cryptographic algorithms...|$|E
40|$|Abstract—The LLL {{algorithm}} is <b>strong</b> <b>algorithm</b> that de-crypts the additional type Knapsack cryptosystem. However, the LLL {{algorithm is}} not applicable in the addition {{in the group}} that rational points of elliptic curves on finite fields do. Therefore, we think the Knapsack cryptosystem constructed on elliptic curves. By using the pairing for the decryption, it is shown {{to be able to}} make the computational complexity of the decryption a polynomial time by making the decryption function by the pairing value. I...|$|E
5000|$|COMP128-2 - <b>stronger</b> <b>algorithm</b> {{which still}} clears the 10 {{rightmost}} bits of Kc ...|$|R
25|$|The {{algorithm}} (and {{therefore the}} program code) is simpler than other algorithms, especially compared to <b>strong</b> <b>algorithms</b> that ensure {{a solution to}} the most difficult puzzles.|$|R
50|$|IPsec fully {{supports}} IPv6, AuthIP (which {{allows for}} a second authentication), integration with NAP for authenticating with a health certificate, Network Diagnostics Framework support for failed IPsec negotiation, new IPsec performance counters, and improved detection of cluster node failure and faster renegotiation of security associations. There is support for <b>stronger</b> <b>algorithms</b> for main mode negotiation (<b>stronger</b> DH <b>algorithms</b> and Suite B) and data integrity and encryption (AES with CBC, AES-GMAC, SHA-256, AES-GCM).|$|R
40|$|Security of data in army {{stations}} is {{an important}} issue. In early systems, {{at the time of}} information transmission between two army stations, it can be hacked by terrorists, spies and enemies. Cryptography is a very important system employed for this purpose. There are various types of algorithms available for encryption and decryption of data and new algorithms are evolving. Polyalphabetic substitution cipher is a <b>strong</b> <b>algorithm</b> used for security of data in army stations. In this paper, various techniques of security of data and one the algorithm using polyalphabetic substitution cipher are discussed...|$|E
40|$|Abstract — In this paper, {{we present}} a novel boosted robot vision control algorithm. The method {{utilizes}} on-line boosting to produce a strong vision-based robot control starting from two weak algorithms. The notion of weak and strong algorithms has been presented {{in the context of}} robot vision control, in presence of uncertainty in the measurement process. Appropriate probabilistic error functions are defined for the weak algorithm to evaluate their suitability in the task. An on-line boosting algorithm is employed to derive a final <b>strong</b> <b>algorithm</b> starting from two weak algorithms. This strong one has superior performance both in image and Cartesian spaces. Experiments justify this claim. I...|$|E
40|$|Abstract — We {{present a}} planner for {{addressing}} a difficult, yet under-investigated class of planning problems: Fully Observable Non-Deterministic planning problems with strong solutions. Our strong planner employs a new data structure, MRDAG (multi-root directed acyclic graph), to define how the solution space should be expanded. We further equip a MRDAG with heuristics to ensure planning towards the relevant search direction. We performed extensive experiments to evaluate MRDAG and the heuristics. Results show that our <b>strong</b> <b>algorithm</b> achieves impressive {{performance on a}} variety of benchmark problems: on average it runs more than three orders of magnitude faster than the state-of-the-art planners, MBP and Gamer, and demonstrates significantly better scalability...|$|E
50|$|By default, ONE-NET {{uses the}} Extended Tiny Encryption Algorithm (XTEA) version 2 with 32 {{iterations}} (XTEA2-32). The ONE-NET protocol provides extensions to even {{higher levels of}} encryption. Encryption is integral to the ONE-NET protocol, there are no unencrypted modes. Alternate encryption ID tag allows extension to <b>stronger</b> <b>algorithms.</b>|$|R
30|$|Next, we {{introduce}} a <b>strong</b> convergence <b>algorithm</b> for the split common solution problem.|$|R
30|$|In this section, we {{introduce}} two <b>strong</b> convergence <b>algorithms</b> for (HSP); see Theorem  4.1 and Theorem 4.2.|$|R
40|$|FPGA自動設計フローの中で配置処理は最も時間を費やす工程の一つである. 近年では,FPGAの性能向上によって実装回路の大規模化が進んでおり,配置処理の高速化は重要な課題となっている. 現在,その配置はSAに基づいて行われるのが一般的である. SAの並列化は多く研究されてきたが,SAは逐次性の強いアルゴリズムであり並列化にはあまり適していない. そこで,近年注目されているアントコロニー最適化法をFPGA配置問題に適用する. アントコロニー最適化法は複数のエージェントが解の探索を独立して行う点で,並列化による高速化などの効果が期待できる. 本研究では,PCクラスタ上でアントコロニー最適化法を用いたFPGA配置ツールの並列化を行い,その評価と検討を行う. Placement {{processing}} {{is one of}} {{the processes}} which spends time most in FPGA design automation flow. Accordingly FPGAs have improved circuit performance, the circuit scale that is implemented by FPGAs becomes larger. So the computation time devoted to placement has grown dramatically. Now, the placement based on SA is performed in VPR. SA is a <b>strong</b> <b>algorithm</b> of sequentiality and is not suitable for parallelization. Therefore, Ant Colony Optimization which attracts attention in recent years is applied to FPGA placement problem. In this research, FPGA placement tool which used Ant Colony Optimization on PC cluster is parallelized, and the evaluation and examination are performed...|$|E
40|$|This memo {{provides}} {{information for the}} Internet community. It does not specify an Internet standard of any kind. Distribution of this memo is unlimited. Copyright Notice Copyright (C) The Internet Society (2001). All Rights Reserved. This memo specifies how to incorporate International Data Encryption Algorithm (IDEA) into CMS or S/MIME as an additional <b>strong</b> <b>algorithm</b> for symmetric encryption. For organizations who make use of IDEA for data security purposes it is of high interest that IDEA is also available in S/MIME. The intention of this memo is to provide the OIDs and algorithms required that IDEA can be included in S/MIME for symmetric content and key encryption. 1...|$|E
40|$|Apart from pure {{identification}} systems, transponder {{systems with}} embedded sensors gain increasing importance. In contrast to low-end Radio Frequency Identification (RFID) systems these transponders usually require more complex protocols which {{need to be}} adjusted to a specific application. In this contribution the demanded flexibility is achieved by using the 8 -bit-microcontroller IMS 3311. In order to guarantee privacy and security of transferred information, {{it is essential that}} transmitted data is encrypted by a <b>strong</b> <b>algorithm.</b> Therefore, the Advanced Encryption Standard (AES) is used. The main focus of this paper is the development of an area-efficient AES coprocessor which accelerates the processing of the controller's ciphering tasks. The AES unit requires only 2168 logic gates and takes less than 650 clock cycles for either encryption or decryption of a 128 -bit block...|$|E
30|$|In [1], {{the author}} {{established}} weak convergence <b>algorithms</b> and <b>strong</b> convergence <b>algorithms</b> for SEP (see [1] for more details).|$|R
30|$|<b>Strong</b> {{convergence}} <b>algorithms</b> for {{the split}} common solution problem for Lipschitzian accretive mappings and nonexpansive mappings (see Corollary  4.1 below).|$|R
40|$|We {{deal with}} text processing, pattern {{matching}} and syntax analysis every day, and new areas emerging. We can consider programming languages, communication protocols or a simple text as well-know examples. Therefore, we need new effective methods which fit into these areas. In this work, we explore {{the topics of}} parallel grammars, E 0 L-systems and multigenerative grammar systems. The goal {{is to improve the}} Cocke-Younger-Kasami algorithm and present a <b>stronger</b> <b>algorithm</b> of analysis...|$|R
40|$|There is {{encryption}} {{algorithms used}} in GSM. This is for to encrypt the information when transmit from mobile station to base station during communication. As stated that A 5 / 1 is <b>strong</b> <b>algorithm</b> but it cryptanalysis by cryptanalysts. This paper modified concept to improve A 5 / 1 encryption algorithm by consideration of feedback combining function of LFSRs (Linear feedback shift register) use in A 5 / 1 and {{modified version of}} A 5 / 1 is fast and easy to implement. This is proved by the comparison of existing A 5 / 1 and modified A 5 / 1 with novel key stream generator using pseudorandom binary sequence. Basic security of A 5 / 1 analysis by statistical tests given by national institute {{of standards and technology}} (NIST) in order to ensure the quality of bit stream produced by the generator...|$|E
40|$|Symbolic {{non-deterministic}} planning represents action {{effects as}} sets of possible next states. In this article, we {{move toward a}} more probabilistic uncertainty model by distinguishing between likely primary effects and unlikely secondary effects of actions. We consider the practically important case, where secondary effects are failures, and introduce n-fault tolerant plans that are robust for up to n faults occurring during plan execution. Fault tolerant plans are more restrictive than weak plans, but more relaxed than strong cyclic and strong plans. We show that optimal n-fault tolerant plans can be generated by the usual <b>strong</b> <b>algorithm.</b> However, due to non-local error states, it is often beneficial to decouple the planning for primary and secondary effects. We employ this approach for two specialized algorithms 1 -FTP (blind) and 1 -GFTP (guided) and demonstrate their advantages experimentally in significant real-world domains...|$|E
40|$|We {{present a}} new {{approach}} to the distributed visualization of geometric algorithms that emphasizes the position of the end user. Concepts are introduced that enable a more flexible usage of visualized geometric algorithms, while keeping the task of adapting existing algorithms to the new scheme as simple as possible. A main proposition is that interactivity should not be built into the visualized algorithms, but into the visualizing system. With this in mind, we devise a visualization model for geometric algorithms that incorporates <b>strong</b> <b>algorithm</b> execution control, flexible manipulation of geometric input/output data and adjustable view attributes. The new visualization model is implemented in the Vega 1 system. Vega offers distributed visualization of geometric algorithms based on source code annotation and supports the standard libraries LEDA and CGAL. 1 Introduction A picture is worth a thousand words [...] -this well-known proverb illustrates why visualization is so important in [...] ...|$|E
30|$|<b>Strong</b> {{convergence}} <b>algorithms</b> for {{the split}} common solution problem for Lipschitzian accretive mappings and demicontractive nonexpansive mappings (see Theorem  4.1 below).|$|R
3000|$|..., p, θ) in each round. Finally, it cascades every best weak {{classifier}} of {{the rounds}} to a <b>strong</b> classifier. <b>Algorithm</b> 1 describes the Adaboost algorithm [8].|$|R
5000|$|String symbols = {"0", [...] "1", [...] "2", [...] "3", [...] "4", [...] "5", [...] "6", [...] "7", [...] "8", [...] "9", [...] "a", [...] "b", [...] "c", [...] "d", [...] "e", [...] "f"};int length = 10;Random random = SecureRandom.getInstanceStrong (...) // as of JDK 8, {{this should}} return the <b>strongest</b> <b>algorithm</b> {{available}} to the JVMStringBuilder sb = new StringBuilder(length);for (int i = 0; i < length; i++) { int indexRandom = random.nextInt( [...] symbols.length [...] ); sb.append( [...] symbolsindexRandom [...] );}String password = sb.toString (...) ...|$|R
40|$|Even {{amidst the}} {{hustle and bustle}} of busy lives, {{numerous}} people dream of playing a musical instrument. Unfortunately, many may never get a chance to touch one. But this doesn’t stop them from ‘air drumming’ or playing ‘air guitar’ passionately while listening to their favorite tunes. To encourage this passion for music, especially {{in the absence of a}} real instrument, we introduce to you the Virtual Air Guitar. This application allows one to showcase their guitar skills, regardless of their knowledge of playing a real guitar. It uses color tracking to detect inputs and a sound module incorporating the Karplus <b>Strong</b> <b>algorithm</b> to generate musical notes as an output. As a result, a simple webcam and brightly colored gloves are required to use the application. This application has enormous potential as a base for interactive guitar games, teaching music, and of course, to compose guitar based songs...|$|E
40|$|AbstractWe {{describe}} {{several new}} algorithms for Byzantine agreement. The {{first of these}} is a simplification of the original exponential-time Byzantine agreement algorithm due to Pease, Shostak, and Lamport, and is of comparable complexity to their algorithm. However, its proof is very intuitively appealing. A technique of shifting between algorithms for solving the Byzantine agreement problem is then studied. We present two families of algorithms obtained by applying a shift operator to our first algorithm. These families obtain the same rounds to message length trade-off as do Coan's families but do not require the exponential local computation time (and space) of his algorithms. We also describe a modification of an O(n) -resilient algorithm for Byzantine agreement of Dolev, Reischuk, and Strong. Finally, we obtain a hybrid algorithm that dominates all our others, by beginning execution of an algorithm in one family, first shifting into an algorithm of the second family, and finally shifting into an execution of the adaptation of the Dolev, Reischuk, and <b>Strong</b> <b>algorithm...</b>|$|E
40|$|Cryptography is a {{security}} technique {{that must be}} applied in both communication sides to protect the data during its transmission through the network {{from all kinds of}} attack. On the sender side, the original data will be changed into different symbols or shapes by using a known key; this is called encryption. On the other communication side, the decryption process will be done and the data will be returned to its former shape by using the agreed key. The importance of cryptography is to fulfil the communication security requirements. Real time applications (RTA) are vulnerable for the moment because of their big size. However, some of the current algorithms are not really appropriate for use with these kinds of information. In this paper, a novel symmetric block cipher cryptography algorithm has been illustrated and discussed. The system uses an 8 x 8 x 8 cube, and each cell contains a pair of binary inputs. The cube can provide a huge number of combinations that can produce a very <b>strong</b> <b>algorithm</b> and a long key size. Due to the lightweight and fast technique used in this idea, it is expected to be extremely rapid compared to the majority of current algorithms, such as DES and AES...|$|E
50|$|Hamachi is stated to use <b>strong,</b> industry-standard <b>algorithms</b> {{to secure}} and {{authenticate}} {{the data and}} its security architecture is open. Despite this security cannot necessarily be guaranteed.|$|R
40|$|Abstract—Data {{encryption}} standard {{in spite of}} being a great algorithm {{in terms of a}} good combination of confusion and diffusion steps, doesn’t largely used because of a weak key concept. The key used in DES is only 64 bits (or 56 bits) long. This paper introduces a concept of pre distributed powerful key for DES. In this for every round of DES a new key is getting used which makes cryptanalyst to attack not 56 bits but 16 keys of 56 bits each which makes it a much <b>stronger</b> <b>algorithm...</b>|$|R
40|$|In {{the past}} few years, there have been {{significant}} research advances {{in the analysis of}} hash functions and it was shown that none of the hash algorithm is secure enough for critical purposes whether it is MD 5 or SHA- 1. Nowadays scientists have found weaknesses in a number of hash functions, including MD 5, SHA and RIPEMD so {{the purpose of this paper}} is combination of some function to reinforce these functions and also increasing hash code length upto 256 that makes <b>stronger</b> <b>algorithm</b> against collision attests...|$|R
40|$|Non-determinism {{is often}} caused by {{infrequent}} errors that make otherwise deterministic actions fail. In this paper, we introduce fault tolerant planning {{to address this}} problem. An £-fault tolerant plan is guaranteed to recover from up to £ errors occurring during its execution. We show how optimal £-fault tolerant plans can be generated via the strong universal planning algorithm. This algorithm uses an implicit search technique based on the reduced Ordered Binary Decision Diagram (OBDD) that is particularly well suited for non-deterministic planning and has outperformed most alternative approaches. However, the OBDDs used to represent the blind backward search of the <b>strong</b> <b>algorithm</b> often blow up. A heuristic version of the algorithm has recently been proposed but is incapable of dynamically guiding the recovery {{part of the plan}} toward error states. To address this problem, we introduce two specialized algorithms 1 -FTP (blind) and 1 -GFTP (guided) for 1 -fault tolerant planning that decouples the synthesis of the recovery and nonrecovery part of the plan. Our experimental evaluation includes 7 domains of which 3 are significant real-world cases. It verifies that 1 -GFTP efficiently can handle non-local fault states and demonstrates that it due to this property can outperform guided fault tolerant planning via strong planning. In addition, 1 -FTP often outperforms strong planning due to an aggressive expansion strategy of the recovery plan...|$|E
40|$|Abstract: Problem statement: In {{the last}} decade, many {{hardware}} designs of elliptic curves cryptography have been developed, aiming {{to accelerate the}} scalar multiplication process, mainly those based on the Field Programmable Gate Arrays (FPGA), the major issue concerned the ability of embedding this strategic and <b>strong</b> <b>algorithm</b> in a very few hardware. That is, finding an optimal solution to the one to many problem: Portability against power consumption, speed against area and maintaining security at its highest level. Our strategy is to hardware execute the ECC algorithm that reposes {{on the ability of}} making the scalar multiplication over the GF(2163) in a restricted number of clock cycles, targeting the acceleration of the basic field operations, mainly the multiplication and the inverse process, under the constraint of hardware optimization. Approach: The research was based on using the efficient Montgomery add and double algorithm, the Karatsuba-Offman multiplier and the Itoh-Tsjuii algorithm for the inverse component. The hardware implementation was based upon an optimized Finite State Machine (FSM), with a single cycle 163 bits multiplier and a script generated field squarer. The main characteristics of the design concerned the elimination of the different internal component to component delays, the minimization of the global clocking resources and a strategic separation of the data path from the control part. Results: The working frequency of our design attained the 561 MHz...|$|E
40|$|La transmisión progresiva de redes tetraédricas en el dominio wavelet es muy eficiente. Sólo es necesario transmitir primero la red base junto con los coeficientes obtenidos durante el análisis multirresolución, mientras que los detalles pueden ser transmitidos después, por paquetes y en desorden. En este trabajo {{proponemos}} un algoritmo simple y robusto para visualización progresiva de tales redes tetraédricas. Adaptamos la red en tiempo de ejecución para mostrar el máximo detalle posible mediante un esquema de refinamiento rojo/verde al tiempo que aumentamos la eficiencia del rendering reduciendo el detalle en las zonas no visibles con respecto al punto de vista. Currently, {{the progressive}} transmission of tetrahedral meshes in the wavelet domain is very efficient. It {{is necessary to}} pass on first the base mesh along with the coefficient obtained during the multi resolution analysis before transmitting the details. The details can be transmitted in packages without a specific order. In this paper we suggest a simple but <b>strong</b> <b>algorithm</b> for progressive visualization of tetrahedral meshes in the wavelet domain. We adjust the mesh during the execution time using the red/green refinement technique {{in order to show}} all the possible details. At the same time, we improve the volume renderer performance by reducing its work load in all the non visible parts with regard to the point of view...|$|E
40|$|Data {{encryption}} standard {{in spite of}} being a great algorithm {{in terms of a}} good combination of confusion and diffusion steps, doesn’t largely used because of a weak key concept. The key used in DES is only 64 bits (or 56 bits) long. This paper introduces a concept of pre distributed powerful key for DES. In this for every round of DES a new key is getting used which makes cryptanalyst to attack not 56 bits but 16 keys of 56 bits each which makes it a much <b>stronger</b> <b>algorithm...</b>|$|R
30|$|We give <b>strong</b> {{convergence}} <b>algorithms</b> for {{the split}} common fixed point problem of quasi-nonexpansive mappings. Our results improve and generalize some well-known results in [3, 11] and so on.|$|R
30|$|Next we {{prove the}} <b>strong</b> {{convergence}} of <b>Algorithm</b> 3.1.|$|R
