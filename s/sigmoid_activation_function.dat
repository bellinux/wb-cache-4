77|4088|Public
50|$|All {{problems}} {{mentioned above}} {{can be handled}} by using a normalizable <b>sigmoid</b> <b>activation</b> <b>function.</b> One realistic model stays at zero until input current is received, {{at which point the}} firing frequency increases quickly at first, but gradually approaches an asymptote at 100% firing rate. Mathematically, this looks like , where the hyperbolic tangent function can be replaced by any sigmoid function. This behavior is realistically reflected in the neuron, as neurons cannot physically fire faster than a certain rate. This model runs into problems, however, in computational networks as it is not , a requirement to calculate backpropagation.|$|E
40|$|This paper {{presents}} a digital VLSI {{implementation of a}} feed-forward neural network classifier based on the saturating linear activation function. The architecture consists of one-hidden layer performing the weighted sum followed by a saturating linear activation function. The hardware implementation of such a network {{presents a}} significant advantage in terms of circuit complexity as compared to a network based on a <b>sigmoid</b> <b>activation</b> <b>function,</b> but without compromising the classification performance. Simulation results on two benchmark problems show that feedforward neural networks with the saturating linearity perform as well as networks with the <b>sigmoid</b> <b>activation</b> <b>function.</b> The architecture can also handle variable precision resulting in a higher computational resources at lower precision...|$|E
40|$|FPGA, ???????? ?????????? ???????? ??????? ???????? ??????? ?? ??????????? ???????? ?????? FPGA, ?????????? ?????????? ???????? ??????? ????????? ?????, ??????? ????? ?? RBF-??????, ????????? ?????? ???????? ?? ?????? ??????? ??????????????. In {{this paper}} {{describes}} methods of software and hardware implementation of artificial neuron with <b>sigmoid</b> <b>activation</b> <b>function</b> by means of FPGA, provides step by step algorithm of artificial neuron and the corresponding resource occupied FPGA, considered technology of artificial neural networks, in particular such as RBF-network, dynamic network Hopfild and feed-forward neural network...|$|E
50|$|One of {{the first}} {{versions}} of the theorem was proved by George Cybenko in 1989 for <b>sigmoid</b> <b>activation</b> <b>functions.</b>|$|R
50|$|In 1989, {{the first}} proof was {{published}} by Cybenko for <b>sigmoid</b> <b>activation</b> <b>functions</b> and was generalised to feed-forward multi-layer architectures in 1991 by Hornik.|$|R
3000|$|... [...]. Although it is {{more common}} to use <b>sigmoid</b> <b>activation</b> <b>functions,</b> we note that the {{identity}} function can just {{be thought of as}} a suitably scaled tanh function over the desired range.|$|R
40|$|We {{introduce}} a new method for proving explicit upper bounds on the VC Dimension of general functional basis networks, and prove as an application, for the first time, the VC Dimension of analog neural networks with the <b>sigmoid</b> <b>activation</b> <b>function</b> oe(y) = 1 = 1 + e to be bounded by a quadratic polynomial {{in the number of}} programmable parameters...|$|E
40|$|We {{introduce}} a new method for proving explicit upper bounds on the VC Dimension of general functional basis networks, and prove as an application, for the first time, the VC Dimension of analog neural networks with the <b>sigmoid</b> <b>activation</b> <b>function</b> oe(y) = 1 = 1 + e to be bounded by a quadratic polynomial {{in the number of}} programmable parameters. 0 Introduction The most commonly used activation function in various neural networks applications is the sigmoid oe(y) = 1 = 1 + e (cf. [HKP 91]). We refer to [AB 92], [M 93], and [MS 93] for all the necessary background on the computation by neural networks and the VC dimension (particularly, to the connection between their computational power, and the sample complexity). In Maass's 1993 lecture notes [M 93], Open Problem 10 (see also [GJ 93] and [MS 93]) asks: Is the VC-dimension of analog neural nets with the <b>sigmoid</b> <b>activation</b> <b>function</b> oe(y) = 1 = 1 + e bounded by a polynomial in the number of programmable parameters ? (I [...] ...|$|E
30|$|Recently, due {{to their}} strong {{regression}} capabilities, deep neural networks (DNNs) [8, 9] have also been utilized in speech dereverberation. In [10, 11], a DNN-based single-microphone dereverberation system was proposed by adopting a <b>sigmoid</b> <b>activation</b> <b>function</b> at the output layer and min-max normalization of target features. An improved DNN dereverberation system we proposed recently [12] adopted a linear output layer and globally normalized the target features into zero mean and unit variance, achieving the state-of-the-art performances.|$|E
30|$|We first give a brief {{introduction}} of the DNN and LSTM network structures used as a baseline. A DNN contains a series of hidden layers, which for speech applications is most commonly fully connected with <b>sigmoid</b> <b>activation</b> <b>functions.</b>|$|R
40|$|In {{this paper}} we {{present the results}} of {{experimental}} work that demonstrates the generation of linear and <b>sigmoid</b> <b>activation</b> <b>functions</b> in a digital stochastic bit-stream neuron. These <b>activation</b> <b>functions</b> are generated by a stochastic process and require no additional hardware, allowing the design of an individual neuron to be extremely compact...|$|R
5000|$|... #Caption: A {{taxonomy}} of the {{artificial neural network}} architectures that are most popular in EPF (see Weron, 2014). Input nodes are denoted by filled circles, output nodes by empty circles, and nodes in the hidden layer by empty circles with a dashed outline. The <b>activation</b> <b>functions</b> for RBF networks are radial basis functions, whereas multi-layer perceptrons (MLP) typically use piecewise linear or <b>sigmoid</b> <b>activation</b> <b>functions</b> (illustrated in circles).|$|R
40|$|An {{information}} processing algorithm which simulates the way biological neural systems process information, {{and one of}} the most popular machine learning algorithms, ANN (Artificial Neural Network) has been extensively used for Data Mining, which extracts hidden patterns and valuable information from large databases. Data Mining is commonly used in a wide range of practices such as accounting, marketing, fraud detection, scientific discovery, etc. This paper introduces a new adaptive Higher Order Neural Network (HONN) model and applies it in data mining tasks such as determining liver disorders and predicting breast cancer recurrences. A new activation function which is a combination of sine and sigmoid functions is used as the neuron activation function for the new HONN model. There are free parameters in the new activation function. The paper compares the new HONN model against a Multi-Layer Perceptron (MLP) with the <b>sigmoid</b> <b>activation</b> <b>function,</b> an RBF Neural Network with the gaussian activation function, and a Recurrent Neural Network (RNN) with the <b>sigmoid</b> <b>activation</b> <b>function.</b> Experimental results show that the new HONN model offers several advantages over conventional ANN models such as improved generalisation capabilities as well as abilities in handling missing values in a dataset...|$|E
40|$|This paper aims {{to place}} neural {{networks}} {{in the context of}} boolean circuit complexity. We define appropriate classes of feedforward neural networks with specified fan-in, accuracy of computation and depth and using techniques of communication complexity proceed to show that the classes fit into a well-studied hierarchy of boolean circuits. Results cover both classes of <b>sigmoid</b> <b>activation</b> <b>function</b> networks and linear threshold networks. This provides a much needed theoretical basis {{for the study of the}} computational power of feedforward neural networks...|$|E
30|$|Table  1 {{presents}} {{experimental results}} of individual neural networks and hybrid neural model using standard <b>sigmoid</b> <b>activation</b> <b>function.</b> The input feature vector includes average dew point, minimum temperature, maximum temperature, mean temperature, average relative moistness, precipitation, normal wind speed, high wind speed, average cloudiness and output layer {{composed of two}} neurons to represent rainy and dry weather. The hybrid neural model has exhibited better performance as compared to stand-alone networks. Additionally, non-linear models have an issue of limited capacity that could be covered using hybrid neural models.|$|E
40|$|A {{hybrid model}} for time series {{forecasting}} is proposed. It is a stacked neural network, containing one normal multilayer perceptron with bipolar <b>sigmoid</b> <b>activation</b> <b>functions,</b> {{and the other}} with an exponential <b>activation</b> <b>function</b> in the output layer. As shown by the case studies, the proposed stacked hybrid neural model performs well on a variety of benchmark time series. The combination of weights of the two stack components that leads to optimal performance is also studied...|$|R
50|$|Cybenko {{has served}} as an advisor for the Defense Science Board and several other {{government}} panels and is the founding editor-in-chief of the IEEE Security & Privacy magazine.His current research interests are distributed information, control systems, and signal processing, {{with a focus on}} applications to security and infrastructure protection.He is known for proving the universal approximation theorem for artificial neural networks with <b>sigmoid</b> <b>activation</b> <b>functions.</b>|$|R
40|$|Recently Barron (1993) {{has given}} rates for hidden layer {{feedforward}} networks with <b>sigmoid</b> <b>activation</b> <b>functions</b> approximating {{a class of}} functions satisfying a certain smoothness condition. These rates do {{not depend on the}} dimension of the input space. We extend Barron's results to feedforward networks with possibly non-sigmoid <b>activation</b> <b>functions</b> approximating mappings and their derivatives simultaneously. Our conditions are similar but not identical to Barron's, but we obtain the same rates of approximation, showing that the approximation error decreases at rates as fast as n Γ 1 2, where n is the number of hidden units. The dimension of the input space appears only in the constants of our bounds. 1 Introduction Recent results of Barron (1993), building on key work of Jones (1992), have established general results describing the degree of approximation properties of single hidden layer feedforward networks with <b>sigmoid</b> hidden layer <b>activation</b> <b>function.</b> Barron's results follo [...] ...|$|R
30|$|The CEN is a {{specially}} crafted network composed of three distinct layers: a contrast normalising convolutional layer, a generic convolutional layer, and {{a mixture of}} experts (ME) convolutional layer. Upon providing the network with a localised image patch, the contrast normalising layer performs Z score normalisation on the image prior to convolving with a ReLU-based kernel of the generic convolution layer. The first two layers of the network essentially prepare {{the data for the}} third and final ME-layer which produces the final alignment probabilities for the image patch. The ME-layer is a convolutional layer with a kernel using a <b>sigmoid</b> <b>activation</b> <b>function</b> which produces the response map or probabilities of landmark alignment within an image patch. The response maps are then combined with a non-negative weight final layer which too uses a <b>sigmoid</b> <b>activation</b> <b>function</b> to compute Di(xi;I). Zadeh et al. noted that a 1 × 1 kernel size with no pooling layers was selected to increase the resolution of the landmark prediction space and that the ME-layer {{had a significant impact on}} the overall performance. Unlike other CNNs, increasing the depth of the CEN did not improve the performance of the network while changes to the ME-layer such as removing the constraint of non-negative weights result in a significant reduction in performance.|$|E
40|$|Inspired by the brain, deep neural {{networks}} (DNN) {{are thought to}} learn abstract representations through their hierarchical architecture. However, at present, how this happens is not well understood. Here, we demonstrate that DNN learn abstract representations by a process of demodulation. We introduce a biased <b>sigmoid</b> <b>activation</b> <b>function</b> {{and use it to}} show that DNN learn and perform better when optimized for demodulation. Our findings constitute the first unambiguous evidence that DNN perform abstract learning in practical use. Our findings may also explain abstract learning in the human brain...|$|E
40|$|Abstract—This paper {{presents}} a least-square based analytic solu-tion of the weights of a multilayer {{feedforward neural network}} with a single hidden layer and a <b>sigmoid</b> <b>activation</b> <b>function,</b> which today constitutes {{the most common type}} of artificial neural networks. This solution has the potential to be effective for large-scale neural networks with many hidden nodes, where backpropagation is known to be relatively slow. At this stage, more research is required to improve the generalization abilities of the proposed method. Keywords-analytic; FNN; large-scale; least square method; neural network; sigmoid I...|$|E
40|$|Abstract. In recent years, {{there has}} been a lot of {{interest}} in the use of discrete-time recurrent neural nets (DTRNN) to learn finite-state tasks, and in the computational power of DTRNN, particularly in connection with finite-state computation. This paper describes a simple strategy to devise stable encodings of sequential finite-state translators (SFST) in a second-order DTRNN with units having bounded, strictly growing, continuous <b>sigmoid</b> <b>activation</b> <b>functions.</b> The strategy relies on bounding criteria based on a study of the conditions under which the DTRNN is actually behaving as a SFST. 1...|$|R
40|$|One of {{the most}} {{important}} properties of neural nets (NN) for control purposes is the universal approximation property. Unfortunately, this property is generally proven for continuous functions. In most real industrial control systems there are nonsmooth functions (e. g. piecewise continuous) for which approximation results in the literature are sparse. Examples include friction, deadzone, backlash, and so on. It is found that attempts to approximate piecewise continuous <b>functions</b> using smooth <b>activation</b> <b>functions</b> require many NN nodes and many training iterations, and still do not yield very good results. Therefore, a novel neural network structure is given for approximation of piecewise continuous functions of the sort that appear in friction, deadzone, backlash and other motion control actuator nonlinearities. The novel NN consists of neurons having standard <b>sigmoid</b> <b>activation</b> <b>functions,</b> plus some additional neurons having a special class of non-smooth <b>activation</b> <b>functions</b> termed 'jum [...] ...|$|R
40|$|This paper {{describes}} a non-invasive system for respiratory monitoring using a Micro Electro Mechanical Systems (MEMS) flow sensor and an IMU (Inertial Measurement Unit) accelerometer. The designed system {{is intended to}} be wearable and used in a hospital or at home to assist people with respiratory disorders. To ensure the accuracy of our system, we proposed a calibration method based on ANN (Artificial Neural Network) to compensate the temperature drift of the silicon flow sensor. The <b>sigmoid</b> <b>activation</b> <b>functions</b> used in the ANN model were computed with the CORDIC (COordinate Rotation DIgital Computer) algorithm. This algorithm was also used to estimate the tilt angle in body position. The design was implemented on reconfigurable platform FPGA...|$|R
40|$|An {{artificial}} {{neural network}} (ANN) type multilayerperceptron with <b>sigmoid</b> <b>activation</b> <b>function</b> and errorbackpropagation learning algorithm was applied to predict heat transfer performances {{in a wide range}} of pressure and wall superheat at bubble boiling in free convection of two single component systems. For both systems the ANN prediction results were compared with the commonly used conventional correlation and gave in one case a comparable in the other a better accuracy. To improve the accuracy of ANN prediction it is possible to use interpolated experimental data for training and to optimise the network configuration...|$|E
40|$|Traditionally, neural {{networks}} used a <b>sigmoid</b> <b>activation</b> <b>function.</b> Recently, {{it turned out}} that piecewise linear activation functions are much more efficient [...] especially in deep learning applications. However, so far, there have been no convincing theoretical explanation for this empirical efficiency. In this paper, we show that, by using different uncertainty techniques, we can come up with several explanations for the efficiency of piecewise linear {{neural networks}}. The existence of several different explanations makes us even more confident in our results [...] and thus, in the efficiency of piecewise linear activation functions...|$|E
40|$|Abstract — In {{this work}} we refine an initial {{grasping}} behavior based on 3 D edge information by learning. Based {{on a set}} of autonomously generated evaluated grasps and relations between the semi-global 3 D edges, a prediction function is learned that computes a likelihood for the success of a grasp using either an offline or an online learning scheme. Both methods are implemented using a hybrid artificial neural network containing standard nodes with a <b>sigmoid</b> <b>activation</b> <b>function</b> and nodes with a radial basis function. We show that a significant performance improvement can be achieved. I...|$|E
40|$|Standard {{feedforward}} {{neural networks}} {{benefit from the}} nice theoretical properties of mixtures of <b>sigmoid</b> <b>activation</b> <b>functions,</b> but they may fail in several practical learning tasks. These tasks would be better faced by relying on a more appropriate, problem-specific basis of <b>activation</b> <b>functions.</b> The paper presents a connectionist model which exploits adaptive <b>activation</b> <b>functions.</b> Each hidden unit in the network {{is associated with a}} specific pair (f(·),p(·)), where f(·) is the <b>activation</b> <b>function</b> and p(·) is the likelihood of the unit being relevant to the computation of the network output over the current input. The function f(·) is optimized in a supervised manner, while p(·) is realized via a statistical parametric model learned through unsupervised (or, partially supervised) estimation. Since f(·) and p(·) influence each other's learning process, the overall machine is implicitly a co-trained coupled model and, in turn, a flexible, non-standard neural architecture. Feasibility of the approach is corroborated by empirical evidence yielded by computer simulations involving regression and classification tasks...|$|R
40|$|This study {{compares the}} {{performance}} of multilayer perceptron neural networks and maximum-likelihood doubly-constrained models for commuter trip distribution. Our experiments produce overwhelming evidence at variance with the existing literature that the predictive accuracy of neural network spatial interaction models is inferior to that of maximum-likelihood doubly-constrained models with an exponential function of distance decay. The study points to several likely causes of neural network underperformance, including model non-transferability, insufficient ability to generalize, and reliance on <b>sigmoid</b> <b>activation</b> <b>functions,</b> and their inductive nature. It is concluded that current perceptron neural networks do not provide an appropriate modeling approach to forecasting trip distribution over a planning horizon for which distribution predictors (number of workers, number of residents, commuting distance) are beyond their base-year domain of definition. ...|$|R
3000|$|... [...]), where tanh and <b>sigmoid</b> are the <b>activation</b> <b>functions.</b> The {{probability}} {{of the next}} word is computed via the softmax function in the output layer, where W [...]...|$|R
40|$|Abstract- This paper proposes new {{principal}} component analysis-wavelet neural network hybrid (PCA-TAWNN) architecture trained by Threshold Accepting (TA) algorithm to predict bankruptcy in banks. This architecture {{consists of an}} input layer, the {{principal component}} layer consisting of a few selected principal components, a hidden layer with wavelet activation function and finally an output layer with a <b>sigmoid</b> <b>activation</b> <b>function.</b> The effectiveness of PCA-TAWNN is tested on Turkish, Spanish and UK banks bankruptcy datasets and two benchmark datasets Wine and WBC. We observed that PCA-TAWNN convincingly outperformed other techniques in terms of Area under ROC curve (AUC) in 10 -fold cross-validation...|$|E
30|$|All layers {{were trained}} {{concurrently}} so that, in training mode, targets were presented at all even layers: layers 2, 4, 6 and 8. Since even layers are trained with a softmax activation function, its outputs {{can be seen}} as BPC probabilities. The odd uses a <b>sigmoid</b> <b>activation</b> <b>function.</b> All the network weights and bias are adjusted using batch training with an RPROP algorithm[21] so as to minimize the minimum-cross-entropy error between the network output and the target values. The choice of the error function followed Bishop’s suggestion[32], which was later clarified by Dunne and Campbell[33]. It states that the softmax activation function should couple with the cross-entropy penalty function.|$|E
40|$|We {{investigate}} the computational power of continuous-time symmetric Hopfield nets. As is well known, such networks have very constrained, Liapunov-function controlled dynamics. Nevertheless, {{we show that}} they are universal and efficient computational devices, in the sense that any convergent fully parallel computation by a network of n discrete-time binary neurons, with in general asymmetric interconnections, can be simulated by a symmetric continuous-time Hopfield net containing only 14 n + 6 units using the saturated-linear <b>sigmoid</b> <b>activation</b> <b>function.</b> In terms of standard discrete computation models this result implies that any polynomially space-bounded Turing machine can be simulated by a polynomially size-increasing sequence of continuous-time Hopfield nets...|$|E
40|$|Abstract — The Projection Pursuit Learner is a multi-class {{classifier}} {{that resembles}} a two-layer neural network in which the <b>sigmoid</b> <b>activation</b> <b>functions</b> of the hidden neurons {{have been replaced by}} an interpolating polynomial. This modification increases the flexibility of the model but also makes it more inclined to get stuck in a local minimum during gradient-based training. This problem can be alleviated to a certain extent by replacing the random initialization of the parameters by proper heuristics. In this paper we propose to initialize the projection directions by means of feature space transformation methods such as independent component analysis (ICA), principal component analysis (PCA), linear discriminant analysis (LDA) and springy discriminant analysis (SDA). We find that with this refinement the number of processing units can be reduced by 10 - 40 %. I...|$|R
40|$|Abstract-This paper {{introduces}} the new concept of Artificial Neural Networks (ANNs) in estimating speed and controlling the separately excited DC motor. The neural control scheme {{consists of two}} parts. One is the neural estimator, {{which is used to}} estimate the motor speed. The other is the neural controller, which is used to generate a control signal for a converter. These two networks are trained by Levenberg-Marquardt back propagation algorithm. Standard three layer feed forward neural network with <b>sigmoid</b> <b>activation</b> <b>functions</b> in the input and hidden layers and purelin in the output layer is used. Simulation results are presented to demonstrate the effectiveness and advantage of the control system of DC motor with ANNs in comparison with the conventional control scheme. Keywords [...] DC motor, artificial neural networks, control system...|$|R
40|$|This paper {{presents}} {{a new approach}} for improving performances of magnetic levitation system. Controlled parameter is the amplitude which levitated object achieves during movement from one levitation position to another. Two position levitation with improved amplitude performances is obtained by implementing orthogonal neural network in standard levitation control logic. Proposed network is a nonlinear autoregressive neural network with newly developed <b>activation</b> <b>function</b> based on orthogonal polynomials. Performed experiments on a system with default control logic showed {{that it could not}} provide stable two position levitation when specified amplitude of the levitation object is greater than 10 - 4 m. Artificial network was trained using real experimental data and it was based on standard tangent and <b>sigmoid</b> <b>activation</b> <b>functions.</b> Default <b>activation</b> <b>functions</b> were then substituted with a newly developed orthogonal polynomial functions. The amplitude 10 - 3 m was achieved with stable two position levitation after parameter optimization. It is proven that simple control logic with nonlinear autoregressive neural network and proper <b>activation</b> <b>function</b> can provide improved amplitude performances...|$|R
