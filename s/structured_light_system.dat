86|10000|Public
5000|$|Though {{the vast}} {{majority}} of machine vision applications are solved using two-dimensional imaging, machine vision applications utilizing 3D imaging are a growing niche within the industry. The most commonly used method for 3D imaging is scanning based triangulation which utilizes motion of the product or image during the imaging process. A laser is projected onto the surfaces of an object and viewed from a different angle. In machine vision this is accomplished with a scanning motion, either by moving the workpiece, or by moving the camera & laser imaging system. The line is viewed by a camera from a different angle; the deviation of the line represents shape variations. Lines from multiple scans are assembled into a depth map or point cloud. [...] Stereoscopic vision is used in special cases involving unique features present in both views of a pair of cameras. Other 3D methods used for machine vision are time of flight, grid based and stereoscopic. One method is grid array based systems using pseudorandom <b>structured</b> <b>light</b> <b>system</b> as employed by the Microsoft Kinect system circa 2012.|$|E
40|$|Abstract For a {{portable}} <b>structured</b> <b>light</b> <b>system,</b> {{it must be}} {{easy to use and}} flexible. So the inconvenient and expensive equipment for calibration such as two or three orthogonal planes or extra fixed equipment should not be considered. For the purpose of fast 3 D acquisition, the projection matrices of {{a portable}} <b>structured</b> <b>light</b> <b>system</b> should be estimated. This paper proposes a flexible calibration method to meet the requirements of the portable <b>structured</b> <b>light</b> <b>system</b> through a surface plane. A calibration board is attached to the surface plane, and a reference pattern is also projected by an LCD projector onto the surface plane. The camera observes the surface plane at a few different positions. Then, the world-to-image point pairs for the camera and projector are obtained based on the cross ratio and epipolar geometry, and the system is thus calibrated. The experiments conducted for the proposed calibration method demonstrate its accuracy and robustness. Key words Projector calibration, camera calibration, portable <b>structured</b> <b>light</b> <b>system</b> Research on shape reconstruction and object recognition by projecting structured light stripes onto objects has been active since the early 1970 s. Due to their fast speed and non-contact nature, structured light techniques have found numerous applications. A basic <b>structured</b> <b>light</b> <b>system</b> consists of a camera and a light (or laser) stripe projector...|$|E
40|$|This paper {{discusses}} {{a method}} for obtaining accurate 3 D measurements using a temporally encoded <b>structured</b> <b>light</b> <b>system.</b> An objective of the work {{was to have a}} balance in the accuracy of all components in the system. This was achieved by including lens distortion in the models for both the camera and projector which comprise the <b>structured</b> <b>light</b> <b>system.</b> In addition, substripe estimation was used to estimate projector stripe values as a complement to subpixel estimators used for locating image features. Experimental evaluation shows {{that it is important to}} use substripe estimation and incorporate lens distortion in the projector model. 1 Introduction Structured light is a general concept and there are a number of ways of exploiting it to obtain 3 D information e. g. [1]. The <b>structured</b> <b>light</b> <b>system</b> (SLS) considered in this paper consists of a projector and a camera. The projector projects a sequence of band patterns which together identify one of 256 stripe values at each visible [...] ...|$|E
5000|$|... 2007 Tent <b>structures</b> and <b>lighting</b> <b>system,</b> Royal Terminal, Jeddah, Saudi Arabia.|$|R
40|$|Abstract − <b>Structured</b> <b>light</b> <b>systems</b> {{are widely}} used for {{three-dimensional}} shape reconstruction. The accuracy of the vision system parameters {{is of paramount importance}} for the accuracy of the 3 D reconstruction. Often the extrinsic parameters change over time due to environmental conditions, in order to compensate for <b>structured</b> <b>light</b> parameters change, a self-recalibration method based on planar homography is proposed in this paper. The method uses the planar homography of corresponding points in two camera views to identify the relative position and orientation between the two cameras: one is the “real ” camera, while the other is a reference solid considered as a “pseudocamera”. The laser plane of the <b>structured</b> <b>light</b> vision <b>system</b> is the constraining plane for the two cameras corresponding points...|$|R
40|$|Phase-shifted sinusoids are {{commonly}} used as projection patterns in <b>structured</b> <b>light</b> <b>systems</b> consisting of projectors and cameras. They require few image captures per 3 D reconstruction and have low decoding complexity. Recently, <b>structured</b> <b>light</b> <b>systems</b> with a projector {{and a pair of}} stereo cameras have been used in order to overcome the traditional phase discontinuity problem and allow for the reconstruction of scenes with multiple objects. In this paper, we propose a new approach to the phase unwrapping process in such systems. Rather than iterating through all pixels in the two cameras to determine the global phase of each pixel, we iterate through the projector pixels to solve for correspondences between the two camera views. An energy minimization framework is applied to these initial estimated correspondences to enforce smoothness and to fill in missing pixels. Unlike existing approaches, our method allows simultaneous unwrapping of both camera images and enforces consistency across them. We demonstrate the effectiveness of our approach experimentally on a few scenes. Index Terms — <b>Structured</b> <b>light,</b> phase unwrapping, stereo cameras, 3 D depth captur...|$|R
40|$|Abstract—A {{calibration}} method for a <b>structured</b> <b>light</b> <b>system</b> by observing a planar object from unknown viewpoints is proposed. A <b>structured</b> <b>light</b> <b>system</b> captures a 3 D shape by a camera that observes a light stripe on an object illuminated by a projector. The 3 D shape, {{obtained from the}} system defined by a pinhole model for the projection of a light stripe, is solved using the equation of a plane model for the projector. The coefficients of each light stripe’s equation are estimated using the 4 × 3 image-to-camera transformation matrix that is expressed by camera parameters. Experimental results demonstrate {{a high degree of}} accuracy when following the proposed approach. I...|$|E
40|$|This paper {{presents}} the results of a study in which a close-range <b>structured</b> <b>light</b> <b>system</b> is used for the digitization of a cultural heritage object. A Herakles statue, named “Weary Herakles ” and located in the Antalya Museum, Turkey was scanned by a Breuckmann optoTOP-HE system. The work comprises the essential steps of the 3 D object modeling pipeline, i. e. digitization, registration, surface triangulation, editing, texture mapping and visualization. 3 D recording, modeling and visualization of cultural heritage objects is an expanding application area. This paper addresses the capabilities of some of the current technology in this field. Particular emphasis is given to a coded <b>structured</b> <b>light</b> <b>system</b> as an option for data acquisition. 1...|$|E
40|$|<b>Structured</b> <b>light</b> <b>system</b> {{calibration}} {{has been}} widely studied over the decades, {{and a variety of}} calibration approaches have been proposed. Among these methods, the flexible method using flat checkerboard is widely adopted. However, {{there is a lack of}} studies on selecting the optimal checker size for high accuracy calibration, whilst it is vital to understanding this factor. This paper presents a systematic study on how the checker size affects the calibration accuracy for a <b>structured</b> <b>light</b> <b>system,</b> and provides a general guideline to select the optimal size. For this initial study, 7 different checker sizes are selected, and experiments demonstrated that the system achieved the best calibration accuracy within a certain range of checker size...|$|E
50|$|A {{virtual reality}} headset {{provides}} virtual reality for the wearer. VR headsets {{are widely used}} with computer games {{but they are also}} used in other applications, including simulators and trainers. They comprise a stereoscopic head-mounted display (providing separate images for each eye), stereo sound, and head motion tracking sensors (which may include gyroscopes, accelerometers, <b>structured</b> <b>light</b> <b>systems,</b> etc.). Some VR headsets also have eye tracking sensors and gaming controllers.|$|R
40|$|Temporally encoded <b>structured</b> <b>light</b> <b>systems</b> {{are one of}} {{the many}} types of active 3 D range sensors available. 3 D data is {{obtained}} by observing light patterns projected into the scene. In some situations, e. g., calibration, it is necessary to determine the point on the projector emitter plane that corresponds to a given image-plane location, even though the emitter plane is discretised into a finite number of stripes. Thus, a substripe estimator is required. This paper describes three such estimators, and their performance is compared...|$|R
40|$|Abstract. A {{practical}} way for obtaining depth in computer vision {{is the use}} of <b>structured</b> <b>light</b> <b>systems.</b> For panoramic depth reconstruction several images are needed which most likely implies the construction of a sensor with mobile elements. Moreover, misalignments can appear for non-static scenes. Omnidirectional cameras offer a much wider field of view than the perspective ones, capture a panoramic image at every moment and alleviate the problems due to occlusions. This paper is focused on the idea of combining omnidirectional vision and <b>structured</b> <b>light</b> with the aim to obtain panoramic depth information. The resulting sensor is formed by a single catadioptric camera and an omnidirectional light projector. ...|$|R
30|$|To keep {{balance between}} {{calibration}} accuracy and time complexity, an improved calibration method is proposed {{to decrease the}} complexity of the calibration procedure by simplifying the extrinsic calibration of the <b>structured</b> <b>light</b> <b>system.</b> A white plate with a matrix of hollow black ring markers is used to calibrate the dual-camera <b>structured</b> <b>light</b> <b>system.</b> The system calibration process can be divided into three steps. 1) Calibrating the right camera and the <b>structured</b> <b>light</b> <b>system</b> with the left camera by establishing corresponding point pairs between projector pixel coordinate and left camera pixel coordinate of discrete markers on a plate surface. The corresponding projector pixel coordinate of each marker is determined by measuring the absolute phase from projected vertical and horizontal sinusoidal fringe patterns on the plate surface. 2) Computing the transformation between left camera and right camera using intrinsic parameters of two cameras, the center of each marker coordinates in two camera images and world coordinates of the center of each marker. 3) Calculating extrinsic parameters of the <b>structured</b> <b>light</b> <b>system</b> with the right camera using the aforementioned obtained parameters. 3 D cloud data sets for the two projector-camera pairs obtained by the calibrated system are matched based on the variant ICP (iterative closest point) algorithm [24]. We simplified the system calibration and achieved the high measurement accuracy by using the variant ICP algorithm. The rest of this paper is organized as follows. The principle and details of the proposed calibration method are described in Section “Theories”. Experimental results are presented in Section “Experiments and Results”. Section “Discussions” presents the conclusion and remarks about future work.|$|E
40|$|Temporally {{dithered}} codes {{have recently}} been used for depth reconstruction of fast dynamic scenes using off-theshelf DLP Projectors. Even though temporally dithered codes overcome the DLP projector's limited frame rate, limitations with the optics create challenges for using these codes in an actual <b>structured</b> <b>light</b> <b>system.</b> Specifically, to maximize the amount of light leaving the projector, projector lenses are designed to have large apertures resulting in projected patterns that appear in focus over a narrow depth of field. In this paper, we propose a method to design temporally dithered codes in order to extend the virtual depth of field of a <b>structured</b> <b>light</b> <b>system.</b> By simulating the PWM sequences of a DLP projector and the blurring process from the projecto...|$|E
40|$|Abstract # 2466 Sponsorship: STFCIt {{is common}} for stereo cameras {{to be placed on}} {{planetary}} landers and rovers. The techique described here, in some instances, could replace one of the cameras in a stereo system with a very light weight passive <b>structured</b> <b>light</b> <b>system.</b> This would save money, weight, and power...|$|E
40|$|In {{order to}} perform {{high-speed}} three-dimensional (3 D) shape measurements with <b>structured</b> <b>light</b> <b>systems,</b> high-speed projectors are required. One possibility is an array projector, which allows pattern projection at several tens of kilohertz by switching {{on and off}} the LEDs of various slide projectors. The different projection centers require a separate analysis, as the intensity received by the cameras depends on the projection direction and the object's bidirectional reflectance distribution function (BRDF). In this contribution, we investigate the BRDF-dependent errors of array-projection-based 3 D sensors and propose an error compensation process...|$|R
40|$|The {{goal of this}} {{bachelor}} {{thesis is}} to determine measurement deviations of the optical system ATOS Triple Scan, when the chalk and titanium coating is applied, by statistical data analysis. In the theoretical part, a short introduction to 3 D reconstruction of real objects, distribution of <b>structured</b> <b>light</b> <b>systems</b> according to the pattern structure and existing knowledge about the scanning system accuracy is given. The practical part {{is based on the}} experimental measurement the calibration elements with specific chalk and titanium coating. As the result, the measurement uncertainty and the coating thickness is determined...|$|R
40|$|We {{present a}} <b>structured</b> <b>lighting</b> <b>system</b> for {{creating}} high- resolution stereo datasets of static indoor scenes with highly accurate ground-truth disparities. The system includes novel techniques for efficient 2 D subpixel correspondence search and self-calibration of cameras and projectors with modeling of lens distortion. Combining disparity estimates from multiple projector positions {{we are able}} to achieve a dis- parity accuracy of 0. 2 pixels on most observed surfaces, including in half- occluded regions. We contribute 33 new 6 -megapixel datasets obtained with our system and demonstrate that they present new challenges {{for the next generation of}} stereo algorithms...|$|R
40|$|The {{principal}} {{frame and}} principal quadric form a fundamental {{description of a}} smooth surface at a point. This paper is concerned with estimating these from the 3 D data acquired by a sensor such as a <b>structured</b> <b>light</b> <b>system.</b> Eight dierent estimation methods are described and a comparison of their performance is presented...|$|E
30|$|The {{simplest}} <b>structured</b> <b>light</b> <b>system</b> {{based on}} fringe projection [7] is usually {{composed of a}} camera and a projector. The projector projects a series of fringe pattern images onto the measured object surface. From another viewpoint, the fringe patterns are deformed {{with regard to the}} object surface and captured by a camera. The 3 D shape of the measured object is obtained by using a triangulation technique. However, the measuring range is limited to the intersection of the camera-projector fields of view. Points not projected by the projector and/or observed by the camera cannot be measured. Moreover, the camera resolution has its limits. On the contrary, a dual-camera <b>structured</b> <b>light</b> <b>system</b> [8] consisting of two cameras and a projector can increase the sensing range and its spatial resolution using each camera-projector pair to measure the partial area of an object.|$|E
40|$|This paper {{describes}} {{the results from}} a feasibility analysis performed on two different <b>structured</b> <b>light</b> <b>system</b> designs and the image processing algorithms they require for dent detection and localization. The impact of each <b>structured</b> <b>light</b> <b>system</b> is analyzed {{in terms of their}} mechanical realization and the complexity of the image processing algorithms required for robust dent detection. The two design alternatives considered consist of projecting vertical or horizontal laser stripes on the drum surface. The first alternative produces straight lines in the image plane and requires scanning the drum surface horizontally, whereas the second alternative produces conic curves on the camera plane and requires scanning the drum surface vertically. That is, the first alternative favors image processing against mechanical realization while the second alternative favors mechanical realization against image processing. The results from simulated and real structured light systems are presented a [...] ...|$|E
40|$|Color-encoded <b>structured</b> <b>lighting</b> <b>systems</b> {{are widely}} used for threedimensional data {{acquisition}} based on machine vision. Calibration of such a system is a laborious and tedious task. This paper presents a novel method for self-recalibration of such a vision system. The relative pose between the projector and camera {{of the system is}} automatically determined by taking a single view of the scene, so that the 3 -D measurements and reconstruction can be performed efficiently even if it is moved {{from one place to another}} or the configuration of the system is changed. Experiments were carried out to demonstrate the implementation of the proposed method...|$|R
40|$|Stripe {{extraction}} {{methods are}} a fundamental step for the <b>structured</b> <b>light</b> three-dimensional measurement <b>systems.</b> This work proposes an efficient and robust stripe extraction method for <b>structured</b> <b>light</b> measurement <b>system.</b> In this paper, a novel method for extracting the stripes from <b>structured</b> laser <b>lights</b> is presented {{which is based}} on the Steger stripe extraction algorithm used to identify the <b>structure</b> of <b>light</b> stripe and smoothing spline fitting algorithm for interpolation to obtain single continuous stripe and the coordinates of the clustered stripes are calculated to form a phase map through a cubic polynomial interpolation procedure. The experimental results show that the work is particularly efficient and robust. Sultan Qaboos University; Universiti Sains Malaysia (USM...|$|R
40|$|<b>Structured</b> <b>light</b> <b>systems</b> {{are popular}} {{in part because}} they can be {{constructed}} from off-the-shelf low cost components. In this paper we quantitatively show how common design parameters affect precision and accuracy in such systems, supplying a much needed guide for practitioners. Our quantitative measure is the established VDI/VDE 2634 (Part 2) guideline using precision made calibration artifacts. Experiments are performed on our own <b>structured</b> <b>light</b> setup, consisting of two cameras and a projector. We place our focus on the influence of calibration design parameters, the calibration procedure and encoding strategy and present our findings. Finally, we compare our setup to a state of the art metrology grade commercial scanner. Our results show that comparable, and in some cases better, results can be obtained using the parameter settings determined in this study...|$|R
40|$|A <b>structured</b> <b>light</b> <b>system</b> {{provides}} a dense sampling of 3 D coordinates from visible surfaces in a scene. This paper compares several methods {{for determining the}} principle frame, Gaussian curvature, and mean curvature at each visible surface point from this data. The principle frame, etc, parameterize a model which completely characterizes the local surface geometry...|$|E
40|$|Abstract — A {{structured}} light vision system using pattern projection {{is useful for}} robust reconstruction of 3 D objects. One of the major tasks in using such a system is the calibration of the sensing system. This paper presents a new method by which a 2 DOF <b>structured</b> <b>light</b> <b>system</b> can be automatically recalibrated, {{if and when the}} relative pose between the camera and the projector is changed. A distinct advantage of this method is that neither an accurately designed calibration device nor the prior knowledge of the motion of the camera or the scene is required. Several important cues for self-recalibration, including geometrical cue, illumination cue, and focus cue, are explored. The sensitivity analysis shows that high accuracy in depth value can be achieved with this calibration method. Some experimental results are presented to demonstrate the calibration technique. Index Terms [...] <b>Structured</b> <b>light</b> <b>system,</b> active vision, recalibration, pattern projection, 3 D reconstruction...|$|E
40|$|Calibration of a <b>structured</b> <b>light</b> <b>system</b> is {{considered}} in this paper. The system {{consists of a}} projector whose output is controlled by a liquid crystal shutter, and a camera to capture the images. A sequence of patterns is projected to temporally encode 256 stripes on the visible surfaces. The transformation from 3 D world coordinates to camera image plane coordinates and projector stripe ids are both modelled using perspective transformation matrices. The space of such matrices is characterised and a singular value decomposition procedure is described for estimating their elements. The accuracy of the calibration system so attained is investigated and improvements to the method are suggested. 1. Introduction The <b>structured</b> <b>light</b> <b>system</b> consists of a liquid crystal shutter controlling the output of a light source, and a CCD video camera to capture image of the scene. The liquid crystal shutter generates 8 different band images. The combination of these produces a temporal encoding of 2 [...] ...|$|E
40|$|Today, <b>structured</b> <b>light</b> <b>systems</b> {{are widely}} used in {{applications}} such as robotic assembly, visual inspection, surgery, entertainment, games and digitization of cultural heritage. Current <b>structured</b> <b>light</b> methods are faced with two serious limitations. First, {{they are unable to}} cope with scene regions that produce strong highlights due to specular reflection. Second, they cannot recover useful information for regions that lie within shadows. We observe that many <b>structured</b> <b>light</b> methods use illumination patterns that have translational symmetry, i. e., two-dimensional patterns that vary only along one of the two dimensions. We show that, for this class of patterns, diffusion of the patterns along the axis of translation can mitigate the adverse effects of specularities and shadows. We show results for two applications - 3 D scanning using phase shifting of sinusoidal patterns and separation of direct and global components of light transport using high-frequency binary stripes...|$|R
40|$|Experimental {{studies of}} giant resonances, nuclear <b>structure,</b> <b>light</b> mass <b>systems,</b> and heavy mass systems are summarized. Theoretical studies of nuclear structure, and {{dynamics}} are described. Electroweak interactions; atomic and surface physics; applied nuclear physics; and nuclear medicine are discussed. Instrumentation, cyclotron, ion source, target laboratory, electronics, and computing technology are mentioned...|$|R
40|$|One-shot <b>structured</b> <b>light</b> <b>systems</b> for 3 D depth {{reconstruction}} {{often use}} a periodic illumination pattern. Finding corresponding {{points in the}} image and projector plane, needed for a triangulation, boils down to phase estimation. The 2 πN ambiguities in the phase cause ambiguities in the reconstructed depth. This paper solves these ambiguities by constraining the solution space to scenes that only contain objects with flat surfaces, i. e. polyhedrons. We develop a new particle filter that estimates the depth and solves the ambiguity problem. A state model is proposed for piecewise continuous signals. This state model is worked out to find the optimal proposal density of the particle filter. The approach is validated with a demonstration...|$|R
40|$|A <b>structured</b> <b>light</b> <b>system</b> {{provides}} a dense sampling of 3 D coordinates from visible surfaces in a scene. This paper compares several methods {{for determining the}} principle frame, Gaussian curvature, and mean curvature at each visible surface point from this data. The principle frame, etc, parameterize a model which completely characterizes the local surface geometry. Given the local surface model, surface folds and discontinuities such as occlusion boundaries can be detected, and the inter-lying surface regions classified by type (parabolic, elliptic, hyperbolic, elliptic). This data {{can be used to}} identify features on the objects, such as bumps, and determine surface properties, such as symmetry axes. Methods for performing such segmentation are discussed. 1. Introduction <b>Structured</b> <b>light</b> <b>system</b> are one of many types of machine vision sensors which are able to provide 3 D coordinate information about a dense set of points on the visible surfaces of the objects in a scene. Others include [...] ...|$|E
40|$|This paper {{describes}} {{an alternative approach}} to interpreting the data from a temporarily encoded <b>structured</b> <b>light</b> <b>system.</b> Instead of focusing on the stripes generated in the image by such a sensor, the transitions between stripes are of primary importance. Transitions correspond to projector shutter features significantly smaller that the stripes and hence their spatial location is much more well defined. This leads to higher accuracy 3 D data. An Alternative Interpretation of <b>Structured</b> <b>Light</b> <b>System</b> Data 1 1. Introduction One way in which structured light {{can be used to}} determine 3 D positions in space is to project a plane of light into the scene (Jarvis 1993). The intersection of this plane with the visible surfaces in the scene forms an illuminated stripe on those surfaces. The illuminated stripe can easily be detected in the image plane. The position of the stripe in 3 -space (i. e., the position of the illuminated surface points) can be determined by finding the intersection of the [...] ...|$|E
40|$|In {{this paper}} a new {{approach}} to 3 D human body tracking is proposed. A sparse 3 D reconstruction of the subject to be tracked is made using a <b>structured</b> <b>light</b> <b>system</b> consisting of a precalibrated LCD projector and a camera. At a number of points-of-interest, easily detectable features are projected. The resulting sparse 3 D reconstruction is used to estimate the body pose of the tracked person. This new estimate of the body pose is then fed back to the <b>structured</b> <b>light</b> <b>system</b> and allows to adapt the projected patterns, i. e. decide where to project features. Given the observations, a physical simulation is used to estimate the body pose by attaching forces to the limbs of the body model. The sparse 3 D observations are augmented by denser silhouette information, {{in order to make the}} tracking more robust. Experiments demonstrate the feasibility of the proposed approach and show that the high speeds that are required due to the closed feedback loop can be achieved...|$|E
40|$|This {{paper is}} {{focusing}} on the calibration of a 3 D measurement system based on a <b>structured</b> <b>light</b> technique. Key to the proposed method for calibration {{of the system is}} to uncouple the complicated relationship between the projector and camera, and separate the calibration procedure into two parts. The first part is to calibrate the camera. After that, {{with the help of the}} parameters of the calibrated camera and the relation between the target feature points of camera calibration and projector calibration, the second part is to calibrate the projector. The proposed calibration method is fast, accurate and easy to implement. It significantly simplifies the calibration procedures of <b>structured</b> <b>light</b> <b>systems.</b> Both analytical and numerical solutions of the system calibration can be obtained. In this paper, the principle of the proposed method is described and some satisfactory experimental results are presented. © 2008 IEEE...|$|R
30|$|The {{second group}} of test data is from the 2014 Middlebury Stereo Dataset [38]. The stereo image pairs in this dataset were {{generated}} using a <b>structured</b> <b>lighting</b> <b>system,</b> and were meant to present new challenges {{for the next generation}} of stereo algorithms. Of the 33 stereo image pairs in the 2014 Middlebury Stereo Dataset, only 23 of them have accompanying ground-truth disparity maps. Therefore, we used these 23 stereo pairs in quarter resolution to form the {{second group of}} test data: 1) adirondack, 2) jadeplant, 3) motorcycle, 4) piano, 5) pipes, 6) playroom, 7) playtable, 8) recycle, 9) shelves, 10) vintage, 11) backpack, 12) bicycle 1, 13) cable, 14) classroom 1, 15) couch, 16) flowers, 17) mask, 18) shopvac, 19) sticks, 20) storage, 21) sword 1, 22) sword 2, and 23) umbrella.|$|R
40|$|Automated digital photogrammetric {{systems are}} {{considered}} to be passive three-dimensional vision systems since they obtain object coordinates from only the information contained in intensity images. Active 3 -D vision systems, such as laser scanners and <b>structured</b> <b>light</b> <b>systems</b> obtain the object coordinates from external information such as scanning angle, time of flight, or shape of projected patterns. Passive systems provide high accuracy on well defined features, such as targets and edges however, unmarked surfaces are hard to measure. These systems may also be difficult to automate in unstructured environments since they are highly affected by the ambient <b>light.</b> Active <b>systems</b> provide their own illumination and the features to be measured so they can easily measure surfaces in most environments. However, they have difficulties with varying surface finish or sharp discontinuities such as edges. Therefore each type of sensor is more suited for a specific type of objects and features, a [...] ...|$|R
