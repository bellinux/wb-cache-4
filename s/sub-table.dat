23|25|Public
5000|$|The tie-breaking {{criteria}} for teams level on points consider only {{the results of}} matches between those teams (in this case, this excludes their results against Tunisia). This {{is shown in the}} <b>sub-table</b> above. All three teams were level on points and goal difference, and were ranked based on goals scored: Zambia 4, Cameroon 3, Gabon 2.|$|E
50|$|Quotient {{filters are}} AMQs and, as such, provide {{many of the}} same {{benefits}} as Bloom filters. A large database, such as Webtable may be composed of smaller sub-tables each of which has an associated filter. Each query is distributed concurrently to all sub-tables. If a <b>sub-table</b> does not contain the requested element, its filter can quickly complete the request without incurring any I/O.|$|E
30|$|We {{explored}} modelling approach {{premised on}} the binary logistic regression to determine a better level of delay threshold that optimally evaluates the dynamics of air traffic delay during departure and arrival at an airport (Santos and Robin 2010). Four different scenarios were evaluated for both cases of departures and arrivals. The study established that at Entebbe International Airport, departure delay threshold of air traffic flow operations of 60  % provided the best and stable model characteristics. Variations of levels of significance for parameters of delay were detected at different delay thresholds, thus generating different numbers of significant parameters. For example, in both Tables  4 and 5; <b>sub-table</b> (d) presented the worst levels of parameter sensitivity with the least number of significant variables while <b>sub-table</b> (b) provided more stable models in both cases (Wesonga and Nabugoomu 2014; Helmuth et al. 2011).|$|E
40|$|We {{describe}} {{two classes}} of software systems that release tabular summaries of an underlying database. Table servers respond to user queries for (marginal) <b>sub-tables</b> of the "full" table summarizing the entire database, and are characterized by dynamic assessment of disclosure risk, in light of previously answered queries. Optimal tabular releases are static releases of sets of <b>sub-tables</b> that are characterized by maximizing {{the amount of information}} released, as given by a measure of data utility, subject to a constraint on disclosure risk. Underlying abstractions [...] - primarily associated with the query space, as well as released and unreleasable <b>sub-tables</b> and frontiers, computational algorithms and issues, especially scalability, and prototype software implementations are discussed...|$|R
40|$|We {{describe}} and illustrate NISS-developed optimal tabular release technology, which releases sets of <b>sub-tables</b> of large contingency tables that maximize data utility (in our examples, {{the number of}} <b>sub-tables</b> released) subject to a constraint on disclosure risk (tightness of bounds on small-count, risky cells in the underlying table). This approach explicitly accommodates the mandate of Federal statistical agencies to protect data confidentiality and their mission to disseminate information derived from the data. ...|$|R
30|$|Additional graphs are {{provided}} in “Appendix 2 '' for visual checks on possible relationships among pairs of variables. The data set that is used in subsequent estimations and analyses is provided in <b>sub-tables</b> of Table 10 in the “Appendix 3 ''.|$|R
30|$|Next, {{consider}} the probabilities of falling into different financial positions (A = Strong, B = Adversely Affected, or C = Indebted/Destitute) conditional upon levels of {{average cost of}} treatment and insurance reimbursements (<b>sub-table</b> BURDEN 4). The baseline category for this regression probability has no negative financial effect; the two reference categories for AvgCost and Insurance Level are LowCost and High reimbursement, respectively.|$|E
30|$|Finally, {{estimation}} {{results are}} reported {{based on data}} provided in the ENV 2 <b>sub-table,</b> which model {{the probability of a}} patient paying high or medium “extra thank-you money” conditional upon income ranks and/or severity of illness. The baseline category is “paying negligible thank-you” for the response variable. For “Ill 2 ” the reference category is “light sickness” and for “Income Rank”, the reference is “Medium”.|$|E
40|$|Random {{perturbation}} is {{a promising}} technique for privacy preserving data mining. It retains an original sensitive value {{with a certain}} probability and replaces it with a random value from the domain with the remaining probability. If the replacing value is chosen from a large domain, the retention probability must be small to protect privacy. For this reason, previous randomizationbased approaches have poor utility. In this paper, we propose an alternative way to randomize sensitive values, called small domain randomization. First, we partition the given table into sub-tables that have smaller domains of sensitive values. Then, we randomize the sensitive values within each <b>sub-table</b> independently. Since each <b>sub-table</b> has a smaller domain, a larger retention probability is permitted. We propose this approach {{as an alternative to}} classical partition-based approaches to privacy preserving data publishing. There are two key issues: ensure the published sub-tables do not disclose more private information than what is permitted on the original table, and partition the table so that utility is maximized. We present an effective solution. 1...|$|E
40|$|A {{new method}} for more {{equitable}} comparison of various hash table techniques is presented. It {{is applied to}} some popular techniques: open addressing, coalescent chaining and separate chaining. Another method, indexed <b>sub-tables,</b> is also examined with more details and shown to present some interesting features. © 1979 BIT Foundations. SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|R
40|$|Aim of {{the thesis}} is {{evaluation}} of road network {{as an integral}} part of common facilities plan. To get the results I chose two cadastral areas situated at different altitudes in which I examined behaviour of road network based on other common facilities. Basically it was a survey on how the road network in each territory adjoins the elements of Territorial System of Ecological Stability that helps reduce erosion vulnerability and whether it can improve the water situation in the territory. Results are given in the <b>sub-tables</b> that describe the road network in interaction with other common facilities conducted before and after Comprehensive Landscaping. These tables are supplemented by a recommendation which might ensure greater efficiency in the future. <b>Sub-tables</b> also indicate the approach of designers when designing road networks in typologically different territories. All results are graphically presented both in the text and in the annexes...|$|R
50|$|Working Table III {{deals with}} {{questions}} of {{both internal and external}} security. The aim is to establish a stable security environment in the region and to promote regional co-operation in fighting organised crime and corruption and on migration issues. It is divided into two <b>sub-tables.</b> The first one deals with Justice and Home Affairs and the second one with Defence and Security Sector Reform issues.|$|R
30|$|In detail, {{when the}} number of basis vectors varies from 10 to 300, the average {{precision}} increases from 68.8 to 91.17  % for the sunglasses subset and from 58.83 to 87.75  % for the scarf subset. But the rate is not stable {{if we look at the}} <b>sub-table</b> for region size, it goes up and down unpredictably. The wider the local region is, the smaller size of the basis is needed to achieve high recognition rate. This implies the optimal precision achieved with the appropriate choice of sufficient number of basis vectors and suitable regions size.|$|E
40|$|Asmussen & Edwards (1983) defined {{necessary}} and sufficient conditions for collapsibil-ity of a hierarchical log linear {{model for a}} multidimensional contingency table. We have shown that for decomposable log linear models these conditions can be combined with various graph-theoretic algorithms to provide useful classes of sub-tables which are collapsible onto. In particular, the SAHR algorithm finds the minimal set onto which the model can be collapsed and which contains a <b>sub-table</b> of interest. In the context of expert systems, by reducing a probabilistic influence network onto only the relevant nodes, the algorithms reduce the required computation and simplify interpretation...|$|E
40|$|Motivated by the {{insufficiency}} of {{the existing}} quasi-identifier/sensitiveattribute (QI-SA) framework on modeling real-world privacy requirements for data publishing, we propose a novel versatile publishing scheme with which privacy requirements can be specified as an arbitrary set of privacy rules over attributes in the microdata table. To enable versatile publishing, we introduce the Guardian Normal Form (GNF), a novel method of publishing multiple subtables such that each <b>sub-table</b> is anonymized by an existing QI-SA publishing algorithm, while the combination of all published tables guarantees all privacy rules. We devise two algorithms, Guardian Decomposition (GD) and Utility-aware Decomposition (UAD), for decomposing a microdata table into GNF, and present extensive experiments over real-world datasets to demonstrate the effectiveness of both algorithms...|$|E
5000|$|Multivalue {{databases}} are [...] "lumpy" [...] data, in {{that they}} can store {{exactly the same way}} as relational databases, but they also permit a level of depth which the relational model can only approximate using <b>sub-tables.</b> This is nearly identical to the way XML expresses data, where a given field/attribute can have multiple right answers at the same time. Multivalue {{can be thought of as}} a compressed form of XML.|$|R
30|$|The {{method of}} log linear {{analysis}} was used to describe the relationship between the dependent variables (self-reported exceeding the speed limit on a certain type of road) and a set of independent variables (logit models) [31]. Log linear analysis uses a generalised linear model, which estimates the cell counts of a Table, using different combinations of the margins. In a two-way Table the margins are the row and the column totals. In higher dimensional Tables, the margins are the <b>sub-Tables</b> of lower dimension.|$|R
40|$|Dissemination of {{information}} derived from large contingency tables formed from confidential data {{is a major}} problem faced by statistical agencies. In this paper we present solutions to several computational and algorithmic issues that arise in the dissemination of cross-tabulations (marginal <b>sub-tables)</b> from a single underlying table. These include data structures that exploit sparsity and support efficient computation of marginals as well as algorithms such as iterative proportional fitting, and a generalized form of the shuttle algorithm that computes sharp bounds on (small, confidentiality threatening) cells in the full table from arbitrary sets of released marginals. We give examples illustrating the techniques...|$|R
30|$|Algorithm 4 {{shows the}} details of RLSA. Firstly, the data table T is {{partitioned}} and sorted according to the normal process (Line 1). After that, the position of each data in T will not be changed and we generate a <b>sub-table</b> ST by random sampling with rate s (Line 2). It should be explained that not only the source data are sampled, but also the index of the sampled data in table S is recorded. Then we calculate the boundaries of each window based on each row in ST and get the subscript of r (Line 3 - 6). Next, traverse data from the current row to the both sides until the subscript exceeds window range and invoke adjust_transfunc for each row that is traversed (Line 7 – 15). In fact, it’s a range style computing process. Finally, return the final results (Line 16).|$|E
30|$|Table 2 {{presents}} the annotated data statistics. The <b>sub-table</b> (a) re{{presents the}} extracted and selected images obtained from five different cameras {{that are used}} for both training and test. The sub-classes within the person class indicate full body, bent body, upper body, head, person group, etc. in order. ‘Full’, ‘cluster’ and ‘upper’ classes are the majority ones, whereas the remaining three classes occupy only 16 %. As we mentioned above, ‘etc’ class includes images {{that are difficult to}} classify and the other class images are also not easy to be automatically identified. Among the classes in non-person, ‘etc.’ class is remarkably large, followed by the class ‘Car-part’. The other two classes include only 6 % of the total images. It means that most moving objects are either persons or cars and the remaining images caught by background subtraction are mostly wrongly detected objects. Considering the difficulty of image collection, our goal in this study is to classify images into two classes, person and non-person.|$|E
40|$|Abstract: Compared with Hard Drive Disk (HDD), SSD {{has a lot}} of advantages, such as high random read performance, {{low power}} {{consumption}} and lightweight form. Therefore it is envisioned to be next generation data storage instead of HDD. However, the enhancement of query performance for flash-based database {{is not the same as}} the IO ratio of SSD to HDD. The reason is existing databases which are designed for HDD can not take full advantage of high IO performance of SSD. In this paper, a new join algorithm, Sub-Join, is proposed. Sub-Join first projects the column of join and primary key as <b>Sub-Table,</b> and then executes join operations on Sub-Tables. Finally results are gotten from original table according to the result of join on Sub-Tables. The compared experiments with Oracle Berkeley DB show Sub-Join outperforms original indexed nested-loop join at the ratio of about 40 %~ 100 %. The result strongly shows the high efficiency o...|$|E
40|$|Spatio-temporal {{reasoning}} is extensively {{used in many}} areas of computer vision and Artificial Intelligence. Different models for spatio-temporal reasoning are proposed based on topological and directional relations sepa- rately in respective domains. Reasoning about moving objects in a spatial scene or description about the two-dimensional scene needs both the reasoning systems simultaneously. We introduced a reasoning system of a two-dimensional spatial scene based on Combined Topological and Directional(CTD) relations method, where we obtain both topological and orientation information simul- taneously. Entities in composite tables follows the certain mathematical rule, these rules are elaborated through the certain examples and the composition table for topological relations are rearranged and divided into <b>sub-tables...</b>|$|R
40|$|With the {{continuous}} advances in optical communications technology, the link transmission speed of Internet backbone {{has been increasing}} rapidly. This in turn demands more powerful IP address lookup engine. In this paper, we propose a power-efficient parallel TCAM-based lookup engine with a distributed logical caching scheme for dynamic load-balancing. In order to distribute the lookup requests among multiple TCAM chips, a smart partitioning approach called pre-order splitting divides the route table into multiple <b>sub-tables</b> for parallel processing. Meanwhile, by virtual of the cache-based load balancing scheme with slow-update mechanism, a speedup factor of N- 1 can be guaranteed for a system with N (N> 2) TCAM chips, even with unbalanced bursty looku...|$|R
40|$|The {{problem of}} the proper {{dimension}} of a Multiple Correspondence Analysis (MCA) is discussed, based on both the re-evaluation of the explained inertia sensu Benzécri (1979) and Greenacre (2006) and a test proposed by Ben Ammou and Saporta (1998). This leads to the consideration of a better reconstruction of the off-diagonal <b>sub-tables</b> of the Burt’s table crossing the nominal characters taken into the account. Thus, Greenacre (1988) Joint Correspondence Analysis (JCA) is introduced and {{the results obtained on}} two applications are shown. The quality of reconstruction of both MCA and JCA solutions are compared to the Simple Correspondence Analysis results of the two-way tables. It results that JCA’s reduced-dimensional reconstruction is much better than the MCA’s one, that reveals highly biased and non-monotonous...|$|R
40|$|This study {{describes}} a novel design of Java table browser using XML(jTBX). A standard table-browsing environment {{is provided in}} the form of Java applet which can be opened in any web browser such as Netscape, or Internet Explore with Java 1. 2 plug-in installed. Some manipulation functions to tables are supported for various level of objects in table(table, <b>sub-table,</b> column, row, cell). A hierarchical Java tree provides the table of content (TOC) of available tables. Three tier architecture is used in the jTBX system design. Remote database tier provides raw data from distributed sites. Web server tier generates the response in the standard XML format to the requests from the client side tier (table browser). Metadata for tables are integrated into the XML files (or streams) before being used by client Java applet. Multiple threads are generated for a large table transporting. Headings: Technical Tools and Concept – Java table browser using XML System Structure – Three tier web structure Data transporting method—Multiple Java thread...|$|E
40|$|We {{consider}} the problem {{where we have}} a multi-way table of means, indexed by several factors, where each factor can have a large number of levels. The entry in each cell is the mean of some response, averaged over the observations falling into that cell. Some cells may be very sparsely populated, and in extreme cases, not populated at all. We might still like to estimate an expected response in such cells. We propose here a novel hierarchical ANOVA (HANOVA) representation for such data. Sparse cells will lean more on the lower-order interaction model for the data. These in turn could have components that are poorly represented in the data, in which case they rely on yet lower-order models. Our approach leads to a simple hierarchical algorithm, requiring repeated calculations of <b>sub-table</b> means of modified counts. The algorithm has shown superiority over the unshrinked methods in both simulations and real data sets. Comment: 14 pages, 1 figur...|$|E
40|$|ABSTRACT: Tu et al {{present an}} {{analysis}} of the equivalence of three paradoxes, namely, Simpson's, Lord's, and the suppression phenomena. They conclude that all three simply reiterate the occurrence of a change in the association of any two variables when a third variable is statistically controlled for. This is not surprising because reversal or change in magnitude is common in conditional analysis. At the heart of the phenomenon of change in magnitude, with or without reversal of effect estimate, is the question of which to use: the unadjusted (combined table) or adjusted (<b>sub-table)</b> estimate. Hence, Simpson's paradox and related phenomena are a problem of covariate selection and adjustment (when to adjust or not) in the causal analysis of non-experimental data. It cannot be overemphasized that although these paradoxes reveal the perils of using statistical criteria to guide causal analysis, they hold neither the explanations of the phenomenon they depict nor the pointers on how to avoid them. The explanations and solutions lie in causal reasoning which relies on background knowledge, not statistical criteri...|$|E
40|$|Figure 1 : Overview of our {{construction}} for a voxelized Lucy model, {{colored by}} mapping x, y, and z coordinates to red, green, and blue respectively (far left). The 3. 5 million voxels (left) are input as 32 -bit keys and placed into buckets of ≤ 512 items, averaging 409 each (center). Each bucket then builds a cuckoo hash with three <b>sub-tables</b> and stores {{them in a}} larger structure with 5 million entries (right). Close-ups follow the progress of a single bucket, showing the keys allocated to it (center; the bucket is linear and wraps around left to right) and each of its completed cuckoo <b>sub-tables</b> (right). Finding any key requires checking only three possible locations. We demonstrate an efficient data-parallel algorithm for building large hash tables of millions of elements in real-time. We consider two parallel algorithms for the construction: a classical sparse perfect hashing approach, and cuckoo hashing, which packs elements densely by allowing an element to be stored in one of multiple possible locations. Our construction is a hybrid approach that uses both algorithms. We measure the construction time, access time, and memory usage of our implementations and demonstrate real-time performance on large datasets: for 5 million key-value pairs, we construct a hash table in 35. 7 ms using 1. 42 times as much memory as the input data itself, and we can access all the elements in that hash table in 15. 3 ms. For comparison, sorting the same data requires 36. 6 ms, but accessing all the elements via binary search requires 79. 5 ms. Furthermore, we show how our hashing methods {{can be applied to}} two graphics applications: 3 D surface intersection for moving data and geometric hashing for image matching...|$|R
40|$|Abstract. Biclustering {{numerical}} data became a popular data-mining {{task in the}} beginning of 2000 ’s, especially for analysing gene expression data. A bicluster reflects a strong association between a subset of objects and a subset of attributes in a numerical object/attribute data-table. So called biclusters of similar values can be thought as maximal <b>sub-tables</b> with close values. Only few methods address a complete, correct and non redundant enumeration of such patterns, which is a well-known intractable problem, while no formal framework exists. In this paper, we introduce important links between biclustering and formal concept analysis. More specifically, we originally show that Triadic Concept Analysis (TCA), provides a nice mathematical framework for biclustering. Interestingly, existing algorithms of TCA, that usually apply on binary data, can be used (directly or with slight modifications) after a preprocessing step for extracting maximal biclusters of similar values...|$|R
40|$|AbstractThis paper {{proposes a}} method for {{construction}} of a clinical pathway based on attribute and sample clustering, called dual clustering. The method consists of the following four steps: first, histories of nursing orders are extracted from hospital information system. Second, orders are classified into several groups by using clustering on the pricipal components (sample clustering). Third, attributes clustering {{is applied to the}} data. Finally, original temporal data are split into several <b>sub-tables</b> and the first step will be repeated in a recursive way. After the grouping results are stable, a new pathway will be constructed from all the induced results. The method was applied to datasets of a disease extracted from a hospital information system. The results show that the proposed method constructed a clinical pathway, which was not only similar to the pathway manually acquired from medical experts but also discovered nursing orders which they forget to include...|$|R
40|$|This release adds {{a simple}} plugin {{architecture}} to allow third party scripts {{to be easily}} added. A factor plotting plugin has been added using this method. This will be provided by default. The other major change is to allow table selections and properties to be saved with projects. Plots can therefore be re-loaded from the last selections and plot settings. Finally numpy functions can now {{be used in the}} function evaluation bar. Columns generated from functions are remembered so that you can keep track of how the data was made, a bit like a spreadsheet formula. Changes Implemented plugin system with sample seaborn plugin Moved batch file rename tool to plugins Meta data now saved with project files - allows saving plots and selections Can add a table to plot using <b>sub-table</b> contents Integrated table copy and paste into toolbars Changed function evaluation so that math functions work Column-wise functions can be applied Added ability to show error bars in plots Various fixes for table selections Added melt function for converting to long form dat...|$|E
40|$|Tu et al {{present an}} {{analysis}} of the equivalence of three paradoxes, namely, Simpson's, Lord's, and the suppression phenomena. They conclude that all three simply reiterate the occurrence of a change in the association of any two variables when a third variable is statistically controlled for. This is not surprising because reversal or change in magnitude is common in conditional analysis. At the heart of the phenomenon of change in magnitude, with or without reversal of effect estimate, is the question of which to use: the unadjusted (combined table) or adjusted (<b>sub-table)</b> estimate. Hence, Simpson's paradox and related phenomena are a problem of covariate selection and adjustment (when to adjust or not) in the causal analysis of non-experimental data. It cannot be overemphasized that although these paradoxes reveal the perils of using statistical criteria to guide causal analysis, they hold neither the explanations of the phenomenon they depict nor the pointers on how to avoid them. The explanations and solutions lie in causal reasoning which relies on background knowledge, not statistical criteria. © 2008 Arah; licensee BioMed Central Ltd...|$|E
40|$|AbstractFeature {{selection}} is a challenging problem {{in many areas}} such as pattern recognition, machine learning and data mining. Rough set theory, as a valid soft computing tool to analyze various types of data, has been widely applied to select helpful features (also called attribute reduction). In rough set theory, many feature selection algorithms have been developed in the literatures, however, they are very time-consuming when data sets are in a large scale. To overcome this limitation, we propose in this paper an efficient rough feature selection algorithm for large-scale data sets, which is stimulated from multi-granulation. A <b>sub-table</b> of a data set {{can be considered as}} a small granularity. Given a large-scale data set, the algorithm first selects different small granularities and then estimate on each small granularity the reduct of the original data set. Fusing all of the estimates on small granularities together, the algorithm can get an approximate reduct. Because of that the total time spent on computing reducts for sub-tables is much less than that for the original large-scale one, the algorithm yields in a much less amount of time a feature subset (the approximate reduct). According to several decision performance measures, experimental results show that the proposed algorithm is feasible and efficient for large-scale data sets...|$|E
40|$|Biclustering {{numerical}} data became a popular data-mining {{task in the}} beginning of 2000 's, especially for analysing gene expression data. A bicluster reflects a strong association between a subset of objects and a subset of attributes in a numerical object/attribute data-table. So called biclusters of similar values can be thought as maximal <b>sub-tables</b> with close values. Only few methods address a complete, correct and non redundant enumeration of such patterns, which is a well-known intractable problem, while no formal framework exists. In this paper, we introduce important links between biclustering and formal concept analysis. More specifically, we originally show that Triadic Concept Analysis (TCA), provides a nice mathematical framework for biclustering. Interestingly, existing algorithms of TCA, that usually apply on binary data, can be used (directly or with slight modifications) after a preprocessing step for extracting maximal biclusters of similar values. Comment: Concept Lattices and their Applications (CLA) (2011...|$|R
40|$|Abstract. In {{this paper}} {{we present a}} new {{approach}} to handling in-complete information and classifier complexity reduction. We describe a method, called D 3 RJ, that performs data decomposition and decision rule joining to avoid the necessity of reasoning with missing attribute values. In the consequence more complex reasoning process is needed than in the case of known algorithms for induction of decision rules. The original incomplete data table is decomposed into <b>sub-tables</b> without missing values. Next, methods for induction of decision rules are applied to these sets. Finally, an algorithm for decision rule joining is used to obtain the final rule set from partial rule sets. Using D 3 RJ method it is possible to obtain smaller set of rules and next better classification accuracy than standard decision rule induction methods. We provide an empirical evaluation of the D 3 RJ method accuracy and model size on data with missing values of natural origin. ...|$|R
40|$|International audienceBiclustering {{numerical}} data became a popular data-mining task at the be-ginning of 2000 's, especially for gene expression data analysis and recommender sys-tems. A bicluster reflects a strong association between {{a subset of}} objects and a subset of attributes in a numerical object/attribute data-table. So-called biclusters of similar values can be thought as maximal <b>sub-tables</b> with close values. Only few methods address a complete, correct and non-redundant enumeration of such patterns, a well-known intractable problem, while no formal framework exists. We introduce impor-tant links between biclustering and Formal Concept Analysis (FCA). Indeed, FCA is known to be, among others, a methodology for biclustering binary data. Handling {{numerical data}} is not direct, and we argue that Triadic Concept Analysis (TCA), the extension of FCA to ternary relations, provides a powerful mathematical and algorithmic framework for biclustering numerical data. We discuss hence both theo-retical and computational aspects on biclustering numerical data with triadic concept analysis. These results also scale to n-dimensional numerical datasets...|$|R
