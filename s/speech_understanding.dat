808|449|Public
5|$|DARPA {{was deeply}} {{disappointed}} with researchers {{working on the}} <b>Speech</b> <b>Understanding</b> Research program at CMU and canceled an annual grant of three million dollars.|$|E
25|$|Recent {{hearing aids}} include {{wireless}} hearing aids. One hearing aid can transmit {{to the other}} side so that pressing one aid's program button simultaneously changes the other aid, so that both aids change background settings simultaneously. FM listening systems are now emerging with wireless receivers integrated with the use of hearing aids. A separate wireless microphone can be given to a partner to wear in a restaurant, in the car, during leisure time, in the shopping mall, at lectures, or during religious services. The voice is transmitted wirelessly to the hearing aids eliminating the effects of distance and background noise. FM systems have shown to give the best <b>speech</b> <b>understanding</b> in noise of all available technologies.|$|E
25|$|Most older {{hearing aids}} have only an {{omnidirectional}} microphone. An omnidirectional microphone amplifies sounds equally from all directions. In contrast, a directional microphone amplifies sounds from one direction more than sounds from other directions. This means that sounds originating {{from the direction}} the system is steered toward are amplified more than sounds coming from other directions. If the desired speech arrives {{from the direction of}} steering and the noise is from a different direction, then compared to an omnidirectional microphone, a directional microphone provides a better signal to noise ratio. Improving the signal-to-noise ratio improves <b>speech</b> <b>understanding</b> in noise. Directional microphones {{have been found to be}} the second best method to improve the signal-to-noise ratio (the best method was an FM system, which locates the microphone near the mouth of the desired talker).|$|E
5000|$|... {{difficulty}} <b>understanding</b> <b>speech,</b> {{especially women}} and children ...|$|R
5000|$|Sudden {{difficulty}} speaking, slurring {{of words}} or trouble <b>understanding</b> <b>speech</b> ...|$|R
5000|$|<b>Understanding</b> <b>speech</b> from {{far-field}} microphones - i.e. {{handling the}} reverbation and background noise ...|$|R
50|$|Auditory only <b>speech</b> <b>understanding</b> can be {{achieved}} when good central temporal processing abilities are present / Hochmair-Desoyer I.J., Hochmair E.S., Stiglbrunner H.K.: Psychoacoustic temporal processing and <b>speech</b> <b>understanding</b> in cochlear implant patients, Cochlear Implant, Ed.: R.A. Schindler and M.M. Merzenich, Raven Press, New York, pp. 291 - 304, 1985/.|$|E
5000|$|... 1989 The Voyager <b>Speech</b> <b>Understanding</b> System: A Progress Report ...|$|E
5000|$|... 1990 The VOYAGER <b>speech</b> <b>understanding</b> system: {{preliminary}} {{development and}} evaluation ...|$|E
50|$|On September 11, 2005, the Dalai Lama visited Wood River High School in Hailey {{to give a}} <b>speech</b> on <b>understanding</b> and {{friendship}} in remembrance of the September 11, 2001 Attacks and offered condolences to the many thousands affected by the recent Hurricane Katrina.|$|R
50|$|Among the signs/symptoms of {{arteriosclerosis}} are: sudden weakness, facial {{or lower}} limbs numbness, confusion, difficulty <b>understanding</b> <b>speech</b> and problems seeing.|$|R
40|$|Vol. IV, No. 4 | [Articles:] "Gandhi's Grandson to Keynote CSUN Week of Dialogue - Midday Tuesday <b>Speech</b> on <b>Understanding</b> Race will be Followed by Evening Exchange on Nonviolence"; "Deafness Center 'Teleclass' Reaches National Audience - Two-Hour Broadcast to All 50 States Highlights CSUN's Growing Technological Capacity...|$|R
5000|$|... "Optimal Search Strategies for <b>Speech</b> <b>Understanding</b> Control", Artificial Intelligence 18:3:295-326, May 1982.|$|E
5000|$|... 1971-75: DARPA's {{frustration}} with the <b>Speech</b> <b>Understanding</b> Research program at Carnegie Mellon University, ...|$|E
50|$|Bilateral SNHL gives less diplacusis, but pitch distortions may persist. This {{may cause}} {{problems}} {{with music and}} <b>speech</b> <b>understanding.</b>|$|E
40|$|An {{overview}} of artificial intelligence (AI), its core ingredients, and its applications is presented. The knowledge representation, logic, problem solving approaches, languages, and computers pertaining to AI are examined, {{and the state}} of the art in AI is reviewed. The use of AI in expert systems, computer vision, natural language processing, <b>speech</b> recognition and <b>understanding,</b> <b>speech</b> synthesis, problem solving, and planning is examined. Basic AI topics, including automation, search-oriented problem solving, knowledge representation, and computational logic, are discussed...|$|R
40|$|As we {{articulate}} speech, {{we usually}} move {{the head and}} exhibit various facial expressions. This visual aspect of <b>speech</b> aids <b>understanding</b> and helps communicating additional information, such as the speaker's mood. In this paper we analyze quantitatively head and facial movements that accompany speech and investigate how {{they relate to the}} text's prosodic structure...|$|R
50|$|Papa Legba is a loa in Haitian Vodou, {{who serves}} as the {{intermediary}} between the loa and humanity. He stands at a spiritual crossroads and gives (or denies) permission {{to speak with the}} spirits of Guinee, and is believed to speak all human languages. In Haiti, he is the great elocutioner. Legba facilitates communication, <b>speech,</b> and <b>understanding.</b>|$|R
5000|$|Prior to that (1976), {{was one of}} the co-inventors of {{the first}} {{continuous}} <b>speech</b> <b>understanding</b> systems, Hearsay-II, which became the “blackboard architecture.” ...|$|E
50|$|The {{results of}} {{international}} {{studies have shown}} a highly synergistic effect between hearing aid and cochlear implant technology, particularly evident in <b>speech</b> <b>understanding</b> in noise, pitch discrimination and music appreciation.|$|E
50|$|An {{audiometer}} typically transmits recorded sounds such as pure tones or {{speech to}} the headphones of the test subjectat varying frequencies and intensities, and records the subject's responses to produce an audiogram of threshold sensitivity, or <b>speech</b> <b>understanding</b> profile.|$|E
50|$|In {{addition}} to exhibiting the above symptoms, many adults also experience aphasia, {{which is a}} difficulty in expressing oneself when speaking, in <b>understanding</b> <b>speech,</b> or in reading and writing.|$|R
40|$|Audiovisual {{integration}} {{occurs when}} one perceives speech by using both auditory and visual cues. Research {{has shown that}} using both components improves and allows for quicker processing of speech. Children with an Autism Spectrum Disorder have deficits in communication skills ranging from difficulties in producing <b>speech</b> and <b>understanding</b> <b>speech.</b> This {{may be due to}} poor audiovisual integration. The following study focuses on different training methods for integrating both audio and visual cues along with a blurring manipulation within the face to improving audiovisual integration. Although there were significant differences between the different training conditions and significant differences between the different testing conditions, there was no interaction between the two. However, during the training phase, there were significant differences in that all three experimental groups significantly recognized more spoken words than the control group...|$|R
40|$|In {{order to}} {{initiate}} the formation of critical citizens from the Basic Education, this research aimed to investigate how a group of teachers from the local public from Londrina in Parana conceives of Science-Technology-Society-Environment Approach (CTSA) for the teaching of scientific content. Data were collected through questionnaires and speeches from the participating during a course entitled Science-Technology-Society, offered {{in the course of}} Specialization in Education and Technology of the Federal Technological University of Paraná, Brazil. According to the data obtained initially unaware of this entire group approach to teaching. Soon, they were not as elaborate, plan and develop scientific content prioritizing CTSA relations. Therefore, this research promoted participants the innovation pedagogical and methodological knowledge to work the CTSA relationships in the classroom, because teachers revealed in his <b>speeches</b> <b>understanding</b> of {{how important it is to}} develop the contents prioritizing CTSA relations to form citizens critical to society. Based on these data, we stress the importance of teaching continuing education that enables the study of the CTSA approach to mobilize a more qualitative basic education...|$|R
50|$|As {{part of the}} Universities Excellence Initiative, the {{university}} was awarded a Cluster of Excellence for its initiative Hearing4all. The cluster deals with research into the improvement of <b>speech</b> <b>understanding</b> in background noise and has a funding of €34 million.|$|E
50|$|In {{what has}} been called the Feigenbaum test, the {{inventor}} of expert systems argued for subject specific expert tests. A paper by Jim Gray of Microsoft in 2003 suggested extending the Turing test to <b>speech</b> <b>understanding,</b> speaking and recognizing objects and behavior.|$|E
50|$|Like {{computer}} vision versus image processing, computer audition versus audio engineering deals with understanding of audio rather than processing. It also differs from problems of <b>speech</b> <b>understanding</b> by machine since {{it deals with}} general audio signals, such as natural sounds and musical recordings.|$|E
6000|$|... "I think," [...] Abdul plucked at Mr. Groombride's sleeves, [...] "I {{think perhaps}} it is better now, Sar, if you give your fine little native <b>speech.</b> They not <b>understanding</b> English, but much pleased at your condescensions." ...|$|R
40|$|Abstract — Hidden-Markov Models (HMMs) {{have been}} widely used for <b>speech</b> processing, <b>understanding,</b> and {{synthesis}} with great success. The purpose of this work is to apply this prior knowledge and investigate the effectiveness of HMMs on shortduration percussive musical signals. Three main topics of interest are investigated: isolated instrument recognition, isolated rhythm transcription for the purpose of genre recognition, and isolated instrument synthesis. Overall, satisfactory results were achieved with clear motivation for improvement. I...|$|R
5000|$|T. multiceps: Diagnosed for {{the first}} time in 1913 in Paris, when a man {{presented}} symptoms of CNS nerve degeneration. He had convulsions and trouble speaking/ <b>understanding</b> <b>speech.</b> During his autopsy, two coenuri were found in his brain.|$|R
50|$|William Aaron Woods (born June 17, 1942), {{generally}} {{known as}} Bill Woods, is a researcher in natural language processing, continuous <b>speech</b> <b>understanding,</b> knowledge representation, and knowledge-based search technology. He is currently interested in using technology {{to help people}} organize and use information in organizations.|$|E
5000|$|Dr. James Baker {{laid out}} the {{description}} of a <b>speech</b> <b>understanding</b> system called DRAGON in 1975. In 1982 he and Dr. Janet M. Baker, his wife, founded Dragon Systems to release products centered around their voice recognition prototype. He was President {{of the company and}} she was CEO.|$|E
5000|$|Clark in December 1978 {{arranged}} {{that his}} audiologist present open-set words {{to his first}} patient, {{who was able to}} identify several correctly. Clark realised then that this was the breakthrough in providing <b>speech</b> <b>understanding</b> that everyone had been hoping for."it was the moment I had been waiting for. I went into the adjoining room and cried for joy." ...|$|E
40|$|Age {{demographics}} {{have led}} {{to an increase in}} the proportion of the population suffering from some form of hearing loss. The introduction of object-based audio to television broadcast has the potential to improve the viewing experience for millions of hearing impaired people. Personalization of object-based audio can assist in overcoming difficulties in <b>understanding</b> <b>speech</b> and <b>understanding</b> the narrative of broadcast media. The research presented here documents a Multi-Dimensional Audio (MDA) implementation of object-based clean audio to present independent object streams based on object category elicitation. Evaluations were carried out with hearing impaired people and participants were able to personalize audio levels independently for four object-categories using an on-screen menu: speech, music, background effects and foreground effects related to on-screen events. Results show considerable preference variation across subjects but indicate that expanding object-category personalization beyond a binary speech/non-speech categorization can substantially improve the viewing experience for some hearing impaired people...|$|R
40|$|This paper {{describes}} recent {{progress at}} Tokyo Institute of Technology and the author's perspectives for making speech recognition systems more flexible {{at both the}} acoustic and linguistic processing levels. Specifically, it describes a broadcast news transcription system, a multimodal dialogue system for information retrieval, neural-network-based HMM adaptation for noisy speech, online incremental speaker adaptation combined with automatic speaker-change detection, message-driven <b>speech</b> recognition and <b>understanding,</b> <b>speech</b> summarization, a Japanese national project on spontaneous speech corpus and processing technology, and speech recognition in the ubiquitous/wearable computing environment. For processing spontaneous speech, indispensable will be a paradigm shift from <b>speech</b> recognition to <b>understanding</b> where underlying messages of the speaker are extracted, instead of transcribing all the spoken words. Building a large corpus of spontaneous speech to construct reliable acoustic and linguistic models is also crucial. Due principally to the technology of making computers smaller, more powerful and cheaper, the ubiquitous and wearable computing era is expected to come into being in the initial years of the 21 st century. In such an environment, speech recognition will be widely used {{as one of the}} principal methods of humancomputer interaction...|$|R
40|$|<b>Understanding</b> <b>speech</b> in the {{presence}} of noise is a difficult task for cochlear implant patients. This study examines the real-world effectiveness of different signal processing approaches available in the Cochlear Nucleus Freedom device to enhance speech perception in noise for cochlear implant recipients...|$|R
