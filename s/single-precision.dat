387|0|Public
25|$|In binary <b>single-precision</b> floating-point, this is {{represented}} as s=1.10010010000111111011011 with e=1.|$|E
25|$|Until the {{discovery}} of the number field sieve (NFS), QS was the asymptotically fastest known general-purpose factoring algorithm. Now, Lenstra elliptic curve factorization has the same asymptotic running time as QS (in the case where n has exactly two prime factors of equal size), but in practice, QS is faster since it uses <b>single-precision</b> operations instead of the multi-precision operations used by the elliptic curve method.|$|E
25|$|A {{compiler}} {{may also}} use the static type of a value to optimize the storage it needs and the choice of algorithms for operations on the value. In many C compilers the float data type, for example, is represented in 32 bits, {{in accord with the}} IEEE specification for <b>single-precision</b> floating point numbers. They will thus use floating-point-specific microprocessor operations on those values (floating-point addition, multiplication, etc.).|$|E
25|$|MIPS V added a {{new data}} type, the Paired Single (PS), which {{consisted}} of two <b>single-precision</b> (32-bit) floating-point numbers stored in the existing 64-bit floating-point registers. Variants of existing floating-point instructions for arithmetic, compare and conditional move were added to operate on this data type in a SIMD fashion. New instructions were added for loading, rearranging and converting PS data. It was the first instruction set to exploit floating-point SIMD with existing resources.|$|E
25|$|The {{justification}} for this definition {{is to preserve}} {{the sign of the}} result in case of arithmetic underflow. For example, in the <b>single-precision</b> computation 1/(x/2), where x = ±2−149, the computation x/2 underflows and produces ±0 with sign matching x, and the result will be ±∞ with sign matching x. The sign will match that of the exact result ±2150, but the magnitude of the exact result is too large to represent, so infinity is used to indicate overflow.|$|E
25|$|The FireStream line is {{a series}} of add-on {{expansion}} cards released from 2006 to 2010, based on standard Radeon GPUs but designed to serve as a general-purpose co-processor, rather than rendering and outputting 3D graphics. Like the FireGL/FirePro line, they were given more memory and memory bandwidth, but the FireStream cards do not necessarily have video output ports. All support 32-bit <b>single-precision</b> floating point, and all but the first release support 64-bit double-precision. The line was partnered with new APIs to provide higher performance than existing OpenGL and Direct3D shader APIs could provide, beginning with Close to Metal, followed by OpenCL and the Stream Computing SDK, and eventually integrated into the APP SDK.|$|E
500|$|Since this {{algorithm}} {{relies heavily}} on the bit-level representation of <b>single-precision</b> floating point numbers, a short overview of this representation is provided here. In order to encode a non-zero real number [...] as a single precision float, {{the first step is}} to write [...] as a normalized binary number: ...|$|E
2500|$|Squaring it with <b>single-precision</b> floating-point {{hardware}} (with rounding) gives ...|$|E
2500|$|... (NB. This website {{contains}} {{open source}} floating-point IP cores {{for the implementation}} of floating-point operators in FPGA or ASIC devices. The project double_fpu contains verilog source code of a double-precision floating-point unit. The project fpuvhdl contains vhdl source code of a <b>single-precision</b> floating-point unit.) ...|$|E
2500|$|The way {{in which}} the significand (including its sign) and {{exponent}} are stored in a computer is implementation-dependent. The common IEEE formats are described in detail later and elsewhere, but as an example, in the binary <b>single-precision</b> (32-bit) floating-point representation, , and so the significand is a string of 24 bits. [...] For instance, the number π's first 33 bits are: ...|$|E
2500|$|The IBM 7094, also {{introduced}} in 1962, supports <b>single-precision</b> and double-precision representations, {{but with no}} relation to the UNIVAC's representations. Indeed, in 1964, IBM introduced proprietary hexadecimal floating-point representations in its System/360 mainframes; these same representations are still available for use in modern z/Architecture systems. [...] However, in 1998, IBM included IEEE-compatible binary floating-point arithmetic to its mainframes; in 2005, IBM also added IEEE-compatible decimal floating-point arithmetic.|$|E
2500|$|VFP (Vector Floating Point) {{technology}} is an FPU (Floating-Point Unit) coprocessor extension to the ARM architecture (implemented differently in ARMv8 - coprocessors not defined there). It provides low-cost <b>single-precision</b> and double-precision floating-point computation fully compliant with the ANSI/IEEE Std 754-1985 Standard for Binary Floating-Point Arithmetic. VFP provides floating-point computation {{suitable for a}} wide spectrum of applications such as PDAs, smartphones, voice compression and decompression, three-dimensional graphics and digital audio, printers, set-top boxes, and automotive applications. The VFP architecture was intended to support execution of short [...] "vector mode" [...] instructions but these operated on each vector element sequentially and thus did not offer the performance of true single instruction, multiple data (SIMD) vector parallelism. This vector mode was therefore removed shortly after its introduction, to be replaced with the much more powerful NEON Advanced SIMD unit.|$|E
2500|$|The Advanced SIMD {{extension}} (aka NEON or [...] "MPE" [...] Media Processing Engine) is {{a combined}} 64- and 128-bit SIMD instruction set that provides standardized acceleration for media and signal processing applications. NEON {{is included in}} all Cortex-A8 devices, but is optional in Cortex-A9 devices. NEON can execute MP3 audio decoding on CPUs running at 10MHz, and can run the GSM adaptive multi-rate (AMR) speech codec at no more than 13MHz. It features a comprehensive instruction set, separate register files, and independent execution hardware. NEON supports 8-, 16-, 32-, and 64-bit integer and <b>single-precision</b> (32-bit) floating-point data and SIMD operations for handling audio and video processing as well as graphics and gaming processing. In NEON, the SIMD supports up to 16operations at the same time. The NEON hardware shares the same floating-point registers as used in VFP. Devices such as the ARM Cortex-A8 and Cortex-A9 support 128-bit vectors, but will execute with 64bits at a time, whereas newer Cortex-A15 devices can execute 128bits at a time.|$|E
2500|$|Each SPE is a dual {{issue in}} order {{processor}} {{composed of a}} [...] "Synergistic Processing Unit", SPU, and a [...] "Memory Flow Controller", MFC (DMA, MMU, and bus interface). SPEs don't have any branch prediction hardware (hence there is a heavy burden on the compiler). Each SPE has 6 execution units divided among odd and even pipelines on each SPE : The SPU runs a specially developed instruction set (ISA) with 128-bit SIMD organization for single and double precision instructions. With {{the current generation of}} the Cell, each SPE contains a 256KiB embedded SRAM for instruction and data, called [...] "Local Storage" [...] (not to be mistaken for [...] "Local Memory" [...] in Sony's documents that refer to the VRAM) which is visible to the PPE and can be addressed directly by software. Each SPE can support up to 4 GiB of local store memory. The local store does not operate like a conventional CPU cache since it is neither transparent to software nor does it contain hardware structures that predict which data to load. The SPEs contain a 128-bit, 128-entry register file and measures 14.5mm2 on a 90nm process. An SPE can operate on sixteen 8-bit integers, eight 16-bit integers, four 32-bit integers, or four <b>single-precision</b> floating-point numbers in a single clock cycle, as well as a memory operation. Note that the SPU cannot directly access system memory; the 64-bit virtual memory addresses formed by the SPU must be passed from the SPU to the SPE memory flow controller (MFC) to set up a DMA operation within the system address space.|$|E
5000|$|... 5 Double {{precision}} {{performance of}} the GTX Titan & GTX Titan Black is either 1/3 or 1/24 of <b>single-precision</b> performance depending on a user-selected configuration option in the driver that boosts <b>single-precision</b> performance if double-precision is set to 1/24 of <b>single-precision</b> performance, while other Kepler chips' double precision performance is fixed at 1/24 of <b>single-precision</b> performance. GeForce 700 series Maxwell chips' double precision performance is 1/32 of <b>single-precision</b> performance.|$|E
5000|$|Note: The Cortex-M4 and M33 has a silicon FPU {{option of}} <b>single-precision</b> (SP). The Cortex-M7 has silicon FPU options of <b>single-precision</b> (SP), or both <b>single-precision</b> (SP) and {{double-precision}} (DP). If the Cortex-M4 / M7 / M33 has a FPU, {{then it is}} known as the Cortex-M4F / Cortex-M7F / Cortex-M33F.|$|E
5000|$|A [...] "Streaming Multiprocessor" [...] {{corresponds}} to AMD's Compute Unit. An SMP encompasses 128 <b>single-precision</b> ALUs ("CUDA cores") on GP104 chips and 64 <b>single-precision</b> ALUs on GP100 chips.|$|E
50|$|Here {{we start}} with 0 in 32-bit <b>single-precision</b> and {{repeatedly}} add 1 until the operation is idempotent. The result is equal to 224 since the significand for a <b>single-precision</b> number in this example contains 24 bits.|$|E
50|$|The floating-point unit (FPU) was a fast <b>single-precision</b> (32-bit) design, for {{reduced cost}} and to benefit SGI, whose {{mid-range}} 3D graphics workstations relied mostly on <b>single-precision</b> math for 3D graphics applications. It was fully pipelined, {{which made it}} significantly better {{than that of the}} R4700. The R5000 implements the multiply-add instruction of the MIPS IV ISA. <b>Single-precision</b> adds, multiplies and multiply-adds have a four-cycle latency and a one cycle throughput. <b>Single-precision</b> divides have a 21-cycle latency and a 19-cycle throughput, while square roots have a 26-cycle latency and a 38-cycle throughput. Division and square-root was not pipelined. Instructions that operate on double precision numbers have a significantly higher latency and lower throughput except for add, which has identical latency and throughput with <b>single-precision</b> add. Multiply and multiply-add have a five-cycle latency and a two-cycle throughput. Divide has a 36-cycle latency and a 34-cycle throughput. Square root has a 68-cycle latency and a 66-cycle throughput.|$|E
50|$|This gives from 6 to 9 {{significant}} decimal digits precision. If a decimal string with at most 6 significant digits {{is converted}} to IEEE 754 <b>single-precision</b> representation, and then converted back to a decimal string with {{the same number of}} digits, the final result should match the original string. If an IEEE 754 <b>single-precision</b> number {{is converted to}} a decimal string with at least 9 significant digits, and then converted back to <b>single-precision</b> representation, the final result must match the original number.|$|E
50|$|The 16-core Parallella has roughly 5.0 GFLOPs/W, and the 64-core Epiphany-IV {{made with}} 28 nm {{estimated}} as 50 GFLOPs/W (<b>single-precision),</b> and 32-board {{system based on}} them has 15 GFLOPS/W. For comparison, top GPUs from AMD and Nvidia reached 10 GFLOPs/W for <b>single-precision</b> in 2009-2011 timeframe.|$|E
5000|$|<b>Single-precision</b> numbers occupy 32 bits. In single precision: ...|$|E
5000|$|Here {{are some}} {{examples}} of <b>single-precision</b> IEEE 754 representations: ...|$|E
5000|$|One Fermi SM {{combines}} 32 <b>single-precision</b> (FP32) shader processors ...|$|E
5000|$|One Maxwell SM {{combines}} 128 <b>single-precision</b> (FP32) shader processors ...|$|E
5000|$|One Tesla SM {{combines}} 8 <b>single-precision</b> (FP32) shader processors ...|$|E
5000|$|... #Subtitle level 3: Converting from <b>single-precision</b> binary to decimal ...|$|E
5000|$|Squaring it with <b>single-precision</b> floating-point {{hardware}} (with rounding) gives ...|$|E
5000|$|... 10 digits (mantissa) + 2 digits (exponent) in <b>single-precision</b> mode.|$|E
5000|$|... #Subtitle level 2: IEEE 754 <b>single-precision</b> binary floating-point format: binary32 ...|$|E
50|$|Consider {{encoding}} {{the value}} −118.625 as an IBM <b>single-precision</b> floating-point value.|$|E
5000|$|A <b>single-precision</b> binary floating-point {{number is}} stored in a 32-bit word: ...|$|E
50|$|SSE {{floating}} point SIMD. Four <b>single-precision</b> {{floating point}} numbers per clock cycle.|$|E
5000|$|Four {{double-precision}} floating-point pipelines, {{which can}} also act as eight <b>single-precision</b> pipelines ...|$|E
5000|$|<b>Single-precision</b> {{forms of}} some {{floating}} point instructions, {{in addition to}} double-precision forms ...|$|E
50|$|On the GP104 chip an SM {{consists}} of 128 <b>single-precision</b> ALUs ("CUDA cores"), on the GP100 of 64 <b>single-precision</b> ALUs. Due to different {{organization of the}} chips, like number of double precision ALUs, the theoretical double precision performance of the GP100 is half of the theoretical one for single precision; the ratio is 1/32 for the GP104 chip.|$|E
