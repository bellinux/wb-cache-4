244|0|Public
2500|$|For {{a general}} {{distribution}} of the process , {{it may be hard}} to solve these dynamic programming equations. The situation simplifies dramatically if the process [...] is <b>stagewise</b> independent, i.e., [...] is (stochastically) independent of [...] for [...] In this case, the corresponding conditional expectations become unconditional expectations, and the function , [...] does not depend on [...] That is, [...] is the optimal value of the problem ...|$|E
5000|$|It {{is easily}} {{modified}} to produce efficient algorithms for other methods producing similar results, like the lasso and forward <b>stagewise</b> regression.|$|E
5000|$|... "The optimal {{design of}} <b>stagewise</b> {{adiabatic}} reactors." [...] Paper {{presented at the}} AIChE/ORSA Symposium on Optimization in Chemical Engineering, New York, 1960.|$|E
50|$|He also attends festivals such as Camp Bestival in Dorset to {{entertain}} children. Fletcher is a patron for Bournemouth-based performing arts school <b>Stagewise.</b>|$|E
5000|$|... <b>stagewise</b> gas-liquid contactors: the vapor-liquid {{equilibrium}} {{is reached}} within {{each stage of}} the equipment and mass transfer happens in a part only {{of the volume of}} each stage.|$|E
50|$|Matching pursuit {{is related}} to the field of {{compressed}} sensing and has been extended by researchers in that community. Notable extensions are Orthogonal Matching Pursuit (OMP), <b>Stagewise</b> OMP (StOMP), compressive sampling matching pursuit (CoSaMP), Generalized OMP (gOMP), and Multipath Matching Pursuit (MMP).|$|E
5000|$|Royce's 1970 {{paper is}} {{generally}} considered to be the paper which defined the <b>stagewise</b> [...] "waterfall" [...] model of the software process. But it is surprising to see both that the earlier Benington and Hosier papers had good approximations to the waterfall model, and that Royce's paper already incorporates prototyping as an essential step compatible with the waterfall model.|$|E
5000|$|For {{a general}} {{distribution}} of the process , {{it may be hard}} to solve these dynamic programming equations. The situation simplifies dramatically if the process [...] is <b>stagewise</b> independent, i.e., [...] is (stochastically) independent of [...] for [...] In this case, the corresponding conditional expectations become unconditional expectations, and the function , [...] does not depend on [...] That is, [...] is the optimal value of the problem ...|$|E
50|$|The {{meaning of}} p-values in {{sequential}} analyses also changes, because when using sequential analyses, {{more than one}} analysis is performed, and the typical definition of a p-value as the data “at least as extreme” as is observed needs to be redefined. One solution is to order the p-values {{of a series of}} sequential tests based on the time of stopping and how high the test statistic was at a given look, which is known as <b>stagewise</b> ordering, first proposed by Armitage.|$|E
5000|$|In related work, Swerling made {{significant}} contributions to the optimal estimation of orbits of satellites and trajectories of missiles. [...] Working in the fields of least-squares estimation and signal processing, Swerling published papers in 1958 and 1959 on [...] "stagewise" [...] smoothing, the first efforts to exploit the computational advantages of applying recursion to least-squares problems. His work, particularly [...] "First-Order Error Propagation in a <b>Stagewise</b> Smoothing Procedure for Satellite Observations," [...] anticipated that of Rudolf E. Kálmán, whose linear quadratic estimation technique became known as the Kalman filter.|$|E
40|$|<b>Stagewise</b> {{estimation}} is a slow-brewing {{approach for}} model building that has recently experienced a revival {{due to its}} computational efficiency, its flexibility in handling complex data structures, and its intrinsic connections with penalized estimation. Synthesizing generalized estimating equations to handle correlated non-Gaussian data with <b>stagewise</b> techniques, this thesis proposes general <b>stagewise</b> estimation approaches that perform model selection {{in the presence of}} complex covariate structures. First, the setting where there is a prior covariate grouping structure or hierarchy is considered. As the grouping structure in practice is often not ideal as even important groups may contain unimportant variables, the key is to simultaneously conduct group selection and within-group variable selection, or in other words, bi-level selection. This thesis presents two approaches to address the challenge. The first is the bi-level <b>stagewise</b> estimating equations (BiSEE) approach, which is shown to correspond to the sparse group lasso penalized regression. The second is the hierarchical <b>stagewise</b> estimating equations (HiSEE) approach that can handle a more general hierarchical grouping structure, in which each <b>stagewise</b> estimation step itself is executed as a hierarchical selection process based on the grouping structure. The second setting explored is regression with interaction terms. As it is often required that main effect terms be included when an interaction term is part of a model, the goal is to perform variable selection that maintains the variable hierarchy. Two approaches are proposed by this thesis. The first is a hierarchical lasso <b>stagewise</b> estimating equations approach, which is shown to directly correspond to the hierarchical lasso penalized regression. The second is a <b>stagewise</b> active set approach, which enforces the variable hierarchy by conforming the selection to a properly growing active set in each <b>stagewise</b> estimation step. Simulation studies are presented to show the efficacy and superior computational efficiency of the proposed approaches. The approaches are also used to study the association between the suicide-related hospitalization rates among 15 [...] 19 year olds in Connecticut and the characteristics of the school districts in which they reside...|$|E
40|$|Forward <b>stagewise</b> {{regression}} {{follows a}} very simple strategy for constructing a sequence of sparse regression estimates: it starts with all coefficients equal to zero, and iteratively updates the coefficient (by a small amount ϵ) of the variable that achieves the maximal absolute inner product with the current residual. This procedure has an interesting connection to the lasso: under some conditions, {{it is known that}} the sequence of forward <b>stagewise</b> estimates exactly coincides with the lasso path, as the step size ϵ goes to zero. Furthermore, essentially the same equivalence holds outside of least squares regression, with the minimization of a differentiable convex loss function subject to an ℓ_ 1 norm constraint (the <b>stagewise</b> algorithm now updates the coefficient corresponding to the maximal absolute component of the gradient). Even when they do not match their ℓ_ 1 -constrained analogues, <b>stagewise</b> estimates provide a useful approximation, and are computationally appealing. Their success in sparse modeling motivates the question: can a simple, effective strategy like forward <b>stagewise</b> be applied more broadly in other regularization settings, beyond the ℓ_ 1 norm and sparsity? The current paper is an attempt to do just this. We present a general framework for <b>stagewise</b> estimation, which yields fast algorithms for problems such as group-structured learning, matrix completion, image denoising, and more. Comment: 56 pages, 15 figure...|$|E
40|$|We {{consider}} the least angle regression and forward <b>stagewise</b> algorithms for solving penalized least squares regression problems. In Efron et al. (2004) {{it is proven}} that the least angle regression algorithm, with a small modification, solves the lasso (L 1 constrained) regression problem. Here we give an analogous result for incremental forward <b>stagewise</b> regression, showing that it fits a monotone version of the lasso. We also study a condition under which the coe#cient paths of the lasso are monotone, and hence the di#erent algorithms all coincide. Finally, we compare the lasso and forward <b>stagewise</b> procedures in a simulation study involving {{a large number of}} correlated predictors...|$|E
40|$|Recent {{approaches}} to collaborative filtering {{have concentrated on}} estimating an algebraic or statistical model, and using the model for predicting missing ratings. In this paper we observe that different models have relative advantages in {{different regions of the}} input space. This motivates our approach of using <b>stagewise</b> linear combinations of collaborative filtering algorithms, with non-constant combination coefficients based on kernel smoothing. The resulting <b>stagewise</b> model is computationally scalable and outperforms a wide selection of state-of-the-art collaborative filtering algorithms. ...|$|E
40|$|The {{differential}} {{dynamic programming}} algorithm (DDP) and the <b>stagewise</b> Newton procedure are two typical examples of efficient local procedures for discrete-time optimal control (DTOC) problems. It is desirable to generalize these local procedures to globally convergent methods. One successful globalization was recently proposed by Coleman and Liao [3] which combines the trust region idea with Pantoja's <b>stagewise</b> Newton procedure. In this {{paper we propose}} several algorithms for DTOC problems which combine a modified "dogleg" algorithm with DDP or Pantoja's Newton procedure. These algorithms possess advantages of both the dogleg algorithm and the DDP or the <b>stagewise</b> procedure, i. e., they have strong global and local convergence properties yet remain economical. Numerical results are presented to compare these algorithms and the Coleman-Liao algorithm. 1 This research was partially supported by the Cornell Theory Center, which receives major funding from the National Science Foundation [...] ...|$|E
40|$|Abstract — Recent {{advances}} {{in computer technology}} allow the implementation of some important methods that were assigned lower priority in the past due to their computational burdens. Second-order backpropagation (BP) is such a method that computes the exact Hessian matrix of a given objective function. We describe two algorithms for feed-forward neural-network (NN) learning with emphasis on how to organize Hessian elements into a so-called stagewise-partitioned block-arrow matrix form: (1) <b>stagewise</b> BP, {{an extension of the}} discrete-time optimal-control <b>stagewise</b> Newton of Dreyfus 1966; and (2) nodewise BP, based on direct implementation of the chain rule for differentiation attributable to Bishop 1992. The former, a more systematic and cost-efficient implementation in both memory and operation, progresses in the same layer-by-layer (i. e., <b>stagewise)</b> fashion as the widely-employed first-order BP computes the gradient vector. We also show intriguing separable structures of each block in the partitioned Hessian, disclosing the rank of blocks. I...|$|E
40|$|Discrete-time {{optimal control}} (DTOC) {{problems}} are large-scale optimization {{problems with a}} dynamic structure. In previous work this structure has been exploited to provide very fast and efficient local procedures. Two examples are the differential dynamic programming algorithm (DDP) and the <b>stagewise</b> Newton procedure [...] both require only O(N) operations per iteration, where N {{is the number of}} timesteps. Both exhibit a quadratic convergence rate. However, most algorithms in this category do not have a satisfactory global convergence strategy. The most popular global strategy is shifting: this sometimes works poorly {{due to the lack of}} automatic adjustment to the shifting element. In this paper we propose a method that incorporates the trust region idea with the local <b>stagewise</b> Newton's method. This method possesses advantages of both the trust region idea and the <b>stagewise</b> Newton's method, i. e., our proposed method has strong global and local convergence properties yet remains economical [...] ...|$|E
40|$|Algorithms for {{simultaneous}} shrinkage {{and selection}} in regression and classification provide attractive solutions to knotty old statistical challenges. Nevertheless, {{as far as}} we can tell, Tibshirani’s Lasso algorithm has had little impact on statistical practice. Two particular reasons for this may be the relative inefficiency of the original Lasso algorithm and the relative complexity of more recent Lasso algorithms [e. g., Osborne, Presnell and Turlach (2000) ]. Efron, Hastie, Johnstone and Tibshirani have provided an efficient, simple algorithm for the Lasso as well as algorithms for <b>stagewise</b> regression and the new least angle regression. As such this paper is an important contribution to statistical computing. 1. Predictive performance. The authors say little about predictive performance issues. In our work, however, the relative out-of-sample predictive performance of LARS, Lasso and Forward <b>Stagewise</b> (and variants thereof) takes center stage. Interesting connections exist between boosting and <b>stagewise</b> algorithms so predictive comparisons with boosting are also of interest...|$|E
30|$|Another kind of {{reconstruction}} algorithms are named greedy pursuit algorithms including orthogonal matching pursuit (OMP)[18], subspace pursuit (SP)[19], <b>stagewise</b> orthogonal matching pursuit (StOMP)[20], regularized orthogonal matching pursuit (ROMP)[21] and sparsity adaptive matching pursuit (SAMP)[22].|$|E
40|$|We {{consider}} the least angle regression and forward <b>stagewise</b> algorithms for solving penalized least squares regression problems. In Efron, Hastie, Johnstone & Tibshirani (2004) it is {{proved that the}} least angle regression algorithm, with a small modification, solves the lasso regression problem. Here we give an analogous result for incremental forward <b>stagewise</b> regression, showing that it solves {{a version of the}} lasso problem that enforces monotonicity. One consequence of this is as follows: while lasso makes optimal progress in terms of reducing the residual sum-of-squares per unit increase in L_ 1 -norm of the coefficient β, forward stage-wise is optimal per unit L_ 1 arc-length traveled along the coefficient path. We also study a condition under which the coefficient paths of the lasso are monotone, and hence the different algorithms coincide. Finally, we compare the lasso and forward <b>stagewise</b> procedures in a simulation study involving a large number of correlated predictors. Comment: Published at [URL] in the Electronic Journal of Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|E
3000|$|... 1 -relaxation and greedy {{approaches}} {{based on}} the cosparsity of the signal. This paper presents a novel greedy-like algorithm, called Cosparsity-based <b>Stagewise</b> Matching Pursuit (CSMP), where the cosparsity of the target signal is estimated adaptively with a <b>stagewise</b> approach composed of forward and backward processes. In the forward process, the cosparsity is estimated and the signal is approximated, followed by the refinement of the cosparsity and the signal in the backward process. As a result, the target signal can be reconstructed without the prior information of the cosparsity level. Experiments show that {{the performance of the}} proposed algorithm is comparable to those of the l [...]...|$|E
40|$|This {{paper is}} {{concerned}} with rank aggregation, which aims to combine multiple input rankings {{to get a better}} ranking. A popular approach to rank aggregation is based on probabilistic models on permutations, e. g., the Luce model and the Mallows model. However, these models have their limitations in either poor expressiveness or high computational complexity. To avoid these limitations, in this paper, we propose a new model, which is defined with a coset-permutation distance, and models the generation of a permutation as a <b>stagewise</b> process. We refer to the new model as coset-permutation distance based <b>stagewise</b> (CPS) model. The CPS model has rich expressiveness and can therefore be used in versatile applications, because many different permutation distances can be used to induce the coset-permutation distance. The complexity of the CPS model is low because of the <b>stagewise</b> decomposition of the permutation probability and the efficient computation of most coset-permutation distances. We apply the CPS model to supervised rank aggregation, derive the learning and inference algorithms, and empirically study their effectiveness and efficiency. Experiments on public datasets show that the derived algorithms based on the CPS model can achieve state-ofthe-art ranking accuracy, and are much more efficient than previous algorithms. ...|$|E
40|$|The naïve Bayes {{approach}} is a simple but often satisfactory method for supervised classification. In this paper, {{we focus on the}} naïve Bayes model and propose the application of regularization techniques to learn a naïve Bayes classifier. The main contribution of the paper is a <b>stagewise</b> version of the selective naïve Bayes, which can be considered a regularized version of the naïve Bayes model. We call it forward <b>stagewise</b> naïve Bayes. For comparison’s sake, we also introduce an explicitly regularized formulation of the naïve Bayes model, where conditional independence (absence of arcs) is promoted via an L 1 /L 2 -group penalty on the parameters that define the conditional probability distributions. Although already published in the literature, this idea has only been applied for continuous predictors. We extend this formulation to discrete predictors and propose a modification that yields an adaptive penalization. We show that, whereas the L 1 /L 2 group penalty formulation only discards irrelevant predictors, the forward <b>stagewise</b> naïve Bayes can discard both irrelevant and redundant predictors, which are known to be harmful for the naïve Bayes classifier. Both approaches, however, usually improve the classical naïve Bayes model’s accuracy...|$|E
30|$|Many {{state-of-the-art}} greedy algorithms nowadays {{are based}} on OMP. Examples include regularized OMP (ROMP) [55, 56], <b>stagewise</b> OMP (StOMP) [57], compressive sampling matching pursuit (CoSaMP) [58], probability OMP (PrOMP) [59], look ahead OMP [60], OMP with replacement (OMPR) [61], A* OMP [62] etc.|$|E
40|$|Two <b>stagewise</b> {{classification}} algorithms are given, {{one with}} Type I error control and one without. They use {{a test of}} additional classifi-cation accuracy at each stage to decide which groups of variables to add or drop, if any. The standardized differenee in estimated Bayes risk between two subsets of groups of allocation variables is the test statistic used. For a multinormal example, the algorithms are compared by the estimated Bayes risks of their ultimately selected subsets. Stepwise and simul-taneous stepdown classifications do not perform as well as minimal-best-subset classification. To improve the optimality of a subset seleeted by a <b>stagewise</b> classification, {{it is necessary to}} append an extra test of aecuraey of the seleeted subset versus the full set of groups...|$|E
40|$|This article {{presents}} {{a computer program}} developed for teaching of the <b>stagewise</b> distillation process, {{to be used by}} undergraduate chemical engineering students. Tt exposes a separation processes overview, emphasizing in distillation, its origins, importance and prospects. Phase equilibrium is then studied, as well as the equilibrium stage concept, both fundamental for <b>stagewise</b> distillation calculations. In the latter aspect an explanation of representative mathematic models is presented in a relatively complete way for binary mixtures, whereas an approximation is made to the study of multicomponent mixture models. Tutor programs are implemented presenting step-by-step procedures to solve user specified problems at the time results are obtained. Moreover, with a problem solving focused intention, simulation programs are added for the different studied models...|$|E
40|$|Many {{regression}} and classification algorithms proposed {{over the}} years {{can be described as}} greedy procedures for the <b>stagewise</b> minimization of an appropriate cost function. Some examples include additive models, matching pursuit, and boosting. In this work we focus on the classification problem, for which many recent algorithms have been proposed and applied successfully. For a specific regularized form of greedy <b>stagewise</b> optimization, we prove consistency of the approach under rather general conditions. Focusing on specific classes of problems we provide conditions under which our greedy procedure achieves the (nearly) minimax rate of convergence, implying that the procedure cannot be improved in a worst case setting. We also construct a fully adaptive procedure, which, without knowing the smoothness parameter of the decision boundary, converges {{at the same rate as}} if the smoothness parameter were known. 1...|$|E
30|$|The {{second class}} is greedy search {{algorithms}} identifying the support (position of nonzero element) of the sparse signal sequentially. In each iteration of these algorithms, correlations between each column of Φ and the modified measurement (residual) are compared and the index (indices) {{of one or}} multiple columns that are most strongly correlated with the residual is identified as the support. In general, the computational complexity of greedy algorithms is {{much smaller than the}} LP based techniques, in particular for the highly sparse signals (signals with small K). Algorithms contained in this category include orthogonal matching pursuit (OMP) [1], regularized OMP (ROMP) [18], <b>stagewise</b> OMP (DL Donoho, I Drori, Y Tsaig, JL Starck: Sparse solution of underdetermined linear equations by <b>stagewise</b> orthogonal matching pursuit, submittd), and compressive sampling matching pursuit (CoSaMP) [16].|$|E
40|$|Variable {{selection}} plays {{a significant}} role in statistics. There are many variable selection methods. Forward <b>stagewise</b> regression takes a different approach among those. In this thesis Least Angle Regression (LAR) is discussed in detail. This approach has similar principles as forward <b>stagewise</b> regression but does not suffer from its computational difficulties. By using a small artificial data set and the well-known Longley data set, the LAR algorithm is illustrated in detail and the coefficient profiles are obtained. Furthermore a penalized approach to variable reduction called the LASSO is discussed, and it is shown how to compute its coefficient profiles efficiently using the LAR algorithm with a small modification. Finally, a method called K-fold cross validation used to select the constraint parameter for the LASSO is presented and illustrated with the Longley data...|$|E
30|$|Much of {{the work}} in CS is handled by the {{reconstruction}} phase which uses wavelet basis to reconstruct the dataset from the compressed samples. Different algorithms exist in the literature. Here, we use a greedy algorithm called <b>stagewise</b> orthogonal matching pursuit (StOMP) [21] that has been empirically demonstrated to be very efficient.|$|E
40|$|We {{introduce}} <b>stagewise</b> processing in error-correcting {{codes and}} image restoration, by extracting {{information from the}} former stage and using it selectively to improve {{the performance of the}} latter one. Both mean- eld analysis using the cavity method and simulations show that it has the advantage of being robust against uncertainties in hyperparameter estimation...|$|E
40|$|Differential Dynamic Programming (DDP) and <b>stagewise</b> Newton's method {{are both}} quadratically {{convergent}} algorithms for solving discrete time optimal control problem. Although these two algorithms share many theoretical similarities, they demonstrate significantly different numerical performance. In this paper, we will compare and analyze these two algorithms in detail, and derive another quadratically convergent algorithm {{which is a}} combination of the DDP algorithm and Newton's method. This new second-order algorithm {{plays a key role in}} the explanation of the numerical differences between the DDP algorithm and Newton's method. The detailed algorithmic and structural differences for these three algorithms and their impact on numerical performance will be discussed and explored. Two test problems with various dimensions solved by these three algorithms will be presented. One nonlinear test problem demonstrates that the DDP algorithm can be as much as 28 times faster than the <b>stagewise</b> New [...] ...|$|E
40|$|The {{performance}} of EM in learning mixtures of product distributions often {{depends on the}} initialization. This can be problematic in crowdsourcing and other applications, e. g. when {{a small number of}} 'experts' are diluted by a large number of noisy, unreliable participants. We develop a new EM algorithm that is driven by these experts. In a manner that differs from other approaches, we start from a single mixture class. The algorithm then develops the set of 'experts' in a <b>stagewise</b> fashion based on a mutual information criterion. At each stage EM operates on this subset of the players, effectively regularizing the E rather than the M step. Experiments show that <b>stagewise</b> EM outperforms other initialization techniques for crowdsourcing and neurosciences applications, and can guide a full EM to results comparable to those obtained knowing the exact distribution. Comment: 9 page...|$|E
40|$|First, I congratulate {{the authors}} {{for a truly}} {{stimulating}} paper. The paper resolves {{a number of important}} questions but, at the same time, raises many others. I would like to focus my comments to two specific points. 1. The similarity of <b>Stagewise</b> and LARS fitting to the Lasso suggests that the estimates produced by <b>Stagewise</b> and LARS fitting may minimize an objective function that is similar to the appropriate Lasso objective function. It is not at all (at least to me) obvious how this might work though. I note, though, that the construction of such an objective function may be easier than it seems. For example, in the case of bagging [Breiman (1996) ] or subagging [Bühlmann and Yu (2002) ], an “implied ” objective function can be constructed. Suppose that ̂ θ 1, [...] ., ̂ θm are estimates (e. g., computed from subsamples or bootstrap samples) that minimize, respectively, objective functions Z 1, [...] .,Zm and define ̂θ = g (̂ θ 1, [...] ., ̂ θm); then ̂ θ minimizes the objective function Z(t) = inf{Z 1 (t 1) + · · · + Zm(tm) :g(t 1, [...] .,tm) = t}. (Thanks to Gib Bassett for pointing this out to me.) A similar construction for <b>stagewise</b> fitting (or LARS in general) could facilitate the analysis of the statistical properties of the estimators obtained via these algorithms. 2. When I first started experimenting with the Lasso, I was impressed by its robustness to small changes in its tuning parameter relative to more classical stepwise subset selection methods such as Forward Selection and Backward Elimination. (This is well illustrated by Figure 5; at its best, Forward Selection is comparable to LARS, <b>Stagewise</b> and the Lasso but This is an electronic reprint of the original article published by the Institute of Mathematical Statistics in The Annals of Statistics, 2004, Vol. 32, No. 2, 458 – 460. This reprint differs from the original in pagination and typographic detail. 1...|$|E
3000|$|The CSMP {{algorithm}} {{adopts a}} <b>stagewise</b> approach [6] {{to estimate the}} real cosparsity in each stage in the forward process, which only requires the step size s to be set in initialization. Here, l {{is defined as the}} real cosparsity of the original signal, and s should not be larger than d[*]−[*]l normally [11]. The initial cosupport Λ [...]...|$|E
40|$|We {{propose a}} new,robust Boosting method {{by using a}} sigmoidal {{function}} as a loss function. In deriving the method,the <b>stagewise</b> additive modelling methodology is blended with the gradient descent algorithms. Based on intensive numerical experiments,we show that the proposed method in actually better tha AdaBoot in test error rates {{in the case of}} noisy,mislabeled situation. Includes bibliographical reference...|$|E
