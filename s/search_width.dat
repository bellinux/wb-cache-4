19|38|Public
50|$|Decision rule 3 is also {{optional}} and {{it focuses}} on the neighborhood constraint. GeoMod can geographically limit the simulated change to pixels {{that are on the}} edge between category 1 and category 2. The model can thus apply a user-defined minimum <b>search</b> <b>width</b> to constrain where simulated change occurs.|$|E
3000|$|... is the <b>search</b> <b>width.</b> This <b>search</b> <b>width</b> {{contains}} the correct {{position of the}} partial in nearly 100 % of the cases; a broader search region was avoided {{in order to reduce}} the chance of interference from other sources. If the position of the partial is less than [...]...|$|E
30|$|Given that {{participants}} opted to search using a common <b>search</b> <b>width,</b> {{an important question}} is, how exhaustive is each participants’ search within their search corridor? Accuracy varied markedly across participants despite using the common <b>search</b> <b>width</b> and so using the common <b>search</b> <b>width</b> did not ensure that search was exhaustive. At least for some participants, the failure to search exhaustively {{was associated with a}} faster movement time reflected in the reduced modal time before turning. The implication of speeded search is that areas of the search corridor were left unexplored as forward body motion occurred at a rate too fast for the sweep of left-to-right head movements and associated eye movements. The fact that accuracy was associated with confidence is consistent with participants having some insight into the likelihood of {{the success or failure of}} their attempt at an exhaustive search (metacognitive awareness).|$|E
40|$|ABSTRACT: When using {{transceivers}} (or avalanche beacons) {{to search}} for fully buried avalanche victims, the search strategy depends on the signal <b>search</b> strip <b>width</b> which influences the search time until the first signal from the buried subjects can be received by the rescuer. It depends on technical characteristics of the avalanche rescue devices, the avalanche scenario {{as well as the}} rescuer’s behaviour. The larger the signal <b>search</b> strip <b>width,</b> the shorter is the search time and therefore the higher the survival chance of the buried subject. However, if the <b>search</b> strip <b>width</b> is chosen too large, the probability to miss a buried subject increases, which makes time-consuming multiple searches necessary – and decreases survival chances. Therefore, the <b>search</b> strip <b>width</b> needs to optimized. Only a few years ago, with the advent of digital transceivers, it was realized that the <b>search</b> strip <b>width</b> is not a universal constant but is a device specific property depending primarily on the range of the transceiver. Several approaches on how to determine the signal <b>search</b> strip <b>width</b> have been presented in the past. Most of them use rather conservative assumptions for the different input variables. A newly developed simulation approach for the optimization of the <b>search</b> strip <b>width</b> allows considering more realistic (rather than worst case) assumptions. Preliminary results suggest that the optimal signal <b>search</b> strip <b>width</b> is higher than previously assumed. In future applications, the simulation may be used to optimize a broad variety of search parameters or even entire search systems...|$|R
40|$|Abstract: The {{performance}} of {{four types of}} avalanche rescue beacons was tested during the winter 2001 in Switzerland. During the first field test the search times were compared. The second test focused on range measurements to determine <b>search</b> strip <b>width.</b> Results show that maximal range of different beacons varies between about 25 and 100 m. Differences in search times are as well significant, but relatively small compared with the total time for recovery of a buried victim. A recently proposed method to determine <b>search</b> strip <b>width</b> based on maximum range measurements in coaxial antenna orientation was verified and can be recommended...|$|R
30|$|The Vmax {{equation}} with <b>search</b> space <b>width</b> {{that has}} been presented by Shi and Eberhart (1999) is applied to converge more the search space. As a result of using this approach, particle’s velocity and position must be limited through a defined space width.|$|R
30|$|Our second {{prediction}} {{was that}} search {{would be more}} accurate as <b>search</b> <b>width</b> narrowed. This hypothesis {{was based on the}} idea that a narrow <b>search</b> <b>width</b> would better facilitate the search of close space using the fine-grained spatial acuity of foveal vision. To explore this prediction, participants were split into two groups – those who primarily moved left to right (n[*]=[*] 11) and those who primarily moved top to bottom (n[*]=[*] 16). There was one participant who did not fall into either category, conducting a hybrid strategy, and was therefore removed from subsequent analysis. We took the number of changes of direction made by each participant along their dominant axis (i.e. whether they were in the top-to-bottom or left-to-right group), added 1 (to take into account the number of sweeps both up and down, i.e. five turns would mean six sweeps), and divided the length of the axis being travelled across by this figure (i.e. for those in the top-to-bottom group, the length of the axis being travelled across in meters, which was 5, would be divided by the first figure calculated). This normalized the data, as calculating the <b>search</b> <b>width</b> took into account the length of each axis.|$|E
3000|$|The {{discussion}} {{above is}} valid for significant F 0 estimation errors—precision errors, {{in which the}} estimated frequency deviates by at most a few Hertz from the actual value, are easily compensated by the algorithm as it uses a <b>search</b> <b>width</b> of [...]...|$|E
30|$|Gilchrist et al. (2001) used a regular, visible search array (although {{targets were}} hidden in film {{canisters}} and required checking, these canisters were clearly {{laid out on}} the floor), but what happens when targets are hard to find and potential target locations are not arranged in a regular fashion? Critical to understanding how humans might conduct effective target search in such circumstances is what is meant by trying to search exhaustively across space. Consider the case where a given search area is wider than that which can be searched in a single pass. Within Search Theory (Koopman, 1946, 1980), the distance to the left and right of each location that still allows target detection defines an area known as the effective <b>search</b> <b>width</b> (ESW). The technical limit of sensors (human vision, radar, etc.) determines the width of the ESW and therefore how close together neighboring passes of a single path should be. With respect to humans, we refer to the width of the search corridor rather than the ESW as human search is not deterministic. Whether a particular setting of <b>search</b> <b>width</b> is considered effective for target search is dependent on knowing the accuracy of target detection within a search corridor.|$|E
40|$|ABSTRACT: Searching in {{parallel}} {{is a technique}} {{that can be used}} in avalanche rescue scenarios where manpower and leadership are abundant and the number of buried victims is unknown. It is es-pecially applicable at ski resorts and by search-and-rescue teams. The technique involves using as many searchers as possible with the objective of narrowing each rescuer’s <b>search</b> strip <b>width.</b> The rescue leader adjusts the <b>search</b> strip <b>width</b> according to the size of the debris area and the number of available searchers. Each searcher commits to the coarse and fine search only after their distance reading is lower than their designated <b>search</b> strip <b>width.</b> This eliminates redundancy and ensures that each searcher locates a different victim. If more than one victim is buried, then these search strips can be adjusted by the rescue leader, if necessary, after each victim is pinpointed. Searching {{in parallel}} reduces the necessity of using complex multiple-burial search methods and technologies. It can be used to harness large pools of relatively unskilled searchers with only basic training in the technique. Searching in parallel is more feasible than in the past due to the simplicity of modern digital avalanche transceivers. It should be considered for use by rescue teams and in profes-sional avalanche courses, but only after the basics have been mastered, such as single-victim search-ing, probing, and shovelling...|$|R
40|$|We {{show that}} <b>searching</b> a <b>width</b> /' maze is {{complete}} for II, i. e., for the /"th {{level of the}} AC hierarchy. Equivalently, st-connectivity for width /' grid graphs is complete for II. As an application, we {{show that there is}} a data structure solving dynamic st-connectivity for constant width grid graphs with time bound O (log log n) per operation on a random access machine. The dynamic algorithm is derived from the parallel one in an indirect way using algebraic tools...|$|R
40|$|A {{constraint}} satisfaction problem revolves finding values {{for a set}} of variables subject of a set of constraints (relations) on those variables Backtrack search is often used to solve such problems. A relationship involving the structure of the constraints i described which characterizes tosome degree the extreme case of mimmum backtracking (none) The relationship involves a concept called "width," which may provide some guidance in the representation f {{constraint satisfaction}} problems and the order m which they are <b>searched</b> The <b>width</b> concept is studied and applied, in particular, to constraints which form tree structures...|$|R
40|$|The Monte Carlo Rollout method (MCR) {{is a novel}} {{approach}} to solve combinatorial optimization problems with uncertainties approximatively. It combines ideas from Rollout algorithms for combinatorial optimization and the Monte Carlo Tree Search in game theory. In this paper {{the results of an}} investigation of applying the MCR to a Scheduling Problem are shown. The quality of the MCR method depends on the model parameters, search depth and <b>search</b> <b>width,</b> which are strong linked to process parameters. These dependencies are analyzed by different simulations. The paper also deals with the question whether the Lookahead Pathology occurs...|$|E
30|$|Experiment 1 a {{revealed}} individuals typically use an ‘S’-shaped {{search strategy}} {{with a common}} <b>search</b> <b>width</b> of between 1 and 2  m and with targets being found throughout the search time. Furthermore, when searching from top to bottom, increased accuracy is associated with slow search (i.e. longer time before changing direction) but with variable speeds (i.e. stopping to check targets). For all participants, targets were detected at an even rate throughout searching. The simplest account we offer of these data is that task performance was not affected by reducing vigilance over {{the time course of}} searching. This basic account would lead to a linear fit between the ordinal number of targets and the detection time of targets.|$|E
40|$|Backward {{beam search}} for {{dependency}} analysis of Japanese is proposed. As dependencies normally go {{from left to}} right in Japanese, it is effective to analyze sentences backwards (from right to left). The analysis is based on a statistical method and employs a beam search strategy. Based on experiments varying the beam <b>search</b> <b>width,</b> we found that the accuracy is not sensitive to the beam width and even the analysis with a beam width of 1 gets almost the same dependency accuracy as the best accuracy using a wider beam width. This suggested a deterministic algorithm for backwards Japanese dependency analysis, although still the beam search is effective as the N-best sentence accuracy is quite high. The time of analysis is observed to be quadratic in the sentence length...|$|E
50|$|Hohentwiel (Fug 200); Operated at {{wavelength}} between 52 and 57 cm. Range {{was between}} 10 km {{for a small}} vessel like a surfaced submarine to 70 km for a large ship. Under the best circumstances it could see the coast at approx 150 km. It had separate antennae for transmit and receive. The transmit antenna was centrally mounted, pointing forward, while the two receive antennae were mounted either side, pointing outwards by 30 degrees, giving it a <b>search</b> beam <b>width</b> of about 120 degrees. Each antenna array consisted of sixteen horizontally polarised dipoles, mounted in four groups of four in a vertical stack.|$|R
40|$|An {{important}} {{application of}} distance geometry to biochemistry studies the embeddings of the vertices of a weighted graph in the three-dimensional Euclidean space {{such that the}} edge weights are equal to the Euclidean distances between corresponding point pairs. When the graph represents the backbone of a protein, one can exploit the natural vertex order {{to show that the}} search space for feasible embeddings is discrete. The corresponding decision problem can be solved using a binary tree based search procedure which is exponential in the worst case. We discuss assumptions that bound the <b>search</b> tree <b>width</b> to a polynomial size...|$|R
40|$|ABSTRACT: We {{have used}} a Differential Global Positioning System (DGPS) for {{tracking}} the search trajectories of a rescuer at three different orientations of the antenna of a transmitted beacon. Hence, {{it is possible to}} determine the useful range of the <b>width</b> of <b>search</b> strips for different beacons. All commercially available (digital) beacons in Winter 2007 / 2008 were tested at a 50 x 50 m area in a field study. As examples three runs of two beacons with different antenna positions of the transmitter is presented. A buried transmitted beacon with a vertical orientation of the antenna represents the worst case scenario for determining the <b>search</b> strip <b>width...</b>|$|R
30|$|The {{movement}} data recorded {{from the}} individual within each dyad do allow us to explore one other issue (note that computation of <b>search</b> <b>width</b> and search speed makes no sense as the data come from only one member of each dyad). As in Experiments 1 a and 1 b, each participant can be classified as searching primarily top to bottom or left to right or using a hybrid strategy (see Fig.  9). Six participants used a top-to-bottom strategy (two independent and four split) and eight (three independent and five split) used a left-to-right strategy. Using a Fisher’s exact test and comparing these data against those from Experiment 1 (where across 1 a and 1 b, 23 used a top-to-bottom strategy and 16 used a left-to-right strategy), revealed a non-significant result (p[*]=[*] 0.358). Working in dyads did not significantly change the likelihood of direction of searching.|$|E
40|$|SUMMARY. The {{conventional}} line transect approach of estimating effective <b>search</b> <b>width</b> from the perpendicular distance distribution is inappropriate in {{certain types of}} surveys, e. g., when an unknown fraction of the animals on the track line is detected, the animals can be observed only at discrete points in time, there are errors in positional measurements, and covariate heterogeneity exists in detectability. For such situations a hazard probability framework for independent observer surveys is developed. The likelihood of the data, including observed positions of both initial and subsequent observations of animals, is established under the assumption of no measurement errors. To account for measurement errors and possibly other complexities, this likelihood is modified by a function estimated frgm extensive simulations. This general method of simulated likelihood is explained and the methodology applied to data from a double-platform survey of minke whales in the northeastern Atlantic in 1995...|$|E
40|$|One-aircraft tandem survey- {{analysis}} of pod sighting data from line transect aerial surveys using one aircraft to repeat selected track segments. Summary The ML point estimate for g(O) from porpoise sightings unstratified by sighting conditions was 0 · 68 and the effective <b>search</b> <b>width</b> 356 metres. The 95 °k confidence interval on the g(O) estimate was from 0 · 21 from 1 · 0. For sighting~ stratified by subjective sighting conditions the g(O) estimate was 0 · 86 under good and 0 · 24 under moderate conditions. The improvement in fit {{provided by the}} subjective assessment of sighting conditions was highly significant. The point estimates for g(O) and for the mean rate of pod displacement are about twice thos_e ~rom SCANS data. __C~_IJ_strainir 1 g pod displacement rate to that estimated from SCANS gives g(O) esti~ates identical to those from SCANS (0 · 25 for unstratified sightings). The observed differences are not statistically significant and are {{probably due to the}} limited number of trailing trac...|$|E
30|$|To clearly map the deformation, I {{applied a}} Gaussian filter {{in order to}} model the long-wavelength noise (Additional file 1 : Fig. S 1 b) and then {{subtracted}} the modeled noise from the original MAI result (Additional file 1 : Fig. S 1 c). I have <b>searched</b> the <b>width</b> of Gaussian filter from 10 to 50  km with an interval of 10  km so as to reproduce well the long-wavelength noise by trial and error, and resultantly a width of 30  km {{was used for the}} filtering. Figure  2 b shows the noise-reduced MAI image. Northward ground movement clearly appears on the western side of the Hinagu fault zone, with maximum movement of approximately 20  cm. On the other hand, no significant southward movement is shown on the eastern side.|$|R
40|$|A model-independent <b>search</b> for narrow <b>width</b> {{resonant}} {{pair production}} of the Higgs boson where both 125 GeV bosons decay into bottom quarks is presented. The search is performed in proton-proton collision data corresponding to an integrated luminosity of 17. 93 fb^- 1 at √(s) = 8 TeV recorded by the CMS detector at the LHC. No evidence for a signal is observed. Upper limits on the production cross section for such a resonance, in the mass range of 270 GeV to 1100 GeV, are reported...|$|R
40|$|A <b>search</b> for narrow <b>width</b> {{resonances}} between {{masses of}} 270 GeV and 1100 GeV decaying to pairs of Higgs bosons has been {{performed in the}} nal state with four bottom quarks using 17. 93 of proton-proton collisions collected at 8 TeV center of mass energy with the CMS experiment at the Large Hadron Collider. No statistically signicant signal excess is observed. Upper limits at 95 % CLS on the production cross section for such a resonance are set as function of the resonance mass considering both the spin- 0 and spin- 2 hypotheses...|$|R
30|$|The {{route maps}} showed {{participants}} tended to search using an ‘S’-shaped strategy, though sometimes {{embedded within a}} more complex pattern (see Fig.  3). This was confirmed {{in the analysis of}} data extracted using the Fourier analysis. Given that all participants exhibited regularity in their search path, we were unable to explore whether a regular search path predicted higher accuracy or not. However, the regularity of the ‘S’-shaped path made it possible for participants to define a search path that rarely contained crossovers and where the <b>search</b> <b>width</b> varied between 1 and 2  m. Following an ‘S’-shaped path minimized the memory demands inherent in the task relative to if a more irregular path was followed (Gilchrist et al., 2001). Presumably this width of search corridor adopted by participants was set according to their beliefs about the salience of targets {{in the context of the}} environment in which they were being sought. More or less salient targets would, respectively, lead to use of a wider or narrower search corridor.|$|E
40|$|To {{optimize}} a combinatorial problem {{one can use}} complex algorithms, e. g. branchand- bound algorithms. However, {{these are}} time consuming for extensive problems. By the need of real-time decisions in industrial applications, complex algorithms are inapplicable. Additionally, {{as a consequence of}} changes, solutions have to be calculated very often to adapt plans to the changes. Another aspect that makes a fast solution necessary. The Monte Carlo rollout method (MCR) is a novel approach for the approximate solution of combinatorial optimization problems. The MCR approach combines ideas from rollout algorithms for combinatorial optimization and the Monte Carlo tree search in game theory. In this paper the results of an investigation of applying the MCR to a repairing bin packing problem and a scheduling problem are shown. Influences of the model parameters, search depth and <b>search</b> <b>width,</b> are examined as well as the influence of process parameters. It also deals with the quest ion as to whether the Lookahead Pathology occurs as identified in game theory...|$|E
40|$|The article {{addresses}} {{an issue}} of expansion of maritime rescue systems with new components which are Wing-In-Ground-Effect Crafts (WIG craft). Due to operational characteristic, use of phenomenon of ground effect, WIG crafts are extremely economical and achieve relatively high speeds. In addition, they possess also amphibious features. This represents a huge potential for participation in search and rescue actions. The article discusses the phenomenon of a ground effect and its operation performance benefits and limitations. The applications of the WIG craft for specific actions such as conducting search, medical transport, supply of equipment and crews, patrolling are specified. The article analyzes the Search Effort of the WIG craft and marine search unit SAR- 1500, actually used on Polish coast. The study consists in determining {{the relationship between the}} altitude of the observer, his speed and <b>search</b> <b>width.</b> Next the calculation of available search effort is conducted. The results enable to compare the ability of different types of the patrol units. Słowa kluczowe: ratownictwo morskie, patrolowanie, efekt przypowierzchniow...|$|E
30|$|The {{number of}} columns (15 + n) that are read per row {{depends on the}} number of PPUs that the {{accelerator}} is scaled to. Once a vertical sweep of these columns is performed, a horizontal shift to the right is performed to start the next vertical sweep. The granularity of this horizontal-shift to the right is exactly equal to the number of PPUs being employed. Therefore, the number of vertical sweeps that are required to sweep a <b>search</b> window of <b>width</b> w, in a top-down left-to-right manner is given by ⌈(w−(15 +n))/n⌉.|$|R
40|$|The {{separation}} of the retinal vessel network into distinct arterial and venous vessel trees is of high interest. We propose an automated method for identification and {{separation of}} retinal vessel trees in a retinal color image by converting a vessel segmentation image into a vessel segment map and identifying the individual vessel trees by graph <b>search.</b> Orientation, <b>width,</b> and intensity of each vessel segment are utilized to find the optimal graph of vessel segments. The separated vessel trees are labeled as primary vessel or branches. We utilize the separated vessel trees for arterial-venous (AV) classification, based on the color properties of the vessels in each tree graph. We applied our approach to a dataset of 50 fundus images from 50 subjects. The proposed method resulted in an accuracy of 91. 44 % correctly classified vessel pixels as either artery or vein. The accuracy of correctly classified major vessel segments was 96. 42 %...|$|R
50|$|In fact, Chess 4.0 set the {{paradigm}} that was {{and still is}} followed essentially by all modern Chess programs today. Chess 4.0 type programs won out {{for the simple reason that}} their programs played better chess. Such programs did not try to mimic human thought processes, but relied on full width alpha-beta and negascout searches. Most such programs (including all modern programs today) also included a fairly limited selective part of the search based on quiescence searches, and usually extensions and pruning (particularly null move pruning from the 1990s onwards) which were triggered based on certain conditions in an attempt to weed out or reduce obviously bad moves (history moves) or to investigate interesting nodes (e.g. check extensions, passed pawns on seventh rank, etc.). Extension and pruning triggers have to be used very carefully however. Over extend and the program wastes too much time looking at uninteresting positions. If too much is pruned, there is a risk cutting out interesting nodes. Chess programs differ in terms of how and what types of pruning and extension rules are included as well as in the evaluation function. Some programs are believed to be more selective than others (for example Deep Blue was known to be less selective than most commercial programs because they could afford to do more complete full <b>width</b> <b>searches),</b> but all have a base full <b>width</b> <b>search</b> as a foundation and all have some selective components (Q-search, pruning/extensions).|$|R
40|$|The paper {{presents}} a tree search algorithm for the three-dimensional container loading problem (3 D-CLP). The 3 D-CLP {{is the problem}} of loading a subset of a given set of rectangular boxes into a rectangular container so that the packed volume is maximized. The method has two variants: the packing variant guarantees full support from below for all packed boxes, while this constraint is not taken into account by the cutting variant. The guillotine cut constraint is considered by both variants. The method is mainly based on two concepts. On the one hand the block building approach is generalized. Not only blocks of identical boxes in the same spatial orientation are applied but also blocks of different boxes with small inner gaps. On the other hand the tree search is carried out in a special fashion called a partitioncontrolled tree search (PCTRS). This makes the search both efficient and diverse, enabling a sufficient <b>search</b> <b>width</b> as well as a suitable degree of foresight. The approach achieves excellent results for the well-known 3 D-CLP instances suggested by Bischoff and Ratcliff with reasonable computing time. ...|$|E
30|$|For humans, {{without making}} head and eye {{movements}} {{to the left}} and right, the <b>search</b> <b>width</b> will be determined by the limits of foveal and parafoveal vision to discriminate targets. However, head and eye movements can be made to overcome the limits of foveal vision so long as they are calibrated with the forward speed of travel (along the search corridor being ‘swept’). If the speed of sweeping along a search path is too fast to allow search of adjoining spaces to the left, right, and center, then search will be incomplete. We conclude that the likelihood of search being conducted in a fine-grained and exhaustive manner will depend on a range of factors. In the absence of existing data, it seems likely that search will be subject to the strategic decisions made by searchers regarding systematicity of search and the trade-off of their chosen width of search corridor for sweeping, with their forward speed of search. Experiment 1 a was designed to reveal evidence of individuals using a search strategy (in terms of the forward speed of search, the width of the corridor being swept and consistency of a chosen strategy) and to assess the influence of this strategy on search accuracy and overall speed.|$|E
40|$|Subgroup Discovery (SD) aims to find coherent, easy-to-interpret subsets of the dataset at hand, where {{something}} exceptional is going on. Since the resulting subgroups are {{defined in terms}} of conditions on attributes of the dataset, this data mining task is ideally suited to be used by non-expert analysts. The typical SD approach uses a heuristic beam search, involving parameters that strongly influence the outcome. Unfortunately, these parameters are often hard to set properly for someone who is not a data mining expert; correct settings depend on properties of the dataset, and on the resulting search landscape. To remove this potential obstacle for casual SD users, we introduce ROCsearch [1], a new ROC-based beam search variant for Subgroup Discovery. On each search level of the beam search, ROCsearch analyzes the interme-diate results in ROC space to automatically determine a sensible <b>search</b> <b>width</b> for the next search level. Thus, beam search parameter setting is taken out of the do-main expert’s hands, lowering the threshold for using Subgroup Discovery. Also, ROCsearch automatically adapts its search behavior to the properties and re-sulting search landscape of the dataset at hand. Aside from these advantages, we also show that ROCsearch is an order of magnitude more efficient than traditional beam search, while its results are equivalent and on large datasets even better than traditional beam search results...|$|E
40|$|The {{importance}} of width {{as a resource}} in resolution theorem proving has been emphasized in work of Ben-Sasson and Wigderson. Their results show that lower bounds {{on the size of}} resolution refutations can be proved in a uniform manner by demonstrating lower bounds on the width of refutations, {{and that there is a}} simple dynamic programming procedure for automated theorem proving, based on the <b>search</b> for small <b>width</b> proofs. The present article shows that the problem of determining, given a set of clauses Σ and an integer k, whether Σ has a refutation of width k, is EXPTIME-complete. The proof is by a reduction from the (∃, k) -pebble game, proved EXPTIME-complete by Kolaitis and Panttaja...|$|R
40|$|ABSTRACT: The time {{required}} to search a buried person using an avalanche beacon contributes significantly to the overall rescue time. This paper investigates the technical requirements and psychological demands with respect to minimal search time. The earlier ones define a beacon's technical specification, the latter ones {{the design of the}} user interface. The first search phase is mainly characterized by the <b>search</b> strip <b>width</b> defined by receiver sensitivity. Even the {{time required}} to get the beacon ready to receive is subject to optimization. Once the first signal is detected, confusion caused by multiple transmitters costs valuable time. Hence a reliable and user-friendly multi-burial detection and signal isolation is crucial. Pin-pointing finally imposes the highest mental workload on the searcher. An intuitive user interface freeing the searcher from the need to compare numbers promises the largest gain here. For multi-burial accidents the influence of these beacon characteristics on search success and search time is discussed. This analysis includes numerical simulation of multi-burial search as well as a qualitative discussion of user interface requirements based on the accident reports and statements of the persons involved. The results may serve {{as a starting point for}} the development of a more generalized framework...|$|R
40|$|In {{this paper}} we relate {{different}} formulations of the DPLL(T) procedure. The first formulation {{is based on}} a system of rewrite rules, which we denote DPLL(T). The second formulation is an inference system of, which we denote LKDPLL(T). The third formulation is the application of a standard proof-search mechanism in a sequent calculus LKp(T) introduced here. We formalise an encoding from DPLL(T) to LKDPLL(T) that was, to our knowledge, never explicitly given and, in the case where DPLL(T) is extended with backjumping and Lemma learning, never even implicitly given. We also formalise an encoding from LKDPLL(T) to LKp(T), building on Ivan Gazeau's previous work: we extend his work in that we handle the "-modulo-Theory" aspect of SAT-modulo-theory, by extending the sequent calculus to allow calls to a theory solver (seen as a blackbox). We also extend his work in that we handle advanced features of DPLL such as backjumping and Lemma learning, etc. Finally, we re fine the approach by starting to formalise quantitative aspects of the simulations: the complexity is preserved (number of steps to build complete proofs). Other aspects remain to be formalised (non-determinism of the <b>search</b> / <b>width</b> of <b>search</b> space) ...|$|R
