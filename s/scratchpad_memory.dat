181|65|Public
25|$|Adapteva epiphany is {{targeted}} as a coprocessor, featuring a network on a chip <b>scratchpad</b> <b>memory</b> model, {{suitable for a}} dataflow programming model, which should be suitable for many machine learning tasks.|$|E
25|$|Channel {{architecture}} avoids {{this problem}} by using a separate, independent, low-cost processor. Channel processors are simple, but self-contained, with minimal logic and sufficient on-board <b>scratchpad</b> <b>memory</b> (working storage) to handle I/O tasks. They are typically not powerful or flexible enough {{to be used as}} a computer on their own and can be construed as a form of coprocessor.|$|E
25|$|The {{memory access}} pattern of AI {{calculations}} differs from graphics: a more predictable but deeper dataflow, benefiting {{more from the}} ability to keep more temporary variables on-chip (e.g. in <b>scratchpad</b> <b>memory</b> rather than caches); GPUs by contrast devote silicon to efficiently dealing with highly non-linear gather-scatter addressing between texture maps and frame-buffers, and texture filtering, as is needed for their primary role in 3D rendering.|$|E
40|$|Safety-critical {{embedded}} systems having to meet real-time constraints {{are expected to}} be highly predictable in order to guarantee at design time that certain timing deadlines will always be met. This requirement usually prevents designers from utilizing caches due to their highly dynamic, thus hardly predictable behavior. The integration of <b>scratchpad</b> <b>memories</b> represents an alternative approach which allows the system to benefit from a performance gain comparable to that of caches {{while at the same time}} maintaining predictability. In this work, we compare the impact of <b>scratchpad</b> <b>memories</b> and caches on worst case execution time (WCET) analysis results. We show that caches, despite requiring complex techniques, can have a negative impact on the predicted WCET, while the estimated WCET for <b>scratchpad</b> <b>memories</b> scales with the achieved performance gain at no extra analysis cost. ...|$|R
40|$|The {{increasing}} number of cores in manycore architectures causes important power and scalability problems in the memory subsystem. One solution is to introduce <b>scratchpad</b> <b>memories</b> alongside the cache hierarchy, forming a hybrid <b>memory</b> system. <b>Scratchpad</b> <b>memories</b> are more power-efficient than caches {{and they do not}} generate coherence traffic, but they suffer from poor programmability. A good way to hide the programmability difficulties to the programmer is to give the compiler the responsibility of generating code to manage the <b>scratchpad</b> <b>memories.</b> Unfortunately, compilers do not succeed in generating this code in the presence of random memory accesses with unknown aliasing hazards. This paper proposes a coherence protocol for the hybrid memory system that allows the compiler to always generate code to manage the <b>scratchpad</b> <b>memories.</b> In coordination with the compiler, memory accesses that may access stale copies of data are identified and diverted to the valid copy of the data. The proposal allows the architecture to be exposed to the programmer as a shared memory manycore, maintaining the programming simplicity of shared memory models and preserving backwards compatibility. In a 64 -core manycore, the coherence protocol adds overheads of 4...|$|R
40|$|Submitted {{on behalf}} of EDAA ([URL] audienceSafety-critical {{embedded}} systems having to meet real-time constraints {{are expected to be}} highly predictable in order to guarantee at design time that certain timing deadlines will always be met. This requirement usually prevents designers from utilizing caches due to their highly dynamic, thus hardly predictable behavior. The integration of <b>scratchpad</b> <b>memories</b> represents an alternative approach which allows the system to benefit from a performance gain comparable to that of caches {{while at the same time}} maintaining predictability. In this work, we compare the impact of <b>scratchpad</b> <b>memories</b> and caches on worst case execution time (WCET) analysis results. We show that caches, despite requiring complex techniques, can have a negative impact on the predicted WCET, while the estimated WCET for <b>scratchpad</b> <b>memories</b> scales with the achieved Performance gain at no extra analysis cost...|$|R
2500|$|Architectural {{experiments}} are continuing {{in a number}} of directions, e.g. the Cyclops64 system uses a [...] "supercomputer on a chip" [...] approach, in a direction away from the use of massive distributed processors. Each 64-bit Cyclops64 chip contains 80 processors, and the entire system uses a globally addressable memory architecture. The processors are connected with non-internally blocking crossbar switch and communicate with each other via global interleaved memory. There is no data cache in the architecture, but half of each SRAM bank {{can be used as a}} <b>scratchpad</b> <b>memory.</b> Although this type of architecture allows unstructured parallelism in a dynamically non-contiguous memory system, it also produces challenges in the efficient mapping of parallel algorithms to a many-core system.|$|E
2500|$|To {{achieve the}} high {{performance}} needed for mathematically intensive tasks, such as decoding/encoding MPEG streams, generating or transforming three-dimensional data, or undertaking Fourier analysis of data, the Cell processor marries the SPEs and the PPE via EIB to give access, via fully cache coherent DMA (direct memory access), to both main memory {{and to other}} external data storage. To {{make the best of}} EIB, and to overlap computation and data transfer, each of the nine processing elements (PPE and SPEs) is equipped with a DMA engine. [...] Since the SPE's load/store instructions can only access its own local <b>scratchpad</b> <b>memory,</b> each SPE entirely depends on DMAs to transfer data to and from the main memory and other SPEs' local memories. A DMA operation can transfer either a single block area of size up to 16KB, or a list of 2 to 2048 such blocks. One of the major design decisions in the architecture of Cell is the use of DMAs as a central means of intra-chip data transfer, with a view to enabling maximal asynchrony and concurrency in data processing inside a chip.|$|E
5000|$|Intel's Knights Landing {{processor}} has a 16 GB MCDRAM {{that can}} be configured as either a cache, <b>scratchpad</b> <b>memory,</b> or divided into some cache and some <b>scratchpad</b> <b>memory.</b>|$|E
5000|$|CELL, a {{multicore}} processor with features fairly consistent with vision processing units (SIMD instructions & datatypes suitable for video, and on-chip DMA between <b>scratchpad</b> <b>memories).</b>|$|R
40|$|In {{contrast}} to standard PCs and many high-performance computer systems, {{systems that have}} to meet real-time requirements usually do not feature caches, since caches primarily improve the average case performance, whereas their impact on WCET is generally hard to predict. Especially in embedded systems, <b>scratchpad</b> <b>memories</b> have become popular. Since these small, fast memories can be controlled by the programmer or the compiler, their behavior is perfectly predictable. In this paper, we study {{for the first time}} the impact of <b>scratchpad</b> <b>memories</b> on worst case execution time (WCET) prediction. Our results indicate that scratchpads can significantly improve WCET at no extra analysis cost. ...|$|R
40|$|In {{order to}} meet the {{requirements}} concerning both performance and energy consumption in embedded systems, new memory architectures are being introduced. Beside the well-known use of caches in the memory hierarchy, processor cores today also include small onchip <b>memories</b> called <b>scratchpad</b> <b>memories</b> whose usage is not controlled by hardware, but rather by the programmer or the compiler. Techniques for utilization of these scratchpads have been known for some time. Some new processors provide more than one scratchpad, making it necessary to enhance the workflow such that this complex memory architecture can be efficiently utilized. In this work, we present an energy model and an ILP formulation to optimally assign memory objects to different partitions of <b>scratchpad</b> <b>memories</b> at compile time, achieving energy savings of up to 22 % compared to previous approaches...|$|R
5000|$|... adapteva Epiphany Architecture, a manycore chip using PGAS <b>scratchpad</b> <b>memory</b> ...|$|E
5000|$|... {{providing}} a limited CPU stack or other on-chip <b>scratchpad</b> <b>memory</b> to reduce memory access ...|$|E
5000|$|RAM: 2 KB (128×64×2 bits) for the framebuffer {{plus the}} 64 bytes of <b>scratchpad</b> <b>memory</b> ...|$|E
5000|$|Compute {{elements}} {{may have}} differing types of interconnect aside from basic memory/bus interfaces. This may include dedicated network interfaces, Direct memory access (DMA) devices, mailboxes, FIFOs, and <b>scratchpad</b> <b>memories,</b> etc. Furthermore, certain portions of a heterogeneous {{system may be}} cache-coherent, whereas others may require explicit software-involvement for maintaining consistency and coherency.|$|R
50|$|The {{asynchronous}} {{array of}} simple processors (AsAP) architecture comprises a 2-D array of reduced complexity programmable processors with small <b>scratchpad</b> <b>memories</b> interconnected by a reconfigurable mesh network. AsAP {{was developed by}} researchers in the VLSI Computation Laboratory (VCL) at the University of California, Davis and achieves high performance and energy-efficiency, while using a relatively small circuit area.|$|R
40|$|Hard {{real-time}} tasks {{must meet}} their deadline in all situations, {{including in the}} worst-case one, otherwise {{the safety of the}} controlled system is jeopardized. In addition to this stringent demand for predictability, an increasing number of har real-time applications need to be fast as well. As a consequence, architectures with caches and/or on-chip static RAM (<b>scratchpad</b> <b>memories)</b> are of interest for such applications. As compared to unlocked caches which may raise predictability issues for some cache replacement policies [5], locked caches and software-controlled on-chip static RAM are more easily amenable to timing analysis. We propose in this paper an algorithm for off-line selection of the contents of on-chip memories. The algorithm supports two types of on-chip memories, namely locked caches and <b>scratchpad</b> <b>memories.</b> The contents of on-chip memory, although selected off-line, is changed at run-time, for the sake of scalability with respect to task size. The algorithm allows to make a quantitative comparison of worst-case performance of applications using these two kinds of on-chip memories. Experimental results show that the algorithm yields to good ratios of on-chip memory accesses on the worst-case execution path, with a tolerable reload overhead, for both types of on-chip memories. Furthermore, we highlight the circumstances under which one type of on-chip memory is more appropriate than the other depending of architectural parameters (cache block size) and application characteristics (basic block size) ...|$|R
50|$|The Adapteva Epiphany {{architecture}} is a manycore network on a chip processor with <b>scratchpad</b> <b>memory</b> addressable between cores.|$|E
50|$|The {{processing}} cores feature 64 KB of <b>scratchpad</b> <b>memory</b> {{for data}} (and 16 KB for instructions) and communicate via a network on a chip, {{instead of having}} a traditional cache hierarchy.|$|E
5000|$|Adapteva epiphany is {{targeted}} as a coprocessor, featuring a network on a chip <b>scratchpad</b> <b>memory</b> model, {{suitable for a}} dataflow programming model, which should be suitable for many machine learning tasks.|$|E
40|$|Abstract—To {{efficiently}} use multicore processors we need {{to ensure}} that almost all data communication stays on chip, i. e., the bits moved between tasks executing on different processor cores do not leave the chip. Different forms of on-chip com-munication are supported by different hardware mechanism, e. g., shared caches with cache coherency protocols, core-to-core networks-on-chip, and shared <b>scratchpad</b> <b>memories.</b> In this paper we explore the different hardware mechanism for on-chip communication and how they support or favor different models of communication. Furthermore, we discuss the usability of the different models of communication for real-time systems. Keywords-multicore communication, real-time systems, time-predictable systems I...|$|R
40|$|<b>Scratchpad</b> <b>memories</b> (SPMs) {{have become}} a {{promising}} on-chip storage solution for embedded systems from an energy, performance and predictability perspective. The thermal behavior {{of these types of}} memories has not been considered in detail. The thermal behavior in silicon devices {{plays an important role in}} the reliability of these systems and static (leakage) power consumption. In this short note, we outline a scheme to reduce the peak temperature and thermal cycling of SPMs in applications that have regular access patterns on their data structures. The key idea of our method is to physically distribute these accesses evenly over the whole memory area. KEYWORDS...|$|R
40|$|<b>Scratchpad</b> <b>memories</b> {{have now}} {{emerged as an}} {{alternative}} to caches for energy constrained embedded systems. However, effectively mapping data on them while considering energy/timing trade-offs remains a challenge. We present SAMOSA as a technique for mapping streaming applications to scratchpad based MPSoCs. The contribution of this approach is a representation and transformation of the mapping problems [...] buffer dimensioning and allocation [...] ? to a constraint-based optimization problem. SAMOSA was used to explore energy-execution time trade-offs for mapping the H. 264 decoder to a scratchpad-based MPSoC. Results show that scratchpad awareness has significant impacts on the energy-execution time trade-offs. status: publishe...|$|R
50|$|AMD TrueAudio {{is found}} on-die of select AMD {{graphics}} cards and APUs. A die can house multiple AMD TrueAudio DSP cores, each having 32KiB instruction and data caches and 8KiB of <b>scratchpad</b> <b>memory</b> for local operation.|$|E
50|$|The integer {{register}} file contained forty 64-bit registers, {{of which}} thirty-two are {{specified by the}} Alpha Architecture and eight are for use by PALcode as <b>scratchpad</b> <b>memory.</b> The register file has four read ports and two write ports evenly divided between the two integer pipelines.|$|E
50|$|Digital signal {{processors}} have similarly generalised {{over the}} years. Earlier designs used <b>scratchpad</b> <b>memory</b> fed by DMA, but modern DSPs such as Qualcomm Hexagon often include {{a very similar}} set of caches to a CPU (e.g. Modified Harvard architecture with shared L2, split L1 I-cache and D-cache).|$|E
40|$|An {{increasing}} number of processor architectures support scratch-pad memory - software managed on-chip memory. Scratch-pad memory provides low latency data storage, like on-chip caches, but under explicit software control. The simple design and predictable nature of <b>scratchpad</b> <b>memories</b> has seen them incorporated {{into a number of}} embedded and real-time system processors. They are also employed by multi-core architectures to isolate processor core local data and act as low latency inter-core shared memory. Managing scratch-pad memory by hand is time consuming, error prone and potentially wasteful; tools that automatically manage this memory are essential for its use by general purpose software. While there has been promising work in compile time allocation of scratch-pad memory, there will always be applications which require run-time allocation. Modern dynamic memory management techniques are too heavy-weight for scratch-pad management. This paper presents the Scratch-Pad Memory Allocator, a light-weight memory management algorithm, specifically designed to manage small on-chip memories. This algorithm uses a variety of techniques to reduce its memory footprint while still remaining effective, including: representing memory both as fixed-sized blocks and variable-sized regions within these blocks; coding of memory state in bitmap structures; and exploiting the layout of adjacent regions to dispense with boundary tags for split and coalesce operations. We compare the performance of this allocator against Doug Lea's malloc implementation for the management of core-local and inter-core shared <b>scratchpad</b> <b>memories</b> under real world memory traces. This algorithm manages small memories efficiently and scales well under load when multiple competing cores access shared memory. </p...|$|R
3000|$|Similarly, Edwards and Lee {{discussed}} as [...] "It {{is time for}} a new era of processors whose temporal behavior is as easily controlled as their logical function" [...] [7]. A first simulation of their PRET architecture is presented in [8]. PRET implements the SPARC V 8 instruction set architecture (ISA) in a six-stage pipeline and performs chip level multithreading for six threads to eliminate data forwarding and branch prediction. <b>Scratchpad</b> <b>memories</b> are used instead of instruction and data caches. The shared main memory is accessed via a TDMA scheme, called memory wheel, similar to the TDMA-based arbiter used in the JOP CMP system [9]. The SPARC ISA is extended with a deadline instruction that stalls the current thread until the deadline is reached. This instruction is used to perform time-based, instead of lock-based, synchronization for access to shared data.|$|R
40|$|Partitioning {{a memory}} into {{multiple}} blocks {{that can be}} independently accessed is a widely used technique to reduce its dynamic power. For embedded systems, its benefits can be even pushed further by properly matching the partition to the memory access patterns. When leakage energy comes into play, however, idle memory blocks must be put into a proper low-leakage sleep state to actually save energy when not accessed. In this case, the matching becomes an instance of the power management problem, because moving to and from this sleep state requires additional energy. An effective {{solution to the problem}} of the leakage-aware partitioning of a memory into disjoint sub blocks, in particular, target <b>scratchpad</b> <b>memories,</b> which are commonly used in some embedded systems as a replacement for caches. By this approach, it is able to provide an optimal solution to the leakage-aware partitioning problem...|$|R
50|$|They {{may include}} direct {{interfaces}} to take data from cameras (bypassing any off chip buffers), {{and have a}} greater emphasis on on-chip dataflow between many parallel execution units with <b>scratchpad</b> <b>memory,</b> like a manycore DSP. But, like video processing units, they may have a focus on low precision fixed point arithmetic for image processing.|$|E
50|$|Strided {{or simple}} 2D,3D access {{patterns}} (e.g. stepping through multi-dimensional arrays) are similarly easy to predict, and {{are found in}} implementations of linear algebra algorithms and image processing. Loop tiling is an effective approach. Some systems with DMA provided a strided mode for transferring data between subtile of larger 2D arrays and <b>scratchpad</b> <b>memory.</b>|$|E
50|$|Channel {{architecture}} avoids {{this problem}} by using a separate, independent, low-cost processor. Channel processors are simple, but self-contained, with minimal logic and sufficient on-board <b>scratchpad</b> <b>memory</b> (working storage) to handle I/O tasks. They are typically not powerful or flexible enough {{to be used as}} a computer on their own and can be construed as a form of coprocessor.|$|E
40|$|Caches {{are notorious}} for their unpredictability. It is {{difficult}} or even impossible to predict if a memory access {{will result in a}} definite cache hit or miss. This unpredictability is highly undesired especially when designing real-time systems where the worst-case execution time (WCET) {{is one of the key}} metrics. <b>Scratchpad</b> <b>memories</b> (SPMs) have proven to be a fully predictable alternative to caches. In contrast to caches, however, SPMs require dedicated compiler support. This paper presents an optimal static SPM allocation algorithm for program code. It minimizes WCETs by placing the most beneficial parts of a program’s code in an SPM. Our results underline the effectiveness of the proposed techniques. For a total of 73 realistic benchmarks, we reduced WCETs on average by 7. 4 % up to 40 %. Additionally, the run times of our ILP-based SPM allocator are negligible...|$|R
40|$|Hybrid on-chip {{memories}} that combine Non-Volatile Memories (NVMs) with SRAMs promise {{to mitigate the}} increasing leakage power of traditional on-chip SRAMs. We present HaVOC: a run-time memory manager that virtualizes the hybrid on-chip memory space and supports efficient sharing of distributed <b>ScratchPad</b> <b>Memories</b> (SPMs) and NVMs. HaVOC allows programmers and the compiler to partition the application’s address space and generate data/code layouts considering virtualized hybrid-address spaces. We define a data volatility metric used by our hybrid-memory-aware compilation flow to generate memory allocation policies that are enforced at run-time by a filter-inspired dynamic memory algorithm. Our experimental results {{with a set of}} embedded benchmarks executing simultaneously on a Chip-Multiprocessor with hybrid NVM/SPMs show that HaVOC is able to reduce execution time and energy by 60. 8 % and 74. 7 % respectively with respect to traditional multi-tasking-based SPM allocation policies...|$|R
40|$|Focusing on {{embedded}} applications, <b>scratchpad</b> <b>memories</b> (SPMs) {{look like}} a best-compromise solution when taking into account performance, energy consumption and die area. The main challenge in SPM design is mapping <b>memory</b> locations to <b>scratchpad</b> locations. This paper describes an algorithm to optimally solve such a mapping problem by means of Dynamic Programming applied to a synthesizable hardware architecture. The algorithm works by mapping segments of external memory to physically partitioned banks of an on-chip SPM; this architecture provides significant energy savings. The algorithm does not require any user-set bound {{on the number of}} partitions and takes into account partitioning overhead. Improving on previous solutions, execution time is polynomial in the input size. Strategies to optimize memory requirements and speed of the algorithm are exploited. Additionally, we integrate this algorithm in a complete and automated design, simulation and synthesis flow...|$|R
