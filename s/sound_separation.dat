87|237|Public
5000|$|Then, {{segregation}} (sight and <b>sound</b> <b>separation)</b> between juvenile {{and adult}} offenders ...|$|E
50|$|Monaural <b>sound</b> <b>separation</b> {{first began}} with {{separating}} voices based on frequency. There were many early developments based on segmenting different speech signals through frequency. Other models followed on this process, {{by the addition}} of adaption through state space models, batch processing, and prediction-driven architecture. The use of CASA has improved the robustness of ASR and speech separation systems.|$|E
50|$|The second, SQ, was {{developed}} and marketed by Columbia Records and Sony and entered the US market in April 1971. The SQ format was also used by companies such as EMI in Great Britain, who pressed several SQ album releases. The <b>sound</b> <b>separation</b> of the SQ system was greatly improved {{by the introduction of}} SQ Full Logic decoding in 1975 using the Motorola chips MC1312, MC1314 & MC1315.|$|E
40|$|It is {{necessary}} {{that the number}} of the observation signals equals to the number of source signals, if independent component analysis is used to perform the <b>sound</b> source <b>separation.</b> It is difficult to perform <b>sound</b> source <b>separation</b> from a stereo music sound signal when the number of sound sources are more than two. We propose the technique to perform <b>sound</b> source <b>separation</b> from a stereo music sound signal {{that the number of}} sound sources is more than two using the frequency analysis and independent component analysis. 1...|$|R
40|$|Monophonic <b>sound</b> source <b>separation</b> is an {{essential}} subject on the fields where sound, such as voice, music and noise, is dealt with. In particular, unsupervised approaches to this problem have high versatility in comparison with supervised approaches. Non-negative matrix factorization is {{the most frequently used}} algorithm for the monophonic <b>sound</b> source <b>separation</b> without prior knowledge. This is also applied to various applications, including data clustering, face recognition, gene expression classification. However, non-negative matrix factorization cannot be efficiently used in online learning. In order to solve this difficulty, the non-negative sparse autoencoder was proposed in the literature. Although several successful applications have been reported, this is not yet applied to the monophonic <b>sound</b> source <b>separation.</b> This paper shows that the non-negative sparse autoencoder can perform the monophonic <b>sound</b> source <b>separation</b> without prior knowledge in online learning...|$|R
5000|$|... #Subtitle level 3: Lost in the <b>Sound</b> of <b>Separation</b> (2008-2009) ...|$|R
50|$|Super 8mm {{was also}} {{specified}} with an optical sound track. This occupied {{the same location}} as the magnetic track. Picture to <b>sound</b> <b>separation</b> in this format was 22 frames. Projectors and cameras obviously could not record sound in this system, but optical sound package movies became briefly popular, particularly in Europe (mainly because they were cheaper to produce - though the projectors cost more). Although the optical sound should have been inferior in quality to magnetic sound (running at 3.6 inches per second for 24 frames per second), in practice it was often much better, largely because packaged movie magnetic sound was often poorly recorded.|$|E
5000|$|WNHU (88.7 FM) is a {{radio station}} {{broadcasting}} a variety format. Licensed to West Haven, Connecticut, United States, the station serves the New Haven area. The station {{is owned by the}} University of New Haven. Broadcasting at 88.7 MHz in stereo, the station first signed on at 1600 EDT on June 4, 1973 in mono. The station went stereo during [...] "Tommy's [...] " [...] 11am - 2 pm Tuesday show, who was the Music director from 1975-78. He selected the prologue from the soundtrack to the movie [...] "Tommy [...] "which provided listeners with an intense train <b>sound</b> <b>separation</b> effect just before Tommy's birth occurred symbolizing a new era for the station, now in stereo. Throughout its history, the station has been staffed by volunteer students, alumni, and members of the general community. The General Manager and Chief Engineer are paid positions, while Station Manager and Program Director are student positions with stipends on tuition. Other management members include Website & Technology Director, Production Director, and Operations Manager.|$|E
5000|$|Zappa {{originally}} {{proposed to}} record the album as an [...] "ethnic field recording" [...] {{in the house where}} the band lived. Working with Zappa and engineer Dick Kunc the band recorded some provisional backing tracks at the Woodland Hills house with <b>sound</b> <b>separation</b> obtained simply by having different instruments in different rooms. Zappa thought these provisional recordings turned out well, but Van Vliet became suspicious that Zappa was trying {{to record the}} album on the cheap and insisted on using a professional studio. Zappa would say of Van Vliets approach that it was [...] "impossible to tell him why things should be such and such a way. It seemed to me that {{if he was going to}} create a unique object, that the best thing for me to do was to keep my mouth shut as much as possible and just let him do whatever he wanted to do whether I thought it was wrong or not." [...] One of the tracks recorded by Zappa and Kunc at the house, [...] "Hair Pie: Bake 1", appeared on the finished album. Three other tracks appearing on the album were recorded on a cassette recorder at the house, the a cappella poems [...] "The Dust Blows Forward n The Dust Blows Back" [...] and [...] "Orange Claw Hammer," [...] and the improvised blues [...] "China Pig" [...] with former Magic Band member Doug Moon accompanying Van Vliet on guitar. [...] "The Blimp" [...] was recorded by Zappa in his studio while on the phone with Van Vliet prior to the albums sessions; Jeff Cotton was put on the phone to recite Van Vliets latest poem, which Zappa recorded and put over a Mothers of Invention backing track (which had been known to the Mothers, unacknowledged on Trout Masks credits, as [...] "Charles Ives").|$|E
5000|$|Underoath - Define the Great Line, Lost in the <b>Sound</b> of <b>Separation</b> ...|$|R
30|$|<b>Sound</b> source <b>separation</b> is {{the signal}} {{processing}} task {{that deals with}} the extraction of unknown signals or sources from an audio mixture. In the case of musical signals, a possible <b>sound</b> source <b>separation</b> task would be to obtain independent signals for the saxophone, piano, bass, and percussion, given a recording or audio mixture of a jazz quartet.|$|R
40|$|We {{present a}} system for upmixing mono {{recordings}} to stereo {{through the use of}} <b>sound</b> source <b>separation</b> techniques. The use of <b>sound</b> source <b>separation</b> has the advantage of allowing sources to be placed at distinct points in the stereo field, resulting in more natural sounding upmixes. The system separates an input signal into a number of sources, which can then be imported into a digital audio workstation for upmixing to stereo. Considerations {{to be taken into account}} when upmixing are discussed, and a brief overview of the various <b>sound</b> source <b>separation</b> techniques used in the system are given. The effectiveness of the proposed system is then demonstrated on real-world mono recordings...|$|R
30|$|The {{developed}} percussive/harmonic <b>sound</b> <b>separation</b> is {{detailed in}} Algorithm 1.|$|E
40|$|Abstract—Monaural musical <b>sound</b> <b>separation</b> {{has been}} {{extensively}} studied recently. An important problem in separation of pitched musical sounds is the estimation of time–frequency regions where harmonics overlap. In this paper, we propose a sinusoidal modeling-based separation system that can effectively resolve overlapping harmonics. Our strategy {{is based on the}} observations that harmonics of the same source have correlated amplitude envelopes and that the change in phase of a harmonic is related to the instrument’s pitch. We use these two observations in a least squares estimation framework for separation of overlapping harmonics. The system directly distributes mixture energy for harmonics that are unobstructed by other sources. Quantitative evaluation of the proposed system is shown when ground truth pitch information is available, when rough pitch estimates are provided {{in the form of a}} MIDI score, and finally, when a multipitch tracking algorithm is used. We also introduce a technique to improve the accuracy of rough pitch estimates. Results show that the proposed system significantly outperforms related monaural musical <b>sound</b> <b>separation</b> systems. Index Terms—Common amplitude modulation (CAM), musical <b>sound</b> <b>separation,</b> sinusoidal modeling, time–frequency masking, underdetermined <b>sound</b> <b>separation.</b> I...|$|E
40|$|Monaural musical <b>sound</b> <b>separation</b> {{attempts}} to segregate different instrument lines from single-channel polyphonic music. We propose {{a system that}} decomposes an input into timefrequency units using an auditory filterbank and utilizes pitch to label which instrument line each time-frequency unit is assigned to. The system is conceptually simple and computationally efficient. Systematic evaluation shows that, despite its simplicity, the proposed system achieves a competitive level of performance. Index Terms — musical <b>sound</b> <b>separation,</b> computational auditory scene analysis, pitch-based labeling 1...|$|E
40|$|Measuring the {{contribution}} of a particular sound source to the ambient sound level at an arbitrary location is impossible without some form of <b>sound</b> source <b>separation.</b> This made it difficult, if not impossible, to design automated systems that measure {{the contribution}} of a target sound to the ambient sound level. This paper introduces <b>sound</b> source <b>separation</b> technology {{that can be used}} to measure {{the contribution of}} a sound source, a passing plane, in environments where planes are not the dominant sound source. This <b>sound</b> source <b>separation</b> and classification technology, developed by Sound Intelligence makes it, in principle, possible to monitor the temporal development of any soundscape...|$|R
5000|$|Letters reserve {{their own}} phonetic <b>sound</b> in <b>separation</b> and in {{combination}} in a word.|$|R
5000|$|Lí - Pears (梨, lí) {{are also}} {{uncommon}} gifts as they <b>sound</b> like <b>separation</b> (离, lí).|$|R
30|$|As {{shown in}} Fig.  2, {{developed}} using HARK, audition services perform sound localization, <b>sound</b> <b>separation,</b> and voice/non-voice recognition from the four-channel audio stream {{coming from the}} data collection module.|$|E
30|$|An {{emerging}} {{approach for}} general <b>sound</b> <b>separation</b> exploits the knowledge {{from the human}} auditory system. In an influential book, Bregman proposed that the auditory system employs a process called auditory scene analysis (ASA) to organize an acoustic mixture into different perceptual streams which correspond to different sound sources [2]. The perceptual process is believed to involve two main stages: The segmentation stage and the grouping stage [2]. In the segmentation stage, the acoustic input is decomposed into time-frequency (TF) segments, each of which mainly originates from a single source [3, Chapter 1]. In the grouping stage, segments from the same source are grouped according {{to a set of}} grouping principles. Grouping has two types: primitive grouping and schema-based grouping. The principles employed in primitive grouping include proximity in frequency and time, harmonicity/pitch, synchronous onset and offset, common amplitude/frequency modulation, and common spatial information. Human ASA has inspired researchers to investigate computational auditory scene analysis (CASA) for <b>sound</b> <b>separation</b> [3]. CASA exploits the intrinsic properties of sounds for separation and makes relatively minimal assumptions about specific sound sources. Therefore it shows considerable potential as a general approach to <b>sound</b> <b>separation.</b> Recent CASA-based speech separation systems have shown promising results in separating target speech from interference [3, Chapters 3 and 4]. However, building a successful CASA system for musical <b>sound</b> <b>separation</b> is challenging, and a main reason is the problem of overlapping harmonics.|$|E
30|$|After {{many years}} of <b>sound</b> <b>{{separation}}</b> research, results suggest that separation performance can be improved when prior information about the sources is available. The inclusion of known information about the sources in the separation scheme {{is referred to as}} informed sound source separation (ISS) and comprises, among others, the use of musical instrument digital interface (MIDI)-like musical scores, the use of pitch tracks of one or several sources, oracle <b>sound</b> <b>separation</b> where the original sources are available, and the extraction of model parameters from training data of a particular sound source. The reader is referred to [2] for a general overview of informed sound source separation approaches.|$|E
5000|$|Underoath, Lost in the <b>Sound</b> of <b>Separation</b> (Tooth & Nail Records and Solid State Records) (P/E) ...|$|R
40|$|In {{this paper}} a novel {{approach}} to single microphone Acoustic Echo cancellation (AEC) is presented. This approach performs AEC by employing techniques developed for monaural <b>sound</b> source <b>separation.</b> It is shown that the AEC problem can be cast in a monaural <b>sound</b> source <b>separation</b> framework and through this framework significant echo suppression can be achieved. The new approach is evaluated through experiments on simulated data. 1...|$|R
40|$|We first present {{our studies}} on how clock {{synchronization}} errors affect {{the performance of}} <b>sound</b> source <b>separation</b> with a distributed microphone array. We show that our previouslyproposed energy-based <b>sound</b> source <b>separation</b> method is robust to constant clock shift errors but is more sensitive to clock drift errors. We then propose a novel technique to address the clock drift errors. The key observation is that {{as the amount of}} clock drift increases, so does the correlation between the energies of the separated sources which are obtained from the Independent Component Analysis (ICA). Based on this observation, we propose an optimization technique to solve the clock drifting parameter. Experiment results are shown to validate our approach. Index Terms — <b>Sound</b> source <b>separation,</b> synchronization, clock drif...|$|R
30|$|Monaural musical <b>sound</b> <b>separation</b> has {{received}} significant attention recently. Analyzing a musical signal is difficult in general {{due to the}} polyphonic nature of music, but extracting useful information from monophonic music is considerably easier. Therefore a musical <b>sound</b> <b>separation</b> system {{would be a very}} useful processing step for many audio applications, such as automatic music transcription, automatic instrument identification, music information retrieval, and object-based coding. A particularly interesting application of such a system is signal manipulation. After a polyphonic signal is decomposed to individual sources, modifications, such as pitch shifting and time stretching, can then be applied to each source independently. This provides infinite ways to alter the original signal and create new sound effects [1].|$|E
40|$|All <b>sound</b> <b>separation</b> systems {{based on}} {{perception}} assume a bottom-up or Marr-like {{view of the}} world. Sound is processed by a cochlear model, passed to an analysis system, grouped into objects, and then passed to higher level processing systems. The information flow is strictly bottom up, with no information flowing down from higher level expectations. Is this the right approach? This paper summarizes the existing bottom-up perceptual models, and the evidence for more top-down processing. This paper describes many of the auditory and visual effects that indicate top-down information flow. Hopefully this paper will generate discussion {{about the role of}} top-down processing, whether this information should be included in <b>sound</b> <b>separation</b> models, and how to build testable architectures. ...|$|E
30|$|We use three recent {{state-of-the-art}} percussive/harmonic <b>sound</b> <b>separation</b> {{methods to}} evaluate the developed method: HPSS [1], MFS [7] and NMPCF [8]. HPSS and MFS are implemented in this study, whereas the separation results from NMPCF have been provided directly by the authors.|$|E
40|$|Abstract. A <b>sound</b> source <b>separation</b> {{technique}} {{based on}} a bio-inspired neural network, capable of functioning in more than two-source mix-tures, is proposed. Separation results are compared with other proposed techniques in the literature using quantitative evaluation criteria. 1 The <b>sound</b> source <b>separation</b> problem In our life we are confronted to situation in which a mixture of sound sources {{is present in the}} environment and our goal is to extract one of the sources among others. While the auditory system may not always succeed in this goal, the range of situations in which recognition is possible in the presence of competing sources highlights the flexibility and robustness of human in speech perception. Here we propose a technique that roughly simulates the behavior of the auditory pathway. Our separation technique uses the Computational Auditory Scene Analysis [1]. 2 The proposed model An enhanced FIR Gammatone filterbank is used to mimic the behavior of the cochlea [2]. From the output of the cochlear channels two different anthropo-morphic maps are generated. The Cochleotopic / AMtopic and Cochleotopic/ Spectrotopic Maps, which try to mimic partially the behavior of the peripheral auditory pathway are generated. These maps are based on the reassigned FFT (Fast Fourier Transform) and envelope detection [3]. A two-layered network of spiking neurons is used to perform cochlear channel selection (Fig. 2) based on temporal correlation: neurons associated to those channels belonging to the same sound source synchronize. 2. 1 Three-source <b>sound</b> source <b>separation</b> In our previous works, we applied our proposed model to two-source <b>sound</b> source <b>separation</b> [3] [2]. Most of other proposed models in the literature for ASA-based <b>sound</b> source <b>separation</b> deal only with two-source <b>sound</b> source <b>separation</b> (see [4] [5] [6] [7] [8]). One of the exceptions to this general tendency is the work b...|$|R
30|$|As {{mentioned}} in Section 1, musical audio signals {{should be separated}} into instrument parts beforehand to boost and reduce the volume of those parts. Although a number of <b>sound</b> source <b>separation</b> methods [11 – 14] have been studied, most of them still focus on dealing with music performed on either pitched instruments that have harmonic sounds or drums that have inharmonic sounds. For example, most separation methods for harmonic sounds [11 – 14] cannot separate inharmonic <b>sounds,</b> while most <b>separation</b> methods for inharmonic sounds, such as drums [15], cannot separate harmonic ones. <b>Sound</b> source <b>separation</b> methods based on the stochastic properties of audio signals, for example, independent component analysis and sparse coding [16 – 18], treat particular kind of audio signals which are recorded with a microphone array or have small number of simultaneously voiced musical notes. However, these methods cannot separate complex audio signals such as commercial CD recordings. We describe our <b>sound</b> source <b>separation</b> method which can separate complex audio signals with both harmonic and inharmonic sounds in this section.|$|R
50|$|Applications {{of sound}} source {{localization}} include <b>sound</b> source <b>separation,</b> <b>sound</b> source tracking, and speech enhancement. Sonar uses sound source localization techniques {{to identify the}} location of a target. 3D sound localization is also used for effective human-robot interaction. With the increasing demand for robotic hearing, some applications of 3D sound localization such as human-machine interface, handicapped aid, and military applications, are being explored.|$|R
30|$|As mentioned, {{overlapping}} harmonics are not {{as common}} in speech mixtures as in polyphonic music. This problem has not received much attention in the CASA community. Even those CASA systems specifically developed for musical <b>sound</b> <b>separation</b> [5, 6] do not address the problem explicitly.|$|E
30|$|We {{present a}} system for the {{automatic}} separation of solo instruments and music accompaniment in polyphonic music recordings. Our approach {{is based on a}} pitch detection front-end and a tone-based spectral estimation. We assess the plausibility of using <b>sound</b> <b>separation</b> technologies to create practice material in a music education context. To better understand the <b>sound</b> <b>separation</b> quality requirements in music education, a listening test was conducted to determine the most perceptually relevant signal distortions that need to be improved. Results from the listening test show that solo and accompaniment tracks pose different quality requirements and should be optimized differently. We propose and evaluate algorithm modifications to better understand their effects on objective perceptual quality measures. Finally, we outline possible ways of optimizing our separation approach to better suit the requirements of music education applications.|$|E
30|$|Pitch {{estimation}} in {{noisy environment}} {{is closely related}} to <b>sound</b> <b>separation.</b> If, on one hand, the mixed sound is separated, the pitch of each sound can be obtained relatively easily. On the other hand, pitch is a very efficient grouping cue for <b>sound</b> <b>separation</b> and widely used in previous systems [8, 9, 15]. In the Hu and Wang model, a continuous pitch estimation method is proposed based on correlogram in which the T-F units are merged into segments according to cross-channel correlation and time continuity. Each segment is expected to be dominated by a single voiced sound. At first, they employed the longest segment as a criterion to initially separate the segments into foreground and background. And then, the pitch contour is formed using units in foreground and followed by sequential linear interpolation, more details can be found in [9].|$|E
25|$|Even {{before its}} release, the album had been welcomed with {{critical}} acclaim. In its first week, Lost in the <b>Sound</b> of <b>Separation</b> debuted at No. 8 on the Billboard 200 charts, selling around 56,000 copies in the US alone.|$|R
5000|$|E: Early French VHF system (B&W only); {{very good}} (near HDTV) picture quality but {{uneconomical}} use of bandwidth. <b>Sound</b> carrier <b>separation</b> +11.15 MHz on odd numbered channels, -11.15 MHz on even numbered channels. Discontinued in 1984 (France) and 1985 (Monaco). https://web.archive.org/web/20120830232230/http://www.pembers.freeserve.co.uk/World-TV-Standards/Transmission-Systems.html ...|$|R
30|$|Auditory {{scenes are}} {{naturally}} complex, having usually many overlapping sound events active {{at the same}} time. Hence, the detection of overlapping sound events is an important aspect for more robust and realistic sound event detection system. Recent developments in the <b>sound</b> source <b>separation</b> provide interesting possibilities to tackle this problem. In the early studies, <b>sound</b> source <b>separation</b> has already proven to substantially increase {{the accuracy of the}} event detection [40]. Further, the event priors for the overlapping sound events are difficult to model because of high number of possible combinations and transitions between them. Latent semantic analysis has emerged as a interesting solution to learn associations between overlapping events [29], but the area requires more studying to apply it efficiently to the overlapping event detection.|$|R
