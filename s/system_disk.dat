68|2067|Public
25|$|The {{original}} {{core of the}} Nice {{model is}} a triplet of papers published in the general science journal Nature in 2005 by an international collaboration of scientists: Rodney Gomes, Hal Levison, Alessandro Morbidelli and Kleomenis Tsiganis. In these publications, the four authors proposed that after the dissipation of the gas and dust of the primordial Solar <b>System</b> <b>disk,</b> the four giant planets (Jupiter, Saturn, Uranus and Neptune) were originally found on near-circular orbits between ~5.5 and ~17 astronomical units (AU), much more closely spaced and compact than in the present. A large, dense disk of small, rock and ice planetesimals, their total about 35 Earth masses, extended from the orbit of the outermost giant planet to some 35 AU.|$|E
5000|$|Hierarchical File System: Appears as a {{mountable}} HFS file <b>system</b> <b>disk</b> image (sans partition table).|$|E
5000|$|KAY-1024/3SL/TURBO, {{motherboard}}, Nemo-bus. Supplied as: motherboard {{with three}} slots for additional devices, {{floppy disk controller}} [...] "BETA-TURBO", description (single sheet of paper), <b>system</b> <b>disk.</b>|$|E
5000|$|In Windows 2000 {{and later}} {{operating}} <b>systems,</b> <b>Disk</b> Defragmenter has the following limitations: ...|$|R
5000|$|Unlike {{previous}} Mac models {{but like}} the Performas, the 145B did not ship with {{a full set of}} <b>system</b> <b>disks.</b> <b>System</b> 7.1 was preinstalled on the internal hard disk, and a single <b>system</b> start-up <b>disk</b> was included. The package also included two utilities that provide basic backup and restore functions. Unlike the 140 and 145, the 145B cannot run System 7.0.1 - it needs at least System 7.1.|$|R
50|$|ODS-2 is the {{standard}} VMS file system, and remains the most common file <b>system</b> for <b>system</b> <b>disks</b> (the disk on which the operating system is installed).|$|R
5000|$|A {{registry}} cleaner cannot {{repair a}} registry hive that cannot be mounted by the system, making the repair via [...] "slave mounting" [...] of a <b>system</b> <b>disk</b> impossible.|$|E
50|$|As of July 2017, {{there is}} an {{abandonware}} site (winworldpc.com) that has available for download a disk image for {{the latest version of}} MS-DOS for the Tandy 2000. It includes instructions for using the IBM 1.2MB 5.25-inch disk drive (80-track) to create a <b>system</b> <b>disk</b> bootable in the Tandy 2000 5.25-inch drive. This procedure {{can also be used to}} create a bootable 3.5-inch <b>system</b> <b>disk</b> using an ordinary 720KB 3.5-inch PC drive; this will boot a Tandy 2000 with its 5.25-inch boot drive replaced with a like 3.5-inch PC drive.|$|E
50|$|A {{virtual server}} has a <b>system</b> <b>disk</b> of a pre-defined size. Additional disks, with sizes {{configurable}} from 10 GB to 10 TB, can {{be attached to}} a server {{when it is not}} running.|$|E
50|$|Disk {{space is}} an {{informal}} {{term for the}} amount of computer data storage available on one or more storage devices, file <b>systems,</b> <b>disk</b> partitions, volumes, or logical disks.|$|R
5000|$|The Commodore Amiga's {{operating}} <b>system's</b> <b>disk</b> repair program Diskdoctor occasionally renames a disk [...] "Lazarus" [...] if {{it feels}} {{it has done}} a particularly good job of rescuing damaged files.|$|R
50|$|It {{includes}} Norton Image, Norton System Doctor, Norton Protection, Rescue <b>Disk,</b> <b>System</b> Information, Norton <b>Disk</b> Doctor, Norton UnErase, Space Wizard, Speed <b>Disk,</b> <b>System</b> Information. DOS tools include Disk Editor, Disk Doctor, Pre-Installation TuneUp.|$|R
50|$|ProDOS <b>system</b> <b>disk</b> {{images can}} be {{downloaded}} legally {{from a number of}} user group web sites. It can also be purchased on disk from Syndicomm, which distributes it under license from Apple Computer.|$|E
50|$|System Configuration History - Information {{about all}} {{supplied}} software products installed {{on a system}} is maintained on a <b>system</b> <b>disk</b> file. Users can also send entries to the file for application products they develop.|$|E
50|$|With {{the release}} of V1.6 of OneKey, the utility changed from {{freeware}} to paid software, subsequently making {{the features of the}} free edition very limited. All editions require enough space on the <b>system</b> <b>disk</b> to store the system image.|$|E
40|$|Simulation and {{modeling}} are useful tools in prototyping advances {{in many areas}} of technology. The performance of modern processors advances much faster than that of storage components making <b>disk</b> <b>systems</b> prime candidates for simulation. Simulating <b>disk</b> <b>systems</b> allows researchers to determine and quantify the performance impact of alternative file <b>system</b> designs and <b>disk</b> data rearrangement techniques. This paper describes a <b>System</b> Assisted <b>disk</b> I/O Simulation Technique (SAST). This technique uses a real <b>system</b> to estimate <b>disk</b> request service times, relieving the researcher from the burden of modeling the target system. The <b>system</b> accepts <b>disk</b> trace data as input, and accurately predicts disk service times. In addition, we describe a disk trace collection technique capable of collecting accurate trace data from the Windows 95 operating system. 1...|$|R
50|$|Programs can {{be saved}} using the Famicom Data Recorder {{cassette}} tape drive. Family BASIC was not designed to be compatible with the Famicom <b>Disk</b> <b>System.</b> The <b>Disk</b> <b>System's</b> RAM adapter {{requires the use of}} the Famicom's cartridge slot, which prevents using the slot for the Family BASIC cartridge.|$|R
5000|$|XRC is a z/Series {{asynchronous}} {{disk mirroring}} technique which is effective over any distance. It keeps the data time consistent across multiple ESS (Enterprise Storage Server) or HDS (Hitachi Data <b>Systems)</b> <b>disk</b> subsystems at the recovery site.|$|R
50|$|A disk is {{identified}} by either its physical name or (more often) by a user-defined logical name. For example, the boot device (<b>system</b> <b>disk)</b> {{may have the}} physical name $3$DKA100, but it is generally referred to by the logical name SYS$SYSDEVICE.|$|E
5000|$|ProDOS 8 version 2.x {{requires}} a 65C02 or later (65802, 65816) CPU. ProDOS 8 2.x runs in 64kB, but the utility {{programs on the}} <b>system</b> <b>disk</b> require 128kB. Systems with a 6502 CPU instead of a 65C02 must use ProDOS 8 versions prior to version 2.0.|$|E
5000|$|VOS uses a {{proprietary}} file naming syntax {{which includes the}} system name, module name, disk number, and directory, with components separated by the [...] ">" [...] symbol. Typically the <b>system</b> <b>disk</b> will be housed in the same module as the CPU, , so a system file for a VOS cluster would be referenced as ...|$|E
40|$|Optical <b>Disk</b> <b>Systems</b> have {{significant}} advantages over conventional magnetic mass storage media for very large database applications. Among other features, optical <b>disk</b> <b>systems</b> offer large capacity and high transfer rate. A critical problem {{is how to}} integrate the optical <b>disk</b> <b>system</b> into a total application system environment while maintaining the high performance capabilities of the optical disk. In this paper the performance of optical <b>disk</b> <b>system</b> configurations under realistic applicaiion environments is analyzed via queueing models. The results provide several important guidelines {{for the use of}} optical <b>disk</b> <b>systems</b> on large applications. 1...|$|R
50|$|With the {{supported}} {{capability of}} rolling upgrades and multiple <b>system</b> <b>disks,</b> cluster configurations {{can be maintained}} on-line and upgraded incrementally. This allows cluster configurations to continue to provide application and data access while {{a subset of the}} member nodes are upgraded to newer software versions.|$|R
5000|$|<b>System</b> <b>disks</b> {{which not}} only include the {{software}} above, but also contain a complete directory of all users with pertinent data about each (name, extension number, voice-mail preferences, and pointers {{to each of the}} messages stored on the message disk that belong to them); ...|$|R
5000|$|In this job, the {{assembler}} {{leaves the}} result of its assembly in the temporary area of the <b>system</b> <b>disk,</b> and the XEQ command executes {{the content of the}} temporary area. The odd-looking [...] has two meanings: end of assembler source, {{and the name of the}} entry point of the routine, which has the label START.|$|E
50|$|Linux {{supports}} numerous file systems, but common {{choices for}} the <b>system</b> <b>disk</b> on a block device include the ext* family (ext2, ext3 and ext4), XFS, JFS, ReiserFS and btrfs. For raw flash without a flash translation layer (FTL) or Memory Technology Device (MTD), there are UBIFS, JFFS2 and YAFFS, among others. SquashFS is a common compressed read-only file system.|$|E
50|$|The Carter cluster {{consists}} of HP Proliant compute nodes with two 8-core Intel Xeon-E5 processors (16 cores per node), either 32 gigabytes or 64 GB of memory, and a 500 GB <b>system</b> <b>disk.</b> NVIDIA Tesla GPU-accelerated nodes also are available. All nodes have 56 Gbit/s FDR Infiniband connections from Mellanox. Carter {{was the first}} cluster to employ this generation of Mellanox interconnects.|$|E
40|$|In {{the last}} decade, {{parallel}} <b>disk</b> <b>systems</b> have increasingly become popular for data-intensive applications running on high-performance computing platforms. Conservation {{of energy in}} parallel <b>disk</b> <b>systems</b> has a strong impact {{on the cost of}} cooling equipment and backup powergeneration. This is because a significant amount of energy is consumed by parallel disks in highperformance computing centers. Although a wide range of energy conservation techniques have been developed for <b>disk</b> <b>systems,</b> research on reliability analysis for energy-efficient parallel <b>disk</b> <b>systems</b> is still in its infancy. In this paper, we make use of a Markov process to develop a quantitative reliability model for energy-efficient parallel <b>disk</b> <b>systems</b> using data mirroring. With the new model in place, a reliability analysis tool is developed to efficiently evaluate reliability of fault-tolerant parallel <b>disk</b> <b>systems</b> with two power modes. More importantly, the reliability model makes it possible to provide a good compromise between energy efficiency and reliability in energy-efficient and fault-tolerant parallel <b>disk</b> <b>systems.</b> 1...|$|R
40|$|The large <b>disk</b> <b>systems</b> {{offered by}} IBM, UNIVAC, Digital Equipment Corporation, and Data General were examined. In particular, these <b>disk</b> <b>systems</b> were {{analyzed}} {{in terms of}} how well available operating systems take advantage of the respective disk controller's transfer rates, and to what degree all available data for optimizing disk usage is effectively employed. In the course of this analysis, generic functions and components of <b>disk</b> <b>systems</b> were defined and the capabilities of the surveyed <b>disk</b> <b>system</b> were investigated...|$|R
5000|$|Macintosh Plus <b>System</b> Tools <b>disk</b> with updated <b>system</b> {{software}} ...|$|R
50|$|In {{a cluster}} with a Lustre file system, the system network {{connecting}} the servers and the clients is implemented using Lustre Networking (LNet), which provides the communication infrastructure {{required by the}} Lustre file <b>system.</b> <b>Disk</b> storage {{is connected to the}} Lustre MDS and OSS server nodes using direct attached storage (SAS, FC, iSCSI) or traditional storage area network (SAN) technologies.|$|E
50|$|Most disk {{encryption}} systems overwrite their cached encryption keys as encrypted disks are dismounted.Therefore, {{ensuring that}} all encrypted disks are dismounted (secured) when {{the computer is}} in a position where it may be stolen may eliminate this risk, and also represents best practice. This mitigation is typically not possible with the <b>system</b> <b>disk</b> that the operating system is running on.|$|E
50|$|Portable Native Client (PNaCl) is an architecture-independent version. PNaCl apps are {{compiled}} ahead-of-time. PNaCl {{is recommended}} over NaCl for most use cases. The general concept of NaCl (running native code in web browser) has been implemented before in ActiveX, which, {{while still in}} use, has full access to the <b>system</b> (<b>disk,</b> memory, user-interface, registry, etc.). Native Client avoids this issue by using sandboxing.|$|E
5000|$|... #Caption: Artist's {{impression}} of the early Solar <b>System's</b> planetary <b>disk</b> ...|$|R
40|$|Abstract. In {{this paper}} we study data {{replication}} in a mirrored <b>disk</b> <b>system.</b> Free <b>disk</b> space is exploited by keeping replicas of specific cylinders at appropriate disk locations. Assuming an organ-pipe arrangement we calculate the expected seek distance by varying the probability cylinder access under different distributions. Also, analytic formulae are derived for the expected seek distance under replication and {{comparison with the}} conventional (without replication) mirrored <b>disk</b> <b>system</b> is performed. ...|$|R
40|$|Performance of disk I/O schedulers is {{affected}} by many factors, such as workloads, file <b>systems,</b> and <b>disk</b> <b>systems.</b> <b>Disk</b> scheduling performance can be improved by tuning scheduler parameters, such as the length of read timers. Scheduler performance tuning is mostly done manually. To automate this process, we propose four self-learning disk scheduling schemes: Change-sensing Round-Robin, Feedback Learning, Per-request Learning, and Two-layer Learning. Experiments show that the novel Two-layer Learning Scheme performs best. It integrates the workload-level and request-level learning algorithms. It employs feedback learning techniques to analyze workloads, change scheduling policy, and tune scheduling parameters automatically. We discuss schemes to choose features for workload learning, divide and recognize workloads, generate training data, and integrate machine learning algorithms into the Two-layer Learning Scheme. We conducted experiments to compare the accuracy, performance, and overhead of five machine learning algorithms: Decision Tree, Logistic Regression, Na√Øve Bayes, Neural Network, and Support Vector Machine Algorithms. Experiments with real-world and synthetic workloads show that selflearning disk scheduling can adapt {{to a wide variety}} of workloads, file <b>systems,</b> <b>disk</b> <b>systems,</b> and user preferences. It outperforms existing disk schedulers by as much as 15. 8 % while consuming less than 3 %- 5 % of CPU time...|$|R
