78|0|Public
50|$|The SpidoSpeed Rose Gold Black and SpidoSpeed Green {{take that}} idea of <b>skeletonisation</b> further than was {{previously}} {{seen in the}} SpidoLite Tech. These new Spidos are skeletonised {{not just in the}} outercase but also into the dial and movement.|$|E
50|$|Another {{aspect of}} Linde Werdelin, {{especially}} {{prevalent in the}} Spido family, is the extreme <b>skeletonisation</b> of the case, which is done both for style and to lighten the watch dramatically. This increases the sports aspect and Jorn and Morten found their inspiration for this in the Formula 1 industry. Further reference to the motor racing industry {{is evident in the}} SpidoSpeed, from the material used on the dial (which was previously used as dashboards in 1940s racing cars) to the small holes on the subdials resembling the brake discs to the round cut-out details on the strap which make reference to driving gloves.|$|E
5000|$|During {{the next}} 75 million years, plants evolved {{a range of}} more complex organs, such as roots and seeds. There is no {{evidence}} of any organism being fed upon until the middle-late Mississippian, [...] There was a gap of 50 to 100 million years between the time each organ evolved and the time organisms evolved to feed upon them; {{this may be due to}} the low levels of oxygen during this period, which may have suppressed evolution. Further than their arthropod status, the identity of these early herbivores is uncertain.Hole feeding and <b>skeletonisation</b> are recorded in the early Permian, with surface fluid feeding evolving by the end of that period.|$|E
50|$|CNN {{processors}} {{can be used}} as Reaction-Diffusion (RD) processors. RD processors are spatially invariant, topologically invariant, analog, parallel processors {{characterized by}} reactions, where two agents can combine to create a third agent, and diffusions, the spreading of agents. RD processors are typically implemented through chemicals in a Petri dish (processor), light (input), and a camera (output) however RD processors can also be implemented through a multilayer CNN processor. RD processors can be used to create Voronoi diagrams and perform <b>skeletonisation.</b> The main difference between the chemical implementation and the CNN implementation is that CNN implementations are considerably faster than their chemical counterparts and chemical processors are spatially continuous whereas the CNN processors are spatially discrete. The most researched RD processor, Belousov-Zhabotinsky (BZ) processors, has already been simulated using a four-layer CNN processors and has been implemented in a semiconductor.|$|E
40|$|A new shape {{descriptor}} {{obtained by}} <b>skeletonisation</b> of noisy binary images is presented. Skeleton extraction is performed {{by using an}} algorithm based on {{a new class of}} parametrised binary morphological operators, taking into account statistical aspects. Parameters are adaptively selected during the successive iterations of the <b>skeletonisation</b> operation to regulate the characteristics of the shape descriptor. A probabilistic interpretation of the scheduling strategy used for parameters is proposed by analogy to stochastic optimisation techniques. <b>Skeletonisation</b> results on patterns extracted by a change-detection method in a visual-based surveillance application are reported. Results show the greater robustness of the proposed method as compared with other morphological approaches...|$|E
40|$|<b>Skeletonisation</b> is a {{key process}} in {{character}} recognition in natural images. Under the assumption that a character is made of a stroke of uniform colour, with small variation in thickness, the process of recognising characters can be decomposed in the three steps. First the image is segmented, then each segment {{is transformed into a}} set of connected strokes (<b>skeletonisation),</b> which are then abstracted in a descriptor {{that can be used to}} recognise the character. The main issue with <b>skeletonisation</b> is the sensitivity with noise, and especially, the presence of holes in the masks. In this article, a new method for the extraction of strokes is presented, which address the problem of holes in the mask and does not use any parameters. CADICS; ELLIIT; CUA...|$|E
40|$|Abstract — This paper {{describes}} {{two approaches}} to approximation of handwriting strokes {{for use in}} writer identification. One approach {{is based on a}} thinning method and produces raster skeleton whereas the other approximates handwriting strokes by cubic splines and produces a vector skeleton. The vector <b>skeletonisation</b> method is designed to preserve the individual features that can distinguish one writer from another. Extraction of structural character-level features of handwriting is performed using both <b>skeletonisation</b> methods and the results are compared. Use of the vector <b>skeletonisation</b> method resulted in lower error rate during the feature extraction stage. It also enabled to extract more structural features and improved the accuracy of writer identification from 78 % to 98 % in the experiment with 100 samples of grapheme “th ” collected from 20 writers. I...|$|E
40|$|AbstractNailfold {{capillaroscopy}} (NC) is {{a routine}} technique {{used to assess}} the characteristics and morphology of nailfold capillaries. Observation of micro-blood vessels in the nailfold is important for diagnosing diseases that lead to morphological changes of capillaries such as scleroderma, Raynaud's phenomenon and other connective tissue diseases. In order to support a computer-aided diagnosis approach to analysing NC images, several approaches have been proposed in the literature aiming to extract capillaries. In general, such capillary <b>skeletonisation</b> algorithms involve an image pre-processing step, followed by binarisation and finally extraction and definition of the capillary skeletons. Since image denoising and enhancement in the pre-processing step can {{have a major impact on}} the subsequent analysis, in this paper, we evaluate the performance of five enhancement techniques for the purpose for nailfold capillary <b>skeletonisation.</b> In particular, we investigate the α-trimmed filter, bilateral filter, bilateral enhancer, anisotropic diffusion filter and non-local means and integrate them with three capillary extraction algorithms from the literature. We report visual and quantitative performance on a set of diverse NC images. The obtained results indicate that a relatively simple α-trimmed filter, combined with a <b>skeletonisation</b> algorithm incorporating a difference-of-Gaussian approach to address non-uniform lighting and an iterative rule-based <b>skeletonisation</b> procedure, leads to the best results when comparing the obtained skeletonisations to a manually obtained ground truth...|$|E
40|$|The {{extended}} Euclidean distance transform Shape representation {{has always}} {{played a central}} role in computer vision. Skeletal shape descriptors which make symmetry explicit are an important class of shape repre- sentations. The goal of this thesis is to study the problems encountered using skeletal shape descriptors. The thesis unites three main themes of work: A filter based approach to <b>skeletonisation,</b> <b>skeletonisation</b> using parallel wave propagation and <b>skeletonisation</b> using an extended Euclidean distance transform. The distance transform approach to <b>skeletonisation</b> computes a skeleton by identi- fying the so called local maxima of the distance transform. A new method has been proposed to detect these features using a filter-base approach inspired by models of processes in the human visual system. Further improvements were made by using a filter designed to detect a specific geometric feature on the distance transform which corresponded to the skeletal points. This improved the quality of skeletons obtained but could only compute restricted skeletal descriptions. The wave propagation algorithm of Brady and Scott has been studied; they origi- nally implemented this on a simulator of the Connection Machine. The issues of map- ping the algorithm onto an array of transputers have been investigated. An efficient implementation was realised by reducing synchronisation and data transfer overheads. It was found that the algorithm could compute more general shape descriptions than the distance transform approach but the quality of skeletons produced was not as good. Using standard techniques from singularity theory, an analysis of distance functions from object boundaries has been undertaken. This resulted in the formal definition of a new extended Euclidean distance transform. An algorithm has been devised to perform <b>skeletonisation</b> using the extended distance transform. This combined the advantages of the filter and wave based techniques in that it produced skeletons of a high quality which made more symmetries explicit than the standard distance transform approach. In addition, the extended distance transform provides an elegant unifying framework for work on skeletal shape descriptions. Abstract Above all I would like to thank my supervisor Roberto Cipolla for his support, advic...|$|E
40|$|In this paper, a novel <b>skeletonisation</b> {{algorithm}} is presented that draws on techniques developed for mobile robot mapping and navigation {{and offers a}} number of advantages over existing <b>skeletonisation</b> methods. First, because the algorithm works by hopping from one landmark position in the image to another, it has to visit far fewer pixels to find a skeleton compared to conventional algorithms. Second, unlike other techniques, the exploratory nature of the algorithm allows it to identify junctions and end-points on the fly, which facilitates later high-level symbolic processing. Finally, the method is more generic than others, {{in the sense that}} it can be adjusted to compute skeletons containing a variety of different sorts of morphological information. 1...|$|E
40|$|Topological skeletons are shape {{descriptors}} {{that have}} been applied successfully in practical applications. However, many <b>skeletonisation</b> methods lack accessibility, mainly due {{to the need for}} manual parameter adjustment and the shortage of tools for comparative analysis. In this paper we address these problems. We propose two new homotopypreserving thinning algorithms: Flux-ordered adaptive thinning (FOAT) extends existing flux-based thinning methods by a robust automatic parameter adjustment, maximal disc thinning (MDT) combines maximal disc detection on Euclidean distance maps with homotopic thinning. Moreover, we propose distinct quality measures that allow to analyse the properties of <b>skeletonisation</b> algorithms. Tests of the new algorithms and quality assessment tools are conducted on the widely used shape database CE-Shape- 1...|$|E
40|$|In this {{document}} it is described {{the implementation of}} the flux driven automatic centerline extraction algorithm proposed by Bouix et al. in 2004. This is based on a <b>skeletonisation</b> algorithm initially proposed by Siddiqi et al. in 2002, using properties of an average outward flux measure to distinguish skeletal points from non-skeletal ones. This information is combined with a topology preserving thinning procedure to obtain the final result. This implementation combines this <b>skeletonisation</b> algorithm with other techniques as described in the paper of Bouix et al. to produce an ITK filter that generates as output the skeleton, as a binary object represented in an image, of the input surface, represented as a distance transform image. In this work is described this medial curve extraction procedure following the IT...|$|E
40|$|Biometric {{recognition}} encompasses numerous modern strategies. Among them, fingerprint {{reputation is}} {{taken into consideration}} {{to be the most}} effective approach for utmost security authentication. As industrial incentives boom, many new technologies for user identity are being advanced, each with its very own strengths and weaknesses and a potential area of interest marketplace. Fingerprint matching consists of a different process like filtering or preprocessing, binarisation, thinning or <b>skeletonisation,</b> postprocessing, feature extraction, and matching. Out of these fingerprint thinning or <b>skeletonisation</b> is one of the important processes in fingerprint identification or verification systems. Fingerprint thinning or <b>skeletonisation</b> is the manner or technique of lowering the thickness of every line of a fingerprint pattern or ridge pattern to just a single pixel width. After extracting the minutiae from the improved, binarised and thinned image some post-processing is carried out on this final fingerprint image to take away any spurious minutiae. The techniques on this class are of types–crossing number based and morphology-based totally. In this paper even though a new method for thinning is not proposed but a real attempt is made to explain the Edge prediction based thinning process. The Edge Prediction based Skelton formation is totally based on the conditional thinning set of rules, which is used to carry out thinning. The Edge Prediction based thinning process is explained with the help of workflow, algorithm, and flowchart...|$|E
40|$|The path {{planning}} {{is not a}} trivial problem of artificial intelligence. An agent has to find a path from one state (or position) to another whilst avoiding contact with obstacles. The configuration space used for representation of all agent states is usually continuous, which makes the problem even more complex. <b>Skeletonisation</b> is one of approaches, which "discretises" continuous space and reduces it to a graph search problem...|$|E
40|$|In this thesis, a {{combination}} of <b>skeletonisation</b> and graph matching techniques, coupled with a blend of supervised and unsupervised learning methodology {{is applied to the}} task of characterising and classifying natural shapes. A novel navigation-based <b>skeletonisation</b> algorithm is used to gather low level structural and morphological information about the shape. Subsequently, the data are converted into a series of attributed graphs, which characterise the image. Graphs of the same type can then be compared using an approximate graph matcher, which identifies a degree of similarity between them. Each degree of similarity corresponds to a data point in a conceptual space (as defined by Gärdenfors). The proposed method is applied to two distinct problems; the classification of leaf types, and the characterisation of river networks. The classification and characterisation systems are tested on a database of images of leaves and a collection of satellite images respectively. The novel navigation-based <b>skeletonisation</b> algorithm features several advantages; first, it allows the collection of topological and morphological information on the fly. This eliminates the need for any post-processing on the extracted skeletons. In addition, the adaptation of the algorithm to suit different applications is facilitated by the fact that any sort of morphological information can be included without alterations to the function of the algorithm. The conversion of the skeletons to attributed graphs is simplified by the existence of structural and morphological flags in the skeletal points. Last, concepts are created in the resulting conceptual space by means of a best-guess approach as well as a mechanism for accommodating external user input. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|E
40|$|In {{this paper}} an {{efficient}} queue-based algorithm for order independent homotopic thinning is proposed. This generic algorithm {{can be applied}} to various thinning versions: homotopic marking, anchored <b>skeletonisation,</b> and the computation of the skeleton of influence zones based on local pixel characterisations. An example application of the proposed method to detect the medial axis of wide river networks from satellite imagery is also presented. JRC. H. 6 -Spatial data infrastructure...|$|E
40|$|Many <b>skeletonisation</b> {{algorithms}} for discrete volumes {{have been}} proposed. Despite its simplicity, the one given here still has many theoretically favorable properties. Actually, {{it provides a}} connected surface skeleton that allows shapes to be reconstructed with bounded error. It {{is based on the}} application of directional erosions, while retaining those voxels that introduce disconnections. This strategy is proved to be specially well-suited for extreme vertex encoded volumes, leading to a fast thinning algorithm. Preprin...|$|E
40|$|Abstract. GIS systems often rely on low-level, pixel-based {{representations}} of satellite scenes. The {{purpose of this}} paper is to show the advantages of using an intermediate representation incorporating multiple criteria in scene characterisation, as well as a framework for monitoring changes over time based on features of interest. A Conceptual Spaces framework, in conjunction with navigation-based <b>skeletonisation</b> are employed for this purpose. We evaluate our system on satellite images of rivers and lakes. ...|$|E
40|$|The {{distance}} transform {{has been}} proposed for use in computer vision {{for a number of}} applications such as matching and <b>skeletonisation.</b> This paper proposes two things: (1) a multi-scale distance transform to overcome the need to choose edge thresholds and scale and (2) the addition of various saliency factors such as edge strength, length and curvature to the basic distance transform to improve its effectiveness. Results are presented for applications of matching and snake fitting. ...|$|E
40|$|Terrestrial laser {{scanners}} capture 3 D geometry {{as a point}} cloud. This paper {{reports on}} a new algorithm aiming at the <b>skeletonisation</b> of a laser scanner point cloud, representing a botanical tree without leafs. The resulting skeleton can subsequently be applied to obtain tree parameters like length and diameter of branches for botanic applications. Scanner-produced point cloud data are not only subject to noise, but also to undersampling and varying point densities, making it challenging to extract a topologically correct skeleton. The <b>skeletonisation</b> algorithm proposed in this paper consists of three steps: (i) extraction of a graph from an octree organization, (ii) reduction of the graph to the skeleton and (iii) embedding of the skeleton into the point cloud. The results are validated on laser scanner point clouds representing botanic trees. On a reference tree, the mean and maximal distance of the point cloud points to the skeleton could be reduced from 1. 8 to 1. 5 cm for the mean and from 15. 6 to 10. 5 cm for the maximum, compared to results from a previously developed method. Earth Observation and Space SystemsAerospace Engineerin...|$|E
40|$|Forensic {{anthropologists and}} taphonomists are often tasked with {{interpreting}} {{the sequence of}} events from death through decomposition to <b>skeletonisation.</b> Discovery of burnt bone often evokes questions as to the condition of the body prior to the burn event. The {{purpose of this study was}} to evaluate features of thermal damage on bones in relationship to the condition of the bone (dry/wet) and progression of decomposition. Twenty-five pigs in various stages of decomposition (fresh, early, advanced, early & late <b>skeletonisation)</b> were exposed to fire for 30 minutes. The skeletal elements were scored and features included: colour change (unaltered, charred, calcined), brown and heat borders, heat lines, delineation, greasy bone, joint shielding, predictable and minimal cracking, delamination and heatinduced fractures. Colour changes were scored according to a ranked percentage scale (0 – 3) and the remaining traits as absent or present (0 / 1). Cohen’s Kappa statistics evaluated intraand interobserver error. Density plots and frequency distributions were constructed and multiple regression (categorical variables) and transition analysis were employed. The majority (8) of the 13 traits displayed potential to predict decomposition stage from burned remains. An increase in calcined and charred bone occurred synchronously with an advancement in decomposition. The organic composition of bone and presence of flesh affect the characteristics features of burned bone. Greasy bone occurred most often in the early/fresh stages (fleshed bone). Heat borders, heat lines, delineation, joint shielding, predictable and minimal cracking were associated with wet tissue/bone; whereas brown burn/borders, delamination and other heat-induced fractures were associated with early and late <b>skeletonisation.</b> No statistically significant differences were noted among observers for the majority of the traits except for predictable and minimal cracking and heat-induced fractures in the cranium. Heat-induced changes may assist in estimating decomposition stage from unknown, burnt remains and thereby aid in a providing an indication as to the condition of the bone prior to the burn event. Thesis (PhD) [...] University of Pretoria, 2014. gm 2014 Anatomyunrestricte...|$|E
40|$|International Conference on Discrete Geometry for Computer Imagery (DGCI), 2003, Nápoles (Italia) Many <b>skeletonisation</b> {{algorithms}} for discrete volumes {{have been}} proposed. Despite its simplicity, the one given here still has many theoretically favorable properties. Actually, {{it provides a}} connected surface skeleton that allows shapes to be reconstructed with bounded error. It {{is based on the}} application of directional erosions, while retaining those voxels that introduce disconnections. This strategy is proved to be specially well-suited for extreme vertex encoded volumes, leading to a fast thinning algorithm. Peer Reviewe...|$|E
40|$|This paper {{presents}} {{a method for}} segmenting binary patterns into seven mutually exclusive categories: core, islet, loop, bridge, perforation, edge, and branch. This is achieved by applying a series of morphological transformations such as erosions, geodesic dilations, reconstruction by dilation, anchored <b>skeletonisation,</b> etc. The proposed method depends on a single parameter only {{and can be used}} for characterising binary patterns with emphasis on connections between their parts as measured at varying analysis scales. This is illustrated on two examples related to land cover maps and circuit board defect detection. JRC. H. 6 -Spatial data infrastructure...|$|E
40|$|This thesis {{deals with}} {{recognition}} by retina (identification, verification). In introduction we describe information about human eye, its diseases with focus on retina impact. Further (in main part) we conduct SW analyses based on biometry retina requirements {{and design of}} SW application for retina recognition. It is based on processing pipeline design (sequential application of image filters). This pipeline mostly contains filters focused on edge detection, adaptive threshold and <b>skeletonisation.</b> Finally, basic SW functions includes users registration (enroll), identification, verification. In conclusion we discuss experimental results and success of designed SW in practical application...|$|E
40|$|International audienceMost of {{existing}} computer methods to analyse {{the perception of}} the open spaces do not consider the Gibsonian motion perspective, i. e.,the gradual changes in the visual field in the rate of displacement. This paper describes a new approach considering this motion perspective to analyse paths and routes in urban environments. Based on Teller 's spherical metric, it proposes to use the <b>skeletonisation</b> as a way to trace the variations of the sky shape between successive views. It is thereby possible to structure the route into sequences,according to its potential interest in the urban environment...|$|E
40|$|This report {{describes}} {{the design of}} a feature-based gait classification system. The system is an extension to an existing posture analysis system, and is designed to be implemented using FPGA architecture. The gait matching algorithm is based on extracting a skeleton that corresponds to a blob, and analysing various cues in the movement of the skeleton through a sequence of frames. The project has three main contributions: 1. The design of a new <b>skeletonisation</b> system, based on an enhanced radial projection algorithm, which builds on earlier work on gait classification. The <b>skeletonisation</b> algorithm uses low-pass filtering of the radial projection to extract a subject from the frame, and generates a corresponding skeleton with a centroid and three extremities. 2. A gait classifier that uses multiple cues of the moving skeleton to decide which of the reference gait types most closely matches the current gait sequence. This includes analysing the sequences to select cues that best distinguish between the different gait types, devising a formula to give match scores to different gaits and cues, and bringing these scores together into a combined match score. 3. Evaluation of the accuracy of the gait classification algorithm. Tests were done using the UbiSense dataset featuring two subjects and five gait types. The system is able to classify 79. 8 % of the test sequences into the correct gait types. When deciding whether the gait is normal or abnormal, the classifier achieves a 98. 4 % accuracy...|$|E
40|$|Many <b>skeletonisation</b> {{algorithms}} for discrete volumes {{have been}} proposed. The one given {{here can be}} fully implemented in terms of regularized boolean differences, unions and intersections of volumes, without relying on large look-up tables. Despite its simplicity, {{it is able to}} compute connected subvoxel surface skeletons that allow volumes to be reconstructed with the desired predefined error. It is based on the application of directional erosions, while retaining those regions that introduce disconnections. This strategy is proved to be specially well-suited for processing extreme vertices encoded volumes, as the intensive use of Boolean operations of the method can be substantially speeded up using the extreme vertices model. Postprint (published version...|$|E
40|$|Abstract. Distance {{fields are}} a widely {{investigated}} area within {{the area of}} Volume Graphics. Research is divided between applications; such as – <b>skeletonisation,</b> hypertexture, voxelisation, acceleration of rendering techniques, correlation and collision detection; and the fundamental algorithmic calculation of the distance fields. This paper concentrates on the latter by presenting a new method for calculating distance fields and comparing it with the current best approximate method and the true Euclidean distance field. Details are given of the algorithm, and the acceleration methods that are used for calculating the true distance field. Brief descriptions of applications for these accurate distance fields are given {{at the end of}} the paper. ...|$|E
40|$|This paper {{presents}} a novel hybrid preprocessing method for improving noise resilience and improved computational efficiency of image <b>skeletonisation.</b> Common techniques for extracting the topological {{skeleton of a}} shape include distance transforms, thinning, and geometric analysis. All of these methods are sensitive to noise, and can suffer from instability and unwanted spurs. In the case of needing to match skeletons from different images, instability can be a significant problem. Skeleton stability using the proposed preprocessing method is evaluated {{for a range of}} existing medial axis transforms. It is shown to be more effective for suppressing unwanted spurs and improving stability against other preprocessing techniques. Full Tex...|$|E
40|$|Homotopic {{thinning}} algorithms {{have long}} been investigated in pattern recognition and image analysis. However, they are order dependent {{in the sense that}} the output depends upon the order used for processing the image pixels (as well as the order in which homotopic structuring elements are applied for algorithms based on sequential homotopic thinning). In this paper, we tackle this problem by introducing the notion of order independent homotopic thinning. In its basic form, it leads to an order independent homotopic marking algorithm. However, when considering appropriate anchor points, it leads to binary and grey tone order independent <b>skeletonisation</b> algorithms. An application to the extraction of crest lines on digital elevation models is briefly presented before concluding...|$|E
40|$|In this work, {{we attempt}} to tackle the problem of {{skeletal}} tracking of a human body using the Microsoft Kinect sensor. We use cues from the RGB and depth streams from the sensor to fit a stick skeleton model to the human upper body. A variety of Computer Vision techniques are used with a bottom up approach to estimate the candidate head and upper body postitions using haar-cascade detectorsa and hand positions using skin segmentation data. The data is finally integrated with the Extended Distance Transform <b>skeletonisation</b> algorithm to obtain a fairly accurate estimate of the the skeleton parameters. The results presented show that this method can be extended to perform in real time. ...|$|E
40|$|International audienceAutomatic {{labelling}} {{and identification}} of cortical sulci {{which can be}} used as landmarks in registration between patients or between different modalities. Methods used to achieve this goal are the segmentation of the brain, the detection of sulci using classification and threshold or morphologic tool, the statistical or structural recognition techniques. Our previous work proceeds with segmentation of the brain by thresholding the LCR, and <b>skeletonisation</b> of the LCR. We present here a method based on geometrical feature (curvature) which doesn't require the fine segmentation of the LCR-Grey Matter or of the Grey Matter-White Matter interface. Although our previous work [1] used the superficial trace of sulci, we detect the roof of the sulci, which presents less intersection between sulci than the superficial trace of sulci...|$|E
40|$|This article {{deals with}} the problem of vessel edge and {{centerline}} detection using classical image processing techniques due to their simpleness and easiness to be implemented. The method is divided into four steps: the vessel enhancement which implies a non-linear filtering proposed by Frangi, the thresholding using Otsu method and the contour detection using the Canny edge detector due to its good performances for the small vessels and the morphological <b>skeletonisation.</b> The algorithms are tested on real data collected from a cardiac catheterism laboratory and it is accurate for images with good spatial resolution (512 * 512). The output image can be used for further processing in order to find the vessel length or its radius. Comment: 12 pages, 1 figures, 2 tables with figures, journa...|$|E
40|$|Synchrotron X-ray microtomography and <b>skeletonisation</b> method {{were used}} to study the true 3 D network {{structures}} and morphologies of the Fe-rich intermetallic phases in Al- 5. 0 %Cu- 0. 6 %Mn alloys with 0. 5 % and 1. 0 % Fe. It was found that, the Fe-phases in the 1. 0 %Fe alloy have node lengths of 5 - 25 m; while those in the 0. 5 %Fe alloy are of 3 - 17 m. The Fe-phases in the 1. 0 %Fe alloy also developed sharper mean curvature with wider distribution {{than those in the}} 0. 5 %Fe alloy. Combining SEM studies of the deeply-etched samples, the true 3 D structures of 4 different type Fe-phases in both alloys are also revealed and demonstrated. Comment: 1...|$|E
40|$|Forensic anthropologists are {{tasked with}} {{interpreting}} {{the sequence of}} events from death to the discovery of a body. Burned bone often evokes questions as to the timing of burning events. The {{purpose of this study was}} to assess the progression of thermal damage on bones with advancement in decomposition. Twenty-five pigs in various stages of decomposition (fresh, early, advanced, early and late <b>skeletonisation)</b> were exposed to fire for 30 min. The scored heat-related features on bone included colour change (unaltered, charred, calcined), brown and heat borders, heat lines, delineation, greasy bone, joint shielding, predictable and minimal cracking, delamination and heat-induced fractures. Colour changes were scored according to a ranked percentage scale (0 – 3) and the remaining traits as absent or present (0 / 1). Kappa statistics was used to evaluate intra- and inter-observer error. Transition analysis was used to formulate probability mass functions [P(X = jji) ] to predict decomposition stage from the scored features of thermal destruction. Nine traits displayed potential to predict decomposition stage from burned remains. An increase in calcined and charred bone occurred synchronously with advancement of decomposition with subsequent decrease in unaltered surfaces. Greasy bone appeared more often in the early/fresh stages (fleshed bone). Heat borders, heat lines, delineation, joint shielding, predictable and minimal cracking are associated with advanced decomposition, when bone remains wet but lacks extensive soft tissue protection. Brown burn/borders, delamination and other heat-induced fractures are associated with early and late <b>skeletonisation,</b> showing that organic composition of bone and percentage of flesh present affect the manner in which it burns. No statistically significant difference was noted among observers for the majority of the traits, indicating that they can be scored reliably. Based on the data analysis, the pattern of heat-induced changes may assist in estimating decomposition stage from unknown, burned remains. National Research Foundation (NRF) of South Africa. [URL]...|$|E
40|$|We {{present a}} {{perspective}} {{on the nature of}} hybridness withinhybrid symbolic/connectionistmodels of cognition. We discuss the qualities that differentiate apparently hybrid models from those which truly accrue properties of the complementary types of technology, and argue that the latter type of model is constituted by a stringent set of mappings between the component systems, mappings which preserve the semantic content of representations and algorithmic relations over them, as well as critical functional properties of the underlying technology. These mappings are exemplified in the techniques of <b>skeletonisation</b> and implementation, but are absent from cluster-analytic techniques and from hybrid models arising from the juxtaposition of separate connectionist and symbolic physical systems. The development of additional constraints for defining appropriate mappings between hybrid models and the resultant types of hybrid models remain areas for future research. ...|$|E
