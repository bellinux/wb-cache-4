21|218|Public
5000|$|Calculate the {{percentage}} difference between today's and yesterday's value in that final <b>smoothed</b> <b>series.</b>|$|E
5000|$|... 2.Ratios of data {{to trend}} computed(called“centered ratios”)---which means remove the <b>smoothed</b> <b>series</b> from Y to leave S and E.|$|E
50|$|SSA's {{applicability}} to {{any kind}} of stationary or deterministically trending series has been extended to the case of a series with a stochastic trend, also known as a series with a unit root. In Hassani and Thomakos (2010) and Thomakos (2010) the basic theory on the properties and application of SSA in the case of series of a unit root is given, along with several examples. It is shown that SSA in such series produces a special kind of filter, whose form and spectral properties are derived, and that forecasting the single reconstructed component reduces to a moving average. SSA in unit roots thus provides an `optimizing' non-parametric framework for smoothing series with a unit root. This line of work is also extended to the case of two series, both of which have a unit root but are cointegrated. The application of SSA in this bivariate framework produces a <b>smoothed</b> <b>series</b> of the common root component.|$|E
5000|$|In 1970, Pelletier {{developed}} a continuous price series which was trademarked [...] "Perpetual Contracts". It is a weighted average between two futures contracts. It provided a <b>smooth</b> <b>series</b> that was ideal for back-testing futures trading systems.|$|R
40|$|Potential {{output is}} an {{important}} concept in economics. Policymakers often use a one-sector neoclassical model to think about long-run growth, and often assume that potential output is a <b>smooth</b> <b>series</b> {{in the short run}} [...] approximated by a medium- or long-run estimate. But in both the short and long run, the one-sector model falls short empirically, reflecting the importance of rapid technical change in producing investment goods; and few, if any, modern macroeconomic models would imply that, at business cycle frequencies, potential output is a <b>smooth</b> <b>series.</b> Discussing these points allows us to discuss a range of other issues that are less well understood, and where further research could be valuable. Input-output analysis; Productivity; Monetary policy...|$|R
3000|$|Given two {{stationary}} series X_t and Y_t, we {{say that}} X_t is smooth if ρ _ 1 ^X> 0. If also ρ _ 1 ^X>ρ _ 1 ^Y, then X_t is smoother than [...] Y_t. If ρ _ 1 ^Y< 0, the series Y_t is nonsmooth. A <b>smooth</b> <b>series</b> is therefore <b>smoother</b> than all white noise series and all nonsmooth series. If Y_t is nonsmooth and ρ _ 1 ^Y<ρ _ 1 ^X holds, then Y_t is [...] more nonsmooth than X_t. A nonsmooth series is thus more nonsmooth than all white noise <b>series</b> and all <b>smooth</b> <b>series.</b> To examine if visual impressions of smoothness or nonsmoothness align with the conclusions of these formal criteria, differences of scale must be accounted for, see Sect. 13.2 and 13.3 and associated figures for illustrations.|$|R
40|$|This article {{presents}} a brief survey about core inflation and shows the first results {{obtained in the}} computation of such indicator for the Brazilian IPCA for the period from January/ 1996 to ay/ 2000. The performance of five alternative measures of core inflation (exclusion method, symmetric trimmed mean, symmetric trimmed mean with <b>smoothed</b> <b>series,</b> weighted median and double weighted indicator) is evaluated and compared. The preliminary {{results show that the}} double weighted measure and the 20 % trimmed mean with <b>smoothed</b> <b>series</b> performed better. ...|$|E
30|$|As we {{have just}} mentioned, in this section we {{formally}} introduce Ucarima models, obtain their reduced form representation, review maximum likelihood estimation in the frequency domain, apply Wiener–Kolmogorov filtering theory to optimally extract the unobserved components and derive the time series properties of the <b>smoothed</b> <b>series.</b>|$|E
40|$|Loehle and McCulloch (LM, {{submitted}} to Energy and Environment as a correction to Loehle 2007) construct a Global Temperature Reconstruction for 16 AD – 1935 AD based on 18 peer-reviewed published non-treering proxy series. These {{are the same}} 18 series used by Loehle (2007), with the dating of 4 of the series corrected by a 50 -year shift. Each series was newly interpolated and smoothed with a 29 year moving average by CL over the period 1 AD – 1980 AD, to the extent available. Each series was converted to bimillennial anomalies by subtracting out its own mean. The global temperature reconstruction is the unweighed average of these anomalies. Because the number of available series drops abruptly from 11 to 8 in 1935, i. e. to {{less than half the}} maximum number of series, the reconstruction was terminated in 1935. The 18 <b>smoothed</b> <b>series</b> and their residuals about the global average are individually graphed {{at the end of this}} note. The <b>smoothed</b> <b>series,</b> as used in the reconstruction, are online vi...|$|E
40|$|Both, Federal Reserve Bank of Minneapolis. The views {{expressed}} herein {{are those of}} the authors and not necessarily those of the Federal Reserve Bank of Minneapolis or the Federal Reserve System. In previous research, we and other have represented time series as the sum of a <b>smooth</b> <b>series</b> and another <b>series.</b> The <b>smooth</b> <b>series</b> {ti} n i= 1 associated with a given time series {yi} n i= 1 is the solution to the problem: min {ti} n i= 1 nX n− 1 X (yi − ti) 2 + s [(ti − ti− 1) − (ti+ 1 − ti) ] 2. i= 1 i= 1 One method of solution is to solve the system of n linear equations which constitutes the first-order conditions for this convex programming problem. With this method, computing time and storage requirement are large if n is large. This subroutine efficiently solves the program. Storage costs and computational tim...|$|R
40|$|Taking {{some form}} of moving averages yields a <b>smoothing</b> of time <b>series</b> which is delayed. However, taking moving averages in the reverse time {{direction}} gives a smoothing which is in advance. The two resulting <b>smoothed</b> time <b>series</b> are pointwise averaged getting as result a smoothed version with "no delay"...|$|R
40|$|This paper {{investigates the}} {{consequences}} of non-stationarity for the principal components analysis and suggests a data transformation that allows obtaining <b>smoother</b> <b>series</b> for the first principal component {{to be used as}} a core inflation indicator. The paper also introduces a theoretical model, which allows interpreting core inflation as a common stochastic trend to the year-on-year rates of change of the price indices of the basic CPI items. Finally, it is shown that the first principal component computed in real time meets the evaluation criteria introduced in Marques et al. (2000). ...|$|R
40|$|For many {{practical}} applications, such as {{planning for}} satellite orbits and space missions, {{it is important}} to estimate the future values of the sunspot numbers. There have been numerous methods used for this particular case of time series prediction, including recently neural networks. In this paper we present genetic programming technique employed to sunspot series prediction. The paper investigates practical solutions and heuristics for an effective choice of parameters and functions of genetic programming. The results obtained expect the maximum in the current cycle of the <b>smoothed</b> <b>series</b> monthly sunspot numbers is 164 ± 20, and 162 ± 20 for the next cycle maximum, at the 95 % level of confidence. These results are discussed and compared with other predictions...|$|E
40|$|We found a {{significant}} positive correlation between local summer air temperature (May-September) and the annual sediment mass accumulation rate (MAR) in Lake Silvaplana (46 °N, 9 °E, 1800 m a. s. l.) {{during the twentieth century}} (r = 0. 69, p < 0. 001 for decadal <b>smoothed</b> <b>series).</b> Sediment trap data (2001 - 2005) confirm this relation with exceptionally high particle yields during the hottest summer of the last 140 years in 2003. On this base we developed a decadal-scale summer temperature reconstruction back to AD 1580. Surprisingly, the comparison of our reconstruction with two other independent regional summer temperature reconstructions (based on tree-rings and documentary data) revealed {{a significant}} negative correlation for the pre- 1900 data (ie, late ‘Little Ice Age’). This demonstrates that the correlation between MAR and summer temperature is not stable in time and the actualistic principle does not apply in this case. We suggest that different climatic regimes (modern/‘Little Ice Age’) lead to changing state conditions in the catchment and thus to considerably different sediment transport mechanisms. Therefore, we calibrated our MAR data with gridded early instrumental temperature series from AD 1760 - 1880 (r = - 0. 48, p < 0. 01 for decadal <b>smoothed</b> <b>series)</b> to properly reconstruct the late LIA climatic conditions. We found exceptionally low temperatures between AD 1580 and 1610 (0. 75 °C below twentieth-century mean) and during the late Maunder Minimum from AD 1680 to 1710 (0. 5 °C below twentieth-century mean). In general, summer temperatures did not experience major negative departures from the twentieth-century mean during the late ‘Little Ice Age’. This compares well with the two existing independent regional reconstructions suggesting that the LIA in the Alps was mainly a phenomenon of the cold season...|$|E
40|$|Abstract – Return {{series to}} broad asset classes often possess {{histories}} of unequal length {{as well as}} the presence of smoothing. Covariances typically revert to using the common, though shorter, series length while covariances for <b>smoothed</b> <b>series</b> are necessarily biased downward. These pose serious problems which, if left unresolved, will generate suboptimal and misleading allocations across asset classes. This paper discusses and draws together elements of the underlying theory in proposing an informationally efficient covariance estimator. This estimator is then compared to conventional covariance estimates in an empirical application using data from seven asset classes typically considered by institutional investors. Specifically, we show that covariance estimates are sensitive to both truncated estimates involving shorter series and the effects of smoothing which, if left unattended, will produce significantly biased allocations and higher portfolio risk...|$|E
50|$|Between 1870 and 1885, De Forest {{published}} {{more than}} twenty articles on statistics, using in some of them {{an early version of}} the Monte Carlo method to <b>smooth</b> time <b>series.</b>|$|R
3000|$|... [...]) are the variance-covariance {{matrices}} of x⃗_s(t_i), x⃗_f(t_i), and x⃗_b(t_i), respectively. As seen in Fig. 1, {{this gives}} more <b>smoothed</b> time <b>series</b> {{compared to the}} forward and backward Kalman filter solutions.|$|R
40|$|International audienceTime <b>series</b> <b>smoothing</b> {{functions}} {{have been}} frequently applied to fit multi-temporal vegetation index for better extraction of plant seasonal/growing parameters. Questions are raised that whether the smoothing {{is necessary for}} crop mapping. Four time <b>series</b> <b>smoothing</b> functions, namely, HANTS, Savitzky-Golay (S-G), double logistics and asymmetric Gaussian, were used to smooth 23 MODIS 16 -days composite NDVI images in one year. The effectiveness were compared through visual check, correlation coefficient R, {{root mean square error}} (RMSE), and local signal noise ratio (SNR). The best <b>smoothing</b> time <b>series</b> NDVI images, along with the original time series images, were then used to map corn and soybeans by spectral angle mapper (SAM) method and their mapping accuracies were compared. Comparison of smoothing results showed that S-G fitted data got the strongest correlation coefficient R, the lowest RMSE and lower local SNR. Comparison of mapping results further showed that time smoothing function does not improve the classification accuracy obviously with the same training sample and same temporal bands. The whole analysis indicates that it is the mapping method that matters more than time <b>series</b> <b>smoothing</b> function for classification precision...|$|R
40|$|Studies {{have shown}} that many {{consumers}} and businesses fail to invest in energy efficiency improvements despite seemingly ample financial incentives to do so - the so called energy efficiency gap or paradox. Attempts to explain this gap often focus on searching costs, information frictions and behavioral factors. Using data on Norwegian electricity prices and Google searches for heat pumps, I suggest that the inherently spikey nature of many electricity markets has a strong and significant positive effect on searching for information on energy efficiency goods. Because consumers pay for electricity based on at least monthly averages of the wholesale price, I can identify the informational and behavioral effect by decomposing prices into smoothed and deviation components using a novel method of measuring spikiness, comparing the actual price series {{with a range of}} deviations from Loess <b>smoothed</b> <b>series...</b>|$|E
40|$|More robust {{statistical}} methods {{are required for}} examining phenological time series as they are often noisy and non-stationary. Wavelet analytic methods easily handle such data. A maximal overlap discrete wavelet transform (MODWT) analysis of flowering records (1940 – 1970) of Eucalyptus tricarpa, E. leucoxylon, E. microcarpa and E. polyanthemos identified four subcomponents in each flowering series: characterized as a non-flowering phase, duration, annual and intensity cycles. A decreasing overall trend in flowering was identified by the MODWT <b>smoothed</b> <b>series.</b> Wavelet correlation found the same contemporaneous effects of climate on flowering for Eucalyptus tricarpa and E. leucoxylon, and for E. microcarpa and E. polyanthemos. Wavelet cross-correlation analysis identified the cyclical influence of temperature and rainfall on peak flowering intensity (P < 0. 0001). For each species there are 6 months of the annual cycle in which any given climate variable positively influences flowering intensity and 6 months of negative influence. For all species, rainfall exerts a negative influence when temperature is positive...|$|E
40|$|This chapter {{discusses}} wavelet analysis {{which is}} a robust statistical method capable of handling noisy and non-stationary data which phenological time series often are. We used a maximal overlap discrete wavelet transform (MODWT) analysis to examine the flowering records (1940 – 1970) of E. leucoxylon and Eucalyptus tricarpa, E. microcarpa and E. polyanthemos. We identified four subcomponents in each flowering series: characterised as a non-flowering phase, duration, annual and intensity cycles. A decreasing overall trend in flowering was identified by the MODWT <b>smoothed</b> <b>series.</b> Wavelet correlation found the same contemporaneous effects of climate on flow-ering for E. leucoxylon and Eucalyptus tricarpa, and for E. microcarpa and E. polyanthemos. Wavelet cross-correlation analysis identified the cyclical influence of temperature and rainfall on peak flowering intensity. For each species there are 6 months of the annual cycle in which any given climate variable positively influences flowering intensity and 6 months of negative influence. For all species, rainfall exerts a negative influence when temperature is positive. ...|$|E
3000|$|... 13 As {{quarterly}} {{changes for}} these variables are very volatile {{and thus more}} difficult to analyse {{whether there is a}} trend or not, we <b>smooth</b> the <b>series</b> by taking a simple moving average that includes the quarter itself, one quarter before, and one quarter after.|$|R
40|$|We obtain inner estimations, around special {{eigenvalue}}s, for the eigenvalue set of {{a properly}} nonlinear closed convex process. We also consider a differential inclusion {{associated with a}} general closed convex process and we construct <b>smooth</b> power <b>series</b> solutions of exponential type for some initial states...|$|R
5000|$|In {{a feature}} written for Anime News Network, Reed Nelson states that [...] "The {{transition}} Ōsumi's seinen-themed {{is not entirely}} <b>smooth,</b> but <b>series</b> is a fascinating watch for the curious, and can give new viewers {{a glimpse into the}} variety the franchise offers as a whole".|$|R
40|$|One of the {{traditional}} motivations for building quarterly macroeconometric models is the demand for quarterly forecasts. Models based on annual data conceal higher frequency information and are not considered suf ciently informative to policy makers. Two dif culties may encumber quarterly macroeconometric modelling: the lack of observations, i. e. variables not being observed at the quarterly frequency, and the seasonal pattern in the data. Many methods have been suggested to deal with missing observations. Most of them employ <b>smoothed</b> <b>series</b> of approximations for the missing observations and necessitate seasonal adjustment of all other series in the models as well. Seasonal adjustment is no longer beyond criticism; the current view is that seasonal information should be employed rather than ltered out. An alternative method when confronted with missing quarterly observations is to generate forecasts with an annual model, to disaggregate these annual series into quarterly observations, and to add a seasonal pattern if required. This paper investigates under which circumstances this approach is preferable to forecasting with a quarterly model (partly) based on approximations of variables with missing observations...|$|E
40|$|Summary. The common {{practice}} of collecting long-term {{data in the}} form of calendar monthly and annual means embodies several apparently undesirable features. In an effort t o improve such procedures, a sequence of high quality smoothing filters is proposed, producing from an initial data series at hourly or similar time interval a sequence of three <b>smoothed</b> <b>series</b> at: (1) 2 per day, (2) 12 per year and (3) one per year respectively. The filters are applied for demonstration to a 66 year series of sea-levels from Newlyn, Cornwall, with associated barometric and temperature data. Series (1) is used to provide an unusually smooth profile of the sub-tidal energy spectrum, which shows an unexpected dichotomy above and below 0. 13 cycles day-’, partially explained in terms of coherence with the pressure series. The same series is analysed at the frequencies of the lunar tidal components, with improved noise: signal ratio when barometric pressure is added to the sea-level data with appropriate scaling. Both Mf and Mm compo-nents are found to be close to equilibrium...|$|E
40|$|This paper compares {{alternative}} {{procedures to}} mitigate the procyclicality of the new risk-sensitive bank capital regulation (Basel II). We estimate {{a model of the}} probabilities of default (PDs) of Spanish firms during the period 1987 - 2008, and use the estimated PDs to compute the corresponding series of Basel II capital requirements per unit of loans. These requirements move significantly along the business cycle, ranging from 7. 6 % (in 2006) to 11. 9 % (in 1993). The comparison of the different procedures is based on the criterion of minimizing the root mean square deviations of each <b>smoothed</b> <b>series</b> with respect to the Hodrick-Prescott trend of the original series. The results show that the best procedures are either to smooth the inputs of the Basel II formula by using through-the-cycle PDs or to smooth the output with a multiplier based on GDP growth. Our discussion concludes that the latter is better in terms of simplicity, transparency, and consistency with banks ’ risk pricing and risk management systems. For the portfolio of Spanish commercial and industrial loans and a 45 % loss given default (LGD), the multiplier would amount to a 6. 5 % surcharge for each standard deviation in GDP growth. The surcharge would be significantly higher with cyclically-varying LGDs...|$|E
40|$|This paper {{presents}} {{an analysis of}} the time series of yearly mean temperatures and yearly precipitation during 1923 – 1988 for Veracruz (19 º 12 ´ N, 96 º 8 ´ W, 16 masl), Xalapa (19 º 32 ´ N, 96 º 55 ´ W, 1 420 masl) and Las Vigas (19 º 38 ´ N, 97 º 5 ´ W, 2 421 masl). In order to weigh in preliminary form the impact of the increase of atmospheric CO 2 concentrations, the presence of El Niño or Southern Oscilation (ENSO), the tropical atmospheric disturbances in the Pacific and the Atlantic Oceans, and the urbanization on both the rainfal and the temperature, we use statistical techniques as <b>smoothing</b> <b>series,</b> box plots, simple correlation and canonical correlation analysis. The conclusions carry out conjetures for future research...|$|R
30|$|Smoothing. Apply the kernel {{regression}} estimator of Nadaraya and Watson [34] {{to the user}} temporal data to obtain a <b>smoothed</b> time <b>series</b> t. The <b>smoothing</b> process gets rid of very sharp and punctual fluctuations, which are very frequent in human activity time series. Examples of raw curves compared to their smoothed versions are shown in Figure  4 (bottom).|$|R
40|$|We generalize Zagier's work on regularized {{integral}} to the adelic setting and to <b>smooth</b> Eisenstein <b>series,</b> guided by a spectral analytic viewpoint. We prove a singular case of the triple product formula for non rapidly decreasing functions constructed from the theory of regularized integral. The formula will be applied to a subconvexity problem. Comment: arXiv admin note: text overlap with arXiv: 1604. 0855...|$|R
40|$|Abstract: In certain {{rivers that}} drain very flat terrains in coastal areas, the streamflow series {{observed}} at a flow-gauging station may {{come under the}} direct influence of the backwater effects of tides. The phenomena may be negligible under conditions of high flows but can be critical under some extreme low-flow conditions. The errors in low flow estimation are large if a proper de-noising is not implemented to remove {{the effects of the}} tidal effects. Scrutinizing the hydrologic time series using a standard time-frequency do-main based Fourier transform methodology cannot resolve conclusively the sources of the noise. However, a new perspective can be obtained by using a wavelet transformation to analyze the time series in the time-scale domain. By using this approach, a case study involving a streamflow series observed at Kapit, Sarawak, Malaysia yielded conclusive evidence of the influence of tides at the flow-gauging site during the low flow period. Upon confirma-tion that the noise is indeed of tidal origin, the observed water level series was subjected to an appropriate wavelet-based de-noising procedure to de-rive a <b>smoothed</b> <b>series.</b> Then, together with an established rating curve, a de-noised discharge series could also be approximated. Low-flow quantiles were subsequently derived by fitting a suitable frequency distribution to the annual minimum series abstracted from the de-noised discharge series. The methodology presented illustrates the potential of using wavelet analysis methods in solving other similar problems...|$|E
40|$|This paper {{estimates}} the primary {{structural budget balance}} (eliminating the effects due to cyclical fluctuations in GDP and oil price) for the central government and public administrations from 1997 to {{the second quarter of}} 2010. Some adjustments were made on the IMF methodology to account for singularities of the Brazilian case. These procedures demand estimates for the trend GDP, as well as elasticities of the main components of the government budget with respect to GDP and oil price. To obtain a <b>smoothed</b> <b>series</b> for GDP (and oil price), we used the Hodrick-Prescott filter. To estimate the elasticities, we applied various techniques allowing for nonlinearities in the data. We did not perform any adjustment for the expenditure series, given they seem to have exhibited a procyclical behavior. In a nutshell, the statistics allow us to distinguish two big cycles of fiscal policy in the period under analysis: one of fiscal contraction, which was extended until the end of 2005; and another of fiscal expansion, from 2006 on. The calculations performed with the most conservative elasticities also suggest that the fiscal expansion in 2009, during the crises, was of approximately, 7 % of GDP in the federal level and 0, 2 - 0, 4 % of GDP in the public administrations as a whole. The partial results for 2010 indicate an impulse in the public administrations expenditure (from 0, 2 - 0, 4 to 0, 4 - 0, 8 % of GDP). Nonetheless, the regional governments seemed now to have played a more central role, probably due to the proximity of the elections...|$|E
40|$|The {{series of}} mean daily {{temperature}} of air recorded {{over a period of}} 215 years is used for analysing the dimensionality and the predictability of the atmospheric system. The total number of data points of the series is 78527. Other 37 versions of the original series are generated, including ``seasonally adjusted'' data, a <b>smoothed</b> <b>series,</b> series without annual course, etc. Modified methods of Grassberger and Procaccia are applied. A procedure for selection of the ``meaningful'' scaling region is proposed. Several scaling regions are revealed in the ln C(r) versus ln r diagram. The first one in the range of larger ln r has a gradual slope and the second one in the range of intermediate ln r has a fast slope. Other two regions are settled in the range of small ln r. The results lead us to claim that the series arises from the activity of at least two subsystems. The first subsystem is low-dimensional (d_f= 1. 6) and it possesses the potential predictability of several weeks. We suggest that this subsystem is connected with seasonal variability of weather. The second subsystem is high-dimensional (d_f> 17) and its error-doubling time is about 4 - 7 days. It is found that the predictability differs in dependence on season. The predictability time for summer, winter and the entire year (T_ 2 approx. 4. 7 days) is longer than for transition-seasons (T_ 2 approx. 4. 0 days for spring, T_ 2 approx. 3. 6 days for autumn). The role of random noise and the number of data points are discussed. It is shown that a 15 -year-long daily temperature series is not sufficient for reliable estimations based on Grassberger and Procaccia algorithms. Comment: 27 pages (LaTex version 2. 09) and 15 figures as. ps files, e-mail: ar@kamet. troja. mff. cuni. c...|$|E
30|$|The subtle {{inflation}} {{followed by}} rapid deflation recorded during each event {{seems to be}} associated with a causal process associated with each sub-Plinian eruption. However, it is unclear whether this hypothesis is plausible, as similar-sized strain variations that are not associated with volcanic processes may have also been measured. Further {{research is needed to determine}} whether similar strain changes are only recorded before the three sub-Plinian eruptions that form the focus of this study. We investigated this possibility by analyzing the rate of variation in the E 1 component strain. To obtain the strain rate values, the derivative of the <b>smoothed</b> time <b>series</b> trend was calculated. The <b>smoothed</b> time <b>series</b> trend was obtained following Tamura et al. (1991), with ABIC used to determine the optimum parameter that controls the smoothness of the trend. Because of the smoothing process, phenomena with time scales less than 30 minutes are ignored; however, it does not affect the following discussion.|$|R
30|$|Baxter-King {{band pass filter}} is {{a method}} of <b>smoothing</b> the time <b>series,</b> which is a {{modification}} of the Hodrick–Prescott filter that provides wider opportunities for removing cyclical component from a time series.|$|R
50|$|The Prandtl-Meyer {{expansion}} {{can be seen}} as {{the physical}} explanation of the operation of the Laval nozzle. The contour of the nozzle creates a <b>smooth</b> and continuous <b>series</b> of Prandtl-Meyer expansion waves.|$|R
