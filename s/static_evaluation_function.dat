26|6725|Public
50|$|An {{evaluation}} function, {{also known}} as a heuristic evaluation function or <b>static</b> <b>evaluation</b> <b>function,</b> is a function used by game-playing programs to estimate the value or goodness of a position in the minimax and related algorithms. The evaluation function is typically designed to prioritize speed over accuracy; the function looks only at the current position and does not explore possible moves (therefore static).|$|E
50|$|There {{are many}} other search methods, or metaheuristics, which are {{designed}} {{to take advantage of}} various kinds of partial knowledge one may have about the solution. Heuristics {{can also be used to}} make an early cutoff of parts of the search. One example of this is the minimax principle for searching game trees, that eliminates many subtrees at an early stage in the search. In certain fields, such as language parsing, techniques such as chart parsing can exploit constraints in the problem to reduce an exponential complexity problem into a polynomial complexity problem. In many cases, such as in Constraint Satisfaction Problems, one can dramatically reduce the search space by means of Constraint propagation, that is efficiently implemented in Constraint programming languages.The search space for problems can also be reduced by replacing the full problem with a simplified version. For example, in computer chess, rather than computing the full minimax tree of all possible moves for the remainder of the game, a more limited tree of minimax possibilities is computed, with the tree being pruned at a certain number of moves, and the remainder of the tree being approximated by a <b>static</b> <b>evaluation</b> <b>function.</b>|$|E
40|$|Programming {{computers}} to play board games against human players {{has long been}} used as a measure for the development of artificial intelligence. The standard approach for computer game playing is to search for the best move from a given game state by using minimax search with <b>static</b> <b>evaluation</b> <b>function.</b> The <b>static</b> <b>evaluation</b> <b>function</b> is critical to the game playing performance but its design often relies on human expert players. This paper discusses how temporal differences (TD) learning can be used to construct a <b>static</b> <b>evaluation</b> <b>function</b> through self-playing and evaluates the effects for various parameter settings. The game of Kalah, a non-chance game of moderate complexity, is chosen as a testbed. The empirical result shows that TD learning is particularly promising for constructing a good evaluation function for the end games and can substantially improve the overall game playing performance in learning the entire game. DOI:  10. 18495 /comengapp. 21. 17518...|$|E
40|$|We explore an {{application}} to the game of Go of a reinforcement learning approach based on a linear <b>evaluation</b> <b>function</b> and large numbers of binary features. This strategy has proved effective in game playing programs and other reinforcement learning applications. We apply this strategy to Go by creating over a million features based on templates for small fragments of the board, and then use temporal difference learning and self-play. This method identifies hundreds of low level shapes with recognisable significance to expert Go players, and provides quantitive estimates of their values. We analyse the relative contributions to performance of templates of different types and sizes. Our results show that small, translation-invariant templates are surprisingly effective. We assess the performance of our program by playing against the Average Liberty Player {{and a variety of}} computer opponents on the 9 × 9 Computer Go Server. Our linear <b>evaluation</b> <b>function</b> appears to outperform all other <b>static</b> <b>evaluation</b> <b>functions</b> that do not incorporate substantial domain knowledge. ...|$|R
40|$|In {{many cases}} {{programs}} length's increase (known as "bloat", "fluff " and increasing "structural complexity") during artificial evolution. We show bloat is not specific to genetic programming and suggest it {{is inherent in}} search techniques with discrete variable length representations using simple <b>static</b> <b>evaluation</b> <b>functions.</b> We investigate the bloating characteristics of three nonpopulation and one population based search techniques using a novel mutation operator. An artificial ant following the Santa Fe trail problem is solved by simulated annealing, hill climbing, strict hill climbing and population based search using two variants of the the new subtree based mutation operator. As predicted bloat is observed when using unbiased mutation and is absent in simulated annealing and both hill climbers when using the length neutral mutation however bloat occurs with both mutations when using a population. We conclude {{that there are two}} causes of bloat. 1 Introduction In earlier work [Lan [...] ...|$|R
40|$|We propose ff-fi-evaluation {{functions}} {{that can be}} used in game-playing programs as a substitute for the traditional <b>static</b> <b>evaluation</b> <b>functions</b> without loss of functionality. The main advantage of an ff-fi-evaluation function is that it can be implemented with a much lower time complexity than the traditional counterpart and so provides a significant speedup for the evaluation of any game position which eventually results in better play. We describe an implementation of the ff-fi-evaluation function using a modification of the classical classification and regression trees and show that a typical call to this function involves the computation of only a small subset of all features that may be used to describe a game position. We show that an iterative bootstrap process can be used to learn ff-fi-evaluation functions efficiently and describe some of the experience we made with this new approach applied to a game called malawi. 1 Introduction Game playing programs especially those for tw [...] ...|$|R
40|$|Arimaa is a {{strategic}} board game for two players. It was designed {{with the aim}} {{that it will be}} hard to create a computer program that could defeat the best human players. In this thesis, we focus on the design of the <b>static</b> <b>evaluation</b> <b>function</b> for Arimaa. The purpose of a <b>static</b> <b>evaluation</b> <b>function</b> is to determine which player is leading in a given position and how significant the lead is. We have divided the problem into a few parts, which were solved separately. We paid most attention to the efficient recognition of important patterns on the board, such as goal threats. The basic element of the proposed evaluation function is mobility. For each piece, the number of steps that the piece would need to get to other places on the board is estimated. We also examined machine learning. We developed a new algorithm for learning a <b>static</b> <b>evaluation</b> <b>function</b> from expert games. An implementation of an Arimaa playing program, which demonstrates the proposed methods, is part of the thesis. Powered by TCPDF (www. tcpdf. org...|$|E
40|$|Evolutionary {{computation}} (EC) is the sub-discipline {{of artificial}} intelligence that iteratively derives solutions using techniques from genetics. In this work, {{we present a}} genetic algorithm that evolves a heuristic <b>static</b> <b>evaluation</b> <b>function</b> (SEF) function {{to be used in}} a real-time search navigation scheme of an autonomous agent. This coupling of algorithmic techniques (GAs with real time search by autonomous agents) makes for interesting formalistic and implementation challenges. Genetic evolution implies the need for a fitness function to guide a convergence in the solution being created. Thus, as part of this work, we present a fitness function that dictates the efficacy of a generated <b>static</b> <b>evaluation</b> <b>function.</b> In this work, we present algorithmic and formalistic designs, implementation details, and performance results of this multi-layered software endeavor...|$|E
40|$|Heuristic search {{effectiveness}} depends directly {{upon the}} quality of heuristic evaluations of states in the search space. We show why ordinal correlation is relevant to heuristic search, present a metric for assessing {{the quality of}} a <b>static</b> <b>evaluation</b> <b>function,</b> and apply it to learn feature weights for a computer chess program...|$|E
40|$|Abstract- In {{many cases}} {{programs}} length’s increase (known as “bloat”, ‘‘fluff ” and increasing “structural complexity”) during artificial evolution. We show bloat is not specific to genetic programming and suggest it {{is inherent in}} search techniques with discrete variable length representations using simple <b>static</b> <b>evaluation</b> <b>functions.</b> We investigate the bloating characteristics of three non-population and one population based search techniques using a novel mutation operator. An artificial ant following the Santa Fe trail problem is solved by simulated annealing, hill climbing, strict hill climbing and population based search using two variants of the the new subtree based mutation operator. As predicted bloat is observed when using unbiased mutation and is absent in simulated annealing and both hill climbers when using the length neutral mutation however bloat occurs with both mutations when using a population. We conclude {{that there are two}} causes of bloat 1) search operators with no length bias tend to sample bigger trees and 2) competition within populations favours longer programs as they can usually reproduce more accurately. I...|$|R
40|$|It {{is known}} that bounds on the minimax values of nodes in a game tree {{can be used to}} reduce the compu-tational {{complexity}} of minimax search for two-player games. We describe a very simple method to esti-mate bounds on the minimax values of interior nodes of a game tree, and use the bounds to improve min-imax search. The new algorithm, called forward es-timation, does not require additional domain knowl-edge other than a <b>static</b> node <b>evaluation</b> <b>function,</b> and has small constant overhead per node expansion. We also propose a variation of forward estimation, which provides a tradeoff between computational complexity and decision quality. Our experimental results show that forward estimation outperforms alpha-beta prun-ing on random game trees and the game of Othello. 1...|$|R
40|$|This paper {{discusses}} {{several issues}} in applying genetic programming to image classification problems in geoscience and remote sensing. In particular, this paper examines the role in using dynamic and <b>static</b> fitness <b>evaluation</b> <b>functions.</b> This paper also examines {{a few of}} the aspects in human-computer interactions that facilitate computer-assisted learning and problem solving (i. e., scaffolding) for our system. We describe a possible means for visualizing and summarizing a solution space without having to resort to an exhaustive search of individuals. age for evaluation becomes daunting; the cost of using just one image as a fitness set becomes prohibitive. Of course, one could reduce the amount of computation involved by reducing the size of the fitness set from whole images to subimage samples. Although the idea of using subimage samples may seem straightforward, in practice, it is not. One often encounters situations where there exists incomplete knowledge about the var [...] ...|$|R
40|$|This work {{studies the}} {{application}} of genetic algorithms to the domain of game playing, emphasising on learning a <b>static</b> <b>evaluation</b> <b>function.</b> Learning involves experience generation, hypothesis generation and hypothesis evaluation. Most learning systems use preclassi ed examples to guide the search in the hypothesis space and to evaluate current hypotheses. In game learning, it is very di cult to get classi ed examples. Genetic Algorithms provide an alternative approach. Competing hypotheses are evaluated by tournaments. New hypotheses are generated by genetic operators. We introduce a new framework for applying genetic algorithms to game evaluation-function learning. The evaluation function is learned by its derivatives rather than learning the function itself. We introduce a new genetic operator, called derivative crossover, that accelerates the search for <b>static</b> <b>evaluation</b> <b>function.</b> The operator performs cross-over on the derivatives of the chromosomes. We have demonstrated experimentally {{the advantage of the}} derivative crossover for learning an evaluation function...|$|E
40|$|Fail High Reductions (FHR) is a {{new method}} to guide the search in game trees in a very {{selective}} manner. The main {{idea is that the}} search algorithm should not spend too much effort searching subtrees, which look too good {{to become part of the}} principal variation even at their roots. More precisely, a fail high node is a node v with a search window [ff; fi] at which a <b>static</b> <b>evaluation</b> <b>function</b> e produces a cutoff. The FHR-algorithm reduces the search depths at these fail high nodes thus searching their subtrees with less effort. FHR is domain independent in the sense that it only uses a <b>static</b> <b>evaluation</b> <b>function,</b> which always is assumed to exist, and the search windows to guide the search. In this paper we describe the incorporation of FHR in our chess program Zugzwang. Three different tests have been conducted to compare Zugzwang with and without FHR: First, we compare the results obtained from running the algorithm on the positions of the WinAtChess test suite. Second, we look at th [...] ...|$|E
40|$|We {{develop a}} {{real-time}} algorithm {{based on a}} Monte-Carlo game tree search for solving a quantified constraint satisfaction problem (QCSP), which is a CSP where some variables are universally quantified. A universally quantified variable represents a choice of nature or an adversary. The goal of a QCSP {{is to make a}} robust plan against an adversary. However, obtaining a complete plan off-line is intractable when the size of the problem becomes large. Thus, we need to develop a realtime algorithm that sequentially selects a promising value at each deadline. Such a problem has been considered in the field of game tree search. In a standard game tree search algorithm, developing a good <b>static</b> <b>evaluation</b> <b>function</b> is crucial. However, developing a good <b>static</b> <b>evaluation</b> <b>function</b> for a QCSP is very difficult since it must estimate the possibility that a partially assigned QCSP is solvable. Thus, we apply a Monte-Carlo game tree search technique called UCT. However, the simple application of the UCT algorithm does not work since the player and the adversary are asymmetric, i. e., finding a game sequence where the player wins is very rare. We overcome this difficulty by introducing constraint propagation techniques. We experimentally compare the winning probability of our UCT-based algorithm and the state-of-the-art alpha-beta search algorithm. Our results show that our algorithm outperforms the state-of-the-art algorithm in large-scale problems...|$|E
40|$|AbstractMinimaxing {{has been}} very {{successful}} in game-playing practice, although a complete explanation of why {{it has been that}} successful has not yet been given. In particular, it has not been shown why it should be useful—as it is in practice—to use multivalued <b>evaluation</b> <b>functions.</b> Such functions have many distinct values as their result and can discriminate between positions according to the heuristic knowledge represented in these values. In this paper, we modify a basic pathological model by adding two assumptions regarding multivalued <b>evaluation</b> <b>functions.</b> These assumptions, non-uniformity of error distribution and dependency of heuristic values, directly relate to the properties of multivalued <b>evaluation</b> <b>functions</b> as used in practice. Simulation studies of our multivalued model have exhibited sharp error reductions for deeper searches using minimaxing. This behavior corresponds to observations in practice. The error reductions are primarily due to the improved evaluation quality as search depth increases. This phenomenon of lower probability of <b>static</b> <b>evaluation</b> errors with increasing search depth is revealed through our model, although the same <b>evaluation</b> <b>function</b> is used {{at all levels of the}} tree, and although its general error probability is independent of the depth. Essentially, with increasing search depth, the <b>evaluation</b> <b>function</b> is more frequently used on such positions which can be more reliably evaluated by a multivalued function with the assumed properties. This effect together with the ability to discriminate between positions of different “goodness” leads to the benefits of using multivalued <b>evaluation</b> <b>functions</b> (of appropriate granularity) for minimaxing...|$|R
40|$|This {{combination}} thesis within Computer Science and Communication utilizes {{the board}} game Checkers {{as a case}} to reveal the basic elements for the generally acknowledged success using artifi-cial intelligence in board games and to which extent these experiences and methods can be general-ized to include aspects of human-like intelligence. Artificial intelligence at this level {{is referred to as}} strong artificial intelligence. The theory behind artificial intelligence for board games is outlined, in-cluding context issues such as the frame problem, Hamlet’s problem, the prediction problem and the commonsense-knowledge problem, where in particular the frame problem poses difficulties for de-veloping artificial intelligence beyond rule governed cases. A so called MinMax algorithm implemen-tation has been optimized by use of methods to reduce calculation time known as Alpha-Beta prun-ing, Move sorting and Hash table. MinMax algorithm has, due to its algorithmic approach strategic weaknesses leading to unsuitable behavior. This has led to experiments with a paradigm hybrid where the <b>evaluation</b> <b>function</b> {{has been replaced by a}} neural network in an attempt to create a high level stra-tegic artificial player and to investigate its potential for approaching strong artificial intelligence. The neural network has been implemented using temporal-difference method, by which experience from earlier draws is part of incremental learning. Various types of training passes for neural net-works and tests of several artificial players at strategically different levels have been conducted. As input methods for neural networks both board-mapping and feature-mapping has been implemented and tested. With a phenomenological approach test persons have been observed while playing Check-ers. The objective of the empirical study was to unravel how human players act and interact while playing Checkers and how this would relate to abilities of an artificial player as described above. We discuss views and statements from selected, renowned artificial intelligence researchers on the phi-losophical perspectives on strong artificial intelligence. Based on the case study and the discussion on philosophical perspectives we conclude that current methods used in artificial players for Checkers does not have the potential for including intelligent behavior and that current level of artificial intelligence for board games is far from strong artificial intelligence. From the artificial player implementations and tests we conclude that the combination of the two paradigms MinMax algorithm and neural networks shows good performance and adds to the algorithm ability to adapt to opponent behavior and to learn from own faults. Thus neural networks do compete favorably with <b>static</b> <b>evaluation</b> <b>functions</b> and the combination is considered a suitable ex-tension of the MinMax algorithm. However, the combination cannot bring the artificial player of Checkers closer to strong artificial intelligence. We can conclude that an artificial player at the level investigated cannot participate in the psychological game which plays an important role when humans play. Along with the philosophical discussion we conclude that an artificial player can be trained to play Checkers at high level but not intelligent behavior as board games is strictly rule governed with-out any unforeseen events. Artificial intelligence for board games cannot be used directly to create intelligent behavior. The artificial player does not think, nor is he understanding or being conscious, when selecting his move...|$|R
40|$|In {{the paper}} {{arguments}} are given why {{the concept of}} <b>static</b> <b>evaluation</b> {{has the potential to}} be a useful extension to Monte Carlo tree search. A new concept of modeling <b>static</b> <b>evaluation</b> through a dynamical system is introduced and strengths and weaknesses are discussed. The general suitability of this approach is demonstrated. Comment: IEEE Transactions on Computational Intelligence and AI in Games, vol 3 (2011), no...|$|R
40|$|We {{present a}} very simple {{selective}} search algorithm for two-player games. It always expands next the frontier node that determines the minimax value of the root. The algorithm requires no information other than a <b>static</b> <b>evaluation</b> <b>function,</b> and its time overhead per node {{is similar to that}} of alpha-beta minimax. We also present an implementation of the algorithm that reduces its space complexity from exponential to lin-ear in the search depth, at the cost of increased time complexity. In the game of Othello, using the evalu-ation function from BiIl (Lee & Mahajan 1990), best-first minimax outplays alpha-beta at moderate depths. A hybrid best-first extension algorithm, which com-bines alpha-beta and best-first minimax, performs sig...|$|E
40|$|Traditional {{evolutionary}} optimization algorithms {{assume a}} <b>static</b> <b>evaluation</b> <b>function,</b> {{according to which}} solutions are evolved. Incremental evolution is an approach through which a dynamic evaluation function is scaled over {{time in order to}} improve the performance of evolutionary optimization. In this paper, we present empirical results that demonstrate the e ectiveness of this approach for genetic programming. Using two domains, a two-agent pursuit-evasion game and the Tracker [6] trail-following task, we demonstrate that incremental evolution is most successful when applied near the beginning of an evolutionary run. We alsoshowthat incremental evolution can be successful when the intermediate evaluation functions are more di cult than the target evaluation function, as well as when they are easier than the target function. 1...|$|E
40|$|A common {{approach}} to game playing in Artificial Intelligence {{involves the use}} of the Minimax algorithm and a <b>static</b> <b>evaluation</b> <b>function.</b> In this work I investigated using neural networks to replace hand-tuned static evaluation functions. Networks were also trained to evaluate board po-sitions to greater depth levels using Minimax. The networks begin as a random networks and by playing against a random player, the networks are able to match the teacher’s performance. This work provides evidence that board representation affects the ability of the network to evaluate Konane board positions. Networks that are taught to output a real-value board evaluation outperform those taught to directly compare two boards after a considerable amount of training. However, the latter networks show more consistent behavior during training, and quickly learn to play at a reasonably good skill level. ...|$|E
40|$|The {{problem of}} {{evolving}} an artificial ant {{to follow the}} Santa Fe trail is used to demonstrate the well known genetic programming feature of growth in solution length. Known variously as "bloat", "redundancy", "introns", "fluff ", "Structural Complexity" with antonyms "parsimony", "Minimum Description Length" (MDL) and "Occam's razor". Comparison with runs with and without fitness selection pressure shows the tendency for solutions to grow in size is caused by fitness based selection. We argue that such growth is inherent in using a fixed <b>evaluation</b> <b>function</b> with a discrete but variable length representation. Since with simple <b>static</b> <b>evaluation</b> search converges to mainly finding trial solutions with the same fitness as existing trial solutions. In general variable length allows many more long representations of a given solution than short ones of the same solution. Thus with an unbiased random search we expect longer representations to occur more often and so representation length tends [...] ...|$|R
40|$|The {{difficulty}} {{to write}} successful 19 x 19 go programs lies {{not only in}} the combinatorial complexity of go but also in the complexity of designing a good <b>evaluation</b> <b>function</b> containing a lot of knowledge. Leaving these obstacles aside, this paper defines very-little-knowledge <b>evaluation</b> <b>functions</b> used by programs playing on very small boards. The <b>evaluation</b> <b>functions</b> are based on two mathematical tools, distance and dimension, and not on domaindependent knowledge. After a qualitative assessment of each <b>evaluation</b> <b>function,</b> we built several programs playing on 4 x 4 boards by using tree search associated with these <b>evaluation</b> <b>functions.</b> We set up an experiment to select the best programs and identify the relevant features of these <b>evaluation</b> <b>functions.</b> Thanks to the results obtained by these very-little-knowledge-based programs, we can foresee the usefulness of each <b>evaluation</b> <b>function...</b>|$|R
40|$|The C++ {{language}} o#ers a two layer evaluation model. Thus, it {{is possible}} to evaluate a program in two steps: the so-called <b>static</b> and dynamic <b>evaluations.</b> <b>Static</b> <b>evaluation</b> is used for reducing the amount of work done at execution-time. Programs executed statically (called metaprograms) are written in C++ through an intensive use of template classes...|$|R
40|$|Since 1967 {{there has}} again been great {{interest}} in chess programming. This paper demonstrates that the structure of today's most successful programs cannot be extended to play Master level chess. Certain basic requirements of a Master player's performance are shown to be outside the performance limits to which a program of this type could be extended. The paper also examines a basic weakness in the tree-searching model approach when applied to situations that cannot be searched to completion. This is the Horizon Effect, which causes unpredictable evaluation errors due to an interaction between the <b>static</b> <b>evaluation</b> <b>function</b> and the rules for search termination. The outline of a model of chess playing that avoids the Horizon Effect and appears extendable to play Master level chess is presented, together with some results already achieved...|$|E
40|$|We {{present a}} very simple {{selective}} minimax search algorithm for two-player gaines. It ahvays expands next the frontier node {{at the end of}} the principal variation, or current best line of play, which is the node that determines the minimax value of the root. The algorithm requires no information other than a <b>static</b> <b>evaluation</b> <b>function,</b> and its time overhead per node is similar to that of alpha-beta minimax. On random game trees, our algorithm outperforms an efficient implementation of alpha-beta, giving both the same amount of computation. In the game of Othello, using the evaluation function from Bill, the world’s best program, best-first minimax also outplays alpha-beta. We present an implementation of the algorithm that reduces its space complexity from exponential to linear in the search depth, at the cost of increased time complexity. Finally, we present a hybrid best-first extension algorithin that combines alpha-beta and best-first minimax, and performs significantly better than either pure algorithm in both domains. ...|$|E
40|$|Random minimaxing {{introduced}} in [BEAL 94] {{is the process}} of using a random <b>static</b> <b>evaluation</b> <b>function</b> for scoring the leaf nodes of a full-width game tree and then computing the best move using the standard minimax procedure. The experiments carried out by Beal and Smith [BEAL 94] using random minimaxing in chess showed that the strength of play increases as the depth of the lookahead is increased. We investigate random minimaxing from a combinatorial point of view in order to obtain a theoretical justification for Beal and Smith's experiments. In particular we show that, with respect to chess, random minimaxing with the depth of lookahead equal to two is "stronger" than random minimaxing with the depth of lookahead equal to one, under the assumption that the more a move made by the first player restricts the second player's choice of moves (i. e. the second player's mobility) the better that move is. We conjecture that these results can be generalized for depths of lookahead greater than [...] ...|$|E
40|$|The {{probabilistic}} concept formation general {{problem in}} dealing with mixed-data scale environments {{is due to the}} use of different <b>evaluation</b> <b>function</b> for each attribute type. We claim that different behaviors for discrete and continuous <b>evaluation</b> <b>functions</b> are due to an unbalanced contribution for each attribute-type <b>evaluation</b> <b>function</b> inside the main <b>evaluation</b> <b>function.</b> This paper describes an approach based on the difference between the predictability gain for each attribute type. Our approach presents a way to work around for the unbalanced contribution for each attribute-type <b>evaluation</b> <b>functions.</b> Experiments using our approach have shown higher quality in terms of inference ability...|$|R
40|$|ABSTRACT: Conventional robotic {{behaviors}} are directly programmed {{depending on the}} designer’s personal experience. With this method, {{it is difficult to}} create new behavioral designs. In this study, the authors aim to establish a new method of designing robotic behaviors by operating underlying <b>evaluation</b> <b>functions</b> in mathematical forms that can generates behaviors in computer calculations. The characteristics of the method lie in the process of obtaining the <b>evaluation</b> <b>function</b> from a model behavior, which is opposite to the conventional process in which a predefined <b>evaluation</b> <b>function</b> generates the optimum behavior. This method is composed of two stages: (1) acquiring the <b>evaluation</b> <b>function</b> and (2) synthesizing the acquired <b>evaluation</b> <b>functions</b> and creating a new behavioral pattern from them. Our simulation shows that some <b>evaluation</b> <b>functions</b> of generating {{behaviors are}} synthesized using this method...|$|R
40|$|Abstract. The {{problem of}} evolving, using mutation, an {{artificial}} ant {{to follow the}} Santa Fe trail is used to study the well known genetic programming feature of growth in solution length. Known variously as “bloat”, “fluff ” and increasing “structural complexity”, this is often {{described in terms of}} increasing “redundancy ” in the code caused by “introns”. Comparison between runs with and without fitness selection pressure, backed by Price’s Theorem, shows the tendency for solutions to grow in size is caused by fitness based selection. We argue that such growth is inherent in using a fixed <b>evaluation</b> <b>function</b> with a discrete but variable length representation. With simple <b>static</b> <b>evaluation</b> search converges to mainly finding trial solutions with the same fitness as existing trial solutions. In general variable length allows many more long representations of a given solution than short ones. Thus in search (without a length bias) we expect longer representations to occur more often and so representation length to tend to increase. I. e. fitness based selection leads to bloat...|$|R
40|$|Abstract- Coevolutionary {{techniques}} {{in combination with}} particle swarm optimization algorithms and neural networks have shown to be very successful in finding strong game playing agents {{for a number of}} deterministic games. This paper investigates the applicability of a PSO coevolutionary approach to probabilistic games. For the purposes of this paper, a probabilistic variation of the tic-tac-toe game is used. Initially, the technique is applied to a simple deterministic game (tictac-toe), proving its effectiveness with such games. The technique is then applied to a probabilistic 4 x 4 x 4 tic-tactoe game, illustrating scalability to more complex, probabilistic games. The performance of the probabilistic game agent is compared against agents that move randomly. To determine how these game agents compete against strong non-random game playing agents, coevolved solutions are also compared against agents that utilize a strong hand-crafted <b>static</b> <b>evaluation</b> <b>function.</b> Particle swarm optimization parameters/topologies and neural network architectures are experimentally optimized for the probabilistic tic-tac-toe game. ...|$|E
40|$|The fffi-{{algorithm}} or the Negascout algorithm {{are known}} to search game trees efficiently. Many {{research has been done}} in the past to increase the speed of the search by adding heuristics. In this paper, we present a distributed algorithm for game tree search. It is based on the Negascout algorithm and a domain independent selective search technique called Fail High Reductions. The main idea of Fail High Reductions is to use the <b>static</b> <b>evaluation</b> <b>function</b> and compare their values with the results founds so far during the search. By the process of iteratively deepening the game tree search we obtain an algorithm that searches relevant lines of the tree deeper than the irrelevant ones when the running time is fixed. We show that the sequential version of this algorithm is superior to the sequential Negascout algorithm in practical applications. For the parallel algorithm we present efficiency measurements obtained on a Cray T 3 E with up to 512 processors. These measurements show that the d [...] ...|$|E
40|$|Random minimaxing, {{introduced}} by Beal and Smith [1], {{is the process}} of using a random <b>static</b> <b>evaluation</b> <b>function</b> for scoring the leaf nodes of a full width game tree and then computing the best move using the standard minimax procedure. The experiments carried out by Beal and Smith, using random minimaxing in Chess, showed that the strength of play increases as the depth of the lookahead is increased. We investigate random minimaxing from a combinatorial point of view in order {{to gain a better understanding}} of the utility of the minimax procedure and thus obtain a theoretical justification for the results of Beal and Smith's experiments. The concept of domination is central to our theory. Intuitively, a move by white dominates another move when this move gives less choice for black when it is black's turn to move, and subsequently more choice for white when it is white's turn to move. We view domination as a measure of mobility and show that when one move dominates another then its pro [...] ...|$|E
30|$|A {{numerical}} inversion {{test was}} conducted to assess how efficiently the proposed method can reproduce the original distribution of slip from noise-overlapped synthetic displacement data. For comparison, we present results obtained using three types of <b>evaluation</b> <b>functions.</b> First, we used an <b>evaluation</b> <b>function</b> that only included the L 2 smoothness regularization term (Eq.  2) called smoothness. Second, we used an <b>evaluation</b> <b>function</b> that only included the L 1 sparsity regularization term (Eq.  3), called sparsity. The third function is the proposed <b>evaluation</b> <b>function</b> that includes smoothness, discontinuity, and sparsity constraints (Eq.  4), called SDS constraints.|$|R
30|$|The {{results of}} <b>static</b> <b>evaluation</b> of the {{characteristics}} confirmed that the theoretical properties {{are consistent with the}} experimental results. The actuator is heated by atmospheric temperature and produces a displacement using both vapor pressure and thermal expansion of the working fluid.|$|R
40|$|Architecture {{evaluations}} play {{an important}} role in the development and evolution of software systems since they determine how adequate the architecture is for its intended usage. This paper summarizes our practical experience with using <b>static</b> architecture <b>evaluations</b> and gives an overview on when and how <b>static</b> architecture <b>evaluations</b> contribute to architecture development. In particular, we identify ten distinct purposes and needs for <b>static</b> architecture <b>evaluations</b> derived from practical experiences in a set of industrial and academic case studies. 1...|$|R
