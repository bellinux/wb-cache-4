4566|3096|Public
25|$|Digital signal {{processing}} helps to reduce noise and distinguish the <b>speech</b> <b>signal</b> from the overall spectrum of sounds which facilitates speech perception.|$|E
25|$|T. Abatzoglou and J. Mendel, Constrained total least squares, in Proc. IEEE Int. Conf. Acoust., <b>Speech,</b> <b>Signal</b> Process. (ICASSP’87), Apr. 1987, vol. 12, pp.1485–1488.|$|E
25|$|The sensory {{part of a}} neurocomputational {{model of}} speech {{processing}} starts with an acoustic signal of a speech item (acoustic <b>speech</b> <b>signal),</b> generates an auditory representation for that signal and activates a phonemic representations for that speech item.|$|E
40|$|Abstract. A new {{efficient}} {{code for}} <b>speech</b> <b>signals</b> is proposed. To represent <b>speech</b> <b>signals</b> with minimum redundancy we use independent component analysis to adapt features (basis vec-tors) that efficiently encode the <b>speech</b> <b>signals.</b> The learned basis vectors are sparsely distrib-uted and localized in both time and frequency. Time-frequency analysis of basis vectors shows the property similar with the critical bandwidth of human auditory system. Our {{results suggest that}} the obtained codes of <b>speech</b> <b>signals</b> are sparse and biologically plausible...|$|R
40|$|International audienceIn {{this paper}} a new {{geometrical}} approach for separating <b>speech</b> <b>signals</b> is presented. This {{approach can be}} directly applied to separate more than two <b>speech</b> <b>signals.</b> It is based on clustering the observation points,and then fitting aline(hyper-plane) ontoeach cluster. The algorithm quality is shown to be improved by using DCT coefficients of <b>speech</b> <b>signals,</b> as opposed to using speech samples...|$|R
40|$|Abstract. In {{this paper}} a new {{geometrical}} approach for separating <b>speech</b> <b>signals</b> is presented. This {{approach can be}} directly applied to separate more than two <b>speech</b> <b>signals.</b> It is based on clustering the observation points, and then fitting a line (hyper-plane) onto each cluster. The algorithm quality is shown to be improved by using DCT coefficients of <b>speech</b> <b>signals,</b> as opposed to using speech samples. ...|$|R
25|$|The {{activation}} pattern {{within the}} motor map determines the movement pattern of all model articulators (lips, tongue, velum, glottis) for a speech item. In {{order not to}} overload the model, no detailed modeling of the neuromuscular system is done. The Maeda articulatory speech synthesizer is used in order to generate articulator movements, which allows the generation of a time-varying vocal tract form and the generation of the acoustic <b>speech</b> <b>signal</b> for each particular speech item.|$|E
25|$|Although {{listeners}} perceive {{speech as}} {{a stream of}} discrete units (phonemes, syllables, and words), this linearity {{is difficult to see}} in the physical <b>speech</b> <b>signal</b> (see Figure 2 for an example). Speech sounds do not strictly follow one another, rather, they overlap. A speech sound is influenced by the ones that precede and the ones that follow. This influence can even be exerted at a distance of two or more segments (and across syllable- and word-boundaries).|$|E
25|$|One of {{the basic}} {{problems}} {{in the study of}} speech is {{how to deal with the}} noise in the <b>speech</b> <b>signal.</b> This is shown by the difficulty that computer speech recognition systems have with recognizing human speech. These programs can do well at recognizing speech when they have been trained on a specific speaker's voice, and under quiet conditions. However, these systems often do poorly in more realistic listening situations where humans can understand speech without difficulty.|$|E
40|$|This paper {{addresses}} {{the problem of}} reconstructing wideband <b>speech</b> <b>signals</b> from observed narrowband <b>speech</b> <b>signals.</b> The goal of this work {{is to improve the}} perceived quality of <b>speech</b> <b>signals</b> which have been transmitted through narrowband channels or degraded during acquisition. We describe a system, based on linear predictive coding, for estimating wideband speech from narrowband. This system employs both previously identified and novel techniques. Experimental results are provided in order to illustrate the system&# 8242;s ability to improve speech quality. Both objective and subjective criteria are used to evaluate the quality of the processed <b>speech</b> <b>signals.</b> </p...|$|R
40|$|The current paper {{discusses}} {{two approaches}} to enhanced speech in reverberation/noise: machine signal processing and human speech production. We reviewed the speech enhancement techniques, including steady-state suppression and compared the modulation spectra of <b>speech</b> <b>signals</b> {{before and after}} processing. We also introduced the Lombard-like effect of speech in reverberation, and compared the characteristics of <b>speech</b> <b>signals,</b> including the modulation spectra between <b>speech</b> <b>signals</b> uttered in quiet and reverberation. We found that the enhanced <b>speech</b> <b>signals</b> have distinct characteristics that yield higher speech intelligibility. Index Terms: speech enhancement, steady-state suppression, modulation spectrum, intelligibility of speech, reverberation, Lombard effect 1...|$|R
30|$|This paper {{demonstrates}} {{the potential of}} applying CS to <b>speech</b> <b>signals</b> especially voiced <b>speech</b> <b>signals.</b> From the viewpoint of long-term prediction, we analyze the sparsity of voiced <b>speech</b> <b>signals</b> and construct an adaptive sparsifying matrix. Moreover, a CS matrix called TBD matrix is constructed {{in terms of the}} spectral characteristics of voiced <b>speech</b> <b>signals.</b> Finally, the distribution of the projections is analyzed to carry out quantization. And the reconstruction performance of the adaptive quantization and nonadaptive quantization is studied. In addition, under the adaptive quantization, the reconstruction qualities of TBD matrix and the dense Gaussian matrix are empirically compared with different quantization bits. Therefore, we find that the TBD matrix and the adaptive quantization can effectively mitigate the quantization effect on reconstruction of <b>speech</b> <b>signals</b> in the framework of CS.|$|R
25|$|At {{four months}} infants still prefer infant-directed speech to adult-directed speech. Whereas 1-month-olds only exhibit this {{preference}} if the full <b>speech</b> <b>signal</b> is played to them, 4-month-old infants prefer infant-directed speech even when just the pitch contours are played. This shows that between 1 and 4 months of age, infants improve in tracking the suprasegmental {{information in the}} speech directed at them. By 4 months, finally, infants have learned which features {{they have to pay}} attention to at the suprasegmental level.|$|E
25|$|The fuzzy logical {{theory of}} speech {{perception}} developed by Dominic Massaro proposes that people remember speech sounds in a probabilistic, or graded, way. It suggests that people remember {{descriptions of the}} perceptual units of language, called prototypes. Within each prototype various features may combine. However, features are not just binary (true or false), there is a fuzzy value corresponding to how {{likely it is that}} a sound belongs to a particular speech category. Thus, when perceiving a <b>speech</b> <b>signal</b> our decision about what we actually hear is based on the relative goodness of the match between the stimulus information and values of particular prototypes. The final decision is based on multiple features or sources of information, even visual information (this explains the McGurk effect). Computer models of the fuzzy logical theory have been used to demonstrate that the theory's predictions of how speech sounds are categorized correspond to the behavior of human listeners.|$|E
25|$|For speech production, the ACT model {{starts with}} the {{activation}} of a phonemic representation of a speech item (phonemic map). In {{the case of a}} frequent syllable, a co-activation occurs {{at the level of the}} phonetic map, leading to a further co-activation of the intended sensory state at the level of the sensory state maps and to a co-activation of a motor plan state at the level of the motor plan map. In the case of an infrequent syllable, an attempt for a motor plan is generated by the motor planning module for that speech item by activating motor plans for phonetic similar speech items via the phonetic map (see Kröger et al. 2011). The motor plan or vocal tract action score comprises temporally overlapping vocal tract actions, which are programmed and subsequently executed by the motor programming, execution, and control module. This module gets real-time somatosensory feedback information for controlling the correct execution of the (intended) motor plan. Motor programing leads to activation pattern at the level lof the primary motor map and subsequently activates neuromuscular processing. Motoneuron activation patterns generate muscle forces and subsequently movement patterns of all model articulators (lips, tongue, velum, glottis). The Birkholz 3D articulatory synthesizer is used in order to generate the acoustic <b>speech</b> <b>signal.</b>|$|E
30|$|Enhancement {{of noise}} {{corrupted}} <b>speech</b> <b>signals</b> is a challenging task for speech processing systems {{to be deployed}} in real-world applications. In practice, <b>speech</b> <b>signals</b> are usually degraded by additive background noises, reverberation effects and <b>speech</b> <b>signals</b> from other speakers[1]. The primary goal of robust speech processing techniques is to improve intelligibility and quality of noise corrupted speech in perspective human listeners and modify the same and extract robust features that lead to improved performance for speech recognition systems.|$|R
3000|$|Source speech {{files were}} {{recorded}} in an anechoic chamber by four native Canadian French talkers and four native Canadian English talkers. Half of the talkers were male {{and the other half}} female. Clean <b>speech</b> <b>signals</b> were filtered using the modified intermediate reference system (MIRS) send filter according to ITU-T Recommendation P. 830 Annex D [37]. Degraded <b>speech</b> <b>signals</b> were further filtered using the MIRS receive filter. In both instances, <b>speech</b> <b>signals</b> were level adjusted to [...]...|$|R
30|$|Compressed sensing (CS) is {{a rising}} focus {{in recent years}} for its {{simultaneous}} sampling and compression of sparse <b>signals.</b> <b>Speech</b> <b>signals</b> can be considered approximately sparse or compressible in some domains for natural characteristics. Thus, it has great prospect to apply compressed sensing to <b>speech</b> <b>signals.</b> This paper is involved in three aspects. Firstly, the sparsity and sparsifying matrix for <b>speech</b> <b>signals</b> are analyzed. Simultaneously, a kind of adaptive sparsifying matrix based on the long-term prediction of voiced <b>speech</b> <b>signals</b> is constructed. Secondly, a CS matrix called two-block diagonal (TBD) matrix is constructed for <b>speech</b> <b>signals</b> based on the existing block diagonal matrix theory {{to find out that}} its performance is empirically superior to that of the dense Gaussian random matrix when the sparsifying matrix is the DCT basis. Finally, we consider the quantization effect on the projections. Two corollaries {{about the impact of the}} adaptive quantization and nonadaptive quantization on reconstruction performance with two different matrices, the TBD matrix and the dense Gaussian random matrix, are derived. We find that the adaptive quantization and the TBD matrix are two effective ways to mitigate the quantization effect on reconstruction of <b>speech</b> <b>signals</b> in the framework of CS.|$|R
2500|$|Because the <b>speech</b> <b>signal</b> is not linear, {{there is}} a problem of segmentation. It is {{difficult}} to delimit a stretch of <b>speech</b> <b>signal</b> as belonging to a single perceptual unit. As an example, the acoustic properties of the phoneme [...] will depend on the production of the following vowel (because of coarticulation).|$|E
2500|$|Speech {{processing}} [...] {{study of}} speech signals and the processing methods of these signals. The signals are usually processed {{in a digital}} representation, so speech processing {{can be regarded as}} a special case of digital signal processing, applied to <b>speech</b> <b>signal.</b> Aspects of speech processing includes the acquisition, manipulation, storage, transfer and output of digital speech signals.|$|E
2500|$|Adaptive {{directional}} microphones automatically {{vary the}} direction of maximum amplification or rejection (to reduce an interfering directional sound source). The direction of amplification or rejection is varied by the hearing aid processor. The processor attempts to provide maximum amplification in {{the direction of}} the desired <b>speech</b> <b>signal</b> source or rejection {{in the direction of}} the interfering signal source. Unless the user manually temporarily switches to a [...] "restaurant program, forward only mode" [...] adaptive directional microphones frequently amplify the speech of other talkers in a cocktail party type environments, such as restaurants or coffee shops. The presence of multiple speech signals makes it difficult for the processor to correctly select the desired <b>speech</b> <b>signal.</b> Another disadvantage is that some noises often contain characteristics similar to speech, making it difficult for the hearing aid processor to distinguish the speech from the noise. Despite the disadvantages, adaptive directional microphones can provide improved speech recognition in noise ...|$|E
40|$|A {{fast and}} robust three-level binary higher order {{statistics}} (HOS) based algorithm for simultaneous voiced/unvoiced detection and pitch estimation of <b>speech</b> <b>signals</b> in coloured noise environments with low SNR is presented. The {{use of the}} three-level binary <b>speech</b> <b>signals</b> dramatically reduces the computational effort required in evaluating the higher order cumulants. The superior performance of the new algorithm over the conventional autocorrelation method using real <b>speech</b> <b>signals</b> is demonstrated. The algorithm can easily be implemented in digital hardware using simple combinatorial logic...|$|R
40|$|In this {{contribution}} objective {{measures for}} quality assessment of <b>speech</b> <b>signals</b> are evaluated for listeningroom compensation algorithms. Dereverberation of <b>speech</b> <b>signals</b> {{by means of}} equalization of the room impulse response and reverberation suppression has been an active research topic within the last years. However, no commonly accepted objective quality measures exist for assessment of the enhancement achieved by those algorithms. This paper discusses several objective quality measures and their applicability for dereverberation of <b>speech</b> <b>signals</b> focusing on algorithms for listening-room compensation. 1...|$|R
30|$|To {{evaluate}} {{the performance of}} the proposed system, we apply it to recorded <b>speech</b> <b>signals</b> which contain transient noise. Every <b>speech</b> <b>signals</b> and transient noise signals are recorded in real environment, separately. The transient noise signals are acquired by using mobile recoding devices while clicking buttons on the recording devices or tapping the body of the recording devices. We add the transient noise segments to the random points of time of the <b>speech</b> <b>signals.</b> More than one hundred transient noise sequences are added to eight sentences of <b>speech</b> <b>signals.</b> <b>Speech</b> database is recorded by four male and four female speakers, and the total length of the <b>speech</b> <b>signals</b> is about sixteen seconds. The sampling frequency of the speech is 8 kHz. Since the transient noise is recorded in real environment, additive background noise such as fan noise is also included in the recoded noise signal. In other words, the test <b>signals</b> contain clean <b>speech,</b> transient noise, and background noise. The signal-to-noise ratio (SNR) between the desired speech and the background noise is around 15 dB.|$|R
2500|$|One {{acoustic}} {{aspect of}} the <b>speech</b> <b>signal</b> may cue different linguistically relevant dimensions. For example, the duration of a vowel in English can indicate {{whether or not the}} vowel is stressed, or whether it is in a syllable closed by a voiced or a voiceless consonant, and in some cases (like American English [...] and [...] ) it can distinguish the identity of vowels. Some experts even argue that duration can help in distinguishing of what is traditionally called short and long vowels in English.|$|E
5000|$|Consider a <b>speech</b> <b>signal</b> , {{which can}} be {{represented}} in a domain [...] such that , where <b>speech</b> <b>signal</b> [...] , dictionary matrix [...] and the sparse coefficient vector [...] This <b>speech</b> <b>signal</b> {{is said to be}} sparse in domain , if number of significant (non zero) coefficients in sparse vector [...] are , where [...]|$|E
5000|$|If {{measurement}} matrix [...] {{satisfies the}} restricted isometric property (RIP) and is incoherent with dictionary matrix [...] then the reconstructed signal is {{much closer to}} the original <b>speech</b> <b>signal.</b> Different types of measurement matrices like random matrices can be used for speech signals.Estimating the sparsity of <b>speech</b> <b>signal</b> is a problem since <b>speech</b> <b>signal</b> highly varies over time and thus sparsity of <b>speech</b> <b>signal</b> also varies highly over time. If sparsity of <b>speech</b> <b>signal</b> can be calculated over time without much complexity that will be best. If this is not possible then worst-case scenario for sparsity can be considered for a given <b>speech</b> <b>signal.</b> Sparse vector (...) for a given speech signals is reconstructed from less number of measurements (...) using [...] minimization. Then original <b>speech</b> <b>signal</b> is reconstructed form the calculated sparse vector [...] using the fixed dictionary matrix as [...] as [...] = [...] [...] Estimation of both the dictionary matrix and sparse vector from just random measurements only has been done iteratively in.The <b>speech</b> <b>signal</b> reconstructed from estimated sparse vector and dictionary matrix is {{much closer to the}} original signal.Some more iterative approaches to calculate both dictionary matrix and <b>speech</b> <b>signal</b> from just random measurements of <b>speech</b> <b>signal</b> are shown in.Th application of structured sparsity for joint speech localization-separation in reverberant acoustics has been investigated for multiparty speech recognition. Further applications of the concept of sparsity are yet to be studied in the field of speech processing. The idea behind CS for speech signals is that can we come up with some algorithms or methods where we only use those random measurements (...) to do some application-based processing like speaker recognition, speech enhancement, etc.|$|E
40|$|The problem {{addressed}} in this work is that of enhancing <b>speech</b> <b>signals</b> corrupted by additive noise and improving the performance of automatic speech recognizers in noisy conditions. The enhanced <b>speech</b> <b>signals</b> can also improve the intelligibility of speech in noisy conditions for human listeners with hearing impairment {{as well as for}} normal listeners. The original Phase Opponency (PO) model, proposed to detect tones in noise, simulates the processing of the information in neural discharge times and exploits the frequency-dependent phase properties of the tuned filters in the auditory periphery along with the cross-auditory-nerve-fiber coincidence detection to extract temporal cues. The Modified Phase Opponency (MPO) proposed here alters the components of the PO model {{in such a way that}} the basic functionality of the PO model is maintained but the various properties of the model can be analyzed and modified independently of each other. This work presents a detailed mathematical formulation of the MPO model and the relation between the properties of the narrowband signal that needs to be detected and the properties of the MPO model. The MPO speech enhancement scheme is based on the premise that <b>speech</b> <b>signals</b> are composed of a combination of narrow band signals (i. e. harmonics) with varying amplitudes. The MPO enhancement scheme outperforms many of the other speech enhancement techniques when evaluated using different objective quality measures. Automatic speech recognition experiments show that replacing noisy <b>speech</b> <b>signals</b> by the corresponding MPO-enhanced <b>speech</b> <b>signals</b> leads to an improvement in the recognition accuracies at low SNRs. The amount of improvement varies with the type of the corrupting noise. Perceptual experiments indicate that: (a) there is little perceptual difference in the MPO-processed clean <b>speech</b> <b>signals</b> and the corresponding original clean signals and (b) the MPO-enhanced <b>speech</b> <b>signals</b> are preferred over the output of the other enhancement methods when the <b>speech</b> <b>signals</b> are corrupted by subway noise but the outputs of the other enhancement schemes are preferred when the <b>speech</b> <b>signals</b> are corrupted by car noise...|$|R
40|$|When <b>speech</b> <b>signals</b> are {{transmitted}} via radio, {{the process of}} transmission may add noise to the signal of interest (Biddulph, 1994; Coleman, 2004). This study aims to {{examine the effect of}} radio transmission on the quality of <b>speech</b> <b>signals</b> transmitted using a combined acoustic and perceptual approach...|$|R
40|$|Abstract. The paper reviews {{two basic}} time–frequency distributions, {{spectrogram}} and cone–shaped kernel distribution. We study, analyze and compare properties {{and performance of}} these quadratic representations on <b>speech</b> <b>signals.</b> Cone–shaped kernel distribution was successfully applied to speech features extraction due to several useful properties in time–frequency analysis of <b>speech</b> <b>signals.</b> ...|$|R
50|$|The {{study of}} {{phonetics}} grew {{quickly in the}} late 19th century {{partly due to the}} invention of the phonograph, which allowed the <b>speech</b> <b>signal</b> to be recorded. Phoneticians were able to replay the <b>speech</b> <b>signal</b> several times and apply acoustic filters to the signal. By doing so, they were able to more carefully deduce the acoustic nature of the <b>speech</b> <b>signal.</b>|$|E
5000|$|<b>Speech</b> <b>signal</b> {{processing}} {{for processing}} and interpreting spoken words ...|$|E
50|$|LPC {{analyzes}} the <b>speech</b> <b>signal</b> by estimating the formants, removing their {{effects from the}} <b>speech</b> <b>signal,</b> and estimating the intensity and frequency of the remaining buzz. The process of removing the formants is called inverse filtering, and the remaining signal after the subtraction of the filtered modeled signal is called the residue.|$|E
40|$|A robust {{time-varying}} filtering {{procedure for}} <b>speech</b> <b>signals</b> corrupted by mixed Gaussian and impulse noise is presented. It {{is based on}} the robust time-frequency distributions that can provide efficient representation of the noisy <b>speech</b> <b>signals.</b> The proposed approach has been compared with the time-varying filtering procedure based on the standard time-frequency distributions...|$|R
40|$|A vocoder {{technique}} {{is described in}} which <b>speech</b> <b>signals</b> are represented by their short-time phase and amplitude spectra. A complete transmission system utilizing this approach is simulated on a digital computer. The encoding method leads to an economy in transmission bandwidth and to a means for time compression and expansion of <b>speech</b> <b>signals...</b>|$|R
40|$|This paper {{presents}} a novel method for estimating a vocal-tract spectrum from <b>speech</b> <b>signals,</b> {{based on a}} modeling of excita-tion <b>signals</b> of voiced <b>speech.</b> A formulation of linear prediction coding with impulse train is derived and applied to the phase-equalized <b>speech</b> <b>signals,</b> which are converted from the original <b>speech</b> <b>signals</b> by phase equalization. Preliminary {{results show that the}} proposed method improves the robustness of the estima-tion of a vocal-tract spectrum and the quality of re-synthesized speech compared with the conventional method. This technique will be useful for speech coding, speech synthesis, and real-time speech conversion. Index Terms: LPC, vocal-tract spectrum, phase equalization 1...|$|R
