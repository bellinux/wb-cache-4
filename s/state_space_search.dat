156|10000|Public
50|$|<b>State</b> <b>space</b> <b>search</b> explores a state space.|$|E
5000|$|... forward {{chaining}} <b>state</b> <b>space</b> <b>search,</b> possibly enhanced with heuristics ...|$|E
50|$|SSS* is {{a search}} algorithm, {{introduced}} by George Stockman in 1979, that conducts a <b>state</b> <b>space</b> <b>search</b> traversing a game {{tree in a}} best-first fashion {{similar to that of}} the A* search algorithm.|$|E
40|$|Efficient {{design of}} data structures, {{recursive}} algorithms, algorithms for sorting and searching, complexity analysis of algorithms, applications of algorithms and data structures in problems, <b>state</b> <b>spaces,</b> and <b>search</b> strategies in artificial intelligence Laboratory emphasizes object-oriented approach...|$|R
40|$|Abstract. Recently, great {{efforts have}} been {{dedicated}} to researches on the management of large-scale graph-based data, where node disjoint subgraph homeomorphism relation between graphs {{has been shown to}} be more suitable than (sub) graph isomorphism in many cases, especially in those cases where node skipping and node mismatching are desired. How-ever, no efficient algorithm for node disjoint subgraph homeomorphism determination (ndSHD) has been available. In this paper, we propose two computationally efficient ndSHD algorithms based on <b>state</b> <b>spaces</b> <b>searching</b> with backtracking, which employ many heuristics to prune the <b>search</b> <b>spaces.</b> Experimental results on synthetic data sets show that the proposed algorithms are efficient, require relatively little time in most of cases, can scale to large or dense graphs, and can accommodate to more complex fuzzy matching cases. ...|$|R
40|$|Model {{checking}} is {{an automatic}} verification technique to verify {{hardware and software}} systems. However it suffers from state-space explosion problem. In this paper we address this problem {{in the context of}} cryptographic protocols by proposing a security property-dependent heuristic. The heuristic weights the <b>state</b> <b>space</b> by exploiting the security formulae; the weights may then be used to explore the <b>state</b> <b>space</b> when <b>searching</b> for attacks. ...|$|R
50|$|<b>State</b> <b>space</b> <b>search</b> is {{a process}} used {{in the field of}} {{computer}} science, including artificial intelligence (AI), in which successive configurations or states of an instance are considered, with the goal of finding a goal state with a desired property.|$|E
5000|$|A Bidirectional Heuristic Search is a <b>state</b> <b>space</b> <b>search</b> {{from some}} state [...] {{to another state}} , searching from [...] to [...] and from [...] to [...] simultaneously. It returns a valid list of {{operators}} that if applied to [...] will give us [...]|$|E
50|$|Related {{algorithms}} to CDCL are the DP and DPLL algorithm described before. The DP algorithm uses resolution refutation and it {{has potential}} memory access problem. Whereas the DPLL algorithm is OK for randomly generated instances, it is bad for instances generated in practical applications. CDCL is a more powerful approach to solve such problems in that applying CDCL provides less <b>state</b> <b>space</b> <b>search</b> in comparison to DPLL.|$|E
40|$|I {{describe}} how real quantum annealers {{may be used}} to perform local (in <b>state</b> <b>space)</b> <b>searches</b> around specified <b>states,</b> rather than the global searches traditionally implemented in the quantum annealing algorithm. The quantum annealing algorithm is an analogue of simulated annealing, a classical numerical technique which is now obsolete. Hence, I explore strategies to use an annealer in a way which takes advantage of modern classical optimization algorithms, and additionally should be less sensitive to problem mis-specification then the traditional quantum annealing algorithm. Comment: In Proceedings PC 2016, arXiv: 1606. 06513. An extended version of this contribution will appear on arXiv soon which will describe more detailed algorithms, comment more on robustness to problem mis-specification, comment on thermal sampling applications, and discuss applications on real device...|$|R
40|$|When triangulating {{a belief}} network {{we aim to}} obtain a {{junction}} tree of minimum <b>state</b> <b>space.</b> <b>Searching</b> for the optimal triangulation can be cast as a search over all the permutations of the network's vaeriables. Our approach is to embed the discrete set of permutations in a convex continuous domain D. By suitably extending the cost function over D and solving the continous nonlinear optimization task we hope to obtain a good triangulation {{with respect to the}} aformentioned cost. In this paper we introduce an upper bound to the total junction tree weight as the cost function. The appropriatedness of this choice is discussed and explored by simulations. Then we present two ways of embedding the new objective function into continuous domains and show that they perform well compared to the best known heuristic...|$|R
40|$|Recently, great {{efforts have}} been {{dedicated}} to researches on the management of large scale graph based data such as WWW, social networks, biological networks. In the study of graph based data management, node disjoint subgraph homeomorphism relation between graphs is more suitable than (sub) graph isomorphism in many cases, especially in those cases that node skipping and node mismatching are allowed. However, no efficient node disjoint subgraph homeomorphism determination (ndSHD) algorithms have been available. In this paper, we propose two computationally efficient ndSHD algorithms based on <b>state</b> <b>spaces</b> <b>searching</b> with backtracking, which employ many heuristics to prune the <b>search</b> <b>spaces.</b> Experimental results on synthetic data sets show that the proposed algorithms are efficient, require relative little time {{in most of the}} testing cases, can scale to large or dense graphs, and can accommodate to more complex fuzzy matching cases. Comment: 15 pages, 11 figures, submitted to DASFAA 200...|$|R
50|$|The {{simplest}} classical planning(see Automated Planning) algorithms are <b>state</b> <b>space</b> <b>search</b> algorithms. Theseare search algorithms {{in which}} the search space is {{a subset of the}} state space: Eachnode corresponds to a state of the world, each arc corresponds to a state transition,and the current plan corresponds to the current path in the search space.Forward Search and Backward Search are two of main samples of state space planning.|$|E
50|$|<b>State</b> <b>space</b> <b>search</b> often {{differs from}} {{traditional}} computer science search methods {{because the state}} space is implicit: the typical state space graph is much too large to generate and store in memory. Instead, nodes are generated as they are explored, and typically discarded thereafter. A solution to a combinatorial search instance may consist of the goal state itself, or of a path from some initial state to the goal state.|$|E
5000|$|While GPS solved simple {{problems}} such as the Towers of Hanoi that could be sufficiently formalized, it could not solve any real-world problems because search was easily lost in the combinatorial explosion. Put another way, the number of [...] "walks" [...] through the inferential digraph became computationally untenable. (In practice, even a straightforward <b>state</b> <b>space</b> <b>search</b> such as the Towers of Hanoi can become computationally infeasible, albeit judicious prunings of the state space {{can be achieved by}} such elementary AI techniques as alpha-beta pruning and min-max.) ...|$|E
40|$|Abstract. General Game Playing (GGP) aims at {{developing}} game play-ing {{agents that}} {{are able to}} play a variety of games and, in the absence of pre-programmed game specific knowledge, become proficient players. The challenge of making such a player has led to various techniques be-ing used to tackle the problem of game specific knowledge absence. Most GGP players have used standard tree-search techniques enhanced by au-tomatic heuristic learning, neuroevolution and UCT (Upper Confidence bounds applied to Trees) search, which is a simulation-based tree search. In this paper, we explore a new approach to GGP. We use an Ant Colony System (ACS) to explore the game space and evolve strategies for game playing. Each ant in the ACS is a player with an assigned role, and for-ages through the gameâ€™s <b>state</b> <b>space,</b> <b>searching</b> for promising paths to victory. Preliminary results show this approach to be promising. In or-der to test the architecture, we create matches between players using the knowledge learnt by the ACS and random players...|$|R
40|$|The task of {{this work}} is to bring a {{complete}} guide for solving a SOKOBAN game problem. The work follow subsequently issues of studies of problem complexity, its definition, selection of appropriate algorithms, {{as well as their}} implementation and testing. The study demonstratethe manifest, that problem belongs to a NP-hard set of problems as well as PSPACE-complete set of problems. Fundamental and also advanced techniques of <b>state</b> <b>space</b> <b>searching</b> are studied as means for solving defined problem, memory and time characteristics of these techniques are presented and derived. In order to gain statistical information about efficiency of demonstrated algorithms and heuristics, an aplication SokobanSolver was built. This program can solve many SOKOBAN problems automatically. We described principles of functional parts and main data structures of this program and also we discuss program SokobanGenerator, aplication that was built to supply SokobanSolver with many random built SOKOBAN problems. Techniques explained are tested by lately named programs, and results are discussed {{at the very end of}} the paper...|$|R
40|$|When {{the state}} {{dimension}} is large, classical approximate dynamic programming techniques may become computationally unfeasible, since {{the complexity of}} the algorithm grows exponentially with the <b>state</b> <b>space</b> size (curse of dimensionality). Policy search techniques are able to overcome this problem because, instead of estimating the value function over the entire <b>state</b> <b>space,</b> they <b>search</b> for the optimal control policy in a restricted parameterized policy space. This paper presents a new policy parametrization that exploits a single point (particle) to represent an entire region of the <b>state</b> <b>space</b> and can be tuned through a recently introduced policy gradient method with parameter-based exploration. Experiments demonstrate the superior performance of the proposed approach in high dimensional environments...|$|R
40|$|<b>State</b> <b>space</b> <b>search</b> {{problems}} {{abound in}} the artificial intelligence, planning and op-timization literature. Solving such problems is generally NP-hard, {{so that a}} brute-force approach to <b>state</b> <b>space</b> <b>search</b> must be employed. Given the exponential amount of work that <b>state</b> <b>space</b> <b>search</b> problems entail, it is desirable to solve them on large paral-lel machines with significant computational power. In this paper, we analyze the parallel performance of several classes of <b>state</b> <b>space</b> <b>search</b> applications. In particular, we fo-cus {{on the issues of}} grain size, the prioritized execution of tasks and the balancing of load among processors in the system. We demonstrate the corresponding techniques that are used to scale such applications to large scale. Moreover, we tackle the problem of programmer productivity by incorporating these techniques into a general search engine framework designed to solve a broad class of <b>state</b> <b>space</b> <b>search</b> problems. We demon-strate the efficiency and scalability of our design using three example applications, and present scaling results up to 32, 768 processors...|$|E
40|$|<b>State</b> <b>space</b> <b>search</b> {{problems}} {{abound in}} the artificial intelli-gence, planning and optimization literature. Solving such prob-lems is generally NP-hard. Therefore, a brute-force approach to <b>state</b> <b>space</b> <b>search</b> must be employed. It is instructive {{to solve them}} on large parallel machines with significant computational power. However, writing efficient and scalable parallel pro-grams has traditionally been a challenging undertaking. In this paper, we analyze several performance characteristics common to all parallel <b>state</b> <b>space</b> <b>search</b> applications. In particular, {{we focus on the}} issues of grain size, the prioritized execution of tasks and the balancing of load among processors in the system. We demonstrate the techniques that are used to scale such applications to large scale. We have incorporated these techniques into a general search engine framework that is designed to solve a broad class of <b>state</b> <b>space</b> <b>search</b> problems. We demonstrate the efficiency and scalability of our design using three example applications, and present scaling results up to 16, 384 processors. Keywords-Parallel <b>state</b> <b>space</b> <b>search,</b> adaptive grain size control, dynamic load balancing, prioritized execution I...|$|E
40|$|We presenttwo {{recursive}} <b>state</b> <b>space</b> <b>search</b> algorithms {{that are}} based on SSS* and Dual*. Both dominate Alpha-Beta on a node count basis {{and one of them is}} always faster in searching random trees. These results have been derived on arti#cial game trees with negligible leaf node evaluation time. For practical applications with more complex leaf evaluation functions we conjecture that our recursive <b>state</b> <b>space</b> <b>search</b> algorithms perform even better and mighteventually supersede the popular directional search methods...|$|E
40|$|I {{describe}} how real quantum annealers {{may be used}} to perform local (in <b>state</b> <b>space)</b> <b>searches</b> around specified <b>states,</b> rather than the global searches traditionally implemented in the quantum annealing algorithm (QAA). Such protocols will have numerous advantages over simple quantum annealing. By using such searches the effect of problem mis-specification can be reduced, as only energy differences between the searched states will be relevant. The QAA is an analogue of simulated annealing, a classical numerical technique which has now been superseded. Hence, I explore two strategies to use an annealer in a way which takes advantage of modern classical optimization algorithms. Specifically, I show how sequential calls to quantum annealers can be used to construct analogues of population annealing and parallel tempering which use quantum searches as subroutines. The techniques given here can be applied not only to optimization, but also to sampling. I examine the feasibility of these protocols on real devices and note that implementing such protocols should require minimal if any change to the current design of the flux qubit-based annealers by D-Wave Systems Inc. I further provide proof-of-principle numerical experiments based on quantum Monte Carlo that demonstrate simple examples of the discussed techniques...|$|R
40|$|In this paper, we mainly {{focus on}} solving {{scheduling}} problems with model checking, where {{a finite number}} of entities needs to be processed as efficiently as possible, for instance by a machine. To solve these problems, we model them in untimed process algebra, where time is modelled using a special tick action. We propose a set of distributed <b>state</b> <b>space</b> explorations to find schedules for the modelled problems, building on the traditional notion of beam search. The basic approach is called distributed (detailed) beam search, which prunes parts of the <b>state</b> <b>space</b> while <b>searching</b> using an evaluation function in order to find near-optimal schedules in very large <b>state</b> <b>spaces.</b> Variations on this approach are presented, such as distributed flexible, distributed g-synchronised, and distributed priority beam search, which can also practically be used in combinations...|$|R
40|$|This paper {{exploits}} {{parallel computing}} power of graphics cards for the enhanced enumeration of <b>state</b> <b>spaces.</b> We illustrate that modern graphics processing units (GPUs) {{have the potential}} to speed up <b>state</b> <b>space</b> breadth first <b>search</b> significantly. For a bitvector representation of the search frontier, GPU algorithms with one and two bits per state are presented. Efficient perfect hash functions and their inverse are studied for enhanced compression. We establish maximal speed-ups of up to factor 30 and more wrt. single core computation...|$|R
40|$|We {{introduce}} SImulation Verification with Augmentation (SIVA), a {{tool for}} coverage-directed <b>state</b> <b>space</b> <b>search</b> on digital hardware designs. SIVA tightly integrates simulation with symbolic techniques for efficient <b>state</b> <b>space</b> <b>search.</b> Specifically, the core algorithm uses a combination of ATPG and BDD's to generate "directed" input vectors, i. e., inputs which cover behavior not excited by simulation. We also present approaches to automatically generate "lighthouses" that guide the search towards hard-to-reach coverage goals. Experiments demonstrate that our approach is capable of achieving significantly greater coverage than either simulation or symbolic techniques in isolation...|$|E
40|$|This paper {{describes}} a probabilistic approach to <b>state</b> <b>space</b> <b>search.</b> The presented method applies a ranking {{of the design}} states according to their probability of reaching a given target state based on a random walk model. This ranking {{can be used to}} prioritize an explicit or partial symbolic state exploration to find a trajectory from a set of initial states to a set of target states. A symbolic technique for estimating the reachability probability is described which implements a smooth trade-o# between accuracy and computing e#ort. The presented probabilistic <b>state</b> <b>space</b> <b>search</b> complements incomplete verification methods which are specialized in finding errors in large designs...|$|E
40|$|International audienceThis paper proposes {{incremental}} preference elicitation {{methods for}} multiobjective <b>state</b> <b>space</b> <b>search.</b> Our approach consists in integrating weight elicitation and search to determine, in a vector-valued state-space graph, a solution path that best fits the Decision Maker's preferences. We first {{assume that the}} objective weights are imprecisely known and propose a <b>state</b> <b>space</b> <b>search</b> procedure to determine the set of possibly optimal solutions. Then, we introduce incremental elicitation strategies during the search that use queries to progressively reduce the set of admissible weights until a nearly-optimal path can be identified. The validity of our algorithms is established and numerical tests are provided to test their efficiency {{both in terms of}} number of queries and solution times...|$|E
3000|$|... Î± and Î² are {{coefficients}} with values between 0 and 1. Î± {{indicates the}} impact of the number of concepts which are in the user response, and Î² indicates {{the impact of}} distance between concepts in user response and concepts in question. In this study, the optimum values for these coefficients are calculated. To achieve the optimum values <b>state</b> <b>space</b> is <b>searched</b> by changing 0.01 intervals for Î± and Î². The optimum values obtained for these coefficients are equally 0.5. By using these coefficients, the best correlation between the scores obtained from the proposed method and the scores provided by Java's online community, is calculated.|$|R
40|$|Abstract. This paper {{introduces}} {{a new model}} checking algorithm that searches for non-progress cycles, used mainly to check for livelocks. The algorithm performs an incremental depth-first search, i. e., it searches through the graph incrementally deeper. It simultaneously constructs the <b>state</b> <b>space</b> and <b>searches</b> for non-progress cycles. The algorithm is more efficient than the method the model checker SPIN currently uses, and finds shortest (w. r. t. progress) counterexamples. Its only downside {{is the need for}} a subsequent reachability depth-first search (which is not the bottleneck) for constructing a full counterexample. The new algorithm is better combinable with partial order reduction than SPINâ€™s method...|$|R
40|$|Reachability {{analysis}} {{is a powerful}} formal method for analysis of concurrent and distributed finite state systems. It suffers from the <b>state</b> <b>space</b> explosion problem, however: the <b>state</b> <b>space</b> of a system can be far too large to be completely generated. This paper considers two promising methods, Valmari's stubborn set method and Godefroid's sleep set method, to avoid generating all of the <b>state</b> <b>space</b> when <b>searching</b> for undesirable reachable terminal states, also called deadlocks. These methods have been combined by Godefroid, Pirottin, and Wolper to further {{reduce the number of}} inspected states. However, the combination presented by them places assumptions on the stubborn sets used. This paper shows that at least in place/transition nets, the stubborn set method can be combined with the sleep set method {{in such a way that}} all reachable terminal states are found, without having to place any assumption on the stubborn sets used. This result is shown by showing a more general result which [...] ...|$|R
40|$|This paper {{presents}} a signal inversion technique in nondestructive evaluation (NDE) application for defect profile reconstruction using the element-free galerkin (EFG) method and <b>state</b> <b>space</b> <b>search.</b> The advantage of EFG method {{is that it}} relies only {{on a set of}} nodes, instead of a complex mesh to discretize the solution domain. In the inversion procedure, remeshing is avoided to increase the efficiency and accuracy of the solution, which is a major advantage over the traditional finite-element method (FEM). The iterative <b>state</b> <b>space</b> <b>search</b> method using the tree structure is developed for implementing the defect updating scheme. Preliminary results are presented for validation. The robustness of the technique has been shown on noisy signals...|$|E
40|$|This paper proposes {{incremental}} preference elicitation {{methods for}} multiobjective <b>state</b> <b>space</b> <b>search.</b> Our ap-proach consists in integrating weight elicitation and search to determine, in a vector-valued state-space graph, a solution path that best fits the Decision Makerâ€™s preferences. We first {{assume that the}} objective weights are imprecisely known and propose a <b>state</b> <b>space</b> <b>search</b> procedure to determine the set of possibly optimal solu-tions. Then, we introduce incremental elicitation strate-gies during the search that use queries to progressively reduce the set of admissible weights until a nearly-optimal path can be identified. The validity of our algo-rithms is established and numerical tests are provided to test their efficiency {{both in terms of}} number of queries and solution times...|$|E
40|$|This paper {{presents}} a {{design for a}} reactive sys-tem based on the classical planning techniques of problem reduction and <b>state</b> <b>space</b> <b>search.</b> The proposed approach enables a reactive sys-tem to be scaled up to handle larger sets of tasks. Problem reduction synthesizes an ini-tial reactive policy for a given task. When an execution impasse occurs, <b>state</b> <b>space</b> <b>search</b> finds critical choice points, from which control rules are synthesized. These rules alter the policyâ€™s future behavior {{in order to avoid}} the impasse. This technique, called critical choice planning, incrementally debugs the initial pol-icy in the least restrictive way; this "least re-strictive property " makes the technique a per-fect match for problem reduction. Over time, the problem reduction rules are improved via learning from the debugging experiences...|$|E
40|$|Reachability {{analysis}} {{is a powerful}} formal method for analysis of concurrent and distributed finite state systems. It suffers from the <b>state</b> <b>space</b> explosion problem, however: the <b>state</b> <b>space</b> of a system can be far too large to be completely generated. This report considers two promising methods, Valmari's stubborn set method and Godefroid's sleep set method, to avoid generating all of the <b>state</b> <b>space</b> when <b>searching</b> for undesirable reachable terminal states, also called deadlocks. What makes deadlocks especially interesting {{is the fact that}} the verification of a safety property can often be reduced to deadlock detection. The considered methods utilize the independence of transitions to cut down on the number of states inspected during the search. These methods have been combined by Godefroid, Pirottin, and Wolper to further reduce the number of inspected states. Petri nets are a widely used model for concurrent and distributed systems. This report shows that the stubborn set method a [...] ...|$|R
40|$|We {{describe}} the HSP 2. 0 planning algorithm that entered the Second Planning Contest held in AIPS 2000. HSP 2. 0 is a domain independent planning algorithm that implements {{the family of}} heuristic search planners that are characterized by the <b>state</b> <b>space</b> that is <b>searched</b> (either progression or regression <b>space),</b> the <b>search</b> algorithm used, and the heuristic function that is used. This general planner implements a scheduler that tries dierent variants in parallel with dierent (time) resource bounds. We also describe how HSP 2. 0 {{can be used as}} an optimal (and near-optimal) planning algorithm, and compare its performance with the other two optimal planners stan and blackbox. ...|$|R
40|$|In {{automatic}} {{speech recognition}} complex <b>state</b> <b>spaces</b> are <b>searched</b> during the recognition process. By limiting these <b>search</b> <b>spaces</b> the computation time can be reduced, but unfortunately the recognition rate mostly decreases, too. However, especially for time-critical recognition tasks a search-space pruning is necessary. Therefore, we developed a dynamic mechanism to optimize the pruning parameters for time-constrained recognition tasks, e. g. speech recognition for robotic systems, in respect to word accuracy and computation time. With this mechanism an {{automatic speech recognition}} system can process speech signals with an approximately constant processing rate. Compared to a system without such a dynamic mechanism and the same time available for computation, the variance of the processing rate is decreased greatly without a significant loss of word accuracy. Furthermore, the extended system can be sped up to real-time processing, if desired or necessary...|$|R
