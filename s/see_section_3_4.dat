10|10000|Public
40|$|Dry Cleaning {{refers to}} any process to remove {{contamination}} from furs, leather, down leathers, textiles or other objects made of fibres, using organic solvents. 2 CONTRIBUTION TO TOTAL EMISSIONS The most significant pollutants emitted from dry cleaning are NMVOCs. These include chlorinated solvents (<b>see</b> <b>section</b> <b>3.</b> <b>4).</b> Heavy metal and POP emissions {{are unlikely to}} be significant. Source-activity SNAP-code Contribution to total emissions [%], (including emissions from nature). [Source- CORINAIR 90...|$|E
40|$|This {{deliverable}} {{will describe}} in detail results obtained during {{the project for}} four key areas of the Po Plain, the target area for the INGV contribution to the project. The four area are: 1) Coastal Marche region (southeastern Po Plain – see Section 1) 2) Mantova/Mincio River area (central Po Plain – see Section 2) 3) Mirandola/Secchia-Panaro Rivers area (southern-central Po Plain – <b>see</b> <b>Section</b> <b>3)</b> <b>4)</b> Soncino/Oglio River area (western Po Plain – see Section 4...|$|E
40|$|Too many {{students}} in introductory programming classes {{fail to understand}} the significance and utility of the concepts being taught. Their low motivation impacts their learning. One contributing factor is pedagogy that emphasizes computing {{for its own sake}} and assign-ments that are abstract, such as computing the factorial function. Many educators have improved on such traditional approaches by teaching concepts in contexts that students find more relevant, such as games, robots, and media. Now, it is time to take the next step. In this special session, participants will develop and discuss ways to teach introductory programming by means of real-world data analysis problems from science, engineering, business, and the humanities. Students can be motivated to learn programming in order to analyze DNA, predict the outcome of elections, detect fraudulent data, suggest friends in a social network, determine the authorship of texts, and more (<b>see</b> <b>Section</b> <b>3.</b> <b>4</b> for more examples) ...|$|E
40|$|Application Development About {{the cover}} image Metaphorical {{illustration}} of scientific visualization in Virtual Reality. A display {{can be considered}} as a window into a virtual world, see Chapters 1 and 2. About the front cover The Virtual Workbench is in use for interactive exploration and visualization of a cumulus clouds dataset, <b>see</b> <b>Section</b> 6. <b>3.</b> The VRX toolkit (Section 5. 2) is used for this purpose. Central image: A user at the Workbench is studying the vertical air velocity in the interior of a cloud using a direct slicing tool, attached to the Plexipad, <b>see</b> <b>Section</b> 5. 1. This is an ”augmented reality ” photo, <b>see</b> <b>Section</b> <b>3.</b> 5. <b>4.</b> Bottom right image: A user is interactively studying the air flow in and around the clouds using streamlines, <b>see</b> <b>Section</b> 6. <b>3.</b> This is an image from a playback of a Workbench session in the RWB Simulator, <b>see</b> <b>Section</b> <b>3.</b> 5. About the back cover The Virtual Workbench is being used for visualization of real-time Molecular Dynamics and steering of the simulation in the MolDRIVE system (Section 6. 2). Central image: A user at the Workbench is steering a particle of a protein using the spring manipulator, <b>see</b> <b>Section</b> <b>4.</b> <b>3.</b> Bottom left and right images: Particle steering with the spring manipulator in an electrolyte simulation (left). The virtual particle steering method is assisted by a color slicer, which shows the particle potential around it (right) ...|$|R
40|$|This section {{presents}} the hazard characterization of TCE health effects. Because {{of the number}} of studies and their relevance to multiple endpoints, the evaluation of epidemiologic studies of cancer and TCE is summarized in Section 4. 1 (endpoint-specific results are presented in subsequent sections). Genotoxicity data are discussed in Section 4. 2. Due to the large number of endpoints and studies in the toxicity database, subsequent <b>sections</b> (<b>see</b> <b>Sections</b> <b>4.</b> <b>3</b> – <b>4.</b> 10) are organized by tissue/organ system. Each section is further organized by noncancer and cancer endpoints, discussing data from human epidemiologic and laboratory experimental studies. In cases where there is adequate information, the role of metabolism in toxicity, comparisons of toxicity between TCE and its metabolites, and carcinogenic mode of action are also discussed. Finally, Section 4. 11 summarizes the overall hazard characterization and the weight of evidence for noncancer and carcinogenic effects...|$|R
50|$|The cemetery’s roads had {{all been}} {{designed}} for access by horse and carriage. Wyrick {{was among the first}} to see the need for wider roads with expansive curves that could be successfully rounded by motor vehicles with a wider turning radius. His influence in this matter can be <b>seen</b> in <b>sections</b> 2, <b>3</b> <b>4,</b> and 8 and in the Memorial Garden bearing Monsignor Clark’s name.|$|R
40|$|This paper {{presents}} a numediart project {{in collaboration with}} two artistic projects: Musichorégraphie d’appartement by André Serre-Milan [63] and BioDiva by Laurence Moletta (Nios Karma) [34]. The scope of this project was to offer technological forecasting and development consultancy to these two artists that both share a common goal of using gestures to control on audiovisual rendering on stage (see section 2). We developed a first prototype for synchronized recording, visualization and editing of multimodal signals (audio, video, sensors) (see section 3. 1). We updated our technologies for gesture recognition using Dynamic Time Warping (DTW) (see section 3. 2) and mapping interpolation (see section 3. 3). We ported our longterm attention computer vision algorithm from EyesWeb to Max / MSP (<b>see</b> <b>section</b> <b>3.</b> <b>4).</b> We achieved initial promising results for the prototyping of the BioDiva gestural interface (see section 4. 1) and for the analysis using the long-term attention model of the Musichorégraphie d’appartement recording sessions (see section 4. 2). We plan to improve our multimodal signal recording and analysis tool towards a more efficient annotation tool (see section 5. 1). We {{plan to build a}} dedicated wireless sensor interface to overcome all the related issues we met so far (see section 5. 2). We plan to use new textiles for the improvement of sensors, sensor interfaces and integration in performers ’ costumes (see section 5. 3). KEYWORDS Augmented performances, wearable sensors, synchronized multimodal recording, gesture recognition, gestural interface design, computer vision, intelligent textiles, textile sensor...|$|E
40|$|In this paper, {{the authors}} assess {{the extent to}} which the {{inclusion}} of learning cycles in community development projects can contribute to local people’s increased capacity to initiate action. A case study approach was used to test the hypothesis that: Participatory environment and development projects that incorporate learning cycles will result in empowerment among local people. In this research, learning cycles were defined as a ‘continuous process of situation analysis, collaborative planning, action and critical reflection’ (<b>see</b> <b>Section</b> <b>3.</b> <b>4),</b> while empowerment was defined as ‘a process through which individuals, as well as local groups and communities, enhance their capacity to initiate action to improve their well-being by gaining decision-making power’ (see Section 2. 2). Six key research questions were formulated to ensure that all these components were considered when testing the hypothesis (Section 4). The case study involved two sub-projects of a community development and forest/watershed conservation project conducted in the mid-hills of Nepal by the Nepalese Government and the Japan International Cooperation Agency (JICA). In the sections to follow the relationship between participatory development projects and learning cycles are explored. Section 2 defines participatory development and empowerment in more depth, while Section 3 reviews three learning theories—experiential learning, social learning and organisational learning (in particular, multi-looped learning) —to build the analytical framework for the case study. Section 4 describes the research methods, including criteria for selection of case studies, and the field survey methods. Section 5 reports the qualitative field data, and discusses {{the extent to which}} the case study projects demonstrated learning cycles. Section 6 discusses these results further and presents conclusions...|$|E
40|$|There are recent {{successes of}} {{automated}} deduction techniques in various application domains, such as mathematics, classical and nonmonotonic logics, diagnosis, planning and within software engineering. We will briefly review {{some of them}} and observe that these successes have been made only after a careful analysis of the application domain {{as well as the}} deduction system under consideration. The {{purpose of this article is}} to argue that automated deduction systems can be usefully applied in practice, but it is necessary to have available a variety of deduction methods, to understand their properties and their computational power in order to tailor them for the application under consideration. In the early days of automated deduction, research concentrated on the development of general purpose deduction systems. “Applications ” were toy examples, which did not scale up to realistically sized problems. Nowadays, the theorem prover community discovers applications again. The key to success quite often is the specific knowledge of the application domain which is used to optimise the deduction systems, i. e. the knowledge is used to guide the search for a proof or a model. Indeed, there are some recent impressive results which show that such deductive systems are even able to handle realistic problems. Domains to be mentioned here include mathematics (see Sections 1 and 4. 1), planning [18] (see Section 3. 1), model checking (see Section 3. 2), diagnosis (see Section 3. 3), software reuse [22] and verification (<b>see</b> <b>Section</b> <b>3.</b> <b>4),</b> or view deletion in databases [2]. Another realistically sized problem solved by the Protein prover [5] was the analysis of banking fees rule sets, which was successfully tackled wit...|$|E
30|$|Taking risk {{into account}} in {{planning}} {{raises the question of}} transparency of data and information on risk. A first question concerns a fair distribution of benefits and costs to the project partners. CBA analyzes costs and benefits to society (project cost, health benefits/costs, environmental costs/benefits, time savings, etc.). The CBA gives a single ratio that {{does not take into account}} the distribution of these costs and benefits to the different partners and groups involved in the project. The circular dated 9 December 2008 issued by the Ministry of Ecology recommended a comparison between the risks and the social impacts, environmental impacts, and economic impacts of each variant of the project and for each actor concerned (households, companies, vulnerable groups, etc) through a multi-criteria analysis (<b>see.</b> <b>Section</b> <b>3</b> – <b>4</b> for details).|$|R
40|$|This chapter {{presents}} the hazard characterization of trichloroethylene (TCE) health effects. Because {{of the number}} of studies and their relevance to multiple endpoints, the evaluation of epidemiologic studies of cancer and TCE is summarized in Section 4. 1 (endpointspecific results are presented in subsequent sections). Genotoxicity data are discussed in Section 4. 2. Due to the large number of endpoints and studies in the toxicity database, subsequent <b>sections</b> (<b>see</b> <b>Sections</b> <b>4.</b> <b>3</b> − <b>4.</b> 10) are organized by tissue/organ system. Each section is further organized by noncancer and cancer endpoints, discussing data from human epidemiologic and laboratory experimental studies. In cases where there is adequate information, the role of metabolism in toxicity, comparisons of toxicity between TCE and its metabolites, and carcinogenic mode of action (MOA) are also discussed. Finally, Section 4. 11 summarizes the overall hazard characterization and the weight of evidence for noncancer and carcinogenic effects. 4. 1. EPIDEMIOLOGIC STUDIES ON CANCER AND TRICHLOROETHYLENE (TCE) —METHODOLOGICAL OVERVIEW This brief overview of the epidemiologic studies on cancer and TCE below provide...|$|R
40|$|This work {{describes}} DynSys 3 D, {{a framework}} for testing and implementing visualization techniques {{in the area of}} three-dimensional dynamical systems. DynSys 3 D has been designed to meet requirements which allow a fast and modular investigation of dynamical systems. Such requirements are, e. g., extendability, interactivity, and symmetry. Some visualization examples realized with DynSys 3 D illustrate the flexibility of the system. Keywords: visualization, dynamical systems 1 Introduction DynSys 3 D is a flexible environment for implementing and evaluating new ideas in the field of advanced visualization techniques with special emphasis on three-dimensional dynamical systems. Starting with some specific requirements and goals (<b>see</b> <b>Section</b> 2) the system design of DynSys <b>3</b> D (<b>see</b> <b>Sections</b> <b>3</b> and <b>4)</b> was chosen to comply with most of these goals. DynSys 3 D provides an experimental workbench to easily investigate visualization techniques and dynamical systems in a collaborative environment. When start [...] ...|$|R
40|$|PROTHEGO (PROTection of European Cultural HEritage from GeO-hazards) is a {{collaborative}} research project funded in 2015 – 2018 {{in the framework}} of the Joint Programming Initiative on Cultural Heritage and Global Change (JPI-CH) – Heritage Plus. The project aims to make an innovative contribution towards the analysis of geohazards in areas of cultural heritage, and uses novel space technology based on radar interferometry (InSAR) to retrieve information on ground stability and motion in the 400 + UNESCO's World Heritage List monuments and sites of Europe. Dissemination and communication are central to the success of PROTHEGO, and are embedded into its WP 7, which runs throughout the whole lifetime of the project under the leadership of NERC. This report outlines the strategy that NERC in collaboration with ISPRA, CUT, UNIMIB and IGME designed to disseminate PROTHEGO’s objectives, methodologies and achievements and to engage stakeholders and heritage practitioners to maximise the impact of the project. Several dissemination tools are used to achieve PROTHEGO’s dissemination goals, including the development of the project branding (see sections 2. 1. 1 and 2. 1. 2), a dedicated website (see section 2. 1. 4), project leaflets and brochures (see section 2. 1. 3). A publication plan is in place with associated scenarios (see sections 2. 2. 1 and 2. 2. 2) to publicise the project and its results at both national and international level. An internal approval process and copyright responsibilities are identified in line with the Consortium Agreement (see section 2. 2. 3). A record of dissemination activities undertaken by Project Partners and Associate Partners (Table 6) is kept via the List of Outputs (see section 2. 2. 4 and Appendix A). Deliverables with public (PU) dissemination level (as defined in the Description of Work) are made freely available to stakeholders and the public via the project website. Restricted (PP) dissemination level deliverables are stored in the password-protected file sharing platform only for internal use to the Project Partners and the JPI-CH Heritage Plus Coordinator (see section 2. 2. 5). PROTHEGO will capture the needs and requirements of end-users and stakeholders, inform them about the project activities and outputs, and engage them {{from the very beginning of}} the project. This will be achieved through stakeholder-focussed workshops and activities: (i) Initial Consultation Workshop (see section 3. 1); (ii) Public Consultation via Online Survey (see section 3. 2); (iii) Stakeholder and User Workshop (see section 3. 3); and (iv) Final PROTHEGO Workshop (<b>see</b> <b>section</b> <b>3.</b> <b>4).</b> These workshops and activities will allow PROTHEGO to tailor its project outcomes and results to the stakeholders’ needs, to maximise the impact of the project and transfer its research outcomes to the heritage sector, policy makers and the general public...|$|E
40|$|Clouds {{consisting}} of ice particles have {{a crucial role}} in our climate system and profoundly influ- ence the Earth’s radiation. However, these clouds are poorly constrained in climate models mainly due to uncertainties associated with estimation of their ice mass. It has been investigated by sev- eral studies that the combined microwave millimeter/submillimeter spectral region is suitable for observing the mass of ice clouds. One of the major uncertainty sources of the microwave ice mass estimation is that the shape of ice particles is poorly known. It has been shown by several studies that ice particles can take on extremely variable irregular shapes. It is therefore necessary to find shape models that can approximate and simplify the reality. Two common simple shape models frequently used are spheres and spheroids. They can be either solid ({{consisting of}} pure ice) or soft (consisting of a homogeneous mixture of ice and air). This thesis is concerned with examining the applicability of the simple models in estima- tion of ice mass across the microwave spectral region. The focus is put on soft models. Three databases consisting of optical properties of some randomly oriented non-spherical ice particles and aggregates are considered as reference. The practical objective is to determine microphysical characteristics (i. e., shape, volume or mass fraction of air, and refractive index) of a soft model that mimics average optical properties of the reference data across all microwave frequencies from 90 to 874 GHz and size parameters (x) up to 6. It is found that the volume air fraction of a soft model should vary with both size and frequency to give the best fit of the reference data. It is therefore impossible to define a soft model with a single air fraction working in a broad range of frequencies and particle sizes. It is also demon- strated that determining the volume air fraction based on size-density parameterisations results in underestimation of mean optical properties at larger size parameters (x > 0. 5). Furthermore, it is concluded that applying the Maxwell Garnett mixing rule with ice as inclusion and air as matrix media (<b>see</b> <b>section</b> <b>3.</b> <b>4)</b> underestimates the imaginary parts of refractive index of the soft models at lower size parameters (x < 1). Overall, it is concluded that soft models can be tuned to give proper results over a narrow range of frequencies, but such a model lose its accuracy over other range of frequencies. One alternative approach to soft models is applying a sector-like snowflake model to represent the mean optical properties of the reference data. This was suggested by Geer and Baordo [2014] and confirmed by Eriksson et al. [2014]...|$|E
40|$|The {{accurate}} and efficient solution of Maxwell's equation {{is the problem}} addressed by the scientific discipline called Computational ElectroMagnetics (CEM). Many macroscopic phenomena in {{a great number of}} fields are governed by this set of differential equations: electronic, geophysics, medical and biomedical technologies, virtual EM prototyping, besides the traditional antenna and propagation applications. Therefore, many efforts are focussed on the development of new and more efficient approach to solve Maxwell's equation. The interest in CEM applications is growing on. Several problems, hard to figure out few years ago, can now be easily addressed thanks to the reliability and flexibility of new technologies, together with the increased computational power. This technology evolution opens the possibility to address large and complex tasks. Many of these applications aim to simulate the electromagnetic behavior, for example in terms of input impedance and radiation pattern in antenna problems, or Radar Cross Section for scattering applications. Instead, problems, which solution requires high accuracy, need to implement full wave analysis techniques, e. g., virtual prototyping context, where the objective is to obtain reliable simulations in order to minimize measurement number, and as consequence their cost. Besides, other tasks require the analysis of complete structures (that include an high number of details) by directly simulating a CAD Model. This approach allows to relieve researcher of the burden of removing useless details, while maintaining the original complexity and taking into account all details. Unfortunately, this reduction implies: (a) high computational effort, due to the increased number of degrees of freedom, and (b) worsening of spectral properties of the linear system during complex analysis. The above considerations underline the needs to identify appropriate information technologies that ease solution achievement and fasten required elaborations. The authors analysis and expertise infer that Grid Computing techniques can be very useful to these purposes. Grids appear mainly in high performance computing environments. In this context, hundreds of off-the-shelf nodes are linked together and work in parallel to solve problems, that, previously, could be addressed sequentially or by using supercomputers. Grid Computing is a technique developed to elaborate enormous amounts of data and enables large-scale resource sharing to solve problem by exploiting distributed scenarios. The main advantage of Grid is due to parallel computing, indeed if a problem can be split in smaller tasks, that can be executed independently, its solution calculation fasten up considerably. To exploit this advantage, it is necessary to identify a technique able to split original electromagnetic task into a set of smaller subproblems. The Domain Decomposition (DD) technique, based on the block generation algorithm introduced in Matekovits et al. (2007) and Francavilla et al. (2011), perfectly addresses our requirements (<b>see</b> <b>Section</b> <b>3.</b> <b>4</b> for details). In this chapter, a Grid Computing infrastructure is presented. This architecture allows parallel block execution by distributing tasks to nodes that belong to the Grid. The set of nodes is composed by physical machines and virtualized ones. This feature enables great flexibility and increase available computational power. Furthermore, the presence of virtual nodes allows a full and efficient Grid usage, indeed the presented architecture can be used by different users that run different applications...|$|E
30|$|The banking {{algorithms}} {{we propose}} {{are consistent with}} our model of partitioning the array space of signals into disjoint lattices (<b>see</b> <b>Section</b> <b>3).</b> For M< <b>4,</b> these algorithms are basically identical to the exploration algorithm presented in [5] since this approach yields optimal solutions. For M≥ 4, as the running times may be extremely large, we introduce a constraint that significantly reduces the exploration space: no SPM-assigned lattice can cross a bank boundary. This constraint ensures the effectiveness of our approach in {{point of view of}} speed and near-optimality of the results—as Example 4 will show.|$|R
40|$|The present work {{approached}} {{the most relevant}} aspects in the development process of solar cells and modules based on amorphous silicon (a-Si:H) thin films. Since semiconductor characteristics determine the photovoltaic device performance, {{the first stage of}} this research was focused on the search for the correlation between deposition conditions and the optoelectronic properties of a-Si:H and a-SiC:H alloys (see chapter 3). The study allowed the finding of the conditions for the obtainment of good quality a-Si:H/a-SiC:H at low temperatures, compatible with the use of low-cost flexible substrates. After this step, the effort was reoriented to the preparation and characterisation of a-Si:H p-i-n solar cells. The fabrication process of these devices was optimised for the particular case of a dual-chamber PECVD. The design of the a-Si:H p-i-n structure was progressively improved at different deposition temperatures based on a detailed analysis of the optoelectronic behaviour of the resulting solar cell (<b>see</b> <b>section</b> 4. 1). For this purpose, the standard techniques for device characterisation were complemented with a new I-V curve measurement setup that uses LEDs as source of monochromatic light of variable intensity (<b>see</b> <b>section</b> 2. <b>3.</b> <b>4).</b> The result of this evolution was solar cells with efficiencies close to state-of-the-art, which showed good homogeneity and reproducibility. Once enough technological maturity was reached, the work was focused on the implementation of some of the concepts that make a-Si:H technology of interest to the photovoltaic industry. Specifically, different tests of monolithic interconnection of solar cells were carried out, which allowed the fabrication of the first a-Si:H modules entirely developed with Spanish technology (<b>see</b> <b>section</b> 4. 1. 4). Subsequently, the work addressed the issue of cost reduction by means of substrates based on lower-price materials. Firstly, the effect on the device performance of the TCO material used as front electrode was analysed on glass substrates (<b>see</b> <b>section</b> 4. 2). The study made a comparative between different TCOs, focusing on the particular case of AZO and the problem of the fill factor loss. Secondly, the research {{approached the}} fabrication of flexible solar cells by using PET substrates, paying special attention to the solution of peel-off problem (<b>see</b> <b>section</b> <b>4.</b> <b>3).</b> The work concluded with the obtainment of the first fully operative flexible devices developed in CIEMAT. ...|$|R
40|$|My area of {{research}} centers around Computability Theory, a branch of Mathematical Logic. Inside computability theory, I have worked in various different areas. Topics that have particularly interested me are the shape of structure of the Turing Degrees, {{the structure of the}} linear orderings and the relation of embeddability, well-quasi-orderings, and hyperarithmetic theory. There is still part of my work does not fit in any of these categories. My work on the shape of structure of the Turing Degrees is described in Section 2 below. I have recently written a survey paper [Monc] {{on the history of the}} study of the Turing Degree Structure via embeddability results where I mention my contributions to the area. The research I have done on linear orderings and well-quasi-orderings is mostly from the viewpoints of Computable Mathematics and Reverse Mathematics. However, as it is sometimes the case, these proofs in Computable and Reverse Mathematics required a deeper understanding of the objects from classical mathematics: I have obtained interesting results purely on the structure of the embeddability relation on linear orderings. <b>See</b> <b>Sections</b> <b>3</b> and <b>4.</b> I have recently written a survey paper [Mone] about my results on linear orderings. Hyperarithmetic theory appear all over my work. I have written two papers which are about theorie...|$|R
40|$|The {{overarching}} goal of {{the project}} was to develop luminescent materials using aerosol processes for making improved LED devices for solid state lighting. In essence this means improving white light emitting phosphor based LEDs by improvement of the phosphor and phosphor layer. The structure of these types of light sources, displayed in Figure 1, comprises of a blue or UV LED under a phosphor layer that converts the blue or UV light to a broad visible (white) light. Traditionally, this is done with a blue emitting diode combined with a blue absorbing, broadly yellow emitting phosphor such as Y{sub 3 }Al{sub 5 }O{sub 12 }:Ce (YAG). A similar result may be achieved by combining a UV emitting diode and at least three different UV absorbing phosphors: red, green, and blue emitting. These emitted colors mix to make white light. The efficiency of these LEDs is based on the combined efficiency of the LED, phosphor, and the interaction between the two. The Cabot SSL project attempted to improve the over all efficiency of the LED light source be improving the efficiency of the phosphor and the interaction between the LED light and the phosphor. Cabot's spray based process for producing phosphor powders is able to improve the brightness of the powder itself by increasing the activator (the species that emits the light) concentration without adverse quenching effects compared to conventional synthesis. This will allow less phosphor powder to be used, and will decrease the cost of the light source; thus lowering the barrier of entry to the lighting market. Cabot's process also allows for chemical flexibility of the phosphor particles, which may result in tunable emission spectra and so light sources with improved color rendering. Another benefit of Cabot's process is the resulting spherical morphology of the particles. Less light scattering results when spherical particles are used in the phosphor layer (Figure 1) compared to when conventional, irregular shaped phosphor particles are used. This spherical morphology will result in better light extraction and so an improvement of efficiency in the overall device. Cabot is a 2. 5 billion dollar company that makes specialized materials using propriety spray based technologies. It is a core competency of Cabot's to exploit the spray based technology and resulting material/morphology advantages. Once a business opportunity is clearly identified, Cabot is positioned to increase the scale of the production to meet opportunity's need. Cabot has demonstrated the capability to make spherical morphology micron-sized phosphor powders by spray based routes for PDP and CRT applications, but the value proposition is still unproven for LED applications. Cabot believes that the improvements in phosphor powders yielded by their process will result in a commercial advantage over existing technologies. Through the SSL project, Cabot has produced a number of different compositions in a spherical morphology that may be useful for solid state lights, as well as demonstrated processes that are able to produce particles from 10 nanometers to 3 micrometers. Towards the end of the project we demonstrated that our process produces YAG:Ce powder that has both higher internal quantum efficiency (0. 6 compared to 0. 45) and external quantum efficiency (0. 85 compared to 0. 6) than the commercial standard (<b>see</b> <b>section</b> <b>3.</b> <b>4.</b> 4. 3). We, however, only produced these highly bright materials in research and development quantities, and were never able to produce high quantum efficiency materials in a reproducible manner at a commercial scale...|$|E
40|$|The project’s {{electrical}} system {{would consist of}} two key elements: (1) a collector system, which would collect energy generated at 575 volts from each wind turbine, transform the voltage to 34. 5 kV using a pad-mounted transformer, and deliver the energy via underground cables to (2) the project substation, which would further transform the energy delivered by the underground collector system from 34. 5 kV to 230 kV and deliver it, via new interconnection facilities to be built by BPA, to the adjacent existing BPA transmission line and into the regional transmission system. The BPA transmission lines are {{outside the scope of}} this application. Please <b>see</b> <b>Section</b> 2. <b>3.</b> <b>3.</b> <b>4</b> for a more detailed description of the collector system. No transmission facilities would be constructed by Whistling Ridge Energy LLC...|$|R
40|$|An n-dimensional complex {{manifold}} M {{is said to}} be (holomorphically) dominable by ^n {{if there}} is a map F:^n M which is holomorphic such that the Jacobian determinant (DF) is not identically zero. Such a map F is called a dominating map. In this paper, we attempt to classify algebraic surfaces X which are dominable by ^ 2 using a combination of techniques from algebraic topology, complex geometry and analysis. One of the key tools in the study of algebraic surfaces is the notion of Kodaira dimension (defined in section 2). By Kodaira's pioneering work and its extensions, an algebraic surface which is dominable by ^ 2 must have Kodaira dimension less than two. Using the Kodaira dimension and the fundamental group of X, we succeed in classifying algebraic surfaces which are dominable by ^ 2 except for certain cases in which X is an algebraic surface of Kodaira dimension zero and the case when X is rational without any logarithmic 1 -form. More specifically, in the case when X is compact (namely projective), we need to exclude only the case when X is birationally equivalent to a K 3 surface (a simply connected compact complex surface which admits a globally non-vanishing holomorphic 2 -form) that is neither elliptic nor Kummer (<b>see</b> <b>sections</b> <b>3</b> and <b>4</b> for the definition of these types of surfaces). Comment: 39 page...|$|R
40|$|Introduction This thesis {{consists}} of three papers, [B 1], [B 2] and [A-B], where we use integral formulas in several complex variables in dioeerent ways. In [B 1] we use an integral representation formula for holomorphic functions to obtain an inversion formula for a generalization of the Fantappi# transform. This is discussed in section 1. In [B 2] we use another representation formula for holomorphic functions to solve a certain division problem. <b>See</b> <b>section</b> 2, where we in addition give the proof of the main theorem in a simple special case. In [A-B] we construct integral operators that solve the ¯ @ equation. For a discussion, <b>see</b> <b>section</b> <b>3.</b> In <b>section</b> <b>4</b> we give some examples of the various kinds of domains that are treated in the three papers. 1. Quadratic convexity The sets we consider in this section are always open or compact. 1. 1. Various notions of convexity. Let us recall two possible deønitions of convexity in<...|$|R
40|$|Section 2. 5 {{discussed}} {{a combination of}} management approaches which could be effective in reducing damage caused by salinity in Moora. This section describes a computer groundwater modelling study that aimed to assess the impacts of a selection of possible strategies. Note that the modelling was based on limited data {{and a large number}} of assumptions and the results should be viewed with great caution (<b>see</b> warnings in <b>Section</b> <b>3.</b> <b>4).</b> Firstly, a suitable conceptual model was constructed based on the information gained from the drilling investigation and the pumping test, together with topographic and climatic data. This conceptualisation was adapted to the groundwater simulation program MODFLOW (McDonald and Harbaugh 1988) coupled with the pre- and post-processor PMWIN version 5. 0 (Chiang and Kinzelbach 1998) and was calibrated in steady-state against observed groundwater levels. The model was then used to simulate the effects of three different strategies: 'do nothing differently', groundwater abstraction and revegetation...|$|R
40|$|TMDL) {{allocations}} {{account for}} all significant sources of each impairing pollutant (CFR 2010). This technical memorandum identifies the significant point sources of sediment in the Potomac River Washington County watershed. Detailed allocations are provided for those point sources included within the Process Water Waste Load Allocation (WLA) and National Pollutant Discharge Elimination System (NPDES) Regulated Stormwater WLA of the Potomac River Washington County Watershed Sediment TMDL. These allocations are {{designed to meet the}} TMDL threshold. The State reserves the right to allocate the TMDLs among different sources in any manner that protects aquatic life from sediment related impacts. The Potomac River Washington County Watershed Sediment TMDL is presented in terms of an average annual load established to ensure the support of aquatic life. The watershed was evaluated using a single TMDL segment (<b>See</b> <b>Sections</b> 2. <b>3,</b> 2. <b>4,</b> and 4. 2 – 4. 6 of the main report for further details). It was determined that the mainstem Potomac River in Washington County is not impaired by sediment (<b>See</b> <b>Sections</b> 2. <b>3</b> - 2. <b>4).</b> Therefore, this sediment TMDL will be restricted to the tributaries in the watershed draining to the Potomac River and will exclude the mainstem of the Potomac River itself...|$|R
40|$|AbstractWell-known {{interpretations of}} Heyting's {{arithmetic}} of all finite types are the Diller-Nahm λ-interpretation [1] and Kreisel's modified realizability, subsequently called mr-interpretation [4]. For both interpretations one can define hybrids λ q resp. mq. In Section 4 {{a chain of}} interpretations—called M-interpretations—is defined (it was introduced in [6], filling the “gap” between λ-interpretation and mr-interpretation. In this paper it is shwon {{that it is possible}} to prove in one stroke the soundness resp. characterization theorems for all interpretations of HAω 〈〉 (Heyting's arithmetic of all finite types with functionals for coding finite sequences). This is done by means of interpretations of systems which contain set-symbols. For these so called M-interpretations, soundness-resp. characterization theorems can be proved simultaneously (Theorem 2. 51. Special translations of set symbols and of the formula (λωϵW) A — this means, special decisions about the size of the set W; <b>see</b> <b>Sections</b> <b>3</b> and <b>4</b> — yield the corresponding results for all interpretations of HAω〈〉 mentioned. The terminology of set theoretical language — we consider an extension of HAω〈〉 by an extensively weak fragment only, which leads to a conservative extension of HAω〈〉 — is of good use for studying realizing terms of different interpretations: if HAω ⊢A, AM∃υ ∀w AM, and ⊢AM[tM, w] by soundness theorem for M-interpretations, there exists a simple operation which maps v̄ to t̄mr, the realizing term for modified realizability. For interpretations of Heyting's arithmetic — λ-interpretation. M-interpretations and mr-interpretation — this leads to the following stability result for existence theorems: if ∃λ A and t^ resp. tMM is the term computed by λ-interpretation. resp. M-interpretation, with ∃A[tM], then — using extensional equality and ω-rule for equations — we can prove that tλ = tM = tmr (Section 5) ...|$|R
40|$|In {{this paper}} we shall study smooth submanifolds {{immersed}} in a k-step Carnot group G of homogeneous dimension Q. Among other results, we shall prove an isoperimetric inequality for {{the case of a}} C^ 2 -smooth compact hypersurface S with - or without - boundary ∂ S; S and ∂ S are endowed with their homogeneous measures, actually equivalent to the intrinsic (Q- 1) -dimensional and (Q- 2) -dimensional Hausdorff measures with respect to some homogeneous metric ϱ on G; <b>see</b> <b>Section</b> 5. This generalizes a classical inequality, involving the mean curvature of the hypersurface, proven by Michael and Simon [63] and, independently by Allard [1]. In particular, from this result one may deduce some related Sobolev-type inequalities; <b>see</b> <b>Section</b> 7. The strategy of the proof is inspired by the classical one. In particular, we shall begin by proving some linear isoperimetric inequalities. Once this is proven, one can deduce a local monotonicity formula and then conclude the proof by a covering argument. We stress however that there are many differences, due to our different geometric setting. Some of the tools which have been developed ad hoc in this paper are, in order, a "blow-up" theorem, which also holds for characteristic points, and a smooth Coarea Formula for the HS-gradient; <b>see</b> <b>Section</b> <b>3</b> and <b>Section</b> <b>4.</b> Other tools are the horizontal integration by parts formula and the 1 st variation of the H-perimeter already developed in [68], [69], and here generalized to hypersurfaces having non-empty characteristic set. Some natural applications of these results are in the study of minimal and constant horizontal mean curvature hypersurfaces. Moreover we shall prove some purely horizontal, local and global Poincaré-type inequalities as well as some related facts and consequences; <b>see</b> <b>Section</b> 4 and Section 5. Comment: 61 page...|$|R
40|$|Section 2. 4 {{discussed}} {{a combination of}} management approaches which could be effective in reducing damage caused by salinity in Perenjori. This section describes a computer groundwater model that was constructed to assess the impacts of a selection of possible strategies. Note that the modelling was based on limited data {{and a large number}} of assumptions and the results should be viewed with great caution (<b>see</b> warnings in <b>Section</b> <b>3.</b> <b>4).</b> Firstly, a suitable conceptual model was constructed {{based on the results of}} the drilling investigation (Section 2) and topographic and climatic data. This conceptualisation was adapted to the groundwater flow simulation program MODFLOW (McDonald and Harbaugh 1988) coupled with the pre- and postprocessor Visual MODFLOW Version 2. 8 (Waterloo Hydrogeologic 2000) and the modelled head was calibrated against observed groundwater levels. The model was then used to simulate the effects of three different strategies: 'do nothing differently' to determine the impacts of inaction, groundwater pumping and groundwater drainage. <b>Sections</b> <b>3.</b> 1 and 3. 2 describe the construction of the conceptual and computer models and the calibration of the computer model. The strategy simulations and their results are presented in <b>Section</b> <b>3.</b> 3. Please note the warnings in <b>Section</b> <b>3.</b> <b>4</b> when considering the results. 3. 1 Model construction and conceptualisation Conceptually, the groundwater model consisted of three layers: unconfined colluvium, overlying leaky pallid zone, overlying leaky or semi-confined saprolite of the weathered granitic basement. Inflow and outflow boundaries of the model domain are illustrated in Figure 3 - 1. The model domain covered an area of 3. 24 km 2 and incorporated the majority of the bores in the townsite. Each cell in the domain was 20 m 2, resulting in 90 columns and 90 rows, giving 8100 cells. The top of the unconfined layer was taken as the land surface, which was extracted from 2 m-contour digital elevation models (DEMs) for the catchment (map sheet...|$|R
40|$|The context which {{occasions}} {{this study}} is the established multicultural reality of India and the developing multicultural spectrum of Scotland (<b>see</b> <b>Sections</b> <b>3.</b> 2, <b>3.</b> <b>3,</b> <b>3.</b> <b>4).</b> The aims, objectives and/ or policies of the two governments (<b>see</b> <b>Section</b> <b>3.</b> 5) - and the respective boards of education in the two countries (<b>see</b> <b>Section</b> <b>3.</b> 6) - provide the high idealistic foundations that such institutions stand for. The study uses the discourse analysis method (<b>see</b> <b>Section</b> 4. 2 for definition) to evaluate how far the texts used by schools in India and Scotland reflect such high ideals – how far the texts contain multicultural markers (<b>see</b> <b>Section</b> 5. 4 for definition) and how these markers are exploited through the exercises. The study takes into account even the materials containing multicultural markers that may not {{have anything to do with}} social injustice. It is a case study because, due to lack of time and space, the study takes into account only one representative book each from India and Scotland. The study is an endeavour to incorporate two features influenced by Norman Fairclough’s ideas (<b>see</b> <b>Section</b> 5. 2 and Section 1. 5, respectively) : one, a selective discourse analysis approach; and two, an interdisciplinary approach – interdisciplinary in the sense that it integrates discourse analysis, cultural studies and history. As part of the second approach, this study includes the following: first, critical discourse analysis (<b>see</b> <b>Section</b> <b>4.</b> <b>3</b> for definition) of a representative text – one out of the 108 texts from the two books; and second, two chapters – Chapters 2 and 3 – which elucidate on the following ideas and issues: multiculturalism; ethnicity, nation and state; ethnic compositions of the two states; history of multiculturalism in India and Scotland; the policies towards multiculturalism in India and Scotland; and objectives of the two respective boards of education. My research question is answered (<b>see</b> <b>Section</b> 5. 1 for research question; <b>see</b> <b>Section</b> 7. 2 for its answer) and my hypothesis is tested (<b>see</b> <b>Section</b> 5. 1 for hypothesis; <b>see</b> <b>Section</b> 7. 2 for its test) on the basis of the evaluation of the texts by using the discourse analysis method. The study incorporates critical discourse analysis of a representative text in order to complement the findings of the discourse analysis method and to demonstrate how various aspects of a text can be exploited to raise the awareness of learners about cultures and related social issues. The discussion that follows the results incorporates ethics, analysis of the findings, limitations of this study, scope for further research in the area and implications of the research...|$|R
40|$|We {{statistically}} {{quantify the}} amount of substructure in the Milky Way stellar halo using a sample of 4568 halo K giant stars at Galactocentric distances ranging over 5 - 125 kpc. These stars have been selected photometrically and confirmed spectroscopically as K giants from the Sloan Digital Sky Survey's SEGUE project. Using a position-velocity clustering estimator (the 4 distance) and {{a model of a}} smooth stellar halo, we quantify {{the amount of}} substructure in the halo, divided by distance and metallicity. Overall, we find that the halo as a whole is highly structured. We also confirm earlier work using BHB stars which showed that there is an increasing amount of substructure with increasing Galactocentric radius, and additionally find that the amount of substructure in the halo increases with increasing metallicity. Comparing to resampled BHB stars, we find that K giants and BHBs have similar amounts of substructure over equivalent ranges of Galactocentric radius. Using a friends-of-friends algorithm to identify members of individual groups, we find that a large fraction (~ 33 %) of grouped stars are associated with Sgr, and identify stars belonging to other halo star streams: the Orphan Stream, the Cetus Polar Stream, and others, including previously unknown substructures. A large fraction of sample K giants (more than 50 %) are not grouped into any substructure. We find also that the Sgr stream strongly dominates groups in the outer halo for all except the most metal-poor stars, and suggest that this {{is the source of the}} increase of substructure with Galactocentric radius and metallicity. Comment: 49 pages; 24 figures. Accepted to ApJ; revisions made due to the referee's comments, notably an updated result: K giants and BHBs have similar substructure over similar distance ranges; previously we concluded that K giants were more highly structured (<b>see</b> <b>Section</b> <b>4.</b> <b>3).</b> Additionally, we have updated the discussion on false positive groups and group membership trends in Section 5. ...|$|R
40|$|This article {{initiated}} an area {{of research}} called replica theory. The replica theory not only provided answers to questions {{in the field of}} spin glasses, but also provided answers for the problems with the analysis of neural networks. Replica theory was used, for example, to calculate the maximum number of stable patterns a certain pattern recognizing neural network can harbor (<b>see</b> <b>section</b> 1. <b>4.</b> <b>3</b> on the Hopfield model). In the next chapter we will use replica theory extensively. Because most of the replica theory and applications were developed for use in the theory of spin glasses in the context of Ising models, we will often borrow notation and nomenclature from the spin glass models. We write as often the word spins as we use neurons, or we use the word couplings when we discuss the weights or synaptic strengths. We will never talk about thresholds, but always use the term external field. As the reader has noticed, we already have used the physical term temperature for the amount of errors neurons make in firing or in learning, although we did not immerse the neural network in a heat bath. And we used the physical concept of energy. All this is the heritage of spin glass theory. The model we construct and analyze in this thesis is inspired by neural networks and not on spin glasses. However, as we further develop the model in this chapter, we will see that the model 2 The (more) correct quantum mechanical model used for interacting spins represents the magnetic moment of unit i by a linear combination of the three Pauli-spin matrices, coded into the spin-vector ## i. The Hamiltonian of an array of spins is the operator H = P i<j J ij ## i ## j + 0 ## # B, where 0 is the magnetic moment and # B the magnetic field. In the Ising-model the ## are taken to be un [...] ...|$|R
50|$|<b>See</b> <b>section</b> <b>3</b> of the Defamation Act (Northern Ireland) 1955.|$|R
3000|$|... -harmonic functions. <b>See</b> <b>Section</b> <b>3</b> for a {{detailed}} discussion. As an application, we show that a result concerning removable singularities for [...]...|$|R
30|$|This {{disclosure}} {{corresponds to}} the most detailed occupational classification available, the ISCO– 08 classification at <b>4</b> digits. <b>See</b> <b>Section</b> <b>3</b> for further details.|$|R
40|$|This {{document}} presents details concerning analytical derivations and numerical {{experiments that}} support the claims made in the main text ‘Message Passing Algorithms for Compressed Sensing’, submitted for publication in the Proceedings of the National Academy of Sciences, USA. Hereafter ’main text’. One can find here: • Derivations of explicit Formulas for the MSE Map, and the optimal thresholds; <b>see</b> <b>Section</b> <b>3</b> below. • Proof of Theorem 1; <b>see</b> <b>Section</b> <b>3</b> below. • Proof of concavity of the MSE Map, <b>see</b> again <b>Section</b> <b>3</b> below. • Explanation {{of the connection between}} Minimax Thresholding, Minimax Risk, and rigorous proof of formula [19] in the main text; <b>see</b> <b>Section</b> 4 below, Theorem 4. 2. • Formulas for the rate exponent b of Theorem 2 in the main text, expressed in terms of the minimax threshold risk; <b>see</b> <b>Section</b> 5 below...|$|R
3000|$|... {{are also}} {{expected}} to have an increasing trend with the modulation frequency, owing to the low-pass filter behavior of the optical noise (<b>see</b> <b>Sections</b> <b>3</b> and 7).|$|R
3000|$|... used in {{the current}} work is found empirically by {{choosing}} the duration that yields that best recognition performance across our solo and RWC datasets (<b>see</b> <b>section</b> <b>3).</b>|$|R
