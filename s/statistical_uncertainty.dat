1190|1448|Public
25|$|Absolute dates, {{subject to}} a <b>statistical</b> <b>uncertainty,</b> are {{determined}} through radiometric dating using isotope ratios such as 40Ar/39Ar dating, {{which can be used}} to identify the date of solidifying basalt. In the CRBG deposits 40Ar, which is produced by 40K decay, only accumulates after the melt solidifies.|$|E
2500|$|On one [...] "alternative" [...] {{there is}} no disagreement: Fisher himself said, [...] "In {{relation}} to the test of significance, we may say that a phenomenon is experimentally demonstrable when {{we know how to}} conduct an experiment which will rarely fail to give us a statistically significant result." [...] Cohen, an influential critic of significance testing, concurred, [...] "... don't look for a magic alternative to NHST ... It doesn't exist." [...] "... given the problems of statistical induction, we must finally rely, as have the older sciences, on replication." [...] The [...] "alternative" [...] to significance testing is repeated testing. The easiest way to decrease <b>statistical</b> <b>uncertainty</b> is by obtaining more data, whether by increased sample size or by repeated tests. Nickerson claimed to have never seen the publication of a literally replicated experiment in psychology. An indirect approach to replication is meta-analysis.|$|E
5000|$|The <b>statistical</b> <b>uncertainty</b> is the {{standard}} deviation σ, satisfying the equation ...|$|E
40|$|We {{study the}} {{influence}} of measured high cumulants of conserved charges on their associated <b>statistical</b> <b>uncertainties</b> in relativistic heavy-ion collisions. With a given number of events, the measured cumulants randomly fluctuate with an approximately normal distribution, while the estimated <b>statistical</b> <b>uncertainties</b> {{are found to be}} correlated with corresponding values of the obtained cumulants. Generally, with a given number of events, the larger the cumulants we measure, the larger the <b>statistical</b> <b>uncertainties</b> that are estimated. The error-weighted averaged cumulants are dependent on statistics. Despite this effect, however, it is found that the three sigma rule of thumb is still applicable when the statistics are above one million. Comment: 5 pages, 3 figures. To be published in Chinese Physics...|$|R
40|$|We {{investigate}} {{the impact of}} nonaxisymmetric structure on estimates of galaxy inclinations and position angles. A new minimization technique is used to obtain estimates of inclination and position angle from a global fit to either photometric or kinematic data. We discuss possible systematic uncertainties which are much larger than <b>statistical</b> <b>uncertainties.</b> Our investigation reveals that systematic uncertainties associated with fitting photometric data dominate the formal <b>statistical</b> <b>uncertainties.</b> For our sample of inclined galaxies, we estimate that nonaxisymmetric features introduce inclination and position angle uncertainties of ≈ 5 ◦, on average. The magnitudes of these uncertainties weaken the arguments for intrinsically elliptical galaxy disks. Subject headings: galaxies:fundamental parameters —galaxies:photometry — galaxies:kinematics and dynamics 1...|$|R
40|$|Extreme {{response}} {{is an important}} design variable for wind turbines. The <b>statistical</b> <b>uncertainties</b> concerning the extreme response distribution are simulated here with data concerning physical characteristics obtained from measurements. The extreme responses are the flap moment at the blade root and the overturning moment of the support structure of an offshore wind turbine situated in the North Sea. The <b>statistical</b> <b>uncertainties</b> concern {{the choice of the}} distribution model and uncertainties concerning the distribution parameters. The uncertainties are treated with Bayesian analysis. The inclusion of the uncertainties has only marginal effect for the calculated long-term estimates of extreme responses when non-informative priors for the distribution parameters are used. The inclusion of uncertainties may have larger effect if data concerning the moments are obtained from measurement...|$|R
5000|$|The term cosmic {{variance}} is the <b>statistical</b> <b>uncertainty</b> {{inherent in}} {{observations of the}} universe at extreme distances. It has three different but closely related meanings: ...|$|E
5000|$|... with <b>statistical</b> <b>uncertainty</b> (+0.69−0.75, +0.47−0.47, +0.37−0.36) km/s and {{systematic}} uncertainty (1, 2, 0.5) km/s. (Note that V is 7 km/s larger than estimated in 1999 by Dehnen et al.) ...|$|E
50|$|It is {{recommended}} that the confidence interval is plotted along with the data, such that the reader of the plot is able {{to be aware of}} the <b>statistical</b> <b>uncertainty</b> of the values.|$|E
40|$|Monte Carlo {{methods are}} {{beginning}} {{to be used for}} three-dimensional fuel depletion analyses to compute various quantities of interest, including isotopic compositions of used fuel. 1 The TRITON control module, available in the SCALE 5. 1 code system, can perform three dimensional (3 -D) depletion calculations using either the KENO V. a or KENO-VI Monte Carlo transport codes, as well as the two-dimensional (2 - D) NEWT discrete ordinates code. For typical reactor systems, the neutron flux is not spatially uniform. For Monte Carlo simulations, this results in non-uniform <b>statistical</b> <b>uncertainties</b> in the computed reaction rates. For spatial regions where the flux is low, e. g., axial fuel ends, computed quantities, such as isotopic compositions, may have large <b>statistical</b> <b>uncertainties.</b> However, in currently available Monte Carlo depletion codes these <b>statistical</b> <b>uncertainties</b> are not calculated or reported to the user. Consequently, users have no indication of the fidelity of their results in such regions, which can be a significant impediment to the effective use of Monte Carlo methods for design and optimization studies of advanced fuel designs. Additionally, for applications such as criticality safety of used nuclear fuel, the lower depleted end regions tend to dominate the reactivity, and hence must be accurately and/or conservatively represented...|$|R
40|$|Intermediate Mass Black Holes (IMBHs) are {{candidates}} to seed the Supermassive Black Holes (SMBHs), and some could still wander in the Galaxy. In {{the context of}} annihilating dark matter (DM), {{they are expected to}} drive huge annihilation rates, and could therefore significantly enhance the primary cosmic rays (CRs) expected from annihilation of the DM of the Galactic halo. In this proceeding (the original paper is Brun et al. 2007), we briefly explain the method to derive estimates of such exotic contributions to the anti-proton and positron CR spectra, and the associated <b>statistical</b> <b>uncertainties</b> connected to the properties of IMBHs. We find boost factors of order $ 10 ^ 4 $ to the exotic fluxes, but associated with very large <b>statistical</b> <b>uncertainties.</b> Comment: Proceeding of the RICAP 07 conference (Roma, Italy, June 2007...|$|R
40|$|International audienceWe {{provide a}} {{pedagogical}} {{introduction to the}} two main variants of real-space quantum Monte Carlo methods for electronic-structure calculations: variational Monte Carlo (VMC) and diffusion Monte Carlo (DMC). Assuming no prior knowledge on the subject, we review in depth the Metropolis-Hastings algorithm used in VMC for sampling the square of an approximate wave function, discussing details important for applications to electronic systems. We also review in detail the more sophisticated DMC algorithm within the fixed-node approximation, introduced to avoid the infamous Fermionic sign problem, which allows one to sample a more accurate approximation to the ground-state wave function. Throughout this review, we discuss the statistical methods used for evaluating expectation values and <b>statistical</b> <b>uncertainties.</b> In particular, we show how to estimate nonlinear functions of expectation values and their <b>statistical</b> <b>uncertainties...</b>|$|R
5000|$|... 35.4% {{of females}} {{consider}} {{their experience of}} a violent interpersonal offence a crime, 40.9%(with some <b>statistical</b> <b>uncertainty)</b> consider the event to be 'wrong, but not a crime' and 23.7% consider it to be 'just something that happens'.|$|E
50|$|Absolute dates, {{subject to}} a <b>statistical</b> <b>uncertainty,</b> are {{determined}} through radiometric dating using isotope ratios such as 40Ar/39Ar dating, {{which can be used}} to identify the date of solidifying basalt. In the CRBG deposits 40Ar, which is produced by 40K decay, only accumulates after the melt solidifies.|$|E
50|$|In {{particle}} physics, a B-factory, {{or sometimes}} a beauty factory, is a particle collider experiment designed {{to produce and}} detect {{a large number of}} B mesons so that their properties and behaviour can be measured with small <b>statistical</b> <b>uncertainty.</b> Tauons and D mesons are also copiously produced at B-factories, which allows precise studies of their properties.|$|E
40|$|Proceeding of the RICAP 07 conference (Roma, Italy, June 2007) Preprint {{submitted}} to ElsevierInternational audienceIntermediate Mass Black Holes (IMBHs) are candidates to seed the Supermassive Black Holes (SMBHs), and some could still wander in the Galaxy. In {{the context of}} annihilating dark matter (DM), {{they are expected to}} drive huge annihilation rates, and could therefore significantly enhance the primary cosmic rays (CRs) expected from annihilation of the DM of the Galactic halo. In this proceeding (the original paper is Brun et al. 2007), we briefly explain the method to derive estimates of such exotic contributions to the anti-proton and positron CR spectra, and the associated <b>statistical</b> <b>uncertainties</b> connected to the properties of IMBHs. We find boost factors of order $ 10 ^ 4 $ to the exotic fluxes, but associated with very large <b>statistical</b> <b>uncertainties...</b>|$|R
40|$|We give a brief {{review on}} the {{development}} of phenomenological NN interactions and the corresponding quantification of <b>statistical</b> <b>uncertainties.</b> We look into the uncertainty of effective interactions broadly used in mean field calculations through the Skyrme parameters and effective field theory counter-terms by estimating both <b>statistical</b> and systematic <b>uncertainties</b> stemming from the NN interaction. We also comment on the role played by different fitting strategies on the light of recent developments. Comment: 16 pages, 1 figur...|$|R
40|$|Upcoming {{and ongoing}} large area weak lensing surveys will also {{discover}} large samples of galaxy clusters. Accurate and precise masses of galaxy clusters are of major importance for cosmology, for example, in establishing well calibrated observational halo mass functions for comparison with cosmological predictions. We investigate {{the level of}} <b>statistical</b> <b>uncertainties</b> and sources of systematic errors expected for weak lensing mass estimates. Future surveys that will cover large areas on the sky, such as Euclid or LSST and to lesser extent DES, will provide the largest weak lensing cluster samples with {{the lowest level of}} statistical noise regarding ensembles of galaxy clusters. However, the expected low level of <b>statistical</b> <b>uncertainties</b> requires us to scrutinize various sources of systematic errors. In particular, we investigate the bias due to cluster member galaxies which are erroneously treated as background source galaxies due to wrongly assigned photometric redshifts. We find that this effect is significant when referring to stacks of galaxy clusters. Finally, we study the bias due to miscentring, i. e., the displacement between any observationally defined cluster centre and the true minimum of its gravitational potential. The impact of this bias might be significant with respect to the <b>statistical</b> <b>uncertainties.</b> However, complementary future missions such as eROSITA will allow us to define stringent priors on miscentring parameters which will mitigate this bias significantly. Comment: 14 pages, 7 figures; accepted for publication in MNRA...|$|R
50|$|A good set of {{engineering}} tolerances in a specification, by itself, {{does not imply}} that compliance with those tolerances will be achieved. Actual production of any product (or operation of any system) involves some inherent variation of input and output. Measurement error and <b>statistical</b> <b>uncertainty</b> are also present in all measurements. With a normal distribution, the tails of measured values may extend well beyond plus and minus three standard deviations from the process average. Appreciable portions of one (or both) tails might extend beyond the specified tolerance.|$|E
5000|$|According to the 2005 study More {{evidence}} for non-maternal inheritance of mitochondrial DNA?, heteroplasmy is a [...] "newly discovered form of inheritance for mtDNA. Heteroplasmy introduces slight <b>statistical</b> <b>uncertainty</b> in normal inheritance patterns." [...] Heteroplasmy {{may result from}} a mutation during development which is propagated to only {{a subset of the}} adult cells, or may occur when two slightly different mitochondrial sequences are inherited from the mother as a result of several hundred mitochondria being present in the ovum. However, the 2005 study states: ...|$|E
5000|$|The proportionator applies PPS to {{counting}} cells. The PPS {{is employed}} to gain efficiency in the sampling, {{and not to}} produce a weighted estimate, such as a volume weighted estimate. The optical fractionator is the older standard for estimating the number of cells in an unbiased manner. The optical fractionator, and other sampling methods, has some <b>statistical</b> <b>uncertainty.</b> This uncertainty {{is due to the}} variance of the sampling even though the result is unbiased. The efficiency of the sampling can be determined by use of the coefficient of error, or CE. This value describes the variance of the sampling method. Often, biological sampling is done at a CE of [...]05.|$|E
40|$|Abstract. We have {{quantified}} the <b>statistical</b> <b>uncertainties</b> of the low-energy coupling-constants (LECs) of an optimized nucleon-nucleon (NN) interaction from chiral {{effective field}} theory (χEFT) at next-to-next-to-leading order (NNLO). In addition, we have propagated {{the impact of the}} uncertainties of the LECs to two-nucleon scattering phase shifts, effective range parameters, and deuteron observables...|$|R
40|$|In vommenly used estimators, e. g. kalman filter, only <b>statistical</b> <b>uncertainties</b> but no {{errors in}} the model {{structure}} are taken into account. The presented approach also uses information about {{the uncertainty of the}} model to increase the accuracy of the estimation. The algorithm was validated by experiments with a 40 Ah Li-Ion cell...|$|R
40|$|We present {{measured}} dielectron production {{cross sections}} for Ca+Ca, C+C, He+Ca, and d+Ca reactions at 1. 0 A GeV. <b>Statistical</b> <b>uncertainties</b> and systematic effects {{are smaller than}} in previous DLS nucleus-nucleus data. For pair mass 0. 5 GeV/c 2 the Ca+Ca to C+C cross section ratio is significantly larger than the ratio of ApAt values...|$|R
5000|$|Aleatoric {{uncertainty}}: Aleatoric {{uncertainty is}} also known as <b>statistical</b> <b>uncertainty,</b> and is representative of unknowns that differ each time we run the same experiment. For example, a single arrow shot with a mechanical bow that exactly duplicates each launch (the same acceleration, altitude, direction and final velocity) will not all impact the same point on the target due to random and complicated vibrations of the arrow shaft, the knowledge of which cannot be determined sufficiently to eliminate the resulting scatter of impact points. The argument here is obviously in the definition of [...] "cannot". Just because we cannot measure sufficiently with our currently available measurement devices does not preclude necessarily the existence of such information, which would move this uncertainty into the below category.|$|E
50|$|GEWEX Cloud System Study (GCSS) {{task is to}} individualize {{modeling}} {{for different}} types of cloud systems. GCSS identifies 5 types of cloud systems:boundary layer, cirrus, extra tropical layer, precipitating convective, and polar. These cloud systems are generally {{too small to be}} rationalized in large scale climate modeling, this results in inadequate development of equations resulting in greater <b>statistical</b> <b>uncertainty</b> in results. In order to rationalize these process the study observes cloud systems at single fixed positions on earth in order to better estimate their parameters. These four areas are: Azores and Madeira Islands, Barbados, Equatorial Western Pacific, and Atlantic Tropics. The initial data collection is complete, methods developed for land and aircraft based observations can be compared with satellite observations to that better models of cloud system identification can be made at smaller scales.|$|E
5000|$|On one [...] "alternative" [...] {{there is}} no disagreement: Fisher himself said, [...] "In {{relation}} to the test of significance, we may say that a phenomenon is experimentally demonstrable when {{we know how to}} conduct an experiment which will rarely fail to give us a statistically significant result." [...] Cohen, an influential critic of significance testing, concurred, [...] "... don't look for a magic alternative to NHST hypothesis significance testing ... It doesn't exist." [...] "... given the problems of statistical induction, we must finally rely, as have the older sciences, on replication." [...] The [...] "alternative" [...] to significance testing is repeated testing. The easiest way to decrease <b>statistical</b> <b>uncertainty</b> is by obtaining more data, whether by increased sample size or by repeated tests. Nickerson claimed to have never seen the publication of a literally replicated experiment in psychology. An indirect approach to replication is meta-analysis.|$|E
40|$|This report {{describes}} a sensitivity {{study of the}} measurement of W-boson charge and forward-backward asymmetries in p-p collision events produced at a center-of-mass energy of 13 TeV. The <b>statistical</b> <b>uncertainties</b> for the measurement of both observables {{as a function of}} y_ℓℓ, as expected from the analysis of data collected in 2015 at √(13) with the ATLAS detector, are presented...|$|R
40|$|We {{evaluate}} fπ/mρ, fK/mρ, 1 /fρ, and mφ/(fφmρ), extrapolated {{to physical}} quark mass, zero lattice spacing and infinite volume, for lattice QCD with Wilson quarks in the valence (quenched) approximation. The predicted ratios differ from experiment by amounts ranging from 12 % to 17 % equivalent to between 0. 9 and 2. 8 times the corresponding <b>statistical</b> <b>uncertainties.</b> ...|$|R
40|$|The γ‐ray {{telescope}} COMPTEL onboard GRO {{has so far}} located 6 gamma‐ray bursts {{which occurred}} in its ∼ 1 sr field of view. The positions of the sources were derived by the maximum‐entropy method. Systematic and <b>statistical</b> <b>uncertainties</b> for the four strongest bursts are approximately 1 ° to 2 ° and can be reduced in future analysis...|$|R
40|$|<b>Statistical</b> <b>uncertainty</b> of {{different}} filtration techniques for market network analysis is studied. Two measures of <b>statistical</b> <b>uncertainty</b> are discussed. One {{is based on}} conditional risk for multiple decision statistical procedures and another one is based on average fraction of errors. It is shown that for some important cases the second measure is a particular case of the first one. <b>Statistical</b> <b>uncertainty</b> for some popular market network structures is analyzed. Results of numerical evaluation of <b>statistical</b> <b>uncertainty</b> for minimum spanning tree, market graph, maximum cliques and maximum independent sets are given. The most stable structures are derived...|$|E
40|$|Health {{economic}} modelling studies {{are of interest}} to many parties with different responsibilities and diverging interests. Therefore, {{it is obvious that}} recognising the relevance of <b>statistical</b> <b>uncertainty</b> and dealing with it appropriately are required to obtain unbiased results from health {{economic modelling}} studies, especially when those data are being used for reimbursement decisions. In this manuscript we explore the relevance of the incorporation of <b>statistical</b> <b>uncertainty</b> in a health economic model and identify various types of <b>statistical</b> <b>uncertainty.</b> The concepts were applied to a hypothetical Markov model for a hypothetical antiparkinsonian (AP) product. The method was based on the incorporation of probability distributions in the input variables using a second-order Monte Carlo simulation and the definition of minimum relevant differences for clinical and economic input variables and outcomes. Our paper shows that the outcomes of a health economic model might be severely biased when <b>statistical</b> <b>uncertainty</b> is not taken into account, which justifies the need for the incorporation of <b>statistical</b> <b>uncertainty</b> in a health economic model. Modelling, Statistics...|$|E
40|$|Monte Carlo (MC) based dose {{calculation}} methods trade-off accuracy {{at the expense}} of computational time, which is, correlated to the user input values of <b>statistical</b> <b>uncertainty</b> and pixel spacing (1). It was first hinted by low et. al. that noise generated within either the calculated or measured plan distributions can affect the result of the plan verification by method of ‘Gamma Index Analysis’(GI) (2). The purpose of this research experiment is to investigate a possible correlation between added noise from increasing MC <b>statistical</b> <b>uncertainty</b> and increasing the odds of a plan passing the GI verification criteria. For this research experiment, we calculated 10 head and neck radiation therapy treatment plans using the MC dose calculation method within Monaco TPS. We varied the <b>statistical</b> <b>uncertainty</b> values from 5 %, 3 %, 1 % and 0. 25 % and varied the voxel size values from 3 mm, 2 mm and 1 mm. The treatment plans were then administered on an Elekta Versa linear accelerator and measured using Mapcheck dose measurement device. Each plan was evaluated for clinical pass/fail using the GI Analysis with criteria 3 %/ 3 mm and 2 %/ 2 mm. For 1 mm voxel size, 3 %/ 3 mm GI, there was an increase in average gamma pass rates from 98. 91 % calculated at 0. 5 % <b>statistical</b> <b>uncertainty</b> to 99. 61 % calculated at 5 % <b>statistical</b> <b>uncertainty.</b> For 1 mm voxel size, 2 %/ 2 mm GI, there was an increase in average gamma pass rates from 97. 02 % calculated at 0. 5 % <b>statistical</b> <b>uncertainty</b> to 98. 80 % calculated at 5 % <b>statistical</b> <b>uncertainty.</b> At 2 mm and 3 mm voxel sizes, there was not a clear demonstrable increase in average gamma pass rates. The experimental results conclude that the user must be careful when selecting a <b>statistical</b> <b>uncertainty</b> prior to performing a MC dose calculation. The input of a high <b>statistical</b> <b>uncertainty</b> does not lead to more points failing the GI, but paradoxically, can increase the chances that the evaluated radiation therapy plan will pass the acceptance evaluation...|$|E
40|$|The ISO archive {{hosts the}} {{considerable}} number of more than 3200 observations, that were performed in rectangular or triangular chopped mode. Many of them were aimed at {{sources close to the}} ISOPHOT detection limits. A good estimation of the <b>statistical</b> <b>uncertainties</b> is therfore highly important and larger uncertainties in the overall flux calibration become acceptable in these cases...|$|R
40|$|We {{provide a}} {{pedagogical}} {{introduction to the}} two main variants of real-space quantum Monte Carlo methods for electronic-structure calculations: variational Monte Carlo (VMC) and diffusion Monte Carlo (DMC). Assuming no prior knowledge on the subject, we review in depth the Metropolis-Hastings algorithm used in VMC for sampling the square of an approximate wave function, discussing details important for applications to electronic systems. We also review in detail the more sophisticated DMC algorithm within the fixed-node approximation, introduced to avoid the infamous Fermionic sign problem, which allows one to sample a more accurate approximation to the ground-state wave function. Throughout this review, we discuss the statistical methods used for evaluating expectation values and <b>statistical</b> <b>uncertainties.</b> In particular, we show how to estimate nonlinear functions of expectation values and their <b>statistical</b> <b>uncertainties.</b> Comment: Advances in Quantum Chemistry, 2015, Electron Correlation in Molecules [...] ab initio Beyond Gaussian Quantum Chemistry, pp. 000...|$|R
40|$|The {{importance}} of <b>statistical</b> <b>uncertainties</b> in selecting appropriate methods for estimation of extremes P. H. A. J. M. VAN GELDER, TU Delft, Faculty of Civil Engineering and Geosciences, Delft, The Netherlands. <b>Statistical</b> <b>uncertainties</b> in {{the estimation of}} extremes are normally not well considered in present practice or even present research. This paper shows the application of extreme distributions to river variables with non negligible levels of uncertainty and presents an approach to deal with them. Four distribution types (Gumbel, Pearson III (log shifted Gamma), Lognormal and Generalised Pareto) are fitted to peaks over threshold or annual maxima data sets of river discharges. Piecewise exponential distributions are used to summarise the four distribution types and bootstrapping methods for quantifying the uncertainty in the design discharges. The paper furthermore presents relationships between probability density functions, convergence theorems and statistical tests to judge the goodness of fits...|$|R
