2|125|Public
40|$|Managing {{transactions}} with real-time {{requirements and}} disk resident data presents many new problems. In this {{paper we address}} several: How can we schedule transactions with deadlines? How do the real-time constraints affect concurrency control? How does the scheduling of IO requests affect the timeliness of transactions? How should exclusive and <b>shared</b> <b>locking</b> be handled? We describe {{a new group of}} algorithms for scheduling realtime transactions which produce serializable schedules. We present a model for scheduling transactions with deadlines on a single processor disk resident database system, and evaluate the scheduling algorithms through detailed simulation. 1...|$|E
40|$|Abstract:- Embedded {{real-time}} databases {{become a}} basic {{part of the}} embedded systems in many using environments. Caches are used for reducing the gap between processor and off-chip memory. But caches introduce unpredictability in general real-time systems. Although several cache partitioning approaches have been purposed to tackle this problem, there is no scheme designed for real-time database system up to now. In this paper, we present a data centered cache partitioning approach that allows different tasks to have a <b>shared</b> <b>locking</b> partition in cache. The hard real-time tasks will have their own partitions and thus they can perform high predictability. At the same time, a shared non-locking partition is reserved for the soft real-time tasks. In this way we can target performance improvements based on the data that are frequently used by many tasks in the system. Our experiment {{results show that the}} miss rate can be reduced by about 10 %~ 18 % compared with that of a statically partitioned cache and by about 24 %~ 40 % compared with a dynamic cache using LRU replacement policy...|$|E
50|$|If lock {{requests}} {{for the same}} entity are queued, then once a <b>shared</b> <b>lock</b> is granted, any queued <b>shared</b> <b>locks</b> may also be granted. If an exclusive lock is found next on the queue, it must wait until all <b>shared</b> <b>locks</b> have been released. As with exclusive <b>locks,</b> these <b>shared</b> <b>locks</b> should be held for the least time possible.|$|R
50|$|C++14 adds {{a shared}} timed mutex and a {{companion}} <b>shared</b> <b>lock</b> type.|$|R
50|$|<b>Shared</b> <b>locks</b> {{differ from}} {{exclusive}} locks {{in that the}} holder list can contain multiple entries. <b>Shared</b> <b>locks</b> allow all holders to read {{the contents of the}} record knowing that the record cannot be changed until after the lock has been released by all holders. Exclusive locks cannot be obtained when a record is already <b>locked</b> (exclusively or <b>shared)</b> by another entity.|$|R
5000|$|Two {{kinds of}} <b>locks</b> are offered: <b>shared</b> <b>locks</b> and {{exclusive}} locks. In {{the case of}} , different kinds of locks may be applied to different sections (byte ranges) of a file, or else to the whole file. <b>Shared</b> <b>locks</b> can be held by multiple processes at the same time, but an exclusive lock can only be held by one process, and cannot coexist with a <b>shared</b> <b>lock.</b> To acquire a <b>shared</b> <b>lock,</b> a process must wait until no processes hold any exclusive locks. To acquire an exclusive lock, a process must wait until no processes hold either kind of lock. Unlike locks created by , those created by [...] are preserved across s, making them useful in forking servers. It is therefore possible {{for more than one}} process to hold an exclusive lock on the same file, provided these processes share a filial relationship and the exclusive lock was initially created in a single process before being duplicated across a [...]|$|R
50|$|Read-lock (<b>shared</b> <b>lock)</b> is {{associated}} with a database object by a transaction before reading (retrieving the state of) this object.|$|R
25|$|SQL Server allows {{multiple}} {{clients to}} use the same database concurrently. As such, it needs to control concurrent access to shared data, to ensure data integrity—when multiple clients update the same data, or clients attempt to read data that {{is in the process of}} being changed by another client. SQL Server provides two modes of concurrency control: pessimistic concurrency and optimistic concurrency. When pessimistic concurrency control is being used, SQL Server controls concurrent access by using locks. Locks can be either shared or exclusive. Exclusive lock grants the user exclusive access to the data—no other user can access the data as long as the <b>lock</b> is held. <b>Shared</b> <b>locks</b> are used when some data is being read—multiple users can read from data <b>locked</b> with a <b>shared</b> <b>lock,</b> but not acquire an exclusive lock. The latter would have to wait for all <b>shared</b> <b>locks</b> to be released.|$|R
5000|$|Protected Read (PR). This is the {{traditional}} <b>share</b> <b>lock,</b> which indicates a desire to read the resource but prevents other from updating it. Others can however also read the resource.|$|R
5000|$|... boolean locked := false // <b>shared</b> <b>lock</b> {{variable}} procedure EnterCritical (...) { do { while (locked == true) skip // spin until lock seems free } while TestAndSet(locked) // actual atomic locking } ...|$|R
50|$|Lock {{upgrades}} and downgrades {{release the}} old lock before applying the new lock. If an application downgrades an exclusive <b>lock</b> to a <b>shared</b> <b>lock</b> while another application is blocked {{waiting for an}} exclusive lock, the latter application may get the exclusive lock and lock the first application out. This means that lock downgrades can block, which may be counterintuitive.|$|R
5000|$|To {{comply with}} the S2PL protocol, a {{transaction}} needs to comply with 2PL, and release its write (exclusive) locks only after it has ended, i.e., being either committed or aborted. On the other hand, read (<b>shared)</b> <b>locks</b> are released regularly during phase 2. This protocol is not appropriate in B-trees because it causes Bottleneck (while B-trees always starts searching from the parent root).|$|R
5000|$|<b>Shared</b> <b>locks</b> are {{sometimes}} called [...] "read locks" [...] and exclusive locks {{are sometimes}} called [...] "write locks". However, because locks on Unix are advisory, this isn't enforced. Thus {{it is possible}} for a database to have a concept of [...] "shared writes" [...] vs. [...] "exclusive writes"; for example, changing a field in place may be permitted under shared access, whereas garbage-collecting and rewriting the database may require exclusive access.|$|R
40|$|Concurrent FIFO queues are {{a common}} {{component}} of concurrent systems. Using a single <b>shared</b> <b>lock</b> to prevent concurrent manipulations of queue contents reduces system concurrency. Therefore, many algorithms were suggested to increase concurrency while maintaining the correctness of queue manipulations. This paper shows how to automatically interpretation techniques. In particular, we verify all the safety properties originally specified for two concurrent queue algorithms without imposing an a priori bound {{on the number of}} allocated objects and threads. ...|$|R
40|$|Since no Canadian {{evidence}} exists on Canadian IPO lockups, this thesis examines the design, impact and market behavior of {{two types of}} lockups for Canadian IPOs listed on the TSX during the 1997 - 2005 period. We find {{that the existence of}} lockup information in an issuer's prospectus does not significantly reduce IPO underpricing by underwriters. We also find that IPO firms with dual-class share structures tend to have higher proportions of <b>shares</b> <b>locked</b> up and longer escrow lockup periods, and that larger firms have higher proportions of <b>shares</b> <b>locked</b> up as "escrow shares". These findings partially support the evidence reported by Brau et al. (2005) that firms with less transparency will provide a signal that the interests of their insiders are better aligned with outside investors by including lockups in their prospectuses. Significant negative (cumulative) abnormal returns are found only for high-tech firms immediately around unlock days. Lower abnormal trading volumes and relative spreads are found after unlock days for only the sample of IPOs with escrow lockups with stipulated non-zero lockup length. Keywords: IPO, lockup, escrow, abnormal market behavior. JEL Classification: G 10, G 1...|$|R
40|$|AbstractConcurrent FIFO queues are {{a common}} {{component}} of concurrent systems. Using a single <b>shared</b> <b>lock</b> to prevent concurrent manipulations of queue contents reduces system concurrency. Therefore, many algorithms were suggested to increase concurrency while maintaining the correctness of queue manipulations. This paper shows how to automatically verify partial correctness of concurrent FIFO queue algorithms using existing abstract interpretation techniques. In particular, we verify all the safety properties originally specified for two concurrent queue algorithms without imposing an a priori bound {{on the number of}} allocated objects and threads...|$|R
50|$|The Fish Docks {{consist of}} a number of docks <b>sharing</b> common <b>lock</b> entrances, east of the Royal Dock, built and {{expanded}} in stages from the mid 19th century onwards.|$|R
40|$|This paper investigates whether {{shareholder}} lockup {{agreements in}} France and Germany mitigate problems of agency and asymmetric information. Despite minimum requirements {{in terms of}} the length and percentage of <b>shares</b> <b>locked</b> up, lockup agreements are not only highly diverse across firms but also across the different shareholders of a single firm as most firms have different agreements in place for executives, non-executives and venture capitalists. The diversity across firms and types of shareholders can be explained by firm characteristics—such as the level of uncertainty—as well as the type and importance of each shareholder within the firm...|$|R
50|$|Now further {{assume the}} {{physical}} {{arrangement of the}} three processors, P1, P2, and P3, results in a non-uniform memory access time {{to the location of}} the <b>shared</b> <b>lock</b> variable. The order of increasing access time to the lock variable for the three processors is P1 < P2 < P3. So P1 is always the most advantaged at acquiring the lock, followed by P2, with P3 being most disadvantaged. How this situation leads to thread starvation {{in the absence of a}} fairness guarantee is shown in the following illustration of the execution of the above pseudocode by these three processors.|$|R
5000|$|... put {{a lock on}} a resource. WebDAV {{supports}} both <b>shared</b> {{and exclusive}} <b>locks.</b>|$|R
40|$|In {{information}} systems {{based on a}} two-tier distribution architecture, there are usually several clients working with shared resources. When designing such a system you must ensure that each client that accesses such shared resources does not interfere with other clients accessing and modifying the same resources. If the resource in question {{does not have a}} thread safe interface and/or it does not provide concurrency control mechanisms, a Lock Server attached to that shared source can help provide controlled concurrent access which allows each client to work with a consistent view of the resource. This paper discusses the architecture of such a <b>shared</b> <b>Lock</b> Server...|$|R
40|$|We give a clear yet {{rigorous}} correctness proof for Moss's algorithm {{for managing}} {{data in a}} nested transaction system. The algorithm, which {{is the basis of}} concurrency control and recovery in the Argus system, uses read- and write-locks and a stack of versions of each object to ensure the serializability and recoverability of transactions accessing the data. Our proof extends earlier work on exclusive locking to prove that Moss's algorithm generates serially correct executions in the presence of concurrency and transaction aborts. The key contribution is the identification of a simple property of cead operations, called transparency, that permits <b>shared</b> <b>locks</b> to be used for read operations...|$|R
5000|$|Having {{introduced}} {{the usage of}} condition variables, let's use it to revisit and solve the classic bounded producer/consumer problem. The classic solution is to use two monitors, comprising two condition variables <b>sharing</b> one <b>lock</b> on the queue: ...|$|R
5000|$|... #Caption: The tour boat Hiawatha and the laker Cason J. Callaway <b>share</b> the Poe <b>Lock</b> ...|$|R
5000|$|<b>Locking</b> <b>shares</b> up {{in voting}} trusts can in some {{countries}} help deter a hostile takeover.|$|R
50|$|Suffield High School {{participates in}} the North Central Connecticut Conference, and {{supports}} many varsity sports. Girls' athletics include basketball, cheerleading, cross country, dance team, field hockey, golf, indoor track, lacrosse, outdoor track, soccer, softball, swimming, tennis and volleyball. Boys' athletics include baseball, basketball, cross country, football (<b>shared</b> with Windsor <b>Locks),</b> golf, ice hockey (<b>shared</b> with Windsor <b>Locks</b> and Granby), indoor track, lacrosse, outdoor track, soccer, swimming, tennis and wrestling.|$|R
40|$|Erlang is an {{interesting}} example of a concurrent, high-level programming lan-guage. It is very instructive to consider {{how easy it is}} to write complex con-current applications in Erlang compared to concurrent programming languages that are based on <b>shared</b> memory, <b>locks,</b> and synchronization. On the othe...|$|R
40|$|An {{interesting}} {{feature of}} some recent parallel computers {{is the fact}} that the underlying transport mechanism behind the currently dominating message passing interfaces is based on a global address space model. By accessing this global address space directly most of the inherent delays for administering message buffers and queues can be avoided. Using this interface we have implemented a user level distributed shared memory layer using the virtual memory protection mechanisms of the operating system. The synchronisation required for maintaining the coherency of the memory is addressed by implementing a distributed <b>shared</b> <b>lock</b> which exploits the remote atomic store operations provided by the Meiko CS- 2. This allows an asynchronous style of pogramming where the load is dynamically distributed over the nodes of a parallel partition...|$|R
40|$|In this paper, {{we propose}} process locking, a dynamic {{scheduling}} protocol based on ideas of ordered <b>shared</b> <b>locks,</b> {{that allows for}} the correct concurrent and fault-tolerant execution of transactional processes. Transactional processes are well defined, complex structured collections of transactional services. The process structure comprises flow of control between single process steps and also considers alternatives for failure handling purposes. Moreover, the individual steps of a process may have different termination characteristics, i. e., they cannot be compensated once they have committed. All these constraints {{have to be taken}} into consideration when deciding how to interleave processes. However, due to the higher level semantics of processes, standard locking techniques based on <b>shared</b> and exclusive <b>locks</b> on data objects cannot be applied. Yet, process locking addresses both atomicity and isolation simultaneously at the appropriate level, the scheduling of processes, and accounts for the various constraints imposed by processes. In addition, process locking aims at providing a high degree of concurrency while, at the same time, minimizing execution costs. This is done by allowing cascading aborts for rather simple processes while this is prevented for complex, long-running processes within the same framework. 1...|$|R
50|$|In {{addition}} to <b>shared</b> (S) <b>locks</b> and exclusive (X) locks from other locking schemes, like strict two-phase locking, MGL also uses intention shared and intention exclusive locks. IS locks conflict with X locks, while IX locks conflict with S and X locks. The null lock (NL) {{is compatible with}} everything.|$|R
40|$|A {{interesting}} {{feature of}} some recent parallel computers {{is the fact}} that the underlying transport mechanism behind the currently dominating message passing interfaces is based on a global address space model. By accessing this global adress space directly most of the inherent delays for administering message buffers and queues can be avoided. Using this interface we have implemented a user level distributed shared memory layer using the virtual memory protection mechanisms of the operating system. The synchronisation required for maintaining the coherency of the memory is addressed by implementing a distributed <b>shared</b> <b>lock</b> which exploits the remote atomic store operations provided by the Meiko CS- 2. This allows an asynchronous stype of programming where the load is dynamically distributed over the nodes of a parallel partition. (orig.) SIGLEAvailable from TIB Hannover: RO 3476 (180) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekDEGerman...|$|R
40|$|The {{implementation}} of scalable synchronized data structures is notoriously difficult. Recent work in shared-memory multicores {{introduced a new}} synchronization paradigm called flat combining that allows many concurrent accessors to cooperate efficiently to reduce contention on <b>shared</b> <b>locks.</b> In this work we introduce this paradigm to a domain where reducing communication is paramount: distributed memory systems. We implement a flat combining framework for Grappa, a latency-tolerant PGAS runtime, and show {{how it can be}} used to implement synchronized global data structures. Even using simple locking schemes, we find that these flat-combining data structures scale out to 64 nodes with 2 x- 100 x improvement in throughput. We also demonstrate that this translates to application performance via two simple graph analysis kernels. The higher communication cost and structured concurrency of Grappa lead to a new form of distributed flat combining that drastically reduces the amount of communication necessary for maintaining global sequential consistency. ...|$|R
40|$|Significant {{performance}} advantages can {{be gained}} by implementing a database system on a cache-coherent shared memory multiprocessor. However, problems arise when failures occur. A single node (where a node refers to a processor/memory pair) crash may require a reboot of the entire shared memory system. Fortunately, shared memory multiprocessors that isolate individual node failures are currently being developed. Even with these, because of {{the side effects of}} the cache coherency protocol, a transaction executing strictly on a single node may become dependent on the validity of the memory of many nodes thereby inducing unnecessary transaction aborts. This happens when database objects, such as records, and database support structures, such as index structures and <b>shared</b> <b>lock</b> tables, are stored in shared memory. In this paper, we propose crash recovery protocols for shared memory database systems which avoid the unnecessary transaction aborts induced by the failure dependencies that occu [...] ...|$|R
40|$|We {{introduce}} {{a natural and}} flexible framework for collaborative transactions called Consortium. We also define a new model for collaborative interaction that we call “What You See Is What You Want ” (WYSIWYW), an abstraction that permits users {{to choose their own}} interaction modes with respect to the shared workspace. Consortium is not an extension to the transaction model but rather a flexible framework for defining collaborative transactions. The contribution of this work is a framework for collaborative transactions that allows a transaction to: (1) support the abstraction of WYSIWYW’, (2) span mtdtiple sessions, (3) have multiple commit points, (4) have different participants at different times, and (5) differentiate between the concepts of owners and participants. In addition, this paper defines the concept of transaction leasing, identifies policies for joining, leaving, and participating in a transaction, extends the notion of transaction ownership by allowing ownership transfer, and describes mechanisms for <b>sharing</b> <b>locks</b> in collaborative transactions. ...|$|R
40|$|In {{enterprise}} {{information systems}} {{based on a}} multi-tier distribution architecture, there are several clients working with shared resources. When designing such a system one must ensure that each client that accesses such shared resources does not interfere with other clients accessing and modifying the same resources. If the resource in question does not has a thread safe interface and/or it does not provide concurrency control mechanisms, a Lock Server attached to that shared source can help provide a controlled concurrent access which allows each client {{to work with a}} consistent view of the resource. This paper discusses the architecture of such a <b>shared</b> <b>Lock</b> Server. Name Lock Server Aliases Lock Manager Context You are developing an application for a distributed enterprise information system where some clients have to access or modify shared resources (Figure 1). You have decided to design your application based at least on a two-tier distribution architecture. One tier holds [...] ...|$|R
40|$|Customer access {{information}} technologies (CAITs) provide {{a link between}} a firm and its customers. Firms invest in CAITs to reduce costs, increase revenues and market <b>share,</b> <b>lock</b> in existing customers and capture new ones. These benefits, however, are notoriously difficult to measure. This paper proposes an evaluative method for CAlT deployment called value platform analysis, {{that is based on}} a conceptual model drawn from the theory of retail outlet deployment in marketing science. The model focuses on the impact of CAIT features and environmental features on transactions generated by the CAIT. Specific econometric models are developed for deployment. Hypotheses regarding the likely impact of automated teller machine (ATM) location design choices and environmental features on ATM transactions are evaluated. The results indicate {{that there are a number}} of key features influencing ATM performance. Two distinct ATM deployment scenarios emerge: one for servicing a bank's own customers, and another for providing transaction services for customers for a fee. ...|$|R
