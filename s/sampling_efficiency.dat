510|779|Public
50|$|The Hexagonal Fast Fourier Transform (HFFT) {{has been}} {{developed}} to utilize the advantages of hexagonal sampling. It has been proven that the hexagonal grid serves as an optimal sampling lattice for isotropically band-limited two-dimensional signals and has a <b>sampling</b> <b>efficiency</b> which is 13.4% more {{when compared to the}} <b>sampling</b> <b>efficiency</b> obtained from rectangular sampling. Several other advantages of hexagonal sampling include consistent connectivity, higher symmetry, greater angular resolution, and equidistant neighboring pixels. Sometimes, more than one of these advantages compound together thereby increasing the efficiency by 50% in terms of computation and storage when compared to rectangular sampling. Despite of all these advantages of hexagonal sampling over the rectangular sampling, its application has been limited because of its inability to result in the orthogonal rows and columns that can be transformed independently as is done with rectangular samples. For this reason, an efficient FFT with a separable Fourier kernel {{has been developed}} by utilizing the Array Set Addressing (ASA) scheme and is termed as Hexagonal Fast Fourier Transform.|$|E
50|$|This {{project is}} {{essentially}} an augmentation of an existing ARI project (00-3-011) which uses a paired watershed design to {{study the effects of}} timber harvesting practices on water quality. This new proposal enhances the project by monitoring additional water quality parameters, and geomorphic and habitat parameters. Sophisticated interaction between instream sensors and automated water quality samplers will maximize <b>sampling</b> <b>efficiency</b> and statistical validity of the results. Remote data access capabilities will enable monitoring of real-time weather and sampling data to efficiently dispatch field personnel for sample pickup and troubleshoot hydrologic instrumentation.|$|E
50|$|The main {{difference}} between cluster sampling and stratified sampling {{is that in}} cluster sampling the cluster is treated as the sampling unit so sampling is done on a population of clusters (at least in the first stage). In stratified sampling, the sampling is done on elements within each strata. In stratified sampling, a random sample is drawn {{from each of the}} strata, whereas in cluster sampling only the selected clusters are sampled. A common motivation of cluster sampling is to reduce costs by increasing <b>sampling</b> <b>efficiency.</b> This contrasts with stratified sampling where the motivation is to increase precision.|$|E
40|$|The {{presence}} of leverage points in regression {{can have a}} dramatic impact on the finite <b>sample</b> <b>efficiencies</b> (versus least squares under Gaussian errors) of high breakdown estimators. In general, as the x-value of a leverage point becomes more extreme, the finite <b>sample</b> <b>efficiencies</b> decrease and larger sample sizes are required for asymptotic results to apply. However, the least median of squares (LMS) and least trimmed squares (LTS) estimators exhibit higher finite <b>sample</b> than asymptotic <b>efficiencies.</b> On the other hand, the finite <b>sample</b> <b>efficiency</b> of the Schweppe-type one-step generalized M estimator (S 1 S), starting from LTS, which minimizes {{the sum of the}} h smallest squared residuals, converges from below to its asymptotic value as the sample size increases. We suggest that the finite sample variance itself be replaced by a trimmed sample variance which accounts for the long tail behavior encountered in finite samples. An improvement on the S 1 S estimator, which consists of never downweighting any point with one of the h smallest squared LTS residuals, performs quite well with respect to either measure of efficiency, while retaining a high breakdown point. Asymptotic <b>efficiency</b> finite <b>sample</b> <b>efficiency</b> breakdown point exact fit point least median of squares least trimmed squares one-step estimator...|$|R
40|$|<b>Sample</b> <b>efficiency</b> is {{important}} when optimizing parameters of locomotion controllers, since hardware experiments are time consuming and expensive. Bayesian Optimization, a sample-efficient optimization framework, {{has recently been}} widely applied to address this problem, but further improvements in <b>sample</b> <b>efficiency</b> are needed for practical applicability to real-world robots and high-dimensional controllers. To address this, prior work has proposed using domain expertise for constructing custom distance metrics for locomotion. In this work we show how to learn such a distance metric automatically. We use a neural network to learn an informed distance metric from data obtained in high-fidelity simulations. We conduct experiments on two different controllers and robot architectures. First, we demonstrate improvement in <b>sample</b> <b>efficiency</b> when optimizing a 5 -dimensional controller on the ATRIAS robot hardware. We then conduct simulation experiments to optimize a 16 -dimensional controller for a 7 -link robot model and obtain significant improvements even when optimizing in perturbed environments. This demonstrates that our approach is able to enhance <b>sample</b> <b>efficiency</b> for two different controllers, hence is a fitting candidate for further experiments on hardware in the future. Comment: (Rika Antonova and Akshara Rai contributed equally...|$|R
40|$|Despite {{its large}} <b>sample</b> <b>efficiency,</b> the {{truncated}} flat (TF) kernel estimator of long-run covariance matrices is seldom used, because {{it lacks the}} guaranteed positive semidefiniteness and sometimes performs poorly in small samples, compared to other familiar kernel estimators. This paper proposes simple modifications to the TF estimator to enforce the positive definiteness without sacrificing the large <b>sample</b> <b>efficiency</b> and make the estimator more reliable in small samples through better utilization of the biasvariance tradeoff. We study the large sample properties of the modified TF estimators and verify their improved small-sample performances by Monte Carlo simulations...|$|R
40|$|AbstractStudies of the {{detection}} of simple visual patterns at threshold contrast have found that human performance {{is limited by the}} addition of internal noise and by the sub-optimal <b>sampling</b> <b>efficiency</b> of the visual system. Many common visual tasks require {{the detection}} of a signal having a contrast well above threshold, and we sought to measure the internal noise and <b>sampling</b> <b>efficiency</b> for such signals using simple reaction time (RT). Observers were presented with suprathreshold Gabors in dynamic Gaussian white noise and were required to hit a button as soon as each was detected. By comparing the RT variances from humans to those of an ideal observer, visuomotor internal noise and <b>sampling</b> <b>efficiency</b> were measured. The internal noise remains constant and the <b>sampling</b> <b>efficiency</b> increases as the signal contrast increases...|$|E
30|$|The <b>sampling</b> <b>efficiency</b> {{depends on}} both the {{collector}} type and the characteristics and behavior of the aerosol particle, in this case, the pollen type. The average <b>sampling</b> <b>efficiency</b> of the PMF for maize pollen {{was estimated to be}} 0.35 by calibration to a Hirst-type volumetric pollen trap [63], the standard trap used by the European and German Aerobiology Networks.|$|E
40|$|Assessment of the <b>sampling</b> <b>efficiency</b> of an {{electrostatic}} precipitator. Easy-to-use particle sampling techniques {{leading to}} easy-to-analyze samples {{is a growing}} need for nanosafety. Electrostatic precipitation [Dickens 1999] commercialized by TSI (Nanometer Aerosol Sampler model 3089) could be an answer. However, more information is needed {{on the performance of}} this instruments. On the one hand, real time <b>sampling</b> <b>efficiency</b> of the NAS regarding voltage has been studied. On the other hand, the <b>sampling</b> <b>efficiency</b> for 80 nm polystyrene particles has been considered through both real time CPC measurements and TEM analysis: these methods lead to very different results, respectively 62 - 70 % and < 0, 1 %. These results are discussed regarding literature...|$|E
40|$|This paper {{presents}} postponed updates, a {{new strategy}} for TD methods that can improve <b>sample</b> <b>efficiency</b> without incurring the computational and space requirements of model-based RL. By recording the agent’s last-visit experience, the agent can delay its update until the given state is revisited, thereby {{improving the quality of}} the update. Experimental results demonstrate that postponed updates outperforms several competitors, most notably eligibility traces, a traditional way to improve the <b>sample</b> <b>efficiency</b> of TD methods. It achieves this without the need to tune an extra parameter as is needed for eligibility traces. ...|$|R
40|$|Balancing between {{computational}} <b>efficiency</b> and <b>sample</b> <b>efficiency</b> is {{an important}} goal in reinforcement learning. Temporal difference (TD) learning algorithms stochastically update the value function, with a linear time complexity {{in the number of}} features, whereas least-squares temporal difference (LSTD) algorithms are sample efficient but can be quadratic in the number of features. In this work, we develop an efficient incremental low-rank LSTD(λ) algorithm that progresses towards the goal of better balancing computation and <b>sample</b> <b>efficiency.</b> The algorithm reduces the computation and storage complexity to the number of features times the chosen rank parameter while summarizing past samples efficiently to nearly obtain the sample complexity of LSTD. We derive a simulation bound on the solution given by truncated low-rank approximation, illustrating a bias- variance trade-off dependent on the choice of rank. We demonstrate that the algorithm effectively balances computational complexity and <b>sample</b> <b>efficiency</b> for policy evaluation in a benchmark task and a high-dimensional energy allocation domain. Comment: Accepted to IJCAI 201...|$|R
40|$|To {{improve the}} {{convergence}} {{rate and the}} <b>sample</b> <b>efficiency,</b> two efficient learning methods AC-HMLP and RAC-HMLP (AC-HMLP with l 2 -regularization) are proposed by combining actor-critic algorithm with hierarchical model learning and planning. The hierarchical models consisting of the local and the global models, which are learned {{at the same time}} during learning of the value function and the policy, are approximated by local linear regression (LLR) and linear function approximation (LFA), respectively. Both the local model and the global model are applied to generate samples for planning; the former is used only if the state-prediction error does not surpass the threshold at each time step, while the latter is utilized {{at the end of each}} episode. The purpose of taking both models is to improve the <b>sample</b> <b>efficiency</b> and accelerate the convergence rate of the whole algorithm through fully utilizing the local and global information. Experimentally, AC-HMLP and RAC-HMLP are compared with three representative algorithms on two Reinforcement Learning (RL) benchmark problems. The results demonstrate that they perform best in terms of convergence rate and <b>sample</b> <b>efficiency...</b>|$|R
40|$|Stratification, {{dividing}} the statistical population into less heterogeneous subgroups before sampling, can help improve <b>sampling</b> <b>efficiency</b> by improving representativeness and reducing sampling error. This report explores the added <b>sampling</b> <b>efficiency</b> that {{is achieved by}} using the European Environmental stratification for estimating the area covered by the 25 Corine Land Cover (CLC) categories occurring in the semi-natural and managed terrestrial habitats of the wider-countryside. Although the dataset is not ideally suited to assess stratification efficiency for EBONE, the results give some encouragement. The analysis indicates that the pan-European stratification improves <b>sampling</b> <b>efficiency</b> for several land cover categories and performs similar to four more detailed national stratifications, supporting their use {{as a basis for}} designing a pan-European biodiversity observation network...|$|E
40|$|Purpose: Previous {{studies have}} shown that myopes produce an {{asymmetrical}} loss in contrast sensitivity with positive and negative defocusing lenses at low-medium spatial frequencies (1 – 7 c/deg) compared to non-myopes. The measurement of contrast detection in noise allows any given loss in visual sensitivity to be attributed to two components namely <b>sampling</b> <b>efficiency</b> and/or equivalent noise. Previous work has also shown that <b>sampling</b> <b>efficiency</b> gives an indication of the neural effects {{and in the absence of}} <b>sampling</b> <b>efficiency</b> changes, changes in equivalent noise levels are considered to be due to optical factors. We investigate whether the asymmetrical loss shown by myopes to defocus can be explained by changes in <b>sampling</b> <b>efficiency</b> or equivalent noise, or both. Methods: Contrast thresholds in four different levels of externally added noise were measured in five myopic and five non-myopic subjects for Gabors of 3 c/deg. Measurements were obtained under cycloplegia with 0 D, ± 0. 50 D, and ± 1. 00 D defocusing lenses. RMS wavefront aberrations were also measured using a Shack-Hartmann aberrometer. Results: In the absence of noise, there was no significant difference in contrast sensitivity without defocus between myopes and emmetropes. When <b>sampling</b> <b>efficiency</b> and equivalent noise were measured from Contrast Detection in Noise functions, analysis of variance showed no significant difference in <b>sampling</b> <b>efficiency</b> between myopes and non-myopes (p = 0. 145). Myopes showed maximum levels of equivalent noise with + 1. 00 D defocus, decreasing to a minimum with − 1. 00 D defocus (p = 0. 009). Non-myopes showed a symmetrical increase in equivalent noise from zero for both positive and negative defocus. A significant correlation was found between equivalent noise and the total RMS of Wavefront aberrations of the eye (p = 0. 026). Conclusion: Optical factors explain the observed differences in contrast sensitivity with positive and negative defocus between myopes and non-myopes...|$|E
40|$|AbstractPurposePrevious {{studies have}} shown that myopes produce an {{asymmetrical}} loss in contrast sensitivity with positive and negative defocusing lenses at low-medium spatial frequencies (1 – 7 c/deg) compared to non-myopes. The measurement of contrast detection in noise allows any given loss in visual sensitivity to be attributed to two components namely <b>sampling</b> <b>efficiency</b> and/or equivalent noise. Previous work has also shown that <b>sampling</b> <b>efficiency</b> gives an indication of the neural effects {{and in the absence of}} <b>sampling</b> <b>efficiency</b> changes, changes in equivalent noise levels are considered to be due to optical factors. We investigate whether the asymmetrical loss shown by myopes to defocus can be explained by changes in <b>sampling</b> <b>efficiency</b> or equivalent noise, or both. MethodsContrast thresholds in four different levels of externally added noise were measured in five myopic and five non-myopic subjects for Gabors of 3 c/deg. Measurements were obtained under cycloplegia with 0 D, ± 0. 50 D, and ± 1. 00 D defocusing lenses. RMS wavefront aberrations were also measured using a Shack-Hartmann aberrometer. ResultsIn the absence of noise, there was no significant difference in contrast sensitivity without defocus between myopes and emmetropes. When <b>sampling</b> <b>efficiency</b> and equivalent noise were measured from Contrast Detection in Noise functions, analysis of variance showed no significant difference in <b>sampling</b> <b>efficiency</b> between myopes and non-myopes (p= 0. 145). Myopes showed maximum levels of equivalent noise with + 1. 00 D defocus, decreasing to a minimum with − 1. 00 D defocus (p= 0. 009). Non-myopes showed a symmetrical increase in equivalent noise from zero for both positive and negative defocus. A significant correlation was found between equivalent noise and the total RMS of Wavefront aberrations of the eye (p= 0. 026). ConclusionOptical factors explain the observed differences in contrast sensitivity with positive and negative defocus between myopes and non-myopes...|$|E
3000|$|For {{existing}} and novel causal search algorithms, {{what are their}} semantic and syntactic properties (e.g., soundness, consistency, maximum informativeness)? What are their statistical properties (pointwise consistency, uniform consistency, <b>sample</b> <b>efficiency)?</b> What are their computational properties (computational complexity)? [...]...|$|R
40|$|Policy {{gradient}} {{methods have}} achieved remarkable successes in solving challenging reinforcement learning problems. However, it still often {{suffers from the}} large variance issue on policy gradient estimation, which leads to poor <b>sample</b> <b>efficiency</b> during training. In this work, we propose a control variate method to effectively reduce variance for policy gradient methods. Motivated by the Stein's identity, our method extends the previous control variate methods used in REINFORCE and advantage actor-critic by introducing more general action-dependent baseline functions. Empirical studies show that our method significantly improves the <b>sample</b> <b>efficiency</b> of the state-of-the-art policy gradient approaches. Comment: The first two authors contributed equally. Author ordering determined by coin flip over a Google Hangou...|$|R
40|$|Method of moment estimators exhibit {{appealing}} statistical properties, such as asymptotic unbiasedness, for nonconvex problems. However, {{they typically}} require {{a large number}} of samples and are extremely sensitive to model misspecification. In this paper, we apply the framework of M-estimation to develop both a generalized method of moments procedure and a principled method for regularization. Our proposed M-estimator obtains optimal <b>sample</b> <b>efficiency</b> rates (in the class of moment-based estimators) and the same well-known rates on prediction accuracy as other spectral estimators. It also makes it straightforward to incorporate regularization into the sample moment conditions. We demonstrate empirically the gains in <b>sample</b> <b>efficiency</b> from our approach on hidden Markov models. Comment: Appears in Artificial Intelligence and Statistics, 201...|$|R
3000|$|The {{horizontal}} {{pollen flow}} for maize and total pollen was calculated {{according to the}} following formula regarding the <b>sampling</b> <b>efficiency</b> of the PMF: [...]...|$|E
40|$|The {{mosquito}} <b>sampling</b> <b>efficiency</b> of light-trap catches {{and electric}} motor mosquito catches {{were compared with}} that of human biting catches in the Three Gorges Reservoir. There was consistency in the <b>sampling</b> <b>efficiency</b> between light-trap catches and human biting catches for Anopheles sinensis (r = 0. 82, P, 0. 01) and light-trap catches were 1. 52 (1. 35 – 1. 71) times that of human biting catches regardless of mosquito density (r = 0. 33, P. 0. 01), while the correlation between electric motor mosquito catches and human biting catches was found to be not statistically significant (r = 0. 43, P. 0. 01) and its <b>sampling</b> <b>efficiency</b> was below that of human biting catches. It is concluded that light-traps can be used as an alternative to human biting catches of Anopheles sinensis in the study area and is a promising tool for sampling malaria vector populations...|$|E
40|$|Abstract We {{present the}} results of a timed field trial {{comparing}} the bias characteristics and relative <b>sampling</b> <b>efficiency</b> of line-intersect, fixed-area, and point relascope sampling for downed coarse woody material. Seven stands in a managed northern hardwood forest in New Hampshire were inventoried. Significant differences were found among estimates in some stands, indicating a potential for difference in bias in field implementation of the methods. In terms of relative <b>sampling</b> <b>efficiency,</b> results for each method varied among stand. However, point relascope sampling had comparable or better time efficiency than the other methods in most stands...|$|E
40|$|Model-free deep {{reinforcement}} learning algorithms {{have been shown}} to be capable of learning a wide range of robotic skills, but typically require a very large number of samples to achieve good performance. Model-based algorithms, in principle, can provide for much more efficient learning, but have proven difficult to extend to expressive, high-capacity models such as deep neural networks. In this work, we demonstrate that medium-sized neural network models can in fact be combined with model predictive control (MPC) to achieve excellent sample complexity in a model-based {{reinforcement learning}} algorithm, producing stable and plausible gaits to accomplish various complex locomotion tasks. We also propose using deep neural network dynamics models to initialize a model-free learner, in order to combine the <b>sample</b> <b>efficiency</b> of model-based approaches with the high task-specific performance of model-free methods. We empirically demonstrate on MuJoCo locomotion tasks that our pure model-based approach trained on just random action data can follow arbitrary trajectories with excellent <b>sample</b> <b>efficiency,</b> and that our hybrid algorithm can accelerate model-free learning on high-speed benchmark tasks, achieving <b>sample</b> <b>efficiency</b> gains of 3 - 5 x on swimmer, cheetah, hopper, and ant agents. Videos can be found at [URL]...|$|R
40|$|Trust region methods, such as TRPO, {{are often}} used to {{stabilize}} policy optimization algorithms in reinforcement learning (RL). While current trust region strategies are effective for continuous control, they typically require a prohibitively large amount of on-policy interaction with the environment. To address this problem, we propose an off-policy trust region method, Trust-PCL. The algorithm {{is the result of}} observing that the optimal policy and state values of a maximum reward objective with a relative-entropy regularizer satisfy a set of multi-step pathwise consistencies along any path. Thus, Trust-PCL is able to maintain optimization stability while exploiting off-policy data to improve <b>sample</b> <b>efficiency.</b> When evaluated on a number of continuous control tasks, Trust-PCL improves the solution quality and <b>sample</b> <b>efficiency</b> of TRPO...|$|R
40|$|This article {{presents}} and evaluates best-match learning, {{a new approach}} to reinforcement learning that trades off the <b>sample</b> <b>efficiency</b> of model-based methods with the space efficiency of modelfree methods. Best-match learning works by approximating the solution to a set of best-match equations, which combine a sparse model with a model-free Q-value function constructed from samples not used by the model. We prove that, unlike regular sparse model-based methods, bestmatch learning is guaranteed to converge to the optimal Q-values in the tabular case. Empirical results demonstrate that best-match learning can substantially outperform regular sparse modelbased methods, as well as several model-free methods that strive to improve the <b>sample</b> <b>efficiency</b> of temporal-difference methods. In addition, we demonstrate that best-match learning can be successfully combined with function approximation...|$|R
40|$|This paper {{describes}} an effective method for the measurements of large aperture sampling grating, the <b>sampling</b> <b>efficiency</b> the sampling focal length and the sampling angle. By using a 351 nm collimation laser source {{to scan the}} full aperture of the sampling grating in every subregion, the diffraction power of " 0 " order and "+ 1 " order can be obtained synchronously by two standard integrating balls, And then by calculating the <b>sampling</b> <b>efficiency</b> of this subregion and splicing the acquisition data, {{we can get the}} averaged grating <b>sampling</b> <b>efficiency</b> in the full aperture. Based on this method, we can effectively eliminate the effect of the output instability of laser source, decrease the uncertainty of test results. According to the fabrication principle of the sampling grating, measurements for the sampling focal length and the sampling angle can be performed. Test rersults indicate that this method can be used to measure large aperture sampling gratings...|$|E
40|$|This paper {{addresses}} {{the problem of}} improving motion <b>sampling</b> <b>efficiency</b> for motion-compensated prediction (MCP). We provide a theoretical framework for analyzing the effect of motion sampling structure on MCP efficiency. It is shown that the sampling grid in-duced by the quadtree partition in H. 264 /AVC is suboptimal. To improve <b>sampling</b> <b>efficiency,</b> we propose a new pattern, which pro-vides sampling points at both {{the center and the}} top-left corner of a macroblcok. When contrasted with conventional NxN/ 2 block par-tition, the proposed scheme performs consistently and significantly better in subjective and objective quality. Index Terms — Motion Sampling, H. 264 /AVC, OBMC 1...|$|E
40|$|AbstractContrast {{detection}} {{in different}} levels of external visual noise allows a given loss in contrast sensitivity to be attributed to either {{an increase in the}} internal noise of the visual system, a decrease in <b>sampling</b> <b>efficiency,</b> or both. <b>Sampling</b> <b>efficiency</b> indicates how effectively the available stimulus information is utilized by the visual system. The aim {{of this study is to}} investigate the effect of normal ageing on <b>sampling</b> <b>efficiency</b> and internal noise. Contrast thresholds for sine-wave gratings of 6 c/deg were measured in the presence of four (including zero) levels of externally added visual noise in young and older healthy observers. Results showed that sampling efficiencies were significantly lower for the older group compared to the younger, while the internal noise showed no significant change. The implications of the data for the relative contribution of the optical and neural systems on visual function loss with ageing are discussed. Our results suggest that the neural system plays a major role in the loss of contrast sensitivity with ageing in normal, healthy eyes...|$|E
40|$|The two {{parameter}} Burr type X {{distribution is}} considered and its scale parameter is estimated from a censored sample using the classical maximum likelihood method. The estimating equations are modified to get simpler and efficient estimators. Two methods of modification are suggested. The small <b>sample</b> <b>efficiencies</b> are presented. </p...|$|R
40|$|Graduation date: 1981 Generalizations of {{the smooth}} goodness-of-fit test statistics, the {{components}} of Cramer-von Mises type statistics and the Pearson chi-square statistics are considered for censored <b>samples.</b> Asymptotic <b>efficiency</b> comparisons are made between these general goodness-of-fit tests for Weibull and gamma distributions for certain parametric alternatives {{in the case of}} singly censored <b>samples.</b> <b>Efficiency</b> comparisons are also made with the specialized tests of Mann for Weibull distribution and of Locke for gamma distribution. Among these tests the smooth test of order one was found to be most efficient for all cases considered...|$|R
40|$|We derive simpler {{expressions}} under {{a certain}} structure of design matrices for the two-stage Aitken {{estimates of the}} regression coefficients of two seemingly unrelated regression equations. The estimates are shown to have smaller variance than the ordinary least squares estimates for sufficiently large samples. two-stage estimates finite <b>sample</b> <b>efficiency</b> mean square error...|$|R
40|$|Hairdressers have an {{increased}} risk of developing occupational skin diseases due to exposure to skin irritants and sensitizers. In the present work a method of assessing dermal exposure to permanent hair dyes was developed. The sampling performance characteristics of hand wash sampling with bag rinsing were studied for five hair dye compounds. The effect of residence time, sample load and different matrices were studied. Thirty volunteers were exposed to a reference solution of these compounds and to commercial hair dye products. The <b>sampling</b> <b>efficiency</b> after 5 min residence timewas between 70 and 90 % for the dye components in the hair dye products. <b>Sampling</b> <b>efficiency</b> decreases with increasing residence time, making the time of sampling an important factor. Hand wash sampling should be performed as soon as possible after the work task of interest. We conclude that the <b>sampling</b> <b>efficiency</b> is adequate for measurements of dermal exposure to permanent hair dyes. Hand wash sampling with bag rinsing is a useful tool for field studies of dermal exposure assessment in hairdressers...|$|E
40|$|Existing {{localization}} algorithms for mobile sensor {{networks are}} usually {{based on the}} Sequential Monte Carlo (SMC) method. They either suffer from low <b>sampling</b> <b>efficiency</b> or require high beacon density to achieve high localization accuracy. Although papers can be found for solving the above problems separately, there is no solution which addresses both issues. In this paper, we propose an energy efficient algorithm, called WMCL, which can achieve both high <b>sampling</b> <b>efficiency</b> and high localization accuracy in various scenarios. In existing algorithms, a technique called bounding-box is used to improve the <b>sampling</b> <b>efficiency</b> by reducing the scope from which the candidate samples are selected. WMCL can further {{reduce the size of}} a sensor node's bounding-box by a factor of up to 87 percent and, consequently, improve the <b>sampling</b> <b>efficiency</b> by a factor of up to 95 percent. The improvement in <b>sampling</b> <b>efficiency</b> dramatically reduces the computational cost. Our algorithm uses the estimated position information of sensor nodes to improve localization accuracy. Compared with algorithms adopting similar methods, WMCL can achieve similar localization accuracy with less communication cost and computational cost. Our work has additional advantages. First, most existing SMC-based localization algorithms cannot be used in static sensor networks but WMCL can work well, even without the need of experimentally tuning parameters as required in existing algorithms like MSL*. Second, existing algorithms have low localization accuracy when nodes move very fast. We propose a new algorithm in which WMCL is iteratively executed with different assumptions on nodes' speed. The new algorithm dramatically improves localization accuracy when nodes move very fast. We have evaluated the performance of our algorithm both theoretically and through extensive simulations. We have also validated the performance results of our algorithm by implementing it in real deployed static sensor networks. To the best of our knowledge, we are the first to implement SMC-based localization algorithms for wireless sensor networks in real environment. Department of Computin...|$|E
3000|$|... is {{the number}} of pollen grains in the sample, i is the pollen from species i; here, maize pollen, Ei, PMF is the <b>sampling</b> <b>efficiency</b> of the PMF (0.35) for maize pollen {{compared}} with the volumetric standard pollen trap (Hirst type/Burkard).|$|E
40|$|We {{describe}} a novel ion source for analytical mass spectrometry based on femtosecond laser ionization at pressures at and above atmospheric and characterize its performance when coupled to a tandem quadrupole/time-of-flight mass spectrometer. We assess source saturation limits, ionization and <b>sampling</b> <b>efficiencies,</b> the effective ionization volume, {{and limits of}} detection. We demonstrate 100...|$|R
40|$|In {{this note}} we prove that for {{certain types of}} {{regression}} estimators with positive breakdown point, the finite <b>sample</b> <b>efficiencies</b> can be arbitrarily low when the design is unfavorable. In particular, the estimator proposed by Yohai and Zamar (1988), which is asymptotically efficient when the errors are normal, falls in this category. ...|$|R
40|$|Bayesian {{optimization}} {{is renowned}} for its <b>sample</b> <b>efficiency</b> but its application to higher dimensional tasks is impeded by its focus on global optimization. To scale to higher dimensional problems, we leverage the <b>sample</b> <b>efficiency</b> of Bayesian optimization in a local context. The optimization of the acquisition function is restricted to the vicinity of a Gaussian search distribution which is moved towards high value areas of the objective. The proposed informationtheoretic update of the search distribution results in a Bayesian interpretation of local stochastic search: the search distribution encodes prior knowledge on the optimum’s location and is weighted at each iteration by the likelihood of this location’s optimality. We demonstrate the effectiveness of our algorithm on several benchmark objective functions {{as well as a}} continuous robotic task in which an informative prior is obtained by imitation learning...|$|R
