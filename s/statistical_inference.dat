5413|1070|Public
5|$|Martingales {{have many}} {{applications}} in statistics, {{but it has}} been remarked that its use and application are not as widespread as it could be in the field of statistics, particularly <b>statistical</b> <b>inference.</b> They have found applications in areas in probability theory such as queueing theory and Palm calculus and other fields such as economics and finance.|$|E
5|$|Keynes's {{prescription}} for strong public investment had ties to {{his interest in}} uncertainty. Keynes had given a unique perspective on <b>statistical</b> <b>inference</b> in A Treatise on Probability, written in 1921, years before his major economic works. Keynes thought strong public investment and fiscal policy would counter the negative impacts the uncertainty of economic fluctuations can have on the economy. While Keynes's successors {{paid little attention to}} the probabilistic parts of his work, uncertainty may have played a central part in the investment and liquidity-preference aspects of General Theory.|$|E
5|$|Emphasis {{is placed}} not on actual {{arithmetic}} computation, {{but rather on}} conceptual understanding and interpretation. The course curriculum is organized around four basic themes; the first involves exploring data and covers 20–30% of the exam. Students are expected to use graphical and numerical techniques to analyze distributions of data, including univariate, bivariate, and categorical data. The second theme involves planning and conducting a study and covers 10–15% of the exam. Students {{must be aware of}} the various methods of data collection through sampling or experimentation and the sorts of conclusions that can be drawn from the results. The third theme involves probability and its role in anticipating patterns in distributions of data. This theme covers 20–30% of the exam. The fourth theme, which covers 30–40% of the exam, involves <b>statistical</b> <b>inference</b> using point estimation, confidence intervals, and significance tests.|$|E
40|$|Introduction Part I. <b>Statistical</b> <b>inferences</b> {{concerning}} empirical functions. § 1. <b>Statistical</b> <b>inferences</b> concerning empirical {{functions of}} the type (AD). § 2. <b>Statistical</b> <b>inferences</b> concerning empirical {{functions of the}} type (BD). § 3. <b>Statistical</b> <b>inferences</b> concerning empirical functions type (AC). § 4. <b>Statistical</b> <b>inferences</b> concerning empirical functions of the type (BC). Part II. Differentiation of empirical function § 1. Differentiation of empirical functions of the type (AD). § 2. Differentiation of empirical functions due to the type (c). § 3. Functional operations on empirical functions of the type (B) and <b>statistical</b> <b>inferences...</b>|$|R
40|$|Introduction Part IX. Applications of ranges to {{successive}} {{process of}} <b>statistical</b> <b>inferences.</b> § 1. Modified $ t $-test {{in the two}} sample theory. § 2. Successive poolings of data in control charts. Part X. Fiducial inferences from the view point of successive processes of <b>statistical</b> <b>inferences.</b> § 1. Fiducial inferences. § 2. Application of Barnard-Stein method to an interpretation of Behrens-Fisher test...|$|R
40|$|In {{the present}} paper we face, from a general perspective, the {{problems}} posed by <b>statistical</b> <b>inferences</b> {{in the case of}} two modalities. The analysis of the association is performed both from the ipotetical-deductive point of view (tests of significance) and from the inductive one (predictive inferences). That analysis is performed with the view of the foundational aspects of <b>statistical</b> <b>inferences</b> not caring too mach about the mathematical subtleties...|$|R
25|$|There {{are various}} papers in {{scholarly}} journals deriving formal versions of Occam's razor from probability theory, applying it in <b>statistical</b> <b>inference,</b> {{and using it}} {{to come up with}} criteria for penalizing complexity in <b>statistical</b> <b>inference.</b> Papers have suggested a connection between Occam's razor and Kolmogorov complexity.|$|E
25|$|The {{dispute between}} Fisher and Neyman–Pearson was waged on {{philosophical}} grounds, {{characterized by a}} philosopher as a dispute over {{the proper role of}} models in <b>statistical</b> <b>inference.</b>|$|E
25|$|As such, the {{traditional}} <b>statistical</b> <b>inference</b> machinery {{will not work}} with these more complicated models, and in this case, {{it is common to}} instead use a forward simulation-based approach.|$|E
3000|$|... [...]. Such <b>statistical</b> <b>inferences</b> {{would include}} (a) {{the use of}} the sample value of E as an {{appropriate}} estimate of E [...]...|$|R
50|$|In econometrics, <b>statistical</b> <b>inferences</b> may be {{erroneous}} if, {{in addition}} to the observed variables under study, there exist other relevant variables that are unobserved, but correlated with the observed variables.|$|R
3000|$|... [...]. However, the {{necessary}} conditions for making such <b>statistical</b> <b>inferences</b> are rarely met in biological studies. It is typical for such sampling data {{that neither the}} total number of species S [...]...|$|R
25|$|In <b>statistical</b> <b>inference,</b> {{specifically}} predictive inference, {{a prediction}} interval is {{an estimate of}} an interval in which future observations will fall, with a certain probability, given what has already been observed. Prediction intervals are often used in regression analysis.|$|E
25|$|Consider {{independent}} identically distributed (IID) {{random variables}} with a given probability distribution: standard <b>statistical</b> <b>inference</b> and estimation theory defines {{a random sample}} as the random vector given by the column vector of these IID variables. The population being examined is described by a probability distribution that may have unknown parameters.|$|E
25|$|The {{difference}} {{in point of}} view between classic probability theory and sampling theory is, roughly, that probability theory starts from the given parameters of a total population to deduce probabilities that pertain to samples. <b>Statistical</b> <b>inference,</b> however, moves in the opposite direction—inductively inferring from samples to the parameters of a larger or total population.|$|E
50|$|Apart from {{philosophical}} considerations {{about how}} to make <b>statistical</b> <b>inferences</b> and decisions, much of statistical theory consists of mathematical statistics, and is closely linked to probability theory, to utility theory, and to optimization.|$|R
50|$|Methods {{for obtaining}} valid <b>statistical</b> <b>inferences</b> in the {{presence}} of unobserved heterogeneity include the instrumental variables method; multilevel models, including fixed effects and random effects models; and the Heckman correction for selection bias.|$|R
40|$|Previous {{applications}} of {{data envelopment analysis}} (DEA) and its subsequent Malmquist indices to efficiency and productivity measurements have been criticised for not providing <b>statistical</b> <b>inferences</b> regarding the significance of observed results. In this paper, DEA and a Malmquist index are combined with a bootstrap method {{in order to provide}} succinct <b>statistical</b> <b>inferences</b> that determine the performance of grain producers in Eastern Norway. The data cover the period between 1987 and 1997. Results reveal: (i) a significant degree of inefficiency (approximately 11 %) and an average productivity progress of 38 % over the period considered; (ii) the formidable productivity progress observed is primarily explained by technical efficiency changes that enabled producers to catch up with front runners; and (iii) environmental factors, such as weather conditions, impact both efficiency and productivity. Finally, the analysis reveals that using bootstrapping to make <b>statistical</b> <b>inferences</b> suggests that researchers should be careful in making performance comparisons based on conventional DEA methods, as any discovered differences may not be significant. Significance testing of DEA Bootstrapping Efficiency measurement Productivity growth Grain production...|$|R
25|$|Usually a {{constant}} is included {{as one of}} the regressors. For example, we can take x'i1=1 for i=1,...,n. The corresponding element of β is called the intercept. Many <b>statistical</b> <b>inference</b> procedures for linear models require an intercept to be present, so it is often included even if theoretical considerations suggest that its value should be zero.|$|E
25|$|The general ARMA {{model was}} {{described}} in the 1951 thesis of Peter Whittle, who used mathematical analysis (Laurent series and Fourier analysis) and <b>statistical</b> <b>inference.</b> ARMA models were popularized by a 1970 book by George E. P. Box and Jenkins, who expounded an iterative (Box–Jenkins) method for choosing and estimating them. This method was useful for low-order polynomials (of degree three or less).|$|E
25|$|ABC methods {{bypass the}} {{evaluation}} of the likelihood function. In this way, ABC methods widen the realm of models for which <b>statistical</b> <b>inference</b> can be considered. ABC methods are mathematically well-founded, but they inevitably make assumptions and approximations whose impact needs to be carefully assessed. Furthermore, the wider application domain of ABC exacerbates the challenges of parameter estimation and model selection.|$|E
30|$|Consequently, {{based on}} {{computational}} analyses and <b>statistical</b> <b>inferences</b> (“Computational analysis”) {{we conclude that}} operational decisions can be significantly effective {{in a way that}} reduces transportation costs and improves the level of green ratio and customer satisfaction.|$|R
5000|$|Equivalence {{tests are}} a {{variation}} of hypothesis tests used to draw <b>statistical</b> <b>inferences</b> from observed data. In equivalence tests, the null hypothesis is defined as an effect {{large enough to be}} deemed interesting, specified by an equivalence bound. The alternative hypothesis is any effect that is less extreme than said equivalence bound. The observed data is statistically compared against the equivalence bounds. If the statistical test indicates the observed data is surprising, assuming that true effects as least as extreme as the equivalence bounds, a Neyman-Pearson approach to <b>statistical</b> <b>inferences</b> can be used to reject effect sizes larger than the equivalence bounds with a pre-specified Type 1 error rate.|$|R
50|$|In recent decades, econometricians have {{increasingly}} turned {{to use of}} experiments to evaluate the often-contradictory conclusions of observational studies. Here, controlled and randomized experiments provide <b>statistical</b> <b>inferences</b> that may yield better empirical performance than do purely observational studies.|$|R
25|$|One of {{the many}} {{applications}} of Bayes' theorem is Bayesian inference, a particular approach to <b>statistical</b> <b>inference.</b> When applied, the probabilities involved in Bayes' theorem may have different probability interpretations. With the Bayesian probability interpretation the theorem expresses how a subjective degree of belief should rationally change to account for availability of related evidence. Bayesian inference is fundamental to Bayesian statistics.|$|E
25|$|In {{the debate}} {{of the period}} on {{statistics}} (qua data collection) and what is now <b>statistical</b> <b>inference,</b> the BAAS in its Statistical Section (which owed something also to Whewell) opted for data collection. This Section was the sixth, established in 1833 with Babbage as chairman and John Elliot Drinkwater as secretary. The foundation of the Statistical Society followed. Babbage was its public face, backed by Richard Jones and Robert Malthus.|$|E
25|$|The soft-margin {{support vector machine}} {{described}} above {{is an example of}} an empirical risk minimization (ERM) algorithm for the hinge loss. Seen this way, support vector machines belong to a natural class of algorithms for <b>statistical</b> <b>inference,</b> and many of its unique features are due to the behavior of the hinge loss. This perspective can provide further insight into how and why SVMs work, and allow us to better analyze their statistical properties.|$|E
40|$|VAR {{models with}} common {{cyclical}} features imply parsimonious univariate final equations, justifying {{the use of}} both time series aggregation, and homogenous panels with common factors and dynamic heterogeneity. However, conducting <b>statistical</b> <b>inferences</b> based on too restrictive models might be mis-leadin...|$|R
5000|$|Without adequate, {{comprehensive}} observational {{data from}} direct surveys, {{many of the}} <b>statistical</b> <b>inferences</b> made are simply not truly verifiable. All one can then say about the estimates is, that they are [...] "probably fairly accurate, given previous and other concurrent data." ...|$|R
40|$|Linear {{representations}} and {{linear dimension}} reduction techniques are {{very common in}} signal and image processing. Many such applications reduce to solving problems of stochastic optimizations or <b>statistical</b> <b>inferences</b> {{on the set of}} all subspaces, i. e. a Grassmann manifold. Central to solving them is the computation of an "exponential" map (for constructing geodesics) and its inverse on a Grassmannian. Here we suggest efficient techniques for these two steps and illustrate two applications: (i) For image-based object recognition, we define and seek an optimal linear representation using a Metropolis-Hastings type, stochastic search algorithm on a Grassmann manifold. (ii) For <b>statistical</b> <b>inferences,</b> we illustrate computation of sample statistics, such as mean and variances, on a Grassmann manifold...|$|R
25|$|The {{field is}} at the {{intersection}} of mathematics, statistics, computer science, physics, neurobiology, and electrical engineering. The theory has also found applications in other areas, including <b>statistical</b> <b>inference,</b> natural language processing, cryptography, neurobiology, the evolution and function of molecular codes (bioinformatics), model selection in statistics, thermal physics, quantum computing, linguistics, plagiarism detection, pattern recognition, and anomaly detection. Important sub-fields of information theory include source coding, channel coding, algorithmic complexity theory, algorithmic information theory, information-theoretic security, and measures of information.|$|E
25|$|Regression {{analysis}} {{and in particular}} ordinary least squares specifies that a dependent variable depends according to some function upon one or more independent variables, with an additive error term. Various types of <b>statistical</b> <b>inference</b> on the regression assume that the error term is normally distributed. This assumption can be justified by assuming that the error term is actually the sum {{of a large number}} of independent error terms; even if the individual error terms are not normally distributed, by the central limit theorem their sum can be well approximated by a normal distribution.|$|E
25|$|In 1984, Peter Diggle and Richard Gratton {{suggested}} using {{a systematic}} simulation scheme to approximate the likelihood function {{in situations where}} its analytic form is intractable. Their method was based on defining a grid in the parameter space and using it to approximate the likelihood by running several simulations for each grid point. The approximation was then improved by applying smoothing techniques to the outcomes of the simulations. While {{the idea of using}} simulation for hypothesis testing was not new, Diggle and Gratton seemingly introduced the first procedure using simulation to do <b>statistical</b> <b>inference</b> under a circumstance where the likelihood is intractable.|$|E
30|$|Finally, {{the cut-off}} used for <b>statistical</b> <b>inferences</b> from <b>statistical</b> {{parametric}} maps is variable. Although a p value less than 0.001 uncorrected for multiple comparisons is frequently employed [14, 15, 29, 38], less rigid thresholds {{have been applied}} as well [28, 30, 31, 37].|$|R
40|$|The {{interpretations of}} <b>statistical</b> <b>inferences</b> from meta-analyses {{depend on the}} degree of {{heterogeneity}} in the meta-analyses. Several new indices of heterogeneity in meta-analyses are proposed, and assessed the variation/difference of these indices through a large simulation study. The proposed methods are applie...|$|R
50|$|In statistics, the {{multiple}} comparisons, multiplicity or multiple testing problem occurs {{when one considers}} a set of <b>statistical</b> <b>inferences</b> simultaneously or infers a subset of parameters selected based on the observed values. In certain fields it {{is known as the}} look-elsewhere effect.|$|R
