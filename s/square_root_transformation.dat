69|10000|Public
5000|$|<b>Square</b> <b>root</b> <b>transformation</b> (if the {{distribution}} differs moderately from normal) ...|$|E
50|$|A variance-stabilizing {{transformation}} aims {{to remove}} a variance-on-mean relationship, so that the variance becomes constant relative to the mean. Examples of variance-stabilizing transformations are the Fisher transformation for the sample correlation coefficient, the <b>square</b> <b>root</b> <b>transformation</b> or Anscombe transform for Poisson data (count data), the Box-Cox transformation for regression analysis and the arcsine <b>square</b> <b>root</b> <b>transformation</b> or angular transformation for proportions (binomial data). While commonly used for statistical analysis of proportional data, the arcsine <b>square</b> <b>root</b> <b>transformation</b> is not recommended because logistic regression or a logit transformation are more appropriate for binomial or non-binomial proportions, respectively, especially due to decreased type-II error.|$|E
50|$|Graphical {{examination}} of count data may be {{aided by the}} use of data transformations chosen to have the property of stabilising the sample variance. In particular, the <b>square</b> <b>root</b> <b>transformation</b> might be used when data can be approximated by a Poisson distribution (although other transformation have modestly improved properties), while an inverse sine transformation is available when a binomial distribution is preferred.|$|E
30|$|The summary {{statistics}} in Table 1 {{show that the}} distributions of PMG and PML are severely skewed and far from normal. In this study, we alleviated skewness by using <b>squared</b> <b>root</b> <b>transformation</b> on both PMG and PML. Possible heteroscedasticity was also taken into consideration. We used Equation (3) to describe the dynamics of PMG and PML. Different ARMA(l, m)-GARCH(p, q) (l= 1, 2; m= 1, 2; p= 1, q= 1) models were used, and the final models were determined by the Akaike Information Criterion (AIC).|$|R
30|$|In all analyses, {{treatment}} {{effects were}} considered to have strong evidence of significance at the 95 % confidence level, and marginal evidence of significance at the 90 % level. We inspected residuals from nested ANOVA and linear mixed effects regression models of response state and change for constant variance across treatments using Levene’s test of homoscedasticity. When residuals were heteroscedastic, we applied treatment level variance functions using R’s varIdent function. Furthermore, we applied <b>square</b> <b>root</b> <b>transformations</b> on responses that showed increasing residual variance with predicted values.|$|R
40|$|Consider {{variance}} stabilizing {{transformations of}} Poisson distribution [pi]([lambda]), binomial distribution B(n,p) and {{negative binomial distribution}} NB(r,p), with <b>square</b> <b>root</b> <b>transformations</b> for [pi]([lambda]), arcsin transformations for B(n,p) and inverse hyperbolic sine transformations for NB(r,p). We will introduce three terms: critical point, domain of dependence and relative error. By comparing the relative errors of the transformed variances of [pi]([lambda]), B(n,p) and NB(r,p), and comparing the skewness and kurtosis of [pi]([lambda]), B(n,p) and NB(r,p) and their transformed variables, we obtain some better transformations with domains of dependence of the parameters. A new kind of transformation for B(n,p) is suggested. ...|$|R
50|$|Here {{the count}} {{variable}} {{would be treated}} as a dependent variable. Statistical methods such as least squares and analysis of variance are designed to deal with continuous dependent variables. These can be adapted to deal with count data by using data transformations such as the <b>square</b> <b>root</b> <b>transformation,</b> but such methods have several drawbacks; they are approximate at best and estimate parameters that are often hard to interpret.|$|E
5000|$|The Lab {{color space}} {{describes}} mathematically all perceivable {{colors in the}} three dimensions L for lightness and a and b for the color opponents green-red and blue-yellow. The terminology [...] "Lab" [...] originates from the Hunter 1948 color space.Nowadays [...] "Lab" [...] is frequently mis-used as abbreviation for CIEL*a*b* 1976 color space (also CIELAB); the asterisks/stars distinguish the CIE version from Hunter's original version. The difference from the Hunter Lab coordinates is that the CIELAB coordinates are created by a cube root transformation of the CIE XYZ color data, while the Hunter Lab coordinates {{are the result of}} a <b>square</b> <b>root</b> <b>transformation.</b> Other, less common examples of color spaces with Lab representations make use of the CIE 1994 color difference and the CIE 2000 color difference.|$|E
3000|$|... is {{the number}} of pixels in a noisy image P, n 2. For Poisson noise, an element of the {{response}} vector is the square root of pixel (i,j) of the noisy image P, Y^P[r] = √(P(i,j)). The <b>square</b> <b>root</b> <b>transformation</b> is derived from the Box-Cox procedure.|$|E
50|$|The {{logarithm}} and <b>square</b> <b>root</b> <b>transformations</b> {{are commonly}} used for positive data, and the multiplicative inverse (reciprocal) transformation {{can be used for}} non-zero data. The power transformation is a family of transformations parametrized by a non-negative value λ that includes the logarithm, <b>square</b> <b>root,</b> and multiplicative inverse as special cases. To approach data transformation systematically, it is possible to use statistical estimation techniques to estimate the parameter λ in the power transformation, thereby identifying the transformation that is approximately the most appropriate in a given setting. Since the power transformation family also includes the identity transformation, this approach can also indicate whether {{it would be best to}} analyze the data without a transformation. In regression analysis, this approach is known as the Box-Cox technique.|$|R
30|$|Analysis of {{treatment}} effects on vegetation and fuels was conducted using ANOVA for each combination of year and variable, with treatment modeled as a fixed effect and block as a random effect. Treatment differences {{within a year}} were considered significant with an overall experimental a of 0.05. However, {{the large number of}} years tested increased the probability of a Type I error, so we used the Bonferroni correction (Bland and Altman 1995) to adjust test statistics. Individual tests for each year were done at α = 0.00625. We made post hoc comparisons using linear contrasts. Because much of the data did not meet the assumption of normality, it was necessary to use data transformations to normalize the distributions. Logarithmic and <b>square</b> <b>root</b> <b>transformations</b> were used in these analyses.|$|R
40|$|Computer {{simulations}} {{are used}} to examine the significance levels and powers of several tests which have been employed to compare the means of Poisson distributions. In particular, attention is focused on the behaviour of the tests when the means are small, {{as is often the case}} in ecological studies when populations of organisms are sampled using quadrats. Two approaches to testing are considered. The first assumes a log linear model for the Poisson data and leads to tests based on the deviance. The second employs standard analysis of variance tests following data transformations, including the often used logarithmic and <b>square</b> <b>root</b> <b>transformations.</b> For very small means it is found that a deviance-based test has the most favourable characteristics, generally outperforming analysis of variance tests on transformed data; none of the latter appears consistently better than any other. For larger means the standard analysis of variance on untransformed data performs well...|$|R
30|$|We used one-way {{analysis}} of covariance with elevation as a covariate to test for differences among fire management types for start date, duration, burning index, percent fire severity level, size, and return interval. For the percent fire severity analysis, we analyzed each severity level separately using the arcsine <b>square</b> <b>root</b> <b>transformation</b> to normalize the data. We used two-way {{analysis of}} covariance with elevation as a covariate to test for differences among fire management types and fire severity levels for the spatial complexity metrics. We normalized SqP values with the arcsine <b>square</b> <b>root</b> <b>transformation.</b> All hypotheses tests were 2 -tailed and at the 0.05 significance level. Bonferroni multiple comparison tests were used to detect differences among means. For these comparisons, the significance levels were adjusted to 0.0167 for one-way ANOVAs and to 0.0083 two-way ANOVAs.|$|E
3000|$|... {{representing}} the cumulative percentage released {{at each of}} the selected n time points under pH 7.4 and 5.0, respectively. f 2 was described as a similarity factor. The factor f 2 is a logarithmic reciprocal <b>square</b> <b>root</b> <b>transformation</b> of the sum of squared error and it is showing the percentage similarity between the two curves [29].|$|E
30|$|All {{analyses}} were undertaken using SAS software (SAS Institute Inc. 2008). Percentage mortality was {{the dependent variable}} used in analyses and this variable was transformed for analysis using an arcsine <b>square</b> <b>root</b> <b>transformation</b> to meet the underlying assumptions of the models used. Within each plot, mean mortality in 2 m height classes was used within analyses so {{that the effects of}} treatment and tree height on mortality could be examined.|$|E
30|$|The {{effects of}} fire {{severity}} on forest vegetation responses were examined using linear regression. We assessed model assumptions (linearity, homoscedasticity, independence, and normally distributed residuals) using the gvlma (Global Validation of Linear Models Assumption) package (Pena and Slate 2014, R Core Team 2018) and corrected for any violations using <b>square</b> <b>root</b> <b>transformations.</b> We {{examined the relationship}} between fire severity (CBI, given as a continuous variable) and total basal area (all stems ≥ 2  cm DBH), first using a Pearson’s correlation test. We found fire severity and basal area to be correlated (Pearson’s correlation[*]=[*]− 0.77, t 1, 23 [*]=[*]− 5.8, P[*]≤[*] 0.001), so to avoid any issues with multicollinearity, we refrained from including both variables as co-variates in the same model. Additionally, we used simple linear regression to test the effect of fire severity on total basal area and stem density (stems ≥ 2  cm DBH) in each sampling year to visualize the response of each at those time points.|$|R
40|$|Heritabilities {{and genetic}} {{correlations}} were estimated for milk and fat yields and fat percentage from 305 -d, mature-equivalent records of registered and grade Holsteins in California, New York, and Wisconsin. Parameter estimates were obtained with daughter on dam regression within herd-year-season and sire of daughter. Data were pairs of first lactation records (38, 115 in California, 171, 555 in New York, and 136, 031 in Wisconsin) from 1975 through 1984. California had higher means for milk (9046 kg) and fat (323 kg) {{than the other}} two states (approximately 7840 kg and 289 kg), but heritabilities of. 36 and. 34 were not greatly different from those for New York (. 34 and. 33) or Wisconsin (. 38 and. 35). Heritability of fat percentage, however, was smaller for California (. 55 vs. 66 for New York and. 68 for Wisconsin). Logarithmic and <b>square</b> <b>root</b> <b>transformations</b> of the data resulted in little change in heritability estimates. For California, genetic correlation between milk and fat yields (. 70) was larger than for New York (. 55) or Wisconsin (. 56), and correlation between fat yield and fat percentage was smaller (. 31 vs. 45 and. 41). For each state, estimates of heritabilities were larger for registered than grade pairs...|$|R
40|$|Abstract. Let G be a {{countable}} Abelian {{group with}} Z d as a subgroup so that G/Z d is a locally finite group. (An Abelian group is locally finite if every element has finite order.) We can construct a rank one action of G {{so that the}} Z-subaction is 2 -simple, 2 -mixing and only commutes with the other transformations in the action of G. Applications of this construction include a <b>transformation</b> with <b>square</b> <b>roots</b> of all orders but no infinite <b>square</b> <b>root</b> chain, a <b>transformation</b> with countably many nonisomorphic <b>square</b> <b>roots,</b> a new proof of an old theorem of Baxter and Akcoglu on <b>roots</b> of <b>transformations,</b> and a simple map with no prime factors. The last example, originally constructed by del Junco, was the inspiration for this work. Content...|$|R
30|$|In principle, {{it might}} be {{possible}} to reparametrise model (27) in such a way that the single singularity of the information matrix is due to the score of one of the new parameters becoming identically 0. In that case, a <b>square</b> <b>root</b> <b>transformation</b> of this parameter should allow one to derive a joint extremum test of H_ 0 :ψ _x=ψ _u= 0 along the lines of Sect.  3.4. In the interest of space, we shall not explore this possibility.|$|E
3000|$|Data were analyzed, using one- and two-way {{analysis}} of variance (ANOVA) (SPSS 16.0) to evaluate the effect of main treatments (i.e., species/strains of Trichogramma and durations of storage) on parasitoid performance. When statistical difference existed between data sets, Fisher’s LSD test was performed to separate the means. The mean percentage values (X[*]±[*] 0.5) were subjected to angular transformation and whole numbers to <b>square</b> <b>root</b> <b>transformation</b> (√(X+ 0.5)) [...] when large variations were observed among the data sets.|$|E
30|$|All {{analyses}} were undertaken using SAS software (SAS Institute Inc. 2000). Percentage mortality was {{the dependent variable}} used in analyses and this variable was transformed for analysis using an arcsine <b>square</b> <b>root</b> <b>transformation</b> to meet the underlying assumptions of the models used. Within each plot, mean mortality in 2  m height classes (0 – 2.0  m; 2.1 – 4.0  m etc.) was used within analyses so {{that the effects of}} treatment and tree height on mortality could be examined.|$|E
40|$|Abstract. In robotics, a {{key problem}} {{is for a}} robot to explore its {{environment}} and use the information gathered by its sensors to jointly produce a map of its environment, together with an estimate of its position: so-called SLAM (Simultaneous Localization and Mapping) [13]. Various filtering methods – Particle Filtering, and derived Kalman Filter methods (Extended, Unscented) – have been applied successfully to SLAM. We present a new algorithm that applies the <b>Square</b> <b>Root</b> Unscented <b>Transformation</b> [14], previously only applied to feature based maps [7], to particle filtering for grid mapping. Experimental results show improved computational performance on more complex grid maps compared to a well-known existing grid based particle filtering algorithm, GMapping [2]. ...|$|R
30|$|Research {{on the use}} {{of optical}} sensor {{satellite}} imagery for estimating AGB in the miombo ecoregion is more limited. Næsset et al. (2016) used a linear ordinary least squares model and a stepwise predictor variable selection method. As potential predictor variables, these authors included canopy cover predictions and canopy cover gain or loss from Hansen et al. (2013), raw band values from Landsat 7 ETM+ and Landsat 8 OLI, NDVI calculated from the Landsat data, and <b>square</b> and <b>square</b> <b>root</b> <b>transformations</b> for all of these predictor variables. Based on their stepwise predictor variable selection procedure, squared canopy cover was the only predictor variable chosen. While their finding helps to corroborate our result of canopy cover and squared canopy cover in the sigmoidal model, their model resulted in poor AGB prediction with an RMSPE% of 87  %. It is possible that their model using Landsat data was disadvantaged by two aspects. First, they did not include other commonly used vegetation indices (i.e., NDMI, NBR) or texture variables (i.e. standard deviation in a 3 [*]×[*] 3 window around the ground plot), which we found to be useful in our study, as have others (e.g., Lu et al. 2014). Second, the Landsat data that Næsset et al. used differed by at least a year from the time of their ground plot measurements.|$|R
40|$|In {{this paper}} we study the {{geometry}} of the set of real <b>square</b> <b>roots</b> of ± I_ 2. After some introductory remarks, we begin our study by deriving by quite elementary methods the forms of the real <b>square</b> <b>roots</b> of ± I_ 2. We then discuss the interpretations of these <b>square</b> <b>roots</b> as <b>transformations</b> of the cartesian (x,y) -plane. To study {{the geometry of}} the set of <b>square</b> <b>roots</b> of ± I_ 2 we consider a slightly more general set of square matrices of order 2 and show that these sets are hyperboloids of one sheet or hyperboloids of two sheets. From these general results we conclude that the set of involutory matrices of order 2 is a hyperboloid of one sheet and the set of skew-involutory matrices of order 2 is a hyperboloid of two sheets. The relations between the geometrical properties of the hyperboloids and the set of <b>square</b> <b>roots</b> of I_ 2 are also investigated. We then proceed to obtain the forms of the involutory matrices of order 2 by more advanced methods. We have considered two approaches: in the first approach we use {{the concept of a}} function of a matrix and in the second approach we use concepts of split-quaternions. Comment: 32 pages, 11 figure...|$|R
30|$|Comparison of trap catches {{with and}} without lures was {{analysed}} using a general linear model (GLM). The model incorporated fixed effects of treatment, including block number as a random factors. Means were compared using a Dunnet’s post-hoc test. Prior to analysis trap catch data was subjected to <b>square</b> <b>root</b> <b>transformation,</b> conforming to GLM assumption of homogeneity of variance, which was confirmed using levene’s test of equality of error variances. All statistical analyses were carried out using SPSS software (IBM Corporation, USA).|$|E
40|$|We {{formulate}} {{and evaluate}} weighted and {{ordinary least squares}} procedures for estimating the parametric rate function of a nonhomogeneous Poisson process. Special emphasis is given to processes having an exponential rate function, where the exponent may include a polynomial component or some trigonometric components or both. Theoretical and experimental evidence is provided to explain some surprising problems with the weighted least squares procedure. The ordinary least squares procedure {{is based on a}} <b>square</b> <b>root</b> <b>transformation</b> of the "detrended" event times; and the results of an extensive Monte Carlo study are summarized to show {{the advantages and disadvantages of}} this procedure...|$|E
40|$|A rootogram is a {{graphical}} {{data analysis}} technique for summarizing the distributional information of a variable. It consists of: Vertical axis = square root of frequencies or relative frequencies; Horizontal axis = response variable. There are 4 types of rootograms: 1. rootogram (absolute counts); 2. relative rootogram (converts counts to proportions); 3. cumulative rootogram; 4. cumulative relative rootogram. The rootogram is {{a modified version}} of a histogram. It plots the square roots of the frequencies rather than the raw frequencies. Many univariate data sets can be normalized with a <b>square</b> <b>root</b> <b>transformation</b> (particularly counts or measurement data that have a lower bound and tend to be skewed at the upper tail) ...|$|E
40|$|BACKGROUND: Published {{predicted}} {{values for}} {{total lung capacity}} and residual volume are often based on {{a small number of}} subjects and derive from different populations from predicted spirometric values. Equations from the only two large studies gave smaller predicted values for total lung capacity than the smaller studies. A large number of subjects have been studied from a population which has already provided predicted values for spirometry and transfer factor for carbon monoxide. METHODS: Total lung capacity was measured from standard posteroanterior and lateral chest radiographs and forced vital capacity by spirometry in a population sample of 771 subjects. Prediction equations were developed for total lung capacity (TLC), residual volume (RV) and RV/TLC in two groups [...] normal and total. Subjects with signs or symptoms of cardiopulmonary disease were combined with the normal subjects and equations for all subjects were also modelled. RESULTS: Prediction equations for TLC and RV in non-smoking normal men and women were <b>square</b> <b>root</b> <b>transformations</b> which included height and weight but not age. They included a coefficient for duration of smoking in current smokers. The predictive equation for RV/TLC included weight, age, age and duration of smoking for current smokers and ex-smokers of both sexes. For the total population the equations took the same form but the height coefficients and constants were slightly different. CONCLUSION: These population based prediction equations for TLC, RV and RV/TLC provide reference standards in a population that has provided reference standards for spirometry and single breath transfer factor for carbon monoxide...|$|R
40|$|Domoic acid (DA), a neurotoxin {{implicated in}} amnesic {{shellfish}} poisoning episodes, {{is produced by}} the pennate diatom Pseudo-nitzschia multiseries and its presence is associated with natural blooms of this marine microorganism. Laboratory studies indicate that production of the toxin by the diatom is related to physiological stress caused by limitation of nutrients, such as phosphate and silicate. This study attempts to develop predictive models {{of the amount of}} DA present, using data from laboratory physiological studies of the diatom, and from a combination of the laboratory data with field data from one Atlantic and two Pacific Ocean sites between 1988 and 1998. Such models can provide early warning of a potential poisoning episode without the sophisticated chemical determinations of domoic acid and avoid delays due to lack of analytical facilities and expertise. Predictor variables include nutrient ratios and cell abundance of Pseudo-nitzschia multiseries, determinations of which are both simple and inexpensive. Both linear and logistic regression methods are employed. Natural logarithm and <b>square</b> <b>root</b> <b>transformations</b> of predictors and response variables are needed to linearize the modeled relationship. In total, four models are proposed. Three of these models have r 2 ranging between 0. 60 to 0. 80 and are based on between four and seven predictors. Split sample reliability testing indicated that shrinkage on cross-validation for one of these models was below 35 %. The fourth, a logistic regression model developed using the split-sample approach, has 2 predictors. In the training data, the prediction sensitivity and specificity of this model were approximately 76 %. In the Validation data, the specificity increased to 87 %, while the sensitivity declined to 67 %. Peer reviewed: YesNRC publication: Ye...|$|R
40|$|Many stabled {{horses are}} {{maintained}} {{in very different}} conditions from those in which they evolved. The diet of feral horses includes many grasses and browse species, however, most stabled horses are given a single forage. In four replicated trials, twelve competition horses were introduced into each of two identical stables containing a single forage, or six forages for five minutes. To detect novelty effects, {{in the first and}} third trials the single forage was hay. In the second and fourth it was the preferred forage from the preceding trial. Trials were videotaped and 13 mutually exclusive behaviour patterns compared. After these sessions horses were allowed five minutes to choose between stables and duration in each compared. Data were evaluated using Observer 3 and SPSS v 8. <b>Square</b> <b>root</b> <b>transformations</b> normalised the data allowing GLM factorial ANOVA. Similar results were obtained from all trials, and some effects of monotony were detected. Hay was the least preferred forage throughout. Behaviour was significantly different between stables in all trials, e. g.; during the third trial in the single forage stable horses looked over the stable door more frequently (F 1, 11 = 65. 9; p= 0. 001), moved for longer (F 1, 11 = 161. 6; p= 0. 001), foraged on straw bedding longer (F 1, 11 = 35. 9; p= 0. 001), and exhibited behaviour indicative of frustration (F 1, 11 = 8. 5; p= 0. 014) more frequently. When allowed to choose, horses spent more time in the Multiple forage stable (arcsine transformed proportional data, t-test: t= 19. 8; DF= 10; p= 0. 001). In these trials behaviour in stables with single or multiple forages was significantly different. When allowed to choose, horses showed a preference for the multiple forage environment. Further study is required to determine whether these effects persist over longer periods. However, these trials indicate that enrichment of the stable environment through the provision of multiple forages may have welfare benefits for domestic horses, in reducing straw consumption and behaviour indicative of frustration...|$|R
30|$|We {{selected}} models {{based on}} the type of data (including density and proportion), dispersion of the data, model residuals, and AIC. We tested model families and chose the model family with reasonable dispersion and residuals and the lowest AIC (Mazerolle 2017). For models with plant density as response variables, we tested Gaussian, <b>square</b> <b>root</b> <b>transformation,</b> and Poisson error (Appendix B Table 1). Density of non-native plants and annual grasses required transformed zero-inflated models (Fournier et al. 2012, Skaug et al. 2012). For shrub cover and its proportion data, we assumed a binomial error distribution and used the transect length as an offset (Appendix B Table 1). We conducted all analyses in R 3.1. 2 (R Development Core Team 2008).|$|E
30|$|The {{design of}} {{experiment}} (DoE) protocol was designed in Design-Expert version 7.1 (StatEase). The design created was a circumscribed three factor, 5 -level central composite design with 10 repeats; with each factor at level ‘ 0 ’, resulting in 24 runs. Once data had been collected they were checked for normality using a normal probability plot so as to ensure suitability for parametric statistical testing. The PelB productivity data were normalised by a <b>square</b> <b>root</b> <b>transformation.</b> The models produced were subsequently checked for significance and insignificant lack of fit. Models were used to produce contour plots, with each showing the response while two factors varied, and the third factor was maintained at level ‘ 0 ’.|$|E
30|$|In Figure  4 we plot {{the fear}} {{correlation}} (with <b>square</b> <b>root</b> <b>transformation),</b> {{the volume of}} #prayforboston and #bostonstrong against the geo-proximity, social tie and personal visit between Boston and other cities. The fear correlation is computed based on fear indices between April 15 and 21. The volumes of hashtags are calculated as total number of tweets containing the hashtags posted between April 15 and 30. The first-order correlations are shown {{on top of each}} scatterplot. We compute Pearson correlation coefficients for correlations between the transformed fear correlation and the three variables, and compute Spearman’s rank correlation coefficients for correlations involving the hashtag counts (the volumes of #prayforboston and #bostonstrong) due to their skewed distributions.|$|E
40|$|Published studies {{investigating}} foraging behaviour with bulk forages {{have identified}} the importance of variety. Whether restricted sensory variety also affects foraging behaviour on concentrate diets is currently unclear. Foraging was identified as sniff, manipulate, chew or ingest a foodstuff. To investigate this in three replicated trials, up to eight horses were introduced into each of two identical stables containing a single concentrate feed, or four concentrate feeds for five minutes. In order to control for palatability effects each concentrate was presented as the single concentrate option on two occasions within each trial. Trials were videotaped and 12 mutually exclusive behaviour patterns compared. Data was evaluated using Observer 3 and SPSS v 10. <b>Square</b> <b>root</b> <b>transformations</b> normalised the data allowing GLM ANOVA. Commercially available low energy concentrates used in Trials 1 and 2 presented a range of sensory variety. In Trial 3, four otherwise identical base diets were presented flavoured with molasses, garlic, mint or herbs, to test whether manipulating one sensory characteristic was sufficient to effect changes in behaviour and diet selection. When Single or Multiple concentrates were presented significant differences in foraging and non-foraging behaviour were recorded in all three trials e. g. Foraging bouts were longer in Single than Multiple sessions (Trial 1 : F 18. 1 df 7 P< 0. 005 Trial 2 : F 9. 4 df 5 P< 0. 05, Trial 3 F 12 df 7 P< 0. 05), Stand duration was also longer in the Single session (Trial 1 : F 21. 2 df 7 P< 0. 005, Trial 2 : F 15. 7 df 5 P< 0. 01, Trial 3 : F 9. 2 df 7 P< 0. 05). In all trials, multiple session non-foraging behaviour, foraging behaviour and selection patterns resembled that reported for free ranging horses more closely. Further study is required to determine whether these effects persist over longer periods. However, these trials indicate that promoting sensory variety in concentrate diets facilitates the expression of highly motivated foraging behaviour...|$|R
40|$|Abstract: This paper {{addresses}} the stochastic modeling of recorded Times Between Failures (TBF) of electric rail vehicles, {{taking into account}} scheduled maintenance actions these vehicles undergo. The study is based upon historical data from the Athens Electric Railways and a novel non-stationary Functional Series (FS) modeling framework, which allows for the modeling, scheduled maintenance actions incorporation, analysis, as well as failure time prediction. Modeling of TBF series using <b>square</b> <b>root</b> and logarithmic <b>transformations</b> is considered as a simple means of accounting for non-Gaussianity and positiveness. The {{results of the study}} indicate that FS models incorporating scheduled maintenance actions are signi cantly better than models not accounting for them. It is specically shown that knowledge of the maintenance actions signicantly improves prediction of the next Time to Failure...|$|R
30|$|We used {{univariate}} {{analysis of}} variance (ANOVA) to assess all the measured variables using SPSS© (IBM, Armonk, New York, USA). We tested all variables for normality and for homogeneity of variance using the Levene’s test of equality of error variances. When assumptions were not met, we performed a <b>square</b> <b>root</b> or log <b>transformation</b> of the data. Untransformed data are presented in the results. We used the Tukey post-hoc test to examine differences between treatments. We determined significant differences for all tests with a = 0.05. Univariate ANOVA determined {{that there were no}} significant differences in variables among different burned areas within a fire management strategy (i.e., Martinez versus Johnson fires; see Table 1). Thus, we combined variables from all fires within a fire management strategy in the analysis.|$|R
