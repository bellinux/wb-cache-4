864|2416|Public
5|$|The main {{antagonist}} of the story, Nitros Oxide, is {{the self-proclaimed}} fastest racer {{in the galaxy}} who threatens to turn Earth into a concrete parking lot. Preceding Oxide are four boss characters: Ripper Roo, a deranged straitjacket-wearing kangaroo; Papu Papu, the morbidly obese leader of the island's native tribe; Komodo Joe, a Komodo dragon with a <b>speech</b> <b>sound</b> disorder; and Pinstripe Potoroo, a greedy pinstripe-clad potoroo. The four boss characters, along with an imperfect and morally ambiguous clone of Crash Bandicoot named Fake Crash, become accessible as playable characters if the Adventure Mode is fully completed. PlayStation ] - from GamePro.com |url=http://www.gamepro.com/article/reviews/2609/crash-team-racing/ |date=November 24, 2000 |publisher=GamePro |accessdate=September 25, 2009|archiveurl=https://web.archive.org/web/20090426011613/http://www.gamepro.com/article/reviews/2609/crash-team-racing/|archivedate=2009-04-26}} Also appearing as secret characters are Doctor Nefarious Tropy, the self-proclaimed Master of Time and who is unlocked by beating his records in the Time Trial Mode; and Penta Penguin, a neutral penguin who can only be unlocked via a cheat code at the main menu. Contrary to popular belief, Nitros Oxide is not a playable character in the game.|$|E
5|$|Changes {{may affect}} {{specific}} sounds or the entire phonological system. Sound change can consist of {{the replacement of}} one <b>speech</b> <b>sound</b> or phonetic feature by another, {{the complete loss of}} the affected sound, or even the introduction of a new sound in a place where there had been none. Sound changes can be conditioned in which case a sound is changed only if it occurs in the vicinity of certain other sounds. Sound change is usually assumed to be regular, which means that it is expected to apply mechanically whenever its structural conditions are met, irrespective of any non-phonological factors. On the other hand, sound changes can sometimes be sporadic, affecting only one particular word or a few words, without any seeming regularity. Sometimes a simple change triggers a chain shift in which the entire phonological system is affected. This happened in the Germanic languages when the sound change known as Grimm's law affected all the stop consonants in the system. The original consonant * became /b/ in the Germanic languages, the previous * in turn became /p/, and the previous * became /f/. The same process applied to all stop consonants and explains why Italic languages such as Latin have p in words like pater and pisces, whereas Germanic languages, like English, have father and fish.|$|E
25|$|During {{imitation}} the DIVA model organizes its <b>speech</b> <b>sound</b> map and tunes the synaptic projections between <b>speech</b> <b>sound</b> map {{and motor}} map - i.e. tuning of forward motor commands - {{as well as}} the synaptic projections between <b>speech</b> <b>sound</b> map and sensory target regions (see Fig. 4). Imitation training is done by exposing the model to an amount of acoustic speech signals representing realizations of language-specific speech units (e.g. isolated speech sounds, syllables, words, short phrases).|$|E
40|$|Most people acquire {{literacy}} skills with remarkable ease, {{even though the}} human brain is not evolutionarily adapted to this relatively new cultural phenomenon. Associations between letters and <b>speech</b> <b>sounds</b> {{form the basis of}} reading in alphabetic scripts. We investigated the functional neuroanatomy of the integration of letters and <b>speech</b> <b>sounds</b> using functional magnetic resonance imaging (fMRI). Letters and <b>speech</b> <b>sounds</b> were presented unimodally and bimodally in congruent or incongruent combinations. Analysis of single-subject data and group data aligned on the basis of individual cortical anatomy revealed that letters and <b>speech</b> <b>sounds</b> are integrated in heteromodal superior temporal cortex. Interestingly, responses to <b>speech</b> <b>sounds</b> in a modality-specific region of the early auditory cortex were modified by simultaneously presented letters. These results suggest that efficient processing of culturally defined associations between letters and <b>speech</b> <b>sounds</b> relies on neural mechanisms similar to those naturally evolved for integrating audiovisual speech...|$|R
40|$|The motor {{regions that}} control {{movements}} of the articulators activate during listening to speech and contribute to performance in demanding speech recognition and discrimination tasks. Whether the articulatory motor cortex modulates auditory processing of <b>speech</b> <b>sounds</b> is unknown. Here, we aimed {{to determine whether the}} articulatory motor cortex affects the auditory mechanisms underlying discrimination of <b>speech</b> <b>sounds</b> in the absence of demanding speech tasks. Using electroencephalography, we recorded responses to changes in sound sequences, while participants watched a silent video. We also disrupted the lip or the hand representation in left motor cortex using transcranial magnetic stimulation. Disruption of the lip representation suppressed responses to changes in <b>speech</b> <b>sounds,</b> but not piano tones. In contrast, disruption of the hand representation had no effect on responses to changes in <b>speech</b> <b>sounds.</b> These findings show that disruptions within, but not outside, the articulatory motor cortex impair automatic auditory discrimination of <b>speech</b> <b>sounds.</b> The findings provide evidence for the importance of auditory-motor processes in efficient neural analysis of <b>speech</b> <b>sounds...</b>|$|R
40|$|The {{categorisation}} of <b>speech</b> <b>sounds</b> {{by adults}} and children A study of the categorical perception hypothesis and the developmental weighting of acoustic speech cues The categorisation of <b>speech</b> <b>sounds</b> by {{adults and children}} The categorisation of <b>speech</b> <b>sounds</b> by adults and children De classificatie van spraakklanken door volwassen luisteraars en kinderen (met een samenvatting in het Nederlands) PROEFSCHRIFT ter verkrijging van de graad van docto...|$|R
25|$|On {{the other}} hand the <b>speech</b> <b>sound</b> map, if {{activated}} for a specific speech unit (single neuron activation; punctual activation), activates sensory information by synaptic projections between <b>speech</b> <b>sound</b> map and auditory target region map and between <b>speech</b> <b>sound</b> map and somatosensory target region map. Auditory and somatosensory target regions {{are assumed to be}} located in higher-order auditory cortical regions and in higher-order somatosensory cortical regions respectively. These target region sensory activation patterns - which exist for each speech unit - are learned during speech acquisition (by imitation training; see below: learning).|$|E
25|$|Diphthongs use two vowel {{sounds in}} one {{syllable}} {{to make a}} <b>speech</b> <b>sound.</b>|$|E
25|$|Primary {{pediatric}} {{speech and}} language disorders include receptive and expressive language disorders, <b>speech</b> <b>sound</b> disorders, childhood apraxia of speech, stuttering, and language-based learning disabilities.|$|E
50|$|The vocal-auditory channel {{describes}} the way vocal signals {{can be used}} to produce language. The speaker uses a vocal tract (containing most of the speech organs) to produce <b>speech</b> <b>sounds,</b> and the hearer employs an auditory apparatus (the sense of hearing) to receive and process the <b>speech</b> <b>sounds.</b> This is why human language is said to be based on <b>speech</b> <b>sounds</b> produced by the articulatory system and received through the auditory system. The vocal channel is a particularly excellent means through which <b>speech</b> <b>sounds</b> can be accompanied or substituted by gestures, facial expressions, body movement, and way of dressing.|$|R
40|$|This paper {{presents}} an ëlitist approachf̈or extracting automatically well-realized <b>speech</b> <b>sounds</b> with high confidence. The elitist approach uses a speech recognition {{system based on}} Hidden Markov Models (HMM). The HMM are trained on <b>speech</b> <b>sounds</b> which are systematically well-detected in an iterative procedure. The results show that, by using the HMM models defined in the training phase, the speech recognizer detects reliably specific <b>speech</b> <b>sounds</b> with a small rate of errors...|$|R
50|$|Articulation, often {{associated}} with speech production, is the term used to describe how people physically produced <b>speech</b> <b>sounds.</b> For people who speak fluently, articulation is automatic and allows 15 <b>speech</b> <b>sounds</b> to be produced per minute.|$|R
25|$|The neural {{representation}} for speech units {{occurring in}} the <b>speech</b> <b>sound</b> map (see below: DIVA model) is a punctual or local representation. Each speech item or speech unit is represented here by a specific neuron (model cell, see below).|$|E
25|$|During {{imitation}} the DIVA {{model is}} also capable of tuning the synaptic projections from <b>speech</b> <b>sound</b> map to somatosensory target region map, since each new imitation attempt produces a new {{articulation of the}} speech item and thus produces a somatosensory state pattern which {{is associated with the}} phonemic representation of that speech item.|$|E
25|$|A speech unit {{represents}} {{an amount of}} speech items which can be assigned to the same phonemic category. Thus, each speech unit is represented by one specific neuron within the <b>speech</b> <b>sound</b> map, while the realization of a speech unit may exhibit some articulatory and acoustic variability. This phonetic variability is the motivation to define sensory target regions in the DIVA model (see Guenther et al. 1998).|$|E
40|$|This paper {{presents}} an "elitist approach" for extracting automatically well-realized <b>speech</b> <b>sounds</b> with high confidence. The elitist approach uses a speech recognition {{system based on}} Hidden Markov Models (HMM). The HMM are trained on <b>speech</b> <b>sounds</b> which are systematically well-detected in an iterative procedure. The results show that, by using the HMM models defined in the training phase, the speech recognizer detects reliably specific <b>speech</b> <b>sounds</b> with a small rate of errors...|$|R
40|$|Developmental {{dyslexia}} is {{a specific}} reading and spelling deficit affecting 4 % to 10 % of the population. Advances in understanding its origin support a core deficit in phonological processing characterized by difficulties in segmenting spoken words into their minimally discernable <b>speech</b> segments (<b>speech</b> <b>sounds,</b> or phonemes) and underactivation of left superior temporal cortex. A suggested but unproven hypothesis is that this phonological deficit impairs the ability to map <b>speech</b> <b>sounds</b> onto their homologous visual letters, which in turn prevents the attainment of fluent reading levels. The present {{functional magnetic resonance imaging}} (fMRI) study investigated the neural processing of letters and <b>speech</b> <b>sounds</b> in unisensory (visual, auditory) and multisensory (audiovisual congruent, audiovisual incongruent) conditions as a function of reading ability. Our data reveal that adult dyslexic readers underactivate superior temporal cortex for the integration of letters and <b>speech</b> <b>sounds.</b> This reduced audiovisual integration is directly associated with a more fundamental deficit in auditory processing of <b>speech</b> <b>sounds,</b> which in turn predicts performance on phonological tasks. The data provide a neurofunctional account of developmental dyslexia, in which phonological processing deficits are linked to reading failure through a deficit in neural integration of letters and <b>speech</b> <b>sounds...</b>|$|R
50|$|It {{was found}} that the analog ear with its {{asymmetric}} overlapping bands was more reliable in identifying <b>speech</b> <b>sounds</b> than is a conventional frequency spectrum. The second formant is the most significant single measure. <b>Speech</b> <b>sounds</b> of interest include whispered and clipped speech.|$|R
25|$|As an example, in an {{extension}} of a theory for unsupervised learning of invariant visual representations to the auditory domain and empirically evaluated its validity for voiced <b>speech</b> <b>sound</b> classification was proposed. Authors empirically demonstrated that a single-layer, phone-level representation, extracted from base speech features, improves segment classification accuracy and decreases the number of training examples in comparison with standard spectral and cepstral features for an acoustic classification task on TIMIT dataset.|$|E
25|$|For {{the first}} six years of her life, Rousey {{struggled}} with speech and could not form an intelligible sentence due to apraxia, a neurological childhood <b>speech</b> <b>sound</b> disorder. This speech disorder was attributed to being born with her umbilical cord wrapped around her neck. When Rousey was three years old, {{her mother and father}} moved from Riverside, California, to Jamestown, North Dakota, to obtain intensive speech therapy with specialists at Minot State University.|$|E
25|$|Without the {{necessity}} of taking {{an active part in}} the test, even infants can be tested; this feature is crucial in research into acquisition processes. The possibility to observe low-level auditory processes independently from the higher-level ones makes it possible to address long-standing theoretical issues such as whether or not humans possess a specialized module for perceiving speech or whether or not some complex acoustic invariance (see lack of invariance above) underlies the recognition of a <b>speech</b> <b>sound.</b>|$|E
50|$|A University of Maastricht (Netherlands) study {{revealed}} that adult dyslexic readers have under-activated superior temporal cortices, a brain region responsible for the integration of letters and <b>speech</b> <b>sounds.</b> This reduced audiovisual integration is directly associated with a more fundamental deficit in auditory processing of <b>speech</b> <b>sounds,</b> which in turn predicts performance on phonological tasks. The data also provides a neurofunctional account of developmental dyslexia, in which phonological processing deficits are linked to reading failure through a deficit in neural integration of letters and <b>speech</b> <b>sounds.</b>|$|R
40|$|SummaryDevelopmental {{dyslexia}} is {{a specific}} reading and spelling deficit [1] affecting 4 % to 10 % of the population [2, 3]. Advances in understanding its origin support a core deficit in phonological processing [4 – 6] characterized by difficulties in segmenting spoken words into their minimally discernable <b>speech</b> segments (<b>speech</b> <b>sounds,</b> or phonemes) [7, 8] and underactivation of left superior temporal cortex [9, 10]. A suggested but unproven hypothesis is that this phonological deficit impairs the ability to map <b>speech</b> <b>sounds</b> onto their homologous visual letters, which in turn prevents the attainment of fluent reading levels [7, 11]. The present {{functional magnetic resonance imaging}} (fMRI) study investigated the neural processing of letters and <b>speech</b> <b>sounds</b> in unisensory (visual, auditory) and multisensory (audiovisual congruent, audiovisual incongruent) conditions as a function of reading ability. Our data reveal that adult dyslexic readers underactivate superior temporal cortex for the integration of letters and <b>speech</b> <b>sounds.</b> This reduced audiovisual integration is directly associated with a more fundamental deficit in auditory processing of <b>speech</b> <b>sounds,</b> which in turn predicts performance on phonological tasks. The data provide a neurofunctional account of developmental dyslexia, in which phonological processing deficits are linked to reading failure through a deficit in neural integration of letters and <b>speech</b> <b>sounds...</b>|$|R
25|$|Ingressive <b>speech</b> <b>sounds</b> are {{produced}} while the speaker breathes in, {{in contrast to}} most <b>speech</b> <b>sounds,</b> which {{are produced}} as the speaker breathes out. The air {{that is used to}} voice the speech is drawn in rather than pushed out. Ingressive speech can be glottalic, velaric, or pulmonic.|$|R
25|$|Although {{listeners}} perceive {{speech as}} {{a stream of}} discrete units (phonemes, syllables, and words), this linearity {{is difficult to see}} in the physical speech signal (see Figure 2 for an example). Speech sounds do not strictly follow one another, rather, they overlap. A <b>speech</b> <b>sound</b> is influenced by the ones that precede and the ones that follow. This influence can even be exerted at a distance of two or more segments (and across syllable- and word-boundaries).|$|E
25|$|If {{the current}} sensory state {{deviates}} from the intended sensory state, both error maps are generating feedback commands which are projected towards the motor map {{and which are}} capable to correct the motor activation pattern and subsequently the articulation of a speech unit under production. Thus, in total, the activation pattern of the motor map is not only influenced by a specific feedforward command learned for a speech unit (and generated by the synaptic projection from the <b>speech</b> <b>sound</b> map) but also by a feedback command generated {{at the level of}} the sensory error maps (see Fig. 4).|$|E
25|$|Consequently {{two types}} of sensory {{information}} are available if a speech unit is activated {{at the level of}} the <b>speech</b> <b>sound</b> map: (i) learned sensory target regions (i.e. intended sensory state for a speech unit) and (ii) sensory state activation patterns resulting from a possibly imperfect execution (articulation) of a specific speech unit (i.e. current sensory state, reflecting the current production and articulation of that particular speech unit). Both types of sensory information is projected to sensory error maps, i.e. to an auditory error map which is assumed to be located in the superior temporal cortex (like the auditory state map) and to a somatosensosry error map which is assumed to be located in the inferior parietal cortex (like the somatosensory state map) (see Fig. 4).|$|E
50|$|The {{irrelevant}} speech effect {{refers to}} the degradation of serial recall when <b>speech</b> <b>sounds</b> are presented, even if the list items are presented visually. The sounds {{need not be a}} language the participant understands, nor even a real language; human <b>speech</b> <b>sounds</b> are sufficient to produce this effect.|$|R
50|$|In 2013, he {{returned}} with Modern Persian <b>Speech</b> <b>Sounds.</b>|$|R
5000|$|Voice (phonetics): a {{property}} of <b>speech</b> <b>sounds</b> (especially consonants) ...|$|R
25|$|A neural mapping connects two {{cortical}} neural maps. Neural mappings (in {{contrast to}} neural pathways) store training information by adjusting their neural link weights (see artificial neuron, artificial neural networks). Neural mappings {{are capable of}} generating or activating a distributed representation (see above) of a sensory or motor state within a sensory or motor map from a punctual or local activation within the other map (see for example the synaptic projection from <b>speech</b> <b>sound</b> map to motor map, to auditory target region map, or to somatosensory target region map in the DIVA model, explained below; or see for example the neural mapping from phonetic map to auditory state map and motor plan state map in the ACT model, explained below and Fig. 3).|$|E
25|$|Spoken {{words are}} {{sequences}} of motor movements organized around vocal tract gesture motor targets. Vocalization {{due to this}} is copied {{in terms of the}} motor goals that organize it rather than the exact movements with which it is produced. These vocal motor goals are auditory. According to James Abbs 'For speech motor actions, the individual articulatory movements would not appear to be controlled with regard to three- dimensional spatial targets, but rather with regard to their contribution to complex vocal tract goals such as resonance properties (e.g., shape, degree of constriction) and or aerodynamically significant variables'. Speech sounds also have duplicable higher-order characteristics such as rates and shape of modulations and rates and shape of frequency shifts. Such complex auditory goals (which often link—though not always—to internal vocal gestures) are detectable from the <b>speech</b> <b>sound</b> which they create.|$|E
25|$|In {{a classic}} experiment, Richard M. Warren (1970) {{replaced}} one phoneme {{of a word}} with a cough-like sound. His subjects restored the missing <b>speech</b> <b>sound</b> perceptually without any difficulty and could not accurately identify which phoneme had been disturbed. This {{is known as the}} phonemic restoration effect. Another basic experiment compares recognition of naturally spoken words presented in a sentence (or at least a phrase) and the same words presented in isolation. Perception accuracy usually drops in the latter condition. Garnes and Bond (1976) also used carrier sentences when researching the influence of semantic knowledge on perception. They created series of words differing in one phoneme (bay/day/gay, for example). The quality of the first phoneme changed along a continuum. All these stimuli were put into different sentences each of which made sense with one of the words only. Listeners had a tendency to judge the ambiguous words (when the first segment was at the boundary between categories) according to the meaning of the whole sentence.|$|E
40|$|The {{earliest}} {{stages of}} cortical processing of <b>speech</b> <b>sounds</b> {{take place in}} the auditory cortex. Transcranial magnetic stimulation (TMS) studies have provided evidence that the human articulatory motor cortex contributes also to speech processing. For example, stimulation of the motor lip representation influences specifically discrimination of lip-articulated <b>speech</b> <b>sounds.</b> However, the timing of the neural mechanisms underlying these articulator-specific motor contributions to speech processing is unknown. Furthermore, it is unclear whether they depend on attention. Here, we used magnetoencephalography and TMS to investigate the effect of attention on specificity and timing of interactions between the auditory and motor cortex during processing of <b>speech</b> <b>sounds.</b> We found that TMS-induced disruption of the motor lip representation modulated specifically the early auditory-cortex responses to lip-articulated <b>speech</b> <b>sounds</b> when they were attended. These articulator-specific modulations were left-lateralized and remarkably early, occurring 60 - 100 ms after <b>sound</b> onset. When <b>speech</b> <b>sounds</b> were ignored, the effect of this motor disruption on auditory-cortex responses was nonspecific and bilateral, and it started later, 170 ms after sound onset. The findings indicate that articulatory motor cortex can contribute to auditory processing of <b>speech</b> <b>sounds</b> {{even in the absence of}} behavioral tasks and when the sounds are not in the focus of attention. Importantly, the findings also show that attention can selectively facilitate the interaction of the auditory cortex with specific articulator representations during speech processing...|$|R
40|$|For {{nearly two}} decades it has been known that infants' {{perception}} of <b>speech</b> <b>sounds</b> is affected by native language input {{during the first year}} of life. However, definitive evidence of a mechanism to explain these developmental changes in speech perception has remained elusive. The present study provides the first evidence for such a mechanism, showing that the statistical distribution of phonetic variation in the speech signal influences whether 6 - and 8 -month-old infants discriminate a pair of <b>speech</b> <b>sounds.</b> We familiarized infants with <b>speech</b> <b>sounds</b> from a phonetic continuum, exhibiting either a bimodal or unimodal frequency distribution. During the test phase, only infants in the bimodal condition discriminated tokens from the endpoints of the continuum. These results demonstrate that infants are sensitive to the statistical distribution of <b>speech</b> <b>sounds</b> in the input language, and that this sensitivity influences speech perception...|$|R
5000|$|Articulation : the {{production}} of <b>speech</b> <b>sounds</b> in human <b>speech</b> organs.|$|R
