9|2|Public
5000|$|... #Caption: <b>Sound</b> <b>analyser</b> with 8 {{resonator}} balls, by Koenig, 1880, Teylers Instrument Room ...|$|E
40|$|Measurement of {{extremely}} low sound pressure levels is impor-tant for many applications. It {{may be used}} to describe the acoustic environ-ment in recording studios, auditory rooms, concert halls and sound-insu-lated rooms. It may also be used for the measurement of noise emissions from quiet machinery and equipment such as lighting-armatures. Traditional Level Measurements Sound pressure levels are normally measured by the use of a single meas-urement microphone connected to a sound level meter or a <b>sound</b> <b>analyser.</b> The lowest sound pressure that can b...|$|E
40|$|Abstract — A {{comparative}} study of drills {{from a range of}} manufacturers was carried out in the laboratory. These tests were made under a variety of loading conditions to assess the characteristics of the noise likely in the dentist’s surgery. Details of the test setup are given, and some of the issues in analysing the results considered. Simultaneous Recordings were made of the noise on a minidisk recorder, a hand held sound meter, and a <b>sound</b> <b>analyser.</b> The results from these are considered. These are significant for dentist’s hearing, and the patient’s perception of visiting the dentists. The performance of different designs of pneumatic drill varies much with type and load. Index Terms—noise, dentistry, drill...|$|E
40|$|The {{discovery}} of inductive invariants {{lies at the}} heart of static program verification. Presently, many automatic solutions to inductive invariant generation are inflexible, only applicable to certain classes of programs, or unpredictable. An automatic technique that circumvents these deficiencies to some extent is candidate-based invariant generation, whereby a large number of candidate invariants are guessed and then proven to be inductive or rejected using a <b>sound</b> program <b>analyser.</b> This paper describes our efforts to apply candidate-based invariant generation in GPUVerify, a static checker of programs that run on GPUs. We study a set of 383 GPU programs that contain loops, drawn from a number of open source suites and vendor SDKs. Among this set, 253 benchmarks require provision of loop invariants for verification to succeed. We describe the methodology we used to incrementally improve the invariant generation capabilities of GPUVerify to handle these benchmarks, through candidate-based invariant generation, whereby potential program invariants are speculated using cheap static analysis and subsequently either refuted or proven. We also describe a set of experiments that we used to examine the effectiveness of our rules for candidate generation, assessing rules based on their generality (the extent to which they generate candidate invariants), hit rate (the extent to which the generated candidates hold), effectiveness (the extent to which provable candidates actually help in allowing verification to succeed), and influence (the extent to which the success of one generation rule depends on candidates generated by another rule). We believe that our methodology for devising and evaluation candidate generation rules may serve as a useful framework for other researchers interested in candidate-based invariant generation. The candidates produced by GPUVerify help to verify 231 of these 253 programs. An increase in precision, however, has created sluggishness in GPUVerify because more candidates are generated and hence more time is spent on computing those which are inductive invariants. To speed up this process, we have investigated four under-approximating program analyses that aim to reject false candidates quickly and a framework whereby these analyses can run in sequence or in parallel. Across two platforms, running Windows and Linux, our results show that the best combination of these techniques running sequentially speeds up invariant generation across our benchmarks by 1. 17 × (Windows) and 1. 01 × (Linux), with per-benchmark best speedups of 93. 58 × (Windows) and 48. 34 × (Linux), and worst slowdowns of 10. 24 × (Windows) and 43. 31 × (Linux). We find that parallelising the strategies marginally improves overall invariant generation speedups to 1. 27 × (Windows) and 1. 11 × (Linux), maintains good best-case speedups of 91. 18 × (Windows) and 44. 60 × (Linux), and, importantly, dramatically reduces worst-case slowdowns to 3. 15 × (Windows) and 3. 17 × (Linux) ...|$|R
40|$|Artists {{were invited}} {{to respond to a}} series of five minute talks about {{research}} delivered by scientists from the University of Bath recorded as part of Ignite UBath. These artistic interpretations were presented as a series of short videos accompanied by an exhibition, talks and science/art workshops. ([URL] The five minute scientist talk that my proposal for a collaboration responded to was 'Sounding the Future: Acoustic Imaging and our Environment' by Dr Philippe Blondel, from the Centre for Space, Atmosphere and Oceanic Science at the University of Bath. My initial proposal was to visualize radio frequencies of the livestreaming of magnetospheric data from our Ionosphere. However our meeting led to a collaboration with his team's research recordings of the sound of melting glaciers being visualised as a night projection over flood tides. Exhibition Statement: Nicola: "Our meeting led to an increased awareness of the similarities and differences between our interests in sound and its visualization. My art practice involves experimentation with sonic frequency visualisations, often in response to electro-acoustic phenomena or viewer interactivity. We both use <b>sound</b> emission <b>analysers</b> designed for scientific purposes, but my work is self-directed. When Philippe was asked what sounds that he currently found most interesting, he replied the sound of ice cracking in Svalbard as a result of global warming. So we decided to focus on a hydrophone recording of melting glaciers in the Hornsund Fjord on Svalbard, recorded by Philippe’s research team. My suggestion was to visualise these sonic frequencies of melting glaciers projected onto water at night in the Somerset Levels, with its history of flooding due to climate change. The process of projecting sonic frequencies onto high tides in the dark during January made it necessary to create a robust projection trolley with tarpaulin cover to protect the digital equipment. The possibility of filming projections in the Somerset Levels receded as the necessity for a constant source of electricity became more apparent. So my late Grandmother’s house with its fragile sea walls, located beside the low lying estuary at Birdham on the Sussex coast, with its own history of flooding, became our location. Other arctic glacier calving recordings can be heard at intervals, alongside the more subtle underwater recordings of Philippe’s research team. " Philippe: "Hornsund. WAV file is an audio recording taken from a single hydrophone deployed from a small boat (Zodiac size) in the middle of Hornsund Fjord, Svalbard, by my Polish collaborator Prof. Jaroslaw Tegowski, from the University of Gdansk. This was recorded in summer 2009. The recording is at 96 kHz and we can hear the background noise of water in the fjord, clinking from icebergs melting, {{and every now and then}} some louder "thunks" of icebergs capsizing, or some far-away calving from the Hornsund glacier (40 m high above sea water). This is the place where I worked this summer, with Prof. Tegowski and another collaborator in the same project, Prof. Grant Deane (from Scripps Institution of Oceanography, San Diego, USA). ...|$|R
40|$|In this paper, we {{introduce}} a novel framework called SASA (Smart Ambient <b>Sound</b> <b>Analyser)</b> to support different ambient audio mining tasks (e. g., audio classification and location estimation). To gain comprehensive ambient sound modelling, SASA extracts {{a variety of}} acoustic features from different sound components (e. g., music, voice and background), and translates them into structured information. This significantly enhances quality of audio content representation. Further, distinguished from existing approaches, SASA’s multilayered architecture seamlessly integrates mixture models and aPEGASOS (adaptive PEGASOS) SVM algorithm into a unified classification framework. The approach can leverage complimentary strengths of both models. Experimental results based on three large test collections demonstrate the SASA’s advantages over existing methods on various analysis tasks...|$|E
40|$|The {{front end}} of the human {{auditory}} system, the cochlea, converts sound signals from the outside world into neural impulses transmitted along the auditory pathway for further processing. The cochlea senses and separates sound in a nonlinear active fashion, exhibiting remarkable sensitivity and frequency discrimination. Although several electronic models of the cochlea have been proposed and implemented, none of these are able to reproduce all the characteristics of the cochlea, including large dynamic range, large gain and sharp tuning at low sound levels, and low gain and broad tuning at intense sound levels. Here, we implement the Cascade of Asymmetric Resonators (CAR) model of the cochlea on an FPGA. CAR represents the basilar membrane filter in the Cascade of Asymmetric Resonators with Fast-Acting Compression (CAR-FAC) cochlear model. CAR-FAC is a neuromorphic model of hearing based on a pole-zero filter cascade model of auditory filtering. It uses simple nonlinear extensions of conventional digital filter stages that are well suited to FPGA implementations, so that we are able to implement up to 1224 cochlear sections on Virtex- 6 FPGA to process sound data in real time. The FPGA implementation of the electronic cochlea described here may be used as a front-end <b>sound</b> <b>analyser</b> for various machine-hearing applications. Comment: ISCAS- 201...|$|E
40|$|ABSTRACT: The aim of {{the study}} was to compare the lung sounds in {{patients}} with asbestos related pulmonary disorders with findings in high-resolution computed tomography (HRCT), and with lung function variables, in order to find out associations of acoustic changes with radiological fibrosis, emphysema or with pulmonary gas transfer functions. Sixty-four patients with asbestos-related pleural disease, with or without pulmonary disease, were studied. Lung sound recording and analysis was carried out with a computerized lung <b>sound</b> <b>analyser,</b> and HRCT of the chest, as well as forced spirometry and diffusing capacity measurement were performed. The fibrosis score correlated positively with the quartile frequencies of the power spectrum of lung sounds in inspiration (f 50) and expiration (f 50) and crackle count in inspiration, as well as negatively with diffusing capacity. When the patients with crackling sounds and significant fibrosis were excluded (n= 18), emphysema correlated negatively with expiratory quartile frequencies of the power spectrum, with f 25 and f 50. Furthermore, diffusing capacity correlated with inspiratory f 25 and forced expiratory volume in one second with inspiratory f 50 when crackles and fibrosis were excluded. Changes in lung sounds were significantly associated with radiologically verified abnormalities and gas transfer of pulmonary tissue. High sound frequencies were associated with fibrotic changes of the lung while low sound frequencies with pulmonary emphysema. Acoustic analysis gives complementary clinical information for evaluation of asbestos-related pulmonary disorders...|$|E
40|$|BACKGROUND: A {{study was}} {{undertaken}} {{to evaluate the}} reliability of a digital tracheal <b>sound</b> <b>analyser</b> (ELENS-DSA) in predicting nocturnal changes in airways resistance in asthmatic patients. This device allows continuous measurement of {{the proportion of the}} time occupied by wheezing (Wh%). METHODS: Nocturnal polygraphic studies with simultaneous continuous monitoring of tracheal sounds and airways resistance were performed in seven patients with nocturnal asthma. In order to evaluate the possible bias in wheezing estimation, each tracheal sound recording was passed through the automatic analyser and simultaneously monitored with earphones by an experienced observer. RESULTS: The device detected audible wheezing with an optimal sensitivity and specificity of 70 %. Snoring was a minor cause of the relatively poor characteristics of the system. A close correlation (p < 0. 001) between Wh% and airways resistance was observed only in those patients with the highest increase in resistance; when the results of all the subjects were pooled the correlation observed was poor. The predictive value of Wh% in detecting changes in airways resistance during 10 minute intervals was lower than 70 %. The positive and negative predictive values of Wh% were raised to 79 % and 83 %, respectively, for 30 minute intervals. CONCLUSIONS: The ELENS-DSA system is a relatively crude means of detecting wheezing and assessing bronchoconstriction quantitatively. However, it is able to detect accurately nocturnal bronchoconstriction for 30 minute intervals. This finding, along {{with the fact that the}} monitoring is non-invasive, suggests that it may be a promising tool, especially for patients during sleep...|$|E
40|$|The {{physiological}} {{origins and}} physical characteristics of {{sounds from the}} thorax have been reviewed briefly. This thesis presents some signal processing algorithms and classification techniques which {{have been developed for}} the extraction and classification of those sounds. In order to evaluate the recording equipment and signal processing algorithms two simulators were constructed: a laboratory simulator generating lung sounds in a variable background noise environment and a heart sound simulator written such that the generated sound was defined by a set of variables. Four conventional transformation algorithms for the transient extraction process were evaluated. Their considerable user intervention and inconsistent transformed signal {{led to the development of}} the "signal's envelope" algorithm. The signal's envelope method was used to extract transients of interest which were then used for the classification stage. It is shown that, due to the numerical nature of the features used for the classification process, the nearest neighbour clustering algorithm could not correctly classify all the extracted transients. The numerical features were therefore converted into linguistic terms and a fuzzy logic technique was developed to classify the transients. The fuzzy inference engine was robust enough to cope with the small numerical variation in features such that the correct classification was achieved. The other classification method tried was the fuzzy "min-max" clustering algorithm. This also used numerical features for the classification process and was therefore not able to classify all of the extracted transients correctly. A lung <b>sound</b> <b>analyser</b> was constructed using the signal's envelope and fuzzy inference engine. The system was able to extract and classify individual heart sounds, crackles and wheezes from recorded phonograms. In about 4 % of cases, the heart sounds were so indistinct that only a partial classification was achieved. It was concluded that by using simple transducers and sophisticated signal processing and classification algorithms it was possible to construct a chest sound classifier which may be of use in a clinical environment...|$|E
40|$|The human {{auditory}} {{system has}} the ability to segregate complex auditory scenes into a foreground component and a background, allowing us to listen to specific speech sounds from a mixture of sounds. Selective attention plays a crucial role in this process, colloquially known as the ‘cocktail party effect’. It has not been possible to build a machine that can emulate this human ability in real-time. Here, we have developed a framework for the implementation of a neuromorphic sound segregation algorithm in a Field Programmable Gate Array (FPGA). This algorithm is based on the principles of temporal coherence and uses an attention signal to separate a target sound stream from background noise. Temporal coherence implies that auditory features belonging to the same sound source are coherently modulated and evoke highly correlated neural response patterns. The basis for this form of sound segregation is that responses from pairs of channels that are strongly positively correlated belong to the same stream, while channels that are uncorrelated or anti-correlated belong to different streams. In our framework, we have used a neuromorphic cochlea as a frontend <b>sound</b> <b>analyser</b> to extract spatial information of the sound input, which then passes through band pass filters that extract the sound envelope at various modulation rates. Further stages include feature extraction and mask generation, which is finally used to reconstruct the targeted sound. Using sample tonal and speech mixtures, we show that our FPGA architecture is able to segregate sound sources in real-time. The accuracy of segregation is indicated by the high signal-to-noise ratio (SNR) of the segregated stream (90, 77 and 55 dB for simple tone, complex tone and speech, respectively) as compared to the SNR of the mixture waveform (0 dB). This system may be easily extended for the segregation of complex speech signals, and may thus find various applications in electronic devices such as for sound segregation and speech recognition...|$|E

