86|155|Public
5000|$|Runtime Injection {{techniques}} use a <b>software</b> <b>trigger</b> {{to inject}} a fault into a running software system. Faults can be injected via {{a number of}} physical methods and triggers can be implemented {{in a number of}} ways, such as: Time Based triggers (When the timer reaches a specified time an interrupt is generated and the interrupt handler associated with the timer can inject the fault. [...] ); Interrupt Based Triggers (Hardware exceptions and software trap mechanisms are used to generate an interrupt at a specific place in the system code or on a particular event within the system, for instance access to a specific memory location).|$|E
40|$|The LHCb <b>software</b> <b>trigger</b> has two levels: a {{high-speed}} trigger running at 1 MHz with strictly limited latency {{and a second}} level running below 40 kHz without latency limitations. The trigger strategy requires full flexibility {{in the distribution of}} the installed CPU power to the two <b>software</b> <b>trigger</b> levels because of the unknown background levels and event topology distribution at the time the LHC accelerator will start its operation. This requirement suggests using a common CPU farm for both trigger levels fed by a common data acquisition (DAQ) infrastructure. The limited latency budget of the first level of <b>software</b> <b>trigger</b> has an impact on the organization of the CPU farm performing the trigger function for optimal usage of the installed CPU power. We will present the architecture and the design of the hardware infrastructure for the entire LHCb software triggering system based on Ethernet as link technology that fulfills these requirements. The performance of the event-building of the combined traffic of both <b>software</b> <b>trigger</b> levels, as well as the expected scale of the system will be presented. (9 refs) ...|$|E
40|$|This paper {{describes}} {{details of}} the <b>software</b> <b>trigger</b> signal distribution and the reference clock signal distribution for KEKB accelerator. The KEKB accelerator control system employs distributed CPUs along the accelerator and EPICS(Experimental Physics and Industrial Control System). The synchronization between IOCs {{is an important issue}} for precise control and measurement. The <b>software</b> <b>trigger</b> system provides synchronization pulses to each local control room and interrupt signals to the IOCs located in each local control room. The precise triggers and reference clock are distributed by using phase stabilized optical fibers and optical links. The stability under temperature change and the PLL feedback test are mentioned. ...|$|E
40|$|This paper {{presents}} the LHCb trigger system {{with an emphasis}} on the use of RICH data in the <b>software</b> <b>triggers.</b> A global and a local algorithm for fast particle ID are discussed and results of performance measurements are given, which show that complex RICH pattern recognition is well within the time budget of the LHCb trigger computer farm...|$|R
5000|$|After {{passage of}} the health reform bill, Reuters {{followed}} up, with another story by Waas on April 23, 2010 disclosing that WellPoint, the nation's largest health insurance company, had similarly targeted policyholders with breast cancer, shortly after their diagnoses. The Reuters story asserted that WellPoint utilized [...] "a computer algorithm that automatically targeted... every other policyholder recently diagnosed with breast cancer. The <b>software</b> <b>triggered</b> an immediate fraud investigation, as the company searched for some pretext to drop their policies." ...|$|R
5000|$|<b>Software</b> SMI <b>triggered</b> by {{the system}} {{software}} via an I/O access to a location considered special by the motherboard logic (port 0B2h is common).|$|R
40|$|The Large Hadron Collider beauty {{experiment}} (LHCb) is {{a dedicated}} B physics experiment at the LHC. The trigger system is of crucial importance to discriminate the interesting B meson decays against the dominant background from inelastic pp-scattering. In this note, {{an overview of}} the LHCb trigger system will be given. We put a special emphasis on the <b>software</b> <b>trigger</b> level...|$|E
40|$|ABSTRACT: The LHCb {{experiment}} {{will operate}} at a luminosity of 2 × 1033 cm− 2 s− 1 during LHC Run 3. At this rate the present readout and hardware Level- 0 trigger become a limitation, especially for fully hadronic final states. In {{order to maintain}} a high signal efficiency the upgraded LHCb detector will deploy two novel concepts: a triggerless readout and a full <b>software</b> <b>trigger...</b>|$|E
40|$|The LHCb track {{reconstruction}} uses sophisticated {{pattern recognition}} algorithms to reconstruct trajectories of charged particles. Their main {{feature is the}} use of a Hough- transform like approach to connect track segments from different sub-detectors, allowing for having no tracking stations in the magnet of LHCb. While yielding a high efficiency, the track reconstruction is a major contributor to the overall timing budget of the <b>software</b> <b>trigger</b> of LHCb, {{and will continue to be}} so in the light of the higher track multiplicity expected from Run II of the LHC. In view of this fact, key parts of the pattern recognition have been revised and redesigned. In this document the main features which were studied are presented. A staged approach strategy for the track reconstruction in the <b>software</b> <b>trigger</b> was investigated: it allows unifying complementary sets of tracks coming from the different stages of the high level trigger, resulting in a more exible trigger strategy and a better overlap between online and offline reconstructed tracks. Furthermore the use of parallelism was investigated, using SIMD instructions for time-critical parts of the software...|$|E
40|$|The LHCb {{experiment}} faces a {{major challenge}} from the large amounts of data received while the LHC is running. The ability to sort this information in a useful manner is important for working groups to perform physics analyses. Both hardware and <b>software</b> <b>triggers</b> are used to decrease the data rate and then the stripping process is used to sort the data into streams and further into stripping lines. This project studies the hundreds of stripping lines to look for overlaps between {{them in order to}} make the stripping process more efficient...|$|R
50|$|It is not {{possible}} to create a permanent restore point. All restore points will eventually be deleted after the time specified in the RPLifeInterval registry setting is reached or earlier if allotted disk space is insufficient. Even if no user or <b>software</b> <b>triggered</b> restore points are generated allotted disk space is consumed by automatic restore points. Consequently, in systems with little space allocated, if a user does not notice a new problem within a few days, {{it may be too late}} to restore to a configuration from before the problem arose.|$|R
40|$|In {{this paper}} {{we report on}} the work we {{performed}} to extend the logic model checker SPIN with builtin support for the verification of periodic, real-time embedded software systems, as commonly used in aircraft, automobiles, and spacecraft. We first extended the SPIN verification algorithms to model priority based scheduling policies. Next, we added a library to support the modeling of periodic tasks. This library was used in a recent application of the SPIN model checker to verify the engine control software of an automobile, to study the feasibility of <b>software</b> <b>triggers</b> for unintended acceleration events. I...|$|R
40|$|This work {{presents}} an algorithm for fast track reconstruction {{in the main}} tracking stations for {{the lowest level of}} the LHCb <b>software</b> <b>trigger.</b> True signals of the hardware trigger are confirmed by a track with an efficiency of larger than 95 % within 1 ms. The tracking algorithm improves the momentum resolution of the trigger objects significantly and can thus be used to reduce the trigger rate by a factor of two with almost no loss in efficiency. The trigger sequence based on the fast track reconstruction in the main tracker developed within this thesis is a complimentary approach to the existing vertex detector based trigger sequence. It yields comparable efficiency and retention rates. The new approach significantly improves the overall robustness of the LHCb trigger system. A detailed evaluation of its performance is presented here. Additionally the complete <b>software</b> <b>trigger</b> for muons has been rewritten and optimized. It now allows to trigger events without affecting the B meson lifetime dependent acceptance nor the angular dependent acceptance of its decay products. This is crucial for the analysis of the CP violation in the decay Bs [...] >J/psi phi, one of the key measurements of the LHCb physics program...|$|E
40|$|The {{sensitivity}} of the LHCb experiment to B(K_S^ 0 →π^ 0 μ^+μ^-) is analyzed {{in light of the}} 2011, 2012 and 2016 data and the oportunities the full <b>software</b> <b>trigger</b> of the LHCb upgrade provides. Two strategies are considered: the full reconstruction of the decay products and the partial reconstruction using only the dilepton pair and kinematic constraints. In both cases, the sensitivity achieved can surpass the world's current best. Both approaches could be statistically combined, further improving the result...|$|E
40|$|The Muon Spectrometer (MS) of the ALICE {{experiment}} at LHC {{is equipped}} with a HLT (High Level Trigger), whose aim is to improve the accuracy of the trigger cuts delivered at the L 0 stage. A computational challenge of real-time event reconstruction is satisfied to achieve this <b>software</b> <b>trigger</b> cut of the HLT. After the description of the online algorithms, the performance of the online tracker is compared with that of the offline tracker using the measured pp collisions a...|$|E
40|$|Reports {{the present}} status of CAB from {{the points of}} view of {{hardware}} and software. CAB is a microprogrammable system, using fast TTL bit- slice technology, designed to improve the capabilities of a standard CAMAC data acquisition chain. It can be implemented as crate controller, branch driver, or computer to CAMAC (or GPIB to CAMAC) interface. It is well adapted {{to a wide range}} of complex applications, such as <b>software</b> <b>triggering,</b> event filtering, data compaction and formatting, equipment calibration and monitoring. The basic software includes a cross-assembler, an emulating program and an interactive debugger. Improvements are now under development. Today, more than 15 units are used in different laboratories. (0 refs) ...|$|R
40|$|The ATLAS trigger {{system is}} {{responsible}} for reducing the event rate, from the design bunch-crossing rate of 40 MHz, to an average recording rate of 200 Hz, by selecting signal-like events out of the extremely large background. The ATLAS trigger is designed in three levels. The first-level (L 1) is implemented in custom-built electronics, the two levels of the high level <b>trigger</b> (HLT) are <b>software</b> <b>triggers</b> executed on large computing farms. The first-level trigger is comprised of calorimeter, muon and forward triggers to identify event features, such as missing transverse energy, as well as candidate electrons, photons, jets and muons. In this talk {{an overview of the}} performance of the Trigger and Data Acquisition system during 2011 data taking is shown...|$|R
40|$|The LHCb {{experiment}} has fully reconstructed {{close to}} 10 ^ 9 charm hadron decays [...] -by far the world's largest sample. During the 2011 - 2012 running periods, the effective proton-proton beam crossing rate was 11 - 15 MHz while {{the rate at}} which events were written to permanent storage was 3 - 5 kHz. Prompt charm candidates (produced at the primary interaction vertex) were selected using a combination of exclusive and inclusive high level (<b>software)</b> <b>triggers</b> in conjunction with low level hardware triggers. The efficiencies, background rates, and possible biases of the triggers as they were implemented will be discussed, along with plans for the running at 13 TeV in 2015 and subsequently in the upgrade era. Comment: To appear in the proceedings of The 6 th International Workshop on Charm Physics (CHARM 2013...|$|R
40|$|The LHCb {{experiment}} {{will operate}} at a luminosity of 2 × 10 ^ 33 cm^- 2 s^- 1 during LHC Run 3. At this rate the present readout and hardware Level- 0 trigger become a limitation, especially for fully hadronic final states. In {{order to maintain}} a high signal efficiency the upgraded LHCb detector will deploy two novel concepts: a triggerless readout and a full <b>software</b> <b>trigger.</b> Comment: Proceedings of the Workshop on Intelligent Trackers, 14 - 16 May 2014, University of Pennsylvani...|$|E
40|$|AbstractThe LHCb {{experiment}} at CERN {{has recently}} reached an integrated luminosity of 1 fb- 1. While {{the search for}} new physics continues, preparations for extending the physics goals after the planned period of operation until 2018 are already well underway. The proposed 40 MHz upgrade strategy places considerable requirements {{on the design of}} new detectors, necessary for the implementation of a fully ﬂexible <b>software</b> <b>trigger.</b> An overview of the current designs for the upgraded vertex detector are presented, including recent R&D and progress towards a ﬁrst module...|$|E
40|$|During {{the first}} long {{shutdown}} of the LHC (2013 - 2014, LS 1), the LHCb detector remained essentially unchanged, while the trigger {{system has been}} completely revisited. Upgrades to the LHCb computing infrastructure have allowed for high quality decay information to be calculated by the <b>software</b> <b>trigger</b> making a separate offline event reconstruction unnecessary. Reaching the ultimate precision of the LHCb experiment already in real time as the data arrive {{has the power to}} transform the experimental approach to processing large quantities of dat...|$|E
40|$|A {{review of}} recent {{developments}} in triggering and data acquisition was presented. In particular, developments, status, and trends of current HEP experiments are discussed; covering both hardware based and <b>software</b> based <b>triggering</b> techniques. This overview talk will draw heavily from the RT 2003 conference {{that took place in}} Montreal, May 18 - 23, 2003...|$|R
50|$|Hocevar {{is renowned}} for his {{expertise}} in reverse engineering and image processing. He reverse engineered DVD subtitles in 2000 and in 2005, {{it was found}} that the DRM <b>software</b> which <b>triggered</b> the Sony BMG CD copy protection scandal had unlawfully pirated open source code from the VideoLAN project which Hocevar was involved in authoring.|$|R
50|$|More-sophisticated timers {{may have}} {{comparison}} logic {{to compare the}} timer value against a specific value, set by <b>software,</b> that <b>triggers</b> some action when the timer value matches the preset value. This might be used, for example, to measure events or generate pulse width modulated waveforms to control the speed of motors (using a class D digital electronic amplifier).|$|R
40|$|Upgrades to the LHCb {{computing}} {{infrastructure in}} the first long shutdown of the LHC have allowed for high quality decay information to be calculated by the <b>software</b> <b>trigger</b> making a separate offline event reconstruction unnecessary. Furthermore, the storage space of the triggered candidate is {{an order of magnitude}} smaller than the entire raw event that would otherwise need to be persisted. Tesla is an application designed to process the information calculated by the trigger, with the resulting output used to directly perform physics measurements...|$|E
40|$|The {{reliability}} of datagram transmission over Copper Gigabit Ethernet using commodity hardware at sustained Gb/s rate {{is crucial for}} {{the functioning of the}} <b>software</b> <b>trigger</b> layers of the LHCb experiment. We aim to demonstrate that a nowadays available high-end commodity PC can be employed to achieve the required network performance, in particular to implement a sub-farm controller node. To evaluate all components ranging from the physical medium up to the operating system running on the sub-farm controllers, several issues are addressed, such as transmission error rate, packet drop in switching hardware, protocol handling on reception...|$|E
40|$|This note {{describes}} track reconstruction in the LHCb {{tracking system}} upstream of the magnet, combining VELO tracks with {{hits in the}} TT sub-detector. The implementation of the VeloTT algorithm and its performance in terms of track reconstruction efficiency, ghost rate and execution time are presented. The algorithm has been rewritten {{for use in the}} first <b>software</b> <b>trigger</b> level for LHCb Run II. The momentum and charge information obtained for the VeloTT tracks (due to a fringe magnetic field between the VELO and TT sub-detectors) can reduce the total execution time for the full tracking sequence...|$|E
40|$|A {{non-invasive}} technique {{has been used}} to record the electrical activity of the His-Purkinje System (HPS) on the body surface involving shielding, high gain band pass filtering and averaging of bipolar chest ECGs (V 6 -V 6 R). The measurements were made in the Berlin Magnetically Shielded Room (BMSR) which reduces the background noise level (almost white) to less than 0. 4 microVrms (measured for a bandwidth between 0. 1 Hz and 300 Hz), so that individual HPS body surface signals were limited only by patient inherent noise. Signal amplitudes greater than 1 microV were detectable in single beats. In addition, for best resolution, signal averaging was accomplished with <b>software</b> <b>triggers</b> referred to either the QRS complex or the P wave. A P-wave trigger is of most interest in clinical use because averaging techniques, even in the case of A-V blocks of different degree, can be applied...|$|R
40|$|Machine {{learning}} {{tools are}} commonly used in modern high energy physics (HEP) experiments. Different models, such as boosted decision trees (BDT) and artificial neural networks (ANN), are widely used in analyses {{and even in the}} <b>software</b> <b>triggers.</b> In most cases, these are classification models used to select the "signal" events from data. Monte Carlo simulated events typically take part in training of these models. While the results of the simulation are expected to be close to real data, in practical cases there is notable disagreement between simulated and observed data. In order to use available simulation in training, corrections must be introduced to generated data. One common approach is reweighting - assigning weights to the simulated events. We present a novel method of event reweighting based on boosted decision trees. The problem of checking the quality of reweighting step in analyses is also discussed...|$|R
40|$|We {{have built}} a gas-phase argon {{ionization}} detector to measure small nuclear recoil energies (< 10 keVee). In this paper, we describe the detector response to X-ray and gamma calibration sources, including analysis of pulse shapes, <b>software</b> <b>triggers,</b> optimization of gas content, and energy- and position-dependence of the signal. We compare our experimental results against simulation using a 5. 9 -keV X-ray source, as well as higher-energy gamma sources up to 1332 keV. We conclude {{with a description of}} the detector, DAQ, and software settings optimized for a measurement of the low-energy nuclear quenching factor in gaseous argon. This work was performed {{under the auspices of the}} U. S. Department of Energy by Lawrence Livermore National Laboratory in part under Contract W- 7405 -Eng- 48 and in part under Contract DE-AC 52 - 07 NA 27344. Funded by Lab-wide LDRD. LLNL-JRNL- 415990 -DRAFT. Comment: 29 pages, single-column, double-spaced, 21 figure...|$|R
40|$|The LHCb {{experiment}} at CERN {{has recently}} reached an integrated luminosity of 1 fb - 1. While {{the search for}} new physics continues, preparations for extending the physics goals after the planned period of operation until 2018 are already well underway. The proposed 40 MHz upgrade strategy places considerable requirements {{on the design of}} new detectors, necessary for the implementation of a fully ﬂexible <b>software</b> <b>trigger.</b> An overview of the current designs for the upgraded vertex detector are presented, including recent R&D and progress towards a ﬁrst module...|$|E
40|$|We {{describe}} the observation and mitigation of anomalous, large signals, {{observed in the}} barrel part of the CMS Electromagnetic Calorimeter during proton collisions at LHC. Laboratory and beam tests, as well as simulations, {{have been used to}} understand their origin. They are ascribed to direct energy deposition by particles in the avalanche photodiodes used for light readout. A reprogramming of the front-end electronics has allowed a majority of these anomalous signals to be identified and rejected at the first (hardware) trigger level with minimum impact on physics performance. Further rejection is performed in the high-level <b>software</b> <b>trigger</b> and offline analyses...|$|E
40|$|In a {{synchronized}} multi {{camera system}} {{it is imperative}} that the synchronization error between the different cameras is as close to zero as possible and the jitter of the presumed frame rate is as small as possible. It is even more important when these systems are used in an autonomous vehicle trying to sense its surroundings. We would never hand over the control to a autonomous vehicle if we couldn't trust the data it is using for moving around. The purpose of this thesis was to build a synchronization setup for a multi camera system using state of the art RayTrix digital cameras that will be used in the iQMatic project involving autonomous heavy duty vehicles. The iQMatic project is a collaboration between several Swedish industrial partners and universities. There was also software development for the multi camera system involved. Different synchronization techniques were implemented and then analysed against the system requirements. The two techniques were hardware trigger i. e. external trigger using a microcontroller, and <b>software</b> <b>trigger</b> using the API from the digital cameras. Experiments were conducted by testing the different trigger modes with the developed multi camera software. The conclusions show that the hardware trigger is preferable in this particular system by showing more stability and better statistics against the system requirements than the <b>software</b> <b>trigger.</b> But the thesis also show that additional experiments are needed for a more accurate analysis. iQMati...|$|E
25|$|In 1997, the {{combined}} company, Pure Atria, {{was acquired by}} Rational <b>Software,</b> which <b>triggered</b> a 42% drop in both companies' stocks after the deal was announced. Hastings was appointed Chief Technical Officer of {{the combined}} companies and left soon after the acquisition. After Pure Software, Hastings spent two years {{thinking about how to}} avoid similar problems at his next startup.|$|R
40|$|PVM for {{parallel}} processing. Whether on {{a dedicated}} farm of computers or using idle cycles, an application of any size could then easily enjoy the comfort of automated, fault-tolerant event processing. EVPRO aims to minimize application-specific event processing software, whose high development costs can only be justified for the largest of applications. A casual user may provide EVPRO with only the processing program and the data to be processed. A more complex or realtime application would tune EVPRO to its needs; for example, integrating custom hardware for the flow of event data. Making optimal use of the available computing resources, EVPRO would manage {{all aspects of the}} event processing. Monte Carlo production, event reconstruction and <b>software</b> <b>triggers</b> could use EVPRO, as could any computing application, inside or outside of HEP, which can be expressed in terms of events. In principle, event processing is a solved or even a trivial problem. Given an implementor, EVP...|$|R
40|$|Abstract—Many {{computer}} vision applications require synchronization between image acquisition and external trigger events. Hardware and <b>software</b> <b>triggering</b> are widely used, but have several limitations. Soft synchronization is investigated, which operates by time tagging both trigger events and images {{in a video}} stream, and selecting the image corresponding to each trigger event. A stochastic model is developed for soft synchronization; and, based on the model, the uncertainty interval and confidence for correct image selection are determined, and an efficient calibration method is derived. Soft synchronization is experimentally demonstrated on a linux image processing computer with a camera connected by the IEEE- 1394 serial bus. To minimize timing variability, time tags are associated with images and events in the corresponding interrupt service routines within the operating system of the image processing computer. The model-based analysis applied to the experimental hardware shows that image/event synchronization within ± 1 / 2 inter-frame interval can be achieved with 99 % confidence. Experiments confirm this result. Index Terms—Machine vision, Frame synchronization, Manufacturing automatio...|$|R
