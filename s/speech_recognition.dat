10000|836|Public
5|$|The {{new field}} was unified and {{inspired}} {{by the appearance of}} Parallel Distributed Processing in 1986—a two volume collection of papers edited by Rumelhart and psychologist James McClelland. Neural networks would become commercially successful in the 1990s, when they began to be used as the engines driving programs like optical character recognition and <b>speech</b> <b>recognition.</b>|$|E
5|$|Miller's Language and Communication {{was one of}} {{the first}} {{significant}} texts in the study of language behavior. The book was a scientific study of language, emphasizing quantitative data, and was based on the mathematical model of Claude Shannon's information theory. It used a probabilistic model imposed on a learning-by-association scheme borrowed from behaviorism, with Miller not yet attached to a pure cognitive perspective. The first part of the book reviewed information theory, the physiology and acoustics of phonetics, <b>speech</b> <b>recognition</b> and comprehension, and statistical techniques to analyze language. The focus was more on speech generation than recognition. The second part had the psychology: idiosyncratic differences across people in language use; developmental linguistics; the structure of word associations in people; use of symbolism in language; and social aspects of language use.|$|E
25|$|<b>Speech</b> <b>recognition</b> {{functionality}} {{has been}} removed from the individual programs in the Office 2007 suite, as Windows <b>Speech</b> <b>Recognition</b> was integrated into Windows Vista. Windows XP users must install a previous version of Office to use <b>speech</b> <b>recognition</b> features.|$|E
40|$|Feature {{extraction}} is a {{very important}} part in <b>speech</b> emotion <b>recognition,</b> and in allusion to feature extraction in <b>speech</b> emotion <b>recognition</b> problems, this paper proposed a new method of feature extraction, using DBNs in DNN to extract emotional features in speech signal automatically. By training a 5 layers depth DBNs, to extract speech emotion feature and incorporate multiple consecutive frames to form a high dimensional feature. The features after training in DBNs were the input of nonlinear SVM classifier, and finally <b>speech</b> emotion <b>recognition</b> multiple classifier system was achieved. The <b>speech</b> emotion <b>recognition</b> rate of the system reached 86. 5 %, which was 7 % higher than the original method...|$|R
40|$|<b>Speech</b> Emotion <b>Recognition</b> is a currenttopic of {{research}} {{since it has}} wide range ofapplications. <b>Speech</b> Emotion <b>Recognition</b> is a vitalpart of affective human interaction and has become anew challenge to speech processing. The workpresented in this paper focus on study of variousspeech emotions recognition methods...|$|R
30|$|The overall {{recognition}} rate will reduce {{due to the}} increase of emotional confusion in multiple <b>speech</b> emotion <b>recognition.</b> Inspired by the above methods, for multiple <b>speech</b> emotion <b>recognition,</b> we proposed a <b>speech</b> emotion <b>recognition</b> method based on the decision tree SVM model with Fisher feature selection. In this method, a high-performance decision tree SVM classifier is established by calculating the degree of emotional confusion, to realize the two step classification of the first rough classification and the fine classification. For each of decision tree SVM, we filter out the feature parameters of higher distinguish ability by Fisher criterion to gain an optimal feature set. Finally, this model is used for <b>speech</b> emotion <b>recognition.</b> Thus, a better emotional classification performance can be obtained.|$|R
25|$|Windows <b>Speech</b> <b>Recognition</b> {{allows the}} user to control his/her machine through voice commands, and enables {{dictation}} into many applications. The application has a fairly high recognition accuracy and provides a set of commands that assists in dictation. A brief speech-driven tutorial is included to help familiarize a user with <b>speech</b> <b>recognition</b> commands. Training could also be completed to improve the accuracy of <b>speech</b> <b>recognition.</b>|$|E
25|$|Windows Vista is {{the first}} Windows {{operating}} system to include fully integrated support for <b>speech</b> <b>recognition.</b> Under Windows 2000 and XP, <b>Speech</b> <b>Recognition</b> was installed with Office 2003, or was included in Windows XP Tablet PC Edition.|$|E
25|$|Reddy's early {{research}} was conducted at the AI labs at Stanford, first {{as a graduate student}} and later as an Assistant Professor, and at CMU since 1969. His AI research concentrated on perceptual and motor aspect of intelligence such as speech, language, vision and robotics. Over a span of five decades, Reddy and his colleagues created several historic demonstrations of spoken language systems, e.g., voice control of a robot, large vocabulary connected <b>speech</b> <b>recognition,</b> speaker independent <b>speech</b> <b>recognition,</b> and unrestricted vocabulary dictation. Reddy and his colleagues have made seminal contributions to Task Oriented Computer Architectures, Analysis of Natural Scenes, Universal Access to Information, and Autonomous Robotic Systems. Hearsay I {{was one of the first}} systems capable of continuous <b>speech</b> <b>recognition.</b> Subsequent systems like Hearsay II, Dragon, Harpy, and Sphinx I/II developed many of the ideas underlying modern commercial <b>speech</b> <b>recognition</b> technology as summarized in his recent historical review of <b>speech</b> <b>recognition</b> with Xuedong Huang and James K. Baker.|$|E
40|$|<b>Speech</b> emotion <b>recognition</b> is {{currently}} an active research subject and has attracted extensive {{interest in the}} science community due to its vital application to human-robot interaction. Most <b>speech</b> emotion <b>recognition</b> systems employ high-dimensional speech features, indicating human emotion expression, to improve emotion recognition performance. To effectively {{reduce the size of}} speech features, in this paper, a new nonlinear dimensionality reduction method, called ‘enhanced kernel isometric mapping’ (EKIsomap), is proposed and applied for <b>speech</b> emotion <b>recognition</b> in human-robot interaction. The proposed method is used to nonlinearly extract the low-dimensional discriminating embedded data representations from the original high-dimensional speech features with a striking improvement of performance on the <b>speech</b> emotion <b>recognition</b> tasks. Experimental results on the popular Berlin emotional speech corpus demonstrate the effectiveness of the proposed method...|$|R
40|$|In this paper, a novel {{approach}} for mandarin <b>speech</b> emotion <b>recognition,</b> that is mandarin <b>speech</b> emotion <b>recognition</b> based on high dimensional geometry theory, is proposed. The human emotions are classified into 6 archetypal classes: fear, anger, happiness, sadness, surprise and disgust. According to the characteristics of these emotional speech signals, the amplitude, pitch frequency and formant are used as the feature parameters for <b>speech</b> emotion <b>recognition.</b> The new method called high dimensional geometry theory is applied for recognition. Compared with traditional GSVM model, the new method has some advantages. It is noted that this method has significant values for researches and applications henceforth...|$|R
40|$|In {{order to}} improve the {{performance}} of the <b>speech</b> emotion <b>recognition</b> system and reduce the computing complexity, a <b>speech</b> emo- tion <b>recognition</b> based on optimized coefficients number for spectral fea- tures is proposed. Experimental studies are performed over the Berlin emotional Database, using support vector machine (SVM) classifier and five spectral features MFCC, LPC, LPCC, PLP, and PLP-RASTA. The experiment result shows that the <b>speech</b> emotion <b>recognition</b> based on coefficients number can improve {{the performance of the}} emotion recog- nition system effectively. abstract environment...|$|R
25|$|In 2003, LSTM {{started to}} become {{competitive}} with traditional speech recognizers. In 2007, the combination with CTC achieved first good results on speech data. In 2009, a CTC-trained LSTM {{was the first}} RNN to win pattern recognition contests, when it won several competitions in connected handwriting recognition. In 2014, Baidu used CTC-trained RNNs to break the Switchboard Hub5'00 <b>speech</b> <b>recognition</b> benchmark, without traditional speech processing methods. LSTM also improved large-vocabulary <b>speech</b> <b>recognition,</b> text-to-speech synthesis, for Google Android, and photo-real talking heads. In 2015, Google's <b>speech</b> <b>recognition</b> experienced a 49% improvement through CTC-trained LSTM.|$|E
25|$|Communication Access Real-Time Translation (CART) stenographers, who use a {{computer}} with using either stenotype or Velotype keyboards to transcribe stenographic input for presentation as captions within 2–3 seconds of the representing audio, must caption anything which is purely live and unscripted; however, the most recent developments include operators using <b>speech</b> <b>recognition</b> software and revoicing the dialog. <b>Speech</b> <b>recognition</b> technology has advanced so quickly in the United States that about 50% of all live captioning is through <b>speech</b> <b>recognition</b> as of 2005. Real-time captions look different from offline captions, as they are presented as a continuous flow of text as people speak.|$|E
25|$|This {{theory can}} also be {{extended}} for the <b>speech</b> <b>recognition</b> domain.|$|E
30|$|Berlin {{database}} {{is often}} used in verification and comparison of algorithm for <b>speech</b> emotion <b>recognition.</b> The results with CEP procedure show a high classify accuracy, which indicates that CEP procedure works well for <b>speech</b> emotion <b>recognition.</b> So this method {{can be used to}} analyze the emotional speech extracted from the violence simulation experiment in the following work.|$|R
40|$|In this {{dissertation}} {{the practical}} <b>speech</b> emotion <b>recognition</b> technology is studied, including several cognitive related emotion types, namely fidgetiness, confidence and tiredness. The {{high quality of}} naturalistic emotional speech data {{is the basis of}} this research. The following techniques are used for inducing practical emotional speech: cognitive task, computer game, noise stimulation, sleep deprivation and movie clips. A practical <b>speech</b> emotion <b>recognition</b> system is studied based on Gaussian mixture model. A two-class classifier set is adopted for performance improvement under the small sample case. Considering the context information in continuous emotional speech, a Gaussian mixture model embedded with Markov networks is proposed. A further study is carried out for system robustness analysis. First, noise reduction algorithm based on auditory masking properties is fist introduced to the practical <b>speech</b> emotion <b>recognition.</b> Second, to deal with the complicated unknown emotion types under real situation, an emotion recognition method with rejection ability is proposed, which enhanced the system compatibility against unknown emotion samples. Third, coping with the difficulties brought by a large number of unknown speakers, an emotional feature normalization method based on speaker-sensitive feature clustering is proposed. Fourth, by adding the electrocardiogram channel, a bi-modal emotion recognition system based on speech signals and electrocardiogram signals is first introduced. The <b>speech</b> emotion <b>recognition</b> methods studied in this dissertation may be extended into the cross-language <b>speech</b> emotion <b>recognition</b> and the whispered <b>speech</b> emotion <b>recognition.</b> Comment: in Chines...|$|R
30|$|To further {{improve the}} {{performance}} of <b>speech</b> emotion <b>recognition,</b> an effective emotion recognition model needs to be constructed. Currently, some classifiers are extensively used in <b>speech</b> emotion <b>recognition,</b> including Gaussian mixture model (GMM) [11], artificial neural network (ANN) [12], support vector machine (SVM), etc. Among them, the SVM has a unique advantage in solving nonlinear, small sample, and high dimensional pattern recognition problems, so it is widely used in <b>speech</b> emotion <b>recognition</b> [13, 14]. In [15], Zhang et al. proposed an improved leaping algorithm to optimize the SVM classifier, and this algorithm was applied to <b>speech</b> emotion <b>recognition.</b> In [16], an integrated system of hidden Markov model (HMM) and SVM, combining advantages on capability of dynamic time warping of HMM and pattern recognition of SVM, had been proposed to implement emotion classification, which achieved an 18.3 % improvement compared to the method using HMM in the experiment of speaker independent emotion classification. Work in [17] applied the GMM-MAP/SVM generative models and discriminative models to <b>speech</b> emotion <b>recognition,</b> which increased the average emotion recognition by 6.1 % compared to the method using SVM. In addition, the binary decision tree SVM recognition model had also been applied to multiple emotion recognition, which obtained good performance [18, 19].|$|R
25|$|Hidden Markov {{models are}} the basis for most modern {{automatic}} <b>speech</b> <b>recognition</b> systems.|$|E
25|$|The {{microphones}} detect pitch using {{digital signal}} processing, which analyses {{the frequency of}} the incoming signal via Fast Fourier transform. The frequency is then compared to stored information to evaluate if the note is correct. Regular singing segments do not feature <b>speech</b> <b>recognition,</b> and so humming into the microphones at the correct pitch will also score points. Rap sections use a combination of <b>speech</b> <b>recognition</b> and rhythm detection.|$|E
25|$|<b>Speech</b> <b>recognition</b> in Vista {{utilizes}} version 5.3 of the Microsoft Speech API (SAPI) and version 8 of the Speech Recognizer.|$|E
30|$|For the {{evaluation}} of the proposed perception system, we first conducted three kinds of emotion-recognition experiments independently: facial expression <b>recognition,</b> <b>speech</b> emotion <b>recognition,</b> and musical mood recognition. We then investigated the performance improvement in bimodal emotion recognition based on the proposed fusion process. Finally, music-aided multimodal emotion recognition was evaluated.|$|R
40|$|In this paper, the {{algorithm}} based on graph learning and graph embedding framework, Speaker-Penalty Graph Learning (SPGL), is {{proposed in the}} research of <b>speech</b> emotion <b>recognition</b> {{to solve the problems}} caused by different speakers. Graph embedding framework theory is used to construct the dimensionality reduction stage of <b>speech</b> emotion <b>recognition.</b> Special penalty and intrinsic graphs of the graph embedding framework is proposed to penalize the impacts from different speakers in the task of <b>speech</b> emotion <b>recognition.</b> The original <b>speech</b> emotion features are extracted by various categories, reflecting different characteristics of each speech sample. According to the experiments in speech emotion corpus using different classifiers, the proposed method with linear and kernelized mapping forms can both achieve relatively better performance than the state-of-the-art dimensionality reduction methods...|$|R
40|$|Abstract — In human machine {{interface}} application, emotion <b>recognition</b> from the <b>speech</b> signal has been research topic since many years. To identify the emotions from the speech signal, many systems have been developed. In this paper <b>speech</b> emotion <b>recognition</b> based on the previous technologies which uses different classifiers for the emotion recognition is reviewed. The classifiers are used to differentiate emotions such as anger, happiness, sadness, surprise, neutral state, etc. The database for the <b>speech</b> emotion <b>recognition</b> system is the emotional speech samples and the features extracted from these speech samples are the energy, pitch, linear prediction cepstrum coefficient (LPCC), Mel frequency cepstrum coefficient (MFCC). The classification performance is based on extracted features. Inference about the performance and limitation of <b>speech</b> emotion <b>recognition</b> system based on the different classifiers are also discussed...|$|R
25|$|Internal {{research}} on <b>speech</b> <b>recognition</b> {{was carried out}} and implemented for discrete word recognition but was never released to the field.|$|E
25|$|Jurafsky, D. and J. Martin. 2008. Speech and {{language}} processing: An introduction to natural language processing, computational linguistics, and <b>speech</b> <b>recognition.</b> Delhi, India: Pearson Education.|$|E
25|$|Deep {{learning}} in artificial neural networks with many layers has transformed many important subfields of artificial intelligence, including computer vision, <b>speech</b> <b>recognition,</b> {{natural language processing}} and others.|$|E
40|$|Abstract-Software {{development}} using programming languages requires keyboard {{for input}} and all programming languages are mostly text oriented. This text oriented nature of programming languages is {{a barrier to}} persons suffering from arms disability. A person having brilliant mind and potential for programming skills, but suffering from arm injuries or being disabled could not become a programmer. To be a good developer a human must memorize the syntax and keywords of a programming language. In our research work we propose a methodology for JAVA programming language where a programmer will speak and code in JAVA will be written accordingly. Structure of special program constructs will also be created simultaneously. Key words:Automatic <b>speech</b> <b>recognitions,</b> <b>Speech</b> API, Share...|$|R
40|$|Cross-corpus <b>speech</b> emotion <b>recognition</b> can be {{a useful}} {{transfer}} learning technique to build a robust <b>speech</b> emotion <b>recognition</b> system by leveraging information from various speech datasets - cross-language and cross-corpus. However, more {{research needs to be}} carried out to understand the effective operating scenarios of cross-corpus <b>speech</b> emotion <b>recognition,</b> especially with the utilization of the powerful deep learning techniques. In this paper, we use five different corpora of three different languages to investigate the cross-corpus and cross-language emotion recognition using Deep Belief Networks (DBNs). Experimental results demonstrate that DBNs with generalization power offers better accuracy than a discriminative method based on Sparse Auto Encoder and SVM. Results also suggest that using a large number of languages for training and using a small fraction of target data in training can significantly boost accuracy compared to using the same language for training and testing...|$|R
5000|$|Greste has {{advocated}} widely {{for freedom}} of the press and free <b>speech.</b> In <b>recognition</b> of his efforts, he was awarded the 2015 Australian Human Rights Medal ...|$|R
25|$|Advanced audio capabilities: Audio {{processing}} capabilities include sophisticated {{acoustic noise}} suppression and echo cancellation, beam formation {{to identify the}} current sound source, and integration with Windows <b>speech</b> <b>recognition</b> API.|$|E
25|$|Another {{approach}} is to use <b>speech</b> <b>recognition</b> software. In the past the hardware costs alone were so high {{that this was not}} a viable option in public schooling. However, in recent years as technology improved and prices fell, schools around the world introduced tablet computers to the classroom. And so the required computing power is already in the hands of an increasing number of children. One of the major advantages of using <b>speech</b> <b>recognition</b> software is that it can give feedback and so can be used to help improve pronunciation.|$|E
25|$|Support {{for four}} new {{languages}} for <b>speech</b> <b>recognition</b> – French, Spanish, Italian, and Japanese. Additionally it would add support for regional dialects of these languages along with English.|$|E
40|$|Features for <b>speech</b> emotion <b>recognition</b> {{are usually}} {{dominated}} by the spectral magnitude information while they ignore {{the use of the}} phase spectrum because of the difficulty of properly interpreting it. Motivated by recent successes of phase-based features for speech processing, this paper investigates the effectiveness of phase information for whispered <b>speech</b> emotion <b>recognition.</b> We select two types of phase-based features (i. e., modified group delay features and all-pole group delay features), both which have shown wide applicability to all sorts of different speech analysis and are now studied in whispered <b>speech</b> emotion <b>recognition.</b> When exploiting these features, we propose a new <b>speech</b> emotion <b>recognition</b> framework, employing outer product in combination with power and L 2 normalization. The according technique encodes any variable length sequence of the phase-based features into a fixed dimension vector regardless of the length of the input sequence. The resulting representation is fed to train a classification model with a linear kernel classifier. Experimental results on the Geneva Whispered Emotion Corpus database, including normal and whispered phonation, demonstrate the effectiveness of the proposed method when compared with other modern systems. It is also shown that, combining phase information with magnitude information could significantly improve performance over the common systems solely adopting magnitude information...|$|R
40|$|Abstract — A {{complete}} emotional {{expression in}} natural face-to-face conversation typically contains a complex temporal course. In this paper, we propose a temporal course modeling-based error weighted cross-correlation model (TCM-EWCCM) for <b>speech</b> emotion <b>recognition.</b> In TCM-EWCCM, a TCM-based cross-correlation model (CCM) is first used {{to not only}} model the temporal evolution of the extracted acoustic and prosodic features individually but also construct the statistical dependencies among paired acoustic-prosodic features in different emotional states. Then, a Bayesian classifier weighting scheme named error weighted classifier combination is adopted to explore {{the contributions of the}} individual TCM-based CCM classifiers for different acoustic-prosodic feature pairs to enhance the <b>speech</b> emotion <b>recognition</b> accuracy. The results of experiments on the NCKU-CASC corpus demonstrate that modeling the complex temporal structure and considering the statistical dependencies as well as contributions among paired features in natural conversation speech can indeed improve the <b>speech</b> emotion <b>recognition</b> performance. I...|$|R
40|$|The {{impact of}} the {{classification}} method and features selection for the <b>speech</b> emotion <b>recognition</b> accuracy is discussed in this paper. Selecting the correct parameters {{in combination with the}} classifier {{is an important part of}} reducing the complexity of system computing. This step is necessary especially for systems that will be deployed in real-time applications. The reason for the development and improvement of <b>speech</b> emotion <b>recognition</b> systems is wide usability in nowadays automatic voice controlled systems. Berlin database of emotional recordings was used in this experiment. Classification accuracy of artificial neural networks, k-nearest neighbours, and Gaussian mixture model is measured considering the selection of prosodic, spectral, and voice quality features. The purpose was to find an optimal combination of methods and group of features for stress detection in human speech. The research contribution lies in the design of the <b>speech</b> emotion <b>recognition</b> system due to its accuracy and efficiency...|$|R
