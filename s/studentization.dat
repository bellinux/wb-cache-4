60|2|Public
50|$|Samuelson's {{inequality}} may {{be considered}} a reason why <b>studentization</b> of residuals should be done externally.|$|E
5000|$|DFFITS is a {{diagnostic}} meant {{to show how}} influential a point is in a statistical regression. It was proposed in 1980. It {{is defined as the}} Studentized DFFIT, where the latter is the change in the predicted value for a point, obtained when that point is left out of the regression; <b>Studentization</b> is achieved by dividing by the estimated standard deviation of the fit at that point: ...|$|E
50|$|In statistics, <b>Studentization,</b> {{named after}} William Sealy Gosset, who wrote under the {{pseudonym}} Student, is the adjustment consisting of division of a first-degree statistic {{derived from a}} sample, by a sample-based estimate of a population standard deviation. The term is also used for the standardisation of a higher-degree statistic by another statistic of the same degree: for example, {{an estimate of the}} third central moment would be standardised by dividing by the cube of the sample standard deviation.|$|E
40|$|AbstractAn {{asymptotic}} {{theory is}} developed for series estimation of nonparametric and semiparametric regression models for cross-sectional data under conditions on disturbances {{that allow for}} forms of cross-sectional dependence and heterogeneity, including conditional and unconditional heteroscedasticity, along with conditions on regressors that allow dependence and do not require existence of a density. The conditions aim to accommodate various settings plausible in economic applications, and can apply also to panel, spatial and time series data. A mean square rate of convergence of nonparametric regression estimates is established followed by asymptotic normality of a quite general statistic.  Data-driven <b>studentizations</b> that rely on single or double indices to order the data are justified. In a partially linear model setting, Monte Carlo investigation of finite sample properties and two empirical applications are carried out...|$|R
50|$|Moreover, {{and most}} importantly, the residuals, unlike the errors, do not {{all have the}} same {{variance}}: the variance decreases as the corresponding x-value gets farther from the average x-value. This is a feature of the regression better fitting values at the ends of the domain, not the data itself, and is also reflected in the influence functions of various data points on the regression coefficients: endpoints have more influence. This can also be seen because the residuals at endpoints depend greatly on the slope of a fitted line, while the residuals at the middle are relatively insensitive to the slope. The fact that the variances of the residuals differ, even though the variances of the true errors are all equal to each other, is the principal reason for the need for <b>studentization.</b>|$|E
40|$|Several {{problems}} emerging {{with the}} <b>studentization</b> of M-estimators of regression model are briefly discussed and reasons for an enlargement of famous Hampel's program are given. Paper then presents a proposal of a scale-equivariant and regression-invariant scale estimator with an acceptable computational complexity. Finally numerical results of estimation of scale for several famous data sets are offered as an empirical evidence of finite-sample {{behavior of the}} estimator. Complexity of computation in robust estimation, diversity of estimators with high breakdown point, scale estimator in regression framework, invariance and equivariance, <b>studentization</b> of residuals...|$|E
40|$|The {{variance}} stabilizing {{transformation and}} the <b>studentization</b> {{have a simple}} relation on the skewness and the mean. The resultant relation implies that the former makes a better normal approximation than the latter for estimators of correlation coefficient in some cases, including an elliptical case and a missing case. Mean Missing Normal approximation Skewness...|$|E
40|$|There {{are only}} {{relatively}} few confidence coefficients and levels available for distribution-free confidence intervals and {{test for the}} median based on the sign statistic. This problem can be overcome by interpolating confidence intervals or by studentizing the median by an estimate of its standard error. Such methods are discussed and then compared via simulation. median standard error estimate <b>studentization</b> interpolation...|$|E
40|$|ABSTRACT. The {{original}} <b>Studentization</b> was {{the conversion}} of a sample mean departure into the familiar t-statistic, plus the derivation of the corresponding Stu-dent distribution function; the observed value of the distribution function is the observed p-value, as presented in an elemental form. We examine this process in a broadly general context: a null statistical model is available together with observed data; a statistic t(y) has been proposed as a plausible measure of {{the location of the}} data relative to what is expected under the null; a modified statistic, say t̃(y), is developed that is ancillary; the corresponding distribution function is determined, exactly or approximately; and the observed value of the distribution function is the p-value or percentage position of the data with respect to the model. Such p-values have had extensive coverage in the recent Bayesian literature, with many variations and some preference for two versions labelled pppost and pcpred. The bootstrap method also directly addresses this <b>Studentization</b> process. We use recent likelihood theory that gives a third order factorization of a reg-ular statistical model into a marginal density for a full dimensional ancillary an...|$|E
40|$|An {{asymptotic}} {{theory is}} developed for nonparametric and semiparametric series estimation under general cross-sectional dependence and heterogeneity. A uniform rate of consistency, asymptotic normality, and su ¢ cient conditions for p n convergence, are established, and a data-driven <b>studentization</b> new to cross-sectional data is justi 8 ̆ 5 ed. The conditions accommodate various cross-sectional settings plausible in economic applications, and apply also to panel and time series data. Strong, {{as well as}} weak dependence are covered, and conditional heteroscedasticity is allowed...|$|E
30|$|Motivated by this background, {{this paper}} {{suggests}} outlier diagnostics based on <b>studentization</b> and ER. The {{rest of the}} paper is organized as follows; Some OLS leverage statistics and residuals are elaborated on in the next section; RQ leverage statistics and residuals are discussed in “Regression quantiles leverage statistics and residuals” section; “Studentized residuals in the quantile regression scenario” section dwells {{on the construction of}} the suggested RQ studentized residual statistics; Applications are given in “Applications” section while conclusions are given in the last section.|$|E
40|$|AbstractAn {{asymptotic}} {{expansion of the}} joint distribution of k largest characteristic roots CM(i) (SiS 0 − 1), i = 1,…, k, is given, where S'is and S 0 are independent Wishart matrices with common covariance matrix Σ. The modified second-approximation procedure to the upper percentage points of the maximum of CM(i) (SiS 0 − 1), i = 1,…, k, is also considered. The evaluation of the expansion {{is based on the}} idea for <b>studentization</b> due to Welch and James with the use of differential operators and of the perturbation procedure...|$|E
40|$|This paper {{considers}} the {{multiple linear regression}} modelYi=xi'[beta]+[var epsilon]i,i=i,Â  [...] .,Â n, wherexi's are knownp- 1 vectors,[beta]is ap- 1 vector of parameters, and[var epsilon] 1,[var epsilon] 2, [...] . are stationary, strongly mixing random variables. Let[beta]ndenote anM-estimator of[beta]corresponding to some score function[psi]. Under some conditions on[psi],xi's and[var epsilon]i's, a two-term Edgeworth expansion for Studentized multivariateM-estimator is proved. Furthermore, it is shown that the moving block bootstrap is second-order correct for some suitable bootstrap analog of Studentized[beta]n. Edgeworth expansion moving block bootstrap M-estimators multiple linear regression stationarity strong mixing <b>Studentization</b> (null) ...|$|E
40|$|Nonparametric methods play {{a central}} role in modern {{empirical}} work. While they provide inference procedures that are more robust to parametric misspecification bias, they may be quite sensitive to tuning parameter choices. We study the effects of bias correction on confidence interval coverage in the context of kernel density and local polynomial regression estimation, and prove that bias correction can be preferred to undersmoothing for minimizing coverage error and increasing robustness to tuning parameter choice. This is achieved using a novel, yet simple, <b>Studentization,</b> which leads to a new way of constructing kernel-based bias-corrected confidence intervals. In addition, for practical cases, we derive coverage error optimal bandwidths and discuss easy-to-implement bandwidth selectors. For interior points, we show that the MSE-optimal bandwidth for the original point estimator (before bias correction) delivers the fastest coverage error decay rate after bias correction when second-order (equivalent) kernels are employed, but is otherwise suboptimal because it is too "large". Finally, for odd-degree local polynomial regression, we show that, as with point estimation, coverage error adapts to boundary points automatically when appropriate <b>Studentization</b> is used; however, the MSE-optimal bandwidth for the original point estimator is suboptimal. All the results are established using valid Edgeworth expansions and illustrated with simulated data. Our findings have important consequences for empirical work as they indicate that bias-corrected confidence intervals, coupled with appropriate standard errors, have smaller coverage error and are less sensitive to tuning parameter choices in practically relevant cases where additional smoothness is available...|$|E
40|$|A {{central limit theorem}} {{is given}} for certain {{weighted}} partial sums of a covariance stationary process, assuming it is linear in martingale differences, but without any restriction on its spectrum. We apply the result to kernel nonparametric fixed-design regression, giving a single central limit theorem which indicates how error spectral behavior at only zero frequency influences the asymptotic distribution and covers long-range, short-range and negative dependence. We show how the regression estimates can be Studentized {{in the absence of}} previous knowledge of which form of dependence pertains, and show also that a simpler <b>Studentization</b> is possible when long-range dependence can be taken for granted. ...|$|E
40|$|By {{using the}} {{empirical}} likelihood (EL), {{we consider the}} construction of pointwise confidence intervals (CIs) for nonparametric nonlinear nonstationary regression models with nonlinear nonstationary heterogeneous errors. It {{is well known that}} the EL-based CI has attractive properties such as data dependency and automatic <b>studentization</b> in cross-sectional and weak-dependence models. We extend EL theory to the nonparametric nonlinear nonstationary regression model and show that the log-EL ratio converges to a chi-squared random variable with one degree of freedom. This means that Wilks' theorem holds even if the covariate follows a nonstationary process. We also conduct empirical analysis of Japan's inverse money demand to demonstrate the data-dependency property of the EL-based CI...|$|E
40|$|General {{formulas}} of the asymptotic cumulants of a studentized parameter estimator {{are given}} {{up to the}} fourth order with the added higher-order asymptotic variance. Using the sample counterparts of the asymptotic cumulants, formulas for the Cornish-Fisher expansions with third-order accuracy are obtained. Some new methods of monotonic transformations of the studentized estimator are presented. In addition, similar transformations of a fixed normal deviate are proposed up to the same order with some asymptotic comparisons to the transformations of the studentized estimator. Applications to a mean and a binomial proportion are shown with simulations for estimation of the proportion. Asymptotic expansions Cornish-Fisher expansion <b>Studentization</b> Monotonic transformation Asymptotic cumulants...|$|E
40|$|Dedicated to {{the memory}} of Tibor Nemetz This paper investigates {{weighted}} approximations for studentized U-statistics type processes, both with symmetric and antisymmetric kernels, only under the assumption that the distribution of the projection variate is in the domain of attraction of the normal law. The classical second moment condition E|h(X 1, X 2) | 2 < ∞ is also relaxed in both cases. The results can be used for testing the null assumption of having a random sample versus the alternative that there is a change in distribution in the sequence. Key Words and Phrases: Weighted approximations in probability, functional limit theorems, U-statistics type processes, <b>Studentization,</b> change in distribution, symmetri...|$|E
40|$|It is {{well known}} that {{conventional}} Wald-type inference in the context of quantile regression is complicated by the need to construct estimates of the conditional densities of the response variables at the quantile of interest. This note explores the possibility of circumventing the need to construct conditional density estimates in this context with scale statistics that are explicitly inconsistent for the underlying conditional densities. This method of <b>studentization</b> leads conventional test statistics to have limiting distributions that are nonstandard but have the convenient feature of depending explicitly on the user’s choice of smoothing parameter. These limiting distributions depend on the distribution of the conditioning variables but can be straightforwardly approximated by resampling. ...|$|E
40|$|An {{asymptotic}} {{expansion of the}} joint distribution of k largest characteristic roots CM(i) (SiS 0 - 1), i = 1, [...] ., k, is given, where S'is and S 0 are independent Wishart matrices with common covariance matrix [Sigma]. The modified second-approximation procedure to the upper percentage points of the maximum of CM(i) (SiS 0 - 1), i = 1, [...] ., k, is also considered. The evaluation of the expansion {{is based on the}} idea for <b>studentization</b> due to Welch and James with the use of differential operators and of the perturbation procedure. {{asymptotic expansion}} differential operators largest characteristic roots multivariate F matrices perturbation procedure simultaneous confidence bounds simultaneous MANOVA test modified second approximation...|$|E
40|$|Condence {{intervals}} in econometric {{time series}} regressions suffer from notorious coverage problems. This {{is especially true}} when the dependence in the data is noticeable and sample sizes are small to moderate, {{as is often the}} case in empirical studies. This paper suggests using the studentized block bootstrap and discusses practical issues, such as the choice of the block size. A particular data-dependent method is proposed to automate the method. As a side note, it is pointed out that symmetric confidence intervals are preferred over equal-tailed ones, since they exhibit improved coverage accuracy. The improvements in small sample performance are supported by a simulation study. Bootstrap, confidence intervals, <b>studentization,</b> time series regressions, prewhitening...|$|E
40|$|In applied work, the two-parameter {{exponential}} distribution gives useful representations of many physical situations. Confidence interval for the scale parameter and predictive interval {{for a future}} independent observation have been studied by many, including Petropoulos (2011) and Lawless (1977), respectively. However, interval estimates for the threshold parameter have not been widely examined in statistical literature. The aim {{of this paper is}} to, first, obtain the exact significance function of the scale parameter by renormalizing the p∗-formula. Then the approximate <b>Studentization</b> method is applied to obtain the significance function of the threshold parameter. Finally, a predictive density function of the two-parameter {{exponential distribution}} is derived. A real-life data set is used to show the implementation of the method. Simulation studies are then carried out to illustrate the accuracy of the proposed methods...|$|E
40|$|We {{establish}} valid Edgeworth expansions for {{the distribution}} of smoothed nonparametric spectral estimates, and of studentized versions of linear statistics such as the same mean, where the <b>studentization</b> employs such a nonparametric spectral estimate. Particular {{attention is paid to}} the spectral estimate at zero frequency and, correspondingly, the studentized sample mean, to reflect econometric interest in autocorrelation-consistent or long-run variance estimation. Our main focus is on stationary Gaussian series, though we discuss relaxation of the Gaussianity assumption. Only smoothness conditions on the spectral density that are local to the frequency of interest are imposed. We deduce empirical expansions from our Edgeworth expansions designed to improve on the normal approximation in practice, and also a feasible rule of bandwidth choice. Edgeworth expansions, nonparametric spectral estimates, stationary Gaussian series, studentized sample mean, bandwidth choice. ...|$|E
40|$|The block {{bootstrap}} {{confidence interval}} based on dependent data can outperform the computationally more convenient normal approximation only with non-trivial <b>Studentization</b> which, {{in the case}} of complicated statistics, calls for highly specialist treatment. We propose two different approaches to improving the accuracy of the block bootstrap confidence interval under very general conditions. The first calibrates the coverage level by iterating the block bootstrap. The second calculates Studentizing factors directly from block bootstrap series and requires no non-trivial analytic treatment. Both approaches involve two nested levels of block bootstrap resampling and yield high-order accuracy with simple tuning of block lengths at the two resampling levels. A simulation study is reported to provide empirical support for our theory. The 1 st Institute of Mathematical Statistics Asia Pacific Rim Meeting (IMS-APRM), Seoul, Korea, 28 June- 1 July 2009...|$|E
40|$|Asymptotic cumulants of {{functions}} of multinomial sample proportions {{with and without}} <b>studentization</b> up to the fourth order are derived, where observed proportions are possibly added by some quantities. Some of the asymptotic cumulants of non-studentized estimators are invariant {{with respect to the}} added quantities used. On the other hand, most of the asymptotic cumulants for studentized estimators are the same as those for the estimators without the added quantities when the estimator of the asymptotic variance of the non-studentized estimator is appropriately constructed to avoid the problem of sampling zeroes or empty cells. Especially, when the quantities of order O(1 ⁄n) are used, all the asymptotic cumulants of the studentized estimators up to the fourth order are the same as those for the estimators without the added quantities. A numerical example using the log odds-ratio and Yule’s coefficients is illustrated...|$|E
40|$|This paper {{considers}} studentized {{tests in}} time series regressions with nonparametrically autocorrelated errors. The <b>studentization</b> {{is based on}} robust standard errors with truncation lag M = bT for some constant b ∈ (0 � 1] and sample size T � It is shown that the nonstandard fixed-b limit distributions of such nonparametrically studentized tests provide more accurate approximations to the finite sample distributions than the standard small-b limit distribution. We further show that, for typical economic time series, the optimal bandwidth that minimizes a weighted average of type I and type II errors is larger by {{an order of magnitude}} than the bandwidth that minimizes the asymptotic mean squared error of the corresponding long-run variance estimator. A plug-in procedure for implementing this optimal bandwidth is suggested and simulations (not reported here) confirm that the new plug-in procedure works well in finite samples...|$|E
40|$|This paper {{introduces}} a computationally efficient bootstrap procedure for obtaining multiplicity-adjusted p-values {{in situations where}} multiple hypotheses are tested simultaneously. This new testing procedure accounts for the mutual dependence of the individual statistics, and is shown under weak conditions to maintain asymptotic control of the generalized familywise error rate. Moreover, the estimated critical values (p-values) obtained via our procedure are less sensitive to the inclusion of true hypotheses and, as a result, our test has greater power to identify false hypotheses even as the collection of hypotheses under test increases in size. Another attractive feature of our test is that it leads naturally to balance among the individual hypotheses under test. This feature is especially attractive in settings where balance is desired but alternative approaches, such as those based on <b>studentization,</b> are difficult or infeasible. Bootstrap, familywise error, multiple testing, step-down, balanced testing...|$|E
40|$|We have a {{statistic}} for assessing an observed data point relative to {{a statistic}}al model but find that its distribution function depends on the parameter. To obtain the corresponding p-value, we require the minimally modified statistic that is ancillary; this process is called <b>Studentization.</b> We use recent likelihood theory to develop a maximal third-order ancillary; this gives immediately a candidate Studentized statistic. We show that the corresponding p-value is higher-order Un(0, 1), is equivalent to a repeated bootstrap version of the initial statistic and agrees with a special Bayesian modification of the original statistic. More importantly, the modified statistic and p-value are available by Markov chain Monte Carlo simulations and, in some cases, by higher-order approximation methods. Examples, including the Behrens [...] Fisher problem, are given to indicate the ease and flexibility of the approach. Copyright 2008, Oxford University Press. ...|$|E
40|$|Suppose that {{inference}} about {{parameters of}} interest is {{to be based on}} an unbiased estimating function that is U-statistic of degree 1 or 2. We define suitable studentized versions of such estimating functions and consider asymptotic approximations as well as an estimating function bootstrap (EFB) method based on resampling the estimated terms in the estimating functions. These methods are justified asymptotically and lead to confidence intervals produced directly from the studentized estimating functions. Particular examples in this class of estimating functions arise in La estimation as well as Wilcoxon rank regression and other related estimation problems. The proposed methods are evaluated in examples and simulations and compared with a recent suggestion for inference in such problems which relies on resampling an underlying objective functions with U-statistic structure. Bootstrap; Estimating functions; La estimation; Resampling methods; U-statistics; <b>Studentization,...</b>|$|E
40|$|The block {{bootstrap}} {{confidence interval}} for dependent data can outperform the conventional normal approximation only with nontrivial <b>studentization</b> which, {{in the case}} of complicated statistics, calls for specialist treatment and often results in unstable endpoints. We propose two double block bootstrap approaches for improving the accuracy of the block bootstrap confidence interval under very general conditions. The first approach calibrates the nominal coverage level and the second calculates studentizing factors directly from a block bootstrap series without the need for nontrivial analytical treatment. We prove that the two approaches reduce the coverage error of the block bootstrap interval by an order of magnitude with simple tuning of block lengths at the two block bootstrapping levels. Empirical properties of the procedures are investigated by simulations and application to an econometric time series. © 2009 Biometrika Trust. link_to_subscribed_fulltex...|$|E
40|$|We {{tackle the}} {{classical}} two-sample spherical location problem for directional data by having recourse to the Le Cam methodology, habitually used in classical " multivariate analysis. More precisely we construct locally and asymptotically optimal (in the maximin sense) parametric tests, which we then turn into semi-parametric ones in two distinct ways. First, {{by using a}} <b>studentization</b> argument; this leads to so-called pseudo-FvML tests. Second, by resorting to the invariance principle; this leads to e cient rank-based tests. Within each construction, the semi-parametric tests inherit optimality under a given distribution (the FvML in the rst case, any rotationally symmetric one in the second) from their parametric counterparts and also improve on the latter by being valid under the whole class of rotationally symmetric distributions. Asymptotic relative e ciencies are calculated and the nite-sample behavior of the proposed tests is investigated {{by means of a}} Monte Carlo simulation...|$|E
40|$|Edgeworth {{expansions}} {{of length}} two are {{established for the}} sample quantile, preprivoted by a smoothed bootstrap, and for a studentized sample quantile, where we studentize {{by means of a}} kernel density estimate. As a consequence, it turns out that that two-sides confidence intervals for the underlying quantile which are based on these approaches have essentially equal coverage probabilities with level errors being of order o(n-), where n is the sample size. The smoothed bootstrap therefore outperforms competitors such as the percentile method or sign test method which have level errors only of order O(n-). In case of one-sided confidence intervals, the smoothed bootstrap is superior to <b>studentization</b> if and only if the derivative of the underlying density at the quantile is positive. Ams 1980 Subject Classifications: Primary 62 E 20; Secondary 62 G 15 Bootstrap confidence interval coverage error Edgeworth expansion kernel density estimator prepivoting smoothed bootstrap...|$|E
40|$|In a Bayesian model, we de#ne an outlier as an {{observation}} which is #surprising " relative to its predictive distribution, under the model, given {{the remainder of}} the data. Hence #outlyingness" can be measured by the posterior predictive p-value of anyinteresting scalar summary of the #possibly multivariate# observation. For this calculation, we exclude the case of interest from the data, analogously to <b>studentization</b> of regression residuals. When Bayesian inference about the parameters is conducted by drawing a sample from their posterior distribution, as with a Markov Chain Monte Carlo sampler, the p-value can be calculated by reweighting the sample to re#ect deletion of the target observation and then drawing from the predictive distribution. Therefore the case-deletion weighting methods of Bradlow and Zaslavsky # 1997 a# are useful. Avariety of outlier checks are illustrated using hierarchical models for two data sets, a standard linear hierarchical model for rat growth a [...] ...|$|E
40|$|When testing {{symmetry}} of a univariate density, (parametric classes of) densities skewed {{by means of}} the general probability transform introduced in [7] are appealing alternatives. This paper first proposes parametric tests of symmetry that are locally and asymptotically optimal (in the Le Cam sense) against such alternatives. To improve on these parametric tests, which are valid under well-specified density types only, we turn them into semiparametric tests, either by using a standard <b>studentization</b> approach or by resorting to the invariance principle. The second approach leads to robust yet efficient signed-rank tests, which include the celebrated sign and Wilcoxon tests as special cases, and turn out to be Le Cam optimal irrespective of the underlying original symmetric density. Optimality, however, is only achieved under well-specified “skewing mechanisms”, and we therefore evaluate the overall performances of our tests by deriving their asymptotic relative efficiencies with respect to the classical test of skewness. A Monte-Carlo study confirms the asymptotic results. ...|$|E
40|$|In econometric applications, often several {{hypothesis}} {{tests are}} carried out at once. The problem then becomes how to decide which hypotheses to reject, accounting for the multitude of tests. This paper suggests a stepwise multiple testing procedure that asymptotically controls the familywise error rate. Compared to related single-step methods, the procedure is more powerful and often will reject more false hypotheses. In addition, we advocate the use of <b>studentization</b> when feasible. Unlike some stepwise methods, the method implicitly captures the joint dependence structure of the test statistics, which results in increased ability to detect false hypotheses. The methodology {{is presented in the}} context of comparing several strategies to a common benchmark. However, our ideas can easily be extended to other contexts where multiple tests occur. Some simulation studies show the improvements of our methods over previous proposals. We also provide an application to a set of real data. Copyright The Econometric Society 2005. ...|$|E
40|$|This paper {{develops}} an asymptotic {{theory of}} the series estimation under general crosssectional dependence and heterogeneity, covering both nonparametric and semiparametric estimates. A uniform rate of consistency, asymptotic normality and sufficient conditions for the √ n rate of convergence are established. A new data-driven <b>studentization</b> that dispenses {{with the need for}} "distance measures", as required by the spatial HAC estimation, is introduced and leads to asymptotically correct inference. Conditions imposed on dependence are carefully formulated to accommodate various cross-sectional settings plausible in economic applications and readily apply to panel and time series data. Strong, as well as weak dependence are covered, filling the current gap in the theoretical literature, and conditional heteroscedasticity is allowed. A finite sample Monte Carlo study indicates a highly satisfactory performance of the estimation and testing procedures. Applying the methods of the paper to two illustrative examples reveals some subtle yet interesting differences in results, compared to that under the assumption of independence...|$|E
