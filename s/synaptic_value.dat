0|36|Public
40|$|Dynamic {{learning}} of random stimuli {{can be described}} as a random walk among the stable <b>synaptic</b> <b>values.</b> It is shown that prototype extraction can take place in material attractor neural networks when the stimuli are correlated and hierarchically organized. The network learns a set of attractors representing the prototypes in a completely unsupervised fashion and is able to modify its attractors when the input statistics change. Learning and forgetting rates are computed. Keywords: attractor neural networks, stochastic learning 1 INTRODUCTION Unsupervised {{learning of}} uncorrelated patterns in attractor networks was recently described 1 as a stochastic process on the distribution of <b>synaptic</b> <b>values</b> characterizing the network. In this approach synapses have a finite number of stable states (efficacies) and learning is schematized as a random walk among them. Probability of each step is determined by the activities of the two neurons connected by the synapse. Neuronal states are in turn [...] ...|$|R
30|$|Where 푋푖 is the 푖푡h input vector, 푉푖푗 is the <b>synaptic</b> weight <b>value</b> {{between the}} 푖푡hinput neuron and 푗푡hhidden neuron, and 푗 is the bias value. The output y at the single output neuron is {{calculated}} as in Eq. 3.|$|R
40|$|A {{theory of}} {{temporally}} asymmetric Hebb (TAH) rules which depress or potentiate synapses depending upon whether the postsynaptic cell fires {{before or after}} the presynaptic one is presented. Using the Fokker-Planck formalism, we show that the equilibrium synaptic distribution induced by such rules is highly sensitive to the manner in which bounds on the allowed range of <b>synaptic</b> <b>values</b> are imposed. In a biologically plausible multiplicative model, we find that the synapses in asynchronous networks reach a distribution that is invariant to the firing rates of either the pre- or post-synaptic cells. When these cells are temporally correlated, the synaptic strength varies smoothly with the degree and phase of synchrony between the cells. Comment: 3 figures, minor corrections of equations and tex...|$|R
40|$|This work {{introduces}} {{a system for}} an evolutionary design of virtual organisms capable of effective movement in a simulated environment. The morphology and the control system are simultaneously developed by an evolutionary algorithms. The system also allows to design organisms in an editor and evolution of the control system with an immutable morphology. The quality evaluation and viewing of evolved organisms is done in a simulated 3 D physical environment. The work put stress on the optimization of time and computing complexity of the evolutionary process. This optimization is achieved by using symmetry of organisms and their movement with HyperNEAT-generative encoding of <b>synaptic</b> <b>values.</b> Further optimization is achieved by limiting the variety of mutual module connections and focusing on the harmonic movement of organisms...|$|R
40|$|We {{present a}} hybrid analog/digital very large scale {{integration}} (VLSI) {{implementation of a}} spiking neural network with programmable synaptic weights. The <b>synaptic</b> weight <b>values</b> are stored in an asynchronous Static Random Access Memory (SRAM) module, which is interfaced to a fast current-mode event-driven DAC for producing synaptic currents with the appropriate amplitude values. These currents are further integrated by current-mode integrator synapses to produce biophysically realistic temporal dynamics. The synapse output currents are then integrated by compact and efficient integrate and fire silicon neuron circuits with spike-frequency adaptation and adjustable refractory period and spike-reset voltage settings. The fabricated chip comprises a total of 32 $times$ 32 SRAM cells, 4 $times$ 32 synapse circuits and 32 $times$ 1 silicon neurons. It acts as a transceiver, receiving asynchronous events in input, performing neural computation with hybrid analog/digital circuits on the input spikes, and eventually producing digital asynchronous events in output. Input, output, and <b>synaptic</b> weight <b>values</b> are transmitted to/from the chip using a common communication protocol based on the Address Event Representation (AER). Using this representation {{it is possible to}} interface the device to a workstation or a micro-controller and explore the effect of different types of Spike-Timing Dependent Plasticity (STDP) learning algorithms for updating the <b>synaptic</b> weights <b>values</b> in the SRAM module. We present experimental results demonstrating the correct operation of all the circuits present on the chip...|$|R
40|$|A {{learning}} attractor {{neural network}} (LANN) {{with a double}} dynamics of neural activities and synaptic efficacies, operating on two different time scales is studied by simulations in preparation for an electronic implementation. The present network includes several quasi-realistic features: neurons are represented by their afferent currents and output spike rates; excitatory and inhibitory neurons are separated; attractor spike rates as well as coding levels in arriving stimuli are low; learning takes place only between excitatory units. Synaptic dynamics is an unsupervised, analog Hebbian process, but long term memory {{in the absence of}} neural activity is maintained by a refresh mechanism which on long time scales discretizes the <b>synaptic</b> <b>values,</b> converting learning into an asynchronous stochastic process induced by the stimuli on the synaptic efficacies. This network is intended to learn a set of attractors from the statistics of freely arriving stimuli, which are represente [...] ...|$|R
40|$|We {{show that}} {{stochastic}} learning of attractors {{can take place}} in a situation in which either only potentiation or only depression of synaptic efficacies are caused in a structured Hebbian way. In each case the transition in the opposite sense take place at random, but occurs only upon presentation of a stimulus. The outcome is an associative memory with the palimpsest property. It is shown that structured potentiation produces more effective learning than structured depression, i. e. it creates a network with a much higher number of retrievable memories. Introduction Unsupervised learning of uncorrelated stimuli in attractor networks was recently described [1] as a stochastic process on the distribution of <b>synaptic</b> <b>values</b> characterizing the network. In this approach synapses have a finite number of stable states (efficacies). Learning is schematized as a random walk among them. Probability of each step is determined by the activities of the two neurons connected by the synapse. Neuron [...] ...|$|R
40|$|In {{this paper}} an {{analogue}} two-quadrant multiplier {{suited for the}} implementation of large arrays of multipliers, {{as in the case of}} analogue VLSI implementation of artificial neural networks, is presented. To pursue low power consumption and to minimize the silicon area a translinear current mode approach has been adopted. Moreover, we coded the <b>synaptic</b> weight <b>value</b> as the absolute value and sign, thus splitting the circuit into a simple one-quadrant multiplier and a weight sign management. Experimental results are reported and discussed...|$|R
3000|$|... without {{considering}} a refractory period raises mathematical problems. One can in principle have uncountably many spikes in a finite time interval {{leading to the}} divergence of physical quantities like energy. Also, one can generate nice causal paradoxes [37]. Take a loop with two neurons one excitatory and one inhibitory and assume instantaneous propagation (the α profile is then represented by a Dirac distribution). Then, depending on the <b>synaptic</b> weights <b>value,</b> one can {{have a situation where}} neuron 1 fires instantaneously and make instantaneously 2 firing which prevents instantaneously 1 from firing and so on. So taking the limit [...]...|$|R
40|$|We {{discuss the}} long term {{maintenance}} of acquired memory in synaptic connections of a perpetually learning electronic device. This is affected by ascribing each synapse {{a finite number of}} stable states in which it can maintain for indefinitely long periods. Learning uncorrelated stimuli is expressed as a stochastic process produced by the neural activities on the synapses. In several interesting cases the stochastic process can be analyzed in detail, leading to a clarification of the performance of the network, as an associative memory, during the process of uninterrupted learning. The stochastic nature of the process and the existence of an asymptotic distribution for the <b>synaptic</b> <b>values</b> in the network imply generically that the memory is a palimpsest but capacity is as low as log N for a network of N neurons. The only way we find for avoiding this tight constraint is to allow the parameters governing the learning process (the coding level of the stimuli; the transition probabilities for potentiation and depression and the number of stable synaptic levels) to depend on the number of neurons. It is shown that a network with synapses that have two stable states can dynamically learn with optimal storage efficiency, be a palimpsest, and maintain its (associative) memory for an indefinitely long time provided the coding level is low and depression is equilibrated against potentiation. We suggest that an option so easily implementable in material devices would not have been overlooked by biology. Finally we discuss the stochastic learning on synapses with variable number of stable synaptic states...|$|R
40|$|Experiments {{in visual}} cortex {{have shown that}} the firing rate of a neuron in {{response}} to the simultaneous presentation of a preferred and non-preferred stimulus within the receptive field is intermediate between that for the two stimuli alone (stimulus competition). Attention directed to one of the stimuli drives the response towards the response induced by the attended stimulus alone (selective attention). This study shows that a simple feedforward model with fixed <b>synaptic</b> conductance <b>values</b> can reproduce these two phenomena using synchronization in the gamma-frequency range to increase the effective synaptic gain for the responses to the attended stimulus. The performance of the model is robust to changes in the parameter values. The model predicts that the phase locking between presynaptic input and output spikes increases with attention...|$|R
40|$|The {{cellular}} {{bases of}} learning are currently under active investigation by both experimental and theoretical means. In this paper, a simple neuronal wiring diagram is proposed that can reproduce both simple and higher-order behavioral paradigms seen in invertebrate classical conditioning experiments. Learning {{in this model}} does not take place by modification of <b>synaptic</b> strength <b>values.</b> Instead, the model uses a layer of interneurons with modifiable thresholds for spike initiation, {{as suggested by the}} plasticity mechanisms thought to operate in Hermissenda [Alkon, D. L. (1983) Sci. Am. 249, 70 - 84]. The model therefore has an advantage in plausibility compared with more standard models using Hebb synapses or their functional equivalents, which have not yet been demonstrated in any invertebrate organism...|$|R
40|$|Abstract Experiments {{in visual}} cortex {{have shown that}} the firing rate of a neuron in {{response}} to the simultaneous presentation of a preferred and non-preferred stimulus within the receptive field is intermediate between that for the two stimuli alone (stimulus competition). Attention directed to one of the stimuli drives the response towards the response induced by the attended stimulus alone (selective attention). This study shows that a simple feedforward model with fixed <b>synaptic</b> conductance <b>values</b> can reproduce these two phenomena using synchronization in the gamma-frequency range to increase the effective synaptic gain for the responses to the attended stimulus. The performance of the model is robust to changes in the parameter values. The model predicts that the phase locking between presynaptic input and output spikes increases with attention...|$|R
40|$|Similar {{activity}} patterns at both neuron {{and network}} levels can arise from {{different combinations of}} membrane and <b>synaptic</b> conduc-tance <b>values.</b> A strategy bywhich neuronsmay preserve their electrical output is via cell type-dependent balances of inward and outward currents. Measurements ofmRNA transcripts that encode ion channel proteinswithinmotor neurons in the crustacean cardiac ganglion recently revealed correlations between certain channel types. To determine whether balances of intrinsic currents potentially resulting from such correlations preserve certain electrical cell outputs, we developed a nominal biophysical model of the crustacean cardiac ganglion using biological data. Predictions from the nominal model showed that coregulation of ionic currents may preserve the key characteristics ofmotor neuron activity. We then developed amethodology of sampling amultidimensional parameter space to select an appropriate model set for meaningful comparison with variations in correlations seen in biological datasets...|$|R
50|$|A long-term, {{concurrent}} confocal microscopy and electrophysiology investigation {{conducted on}} cortical rat in-vitro neural networks (age > 3 weeks in-vitro) growing on Multi Electrode Arrays examined {{the correlation between}} network activity levels {{and changes in the}} sizes of individual synapses. Specifically, long-term fluorescent microscopy was used to track changes in the quantity (fluorescence) of PSD-95 molecules at individual synapses over timescales of several days. Since PSD-95 molecules anchor post-synaptic AMPA and NMDA receptors, they serve as reliable quantitative markers for post-synaptic transmembrane glutamate receptors. This investigation consisted of two sets of experiments. In the first set, synapse-morphology and spontaneous neural activity were monitored for about 90 hours (i.e. no external stimuli or pharmaceutical manipulations were used to perturb the neuronal networks). During this period, the sizes of individual synapses were observed to fluctuate considerably; yet distributions of synaptic sizes as well as average <b>synaptic</b> size <b>values</b> remained remarkably constant. It was found that ongoing activity acted to constrain synaptic sizes by increasing the tendency of large synapses to shrink and increasing the tendency of small synapses to grow. Thus, activity acted to maintain distributions of synaptic sizes (at the population level) within certain limits. In the second set of experiments the same analysis was performed after the addition of TTX to block all spontaneous activity. This led to a broadening of synaptic size distributions and to increases in average <b>synaptic</b> size <b>values.</b> When individual synapses were followed over time, their sizes were still found to fluctuate significantly, however now, no relationships were found between the extent or direction of size changes and initial synaptic size. In particular, no evidence was found that changes in synaptic size scaled with initial synaptic size. This indicated that the homeostatic growth in AMPA receptor content associated with the suppression of activity is a population phenomenon, that results from the loss of activity-dependent constraints, not from the scaling of AMPA receptor content at individual synapses.|$|R
40|$|International audiencePolychronization {{has been}} {{proposed}} as a possible way to investigate the notion of cell assemblies and to understand their role as memory supports for information coding. In a spiking neuron network, polychronous groups (PGs) are small subsets of neurons that can be activated in a chain reaction according to a specific time-locked pattern. PGs can be detected in a neural network with known connection delays and visualized on a spike raster plot. In this paper, we specify the definition of PGs, making a distinction between structural and dynamical polychronous groups. We propose two algortihms to scan for structural PGs supported by a given network topology, one based {{on the distribution of}} connection delays and the other taking into account the <b>synaptic</b> weight <b>values.</b> At last, we propose a third algorithm to scan for the PGs that are actually activated in the network dynamics during a given time window...|$|R
40|$|In {{this paper}} we present an {{asynchronous}} VLSI neuromorphic architecture comprising {{an array of}} integrate and fire neurons and dynamic synapse circuits with programmable weights. To store <b>synaptic</b> weight <b>values,</b> we designed a novel asynchronous SRAM block, integrated it on chip and connected it to the dynamic synapse circuits, via a fast current-mode DAC. The control and data signals used for programming the weights into the SRAM, {{as well as the}} standard input and output signals, are encoded using the AER representation. The device acts as a transceiver, both receiving Address-Events in input and transmitting them as output spikes. The possibility of changing the synaptic weights via the AER protocol allows the flexibility of exploring different STDP learning algorithms in a mixed SW/HW setup. We provide experimental results measured from the chip that demonstrate the correct behavior of all the circuit blocks implemented on the chip...|$|R
40|$|Artificial Neural {{networks}} have found many applications in various {{fields such as}} function approximation, time-series prediction, and adaptive control. The performance of a neural network depends on many factors, including the network structure, the selection of activation functions, the learning rate of the training algorithm, and initial <b>synaptic</b> weight <b>values,</b> etc. Genetic algorithms are inspired by Charles Darwin’s theory of natural selection (“survival of the fittest”). They are heuristic search techniques {{that are based on}} aspects of natural evolution, such as inheritance, mutation, selection, and crossover. This research utilizes a genetic algorithm to optimize multi-layer feedforward neural network performance and structure. The goal is to minimize both the function of output errors and the number of connections of network. The algorithm is modeled in C++ and tested on several different data sets. Computer simulation results show that the proposed algorithm can successfully determine the appropriate network size for optimal performance. This research also includes studies of the effects of population size, crossover type, probability of bit mutation, and the error scaling factor...|$|R
30|$|Accurate {{forecasting}} {{of changes}} in stock market indices can provide financial managers and individual investors with strategically valuable information. However, predicting the closing prices of stock indices remains a challenging task because stock price movements are characterized by high volatility and nonlinearity. This paper proposes a novel condensed polynomial neural network (CPNN) for the task of forecasting stock closing price indices. We developed a model that uses partial descriptions (PDs) and is limited to only two layers for the PNN architecture. The outputs of these PDs along with the original features are fed to a single output neuron, and the <b>synaptic</b> weight <b>values</b> and biases of the CPNN are optimized by a genetic algorithm. The proposed model was evaluated by predicting the next day’s closing price of five fast-growing stock indices: the BSE, DJIA, NASDAQ, FTSE, and TAIEX. In comparative testing, the proposed model proved its ability to provide closing price predictions with superior accuracy. Further, the Deibold-Mariano test justified the statistical significance of the model, establishing that this approach can be adopted as a competent financial forecasting tool.|$|R
40|$|The {{integration}} of synaptic inputs in a neuron can be nonlinear {{not just at}} the axon, but also locally in the dendrites if they are imbued with active voltage-gated ion channels. For example, CA 1 pyramidal neurons have high densities of sodium and potassium and currents in their dendrites, and these densities can vary substantially in the arbor [1, 2]. Such nonlinearities can lead to com-partmentalized responses to inputs [3], with branches acting as individual nonlinear units in which dendritic spikes occur. A cell can thus function as a multi-layered network with the soma as final output. This motivates determining when a given set of synaptic inputs {{is large enough to}} generate a local dendritic spike, or, alterna-tively, determining the <b>synaptic</b> conductance <b>value(s)</b> at threshold for producing a spike. Above- and below-threshold conditions are known to be separated by the threshold or critical surface [4]. Here it is shown that the synaptic conductance leading to a threshold solution can be found by modifying Newton methods developed to find steady-state solu-tions in fluid mechanics [5]. Consider a general form of the cable equation...|$|R
40|$|This item also {{falls under}} Society for Neuroscience copyright. For more information, please visit [URL]. Link active as of 1 / 29 / 2011. Link {{maintenance}} {{is the responsibility}} of the Society for Neuroscience. Digital Object Identifier 10. 1523 /JNEUROSCI. 6435 - 09. 2010 Similar activity patterns at both neuron and network levels can arise from different combinations of membrane and <b>synaptic</b> conductance <b>values.</b> A strategy by which neurons may preserve their electrical output is via cell type-dependent balances of inward and outward currents. Measurements of mRNA transcripts that encode ion channel proteins within motor neurons in the crustacean cardiac ganglion recently revealed correlations between certain channel types. To determine whether balances of intrinsic currents potentially resulting from such correlations preserve certain electrical cell outputs, we developed a nominal biophysical model of the crustacean cardiac ganglion using biological data. Predictions from the nominal model showed that coregulation of ionic currents may preserve the key characteristics of motor neuron activity. We then developed a methodology of sampling a multidimensional parameter space to select an appropriate model set for meaningful comparison with variations in correlations seen in biological datasets...|$|R
40|$|One of {{the most}} pervading {{concepts}} underlying computa- tional models of information processing in the brain is linear input integration of rate coded uni-variate information by neurons. After a suitable learning process this results in neuronal structures that statically represent knowledge as a vector of real <b>valued</b> <b>synaptic</b> weights. Although this general framework {{has contributed to the}} many successes of connectionism, in this paper we argue that for all but the most basic of cognitive processes, a more complex, multi-variate dynamic neural coding mechanism is required - knowledge should not be spacially bound to a particular neuron or group of neurons. We conclude the paper with discussion of a simple experiment that illustrates dynamic knowledge representation in a spiking neuron connectionist system...|$|R
30|$|As a case study, we {{investigate}} {{the role of}} the general anesthetic propofol on rebound spiking in the central nervous system [17]. Many general anesthetics, including propofol, prolong the duration of GABAergic inhibitory postsynaptic currents (IPSCs), and this action contributes to the behavioral properties of these drugs [1]. Mathematically speaking, propofol changes the slow time-scale of the deactivation of the GABAa receptor channel. Paradoxically, low doses of propofol causes excitation rather than sedation. This behavior can already be observed in an isolated single cell model that receives GABAergic IPSCs [18]. We adapt this propofol neuron model formulated in [18] slightly in order to more clearly emphasize the role of folded saddle canards in the observed dynamic behavior. We note that only minimal adjustments have been made in order to preserve the qualitative behavior of the model, namely the observation of post-inhibitory rebound spiking for a window of GABAa <b>synaptic</b> time-scale <b>values.</b> The modification consists of two parameter changes, the details of which are given below. The essential difference is that this modification shifts the resting membrane potential to a lower, more hyperpolarized, voltage value, allowing a more uniform separation of time-scales over a range of GABAa time-scale values. This modification enables us to make full use of the machinery of GSPT. More details of the relation between the two models {{can be found in the}} last section of the paper.|$|R
40|$|Computer {{assistance}} {{has reached}} virtually in every domain {{with in the}} field of medical imaging. Dedicated Computer aided diagnosis (CAD) tools with proven clinical impact exist for narrow range of applications. Medical imaging modalities such as X-Rays, CT, MRI, CT-PET, and PET provide visual information for accurate diagnosis and indexed medical treatment. Now a days Medical databases are used automatically to classify the visual features for retrieving image which provides a Indexed reference for easy therapy. Medical image retrieval provides an archive for identifying the similar features with the given query image. In this work it is proposed to implement a novel feature selection mechanism using discrete sine transform. This classification results use support vector machine (SVM) which classifies kernel function, Regression <b>values,</b> <b>Synaptic</b> weights, Activation functions using multilayer perceptron neural network. The results obtained are performed with noise and blur to obtain noise free image which is further computed with statistical values and histogram processing to determine the accuracy of similar feature extracted...|$|R
40|$|The phonetic context {{has a large}} {{effect on}} stop consonants in a {{continuous}} speech signal [1]. Therefore recognition systems that model allophones using context-dependent Hidden Markov Models have been implemented [3]. HMMs have a great ability for the segmentation in the temporal domain [4][6] but have some difficulties in the recognition because the MLE training (Maximum Likelihood Estimation) is not discriminant, whereas the discrimination {{is one of the}} abilities of the Artificial Neural Networks models. In the last three years we have developed a new ANN model named OWE (Orthogonal Weight Estimator) [9][10]. The principle of the OWE is a ANN that classifies an input pattern according to contextual environment. This new ANN architecture tackles the problem of context dependent behaviour training. Roughly, the principle is based on main MLP (Multilayered Perceptron) in which each <b>synaptic</b> weight connection <b>value</b> is estimated by another MLP (an OWE) with respect to context representatio [...] ...|$|R
40|$|We {{model the}} motion of a {{receptor}} on the membrane surface of a synapse as free Brownian motion in a planar domain with intermittent trappings in and escapes out of corrals with narrow openings. We compute the mean confinement time of the Brownian particle in the asymptotic limit of a narrow opening and calculate the probability to exit through a given small opening, when the boundary contains more than one. Using this approach, {{it is possible to}} describe the Brownian motion of a random particle in an environment containing domains with small openings by a coarse grained diffusion process. We use the results to estimate the confinement time {{as a function of the}} parameters and also the time it takes for a diffusing receptor to be anchored at its final destination on the postsynaptic membrane, after it is inserted in the membrane. This approach provides a framework for the theoretical study of receptor trafficking on membranes. This process underlies synaptic plasticity, which relates to learning and memory. In particular, it is believed that the memory state in the brain is stored primarily in the pattern of <b>synaptic</b> weight <b>values,</b> which are controlled by neuronal activity. At a molecular level, the synaptic weight is determined by the number and properties of protein channels (receptors) on the synapse. The synaptic receptors are trafficked in and out of synapses by a diffusion process. Following their synthesis in the endoplasmic reticulum, receptors are trafficked to their postsynaptic sites on dendrites and axons. In this model the receptors are first inserted into the extrasynaptic plasma membrane and then random walk in and out of corrals through narrow openings on their way to their final destination...|$|R
40|$|Selecting {{responses}} in working memory while processing combinations of stimuli depends strongly on their relations stored in long-term memory. However, {{the learning of}} XOR-like combinations of stimuli and responses according to complex rules raises {{the issue of the}} non-linear separability of the responses within the space of stimuli. One proposed solution is to add neurons that perform a stage of non-linear processing between the stimuli and responses, at the cost of increasing the network size. Based on the non-linear integration of synaptic inputs within dendritic compartments, we propose here an inter-synaptic (IS) learning algorithm that determines the probability of potentiating/depressing each synapse {{as a function of the}} co-activity of the other synapses within the same dendrite. The IS learning is effective with random connectivity and without either a priori wiring or additional neurons. Our results show that IS learning generates efficacy values that are sufficient for the processing of XOR-like combinations, on the basis of the sole correlational structure of the stimuli and responses. We analyze the types of dendrites involved in terms of the number of synapses from pre-synaptic neurons coding for the stimuli and responses. The <b>synaptic</b> efficacy <b>values</b> obtained show that different dendrites specialize in the detection of different combinations of stimuli. The resulting behavior of the cortical network model is analyzed as a function of inter-synaptic vs. Hebbian learning. Combinatorial priming effects show that the retrospective activity of neurons coding for the stimuli trigger XOR-like combination-selective prospective activity of neurons coding for the expected response. The synergistic effects of inter-synaptic learning and of mixed-coding neurons are simulated. The results show that, although each mechanism is sufficient by itself, their combined effects improve the performance of the network...|$|R
40|$|Current {{methods used}} to study neural {{communication}} {{have not been able}} to achieve both good spatial and temporal resolution of recordings. There are two ways to record synaptic potentials from nerve endings: recordings using single or dual intracellular or extra cellular metal electrodes give good temporal resolution but poor spatial resolution, and recording activity with fluorescent dyes gives good spatial resolution but poor temporal resolution. Such medical research activity in the area of neurological signal detection has thus identified a requirement for the design of a CMOS circuit that contains an array of independent sensors. As both spatial and temporal distribution of acquired data is required in this application, the circuit must be capable of continuous measurement of synaptic potentials from an array of points on a tissue sample, with a 10 µm separation between sensor points. The major requirement for the circuit is that it is capable of sensing synaptic potentials of the order of several mV, with a resolution of 0. 05 mV. For data recording purposes, the circuit must amplify these synaptic potentials and digitise them together with their locations in the sensor array. Finally, the circuit must be biologically inert, to avoid specimen deterioration. This paper presents the design of a prototype single-chip circuit, which provides a 6 x 3 array of independent synaptic potential sensors. The signal from each of the sensors is amplified and time-multiplexed into an on-chip A/D converter. The circuit provides an 8 -bit <b>synaptic</b> potential <b>value,</b> together with an 8 -bit field containing array location and trigger signals suitable for external data acquisition instrumentation. Our test circuit is implemented in a low-cost 0. 5 um, 5 V CMOS process. The fabricated die is mounted in a standard 40 pin DIP ceramic package, with no lid to allow direct contact of the die surface with the tissue sample. The only post-processing step required for these packages is to encapsulate the exposed bond wires to ensure that the device is biologically inert. No further processing of the silicon die is required. Both the circuit design and the chip performance will be presented in the seminar...|$|R
40|$|Abstract Background The {{notion of}} the nucleus tractus solitarius (NTS) as a {{comparator}} evaluating the error signal between its rostral neural structures (RNS) and the cardiovascular receptor afferents into it has been recently presented. From this perspective, stress can cause hypertension via set point changes, so offering an answer to an old question. Even though the local blood flow to tissues is influenced by circulating vasoactive hormones and also by local factors, there is yet significant sympathetic control. It is well established {{that the state of}} maturation of sympathetic innervation of blood vessels at birth varies across animal species and it takes place mostly during the postnatal period. During ontogeny, chemoreceptors are functional; they discharge when the partial pressures of oxygen and carbon dioxide in the arterial blood are not normal. Methods The model is a simple biological plausible adaptative neural network to simulate the development of the sympathetic nervous control. It is hypothesized that during ontogeny, from the RNS afferents to the NTS, the optimal level of each sympathetic efferent discharge is learned through the chemoreceptors' feedback. Its mean discharge leads to normal oxygen and carbon dioxide levels in each tissue. Thus, the sympathetic efferent discharge sets at the optimal level if, despite maximal drift, the local blood flow is compensated for by autoregulation. Such optimal level produces minimum chemoreceptor output, which must be maintained by the nervous system. Since blood flow is controlled by arterial blood pressure, the long-term mean level is stabilized to regulate oxygen and carbon dioxide levels. After development, the cardiopulmonary reflexes {{play an important role in}} controlling efferent sympathetic nerve activity to the kidneys and modulating sodium and water excretion. Results Starting from fixed RNS afferents to the NTS and random <b>synaptic</b> weight <b>values,</b> the sympathetic efferents converged to the optimal values. When learning was completed, the output from the chemoreceptors became zero because the sympathetic efferents led to normal partial pressures of oxygen and carbon dioxide. Conclusions We introduce here a simple simulating computational theory to study, from a neurophysiologic point of view, the sympathetic development of cardiovascular regulation due to feedback signals sent off by cardiovascular receptors. The model simulates, too, how the NTS, as emergent property, acts as a comparator and how its rostral afferents behave as set point. </p...|$|R
40|$|Alpha (greater than 40 microns) and gamma (less than 30 microns) motoneurons in inspiratory motor nuclei of the {{thoracic}} {{spinal cord}} {{of the adult}} cat were labelled retrogradely by the intramuscular injection of HRP. Small (less than 30 microns) unlabelled neurons within 200 - 300 microns of labelled motoneurons were analysed qualitatively and quantitatively with both the light and electron microscope. Most of these small unlabelled neurons had inconspicuous nucleoli, wrinkled nuclear membranes, low numbers of nuclear pores, and Nissl bodies which were either small or had {{the form of an}} amorphous perinuclear band. Such Nissl bodies were composed primarily of aggregates of polyribosomes within which short fragments of granular endoplasmic reticulum were distributed. Alpha and gamma motoneurons in contrast had prominent nucleoli, smooth-contoured nuclei, more nuclear pores and large, discrete Nissl bodies. Such Nissl bodies were composed primarily of several lamellae of granular endoplasmic reticulum with linear arrays of polyribosomes arranged between individual cisternae. Alpha motoneurons had most synaptic terminals on their cell bodies, gamma motoneurons had least and small unlabelled neurons had intermediate <b>values.</b> <b>Synaptic</b> terminals of the S-, F- T- and C-type were observed on alpha motoneurons, whereas only S- and F-types were observed on gamma motoneurons and small unlabelled neurons. Since they were unlabelled and differed morphologically from both alpha and gamma motoneurons, but were similar to small interneurons described elsewhere in the spinal cord and brain, it is suggested that the small unlabelled neurons located in the external intercostal and levator costae motor pools are interneurons. The functional significance of some of the morphological features which distinguish interneurons from motoneurons is discussed...|$|R
40|$|The advancements {{in systems}} and {{synthetic}} biology have been broadening {{the range of}} realizable systems with increasing complexity both in vitro and in vivo. Systems for digital logic operations, signal processing, analog computation, program flow control, {{as well as those}} composed of different functions – for example an on-site diagnostic system based on multiple biomarker measurements and signal processing – have been realized successfully. However, the efforts to date tend to tackle each design problem separately, relying on ad hoc strategies rather than providing more general solutions based on a unified and extensible architecture, resulting in long development cycle and rigid systems that require redesign even for small specification changes. Inspired by well-tested techniques adopted in electronics design automation (EDA), this work aims to remedy current design methodology by establishing a standardized, complete flow for realizing biomolecular systems. Given a behavior specification, the flow streamlines all the steps from modeling, synthesis, simulation, to final technology mapping onto implementing chassis. The resulted biomolecular systems of our design flow are all built on top of an FPGA-like reconfigurable architecture with recurring modules. Each module is designed the function of eachmodule depends on the concentrations of assigned auxiliary species acting as the “tuning knobs. ” Reconfigurability not only simplifies redesign for altered specification or post-simulation correction, but also makes post-manufacture fine-tuning – even after system deployment – possible. This flexibility is especially important in synthetic biology due to the unavoidable variations in both the deployed biological environment and the biomolecular reactions forming the designed system. In fact, by combining the system’s reconfigurability and neural network’s self-adaptiveness through learning, we further demonstrate the high compatibility of neuromorphic computation to our proposed architecture. Simulation results verified that with each module implementing a neuron of selected model (ex. spike-based, threshold-gate-like, etc.), accompanied by an appropriate choice of reconfigurable properties (ex. threshold <b>value,</b> <b>synaptic</b> weight, etc.), the system built from our proposed flow can indeed perform desired neuromorphic functions...|$|R
40|$|Realization of the {{conventional}} Von Neumann architecture faces increasing challenges due to growing process variations, device reliability and power consumption. As an appealing architectural solution, brain-inspired neuromorphic computing has drawn {{a great deal of}} research interest due to its potential improved scalability and power efficiency, and better suitability in processing complex tasks. Moreover, inherit error resilience in neuromorphic computing allows remarkable power and energy savings by exploiting approximate computing. This dissertation focuses on a scalable and energy efficient neurocomputing architecture which leverages emerging memristor nanodevices and a novel approximate arithmetic for cognitive computing. First, brain-inspired digital neuromorphic processor (DNP) architecture with memristive synaptic crossbar is presented for large scale spiking neural networks. We leverage memristor nanodevices to build an N ?N crossbar array to store not only multibit <b>synaptic</b> weight <b>values</b> but also the network configuration data with significantly reduced area cost. Additionally, the crossbar array is accessible both column- and row-wise to significantly expedite the synaptic weight update process for on-chip learning. The proposed digital pulse width modulator (PWM) readily creates a binary pulse with various durations to read and write the multilevel memristors with low cost. Our design integrates N digital leaky integrate-and-fire (LIF) silicon neurons to mimic their biological counterparts and the respective on-chip learning circuits for implementing spike timing dependent plasticity (STDP) learning rules. The proposed column based analog-to-digital conversion (ADC) scheme accumulates the pre-synaptic weights of a neuron efficiently and reduces silicon area by using only one shared arithmetic unit for processing LIF operations of all N neurons. With 256 silicon neurons, the learning circuits and 64 K synapses, the power dissipation and area of our design are evaluated as 6. 45 mW and 1. 86 mm 2, respectively, in a 90 nm CMOS technology. Furthermore, arithmetic computations contribute significantly to the overall processing time and power of the proposed architecture. In particular, addition and comparison operations represent 88. 5 % and 42. 9 % of processing time and power for digital LIF computation, respectively. Hence, by exploiting the built-in resilience of the presented neuromorphic architecture, we propose novel approximate adder and comparator designs to significantly reduce energy consumption with a very low er- ror rate. The significantly improved error rate and critical path delay stem from a novel carry prediction technique that leverages the information from less significant input bits in a parallel manner. An error magnitude reduction scheme is proposed to further reduce amount of error once detected with low cost in the proposed adder design. Implemented in a commercial 90 nm CMOS process, it is shown that the proposed adder is up to 2. 4 ? faster and 43 % more energy efficient over traditional adders while having an error rate of only 0. 18 %. Additionally, the proposed com- parator achieves an error rate of less than 0. 1 % and an energy reduction of up to 4. 9 ? compared to {{the conventional}} ones. The proposed arithmetic has been adopted in a VLSI-based neuromorphic character recognition chip using unsupervised learning. The approximation errors of the proposed arithmetic units have been shown to have negligible impacts on the training process. Moreover, the energy saving of up to 66. 5 % over traditional arithmetic units is achieved for the neuromorphic chip with scaled supply levels...|$|R
40|$|Intracellular {{recordings}} {{were made}} from turtle cochlear hair cells {{in order to examine}} the properties of the post-synaptic potentials evoked by electrical stimulation of the efferent axons. Single shocks to the efferents generated a hair cell membrane hyperpolarization with an average amplitude generally less than 1 mV and lasting for about 100 ms. With short trains of shocks, the size of the post-synaptic potential grew markedly to a maximum of 20 - 30 mV. The interaction between pairs of shocks separated by a varying interval was studied. For an interval of 4 ms, the response to the second shock was increased on average by a factor of 3 and the conditioning effect of the first shock decayed with a time constant of about 100 ms. We suggest the augmentation in response to trains of shocks may be partly due to facilitation of efferent transmitter release. The efferent post-synaptic potentials could be reversibly abolished by perfusion with perilymphs containing 3 microM-curare or atropine, and infusion of acetylcholine gave a transient membrane hyperpolarization. These observations are consistent with efferent action being mediated via a cholinergic synapse onto the hair cells. The post-synaptic potentials could be reversed in polarity by injection of hyperpolarizing currents through the recording electrode. The reversal potential was estimated as about - 80 mV, 30 mV negative to the resting potential. Near reversal, a small brief depolarization was evident and may constitute a minor component of the <b>synaptic</b> response. The <b>value</b> of the reversal potential was unaffected by substitution of the perilymphatic chloride, but was altered in a predictable manner by changes in extracellular potassium concentration indicating that the post-synaptic potentials arise mainly by an increase in the permeability of the hair cell membrane to potassium ions. Throughout the post-synaptic hyperpolarization there was a reduction in the sensitivity of the hair cell to tones at its characteristic frequency. The desensitization, maximal for low sound pressures, varied in different cells from a factor of 1. 6 to 28. At the peak of the largest synaptic potentials, the receptor potential remained negative to the resting potential with all but the loudest characteristic frequency tone s. We suggest that there are two factors in efferent inhibition; one a r duction in the receptor potential at the hair cell's characteristic frequency and the other a hyperpolarization of its membrane potential which should reduce the release of excitatory transmitter onto the afferent terminals...|$|R
40|$|Alzheimer's disease (AD), both {{sporadic}} and genetic, is {{a chronic}} disorder characterized by {{activation of the}} amyloid/tau cascade in the hippocampus and isocortex. Besides neuroprotective approaches, also neurorestorative strategies for AD are under intensive investigations. [1] The melanocortin system consists of endogenous neuropeptides of the adrenocorticotropin/melanocyte-stimulating hormone (ACTH/MSH) family, acting via five different metabotropic melanocortin receptor subtypes (MC 1 -MC 5). Melanocortins also induce neuroprotection associated with long-lasting functional recovery and counteraction of cognitive decline, as found in acute experimental neurodegenerative conditions and more recently in a chronic neurodegenerative disease as AD. [2] Further, these endogenous peptides have been by us reported to stimulate neurogenesis in an acute neurodegenerative disorder as ischemic stroke. [3] Here we investigated the possible neuroprotective and neurogenic effect of melanocortins in AD with a medium level of severity by using 24 week-old (at {{the start of the}} study) APPSwe transgenic mice (Tg 2576). METHODS: Tg 2576 mice were treated (once daily on days 1 - 50) with a nanomolar dose of the melanocortin analog [Nle 4,D-Phe 7]α-melanocyte-stimulating hormone (NDP-α-MSH). Animals were prepared for 5 -bromo- 2 ’-deoxyuridine (BrdU) labeling of proliferating cells at days 1 - 11 of the study, and histological and immunohistochemical studies of the brain were performed for the assessment of neurogenesis. Further, the mouse ability to learn and recall was evaluated by means of the Morris water-maze test at the twenty-seventh week (starting 14 days after the first BrdU injection) and thirty-first week of age. Within 90 min {{the end of the last}} behavioural test (day 50 of the study; 31 week-old mice) animals were killed and the brains were removed and processed for histological examination. The whole hippocampi were dissected from brains of some animals to perform western blot analysis of the Zif 268 protein (Zif 268 protein is transiently expressed after <b>synaptic</b> activation). All <b>values</b> were analyzed by means of two-way repeated measures ANOVA (behavioral data) or one-way ANOVA (all other data), both followed by the Student-Newman-Keuls’ test. A value of p < 0. 05 was considered significant. RESULTS: Treatment of Tg 2576 mice with the melanocortin analog [Nle 4,D-Phe 7]α-melanocyte-stimulating hormone (NDP-α-MSH) reduced cerebral cortex/hippocampus level of Aβ deposit (p < 0. 001), increased hippocampus Zif 268 expression (p < 0. 001), improved brain histological picture and cognitive functions (p < 0. 001), relative to saline-treated Tg 2576 animals, and no signs of toxicity were recorded. Further, immunohistochemical examination of the hippocampus on day 50 (end of the study) showed, in the dentate gyrus of NDP-α-MSH-treated Tg 2576 mice, a very elevated number of BrdU immunoreactive cells colocalized with NeuN (indicator of mature neurons) and Zif 268 (indicator of functionally integrated neurons), in comparison with saline-treated Tg 2576 animals (p < 0. 001); no newly formed astrocytes were found. Animal pretreatment (before each administration of NDP-α-MSH) with the selective melanocortin MC 4 receptor antagonist HS 024 prevented all favourable effects of NDP-α-MSH (p < 0. 001). CONCLUSIONS: Our data suggest that MC 4 receptor-stimulating melanocortins are able to counteract cognitive decline in experimental AD not only by affording neuroprotection, but also by inducing intense neurogenesis. These agents could be candidates for an innovative and safe strategy to counteract AD progression in humans...|$|R

