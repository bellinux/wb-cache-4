56|347|Public
50|$|A {{rectilinear}} grid is a tessellation by rectangles or parallelepipeds {{that are}} not, in general, all congruent to each other. The cells {{may still be}} indexed by integers as above, but the mapping from indexes to vertex coordinates is less uniform than in a regular grid. An example of a rectilinear grid that is not regular appears on logarithmic <b>scale</b> <b>graph</b> paper.|$|E
50|$|The {{backscattering}} {{method is}} also employed in fiber optics applications to detect optical faults. Light propagating through a {{fiber optic cable}} gradually attenuates due to Rayleigh scattering. Faults are thus detected by monitoring the variation of part of the Rayleigh backscattered light. Since the backscattered light attenuates exponentially as it travels along the optical fiber cable, the attenuation characteristic is represented in a logarithmic <b>scale</b> <b>graph.</b> If {{the slope of the}} graph is steep, then power loss is high. If the slope is gentle, then optical fiber has a satisfactory loss characteristic.|$|E
40|$|Abstract. Vertex centric {{models for}} large <b>scale</b> <b>graph</b> {{processing}} are gaining traction {{due to their}} simple distributed programming abstraction. However, pure vertex centric algorithms under-perform due to large communication overheads and slow iterative convergence. We introduce GoFFish a scalable sub-graph cen-tric framework co-designed with a distributed persistent graph storage for large <b>scale</b> <b>graph</b> analytics on commodity clusters, offering the added natural flexibil-ity of shared memory sub-graph computation. We map Connected Components, SSSP and PageRank algorithms to this model and empirically analyze them for several real world graphs, demonstrating orders of magnitude improvements, in some cases, compared to Apache Giraph’s vertex centric framework. ...|$|E
40|$|Small-world {{architectures}} may be {{implicated in}} a range of phenomena from disease propagation to networks of neurons in the cerebral cortex [1 - 2]. While most of the recent attention on small-world networks has focussed on the effect of introducing disorder/randomness into a regular graph, we show that that the fundamental mechanism behind the small-world phenomenon is not disorder/randomness, but the presence of edges of many different length scales. Consequently, in order to explain the small-world phenomenon, we introduce the concept of multiple <b>scale</b> <b>graphs</b> and then state the multiple length scale hypothesis. Multiple <b>scale</b> <b>graphs</b> form a unifying conceptual framework for the study of evolving graphs. Moreover, small-world behavior in randomly rewired graphs is a consequence of features common to all multiple <b>scale</b> <b>graphs.</b> To support the multiple length <b>scale</b> hypothesis, novel <b>graph</b> architectures are introduced that need not be a result of random rewiring of a regular graph. In each case it is shown that whenever the graph exhibits small-world behavior, it also has edges of diverse length scales. We also show that the distribution of the length scales of the new edges is significantly more important than whether the new edges are long range, medium range or short range. ...|$|R
5000|$|... #Subtitle level 4: Edge dual {{transformation}} {{to generate}} <b>scale</b> free <b>graphs</b> with desired properties ...|$|R
40|$|<b>Scale</b> free <b>graphs</b> can {{be found}} very often as models of real {{networks}} and are characterized by a power law degree distribution, that is, for a constant γ≥ 1 the number of vertices of degree d is proportional to d^-γ. Experimental {{studies show that the}} eigenvalue distribution also follows a power law for the highest eigenvalues. Hence it has been conjectured that the power law of the degrees determines the power law of the eigenvalues. In this paper we show that we can construct a <b>scale</b> free <b>graph</b> with non highest eigenvalue power law distribution. For γ= 1 we can construct a <b>scale</b> free <b>graph</b> with small spectrum and a regular graph with eigenvalue power law distribution...|$|R
30|$|Tables 7, 8, 9 and 10 {{represent}} the running {{time of our}} approaches on the directed graph and LT model and IC. We can observe clearly that the LT model for our approach is faster than other approaches such as degree discount heuristics and that even the running time under IC for our algorithm still acceptable to be applied on large <b>scale</b> <b>graph.</b>|$|E
40|$|AbstractMassive {{datasets}} {{are becoming}} more prevalent. In this paper, we propose an algorithm to process a large symmetric matrix of billion <b>scale</b> <b>graph</b> in order to extract knowledge from graph dataset. For example, interesting patterns like the people who frequently visit your page and the most number of participating triangles can be obtained using the algorithm. These interesting patterns are discovered by computation of several eigen values and eigen vectors. The main challenge in analyzing the graph data are simplifying the graph, counting the triangles, finding trusses. These challenges are addressed in the proposed algorithm by using orthogonalization, parallelization and blocking techniques. The proposed algorithm is able to run on highly scalable MapReduce environment. we use a social network dataset (facebook approximately 2 to 7 TB of data) to evaluate the algorithm. we also show experimental results {{to prove that the}} proposed algorithm scale well and efficiently process the billion <b>scale</b> <b>graph...</b>|$|E
40|$|Initial {{release of}} CONCUSS: Combatting Network Complexity Using Structural Sparsity. CONCUSS is a Python {{software}} tool for large <b>scale</b> <b>graph</b> analytics whose {{efficiency and scalability}} come from exploiting the underlying "structural sparsity" of the data. Current modules use low-treedepth colorings (guaranteed in graph classes of bounded expansion) and allow users to {{count the number of}} occurences of a specific pattern within a graph (i. e. subgraph isomorphism counting) ...|$|E
40|$|Engineering is a {{profession}} where graphical presentation {{is very important}} in understanding and verifying results. Geometric proportions, spacing, and other features can be clearly perceived if a <b>scaled</b> <b>graph</b> is displayed together with the calculations, and thus, the engineering student can make better decisions about the final design. Overall, these spreadsheets with graphical capabilities help the learning process. The use of Excel spreadsheets in engineering education and professional practice is frequent because this tool is versatile and powerful. However, a deficiency of spreadsheets is the lack of graphic representation. This may be solved by using the programming tools of Visual Basic for Applications (VBA) that is included in the package of Microsoft Office. This paper presents a method to include <b>scaled</b> <b>graphs</b> into a spreadsheet, which completes the engineering calculations and helps in the final decision to accept or modify a design. These graphs are also useful in the drafting process, because the graphs can be easily transferred into any computer assisted drafting (CAD) program. Several actual class examples are included...|$|R
40|$|In {{this paper}} we study the {{behavior}} of maximum out/in-degree of binomial/Poisson random <b>scaled</b> sector <b>graphs</b> {{in the presence of}} random vertex and edge faults. We prove that the probability distribution of maximum degrees for random faulty <b>scaled</b> sector <b>graphs</b> with n vertices, where each vertex spans a sector of α radians, with radius r_n≪√(n/n), become concentrated on two consecutive integers, under some natural assumptions of faulty probabilities. Comment: 14 pages, 1 figur...|$|R
40|$|Abstract. Crossing {{minimization}} {{problem is}} a classic and very important problem in graph drawing [1]; the results directly affect {{the effectiveness of the}} layout, especially for very large <b>scale</b> <b>graphs.</b> But in many cases crossings cannot be avoided. In this paper we present two models for bipartite graph drawing, aiming to reduce crossings that cannot be avoided in the traditional bilayer drawings. We characterize crossing minimization problems in these models, and prove that they are N Pcomplete. ...|$|R
40|$|Large <b>scale</b> <b>graph</b> {{processing}} is a {{major research}} area for Big Data exploration. Vertex centric programming models like Pregel are gaining traction due to their simple abstraction that allows for scalable execution on distributed systems naturally. However, there are limitations to this approach which cause vertex centric algorithms to under-perform due to poor compute to communication overhead ratio and slow convergence of iterative superstep. In this paper we introduce GoFFish a scalable sub-graph centric framework co-designed with a distributed persistent graph storage for large <b>scale</b> <b>graph</b> analytics on commodity clusters. We introduce a sub-graph centric programming abstraction that combines the scalability of a vertex centric approach with the flexibility of shared memory sub-graph computation. We map Connected Components, SSSP and PageRank algorithms to this model to illustrate its flexibility. Further, we empirically analyze GoFFish using several real world graphs and demonstrate its significant performance improvement, orders of magnitude in some cases, compared to Apache Giraph, the leading open source vertex centric implementation. Comment: Under review by a conference, 201...|$|E
40|$|Anomaly {{detection}} {{in internet}} traffic today is {{largely based on}} quantifying traffic data. This thesis proposes a new algorithm SpreadRank, which detects spreading of internet traffic as an additional metric for traffic anomaly detection. SpreadRank uses large <b>scale</b> <b>graph</b> processing to calculate spreading from multiple gigabytes of NetFlow data obtained from core routers. Studying spreading is a useful tool in determining {{the role of an}} end-host and in identifying malicious behaviour. </p...|$|E
40|$|Shows {{original}} warranty survey tracts, tract acreages, assignees' names, {{dates of}} assignment, types of tract survey points (post, pole, tree species, etc.), streams, and county boundaries. "Entered according to Act of Congress {{in the year}} 1881 by John Gardiner C. E. in the Office of the Librarian of Congress at Washington. "Printed in northern/southern segments (sheets). LC sheets imperfect: Fold-lined, torn at fold lines. DLCLC Land ownership maps, 811 Includes text, notes, <b>scale</b> <b>graph,</b> key to personal-name abbreviations, and commonwealth coat-of-arms...|$|E
50|$|Both {{ends of a}} link are {{represented}} as a square to reinforce the above effect even at small <b>scales.</b> Directed <b>graphs</b> also incorporate arrowheads.|$|R
40|$|We {{present an}} exact {{analytical}} {{solution of the}} spectral problem of quasi one-dimensional <b>scaling</b> quantum <b>graphs.</b> Strongly stochastic in the classical limit, these systems are frequently employed as models of quantum chaos. We show that despite their classical stochasticity all <b>scaling</b> quantum <b>graphs</b> are explicitly solvable in the form $E_n=f(n) $, where $n$ is the sequence number of the energy level of the quantum graph and $f$ is a known function, which depends only on the physical and geometrical properties of the quantum graph. Our method of solution motivates a new classification scheme for quantum graphs: we show that each quantum graph can be uniquely assigned an integer $m$ reflecting its level of complexity. We show that a taut string with piecewise constant mass density provides an experimentally realizable analogue system of <b>scaling</b> quantum <b>graphs.</b> Comment: 40 pages, 10 figure...|$|R
50|$|This {{series is}} used to define the <b>scales</b> for <b>graphs</b> and for {{instruments}} that display in a two-dimensional form with a graticule, such as oscilloscopes.|$|R
30|$|The two herbal {{formulations}} and Soframycin ointment {{were applied}} on wound once daily for 15  days {{starting from the}} first day of wounding. Wound contraction was measured for 15  days at interval of 2  days [15]. Contraction which mainly contributes for wound closure was studied by tracing the raw wound area on transparent paper every alternate day till wounds were completely covered with epithelium. These wound tracings were retraced on a millimeter <b>scale</b> <b>graph</b> paper, to determine the wound area. Wound contraction (WC) was calculated as a percentage change in the initial wound size i.e.|$|E
3000|$|In general, vertex-centric models {{express the}} graph {{processing}} job from a vertex perspective {{where they are}} executed iteratively for each vertex in the graph. The Pregel system (Malewicz et al. 2010), introduced by Google, has pioneered the domain of large <b>scale</b> <b>graph</b> processing systems using the Bulk Synchronous Parallel (BSP) programming model and by relying on a “think like a vertex” programming model. The introduction of Google’s Pregel has triggered {{a lot of interest}} in the domain of large-scale graph processing and inspired the development of several Pregel-based systems which have been attempting to exploit different optimization opportunities. For example, Apache Giraph [...]...|$|E
40|$|SnapVX is a {{high-performance}} Python solver for convex optimization problems defined on networks. For these problems, {{it provides a}} fast and scalable solution with guaran-teed global convergence. SnapVX combines the capabilities of two open source software packages: Snap. py and CVXPY. Snap. py is a large <b>scale</b> <b>graph</b> processing library, and CVXPY provides a general modeling framework for small-scale subproblems. SnapVX of-fers a customizable yet easy-to-use interface with “out-of-the-box ” functionality. Based on the Alternating Direction Method of Multipliers (ADMM), {{it is able to}} efficiently store, analyze, and solve large optimization problems from a variety of different appli-cations. Documentation, examples, and more {{can be found on the}} SnapVX website a...|$|E
40|$|We {{investigate}} {{the effect of}} <b>graph</b> <b>scale</b> on risky choices. By (de) compressing the scale, we manipulate the relative physical distance between options on a given attribute in a coordinate graphical context. In Experiment 1, the risky choice changes {{as a function of}} the <b>scale</b> in the <b>graph.</b> In Experiment 2, we show that the type of <b>graph</b> <b>scale</b> also affects decision times. In Experiment 3, we examine the <b>graph</b> <b>scale</b> effect by using real money among students who have taken statistics courses. Consequently, the scale effects still appear even when we control the variations in calculation ability and increase the gravity with which participants view the consequence of their decisions. This finding is inconsistent with descriptive invariance of preference. The theoretical implications and practical applications of the findings are discussed...|$|R
50|$|Starting with <b>scale</b> free <b>graphs</b> {{with low}} degree {{correlation}} and clustering coefficient, one can generate new graphs with much higher degree correlations and clustering coefficients by applying edge-dual transformation.|$|R
40|$|AbstractRandom <b>scaled</b> sector <b>graphs</b> were {{introduced}} as a generalization of random geometric graphs to model networks of sensors using optical communication. In the random <b>scaled</b> sector <b>graph</b> model vertices are placed uniformly at random into the [0, 1] 2 unit square. Each vertex i is assigned uniformly at random sector Si, of central angle αi, {{in a circle}} of radius ri (with vertex i as the origin). An arc is present from vertex i to any vertex j, if j falls in Si. In this work, we study the value of the chromatic number χ(Gn), directed clique number ω(Gn), and undirected clique number ω 2 ^(Gn) for random <b>scaled</b> sector <b>graphs</b> with n vertices, where each vertex spans a sector of α degrees with radius rn=lnnn. We prove that for values α π w. h. p., χ(Gn) and ω 2 ^(Gn) are Θ(lnn), being the same for random scaled sector and random geometric graphs, while ω(Gn) is Θ(lnnlnlnn) ...|$|R
40|$|Abstract—This paper {{presents}} an in-depth analysis of charac-terization for an irregular application – computing betweenness centrality (BC) – on multi-core architectures. BC algorithm {{is widely used}} in large <b>scale</b> <b>graph</b> analysis applications, which play an increasingly important role in high performance com-puting community. Through a joint study of architecture and application, we find that dynamically non-contiguous memory access, unstructured parallelism and low arithmetic intensity in BC program pose an obstacle to an efficient execution on parallel architectures. The experimental results report a comparison between Intel Clovertown and Sun Niagara 1 for running such irregular program. Finally, several implications on mulit-core architecture and programming are proposed. I...|$|E
40|$|Even if Pregel scales {{better than}} MapReduce in graph {{processing}} by reducing iteration's disk I/O, while offering an easy programming model using " think like vertex " approach, large <b>scale</b> <b>graph</b> processing is still challenging {{in the presence}} of high degree vertices: Communication and load imbalance among processing nodes can have disastrous effects on performance. In this paper, we introduce a scalable MapReduce graph partitioning approach for high degree vertices using a master/slave partitioning allowing to balance communication and computation among processing nodes during all the stages of graph processing. Cost analysis and performance tests of this partitioning are given to show the effectiveness and the scalability of this approach in large scale systems...|$|E
40|$|This inquiry-based math/science {{curriculum}} {{explores the}} conceptual tools which have enabled {{scientists and engineers}} to launch satellites like GLAST into space, and {{to make sense of}} the data received. In this curriculum, students will measure, <b>scale,</b> <b>graph</b> and problem solve, using examples derived from GLAST. They will compare quantities as orders of magnitude, become familiar with scientific notation, and develop a concrete understanding of exponents and logarithms; all skills needed to understand the very large and very small quantities characteristic of astronomical observations. The lessons instruct students in logarithms, preparing them for further physical and space science studies. Note: In 2008, GLAST was renamed Fermi, for the physicist Enrico Fermi. Educational levels: High school, Informal education...|$|E
40|$|Interested in a {{full time}} {{position}} in the fields machine learning and data mining. • Have research experience in both, academic and industrial research — includes 6 years of graduate work, and two summer internships at IBM T J Watson Research Center NY. During my internships, I worked on Watson, IBM’s famous question answering system. • Experience in problems like classification, regression, search/ranking, multitask learning, semisupervised learning, visualization, dimensionality reduction, multidimensional <b>scaling,</b> <b>graph</b> embedding, network localization etc. • Published papers at conferences like KDD, NIPS, AISTATS, CIKM, IJCAI. • Already defended my thesis and immediately available, though will officially graduate in Dec 2012...|$|R
50|$|The <b>scales</b> of a <b>graph</b> {{are often}} used to exaggerate or {{minimize}} differences.|$|R
40|$|We {{introduce}} a parametrization of loop momenta which {{allows us to}} perform one of the Feynman integrations in a very transparent way. This leads to expressions which can easily be related to terms resulting from time-ordered perturbation theory in the infinite momentum frame. To exemplify our method we consider some simple Feynman integrals. As another example we discuss the covariant expressions of Landshoff, Polkinghorne, and Short for the <b>scaling</b> <b>graph</b> and the electromagnetic form factor. We indicate how to substitute the Sudakov parametriza-tion in their work {{in order to simplify}} their discussion and to make comparisons with the work of Gunion, Brodsky, and Blankenbecle...|$|R
30|$|To sum up, {{according}} to experiments on large <b>scale</b> <b>graph</b> with different topology and size, we can notice that our approach perform very well on undirected graph under the IC model, which {{is illustrated in}} Fig. 1 and Table 2. This can be justified by score value of neighborhood computed from active node till its radius, this helps to identify the node with most direct and indirect relationship. This permits to quantify in an efficient manner the potential users to be targeted and that adding the constraint of a selection threshold value for each node score favorize node with a certain efficiency and that separate each consecutively selected node by {{a certain number of}} hops enables targeting nodes that have an influence on users from a distinct region of influence. This computation of neighborhood number of score value for each node makes our approach a little bit time consuming but still perform well on large <b>scale</b> <b>graph</b> and therefore adequate for the real-world application. And as another observation for results of our approach under the LT model for undirected and directed setting, the one can observe that our approach gives lower results in term of influence spread and as discussed before that separating the selection of seed set by a certain hops reduces the influence of LT model, since this model needs a number of users to be activated in order that it can activate new nodes in its immediate neighborhood, this could be improved further by controlling the multi-hops distance in which we separate the selected nodes. However, the approach still doing well in term of time complexity and improved significantly compared to our previous approaches [9, 21].|$|E
40|$|Efficient {{indexing}} {{techniques have}} been developed for the exact and approximate substructure search in large <b>scale</b> <b>graph</b> databases. Unfortunately, the retrieval problem of structures with categorical or geometric distance constraints is not solved yet. In this paper, we develop a method called PIS (Partition-based Graph Index and Search) to support similarity search on substructures with superimposed distance constraints. PIS selects discriminative fragments in a query graph and uses an index to prune the graphs that violate the distance constraints. We identify a criterion to distinguish the selectivity of fragments in multiple graphs and develop a partition method to obtain a set of highly selective fragments, which is able to improve the pruning performance. Experimental results show that PIS is effective in processing real graph queries...|$|E
40|$|The {{ability to}} handle large <b>scale</b> <b>graph</b> data is crucial to an {{increasing}} number of applications. Much work has been dedicated to supporting basic graph operations such as subgraph matching, reachability, regular expression matching, etc. In many cases, graph indices are employed to speed up query processing. Typically, most indices require either super-linear indexing time or super-linear indexing space. Unfortunately, for very large graphs, super-linear approaches are almost always infeasible. In this paper, we study the problem of subgraph matching on billion-node graphs. We present a novel algorithm that supports efficient subgraph matching for graphs deployed on a distributed memory store. Instead of relying on super-linear indices, we use efficient graph exploration and massive parallel computing for query processing. Our experimental results demonstrate the feasibility of performing subgraph matching on web-scale graph data. 1...|$|E
40|$|The st-connectivity problem (ST-CON) is a {{decision}} problem that asks, for vertices ss and tt in a graph, if tt is reachable from ss. Although originally defined for directed graphs, {{it can also be}} studied on undirected graphs and used as a building block for solving more complex tasks on large <b>scale</b> <b>graphs.</b> We present solutions to ST-CON based on a high performance Breadth First Search (BFS) executed on clusters of Graphics Processing Units (GPUs) using the Nvidia CUDA platform. To measure performances, we use the number of ST-CONs per second. We present the results for two different implementations that highlight the impact of atomic operations in CUDA...|$|R
40|$|There {{is growing}} {{interest}} in studying large <b>scale</b> <b>graphs</b> having millions of vertices and billions of edges, {{up to the point}} that a specific benchmark, called Graph 500, has been defined to measure the performance of graph algorithms on modern computing architectures. At first glance, Graphics Processing Units (GPUs) are not an ideal platform for the execution of graph algorithms that are characterized by low arithmetic intensity and irregular memory access patterns. For studying really large graphs, multiple GPUs are required to overcome the memory size limitations of a single GPU. In the present paper, we propose several techniques to minimize the communication among GPUs...|$|R
50|$|It is also {{considered}} as <b>scaled</b> and rotated <b>graph</b> of the hyperbolic cosine function.|$|R
