1|8|Public
40|$|The {{relationship}} between trees, grass and soil {{in a dry}} savanna in Mali was investigated, to identify variables that are most relevant to assess vegetation units. A 65 ha plateau was inventoried using a <b>systematic</b> <b>square</b> grid sampling pattern. Thirteen soil or topography variables, and tree and grass characteristics were measured at each sampling point. Multivariate {{analysis was used to}} separately analyse soil, tree and grass data, and to characterize tree–grass and tree–soil relationships. Four units of soils, four units of tree formations, and four units of grass formations were identified. There was a correspondence between these groups, indicative of four vegetation units: thicket, bare land, shrub savanna and tree savanna. Soil depth and soil texture were the soil variables that best related to tree vegetation. A negative correlation was found between tree basal area and grass dry biomass. Finally, vegetation units, as identified from tree species composition, had contrasted diameter structures and densities...|$|E
40|$|This paper {{describes}} {{the results of}} superimposing certain types of 5 x 5 latin squares on a wheat uniformity trial. The purpose {{was to investigate the}} bias in the estimate of error when certain <b>systematic</b> <b>squares</b> (knight's move and diagonal) are chosen and to compare the results under conditions in Brazil with those obtained by Tedin in a similar investigation. The following conclusions are drawn : a) when latin squares are chosen by a random process, as recommended by Fisher, the observed distribution of the variance ratio of "treatments" compared with error is i n agreement with that given by theory; b) the estimation of error variance is biased when <b>systematic</b> <b>squares</b> are employed. In agreement with Tedin, we find that the knight's move square furnishes an overestimate, and the diagonal square an underestimate of error variance. The results show that, under the conditions of our trial, the <b>systematic</b> <b>squares</b> suffer from the same disadvantages which have been noted elsewhere...|$|R
40|$|This thesis {{examines}} the temporal characteristics and climatology of wintertime carbon monoxide (CO) concentrations in central Phoenix (AZ). Variations in CO concentrations on several temporal scales are explored {{to determine the}} relationship between atmospheric conditions, CO concentrations, and motor vehicle traffic levels {{as well as to}} spotlight the timing of high CO concentrations. Typical surface atmospheric conditions that are related to high and low CO mornings are revealed through compositing. Finally, a set of CO forecasting models are developed that predict 3 AM 8 hour average CO concentrations. Important results are as follows. The first three weeks of December have the most CO exceedance days. High CO concentrations are associated with anticyclonic conditions while low CO concentrations are linked to the passage of a cold front or trough. Final models had little <b>systematic</b> mean <b>squared</b> error and predictions that were within 1 ppm of the observed CO concentration...|$|R
40|$|Purpose. To explore Monte Carlo {{simulation}} for {{interpretation of}} small area variations in surgical rates. Methods. Simulation {{was used to}} generate sets of surgical rates under the null hypothesis of equal rates. The distributions of the extremal quotient, coefficient of variation, chi <b>square,</b> <b>systematic</b> component of variation, and case count were described. The null hypothesis was modified to allow reasonable variability. Results. The chi-square, CV, and CC had interpretable values and adequate power over the range of parameters tested. The EQ and the SCV did not. Only {{two of the five}} operations studied had greater variability than expected under a modified null hypothesis, compared with five out of five under traditional testing. Conclusions. Simulation can estimate the distributions of nonstandard statistics. The new defined case count adds valuable policy-relevant information. Modification of the null hypothesis improves decisions about which variations need further investigation...|$|R
40|$|The {{total number}} of myelinated fibres in four unifascicular tibial nerves from {{diabetic}} rats has been counted and measured {{in order to assess}} the merits of various schemes for estimating group mean fibre diameter. The average nerve trunk contained some 2960 fibres which were measured in just under five hours. With different sampling designs, the average measurement time per nerve was reduced to between 17 and 69 minutes, with little consequent loss of reliability or precision of estimated mean fibre size. The most efficient schemes were those taking <b>systematic</b> samples of <b>squares</b> or sectors. A modification of a method relying on complete strips across two diameters of each nerve was the least efficient sampling approach. It had the additional disadvantage of introducing systematic errors which could affect the accuracy of measurements made on nerve trunks with heterogeneous spatial distributions of fibre size and number. This paper completes an investigation into random sampling errors influencing morphometric estimates of fibre size based on uni- or multifascicular nerve trunks...|$|R
50|$|In {{systematic}} and grid sampling, samples are taken at regularly spaced intervals over space or time. An initial location or time is chosen at random, {{and then the}} remaining sampling locations are defined so that all locations are at regular intervals over an area (grid) or time (systematic). Examples Systematic Grid Sampling - <b>Square</b> Grid <b>Systematic</b> Grid Sampling - Triangular Grids of <b>systematic</b> grids include <b>square,</b> rectangular, triangular, or radial grids.Cressie, 1993. In random systematic sampling, an initial sampling location (or time) is chosen at random and the remaining sampling sites are specified {{so that they are}} located according to a regular pattern. Random systematic sampling is used to search for hot spots and to infer means, percentiles, or other parameters and is also useful for estimating spatial patterns or trends over time. This design provides a practical and easy method for designating sample locations and ensures uniform coverage of a site, unit, or process.|$|R
40|$|The {{definitive}} editor {{version is}} available at: [URL] [URL] crop coefficient equations were derived {{as a function}} of fraction of thermal units from lysimeter measured corn evapotranspiration (ETc-lys) during 1997 and 1998, and reference evapotranspiration obtained from: a) lysimeter measurements (Kcmes) or FAO Penman-Monteith (ETo-PM) estimates (Kcest-PM). For validation, corn evapotranspiration (ETc-est) was estimated in 2005 and 2006 from ETo-PM and: a) the equation for Kcmes with (ETc-est-lyslc) or without (ETc-est-lys) locally calibrated ETo-PM; b) the equation for Kcest-PM; and c) the FAO approach (ETc-est-FAO). The ETc-est_lys estimates showed the lowest bias (0. 09 mm day- 1); the ETc-est-PM and ETc-est-FAO, the highest (0. 50 - 0. 51 mm day- 1). However, the root mean square error (RMSE, 1. 23 - 1. 27 mm day- 1) and the index of agreement (IA, around 0. 94) of the ETc-est-lys, ETc-est-lyslc and ETc-est-PM were similar. Therefore, ETc-est-lys is recommended although the ETc-est-lyslc was almost as accurate. The ETc-est-PM is less recommended due to poorer bias and <b>systematic</b> mean <b>square</b> error, and a general underestimation except for low corn ET values. For real time irrigation scheduling, the ETc-est-FAO should be avoided as RMSE (1. 35 mm day- 1), IA (0. 93) and bias were slightly worse, corn ET was overestimated but for high values, and the length of the four phenological stages must be known in advance. This research has been funded through projects HID 96 - 1295 -C 04 - 04 and AGL- 2004 - 06675 -CO 3 - 02 (Spanish Ministry of Education), and PIP 090 / 2005 (Regional Government of Aragón). Peer reviewe...|$|R
40|$|The {{academic}} literature is showing a {{growing interest in}} such trading rules as Moving Average. The majority of researches were made using simple moving average. Although semi-professional traders use the technical analysis methods {{to predict the future}} stock prices, to identify the stock trend changes, OMX Baltic Benchmark Index was never tested. Previous researches on the S&P 500 Index using the most widely used method of technical analysis – Moving Averages are more or less appellative. Technical analysis is opponent to classical economic theory but investors use it widely all over the world. Technical Analysis methods can be less or more effective than it was thought until nowadays. This paper compares 2 trading rules of technical analysis – exponential smoothing method and simple moving average rule. Both methods were applied to US index S&P 500 and OMX Baltic Benchmark Index and the results were compared using <b>systematic</b> error (mean <b>square</b> error, the mean absolute deviation, mean forecast error, the mean absolute percentage error) and tracking signal evaluation, bias distribution estimation and appropriate Constanta level finding for each market forecast: the case of Standard and Poor's 500 and OMX Baltic Benchmark Index...|$|R
40|$|The {{financial}} crisis of 2008 – 2009 caused lots of discussions between Academia {{and as a}} result researches on {{financial crisis}} and bubble prediction possibilities appeared. Academia shows its growing interest in the issue during the last decade. The majority of researches made are based on different forms of forecast used. Some of previous studies claim that the trend of the stock market can be forecasted using moving average method. After the finance market crashed, a need to forecast further possible bubbles arises. As the economics of the Baltic States is very sensitive to such bubbles {{it is very important to}} forecast preliminary the trends of the finance markets ant to plan the right actions in order to temper such bubble influence on the national economics. Although economic theory is opposite to the technical analysis theory which is the main tool for traders in stock markets it is used widely. This paper examines whether a proper technical analysis rule such as Exponential Moving Average (EMA) has a predictive power on stock markets in the Baltic States. The method is applied to OMX Baltic Benchmark Index and industrial indexes as they are more or less sensitive to the main index fluctuations. The results were compared using <b>systematic</b> error (mean <b>square</b> error, the mean absolute deviation, mean forecast error, the mean absolute percentage error) and tracking signal evaluation, CAPM method and appropriate period of EMA finding for each market forecast. A graphical analysis was used in order to determine whether EMA can forecast the main trends of the stock market fluctuations. The conclusions made during the research suggest new research issues and new hypotheses for its further testing...|$|R

