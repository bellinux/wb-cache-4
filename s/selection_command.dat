12|121|Public
50|$|There {{are several}} {{standard}} broadcast commands, {{as well as}} commands used to address a particular device. The master can send a <b>selection</b> <b>command,</b> then the address of a particular device. The next command is executed only by the addressed device.|$|E
5000|$|The term {{command is}} ambiguous. For example, move up, move up may {{refer to a}} single (move up) command that should be {{executed}} twice, or it may refer to two commands, each of which happens {{to do the same}} thing (move up). If the former command is added twice to an undo stack, both items on the stack refer to the same command instance. This may be appropriate when a command can always be undone the same way (e.g. move down). Both the Gang of Four and the Java example below use this interpretation of the term command. On the other hand, if the latter commands are added to an undo stack, the stack refers to two separate objects. This may be appropriate when each object on the stack must contain information that allows the command to be undone. For example, to undo a delete <b>selection</b> <b>command,</b> the object may contain a copy of the deleted text so that it can be re-inserted, if the delete <b>selection</b> <b>command</b> must be undone. Note that using a separate object for each invocation of a command is also an example of the chain of responsibility pattern.|$|E
3000|$|FDD: since uplink and {{downlink}} channels {{operate at}} different frequency bands, feedback mechanisms are required. First of all, relays {{belonging to the}} decoding subset send a signalling message to the destination (i.e., BS) indicating {{that they are able}} to relay the message. This signalling message can be, for instance, a pilot sequence used by the BS to estimate the instantaneous SNRs of the different relays. Once the different SNRs are estimated, the BS selects the relay with the best quality and broadcasts this decision via a <b>selection</b> <b>command</b> (only [...]...|$|E
50|$|Sirius {{supports}} several predefined atom-residue {{sets and}} color schemes, allows editing of scripts using the Command Panel interface, and logical operators and parentheses {{can be used}} to create complex <b>selection</b> <b>commands.</b>|$|R
40|$|Toolglass [Bier et al. 1993] {{demonstrated}} a two-handed <b>command</b> <b>selection</b> technique that combined <b>command</b> <b>selection</b> and direct manipulation. While empirical evaluations showed a speed advantage for ToolGlass, {{they did not}} examine {{the relative importance of}} two possible factors in its improved performance: (1) the use of two hands and (2) the merging of <b>command</b> <b>selection</b> and direct manipulation. We conducted a study comparing the relative benefits of three <b>command</b> <b>selection</b> techniques that merge <b>command</b> <b>selection</b> and direct manipulation: one two-handed technique, Toolglass, and two one-handed techniques, namely, control menus [Pook et al. 2000] and FlowMenu [Guimbretière and Winograd 2000]. Participants performed sequences of operations that required both selecting a color and designating the endpoints of a line. Our results show that control menus and FlowMenu are significantly faster than Toolglass. Further analysis suggests that the merging of <b>command</b> <b>selection</b> and direct manipulation is {{the most important factor in}} the performance of all three techniques...|$|R
40|$|We {{explore the}} {{influence}} of handedness and the way <b>command</b> <b>selection</b> is integrated with direct manipulation on the speed of <b>command</b> <b>selection.</b> Two empirical studies provide converging evidence that it is the merging of <b>command</b> <b>selection</b> and direct manipulation that benefits performance most, and this is especially effective in a single-handed technique. These findings provide empirical evidence of the importance of merging <b>command</b> <b>selection</b> and direct manipulation in menu design, a factor not often taken into consideration in the past. The findings also yield new insight into the relative importance of handedness and merging in the Toolglass technique...|$|R
40|$|In {{this article}} we present the gesture frame, a system based on quasi-electrostatic field sensing. The system captures the arm {{gestures}} of the user and translates pointing gestures into screen coordinates and <b>selection</b> <b>command,</b> which become a gesture-based, hands-free interface for browsing and searching multimedia archives of an information kiosk in public spaces. The interface is intuitive and body-centered, and the playful interaction allows visitors to experience a new and magical means to communicate with computers. The system can be placed on or behind any surface {{and can be used}} as a user interface in conjunction with any display device...|$|E
40|$|We {{designed}} the Eyebrow-Clicker, a camera-based {{human computer interface}} system that implements {{a new form of}} binary switch. When the user raises his or her eyebrows, the binary switch is activated and a <b>selection</b> <b>command</b> is issued. The Eyebrow-Clicker thus replaces the "click" functionality of a mouse. The system initializes itself by detecting the user's eyes and eyebrows, tracks these features at frame rate, and recovers in the event of errors. The initialization uses the natural blinking of the human eye to select suitable templates for tracking. Once execution has begun, a user therefore never has to restart the program or even touch the computer. In our experiments with human-computer interaction software, the system successfully determined 93 % of the time when a user raised his eyebrows. Office of Naval Research; National Science Foundation (IIS- 0093367...|$|E
40|$|We {{present a}} {{quantitative}} analysis of delimiters for pen gestures. A delimiter is “something different ” in the input stream that a computer {{can use to}} determine the structure of input phrases. We study four techniques for delimiting a selection-action gesture phrase consisting of lasso selection plus marking-menu-based command activation. Pigtail is a new technique that uses a small loop to delimit lasso selection from marking (Fig. 1). Handle adds a box {{to the end of}} the lasso, from which the user makes a second stroke for marking. Timeout uses dwelling with the pen to delimit the lasso from the mark. Button uses a button press to signal when to delimit the gesture. We describe the role of delimiters in our Scriboli pen interaction testbed, and show how Pigtail supports scope <b>selection,</b> <b>command</b> activation, and direct manipulation all in a single fluid pen gesture...|$|E
40|$|We {{present the}} results of a study {{comparing}} the relative benefits of three <b>command</b> <b>selection</b> techniques that merge <b>command</b> <b>selection</b> and direct manipulation: one twohanded technique, Toolglass [2], and two one-handed techniques, control menus [15] and FlowMenu [6]. Our results show that control menus and FlowMenu are significantly faster than Toolglass. Further analysis suggests that merging <b>command</b> <b>selection</b> and direct manipulation is the key factor in the performance of all three techniques...|$|R
50|$|Whereas {{the file}} menu {{commonly}} contains commands about handling of files, such as open, save, and print, the edit menu commonly contains commands {{relating to the}} handling of information within a file, e.g. cut and paste and <b>selection</b> <b>commands.</b> In addition, {{it may also be}} home to the undo and redo commands, especially in word processors. It may also contain commands for locating information, e.g. find commands. In graphics-oriented programs, it often contains commands relating to the manipulation of images, for example the crop command.|$|R
40|$|This paper {{describes}} {{a model for}} a complex human-machine system where a human operator controls a remote robot through the mediation of a distributed virtual environment with a language interface. The system combines speech controlled graphical immersive environments with the live video from a robot working in a real environment. The worlds are synchronized and updated based on operator <b>selections,</b> <b>commands</b> and robot actions. This system allows the user to have a powerful tool {{with a high level of}} abstraction to create and control autonomous robots, thus making possible the realization of single and multiple autonomous robot applications. ...|$|R
30|$|If a {{transponder}} gets in range, {{the reader}} is able to establish the communication by sending the request (e.g., REQA) and an anti-collision command (needed for multi-transponder communications). The transponder answers to this anti-collision command with its unique identification number (UID). This procedure {{is followed by a}} <b>selection</b> <b>command,</b> which elevates the transponder to the ready state. During this card detection phase, data are already exchanged between the reader and the transponder. If this phase is modified to determine the PTF, the reader would be able to scale the magnetic field strength accordingly before the communication process begins (e.g., reading a digital business card). The challenge of this approach is getting the information needed for the determination during this phase (a detailed description is given in Section 4) [1]. One issue, which is not covered by this approach, is the changing physical relation factor during communication. An initial magnetic field scale can lead to an oversupply or to an undersupply of the transponder during the communication process.|$|E
40|$|Two video-based human-computer {{interaction}} tools are introduced that can activate a binary switch and issue a <b>selection</b> <b>command.</b> “BlinkLink,” {{as the first}} tool is called, automatically detects a user’s eye blinks and accurately measures their durations. The system is intended to provide an alternate input modality to allow people with severe disabilities to access a computer. Voluntary long blinks trigger mouse clicks, while involuntary short blinks are ignored. The system enables communication using “blink patterns:” sequences of long and short blinks which are interpreted as semiotic messages. The second tool, “EyebrowClicker, ” automatically detects when a user raises his or her eyebrows and then triggers a mouse click. Both systems can initialize themselves, track the eyes at frame rate, and recover {{in the event of}} errors. No special lighting is required. The systems have been tested with interactive games and a spelling program. Results demonstrate overall detection accuracy of 95. 6 % for BlinkLink and 89. 0 % for EyebrowClicker...|$|E
40|$|This paper {{presents}} a FSM based Programmable Built-In Self-Test (BIST) approach for testing memory modules in SOC(system on chip) The BIST can be flexible and selectable on-line by <b>selection</b> <b>command</b> so a test algorithm from predetermined set of algorithms that are {{built in the}} memory BIST,can be used Thus, the proposed scheme supports the various memory test algorithms to test different types of memory modules in SOC [...] In general, {{there are a variety}} of heterogeneous memory modules in SOC, and it is not possible to test all of them with a single algorithm. Thus, the proposed scheme greatly simplifies the testing process. Besides, the proposed is more efficient in terms of circuit size and test data to be applied, and it requires less time to configure the BIST. We also develop a programmable memory BIST generator that automatically produces RTL model of the proposed BIST architecture for a given set of test algorithms...|$|E
5000|$|... #Caption: The Windows 2000 Recovery Console <b>selection,</b> login, and <b>command</b> prompts ...|$|R
5000|$|Side Two of this LP {{consists}} of musical <b>selections</b> from the <b>Command</b> catalogue: ...|$|R
5000|$|A wide <b>selection</b> of editing <b>commands,</b> {{including}} indenting and unindenting, paragraph reformatting {{and line}} joining.|$|R
40|$|In {{order to}} {{evaluate}} the capability of ETM+ remotely- sensed data to provide "Forest- shrub land- Rangeland" cover type map in areas near the timberline of northern forests of Iran, the data was analyzed in a portion of nearly 790 ha located in Neka- Zalemroud region. First, ortho-rectification process was implemented to correct the geometric errors of the image, which yielded 0 / 68 and 0 / 69 pixels of RMS error toward X and Y axis, respectively. The original multi-spectral bands were fused to the panchromatic band using PANSHARP Statistical module. The ground truth map was prepared using 1 ha field plots in a systematic- random sampling grid. Vegetative form of trees, shrubs and rangelands was recorded as a criterion to allocate the plots. A set of channels including original bands, NDVI and IR/R indices, and first components of PCA was used for classification procedure. Automatic band <b>selection</b> <b>command</b> was used to select the appropriate channel set [...] Classification was carried out using ML classifier on both original and fused data sets. It showed 67 % of overall accuracy and 0 / 43 of Kappa coefficient in original data set. Due to the results present presented above, it's concluded that ETM+ data has an intermediate capability to fulfill the spectral variations of 3 form- based classes, in the studied area. Furthermore, applying complementary methods to minimize the background spectral effect is proposed for future studies...|$|E
40|$|Comprising of a {{potentially}} large team of autonomous cooperative robots locally interacting and communicating with each other, robot swarms provide a natural diversity of parallel and distributed functionalities, high flexibility, potential for redundancy, and fault-tolerance. The use of autonomous mobile robots {{is expected to}} increase in the future and swarm robotic systems are envisioned to play important roles in tasks such as: search and rescue (SAR) missions, transportation of objects, surveillance, and reconnaissance operations. To robustly deploy robot swarms on the field with humans, this research addresses the fundamental problems in the relatively new field of human-swarm interaction (HSI). Four groups of core classes of problems have been addressed for proximal interaction between humans and robot swarms: interaction and communication; swarm-level sensing and classification; swarm coordination; swarm-level learning. The primary contribution of this research aims to develop a bidirectional human-swarm communication system for non-verbal interaction between humans and heterogeneous robot swarms. The guiding field of application are SAR missions. The core challenges and issues in HSI include: How can human operators interact and communicate with robot swarms? Which interaction modalities can be used by humans? How can human operators instruct and command robots from a swarm? Which mechanisms can be used by robot swarms to convey feedback to human operators? Which type of feedback can swarms convey to humans? In this research, to start answering these questions, hand gestures have been chosen as the interaction modality for humans, since gestures are simple to use, easily recognized, and possess spatial-addressing properties. To facilitate bidirectional interaction and communication, a dialogue-based interaction system is introduced which consists of: (i) a grammar-based gesture language with a vocabulary of non-verbal commands that allows humans to efficiently provide mission instructions to swarms, and (ii) a swarm coordinated multi-modal feedback language that enables robot swarms to robustly convey swarm-level decisions, status, and intentions to humans using multiple individual and group modalities. The gesture language allows humans to: select and address single and multiple robots from a swarm, provide commands to perform tasks, specify spatial directions and application-specific parameters, and build iconic grammar-based sentences by combining individual gesture commands. Swarms convey different types of multi-modal feedback to humans using on-board lights, sounds, and locally coordinated robot movements. The swarm-to-human feedback: conveys to humans the swarm's understanding of the recognized commands, allows swarms to assess their decisions (i. e., to correct mistakes: made by humans in providing instructions, and errors made by swarms in recognizing commands), and guides humans through the interaction process. The second contribution of this research addresses swarm-level sensing and classification: How can robot swarms collectively sense and recognize hand gestures given as visual signals by humans? Distributed sensing, cooperative recognition, and decision-making mechanisms have been developed to allow robot swarms to collectively recognize visual instructions and commands given by humans in the form of gestures. These mechanisms rely on decentralized data fusion strategies and multi-hop messaging passing algorithms to robustly build swarm-level consensus decisions. Measures have been introduced in the cooperative recognition protocol which provide a trade-off between the accuracy of swarm-level consensus decisions and the time taken to build swarm decisions. The third contribution of this research addresses swarm-level cooperation: How can humans select spatially distributed robots from a swarm and the robots understand that they have been selected? How can robot swarms be spatially deployed for proximal interaction with humans? With the introduction of spatially-addressed instructions (pointing gestures) humans can robustly address and select spatially- situated individuals and groups of robots from a swarm. A cascaded classification scheme is adopted in which, first the robot swarm identifies the <b>selection</b> <b>command</b> (e. g., individual or group selection), and then the robots coordinate with each other to identify if they have been selected. To obtain better views of gestures issued by humans, distributed mobility strategies have been introduced for the coordinated deployment of heterogeneous robot swarms (i. e., ground and flying robots) and to reshape the spatial distribution of swarms. The fourth contribution of this research addresses the notion of collective learning in robot swarms. The questions that are answered include: How can robot swarms learn about the hand gestures given by human operators? How can humans be included in the loop of swarm learning? How can robot swarms cooperatively learn as a team? Online incremental learning algorithms have been developed which allow robot swarms to learn individual gestures and grammar-based gesture sentences supervised by human instructors in real-time. Humans provide different types of feedback (i. e., full or partial feedback) to swarms for improving swarm-level learning. To speed up the learning rate of robot swarms, cooperative learning strategies have been introduced which enable individual robots in a swarm to intelligently select locally sensed information and share (exchange) selected information with other robots in the swarm. The final contribution is a systemic one, it aims on building a complete HSI system towards potential use in real-world applications, by integrating the algorithms, techniques, mechanisms, and strategies discussed in the contributions above. The effectiveness of the global HSI system is demonstrated {{in the context of a}} number of interactive scenarios using emulation tests (i. e., performing simulations using gesture images acquired by a heterogeneous robotic swarm) and by performing experiments with real robots using both ground and flying robots...|$|E
5000|$|Selecting 4-bit or 8-bit mode {{requires}} careful <b>selection</b> of <b>commands.</b> There aretwo primary considerations. First, with D3-D0 unconnected, {{these lines}} willalways appear low (0b0000) to the HD44780 {{when it is}} in 8-bit mode. Second, theLCD may initially be {{in one of three}} states: ...|$|R
40|$|International audienceSelecting {{commands}} is {{ubiquitous in}} current GUIs. While {{a number of}} studies have focused on improving rapid <b>command</b> <b>selection</b> through novel interaction techniques, new interface design and innovative devices, user performance in this context has received little attention. Inspired by a recent study which formulated information-theoretic hypotheses to support experimental results on <b>command</b> <b>selection,</b> we aim at explaining user performance from an information-theoretic perspective. We design an ad-hoc <b>command</b> <b>selection</b> experiment for information-theoretic analysis, and explain theoretically why the transmitted information from the user to the computer levels off as difficulty increases. Our reasoning is based on basic information-theoretic concepts such as entropy, mutual information and Fano’s inequality. This implies a bell-shaped behavior of the throughput and therefore an optimal level of difficulty for a given input technique...|$|R
40|$|International audienceThis paper {{presents}} Leaf menu, a {{new type}} of contextual linear menu that supports curved gesture shortcuts. By providing an alternative to keyboard shortcuts, the Leaf menus can be used for the <b>selection</b> of <b>commands</b> on tabletops, but its key benefit is its adequacy to small handheld touchscreen devices (PDA, Smartphone). Indeed Leaf menus define a compact and known layout inherited from linear menus, they support precise finger interaction, they manage occlusion and they can be used in close proximity to the screen borders. Moreover, by providing stroke shortcuts, they favour the <b>selection</b> of frequent <b>commands</b> in expert mode and make eye-free selection possible...|$|R
40|$|Photograph {{taken for}} {{a story in the}} Oklahoma Times newspaper. Caption: "Navy ROTC midshipmen also learned their student "old man" will be Midshipman 1 -c William S. Clark, a Falls Church, Va., senior Clark's <b>selection</b> to <b>command</b> the navy future {{officers}} was announced by Capt. Alfred F. Gerken, naval science professor. ...|$|R
40|$|Our {{ability to}} develop robust {{multimodal}} systems will depend on knowledge of the natural integration patterns that typify people's combined use of different input modes. To provide a foundation for theory and design, the present research analyzed multimodal interaction while people spoke and wrote to a simulated dynamic map system. Task analysis revealed that multimodal interaction occurred most frequently during spatial location commands, and with intermediate frequency during <b>selection</b> <b>commands.</b> In addition, microanalysis of input signals identified sequential, simultaneous, point-and-speak, and compound integration patterns, as well as data on the temporal precedence of modes and on inter-modal lags. In synchronizing input streams, the temporal precedence of writing over speech was a major theme, with pen input conveying location information first in a sentence. Linguistic analysis also revealed that the spoken and written modes consistently supplied complementary semantic informati [...] ...|$|R
40|$|This paper {{describes}} a distributed virtual environment application that combines robotics in both virtual and real worlds. The motivation for this system comes from research in {{virtual and augmented}} reality, autonomous robotics and computer vision. The system combines graphical immersive environments with the live video from a robot working in a real environment. The worlds are synchronized and updated based on both the operator <b>selections,</b> <b>commands</b> and robot actions. This system allows the user to have a powerful tool to create and control autonomous robots, thus making possible the realization of single and multiple autonomous robot applications. Keywords: virtual environments, distributed synthetic worlds, immersive environments, augmented reality, autonomous robotics, computer vision, teleoperation, handicap assistance. 1 Introduction In this paper we describe our current work with an application of distributed virtual worlds that combines robotics in both virtual and real worlds [...] ...|$|R
50|$|A U.S. Army and U.S. Marine captain {{generally}} commands company-sized units. When {{given such}} a command, they bear the title company commander. Captains also instruct at service schools and combat training centers {{and are often}} staff officers at the battalion level. Marine captains also serve as officer <b>selection</b> officers, <b>commanding</b> recruiting stations for commissioned officers.|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedOfficer Master File data {{were used to}} reconstruct and analyze the career paths of a sample of 1, 084 year group 1958 - 1963 Surface Warfare Officers. Of particular concern were the developmental characteristics, apart from performance, which distinguished the careers of commander-command selectees from those of nonselectees. A wide variety of career development opportunities {{were found to have}} been provided to Surface Warfare Officers to enable them to qualify for <b>command</b> <b>selection,</b> and the absence of any absolute path to selection or nonselection was confirmed. There were, however, certain individual billet types, combination of billets and commissioning sources which, if experienced by Surface Warfare Officers, tended to increase or decrease their probability of later <b>command</b> <b>selection.</b> In this regard, the singular importance of the lieutenant commander executive officer tour to <b>command</b> <b>selection</b> was documented. [URL] Commander, United States Nav...|$|R
25|$|Twenty {{naval ships}} {{have been named}} after {{hospital}} corpsmen. Prior to <b>selection</b> to the <b>command</b> master chief program, the 11th MCPON, Joe R. Campa, was a hospital corpsman.|$|R
30|$|Chen, N.Y., F. Guimbretière, and C.E. Löckenhoff, Relative role of merging and two-handed {{operation}} on <b>command</b> <b>selection</b> speed. Int. J. Hum.-Comput. Stud., 2008. 66 (10): p. 729 – 740.|$|R
50|$|LaunchBar also {{provides}} capabilities beyond application launching, such as file management and piping the current <b>selection</b> to a <b>command</b> line utility, along with clipboard management and a built-in calculator.|$|R
2500|$|The <b>command</b> <b>selection</b> {{menu input}} system, where the player chooses from a menu list of {{commands}} either through keyboard shortcuts or scrolling down the menu, {{was introduced in}} 1983, and would largely replace the verb-noun parser input method over the years. The earliest known title to use the <b>command</b> <b>selection</b> menu system was the Japanese adventure game Spy 007 (スパイ00.7), published in April 1983, and it was followed soon after by several other Japanese adventure games in 1983. These included the eroge title Joshiryo Panic, authored by Tadashi Makimura and published by Enix for the FM-7 in June and slightly earlier for the FM-8; Atami Onsen Adventure (熱海温泉アドベンチャー), released by Basic System (ベーシックシステム) in July for the FM-7 and slightly earlier for the PC-8001; Planet Mephius, released in July; and Tri-Dantal (トリダンタル), authored by Y. Takeshita and published by Pax Softnica for the FM-7 in August. The game that popularized the <b>command</b> <b>selection</b> system was the 1984 adventure game [...] (Okhotsk ni Kiyu: Hokkaido Chain Murders), designed by Yuji Horii (his second mystery adventure game after Portopia) and published by ASCII for the PC-8801 and PC-9801. Its replacement of the traditional verb-noun text parser interface with the <b>command</b> <b>selection</b> menu system {{would lead to the}} latter becoming a staple of adventure games as well as role-playing games (through Horii's 1986 hit Dragon Quest in the latter case).|$|R
50|$|Messiah's Armature system {{provides}} a visual method for creating character and program control. Armatures {{can be used}} for anything from stick figure animation and facial animation controls, to character <b>selection</b> sheets and <b>command</b> interfaces.|$|R
50|$|The <b>command</b> <b>selection</b> {{menu input}} system, where the player chooses from a menu list of {{commands}} either through keyboard shortcuts or scrolling down the menu, {{was introduced in}} 1983, and would largely replace the verb-noun parser input method over the years. The earliest known title to use the <b>command</b> <b>selection</b> menu system was the Japanese adventure game Spy 007 (スパイ00.7), published in April 1983, and it was followed soon after by several other Japanese adventure games in 1983. These included the eroge title Joshiryo Panic, authored by Tadashi Makimura and published by Enix for the FM-7 in June and slightly earlier for the FM-8; Atami Onsen Adventure (熱海温泉アドベンチャー), released by Basic System (ベーシックシステム) in July for the FM-7 and slightly earlier for the PC-8001; Planet Mephius, released in July; and Tri-Dantal (トリダンタル), authored by Y. Takeshita and published by Pax Softnica for the FM-7 in August. The game that popularized the <b>command</b> <b>selection</b> system was the 1984 adventure game Okhotsk ni Kiyu: Hokkaido Rensa Satsujin Jiken (Okhotsk ni Kiyu: Hokkaido Chain Murders), designed by Yuji Horii (his second mystery adventure game after Portopia) and published by ASCII for the PC-8801 and PC-9801. Its replacement of the traditional verb-noun text parser interface with the <b>command</b> <b>selection</b> menu system {{would lead to the}} latter becoming a staple of adventure games as well as role-playing games (through Horii's 1986 hit Dragon Quest in the latter case).|$|R
40|$|The {{purpose of}} this thesis was to {{identify}} those statistically significant variables associated with promotion to lieutenant colonel and <b>selection</b> for <b>command</b> of a Marine Aviation Logistics Squadron (MALS) or Center for Naval Aviation Technical Training Marine Unit for Aviation Maintenance Officers (AMOs) and Aviation Supply Officers (AVNSUPOs). A data set was constructed for the 102 in-zone AMOs and AVNSUPOs competing for promotion, consisting of demographic and Fitness Report (FITREP) data for each officer covering Fiscal Years 2004 - 2012. Utilizing logistic regression, the findings concluded that serving as a MALS Executive Officer (XO), receiving a Meritorious Service Medal, and scoring above the Reviewing Officers' (RO) average scores improve one's probability for selection. Serving in combat {{was not a significant}} factor for promotion. Because information on <b>command</b> <b>selection</b> was not available from Marine Corps Officer Assignments Plans and Programs, {{it was not possible to}} model for <b>command</b> <b>selection.</b> Instead, the following descriptive statistics provide insight on the type of officer selected to command. Forty percent have served as Operations Officers. Forty-three percent served as XOs. Fifty-one percent of the officers scored above their ROs' average markings. Only 37 % have at least one combat FITREP as a major. US Marina Corps (USMC) autho...|$|R
5000|$|Guarded {{commands}} {{are suitable}} for quasi-delay-insensitive circuit design because the repetitionallows arbitrary relative delays for the <b>selection</b> of different <b>commands.</b> In this application,a logic gate driving a node y in the circuit consists of two guarded commands, as follows: ...|$|R
