8295|5163|Public
2500|$|A {{standard}} {{statistical procedure}} involves {{the test of}} the relationship between two statistical data sets, or a data set and <b>synthetic</b> <b>data</b> drawn from idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, and this is compared as an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is falsely rejected giving a [...] "false positive") and Type II errors (null hypothesis fails to be rejected and an actual difference between populations is missed giving a [...] "false negative"). Multiple problems have come to be associated with this framework: ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis.|$|E
2500|$|A {{statistical}} hypothesis, {{sometimes called}} confirmatory data analysis, is a hypothesis that is testable {{on the basis}} of observing a process that is modeled via a set of random variables. [...] A statistical hypothesis test is a method of statistical inference. Commonly, two statistical data sets are compared, or a data set obtained by sampling is compared against a <b>synthetic</b> <b>data</b> set from an idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, and this is compared as an alternative to an idealized null hypothesis that proposes no relationship between two data sets. The comparison is deemed statistically significant if the relationship between the data sets would be an unlikely realization of the null hypothesis according to a threshold probabilitythe significance level. Hypothesis tests are used in determining what outcomes of a study would lead to a rejection of the null hypothesis for a pre-specified level of significance. The process of distinguishing between the null hypothesis and the alternative hypothesis is aided by identifying two conceptual types of errors (type 1 & type 2), and by specifying parametric limits on e.g. how much type 1 error will be permitted.|$|E
50|$|<b>Synthetic</b> <b>data</b> {{are used}} in the process of data mining. Testing and {{training}} fraud detection systems, confidentiality systems and any type of system is devised using <b>synthetic</b> <b>data.</b> As described previously, <b>synthetic</b> <b>data</b> may seem as just a compilation of “made up” data, but there are specific algorithms and generators that are designed to create realistic data. This <b>synthetic</b> <b>data</b> assists in teaching a system how to react to certain situations or criteria. Researcher doing clinical trials or any other research may generate <b>synthetic</b> <b>data</b> to aid in creating a baseline for future studies and testing. For example, intrusion detection software is tested using <b>synthetic</b> <b>data.</b> This data is a representation of the authentic data and may include intrusion instances that are not found in the authentic data. The <b>synthetic</b> <b>data</b> allows the software to recognize these situations and react accordingly. If <b>synthetic</b> <b>data</b> was not used, the software would only be trained to react to the situations provided by the authentic data and it may not recognize another type of intrusion.|$|E
40|$|In {{order to}} {{generate}} <b>synthetic</b> basket <b>data</b> sets for better benchmark testing, {{it is important}} to integrate characteristics from real-life databases into the <b>synthetic</b> basket <b>data</b> sets. The characteristics {{that could be used for}} this purpose include the frequent itemsets and association rules. The problem of generating <b>synthetic</b> basket <b>data</b> sets from frequent itemsets is generally referred to as inverse frequent itemset mining. In this paper, we show that the problem of approximate inverse frequent itemset mining is NP-complete. Then we propose and analyze an approximate algorithm for approximate inverse frequent itemset mining, and discuss privacy issues related to the <b>synthetic</b> basket <b>data</b> set. In particular, we propose an approximate algorithm to determine the privacy leakage in a <b>synthetic</b> basket <b>data</b> set...|$|R
40|$|This report {{presents}} {{an approach to}} predict pancreatic cancer using Support Vector Machine Classification algorithm. The research objective of this project it to predict pancreatic cancer on just genomic, just clinical and combination of genomic and clinical data. We have used real genomic data having 22, 763 samples and 154 features per sample. We have also created <b>Synthetic</b> Clinical <b>data</b> having 400 samples and 7 features per sample in order to predict accuracy of just clinical data. To validate the hypothesis, we have combined <b>synthetic</b> clinical <b>data</b> with subset of features from real genomic data. In our results, we observed that prediction accuracy, precision, recall with just genomic data is 80. 77 %, 20 %, 4 %. Prediction accuracy, precision, recall with just <b>synthetic</b> clinical <b>data</b> is 93. 33 %, 95 %, 30 %. While prediction accuracy, precision, recall for combination of real genomic and <b>synthetic</b> clinical <b>data</b> is 90. 83 %, 10 %, 5 %. The combination of real genomic and <b>synthetic</b> clinical <b>data</b> decreased the accuracy since the genomic data is weakly correlated. Thus we conclude {{that the combination of}} genomic and clinical data does not improve pancreatic cancer prediction accuracy. A dataset with more significant genomic features might help to predict pancreatic cancer more accurately...|$|R
5000|$|The {{difference}} vector [...] is computed as {{the difference}} between experimental and <b>synthetic</b> seismic <b>data.</b>|$|R
50|$|<b>Synthetic</b> <b>data</b> is {{also used}} to protect the privacy and {{confidentiality}} of a set of data. Real data contains personal/private/confidential information that a programmer, software creator or research project may not want to be disclosed. <b>Synthetic</b> <b>data</b> holds no personal information and cannot be traced back to any individual; therefore, the use of <b>synthetic</b> <b>data</b> reduces confidentiality and privacy issues.|$|E
5000|$|<b>Synthetic</b> <b>data</b> are {{generated}} to meet specific needs or certain conditions {{that may not}} be found in the original, real data. This can be useful when designing any type of system because the <b>synthetic</b> <b>data</b> are used as a simulation or as a theoretical value, situation, etc. This allows us to take into account unexpected results and have a basic solution or remedy, if the results prove to be unsatisfactory. <b>Synthetic</b> <b>data</b> are often generated to represent the authentic data and allows a baseline to be set. Another use of <b>synthetic</b> <b>data</b> is to protect privacy and confidentiality of authentic data. As stated previously, <b>synthetic</b> <b>data</b> is used in testing and creating many different types of systems; below is a quote from the abstract of an article that describes a software that generates <b>synthetic</b> <b>data</b> for testing fraud detection systems that further explains its use and importance."This enables us to create realistic behavior profiles for users and attackers. The data is used to train the fraud detection system itself, thus creating the necessary adaptation of the system to a specific environment." ...|$|E
50|$|The {{creation}} of <b>synthetic</b> <b>data</b> is an involved process of data anonymization; {{that is to}} say that <b>synthetic</b> <b>data</b> is a subset of anonymized data. <b>Synthetic</b> <b>data</b> is used in a variety of fields as a filter for information that would otherwise compromise the confidentiality of particular aspects of the data. Many times the particular aspects come about in the form of human information (i.e. name, home address, IP address, telephone number, social security number, credit card number, etc.).|$|E
5000|$|A <b>synthetic</b> seismic <b>data</b> [...] is {{computed}} by {{the forward}} model, using the model impedance above.|$|R
40|$|Abstract. A {{perturbation}} {{model for}} the generation of synthetic textlines from existing cursively handwritten lines of text produced by human writers is presented. Our goal {{is to improve the}} performance of an off-line cursive handwriting recognition system by providing it with additional <b>synthetic</b> training <b>data.</b> It can be expected that by adding <b>synthetic</b> training <b>data</b> the variability of the training set improves, which leads to a higher recognition rate. On the other hand, <b>synthetic</b> training <b>data</b> may bias a recognizer towards unnatural handwriting styles, which could lead to a deterioration of the recognition rate. In this paper we will show that for certain configurations of the parameters of the recognizer and the synthetic handwriting generation process, the recognition performance can be significantly improved. 1...|$|R
30|$|Six {{different}} automatic PET delineation {{methods were}} evaluated using NEMA IQ phantom and three additional <b>synthetic</b> phantom <b>data</b> sets created in-house.|$|R
50|$|The {{history of}} the {{generation}} of <b>synthetic</b> <b>data</b> dates back to 1993. In 1993, the idea of original fully <b>synthetic</b> <b>data</b> was created by Rubin. Rubin originally designed this to synthesize the Decennial Census long form responses for the short form households. He then released samples that did not include any actual long form records - in this he preserved anonymity of the household. Later that year, the idea of original partially <b>synthetic</b> <b>data</b> was created by Little. Little used this idea to synthesize the sensitive values on the public use file.|$|E
50|$|Test Data Director (tDD) is an {{enterprise}} grade <b>synthetic</b> <b>data</b> generation tool that provides quality assurance engineers with production-grade <b>synthetic</b> <b>data</b> {{to test their}} products on. It currently supports like Oracle, IBM DB2 and Microsoft SQL Server. The product is in rapid development and will soon introduce support for many more databases and file formats.|$|E
50|$|In 1994, Fienberg came up {{with the}} idea of {{critical}} refinement, in which he used a parametric posterior predictive distribution (instead of a Bayes bootstrap) to do the sampling. Later, other important contributors to the development of <b>synthetic</b> <b>data</b> generation were Trivellore Raghunathan, Jerry Reiter, Donald Rubin, John M. Abowd, and Jim Woodcock. Collectively they {{came up with}} a solution for how to treat partially <b>synthetic</b> <b>data</b> with missing data. Similarly they {{came up with the}} technique of Sequential Regression Multivariate Imputation.|$|E
40|$|We propose {{using the}} forward {{propagated}} source wave to create <b>synthetic</b> receiver <b>data</b> on {{the surfaces of}} the computational domain where real receiver data is not available {{as a means of}} exploiting known information about reflector locations in Reverse Time Migration. The inclusion of <b>synthetic</b> boundary <b>data</b> can make true amplitude imaging possible, and reduce the artifacts associated with the inclusion of multiples. Here, we describe the new method, present synthetic examples, and propose an appropriate imaging condition...|$|R
50|$|Note: SEDRIS {{used to be}} {{an acronym}} for <b>Synthetic</b> Environment <b>Data</b> Representation and Interchange Specification but it is now used as a noun.|$|R
40|$|Elastic least-squares {{reverse time}} {{migration}} (LSRTM) {{is used to}} invert <b>synthetic</b> particle-velocity <b>data</b> and crosswell pressure field data. The migration images consist of both the P- and Svelocity perturbation images. Numerical tests on <b>synthetic</b> and field <b>data</b> illustrate the advantages of elastic LSRTM over elastic reverse time migration (RTM). In addition, elastic LSRTM images are better focused and have better reflector continuity than do the acoustic LSRTM images...|$|R
5000|$|Researchers {{test the}} {{framework}} on <b>synthetic</b> <b>data,</b> which is [...] "the {{only source of}} ground truth on which they can objectively assess {{the performance of their}} algorithms".|$|E
50|$|Surrogate or {{analogous}} {{data may}} refer to data used to supplement available data {{from which a}} mathematical model is built. Under this definition, it may be generated (i.e., <b>synthetic</b> <b>data)</b> or transformed from another source.|$|E
50|$|<b>Synthetic</b> <b>data</b> is {{the name}} given to tables and {{formulae}} derived from the analysis of accumulated work measurement data, arranged in a form suitable for building up standard times, machine process times, etc. by synthesis.|$|E
30|$|Monthly {{meteorological}} data {{available on the}} NASA Web site [18] {{have been used for}} generating hourly <b>synthetic</b> meteorological <b>data</b> (horizontal global irradiance and ambient temperature) with the aid of PVSYST software [26].|$|R
40|$|The Red Sea is geologically {{interesting}} due to {{its unique}} structures and abundant mineral and petroleum resources, yet no digital geologic models or <b>synthetic</b> seismic <b>data</b> of the Red Sea are publicly available for testing algorithms to image and analyze the area's interesting features. This study compiles a 2 D viscoelastic model of the Red Sea and calculates a corresponding multicomponent <b>synthetic</b> seismic <b>data</b> set. The models and data sets are made publicly available for download. We hope this effort will encourage interested researchers to test their processing algorithms on this data set and model and share their results publicly as well...|$|R
5000|$|... #Caption: Kernel density {{estimate}} with diagonal bandwidth for <b>synthetic</b> normal mixture <b>data.</b>|$|R
5000|$|... {{clear all}} [...] % {{generate}} <b>synthetic</b> <b>data</b> data=randn(500,1)+3.5, randn(500,1);; % call the routine, {{which has been}} saved in the current directory [...] bandwidth,density,X,Y=kde2d(data); % plot the data and the density estimate contour3(X,Y,density,50), hold on plot(data(:,1),data(:,2),'r.','MarkerSize',5) ...|$|E
5000|$|SEDRIS main sponsor is the DoD Modeling and Simulation Coordination Office (M&S CO), {{formerly}} the Defense Modeling and Simulation Office (DMSO). It {{is intended to}} enable the interchange of <b>synthetic</b> <b>data</b> including terrain, features and models.|$|E
50|$|Synthesis {{is a work}} {{measurement}} technique for building up {{the time for a}} job at a defined level of performance by totaling element times obtained previously from time studies on other jobs containing the elements concerned, or from <b>synthetic</b> <b>data.</b>|$|E
40|$|Usage-based {{statistical}} testing employs {{knowledge about}} the actual or anticipated usage profile of the system under test for estimating system reliability. For many systems, usage-based statistical testing involves generating <b>synthetic</b> test <b>data.</b> Such data must possess the same statistical characteristics as the actual data that the system will process during operation. <b>Synthetic</b> test <b>data</b> must further satisfy any logical validity constraints that the actual data is subject to. Targeting data-intensive systems, we propose an approach for generating <b>synthetic</b> test <b>data</b> that is both statistically representative and logically valid. The approach works by first generating a data sample that meets the desired statistical characteristics, without {{taking into account the}} logical constraints. Subsequently, the approach tweaks the generated sample to fix any logical constraint violations. The tweaking process is iterative and continuously guided toward achieving the desired statistical characteristics. We report on a realistic evaluation of the approach, where we generate a synthetic population of citizens' records for testing a public administration IT system. Results suggest that our approach is scalable and capable...|$|R
30|$|In Section 2, the {{numerical}} model, <b>synthetic</b> afterslip <b>data</b> {{and the data}} assimilation system are described. Numerical results are given in Section 3. Section 4 provides a summary and discusses potential challenges for practical application.|$|R
30|$|The second {{principle}} {{task was}} to perform an initial test of these methods on both real life plankton data of high contour line variability and a <b>synthetic</b> sample <b>data</b> with similar intra-class structure and symmetry.|$|R
5000|$|There {{are several}} ways of {{implementing}} GEK. The first method, indirect GEK, defines a small but finite stepsize , and uses the gradient information to append <b>synthetic</b> <b>data</b> to the observations , see for example. Indirect Kriging is sensitive to {{the choice of the}} step-size [...] and cannot include observation uncertainties.|$|E
50|$|The {{main purpose}} of Tanagra project is to give {{researchers}} and students a user-friendly data mining software, conforming to the present norms of the software development in this domain (especially {{in the design of}} its GUI and the way to use it), and allowing to analyze either real or <b>synthetic</b> <b>data.</b>|$|E
50|$|The {{design of}} a survey, {{geological}} model and data can be displayed in 3D. The geometry and parameters of model structures can be edited in 3D space. Measured and <b>synthetic</b> <b>data</b> can be viewed in different formats including vectors, lines, surfaces and contours {{in association with the}} models. Results from inversion tools can be displayed as a volume.|$|E
30|$|Using the {{computed}} velocities and {{densities of}} each {{layer of the}} model, <b>synthetic</b> seismic <b>data</b> were generated for both cases. From this seismic data, AI and stacking velocity data are calculated. Dix’s equation is used to compute the interval velocity.|$|R
50|$|An initial guess of {{the model}} {{impedance}} is provided to initiate the inversion process. The forward model uses this initial guess to compute a <b>synthetic</b> seismic <b>data</b> which is subtracted from the observed seismic data to calculate the difference vector.|$|R
40|$|Recent {{works have}} shown that <b>synthetic</b> {{parallel}} <b>data</b> automatically generated by translation models can be effective for various neural machine translation (NMT) issues. In this study, we build NMT systems using only <b>synthetic</b> parallel <b>data.</b> As an efficient alternative to real parallel data, we also present {{a new type of}} synthetic parallel corpus. The proposed pseudo parallel data are distinct from previous works in that ground truth and synthetic examples are mixed on both sides of sentence pairs. Experiments on Czech-German and French-German translations demonstrate the efficacy of the proposed pseudo parallel corpus, which shows not only enhanced results for bidirectional translation tasks but also substantial improvement {{with the aid of a}} ground truth real parallel corpus...|$|R
