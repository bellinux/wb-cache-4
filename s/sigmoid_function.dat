589|459|Public
25|$|Continuous network {{models of}} GRNs are an {{extension}} of the boolean networks described above. Nodes still represent genes and connections between them regulatory influences on gene expression. Genes in biological systems display a continuous range of activity levels and {{it has been argued that}} using a continuous representation captures several properties of gene regulatory networks not present in the Boolean model. Formally most of these approaches are similar to an artificial neural network, as inputs to a node are summed up and the result serves as input to a <b>sigmoid</b> <b>function,</b> e.g., but proteins do often control gene expression in a synergistic, i.e. non-linear, way. However, there is now a continuous network model that allows grouping of inputs to a node thus realizing another level of regulation. This model is formally closer to a higher order recurrent neural network. The same model has also been used to mimic the evolution of cellular differentiation and even multicellular morphogenesis.|$|E
2500|$|Mathematically, a neuron's network {{function}} [...] {{is defined as}} a composition of other functions , that can further be decomposed into other functions. This can be conveniently represented as a network structure, with arrows depicting the dependencies between functions. A widely used type of composition is the nonlinear weighted sum, where , where [...] is a function (commonly referred to as the activation function), such as the hyperbolic tangent or <b>sigmoid</b> <b>function</b> or softmax function or rectifier function. The important characteristic of the activation function is that it provides a smooth transition as input values change, i.e. a small change in input produces a small change in output. The following refers to a collection of functions [...] as a vector [...]|$|E
5000|$|In general, a <b>sigmoid</b> <b>function</b> is real-valued, monotonic, and {{differentiable}} {{having a}} non-negative first derivative which is bell shaped. A <b>sigmoid</b> <b>function</b> is constrained {{by a pair}} of horizontal asymptotes as [...]|$|E
40|$|Abstract — We study a {{class of}} non-convex {{optimization}} problems involving <b>sigmoid</b> <b>functions.</b> We show that <b>sigmoid</b> <b>functions</b> impart a combinatorial element to the optimization variables and make them hybrid of continuous and discrete variables. We formulate versions of the knapsack problem and the bin-packing problem with such hybrid variables. We utilize the approximation algorithms from the combinatorial optimization literature and develop approximation algorithms for these NP-hard hybrid optimization problems. I...|$|R
3000|$|... {{could be}} non-monotonic, {{and are more}} general than the usual <b>sigmoid</b> <b>functions</b> and the {{commonly}} used Lipschitz conditions recently.|$|R
40|$|We study a {{class of}} non-convex {{optimization}} problems involving <b>sigmoid</b> <b>functions.</b> We show that <b>sigmoid</b> <b>functions</b> impart a combinatorial element to the optimization variables and make the global optimization computationally hard. We formulate versions of the knapsack problem, the generalized assignment problem and the bin-packing problem with sigmoid utilities. We merge approximation algorithms from discrete optimization with algorithms from continuous optimization to develop approximation algorithms for these NP-hard problems with sigmoid utilities...|$|R
50|$|See also <b>sigmoid</b> <b>function.</b>|$|E
50|$|The <b>sigmoid</b> <b>function</b> {{limits the}} range of the {{normalized}} data to values between 0 and 1. The <b>sigmoid</b> <b>function</b> is almost linear near the mean and has smooth nonlinearity at both extremes, ensuring that all data points are within a limited range. This maintains the resolution of most values within a standard deviation of the mean.|$|E
50|$|The psychometric {{function}} is a <b>sigmoid</b> <b>function</b> characterised by being ‘s’ shaped in its graphical representation.|$|E
25|$|Babylonians modeled {{exponential}} growth, constrained growth (via {{a form of}} <b>sigmoid</b> <b>functions),</b> and doubling time, {{the latter}} {{in the context of}} interest on loans.|$|R
5000|$|... #Caption: Some <b>sigmoid</b> <b>{{functions}}</b> compared. In {{the drawing}} all functions are normalized {{in such a}} way that their slope at the origin is 1.|$|R
3000|$|... in Assumption  2 {{are allowed}} to be positive, {{negative}} or zero. So, the activation functions in this paper are less conservative than the usual <b>sigmoid</b> <b>functions.</b>|$|R
50|$|The type of {{function}} {{that can be}} used to describe the curve is called a <b>sigmoid</b> <b>function.</b>|$|E
5000|$|... #Caption: The <b>sigmoid</b> <b>function</b> {{remains a}} better {{representation}} than the linear {{function of the}} relationship between predictability and reaction time..|$|E
50|$|Here, the Kronecker delta is {{used for}} {{simplicity}} (cf. the derivative of a <b>sigmoid</b> <b>function,</b> being expressed via the function itself).|$|E
40|$|<b>Sigmoid</b> <b>functions</b> play an {{important}} role in many areas of applied mathematics, including machine learning, population dynamics and probability. We place the study of <b>sigmoid</b> <b>functions</b> in the context of the derivative sub-group of the group of exponential Riordan arrays. Links to families of polynomials are drawn, and it is shown that in some cases these polynomials are orthogonal. In the non-orthogonal case, transformations are given that produce orthgonal systems. Alternative means of characterisation are given, based on the production (Stieltjes) matrix associated to the relevant Riordan array. Comment: 25 page...|$|R
30|$|It {{is obvious}} that the {{conditions}} in Assumption 2.1 are more general than the usual <b>sigmoid</b> <b>functions</b> and the recently commonly used Lipschitz conditions; see, for example, [3 – 7].|$|R
30|$|It {{is obvious}} that, the {{condition}} in Assumption 3 is more general {{than the usual}} <b>sigmoid</b> <b>functions</b> and the recently commonly used Lipschitz conditions, see, for example, [4 – 10].|$|R
5000|$|Update {{the hidden}} units in {{parallel}} given the visible units: [...] [...] is the <b>sigmoid</b> <b>function</b> and [...] is the bias of [...]|$|E
50|$|A <b>sigmoid</b> <b>function</b> is a bounded {{differentiable}} real {{function that}} is defined for all real input values {{and has a}} non-negative derivative at each point.|$|E
5000|$|If the {{activation}} function is the <b>sigmoid</b> <b>function</b> and the weights are general, then the VC dimension {{is at least}} [...] and at most [...]|$|E
5000|$|The {{generalised}} {{logistic function}} or curve, {{also known as}} Richards' curve, originally developed for growth modelling, {{is an extension of}} the logistic or <b>sigmoid</b> <b>functions,</b> allowing for more flexible S-shaped curves: ...|$|R
50|$|A {{wide variety}} of <b>sigmoid</b> <b>functions</b> {{have been used as}} the {{activation}} function of artificial neurons, including the logistic and hyperbolic tangent <b>functions.</b> <b>Sigmoid</b> curves are also common in statistics as cumulative distribution functions (which go from 0 to 1), such as the integrals of the logistic distribution, the normal distribution, and Student's t probability density functions.|$|R
30|$|The {{network was}} built by using MATLAB’s trainbr <b>function.</b> The {{logistic}} <b>sigmoid</b> <b>functions</b> are used for the activation function in each neuron and a linear transfer function, {{which is used to}} calculate the network output [57].|$|R
5000|$|Each {{intermediate}} node outputs a certain increasing function of its input, {{such as the}} sign function or the <b>sigmoid</b> <b>function.</b> This function is called the activation function.|$|E
5000|$|A <b>sigmoid</b> <b>function</b> is a {{mathematical}} function having a characteristic [...] "S"-shaped curve or sigmoid curve. Often, <b>sigmoid</b> <b>function</b> {{refers to the}} special case of the logistic function shown in the first figure and defined by the formulaOther examples of similar shapes include the Gompertz curve (used in modeling systems that saturate at large values of x) and the ogee curve (used in the spillway of some dams). Sigmoid functions have domain of all real numbers, with return value monotonically increasing most often from 0 to 1 or alternatively from −1 to 1, depending on convention.|$|E
5000|$|Fuzzy {{sets are}} often defined as {{triangle}} or trapezoid-shaped curves. They {{can also be}} defined using a <b>sigmoid</b> <b>function.</b> One common case is the standard logistic function defined as [...] which has the following symmetry property: ...|$|E
5000|$|Output level, {{the data}} {{goes through a}} <b>sigmoid</b> {{transition}} <b>function</b> ...|$|R
40|$|Abstract—Neural {{networks}} with threshold activation {{functions are}} highly desirable {{because of the}} ease of hardware implementation. However, the popular gradient-based learning algorithms cannot be directly used to train these networks as the threshold functions are nondifferentiable. Methods available in the literature mainly focus on approximating the threshold activation <b>functions</b> by using <b>sigmoid</b> <b>functions.</b> In this paper, we show theoretically that the recently developed extreme learning machine (ELM) algorithm {{can be used to}} train the neural networks with threshold functions directly instead of approximating them with <b>sigmoid</b> <b>functions.</b> Experimental results based on real-world benchmark regression problems demonstrate that the generalization performance obtained by ELM is better than other algorithms used in threshold networks. Also, the ELM method does not need control variables (manually tuned parameters) and is much faster. Index Terms—Extreme learning machine (ELM), gradient descent method, threshold neural networks. I...|$|R
30|$|An easy {{solution}} to the issues raised in this paper, is to avoid steep firing rate functions. If β is fairly small, then standard ODE theory [9, 10] and textbook material about their numerical treatment can be used, provided that the source term q(t) is continuous. Nevertheless, steep <b>sigmoid</b> <b>functions</b> are popular in computational neuroscience.|$|R
50|$|The {{hyperbolic}} tangent function, tanh, limits {{the range of}} the normalized data to values between −1 and 1. The {{hyperbolic tangent}} function is almost linear near the mean, but has a slope of half that of the <b>sigmoid</b> <b>function.</b> Like sigmoid, it has smooth, monotonic nonlinearity at both extremes. Also, like the <b>sigmoid</b> <b>function,</b> it remains differentiable everywhere and the sign of the derivative (slope) is unaffected by the normalization. This ensures that optimization and numerical integration algorithms can continue to rely on the derivative to estimate changes to the output (normalized value) that will be produced by changes to the input in the region near any linearisation point.|$|E
50|$|Many natural processes, such {{as those}} of complex system {{learning}} curves, exhibit a progression from small beginnings that accelerates and approaches a climax over time. When a specific mathematical model is lacking, a <b>sigmoid</b> <b>function</b> is often used.|$|E
5000|$|Other {{functions}} {{are also used}} to increase nonlinearity, for example the saturating hyperbolic tangent , , and the <b>sigmoid</b> <b>function</b> [...] ReLU is preferable to other functions, because it trains the neural network several times faster without a significant penalty to generalisation accuracy.|$|E
30|$|According to (2), we {{can obtain}} {{that the total}} energy {{consumption}} for any meter at a home is linearly related to {{the square of the}} transmission distance. Thus, the argument x of the <b>sigmoid</b> <b>functions</b> to calculate the probability f Di ab and f Ti ab is the normalized square of the transmission distance.|$|R
30|$|We {{have seen}} that point neuron models with a Heaviside firing rate {{function}} can have several solutions. One therefore might wonder if different subsequences of {u_β} can converge to different solutions of the limit problem. In this section we present an example which shows that this can happen, even though the involved <b>sigmoid</b> <b>functions</b> satisfy Assumption A.|$|R
30|$|Approximate {{continuous}} shape function: shape constraints play {{an important}} role in discriminative frequency filter banks. Few investigations have been conducted to compare different shape constraints, because that commonly used shapes such as triangular shapes are piecewise differentiable. We use steep <b>sigmoid</b> <b>functions</b> and other basic functions to approximate desired shapes. This makes a further study on shape constraints possible.|$|R
