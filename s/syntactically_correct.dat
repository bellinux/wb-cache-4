286|28|Public
25|$|CSP {{has been}} imbued with several {{different}} formal semantics, which define {{the meaning of}} <b>syntactically</b> <b>correct</b> CSP expressions. The theory of CSP includes mutually consistent denotational semantics, algebraic semantics, and operational semantics.|$|E
2500|$|Here is a {{context-free}} grammar for <b>syntactically</b> <b>correct</b> infix algebraic expressions in the variables x, y and z: ...|$|E
2500|$|After {{the start}} of the Usenet discussion, most {{comments}} were critical of the Bogdanovs' work. For example, John C. Baez stated that the Bogdanov papers are [...] "a mishmash of superficially plausible sentences containing the right buzzwords in approximately the right order. There is no logic or cohesion in what they write." [...] Jacques Distler voiced a similar opinion, proclaiming [...] "The Bogdanov's [...] papers consist of buzzwords from various fields of mathematical physics, string theory and quantum gravity, strung together into <b>syntactically</b> <b>correct,</b> but semantically meaningless prose." ...|$|E
5000|$|Set up a clear framework: {{the analyst}} needs {{to ensure that}} the {{visualization}} is <b>syntactically</b> and semantically <b>correct.</b> For example, when using an icon, the element should bear resemblance to the thing it represents, with size, color, and position all communicating meaning to the viewer.|$|R
40|$|An {{important}} step towards making reuse {{work is the}} automation of the software design process. That means to support the development and maintenance of high quality software documents †, {{as well as their}} (re-) use at early stages of the software life cycle. In this context high quality means that the documents are <b>syntactically</b> and semantically <b>correct...</b>|$|R
40|$|The Entity-Attribute-Value (EAV) data {{representation}} {{is widely used}} in both clinical patient record systems (CPRSs) and clinical study data management systems (CSDMS). Both of these systems are heterogeneous in that many types of data are also represented conventionally, a situation that complicates data display, editing and ad hoc query. Seamless functioning of such systems mandates the presence of developer-defined metadata (data describing {{the rest of the}} database) that records, among other things, how individual parameters are represented within the system. We illustrate a web-based ad hoc query tool that relies on the metadata to generate <b>syntactically</b> and semantically <b>correct</b> SQL...|$|R
2500|$|When I utter a {{sentence}} I draw upon various syntactical rules (sedimented in my practical {{consciousness of the}} language) {{in order to do}} so. These structural features of the language are the medium whereby I generate the utterance. But in producing a <b>syntactically</b> <b>correct</b> utterance I simultaneously contribute to the reproduction of the language as a whole. ...The relation between moment and totality for social theory... a dialectic of presence and absence which ties the most minor or trivial forms of social action to structural properties of the overall society, and to the coalescence of institutions over long stretches of historical time.|$|E
2500|$|Programming {{languages}} include {{features to}} help prevent bugs, such as static type systems, restricted namespaces and modular programming. For example, when a programmer writes (pseudocode) LET REAL_VALUE PI = [...] "THREE AND A BIT", although this may be <b>syntactically</b> <b>correct,</b> the code fails a type check. Compiled languages catch this without having to run the program. Interpreted languages catch such errors at runtime. Some languages deliberate exclude features that easily lead to bugs, {{at the expense of}} slower performance: the general principle being that, it is almost always better to write simpler, slower code than inscrutable code that runs slightly faster, especially considering that maintenance cost is substantial. For example, the Java programming language does not support pointer arithmetic; implementations of some languages such as Pascal and scripting languages often have runtime bounds checking of arrays, at least in a debugging build.|$|E
5000|$|... is <b>syntactically</b> <b>correct,</b> {{its purpose}} is not evident. Contrast this with: weekly_pay = hours_worked * pay_rate; ...|$|E
40|$|Summary: Velvet is {{a popular}} {{open-source}} de novo genome as-sembly software tool which is run from the Unix command line. Most of the problems experienced by new users of Velvet revolve around constructing <b>syntactically</b> and semantically <b>correct</b> command lines, getting input files into acceptable formats, and assessing the output. Here we present VAGUE (Velvet Assembler Graphical User Envi-ronment), a multi-platform graphical front-end for Velvet. VAGUE aims to make sequence assembly accessible to a wider audience, and to facilitate better usage amongst existing users of Velvet. Availability and Implementation: VAGUE is implemented in JRuby and targets the Java Virtual Machine. It is available under an open-source GPLv 2 licence fro...|$|R
40|$|In the {{rule-based}} equivalent transformation (RBET) paradigm, where computation {{is based}} on meaning-preserving transformation of declarative descriptions, a set of rewriting rules {{is regarded as a}} program. The syntax for a large class of rewriting rules is determined. The incorporation of meta-variables of two different kinds enables precise control of rewriting-rule instantiations. As a result, the applicability of rewriting rules and the results of rule applications can be rigorously specified. A theoretical basis for justifying the correctness of rewriting rules is established. Reverse transformation operation in the RBET framework is discussed, and it is shown that a correct rewriting rule is reversible, i. e., a correct rewriting rule can in general be constructed by <b>syntactically</b> reversing another <b>correct</b> rewriting rule...|$|R
40|$|The {{present study}} {{investigated}} the effects of auditory selective attention on the processing of syntactic information in music and speech using event-related potentials. Spoken sentences or musical chord sequences were either presented in isolation, or simultaneously. When presented simultaneously, participants had to focus their attention either on speech, or on music. Final words of sentences and final harmonies of chord sequences were <b>syntactically</b> either <b>correct</b> or incorrect. Irregular chords elicited an early right anterior negativity (ERAN), whose amplitude was decreased when music was simultaneously presented with speech, compared to when only music was presented. However, the amplitude of the ERAN-like waveform elicited when music was ignored {{did not differ from}} the conditions in which participants attended the chord sequences. Irregular sentences elicited an early left anterior negativity (ELAN), regardless of whether speech was presented in isolation, was attended, or was to be ignored. These findings suggest that the neural mechanisms underlying the processing of syntactic structure of music and speech operate partially automatically, and, in the case of music, are influenced by different attentional conditions. Moreover, the ERAN was slightly reduced when irregular sentences were presented, but only when music was ignored. Therefore, these findings provide no clear support for an interaction of neural resources for syntactic processing already at these early stages. ...|$|R
5000|$|... a {{context-free}} grammar which describes how lexemes {{may be combined}} to form a <b>syntactically</b> <b>correct</b> program.|$|E
50|$|This {{document}} is <b>syntactically</b> <b>correct</b> RD,which attempts {{to follow the}} major conventions on section naming as well.|$|E
5000|$|Here is a {{context-free}} grammar for <b>syntactically</b> <b>correct</b> infix algebraic expressions in the variables x, y and z: ...|$|E
40|$|The goal of {{this thesis}} is {{creation}} of an XML data editor. The editor contains functionality to simplify creation of <b>syntactically</b> and semantically <b>correct</b> XML documents. The main feature of the editor is automatic inference of XML schema {{for a set of}} input XML documents. User may edit XML data by either a text view or a tree view. It is possible to check well-formedness and validity of edited XML data. There is a code completion - overview of feasible elements on given level - feature in the text view available. Implemented solution facilitates an easy extension of its main feature - schema inference. The program is written in C# programming language and is designed for use on Microsoft Windows operating systems. Powered by TCPDF (www. tcpdf. org...|$|R
40|$|In present days, most of {{the design}} {{activity}} is performed {{at a high level}} of abstraction, thus designers need to be sure that their designs are <b>syntactically</b> and semantically <b>correct</b> before starting the automatic synthesis process. The goal {{of this paper is to}} propose an automatic input pattern generation tool able to assist designers in the generation of a test bench for difficult parts of small- or medium- sized digital protocol interfaces. The proposed approach exploit a Genetic Algorithm connected to a commercial simulator for cultivating a set of input sequence able to execute given statements in the interface description. The proposed approach has been evaluated on the new ITC' 99 benchmark set, a collection of circuits offering a wide spectrum of complexity. Experimental results show that some portions of the circuits remained uncovered, and the subsequent manual analysis allowed identifying design redundancies...|$|R
40|$|We {{introduce}} a new genetic operator, Reduction, that rectifies decision trees not <b>correct</b> <b>syntactically</b> {{and at the same}} time removes the redundant sec-tions within, while preserving its accuracy during operation. A novel approach to crossover is pre-sented that uses the reduction operator to system-atically extract building blocks spread out over the entire second parent to create a subtree that is valid and particularly useful in the context it replaces the subtree in the first parent. The crossover introduced also removes unexplored code from the offspring and hence prevents redundancy and bloating. Overall, reduction can be viewed as a local optimization step that directs the population, generated initially or over generations through crossovers, to potentially good regions in the search space so that reproduc-tion is performed in a highly correlated landscape with a global structure. Lexical convergence is also ensured implying identical individuals always pro-duce the same offspring. ...|$|R
50|$|A <b>syntactically</b> <b>correct</b> program {{then would}} be: PROGRAM DEMO1 BEGIN A:=3; B:=45; H:=-100023; C:=A; D123:=B34A; BABOON:=GIRAFFE; TEXT:="Hello world!"; END.|$|E
50|$|Not all <b>syntactically</b> <b>correct</b> {{programs}} are semantically correct. Many <b>syntactically</b> <b>correct</b> {{programs are}} nonetheless ill-formed, per the languages rules; and may (depending {{on the language}} specification and the soundness of the implementation) result in an error on translation or execution. In some cases, such programs may exhibit undefined behavior. Even when a program is well-defined within a language, it may still have a meaning that is not intended by the person who wrote it.|$|E
50|$|Every fully inflected Inuktitut verb can act {{alone as}} a proposition. No other words are {{required}} to form a <b>syntactically</b> <b>correct</b> sentence.|$|E
40|$|One of {{the goals}} of Model Driven Engineering (MDE) is to enable automation. It is, however, a challenge, when {{pursuing}} this automation, to include textual requirements as a model. It is even more challenging to maintain the consistency of requirements and design models, when changes are made to one of them during an iterative process. In this paper, we propose a modeling environment, built as part of the Symbiosis framework, to enable the continuous synchronization between textual requirements and the core design model: Object Model. The co-evolution is successful due to a fact-oriented approach. We performed two case studies to evaluate Symbiosis. Results show, that it does not take much effort to follow our methodology with the Symbiosis tool support, to derive a <b>syntactically</b> and semantically <b>correct</b> and complete design model that fully conforms to textual requirements. Keywords: Requirements, Natural Language, Fact Oriented Modeling, Model Driven Engineering, Object Model and Co-evolution...|$|R
40|$|We {{present a}} novel {{approach}} for generating <b>syntactically</b> and semantically <b>correct</b> SQL queries for testing relational database systems. We leverage the SAT-based Alloy tool-set to reduce the problem of generating valid SQL queries into a SAT problem. Our approach translates SQL query constraints into Alloy models, which enable it to generate valid queries that cannot be automatically generated using conventional grammar-based generators. Given a database schema, our new approach combined with our previous work on ADUSA, automatically generates (1) syntactically and semantically valid SQL queries for testing, (2) input data to populate test databases, and (3) expected result of executing the given query on the generated data. Experimental results show that not only can we automatically generate valid queries which detect bugs in database engines, but also {{we are able to}} combine this work with our previous work on ADUSA to automatically generate input queries and tables as well as expected query execution outputs to enable automated testing of database engines...|$|R
40|$|In this paper, we have {{proposed}} a methodology towards improved business process model redesign based on QoS. We have extended an existing framework that generates an exhaustive space of process models from a set of capability library. The solution space is pruned based on goal and constraints considered thereafter. An algebraic framework is deployed that permits integrated multi-dimensional assessments of QoS factors for choosing path from the reduced space towards derivation of an optimal business process model by comparing the QoS values on both quantitative and qualitative scales. The proposed methodology ensures that while deriving a solution, no possible superior business process model is left out. Further, the designs that do not satisfy the given constraints are eliminated. Eventually, the extended and improved framework provides a comprehensive, both <b>syntactically</b> and semantically <b>correct,</b> consistent and improved business process that adheres to the target business goals and constraints specific to a business house. A use case is used to describe our methodology...|$|R
5000|$|... {{the set of}} <b>syntactically</b> <b>correct</b> {{programs}} in a given programming language (the syntax of which is usually defined by a context-free grammar); ...|$|E
5000|$|Let wff {{stand for}} a well-formed formula (or <b>syntactically</b> <b>correct</b> formula) of {{elementary}} geometry. Tarski and Givant (1999: 175) proved that elementary geometry is: ...|$|E
5000|$|Beginning {{with the}} {{sentence}} symbol S, and applying the phrase structure rules successively, finally applying replacement rules to substitute actual {{words for the}} abstract symbols, {{it is possible to}} generate many proper sentences of English (or whichever language the rules are specified for). If the rules are correct, then any sentence produced in this way ought to be grammatically (<b>syntactically)</b> <b>correct.</b> It is also to be expected that the rules will generate <b>syntactically</b> <b>correct</b> but semantically nonsensical sentences, such as the following well-known example: ...|$|E
40|$|Millions of {{individuals}} have acquired {{or have been}} born with neuro-motor conditions that limit the control of their muscles, including those that manipulate the articulators of the vocal tract. These conditions, collectively called dysarthria, result in speech that {{is very difficult to}} understand, despite being generally <b>syntactically</b> and semantically <b>correct.</b> This difficulty is not limited to human listeners, but also adversely affects the performance of traditional automatic speech recognition (ASR) systems, which in some cases can be completely unusable by the affected individual. This dissertation describes research into improving ASR for speakers with dysarthria by means of incorporated knowledge of their speech production. The document first introduces theoretical aspects of dysarthria and of speech production and outlines related work in these combined areas within ASR. It then describes the acquisition and analysis of the TORGO database of dysarthric articulatory motion and demonstrates several consistent behaviours among speakers in this database, including predictable pronunciation errors, for example. Articulatory data are then used to train augmented ASR systems that model the statistical relationships between vocal tract configurations and their acoustic consequences. I show that dynami...|$|R
40|$|Teamwork is {{the typical}} {{characteristic}} of software development, because the taskscan be splitted and parallelized. The independently working developers use SoftwareConfiguration Management (SCM) systems to apply version control to their files and tokeep them consistent. Several SCM systems allow {{working on the}} same files concurrently,and attempt to auto-merge the files {{in order to facilitate}} the reconciliation of the parallelmodifications. The merge should produce <b>syntactically</b> and semantically <b>correct</b> sourcecode files, therefore, developers are often involved into the resolution of the conflicts. However, when a general textual-based approach reports a successful merge, the outputcan still be failed in compile time, because semantic correctness cannot be ensuredtrivially. Renaming an identifier consists of many changes, and can cause semantic errorsin the output of the merge, which subsequently have to be corrected manually. This paperintroduces that matching the identifier declarations, e. g. class, field, method, localvariables, with their corresponding references in the abstract syntax trees of the revisions,and considering the detected renamings during the merge takes closer to semanticcorrectness. The problem is illustrated and a solution is elaborated in this work...|$|R
40|$|Abstract. We {{investigate}} {{a new paradigm}} {{in the context of}} learning in the limit, namely, learning correction grammars for classes of r. e. languages. Knowing a language may feature a representation of the target language in terms of two sets of rules (two grammars). The second grammar is used to make corrections to the first grammar. Such a pair of grammars {{can be seen as a}} single description of (or grammar for) the language. We call such grammars correction grammars. Correction grammars capture the observable fact that people do correct their linguistic utterances during their usual linguistic activities. Is the need for self-corrections implied by using correction grammars instead of normal grammars compensated by a learning advantage? We show that learning correction grammars for classes of r. e. languages in the TxtEx-model (i. e., converging to a single correct correction grammar in the limit) is sometimes more powerful than learning ordinary grammars even in the TxtBc-model (where the learner is allowed to converge to infinitely many <b>syntactically</b> distinct but <b>correct</b> conjectures in the limit). For each n ≥ 0, there is a similar learning advantage, again in learning correctio...|$|R
50|$|CSP {{has been}} imbued with several {{different}} formal semantics, which define {{the meaning of}} <b>syntactically</b> <b>correct</b> CSP expressions. The theory of CSP includes mutually consistent denotational semantics, algebraic semantics, and operational semantics.|$|E
5000|$|Not all ambiguities can {{be safely}} removed from ACE without {{rendering}} it artificial. To deterministically interpret otherwise <b>syntactically</b> <b>correct</b> ACE sentences we use a small set of interpretation rules. For example, if we write: ...|$|E
50|$|The {{second example}} would {{theoretically}} print the variable Hello World {{instead of the}} words Hello World. However, a variable in Java cannot have a space in between, so the <b>syntactically</b> <b>correct</b> line would be System.out.println(Hello_World).|$|E
40|$|Abstract: Teamwork is {{the typical}} {{characteristic}} of software development, because the tasks can be splitted and parallelized. The independently working developers use Software Configuration Management (SCM) systems to apply version control to their files {{and to keep}} them consistent. Several SCM systems allow working on the same files concurrently, and attempt to auto-merge the files {{in order to facilitate}} the reconciliation of the parallel modifications. The merge should produce <b>syntactically</b> and semantically <b>correct</b> source code files, therefore, developers are often involved into the resolution of the conflicts. However, when a general textual-based approach reports a successful merge, the output can still be failed in compile time, because semantic correctness cannot be ensured trivially. Renaming an identifier consists of many changes, and can cause semantic errors in the output of the merge, which subsequently have to be corrected manually. This paper introduces that matching the identifier declarations, e. g. class, field, method, local variables, with their corresponding references in the abstract syntax trees of the revisions, and considering the detected renamings during the merge takes closer to semantic correctness. The problem is illustrated and a solution is elaborated in this work...|$|R
40|$|International audienceState-of-the-art speech {{recognition}} systems steadily increase their performance using different variants of deep neural networks and postprocess the results by employing N-gram statistical models trained {{on a large}} amount of data coming from the general-purpose domain. While achieving an excellent performance regarding Word Error Rate (17. 343 % on our Human-Robot Interaction data set), state-of-the-art systems generate hypotheses that are grammatically incorrect in 57. 316 % of the cases. Moreover, if employed in a restricted domain (e. g. Human-Robot Interaction), around 50 % of the hypotheses contain out-of-domain words. The latter are confused with similarly pronounced in-domain words and cannot be interpreted by a domain-specific inference system. The state-of-the-art {{speech recognition}} systems lack a mechanism that addresses syntactic correctness of hypotheses. We propose a system that can detect and repair grammatically incorrect or infrequent sentence forms. It is inspired by a computational neuroscience model that we developed previously. The current system is still a proof-of-concept version of a future neurobiologically more plausible neural network model. Hence, the resulting system postprocesses sentence hypotheses of state-of-the-art speech recognition systems, producing in-domain words in 100 % of the cases, <b>syntactically</b> and grammatically <b>correct</b> hypotheses in 90. 319 % of the cases. Moreover, it reduces the Word Error Rate to 11. 038 %...|$|R
40|$|An {{efficient}} and flexible business process not only helps an organization {{to meet the}} requirements of the evolving surroundings but also may facilitate a competitive advantage over other companies towards delivering the desired services. This is even more critical for an emerging paradigm like cloud based deployment. In this paper, we introduce a novel mechanism to generate the business process suitable for specific organizations. The approach provides an automated way to build the possible business processes for a given set of tasks that fulfills the goal and satisfies the constraints of an organization. In step 1, we show how to generate the finite space of all possible designs for a given set of tasks. Secondly, we accumulate the effect of each step to deduce the final effect of each possible process design and to ensure that the redesigned set of steps still realizes the service goal. The designs not meeting the service goals are eliminated from the space. In step 3, the rest of the designs are checked for the constraint satisfaction subject to some specific cases. The framework provides a comprehensive, both <b>syntactically</b> and semantically <b>correct,</b> consistent business process generation methodology that adheres to the target business goals and constraints...|$|R
