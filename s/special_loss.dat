18|55|Public
25|$|While private {{nuisance}} is always actionable, public nuisance is not. A claimant of public nuisance has to establish <b>special</b> <b>loss</b> {{over and above}} the inconvenience suffered by the public in general, as public nuisance is a crime and it would be unreasonable for everyone inconvenienced by it to be allowed to claim. This distinction was followed in India, along with the UK principles of nuisance.|$|E
30|$|Then for {{the loss}} {{function}}. In this section we can choose some <b>special</b> <b>loss</b> functions, such as the LAD loss function, the OLS loss function and the Huber loss function. In this paper we choose the LAD loss function and the Huber loss function.|$|E
40|$|A {{model is}} {{created for the}} number of {{integrated}} circuits that are good from each wafer on which they are fabricated. The goal is to separate the random or common cause loss from the systematic or <b>special</b> <b>loss.</b> The random loss from this type of process is modeled so that false alarms indicating systematic loss are less likely to occur and so that the structure of the systematic loss can be determined...|$|E
50|$|Tokyo Electric Power {{could face}} 2 {{trillion}} yen ($23.6 bln) in <b>special</b> <b>losses</b> {{in the current}} business year to March 2012 to compensate communities near its crippled Fukushima nuclear plant, according to JP Morgan.|$|R
2500|$|TEPCO {{could face}} 2 {{trillion}} yen ($23.6 bln) in <b>special</b> <b>losses</b> {{in the current}} business year to March 2012 to compensate communities near its crippled Fukushima I nuclear plant, according to JPMorgan Chase. As of June 2011, TEPCOs stock has [...] "slumped 91 percent, erasing 3.2 trillion yen ($40 billion) in market value".|$|R
50|$|Teen Fit Camp was an Australian {{reality show}} {{broadcast}} by Network Ten. It followed {{a group of}} overweight Australian teenagers chosen {{to participate in a}} <b>special</b> weight <b>loss</b> program.|$|R
30|$|A {{method for}} solving {{the second of}} the {{shortcomings}} of the FCM algorithm is based on the exploitation of the robust estimators especially the M-estimator [16]. This proposal was studied by Frigui and Krishnapuram [17], who designed their own robust estimator, based on <b>special</b> <b>loss</b> and weight functions. In this algorithm, the robust estimators allow the control of the influence of outliers in calculating the centers of the all groups. This paper is focused on the second shortcoming of the FCM algorithm. For this, we propose to use classical M-estimators and as the robust framework [18], states to use their loss and influence functions instead. We extent the proposed algorithm in a bidimensional clustering approach using the chromatic subspace in the IJK color space to segment color images [19].|$|E
40|$|This work {{proposes a}} means for {{interconnecting}} optimal sample statistics with parameters of the process output distribution irrespective of the specific way in which these parameters change during transition to the out-of-control state (jumps, trends, cycles, etc). The approach, based on minimization of the loss incurred by {{the two types of}} decision errors, leads to a unique sample statistic and, therefore, to a single control chart. The optimal sample statistics are obtained as a solution of the developed optional boundary equation. The paper demonstrates that, for particular conditions, this equation leads to the same statistics as are obtained through the Neyman-Pearson fundamental lemma. Application examples of the approach when the process output distribution is Gamma and Weibull are given. A <b>special</b> <b>loss</b> function representing out-of-control state detection as a pattern recognition problem is presented. ...|$|E
40|$|Machine {{learning}} {{is an area}} of computer science concerned with the study of algorithms that reveal patterns and rules from data sets. Genomic profiles describe alterations of a genome, like copy number variations. Cancer often originates from a combination of genomic alterations. In this thesis, I consider machine learning and its application to genomic profiles. The main aspects of this work can be summarised as follows: First, I discuss several machine learning methods, with particular regard to genomic profiles, and then develop a <b>special</b> <b>loss</b> function for survival data. Next, I introduce a framework to find aberration patterns associated with a particular tumour type or disease state. This workflow starts with pre-processing, feature selection and discretisation of genomic profiles, includes strategies to deal with missing values and provides a multi-resolutional ana-lysis. Then, training and analysis of a classifier is performed. Additionally, I introduce an explanation component that emphasizes impor...|$|E
50|$|In 2007, TEPCO {{was forced}} to shut the Kashiwazaki-Kariwa Nuclear Power Plant after the Niigata-Chuetsu-Oki earthquake. That year it posted its first loss in 28 years. Corporate losses {{continued}} until the plant reopened in 2009. Following the March 2011 Tōhoku earthquake and tsunami, its power plant at Fukushima Daiichi {{was the site of}} one of the world's most serious continuing nuclear disaster. TEPCO could face ¥ ($) in <b>special</b> <b>losses</b> in the current business year to March 2012, and the Japanese government plans to put TEPCO under effective state control to guarantee compensation payments to the people affected by the accident. The Fukushima disaster displaced 50,000 households in the evacuation zone because of leaks of radioactive materials into the air, soil and sea.|$|R
50|$|Bruno's {{position}} {{became more}} tenuous in February 2008 after the <b>special</b> election <b>loss</b> of the heavily Republican 48th District in Watertown, which had formerly been held by Sen. James W. Wright. This loss diminished the Republican Senate majority {{to a single}} seat, and press speculation centered on whether the remaining GOP senate caucus would cause Bruno to step down.|$|R
50|$|On June 22, 1935, Seabiscuit {{broke his}} maiden at Narragansett and equaled the five-furlong track record. Four {{days later in}} the Watch Hill Claiming Stakes he once again broke the track record, this time by a full second. In 1937, Seabiscuit {{finished}} third in the Narragansett <b>Special.</b> The <b>loss</b> ended a streak of seven consecutive stakes wins for Seabiscuit, one shy of Discovery's record.|$|R
40|$|In this paper, {{we study}} {{discriminant}} function based minimum recognition error rate pattern recognition approach. This ap-proach departs from the conventional paradigm which links a classification/recognition task {{to the problem}} of distribution esti-mation. Instead, it takes a discriminant function based statistical pattern recognition approach and the goodness of this approach to classification error rate minimization is established through a <b>special</b> <b>loss</b> function. It is meaningful even when the model correctness assumption is known not valid. The use of discrimi-nant function has a significant impact on classifier design, since in many realistic applications, such as speech recognition, the true distribution form of the source is rarely known precisely and without model correctness assumption, the classical optimality theory of the distribution estimation approach can not be applied directly. We discuss issues in this new classifier design paradigm and present various extensions of this approach for applications in speech processing. 1...|$|E
40|$|In the {{asymptotic}} {{setting of}} the change-point estimation problem the limiting behavior of Bayes procedures for a general loss function is studied. It is demonstrated that {{the distribution of the}} difference between the Bayes estimator and the parameter converges to the distribution of a (nondegenerate) random variable. The sequence of minimum Bayes risks is shown to converge to its supremum, and an explicit formula for this limit is obtained for the geometric prior distribution. <b>Special</b> <b>loss</b> functions, in particular, the linex loss, are considered in detail. 1. Introduction and Summary. In this paper we study the classical change-point estimation problem in the Bayesian setting. Point estimation of the change moment, when the distribution of the sample switches from one distribution to another, was initiated in [6] where the limiting distribution of the maximum likelihood estimator was derived. The convergence results for various non-parametric statistics are obtained in [4], [5]. More [...] ...|$|E
40|$|Abstract—In {{wireless}} sensor networks (WSNs), {{since many}} basic scientific works heavily rely on the complete sensory data, data recovery is an indispensable operation against the data loss. Several works have studied the missing value problem. However, existing solutions cannot achieve satisfactory accuracy due to <b>special</b> <b>loss</b> patterns and high loss rates in WSNs. In this work, we propose a multiple attributes-based recovery algorithm which can provide high accuracy. Firstly, based on two real datasets, the Intel Indoor project and the GreenOrbs project, we reveal that such correlations are strong, e. g., the change of temperature and light illumination usually has strong correlation. Secondly, motivated by this observation, we develop a Multi-Attribute-assistant Compressive-Sensing-based (MACS) algorithm to optimize the recovery accuracy. Finally, real trace-driven simulation is performed. The results show that MACS outperforms the existing solutions. Typically, MACS can recover all data with less than 5 % error when the loss rate is less than 60 %. Even when losing 85 % data, all missing data can be estimated by MACS with less than 10 % error. I...|$|E
5000|$|When the {{conversion}} occurs, the injured party should receive full compensation for actual <b>losses.</b> <b>Special</b> damages may be recovered in an action for conversion for any injury proximately resulting from {{the conversion}}. The Restatement (Second) of Torts indicates these damages can consist of: ...|$|R
40|$|Loss aversion is {{traditionally}} {{defined in the}} context of lotteries over monetary payoffs. This paper extends the notion of loss aversion to a more general setup where outcomes (consequences) may not be measurable in monetary terms and people may have fuzzy preferences over lotteries, i. e. they may choose in a probabilistic manner. The implications of loss aversion are discussed for expected utility theory and rankdependent utility theory as well as for popular models of probabilistic choice such as the constant error/tremble model and a strong utility model (that includes the Fechner model of random errors and Luce choice model as <b>special</b> cases). <b>Loss</b> aversion, more loss averse than, nonmonetary outcomes, probabilistic choice, rank-dependent utility theory...|$|R
30|$|Disaster relief can be {{regarded}} as a <b>special</b> type of <b>loss</b> transfer. As long as the government provides aid to affected people, part of their loss is borne by the government. The benefit {{from the point of view}} of the affected people is purely the aid provided by the government, which equals the cost to the government of providing that aid, assuming that there are no transaction costs.|$|R
40|$|A {{family of}} {{classification}} algorithms generated from Tikhonov regularization schemes are considered. They involve multi-kernel spaces and general convex loss functions. Our main {{purpose is to}} provide satisfactory estimates for the excess misclassification error of these multi-kernel regularized classifiers. The error analysis consists of two parts: regularization error and sample error. Allowing multi-kernels in the algorithm improves the regularization error and approximation error, which is one advantage of the multi-kernel setting. For a general loss function, we show how to bound the regularization error by the approximation in some weighted L q spaces. For the sample error, we use a projection operator. The projection {{in connection with the}} decay of the regularization error enables us to improve convergence rates in the literature even for the one kernel schemes and <b>special</b> <b>loss</b> functions: least square loss and hinge loss for support vector machine soft margin classifiers. Existence of the optimization problem for the regularization scheme associated with multi-kernels is verified when the kernel functions are continuous with respect to the index set. Gaussian kernels with flexible variances and probability distributions with some noise conditions are demonstrated to illustrate the general theory...|$|E
40|$|AbstractA {{family of}} {{classification}} algorithms generated from Tikhonov regularization schemes are considered. They involve multi-kernel spaces and general convex loss functions. Our main {{purpose is to}} provide satisfactory estimates for the excess misclassification error of these multi-kernel regularized classifiers when the loss functions achieve the zero value. The error analysis consists of two parts: regularization error and sample error. Allowing multi-kernels in the algorithm improves the regularization error and approximation error, which is one advantage of the multi-kernel setting. For a general loss function, we show how to bound the regularization error by the approximation in some weighted Lq spaces. For the sample error, we use a projection operator. The projection {{in connection with the}} decay of the regularization error enables us to improve convergence rates in the literature even for the one-kernel schemes and <b>special</b> <b>loss</b> functions: least-square loss and hinge loss for support vector machine soft margin classifiers. Existence of the optimization problem for the regularization scheme associated with multi-kernels is verified when the kernel functions are continuous with respect to the index set. Concrete examples, including Gaussian kernels with flexible variances and probability distributions with some noise conditions, are used to illustrate the general theory...|$|E
40|$|In {{observational}} studies, propensity {{scores are}} commonly estimated by maxi- mum likelihood but {{may fail to}} balance high-dimensional pre-treatment covariates even after specification search. We introduce a general framework that unifies and generalizes several recent proposals to improve covariate balance when designing an observational study. In- stead of the likelihood function, we propose to optimize <b>special</b> <b>loss</b> functions [...] -covariate balancing scoring rules (CBSR) [...] -to estimate the propensity score. A CBSR is uniquely determined by the link function in the GLM and the estimand (a weighted average treatment effect). We show CBSR does not lose asymptotic efficiency to the Bernoulli likelihood in estimating the weighted average treatment effect compared, but CBSR is much more robust in finite sample. Borrowing tools developed in statistical learning, we propose practical strategies to balance covariate functions in rich function classes. This is useful to estimate the maximum bias of the inverse probability weighting (IPW) estimators and construct honest confidence interval in finite sample. Lastly, we provide several numerical examples to demonstrate the trade-off of bias and variance in the IPW-type estimators and the trade-off in balancing different function classes of the covariates. Comment: 28 pages, 2 figures, 3 table...|$|E
40|$|A generic unital {{positive}} operator-valued measure (POVM), which transforms a given stationary pure {{state to}} an arbitrary statistical state with perfect decoherence, is presented. This {{allows one to}} operationally realize thermalization as a <b>special</b> case. The <b>loss</b> of information due to randomness generated by the operation is discussed by evaluating the entropy. Thermalization of the bipartite spin- 1 / 2 system is discussed as an illustrative example. Comment: 10 pages, no figure...|$|R
50|$|Points are {{earned in}} a variety of ways and can be won {{in a variety}} of match types. 10 points for {{submission}} victory, 7 points for a pinfall victory, 5 points for a countout victory, 3 points for a DQ victory, 2 points for a draw, and minus 10 points for a DQ <b>loss.</b> <b>Special</b> matches can also be made where the winner of the match can be awarded up to 25 points.|$|R
40|$|ABSTRACT: The {{main purpose}} of the present paper is to make an {{appraisal}} of the existing Power Distribution Sector in India with <b>special</b> focus on <b>loss</b> reduction and efficiency improvement of power supply. Different major aspects of technical and non-technical losses have been identified and on the basis of that a number of remedial measures have been suggested for loss reduction and to facilitate the improvement of overall efficiency of the power distribution system. This may provide further inputs to energy planners and managers...|$|R
40|$|I am {{privileged to}} be in this {{interesting}} place honoring Ettore Majorana. Of course I have had no personal contact with him — he disappeared before I appeared. However, it is surprising, that I did not encounter his name nor his achievements during my physics education. He is hardly mentioned in the usual textbooks, at least in the American ones. This is a great loss! To me it was a <b>special</b> <b>loss</b> for the following reason. When I was preparing with Hans Bethe our quantum mechanics textbook, I became fascinated by the Thomas-Fermi theory, and I strived to give a complete discussion in our text. But at that time I knew nothing about Majorana’s work in this area, and so could not include it. Again, when we were writing the chapters on Dirac theory, I wondered why only charged fermions are considered. The resolution of my puzzlement lay in the Majorana representation, about which I learned only later, principally through Julian Schwinger’s writings. Schwinger apparently appreciated the Majorana approach and in his discussions of Dirac theory, the charge carrying fermion field is usually presented as the complex superposition of two real fields, in complete analogy to the description of charged boson fields. Another connection between Majorana and Schwinger can be noted. The last topic that Schwinger researche...|$|E
40|$|Abstract—Data loss is {{ubiquitous}} in {{wireless sensor networks}} (WSNs) mainly due to the unreliable wireless transmission, which results in incomplete sensory data sets. However, the completeness of a data set directly determines its availability and usefulness. Thus, sensory data recovery is an indispensable operation against the data loss problem. However, existing solutions cannot achieve satisfactory accuracy due to <b>special</b> <b>loss</b> patterns and high loss rates in WSNs. In this work, we propose a novel sensory data recovery algorithm which exploits the spatial and temporal joint-sparse feature. Firstly, by mining two real datasets, namely the Intel Indoor project and the GreenOrbs project, we find that: (1) for one attribute, sensory readings at nearby nodes exhibit inter-node correlation; (2) for two attributes, sensory readings at the same node exhibit inter-attribute correlation; (3) these inter-node and inter-attribute correlations can be modeled as the spatial and temporal joint-sparse features, respectively. Secondly, motivated by these observations, we propose two Joint-Sparse Sensory Data Recovery (JSSDR) algorithms to promote the recovery accuracy. Finally, real data-based simulations show that JSSDR outperforms existing solutions. Typically, when the loss rate is less than 65 %, JSSDR can estimate missing values with less than 10 % error. And when the loss rate reaches as high as 80 %, the missing values can be estimated by JSSDR with less than 20 % error. Index Terms—Wireless sensor networks, data loss, sensory data recovery, joint-sparse, compressive sensing I...|$|E
40|$|This paper first {{refers to}} the key concept of {{recognition}} of asset losses under the corporate tax law. The tax law basically restricts the loss deduction and imposes requirements of "settlements" with a fact of physical or monetary damage for the <b>special</b> <b>loss</b> deduction unless potential nonrecognized losses may be deducted under the accounting standards {{from the viewpoint of}} disclosure for asset fair values. This loss deduction rule is derived from the foreseeability and legal stability in calculation of taxable income. This paper secondly explains the content and legislative context of the recent amendments in the depreciation system and allowance expenses. Some allowance systems has been repealed in order to enlarge the tax base and increase the tax revenue, however, the accelerated depreciation, newly introduced system in the recent corporate tax reform, brought a broad accrual expense, where it caused an opposite result to the tax base. I would rather mention the background of the past tax reforms and suggest the range of estimated accrual expenses should be more broadened. Finally, this paper would clarify the contemporary signification in the loss deduction rule. ASBJ has issued the cumulative accounting standards for the global convergence, and is now required the final decision for the IFRS adoption. It would likely be said that harmonization between the accounting and tax enforcement would be continuously pursed through this convergence process. The loss deduction rule would have a vital role in the fair value measurement in tax accounting, where tax income and each tax item on a balance sheet are measured by an index with high accuracy and legal settlement...|$|E
40|$|An {{approach}} is described for tracking the stationary {{point of a}} response curve {{in the presence of}} observation noise and random disturbances of the extremum. Unlike many such schemes, the one considered requires only infrequent alterations to the setting, these changes being made when high accuracy can be anticipated. A <b>special</b> family of <b>loss</b> functions is investigated and the case of quadratic loss is considered in detail, the performance being compared with that of a derivative-sensing scheme. Remarks are made about possible extensions. 1...|$|R
40|$|In {{this paper}} the authors shows the {{research}} made for improving high-power audio speaker devices performance using permanent NdFeB magnets <b>special</b> technology. Magnetic <b>losses</b> inside these audio devices {{are due to}} mechanical system frictions and to thermal effect of Joules eddy currents. In this regard, by special technology, were made conical surfaces at top plate and center pin. Analysing results obtained by modelling the magnetic circuit finite element method using electronic software package,was measured increase efficiency by over 10 %, from 1, 136 T to 13 T...|$|R
40|$|Non-Markovian local in time master {{equations}} give {{a relatively}} simple way to describe the dynamics of open quantum systems with memory effects. Despite their simple form, {{there are still many}} misunderstandings related to the physical applicability and interpretation of these equations. Here we clarify these issues both in the case of quantum and classical master equations. We further introduce the concept of a classical non-Markov chain signified through negative jump rates in the chain configuration. Comment: <b>Special</b> issue on <b>loss</b> of coherence and memory effects in quantum dynamics, J. Phys. B., to appea...|$|R
40|$|The {{dissertation}} {{deals with}} some empirical Bayes test procedures and statistical selection and ranking procedures. ^ The first two chapters {{are related to}} empirical Bayes test procedures. In the first chapter, empirical Bayes tests in some general cases are investigated. Under the traditional empirical Bayes framework, the conditional distributions of observations at different stages are independent and identical. Only keeping the independence assumption, we develop a new general approach for constructing asymptotic optimal empirical Bayes tests for nonidentical cases as well as identical cases. Convergence rates of the constructed empirical Bayes tests are studied. In two special cases, the best convergence rates can be achieved for the empirical Bayes tests by this approach. Moreover, simulations for these two cases demonstrate prominent results. Chapter 2 applies the general approach in Chapter 1 to the double exponential distribution. A special relation between the prior distribution function and the marginal distribution function for the double exponential distribution is found and used to construct empirical Bayes tests for this distribution. ^ The other two chapters are related to statistical selection. In Chapter 3, isotonic subset selection procedures for selecting populations better than a standard are investigated for the double exponential location parameter problem. These isotonic procedures are compared based on expected number of bad populations in the selected subset. All these selection procedures are optimal {{with respect to a}} <b>special</b> <b>loss</b> function. To develop selection procedures which are optimal for a class of loss functions, the Bayes P* subset selection procedures are proposed and verified to the problem of selecting populations better than a standard in Chapter 4. Implementations of these subset selection procedures to normal, binomial and Poisson distributions are presented. ...|$|E
40|$|ABSTRACT: We {{show that}} the {{asymmetric}} effects of income taxes and special items for profit and loss firms contribute substantially to a discontinuity at zero {{in the distribution of}} earnings. Income taxes draw profits towards zero while <b>special</b> items pull <b>loss</b> observations away from zero. These earnings components are thus expected to contribute to a discontinuity {{even in the absence of}} discretion. We show our results are not an artifact of deflation, and that other prominent components of earnings do not have similar affects on the earnings distribution around zero...|$|R
40|$|In {{this paper}} {{the task of}} emotion {{recognition}} from speech is considered. Proposed approach uses deep recurrent neural network trained on a sequence of acoustic features calculated over small speech intervals. At the same time <b>special</b> probabilistic-nature CTC <b>loss</b> function allows to consider long utterances containing both emotional and unemotional parts. The effectiveness of such an approach is shown in two ways. First one is the comparison with recent advances in this field. While second way implies measuring human performance on the same task, which also was done by authors...|$|R
40|$|This paper {{considers}} {{the problem of}} point prediction based on a predictive distribution, representing the uncertainty about the outcome. The issue explored is the reporting of a single characteristic, typically the mean, the median or the mode, {{in the context of}} a skewed distribution and asymmetric <b>loss.</b> <b>Special</b> attention is given to the two-piece normal distribution and asymmetric piecewise linear and quadratic loss. The practical context for the issue is the yearly reporting of remaining petroleum resources given by the authorities to stakeholders that may ask for just a single number...|$|R
40|$|We {{present a}} class of {{flexible}} and tractable static factor models for the term structure of joint default probabilities, the factor copula models. These high dimensional models remain parsimonious with pair copula constructions, and nest many standard models as <b>special</b> cases. The <b>loss</b> distribution of a portfolio of contingent claims can be exactly and efficiently computed when individual losses are discretely supported on a finite grid. Numerical examples study the key features affecting the loss distribution and multi-name credit derivatives prices. An empirical exercise illustrates the flexibility of our approach by fitting credit index tranche prices. Comment: 31 pages, 8 figures, 2 table...|$|R
50|$|A Biography channel <b>special</b> {{detailed}} the <b>loss</b> of Elton's voice in 1986 while on tour in Australia. Shortly thereafter he underwent throat surgery, which permanently altered his voice. Several non-cancerous polyps {{were removed from}} his vocal cords, resulting in a change in his singing voice. In 1987, he won a libel case against The Sun which published false allegations of sex with rent boys. In 1988, he performed five sold-out shows at Madison Square Garden in New York, giving him 26 for his career. Netting over , 2,000 items of Elton John's memorabilia were auctioned off at Sotheby's in London.|$|R
