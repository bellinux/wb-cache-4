16|33|Public
40|$|In this paper, {{we present}} a hybrid text {{segmentation}} approach for embedded text in images, aiming to combining {{the advantages of the}} difference-based methods and the similarity-based methods together. First a new <b>stroke</b> <b>edge</b> filter is applied to obtain <b>stroke</b> <b>edge</b> map. Then a two-threshold method based on the improved Niblack thresholding technique is utilized to identify stroke edges. Those pixels between the edge pairs above the high threshold are collected to estimate the representative of stroke color, so that stroke pixels are further extracted by computing the color similarity. Finally some heuristic rules are devised to integrate <b>stroke</b> <b>edge</b> and stroke region information to obtain better segmentation results. The experimental results show that our approach can effectively segment text from background. Index Terms—Text segmentation, edge filter...|$|E
30|$|For {{what was}} {{mentioned}} above, {{to recover the}} over-smoothed <b>stroke</b> <b>edge</b> details, guided filter is employed in this section.|$|E
40|$|Abstract — Revamping {{of ancient}} {{degraded}} document images is a grueling task due their foreground text and background which is degraded due to uneven illumination, dust, water marks, smear, strain, ink bleed and low contrast etc. The proposed Binarization technique addresses {{this problem by}} using adaptive image contrast which {{is a combination of}} the local image gradient and local image contrast that is stoic to text and background variation. In the proposed technique, for an input ancient degraded document image an adaptive contrast map is first constructed. The contrast map is then binarized and combined with Canny’s edge map to recognize the text <b>stroke</b> <b>edge</b> pixels. The text of document is further segmented by a local threshold that is concluded based on the intensities of detected text <b>stroke</b> <b>edge</b> pixels within a local window. Dataset of different languages like Modi, Marathi and English are used as input in handwritten and printed form. Modi, Marathi, English database are from year 1908, 1957, 1922. The proposed system is simple, required minimum parameter tuning, and give the superior performance compared with other techniques...|$|E
5000|$|Patrick Chan started skating at age five. He {{originally}} {{wanted to}} learn to skate to play hockey, but soon became interested in figure skating. His coach, Osborne Colson, made him spend 30 minutes a day on basic <b>stroking,</b> <b>edge</b> work, cross-cutting and balance drills. Chan said, [...] "I tell people I owe the flow in my knees and the flow I generate from my edges to Mr. Colson. He {{knew he had to}} pull everything apart and start from the ground up on the basics of skating." ...|$|R
40|$|We {{present an}} {{approach}} to synthesize the subtle 3 D re-lief and texture of oil painting brush strokes from a single photograph. This task is unique from traditional synthesize algorithms due to its mixed modality between the input and output; i. e., {{our goal is to}} synthesize surface normals given an intensity image input. To accomplish this task, we pro-pose a framework that first applies intrinsic image decom-position to produce a pair of initial normal maps. These maps are combined into a conditional random field (CR-F) optimization framework that incorporates additional in-formation derived from a training set consisting of normals captured using photometric stereo on oil paintings with sim-ilar brush styles. Additional constraints are incorporated into the CRF framework to further ensures smoothness and preserve brush <b>stroke</b> <b>edges.</b> Our results show that this ap-proach can produce compelling reliefs that are often indis-tinguishable from results captured using photometric stere-o. 1...|$|R
40|$|Abstract — Document images often {{suffer from}} {{different}} types of degradation that renders the document image binarization a challenging task. Document image binarization is usually performed in the pre-processing stage of different document image analysis applications. Different image pre-processing methods for document image are compared. The Total Variation method of pre-processing shows the best performance over a variety of methods. A robust method of document image denoising by NLM filter is also presented. Different types of document degradation are then compensated by using the estimated document background surface. The document text is segmented by a local threshold that is estimated based on the detected text <b>stroke</b> <b>edges.</b> A self-training learning framework for document image binarization is proposed. Based on reported binarization methods, the proposed framework divides document image pixels into three categories, namely, foreground pixels, background pixels and uncertain pixels. A classifier is then trained by learning from the document image pixels in the foreground and background categories. Finally, the uncertain pixels are classified using the learned pixel classifier...|$|R
40|$|Abstract—Segmentation of {{text from}} badly {{degraded}} document images {{is a very}} challenging task due to the high inter/intravariation between the document background and the foreground text of different document images. In this paper, we propose a novel document image binarization technique that addresses these issues by using adaptive image contrast. The adaptive image contrast {{is a combination of}} the local image contrast and the local image gradient that is tolerant to text and background variation caused by different types of document degradations. In the proposed technique, an adaptive contrast map is first constructed for an input degraded document image. The contrast map is then binarized and combined with Canny’s edge map to identify the text <b>stroke</b> <b>edge</b> pixels. The document text is further segmented by a local threshold that is estimated based on the intensities of detected text <b>stroke</b> <b>edge</b> pixels within a local window. The proposed method is simple, robust and involves minimum parameter tuning. It has been tested on three public datasets that were used in the recent Document Imag...|$|E
40|$|Abstract- Segmentation of {{text from}} poorly {{documented}} images {{is a very}} difficult task due to high mutation between the document background and foreground text of various document images. In this paper, a binarization technique is significantly designed for historical document images. This existing binarization technique points either on finding an appropriate global threshold for each area in order to remove strains, smear and uneven illuminations. In binarization process an adaptive contrast map is first constructed for an input degraded document image. Adaptive image contrast is a combination of local image contrast and local image gradient. This contrast map is then binarized and combined with Canny’s edge map to detect text <b>stroke</b> <b>edge</b> pixels. The document is further segmented by local threshold that is estimated based on the intensities of detected text <b>stroke</b> <b>edge</b> pixel within that local window. This method is simple, robust and includes minimum parameter tuning. Our approach applies a global threshold and detects image areas that are more likely to still contain noise. Each of these areas is reprocessed separately to achieve better quality of binarization...|$|E
40|$|Document image {{binarization}} {{is a vital}} pre-processing {{technique for}} document image analysis that segments text from badly degraded document images. In this paper, we propose a robust document image binarization technique {{that is based on}} the concept of adaptive image contrast. The adaptive image contrast which is formed by combining local image contrast and the local image gradient makes it tolerant to text and background variation caused by different types of document degradations. In the proposed technique the adaptive contrast map is binarized and text <b>stroke</b> <b>edge</b> pixels are detected using Canny’s algorithm. The document text is further segmented by a local threshold that is assessed in light of the intensities of detected text <b>stroke</b> <b>edge</b> pixels within a local window. The above mentioned process has been rehashed by combining adaptive image contrast with Sobel’s Edge detection technique and Total Variation Edge Detection technique respectively A comparison between these techniques is then {{made on the basis of}} Peak-signal to Noise Ratio and Mean Square Error values. These methods have been tested on images suffering from different types of degradations. It has been found out that adaptive image contrast used with Canny’s edge detection technique gives the best results. General Terms Document image analysis, bimodal pattern, edge detection, segmentation...|$|E
50|$|Torgashev {{fractured}} {{his right}} ankle in June 2015 while practicing a quad toe loop. As a result, {{he missed the}} entire 2015-16 skating season. He worked on his <b>edges,</b> <b>stroking</b> and speed after returning to the ice. He returned to competition in July 2016.|$|R
50|$|Saffron {{exploits}} {{the inherent}} properties of distance fields to provide continuous stroke modulation (CSM), the continuous modulation of both <b>stroke</b> weight and <b>edge</b> sharpness. This {{allows users to}} tune the appearance of text to suit individual preference. For example, sharper, softer, thinner, and thicker {{versions of the same}} typeface can be rendered by using different CSM parameters.|$|R
50|$|In this work, the artist's {{skills are}} {{demonstrated}} through the vigorous drawing and meticulous <b>strokes</b> on the <b>edges</b> {{as well as}} the sculptural depiction of the figures which is established by using light, contrasting and intense color. The Conversion of Saint Paul shows the debt owed by Maíno towards Italian painting and the artistic stimulation he found in Rome, where he lived between 1605 and 1610.|$|R
40|$|Nowadays a very {{difficult}} is Segmentation of text from degraded document images due to the high inter/intra variation between the document background and the foreground text of different document images. In this paper, we propose a novel canny’s map binarization technique that addresses these issues by using adaptive image contrast. In the proposed system using canny’s map detector to detect text stroke and background estimate using post processing procedure for degraded document images. The adaptive image contrast {{is a combination of}} the local image contrast and the local image gradient that is tolerant to text and background variation caused by different types of document degradations. In the proposed method, an contrast map is first constructed for an input degraded image. The contrast map is then binarized into 0 ’s and 1 ’s and combined with Canny’s edge map to detect the text <b>stroke</b> <b>edge</b> pixels. The document text is segmented by a local threshold value that is estimated based on the intensities of detected text <b>stroke</b> <b>edge</b> pixels within a local window using edge detection techniques. The proposed method is simple, robust, and involves many parameter. Experiments on the Bickley diary dataset that consists of several challenging bad quality document images also show the superior performance. Keywords::-Adaptive image contrast, document analysis, document image processing, degraded document image binarization,pixel classification...|$|E
40|$|Segmentation of {{text from}} badly {{degraded}} document images {{is a very}} challenging task due to the high inter/intravariation between the document background and the foreground text of different document images. In this paper, we propose a novel document image binarization technique that addresses these issues by using adaptive image contrast. The adaptive image contrast {{is a combination of}} the local image contrast and the local image gradient that is tolerant to text and background variation caused by different types of document degradations. In the proposed technique, an adaptive contrast map is first constructed for an input degraded document image. The contrast map is then binarized and combined with Canny’s edge map to identify the text <b>stroke</b> <b>edge</b> pixels. The document text is further segmented by a local threshold that is estimated based on the intensities of detected text <b>stroke</b> <b>edge</b> pixels within a local window. The proposed method is simple, robust and involves minimum parameter tuning. It has been tested on three public datasets that were used in the recent Document Image Binarization Contest (DIBCO) 2009 & 2011 and Handwritten Document Image Binarization Contest (H-DIBCO) 2010 and achieves accuracies of 93. 5 %, 87. 8 % and 92. 03 %, respectively, that are significantly higher than or close to that of the best-performing methods reported in the three contests. Experiments on the Bickley diary dataset that consists of several challenging bad quality document images also show the superior performance of our proposed method, compared with other techniques...|$|E
40|$|Abstract: Degraded {{document}} {{images is}} very difficult to segment in text format due the high inter /intra variation between the document background and foreground text from different document images. In this paper we propose a binarization technique for recovering degraded document images. In the proposed technique firstly adaptive image contrast map is constructed by giving input degraded document images and detect the text <b>stroke</b> <b>edge</b> pixel. Text of the document is segmented by using local threshold estimation then further applying the post processing to improve document binarization quality. The proposed method is simple, effective and involves minimum parameter tuning. Key Words: Degraded document images, adaptive image contrast, binarization technique...|$|E
50|$|Players are {{permitted}} to play the ball {{with the flat of}} the 'face side' and with the edges of the head and handle of the field hockey stick with the exception that, for reasons of safety, the ball may not be struck 'hard' with a forehand <b>edge</b> <b>stroke,</b> because of the difficulty of controlling the height and direction of the ball from that stroke.|$|R
40|$|We look at {{correcting}} {{breaks in}} the Pen-Opposed Extended <b>Edge</b> <b>Strokes,</b> particularly from crossed pen trails. 1 Introduction In [13], we extracted <b>strokes</b> from extended <b>edges.</b> Unfortunately, and as shown by Figure 1. 1 there are broken strokes due to failures in the original extended edges, and at pen crossings. Our goal is to correct for these breaks. 2 Prediction Masking We propose to use the ends of each stroke to predict future behaviour. The prediction will be formed as a binary image through which other strokes can be masked. Most of the difficulty is in defining {{the area of the}} stroke from which we will be making our prediction, and correcting for ink thickness. Making the Prediction We predict the future behaviour of a stroke by linear extrapolation of the ends 1, drawing upon experience from [9]. The process is best viewed graphically, and in the following description, the (Image Δ) refer to images in Figure 2. 2. 1 The original concept was outlined by Chris deSil [...] ...|$|R
2500|$|The scoring of a four or six {{by a good}} {{aggressive}} shot {{displays a}} certain amount of mastery by the batsman over the bowler, and is usually greeted by applause from the spectators. [...] Fours resulting from an <b>edged</b> <b>stroke,</b> or from a shot that did not come off as the batsman intended, are considered bad luck to the bowler. [...] As a batsman plays himself in and becomes more confident as his innings progresses, the proportion of his runs scored in boundaries often rises.|$|R
40|$|Segmentation of badly {{degraded}} document {{images is}} done for discriminating a text from background images {{but it is a}} very challenging task. So, to make a robust document images, till now many binarization techniques are used. But in existing binarization techniques thresholding and filtering is unsolved problem. In the existing method, an Adaptive contrast map is first constructed then binarized and combined with cannny edge map to identify text <b>stroke</b> <b>edge</b> pixels, the documented is further segmented by local threshold. So the existing methods are divided into four main steps out of which last two steps used two different algorithms. In the proposed method, we can modify algorithms and test degraded document images then compare the result that come from previous paper results...|$|E
40|$|Image {{binarization}} is {{the separation}} of each pixel values into two collections, black as a foreground and white as a background. Thresholding technique is used for document image binarization. Image binarization plays vital role in segmentation of text from the document images that are badly degraded due to the high inter variations between the foreground text of document images and document background. This paper, proposes technique to address the issues of degraded images using adaptive image contrast. The adaptive image contrast technique {{is a combination of}} the local image contrast and the local image gradient. And they are tolerant to variation of text and background. Such variations are caused by number of document degradations. The proposed technique, constructs adaptive contrast map for degraded image. the contrast map is combined with Canny’s edge map, for the identification of text <b>stroke</b> <b>edge</b> pixels. Thresholding technique can be applied as global technique and local technique. Global thresholding is suitable for a document where there is uniform contrast delivery of background and foreground. However global thresholding fails to the applications where difference in contrast, Extensive background noise and difference in brightness exists. in such circumstances categorization of many pixels as a foreground or as a background is not so easy. Local thresholding plays significant role in such cases. Local thresholding technique uses local threshold t; w. r. t. local window to segment the document image. this local threshold t is estimated based on the intensities of detected text <b>stroke</b> <b>edge</b> pixels. The proposed method is simple, robust, and involves minimum parameter tuning. It has been tested on three public datasets that are used in the recent document image binarization contest (DIBCO) 2009 & 2011 and handwritten-DIBCO 2010...|$|E
40|$|Document image {{binarization}} is {{of great}} significance in the document image analysis and recognition process because it affects further steps of the recognition process. The variation between the foreground text and the background of different document images is a challenging task. This paper presents a new document image binarization technique that focus on these issues using adaptive image contrast. The adaptive image contrast {{is a combination of}} the local image contrast and the local image gradient so as to tolerate the text and background variation caused by different types of document degradations. In the proposed image binarization technique, an adaptive contrast map is first constructed for an input degraded document image which is then adaptively binarized and combined with Sobel’s edge detector to identify the text <b>stroke</b> <b>edge</b> pixels. The document text is further segmented using a local threshold. Keywords:- Binarization, thresholding, adaptive image contrast, stroke width. I...|$|E
40|$|The {{aim of this}} {{research}} is to automatically process video and still images such that they display the qualities of a traditional impressionist painting. The research discusses the implementation of the techniques put forth in [1]. In addition, some of the techniques put forth in [1] have been adapted or extended, and some small innovations are presented. The techniques discussed include using gradient information to orientate individual brush strokes, clipping each brush <b>stroke</b> to the <b>edges</b> found in the image, and using optical flow to move brush strokes across multiple frames in video sequences...|$|R
5000|$|Like the gladius, the pugio was {{probably}} a stabbing weapon, the type of weapon {{said to have been}} preferred by the Romans. Of them, late Roman writer Vegetius, says: A <b>stroke</b> with the <b>edges,</b> though made with ever so much force, seldom kills, .... On the contrary, a stab, though it penetrates but two inches, is generally fatal. ... the body is covered while a thrust is given, and the adversary receives the point before he sees the sword. This was the method of fighting principally used by the Romans ...|$|R
40|$|International audience—In this work, {{we propose}} to {{recognize}} handwrittenmathematical expressions by merging multiple 1 D sequences oflabels {{produced by a}} sequence labeler. The proposed solutionaims at rebuilding a 2 D expression from several 1 D labeledpaths. An online math expression is a sequence of strokeswhich is later used to build a graph considering both temporal and spatial orders among these strokes. In this graph,node corresponds to <b>stroke</b> and <b>edge</b> denotes the relationshipbetween a pair of strokes. Next, we select 1 D paths from thebuilt graph {{with the expectation that}} these paths could catch allthe strokes and the relationships between pairs of strokes. Asan advanced and strong sequence classiﬁer, BLSTM networksare adopted to label the selected 1 D paths. We set differentweights to these 1 D labeled paths and then merge them torebuild a label graph. After that, an additional post-processwill be performed to complete the edges automatically. We testthe proposed solution and compare the results to the state ofart in online math expression recognition domain...|$|R
30|$|After the L 0 {{gradient}} minimization operation, we {{obtain a}} random-noise-free image, {{as shown in}} Fig.  3 b. However, we find that some <b>stroke</b> <b>edge</b> details are removed as random noise by the L 0 minimization operation, {{as indicated by the}} yellow box in Fig.  3. The guided filter is an edge-preserving filter [16]. Its goal is to smooth input images by calculating the content of the guidance image. Many studies shows that edges in an image after using guided filter will change differently. For step edges, it is still step edges after using guided filter, but their ranges become smaller, which means that the step edges become smoother after guided filter; For ridge edges, if the ridge edges with small size are unaffected by the other edges, their variances are close to 0, then the ridge edges will disappear and tend to the background; Valley edges will become larger than the input. From what was mentioned above, we can see that the guided filter has well preserving ability on image edges. Therefore, it can be used for image texture recover.|$|E
40|$|Abstract—Degraded {{document}} {{images are}} often suffered from {{different types of}} degradation that renders the document image binarization a challenging task. The handwritten text within the degraded documents often shows {{a certain amount of}} variation in terms of the stroke width, stroke brightness, stroke connection, and document background {{these are some of the}} issues in the degraded document images. A novel document image binarization technique that segments the text from badly degraded document images accurately. A novel document image binarization technique that addresses these issues by using synergized image contrast. The synergized image contrast is a combination of the local image contrast and the local image gradient that is tolerant to text and background variation caused by different types of document degradations. For a given document image, initially the contrast map is constructed. The contrast map is then combined with the improved Canny’s edge map to identify the text <b>stroke</b> <b>edge</b> pixels. A new edge expansion model is presented for recovering broken edges of the words or characters on the front side. The document text is further segmented by a local threshold. Aim of this project is to extract clear textual image in simple and efficient manner...|$|E
40|$|Document images often {{suffer from}} {{different}} types of degradation that renders the document image binarization a challenging task. This paper presents a document image binarization technique that segments the text from badly degraded document images accurately. The proposed technique {{is based on the}} observations that the text documents usually have a document background of the uniform color and texture and the document text within it has a different intensity level compared with the surrounding document background. Given a document image the proposed technique first estimates a document background surface through an iterative polynomial smoothing procedure. Different types of document degradation are then compensated by using the estimated document background surface. The text <b>stroke</b> <b>edge</b> is further detected from the compensated document image by using L 1 -norm image gradient. Finally, the document text is segmented by a local threshold that is estimated based on the detected text stroke edges. The proposed technique was submitted to the recent Document Image Binarization Contest (DIBCO) held under the framework of IC-DAR 2009 and has achieved the top performance among 43 algorithms that are submitted from 35 international research groups...|$|E
40|$|International audienceIn this paper, {{we develop}} a {{comprehensive}} representation model for handwriting, which contains both morphological and topological information. An adapted Shape Context descriptor built on structural points is employed {{to describe the}} contour of the text. Graphs are first constructed by using the structural points as nodes and the skeleton of the <b>strokes</b> as <b>edges.</b> Based on graphs, Topological Node Features (TNFs) of n-neighbourhood are extracted. Bag-of-Words representation model based on the TNFs is employed to depict the topological characteristics of word images. Moreover, a novel approach for word spotting application by using the proposed model is presented. The final distance is a weighted mixture of the SC cost, and the TNF distribution comparison. Linear Discriminant Analysis (LDA) is used to learn the optimal weight for {{each part of the}} distance with the consideration of writing styles. The evaluation of the proposed approach shows the significance of combining the properties of the handwriting from different aspects...|$|R
40|$|International audienceIn this study, we {{extend the}} chain-{{structure}}dBLSTM to tree structure topology and apply this new networkmodel for online math expression recognition. The proposedsystem addresses the recognition task as a graph buildingproblem. The input expression is {{a sequence of}} strokes fromwhich an intermediate graph is derived using temporal andspatial relations among strokes. In this graph, a node correspondsto a <b>stroke</b> and an <b>edge</b> denotes {{the relationship between a}} pairof strokes. Then several trees are derived from the graph andlabeled with Tree-based BLSTM. The last step is to merge theselabeled trees to build an admissible label graph (LG) modeling 2 -D formulas uniquely. The proposed system achieves competitiveresults in online math expression recognition domain...|$|R
40|$|Pattern {{recognition}} {{is an important}} step in map generalization. Pattern recognition in street network is significant for street network generalization. A grid is characterized by a set of mostly parallel lines, which are crossed by a second set of parallel lines with roughly right angle. Inspired by object recognition in image processing, this paper presents an approach to the grid recognition in street network based on graph theory. Firstly, the bridges and isolated points of the network are identified and deleted repeatedly. Secondly, the similar orientation graph is created, in which the vertices represent street segments and the edges represent the similar orientation relation between streets. Thirdly, the candidates are extracted through graph operators such as finding connected component, finding maximal complete sub-graph, join and intersection. Finally, the candidate are evaluated by deleting bridges and isolated lines repeatedly, reorganizing them into stroke models, changing these stroke models into street intersection graphs in which vertices represent <b>strokes</b> and <b>edges</b> represent <b>strokes</b> intersecting each other, and then calculating the clustering coefficient of these graphs. Experimental result shows the proposed approach is valid in detecting the grid pattern in lower degradation situation...|$|R
40|$|Abstract—This paper {{presents}} a recognition {{system based on}} Hidden Markov Model (HMM) for isolated online handwritten mathematical symbols. We design a continuous left to right HMM for each symbol class and use four online local features, including a new feature: normalized distance to <b>stroke</b> <b>edge.</b> A variant of segmental K-means is used to get initialization of the Gaussian Mixture Models ’ parameters which represent the observation probability distribution of the HMMs. The system obtains top- 1 recognition rate of 82. 9 % and top- 5 recognition rate of 97. 8 % on a dataset containing 20281 training samples and 2202 testing samples of 93 classes of symbols. For multistroke symbols, the top- 1 recognition rate is 74. 7 % and the top- 5 recognition rate is 95. 5 %. For single-stroke symbols, the top- 1 recognition rate is 86. 8 % and the top- 5 recognition rate is 98. 9 %. (MacLean et al., 2010) applied dynamic time warping algorithm on all the 70 classes of single-stroke symbols. Their top- 1 recognition rate is 85. 8 %, and top- 5 recognition rate is 97. 0 %. Our system gets top- 1 recognition rate of 85. 5 % and top- 5 recognition rate of 99. 1 % on the same 70 classes of singlestroke symbols. Keywords-Hidden Markov Model; mathematical symbol recognition; segmental K-mean...|$|E
40|$|This paper {{describes}} a computational approach to style conversion of cartoon animation. Our system works in two stages: the internal color shading phase, in which characters painted by the conventional method are segmented into their constituent regions and edges first, textures painted using different media are then {{added to the}} identified regions, and the line rendering phase, in which curvature of edges detected during the first phase is calculated and used to pick up turning points from <b>edges,</b> <b>strokes</b> with defined shapes and textures are then placed in the positions determined by turning points along the contour as well as internal lines of characters. Our system obviates the tedium of creating more expressive rendering by hand, and keeps frame-to-frame coherence in the resultant animations...|$|R
5000|$|New Zealand greenshell mussel {{are often}} parasitized by pea crabs. In 2015, New Zealand {{researchers}} Oliver Trottier and Andrew Jeffs from University of Auckland studied [...] the mate location behaviour of male New Zealand pea crabs, which were observed when dwelling in the mussel. Given the cryptic behaviour {{of the male}} crabs, a trapping system was developed to determine whether male crabs would exit their mussel hosts {{in response to an}} upstream female crab. Observations of the nocturnal mate-finding behaviour of male crabs were made in darkness using infrared video recordings. Male crabs were often observed <b>stroking</b> the mantle <b>edge</b> of the mussel whilst attempting to gain entry, successfully increasing mussel valve gape during entry from 3.7 to 5.5 mm.|$|R
50|$|Due {{to their}} nocturnal and secretive nature, these frogs are not {{commonly}} seen. The {{best time to}} see western chorus frogs is on warm nights {{when they come out}} to call. Both males and sometimes females call in large choruses. Males use a special call to attract several potential mates to breeding sites. The western chorus frog call can be heard from half a mile away. The call is a very distinct “cree-ee-ee—eeek”, but can be confused with the upland chorus frog. Calling can occur for 0.5-2.0 seconds and can occur 18-20 times in a minute. The higher the temperature, the more frequent calls occur in a minute, (30-90 calls per minute). One can imitate the “cree-ee-eek” call of the western chorus frog by <b>stroking</b> the <b>edge</b> of a pocket comb, but not that well.The western chorus frog relies heavily on secrecy to keep themselves safe from predators. Any disturbances to the frog's environment causes them to stop calling and dive into the depths of whatever water source they reside near, under leaf litter, logs, rocks, or loose soil, for minutes. These terrestrial hiding spots serve not only as hiding places, but also as hibernation places for the frogs during the winter.|$|R
5000|$|There {{have been}} only three parts of a hockey stick ever named in the rules: the head, the handle, and the splice. Originally (until 2004) the handle was the part above the bottom end of the splice and the head was the part below the bottom end of the splice. Other terms in common use are [...] "grip", which refers to {{the part of the}} stick held, {{particularly}} that area held with two hands when hitting the ball. Most sticks have a round grip which is covered in a non-slip, sweat absorbent, fabric tape. The handle remains rounded on the reverse, back or right hand-side but becomes gradually flat on the [...] "face" [...] side and also becomes wider, changing from a diameter of approximately 30 mm to a flat width of approximately 46 mm (the permitted maximum was 2 in—now 51 mm). This flat area above the curve of the head is generally referred to as the [...] "shaft". The head of the stick is generally thought of as the curved part. The right side is called the face, the upturn the [...] "toe" [...] and the bend of the head where it joins the shaft the [...] "heel". In recent times using the edges of the stick (as well as the face side) to strike at the ball has been permitted and thus [...] "forehand edge stroke" [...] and [...] "reverse edge stroke" [...] will be found in rule terminology. Forehand and reverse stroke refers to the taking of these strokes, from the right or left hand side of the body respectively, as the stick may be used [...] "face up" [...] or [...] "face down" [...] to make an <b>edge</b> <b>stroke,</b> the two <b>edges</b> of the stick are not separately named but simply referred to as edges. Initially there were six rule requirements applying to the hockey stick: ...|$|R
40|$|Traditional oil {{painting}} works enable the spectators {{to feel the}} various impressions {{because it can be}} shown differently by the changes of light effect. The reason of {{oil painting}}’s distinguishing feature is that it contains the texture and volume of color expressed by thickness of used pigments and brushing. In this paper, we present a novel method that reproduces oil painting-like image from a source picture based on a virtual light and nonphotorealistic rendering technique. To generate the oil paintinglike image as an output, the system first performs stroke distribution, which is to determine where a brush is located for <b>stroking,</b> using <b>edge</b> detection and image segmentation on an input picture. And the intermediate image is constructed with the suitable color, orientation and size of brush at each stroke point. At last, the system applies light effect to the intermediate image and generates the oil painting-like image. effect generates the oil painting-like image as an output of our system. We give a detailed description about our algorithm in Section 2. The experimental results are illustrated in Section 3. We then conclude the paper in Section 4. 2 PROPOSED SYSTEM This paper proposes a non-photorealistic rendering system to create an oil painting-like image from an input picture. The system consists of three modules, stroke distribution, painterly rendering, and intermediate image transformation. Figure 1 shows the process of proposed system...|$|R
