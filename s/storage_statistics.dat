6|24|Public
5000|$|In 2006, ISE and Goldman Sachs jointly {{acquired}} {{all of the}} Longitude {{assets to}} conduct auctions on economic data options {{in partnership with the}} Chicago Mercantile Exchange (CME) and energy <b>storage</b> <b>statistics</b> in partnership with the New York Mercantile Exchange.|$|E
40|$|International audienceGlobal {{sensitivity}} analysis {{is an important}} step for analyzing and validating numerical simulations. One classical approach consists in computing statistics on the outputs from well-chosen multiple simulation runs. Simulation results are stored to disk and statistics are computed postmortem. Even if supercomputers enable to run large studies, scientists are constrained to run low resolution simulations with a limited number of probes to keep the amount of intermediate storage manageable. In this paper we propose a file avoiding, adaptive, fault tolerant and elastic framework that enables high resolution global {{sensitivity analysis}} at large scale. Our approach combines iterative statistics and in transit processing to compute Sobol' indices without any intermediate <b>storage.</b> <b>Statistics</b> are updated on-the-fly as soon as the in transit parallel server receives results from one of the running simulations. For one experiment, we computed the Sobol' indices on 10 M hexahedra and 100 timesteps, running 8000 parallel simulations executed in 1 h 27 on up to 28672 cores, avoiding 48 TB of file storage...|$|E
40|$|The {{supply chain}} is {{a network of}} {{suppliers}} and customers offering their products or services to the end consumers. The research aimed at recognizing {{the elements of the}} supply chain of pork in the area of supply, processing and distribution. The information was primarily sourced by the data <b>storage</b> <b>statistics</b> and chosen meat companies listed among the 2000 largest industries in Poland. The domestic supply chain of pork is characterized by a considerable fragmentation of production, processing and distribution. A signifi cant number of entities and the absence of capital tie-ups between the breeders and the meat industry does not foster durable integration. In order to maintain the profi tability of production, the meat establishments diversify their activities. Entities with large production volume more often cooperate with commercial networks, whereas in case of the smaller ones, these are their own distribution networks that prevail. The leaders in the industry are characterized by a wide range of goods, large-scale production, specialization and a closer integration in the supply chain...|$|E
50|$|According to Box Office Mojo's <b>statistics,</b> <b>Storage</b> 24 grossed $646,175 worldwide. It {{entered the}} UK chart at number 13, making $372,153. In Turkey it entered at number 7 in the charts and in Hong Kong it entered at number 10. In the US, it was {{released}} for one day, on one screen, making $72, before being released on DVD and VOD through Magnolia.|$|R
40|$|Abstract. In {{order to}} realize the {{real-time}} monitoring of crop growth environment, this paper applied the Java JDK 6 + Eclipse 3. 5, developed agricultural remote data collection system based on zigbee, greenhouse planting of crops growth environment parameters, such as air temperature, humidity and so on, {{with all kinds of}} sensor dynamic collection, and transmitted to the user through the network client terminal. This makes agricultural personnel through the terminal computer real time control of the growth environment of crops, can take timely control measures, has realized the remote, multiobjective agricultural information, the parameters of the real-time acquisition, display, <b>storage,</b> query, <b>statistics,</b> and other functions, provides a effective way for the agricultural informatization construction...|$|R
50|$|The Neurophysiological Biomarker Toolbox (NBT) is an {{open source}} Matlab toolbox for the {{computation}} and integration of neurophysiological biomarkers (e.g., biomarkers based on EEG or MEG recordings). The NBT toolbox {{has so far been}} used in seven peer-reviewed research articles, and has a broad user base of more than 1000 users. The NBT toolbox provides unique features for analysis of resting-state EEG or MEG recordings.NBT offers a pipeline from data <b>storage</b> to <b>statistics</b> including artifact rejection, signal visualization, biomarker computation, statistical testing, and biomarker databasing. NBT allows for easy implementation of new biomarkers, and incorporates an online wiki (the NBTwiki) that aims at facilitating collaboration among NBT users including extensive help and tutorials. The standardised way of data storage and analysis that NBT proposes allow different research projects to merge, compare, or share their data and biomarker algorithms.|$|R
40|$|The paper {{provides}} information {{on the use of}} liquefied natural gas and distribution in Europe and worldwide. Paper has information displayed on the largest liquefied natural gas exporter and importing countries. View of liquefied natural gas transit, <b>storage</b> <b>statistics</b> and specific features in urope is provided. Has been studied accident statistics in the world linked with liquefied natural gas, as well as the analysis has been made of the most frequent causes of accidents. During this research, calculations was carried out assessing the possible fire of liquefied natural gas (hereinafter LNG) in order to assess its consequences and prevalence, as well as assessing the potential impact of atmospheric in fire spread, also calculations were made to assess the blast wave spread. Other calculations were made in connection with LNG low temperatures and effects that could appear by inhaling evaporated LNG vapours. Research summarizes {{some of the most important}} occupational health and safety risks associated with the LNG storage, accidents and leak consequences, as well as offering some propositions of solving these problems...|$|E
40|$|A {{model for}} multivariate streamflow {{generation}} is presented, {{based on a}} multilayer feedforward neural network. The structure of the model results from two components, the neural network (NN) deterministic component and a random component which {{is assumed to be}} normally distributed. It is from this second component that the model achieves the ability to incorporate effectively the uncertainty associated with hydrological processes, making it valuable as a practical tool for synthetic generation of streamflow series. The NN topology and the corresponding analytical explicit formulation of the model are described in detail. The model is calibrated with a series of monthly inflows to two reservoir sites located in the Tagus River basin (Spain), while validation is performed through estimation of a set of statistics that is relevant for water resources systems planning and management. Among others, drought and <b>storage</b> <b>statistics</b> are computed and compared for both the synthetic and historical series. The performance of the NN-based model was compared to that of a standard autoregressive AR(2) model. Results show that NN represents a promising modelling alternative for simulation purposes, with interesting potential in the context of water resources systems management and optimisation. Keywords: neural networks, perceptron multilayer, error backpropagation, hydrological scenario generation, multivariate time-series. ...|$|E
40|$|The non-Markovian {{dynamics}} of open quantum systems {{is still a}} challenging task, particularly in the non-perturbative regime at low temperatures. While the Stochastic Liouville-von Neumann equation (SLN) provides a formally exact tool to tackle this problem for both discrete and continuous degrees of freedom, its performance deteriorates for long times due to an inherently non-unitary propagator. Here we present a scheme which combines the SLN with projector operator techniques based on finite dephasing times, gaining substantial improvements in terms of memory <b>storage</b> and <b>statistics.</b> The approach allows for systematic convergence and is applicable in regions of parameter space where perturbative methods fail, up to the long time domain. Findings are applied to the coherent and incoherent quantum {{dynamics of}} two- and three-level systems. In the long time domain sequential and super-exchange transfer rates are extracted and compared to perturbative predictions. Comment: 13 pages, 18 figure...|$|R
40|$|Most {{transmitted}} or stored {{information are}} subjected to occasional errors. In most situations, {{the source of this}} information has inherent unstructured redundancy that can be exploited to correct these errors. In addition to the storage requirements, getting the source statistics required to perform the error correction may not be easy. In this paper, we propose and evaluate trained neural nets to transform the unstructured redundancy into a structured one. The new approach, eliminates the need for source <b>statistics</b> <b>storage</b> and also simplifies the decoding process. This idea is applied to correct some of the errors caused by passing a printed Arabic text through an optical character recognition (OCR) device. Simulation results demonstrate the effectiveness of this technique. ...|$|R
40|$|Abstract—We {{consider}} {{the problem of}} efficiently storing n-gram counts for large n over very large corpora. In such cases, the efficient <b>storage</b> of sufficient <b>statistics</b> can have a dramatic impact on system performance. One popular model for storing such data derived from tabular data sets with many attributes is the ADtree. Here, we adapt the ADtree {{to benefit from the}} sequential structure of corpora-type data. We demonstrate the usefulness of our approach on a portion of the well-known Wall Street Journal corpus from the Penn Treebank and show that our approach is exponentially more efficient than the naı̈ve approach to storing n-grams and is also significantly more efficient than a traditional prefix tree. I...|$|R
40|$|Abstract: Comparing {{with the}} old medical record, EMR(Electronic Medical Record) has more {{advantages}} on morden society. While XML plays {{an important part in}} EMR, so people make no efforts to study the XML. There are two data models of XML, which are DTD and XML Schema. Firstly the paper introduces these two data models, then analysis advantages and disadvantages through an instance, finally draws the conclusion. I. EMR and XML EMR (Electronic Medical Record,) fully electronics the traditional handwritten medical record, and provides electronic <b>storage,</b> querying, <b>statistics,</b> data exchange and so on[1]. It includes not only all information in traditional paper record, but also audio-visual graphics and other multimedia information. It is more integrity. However, due to the diversity of information and the complexity of content, also with the development of electronic medical record, there are many issues such as respresentation of data content whicn includes the data structure, medical record storage and many other issues, in which medical records data structure is the most basic problems. Thus xml is an effective means for the description of the medical record. XML (eXtensible Markup Language) is a set of norms created by the World Wide We...|$|R
40|$|The aim of {{this study}} was to {{evaluate}} the knowledge and behavior of dentists regarding toothbrush disinfection. This study included 147 dentists (88 women and 59 men) who were actively employed at a dental school in Ankara, Turkey. Participants were asked to fill out a standard questionnaire, which contained questions regarding their demographics, brushing habits, toothbrush storage and disinfection habits, toothpaste use, knowledge about toothbrush disinfection, and whether they advised their patients about toothbrush <b>storage.</b> Descriptive <b>statistics</b> were calculated, and statistical analyses were performed with t-tests, chi-squared tests, and Fisher exact tests, where appropriate. Among the 147 surveyed dentists, 62. 6 % and 85. 7 % reported that they did not have any knowledge about toothbrush disinfection and did not disinfect their toothbrushes, respectively. However, approximately two thirds of surveyed dentists thought that toothbrush disinfection should be performed by everyone, including healthy individuals. Significant associations were found between knowledge about toothbrush disinfection and the professional title of dentists, how they stored their toothbrushes, and whether their toothbrushes were in contact with each other during storage (p < 0. 05). A minority of dentists reported that they disinfected their toothbrushes...|$|R
40|$|Knowledge and {{behavior}} of dentists in a dental school regarding toothbrush disinfection Abstract: The {{aim of this study}} was to evaluate the knowledge {{and behavior}} of dentists regarding toothbrush disinfection. This study included 147 dentists (88 women and 59 men) who were actively employed at a dental school in Ankara, Turkey. Participants were asked to fill out a standard questionnaire, which contained questions regarding their demographics, brushing habits, toothbrush storage and disinfection habits, toothpaste use, knowledge about toothbrush disinfection, and whether they advised their patients about toothbrush <b>storage.</b> Descriptive <b>statistics</b> were calculated, and statistical analyses were performed with t-tests, chi-squared tests, and Fisher exact tests, where appropriate. Among the 147 surveyed dentists, 62. 6 % and 85. 7 % reported that they did not have any knowledge about toothbrush disinfection and did not disinfect their toothbrushes, respectively. However, approximately two thirds of surveyed dentists thought that toothbrush disinfection should be performed by everyone, including healthy individuals. Significant associations were found between knowledge about toothbrush disinfection and the professional title of dentists, how they stored their toothbrushes, and whether their toothbrushes were in contact with each other during storage (p < 0. 05). A minority of dentists reported that they disinfected their toothbrushes...|$|R
40|$|The {{problem of}} massive-domain stream {{classification}} {{is one in}} which each attribute can take on one {{of a large number of}} possible values. Such streams often arise in applications such as IP monitoring, super-store transactions and financial data. In such cases, traditional models for stream classification cannot be used because the size of the storage required for intermediate <b>storage</b> of model <b>statistics</b> can increase rapidly with domain size. Furthermore, the one-pass constraint for data stream computation makes the problem even more challenging. For such cases, there are no known methods for data stream classification. In this paper, we propose the use of massive-domain counting methods for effective modeling and classification. We show that such an approach can yield accurate solutions while retaining spaceand time-efficiency. We show the effectiveness and efficiency of the sketch-based approach. ...|$|R
40|$|The 3 rd {{version of}} the {{platform}} is working and WP 4, WP 5 and WP 6 tools are deployed as web services. Common Interfaces (CI) and Travelling Object (TO) for new WP 5 and WP 6 have been devised. The Registry, deployed for the 1 st {{version of the}} platform, is operational and has been updated with some new features. It has now more than 120 registered web services. The PANACEA/nmyExperiment portal was deployed to share workflows among users who can then execute those workflows with Taverna. Massive data solutions were developed during the second development cycles {{that have been used}} since then. New <b>statistics,</b> <b>storage</b> and security features have been addressed and studied. This deliverable will present all the work developed for the third version of the platform and its documentation...|$|R
50|$|The bulk of {{the work}} of public health {{informatics}} in the United States, as with public health generally, takes place {{at the state and local}} level, in the state departments of health and the county or parish departments of health. At a state health department the activities may include: collection and <b>storage</b> of vital <b>statistics</b> (birth and death records); collection of reports of communicable disease cases from doctors, hospitals, and laboratories, used for infectious disease surveillance; display of infectious disease statistics and trends; collection of child immunization and lead screening information; daily collection and analysis of emergency room data to detect early evidence of biological threats; collection of hospital capacity information to allow for planning of responses in case of emergencies. Each of these activities presents its own information processing challenge.|$|R
40|$|Most human {{actions are}} a direct {{response}} to stimuli from their five senses. In {{the past few decades}} there has been a growing interest in capturing and storing the information that is obtained from the senses using analog and digital sensors. By storing this data it is possible to further analyze and better understand human perception. While many devices have been created for capturing and storing data, existing software and hardware architectures are aimed towards specialized devices and require expensive high-performance systems. This thesis aims to create a framework that supports capture and monitoring of a variety of sensors and can be scaled to run on low and high-performance systems such as netbooks, laptops and desktop systems. The proposed architecture was tested using aural and visual sensors due to their availability and higher bandwidth requirements compared to other sensors. Four different portable computing devices were used for testing with a varied set of hardware capabilities. On each of the systems the same suite of tests were run to benchmark and analyze CPU, memory, network, and <b>storage</b> usage <b>statistics.</b> From the results it was shown that on all of these platforms capturing data from multiple video, audio and other sensor sources was possible in real-time. Performance was shown to scale based on several factors, but the most important were CPU architecture, network topology and data interfaces used...|$|R
40|$|Abstract: A multivariate {{non-linear}} model for synthetic generation of monthly discharge series is presented. The time series generator is built upon a multilayer {{feedforward neural network}} with an added multivariate random component normally distributed. The usual error backpropagation algorithm is used to train the network, with a sequential training scheme using shuffled patterns for a stochastic search in the weight space. The proposed model generates inputs to a wider methodology developed for simulation of the probabilistic managing of the upper Tagus River system in Spain. The model is validated {{in terms of its}} ability to reproduce some relevant statistics directly related to important features from the water resources system managing point of view, including droughts and <b>storage</b> derived <b>statistics.</b> The generator was inserted into a decision support system, and a variety of possible future hydrological scenarios in the river system were simulated, with particular consideration of demand failures probabilities under different assumptions and previous hydrological states of the system. For comparison purposes, similar experiments were also undertaken using synthetic series generated with a second-order autoregressive multivariate model, AR(2). The results obtained show higher percentages of demand failures when inputs from the artificial neural network generator (ANN) are used. The case study illustrates another practical application of ANN approaches, adequately combined with other frequently used tools in the context of water resources systems planning and management...|$|R
40|$|With {{the rapid}} {{development}} of e-commerce, {{the number of}} goods has become more and more. When commodity screening system is used to store and process mass information, the existing models require all nodes in the distributed system to work in parallel, then the results of each node are integrated to get the final results, the process produces a lot of invalid queries. In order to solve this problem, proposed a new distributed structured data <b>storage</b> method. It <b>statistics</b> the history search results and chooses the high frequency or core columns to be key columns. The data can be stored based key columns and distribute system architecture. Then in the searching stage, only some nodes work when the search refer to key columns. The results show that this method can reduce the tasks and improve the throughout without extra storage consumption...|$|R
40|$|AbstractThe {{coal mine}} flood {{is one of}} five nature {{disasters}} in the mine construction and production process. During the past 20 years, more than 250 mines were flooded, and the direct economic losses reached as high as more than 350 hundred million Yuan. In this article, the regional arrangement of sensors {{in the face of the}} program, considering the water level roadway, aquifer water levels, the top and bottom side pressure and the amount of water before driving and other factors, in combination with hydro-geological information and monitoring data, based on BP neural network and DS theory of levels of evidence coal face integration evaluation model of water bursting. It has been carried on the example analysis in the ore 3 # coal bed in Shanxi, the results of prediction in line with the actual results, and different conditions for many parameters of the mine <b>statistics,</b> <b>storage</b> and evaluation...|$|R
40|$|This paper {{presents}} a new arcing (boosting 1) algorithm called POCA, Parallel Online Continuous Arcing. Unlike traditional arcing algorithms (such as Adaboost), which construct an ensemble by adding and training weak learners sequentially on a round-byround basis, training in POCA is performed over an entire ensemble continuously and in parallel. Since {{members of the}} ensemble are not frozen after an initial learning period (as in traditional arcing) POCA is able to adapt rapidly to non-stationary environments, and because POCA {{does not require the}} explicit <b>storage</b> of exemplar <b>statistics,</b> it is capable of online 2 learning. We present results from experiments conducted using neural network experts which show that POCA is competitive with and more flexible than existing arcing algorithms. 1 Introduction Arcing is an ensemble learning strategy that works by iteratively constructing and combining experts that are increasingly forced to concentrate on "difficult" training [...] ...|$|R
30|$|On {{the other}} hand, heuristics-based {{resource}} management approaches usually count on relevant analytical models to facilitate optimized resource provisions and allocations. For instance, the power models with utilization threshold algorithm [[6]] are argued to assist dynamic VM placement and migration {{so that a}} considerable amount of energy consumption and CO 2 emissions can be reduced compared with static resource allocation approaches. The cognitive trust model with dynamic levels scheduling algorithm [[20]] is proposed as a better resource scheduling technique which rests on resources trustworthiness and reliability parameters matchmaking. The resource allocation and application models with resource allocation and task scheduling algorithms [[21]] are advocated in which real-time task execution information is used to dynamically guide resource allocation actions. The workload prediction models with capacity allocation and load algorithms [[22]] is proposed to minimize overall VM allocation cost while meeting SLA requirements. The cost and time models with deferential evolution algorithms [[23]] would enable generating the optimal tasks schedules to minimize job completion cost and time. The Dual Scheduling of Cloud Services and Computing Resources models with Ranking Chaos algorithm [[24]] are designed to mitigate the inefficient service composition selection and computing resources allocation issues. Additionally, IACRS [[25]] proposes a multi-metric group cloud-ready heuristic algorithm that would deal with compute, network and <b>storage</b> metric <b>statistics</b> simultaneously while performing scaling decisions. InterCloud [[26]] advocates an effective resource scaling approach seen as to distribute workload appropriately across multiple independent cloud data centers without compromising service quality of service (QoS) aspects.|$|R
40|$|Modern {{sequencing}} platforms generate enormous {{quantities of}} data in ever-decreasing amounts of time. Additionally, {{techniques such as}} multiplex sequencing allow one run to contain hundreds of different samples. With such data comes a significant challenge to understand its quality and {{to understand how the}} quality and yield are changing across instruments and over time. As well as the desire to understand historical data, sequencing centres often have a duty to provide clear summaries of individual run performance to collaborators or customers. We present StatsDB, an open-source software package for storage and analysis of next generation sequencing run metrics. The system has been designed for incorporation into a primary analysis pipeline, either at the programmatic level or via integration into existing user interfaces. Statistics are stored in an SQL database and APIs provide the ability to store and access the data while abstracting the underlying database design. This abstraction allows simpler, wider querying across multiple fields than is possible by the manual steps and calculation required to dissect individual reports, e. g. ”provide metrics about nucleotide bias in libraries using adaptor barcode X, across all runs on sequencer A, within the last month”. The software is supplied with modules for <b>storage</b> of <b>statistics</b> from FastQC, a commonly used tool for analysis of sequence reads, but the open nature of the database schema means it can be easily adapted to other tools. Currently at The Genome Analysis Centre (TGAC), reports are accessed through our LIMS system or through a standalone GUI tool, but the API and supplied examples make it easy to develop custom reports and to interface with other packages...|$|R
40|$|The {{spread of}} digital signage and its {{instantaneous}} adaptability of content challenges out-of-home advertising to conduct performance evaluations {{in an online}} fashion. This implies a tremendous increase in the granularity of evaluations {{as well as a}} complete new way of data collection, storage and analysis. In this paper we propose a distributed system for the large-scale online monitoring of poster performance indicators based on the evaluation of mobility data collected by smartphones. In order to enable scalability in the order of millions of users and locations, we use a local data processing paradigm and apply exponential histograms for an efficient <b>storage</b> of visit <b>statistics</b> over sliding windows. In addition to an immediate event centralization we also explore a hierarchical archite cture based on a merging technique for exponential histograms. We provide an evaluation {{on the basis of a}} real-world data set containing more than 300 million GPS points corresponding to the movement activity of nearly 3, 000 persons. The experiments show the accuracy and efficiency of our system...|$|R
40|$|We {{demonstrate}} a generic and extensible cost-based optimization and execution system for XPath queries, named GeCOEX, using a comprehensive suite of query analyzing and administrative tools, named QuOAX. GeCOEX supports many different physical operator implementations and XML storage engines and is agnostic {{to the underlying}} physical data model. Its optimizer is the first generic cost-based optimizer for XPath queries that always picks the cheapest estimated plan, among {{a very large number}} of possible plans, for a wide range of XPath queries and different datasets in a very small fraction of the time required for efficient execution. The QuOAX suite provides administration tools that allow the user to add new – or deactivate already deployed – physical operator implementations, physical operator cost models and rewriting rules and also to make use of different XML <b>storage</b> and XML <b>statistics</b> estimators. QuOAX also provides query plan analysis and visualization tools that allow users to visualize the physical plan chosen by the optimizer or all possible generated physical plans for a given query and to execute any of those plans. QuOAX helps users to i) easily test new XPath processing techniques, comparing them directly with existing ones and identifying the situations to which they show promise, ii) improve the effectiveness of the optimizer and iii) find out the appropriate access methods or indices that are beneficial for a specific workload. Categories and Subject Descriptors...|$|R
40|$|The article {{offers a}} {{scenario}} {{approach to the}} design of learning management system (LMS) structure. It is argued that the LMS structure and it functional components depend on the scenarios of its use in educational process which, in turn, are determined by the type of subject that is based on the main component of education content. There are three categories of the LMS users in the educational process in university: students, teachers, and administrators, and their roles differ from each other. The article provides detailed scenarios of how the LMS is used by various categories of users and describes the structure of the LMS in relation to these scenarios. On the one hand, the LMS represents a structured base of content: that is, it contains theoretical materials as well as tasks and exercises along with certain scales that reflect their difficulty. The weight of these tasks, initially determined by experts, can then be automatically corrected using the methods of mathematical statistics. On the other hand, implementing the described scenarios requires an electronic operating cover of the LMS, among the functions of which is the organization of control and self-checking, as well as providing <b>storage</b> and processing <b>statistics</b> of the LMS use. These statistics should then be automatically turned into the current rating of users which is important for knowledge evaluation and support of learning motivation in students during a semester...|$|R
40|$|The study, {{design and}} {{development}} of a monitoring system for wildlife road crossing problem is addressed in this thesis. Collisions between fauna and vehicles is a relevant issue in several mountain and rural regions and a valuable low-cost solution {{has not yet been}} identified. In particular, the proposed system is composed by a network of sensors installed along road margins, in order to detect wildlife events, (e. g., approaching, leaving or crossing the road), thus to promptly warn the incoming drivers. The sensor nodes communicate wirelessly among the network thus collecting the sensed information in a control unit for data <b>storage,</b> processing and <b>statistics.</b> The detection process is performed by the wireless nodes, which are equipped with low-cost Doppler radars for real-time identification of wildlife movements. In detail, different technologies valuable for solving the problem and related off-the-shelf solutions have been investigated and properly tested in order to validate their actual performance considering the specific problem scenario. A final classification based on specific parameters has allowed identifying the Doppler radar system as the better low-cost technology for contributing to the problem objective. The performance of the proposed system has also been investigated in a real scenario, which has been identified to be the actual pilot site for the monitoring system. This confirms the system capability of movements detection in the road proximity, thus defining a security area along it, where all occurring events may be identified...|$|R
40|$|Introduction: The optimal use of {{information}} technology in health sector requires due attention to human resources training. The {{purpose of this study}} was to determine the future of health information technology positions and professional qualifications of the employees to achieve them. Methods: This qualitative-quantitative study was conducted in 2016. A nonsystematic review of the articles published over the last 10 years was performed in well-known databases and websites using relevant keywords. Positions were extracted and then discussed using the Delphi technique in a panel of experts of 25 members including board members and faculty members of medical universities across the country. Agreedupon positions were confirmed and job descriptions and professional qualifications were identified and compiled. An applied cross-sectional study was conducted on all health information management employees (38 people) of hospitals affiliated with Kashan University of Medical Sciences to determine the existing gap. A researcher-made questionnaire was developed based on the professional qualifications obtained for the expert panel and distributed after being checked for validity. Reliability was approved with Cronbach's alpha (0. 91). Data were analyzed using descriptive statistics in terms of frequency and percentage. Results: The future health information technology positions were found to be health information management, insurance and accounting, information technology, computer applications, and data management. Professional qualifications of statistics and epidemiology, disease classification, information storage and retrieval, health data management, legal considerations and information security, information technology, and software engineering concepts were determined. The most effective qualification was knowledge of storage and retrieval methods. Employees’ skills in statistics and epidemiology were at an average level. Conclusion: New positions are constantly being introduced into the field of health information technology. Continuous curriculum revisions and additional courses for insurance and accounting, data <b>storage</b> and retrieval, <b>statistics</b> and epidemiology are essential...|$|R

