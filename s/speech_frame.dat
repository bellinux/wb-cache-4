129|311|Public
50|$|Background noise spikes {{can often}} be {{confused}} with the <b>speech</b> <b>frame</b> and hence, in order to nullify this issue, a check list for SID computation is Nelapsed >23, old SID is utilized with VAD=0.|$|E
50|$|Whenever a good <b>speech</b> <b>frame</b> is {{detected}} the RX DTX handler shall pass directly to speech decoder.Whenever a lost speech or lost SID frames are detected the substitution or mutation shall be applied.Whenever a valid SID frame result in comfort noise generation.In case of invalid SID frame after consecutive Speech frames the last valid SID frame will be applicable.|$|E
5000|$|The free libgsm codec can encode and decode GSM Full Rate audio. [...] "libgsm" [...] was {{developed}} 1992-1994 by Jutta Degener and Carsten Bormann, then at Technische Universität Berlin. [...] Since a GSM <b>speech</b> <b>frame</b> is 32.5 bytes, this implementation also defined a 33-byte nibble-padded {{representation of a}} GSM frame (which, at a frame rate of 50/s, {{is the basis for}} the incorrect claim that the GSM bit rate is 13.2 kbit/s). This codec can also be compiled into Wine to provide GSM audio support.|$|E
3000|$|... where NCS, NTS, NFS, and NTN {{denote the}} number of {{correctly}} detected <b>speech</b> <b>frames,</b> total <b>speech</b> <b>frames,</b> falsely detected <b>speech</b> <b>frames</b> in silence regions, and total silence frames, respectively. In these experiments, we set N [...]...|$|R
30|$|From Table  3, we {{can observe}} that the {{estimated}} embedding rate {{is similar to}} the real value, and the variance is small. The AZCR-OED of the blended speech increases as the embedding rate decreases, which increases the threshold that distinguishes the stego and pure <b>speech</b> <b>frames.</b> Therefore, many pure <b>speech</b> <b>frames</b> were misjudged to be stego <b>speech</b> <b>frames,</b> which caused the embedding rate to be overestimated.|$|R
5000|$|Zero insertion: {{the lost}} <b>speech</b> <b>frames</b> are {{replaced}} with zero ...|$|R
50|$|The codec {{operates}} on 160 sample frames that span 20 ms, {{so this is}} the minimum transcoder delay possible even with infinitely fast CPUs and zero network latency. The operational requirement is that the transcoder delay should be less than 30 ms. The transcoder delay is defined as the time interval between the instant a <b>speech</b> <b>frame</b> of 160 samples has been received at the encoder input and the instant the corresponding 160 reconstructed speech samples have been out-put by the speech decoder at an 8 kHz sample rate.|$|E
30|$|The {{logarithmic}} {{energy is}} less distorted in a <b>speech</b> <b>frame</b> {{than in a}} non-speech frame.|$|E
3000|$|... {{represents}} the enhanced <b>speech</b> <b>frame.</b> The final enhanced speech signal is reconstructed {{by using the}} standard overlap-and-add method.|$|E
5000|$|... iLBC handles lost <b>frames</b> through graceful <b>speech</b> quality degradation. Lost frames {{often occur}} in {{connection}} with lost or delayed IP packets. Ordinary low-bitrate codecs exploit dependencies between <b>speech</b> <b>frames,</b> which cause errors to propagate when packets are lost or delayed. In contrast, iLBC-encoded <b>speech</b> <b>frames</b> are independent and so this problem will not occur.|$|R
3000|$|Through {{extensive}} experimentation {{on different}} <b>speech</b> <b>frames,</b> {{it is found}} that the negligibility of the cross-correlation terms r [...]...|$|R
30|$|A {{packet loss}} process that {{periodically}} drops a static number of consecutive <b>speech</b> <b>frames</b> {{preceded by a}} given inter-loss gap size.|$|R
3000|$|... (as {{described}} after (12)) strongly {{depends on}} the voicing characteristics of speech frames and the input noise. Because of inherent periodicity of the voiced <b>speech</b> <b>frame,</b> the degree of cross-correlation between two voiced speech frames of a person becomes higher in comparison to that between two unvoiced speech frames which are random in nature. Regarding signal power, the ratio of power of a voiced <b>speech</b> <b>frame</b> and an unvoiced <b>speech</b> <b>frame</b> {{is found to be}} higher in comparison to that of the two voiced speech frames. As white Gaussian noise is considered, the degree of cross-correlation between the speech and noise is found to be negligible and the noise powers in two different frames may not differ significantly. As a result, the effect of input noise is found to be negligible on the power ratio.|$|E
40|$|Line {{spectrum}} frequencies (LSF's) uniquely {{represent the}} {{linear predictive coding}} (LPC) filter of a <b>speech</b> <b>frame.</b> In many vocoders LSF's are used to encode the LPC parameters. In this paper, an interframe differential coding scheme is presented for the LSF's. The LSF's of the current <b>speech</b> <b>frame</b> are predicted by using both the LSF's of the previous frame {{and some of the}} LSF's of the current frame. Then, the difference resulting from prediction is quantized. © 1994 IEE...|$|E
30|$|Step 2 : {{threshold}} process. In DDBSE, the entropy {{of signal}} y(t,k) {{will be used to}} detect the <b>speech</b> <b>frame.</b> The entropy is calculated by considering the amplitudes of the R continuous frames in each frequency point. Just as Fig.  1 shows, although the energy of clean speech signal (denoted by a red line) is much greater than that in the frames nearby, there are still lots of interference caused by noise. In order to detect the <b>speech</b> <b>frame</b> more clearly, a local threshold processing-based approach is adopted, which processes the amplitude of the R continuous frames, tmp(t, k), as follows.|$|E
30|$|For voiced <b>speech</b> <b>frames,</b> LPW will be {{designed}} to retain only the local peaks of the harmonic structure {{as shown in the}} bottom-right graph in Figure 6 (see also Figure 3 (d)) For unvoiced <b>speech</b> <b>frames,</b> the result will be almost flat {{due to the lack of}} local peaks with the target harmonic structure. Unlike the comb weights, the LPW is not uniform over the target frequencies and is more focused on the frequencies where harmonic structures are observed in the input spectrum.|$|R
30|$|From Fig.  5 a, we {{can observe}} that the AZCR-OED values {{of most of}} the stego <b>speech</b> <b>frames</b> are less than the value for the entire blended speech signal, and the AZCR-OED values {{of most of the}} pure <b>speech</b> <b>frames</b> are greater than the value for the entire blended speech signal. Comparing panels (a) and (b) of Fig.  5, we see that the {{estimated}} hidden location of the secret speech is similar to its actual hidden location. These experimental results demonstrate the effectiveness of the algorithm.|$|R
40|$|Abstract: The {{process of}} speech {{production}} in the human system is very complex, possesses nonlinearities, and can only be precisely modeled in terms of nonlinear dynamics. A non-linear speech classification approach is proposed, which classifies speech based on features extracted from Takens’ Method of Delays, a technique used to reconstruct signals into a trajectory in multi-dimensional state space. In this research, two types of speech detection are presented, namely, voiced and usable speech (for speaker identification purposes). The proposed approach {{has been able to}} yield a probability of error of 12 % in noisy environments for voiced speech detection, and 78 % correct usable speech detection by comparing the structures of embedded voiced <b>speech</b> <b>frames</b> with embedded unvoiced <b>speech</b> <b>frames,</b> and embedded usable <b>speech</b> <b>frames</b> with embedded unusable speech. Some applications of this speech detection technique include the enhancement of speaker identification and speech recognition systems. 1...|$|R
30|$|Based on the {{evaluation}} results, {{in the sense}} of the energy test, the copula-based distributions using IFM method were mostly overcome by conventional distributions in the second experimental setup. As only one of parameter estimation methods of copula-based distribution, IFM method, was taken into account in the experimental evaluation, and the IFM method ends up a sub-optimal solution for parameter estimation, it is difficult to have a generic conclusion on copula-based distribution’s benefit in statistical modeling of <b>speech</b> <b>frame.</b> One of future work perspective might therefore be to study the power of statistical modeling of copula-based distribution of <b>speech</b> <b>frame</b> using optimal parameter estimation methods.|$|E
3000|$|... [k]| increases. Therefore, for a noise-corrupted utterance, the {{logarithmic}} magnitude {{spectrum of}} the <b>speech</b> <b>frame</b> is often less vulnerable to noise {{than that of the}} non-speech (noise-only) frame. However, this condition does not hold for the (linear) magnitude spectrum.|$|E
40|$|A {{method and}} device for extrapolating past signal-history data for {{insertion}} into missing data segments {{in order to}} conceal digital <b>speech</b> <b>frame</b> errors. The extrapolation method uses past-signal history that is stored in a buffer. The method is implemented with a device that utilizes a finite-impulse response (FIR) multi-layer feed-forward artificial neural network that is trained by back-propagation for one-step extrapolation of speech compression algorithm (SCA) parameters. Once a speech connection has been established, the speech compression algorithm device begins sending encoded speech frames. As the speech frames are received, they are decoded and converted back into speech signal voltages. During the normal decoding process, pre-processing of the required SCA parameters will occur and the results stored in the past-history buffer. If a <b>speech</b> <b>frame</b> is detected to be lost or in error, then extrapolation modules are executed and replacement SCA parameters are generated and sent as the parameters required by the SCA. In this way, the information transfer to the SCA is transparent, and the SCA processing continues as usual. The listener will not normally notice that a <b>speech</b> <b>frame</b> has been lost because of the smooth transition between the last-received, lost, and next-received speech frames...|$|E
40|$|In this paper, a {{new feature}} {{selection}} method for speaker recognition is proposed {{to keep the}} high quality <b>speech</b> <b>frames</b> for speaker modelling and to remove noisy and corrupted <b>speech</b> <b>frames.</b> In order to obtain robust voice activity detection in variety of acoustic conditions, the spectral subtraction algorithm is adopted to estimate the frame power. An energy based frame selection algorithm is then applied to indicate the speech activity at the frame ame level. The eigenchannel based GMM-UBM speaker peaker recognition system is used to evaluate this proposed method. The experiments are conducted on the 2006 NIST ST Speaker ker Recognition Evaluation core test condition on (telephone hone channel) as well as microphone channel test condition. dition. It demonstrates that this approach can provide vide an efficient ficient way to select high quality <b>speech</b> <b>frames</b> in the noisy environment for speaker recognition. on. n. Index term- speaker recognition, voice activity detection, feature selection, on, n, spectral subtraction, noise reductio...|$|R
50|$|The modem timings {{are also}} relevant, in that each <b>speech</b> vocoder <b>frame</b> outputs 28-bits every 40 ms. Since the modem has an 80 ms modem frame, it can {{transport}} two <b>speech</b> vocoder <b>frames.</b>|$|R
40|$|In this paper, a text {{dependent}} speaker recognition algorithm {{based on}} spectrogram is proposed. The spectrograms have been generated using Discrete Fourier Transform for varying frame sizes with 25 % and 50 % overlap between <b>speech</b> <b>frames.</b> Feature vector extraction {{has been done}} by using the row mean vector of the spectrograms. For feature matching, two distance measures, namely Euclidean distance and Manhattan distance have been used. The results have been computed using two databases: a locally created database and CSLU speaker recognition database. The maximum accuracy is 92. 52 % for an overlap of 50 % between <b>speech</b> <b>frames</b> with Manhattan distance as similarity measure...|$|R
30|$|The ASR {{subsystem}} {{is based}} on the Kaldi open-source toolkit [81] and employs the DNN-based acoustic models. Specifically, a DNN-based context-dependent speech recognizer is trained following the DNN training approach presented in [95]. Forty-dimensional MFCCs, which are augmented with three pitch- and voicing-related features [96] and appended with their delta and double delta coefficients, are firstly extracted for each <b>speech</b> <b>frame.</b> The DNN has 6 hidden layers with 2048 neurons each. Each <b>speech</b> <b>frame</b> is spliced across ±[*] 5 frames to produce 1419 -dimensional vectors that are the input into the first layer. The output layer is a soft-max layer representing the log-posteriors of the context-dependent HMM states. The Kaldi LVCSR decoder generates word lattices [97] using these DNN-based acoustic models.|$|E
40|$|Typically, {{the power}} {{spectrum}} of a <b>speech</b> <b>frame</b> used in speech recognition is estimated for a fixed length window using the fast Fourier transform. Each frequency component represented in this power spectrum is an estimate over that <b>speech</b> <b>frame.</b> The power spectrum calculated {{in this way}} hms a constant time and frequency resolution. An example: {{of this type of}} front-end is the LPC-derived cepstral front-end commonly used is recognition systems today [l]. The acoustic front-end presented in this paper employs both a ' warped frequency and temporal resolutions. We show that EL front-end that utilises both warping functions, outperforms a front-end that employs only a warped frequency scale. We also show that this new front-end is unsuitable for noisy conditions. "I...|$|E
30|$|Energy of each {{short-time}} <b>speech</b> <b>frame</b> in {{the recording}} is classified as either speech or silence using the likelihood ratio test (LRT). Because the test treats each frame independently, a second processing step is used where silence and speech segments that were shorter than four frames are removed.|$|E
30|$|The {{recognition}} accuracy improves as {{the value}} α is increased from 0 to 0.6, and the additional {{improvement in accuracy}} is 4.80 % (from 72.26 % to 77.06 %). Therefore, amplifying the magnitude spectrum of the <b>speech</b> <b>frames</b> correctly is helpful.|$|R
40|$|A {{nonlinear}} Hammerstein {{model is}} proposed for coding speech signals. Using Tsay’s nonlinearity test, we first {{show that the}} great majority of <b>speech</b> <b>frames</b> contain nonlinearities (over 80 % in our test data) when using 20 -millisecond <b>speech</b> <b>frames.</b> Frame length correlates with the level of nonlinearity: the longer the frames the higher the percentage of nonlinear frames. Motivated by this result, we present a nonlinear structure using a frame-by-frame adaptive identification of the Hammerstein model parameters for speech coding. Finally, the proposed structure is compared with the LPC coding scheme for three phonemes /a/, /s/, and /k/ by calculating the Akaike information criterion of the corresponding residual signals. The tests show clearly that the residual of the nonlinear model presented in this paper contains significantly less information compared to that of the LPC scheme. The presented method is a potential tool to shape the residual signal in an encode-efficient form in speech coding...|$|R
3000|$|... [...]). In {{practical}} situations, the <b>speech</b> <b>frames</b> is {{of length}} n= 180 – 220 {{and if we}} choose the compression rate as 5 %, the length of compressed signal is m= 9 – 11. Therefore, the order of magnitude of the keyspace is about 102000.|$|R
30|$|To {{cope with}} first two issues, in most speech {{processing}} problems, speech is processed frame-wise with frames of 20 − 30 ms length where speech signal {{can be considered}} stationary. In AV speech processing, it is convenient to choose <b>speech</b> <b>frame</b> length such that audio and video frame rates are equal.|$|E
30|$|It is {{noteworthy}} that, {{in the context}} of enhancing speech under low levels of SNR, our proposed approach to determine the threshold value in a subband of a silent or <b>speech</b> <b>frame</b> is not only different but also more reasonable with simpler approximation and lesser computation in comparison to that described in[17].|$|E
40|$|Abstract: This paper {{explains}} the VoIP network design for good voice quality {{on low speed}} links and describes the quality measurement results. Many of issues, such as compression of the <b>speech</b> <b>frame</b> and delay variation, are inherent to VoIP. With careful planning and solid network design these effects on VoIP networks can be minimized...|$|E
40|$|Building on {{algorithms}} {{developed in}} earlier work (Hawkins et al., 1994 a, 1994 b; Hawkins, 1997; Hawkins et al., 2002), this study develops {{a new technique}} for improving the accuracy of formant estimates produced by an analysis-by-synthesis formant tracker (DPTRAK, Clermont, 1992). DPTRAK is evaluated by comparing its formant estimates against those obtained manually by the first author when he inspected the spectrogram of each vowel produced by each speaker. Applied to 13 male speakers uttering the 11 monophthongs and eight diphthongs of Australian English, DPTRAK produced results that varied in accuracy across speakers. The percentage of <b>speech</b> <b>frames</b> tracked accurately varied from 99 % for the best speaker through to 58 % for the worst speaker. We develop the SpeechSifter algorithm to sift through the <b>speech</b> <b>frames</b> tracked by the DPTRAK formant tracker (or any other formant tracker) and select only those frames {{that are likely to}} be accurately tracked. This unsupervised algorithm first selects the ideal speaker on which to train a Replicator Neural Net (Hawkins et al., 2002). The trained Replicator Neural Net is then used to screen those <b>speech</b> <b>frames</b> on which the formant tracker is highly likely to have made accurate formant estimates and to discard the rest. We demonstrate the value of this approach. First, we demonstrate that we can accurately predict which speaker will provide the ideal training speaker for the RNN. Next, we apply the trained RNN to a speaker and show that {{that it is possible to}} achieve a 90 % accuracy rate whilst retaining 75 % of the speaker’s original <b>speech</b> <b>frames.</b> This is an improvement on the DPTRAK algorithm which achieves an accuracy rate of only 81 % for this speaker. 1...|$|R
40|$|Framed {{collection}} of a voters' ballot, {{a photograph of}} Nelson Mandela, new R 5 coins and postage stamps, the South African flag and an extract of President Nelson Mandela's inaugural <b>speech.</b> <b>Framed</b> {{collection of}} items relating to the new democratic South Africa of 1994 and its president, Nelson Mandela...|$|R
40|$|Appling {{compressive}} sensing (CS),which theoretically {{guarantees that}} signal sampling and signal compression {{can be achieved}} simultaneously,into audio and speech signal processing {{is one of the}} most popular research topics in recent years. In this paper,K-SVD algorithm was employed to learn a sparse linear prediction dictionary regarding as the sparse basis of underlying speech signals. Compressed signals was obtained by applying random Gaussian matrix to sample original <b>speech</b> <b>frames.</b> Orthogonal matching pursuit (OMP) and compressive sampling matching pursuit (CoSaMP) were adopted to recovery original signals from compressed one. Numbers of experiments were carried out to investigate the impact of <b>speech</b> <b>frames</b> length,compression ratios,sparse basis and reconstruction algorithms on CS performance. Results show that sparse linear prediction dictionary can advance the performance of speech signals reconstruction compared with discrete cosine transform (DCT) matrix...|$|R
