356|69|Public
50|$|A {{factor graph}} is a bipartite graph {{representing}} the factorization of a function. In probability theory and its applications, factor graphs {{are used to}} represent factorization of a probability distribution function, enabling efficient computations, such as the computation of marginal distributions through the <b>sum-product</b> <b>algorithm.</b> One of the important success stories of factor graphs and the <b>sum-product</b> <b>algorithm</b> is the decoding of capacity-approaching error-correcting codes, such as LDPC and turbo codes.|$|E
5000|$|The <b>sum-product</b> <b>algorithm</b> {{is related}} to the {{calculation}} of free energy in thermodynamics. Let Z be the partition function. A probability distribution ...|$|E
5000|$|In practice, the <b>sum-product</b> <b>algorithm</b> is {{used for}} {{statistical}} inference, whereby [...] is a joint distribution or a joint likelihood function, and the factorization depends on the conditional independencies among the variables.|$|E
40|$|Iterative {{decoding}} {{techniques have}} become a viable alternative for constructing high performance coding systems. In particular, the recent success of turbo codes indicates that performance close to the Shannon limit may be achieved. In this thesis, it is showed that many iterative decoding algorithms are special cases of two generic algorithms, the min-sum and <b>sum-product</b> <b>algorithms,</b> which also include non-iterative algorithms such as Viterbi decoding. The min-sum and <b>sum-product</b> <b>algorithms</b> are developed and presented as generalized trellis algorithms, where the time axis of the trellis is replaced by an arbitrary graph, the "Tanner graph". With cycle-free Tanner graphs, the resulting decoding algorithms (e. g., Viterbi decoding) are maximum-likelihood but suffer from an exponentially increasing complexity. Iterative decoding occurs when the Tanner graph has cycles (e. g., turbo codes); the resulting algorithms are in general suboptimal, but significant complexity reductions are possible compared to the cycle-free case. Several performance estimates for iterative decoding are developed, including a generalization of the union bound used with Viterbi decoding and a characterization of errors that are uncorrectable after infinitely many decoding iterations...|$|R
40|$|This pre-release {{contains}} some LDPC BP decoders (for now the min-sum and <b>sum-product</b> <b>algorithms</b> with flooding and layered sequencing). The {{quality of the}} code has been constantly improved and the interface with the user is better now (a lot of things can be directly configured from files and be displayed in the PyBER plotter GUI application). A lot of features have been added: Rayleigh channel, PSK-PAM-QAM-GSM modulations, turbo demodulation (BFERI simulations), self-corrected turbo decoder, SystemC/TLM standard support, etc. It is time for a checkpoint, we {{hope that you will}} enjoy this new release...|$|R
40|$|In this paper, we {{construct}} parity-concatenated trellis {{codes in}} which a trellis code is used as the inner code and a simple parity-check code is used as the outer code. From the Tanner–Wiberg–Loeliger (TWL) graph representation, several iterative decoding algorithms can be derived. However, since the graph of the parity-concatenated code contains many short cycles, the conventional min-sum and <b>sum-product</b> <b>algorithms</b> cannot achieve near-optimal decoding. After some simple modifications, we obtain near-optimal iterative decoders. The modifications include either a) introducing a normalization operation in the min-sum and <b>sum–product</b> <b>algorithms</b> or b) cutting the short cycles which arise in the iterative Viterbi algorithm (IVA). After modification, all three algorithms can achieve near-optimal performance, but the IVA has the least average complexity. We also show that asymptotically maximum-likelihood (ML) decoding and a posteriori probability (APP) decoding can be achieved using iterative decoders with only two iterations. Unfortunately, this asymptotic behavior is only exhibited when the bit-energy-tonoise ratio is above the cutoff rate. Simulation results show that with trellis shaping, iterative decoding can perform within 1. 2 dB of the Shannon limit at a bit error rate (BER) of R IH S for a block size of 20 000 symbols. For a block size of 200 symbols, iterative decoding can perform within 2. 1 dB of the Shannon limit...|$|R
50|$|As {{shown by}} the {{previous}} formula: the complete marginalization is reduced to a sum of products of simpler terms than the ones appearing in the full joint distribution. This {{is the reason why}} it is called the <b>sum-product</b> <b>algorithm.</b>|$|E
50|$|It {{can then}} be shown that the points of {{convergence}} of the <b>sum-product</b> <b>algorithm</b> represent the points where the free energy in such a system is minimized. Similarly, it can be shown that a fixed point of the iterative belief propagation algorithm in graphs with cycles is a stationary point of a free energy approximation.|$|E
5000|$|A popular {{message passing}} {{algorithm}} on factor graphs is the <b>sum-product</b> <b>algorithm,</b> which efficiently computes all the marginals {{of the individual}} variables of the function. In particular, the marginal of variable [...] is defined aswhere the notation [...] means that the summation goes over all the variables, except [...] The messages of the <b>sum-product</b> <b>algorithm</b> are conceptually computed in the vertices and passed along the edges. A message from or to a variable vertex is always a function of that particular variable. For instance, when a variable is binary, the messagesover the edges incident to the corresponding vertex can be represented as vectors of length 2: the first entry is the message evaluated in 0, the second entry is the message evaluated in 1. When a variable belongs {{to the field of}} real numbers, messages can be arbitrary functions, and special care needs to be taken in their representation.|$|E
40|$|In {{this work}} {{we present a}} full analog turbo decoder for hard-disk EPR-IV read {{channels}} in CMOS technology. The design {{is based on a}} current-mode approach developed by Loeliger et al. [2001], for the analog implementation of <b>sum-product</b> <b>algorithms.</b> The circuit's main attractions are the coding gain offered by turbo codes over the uncoded EPR-IV channel, and the relative simplicity and power efficiency of the analog approach over the digital approach. The circuit is developed in a 0. 18 μm CMOS technology and operates at a 1. 8 V power supply, with a total simulated power consumption (including peripheral circuitry) of about 650 mW at 400 Mb/...|$|R
3000|$|In this article, optimal {{equalization}} and decoding {{using the}} MLSE (min-sum) and MAP (<b>sum-product)</b> <b>algorithms</b> were discussed. It was shown how the MLSE algorithm {{can be used}} to determine the most likely sequence of estimates, while the MAP algorithm {{can be used to}} determine optimal posterior probabilities regarding the transmitted symbols or codewords. NI-JED was also discussed, first assuming no interleaving and then assuming that special block interleavers were used for interleaving. As a result, a general model was derived for systems transmitting convolutionally encoded BPSK modulated information through a multipath channel of length L, where the information is interleaved with an interleaver of depth D=d k, and where the uncoded information is encoded with a rate R [...]...|$|R
40|$|Abstract—In this paper, {{we present}} a general {{approach}} to finite-memory detection. From a semi-tutorial perspective, a number of previous results are rederived and new insights are gained within a unified framework. A probabilistic derivation of the well-known Viterbi <b>algorithm,</b> forward–backward, and <b>sum-product</b> <b>algorithms,</b> shows that a basic metric emerges naturally under very general causality and finite-memory conditions. This result implies that detection solutions based on one algorithm can be systematically extended to other algorithms. For stochastic channels described by a suitable parametric model, a conditional Markov property is shown to imply this finite-memory condition. This conditional Markov property, although seldom met exactly in practice, is shown to represent a reasonable and useful approximation in all considered cases. We consider, as examples, linear predictive and noncoherent detection schemes. While good performance for increasing complexity can often be achieved with a finite-memory detection strategy, key issues {{in the design of}} detection algorithms are the computational efficiency and the performance for limited complexity. Index Terms—Adaptive detection, finite-memory detection, forward–backward (FB) algorithm, graph-based detection, iterative detection, maximum a posteriori (MAP) sequence/symbol detection, <b>sum-product</b> (SP) <b>algorithm,</b> Viterbi algorithm (VA). I...|$|R
40|$|The <b>sum-product</b> <b>algorithm</b> is a {{well-known}} procedure for marginalizing an "acyclic" product function whose range is the ground set of a commutative semiring. The algorithm is general enough to include as special cases several classical algorithms developed in information theory and probability theory. We present four results. First, using the <b>sum-product</b> <b>algorithm</b> we show that the variable sets involved in an acyclic factorization satisfy a relation that is a natural generalization of probability-theoretic independence. Second, we show that for the Boolean semiring the <b>sum-product</b> <b>algorithm</b> reduces to a classical algorithm of database theory. Third, we present some methods {{to reduce the amount}} of computation required by the <b>sum-product</b> <b>algorithm.</b> Fourth, we show that with a slight modification the <b>sum-product</b> <b>algorithm</b> can be used to evaluate a general sum-product expression...|$|E
40|$|We {{consider}} a pilot-assisted interleave-division multiple access (IDMA) system transmitting over block-fading channels. We describe this system {{in terms of}} a factor graph and use the <b>sum-product</b> <b>algorithm</b> to develop a receiver that performs joint data detection and channel estimation. Suitable approximations to the messages passed by the <b>sum-product</b> <b>algorithm</b> yield an implementation with a complexity that scales linearly with the number of users. Simulation results demonstrate large performance gains compared to classical receivers performing separate channel estimation and data detection. Index Terms—IDMA, multiuser detection, iterative receivers, factor graphs, <b>sum-product</b> <b>algorithm</b> 1...|$|E
40|$|One way to {{approximate}} inference in richly-connected graphical models {{is to apply}} the <b>sum-product</b> <b>algorithm</b> (a. k. a. probabil-ity propagation algorithm), while ignoring {{the fact that the}} graph has cycles. The <b>sum-product</b> <b>algorithm</b> can be directly applied in Gaussian networks and in graphs for coding, but for many condi-tional probability functions- including the sigmoid function- di-rect application of the <b>sum-product</b> <b>algorithm</b> is not possible. We introduce "accumulator networks " that have low local complexity (but exponential global complexity) so the <b>sum-product</b> <b>algorithm</b> can be directly applied. In an accumulator network, the probability of a child given its parents is computed by accumulating the inputs from the parents in a Markov chain or more generally a tree. After giving expressions for inference and learning in accumulator net-works, we give results on the "bars problem " and on the problem of extracting translated, overlapping faces from an image. ...|$|E
40|$|Based on {{the factor}} graph framework, we derived a Modified Nonparametric Message Passing Algorithm (MNMPA) for soft {{iterative}} channel estimation in a Low Density Parity-Check (LDPC) coded Bit-Interleaved Coded Modulation (BICM) system. The algorithm combines ideas from Particle Filtering (PF) with popular factor graph techniques. A Markov Chain Monte Carlo (MCMC) move step is added after typical sequential Important Sampling (SIS) -resampling to prevent particle impoverishment {{and to improve}} channel estimation precision. To reduce complexity, a new max-sum rule for updating particle based messages is reformulated and two proper update schedules are designed. Simulation results illustrate the effectiveness of MNMPA and its comparison with other <b>sum-product</b> <b>algorithms</b> in a Gaussian or non-Gaussian noise environment. We also studied {{the effect of the}} particle number, pilot symbol spacing and different schedules on BER performance...|$|R
40|$|We {{consider}} {{in this paper}} regular and nearly regular quasi-cyclic low-density parity-check (LDPC) codes, derived from families of difference sets. The codes have girth at least 6, and sparse parity-check matrices. They are designed to perform well when iteratively decoded with the <b>sum-product</b> decoding <b>algorithm</b> and to allow low complexity encoding...|$|R
40|$|In {{applications}} of graphical models arising in do-mains such as computer vision and signal pro-cessing, we often seek {{the most likely}} config-urations of high-dimensional, continuous vari-ables. We develop a particle-based max-product algorithm which maintains a diverse set of pos-terior mode hypotheses, and is robust to initial-ization. At each iteration, the set of hypothe-ses at each node is augmented via stochastic pro-posals, and then reduced via an efficient selec-tion algorithm. The integer program underlying our optimization-based particle selection mini-mizes errors in subsequent max-product mes-sage updates. This objective automatically en-courages diversity in the maintained hypotheses, without requiring tuning of application-specific distances among hypotheses. By avoiding the stochastic resampling steps underlying particle <b>sum-product</b> <b>algorithms,</b> we also avoid common degeneracies where particles collapse onto a sin-gle hypothesis. Our approach significantly out-performs previous particle-based algorithms in experiments focusing on the estimation of human pose from single images. 1...|$|R
40|$|Abstract — This article {{summarizes}} work {{in progress}} on theoretical analysis of the <b>sum-product</b> <b>algorithm.</b> Two families of graphs with quite different characteristics are studied: graphs in which all checks have degree two and graphs with a single cycle. Each family has a relatively simple structure that allows for precise mathematical results about the convergence of the <b>sum-product</b> <b>algorithm.</b> I...|$|E
40|$|A new {{scheduling}} algorithm to the iterative <b>sum-product</b> <b>algorithm,</b> {{which is}} called sub-graph scheduling, will be presented in this paper. The propesed algorithm provides a schedule which has a higher convergence rate than the iterative <b>sum-product</b> <b>algorithm</b> while keeping the complexity of one iteration withot degrading the performance. Our method also gives an explanation {{to the fact that}} turbo decoders have faster convergence rate than LDPC decoders. ...|$|E
30|$|For sources {{with smaller}} entropy, larger RCM weight sets, D, tend to work better, since the <b>sum-product</b> <b>algorithm</b> is {{aided by the}} a priori probability.|$|E
40|$|Abstract — We {{consider}} {{in this paper}} regular and nearly regular quasi-cyclic low-density parity-check (LDPC) codes, derived from families of dif-ference sets. The codes have girth at least 6, and sparse parity-check matrices. They are designed to perform well when iteratively decoded with the <b>sum-product</b> decoding <b>algorithm</b> and to allow low complexity encoding. I...|$|R
40|$|Abstract— Multi-Source Cooperation (MSC) techniques, {{including}} conventional Code Division Multiplexing (CDM) and Classic Network Coding (CNC) are investigated. We adopt a soft <b>sum-product</b> decoding <b>algorithm</b> for the CNC {{technique and}} propose a ?exible Variable-rate Network Coding (VNC) technique. The iterative decoding convergence {{of the multiple}} source computation methods is analysed {{with the aid of}} EXtrinsic Information Transfer (EXIT) charts...|$|R
40|$|Abstract — Low-Density Parity-Check (LDPC) {{codes are}} {{currently}} {{the most promising}} coding technique to achieve the Shannon capacities {{for a wide range}} of channels. <b>Sum-Product</b> decoding <b>algorithm</b> of LDPC codes is an iterative decoding algorithm with excellent performance. Min-Sum decoding algorithm is a kind of modified <b>Sum-Product</b> decoding <b>algorithm</b> with a reduced implementation complexity. Current research upon Min-Sum decoding algorithm is mainly on float-point computation. While in practical implementation, fixed-point is used instead of floatpoint computation to reduce the complexity of decoding and increase throughout of the decoder. Usually the complexity can be reduced via integer quantization. While quantization precision is not that much with integer quantization, and there will be a loss in BER performance. In terms of fixedpoint calculation, a two-stage quantization algorithm is proposed for Min-Sum decoding. The simulation results indicate that compared with float-point calculation, the proposed algorithm can reduce implementation complexity with quantization, and there is only 0. 05 dB loss at the BER 6 performance of 10 �...|$|R
30|$|In this section, we {{describe}} the algorithm associated with the proposed distributed solution for the problem of parameter coordination. The <b>sum-product</b> <b>algorithm</b> [5] can be applied whenever the variables and functions associated with the factor graph are defined on a commutative semi-ring whose elements satisfy the distributive law. For our problem at hand, we apply the variant of <b>sum-product</b> <b>algorithm</b> {{that is based on}} the min-sum commutative semi-ring [12]. Recall that a semi-ring is a mathematical structure (e.g., a set) equivalent to a ring without an additive inverse. In a general way, the binary operations addition and multiplication can be replaced with others as long as the distributed law still holds. In this sense, a min-sum semi-ring simply replaces the addition operation with the minimum operation and the multiplication operation with the addition operation. In this case, the <b>sum-product</b> <b>algorithm</b> is also called the min-sum algorithm.|$|E
40|$|Abstract — Loss {{networks}} are {{a class of}} resource-sharing models which provide a powerful tool to the analysis and design of many communications and networking systems. For most loss networks of practical interest calculating the exact blocking probabilities is a difficult task. In this paper we present a new framework based on probabilistic graphical models to tackle this task. Specifically, we propose to use factor graphs to model the stationary distribution of product-form loss networks. We also propose to use the <b>sum-product</b> <b>algorithm</b> to compute the marginal distributions and the blocking probabilities of all call classes. Through extensive numerical experiments we show that the <b>sum-product</b> <b>algorithm</b> returns very accurate blocking probabilities and greatly outperforms the reduced load approximation for both single-service and multiservice loss networks {{with a variety of}} topologies. In addition, the <b>sum-product</b> <b>algorithm</b> converges very fast and can be implemented in a distributed way. I...|$|E
40|$|An {{iterative}} decoding algorithm of low density parity check(LDPC) {{codes for}} hidden Markov noise channels is presented. The hidden Markov noise channel is an additive noise channel whose noise statistics is modeled by a hidden Markov model(HMM). The proposed decoding algorithm {{consists of two}} parts: the conventional <b>sum-product</b> <b>algorithm</b> for the LDPC codes and a forward-backward likelihood estimation algorithm for the HMM. For some channel parameters, {{it is observed that}} the proposed algorithm can correct burst errors far beyond the random error correcting capability of the LDPC code. 1. Introduction Recently, it has been recognized that the combination of a long low density paritycheck(LDPC) code[1] and the <b>sum-product</b> <b>algorithm</b> gives near capacity performance for the additive white Gaussian channel with reasonable decoding complexity[2]. In this article, an extension of the <b>sum-product</b> <b>algorithm</b> suitable for hidden Markov noise channels is presented. The hidden Markov noise channel [...] ...|$|E
40|$|Since their rediscovery in 1995, low-density parity-check (LDPC) codes have {{received}} wide-spread attention as practical capacity-approaching code candidates. It {{has been shown}} that the class of codes can perform arbitrarily close to the channel capacity, and LDPC codes are also used or suggested for a number of important current and future communication standards. However, the problem of implementing an energy-efficient decoder has not yet been solved. Whereas the decoding algorithm is computationally simple, with uncomplicated arithmetic operations and low accuracy requirements, the random structure and irregularity of a theoretically well-defined code does not easily allow efficient VLSI implementations. Thus the LDPC decoding algorithm can be said to be communication-bound rather than computation-bound. In this thesis, a modification to the <b>sum-product</b> decoding <b>algorithm</b> called early-decision decoding is suggested. The modification is {{based on the idea that}} the values of the bits in a block can be decided individually during decoding. As the <b>sum-product</b> decoding <b>algorithm</b> is a soft-decision decoder, a reliability can b...|$|R
40|$|This paper {{considers}} {{the design of}} close-to-optimal receivers {{in the presence of}} a timing uncertainty. The problem is placed into the factor-graph and the <b>sum-product</b> (SP) <b>algorithm</b> framework. A simplified version of the SP algorithm is considered and the expectation-maximization (EM) algorithm is used to implement it. The proposed approach, combining the SP and EM algorithms, is shown to outperform classical approaches while exhibiting a low complexity. 1...|$|R
40|$|Abstract: In this paper, {{based on}} the {{application}} of the <b>sum-product</b> (SP) <b>algorithm</b> to factor graphs representing the joint a posteriori probability of the transmitted symbols, we propose new iterative softinput soft-output (SISO) detection schemes for intersymbol interference (ISI) channels. For sparse ISI channels, these algorithms have advantages in terms of complexity over optimal detection schemes {{based on the}} BCJR algorithm. They also allow a parallel implementation of the receiver and the possibility of a more efficient complexity reduction...|$|R
40|$|Algorithms {{that must}} deal with {{complicated}} global functions of many variables often exploit {{the manner in which}} the given functions factor as a product of "local" functions, each of which depends on a subset of the variables. Such a factorization can be visualized with a bipartite graph that we call a factor graph. In this tutorial paper, we present a generic message-passing algorithm, the <b>sum-product</b> <b>algorithm,</b> that operates in a factor graph. Following a single, simple computational rule, the <b>sum-product</b> <b>algorithm</b> computes [...] -either exactly or approximately [...] -various marginal functions derived from the global function. A wide variety of algorithms developed in artificial intelligence, signal processing, and digital communications can be derived as specific instances of the <b>sum-product</b> <b>algorithm,</b> including the forward/backward algorithm, the Viterbi algorithm, the iterative "turbo" decoding algorithm, Pearl's belief propagation algorithm for Bayesian networks, the Kalman filter, and certain fast Fourier transform (FFT) algorithms...|$|E
3000|$|... [...]. This {{will be done}} by {{applying}} the message-passing <b>Sum-Product</b> <b>Algorithm</b> (SPA, see [41] and references therein) over the whole factor graph describing the statistical dependence between [...]...|$|E
30|$|Run one {{iteration}} of the <b>sum-product</b> <b>algorithm</b> {{to obtain}} the extrinsic LLR messages passed from each LDGM and RCM check nodes to the VN, and obtain their empirical conditional PDFs.|$|E
40|$|In the {{database}} community, work on information extraction (IE) {{has centered on}} two themes: how to effectively manage IE tasks, and how to manage the uncertainties that arise in the IE process in a scalable manner. Recent work has proposed a probabilistic database (PDB) based declarative IE system that supports a leading statistical IE model, and an associated inference algorithm to answer top-k-style queries over the probabilistic IE outcome. Still, the broader problem of effectively supporting general probabilistic inference inside a PDB-based declarative IE system remains open. In this paper, we explore the in-database implementations {{of a wide variety}} of inference algorithms suited to IE, including two Markov chain Monte Carlo algorithms, the Viterbi and the <b>sum-product</b> <b>algorithms.</b> We describe the rules for choosing appropriate inference algorithms based on the model, the query and the text, considering the trade-off between accuracy and runtime. Based on these rules, we describe a hybrid approach to optimize the execution of a single probabilistic IE query to employ different inference algorithms appropriate for different records. We show that our techniques can achieve up to 10 -fold speedups compared to the non-hybrid solutions proposed in the literature...|$|R
30|$|The {{decoding}} of convolutional codes {{is closely}} related to equalization discussed in Section 3, in that the min-sum (Viterbi MLSE) and <b>sum-product</b> (MAP/BCJR) <b>algorithms</b> can also be used for decoding. The MAP algorithm is an attractive choice for use in iterative equalization/decoding algorithms. It is attractive for two reasons: first because it includes prior probabilistic information in the estimation, and second, because it provides soft posterior estimates regarding individual coded or uncoded symbols, which in turn can be used as prior information in subsequent iterations.|$|R
40|$|A {{new method}} is given for {{performing}} approximate maximum-likelihood (ML) decoding of an arbitrary binary linear code based on observations received from any discrete memoryless symmetric channel. The decoding algorithm {{is based on}} a linear programming (LP) relaxation that is defined by a factor graph or parity-check representation of the code. The resulting “LP decoder” generalizes our previous work on turbo-like codes. A precise combinatorial characterization of when the LP decoder succeeds is provided, based on pseudocodewords associated with the factor graph. Our definition of a pseudocodeword unifies other such notions known for iterative algorithms, including “stopping sets, ” “irreducible closed walks, ” “trellis cycles, ” “deviation sets, ” and “graph covers. ” The fractional distance ��— ™ of a code is introduced, which is a lower bound on the classical distance. It is shown that the efficient LP decoder will correct up to ��— ™ P I errors and that there are codes with ��— ™ a @ I A. An efficient algorithm to compute the fractional distance is presented. Experimental evidence shows a similar performance on low-density parity-check (LDPC) codes between LP decoding and the min-sum and <b>sum-product</b> <b>algorithms.</b> Methods for tightening the LP relaxation to improve performance are also provided...|$|R
