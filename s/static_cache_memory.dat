0|2495|Public
40|$|This work takes a {{fresh look}} at the {{simulation}} of <b>cache</b> <b>memories.</b> It introduces the technique of <b>static</b> <b>cache</b> simulation that statically predicts a large portion of cache references. To efficiently utilize this technique, a method to perform efficient on-the-fly analysis of programs in general is developed and proved correct. This method is combined with <b>static</b> <b>cache</b> simulation for a number of applications. The application of fast instruction cache analysis provides a new framework to evaluate instruction <b>cache</b> <b>memories</b> that outperforms even the fastest techniques published. <b>Static</b> <b>cache</b> simulation is shown {{to address the issue of}} predicting cache behavior, contrary to the belief that <b>cache</b> <b>memories</b> introduce unpredictability to real-time systems that cannot be efficiently analyzed. <b>Static</b> <b>cache</b> simulation for instruction caches provides a large degree of predictability for real-time systems. In addition, an architectural modification through bit-encoding is introduced that provides fu [...] ...|$|R
40|$|<b>Cache</b> <b>memories</b> {{have been}} widely used in order {{to bridge the gap between}} high speed {{processors}} and relatively slow main memories. However they are a source of predictability problems. A lot of progress has been achieved to model caches, in order to determine safe and precise bounds on (i) tasks' WCETs in the presence of caches; (ii) cacherelated preemption delays. An alternative approach to cope with <b>cache</b> <b>memories</b> in real-time systems is to lock their contents so as to make memory access times and cacherelated preemption delays entirely predictable. In this paper, we focus on instruction caches and we describe the state of the art in the so-called <b>static</b> <b>cache</b> locking technique and its related algorithms. Then benefits and problems with this approach are discussed. Eventually refinements and an enhanced technique, dynamic cache locking, are sketched...|$|R
40|$|Abstract — In {{real-time}} systems, <b>cache</b> <b>memory</b> poses {{challenge to}} improve both predictability and performance {{because of its}} adaptive and dynamic behavior. Recent studies indicate that for application-specific embedded systems, static cache-locking helps determining the worst case execution time (WCET) and cache-related preemption delay. In this work, we propose a static instruction cachelocking algorithm that makes the real-time embedded system more predictable by locking the blocks that might cause more cache misses. We obtain CPU utilization for both <b>static</b> <b>cache</b> analysis (no cache-locking) and static cache-locking using Heptane. Experimental results show that our cache-locking algorithm may improve both predictability and performance of real-time systems. Index Terms — Predictability, performance, cachelocking, real-time embedded system, Heptane...|$|R
50|$|With strong {{competitive}} pressures, {{the technology}} employed for each {{component of a}} computer system - principally CPU, memory, and offline storage - is typically selected to minimize the cost needed to attain a given level of performance. Though both microprocessor and memory are implemented as integrated circuits, the prevailing technology used for each differs; microprocessor technology optimizes speed and memory technology optimizes density. For this reason, the integration of memory and processor in the same chip has (for the most part) been limited to static random-access memory (SRAM), which may be implemented using circuit technology optimized for logic performance, rather than the denser and lower-cost dynamic random-access memory (DRAM), which is not. Microprocessor access to off-chip memory costs time and power, however, significantly limiting processor performance. For this reason computer architecture employing a hierarchy of memory systems has developed, in which static memory is integrated with the microprocessor for temporary, easily accessible storage (or cache) of data which is also retained off-chip in DRAM. Since the on-chip <b>cache</b> <b>memory</b> is redundant, its presence adds to cost and power. The purpose of the IRAM research project was to find if (in some computing applications) a better trade-off between cost and performance could be achieved with an architecture in which DRAM was integrated on-chip with the processor, thus {{eliminating the need for}} a redundant <b>static</b> <b>memory</b> <b>cache</b> - even though the technology used was not optimum for DRAM implementation.|$|R
40|$|We {{describe}} <b>cache</b> <b>memory</b> design {{suitable for}} use in FPGA-based cache controller and processor. Cache systems are on-chip memory element used to store data. Cache serves as a buffer between a CPU and its main <b>memory.</b> <b>Cache</b> <b>memory</b> is used to synchronize the data transfer rate between CPU and main <b>memory.</b> As <b>cache</b> <b>memory</b> closer to the micro processor, it is faster than the RAM and main memory. The advantage of storing data on cache, as compared to RAM, {{is that it has}} faster retrieval times, but it has disadvantage of on-chip energy consumption. In term of detecting miss rate in <b>cache</b> <b>memory</b> and less power consumption, The efficient <b>cache</b> <b>memory</b> will proposed by this research work, by implementation of <b>cache</b> <b>memory</b> on FPGA. We believe that our implementation achieves low complexity and low energy consumption in terms of FPGA resource usage. present in <b>cache</b> <b>memory</b> then the term is called „cache hit‟. The advantage of storing data on cache, as compared to RAM, is that it has faster retrieval times, but it has disadvantage of on-chip energy consumption. This paper deals with the design of efficient <b>cache</b> <b>memory</b> for detecting miss rate in <b>cache</b> <b>memory</b> and less power consumption. This <b>cache</b> <b>memory</b> may used in future work to design FPGA based cache controller...|$|R
40|$|The use of caches poses a {{difficult}} tradeoff for architects of real-time systems. While caches provide significant performance advantages, {{they have also}} been viewed as inherently unpredictable since {{the behavior of a}} cache reference depends upon the history of the previous references. The use of caches will only be suitable for realtime systems if a reasonably tight bound on the performance of programs using <b>cache</b> <b>memory</b> can be predicted. This paper describes an approach for bounding the worstcase instruction cache performance of large code segments. First, a new method called <b>Static</b> <b>Cache</b> Simulation is used to analyze a program’s control flow to statically categorize the caching behavior of each instruction. A timing analyzer, which uses the categorization information, then estimates the worst-case instruction cache performance for each loop and function in the program. 1...|$|R
40|$|Technology-driven {{limitations}} {{will soon}} force microprocessor chips to contain multiple processing cores, as the scalability of individual cores peaks but transistor counts continue to increase. To obtain best performance, flexible {{management of the}} on-chip resources, such as <b>cache</b> <b>memory</b> and off-chip bandwidth, is needed. However, control for the dynamic management of these on-chip resources is difficult to design. In this paper, we propose a method for developing such a controller: evolving a recurrent neural network using the Enforced Subpopulations algorithm. The method is tested in a trace-based simulation that measures dynamic assignation of a pool of level-two cache banks {{to a set of}} processing cores. We present results showing that, when the chip is controlled by the neural network, we obtain a 13 % performance improvement over <b>static</b> <b>cache</b> partitioning...|$|R
5000|$|<b>Cache</b> <b>Memory</b> - Number of cache modules 1-32, Module {{capacity}} 8 or 16GB, Maximum <b>cache</b> <b>memory</b> 512GB ...|$|R
40|$|The {{purpose of}} this study is to explore the {{relationship}} between hit ratio of <b>cache</b> <b>memory</b> and design parameters. <b>Cache</b> <b>memories</b> are widely used in the design of computer system architectures to match relatively slow memories against fast CPUs. Caches hold the active segments of a program which are currently in use. Since instructions and data in <b>cache</b> <b>memories</b> can be referenced much faster than the time required to access main <b>memory,</b> <b>cache</b> <b>memories</b> permit the execution rate of the machine to be substantially increased. In order to function effectively, <b>cache</b> <b>memories</b> must be carefully designed and implemented. In this study, a trace-driven simulation study of direct mapped, associative mapped and set-associative mapped <b>cache</b> <b>memories</b> is made. In the simulation, cache fetch algorithm, placement policy, cache size and various parameters related to cache design and the resulting effect on system performance is investigated. The <b>cache</b> <b>memories</b> are simulated using the C language and the simulation results are analyzed for the design and implementation of <b>cache</b> <b>memories.</b> Department of Physics and AstronomyThesis (M. S. ...|$|R
40|$|In multitask, {{preemptive}} real-time systems, {{the use of}} <b>cache</b> <b>memories</b> make difficult {{the estimation}} of the response time of tasks, due to the dynamic, adaptive and nonpredictable behaviour of <b>cache</b> <b>memories.</b> But many embedded and critical applications need the increase of performance provided by <b>cache</b> <b>memories...</b>|$|R
40|$|<b>Cache</b> <b>memories</b> {{are used}} in modern, medium and {{high-speed}} CPUs to hold temporarily those portions {{of the contents of}} main memory which are {believed to be) currently in use. Since instructions and data in <b>cache</b> <b>memories</b> can usually be referenced in 10 to 25 percent of the time required to access main <b>memory,</b> <b>cache</b> <b>memories</b> permit th...|$|R
40|$|Degraded K-user {{broadcast}} channels (BC) are studied when receivers are facilitated with <b>cache</b> <b>memories.</b> Lower {{and upper}} bounds are derived on the capacity-memory tradeoff, i. e., on the largest rate of reliable communication over the BC {{as a function}} of the receivers' cache sizes, and the bounds are shown to match for some special cases. The lower bounds are achieved by two new coding schemes that benefit from non-uniform cache assignment. Lower and upper bounds are also established on the global capacity-memory tradeoff, i. e., on the largest capacity-memory tradeoff that can be attained by optimizing the receivers' cache sizes subject to a total <b>cache</b> <b>memory</b> budget. The bounds coincide when the total <b>cache</b> <b>memory</b> budget is sufficiently small or sufficiently large, characterized in terms of the BC statistics. For small <b>cache</b> <b>memories,</b> it is optimal to assign all the <b>cache</b> <b>memory</b> to the weakest receiver. In this regime, the global capacity-memory tradeoff grows as the total <b>cache</b> <b>memory</b> budget divided by the number of files in the system. In other words, a perfect global caching gain is achievable in this regime and the performance corresponds to a system where all cache contents in the network are available to all receivers. For large <b>cache</b> <b>memories,</b> it is optimal to assign a positive <b>cache</b> <b>memory</b> to every receiver such that the weaker receivers are assigned larger <b>cache</b> <b>memories</b> compared to the stronger receivers. In this regime, the growth rate of the global capacity-memory tradeoff is further divided by the number of users, which corresponds to a local caching gain. Numerical indicate suggest that a uniform cache-assignment of the total <b>cache</b> <b>memory</b> is suboptimal in all regimes unless the BC is completely symmetric. For erasure BCs, this claim is proved analytically in the regime of small cache-sizes. Comment: Submitted to IEEE Transactions on Information Theor...|$|R
40|$|<b>Cache</b> <b>memories</b> {{have been}} {{extensively}} used {{to bridge the}} gap between high speed processors and relatively slow main memories. However, they are a source of predictability problems because of their dynamic and adaptive behavior, and thus need special attention to be used in hard-real time systems. A lot of progress has been achieved in the last ten years to statically predict the worst-case behavior of applications with respect to caches in order to determine safe and precise bounds on tasks WCETs and cache-related preemption delays. An alternative approach to cope with caches in real-time systems is to statically lock their contents such that memory access times and cache-related preemption times are predictable. In this paper, we propose two low-complexity algorithms for selecting the contents of statically-locked caches. We evaluate their performances and compare them with those of a state of the art <b>static</b> <b>cache</b> analysis method...|$|R
40|$|The first {{methods to}} bound {{execution}} time in computer systems with <b>cache</b> <b>memories</b> {{were presented in}} the late eighties [...] - twenty {{years after the first}} <b>cache</b> <b>memories</b> designed. Today, fifteen years later, methods has been developed to bound execution time with <b>cache</b> <b>memories</b> [...] . that were state-of-the-art twenty years ago. This report presents <b>cache</b> <b>memories</b> and real-time from the very basics to the state-of-the-art of <b>cache</b> <b>memory</b> design, methods to use <b>cache</b> <b>memories</b> in real-time systems and the limitations of current technology. Methods to handle intrinsic and extrinsic behavior on instruction and data caches will be presented and discussed, but also close issues like pipelining, DMA and other unpredictable hardware components will be briefly presented. No method is today able to automatically calculate a safe and tight Worst-Case Execution Time WCETc for any arbitrary program that runs on a modern high-performance system [...] - there are always cases where the method will cross into problems. Many of the methods can although give very tight WCETc or reduce the related problems under specified circumstances...|$|R
40|$|Patil, Kaustubh S. : Compositional <b>static</b> <b>cache</b> {{analysis}} using module-level abstraction (Under {{the direction of}} Dr. Frank Mueller). <b>Static</b> <b>cache</b> analysis is utilized for timing analysis to derive worst-case execution time of a program. Such analysis is constrained by the requirement of an inter-procedural analysis for the entire program. But the complexity of cycle-level simulations for entire programs currently restricts the feasibility of <b>static</b> <b>cache</b> analysis to small programs. Com-putationally complex inter-procedural analysis {{is needed to determine}} caching effects, which depend on knowledge of data and instruction references. <b>Static</b> <b>cache</b> simulation tradition-ally relies on absolute address information of instruction and data elements. This thesis presents a framework to perform worst-case <b>static</b> <b>cache</b> analysis for direct-mapped instruction caches using a module-level and compositional approach, thus addressing the issue of complexity of inter-procedural analysis for an entire program. The module-level analysis parameterizes the data-flow information in terms of the starting offset of a module. The compositional analysis stage uses this parameterized data-flow information for each module. Thus, the emphasis here is on handling most of the complexity in the module-level analysis and performing as little analysis as possible at the compositional level. The experimental results show that the compositional analysis framework provides equally accurate predictions when compared with the simulation approach that uses complete inter-procedural analysis. Compositional <b>static</b> <b>cache</b> {{analysis using}} module-level abstraction b...|$|R
40|$|Abstract- <b>Cache</b> <b>memory</b> {{performance}} analysis is a challenging topic upon first introduction. Students must synthesize {{a significant amount}} of computer architecture knowledge, comprehend reasonably complex replacement strategies, and analyze performance. We propose a programming exercise that has students develop a visual <b>cache</b> <b>memory</b> simulator and then use the simulator to analyze several memory reference trace files. Our student learning assessment measured the quality of each team programming exercise solution and each individual’s own cache {{performance analysis}}. In addition, the final exam has several questions related to <b>cache</b> <b>memory.</b> Early results indicate students achieve a better understanding of <b>cache</b> <b>memory</b> and its impact on performance...|$|R
40|$|It {{has been}} {{claimed that the}} {{execution}} time of a program can often be predicted more accurately on an uncached system than on a system with <b>cache</b> <b>memory</b> [5, 20]. Thus, caches are often disabled for critical real-time tasks to ensure the predictability required for scheduling analysis. This work shows that instruction caching can be exploited to gain execution speed without sacrificing predictability. A new method called <b>Static</b> <b>Cache</b> Simulation is introduced which uses control-flow {{information provided by the}} back-end of a compiler. This simulator statically predicts the caching behavior of {{a large portion of the}} instruction cache references of a program. In addition, a fetch-frommemory bit is added to the instruction encoding which indicates whether an instruction shall be fetched from the instruction cache or from main memory. This bitencoding approach provides a significant speedup in execution time (factor 3 - 8) over systems with a disabled instruction cache without any sacrifice in [...] ...|$|R
40|$|Recently, {{research}} on the static prediction of worstcase execution time (WCET) of programs has been extended from simple CISC to pipelined RISC processors, and from uncached architectures to direct-mapped instruction caches. This work goes one step further by introducing a framework to handle WCET prediction for set-associative caches. Generalizing the work of <b>static</b> <b>cache</b> simulation of direct-mapped caches to setassociative caches, a formalization of the new method is given and the operational characteristics are presented and discussed by example. WCET predictions for several programs are presented by combining the <b>static</b> <b>cache</b> analysis for set-associative caches with a timing analysis tool. This approach has the advantage that cache configuration details are handled by <b>static</b> <b>cache</b> simulation but remain transparent to the timing analyzer. It is shown that <b>static</b> <b>cache</b> analysis for setassociative caches results in just as tight timing predictions as reported for direct-mapped caches [...] . ...|$|R
30|$|No <b>cache</b> <b>memory</b> is used.|$|R
40|$|High {{performance}} {{is the major}} concern {{in the area of}} VLSI Design. <b>Cache</b> <b>memory</b> consumes the half of total power in various systems. Thus, the architecture behavior of the cache governs both high performance and low power consumption. Simulator simulates <b>cache</b> <b>memory</b> design in various formats with help of various simulators like simple scalar, Xilinx etc. This paper explores the issue and consideration involved in designing efficient <b>cache</b> <b>memory</b> and we have discussed the <b>cache</b> <b>memory</b> simulation behavior on various simulators. Memory design concept is becoming dominant; memory level parallism is one of the critical issues concerning its performance. We have to propose high performance cache simulation behavior for performance improvement for future mobile processors design and customize mobile devices...|$|R
40|$|Abstract—High {{performance}} {{is the major}} concern in VLSI Design. Thus, the architecture behavior of the cache governs both high performance and low power consumption. High performance simulator simulates <b>cache</b> <b>memory</b> design in various formats with help of various simulators like simplescalar, Xilinx, Top spice 8 etc. This paper explores the issue and consideration involved in designing the efficient <b>cache</b> <b>memory.</b> We have discussed the <b>cache</b> <b>memory</b> simulation behavior on various simulators. We propose high performance cache simulation behavior issues for future mobile processors design and customize mobile devices...|$|R
40|$|Embedded {{microprocessor}} <b>cache</b> <b>memories</b> {{suffer from}} limited observability and controllability creating problems during in-system test. The application of test algorithms for SRAM <b>memories</b> to <b>cache</b> <b>memories</b> thus requires opportune transformations. In this paper {{we present a}} procedure to adapt traditional march tests to testing the data and the directory array of k-way set-associative <b>cache</b> <b>memories</b> with LRU replacement. The basic idea is to translate each march test operation into an equivalent sequence of cache operations able to reproduce the desired marching sequence into the data and the directory array of the cach...|$|R
40|$|Abstract—In this paper, an {{efficient}} technique is proposed {{to manage the}} <b>cache</b> <b>memory.</b> The proposed technique introduces some modifications on the well-known set associative mapping technique. This modification requires a little alteration {{in the structure of}} the <b>cache</b> <b>memory</b> and on the way by which it can be referenced. The proposed alteration leads to increase the set size virtually and consequently to improve the performance and the utilization of the <b>cache</b> <b>memory.</b> The current mapping techniques have accomplished good results. In fact, there are still different cases in which <b>cache</b> <b>memory</b> lines are left empty and not used, whereas two or more processes overwrite the lines of each other, instead of using those empty lines. The proposed algorithm aims at finding {{an efficient}} way to deal with such problem...|$|R
40|$|For high {{performance}} processor design, <b>cache</b> <b>memory</b> size {{is an important}} parameter which directly affects performance and the chip area. Modeling performance and area is required for design tradeoff of <b>cache</b> <b>memory.</b> This paper describes a tool which calculates <b>cache</b> <b>memory</b> performance and area. A designer can try variety of cache parameters to complete the specification of a <b>cache</b> <b>memory.</b> Data examples calculated using this tool are shown. Key Words and Phrases: <b>Cache</b> <b>memory,</b> Design tradeoff, Memory modeling, CAD ii Copyright 1994 by Osamu Okuzawa and Michael J. Flynn iii Contents 1. Introduction 1 2. Related Work 1 3. Performance/Area Workbench 2 3. 1 Tool target 2 3. 2 Input 2 3. 3 Output 3 3. 4 Model & Calculation method 3 3. 5 Tool Implementation 6 3. 6 Using Manual 9 4. Results 10 4. 1 Tool Performance 10 4. 2 Calculation result examples and analysis 11 5. Conclusions 16 iv List of Figures 1 CPI and Area Calculation Program Configuration 7 2 Calculation Process 8 3 32 KB spl [...] ...|$|R
40|$|Recently, energy {{dissipation}} by microprocessors is getting larger, {{which leads to}} a serious problem in terms of allowable temperature and performance improvement for future microprocessors. <b>Cache</b> <b>memory</b> is effective in bridging a growing speed gap between a processor and relatively slow external main memory, and has increased in its size. Almost all of today's commercial processors, not only highperformance microprocessors but embedded ones, have onchip <b>cache</b> <b>memories.</b> However, {{energy dissipation}} in the <b>cache</b> <b>memory</b> will approach or exceed 50 % of the increasing total energy dissipation by processors. An important point to note is that, in the near future, static (leakage) energy will dominate the total energy consumption in deep sub-micron processes. This paper describes <b>cache</b> <b>memory</b> architecture, especially for on-chip multiprocessors, that achieves efficient reduction of leakage energy in <b>cache</b> <b>memories</b> by exploiting gated-Vdd control, software selfinvalidation for L 1 cache, and dynamic data compression for L 2 cache. The simulation results show that our techniques can reduce a substantial amount of leakage energy without large performance degradation...|$|R
50|$|In {{addition}} to the macro-instruction <b>cache</b> <b>memory</b> {{also found in the}} ND-100, the ND-110 had a unique implementation of <b>cache</b> <b>memory</b> on the micro-instruction level. The step known as mapping in the ND-100 was then avoided because the first micro-instruction word of a macro-instruction was written into the control store cache.|$|R
40|$|<b>Cache</b> <b>memories</b> {{bridge the}} growing access-time {{gap between the}} {{processor}} and the main <b>memory.</b> <b>Cache</b> <b>memories</b> use randomisation functions for two purposes: (i) to {{limit the amount of}} search when looking up an address in the cache and (ii) to interleave the access stream over multiple independent banks, allowing multiple simultaneous accesses...|$|R
40|$|The Web-based {{information}} systems, as {{the network}} traffic and slow remote servers {{can lead to}} long delays in the answer delivery. Client memory is largely used to cache data and minimize future interaction with the servers. In this paper, we propose an extended <b>cache</b> <b>memory</b> to store the frequently used data. We observe that the frequently used data contain the images in a Web site. In this paper we propose that all the data which is frequently used must be stored at user end in an extended <b>cache</b> <b>memory.</b> The extended <b>cache</b> <b>memory</b> {{may be in the}} form of pen drive, CD, DVD or it could be saved at user’s machine. The only difference to access the Web between traditional and in our way is that the user have to provide the path of the data which resides at the extended <b>cache</b> <b>memory...</b>|$|R
40|$|This {{document}} {{is a short}} review about the MESI protocol simulator. This simulator is a tool {{which is used to}} teach the <b>cache</b> <b>memory</b> coherence on the computer systems with hierarchical memory system. The MESI protocol is a well known method to the maintenance of the information coherence in the memory system. The MESI simulator is also used to explain the process of the <b>cache</b> <b>memory</b> location (in multilevel <b>cache</b> <b>memory</b> systems). In this paper, we explain the MESI protocol and we show how the simulator works. Besides, we present some classroom practices carried out by using the MESI simulator...|$|R
30|$|Data (D$): Read/Write Hits and Misses, <b>cache</b> <b>memory</b> {{hits and}} misses.|$|R
50|$|These Seagate {{models were}} fitted with 2 MB of <b>cache</b> <b>memory.</b>|$|R
40|$|We {{developed}} a new algorithm for route discovery, nodes management, and mobility handling for on-demand cache routing on mobile Ad-Hoc networks (MANET). We used Ad-Hoc On demand Distance Vector (AODV) protocol as the better known reactive protocol, as well as using Link State algorithm of the Optimize Link State Routing (OLSR) protocol together. We used two levels of <b>caches</b> <b>memory</b> L- 1 and L- 2 along with link state routing table for each node. Which maintaining by using the algorithm of OLSR, which working under the AODV protocol. For mobility handling, we used link state algorithm working under AODV to manage node addition, deletion and movement in the network efficiently. We used the Network simulator NS- 2 version 2. 29 to show the results comparing with the AODV used just <b>cache</b> <b>memory,</b> and comparing with AODV without <b>cache</b> <b>memory.</b> The results shows that our algorithms outperform comparing with AODV without <b>cache</b> <b>memories,</b> and AODV with two levels of <b>cache</b> <b>memory</b> on packet delivery rate, where the link state routing protocol is used to distribute and maintain routing information among various nodes within a domain by using two messages which are Hello messages and Topology Control messages (TC) ...|$|R
50|$|IBM and Hitachi {{models were}} fitted with 128 KB of <b>cache</b> <b>memory.</b>|$|R
40|$|AbstractCompact {{numerical}} schemes provide high-order {{solution of}} PDEs with low dissipation and dispersion. Computer implementation of these schemes requires numerous passes of data through <b>cache</b> <b>memory</b> that considerably reduces performance of these schemes. To reduce this difficulty, a novel {{algorithm is proposed}} here. This algorithm {{is based on a}} wavefront approach and sweeps through <b>cache</b> <b>memory</b> only twice...|$|R
5000|$|<b>Cache</b> <b>memory</b> error-correction {{of up to}} 4 errors per tag or 32-bit word ...|$|R
40|$|Nowadays, the {{computational}} systems (multi and uniprocessors) need {{to avoid}} the cache coherence problem. There are some techniques to solve this problem. The MESI cache coherence protocol is one of them. This paper presents a simulator of the MESI protocol which is used for teaching the <b>cache</b> <b>memory</b> coherence on the computer systems with hierarchical memory system and for explaining {{the process of the}} <b>cache</b> <b>memory</b> location in multilevel <b>cache</b> <b>memory</b> systems. The paper shows a description of the course in which the simulator is used, a short explanation about the MESI protocol and how the simulator works. Then, some experimental results in a real teaching environment are described...|$|R
