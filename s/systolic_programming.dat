3|16|Public
40|$|This paper {{presents}} the New Systolic Language {{as a general}} {{solution to the problem}} <b>systolic</b> <b>programming.</b> The language provides a simple programming interface for systolic algorithms suitable for di erent hardware platforms and software simulators. The New Systolic Language hides the details and potential systolic data streams. Data ows and systolic cell programs for the co-processor are integrated with host functions, enabling a single le to specify a complete systolic program. ...|$|E
40|$|The ReLacs {{language}} is a <b>systolic</b> <b>programming</b> language which simplifies the programmer's task by making explicit the data-flow os systolic algorithms and by exposing the data delivery mechanism. The underlying architecture model is different from other SIMD architectures in that it physically separates computation and data management. We introduce the ReLaCS language as a syntaxic and a semantic extension of the C language. We show in this article that the ReLaCS programming model provides a simple programming method for systolic algorithms, which is applicable {{to a variety of}} parallel machines...|$|E
40|$|This work {{concerns}} {{the application of}} new computer architectures to the creation and manipulation of high-quality audio bandwidth signals. The configuration of both the hardware and software in such systems falls under consideration in the three major sections which present increasing levels of algorithmic concurrency. In the first section, the programs which are described are distributed in identical copies across an array of processing elements; these programs run autonomously, generating data independently, but with control parameters peculiar to each copy: this type of concurrency {{is referred to as}} isonomic}The central section presents a structure which distributes tasks across an arbitrary network of processors; the flow of control in such a program is quasi- indeterminate, and controlled on a demand basis by the rate of completion of the slave tasks and their irregular interaction with the master. Whilst that interaction is, in principle, deterministic, it is also data-dependent; the dynamic nature of task allocation demands that no a priori knowledge of the rate of task completion be required. This type of concurrency is called dianomic? Finally, an architecture is described which will support a very high level of algorithmic concurrency. The programs which make efficient use of such a machine are designed not by considering flow of control, but by considering flow of data. Each atomic algorithmic unit is made as simple as possible, which results in the extensive distribution of a program over very many processing elements. Programs designed by considering only the optimum data exchange routes are said to exhibit systolic^ concurrency. Often neglected in the study of system design are those provisions necessary for practical implementations. It was intended to provide users with useful application programs in fulfilment of this study; the target group is electroacoustic composers, who use digital signal processing techniques in the context of musical composition. Some of the algorithms in use in this field are highly complex, often requiring a quantity of processing for each sample which exceeds that currently available even from very powerful computers. Consequently, applications tend to operate not in 'real-time' (where the output of a system responds to its input apparently instantaneously), but by the manipulation of sounds recorded digitally on a mass storage device. The first two sections adopt existing, public-domain software, and seek to increase its speed of execution significantly by parallel techniques, with the minimum compromise of functionality and ease of use. Those chosen are the general- purpose direct synthesis program CSOUND, from M. I. T., and a stand-alone phase vocoder system from the C. D. P [...] (^ 4) In each case, the desired aim is achieved: to increase speed of execution by two orders of magnitude over the systems currently in use by composers. This requires substantial restructuring of the programs, and careful consideration of the best computer architectures on which they are to run concurrently. The third section examines the rationale behind the use of computers in music, and begins with the implementation of a sophisticated electronic musical instrument capable of a degree of expression at least equal to its acoustic counterparts. It seems that the flexible control of such an instrument demands a greater computing resource than the sound synthesis part. A machine has been constructed with the intention of enabling the 'gestural capture' of performance information in real-time; the structure of this computer, which has one hundred and sixty high-performance microprocessors running in parallel, is expounded; and the <b>systolic</b> <b>programming</b> techniques required to take advantage of such an array are illustrated in the Occam programming language...|$|E
40|$|Generating {{imperative}} <b>programs</b> from <b>systolic</b> specifications {{implies the}} use of a memory-efficient model for representing data used in the original, single-assignment problem. We present a new method of generating data structures, designed for use in <b>systolic</b> <b>program</b> compilers for distributed-memory parallel computers. Our method is based on the mathematical properties of <b>systolic</b> <b>programs</b> and consists of a set of algebraically defined elementary transformations. Using these transformations, the total size of data structures can be optimized, allowing very memory-efficient code to be produced. The method has been implemented as a part of an experimental compiler of the Alpha language, currently under development. Key-words: parallel <b>programming,</b> <b>systolic</b> arrays, distributed memory architectures (R'esum'e : tsvp) Centre National de la Recherche Scientifique Institut National de Recherche en Informatique (URA 227) Universit e de Rennes 1 [...] Insa de Rennes et en Automatique [...] unit e d [...] ...|$|R
40|$|We {{describe}} a methodology for mapping linear recurrence equations to {{a spectrum of}} systolic architectures. First, we design a <b>systolic</b> <b>program</b> in a very general architecture referred to as Basic Systolic Architecture and establish the correctness of the implementation. Next, we show how efficient transformations/implementations of <b>programs</b> for different <b>systolic</b> architectures can be obtained through transformations such as projections and translations. 1 Introduction Designing <b>systolic</b> <b>programs</b> has been an area of considerable research [1, 2, 3, 5, 6, 7, 8, 9]. Among works cited above, the results of [8, 9] are of relevance to our work. In [8], Quinton gives a methodology to map a system of uniform recurrence equation into systolic architecture. This method derives a prototype systolic architecture for a given recurrence equation. However, it suffers {{from the fact that}} it is not easy to map the solutions of linear recurrence equations (LRE) to a given systolic architecture particular [...] ...|$|R
40|$|A {{scheme is}} {{presented}} which transforms <b>systolic</b> <b>programs</b> with a two-dimensional structure to one dimension. The elementary {{steps of the}} transformation are justified by theorems {{in the theory of}} communicating sequential processes and the scheme is demonstrated with an example in occam: matrix composition/decomposition. 1 In t roduct ion We combine two types of formal refinement to transform a two-dimensional <b>systolic</b> <b>program</b> to one dimension. Systolic array ~ are particularly regular distributed processor networks capable of processing large amounts of data quickly by accepting streams of input and producing streams of output [6]. Typical applications are to image or signal processing; ours is an algorithm which subsumes matrix composition and decomposition. Systolic arrays are usually realized in hardware. We are interested inrealizing them in 8 oftware, because then they can run on one of the families of distributed compu~rs (now plentiful) capable of emulating systolic arrays. We are led to express uch software in a distributed progralrm'ring language that provides constructs for process definition and communication. The productio...|$|R
40|$|ToonTalk ™ is a {{general-purpose}} {{concurrent programming}} {{system in which}} the source code is animated and the programming environment is like a video game. Every abstract computational aspect is mapped into a concrete metaphor. For example, a computation is a city, an active object or agent is a house, inter-process communication represented by birds carrying messages between houses, a method or clause is a robot trained by the user and so on. The programmer controls a “programmer persona ” in this video world to construct, run, observe, debug, and modify programs. ToonTalk has been described in detail elsewhere [1]. Here we show how <b>systolic</b> <b>programs</b> can be constructed and animated in ToonTalk. Systolic computations run on multiple processors connected in...|$|R
40|$|We {{present a}} simple method for {{developing}} parallel and <b>systolic</b> <b>programs</b> from data dependence. We derive sequences of parallel computations and communications {{based on data}} dependence and communication delays, and minimize the communication delays and processor idle time. The potential applications for this method include supercompiling, automatic development of parallel <b>programs,</b> and <b>systolic</b> array design. 1 Introduction Given a sequential program consisting of a loop, or a set of equations that recursively define an array, we want to develop a parallel program for a shared or distributed memory parallel computer. There are two problems here. First we have to reveal the parallelism and find the sequences of parallel computations. Second, for distributed memory parallel computers, we want to schedule the computations and communications at compile time so that all data needed arrive before a computation is scheduled to start, and minimize processor idle time and overall communications [...] . ...|$|R
40|$|We {{present an}} {{automatic}} method for mapping {{a system of}} linear recurrence equations onto systolic architectures. First, we show that systolic architectures {{can be derived from}} linear recurrence equations using the notion of directed recurrence equations. Next, we provide a procedure called CUBIZATION to achieve better performance while mapping such equations. The CUBIZATION procedure is completely automated and can be implemented to design <b>systolic</b> <b>programs</b> for a very general architecture referred to as Basic Systolic Architecture (BSA). Using BSA, we obtain specific target architectures. The method is illustrated using a running example of Gauss-Jordan diagonalization. 1 Introduction In [6], we proposed a method to map a system of directed recurrence equations (DRE), a subclass of linear recurrence equations (LRE), which properly includes uniform recurrence equations onto systolic architecture. Here, we extend our method to LREs where the dependency vector at a point is a linear fun [...] ...|$|R
40|$|In this paper, we {{describe}} a systematic method for mapping {{the problem of}} symmetrizing Hessenberg matrices onto systolic architectures. The starting point of our method is a graphical abstraction {{of the system of}} linear recurrence equations that specifies the problem. Using a procedure called cubization, we transform the dependency graph of the problem into a graph we term the Modified Dependency Graph. Using the modified dependency graph, we design <b>systolic</b> <b>programs</b> on a very general architecture referred to as a basic systolic architecture. Next, we map the basic systolic architecture onto a given systolic architecture using a set of semantics-preserving transformations and hence, the correctness of the solution comes for free. Finally, we compare the architectures for symmetrizing Hessenberg matrices obtained by our method with those proposed previously. 1 Introduction The aim {{of this paper is to}} describe a systematic approach for mapping the problem of symmetrizing Hessenberg matr [...] ...|$|R
40|$|In {{this paper}} we {{consider}} <b>systolic</b> <b>programs</b> {{of the most}} common DSP (convolution, FIR, IIR, FFT) and Matrix (multiplication, triangularisation, linear equation solving, modified Faddeev algorithm) algorithms, executed on systolic arrays of various topologies (linear, 2 D mesh, hexagonal). We examine the algorithm-specific parameters (number of I/O paths, unit delays) and program-dependent parameters (program length, data location requirements, basic block lengths, branch behaviour, instruction usage, computation to communication ratio) of our program set, executed on a single processing-cell of systolic arrays. The analysis is based on the static object code We found that basic block lengths are 17. 1 (DSP) and 8. 4 (Matrix) instructions long. The Divide/Square Root operations {{play a major role in}} Matrix algorithms (more than 15 % of the weighted instruction set). Inter-cell communication must be efficient, since the computation to communication ratio is only 1. 2 [...] 1. 4 and is orders of ma [...] ...|$|R
40|$|We {{propose a}} {{systolic}} array for dynamic programming {{which is a}} technique for solving combinatorial optimization problems. We derive a systolic array for single-source shortest path problem, SA-SSSP, and then show that the systolic array serves as dynamic <b>programming</b> <b>systolic</b> array which is applicable to any dynamic programming problem by developing a systolic array for 0 - 1 knapsack problem, SA- 01 KS, with SA-SSSP for a basis...|$|R
40|$|Direct VLSI implementa. tion of pipelined (systolic) pro-cessor arrays {{can lead}} Lo an "over parallclized " design caus-ing the chip Lo have unused or underutilized area. Processor {{displacement}} design is a methodology thal provides a spec-Lrum of designs with differing lime-area trade ofTs. The methodology is motivated, presented in detail, and illus-trated by several examples. Direct {{experience for the}} Tran-sitive Closure and Dynamic <b>Programming</b> <b>systolic</b> arrays is presented...|$|R
40|$|SDEF, a <b>systolic</b> array <b>programming</b> system, is presented. It is {{intended}} to provide 1) sys-tolic algorithm researchers/developers with an executable notation, and 2) the software systems community with a target notation {{for the development of}} higher level systolic software tools. The design issues associated with such a programming system are identified. A spacetime representa-tion of systolic computations is described briefly in order to motivate SDEF’s program notation. The programming system treats a special class of systolic computations, called atomic systolic computations, any one of which can be specified as a set of properties: the computation’s 1) index set (S), 2) domain dependencies (D), 3) spacetime embedding (E), and nodal function (F). These properties are defined and illustrated. SDEF’s user interface is presented. It comprises an editor, a translator, a domain type database, and a systolic array simulator used to test SDEF programs. The system currently runs on a Sun 3 / 50 operating under Unix and Xwindows. Key design choices affecting this implementation are described. SDEF is designed for portability. The problem of porting it to a Transputer array is discussed...|$|R
40|$|The model {{presented}} here for <b>systolic</b> parallelization of <b>programs</b> with multiple loops aims at compilation of sequential programs for parallel computer architectures which support fine-grained communication. Regular loop nests in {{programs can be}} subjected to space-time loop transformations as has been examined {{in the area of}} systolic parallelization. The aim of this work is to show how this kind of parallelized regular loop nests interact and can be combined to a parallel solution for the whole program. For this purpose the interface of loop nests using or producing array variable elements regularly is examined and the necessary reorganization work between loop nests described. The qualitative analysis describes access patterns of array variables, their position patterns in the parallelized loop nest, and communication patterns to transport data elements between parallelized loops. A program containing two convolution calculations is used as an example. While this report focuses on qual [...] ...|$|R
40|$|Wire-exposed, {{programmable}} microarchitectures including Trips [11], Smart Memories [8], and Raw [13] {{offer an}} opportunity to schedule instruction execution and data movement explicitly. This paper proposes stream algorithms, which, along with a decoupled systolic architecture, provide an excellent match for the physical and technological constraints of single-chip tiled architectures. Stream algorithms enable <b>programmed</b> <b>systolic</b> computations for different problem sizes, without incurring the cost of memory accesses. To that end, we decouple memory accesses from computation and move the memory accesses off the critical path. By structuring computations in systolic phases, and deferring memory accesses to dedicated memory processors, stream algorithms can solve many regular problems with varying sizes on a constant-sized tiled array. Contrary to common sense, the compute efficiency of stream algorithms increases as we {{increase the number of}} processing elements. In particular, we show that the compute efficiency of stream algorithms can approach 100 % asymptotically, that is for large numbers of processors and appropriate problem size. ...|$|R
40|$|For {{the purpose}} of {{molecular}} dynamics simulations of large biopolymers we have built a parallel computer with a systolic loop architecture, based on Transputers as computational units, and have programmed it in Occam 11. The computational nodes of the computer are linked together in a <b>systolic</b> ring. The <b>program</b> based on this. topology for large biopolymers increases its computational throughput nearly linearly {{with the number of}} computational nodes. The program developed is closely related to the simulation programs CHARMM and XPLOR, the input files required (force field, protein structure file, coordinates) and output files generated (sets of atomic coordinates representing dynamic trajectories and energies) are compatible with the corresponding files of these programs. Benchmark results of simulations of biopolymers comprising 66, 568, 3 634, 5 797 and 12 637 atoms are compared with XPLOR simulations on conventional computers (Cray, Convex, Vax). These results demonstrate that the software and hardware developed provide extremely cost effective biopolymer simulations. We present also a simulation (equilibrium of X-ray structure) of the complete photosynthetic reaction center of Rhodopseudomonus viridis (12 637 atoms). The simulation accounts for the Coulomb forces exactly, i. e. no cut-off had been assumed...|$|R
40|$|Abstract. This paper {{presents}} a novel method {{to construct a}} dynamic single assignment (DSA) form of array-intensive, pointer-free C programs (or in any other procedural language). A program in DSA form does not perform any destructive update of scalars and array elements, i. e., each element is written at most once. As DSA makes the dependencies between variable references explicit, it facilitates complex analyses and optimizations of programs. This makes it a preferred intermediate form {{for a number of}} compiler techniques of growing importance: parallelization, <b>systolic</b> array design, <b>programming</b> heterogeneous architectures, memory optimization, and verification of source code transformations. Existing transformations into DSA perform a complex data flow analysis that is exponential in the program size and that only accepts input programs where all loop bounds, array indexation and conditionals are (possibly piecewise) affine expressions in the loop iterators. Our method removes irregularities from the data flow by adding copy assignments to the program, and then it can use simple data flow analyses. The contributions of this paper are threefold. The DSA transformation presented scales very well with growing program sizes. It is quadratic in the program size –though our experiments indicate that it tends towards linearity – and is polynomial in the loop nest depth, while other existing methods have exponential complexity. Our DSA transformation overcomes a number of important limitations of existing methods; it allows any expression for loop bounds, indexation and conditionals, as long as constant bounds can be found for loop iterators. We have implemented the method and it is being used in the context of memory optimization and verification of those optimizations. ...|$|R
40|$|Bibliography: pages [82]- 88. The {{purpose of}} this study was to examine the acute {{cardiovascular}} and metabolic responses to passive inversion and inverted exercise, and the physiological adaptations to inversion following an inversion training <b>program.</b> <b>Systolic</b> blood pressure (SBP), diastolic blood pressure (DBP), heart rate (HR), and oxygen consumption (V̇O₂) were measured and recorded in 19 young healthy males (average age 20. 31 years) in seven positions: (a) standing passive (STD), (b) inverted passive (INV), (c) standing recovery postinversion (SRPI), (d) standing exercise (SDE), (e) standing recovery poststanding exercise (SRPSE), (f) inverted exercise (INVE), and (g) inverted recovery postinverted exercise (IRPIE). Ten of the subjects participated in a 5 -week inversion training program. An ANOVA determined statistical differences in the acute responses of all 19 subjects. Passive inversion elicited significant (p̲ <. 05) mean increases in SBP of 28. 76 mmHg, DBP of 16. 21 mmHg, and V̇O₂ of 0. 73 ml·kg·min⁻¹ and a significant decrease (p̲ <. 05) in HR of 22. 36 bpm compared to STD. The differences were attributed to the hemodynamic effects of inversion. Systolic blood pressure and DBP increased during both STDE and INVE (+ 18. 97 / 1. 03 and + 12. 2 / 4. 18 mmHg, respectively) and were significantly greater during INVE compared to STDE (p̲ <. 05). Inverted exercise HR increased to only 2. 05 bpm above the STD rate. Systolic blood pressure and DBP were significantly greater (p̲ <. 05) during IRPIE compared with SRPI and SRPSE. In SRPI and SRPSE SBP, DBP, and HR returned to the level observed in the respective passive positions. HR in IRPIE remained elevated above the INV rate. Posttraining results were analyzed using an ANCOVA with the pretraining values as covariates. There were no significant differences in SBP, DBP, HR, and V̇O₂ between the two groups in INV, INVE, and IRPIE following 5 weeks of inversion training. Thus, INV significantly increased SBP, DBP, and V̇O₂ and decreased HR, due primarily to the hemodynamic effect of inversion. Inverted exercise elicited significantly greater SBP and DBP responses than STDE but no physiological adaptations to inversion training occurred. M. A. (Master of Arts...|$|R

