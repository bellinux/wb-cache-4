0|10000|Public
5000|$|Compositional models: <b>Such</b> <b>models</b> may <b>be</b> <b>defined</b> {{within the}} {{framework}} of information algebras: https://arxiv.org/abs/1612.02587 ...|$|R
5000|$|The {{use of a}} {{historical}} perspective in its design approach allows the team for continual analysis and interpretation of a given culture and its architectural heritage. In contrast to Socialist and Le Corbusier#urbanism urban planning models, Ricardo Bofill Taller de Arquitectura proposes a Mediterranean city <b>model.</b> <b>Such</b> <b>model</b> <b>is</b> <b>defined</b> by a network of public space that connects proportionally scaled streets and squares. “Mediterranean city” {{is at the same}} time cast as an interregional synthesis, a complex region of interrelating regions. This intense set of cultural, social and material interactions highlights the capitalist and cosmopolitan dimensions of the Mediterranean.” ...|$|R
40|$|We {{consider}} a wide class of increasing Lévy processes perturbed {{by an independent}} Brownian motion as a degradation <b>model.</b> <b>Such</b> family contains almost all classical degradation models considered in the literature. Classically failure time associated to <b>such</b> <b>model</b> <b>is</b> <b>defined</b> as the hitting time or the first-passage time of a fixed level. Since sample paths are not in general increasing, we {{consider a}}lso the last-passage time as the failure time following a recent work by Barker and Newby. We address here the problem of determining {{the distribution of the}} first-passage time and of the last-passage time. In the last section we consider a maintenance policy for <b>such</b> <b>models...</b>|$|R
40|$|International audienceWe {{consider}} a wide class of increasing Lévy processes perturbed {{by an independent}} Brownian motion as a degradation <b>model.</b> <b>Such</b> family contains almost all classical degradation models considered in the literature. Classically failure time associated to <b>such</b> <b>model</b> <b>is</b> <b>defined</b> as the hitting time or the first-passage time of a fixed level. Since sample paths are not in general increasing, we {{consider a}}lso the last-passage time as the failure time following a recent work by Barker and Newby (Reliab Eng Syst Saf 94 : 33 - 43, 2009). We address here the problem of determining {{the distribution of the}} first-passage time and of the last-passage time. In the last section we consider a maintenance policy for <b>such</b> <b>models.</b> © 2013 Springer Science+Business Media New York...|$|R
3000|$|... [...]. Nevertheless, even if <b>such</b> {{likelihood}} <b>model</b> can <b>be</b> <b>defined,</b> its evaluation {{may be very}} computationally inefficient. Instead of that, {{a fitness}} function [...]...|$|R
40|$|Abstract This note {{extends the}} {{decorated}} Teichmüller theory {{developed by the}} author to the setting of surfaces with boundary, and there is non-trivial new structure discovered. The main new result (beyond extension of the theory to bordered surfaces) identifies two models for the decorated moduli space of a bordered surface; one <b>such</b> <b>model</b> <b>is</b> <b>defined</b> in analogy to earlier work, {{and the other is}} described by spaces of pairs of distinct labeled points in each component of the geodesic boundary of a hyperbolic surface. The explicit isomorphism relies upon points equidistant to suitable triples of horocycles. As a consequence of this identification, the arc complex of a bordered surface, which has arisen in other related contexts, is found to compactify a space which is homotopy equivalent to the quotient of moduli space by a natural circle action...|$|R
40|$|We {{summarize}} the main {{results of a}} broad analysis on electrostatic, spherically symmetric (ESS) solutions of a class of non-linear electrodynamics models minimally coupled to gravitation. <b>Such</b> <b>models</b> <b>are</b> <b>defined</b> as arbitrary functions of the two quadratic field invariants, constrained by several physical admissibility requirements, and split into different families according to the behaviour of these lagrangian density functions in vacuum and on the boundary of their domains of definition. Depending on these behaviours the flat-space energy of the ESS field can be finite or divergent. For each model we qualitatively study the structure of its associated gravitational configurations, which can be asymptotically Schwarzschild-like or with an anomalous non Schwarzschild-like behaviour at r →∞ (but being asymptotically flat and well behaved anyhow). The extension of these results to the non-abelian case is also briefly considered. Comment: 8 pages, 2 figures, contribution to NED 14 Proceeding...|$|R
5000|$|A <b>model</b> {{may thus}} <b>be</b> <b>defined</b> where AUTOSAR {{elements}} represent the software architecture and EAST-ADL elements extend the AUTOSAR model with orthogonal aspects and represents abstract system information through e.g. function and feature <b>models.</b> <b>Such</b> <b>model</b> can <b>be</b> <b>defined</b> in UML, by applying both an EAST-ADL profile [...] and an AUTOSAR profile, or in a domain specific tool {{based on a}} merged AUTOSAR and EAST-ADL metamodel.|$|R
40|$|This paper {{introduces}} Schur-constant {{equilibrium distribution}} models of dimension n for arithmetic non-negative random variables. <b>Such</b> a <b>model</b> <b>is</b> <b>defined</b> through the (several orders) equilibrium distributions of a univariate survival function. First, the bivariate case is considered and analyzed in depth, stressing the main {{characteristics of the}} Poisson case. The analysis is then extended to the multivariate case. Several properties are derived, including the implicit correlation {{and the distribution of}} the sum...|$|R
40|$|This paper {{introduces}} {{a class of}} Schur-constant survival models, of dimension n, for arithmetic non-negative random variables. <b>Such</b> a <b>model</b> <b>is</b> <b>defined</b> through a univariate survival function that is shown to be n-monotone. Two general representations are obtained, by conditioning on {{the sum of the}} n variables or through a doubly mixed multinomial distribution. Several other properties including correlation measures are derived. Three processes in insurance theory are discussed for which the claim interarrival periods form a Schur-constant model. SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|R
40|$|While EOQ-type {{inventory}} <b>models</b> <b>are</b> 'robust' to {{cost and}} demand estimation errors, cost definition errors can be severe and have {{significant impact on}} decisions. Cost figures for <b>such</b> <b>models</b> must <b>be</b> carefully <b>defined</b> {{in order to avoid}} common accounting pitfalls. An example problem illustrating a common pitfall using average, rather than marginal, cost is presented...|$|R
40|$|AbstractIn {{this paper}} we {{investigate}} the convex hull of single node variable upper-bound flow models with allowed configurations. <b>Such</b> a <b>model</b> <b>is</b> <b>defined</b> {{by a set}} Xρ(Z) ={(x,z) ∈Rn×Z|∑j= 1 nxjρd, 0 ⩽xj⩽ujzj,j= 1,…,n}, where ρ is one of ⩽, = or ⩾, and Z⊂{ 0, 1 }n consists of the allowed configurations. We consider the case when Z consists of affinely independent vectors. Under this assumption, a characterization of the non-trivial facets of the convex hull of Xρ(Z) for each relation ρ is provided, along with polynomial time separation algorithms. Applications in scheduling and network design are also discussed...|$|R
5000|$|The {{properties}} {{of objects in}} general in a specific computer programming language, technology, notation or methodology that uses them. Examples <b>are</b> the object <b>models</b> of Java, the Component Object Model (COM), or Object-Modeling Technique (OMT). <b>Such</b> object <b>models</b> <b>are</b> usually <b>defined</b> using concepts such as class, generic function, message, inheritance, polymorphism, and encapsulation. There is an extensive literature on formalized object models as {{a subset of the}} formal semantics of programming languages.|$|R
40|$|We study Finsler {{black holes}} induced from Einstein gravity as {{possible}} effects of quantum spacetime noncommutativity. <b>Such</b> Finsler <b>models</b> <b>are</b> <b>defined</b> by nonholonomic frames not on tangent bundles but on (pseudo) Riemannian manifolds being compatible with standard theories of physics. We focus on noncommutative deformations of Schwarzschild metrics into locally anisotropic stationary ones with spherical/rotoid symmetry. There are derived the conditions when black hole configurations can be extracted from {{two classes of}} exact solutions depending on noncommutative parameters. The first class of metrics <b>is</b> <b>defined</b> by nonholonomic deformations of the gravitational vacuum by noncommutative geometry. The second class of such solutions is induced by noncommutative matter fields and/or effective polarizations of cosmological constants. Comment: latex 2 e, 11 pt, 29 pages, v 3 accepted to Class. Quant. Grav...|$|R
50|$|In {{computing}} {{the term}} object model {{has a distinct}} second meaning of the general properties of objects in a specific computer programming language, technology, notation or methodology that uses them. For example, the Java object model, the COM object model, or the object <b>model</b> of OMT. <b>Such</b> object <b>models</b> <b>are</b> usually <b>defined</b> using concepts such as class, message, inheritance, polymorphism, and encapsulation. There is an extensive literature on formalized object models as {{a subset of the}} formal semantics of programming languages.|$|R
40|$|We {{present a}} novel method for {{tracking}} myocardial motion from volumetric ultrasound data based on non-rigid image registration using an anatomical free-form deformation model. Traditionally, the B-spline control points of <b>such</b> a <b>model</b> <b>are</b> <b>defined</b> on a rectangular grid in Cartesian space. This arrangement may be suboptimal as it treats the blood pool and myocardium similarly {{and as it}} enforces spatial smoothness in non-physiological directions. In this work, the basis functions are locally oriented along the radial, longitudinal and circumferential direction of the endocardium. This formulation allows us to model the left ventricular motion more naturally. We obtained encouraging accuracy results for the simulated models, with average errors of 0. 8 ± 0. 6 mm (10 % relatively) and 0. 5 ± 0. 4 mm (15 % relatively) compared to the ground truth in high and low contractility models respectively. status: publishe...|$|R
40|$|Linear models {{where the}} {{response}} is a function and the predictors are vectors are useful in analyzing data from designed experiments and other situations with functional observations. Residual analysis and diagnostics <b>are</b> considered for <b>such</b> <b>models.</b> Studentized residuals <b>are</b> <b>defined</b> and their properties are studied. Chi-square quantile-quantile plots are proposed to check the assumption of Gaussian error process and outliers. Jackknife residuals and an associated test are proposed to detect outliers. Cook’s distance <b>is</b> <b>defined</b> to detect influential cases. The methodology is illustrated by an example from a robust design study...|$|R
40|$|Abstract — Probabilistic {{graphical}} <b>models</b> <b>are</b> {{a fundamental}} tool in statistics, machine learning, signal processing, and control. When <b>such</b> a <b>model</b> <b>is</b> <b>defined</b> on a {{directed acyclic graph}} (DAG), one can assign a partial ordering to the events occurring in the corresponding stochastic system. Based {{on the work of}} Judea Pearl and others, these DAG-based “causal factorizations ” of joint probability measures have been used for characterization and inference of functional dependencies (causal links). This mostly expository paper focuses on several connections between Pearl’s formalism (and in particular his notion of “intervention”) and information-theoretic notions of causality and feedback (such as causal conditioning, directed stochastic kernels, and directed information). As an application, we show how conditional directed information can be used to develop an information-theoretic version of Pearl’s “back-door ” criterion for identifiability of causal effects from passive observations. This suggests that the back-door criterion {{can be thought of as}} a causal analog of statistical sufficiency. I...|$|R
40|$|Abstract-The {{acquired}} {{immune system}} provides a microcosm of Darwinian natural selection at the cellular level. When {{more than one of}} these somatic learning systems is considered in a population, an array of interesting effects and interactions become apparent. This work elaborates on a series of multiple immune system models defined in a previous work. A framework for <b>such</b> <b>models</b> <b>is</b> presented that <b>defines</b> mechanisms such as pathogen exposure regimes, immune effector sharing schemes, and evolutionary mechanisms. The previously <b>defined</b> <b>models</b> <b>are</b> re-defined using this framework, although the models described represent {{a small fraction of the}} combinations of the defined mechanisms...|$|R
40|$|Abstract: Environment {{monitoring}} for {{the evaluation}} of it’s degradation needs an auxiliary (supportive) implement such as numerical method. Three-dimensional <b>modelling</b> methods <b>are</b> especially useful for this. <b>Such</b> <b>modelling</b> <b>is</b> a <b>defined</b> set of mathematical and logic relations that determine quantitative relationships between characteristics and factors of <b>model</b> and <b>is</b> a modern research technique. Application of this technique in monitoring of for instead the distribution of contaminant concentrations in different environment elements such as air, underground and surface waters, soils and plants decides about correctness and specification of consideration concerning prognostic analysis and alteration dynamics. Three-dimensional display of research effects of heavy metals contamination of soils on the area degraded by industrial activity is presented in the paper. The contents of Pb, Zn, Ba and As were examined. These contents are characteristic for the monitored area. The obtained research results were used to create the project accomplished by the use of software package produced by INTERGRAPH Microstation MGE. Key words: monitoring, pollution, modelling. ...|$|R
40|$|We {{investigate}} {{the suitability of}} statistical model checking techniques {{for the analysis of}} probabilistic models of software product lines with complex quantitative constraints and advanced feature installation options. <b>Such</b> SPL <b>models</b> <b>are</b> <b>defined</b> in the probabilistic feature-oriented language QFLan. QFLan is a rich process algebra whose operational behaviour interacts with a store of constraints and as such it allows to separate product configuration from product behaviour. The resulting probabilistic configurations and behaviour converge seamlessly in a semantics based on discrete-time Markov chains, thus enabling quantitative analysis. To this aim, we combine a Maude implementation of QFLan, integrated with Microsoft 2 ̆ 7 s SMT constraint solver Z 3, with the distributed statistical model checker MultiVeStA. This enables analyses that range from the likelihood of specific behaviour to the expected average cost of products, in terms of feature attributes. We illustrate our approach by performing quantitative analyses on a bikes product line case study...|$|R
40|$|Probabilistic {{graphical}} <b>models</b> <b>are</b> {{a fundamental}} tool in statistics, machine learning, signal processing, and control. When <b>such</b> a <b>model</b> <b>is</b> <b>defined</b> on a {{directed acyclic graph}} (DAG), one can assign a partial ordering to the events occurring in the corresponding stochastic system. Based {{on the work of}} Judea Pearl and others, these DAG-based "causal factorizations" of joint probability measures have been used for characterization and inference of functional dependencies (causal links). This mostly expository paper focuses on several connections between Pearl's formalism (and in particular his notion of "intervention") and information-theoretic notions of causality and feedback (such as causal conditioning, directed stochastic kernels, and directed information). As an application, we show how conditional directed information can be used to develop an information-theoretic version of Pearl's "back-door" criterion for identifiability of causal effects from passive observations. This suggests that the back-door criterion {{can be thought of as}} a causal analog of statistical sufficiency. Comment: 8 pages, uses ieeeconf. cls; to appear in Proc. 49 th Annual Allerton Conf. on Communication, Control and Computing (2011...|$|R
30|$|The {{models of}} secular {{variation}} {{submitted to the}} previous generation of IGRF (Finlay et al. 2010 b) were {{for the most part}} derived from time-dependent models of the main field itself. In this case, the submitted candidate secular variation was then the instantaneous secular variation at the terminal epoch of the era over which <b>such</b> field <b>models</b> <b>were</b> <b>defined.</b> Those <b>models</b> expressed the time dependency of the internal geomagnetic field either by means of a Taylor expansion or by resorting to splines. An exception was the candidate secular variation model proposed by Kuang et al. (Kuang et al. 2010) in which the time dependency was controlled by an underlying numerical model of the geodynamo. The arsenal of techniques designed in order to combine data with a prognostic numerical model goes by the generic name of data assimilation. Data assimilation, at the heart of numerical weather prediction (e.g., Talagrand et al. 1997), has raised growing interest in the context of terrestrial magnetism over the last decade (Fournier et al. 2010; Hulot et al. 2015).|$|R
40|$|The space-fractional and the time-fractional Poisson {{processes}} <b>are</b> two well-known <b>models</b> of fractional evolution. They can {{be constructed}} as standard Poisson processes with the time variable replaced by a stable subordinator and its inverse, respectively. The aim {{of this paper is}} to study non-homogeneous versions of <b>such</b> <b>models,</b> which can <b>be</b> <b>defined</b> by means of the so-called multistable subordinator (a jump process with non-stationary increments), denoted by H. Firstly, we consider the Poisson process time-changed by H and we obtain its explicit distribution and governing equation. Then, by using the right-continuous inverse of H, we define an inhomogeneous analogue of the time-fractional Poisson process...|$|R
40|$|Abstract. This paper {{describes}} how enterprise <b>models</b> can <b>be</b> made suitable for monitoring and controlling IT security at runtime. A holistic <b>modeling</b> method <b>is</b> proposed that extends enterprise models with runtime information, {{turning them into}} dashboards for managing security incidents and risks, and supporting decision making at runtime. The requirements of <b>such</b> a <b>modeling</b> method <b>are</b> <b>defined</b> and an existing enterprise <b>modeling</b> language <b>is</b> extended with relevant security concepts that also capture runtime information to satisfy these requirements. Subsequently, the resulting <b>modeling</b> method <b>is</b> evaluated against the previously <b>defined</b> requirements. It <b>is</b> also shown that common metamodeling frameworks are not suitable for implementing a modeling environment that results in suitable IT security dashboards. This leads to suggesting implementation of the modeling environment using the eXecutable Modeling Facility. ...|$|R
40|$|Abstract. Separation of {{concerns}} {{is a very}} important principle of software engineering that, in its most general form, refers to the ability to identify, encapsulate and manipulate those parts of a software system that are relevant to a particular concept, goal, or purpose. Aspect Oriented Programming provides means to encapsulate concerns which cannot be modularized using traditional programming techniques. These concerns are called crosscutting concerns. Aspect Mining is a research direction that tries to identify crosscutting concerns in legacy systems. The aim {{of this paper is to}} introduce a new formal model for partitioning based aspect mining. <b>Such</b> a <b>model</b> <b>was</b> not <b>defined</b> in the literature, yet. The applicability of the proposed formal <b>model</b> <b>is</b> studied on three different aspect mining techniques...|$|R
40|$|Modeling {{the spread}} of infections on {{networks}} is a well-studied and important field of research. Most infection and diffusion models require a real value or probability {{on the edges of}} the network as an input, but this is rarely available in real-life applications. Our goal in this paper is to develop a general framework for this task. The general model works with the most widely used infection <b>models</b> and <b>is</b> able to handle an arbitrary number of observations on <b>such</b> processes. The <b>model</b> <b>is</b> <b>defined</b> as a general optimization task and a Particle Swarm heuristic is proposed to solve it. We evaluate the accuracy and speed of the proposed method on a high variety of realistic infection scenarios. Comment: 12 pages, 5 figures. Submitted to IEEE Transactions on Knowledge and Data Engineerin...|$|R
40|$|Reference <b>models</b> have <b>been</b> {{developed}} in various fields of information processing. The aim of <b>such</b> <b>models</b> <b>is</b> to <b>define</b> a unique basis for system development, system usage and {{for education and}} training. One of the first reference models in Computer Graphics was developed by Guedji et al. Today a (Standard) Computer Graphics Reference <b>Model</b> <b>is</b> under development by ISO. In areas like CAD reference <b>models</b> have already <b>been</b> established on a national basis. The emerging Imaging standard also defines a reference model for image operations. In Scientific Visualization various system <b>models</b> have <b>been</b> presented in recent years. These models focus on different aspects, <b>such</b> as a <b>model</b> for the visualization process, error accumulation, output pipelines in the visualization process, semantics of interaction in the visualization process, architecture and hierarchy of software modules, computing architectures and load sharing models, data and image interfaces. These <b>models</b> have <b>been</b> set up by users m ore often than by developers of tools. Each of the above models {{does not reflect the}} meaning of scientific visualization for it's own but just a certain view. Existing standards, like those known from Computer Graphics (GKS, PHIGS, [...] .) are not covered by these models. This paper introduces a reference model for visualization systems and classifies existing models using the criteria of this reference model as a basis for comparison...|$|R
40|$|Abstract. Power {{semiconductors}} can <b>be</b> <b>modeled</b> as {{a thermal}} network of resistors and capacitors. The thermal boundary condition of <b>such</b> a <b>model</b> <b>is</b> typically <b>defined</b> as {{the heat sink}} surface temperature which {{is assumed to be}} constant. In reality, the heat sink surface temperature underneath the power module is not exactly known. In this paper we show how to set up a thermal model of the heat sink in form of a RC thermal equivalent network that can be directly embedded in any circuit simulator. The proposed thermal heat sink model takes into account convection cooling, thermal hotspots on the heat sink base plate, thermal time constants of the heat sink, and thermal coupling between different power modules mounted onto the heat sink. Experimental results are given and show high accuracy of the heat sink model with temperature errors below 10 %...|$|R
40|$|The {{implementation}} of compilers and interpreters for non-trivial languages {{is a complex}} and error prone process, if done by hand. Therefore, formalisms and generator tools have been developed that allow automatic generation of compilers and interpreters from formal specifications. This offers two major advantages: • High level descriptions of language properties, rather than detailed programming of the translation process • High degree of correctness of generated implementations. The high level specifications are more concise and easier to read than a detailed implementation in some programming language Modelica is an object-oriented language for modeling of physical systems {{for the purpose of}} efficient simulation. The language unifies and generalizes previous object-oriented modeling languages. A Modelica <b>model</b> <b>is</b> <b>defined</b> in terms of classes containing equations and definitions. The semantics, i. e. the meaning of <b>such</b> a <b>model</b> <b>is</b> <b>defined</b> via translation of classes, instances and connections into a flat set of constants, variables and equations. This paper describes and defines a formal semantics for Modelica expressed in a high-level specification notation called natural semantics. A compiler generation system called RML produces a a Modelica translator from such a language specification. The generated translator is produced in ANSI C and has comparable performance as hand-written translators. The RML tool has also been used to produce compilers for Java, Pascal and few other languages. MODELICA OVERVIEW Modelica (Modelica 1998) is an object-oriented language for modeling of physical systems for the purpose of efficient simulation. The language unifies and generalizes previous object-oriented modeling languages...|$|R
40|$|Model-based {{development}} {{promises to}} increase produc-tivity by offering modeling languages tailored {{to a specific}} domain. <b>Such</b> <b>modeling</b> languages <b>are</b> typically <b>defined</b> by a metamodel. In response to changing requirements and tech-nological progress, the domains and thus the metamodels are subject to change. Manually migrating existing models to {{a new version of}} their metamodel is tedious and error-prone. Hence, adequate tool support is required to support the maintenance of modeling languages. COPE provides adequate tool support by specifying the coupled evolution of metamodels and models. In this paper, we present the tool support to record the operations carried out on the meta-model directly through an editor. These operations can be enriched by instructions on how to migrate corresponding models. To further reduce migration effort, COPE provides high-level operations which have built-in meaning in terms of the migration of models. ...|$|R
40|$|In {{the design}} and {{optimization}} of power-aware computing systems, it is often desired to estimate power consumption at various levels of abstraction, e. g., at the transistor, gate, RTL, behavioral or transaction levels. Tools for power estimation at these different levels of abstraction require specialized expertise, e. g., understanding of device physics for circuit-level power estimation, and as such are necessarily developed by different research communities. In the optimization of complete platforms however, it is desired {{to be able to}} obtain aggregate power and performance estimates for the different components of a system, and this requires the ability to model the system at a mixture of levels of abstraction. One approach to enabling <b>such</b> cross-abstraction <b>modeling,</b> <b>is</b> to <b>define</b> a mechanism for interchange of data between tools at different layers of abstraction, for both static analysis and simulation-based studies. This document presents preliminary discussions on the requirements of such an interface...|$|R
40|$|Model-based {{software}} development promises to increase productivity by offering modeling languages tailored {{to a problem}} domain. <b>Such</b> <b>modeling</b> languages <b>are</b> often <b>defined</b> by a metamodel. In consequence of changing requirements and technological progress, these modeling languages and thus their metamodels are subject to change. Manually migrating models to {{a new version of}} their metamodel is tedious, error-prone and heavily hampers cost-efficient model-based development practice. Automating model migration in response to metamodel adaptation promises to substantially reduce effort. Unfortunately, {{little is known about the}} types of changes occurring during metamodel adaptation in practice and, consequently, to which degree reconciling <b>model</b> migration can <b>be</b> automated. We analyzed the changes that occurred during the evolution history of two industrial metamodels and classified them according to their level of potential automation. Based on the results, we present a list of requirements for effective tool support for coupled evolution of metamodels and models in practice...|$|R
40|$|Model-based {{development}} {{promises to}} increase productivity by offering modeling languages tailored {{to a specific}} domain. <b>Such</b> <b>modeling</b> languages <b>are</b> typically <b>defined</b> by a metamodel. In response to changing requirements and technological progress, the domains and thus the metamodels are subject to change. Manually migrating existing models to {{a new version of}} their metamodel is tedious and error-prone. Hence, adequate tool support is required to support the maintenance of modeling languages. This paper introduces COPE, an integrated approach to specify the coupled evolution of metamodels and models to reduce migration effort. With COPE, a language is evolved by incrementally composing modular coupled transformations that adapt the metamodel and specify the corresponding model migrations. This modular approach allows to combine the reuse of recurring transformations with the expressiveness to cater for complex transformations. We demonstrate the applicability of COPE in practice by modeling the coupled evolution of two existing modeling languages...|$|R
40|$|The goal of {{geophysical}} inversion {{is to make}} quantitative {{inferences about}} the Earth from remote observations. Because the observations are finite in number and subject to uncertainty, these inferences are inherently probabilistic. A key step <b>is</b> to <b>define</b> {{what it means for}} an Earth model to fit the data. This requires estimation of the uncertainties in the data, both those due to random noise and those due to theoretical errors. But the set of models that fit the data usually contains unrealistic models; i. e., models that violate our a priori prejudices, other data, or theoretical considerations. One strategy for eliminating <b>such</b> unreasonable <b>models</b> <b>is</b> to <b>define</b> an a priori probability density on the space of models, then use Bayes theorem to combine this probability with the data misfit function into a final a posteriori probability density reflecting both data fit and model reasonableness. We show here a case study of the application of the Bayesian strategy to inversion of surface s [...] ...|$|R
40|$|Domain-specific {{modeling}} {{promises to}} increase productivity by offering modeling languages tailored {{to a problem}} domain. <b>Such</b> <b>modeling</b> languages <b>are</b> typically <b>defined</b> by a metamodel. In consequence of changing requirements and technological progress, the problem domains and thus the metamodels are subject to change. Manually migrating models to {{a new version of}} their corresponding metamodel is costly, tedious and error-prone and heavily hampers cost-efficient model-based development in practice. The coupled evolution of a metamodel and its <b>models</b> <b>is</b> a sequence of metamodel changes and their corresponding model migrations. These coupled changes are either metamodelspecific or metamodel-independent. Metamodel-independent changes can be reused to evolve different metamodels and their models which leads to reduction of migration effort. Tool support is necessary in order to benefit from potential reuse. We propose a language that allows for decomposition of a migration into manageable, modular coupled changes. It provides a reuse mechanism for metamodel-independent changes, but {{is at the same time}} expressive enough to cater for complex, metamodelspecific changes...|$|R
