1280|258|Public
25|$|In {{mathematical}} optimization, Dantzig's simplex algorithm (or <b>simplex</b> <b>method)</b> is {{a popular}} algorithm for linear programming.|$|E
25|$|This yields an n-simplex as {{a corner}} of the n-cube, and is a {{standard}} orthogonal simplex. This is the simplex used in the <b>simplex</b> <b>method,</b> which is based at the origin, and locally models a vertex on a polytope with n facets.|$|E
25|$|In large linear-programming {{problems}} A {{is typically}} a sparse matrix and, when the resulting sparsity of B is exploited when maintaining its invertible representation, the revised simplex algorithm {{is much more}} efficient than the standard <b>simplex</b> <b>method.</b> Commercial simplex solvers {{are based on the}} revised simplex algorithm.|$|E
50|$|<b>Simplex</b> <b>methods</b> or {{interior}} point {{methods can}} be applied to solve the linear programming problem.|$|R
40|$|The {{sequential}} <b>simplex</b> <b>methods</b> {{are popular}} and efficient optimization techniques applied in many fields of chemistry and chemical engineering. Simplex optimization {{can be used}} to experimentally determine the optimum conditions, to improve process efficiency, product quality or instrument performance 1 - 6...|$|R
40|$|A {{class of}} {{exterior}} climbing algorithms (ladder algorithms) {{has been developed}} recently. Among this class, the targeted climbing algorithm has demonstrated very good numerical performance. In this paper an improved targeted climbing linear programming algorithm is proposed. A sequence of changing reference points, instead of a fixed reference point as in the original algorithm, {{is used in the}} improved algorithm to speed up the convergence. The improved algorithm is especially suited to solve problems involving many constraints close to the optimal solution [...] -the case that causes many algorithms to slow down dramatically. Numerical tests and comparison with the <b>simplex</b> <b>methods</b> are presented. Our results show that the current algorithm works better on average than the <b>simplex</b> <b>methods</b> and is much faster for a large class of problems...|$|R
25|$|The {{field of}} {{optimization}} is further split in several subfields, {{depending on the}} form of the objective function and the constraint. For instance, linear programming deals with the case that both the objective function and the constraints are linear. A famous method in linear programming is the <b>simplex</b> <b>method.</b>|$|E
25|$|However, the simplex {{algorithm}} has poor worst-case behavior: Klee and Minty {{constructed a}} family of linear programming problems for which the <b>simplex</b> <b>method</b> takes a number of steps exponential in the problem size. In fact, for some time it was not known whether the linear programming problem was solvable in polynomial time, i.e. of complexity class P.|$|E
25|$|Direct methods {{compute the}} {{solution}} to a problem in a finite number of steps. These methods would give the precise answer if they were performed in infinite precision arithmetic. Examples include Gaussian elimination, the QR factorization method for solving systems of linear equations, and the <b>simplex</b> <b>method</b> of linear programming. In practice, finite precision is used and the result is an approximation of the true solution (assuming stability).|$|E
40|$|AbstractThis paper {{gives an}} alternative, unified {{development}} of the primal and dual <b>simplex</b> <b>methods</b> for maximizing cTx subject to Ax=b, x⩾ 0. The calculations are {{described in terms of}} certain canonical bases for the null space of A and the range space of AT. The vectors of these bases are edges of the polyhedron in question at the given basic feasible solution...|$|R
40|$|Second-order cone {{programming}} {{is a useful}} tool for many practical applications and theoretical developments; see [2, 6] for a survey. As a linear program, interior point methods can approximate a second-order cone program in polynomial time. On the other hand, <b>simplex</b> <b>methods</b> remain to be widely used practical procedures for the linear programming because of its “warm start ” abilit...|$|R
30|$|However, {{it can be}} surely {{improved}} applying methods {{which are}} already introduced in the literature [14, 15], where optimization techniques such as genetic algorithms or <b>simplex</b> <b>methods</b> are applied to RMNs in amplifier or antenna scenarios. In addition, {{it can also be}} possible to pre-calibrate the system in-house, for different operating conditions or frequency bands, and to save the optimum states in a LUT.|$|R
25|$|Up to {{this point}} in history, {{optimization}} techniques were known {{for a very long}} time, from the simple methods employed by F.W.Harris to the more elaborate techniques of the calculus of variations developed by Euler in 1733 or the multipliers employed by Lagrange in 1811, and computers were slowly being developed, first as analog computers by Sir William Thomson (1872) and James Thomson (1876) moving to the eletromechanical computers of Konrad Zuse (1939 and 1941). During World War II however, the development of mathematical optimization went through a major boost with the development of the Colossus computer, the first electronic digital computer that was all programmable, and the possibility to computationally solve large linear programming problems, first by Kantorovich in 1939 working for the Soviet government and latter on in 1947 with the <b>simplex</b> <b>method</b> of Dantzig. These methods are known today as belonging to the field of operations research.|$|E
2500|$|A {{tutorial}} for <b>Simplex</b> <b>Method</b> {{with examples}} (also two-phase and M-method).|$|E
2500|$|Greenberg, Harvey J., Klee-Minty Polytope Shows Exponential Time Complexity of <b>Simplex</b> <b>Method</b> University of Colorado at Denver (1997) ...|$|E
40|$|Teaching {{the topic}} of linear models is a complex process. A new {{teaching}} process will be investigated which we will consider in two different ways: on one hand linear models and quadratic programming problems are formulated and solved by statistical methods; {{on the other hand}} the solution of the linear regression model with constraints makes use of the <b>simplex</b> <b>methods</b> of linear and quadratic programming...|$|R
40|$|An {{inversion}} {{scheme for}} local waveforms {{which uses a}} genetic algorithm with memory is applied to retrieve the focal mechanism {{of a set of}} low magnitude earthquakes belonging to a seismic sequence occurred in the Râmnicu Sãrat seismic area. The solutions obtained correlate better with the observed P-wave polarities, as compared with the solutions determined previously by an inversion algorithm based on a combination of the Monte Carlo and <b>Simplex</b> <b>methods...</b>|$|R
3000|$|... is {{the maximum}} {{expected}} offset {{for a single}} registration parameter in positive or negative direction, where we use the offset which gives the best similarity score. The downhill <b>simplex</b> <b>methods</b> is however able to find optimal registration parameters that lay outside the maximum expected offsets. This search method maximizes the similarity function by replacing those registration parameters in the simplex that gives the worst similarity score by a better set using some simple heuristics.|$|R
2500|$|... on {{low-dimensional}} functions, say , {{for example}} by the downhill <b>simplex</b> <b>method</b> or surrogate-based methods (like kriging with expected improvement); ...|$|E
2500|$|The <b>simplex</b> <b>method</b> {{is remarkably}} {{efficient}} {{in practice and}} was a great improvement over earlier methods such as Fourier–Motzkin elimination. However, in 1972, Klee and Minty gave an example, the Klee-Minty cube, [...] showing that the worst-case complexity of <b>simplex</b> <b>method</b> as formulated by Dantzig is exponential time. Since then, for almost every variation on the method, {{it has been shown}} that there is a family of linear programs for which it performs badly. [...] It is an open question if there is a variation with polynomial time, or even sub-exponential worst-case complexity.|$|E
2500|$|During 1946–1947, George B. Dantzig {{independently}} developed general {{linear programming}} formulation {{to use for}} planning problems in US Air Force. In 1947, Dantzig also invented the <b>simplex</b> <b>method</b> {{that for the first}} time efficiently tackled the linear programming problem in most cases. When Dantzig arranged a meeting with John von Neumann to discuss his <b>Simplex</b> <b>method,</b> Neumann immediately conjectured the theory of duality by realizing that the problem he had been working in game theory was equivalent. Dantzig provided formal proof in an unpublished report [...] "A Theorem on Linear Inequalities" [...] on January 5, 1948. In the post-war years, many industries applied it in their daily planning.|$|E
40|$|The {{interaction}} between linear, quadratic programming and regression analysis are explored by both statistical and operations research methods. Estimation and optimization problems are formulated {{in two different}} ways: on one hand linear and quadratic programming problems are formulated and solved by statistical methods, {{and on the other}} hand the solution of the linear regression model with constraints makes use of the <b>simplex</b> <b>methods</b> of linear or quadratic programming. Examples are given to illustrate the ideas...|$|R
40|$|Over {{the past}} decades, Linear Programming (LP) {{has been widely}} used in {{different}} areas and considered {{as one of the}} mature technologies in numerical optimization. However, the complexity offered by state-of-the-art algorithms (i. e. interior-point method and primal, dual <b>simplex</b> <b>methods)</b> is still unsatisfactory for problems in machine learning with huge number of variables and constraints. In this paper, we investigate a general LP algorithm based on the combination of Augmented Lagrangian and Coordinate Descent (AL-CD), giving an iteration complexity of O((log(1 /)) 2) with O(nnz(A)) cost per iteration, where nnz(A) is the number of non-zeros in the m×n constraint matrix A, and in practice, one can further re-duce cost per iteration to the order of non-zeros in columns (rows) corresponding to the active primal (dual) variables through an active-set strategy. The algorithm thus yields a tractable alternative to standard LP methods for large-scale problems of sparse solutions and nnz(A) mn. We conduct experiments on large-scale LP instances from ` 1 -regularized multi-class SVM, Sparse Inverse Covariance Es-timation, and Nonnegative Matrix Factorization, where the proposed approach finds solutions of 10 − 3 precision orders of magnitude faster than state-of-the-art implementations of interior-point and <b>simplex</b> <b>methods.</b> ...|$|R
40|$|Linear {{programming}} (LP) is {{an important}} field of optimization. Even though, interior point methods are polynomial algorithms, many LP practical problems are solved more efficiently by the primal and dual revised <b>simplex</b> <b>methods</b> (RSM); however, RSM has a poor performance in hard LP problems (HLPP) as in the Klee-Minty Cubes problem. Among LP methods, the hybrid method known as Simplex-Genetic (SG) is very robust to solve HLPP. The objective of SG is to obtain the optimal solution of a HLPP, taking advantages from {{each one of the}} combined methods-a genetic algorithm (GA) and the classical primal RSM-. In this paper a new SG <b>method</b> named Improved <b>Simplex</b> Genetic <b>Method</b> (ISG) is presented. ISG combines a GA (with special genetic operators) with both primal and dual RSM. Numerical experimentation using some instances of the Klee-Minty cubes problem shows that ISG has a better performance than both RSM and SG...|$|R
2500|$|Khachiyan's {{algorithm}} was of landmark {{importance for}} establishing the polynomial-time solvability of linear programs. [...] The algorithm {{was not a}} computational break-through, as the <b>simplex</b> <b>method</b> is more efficient for all but specially constructed families of linear programs.|$|E
2500|$|The {{founders of}} this subject are [...] Leonid Kantorovich, a Russian {{mathematician}} who developed linear programming problems in 1939, Dantzig, who published the <b>simplex</b> <b>method</b> in 1947, and John von Neumann, {{who developed the}} theory of the duality in the same year.|$|E
2500|$|However, Khachiyan's {{algorithm}} inspired {{new lines}} {{of research in}} linear programming. In 1984, N. Karmarkar proposed a projective method for linear programming. [...] Karmarkar's algorithm improved on Khachiyan's worst-case polynomial bound (giving [...] ). Karmarkar claimed that his algorithm was much faster in practical LP than the <b>simplex</b> <b>method,</b> a claim that created great interest in interior-point methods. Since Karmarkar's discovery, many interior-point methods have been proposed and analyzed.|$|E
40|$|The {{filter method}} is a {{technique}} for solving nonlinear programming problems. The filter algorithm has two phases in each iteration. The first one reduces a measure of infeasibility, while in the second the objective function value is reduced. In real optimization problems, usually the objective function is not differentiable or its derivatives are unknown. In these cases it becomes essential to use optimization methods where the calculation of the derivatives or the verification of their existence is not necessary: direct search methods or derivative-free methods are examples of such techniques. In this work we present a new direct search <b>method,</b> based on <b>simplex</b> <b>methods,</b> for general constrained optimization that combines the features of <b>simplex</b> and filter <b>methods.</b> This method neither computes nor approximates derivatives, penalty constants or Lagrange multipliers...|$|R
40|$|Linear {{programming}} {{is a form}} of mathematical optimisation that has many important applications, one of them being modelling of production and trade in the petroleum industry. The Cell Broadband Engine offers a potentially very powerful platform for high performance computing, but the hardware imposes certain restrictions that makes development challenging. The aim of this thesis is to investigate the opportunities for implementing a parallel solver for sparse linear programs on the Cell Broadband Engine. We will primarily focus on the standard and revised <b>simplex</b> <b>methods</b> and parallel algorithms derived from them. If time permits, interior point methods may also be explored...|$|R
30|$|The NM {{algorithm}} {{is known as}} one of the best <b>simplex</b> <b>methods</b> for finding the local minimum of a function [22]. For two variables, this method performs a pattern search based on three vertices of a triangle [23]. At each stage, among three initial vertices, the worst vertex at which the objective function achieves the largest value is replaced by a new vertex which is generated by reflection, expansion, contraction, or shrink and leads to smallest objective function value compared to the previous vertices [22]. This process is iterated until converging to a local minimum. The searching strategy is shown in Algorithm 2 [22, 24].|$|R
2500|$|The tableau form used above to {{describe}} the algorithm lends itself to an immediate implementation in which the tableau is maintained as a rectangular (m+1)-by-(m+n+1) array. It is straightforward to avoid storing the m explicit columns of the identity matrix that will occur within the tableau by virtue of B being {{a subset of the}} columns of [...] This implementation {{is referred to as the}} [...] "standard simplex algorithm". The storage and computation overhead are such that the standard <b>simplex</b> <b>method</b> is a prohibitively expensive approach to solving large linear programming problems.|$|E
2500|$|After Dantzig {{included}} an objective function {{as part of}} his formulation during mid-1947, the problem was mathematically more tractable. Dantzig realized that one of the unsolved problems that he mistook as homework in his professor Jerzy Neyman's class (and actually later solved), was applicable to finding an algorithm for linear programs. This problem involved finding the existence of Lagrange multipliers for general linear programs over a continuum of variables, each bounded between zero and one, and satisfying linear constraints expressed in the form of Lebesgue integrals. Dantzig later published his [...] "homework" [...] as a thesis to earn his doctorate. The column geometry used in this thesis gave Dantzig insight that made him believe that the <b>Simplex</b> <b>method</b> would be very efficient.|$|E
2500|$|George Dantzig {{worked on}} {{planning}} methods for the US Army Air Force during World War II using a desk calculator. During 1946 his colleague challenged him to mechanize {{the planning process}} in order to entice him into not taking another job. Dantzig formulated the problem as linear inequalities inspired {{by the work of}} Wassily Leontief, however, at that time he didn't include an objective as part of his formulation. Without an objective, a vast number of solutions can be feasible, and therefore to find the [...] "best" [...] feasible solution, military-specified [...] "ground rules" [...] must be used that describe how goals can be achieved as opposed to specifying a goal itself. Dantzig's core insight was to realize that most such ground rules can be translated into a linear objective function that needs to be maximized. Development of the <b>simplex</b> <b>method</b> was evolutionary and happened over a period of about a year.|$|E
40|$|Recent {{advances}} in micro-computer technology have {{made possible the}} practical application of convex programming algorithms such as used in the <b>simplex</b> <b>methods,</b> the out-of-kilter method, Beale 2 ̆ 7 s method, the supporting plane methods, the ellipsoidal methods, Gomory 2 ̆ 7 s method, and the branch and bound methods to the optimization of construction scheduling problems {{as defined by the}} critical path method. These algorithms have been adapted and coded with the IBM BASIC computer language for the solution of problems involving the minimization of schedule overall costs, total man count, and direct supervision costs to demonstrate the feasibility and practicality of the methods...|$|R
5000|$|<b>Simplex</b> noise, a <b>method</b> for {{constructing}} an n-dimensional noise function comparable to Perlin noise ...|$|R
3000|$|... 6. Their {{values were}} derived for each {{distance}} bin using a search code {{based on the}} downhill <b>simplex</b> Nelder–Mead <b>method,</b> combined with a singular value decomposition (SVD) algorithm (Press et al. 1992) to calculate the linear coefficients at each simplex step.|$|R
