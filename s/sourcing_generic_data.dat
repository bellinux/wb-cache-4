0|1420|Public
40|$|This {{project is}} to provide the U. S. Army Corps of Engineers with a risk {{analysis}} that evaluates the non-routine closure of water flow through the turbines of powerhouses along the Columbia and Snake Rivers. The project is divided into four phases. Phase 1 efforts collected and analyzed relevant plant failure data for hydroelectric generating stations in the United States and Canada. Results from the Phase 1 efforts {{will be used to}} assess the risk (probability times consequences) associated with non-routine shut down of hydroelectric stations, which will be performed in the remaining phases of the project. Results of this project may be used to provide policy recommendations regarding operation and maintenance of hydroelectric stations. The methodology used to complete the Phase 1 of the project is composed of data collection and analysis activities. Data collection included performing site visits, conducting a data survey of hydroelectric stations, conducting an expert panel workshop, and reviewing and tabulating failure <b>data</b> from <b>generic</b> <b>sources.</b> <b>Data</b> analysis included estimating failure rates obtained from the survey data, expert judgment elicitation process, <b>generic</b> <b>data,</b> and combining these failure rates to produce final failure rate parameters. This paper summarizes the data collection analysis, results and discussions for the Phase 1 efforts...|$|R
40|$|This paper {{addresses}} {{the problem of}} using proactive cryptosystems for <b>generic</b> <b>data</b> storage and retrieval. Proactive cryptosystems provide high security and confidentiality guarantees for stored data, and are capable of withstanding attacks that may compromise all the servers in the system over time. However, proactive cryptosystems are unsuitable for <b>generic</b> <b>data</b> storage uses for two reasons. First, proactive cryptosystems are usually used to store keys, which are rarely updated. On the other hand, <b>generic</b> <b>data</b> could be actively written and read. The system must therefore be highly available for both write and read operations. Second, existing share renewal protocols (the critical element to achieve proactive security) are expensive in terms of computation and communication overheads, and are time consuming operations. Since <b>generic</b> <b>data</b> will be voluminous, the share renewal process will consume substantial system resources and cause {{a significant amount of}} system downtime. Two schemes are proposed that combine Byzantine quorum systems and proactive secret sharing techniques to provide high availability and security guarantees for stored data, while reducing the overhead incurred during the share renewal process. Several performance metrics {{that can be used to}} evaluate proactively-secure <b>generic</b> <b>data</b> storage schemes are identified. The proposed schemes are thus shown to render proactive systems suitable for confidential <b>generic</b> <b>data</b> storage...|$|R
5000|$|<b>Generic</b> <b>data</b> {{structure}} for multi-layer network was introduced ...|$|R
5000|$|One {{approach}} to <b>generic</b> <b>data</b> modeling has the following characteristics: ...|$|R
5000|$|A {{reusable}} ALSP Interface {{consisting of}} <b>generic</b> <b>data</b> exchange message protocols; and ...|$|R
5000|$|<b>GENERIC</b> <b>data</b> types can be {{used for}} procedures, {{functions}} and abstract entities.|$|R
40|$|Safety-instrumented {{systems are}} {{implemented}} in the industry to prevent accidents to occur and escalate. The blowdown system on a oil production ship {{is one example of}} such system. If a fire breaks out on the ship, the blowdown system's role is to remove the flammable gases from the current production lines on the ship. This is done by opening of the blowdown valves, that are installed on the different production lines. In this thesis, the blowdown system on a new Teekay ship, and espesically the valves, are applied as case. Since such systems are imporant to maintain the safety on the installation, they are subject to strict reliability performance requirements. Before the SIS is put into operation, it is required to state a certain reliability target for the system. At present, Teekay estimates the reliability based on <b>generic</b> reliability <b>data</b> or reliability data provided from the manufacturer of the equipment. There are uncertainties related to both of these <b>sources.</b> <b>Generic</b> reliability <b>data</b> are collected from different installations where the equipment are operating under different conditions and environment. The reliability is affected by its surroundings, and the <b>generic</b> <b>data</b> then reflects the average reliability in the entire industry. This implies that this data may not be accurate for equipment on a new ship, which has brand new equipment. The manufacturer data, on the other hand are tested under controlled conditions, typically in a laboratory. This implies that the reliability reflects how the equipment performs when it is applied just as intended by the manufacturer. In industry, the equipment will most likely be handled more thoughly, and failures can be introduced during for example, maintenance. This thesis suggests a new way of predicting the reliability. The estimated reliability is based on the mentioned sources, in addition to reliability data collected from the other ships in the Teekay fleet. By using expert opinions (e. g., opinions from operators and engineers) these sources are weighted to create a best possible estimate of the reliability. From these estimates, a probability distribution is constructed. This distribution states how likely the different estimates of the reliability are, where the thesis suggests to choose a rather conservative estimate even if it is not the most likely estimate. After the system is put into operation, failure data becomes available from operation and testing. This data constitutes a reliability estimate based on operational data alone. Since safety systems are designed to be highly reliable, few failures occur and this estimate is uncertain. Because of this, the thesis provides a method on how the operational data can be included in the probability distribution constructed in the design phase. As more operational data become available, the less is the contribution from the design phase assumptions. If it comes to a point where the operational data proves that the reliability of the system differs significantly from what was assumed, it can be assessed whether the regular testing of the system can or should be changed. This thesis suggests that only operational data should be applied to decide such a change. A point which indicates that a sufficient amount of operational data is collected to trust this estimate solely, is provided. Teekay performs annual testing on the entire blowdown system where the results are recorded and applied in further reliability calculations. In addition to these test, they perform monthly manual testing on the blowdown valves. This is because the valves fail more often than the other parts, and the operators on the ship {{want to be sure that}} they are functioning. It is shown how these tests influence on the average availability of the valves, and it is discussed how these tests can be applied as a means to increase the length between the more comprehensive annual tests. The last part of the thesis investigates the uncertainty aspects related to the provided methods, models, and other relevant aspects in the process of collecting and applying reliability data. It is shown that there are a high degree of uncertainty related to all the aspects. Application of <b>generic</b> reliability <b>data</b> is a high contributor to uncertainty. The thesis provides a method on how the <b>generic</b> <b>data</b> can be compared to the data collected in Teekay. This comparison aims at providing a factor which reflects how the specific conditions on Teekay ships influences the reliability, compared to the average reliability found in <b>generic</b> <b>data</b> <b>sources.</b> </p...|$|R
50|$|<b>Generic</b> <b>data</b> {{models are}} generalizations of {{conventional}} data models. They define standardized general relation types, {{together with the}} kinds of things that may be related by such a relation type. The definition of <b>generic</b> <b>data</b> model is similar to the definition of a natural language. For example, a <b>generic</b> <b>data</b> model may define relation types such as a 'classification relation', being a binary relation between an individual thing and a kind of thing (a class) and a 'part-whole relation', being a binary relation between two things, one with the role of part, the other with the role of whole, regardless the kind of things that are related.|$|R
5000|$|BioStudies - a {{database}} {{that serves as}} a <b>generic</b> <b>data</b> archive at EMBL-EBI for biomolecular datasets ...|$|R
5000|$|Repeater {{which is}} used to repeat the content {{according}} to data retrieved from a <b>generic</b> <b>data</b> <b>source</b> ...|$|R
50|$|Boomerang {{grew out}} of the Harmony <b>generic</b> <b>data</b> synchronizer, which {{grew out of}} the Unison file {{synchronization}} project.|$|R
5000|$|... highly {{reusable}} metadata (e.g. xml schemata, <b>generic</b> <b>data</b> models) and/or {{reference data}} (e.g. code lists, taxonomies, dictionaries, vocabularies) ...|$|R
50|$|More <b>generic</b> <b>data</b> {{analysis}} {{programs with}} spread-sheet capabilities include the proprietary Origin and its free clones QtiPlot and SciDAVis.|$|R
50|$|Generic Image Library (GIL), {{is an open}} <b>source</b> <b>generic</b> {{programming}} library {{created by}} Adobe Systems for image-related programming. It was accepted to the Boost C++ Libraries in November 2006 and is included in Boost's latest official release.|$|R
5000|$|Healthcare CRM systems {{automatically}} unify and harmonize {{data from}} Laboratory Information Systems (LIS), billing, supply, courier, payer and other <b>sources.</b> <b>Generic</b> CRM systems expect significant manual entry of tasks and opportunities {{on the part}} of healthcare workers.|$|R
5000|$|<b>Generic</b> <b>Data</b> Types - Only {{available}} for the input / output/ in-out variables of system-defined Program Organisation Units (POUs, see below) ...|$|R
30|$|N.B. {{life cycle}} {{inventory}} data for the battery is not relating to the example installation in Schwerin, but <b>generic</b> <b>data</b> from ecoinvent.|$|R
5000|$|Oracle LogMiner - in {{contrast}} to <b>generic</b> <b>data</b> mining, targets the extraction of information from the internal logs of an Oracle database ...|$|R
40|$|Bayesian {{reliability}} {{requires the}} development of a prior distribution to represent degree of belief about the value of a parameter (such as a component's failure rate) before system specific data become available from testing or operations. <b>Generic</b> failure <b>data</b> are often provided in reliability databases as point estimates (mean or median). A component's failure rate is considered a random variable where all possible values are represented by a probability distribution. The applicability of the <b>generic</b> <b>data</b> <b>source</b> is a significant source of uncertainty that affects the spread of the distribution. This presentation discusses heuristic guidelines for quantifying uncertainty due to <b>generic</b> <b>data</b> applicability when developing prior distributions mainly from reliability predictions...|$|R
3000|$|Data {{description}} and explorative data structure modeling of any <b>generic</b> <b>data</b> matrix. Principal Component Analysis (PCA) is frequently {{used for this}} purpose [...]...|$|R
5000|$|A <b>generic</b> <b>data</b> model {{shall consist}} of generic entity types, such as 'individual thing', 'class', 'relationship', and {{possibly}} a number of their subtypes.|$|R
40|$|Abstract As a {{step toward}} {{automating}} the ability to locate generic objects in an image, we propose an approach based on model-driven correction of an initial low-level scene partition. To accomplish this, we define <b>generic</b> <b>data</b> structures for geometric shapes, along with robust rules for parsing the image geometry and performing a shape-motivated resegmentation. We successfully apply the system {{to the task of}} locating and outlining complex rectilinear cultural objects in aerial imagery. Kev words: Segmentation, <b>generic</b> <b>data</b> structures, aerial imagery. ...|$|R
30|$|We do {{not know}} of related work for our {{research}} question four, that is <b>generic</b> <b>data</b> on problems and measures taken with regard to cross-language links.|$|R
50|$|The {{definition}} of <b>generic</b> <b>data</b> model {{is similar to}} the {{definition of}} a natural language. For example, a <b>generic</b> <b>data</b> model may define relation types such as a 'classification relation', being a binary relation between an individual thing and a kind of thing (a class) and a 'part-whole relation', being a binary relation between two things, one with the role of part, the other with the role of whole, regardless {{the kind of things that}} are related. Given an extensible list of classes, this allows the classification of any individual thing and to specify part-whole relations for any individual object. By standardisation of an extensible list of relation types, a <b>generic</b> <b>data</b> model enables the expression of an unlimited number of kinds of facts and will approach the capabilities of natural languages.Conventional data models, on the other hand, have a fixed and limited domain scope, because the instantiation (usage) of such a model only allows expressions of kinds of facts that are predefined in the model.|$|R
5000|$|<b>Generic</b> <b>data</b> {{models are}} generalizations of {{conventional}} data models. They define standardised general relation types, {{together with the}} kinds of things that may be related by such a relation type.|$|R
5000|$|UN Guidelines {{concerning}} computerized {{personal data}} files of 14 December 1990 <b>Generic</b> <b>data</b> processing activities using digital processing methods. Nonbinding guideline to UN nations calling for national regulation {{in this field}} ...|$|R
40|$|Quantijied {{reliability}} assessment is now employed by many {{organizations in the}} process industries. The objective is to identify critical areas in the plant where design changes are required to meet the safety or production specification. Mechanical system assessment is, however, frequently inhibited {{by the lack of}} representative equipment reliability data. Where these data are not available in-house recourse must be made to <b>generic</b> <b>sources</b> from the public domain. Members of the Process Industries Division’s Mechanical Reliability Committee have recently completed a study to identify and evaluate the principal reliability data sources for use in process system assessments. This paper discusses the data requirements for {{reliability assessment}} and the use of these <b>generic</b> <b>data</b> to provide best-estimates of the reliability statistics for specific applications. ...|$|R
5000|$|Information Storage: Databases {{are often}} used to store large amounts of information. They {{can be used to}} store <b>generic</b> <b>data,</b> like product information, as well as more {{specific}} information pertaining to certain locales.|$|R
5000|$|It {{is useful}} when {{implementing}} <b>generic</b> <b>data</b> structures in C. For example, the Linux kernel uses offsetof (...) to implement container_of (...) , which allows {{something like a}} mixin type to find the structure that contains it: ...|$|R
40|$|Interoperability of {{e-government}} {{systems is}} suggested to increase transparency, efficiency, effectiveness, {{and customer service}} in the public sector. <b>Generic</b> <b>data</b> models are often seen {{as a way for}} achieving especially semantic interoperability. To assess how the contemporary data models support semantic egovernment interoperability, we reviewed literature on data models suggested for the public sector in light of four features: standard modelling language, entityrelationship modelling, vocabulary for data exchange and methodology. The review contributes previous research by introducing a four-feature framework for assessing capability of e-government data models to enhance interoperability and by providing an up-to-date review of the <b>generic</b> <b>data</b> models for this purpose. Godkänd; 2014; 20141105 (terpai) </p...|$|R
40|$|Abstract. This paper {{compares the}} {{accuracy}} of combined classifiers in medical data bases to the same knowledge discovery techniques applied to <b>generic</b> <b>data</b> bases. Specifically, we apply Bagging and Boosting methods for 16 medical and 16 <b>generic</b> <b>data</b> bases and compare the accuracy results with a more traditional approach (C 4. 5 algorithm). Bagging and Boosting methods are applied using different numbers of classifiers and the accuracy is computed using a cross-validation technique. This paper main contribution resides in recommend the most accurate method and possible parameterization for medical data bases and an initial identification of some characteristics that make medical data bases different from generic ones. ...|$|R
30|$|There exist {{two types}} of data {{lifecycle}} models focusing on either general data or Big <b>Data</b> management. The <b>generic</b> <b>data</b> management lifecycles usually cover activities such as generation, collection (curation), storage, publishing, discovery, processing and analysis of data [16].|$|R
30|$|A {{computational}} framework, and a <b>generic</b> <b>data</b> {{model and}} its XML-format implementation are developed and tested. A method is developed {{to analyze the}} proximity {{in the context of}} positional uncertainties of both the utilities and the excavator movement.|$|R
40|$|Complex data {{structures}} such as {{lists and}} trees are di cult to implement inCinaway that programmers {{have little or no}} di culties to maintain and reuse the source code. Thus, converting such data structures into instances of generic C++ components to increase the maintainability of the code is a reasonable task. Replacing data structures with instances of <b>generic</b> <b>data</b> structures is a non-trivial issue; handling dependencies accordingly causes even more e ort in the conversion. This paper points out the issues of replacing data structures with instances of generic components provided by libraries. We brie y describe the transformation process, related issues and give an outlook on future activities in this area. This paper shows the bene ts of <b>generic</b> <b>source</b> code components for reengineering and maintenance activities. ...|$|R
40|$|Presentations {{and demonstrations}} {{of parts of}} CIAO, include the new GUIs {{developed}} for "FirstLook" analysis, data filtering and browsing; SHERPA, the multi-dimensional, multi-missions modelling and fitting application; CHIPS, the Chandra Imaging and Plotting System; <b>generic</b> <b>data</b> manipulation tools and other applications...|$|R
40|$|<b>Generic</b> <b>data</b> {{compression}} algorithms {{is an area}} {{of digital}} processing that is focusing on reducing bit rate of the speech signal for transmission or storage without significant loss of quality. The focus {{of this paper is to}} compress the digital speech using wavelet transform. The main idea behind the <b>generic</b> <b>data</b> compression algorithms is to represent uncompressed speech with minimum number of bits and optimum speech quality. Wavelet transform has been recently proposed for signal analysis. The wavelet transform is useful to remove redundancies and irrelevancies present in the speech signal for the compact representation. The classical subband coding method also known as filter bank method is used in which signal is decomposed into basis of wavelet functions using high pass and low pass filters. Speech coding is a lossy scheme and is implemented here to compress one-dimensional speech signal. Basically, this scheme consists of three operations which are the transform, threshold techniques (by level and global threshold), and run-length encoding operations. Finally the compressed signal is reconstructed. The <b>generic</b> <b>data</b> compression algorithms using filter bank method is implemented and simulated by using wavelet transfor...|$|R
