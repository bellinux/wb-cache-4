2|10000|Public
40|$|A {{computer}} code for fast automatic generation of quasi-three-dimensional grid systems for aerospace configurations is described. The code employs a homotopic method to algebraically generate two-dimensional grids in cross-sectional planes, which are stacked {{to produce a}} three-dimensional grid system. Implementation of the algebraic equivalents of the homotopic relations for generating body geometries and grids are explained. Procedures for controlling grid orthogonality and distortion are described. Test cases with description and <b>specification</b> <b>of</b> <b>inputs</b> are presented in detail. The FORTRAN computer program and notes on implementation and use are included...|$|E
40|$|We {{introduce}} {{a modification of}} the quadratic-Leontieff multi-output cost function that is particularly suitable for the data of the Farm Accountancy Data Network. We present least squares and entropy estimates of that function and compare their results for a sample of crop farms. Our results are encouraging {{for the use of}} entropy estimators in cases in which farms are not assumed to share the same technology. Our approach can be seen {{as an extension of the}} Positive Mathematical Programming approach (Howitt, 1995). The extension consists in an explicit <b>specification</b> <b>of</b> <b>inputs</b> in the cost function and in the possibility of modeling several farms simultaneously. cost function, least squares estimator, entropy estimator, heterogeneity, Farm Management, C 3, D 61, Q 12,...|$|E
30|$|Create each person, storing <b>specifications</b> <b>of</b> {{proposition}} <b>inputs,</b> analog sets, a list {{of which}} propositions should be “perceived” initially, and additional semantic rules for propositions and predicates.|$|R
40|$|AbstractThe paper {{deals with}} the issue of the {{relationship}} between results of examination of the influence of political business cycle {{on the basis of the}} choice <b>of</b> <b>input</b> variables. There were used nominal and effective tax rates, total tax liability, total tax collection in case of individual income tax setting in the Czech Republic in the time period 1993 − 2014. The results of analysis of the effects the electoral cycle on the tax policy determination may differ in relation to the <b>specification</b> <b>of</b> <b>input</b> data and the choice of dependent variables...|$|R
40|$|The codes WINGDES 2 and AERO 2 S {{were easy}} to obtain, and {{technical}} help was readily available. The codes have a long, well-documented history of successful optimizations of various aircraft configurations. The codes {{were easy to}} use, although <b>specification</b> <b>of</b> <b>input</b> data was time-consuming. The Run times were short, allowing the many runs necessary for the Suction Parameter matrix to be accomplished {{within a day or}} two. The results of the optimization appear to be reasonable...|$|R
40|$|Abstract — This paper answers an {{industrial}} question: ”Given the <b>specification</b> <b>of</b> <b>input</b> values, {{is it possible}} to verify that the source code of a program is robust with respect to erroneous inputs and memory alterations?”. We show that such verification is possible but quite complex to perform manually and we propose a semi-automatic solution. Our work is original in two ways: a new notion of software robustness is defined, enforced and verified, and we make use of a static tool in a non standard manner. I...|$|R
40|$|Web Processing Services (WPS) offer {{geoprocessing}} functionalities online. The {{description of}} an offered process to date {{consists of a}} textual description of the operation, and a <b>specification</b> <b>of</b> <b>input</b> and output parameters. This description is limited in detail and makes search, discovery, and composition of services difficult. Two approaches have been proposed to extend these service descriptions: ontology-based descriptions and WPS profiles, with differing complexity and intended use. This contribution confronts the two approaches, and suggests structured metadata as a linkage between service profiles and ontologies as future work...|$|R
40|$|Active {{safety systems}} utilize {{information}} about the vehicle’s state and possibly the surrounding environment to assist the driver if a traffic situation is considered hazardous. The decision to assist the driver is based on sensors that {{are more or less}} subjected to errors. In this paper, we study the influence <b>of</b> <b>input</b> perturbations on decision functions in active safety systems. We present a framework for evaluation <b>of</b> system performance, <b>specification</b> <b>of</b> <b>input</b> requirements and decision function tuning. By introducing a robustness measure, describing the robustness to error for the decision function, we derive efficient methods for analyzing the system performance. The framework is demonstrated on experimental data...|$|R
40|$|Abstract: This paper {{presents}} an embedded system design development environment based on Petri nets. The paper uses an extension to Place/Transition Petri nets allowing the association <b>of</b> external <b>input</b> signals to transitions and {{the association of}} external output signals to transitions and place markings. Additionally, the class provides support for the <b>specification</b> <b>of</b> <b>input</b> and output events. This extension allows the use of Place/Transition Petri nets for the <b>specification</b> <b>of</b> controllers. The paper also shows how to generate executable code from those specifications and details the algorithm used for the stepped execution of the net model. The code generator tool architecture is briefly discussed. Finally, an automation system example is presented. Copyright © 2004 DESDes’ 04 Keywords: Petri-nets, non-autonomous models, controller specification, code generation. 1...|$|R
40|$|This paper {{presents}} {{optimization problem}} formulations to design meander-line antennas for passive UHF {{radio frequency identification}} tags based on given <b>specifications</b> <b>of</b> <b>input</b> impedance, frequency range, and geometric constraints. In this application, {{there is a need}} for directive transponders to select properly the target tag, which in turn must be ideally isotropic. The design of an effective meander-line antenna for RFID purposes requires balancing geometrical characteristics with the microchip impedance. Therefore, there is an issue of optimization in determining the antenna parameters for best performance. The antenna is analyzed by a method of moments. Some results using a deterministic optimization algorithm are shown...|$|R
40|$|Smart {{mobility}} is {{the future}} of transportation services in Germany. The implementation and management of smart mobility is impossible without using big data. At the present time,the analysis of big data in Germany is not fully implemented due to existing challenges. The purpose of this research project is to forecast the impact of big data on smart mobility in Germany with the use of scenario planning. In order to receive the most actual scenarios, the input factors were designed in accordance with extensive literature research, and then ratios between all <b>specifications</b> <b>of</b> <b>input</b> factors were compared and evaluated. Thus four unique scenarios were selected for further detailed interpretation to suggest possible influences of big data on smart mobility in German...|$|R
2500|$|... … indeed, {{there are}} no {{provisions}} in this <b>specification</b> for <b>input</b> <b>of</b> external data or output of computed results.|$|R
40|$|Data envelopment {{analysis}} is frequently {{used to estimate}} efficient frontiers in studies of the non-financial {{as well as the}} financial sectors. However, appropriate interpretation of results requires attention to procedural and data issues that are not always apparent in financial sector studies. This study examines the sensitivity of mean efficiency scores to identification and exclusion <b>of</b> outliers, <b>specification</b> <b>of</b> <b>input</b> and output factors, data heterogeneity and sample size. The results suggest that unless stratification is used to alleviate heterogeneity in financial intermediaries, environmental factors external to managerial control may lead to incorrect conclusions regarding the relative efficiency of individual units within the industry. Such misconceptions can also lead to incorrect evaluation of managerial performance, biased conclusions regarding the benefits or otherwise of industry consolidation and incorrect resource allocation on the part of regulators. Acknowledgments...|$|R
40|$|This paper {{describes}} {{an approach to}} synthesizing personalized speech while maintaining not only speaker voice but also speaker pronunciation peculiarities. Personalization is realized by means of pronunciation models trained on speaker data contained in his/her speech database. Untrained models allow to synthesize speech in neutral normative style. On the segmental level, the transcription model is used. On the prosodic level, models for phrasing, intonation, pause and phoneme duration are used. These prosodic models are derived from comparative acoustic-phonetic study of different speakers data contained in several speech corpora and databases. Personalizing of pronunciation models is carried out during the off-line training of linguistic processor using a speech database annotation. During the on-line speech synthesis mode, personalized pronunciation models are used by the linguistic processor to generate speaker specific target <b>specification</b> <b>of</b> <b>input</b> text. 1...|$|R
40|$|This {{document}} {{provides a}} complete <b>specification</b> <b>of</b> the <b>input</b> language {{for the environmental}} theorem prover. This language {{is essentially the same}} input language used by any of the programs designated by AURA, an automated reasoning program developed at Argonne National Laboratory. The intended audience is the user who is already familiar with the concepts and terms associated with resolution-based theorem provers...|$|R
40|$|We {{present a}} method for {{automatically}} generating input parsers from English <b>specifications</b> <b>of</b> <b>input</b> file formats. We use a Bayesian generative model to capture relevant natural language phenomena and translate the English specification into a specification tree, which is then translated into a C++ input parser. We model the problem as a joint dependency parsing and semantic role labeling task. Our method is based on two sources of information: (1) {{the correlation between the}} text and the specification tree and (2) noisy supervision as determined by the success of the generated C++ parser in reading input examples. Our results show that our approach achieves 80. 0 % F-Score accuracy compared to an F-Score of 66. 7 % produced by a state-of-the-art semantic parser on a dataset <b>of</b> <b>input</b> format <b>specifications</b> from the ACM International Collegiate Programming Contest (which were written in English for humans with no intention of providing support for automated processing) National Science Foundation (U. S.) (Grant IIS- 0835652) Battelle Memorial Institute (PO # 300662...|$|R
40|$|Previous {{publications}} {{regarding the}} design and <b>specifications</b> <b>of</b> <b>input</b> filters for STATCOMs usually deal with the input filter only, and seldom {{pay any attention to}} the influence <b>of</b> the <b>input</b> filters on the performance of the STATCOM systems. A detailed analysis of the influences <b>of</b> <b>input</b> filters on the stability of STATCOM systems and the corresponding design considerations are presented in this paper. Three types <b>of</b> <b>input</b> filters, L filters, LC filters, and LCL filters, are examined separately. The influences of the parameters <b>of</b> <b>input</b> filters on system stability are investigated through frequency domain methods. With direct current control taken as the major control strategy for the STATCOMs, the different situations when adopting different current detection points are covered in this analysis. A comparison between LC filters and LCL filters is also presented with optimized filter parameters. Based on the analysis, the phase margin, as one of the design considerations for the different types <b>of</b> <b>input</b> filters under different current detection schemes, is discussed. This leads to filter parameters that are different than those of the traditional design. Hardware experimental results verify the validity of the above analysis and design...|$|R
40|$|The background, {{theoretical}} concepts, {{and methodology}} {{for construction of}} vector wind profiles based on a statistical model are presented. The derived monthly vector wind profiles are to be applied by the launch vehicle design community for establishing realistic estimates of critical vehicle design parameter dispersions related to wind profile dispersions. During initial studies a number of months are used to establish the model profiles that produce the largest monthly dispersions of ascent vehicle aerodynamic load indicators. The largest monthly dispersions for wind, which occur during the winter high-wind months, are used for establishing the design reference dispersions for the aerodynamic load indicators. This document includes {{a description of the}} computational process for the vector wind model including <b>specification</b> <b>of</b> <b>input</b> data, parameter settings, and output data formats. Sample output data listings are provided to aid the user in the verification of test output...|$|R
40|$|All uses of HTML forms {{may benefit}} from {{validation}} <b>of</b> the specified <b>input</b> field values. Simple validation matches individual values against specified formats, while more advanced validation may involve interdependencies of form fields. There is currently no standard for specifying or implementing such validation. Today, CGI programmers often use Perl libraries for simple server-side validation or program customized JavaScript solutions for client-side validation. We present PowerForms, which is an add-on to HTML forms that allows a purely declarative <b>specification</b> <b>of</b> <b>input</b> formats and sophisticated interdependencies of form fields. While our work {{may be seen as}} inspiration for a future extension of HTML, it is also available for CGI programmers today through a preprocessor that translates a PowerForms document into a combination of standard HTML and JavaScript that works on all combinations of platforms and browsers. The definitions of PowerForms formats are syntactically d [...] ...|$|R
40|$|The {{results of}} using data envelopment {{analysis}} (DEA) to assess the relative efficiency of 45 Canadian universities are reported. Outcomes are obtained from nine different <b>specifications</b> <b>of</b> <b>inputs</b> and outputs. The relative efficiencies are quite consistent across the alternative <b>specifications.</b> A subset <b>of</b> universities - including universities from each of three categories (comprehensive with medical school, comprehensive without medical school, and primarily undergraduate) - are regularly found efficient and a subset quite inefficient but, overall and for most universities, the efficiency scores are relatively high. Simulation of the recent 20 -percent cut in provincial grants to the Alberta universities illustrates how potential efficiency improvements (as implied and measured by this methodology) might be realized but it also illustrates certain limitations. Regression analysis is used {{in an effort to}} identify further determinants of efficiency. While there are limitations to the methodology and the available (especially output) measures which makes the specific efficiency outcomes tentative, this analysis provides insight to university productivity in Canada and its analysis. ...|$|R
40|$|Yacc {{provides}} a general tool for imposing structure on the input {{to a computer}} program. The Yacc user prepares a <b>specification</b> <b>of</b> the <b>input</b> process; this includes rules describing the input structure, code to be invoked when these rules are recognized, and a low-level routine to do the basic input. Yacc then generates a function to control the input process. This function, called a parser, calls the user-supplied low-leve...|$|R
40|$|Deliverable 1. 2 of the WP-E Project ‘Passenger-Oriented Enhanced Metrics’ (POEM), is the {{passenger}} and traffic data input to model. This Deliverable details the results, to date, {{of an investigation}} of available data for the Project using data samples, and presents the <b>specification</b> <b>of</b> data <b>inputs</b> to the model. It also introduces different approaches to resolve the challenge to impute missing passenger data and distribute summarised passengers to individual flights...|$|R
40|$|Process {{simulation}} {{involves the}} evaluation of output variables by the <b>specification</b> <b>of</b> <b>input</b> variables and process parameters. However, in a real process, input data and parameters cannot be known without uncertainty. This fact may limit the utilization of simulation results to predict plant behavior. In order to achieve a more realistic analysis, the procedure of stochastic simulation can be conducted. This technique {{is based on a}} large set of simulation runs where input variables and parameters are randomly selected according to adequate probability density functions. The objective of this work is to illustrate the application of a stochastic simulation procedure to the process of fractionation of orange essential oil, using supercritical carbon dioxide in a multistage extraction column. Analysis of the proposed example demonstrates the importance of the stochastic simulation to develop more reliable designs and operating conditions for a supercritical fluid extraction process...|$|R
40|$|The Small Interactive Image Processing System (SMIP) is {{designed}} to facilitate the acquisition, digital processing and recording of image data as well as pattern recognition in an interactive mode. Objectives of the system are ease of communication with the computer by personnel who are not expert programmers, fast response to requests for information on pictures, complete error recovery as well as simplification of future programming efforts for extension of the system. The SMIP system is intended for operation under OS/MVT on an IBM 360 / 75 or 91 computer equipped with the IBM- 2250 Model 1 display unit. This terminal is used as an interface between user and main computer. It has an alphanumeric keyboard, a programmed function keyboard and a light pen which are used for <b>specification</b> <b>of</b> <b>input</b> to the system. Output from the system is displayed on the screen as messages and pictures...|$|R
40|$|This {{bachelor}} thesis {{describes the}} realization of development of application, which extends the functionality of Integrated study information system on the University of economics in Prague. First part of this thesis is mainly focused on the analytical part of the application. Main topic is the work, practice and experience of a study officer in entrance exams. This part is also focused on the <b>specification</b> <b>of</b> <b>input</b> and output data. And these specifications are {{used to describe the}} new functionality requirements of the application. Second part is mainly about the development of the application. The development is seen and described from it experts point of view and also from users point of view. Each algorithm, procedure and function is explained and the solution of problems is well covered. In the users point of view part there is a simple user guide about how to use this application...|$|R
50|$|An {{important}} aspect, {{with respect}} to which the following methods differ is whether the orthogonalization of the basis functionals is to be performed over the idealized <b>specification</b> <b>of</b> the <b>input</b> signal (e.g. gaussian, white noise) or over the actual realization <b>of</b> the <b>input</b> (i.e. the pseudo-random, bounded, almost-white version of gaussian white noise, or any other stimulus). The latter methods, despite their lack of mathematical elegance, {{have been shown to}} be more flexible (as arbitrary inputs can be easily accommodated) and precise (due to the effect that the idealized version <b>of</b> the <b>input</b> signal is not always realizable).|$|R
40|$|We model {{strategic}} {{interaction in}} a differentiated input {{market as a}} game among two suppliers and n retailers. Each one of the upstream firms chooses the <b>specification</b> <b>of</b> the <b>input</b> which it will offer. Then, retailers choose their type from a continuum of possibilities. The decisions made in these two first stages affect the degree of compatibility between each retailer's ideal <b>input</b> <b>specification</b> and that <b>of</b> the <b>inputs</b> offered by the two upstream firms. In a third stage, upstream firms compete setting input prices. Equilibrium may be of the two-vendor policy or of the technological monopoly type...|$|R
40|$|Gmsh is a 3 D {{finite element}} grid {{generator}} with a build-in CAD engine and post-processor. Its design {{goal is to}} provide a fast, light and user-friendly meshing tool with parametric input and advanced visualization capabilities. Gmsh is built around four modules: geometry, mesh, solver and post-processing. The <b>specification</b> <b>of</b> any <b>input</b> to these modules is done either interactively using the graphical user interface or in ASCII text files using Gmsh's own scripting language. See [URL] for more information...|$|R
50|$|Opposition politicians, {{industry}} commentators and the Association of Train Operating Companies {{were critical}} of {{aspects of the}} scheme, particularly the micromanagement of the proposed trains' <b>specifications,</b> and lack <b>of</b> <b>input</b> from potential operators. Also, the Department for Transport's targets for energy consumption {{were reported to have}} been considered impracticable.|$|R
40|$|An {{intelligent}} {{machine is}} autonomous. An autonomous machine can operate successfully in {{a diversity of}} situations without resort to intervention by “higher level” processes, for example, humans. Physical machines are ultimately force or torque controlled dynamical systems: the <b>specification</b> <b>of</b> <b>input</b> torques, whether via syntactic prescriptions or feedback controllers, results in certain classes of vector fields. Control procedures whose resulting vector fields have globally attracting goal states may properly be said to evince autonomous behavior. In this light {{it makes sense to}} “program” robots using the language of dynamical systems via feedback. This talk will review various procedures developed within the Yale Robotics Lab that result in provably autonomous behavior according to the criterion developed above. These procedures are expressed in a rudimentary “geometric programming language” appropriate to the domain of tasks being undertaken and result in closed loop dynamical systems with global convergence properties (often, the strongest that the topology of the program can allow). A variety of simulation results and physical experimental studies will attest to the practicability of these methods. For more information: Kod*La...|$|R
30|$|In {{addition}} to these maps, the model requires <b>specification</b> <b>of</b> other <b>inputs</b> that are defined by constants; these are depending on the selected rheology. For the Voellmy rheology the inputs needed are: the turbulent coefficient, apparent friction angle, gravity acceleration, unit weight of the flow. For the Bingham rheology the model requires the following inputs: viscosity, yield strength, gravity acceleration, and unit weight of the flow. If the entrainment module is selected for a simulation, the required inputs are: the velocity scour rate coefficient or the height scour rate coefficient.|$|R
40|$|Nonparametric {{efficiency}} {{analysis has}} become a widely applied technique to support industrial benchmarking {{as well as a}} variety of incentive-based regulation policies. In practice such exercises are often plagued by incomplete knowledge about the correct <b>specifications</b> <b>of</b> <b>inputs</b> and outputs. Simar and Wilson (2001) and Schubert and Simar (2011) propose restriction tests to support such specification decisions for cross-section data. However, the typical oligopolized market structure pertinent to regulation contexts often leads to low numbers of cross-section observations, rendering reliable estimation based on these tests practically unfeasible. This small-sample problem could often be avoided with the use of panel data, which would in any case require an extension of the cross-section restriction tests to handle panel data. In this paper we derive these tests. We prove the consistency of the proposed method and apply it to a sample of US natural gas transmission companies in 2003 through 2007. We find that the total quantity of gas delivered and gas delivered in peak periods measure essentially the same output. Therefore only one needs to be included. We also show that the length of mains as a measure of transportation service is non-redundant and therefore must be included...|$|R
40|$|While an {{opportunity}} cost argument suggests that recessions are ideal times to undergo R&D aimed at enhancing productivity, empirical measures of U. S. R&Dactivity are procyclical. To resolve this discrepancy, I propose a calibrated real business cycle model featuring R&D-based growth through horizontal innovations. The model {{is used to}} quantitatively analyze the impact of business cycle shocks under various <b>specifications</b> <b>of</b> the R&D process. I find that the <b>specification</b> <b>of</b> R&D <b>inputs</b> {{is essential for the}} cyclicality of R&D activities. First, the popular knowledge-driven <b>specification</b> <b>of</b> R&D has a hard time to generate both procyclical R&D investment and procyclical R&D labor at the same time. Second, the calibrated multi-input specification generates procyclical R&D investment as well as procyclical employment of scientists. In addition, the endogenous growth mechanism gives rise to amplification of business cycle shocks...|$|R
40|$|This paper {{addresses}} {{the problem of}} system stability of the NASA EOS satellite power system. A potential stability problem exists without a clear <b>specification</b> <b>of</b> the payload <b>input</b> impedance characteristic. Design guidelines are established for {{the control of the}} power system and the individual subcomponents to help insure stability with an unknown complex load. A testbed of the EOS power system is employed to verify the analysis...|$|R
40|$|A {{physicist}} analysing {{data from}} the LHC experiments {{will have to deal}} with data and computing resources that are distributed across multiple locations and have different access methods. Ganga helps by providing a uniform high-level interface to the different low-level solutions for the required tasks, ranging from the <b>specification</b> <b>of</b> <b>input</b> data to the retrieval and post-processing of the output. For LHCb and ATLAS the goal is to assist in running jobs based on the Gaudi/Athena C++ framework. Ganga is written in python and presents the user with a single GUI rather than a set of different applications. It uses pluggable modules to interact with external tools for operations such as querying metadata catalogues, job configuration and job submission. At start-up, the user is presented with a list of templates for common analysis tasks, and information about ongoing tasks is stored from one invocation to the next. Ganga can also be used through a command line interface. This closely mirrors the functionality of the GUI, allowing easy transition from one to the other. This paper describes the Ganga design and functionality, and illustrates its use in the distributed analysis systems of the LHCb and ATLAS experiments in the context of their 2004 data challenges...|$|R
40|$|Format <b>specifications</b> <b>of</b> data <b>input</b> are {{critical}} to model-based fuzz testing. Present methods cannot describe the format accurately, which leads to high redundancy in testing practices. In order to improve testing efficiency, we propose a grammar-driven approach to fuzz testing. Firstly, we build a formal model of data format using higher-order attribute grammars, and construct syntax tree {{on the basis of}} data samples. Secondly, all nodes in the syntax tree are traversed and mutated to generate test cases according to the attribute rules. Experimental results show that the proposed approach can reduce invalid and redundant test cases, and discover potential vulnerabilities of software implementations effectively. </p...|$|R
