34|36|Public
5000|$|... where [...] is the {{complexity}} of the underlying <b>sequential</b> <b>sorting</b> method ...|$|E
5000|$|A common {{strategy}} is to set [...] equal {{to the number of}} processors available. The data is then distributed among the processors, which perform the sorting of buckets using some other, <b>sequential,</b> <b>sorting</b> algorithm.|$|E
40|$|The {{traditional}} sorting technique, <b>sequential</b> <b>sorting,</b> is inefficient {{with increasing}} amounts of data that can be stored on computers. Researchers looking for faster sorting techniques have turned to parallel computing to address the limitations of <b>sequential</b> <b>sorting.</b> This project involves the implementation of three parallel sorting algorithms on CUDA, a parallel computing architecture which implements algorithms on Graphics Processing Units (GPUs) ...|$|E
5000|$|... time {{if enough}} {{processors}} are available. Such a sort {{can perform well}} in practice when combined with a fast stable <b>sequential</b> <b>sort,</b> such as insertion sort, and a fast sequential merge as a base case for merging small arrays.|$|R
50|$|The {{following}} example will {{construct a}} V-optimal histogram having a Sort Value of Value, a Source Value of Frequency, and a Partition Class of Serial. In practice, almost all histograms used in research or commercial products {{are of the}} Serial class, meaning that <b>sequential</b> <b>sort</b> values are placed in either the same bucket, or sequential buckets. For example, values 1, 2, 3 and 4 will be in buckets 1 and 2, or buckets 1, 2 and 3, but never in buckets 1 and 3. That will be taken as an assumption in any further discussion.|$|R
40|$|International audienceThe "ORLA" {{research}} project {{is attempting to}} evaluate the pathways and limitations of recycling steelworks slag through <b>sequential</b> physical <b>sorting.</b> The objective was to concentrate two distinct fractions; one comprising the iron-bearing slag for intra-steelworks recycling, and the other the calcium- and calc-silicate-bearing slag for cement-plant recycling...|$|R
40|$|We propose new <b>sequential</b> <b>sorting</b> {{operations}} by adapting {{techniques and}} methods used for designing parallel sorting algorithms. Although the norm is to parallelize a sequential algorithm to improve performance, we adapt a contrarian approach: we employ parallel computing techniques {{to speed up}} <b>sequential</b> <b>sorting.</b> Our methods can also work for multi-core sorting with minor adjustments that do not necessarily require full parallelization of the original sequential algorithm. The proposed approach leads {{to the development of}} asymptotically efficient deterministic and randomized sorting operations whose practical sequential and multi-core performance, as witnessed by an experimental study, matches or surpasses existing optimized sorting algorithm implementations. We utilize parallel sorting techniques such as deterministic regular sampling and random oversampling. We extend the notion of deterministic regular sampling into deterministic regular oversampling for sequential and multi-core sorting and demonstrate its potential. We then show how these techniques can be used for <b>sequential</b> <b>sorting</b> and also lead to better multi-core sorting algorithm performance as witnessed by the undertaken experimental study...|$|E
40|$|We employ {{techniques}} {{developed and}} used {{in the design of}} parallel sorting algorithms to propose a new framework for <b>sequential</b> <b>sorting.</b> This framework is then used to design new deterministic and randomized sorting methods whose asymptotic worst-case running time can match the existing lower bound for sorting, yet their practical performance, as witnessed by an experimental study, surpasses existing optimized sorting algorithm implementations. We adapt in the proposed new framework techniques used for parallel sorting such as deterministic regular sampling and random oversampling. We extend the notion of deterministic regular sampling into deterministic regular oversampling for <b>sequential</b> <b>sorting</b> and show its potential along with the previously available technique of random oversampling. We then show how our newly developed techniques can utilize and potentially speedup several existing <b>sequential</b> <b>sorting</b> algorithms. Experimental results based on an implementation of our two methods support our efficiency claims. Our new approach maintains better locality of reference and can naturally benefit from multicore architectures. This is to our knowledge the first time that sequential computing can beneficially draw from parallel computing techniques...|$|E
40|$|AbstractA {{variant of}} HEAPSORT, called BOTTOM-UP-HEAPSORT, is presented. It {{is based on}} a new reheap procedure. This <b>sequential</b> <b>sorting</b> {{algorithm}} is easy to implement and beats, on an average, QUICKSORT if n⩾ 400 and a clever version of QUICKSORT (where the split object is the median of 3 randomly chosen objects) if n⩾ 16000. The worst-case number of comparisons is bounded by 1. 5 n log n+O(n). Moreover, the new reheap procedure improves the delete procedure for the heap data structure for all n...|$|E
40|$|This paper {{performs}} a thorough analysis of competing construction methods {{for the design}} of size (SMB) and value (HML) spread portfolios à la Fama-French. This quasi-clinical investigation of methodological choices uncovers substantial differences in the capacity of estimated premiums to translate stock characteristics into returns. A <b>sequential</b> <b>sort</b> of stocks into long and short portfolios conditioned on control variables (“pre-conditioning”) produces factors that best reflect the corresponding fundamental attributes. Our results are stronger when using the whole firm sample to define breakpoints and a triple sort, which ensures the same diversification (in terms of number of firms) across the characteristic-sorted portfolios forming the long and short legs of the factor. Our results are robust to the inclusion of the momentum dimension in the multiple sorting. The best method produces a volatile and insignificant size premium, but a high and stable value premium...|$|R
40|$|Sorting {{is among}} the first of algorithm, than any {{computer}} science student encounters during college and it is considered as a simple and well studied problem. With the advancement in parallel processing many parallel sorting algorithms have been investigated. These algorithms are designed {{for a variety of}} parallel computer architectures. In this paper, a comparative analysis of performance of three different types of <b>sorting</b> algorithms viz. <b>sequential</b> quick <b>sort,</b> parallel quick sort and hyperquicksort is presented. Quick sort is a divideand-conquer algorithm that sorts a sequence by recursively dividing it into smaller subsequences, and has Ө(nlogn) complexity for n data values. The comparative analysis is based on comparing average sorting times and speedup achieved in parallel <b>sorting</b> over <b>sequential</b> quick <b>sort</b> and comparing number of comparisons. The time complexity for each sorting algorithm will also be mentioned and analyzed...|$|R
40|$|Abstract|Sorting is a {{fundamental}} algorithm used extensively in computer science as an interme-diate step in many applications. The performance of sorting algorithms is heavily in uenced {{by the type of}} data being sorted, and the machine being used. To assist in obtaining portable performance for sorting algorithms, we propose an install-time system for automatically constructing <b>sequential</b> and parallel <b>sorts</b> that are highly tuned for the tar-get architecture. Our system has two steps: rst a hybrid <b>sequential</b> divide-and-conquer <b>sort</b> is con-structed and then this algorithm is parallelized us-ing a shared work-queue model. To evaluate our system, we compare automatically generated sort-ing algorithms to sequential and parallel versions of the C++STL sort. The generated sorts are shown to be competitive with STL <b>sort</b> on <b>sequential</b> sys-tems and to outperform the parallel STL sort on a 4 processor Xeon server...|$|R
40|$|In this paper, {{we present}} a novel {{approach}} for par-allel sorting on stream processing architectures. It is based on adaptive bitonic sorting. For sorting n val-ues utilizing p stream processor units, this approach achieves the optimal time complexity O((n log n) /p). While this makes our approach competitive with common <b>sequential</b> <b>sorting</b> algorithms not only from a theoretical viewpoint, it is also very fast from a practi-cal viewpoint. This is achieved by using efficient linear stream memory accesses (and by combining the opti-mal time approach with algorithms optimized for small input sequences). We present an implementation on modern pro-grammable graphics hardware (GPUs). On recent GPUs, our optimal parallel sorting approach has shown to be remarkably faster than <b>sequential</b> <b>sorting</b> on the CPU, {{and it is also}} faster than previous non-optimal sorting approaches on the GPU for sufficiently large input sequences. Because of the excellent scalability of our algorithm with the number of stream processor units p (up to n / log 2 n or even n / log n units, depend-ing on the stream architecture), our approach profits heavily from the trend of increasing number of frag-ment processor units on GPUs, so that we can expect further speed improvement with upcoming GPU gener-ations. 1...|$|E
40|$|We {{propose a}} simple but {{fundamental}} methodological change to Fama and French (1993) factor construction procedure. Consistent with Lambert and Hübner (2013) <b>sequential</b> <b>sorting</b> procedure to classify stocks, our methodology controls ex ante for pricing errors produced by multifactor models. Our size and value factors deliver less specification errors when used to price portfolios, especially regarding low size and high B/M stocks. Furthermore, this alternative framework generates much stronger “turn-of-the-year” size and “through-the-year” book-to-market effects than conventionally documented. The factors also display a slight competitive advantage on the taxonomy of low turnover market anomalies defined by Novy-Marx and Velikov (2015) ...|$|E
40|$|Cost {{and speed}} {{considerations}} often conduct to measure/classify items {{according to two}} comprehensive and exclusive classes/categories (binary scale). Assume items are submitted for sorting according to binary scale (e. g. Type 1 and Type 2) using some sorting machine (SM). Repeatability or consistency testing of the SM is examined by {{the help of the}} correlation coefficient between the random variables denoting the number of items that were classified to the first category (for example) in two <b>sequential</b> <b>sorting</b> procedures. Paradoxical results lead to the conclusion that this measure is not suitable for measuring repeatability and further research is needed in order to find methods for checking consistency...|$|E
40|$|This article {{describes}} the basic concepts {{for the use of}} Webmining, within which are techniques (Rules of Association, <b>sequential</b> patterns, <b>sorting</b> and clustering) and areas (web content mining, mining structure and mining use Web site) of greater use for the information discovery. It also aims to expose the importance of the use of this technique within units of information, bearing in mind that this works from the users' needs, essential to the operation...|$|R
30|$|Sequential mining {{technique}} {{is a kind}} of association analysis, which is mainly used to find <b>sequential</b> patterns. By <b>sorting</b> all the events associated with an object in increasing order of their timestamps, a sequence for the object is obtained.|$|R
40|$|We {{discuss how}} string sorting {{algorithms}} can be parallelized on modern multi-core shared memory machines. As {{a synthesis of}} the best <b>sequential</b> string <b>sorting</b> algorithms and successful parallel sorting algorithms for atomic objects, we propose string sample sort. The algorithm makes effective use of the memory hierarchy, uses additional word level parallelism, and largely avoids branch mispredictions. Additionally, we parallelize variants of multikey quicksort and radix sort that are also useful in certain situations. Comment: 34 pages, 7 figures and 12 table...|$|R
40|$|This paper {{makes an}} effort on parallelizing the {{quicksort}} algorithm using openMP on java platform(JOMP) to sort data by sharing the partitions generated from regular sampling. The basic {{idea is to}} perform the initial partitioning of data sequentially and calling the recursive function in parallel manner, and finally to conduct a performance evaluation for the implementation. The evaluation is based on comparing sorting times with an algorithm which uses initial partitioned set between threads and to the simple <b>sequential</b> <b>sorting</b> algorithm. Comparision study is also made with the OpenMP parallel quicksort implemented using C and the results shows that JOMP gives better result with respect to time complexity than OpenMP in C...|$|E
40|$|New {{performance}} leaps {{has been}} achieved with multiprogramming and multi-core systems. Present parallel programming techniques and environment needs significant changes in programs to accomplish parallelism and also constitute complex, confusing and error-prone constructs and rules. Intel Cilk Plus is a C based computing system that presents a straight forward and well-structured model for the development, verification and analysis of multi-core and parallel programming. In this article, two programs are developed using Intel Cilk Plus. Two <b>sequential</b> <b>sorting</b> programs in C/C++ language are converted to multi-core programs in Intel Cilk Plus framework to achieve parallelism and better performance. Converted program in Cilk Plus is then checked for various conditions using tools of Cilk and after that, comparison of performance and speedup achieved over the single-core sequential program is discussed and reported...|$|E
40|$|In {{this work}} a {{realistic}} machine model, the CMP-model, is investigated. This model captures the cache hierarchies on mainstream CMPs {{as well as}} the ways that these caches interacts. A parallel programming library for benchmarking is presented. The presented library introduces policy based scheduling allowing fair evaluation of scheduling algorithms. The parallel-depth-first scheduler theoretically perform better than the widely used work-stealing scheduler, in the CMP-cache model. Both schedulers are implemented in the benchmarking library. Efficient parallelizations of the well-known <b>sequential</b> <b>sorting</b> algorithms quicksort and multi-way mergesort are analysed based on the CMP-cache model. The parallel quicksort algorithm is based on a parallelization of the in-place sequential partitioning algorithm. The parallel multi-way mergesort is based on a f-way partitioning algorithm. The presented library is used to evaluate the two analysed parallel sorting algorithms using both the implemented schedulers. We find that efficient paralle...|$|E
40|$|A new hidden-line {{algorithm}} is proposed for illustrating objects consisting of plane faces. The algorithm determines the degree of edge and classifies edges and faces into contoural and non-contoural. To reduce memory requirements, <b>sequential</b> files and <b>sorting</b> are used. The {{algorithm is}} particularly intended for illustrating complex objects, such as curved surfaces approximated by plane face...|$|R
40|$|In {{our study}} we {{implemented}} and compared seven <b>sequential</b> and parallel <b>sorting</b> algorithms: bitonic sort, multistep bitonic sort, adaptive bitonic sort, merge sort, quicksort, radix sort and sample <b>sort.</b> <b>Sequential</b> algorithms were implemented on a {{central processing unit}} using C++, whereas parallel algorithms were implemented on a graphics processing unit using CUDA platform. We chose these algorithms because {{to the best of}} our knowledge their sequential and parallel implementations were not yet compared all together in the same execution environment. We improved the above mentioned implementations and adopted them to be able to sort input sequences of arbitrary length. We compared algorithms on six different input distributions, which consisted of 32 -bit numbers, 32 -bit key-value pairs, 64 -bit numbers and 64 -bit key-value pairs. In this report we give a short description of seven sorting algorithms and all the results obtained by our tests. Comment: Technical repor...|$|R
40|$|Sorting long {{sequences}} of keys {{is a problem}} that occurs in many different applications. For embedded systems, a uniprocessor software solution is often not applicable due to the low performance, while realizing multiprocessor sorting methods on parallel computers is much too expensive with respect to power consumption, physical weight, and cost. We investigate cost/performance tradeoffs for hybrid sorting algorithms that use a mixture of <b>sequential</b> merge <b>sort</b> and systolic insertion sort techniques. We propose a scalable architecture for integer sorting that consists of a uniprocessor and an FPGA-based parallel systolic co-processor. Speedups obtained analytically and experimentally and depending on hardware (cost) constraints are determined as a function of time constants of the uniprocessor and the co-processor...|$|R
40|$|With {{refinements}} to the WEAK-HEAPSORT algorithm we {{establish the}} general and practical relevant <b>sequential</b> <b>sorting</b> algorithm RELAXED-WEAK-HEAPSORT executing exactly ndlog ne Γ 2 dlog ne + 1 n log n Γ 0 : 9 n comparisons {{on any given}} input. The number of transpositions is bounded by n plus the number of comparisons. Experiments show that RELAXED-WEAK-HEAPSORT only requires O(n) extra bits. Even if this space is not available, with QUICK-WEAK-HEAPSORT we propose an efficient QUICKSORT variant with n log n+ 0 : 2 n+ o(n) comparisons on the average. Furthermore, we present data showing that WEAK-HEAPSORT, RELAXED-WEAK-HEAPSORT and QUICKWEAK -HEAPSORT beat other performant QUICKSORT and HEAPSORT variants even for moderate values of n. 1 Introduction Similar to Fibonacci-Heaps (Fredman and Tarjan (1987)), Weak-Heaps introduced by Dutton (1992) are obtained by relaxing the heap requirements. More precisely, a Weak-Heap is a binary tree representation of a totally ordere [...] ...|$|E
40|$|In this master's thesis we studied, {{implemented}} and compared sequential and parallel sorting algorithms. We implemented seven algorithms: bitonic sort, multistep bitonic sort, adaptive bitonic sort, merge sort, quicksort, radix sort and sample sort. Sequential algorithms were implemented on a {{central processing unit}} using C++, whereas parallel algorithms were implemented on a graphics processing unit using CUDA architecture. We improved the above mentioned implementations and adopted {{them to be able}} to sort input sequences of arbitrary length. We compared algorithms on six different input distributions, which consist of 32 -bit numbers, 32 -bit key-value pairs, 64 -bit numbers and 64 -bit key-value pairs. The results show that radix sort is the fastest <b>sequential</b> <b>sorting</b> algorithm, whereas radix sort and merge sort are the fastest parallel algorithms (depending on the input distribution). With parallel implementations we achieved speedups of up to 157 -times in comparison to sequential implementations...|$|E
40|$|We adapt {{techniques}} {{employed in}} the design of parallel sorting algorithms to propose new sequential and multi-core sorting operations. The proposed approach is used to develop asymptotically efficient deterministic and randomized sorting operations whose practical sequential and multi-core performance, as witnessed by an experimental study, matches or surpasses existing optimized sorting algorithm implementations. We utilize techniques employed in parallel sorting such as deterministic regular sampling and random oversampling. We extend the notion of deterministic regular sampling into deterministic regular oversampling for sequential and multi-core sorting and demonstrate its potential along with the previously used technique of random oversampling. We then show how our techniques can not only utilize but also potentially speedup several existing <b>sequential</b> <b>sorting</b> algorithms and also lead to better multi-core sorting algorithm performance. Experimental results based on an implementation of our two operations support our claims on sequential and multi-core processors...|$|E
40|$|Analyzing the average-case {{complexity}} of algorithms {{is a very}} practical but very difficult problem in computer science. In the past few years, we have demonstrated that Kolmogorov complexity is an important tool for analyzing the average-case {{complexity of}} algorithms. We have developed the incompressibility method [7]. In this paper, we use several simple examples to further demonstrate the power and simplicity of such method. We prove bounds on the average-case number of stacks (queues) required for <b>sorting</b> <b>sequential</b> or parallel Queueusort or Stacksort...|$|R
40|$|We {{discuss how}} string sorting {{algorithms}} can be parallelized on modern multi-core shared memory machines. As {{a synthesis of}} the best <b>sequential</b> string <b>sorting</b> algorithms and successful parallel sorting algorithms for atomic objects, we first propose string sample sort. The algorithm makes effective use of the memory hierarchy, uses additional word level parallelism, and largely avoids branch mispredictions. Then we focus on NUMA architectures, and develop parallel multiway LCP-merge and -mergesort {{to reduce the number}} of random memory accesses to remote nodes. Additionally, we parallelize variants of multikey quicksort and radix sort that are also useful in certain situations. Comprehensive experiments on five current multi-core platforms are then reported and discussed. The experiments show that our implementations scale very well on real-world inputs and modern machines. Comment: 46 pages, extension of "Parallel String Sample Sort" arXiv: 1305. 115...|$|R
2500|$|Insertions can be {{very slow}} in a <b>sorted</b> <b>sequential</b> file because room for the {{inserted}} record must be made. Inserting a record before the first record requires shifting all of the records down one. Such an operation is just too expensive to be practical. One solution is to leave some spaces. Instead of densely packing all the records in a block, the block can have some free space to allow for subsequent insertions. Those spaces would be marked {{as if they were}} [...] "deleted" [...] records.|$|R
40|$|We {{show the}} {{importance}} of <b>sequential</b> <b>sorting</b> {{in the context of}} in memory parallel sorting of large data sets of 64 bit keys. First, we analyze several sequential strategies like Straight Insertion, Quick sort, Radix sort and CC-Radix sort. As a consequence of the analysis, we propose a new algorithm that we call Sequential Counting Split Radix sort, SCS-Radix sort. SCS-Radix sort is a combination of some of the algorithms analyzed and other new ideas. There are three important contributions in SCS-Radix sort. First, the work saved by detecting data skew dynamically. Second, the exploitation of the memory hierarchy done by the algorithm. Third, the execution time stability of SCS-Radix when sorting data sets with different characteristics. We evaluate the use of SCS-Radix sort {{in the context of a}} parallel sorting algorithm on an SGI Origin 2000. The parallel algorithm is from 1 : 2 to 45 times faster using SCS-Radix sort than using Radix sort or Quick sort. ...|$|E
40|$|Many {{computing}} problems {{benefit from}} dynamic partition of data into smaller chunks with better parallelism and locality. However, {{it is difficult}} to partition all types of inputs with the same high efficiency. This paper presents a new partition method in sorting scenario based on probability distribution, an idea first studied by Janus and Lamagna in early 1980 ’s on a mainframe computer. The new technique makes three improvements. The first is a rigorous sampling technique that ensures accurate estimate of the probability distribution. The second is an efficient implementation on modern, cache-based machines. The last is the use of probability distribution in parallel sorting. Experiments show 10 - 30 % improvement in partition balance and 20 - 70 % reduction in partition overhead, compared to two commonly used techniques. The new method reduces the parallel sorting time by 33 - 50 % and outperforms the previous fastest <b>sequential</b> <b>sorting</b> technique by up to 30 %. ...|$|E
40|$|Many {{computing}} problems {{benefit from}} dynamic partition of data into smaller chunks with better parallelism and locality. Previous partition methods either have high overhead or only apply to uniformly distributed data. This paper presents a new partition method in sorting scenario based on probability distribution, an idea first studied by Janus and Lamagna in early 1980 's on a mainframe computer. Our new method makes three contributions. The firstisa rigorous sampling method that ensures accurate {{estimate of the}} probability distribution. The second is an efficient implementation on modern machines. Finally, it uses probability distribution in parallel sorting. Experiments show 1030 % improvement in partition balance and 20 - 70 % reduction in partition overhead, compared to approaches popular on current systems. When used in the classical problem of sorting, the method not only saves parallel sorting time by 33 - 50 %, but also outperforms by up to 30 % the fastest <b>sequential</b> <b>sorting</b> methods from the recent literature...|$|E
40|$|The use of {{multiprocessor}} architectures {{requires the}} parallelization of sorting algorithms. A parallel sorting algorithm based on horizontal parallelization is presented. This algorithm is suited for large data volumes (external sorting) {{and does not}} suffer from processing skew in presence of data skew. The core of the parallel sorting algorithm is a new adaptive partitioning method. The effect of data skew is remedied by taking samples representing {{the distribution of the}} input data. The parallel algorithm has been implemented on top of a shared disk multiprocessor architecture. The performance evaluation of the algorithm shows that it has linear speedup. Furthermore, the optimal degree of CPU parallelism is derived if I/O limitations are taken into account. 1 Introduction Data sorting {{plays an important role in}} computer science and has been studied extensively [23]. The problem of sorting is easily understood. In the <b>sequential</b> case, <b>sorting</b> of tuples (i. e. data items) has at lea [...] ...|$|R
40|$|We {{introduce}} a probabilistic sequential algorithm for stable sorting n uniformly distributed keys in an arbitrary range. The algorithm runs in linear time and sorts {{all but a}} very small fraction 2 -# n) of the input sequences; the best previously known bound was 2 -# n/(lg n lg lg n)). An EREW PRAM version of the <b>sequential</b> algorithm <b>sorts</b> in O((n/p+lg p) lg n/ lg (n/p + lg n)) time using p # n processors under the same probabilistic conditions. For a CRCW PRAM we improve upon the probabilistic bound of 2 -# n/(lg n lg lg n)) obtained by Rajasekaran and Sen to derive a 2 -# n lg lg n/ lg n) bound. Two architecture independent parallel algorithms described under {{the framework of the}} BulkSynchronous Parallel model are also presented. For varying ratios of n/p they sort in optimal parallel computation time; the former algorithm sorts all but a 2 -# n) fraction of the input sequences whereas the latter algorithm sorts all but a n -#(1) fraction. Additionally, we present experi [...] ...|$|R
40|$|Hadoop is a {{well known}} {{open-source}} implementation of the MapReduce paradigm that has as foundations the map and reduce algorithmic skeletons. In this thesis we study whether other types of skeletons {{can be added to}} Hadoop in order to increase its usability and performance. We investigate on how to modify Hadoop so as to accept other types of jobs and tasks while being as non-intrusive as possible. We pick two data-parallel skeletons: filter and zip, implement one of them (filter) and document why the other (zip) is not a good fit for Hadoop. A further two skeletons (<b>sequential</b> and <b>sort)</b> are then chosen and added to the framework. We develop a library that runs on top of Hadoop and lets the user specify complex chains of jobs in an more natural manner by decoupling the coordination and computation aspects. The skeletons and the library are evaluated in terms of usability and performance and compared with what can be done using the MapReduce implementation of Hadoop. i Acknowledgements Many thanks to my supervisor, Dr. Stratis Viglas for the prompt feedback and advice...|$|R
