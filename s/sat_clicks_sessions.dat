0|55|Public
5000|$|... "Grunt & <b>Click</b> (Stomp 442 <b>Session</b> Recording)" [...] (Bush, Ian, Bello, Benante) - 5:29 ...|$|R
40|$|Personalized search has {{recently}} attracted increasing attention. This paper focuses on utilizing click-through data to personalize the web search results, from a novel perspective based on subspace projection. Specifically, we represent a user profile as a vector subspace spanned by a basis generated from a word-correlation matrix, which {{is able to}} capture the dependencies between words in the “satisfied click” (<b>SAT</b> <b>Click)</b> documents. A personalized score for each document in the original result list returned by a search engine is computed by projecting the document (represented as a vector or another word-correlation subspace) onto the user profile subspace. The personalized scores are then used to re-rank the documents through the Borda’ ranking fusion method. Empirical evaluation is carried out on a real user log data set collected from a prominent search engine (Bing). Experimental results demonstrate the effectiveness of our methods, especially for the queries with high click entropy...|$|R
40|$|The {{success of}} deep {{learning}} {{has been applied}} to many natural lan-guage processing applications such as language model, machine translation, parsing, sentiment analysis and so on. The recent im-provement of neural language model such as Word 2 Vec improves accuracy of word embedding with much lower computational cost. The Word 2 Vec, however, is not properly designed to extract user intentions from search logs due to their sparssness and heterogene-ity such as <b>clicks,</b> <b>sessions,</b> documents and so on. In this paper, we propose Query 2 Vec that simultaneously learns sparse clicks, ses-sions and textual documents to improve query embedding task and collection ranking task by extracting deep intentions from the long tail queries. We tested our model on real click data collected during 2014 from a commercial search engine in Korea. 1...|$|R
40|$|We {{address the}} problem of {{clustering}} the refinements of a user search query. The clusters computed by our proposed algorithm can be used to improve the selection and placement of the query suggestions proposed by a search engine, and can also serve to summarize the different aspects of information relevant to the original user query. Our algorithm clusters refinements based on their likely underlying user intents by combining document <b>click</b> and <b>session</b> cooccurrence information. At its core, our algorithm operates by performing multiple random walks on a Markov graph that approximates user search behavior. A user study performed on top search engine queries shows that our clusters are rated better than corresponding clusters computed using approaches that use only document <b>click</b> or only <b>sessions</b> co-occurrence information. 1...|$|R
30|$|The process {{continues}} until Server A receives {{no further}} messages from other web servers {{for the same}} session. To do this, it uses a timeout scheme {{to decide on the}} end of the <b>click</b> stream <b>session.</b> It is at this point that Server A inserts a record in the Ship_Data table, including the information mentioned at the top of this section.|$|R
50|$|The {{first public}} {{demonstration}} of Windows, in 1983, had a simplistic shell called the Session Control Layer, {{which served as}} a constantly visible menu {{at the bottom of}} the screen. Clicking on Run would display a list of programs that one could launch, and <b>clicking</b> on <b>Session</b> Control would display a list of programs already running so one could switch between them.|$|R
3000|$|... (e.g., 5  s), {{a process}} on the server (first-server, for the session) {{will get the}} {{timestamp}} of the latest <b>click</b> in the <b>session,</b> and stores it in the field Tlast in Ship_Data. After {{an amount of time}} equal to t [...]...|$|R
40|$|This paper {{addresses}} the problem {{how to identify}} named entities from click-through data and classify them into predefined domains accurately. By proposing a novel measurement to measure the importance of contexts 1, the method identifies and ranks named entities effectively. The probabilis-tic ranking model combines multi-information from click-through data such as queries, <b>clicks</b> and <b>sessions.</b> To improve the identification recall, the identification and ranking steps iterate in a bootstrapping manner. Experi-ments on a real data set show that the method is effective...|$|R
40|$|Enormous {{amounts of}} {{information}} about Web site user behavior are collected in Web server logs. However, this information is only useful {{if it can be}} queried and analyzed to provide high-level knowledge about user navigation patterns, a task that requires powerful techniques. This chapter presents a number of approaches that combine data warehousing and data mining techniques in order to analyze Web logs. After introducing the well-known <b>click</b> and <b>session</b> data warehouse (DW) schemas, the chapter presents the subsession schema, which allows fast queries on sequence...|$|R
5000|$|The song [...] "Under the Milky Way" [...] was co-written by Kilbey {{with his}} then-girlfriend Karin Jansson of Pink Champagne. When drummer Richard Ploog {{was unable to}} find the right feel for the song, the band played to a <b>click</b> track and <b>session</b> {{musician}} Russ Kunkel was brought in to add the drums and percussion later.|$|R
40|$|This paper {{describes}} the concepts {{and development of}} a framework that allows {{information to be collected}} about a user’s activities during a web session that may span multiple sites. The framework is integrated with the Model View Controller architecture for web development and can be deployed to log information about <b>sessions,</b> <b>clicks,</b> and impressions. The framework is designed for ease of integration with the deployment environment...|$|R
40|$|Through {{hands-on}} experience and reviewing the literature, two instruction librarians explore and model {{best practices in}} incorporating technology into teaching, assessing and communicating with non-traditional adult students. Session content is applicable for face-to-face, blended, and online instructors. Attendees will walk away with a toolkit of resources, best practices, and further readings. To access handouts for this <b>session,</b> <b>click</b> on the 2 ̆ 2 Download 2 ̆ 2 button on the right...|$|R
40|$|Abstract. User {{reaction}} to traffic congestion can have severe impact on network stability and significant implication for traffic engineering. For example, users who persist in large peer-to-peer transfers despite congestion can drive the network into congestion collapse. On the other hand, users who abort large transfers can smoothen self-similar traffic. We present a tool, called SAX, for studying congestion-induced behavior of web surfers. SAX extracts information from HTTP packet traces and infers <b>clicks,</b> abortions and <b>sessions.</b> Measurements with SAX show how users back-off when congestion occurs. ...|$|R
30|$|The {{applications}} {{were developed}} using Java Server Pages (JSP), where each page contained four java classes: initiation, producer, receiver, and shipment. The first class implements two main functionalities: 1) creating an id for the {{session at the}} moment it starts, i.e., at the “first-server”, and 2) {{in the event of}} a <b>click</b> in the <b>session</b> leading to the server in question, the second functionality is to send the corresponding data to the first-server as elaborated in the design section (session id, IP of visited server, referrer IP, and time stamp).|$|R
40|$|Many {{libraries}} are 2 ̆ 2 techie challenged 2 ̆ 2 - {{if you are}} {{the lone}} tech in a library (or if you work in an understaffed IT department) this session will give you concrete tools and useful resources to make your life easier. From cloud-based tools that require little or no management to publicly available tech help, every library tech will find something useful in this presentation. To access the full presentation for this <b>session,</b> <b>click</b> on the 2 ̆ 7 Download 2 ̆ 7 button located to the right...|$|R
40|$|It {{has been}} said that the {{internet}} is the world’s biggest library, but all the books are on the floor. Librarians are more necessary than ever before in this virtual research environment, as students wade through the vastness of the internet in an effort to find relevant research. How to best help students as they tackle this wilderness? Information literacy tips and tricks will be presented and discussed. To access full presentation for this <b>session,</b> <b>click</b> on the 2 ̆ 7 Download 2 ̆ 7 button located to the right...|$|R
40|$|You {{may have}} heard that Islandora is a digital asset {{management}} system that combines the Drupal and Fedora open source applications…but what does this mean? What does it look like? Join PALS staff at this session for an introduction to Islandora {{and a look at}} its use in a variety of environments. We’ll also talk about how PALS became interested in Islandora and what we hope to accomplish through our investigation of the software and its applications. To access the full presentation for this <b>session,</b> <b>click</b> on the 2 ̆ 2 Link to Full Text 2 ̆ 2 button located to the right...|$|R
40|$|In 2010, Webster University Library {{completed}} {{a comprehensive review}} of their 150 + databases. This session will address the review process from development to execution including information on who was involved and why. To close the loop, this session will attempt to determine if this project was a success. Did the database cancellations promote use of more relevant resources? Did database usage go up? What were the unintended consequences? Quantitative and qualitative measures will be discussed. To access the full presentation for this <b>session,</b> <b>click</b> on the 2 ̆ 2 Download 2 ̆ 2 button located to the right. The handout for this session is available below...|$|R
40|$|Are {{you looking}} for an easy and {{informative}} way to communicate with other library staff, your patrons, or liaison departments? Try creating – or updating the look of – your library’s email newsletters! One free option that we’ve {{fallen in love with}} is MailChimp (www. mailchimp. com). During this hands-on workshop, we’ll help you set up a MailChimp account and walk you through all of the steps to create professional-looking newsletter templates, manage mailing lists, send out mailing ‘campaigns,’ and track analytics through MailChimp reports. To access the full presentation for this <b>session,</b> <b>click</b> on the 2 ̆ 2 Download 2 ̆ 2 button located to the right. The handout for this session is available below...|$|R
40|$|Click-through rate (CTR) {{prediction}} {{and relevance}} ranking are two fundamental problems in web advertising. In this study, we {{address the problem}} of modeling the relationship between CTR and relevance for sponsored search. We used normalized relevance scores comparable across all queries to represent relevance when modeling with CTR, instead of directly using human judgment labels or relevance scores valid only within same query. We classified clicks by identifying their relevance quality using dwell time and session information, and compared all clicks versus selective clicks effects when modeling relevance. Our results showed that the cleaned click signal outperforms raw click signal and others we explored, in terms of relevance score fitting. The cleaned clicks include clicks with dwell time greater than 5 seconds and last <b>clicks</b> in <b>session.</b> Besides traditional thoughts that there is no linear relation between click and relevance, we showed that the cleaned click based CTR can be fitted well with the normalized relevance scores using a quadratic regression model. This relevance-click model could help to train ranking models using processed click feedback to complement expensive human editorial relevance labels, or better leverage relevance signals in CTR prediction...|$|R
40|$|In this paper, {{the task}} is to {{determine}} whether an HTTP session buys an item, or not, and if so, which items will be purchased. An HTTP session {{is a series of}} item <b>clicks.</b> A <b>session</b> has type buy, if it buys at least one item, or non- buy otherwise. Accordingly, data is in (session, item, time) format, which tells us when an item is clicked or purchased during an HTTP session. The main challenge {{comes from the fact that}} (1) user information is not available for clicked or purchased items, which are merely tagged with anony- mous sessions, and (2) suggestions are highly temporal as they are suggested to sessions instead of users. In other words, users which are stable and identi ed are replaced with sessions which are temporal and anonymous. In this work, we propose a feature-based system that predicts the type of a session, and determines which items are going to be purchased. As the main contribution, we have modeled ses- sions separated by the number of unique items, prioritized item-features based on the number of clicks, and utilized cu- mulative statistics of similar items to attenuate the sparsity problem...|$|R
40|$|Search engines {{became a}} de facto place to start {{information}} acquisition on the Web. Though due to web spam phenomenon, search results are not always as good as desired. Moreover, spam evolves that makes the problem of providing high quality search even more challenging. Over the last decade research on adversarial information retrieval has gained {{a lot of interest}} both from academia and industry. In this paper we present a systematic review of web spam detection techniques with the focus on algorithms and underlying principles. We categorize all existing algorithms into three categories based on the type of information they use: content-based methods, link-based methods, and methods based on non-traditional data such as user behaviour, <b>clicks,</b> HTTP <b>sessions.</b> In turn, we perform a subcategorization of link-based category into five groups based on ideas and principles used: labels propagation, link pruning and reweighting, labels refinement, graph regularization, and featurebased. We also define the concept of web spam numerically and provide a brief survey on various spam forms. Finally, we summarize the observations and underlying principles applied for web spam detection. Keywords web spam detection, content spam, link spam, cloaking...|$|R
40|$|What is the {{difference}} between a digital story and vodcast? Why are libraries checking out camcorders and audio recorders? Why is this student asking me for help with iMovie? In this lecture, I will explain the concept and importance of media literacy, approaches to media literacy education, and argue that it is absolutely essential that public, k- 12, and academic libraries provide intentional media literacy programming. To support this case, I will describe Minnesota 2 ̆ 7 s media literacy support model, with example student media projects that illustrate the pedagogical benefits and soft skill set development of these emerging modes of student scholarship. To access the full presentation for this <b>session,</b> <b>click</b> on the 2 ̆ 7 Download 2 ̆ 7 button to the right...|$|R
40|$|Internet {{radio and}} TV [...] tuning into {{information}} and feature programs broadcast via the Internet and receivable {{on a personal}} computer [...] piqued interest among educators, librarians, and instructional technologists in the 1990 s. Then, connectivity and bandwidth issues affected widespread use. However, interest in Internet broadcasting and podcasting has seen a resurgence {{in the last few}} years. Internet radio and TV {{is more than just a}} new toy [...] there's real content online, applicable to the curriculum. Language instruction, music, politics, religion, history, culture, business, science, and more are just a few <b>clicks</b> away. This <b>session</b> will provide background on international Internet radio and TV broadcasting; sources for programs; curricular materials available online; and ideas for application to library and educational services...|$|R
40|$|Librarians {{spend so}} much of their time {{assisting}} others in the research process that they may suffer from information overload {{when it comes to their}} own research. This session is focused on showing librarians tools that will help them become more effective and efficient in their own research. This hands-on session will not only show you where to find useful web tools, but will also give you a chance to try them out and decide which ones are right for you. We will cover tools for research management, collaboration, organization, and more. Come and learn about free web tools that will help you be more productive and organized!To access the full presentation for this <b>session,</b> <b>click</b> on the 2 ̆ 2 Download 2 ̆ 2 button located to the right...|$|R
40|$|Google Analytics is a {{powerful}} tool for libraries. At the University of Central Missouri’s James C. Kirkpatrick Library, Google Analytics is being used to track how patrons use (and don’t use) the library’s website. This has provided a wealth of information – and some surprising findings - that can be used by librarians to improve the overall design of the website, identify geographic areas of users, improve marketing, and make needed adjustments to content so that items can be more readily found by patrons. This poster presentation will examine the data collected at JCKL using Google Analytics and discuss how you can get started using Google Analytics to learn about your patrons. To access the full presentation for this <b>session,</b> <b>click</b> on the 2 ̆ 2 Download 2 ̆ 2 button located to the right...|$|R
40|$|Have {{you ever}} {{worked on a}} big project like writing a manual and wished there was a cheap and easy way {{to keep track of}} who is doing what? There are many web-based tools {{available}} to keep track of both large and small projects for individuals and groups. Jonya Pacey and Rachel Gray, both of SELCO, have taken {{a look at some of}} the project management applications out there so see how they can help track the various tasks associated with projects and programs. Evaluation points include cost, document management, collaboration, item tracking, dependencies, ease of use, and most important, how fun the application is to use. To access the handout for this <b>session,</b> <b>click</b> on the 2 ̆ 7 Download 2 ̆ 7 button located to the right...|$|R
40|$|We will be {{presenting}} {{on various}} apps {{that are available}} for personal and professional productivity on an iPad. Although our {{focus will be on}} use in higher education, this session will be informative for any library worker interested in knowing more about the iPad. For example, we know iPads are being distributed to children in elementary schools throughout Minnesota. We 2 ̆ 7 ll be covering applications that include: Readers - pdf, RSS, and e-books Document/Image editing Video production Whiteboard displays (AirSketch) Recording/Note Taking (Notability) and lots more to explore! We will mainly be presenting various options, without hands on. We will provide a handout with a summary of the apps we plan to cover. And we will have fun!To access the full presentation for this <b>session,</b> <b>click</b> on the 2 ̆ 2 Download 2 ̆ 2 button to the right...|$|R
40|$|In {{a recent}} {{implementation}} of Overdrive, SELCO libraries discovered widely variant circulation, even from libraries that were {{roughly the same}} size. Why were some libraries’ eBook users diving right in while patrons of libraries barely dipped their toes in? Why were some libraries reporting significant barriers to patrons accessing titles while some seemed to flow right along with the service? What can library staff do {{to take advantage of}} the trend and encourage library users to explore library collections? As a the regional coordinators of the project, SELCO staff had a unique position to observe how libraries could make the eBook boom work for their patrons but also the barriers libraries faced with such increased patron demand. To access the full presentation for this <b>session,</b> <b>click</b> on the 2 ̆ 2 Download 2 ̆ 2 button to the right...|$|R
40|$|More {{and more}} users 2 ̆ 7 first {{encounter}} with the library happens online. Are you putting your best foot forward? Can everyone find what they need when they need it? Unless your site is perfect (and none are) the answers are probably no. The answer might be usability testing. If you, like SELCO are {{put off by the}} idea of expensive contracts with outside vendors or the difficulty of reaching quantifiable results, you might be interested in the way SELCO has decided to implement its usability tests. Our usability tests will be conducted all with equipment we currently own. Join us for a discussion of how we arrived at this method and how you, too, can implement some basic usability testing for your users as well. To access the full presentation for this <b>session,</b> <b>click</b> on the 2 ̆ 2 Download 2 ̆ 2 button to the right...|$|R
40|$|E-commerce {{has been}} growing rapidly. Its domain can provide all the right {{ingredients}} for successful data mining {{and it is a}} significant domain of data mining. E commerce refers to buying and selling of products or services over electronic systems such as internet. Various e commerce systems give discount on product and allow user to buy product online. The basic idea used here is to predict the product sale based on discount applied to the product. Our analysis concentrates on how customer behaves when discount is allotted to him. We have developed a model which finds the customer behaviour when discount is applied to the product. This paper elaborates upon how a different technique like <b>session,</b> <b>click</b> stream is used to collect user data online based on discount applied to the product and how statistics is applied to data set to see the variation in the data...|$|R
40|$|Understanding {{influence}} in behavioral data {{has become increasingly}} important in analyzing the cause and effect of human behaviors under various scenarios. Influence modeling enables us to learn not only how human behaviors drive the diffusion of memes spread in different kinds of networks, but also the chain reactions evolve in the sequential behaviors of people. In this thesis, I propose to investigate into appropriate probabilistic models for efficiently and effectively modeling influence, and the applications and extensions of the proposed models to analyze behavioral data in computational sustainability and information search. One fundamental problem in influence modeling is the learning {{of the degree of}} influence between individuals, which we called social infectivity. In {{the first part of this}} work, we study how to efficient and effective learn social infectivity in diffusion phenomenon in social networks and other applications. We replace the pairwise infectivity in the multidimensional Hawkes processes with linear combinations of those time-varying features, and optimize the associated coefficients with lasso regularization on coefficients. In the second part of this work, we investigate the modeling of influence between marked events in the application of energy consumption, which tracks the diffusion of mixed daily routines of household members. Specifically, we leverage temporal and energy consumption information recorded by smart meters in households for influence modeling, through a novel probabilistic model that combines marked point processes with topic models. The learned influence is supposed to reveal the sequential appliance usage pattern of household members, and thereby helps address the problem of energy disaggregation. In the third part of this work, we investigate a complex influence modeling scenario which requires simultaneous learning of both infectivity and influence existence. Specifically, we study the modeling of {{influence in}} search behaviors, where the influence tracks the diffusion of mixed search intents of search engine users in information search. We leverage temporal and textual information in query logs for influence modeling, through a novel probabilistic model that combines point processes with topic models. The learned influence is supposed to link queries that serve for the same formation need, and thereby helps address the problem of search task identification. The modeling of influence with the Markov property also help us to understand the chain reaction in the interaction of search engine users with query auto-completion (QAC) engine within each query session. The fourth part of this work studies how a user's present interaction with a QAC engine influences his/her interaction in the next step. We propose a novel probabilistic model based on Markov processes, which leverage such influence in the prediction of users' click choices of suggested queries of QAC engines, and accordingly improve the suggestions to better satisfy users' search intents. In the fifth part of this work, we study the mutual influence between users' behaviors on query auto-completion (QAC) logs and normal click logs across different query sessions. We propose a probabilistic model to explore the correlation between user' behavior patterns on QAC and click logs, and expect to capture the mutual influence between users' behaviors in QAC and <b>click</b> <b>sessions.</b> Ph. D...|$|R
40|$|Libraries {{often take}} {{challenges}} {{as they are}} presented and make them a success. Such was the case at the University of Minnesota-Twin Cities in August 2011. The challenge was to quickly develop a 20 -minute session for incoming First-Year students. The session had to compete for attendance, based on title, be delivered in a classroom (not the Libraries), include audio, video and engage students. Oh, and {{it needed to be}} repeated eleven times in one day. The solution involved the Cephalonian Method with humorous media from across popular culture and Libraryland. We focused on unique elements from our collections to capture students imagination. When it was all done, we had engaged over 780 students that students were recommending to their peers. In this session, learn about developing a compelling multimedia presentation to use on your campus for Orientation and more. To access full presentation for this <b>session,</b> <b>click</b> on the 2 ̆ 2 Download 2 ̆ 2 button to the right. Additional resources also available below...|$|R
40|$|Have {{you heard}} a lot about {{usability}} testing but never tried it? Does your institution use Libguides, and have you been wondering how usability testing might inform design decisions? Participants in this workshop session will learn why usability testing is important and how to do it with minimal resources. Participants will conduct their own usability test during the workshop, taking on the roles of user participant, observer, and test facilitator. As a case study, librarians from Metropolitan State University will look specifically at LibGuides, a system widely adopted by libraries. In particular, they will review the existing literature of usability testing and LibGuides and demonstrate how usability testing has informed their own LibGuides implementation. Attendees will then run a usability test on Metropolitan State University 2 ̆ 7 s LibGuides pages, and discuss how they might conduct usability testing in their own libraries. To access the full presentation for this <b>session,</b> <b>click</b> on the 2 ̆ 2 Download 2 ̆ 2 button on the right...|$|R
40|$|We {{examine the}} utility of {{relational}} probabilistic methods for modeling user behavior at web sites. Web logs (aka "click streams"), taken as datasets for traditional machine learning algorithms, violate the iid assumption of most algorithms. Requests ("clicks") are not independent within a session, sessions for a visitor are not independent of one another, and page types, in their interaction with behavioral profile, are highly correlated, both by static link structure and dynamic navigation sequence. We compare a series of increasingly sophisticated models, ranging from a simple, non-relational Bayesian network model of an individual click through traditional HMMs to fully relational extensions of HMMs, including visitors, <b>sessions,</b> <b>clicks,</b> and pages as participating entities. We measure {{the performance of the}} series of models on the task of predicting whether or not the current request is the last in a session. Results show a smooth and significant increase in performance, as measured by ROC AUC (area under the curve) ...|$|R
40|$|We {{propose the}} {{usefulness}} of probabilistic relational methods for modeling user behavior at web sites. Web logs (aka &quot;click streams&quot;), server logs, and other data sources, taken as datasets for traditional machine learning algorithms, violate the iid assumption of most algorithms. Requests (&quot;clicks&quot;) are not independent within a session, sessions for a visitor are not independent of one another, and page types, in their interaction with behavioral profile, are highly correlated, both by static link structure and dynamic navigation sequence. We introduce probabilistic relational modeling, and compare a series of increasingly sophisticated models, ranging from a simple, non-relational Bayesian network model of a click through traditional Hidden Markov Models (HMMs) to a new representation, a fully relational extensions of HMMs, that includes visitors, <b>sessions,</b> <b>clicks,</b> and pages as participating entities. We measure {{the performance of the}} series of models on the task of predicting whether or not the current request is the last in a session. Results show a significant increase in performance, as measured by ROC AUC (area under the curve) ...|$|R
