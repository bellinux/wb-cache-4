31|6|Public
40|$|Spatial {{release from}} masking is {{traditionally}} measured with speech in front. The effect of head-orientation {{with respect to}} the speech direction has rarely been studied. Speech-reception thresholds (SRTs) were measured for eight head orientations and four spatial configurations. Benefits of head orientation away from the speech source of up to 8 [*]dB were measured. These correlated with predictions of a model based on better-ear listening and binaural unmasking (r[*]=[*] 0. 96). Use of spontaneous head orientations was measured when listeners attended to long speech clips of gradually diminishing <b>speech-to-noise</b> <b>ratio</b> in a sound-deadened room. Speech was presented from the loudspeaker that initially faced the listener and noise from one of four other locations. In an undirected paradigm, listeners spontaneously turned their heads away from the speech in 56 % of trials. When instructed to rotate their heads in the diminishing <b>speech-to-noise</b> <b>ratio,</b> all listeners turned away from the speech and reached head orientations associated with lower SRTs. Head orientation may prove valuable for hearing-impaired listeners...|$|E
40|$|The {{acceptable}} range of speech level {{as a function}} of background noise level was investigated on the basis of word intelligibility scores and listening difficulty ratings. In the present study, the {{acceptable range}} is defined as the range that maximizes word intelligibility scores and simultaneously does not cause a significant increase in listening difficulty ratings from the minimum ratings. Listening tests with young adult and elderly listeners demonstrated the following. (1) The acceptable range of speech level for elderly listeners overlapped that for young listeners. (2) The lower limit of the acceptable speech level for both young and elderly listeners was 65 dB (A-weighted) for noise levels of 40 and 45 dB (A-weighted), a level with a <b>speech-to-noise</b> <b>ratio</b> of + 15 dB for noise levels of 50 and 55 dB, and a level with a <b>speech-to-noise</b> <b>ratio</b> of + 10 dB for noise levels from 60 to 70 dB. (3) The upper limit of the acceptable speech level for both young and elderly listeners was 80 dB for noise levels from 40 to 55 dB and 85 dB or above for noise levels from 55 to 70 dB...|$|E
40|$|Deposited with {{permission}} of the author. © 2003 Dr. Justin Andrew ZakisDigital hearing aids have {{an increasing number of}} adjustable amplification parameters, which offer the potential of greater customisation of their amplification settings to the needs and preferences of individual aid users. However, an increased number of adjustable amplification parameters, which may have counteracting effects, can increase the complexity of the fitting procedure, and the difficulty in achieving the combination of amplification parameter values that are optimal for the individual aid user. The latter is {{compounded by the fact that}} hearing aids are adjusted with a limited range of acoustic stimuli in a clinic environment, away from the real-life acoustic environments where the amplification provided by hearing aids may be less satisfactory. The customisation of hearing-aid amplification parameters for the individual aid user could be more efficiently and effectively performed with a trainable hearing aid. Such a hearing aid 'learns' the aid user's amplification preferences in everyday acoustic environments, and after sufficient training, automatically varies its amplification settings to the preference of the aid user for the current acoustic environment. However, to the knowledge of this author, the efficacy of trainable hearing aids has not been published in scientific journals. This thesis describes the design, development, and evaluation of a novel trainable hearing aid. In order to evaluate the novel trainable hearing aid, two laboratory experiments and three field trials were conducted to test three main hypotheses: and one sub-hypothesis. For one experiment, the amplification preferences of a hypothetical aid user were communicated to the developed aid under controlled conditions, and used by the aid to calculate a set of trained amplification parameters. The trained amplification parameters resulted in the application of amplification settings that were closer to the hypothetical preferred settings than were the original untrained settings. The results of this experiment supported the first hypothesis. For the other laboratory experiment, the <b>speech-to-noise</b> <b>ratio</b> estimation technique used by the aid was evaluated under controlled conditions (the developed aid periodically estimated the <b>speech-to-noise</b> <b>ratio</b> from a statistical analysis of the microphone signal, and could vary its gain with the <b>speech-to-noise</b> <b>ratio</b> estimates as determined by a trainable amplification parameter). It was found that when speech was present, the <b>speech-to-noise</b> <b>ratio</b> estimation technique used by the novel aid provided estimates that were linearly correlated with the reference estimates over a range of (mainly positive) speech-to- noise ratios, which partly supported sub-Hypothesis 2 a. A training trial was conducted, where the developed aid was fitted to 18 hearing-impaired subjects, who trained the aid in the acoustic environments they encountered in everyday life. The data that was logged in the aids during this trial indicated that, for some compression channels, the relationship between the amplification preferences of the subjects and the <b>speech-to-noise</b> <b>ratio</b> relative to the average for all channels (as estimated by the aid) was significant, which partly supported the second hypothesis. Most subjects who had completed the training trial participated in the comparison trials, where a modified version of the developed aid allowed the subjects to blindly compare their trained and untrained settings in everyday acoustic environments, and log their preferred settings in the aid's non-volatile memory. The results of the comparison trials showed that, in everyday environments, most subjects preferred the trained settings to the untrained settings on a statistically-significant majority of occasions, which supported the third hypothesis. Furthermore, it was found that the proportion of occasions when the trained settings were preferred was positively correlated with the amount of time the subjects used the aid during the training trial. Therefore, the main findings of this research project were that under controlled acoustic conditions, a hearing aid can be trained to provide amplification settings that are closer to hypothetical preferred settings than were the initial untrained settings, and in everyday acoustic environments, hearing aid users can train an aid to provide amplification settings that they prefer to the untrained settings on a significant majority of occasions. Open Acces...|$|E
50|$|Measurement {{of audio}} <b>speech-to-noise</b> <b>ratios</b> {{in a way}} which {{reflects}} interfering effects in a meaningful manner is difficult for the kinds of noise often encountered at mobile system receivers. For this reason {{and as a matter of}} convenience, a subjective rating of the interfering effect of the noise using the term “circuit merit” is commonlyused in place of metered measurements. This method uses a scale of five steps to describe performance. These are listed and defined in the above table. The <b>speech-to-noise</b> <b>ratios</b> in dB included in this table are arbitrary numbers which apply if both speech and noise are measured on either a 2B noise measuring set with F1A line weighting or a 3A noise measuring set with C-message weighting. In making such measurements, noise is measured in the normal manner and speech volume is read by the method used with a VU meter.|$|R
40|$|Empirical {{studies of}} general {{aviation}} (GA) pilot performance are lacking, especially {{with respect to}} envisioned future requirements. Two research studies were conducted to evaluate human performance using new technologies. In the first study, ten participants completed the Modified Rhyme Test (MRT) {{in an effort to}} compare the intelligibility of two text-to-speech (TTS) engines (DECtalk and AT&T’s Natural Voices) as presented in 85 dB(A) aircraft cockpit engine noise. Results indicated significant differences in intelligibility (p ≤ 0. 05) between the two speech synthesizers across the tested <b>speech-to-noise</b> <b>ratios</b> (S/N) (i. e., – 5 dB,- 8 dB, and – 11 dB S/N) with the AT&T engine resulting in superior intelligibility in all of the S/N. The AT&T product was therefore selected as the TTS engine for the second study. In the second study, 16 visual flight rules (VFR) rated pilots were evaluated for their data link performance using a flight simulator (ELITE i-GATE) equipped with a mixed-modality simulated data link within one of two flight conditions. Data link modalities included textual, synthesized speech, digitized speech, and synthesized speech/textual combination. Flight conditions included VFR (unlimited ceiling...|$|R
40|$|State-of-the-art {{binaural}} objective intelligibility measures (OIMs) require individual source signals {{for making}} intelligibility predictions, limiting their usability in real-time online operations. This limitation may {{be addressed by}} a blind source separation (BSS) process, which is able to extract the underlying sources from a mixture. In this study, a speech source is presented with either a stationary noise masker or a fluctuating noise masker whose azimuth varies in a horizontal plane, at two <b>speech-to-noise</b> <b>ratios</b> (SNRs). Three binaural OIMs are used to predict speech intelligibility from the signals separated by a BSS algorithm. The model predictions are compared with listeners' word identification rate in a perceptual listening experiment. The results suggest that with SNR compensation to the BSS-separated speech signal, the OIMs can maintain their predictive power for individual maskers compared to their performance measured from the direct signals. It also reveals that the errors in SNR between the estimated signals {{are not the only}} factors that decrease the predictive accuracy of the OIMs with the separated signals. Artefacts or distortions on the estimated signals caused by the BSS algorithm may also be concerns...|$|R
40|$|Introduction: Background noise {{affects the}} {{listening}} environment inside classrooms, especially for younger children. High background noise level adversely affects not only student speech perception but also teacher vocal hygiene. The current study aimed {{to give an}} overview of the classroom listening conditions in selected government primary schools in India. Materials and Methods: Noise measurements were taken in 23 classrooms of four government primary schools in southern India, using a type 2 sound level meter. In each classroom measurements were taken in occupied and unoccupied conditions. Teacher voice level was measured in the same classrooms. In addition, the classroom acoustical conditions were observed and the reverberation time for each classroom was calculated. Results: The mean occupied noise level was 62. 1 dBA and 65. 6 dBC, and the mean unoccupied level was 62. 2 dBA and 65 dBC. The mean unamplified teacher <b>speech-to-noise</b> <b>ratio</b> was 10. 6 dBA. Both the occupied and unoccupied noise levels exceeded national and international recommended levels and the teacher <b>speech-to-noise</b> <b>ratio</b> was also found to be inadequate in most classrooms. The estimated reverberation time in all classrooms was greater than 2. 6 seconds, which is almost double the duration of accepted standards. In addition, observation of classrooms revealed insufficient acoustical treatment to effectively reduce internal and external noise and minimize reverberation. Conclusion: The results of this study point out the need to improve the listening environment for children in government primary schools in India...|$|E
30|$|For all the stimuli, the {{acoustic}} output was paired with two different white noise signals {{where the average}} values of the <b>speech-to-noise</b> <b>ratio</b> (SNR) were either 6 or 10 dB. The noise {{was added to the}} stimuli to make it difficult, to some extent, to recognize the words based on audio only. This was done with the intention to push participants to focus on the face, not only on the audio channel. The choice of these two SNR values was done after performing several testing experiments. In fact, our purpose was not to degrade the audio too much because we intended to evaluate also audio besides video. We had to find a compromise to be able to evaluate both channels.|$|E
40|$|The {{influence}} of room acoustics and driving noise in vehicle interiors on the subjectively perceived sound quality of conversations between passengers was studied. A laboratory test with 25 participants {{was performed to}} assess the impact of interior transfer functions and driving noise at different velocities. The interference of other factors like varying speech spectra and temporal fluctuations of the driving noise was minimized as far as possible. A very strong relationship of <b>speech-to-noise</b> <b>ratio</b> and the subjective speech quality was found. Consequentially, the speech signal level, due to the {{influence of}} different interior room acoustics, seems to play a more important role than the varied speech timbre or other parameters. Further interrelations of acoustical and psychoacoustical metrics and parameters were investigated as well...|$|E
40|$|Perceptual {{load and}} {{cognitive}} load can be separately manipulated and dissociated in {{their effects on}} speech understanding in noise. The Ease of Language Understanding model assumes a theoretical position where perceptual task characteristics interact with the individual′s implicit capacities to extract the phonological elements of speech. Phonological precision and speed of lexical access are important determinants for listening in adverse conditions. If there are mismatches between the phonological elements perceived and phonological representations in long-term memory, explicit working memory (WM) -related capacities will be continually invoked to reconstruct and infer {{the contents of the}} ongoing discourse. Whether this induces a high cognitive load or not will in turn depend on the individual′s storage and processing capacities in WM. Data suggest that modulated noise maskers may serve as triggers for speech maskers and therefore induce a WM, explicit mode of processing. Individuals with high WM capacity benefit more than low WM-capacity individuals from fast amplitude compression at low or negative input <b>speech-to-noise</b> <b>ratios.</b> The general conclusion is that there is an overarching interaction between the focal purpose of processing in the primary listening task {{and the extent to which}} a secondary, distracting task taps into these processes...|$|R
40|$|Response {{times to}} known message-sets of 2 - 32 monosyllabic words were {{obtained}} over {{a range of}} <b>speech-to-noise</b> (S/N) <b>ratios.</b> Response time was related primarily to word intelligibility, independently whether the intelligibility was changed by varying S/N ratio, size of message-set, or composition of message-set Response times to known defined sets of words have been examined in noise-free environments by several investigators (Davis, Moray and Treisman, 1 % 1; Dichter, 1960; Saslow, 1958). The general conclusion from these studies is that with the task one of merely reproducing words in a noise-free environment, response time is nearly independent of vocabulary size for known small message-sets. It may be noted that, in these previous studies, the words were presented under conditions in which intelligibility was nearly perfect. The results of other experiments suggest that under noisy conditions resulting in lower degrees of intelligibility, response time may indeed be more strongly affected by size of message-set. First, several investigators have shown that reaction time will increase when the confusability among stimuli is increased (Crossman, 1955; Edwards...|$|R
40|$|Objectives: In two {{experiments}} with different subject groups, we explored {{the relationship between}} semantic context and intelligibility by examining the influence of visually presented, semantically related, and unrelated three-word text cues on perception of spoken sentences in stationary noise {{across a range of}} <b>speech-to-noise</b> <b>ratios</b> (SNRs). In addition, in Experiment (Exp) 2, we explored the relationship between individual differences in cognitive factors and the effect of the cues on speech intelligibility. less thanbrgreater than less thanbrgreater thanDesign: In Exp 1, cues had been generated by participants themselves in a previous test session (own) or by someone else (alien). These cues were either appropriate for that sentence (match) or for a different sentence (mismatch). A condition with nonword cues, generated by the experimenter, served as a control. Experimental sentences were presented at three SNRs (dB SNR) corresponding to the entirely correct repetition of 29 %, 50 %, or 71 % of sentences (speech reception thresholds; SRTs). In Exp 2, semantically matching or mismatching cues and nonword cues were presented before sentences at SNRs corresponding to SRTs of 16 % and 29 %. The participants in Exp 2 also performed tests of verbal working memory capacity and the ability to read partially masked text. less thanbrgreater than less thanbrgreater thanResults: In Exp 1, matching cues improved perception relative to the nonword and mismatching cues, with largest benefits at the SNR corresponding to 29 % performance in the SRT task. Mismatching cues did not impair speech perception relative to the nonword cue condition, and no difference in the effect of own and alien matching cues was observed. In Exp 2, matching cues improved speech perception as measured using both the percentage of correctly reported words and the percentage of entirely correctly reported sentences. Mismatching cues reduced the percentage of repeated words (but not the sentence-based scores) compared with the nonword cue condition. Working memory capacity and ability to read partly masked sentences were positively associated with the number of sentences repeated entirely correctly in the mismatch condition at the 29 % SNR. less thanbrgreater than less thanbrgreater thanConclusions: In difficult listening conditions, both relevant and irrelevant semantic context can influence speech perception in noise. High working memory capacity and good linguistic skills are associated with a greater ability to inhibit irrelevant context when uncued sentence intelligibility is around 29 % correct. Funding Agencies|Swedish Research Council||</p...|$|R
40|$|We {{compared}} {{two different}} types of hearing-aid fitting procedures in a double-blind randomized clinical study. Hearing aid fittings based on a purely prescriptive procedure (the NAL-RP formula) were compared to a comparative fitting procedure based on optimizing speech intelligibility scores. Main outcome measures were improvement of speech intelligibility scores in quiet and in noise. Data were related to the real-ear insertion responses that were measured after fitting. For analysis purposes subgroups were composed according to degree of hearing loss, characterized by unaided speech intelligibility in quiet, previous experience with hearing aids, unilateral or bilateral fittings and type of hearing aid. We found equal improvement of speech intelligibility in quiet, while fitting according to the prescriptive formula resulted in a somewhat better performance as expressed by the <b>speech-to-noise</b> <b>ratio</b> in comparison to the comparative procedure. Both procedures resulted in comparable real-ear insertion response...|$|E
40|$|One of {{the most}} {{prevalent}} speech impairments in idiopathic Parkinson’s disease (PD) is hypophonia, a reduction in intensity, which typically decreases intelligibility. Speech amplification devices are a potential solution; however, despite {{the availability of a}} broad range of devices, no previous studies systematically compare their efficacy in PD. This study examined the effects of speech task (Sentence Intelligibility Test versus conversation), background noise (no noise versus 65 dB SPL multi-talker noise), and selected devices (ADDvox, BoomVox, ChatterVox, Oticon, SoniVox, Spokeman, and Voicette) for 11 PD and 10 control participants, using outcome measures of speech intensity, <b>speech-to-noise</b> <b>ratio,</b> intelligibility, sound quality, and speakers’ experience. There were significant differences between the outcome measures for different device types, but experience scores did not always predict effectiveness according to the device hierarchy for the outcome measures. Future {{research is needed to determine}} performance and preference measures that will predict long-term device acceptance in PD...|$|E
40|$|Background {{noise is}} known to {{adversely}} affect speech perception and speech recognition. High levels of background noise in school classrooms may affect student learning, especially for those pupils who are learning in a second language. The current study aimed to determine the noise level and teacher <b>speech-to-noise</b> <b>ratio</b> (SNR) in Hong Kong classrooms. Noise level was measured in 146 occupied classrooms in 37 schools, including kindergartens, primary schools, secondary schools and special schools, in Hong Kong. The mean noise levels in occupied kindergarten, primary school, secondary school and special school classrooms all exceeded recommended maximum noise levels, and noise reduction measures were seldom used in classrooms. The measured SNRs were not optimal and could have adverse implications for student learning and teachers’ vocal health. Schools in urban Asian environments are advised to consider noise reduction measures in classrooms to better comply with recommended maximum noise levels for classrooms. published_or_final_versio...|$|E
40|$|Conversations must be {{shielded}} {{from people in}} an adjacent room if they include confidential information. Word intelligibility tests were performed {{in a total of}} 185 sound fields {{to examine the relationship between}} sound insulation performance and the degree of conversation leakage. The parameters of the test sound fields were background noise level in the adjacent room and the level difference between the two rooms. The background noise level was varied from 30 to 50 dB (A-weighted). The level difference was parametrically varied in terms of eight frequency characteristics and 10 absolute values. The results showed that word intelligibility scores were strongly correlated with the A-weighted <b>speech-to-noise</b> <b>ratio</b> and SNR(uni 32). Equal-intelligibility contours, which can easily show the weighted level difference and A-weighted background noise level required to achieve a certain level of word intelligibility scores, were obtained from a multiple logistic regression analysis. 壁の遮音性能と音声了解度の関係を定量的に示した...|$|E
40|$|International {{standards}} define {{normal hearing}} threshold levels (HTLs) and many studies describe speech recognition in noise (SRN) for adults. Less {{has been published}} on these characteristics for children. This study aims to establish ranges of HTLs and SRN for otologically normal 7 -year-olds. Air conduction HTLs were measured in 189 7 -year-olds within an audiometric booth. Speech recognition was measured adaptively for BKB sentences in noise presented binaurally through headphones, determining the <b>speech-to-noise</b> <b>ratio</b> (SNR) required to score 71 % correct. The mean HTLs of otologically normal 7 -year-olds were significantly lower (better) than those published for young adults at 1 and 2 kHz. Speech recognition in noise was unrelated to HTLs and was higher (worse) for 7 -year-olds (SNR?=??? 4 dB) than has been found for young otologically normal adults (SNR?=??? 6 dB). It is concluded that although the HTLs of 7 -year-old children are generally better than those of young adults, their speech recognition in noise is worse. This confirms that their ability to recognise speech in noise is not fully developed at this age...|$|E
40|$|Background noise poses {{adverse effects}} on speech sounds and affects student learning, {{especially}} for children with developmental disabilities. Sound-field and public address amplification systems can help {{to solve this problem}} by amplifying speech sounds relative to background noise. This study surveyed school classrooms for children with special needs, and compared the performance of a sound-field and a portable public address system in classroom environments. Unoccupied room noise levels and reverberation times were measured in eight classrooms at four Hong Kong schools for children with special needs. Speech levels in each classroom were measured under three conditions: without amplification, with public address system amplification, and with sound-field amplification. Speech-to-noise ratios were calculated for each condition. Noise and unamplified <b>speech-to-noise</b> <b>ratio</b> values exceeded recommended acoustic standards in all classrooms. When sound-field and public address amplification systems were installed, speech-to-noise ratios improved considerably. When either amplification system was used, a uniform sound-field resulted. The applicability of both types of amplification system and their relative merits in special education classrooms are discussed. © 2006 Taylor & Francis. link_to_subscribed_fulltex...|$|E
40|$|The {{intelligibility}} {{of speech}} {{is known to}} be lower if the speaker is non-native instead of native for the given language. This study is aimed at quantifying the overall degradation due to limitations of non-native speakers of Dutch, specifically of Dutch-speaking Americans who have lived in the Netherlands 1 - 3 years. Experiments were focused on phoneme intelligibility and sentence intelligibility, using additive noise as a means of degrading the intelligibility of speech utterances for test purposes. The overall difference in sentence intelligibility between native Dutch speakers and American speakers of Dutch, using native Dutch listeners, was found to correspond to a difference in <b>speech-to-noise</b> <b>ratio</b> (SNR) of approximately 3 dB. The main segmental contribution to the degradation of speech intelligibility by introducing non-native speakers and/or listeners is the confusion of vowels, especially those that do not occur in American English. Vowels that are difficult for second-language speakers to produce are also difficult for second-language listeners to classify; such vowels attract false recognition, reducing the overall recognition rate for all vowels. © 2001 Elsevier Science B. V...|$|E
40|$|Purpose: The classic {{study of}} Sumby and Pollack (1954, JASA, 26 (2), 212 - 215) {{demonstrated}} that visual information aided speech intelligibility under noisy auditory conditions. Their work showed that visual information is especially useful under low signal-to-noise conditions where the auditory signal leaves greater margins for improvement. We investigated whether simulated cataracts {{interfered with the}} ability of participants to use visual cues to help disambiguate the auditory signal {{in the presence of}} auditory noise. Methods: Participants in the study were screened to ensure normal visual acuity (mean of 20 / 20) and normal hearing (auditory threshold ≤ 20 dB HL). Speech intelligibility was tested under an auditory only condition and two visual conditions: normal vision and simulated cataracts. The light scattering effects of cataracts were imitated using cataract-simulating filters. Participants wore blacked-out glasses in the auditory only condition and lens-free frames in the normal auditory-visual condition. Individual sentences were spoken by a live speaker in the presence of prerecorded four-person background babble set to a <b>speech-to-noise</b> <b>ratio</b> (SNR) of - 16 dB. The SNR was determined in a preliminary experiment to support 50...|$|E
40|$|Two {{experiments}} investigated how automobile noise affects intelligibility {{of speech}} signals in both young and middle-aged individuals. In Experiment 1, {{the effect of}} automobile noise was compared to speech babble {{at a number of}} speech-to-noise ratios. In order to achieve the same intelligibility, the <b>speech-to-noise</b> <b>ratio</b> for the speech babble needed to be substantially greater than the automobile noise. In Experiment 2, middle-aged adults between the ages of 50 and 65 were given the sentences in automobile noise. Even though their hearing acuity was not severe enough to warrant a clinical diagnosis, their performance was significantly worse than the younger adults, particularly for sentences that had few contextual cues. In conclusion, although automobile noise is less damaging than speech babble at typical speech-to-noise ratios, speech understanding for individuals with even small amounts of hearing loss is significantly impacted by the noise. Automobile makers therefore should continue their efforts to reduce the noise levels in cars in order to increase speech intelligibility. [Work supported by Ford Motor Company]. SOMMAIRE L'effet du bruit ambiant d'une voiture sur l'intelligibilité de la parole chez les jeunes adultes et les adultes d'âg...|$|E
40|$|Can we model speech {{recognition}} in noise by exploring higher order statistics {{of the combined}} signal? How will changes in these statistics affect speech perception in noise? This study addresses these questions in two experiments. One investigated the relationship between an established "glimpsing" model and the fourth order statistic, kurtosis. The glimpsing model [1] proposes that listeners can explore the local <b>speech-to-noise</b> <b>ratio</b> (SNR) in short time segments (glimpses) and focus on areas where SNR is high. Results showed {{that there is a}} very high correlation between percentages of glimpsing area and kurtosis (r = 0. 99;p < 0. 01), suggesting that kurtosis can serve as a simpler index for measuring glimpsing. The experiment also examined the association between kurtosis and recognition of nonsense words (vowel-consonant-vowel, VCV) in babble modulated noise, also showing very high correlation (r = 0. 97;p < 0. 01). Another separate study focused on the relationship of sparseness to {{speech recognition}} score for VCV words in natural babble noise made of 100 people talking simultaneously [2]. Results show that there is also high correlation between kurtosis and speech recognition score with this noise. Logistic regression analysis to obtain the kurtosis for 50 % correct showed this was achieved at a kurtosis of approximately 1. ...|$|E
40|$|The {{effects of}} {{audibility}} and age on masking for sentences in continuous and interrupted noise {{were examined in}} listeners with real and simulated hearing loss. The absolute thresholds of each of ten listeners with sensorineural hearing loss were simulated in normal-hearing listeners {{through a combination of}} spectrally-shaped threshold noise and multi-band expansion for octave bands with center frequencies from 0. 25 – 8 kHz. Each individual hearing loss was simulated in two groups of three normal-hearing listeners (an age-matched and a non-age-matched group). The <b>speech-to-noise</b> <b>ratio</b> (S∕N) for 50 %-correct identification of hearing in noise test (HINT) sentences was measured in backgrounds of continuous and temporally-modulated (10 Hz square-wave) noise at two overall levels for unprocessed speech and for speech that was amplified with the NAL-RP prescription. The S∕N in both continuous and interrupted noise of the hearing-impaired listeners was relatively well-simulated in both groups of normal-hearing listeners. Thus, release from masking (the difference in S∕N obtained in continuous versus interrupted noise) appears to be determined primarily by audibility. Minimal age effects were observed in this small sample. Observed values of masking release were compared to predictions derived from intelligibility curves generated using the extended speech intelligibility index (ESII) [Rhebergen et al. (2006). J. Acoust. Soc. Am. 120, 3988 – 3997]...|$|E
40|$|When {{listening}} to languages learned {{at a later}} age, speech intelligibility is generally lower than when {{listening to}} one's native language. The main {{purpose of this study}} is to quantify speech intelligibility in noise for specific populations of non-native listeners, only broadly addressing the underlying perceptual and linguistic processing. An easy method is sought to extend these quantitative findings to other listener populations. Dutch subjects listening to Germans and English speech, ranging from reasonable to excellent proficiency in these languages, were found to require a 1 - 7 dB better <b>speech-to-noise</b> <b>ratio</b> to obtain 50 % sentence intelligibility than native listeners. Also, the psychometric function for sentence recognition in noise was found to be shallower for non-native than for native listeners (worst-case slope around the 50 % point of 7. 5 %/dB, compared to 12. 6 %/dB for native listeners). Differences between native and non-native speech intelligibility are largely predicted by linguistic entropy estimates as derived from a letter guessing task. Less effective use of context effects (especially semantic redundancy) explains the reduced speech intelligibility for non-native listeners. While measuring speech intelligibility for many different populations of listeners (languages, linguistic experience) may be prohibitively time consuming, obtaining predictions of non-native intelligibility from linguistic entropy may help to extend the results of this study to other listener populations. © 2002 Acoustical Society of America...|$|E
40|$|Methodology is {{proposed}} for perceptual assessment of both subjective sound quality and speech recognition in such way that {{results can be}} compared between these two aspects. Validation is performed with a noise suppression system applied to hearing instruments. A method termed Interpolated Paired Comparison Rating (IPCR) was developed for time efficient assessment of subjective impression of different aspects of sound quality {{for a variety of}} noise conditions. The method is based on paired comparisons between processed and unprocessed stimuli, and the results are expressed as the difference in signal-to-noise ratio (dB) between these that give equal subjective impression. For tests of speech recognition in noise, validated adaptive test methods can be used that give results in terms of <b>speech-to-noise</b> <b>ratio.</b> The methodology was shown to be sensitive enough to detect significant mean differences between processed and unprocessed speech in noise, both regarding subjective sound quality and speech recognition ability in groups consisting of 30 subjects. An effect on sound quality from the noise suppression equivalent to about 3 – 4 dB is required to be statistically significant for a single subject. A corresponding effect of 3 – 6 dB is required for speech recognition (one-sided test). The magnitude of difference that occurred in the present study for sound quality was sufficient to show significant differences for sound quality within individuals, but {{this was not the case}} for speech recognition...|$|E
40|$|Just-noticeable {{differences}} (JNDs) {{have been}} measured for various features of sounds, but despite its importance to communication, {{there is no}} benchmark for what is a just-noticeable—and possibly meaningful—difference in <b>speech-to-noise</b> <b>ratio</b> (SNR). SNR {{plays a crucial role}} in speech communication for normal-hearing and hearing-impaired listeners. Difficulty hearing speech in background noise—a poor SNR—often leads to dissatisfaction with hearing-assistance devices. While such devices attempt through various strategies to address this problem, it is not currently known how much improvement in SNR is needed to provide a noticeable benefit. To investigate what is a noticeable benefit, we measured the JND in SNR for both normal-hearing and hearing-impaired listeners. Here, we report the SNR JNDs of 69 participants of varying hearing ability, estimated using either an adaptive or fixed-level procedure. The task was to judge which of the two intervals containing a sentence in speech-spectrum noise presented over headphones was clearer. The level of each interval was roved to reduce the influence of absolute level cues. The results of both procedures showed an average SNR JND of 3 [*]dB that was independent of hearing ability. Further experiments using a subset of normal-hearing listeners showed that level roving does elevate threshold. These results suggest that noise reduction schemes may need to achieve a benefit greater than 3 [*]dB to be reliably discriminable...|$|E
40|$|The {{impact of}} {{paintings}} hung on lecture room walls on the speech intelligibility and perception of background noise Mirko Čudina 1, Jurij Prezelj 1 and Milena Pušlar-Čudina 2 The most important parameter for rooms designed for speech (classroom or lecture room) is speech intelligibility. Speech intelligibility {{depends on the}} reverberation time, <b>speech-to-noise</b> <b>ratio</b> and geom-etry of the room, among others. At the Faculty of Mechanical Engineering, University of Ljubljana, the reverberation times in lecture rooms observed exceed the optimal value from 0. 31 to 0. 87 s and in some of them, speech intelligibility is below the threshold of good or even satisfactory. For reducing rever-beration time and improving speech intelligibility, an alternative method was proposed, by hanging art paintings on the room walls. Experiments have shown that by doing so, reverberation time can be reduced by increasing the absorption coefficient by more than 30 % in the most audible part of the frequency spectra (between 500 and 1500 Hz). The absorption coefficient can be increased by using different dimensions of paintings, different air spaces behind the canvas, by appropriate thickness of the paint layers and by adding absorption material behind the canvas. Subjective tests have also shown that paintings with proper colour combinations (pastel colours with prevailing green, blue and grey colours) are appropriate for soothing the undesirable effect of background sound (noise) by changing perception in the brain...|$|E
40|$|This study {{considers}} {{the influences of}} room acoustics and driving noises in vehicle interiors on the subjectively perceived acoustical quality of conversations between passengers. A listening test with 25 participants was performed inside a laboratory {{to assess the impact}} of different vehicle interior transfer functions on the speech quality assessment in four predetermined dimensions. Idealized driving noises at three different vehicle speeds were presented simultaneously with speech samples to quantify the interferences of these noise conditions with varied signal-to-noise ratios. To minimize the influence of different human speakers, four talkers (two male and two female) were selected from commercially available audio books. The respective speech samples were adjusted in level and long-term average speech spectrum to the common values of conversational speech. The automatic reflex of raising one’s voice in noisy environments, called “Lombard Effect” [1], was taken into account for an additional adjustment of speech levels while driving noises were present. A strong relationship between the <b>speech-to-noise</b> <b>ratio</b> and the test participants’ evaluations was found. Thus, one can assume that the speech signals’ attenuation or amplification caused by the different room acoustics of the tested vehicles play a more important role for a sufficient speech quality than the varied speech timbre or other parameters. Only at very high speech-to-noise ratios (≥ 20 dB with A-weighting), room-acoustical parameters such as IACC or the reverberation time are more determining for the speech quality appreciation than the speech’s sound pressure level...|$|E
40|$|The <b>speech-to-noise</b> <b>ratio</b> (SNR) in an {{environment}} plays {{a vital role in}} speech communication for both normal-hearing (NH) and hearing-impaired (HI) listeners. While hearing-assistance devices attempt to deliver as favorable an SNR as possible, there may be discrepancies between noticeable and meaningful improvements in SNR. Furthermore, {{it is not clear how}} much of an SNR improvement is necessary to induce intervention-seeking behavior. Here we report on a series of experiments examining the just-meaningful difference (JMD) in SNR. All experiments used sentences in same-spectrum noise, with two intervals on each trial mimicking examples of pre- and post-benefit situations. Different groups of NH and HI adults were asked (a) to rate how much better or worse the change in SNR was in a number of paired examples, (b) if they would swap the worse for the better SNR (e. g., their current device for another) or (c) if they would be willing to go to the clinic for the given increase in SNR. The mean SNR JMD based on better/worse ratings (one arbitrary unit) was similar to the just-noticeable difference, approximately 3 dB. However, the mean SNR JMD for the more clinically relevant tasks [...] willingness (at least 50 % of the time) to swap devices or attend the clinic for a change in SNR [...] was 6 - 8 dB regardless of hearing ability. This SNR JMD of the order of 6 dB provides a new benchmark, indicating the SNR improvement necessary to immediately motivate participants to seek intervention...|$|E
40|$|Speech {{recognition}} {{performance and}} self-reported benefit from linear analogue and advanced (digital) hearing aidswere compared in 100 first-time hearing aid users withmild-to-moderate {{sensorineural hearing loss}} fitted monaurally with a behind-the-ear (BTE) hearing aid in a single-blind randomized crossover trial. Subjects usedeach aid for 5 weeks in turn, with aid order balancedacross subjects. Three alternative models of digital hearing aid were assigned to subjects according to a balanceddesign. Aid type was disguised to keep subjects blind within practical limitations. Aided speech recognition performance in noise was measured at speech levels of 65 and 75 dB at a <b>speech-to-noise</b> <b>ratio</b> (SNR) of _ 2 dB forclosed sets of single words. Self-rated benefit was measured using the Abbreviated Profile of Hearing Aid Benefit (APHAB) and the Glasgow Hearing Aid Benefit Profile (GHABP). Quality of life, hearing aid use and user preferences were also assessed. Speech recognition scores with the digital aids were significantly better at 75 dB than with the analogue aids. Self-reported benefit (APHAB, GHABP) and improvement in quality of life were generally not significantly different between analogue and digital aids, although aversiveness measured with the APHAB was significantly lower with digital aids,and satisfaction measured with the GHABP was greater. The digital aids were preferred significantly more often than the analogue aids, with 61 subjects choosing their digital aid, 26 choosing the analogue aid, and nine being equivocal. Overall, this study shows advantages for advanced digital over simple linear analogue aids interms of both objective and subjective outcomes, although average differences are not large...|$|E
40|$|Speech {{recognition}} {{was measured}} {{as a function}} of spectral resolution (number of spectral channels) and <b>speech-to-noise</b> <b>ratio</b> in normal-hearing (NH) and cochlear-implant (CI) listeners. Vowel, consonant, word, and sentence recognition were measured in five normal -hearing listeners, ten listeners with the Nucleus- 22 cochlear implant, and nine listeners with the Advanced Bionics Clarion cochlear implant. Recognition was measured {{as a function of}} the number of spectral channels (noise bands or electrodes) at signal-to-noise ratios of + 15, + 10, + 5, 0 dB, and in quiet. Performance with three different speech processing strategies (SPEAK, CIS, and SAS) was similar across all conditions, and improved as the number of electrodes increased (up to seven or eight) for all conditions. For all noise levels, vowel and consonant recognition with the SPEAK speech processor did not improve with more than seven electrodes, while for normal-hearing listeners, performance continued to increase up to at least 20 channels. Speech recognition on more difficult speech materials (word and sentence recognition) showed a marginally significant increase in Nucleus- 22 listeners from seven to ten electrodes. The average implant score on all processing strategies was poorer than scores of NH listeners with similar processing. However, the best Cl scores were similar to the normal-hearing scores for that condition (up to seven channels). CI listeners with the highest performance level increased in performance as the number of electrodes increased up to seven, while CI listeners with low levels of speech recognition did not increase in performance as the number of electrodes was increased beyond four. These results quantity the effect of number of spectral channels on speech recognition in noise and demonstrate that most Cl subjects are not able to fully utilize the spectral information provided by the number of electrodes used in their implant. (C) 2001 Acoustical Society, of America...|$|E
40|$|Objective: Hearing {{instruments}} with adaptive {{directional microphone}} systems attempt to maximize <b>speech-to-noise</b> <b>ratio</b> (SNR) and thereby improve speech recognition in noisy backgrounds. When instruments with adaptive systems are fitted bilaterally, {{there is the}} potential for adverse effects as they operate independently and may give confusing cues or disturbing effects. The present study compared speech recognition performance in 16 listeners fitted bilaterally with the Phonak Claro hearing instrument using omni-directional, fixed directional, and adaptive directional microphone settings as well as mixed microphone settings (an omni-directional microphone on one side and an adaptive directional microphone on the other). Design: Under anechoic conditions, speech was always presented from a loudspeaker {{directly in front of the}} listener (0 degree azimuth) whereas noise was presented from one or two loudspeakers arranged either symmetrically (0, 180, 90 + 270 degrees) or asymmetrically (170 + 240 degrees and 120 + 190 degrees) in the horizontal plane. Adaptive sentence recognition in noise measurement was supplemented by quality ratings. Results: With symmetrical omni-directional settings (Omni/Omni), performance was poorer than a control group of 14 listeners with normal hearing tested unaided: Aided listeners required 4. 3 dB more favorable SNR for criterion performance. In all loudspeaker arrangements in which directional characteristics could be exploited, performance with symmetrical adaptive microphones (Adapt/Adapt) was similar to the control group. The mixed microphone settings did not appear to confer any particular disadvantage for speech recognition from their asymmetric nature, always giving scores significantly better than Omni/Omni. Quality rating scores were consistent with speech recognition performance, showing benefits in terms of clarity and comfort for the Adapt/Adapt and Fixed/Fixed microphone conditions over the Omni/Omni and mixed microphone conditions wherever directional characteristics could be used. Similarly, the mixed microphone conditions were rated more comfortable and quieter for the noise than Omni/Omni. Conclusions: It is concluded that bilateral hearing instruments with adaptive directional microphones confer benefits in terms of speech recognition in noise and sound quality. Independence of the two adaptive control systems does not appear to cause untoward effects. <br/...|$|E
40|$|Age-related hearing {{impairment}} (ARHI) affects 25 - 40 % of individuals {{over the age}} of 65. Despite the high prevalence of this complex trait, ARHI is still poorly understood. We hypothesized that variance in hearing ability with age is largely determined by genetic factors. We collected audiologic data on females of Northern European ancestry and compared different audiogram representations. A web-based <b>speech-to-noise</b> <b>ratio</b> (SNR) hearing test was compared with pure-tone thresholds {{to see if we could}} determine accurately hearing ability on people at home and the genetic contribution to each trait compared. Volunteers were recruited from the TwinsUK cohort. Hearing ability was determined using pure-tone audiometry and a web-based hearing test. Different audiogram presentations were compared for age-correlation and reflection of audiogram shape. Using structural equation modelling based on the classical twin model the heritability of ARHI, as measured by the different phenotypes, was estimated and shared variance between the web-based SNR test and pure-tone audiometry determined using bivariate modelling. Pure-tone audiometric data was collected on 1033 older females (age: 41 - 86). 1970 volunteers (males and females, age: 18 - 85) participated in the SNR. In the comparison between different ARHI phenotypes the difference between the first two principle components (PC 1 -PC 2) best represented ARHI. The SNR test showed a sensitivity and specificity of 89 % and 80 %, respectively, in comparison with pure-tone audiogram data. Univariate heritability estimates ranged from 0. 70 (95 % CI: 0. 63 - 0. 76) for (PC 1 -PC 2) to 0. 56 (95 % CI: 0. 48 - 0. 63) for PC 2. The genetic correlation of PC 1 -PC 2 and SNR was - 0. 67 showing that the 2 traits share variances attributed to additive genetic factors. Hearing ability showed considerable heritability in our sample. We have shown that the SNR test provides a useful surrogate marker of hearing. This will enable a much larger sample to be collected at a fraction of the cost, facilitating future genetic association studies...|$|E
40|$|Headsets for speech {{communication}} {{are used}} {{in a wide range}} of applications. The basic idea is to allow hands-free speech communication, leaving both hands available for other tasks. One typical headset application is aircraft pilot communication. The pilot must be able to communicate with personnel on the ground and at the same time use both hands to control the aircraft. Communication headsets usually consist of a pair of headphones and a microphone attached with an adjustable boom. Headphone design varies widely between different manufacturers and models. In its simplest form, the headphone has an open construction providing little or no attenuation of the environmental noise. In headsets designed for noisy environments, the headphones are mounted in ear cups with cushions that provide some attenuation. The microphone is primarily designed to pick up the speech signal, but if the headset is used in a noisy environment, the background noise will also be picked up and transmitted together with the speech. As a consequence, speech intelligibility at the receive end will be reduced, possibly to zero. To increase the <b>speech-to-noise</b> <b>ratio,</b> it is common to use a directional microphone that has a lower sensitivity to sound incident from other directions than the frontal direction. In addition to this, the microphone electronics are usually equipped with a gate function that completely shuts off the microphone signal if its level drops below a threshold value. The purpose of the gate is to open the channel for transmission only when a speech signal is present. Headsets are frequently used in noisy environments where they suffer from problems of speech intelligibility. Even if an ear-cup type headset is used, the attenuation is relatively poor for low frequencies. Low frequency noise has a masking effect on speech, which significantly reduces the speech intelligibility. Several cases have been reported in which the sound level of the communication signal was increased to hazardous levels by the user to overcome this low frequency masking effect [1, 2]. Ear exposure to the communication system resulted in hearing damage, such as hearing loss, tinnitus and hyperacusis...|$|E
40|$|Understanding {{speech is}} crucial for human communication. Therefore, speech {{audiometry}} plays {{an important part in}} the assessment of the hearing impaired, and improved speech intelligibility is a major goal of hearing aid fitting. The aims of this study were to improve speech test procedures and to validate the speech Intelligibility Index (SII) as a means for predicting speech test results and evaluating hearing aid characteristics. The established Swedish Phonemically Balanced (SPB) word lists were mixed with speech-weighted noise, and normative performance data were obtained with normal-hearing subjects. To facilitate the assessment of individual speech recognition results, functions were derived that related the SII to measured speech recognition scores. A prediction procedure, in which the standard SII was complemented with corrections for supra-threshold deficits, was evaluated on a group of hearing-impaired persons. The modified SII was also evaluated as a means for comparing hearing aid characteristics. Predicted speech recognition based on the SII was compared with speech recognition scores, just-follow-conversation results and subjective judgments of speech intelligibility for different conditions of amplification and background noise. The <b>speech-to-noise</b> <b>ratio</b> + 4 dB was found to be appropriate for clinical use. A pre-mixed test material, called SPBN (Swedish Phonemically Balanced words in Noise), was recorded on CD. Speech recognition scores of hearing impaired persons were on average well predicted with the modified SII, though there were considerable differences among individuals, which confirmed the need for speech recognition tests. A computer program was developed to calculate predicted performance on the SPBN test. When the modified SII was applied for evaluating hearing aids, the calculated intelligibility showed an overall good agreement with measured speech recognition results and also with subjectively judged speech intelligibility. The within subject standard deviation of the predicted scores was estimated to be about 1 %. Conclusions: The SPBN test is suitable for every-day clinical determination of speech recognition scores. The SII based prediction procedure facilitates evaluation of individual speech recognition results. The modified SII can be used in hearing aid fitting procedures in order to evaluate frequency-gain characteristics of hearing aids in terms of expected speech intelligibility in background noise...|$|E
40|$|OBJECTIVES: The aim of {{this study}} was to {{evaluate}} the benefit that listeners obtain from visually presented output from an automatic speech recognition (ASR) system during listening to speech in noise. DESIGN: Auditory-alone and audiovisual speech reception thresholds (SRTs) were measured. The SRT is defined as the <b>speech-to-noise</b> <b>ratio</b> at which 50 % of the test sentences are reproduced correctly. In the auditory-alone SRT tests, the test sentences were presented only auditorily; in the audiovisual SRT test, the ASR output of each test sentence was also presented textually. The ASR system was used in two recognition modes: recognition of spoken words (word output), or recognition of speech sounds or phones (phone output). The benefit obtained from the ASR output was defined as the difference between the auditory-alone and the audiovisual SRT. We also examined the readability of unimodally displayed ASR output (i. e., the percentage of sentences in which ASR errors were identified and accurately corrected). In experiment 1, the readability and benefit obtained from ASR word output (n = 14) was compared with the benefit obtained from ASR phone output (n = 10). In experiment 2, the effect of presenting an indication of the ASR confidence level was examined (n = 14). The effect of delaying the presentation of the text relative to the speech (up to 6 sec) was examined in experiment 3 (n = 24). The ASR accuracy level was varied systematically in each experiment. RESULTS: Mean readability scores ranged from 0 to 46 %, depending on ASR accuracy. Speech comprehension improved when the ASR output was displayed. For example, when the ASR output corresponded to readability scores of only about 20 % correct, the text improved the SRT by about 3 dB SNR in the audiovisual SRT test. This improvement corresponds to an increase in speech comprehension of about 35 % in critical conditions. Equally readable phone and word output provides similar benefit in speech comprehension. For equal ASR accuracies, both the readability and the benefit from the word output generally exceeded the benefits from the phone output. Presenting information about the ASR confidence level did not influence either the readability or the benefit obtained from the word output. Delaying the text relative to the speech moderately decreased the benefit. CONCLUSIONS: The present study indicates that speech comprehension improves considerably by textual ASR output with moderate accuracies. The study shows that this improvement depends on the readability of the ASR output. Word output has better accuracy and readability than phone output. Listeners are therefore better able to use the ASR word output than phone output to improve speech comprehension. The ability of older listeners and listeners with hearing impairments to use ASR output in speech comprehension requires further study. © 2008 Lippincott Williams & Wilkins, Inc...|$|E
