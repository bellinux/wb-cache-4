96|14|Public
25|$|In August 2013 the LHCb team {{revealed}} an anomaly in the angular distribution of B meson decay products {{which could not}} be predicted by the Standard Model; this anomaly had a <b>statistical</b> <b>certainty</b> of 4.5 sigma, just short of the 5 sigma needed to be officially recognized as a discovery. It is unknown what the cause of this anomaly would be, although the Z' boson has been suggested as a possible candidate.|$|E
25|$|Reviews of {{research}} {{dating from the}} 1980s show there is some effect in reducing the number and duration of lesions if aciclovir is applied {{at an early stage}} of an outbreak. Research shows effectiveness of topical aciclovir in both the early and late stages of the outbreak as well as improving methodologically and in terms of <b>statistical</b> <b>certainty</b> from previous studies. Aciclovir trials show that this agent has no role in preventing HIV transmission, but it can help slow HIV disease progression in people not taking anti-retroviral therapy (ART). This finding emphasizes the importance of testing simple, inexpensive non-ART strategies, such as aciclovir and cotrimoxazole, in people with HIV.|$|E
2500|$|In The Selfish Gene, Richard Dawkins {{describes}} {{an approach to}} life-extension that involves [...] "fooling genes" [...] into thinking the body is young. Dawkins attributes inspiration for this idea to Peter Medawar. The basic idea is that our bodies are composed of genes that activate throughout our lifetimes, some when we are young and others when we are older. Presumably, these genes are activated by environmental factors, and the changes caused by these genes activating can be lethal. It is a <b>statistical</b> <b>certainty</b> that we possess more lethal genes that activate in later life than in early life. Therefore, to extend life, {{we should be able}} to prevent these genes from switching on, and {{we should be able to}} do so by [...] "identifying changes in the internal chemical environment of a body that take place during aging... and by simulating the superficial chemical properties of a young body".|$|E
5|$|The {{results of}} {{statistical}} analysis will be strongly affected by any errors {{in identifying the}} inventory of glyphs, {{as well as by}} divergence from a purely syllabic representation, such as a glyph for reduplication. There are also large differences in the frequencies of individual syllables among the Rapanui texts, which makes any direct identification problematic. While Pozdniakov {{has not been able to}} assign any phonetic values with any <b>certainty,</b> <b>statistical</b> results do place constraints on which values are possible.|$|R
40|$|This paper {{presents}} a watermarking scheme for copyright protection of digital images. A binary logo is the copyright label which {{is embedded in}} grayscale or color digital images. A set of integer parameters, selected by the legal owner, controls the watermarking algorithm via a strongly chaotic (mixing) system. Watermark detection is performed without resorting to the original image. The embedded binary logo is reconstructed or the <b>statistical</b> detection <b>certainty</b> is provided indicating the watermark existence. Numerical experiments testify the efficiency of a particular watermarking algorithm as a reliable verification tool for proving copyright ownership of the digital image. 1 Introduction Copyright protection of digital images, audio and video, is a novel and very interesting research topic. The technology of digital services grows rapidly and distributed access to such services through computer networks {{is a matter of}} urgency. However, network access does not protect the intellect [...] ...|$|R
2500|$|Peirce {{held that}} science {{achieves}} <b>statistical</b> probabilities, not <b>certainties,</b> and that spontaneity (absolute chance) is real (see Tychism on his view). Most of his statistical writings promote the frequency interpretation of probability (objective ratios of cases), {{and many of}} his writings express skepticism about (and criticize the use of) probability when such models are not based on objective randomization. Though Peirce was largely a frequentist, his possible world semantics introduced the [...] "propensity" [...] theory of probability before Karl Popper. Peirce (sometimes with Joseph Jastrow) investigated the probability judgments of experimental subjects, [...] "perhaps the very first" [...] elicitation and estimation of subjective probabilities in experimental psychology and (what came to be called) Bayesian statistics.|$|R
50|$|Statistical estimators will {{calculate}} {{an estimated}} {{value on the}} sample series used. The estimates may deviate from the true value {{and the range of}} values which for some probability will contain the true value {{is referred to as the}} confidence interval. The confidence interval depends on the number of observations in the sample series, the dominant noise type, and the estimator being used. The width is also dependent on the <b>statistical</b> <b>certainty</b> for which the confidence interval values forms a bounded range, thus the <b>statistical</b> <b>certainty</b> that the true value is within that range of values. For variable-τ estimators, the &tau;0 multiple n is also a variable.|$|E
50|$|In August 2013 the LHCb team {{revealed}} an anomaly in the angular distribution of B meson decay products {{which could not}} be predicted by the Standard Model; this anomaly had a <b>statistical</b> <b>certainty</b> of 4.5 sigma, just short of the 5 sigma needed to be officially recognized as a discovery. It is unknown what the cause of this anomaly would be, although the Z' boson has been suggested as a possible candidate.|$|E
5000|$|The {{original}} publication {{was based}} on an apparent peak (resonance) in a histogram of the invariant mass of electron-positron pairs produced by protons colliding with a stationary beryllium target, implying the existence of a particle with a mass of 6 GeV which was being produced and decaying into two leptons. An analysis showed that there was [...] "less than one chance in fifty" [...] that the apparent resonance was simply the result of a coincidence. [...] Subsequent data collected by the same experiment in 1977 revealed that the resonance had been such a coincidence after all. [...] However, a new resonance at 9.5 GeV was discovered using the same basic logic and greater <b>statistical</b> <b>certainty,</b> and the name was reused (see Upsilon particle).|$|E
40|$|As modern systems {{become more}} complex, {{it becomes more}} likely that system maintainers will be {{presented}} with conflicting information when testing them. Maintainers need advanced techniques to make an accurate diagnosis of the problem, especially {{in the face of}} large numbers of conflicts. This article describes two approaches to resolving conflicting test results: modified Dempster-Shafer <b>statistical</b> inference and <b>certainty</b> factors. The complexity of modern systems has led to new demands on system diagnostics. As systems grow in complexity, the need for reliable testing and diagnosis grows accordingly. The design of complex systems has been facilitated by advanced computer-aided design/computeraided engineering (CAD/CAE) tools. Unfortunately, test engineering tools have not kept pace with desig...|$|R
5000|$|Charles S. Peirce {{was also}} a pioneer in statistics. Peirce held that science {{achieves}} <b>statistical</b> probabilities, not <b>certainties,</b> and that chance, a veering from law, is very real. He assigned probability to an argument’s conclusion rather than to a proposition, event, etc., as such. Most of his statistical writings promote the frequency interpretation of probability (objective ratios of cases), {{and many of his}} writings express skepticism about (and criticize the use of) probability when such models are not based on objective randomization. Though Peirce was largely a frequentist, his possible world semantics introduced the [...] "propensity" [...] theory of probability. Peirce (sometimes with Jastrow) investigated the probability judgments of experimental subjects, pioneering decision analysis.|$|R
5000|$|Peirce {{held that}} science {{achieves}} <b>statistical</b> probabilities, not <b>certainties,</b> and that spontaneity (absolute chance) is real (see Tychism on his view). Most of his statistical writings promote the frequency interpretation of probability (objective ratios of cases), {{and many of}} his writings express skepticism about (and criticize the use of) probability when such models are not based on objective randomization. Though Peirce was largely a frequentist, his possible world semantics introduced the [...] "propensity" [...] theory of probability before Karl Popper. Peirce (sometimes with Joseph Jastrow) investigated the probability judgments of experimental subjects, [...] "perhaps the very first" [...] elicitation and estimation of subjective probabilities in experimental psychology and (what came to be called) Bayesian statistics.|$|R
5000|$|In The Selfish Gene, Richard Dawkins {{describes}} {{an approach to}} life-extension that involves [...] "fooling genes" [...] into thinking the body is young. Dawkins attributes inspiration for this idea to Peter Medawar. The basic idea is that our bodies are composed of genes that activate throughout our lifetimes, some when we are young and others when we are older. Presumably, these genes are activated by environmental factors, and the changes caused by these genes activating can be lethal. It is a <b>statistical</b> <b>certainty</b> that we possess more lethal genes that activate in later life than in early life. Therefore, to extend life, {{we should be able}} to prevent these genes from switching on, and {{we should be able to}} do so by [...] "identifying changes in the internal chemical environment of a body that take place during aging... and by simulating the superficial chemical properties of a young body".|$|E
50|$|Intravenous {{aciclovir}} {{is effective}} to treat severe medical conditions caused by {{different species of}} the herpes virus family, including severe localized infections of herpes virus, severe genital herpes, chickenpox and herpetic encephalitis. It is also effective in systemic or traumatic herpes infections, eczema herpeticum and Herpes simplex meningitis.Reviews of research dating from the 1980s show there is some effect in reducing the number and duration of lesions if aciclovir is applied {{at an early stage}} of an outbreak. Research shows effectiveness of topical aciclovir in both the early and late stages of the outbreak as well as improving methodologically and in terms of <b>statistical</b> <b>certainty</b> from previous studies. Aciclovir trials show that this agent has no role in preventing HIV transmission, but it can help slow HIV disease progression in people not taking anti-retroviral therapy (ART). This finding emphasizes the importance of testing simple, inexpensive non-ART strategies, such as aciclovir and cotrimoxazole, in people with HIV.|$|E
50|$|Two {{types of}} {{chemically}} absorbent tubes {{are used to}} sample {{for a wide range}} of chemical substances. Traditionally a chemical absorbent 'tube' (a glass or stainless steel tube of between 2 and 10 mm internal diameter) filled with very fine absorbent silica (hydrophilic) or carbon, such as coconut charcoal (lypophylic), is used in a sampling line where air is drawn through the absorbent material for between four hours (minimum workplace sample) to 24 hours (environmental sample) period. The hydrophilic material readily absorbs water-soluble chemical and the lypophylic material absorbs non water-soluble materials. The absorbent material is then chemically or physically extracted and measurements performed using various gas chromatograph or mass spectrometry methods. These absorbent tube methods have the advantage of being usable {{for a wide range of}} potential contaminates. However, they are relatively expensive methods, are time consuming and require significant expertise in sampling and chemical analysis. A frequent complaint of workers is in having to wear the sampling pump (up to 1 kg) for several days of work to provide adequate data for the required <b>statistical</b> <b>certainty</b> determination of the exposure.|$|E
40|$|Currently {{software}} systems {{operate in}} highly dynamic contexts, and consequently {{they have to}} adapt their behavior in response to changes in t heir contexts or/and requirements. Existing approaches trigger adaptations after detecting violations in quality of service (QoS) requirements by just comparing observed QoS values to predefined thresholds without, any <b>statistical</b> confidence or <b>certainty.</b> These threshold-based adaptation approaches may perform unnecessary adaptations, {{which can lead to}} severe shortcomings such as follow-up failures or increased costs. In this paper we introduce a statistical approach based on CUSUM control charts called AuDeQAV - Automated Detection of QoS Attributes Violations. This approach estimates at runtime a current status of the running system, and monitors its QoS attributes and provides early detection of violations in its requirements with a defined level of confidence. This enables timely intervention preventing undesired consequences from the violation or from inappropriate remediation. We validated our approach using a series of experiments and response time datasets from real-world web services...|$|R
40|$|Invasive species {{represent}} a significant {{threat to the}} world economy and environment. Because of this threat there is interest in exploring ways of predicting when a particular species will be invasive in a new environment. This information can then be used by policy makers as they consider the relevant issues in their jurisdiction. Reviews of studies to predict invasiveness {{have been carried out}} by Kolar and Lodge (2001) and updated by Hayes and Barry (2008). A significant body of literature has built up across a range of taxa looking at correlates of invasion success. Several points can be made from this literature. First, while invasiveness cannot be predicted with <b>certainty,</b> <b>statistical</b> approaches can produce models that on average provide significant discrimination, and characterise the nature of this uncertainty. Second, while a range of factors are related to invasiveness in particular circumstances, few consistent predictors have been found. In summary climatic match, history of invasiveness and number of releases/size of release are consistently related to establishment success. This result is not particularly surprising biologically. The factors that are important t...|$|R
40|$|OBJECTIVE: The {{purpose of}} this study was to {{evaluate}} the diagnostic performance of planar 99 mTc methylene diphosphonate bone scintigraphy compared with SPECT and SPECT fused with CT in patients with focal bone lesions of the axial skeleton. SUBJECTS AND METHODS: Thirty-seven patients with 42 focal lesions of the axial skeleton were included in this prospective study. All patients underwent planar scintigraphy, SPECT through the focal lesions, and SPECT-guided CT. SPECT and CT images then were fused digitally. The three types of images were evaluated separately from one another by two experienced reviewers working to consensus. Visibility of the lesions, diagnostic performance, and certainty in diagnosis were evaluated. Performance for specific diagnoses also was evaluated. Histologic, MRI, and clinical follow-up findings were used as the reference standard. RESULTS: Visibility of the lesions was significantly better with SPECT than with planar scintigraphy (p < 0. 0001). Sensitivity and specificity for differentiation of benign and malignant bone lesions were 82 % and 94 % for planar scintigraphy, 91 % and 94 % for SPECT, and 100 % and 100 % for SPECT fused with CT. Differences between the three methods of differentiating benign and malignant lesions did not reach <b>statistical</b> significance. <b>Certainty</b> in diagnosis was significantly higher for SPECT fused with CT than for planar scintigraphy (p = 0. 004) and SPECT (p = 0. 004). A specific diagnosis was made with planar scintigraphy in 64 % of cases, with SPECT in 86 %, and with SPECT fused with CT in all cases. CONCLUSION: Planar scintigraphy may suffice for differentiating benign and malignant lesions of the axial skeleton, but SPECT fused with CT significantly increases certainty in diagnosis and is the best tool for making a specific diagnosis...|$|R
5000|$|If {{a series}} {{which is known}} to be random is {{analysed}} - fair dice falls, or computer-generated pseudo-random numbers - and a trend line is fitted through the data, the chances of an exactly zero estimated trend are negligible. But the trend would be expected to be small. If an individual series of observations is generated from simulations that employ a given variance of noise that equals the observed variance of our data series of interest, and a given length (say, 100 points), a large number of such simulated series (say, 100,000 series) can be generated. These 100,000 series can then be analysed individually to calculate estimated trends in each series, and these results establish a distribution of estimated trends that are to be expected from such random data - see diagram. Such a distribution will be normal according to the central limit theorem except in pathological cases. A level of <b>statistical</b> <b>certainty,</b> S, may now be selected - 95% confidence is typical; 99% would be stricter, 90% looser - and the following question can be asked: what is the borderline trend value V that would result in S% of trends being between −V and +V? ...|$|E
40|$|Report {{discusses}} {{principles of}} structural-life prediction. Generalized methodology developed for structural life prediction, design, and reliability, based upon fatigue criterion. Approach incorporates computed life of elemental stress volumes of complex machine elements to predict system life. Results of coupon fatigue testing incorporated into analysis, allowing for life prediction and component or structural renewal rates, with reasonable <b>statistical</b> <b>certainty...</b>|$|E
40|$|This {{article is}} {{the second part of}} a series of {{introduction}} to powder diffraction method, intended to show how the specimens for powder X-ray diffraction measurements should be prepared. It is generally required for the size of crystallites to be 1 - 10 µm to obtain sufficient <b>statistical</b> <b>certainty</b> about particle orientation, while unwanted effects of grinding powder should also be taken into account. The requirement is closely related to the geometry of conventional powder diffractomters...|$|E
40|$|Boosting, an authorial commitment, and hedging, a authorial mitigating, are {{two issues}} {{interconnected}} {{one another with}} a gaining importance in the last decades (for detail see Gillaerts & Velde, 2010). However, boosting has remained as an issue needing to be studied from different aspects; for instance, cross-linguistic, cross-disciplinary, cross-cultural or comparative while hedging gains {{a great deal of}} attention from researchers. The purpose of the present study is to investigate the corpora in terms of <b>statistical</b> inclusion of <b>certainty</b> markers in the research articles written in English by Turkish, Japanese and Anglophonic authors, and then to explain the results obtained through statistical tests in the sense of linguistic and cultural factors. A corpus of total 60 research articles written by 20 Anglophonic authors, 20 Japanese authors, and 20 Turkish authors of English constituted the data for the present study. The data were scanned by researchers of the present study. Having completed the scanning, the words functioning as boosters were categorized in line with the taxonomy created for the present study. Then, the total certainty markers for each group of scholars were calculated and analyzed through ANOVA test. The test results provided whether there were any statistically significant differences among the groups in terms of including boosters in the research papers. Furthermore, the present study formed a boosting list as a result of dictionary scanning, which may be a reference for further studies, and the most and th...|$|R
40|$|The {{size and}} {{complexity}} of current data mining data sets have eclipsed the limits of traditional statistical techniques. Such large datasets frequently require some form of cluster analysis, usually {{in the form of}} a hierarchical cluster analysis. However the implementation of a traditional hierarchical scheme on large datasets requires an additional cluster validation analysis. Classification and Regression Trees (CART) are a non-parametric regression and classification technique that have become popular within the biotechnology and ecological fields. CARTs intuitive interpretation, and ability to handle large datasets make it easily accessible to the non-statistician by presenting the statistical relationships found {{in the form of a}} binary tree. This paper proposes a supervised clustering algorithm capable of finding real clusters within large datasets by using CART as a means of filtering the clusters found using any hierarchical technique. The supervision performed by CART acts as a filter of the results from a hierarchical cluster analysis by merging or removing poorly defined groups. It is common practice to validate a cluster analysis using descriminant analysis, however this assumes that the correct number of clusters is known. CART implements a selective classification of groups allowing for some groups not to be explicitly classified, a feature not supported by standard descriminant analysis. This selective classification acts in two fold, firstly by filtering or merging clusters that are not validated by the data, and secondly, as a relationship model for the clusters found and provides <b>statistical</b> measures of <b>certainty</b> over the analysis. An example of this method is presented using Sea Surface Temperatures (SST). This is an ideal choice as very little statistical cluster analysis has been implemented on this dataset, yet knowledge of such structure is in high demand. The analysis is performed for one month November for the years 1940 through to 2002, where some of the most useful variation is expected. The supervised clustering technique successful extracted seven meaningful clusters, which predicted with a cross-validated classification rate of 0. 50...|$|R
40|$|Schools in the making: Mapping digital {{spaces of}} {{evidence}} Evidence-based policy practices {{have become increasingly}} popular in education over the last decades. In Flanders (Belgium), for instance, performance measures, standards and examples of good practice increasingly function as a common ground to talk about education today. These measures, standards and examples are, however, not just collected {{in order to create}} an evidence base for policy preparation and political decision making. Rather, through instruments such as feedback reports, publically consultable audits and good practices, the distribution of evidence becomes part of Flanders’ educational governance. Such tendencies and developments do not apply to Flanders alone, but have been broadly recognized in the research literature on education policy. The same applies to the instruments and conceptual tools (European as well as international) that assist in shaping and conducting policy founded on sound evidence, such as standards, benchmarks, examples of good practice, international comparable databases and concomitant global assessment tests such as PISA and TIMSS, and so forth. Drawing on some conceptual underpinnings of socio-technical approaches, this contribution investigates how precisely data, information and knowledge are elevated to the status of evidence. The objects of analysis consisted of three websites, each exemplifying a specific type of governing by evidence as it is conducted in Flanders nowadays: a website of a policy initiative of bottom-up innovation by means of the provision of examples of good practice; the official website of the inspectorate; and a website promoting school feedback as incentive for school improvement. In line with this socio-technical approach, each website was approached as an active device not only doing things itself, but also making the visitors of these websites perform particular actions. In this contribution, we attempt to map the assemblages of these three different websites and, in so doing, to address the following research interests: Which operations does each website, as particular heterogeneous assemblage, perform? How are data and information being staged as evidence? How does each website address its visitors? In sum, we argue that on each website, different social and technical operations are performed that turn information into evidence, and, at once, stage different modes of schools to exist. Methodology, Methods, Research Instruments or Sources Used In this contribution, we adopt a sociotechnical approach to evidence-based policy. This implies the focus not to be on evidence-based policy practices as such, or on the epistemological denomination of ‘good’ or ‘bad’ evidence. Rather, we focus on the status of evidence – that is, on what could be called evidence ‘in the making’. Indeed, in order for information and knowledge to work as evidence, a chain of actors must be mobilized, not only to fabricate this evidence, but also to enact and stage specific material as being evidence. Furthermore this evidence is to be distributed to render it consultable for interested actors. Thus, the studied websites are approached and mapped as socio-technical devices: technical in as far as they perform particular operations and enactments (each website performs particular operations that render information the status of evidence); and social in as far as they constitute specific interactions and patterns of action and meaning that install a particular self-understanding of the person visiting the website (each website addresses its visitors in a particular way and hence urges them to understand themselves in a particular way). Conclusions, Expected Outcomes or Findings In a conclusive section, the three different analyses are compared and common and differing patterns highlighted with a focus on spaces (of equivalence, of meaning, and of action) that correlate with each website. Finally, some critical remarks are formulated on the impacts of these digital spaces of evidence. As such, the objective of this study is not to render the evidence on schools more weak or less valuable and powerful, nor to cause doubts regarding the existence of the evidenced schools. Displaying the socio-technical operations and showing different modes of existence hopefully opens up a space in order to put them side by side, that is, reassemble these practices and to re- or undo some of their operations. Thus, this contribution is aimed at giving opportunities to discuss the very constitution of these assemblages and transform them, in Latour’s (2004) parlance, from matters of fact into matters of (public) concern. The staging of different modes of schools to exist and to become evidence is perhaps a good antidote to, or at least a first step in the struggle against, those regions with imperial ambitions which claim their evidence to speak for itself. References Callon, Michel, Cécile Méadel, and Vololona Rabeharisoa. 2002. The economy of qualities. Economy and society 31 (2) : 194 - 217. Davies, Philip. 1999. What is evidence-based education? British Journal of Educational Studies 47 (2) : 108 - 121. Desrosières, Alain. 1998. The politics of large numbers: A history of statistical reasoning. Cambridge, MA and London: Harvard University Press. Fenwick, Tara. 2010. (un) Doing standards in education with actor‐network theory. Journal of Education Policy 25 (2) : 117 - 133. Grek, Sotiria. 2009. Governing by numbers: the PISA 'effect' in Europe. Journal of Education Policy 24 (1) : 23 - 37. Gorur, Radhika. 2010. ANT on the PISA trail: Following the <b>statistical</b> pursuit of <b>certainty.</b> Educational Philosophy and Theory 43 (1) : 76 - 93. Higgins, Vaughan, and Wendy Larner, eds. 2010. Calculating the Social. Standards and the Reconfiguration of Governing. Hampshire and New York: Palgrave Macmillan Koyama, Jill. 2011. Generating, comparing, manipulating, categorizing: reporting, and sometimes fabricating data to comply with No Child Left Behind mandates. Journal of Education Policy 26 (5) : 701 - 720. Latour, Bruno. 1999. Pandora’s hope: Essays on the reality of science studies. Cambridge, MA: Harvard University Press. Latour, Bruno. 2004. Has critique run out of steam? From matters of fact to matters of concern. Critical Inquiry 30 (2) : 225 - 248. Latour, Bruno. 2005. Reassembling the social. An introduction to actor-network-theory. Cambridge, MA: Oxford University Press. Law, John. 2009. Actor network theory and material semiotics. In The new Blackwell companion to social theory, edited by Bryan Turner, 141 - 58. Oxford, UK: Wiley-Blackwell Publishing. Nimmo, Richie. 2011. Actor-network theory and methodology: social research in a more-than-human world. Methodological Innovations Online 6 (3) : 108 - 119. Rose, Nikolas. 1991. Governing by Numbers: Figuring out democracy. Accounting, Organizations and Society 16 (7) : 673 - 692. Stengers, Isabelle. 2005. The cosmopolitical proposal. In Making things public: Atmospheres of democracy, edited by Bruno Latour and Peter Weibel, 994 – 1003. Cambridge, MA: MIT Press. status: publishe...|$|R
40|$|A {{generalized}} methodology to structural life prediction, design, {{and reliability}} {{based upon a}} fatigue criterion is advanced. The life prediction methodology is {{based in part on}} work of W. Weibull and G. Lundberg and A. Palmgren. The approach incorporates the computed life of elemental stress volumes of a complex machine element to predict system life. The results of coupon fatigue testing can be incorporated into the analysis allowing for life prediction and component or structural renewal rates with reasonable <b>statistical</b> <b>certainty...</b>|$|E
40|$|This {{study is}} part of an ongoing European Aviation Safety Agency (EASA) {{programme}} (‘SAMPLE’). The effects of gas stream flow regimes in the sample transport line and dilution strategies for removal of the volatile fraction on measured PM size distribution are evaluated behind a simulated aero-derivative gas turbine exhaust using a fast mobility DMS 500 particle sizer. The PM size distribution and concentration within the primary transport sample was found to be relatively insensitive to flow regime, with conditions of turbulent flow (lowest residence time) providing the highest number concentrations, and hence least losses. However, given the natural variation of PM production from the combustor source the <b>statistical</b> <b>certainty</b> of these observations require consolidation. A ‘bespoke’ volatile particle removal system based on the European automotive PMP protocol was constructed to allow the effects of dilution ratio and evaporation tube residence time to be investigated. It was shown that both strategies of increasing dilution ratio and residence times in the evaporation tube did not affect the size distribution at the two distinct nucleation and accumulation modes to any <b>statistical</b> <b>certainty.</b> When using high (420 : 1) dilution ratios in the VPR, a third larger (200 nm) mode appears, which requires further investigation...|$|E
40|$|Markarian 501 is {{only the}} second extragalactic source to be {{detected}} with high <b>statistical</b> <b>certainty</b> at TeV energies; it is similar in many ways to Markarian 421. The Whipple Observatory gamma-ray telescope has been used to observe the AGN Markarian 501 in 1996 and 1997, the years subsequent to its initial detection. The apparent variability on the one-day time-scale observed in TeV gamma rays in 1995 is confirmed and compared with the variability in Markarian 421. Observations at X-ray and optical wavelengths from 1997 are also presented. Comment: 4 pages, 2 figures, to appear in proceedings of 25 th ICRC (Durban...|$|E
40|$|General {{description}} {{on research}} questions, objectives and theoretical framework: In recent year test and measurement {{has gained a}} strong position in education. PISA results, league tables, etc. has from a neoliberal perspective created a discourse about a school in crisis (Popkewitz, 2011; Rizvi & Lingard, 2010). At {{the same time a}} growing edu-market (Ball, 2012) with a free school market and homework companies has entered the educational landscape. Education has traditionally in Sweden been a state issue, however this has changed dramatically. This has led to a re-territorialisation of educational responsibility (Tröhler, 2009) where e. g. private school are taken for granted {{and a growing number of}} so called home work companies providing families help with home works. Following the traces of Popkewitz and Wehlage’s (1973) discussion on accountability, this paper briefly describes ongoing research that intend to unfold and disassemble processes of accountability in the intersection between family, education, and the free school market; homework companies. This paper starts out in the societal desire of a more effective schooling where measurements and testing are used to track knowledge production (Rizvi and Lingard, 2010), i. e. the “enthusiasm for accountability that grows out of the sense of frustration and impatience in accomplishing basic educational goals” (Popkewitz and Wehlage, 1973, p. 48). This turns education into school credentials which in turn enables processes of who to hold accountable (Labaree, 2008). Therefore I claim processes and materialization of accountability are of special interest. In a time when “development of measurable objectives is the sine qua non of accountability” (Popkewitz and Wehlage, 1973, p. 49) the notion of accountability serves as “a potent vehicle of expression” (ibid, p. 48) and a vehicle of analyse. The overarching aim of the ongoing research is therefore to unfold and disassemble processes of accountability in a changing educational landscape in Sweden. More specific, this paper reports on research how accountability is flowing, moving, translated, negotiated and materialised in practises of homework companies. Following Nespor’s (2002) research on homework and Fenwick’s (2010 b) view on ANT as a possibility of researching education I will use ANT as theoretical and methodological frameworks. Briefly, the main idea with ANT is that ideas, practices and facts are effects of assemblages and webs of relations between (human and non-human) actors (Gorur 2011). ANT does not privilege the human, actors can be both animate and/or inanimate and treats social relations, including power and organization, as network effects. Likewise several researchers, I will treat networks as assemblages of heterogeneous materials such as videos, written curricula, utterances, people, building, reports (Edwards, 2002). In intersections (such as practices of homework companies), or following the construct of Latour (2005) and Fenwick (2011); in nodes, it is possible to trace interactions, negotiations, and translations to explore how not only actions, but also power and truths comes into being. ANT therefore offers one way of tracing dynamics of assembling and disassembling, embodiment and materialising processes, often unmentioned or considered unintentional in education (Fenwick and Edwards, 2012). Regarding accountability processes, Fenwick and Edwards (2012) has pinpointed the notion in relation to ANT. They claim for example that “ANT concepts help to trace important nuances in these processes, showing how they actually function as messy networks folded into spaces alongside other networks, and how injunctions of accountability are negotiated at different nodes of these networks“ (ibid, p. 115). In addition, researching humans and non-human simultaneously becomes important in this research since there is a risk that students “internalise these forms of self-regulation through representations of their performance … actors make themselves into calculable subjects (ibid, p. 115). Method: This paper reports from a larger research project that is in the bud. Up to this point, processes and practices into two threads has been followed: 1) Homework company websites and 2) interviews of four employees at homework companies. Building upon ANT, website and interviews, non-humans and humans, are treated as active members of networks, transacting themselves, translating ideas and affecting action through relationships. ANT in this research are therefore used both as a theoretical and a methodological approach that not only set the ontological agenda but also leads me to what kind of data to be used. There are several homework companies in Sweden. These companies provide help to families through their study buddies who meet up with student and help them with for example homework and pre-test training. The families pay approximately 300 SEK (30 EUR) for one hour with a study buddy. The study buddies are often students, some of them teacher training students. At this point, two websites from the two of the largest homework companies in Sweden has been read through. In addition, four interviews has been undertaken with four employees (so called study buddies) at two major homework companies, The transcription process is ongoing and the analyse procedure is under development. At the moment, questions from Fenwick’s (2010 a) earlier research are used when going through the first data: “What kinds of connections are continuing to hold, why, and what else is working to hold them in place? What changes occurred in the process of these connections—and what didn’t change?” (ibid, p. 131). In addition to these question, based on Gorur (2013), identification of four moments of translations will be used. Expected outcomes/results This research is ongoing, however, the first preliminary analysis of websites indicates that the companies do not express any kind of accountability regarding overarching goals or curriculum. Instead, the websites express their own methods, tutoring and goal, stating: “In summary, we design tuition for each and every student and customize it for the individual student” [website, company B]. Or they set up help that “suits the student's needs” [website, company A]. This indicates that the companies do not place themselves in relation to the curriculum in Sweden. This place the companies alongside not only curriculum and tests but accountability. The preliminary results of the interviews indicates negotiations and translations. The study buddies says about their employer: “They don't have the same responsibility …. So honestly speaking … I am a little bit sceptic to this…. There is no law or document that hold them accountable. So the whole situation becomes a little bit risky. ”[Martin, interview]. Another states: “The teacher and the student still have the same responsibility as before. Even though there is an extra chief… just because there is a study coach as well it does not make their responsibility smaller. I am just a bonus. I don’t have the full responsibility. It is very safe to feel this way. … I could fail and it wouldn’t matter to me…. ” [Carl, interview] The preliminary conclusion indicates an evasive accountability when it comes to homework companies. A growing edu-market has received a strong position in education, however this strong position seems to be a chimera. When looking at actors in the network it becomes clear that accountability flows, moves and becomes evasive, the opposite of the intention with notion accountability. The discourse of a school in crisis which asks for accountability has created a growing edu-market with no accountability. Intent of publication: Discourse: Studies in the Cultural Politics of Education (Routledge). ISSN: 0159 - 6306 (Print), 1469 - 3739 (Online) References (400 words) Ball, S. (2012). Global Education Inc. : New policy networks and the neoliberal imaginary. London: Routledge. Edwards, R. (2002) Mobilizing lifelong learning: governmentality in educational practices, Journal of Education Policy, 17 (3), pp. 353 - 365, Fenwick, T (2010 a) (un) Doing standards in education with actor‐network theory, Journal of Education Policy, 25 (2), pp. 117 - 133 Fenwick, T. (2010 b). Accountability practices in adult education: Insights from actor-network theory. Studies in the Education of Adults, Vol. 42, Issue 2. Fenwick, T (2011) Reading Educational Reform with Actor Network Theory: Fluid otherings, and ambivalences, Educational Philosophy and Theory, 43 (1), 114 - 134. Fenwick, T. & Edwards, R. (red.) (2012). Researching Education Through Actor-network Theory. John Wiley & Sons, Ltd. (UK) Gorur, R. (2011). ANT on the PISA Trail: Following the <b>statistical</b> pursuit of <b>certainty.</b> Educational Philosophy and Theory, 43 (1), 76 - 93. Gorur, R. (2013). The invisible infrastructure of standards. Critical Studies in Education, 54 (2), pp. 132 - 142. Labaree, D. (2008). The winning ways of a losing strategy: Educationalizing social problems in the United States. Educational Theory, 58 (4), pp 447 - 460. Latour, B. (2005). Reassembling the social: An introduction to actor-network theory. Oxford: Oxford University Press. Nespor, J. 2002. Networks and contexts of reform. Journal of Educational Change 3, pp. 365 – 82. Popkewitz, T. (2011). PISA. In M. A. Pereyra, H. G. Kotthoff & R. Cowen (Eds.), PISA Under Examination: Changing Knowledge, Changing Tests, and Changing Schools (pp. 31 - 46). Sense Publishers. Popkewitz, T. & G. Wehlage. (1973). Accountability: Critique and alternative perspective. Interchange 4 (4), pp 48 - 62. Rizvi, F., & Lingard, B. (2010). Globalizing education policy. London: Routledge. Tröhler, D. (2009) : Harmonizing the Educational Globe. World Polity, Cultural Features, and the Challenges to Educational Research. Studies in Philosophy and Education 29, 7 – 29...|$|R
40|$|Clinical±anatomic {{correlations}} were performed in 25 patients with focal infarcts in the basilar pons to deter-mine whether pontine lacunar syndromes conform to discrete clinical entities, {{and whether there}} is topo-graphic organization of the motor system within the human basis pontis. Twelve clinical signs were scored on a 6 -point scale, neuroimaging lesions were mapped and de®ned with <b>statistical</b> <b>certainty,</b> and structure± function correlation was performed to develop a topo-graphic map of motor function. Clinical ®ndings ranged from major devastation following extensive lesions (pure motor hemiplegia) to incomplete basilar pontine syndrome and restricted de®cits after small focal lesions (ataxic hemiparesis, dysarthria±clumsy hand syndrome, dysarthria±dysmetria and dysarthria±facial paresis) ...|$|E
30|$|The {{probabilistic}} approach, on {{the other}} hand, recognises the existence of inherent uncertainties arising due to factors such as (Bulleit 2008) material heterogeneity, time, human error, statistical limits, restrictions in models and randomness. Uncertainty is considered the norm {{rather than the exception}} (Kirchsteiger 1999), and even when the level of <b>statistical</b> <b>certainty</b> is set to as high as 0.95, there is still a large degree of flexibility that stretches the limits of the band of safe mud pressures. The inconsistency in the design parameters implemented via variable values of input rock properties—elastic modulus, Poisson ratio and void ratio—permits the realisation of a broader range of safe operating conditions vis-à-vis where consistent and/or uniform values of rock properties are employed.|$|E
40|$|Markarian 421 was {{the first}} extragalactic source to be {{detected}} with high <b>statistical</b> <b>certainty</b> at TeV energies. The Whipple Observatory gamma-ray telescope {{has been used to}} observe the Active Galactic Nucleus, Markarian 421 in 1996 and 1997. The rapid variability observed in TeV gamma rays in previous years is confirmed. Doubling times as short as 15 minutes are reported with flux levels reaching 15 photons per minute. The TeV energy spectrum is derived using two independent methods. The implications for the intergalactic infra-red medium of an observed unbroken power law spectrum up to energies of 5 TeV is discussed. Comment: 4 pages, 4 figures, to appear in proceedings of 25 ICRC (Durban...|$|E
40|$|Recent {{approaches}} in causal inference have proposed estimating average causal effects that are local to some subpopulation, often {{for reasons of}} efficiency. These inferential targets are sometimes data-adaptive, {{in that they are}} dependent on the empirical distribution of the data. In this short note, we show that if researchers are willing to adapt the inferential target on the basis of efficiency, then extraordinary gains in precision can be obtained. Specifically, when causal effects are heterogeneous, any asymptotically normal and root-$n$ consistent estimator of the population average causal effect is superefficient for a data-adaptive local average causal effect. Our result illustrates the fundamental gain in <b>statistical</b> <b>certainty</b> afforded by indifference about the inferential target...|$|E
40|$|A new {{theoretical}} {{technique for}} understanding, analyzing and developing optical systems is presented. The approach is statistical in nature, where information about an object under investigation is discovered, by examining deviations from a known reference statistical distribution. A Fourier optics framework and a scalar {{description of the}} propagation of monochromatic light is initially assumed. An object (belonging to a known class of objects) is illuminated with a speckle field {{and the intensity of}} the resulting scattered optical field is detected at a series of spatial locations by point square law detectors. A new speckle field is generated (with a new diffuser) and the object is again illuminated and the intensities are again measured and noted. By making a large number of these statistical measurements - an ensemble averaging process (which in general can be a temporal or a spatial averaging process) - it is possible to determine the statistical relationship between the intensities detected in different locations with a known <b>statistical</b> <b>certainty.</b> It is shown how this physical property of the optical system can be used to identify different classes of objects (discrete objects or objects that vary continuously as a function of several physical parameters) with a specific <b>statistical</b> <b>certainty.</b> The derived statistical relationship is a fourth order statistical process and is related to the mutual intensity of scattered light. A connection between the integration time of the intensity detectors and temporal coherence and hence temporal bandwidth of a quasi-monochromatic light source is discussed. We extend these results to multiple wavelengths and polarizations of light. We briefly discuss how the results may be extended to non-paraxial optical systems...|$|E
40|$|BACKGROUND: Identification {{of acute}} kidney injury (AKI) is {{predominantly}} based {{on changes in}} plasma creatinine concentration, an insensitive marker. Alternative biomarkers have been proposed. The reference change value (RCV), {{the point at which}} biomarker change can be inferred to have occurred with <b>statistical</b> <b>certainty,</b> provides an objective assessment of change in serial tests results in an individual. METHODS: In 80 patients with chronic kidney disease, weekly measurements of blood and urinary biomarker concentrations were undertaken over 6 weeks. Variability was determined and compared before and after adjustment for urinary creatinine and across subgroups stratified by level of kidney function, proteinuria, and presence or absence of diabetes. RESULTS: RCVs were determined for whole blood, plasma, and urinary neutrophil gelatinase-associated lipocalin (111...|$|E
40|$|During the 1960 s and the 1970 s, {{some studies}} {{postulated}} that Jacobs' Syndrome {{can lead to}} aggressive and/or criminal behavior, but the <b>statistical</b> <b>certainty</b> was questioned. In the 1990 s new discoveries in neuroscience and genetics brought new attention to a possible biological component of deviance. In this context forensic psychiatrists were required to express opinions about a young Italian man afflicted by 47,XYY karyotype (Jacobs’ Syndrome) and Borderline Personality Disorder who had frequent violent behaviors against his relatives, being therefore accused of “abuse against family members or partners” (art. 572 Italian Penal Code). The Court disposed for an evaluation of his mental conditions, his imputability and his possible social dangerousness, analyzing the influences on behavior from both his mental and genetic conditions...|$|E
30|$|The {{categorical}} input {{variables for}} this study were the method of rotation (pure, hinged), and degree of rotation: 0 ° (supine), 90 ° (neutral), and 180 ° (pronated). The primary outcome variables were (1) true fracture displacement at lateral and posterior edges, and (2) apparent fracture displacement at the lateral and posterior edges. The true and apparent displacements were compared as a function of method of rotation (pure or hinged)) and degree of rotation. SPSS 15.0 statistical analysis software (SPSS, Inc., Chicago, IL) was used to perform paired-samples t tests to evaluate the main objectives of the study: (1) the effect of positioning on fracture motion, and (2) the difference between true fracture-displacement measurements and apparent fracture-displacement measurements. The associated p values were used to determine <b>statistical</b> <b>certainty.</b>|$|E
