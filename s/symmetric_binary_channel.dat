4|1975|Public
40|$|Abstract 1 – In this paper, {{we study}} the {{transport}} capacity – the transported bit-meters per symbol period per node, {{and the space}} time spectrum power efficiency – the transported bit-meters per Hz per second per square meter per watt for large wireless CDMA ad hoc networks. The node density and processing gain tend to infinity with their ratio kept a constant. The limiting networks consist of infinitely dense, sender uniformly distributed, mutually independent, and memoryless Gaussian channels connecting all pairs of sending and receiving nodes on the entire 2 -D plane. The MMSE receiver, decorrelator, and MF are employed. Gaussian input Gaussian noise channel, binary input Gaussian noise channel and <b>symmetric</b> <b>binary</b> <b>channel</b> are considered. The transport capacity and space time spectrum power efficiency with joint optimization of code rate distribution, MAC and routing protocols are obtained in closed form as functions of entire network profile. I...|$|E
40|$|We {{show that}} the Extrinsic Information about the coded bits of any good (capacity achieving) code {{operating}} over a wide class of discrete memoryless channels (DMC) is zero when channel capacity is below the code rate and positive constant otherwise, that is, the Extrinsic Information Transfer (EXIT) chart is a step function of channel quality, for any capacity achieving code. It follows that, for a common class of iterative receivers where the error correcting decoder must operate at first iteration at rate above capacity (such as in turbo equalization, turbo channel estimation, parallel and serial concatenated coding and the like), classical good codes which achieve capacity over the DMC are not effective and should be replaced by different new ones. Another meaning of the results {{is that a good}} code operating at rate above channel capacity falls apart into its individual transmitted symbols in the sense that all the information about a coded transmitted symbol is contained in the corresponding received symbol and no information about it can be inferred from the other received symbols. The binary input additive white Gaussian noise channel is treated in part 1 of this report. Part 2 extends the results to the <b>symmetric</b> <b>binary</b> <b>channel</b> and to the binary erasure channel and provides an heuristic extension to wider class of channel models...|$|E
40|$|Abstract: We {{show that}} the Extrinsic Information about the coded bits of any good (capacity achieving) code {{operating}} over a wide class of discrete memoryless channels (DMC) is zero when channel capacity is below the code rate and positive constant otherwise, that is, the Extrinsic Information Transfer (EXIT) chart is a step function of channel quality, for any capacity achieving code. It follows that, for a common class of iterative receivers where the error correcting decoder must operate at first iteration at rate above capacity (such as in turbo equalization, turbo channel estimation, parallel and serial concatenated coding and the like), classical good codes which achieve capacity over the DMC are not effective and should be replaced by different new ones. Another meaning of the results {{is that a good}} code operating at rate above channel capacity falls apart into its individual transmitted symbols in the sense that all the information about a coded transmitted symbol is contained in the corresponding received symbol and no information about it can be inferred from the other received symbols. This report comprises two stand-alone parts. Part 1 treats the binary input additive white Gaussian noise channel. Part 2 extends the results to the <b>symmetric</b> <b>binary</b> <b>channel</b> and to the binary erasure channel and provides an heuristic extension to wider class of channel models such as common fading models combined with QPSK and other inputs which might be rigorized by further work. ...|$|E
40|$|We derive an {{asymptotic}} {{formula for}} entropy rate of a hidden Markov chain under certain parameterizations. We also discuss {{applications of the}} asymptotic formula to the asymptotic behaviors of entropy rate of hidden Markov chains as outputs of certain <b>channels,</b> such as <b>binary</b> <b>symmetric</b> <b>channel,</b> <b>binary</b> erasure <b>channel,</b> and some special Gilbert-Elliot channel. © 2006 IEEE. published_or_final_versio...|$|R
40|$|Abstract—We derive an {{asymptotic}} {{formula for}} entropy rate of a hidden Markov chain under certain parameterizations. We also discuss {{applications of the}} asymptotic formula to the asymptotic behaviors of entropy rate of hidden Markov chains as outputs of certain <b>channels,</b> such as <b>binary</b> <b>symmetric</b> <b>channel,</b> <b>binary</b> era-sure <b>channel,</b> and some special Gilbert-Elliot channel. Index Terms—Entropy, entropy rate, hidden Markov chain, hidden Markov model, hidden Markov process. I...|$|R
40|$|It {{is known}} that the Kesten-Stigum {{reconstruction}} bound is tight for roughly <b>symmetric</b> <b>binary</b> <b>channels.</b> In this paper, we will adopt a refined analysis of moment recursion on a weighted version of the magnetization, which is engaged in Sly [2011] to handle the symmetric Potts model, and establish the critical condition of the asymmetric Ising model to make Kesten-Stigum bound the reconstruction threshold on regular $d$-ary trees. Comment: Conference paper: 3 rd International Conference on Electrical, Electronics, Engineering Trends, Communication, Optimization and Sciences (EEECOS) - 201...|$|R
40|$|We {{consider}} {{a process in}} which information is transmitted from a given root node on a noisy d-ary tree network T. We start with a uniform symbol taken from an alphabet A. Each edge of the tree is an independent copy of some channel (Markov chain) M, where M is irreducible and aperiodic on A. The goal is to reconstruct the symbol at the root from the symbols at the nth level of the tree. This model has been studied in information theory, genetics and statistical physics. The basic question is: is it possible to reconstruct (some information on) the root? In other words, does the probability of correct reconstruction tend to 1 /A as n →∞? It is known that reconstruction is possible if dλ 22 (M) 3 ̆e 1, where λ 2 (M) is the second eigenvalue of M. Moreover, in this case it is possible to reconstruct using a majority algorithm which ignores the location of the data at the boundary of the tree. When M is a <b>symmetric</b> <b>binary</b> <b>channel,</b> this threshold is sharp. In this paper we show that, both for the binary asymmetric channel and for the symmetric channel on many symbols, it is sometimes possible to reconstruct even when dλ 22 (M) 3 ̆c 1. This result indicates that, for many (maybe most) tree-indexed Markov chains, the location of the data on the boundary plays a crucial role in reconstruction problems...|$|E
40|$|In this paper, we generalize {{a result}} in [17] and derive an {{asymptotic}} formula for the entropy rate of a hidden Markov chain, observed when a Markov chain {{passes through a}} <b>binary</b> <b>symmetric</b> <b>channel.</b> And we prove an asymptotic formula for the capacity of a <b>binary</b> <b>symmetric</b> <b>channel</b> with input process supported on an irreducible finite type constraint. ...|$|R
5000|$|<b>Binary</b> <b>symmetric</b> <b>channel</b> (BSC), a {{discrete}} memoryless channel {{with a certain}} bit error probability ...|$|R
5000|$|... as {{compared}} to the capacity [...] of the <b>binary</b> <b>symmetric</b> <b>channel</b> with crossover probability p.|$|R
3000|$|In a {{scenario}} with noisy communication links, simply modeled as <b>binary</b> <b>symmetric</b> <b>channels</b> (BSCs), the decision [...]...|$|R
5000|$|Compute {{smoothed}} probabilities {{based on}} other information (i.e. noise variance for AWGN, bit crossover probability for <b>binary</b> <b>symmetric</b> <b>channel)</b> ...|$|R
5000|$|<b>Binary</b> <b>symmetric</b> <b>channel</b> (used in {{analysis}} of decoding error probability {{in case of}} non-bursty bit errors on the transmission channel) ...|$|R
40|$|A polar {{coding scheme}} for fading {{channels}} is proposed in this paper. More specifically, {{the focus is}} Gaussian fading channel with a BPSK modulation technique, where the equivalent channel could be modeled as a <b>binary</b> <b>symmetric</b> <b>channel</b> with varying cross-over probabilities. To deal with variable channel states, a coding scheme of hierarchically utilizing polar codes is proposed. In particular, by observing the polarization of different <b>binary</b> <b>symmetric</b> <b>channels</b> over different fading blocks, each channel use corresponding to a different polarization is modeled as a <b>binary</b> erasure <b>channel</b> such that polar codes could be adopted to encode over blocks. It is shown that the proposed coding scheme, without instantaneous channel state information at the transmitter, achieves {{the capacity of the}} corresponding fading <b>binary</b> <b>symmetric</b> <b>channel,</b> which is constructed from the underlying fading AWGN channel through the modulation scheme. Comment: 6 pages, 4 figures, conferenc...|$|R
40|$|Abstract—A polar {{coding scheme}} for fading {{channels}} is pro-posed in this paper. More specifically, {{the focus is}} Gaussian fading channel with a BPSK modulation technique, where the equivalent channel could be modeled as a <b>binary</b> <b>symmetric</b> <b>channel</b> with varying cross-over probabilities. To deal with variable channel states, a coding scheme of hierarchically utilizing polar codes is proposed. In particular, by observing the polarization of different <b>binary</b> <b>symmetric</b> <b>channels</b> over different fading blocks, each channel use corresponding to a different polarization is modeled as a <b>binary</b> erasure <b>channel</b> such that polar codes could be adopted to encode over blocks. It is shown that the proposed coding scheme, without instantaneous channel state information at the transmitter, achieves {{the capacity of the}} corresponding fading <b>binary</b> <b>symmetric</b> <b>channel,</b> which is constructed from the underlying fading AWGN channel through the modulation scheme. I...|$|R
50|$|The {{correlation}} between two sources in DSC has been modeled as a virtual channel {{which is usually}} referred as a <b>binary</b> <b>symmetric</b> <b>channel.</b>|$|R
40|$|This work {{presents}} a polar coding scheme for fading channels, focusing primarily on fading <b>binary</b> <b>symmetric</b> and additive exponential noise <b>channels.</b> For fading <b>binary</b> <b>symmetric</b> <b>channels,</b> a hierarchical coding scheme is presented, utilizing polar coding both over channel uses and over fading blocks. The receiver uses its channel state information (CSI) to distinguish states, thus constructing an overlay erasure channel over the underlying fading channels. By using this scheme, {{the capacity of}} a fading <b>binary</b> <b>symmetric</b> <b>channel</b> is achieved without CSI at the transmitter. Noting that a fading AWGN channel with BPSK modulation and demodulation corresponds to a fading <b>binary</b> <b>symmetric</b> <b>channel,</b> this result covers a fairly large set of practically relevant channel settings. For fading additive exponential noise channels, expansion coding is used in conjunction to polar codes. Expansion coding transforms the continuous-valued channel to multiple (independent) discrete-valued ones. For each level after expansion, the approach described previously for fading <b>binary</b> <b>symmetric</b> <b>channels</b> is used. Both theoretical analysis and numerical results are presented, showing that the proposed coding scheme approaches the capacity in the high SNR regime. Overall, utilizing polar codes in this (hierarchical) fashion enables coding without CSI at the transmitter, while approaching the capacity with low complexity. Comment: 31 pages, 8 figures, journa...|$|R
40|$|Absfracf-Various {{encoding}} schemes {{are examined}} {{from the point}} of view of minimizing the meau magnitude error of a signal caused by transmission through a <b>binary</b> <b>symmetric</b> <b>channel.</b> A necessary property is developed for optimal codes for any <b>binary</b> <b>symmetric</b> <b>channel</b> and any set of quantization levels. The class of optimal codes is found for the case where the probability of error is small but realistic. This class of codes includes the natural numbering and some unit distance codes, among which are the Gray codes...|$|R
5000|$|We {{express the}} {{likelihood}} [...] as [...] (by using the <b>binary</b> <b>symmetric</b> <b>channel</b> likelihood {{for the first}} [...] bits followed by a uniform prior over the remaining bits).|$|R
50|$|Shannon's noisy coding theorem {{is general}} {{for all kinds}} of channels. We {{consider}} a special case of this theorem for a <b>binary</b> <b>symmetric</b> <b>channel</b> with an error probability p.|$|R
3000|$|Let W:X→Y denote {{a general}} <b>symmetric</b> <b>binary</b> input memoryless <b>channel</b> (B-DMC) and W_N:X^N→Y^N denote a vector channel. If {{channels}} are independent but not identical, then W_N(y_ 1 ^N|x_ 1 ^N)=∏ _i= 1 ^NW_(i)(y_i|x_i) where W_(i): X_i→Y_i, such that their transition probabilities p [...]...|$|R
40|$|Consider a sender {{transmitting}} a given {{sequence of}} bits simultaneously through m <b>binary</b> <b>symmetric</b> <b>channels</b> with error probabilities " 1; : : :; " m such that 0 6 " i 3. 3...|$|R
40|$|Consider a {{scenario}} in which two parties Alice and Bob {{as well as an}} opponent Eve receive the output of a <b>binary</b> <b>symmetric</b> source (e,g. installed in a satellite) over individual, not necessarily independent <b>binary</b> <b>symmetric</b> <b>channels.</b> Alice and Bob share no secret key initially and can only communicate over a public channel completely accessible to Eve. The authors derive a lower bound on the rate at which Alice and Bob can generate secret-key bits about which Eve has arbitrarily little information, This lower bound is strictly positive as long as Eve's <b>binary</b> <b>symmetric</b> <b>channel</b> is not perfect, even if Alice's and Bob's channels are by orders of magnitude less reliable than Eve's channe...|$|R
40|$|In this paper, {{we study}} the {{classical}} problem of noisy constrained {{capacity in the}} case of the <b>binary</b> <b>symmetric</b> <b>channel</b> (BSC), namely, the capacity of a BSC whose input is a sequence from a constrained set. Motivated by a result of Ordentlich and Weissman in [26], we derive an asymptotic formula (when the noise parameter is small) for the entropy rate of a hidden Markov chain, observed when a Markov chain passes through a <b>binary</b> <b>symmetric</b> <b>channel.</b> Using this result we establish an asymptotic formula for the capacity of a <b>binary</b> sym-metric <b>channel</b> with input process supported on an irreducible finite type constraint, as the noise parameter tends to zero. 1. Introduction. Le...|$|R
40|$|Abstract—Channel {{polarization}} is {{a method}} of constructing capacity achieving codes for symmetric binary-input discrete memoryless channels (B-DMCs) [1]. In the original paper, the construction complexity is exponential in the blocklength. In this paper, a new construction method for arbitrary <b>symmetric</b> <b>binary</b> memoryless <b>channel</b> (B-MC) with linear complexity in the blocklength is proposed. Furthermore, new upper bound and lower bound of the block error probability of polar codes are derived for the BEC and arbitrary symmetric B-MC, respectively. I...|$|R
40|$|Abstract—Random coding of {{a channel}} with an erasure option is studied. By {{analyzing}} the large deviations {{behavior of the}} code ensemble, we obtain exact single-letter formulas for the error exponents in lieu of Forney’s lower bounds. The analysis technique we use {{is based on an}} enhancement and specialization of tools for assessing the moments of certain distance enumerators, that were recently used for determining the exponential behavior of other communication systems. We specialize our results to the <b>binary</b> <b>symmetric</b> <b>channel</b> case with uniform random coding distribution and derive an explicit expression for the error exponent which, unlike Forney’s bounds, does not involve optimization over two parameters. We also establish the fact that for the <b>binary</b> <b>symmetric</b> <b>channel</b> case with uniform random coding distribution, the difference between the exact error exponent corresponding to the probability of undetected decoding error and the error exponent corresponding to the erasure event is equal to the threshold parameter. Numerical calculations for the <b>binary</b> <b>symmetric</b> <b>channel</b> with uniform random coding distribution indicate that in this case Forney’s bound coincides with the exact random coding exponent. I...|$|R
50|$|As {{with other}} codes, the maximum {{likelihood}} decoding of an LDPC code on the <b>binary</b> <b>symmetric</b> <b>channel</b> is an NP-complete problem. Performing optimal decoding for a NP-complete code of any useful size is not practical.|$|R
2500|$|A <b>binary</b> <b>symmetric</b> <b>channel</b> (BSC) with {{crossover}} probability p is a binary input, <b>binary</b> output <b>channel</b> that flips the input bit with probability p. The BSC has {{a capacity of}} [...] bits per channel use, where [...] is the binary entropy function to the base-2 logarithm: ...|$|R
50|$|The BEC is, in a sense, error-free. Unlike the <b>binary</b> <b>symmetric</b> <b>channel,</b> {{when the}} {{receiver}} gets a bit, it is 100% {{certain that the}} bit is correct. The only confusion arises when the bit is erased.|$|R
40|$|For {{information}} transmission a <b>binary</b> <b>symmetric</b> <b>channel</b> is used. There is also another noisy <b>binary</b> <b>symmetric</b> <b>channel</b> (feedback channel), and the transmitter observes without delay all the outputs of the forward channel via that feedback channel. The transmission of an exponential number of messages (i. e. the transmission rate is positive) is considered. The achievable decoding error exponent {{for such a}} combination of channels is investigated. It is shown that if the crossover probability of the feedback channel {{is less than a}} certain positive value, then the achievable error exponent is better than the decoding error exponent of the channel without feedback. Comment: 24 pages, 4 figure...|$|R
5000|$|To perform ML {{decoding}} in a <b>Binary</b> <b>symmetric</b> <b>channel,</b> {{one has to}} look-up a precomputed {{table of}} size , mapping [...] to [...]Note that this is already of significantly less complexity {{than that of a}} Standard array decoding.|$|R
40|$|Abstract—We {{study the}} {{capacity}} of rewritable storage cells with discrete memoryless write channels. We consider a con-straint on {{the maximum number of}} rewrites and give a closed-form expression for {{the capacity of}} cells with <b>symmetric</b> write <b>channels.</b> For <b>binary</b> <b>symmetric</b> write <b>channels</b> this result is extended to the case where only noisy observations of the state are available to the encoder and decoder. We also consider a constraint on the expected number of rewrites and derive a lower bound on the capacity. This bound coincides with the capacity for cells with <b>binary</b> <b>symmetric</b> write <b>channels.</b> I...|$|R
40|$|Certain <b>binary</b> {{asymmetric}} <b>channels,</b> such as Z-channels {{in which}} one of the two crossover probabilities is zero, demand optimal ones densities different from 50 %. Some broadcast channels, such as broadcast <b>binary</b> <b>symmetric</b> <b>channels</b> (BBSC) where each component <b>channel</b> is a <b>binary</b> <b>symmetric</b> <b>channel,</b> also require a non-uniform input distribution due to the superposition coding scheme, which is known to achieve the boundary of capacity region. This paper presents a systematic technique for designing nonlinear turbo codes that are able to support ones densities different from 50 %. To demonstrate the effectiveness of our design technique, we design and simulate nonlinear turbo codes for the Z-channel and the BBSC. The best nonlinear turbo code is less than 0. 02 bits from capacity...|$|R
40|$|Channel {{polarization}} is {{a method}} of constructing capacity achieving codes for symmetric binary-input discrete memoryless channels (B-DMCs) [1]. In the original paper, the construction complexity is exponential in the blocklength. In this paper, a new construction method for arbitrary <b>symmetric</b> <b>binary</b> memoryless <b>channel</b> (B-MC) with linear complexity in the blocklength is proposed. Furthermore, new {{upper and lower bounds}} of the block error probability of polar codes are derived for the BEC and the arbitrary symmetric B-MC, respectively. Comment: 5 pages, 3 figures, submitted to ISIT 2009; revise...|$|R
50|$|These {{assumptions}} may {{be reasonable}} for transmissions over a <b>binary</b> <b>symmetric</b> <b>channel.</b> They may be unreasonable for other media, {{such as a}} DVD, where a single scratch on the disk can cause an error in many neighbouring symbols or codewords.|$|R
40|$|We {{prove the}} Courtade-Kumar conjecture, for several classes of n-dimensional Boolean functions, for all n ≥ 2 {{and for all}} values of the error {{probability}} of the <b>binary</b> <b>symmetric</b> <b>channel,</b> 0 ≤ p ≤ 1 / 2. This conjecture states that the mutual information between any Boolean function of an n-dimensional vector of independent and identically distributed inputs to a memoryless <b>binary</b> <b>symmetric</b> <b>channel</b> and the corresponding vector of outputs is upper-bounded by 1 -H(p), where H(p) represents the binary entropy function. That is, let X=[X_ 1 [...] . X_n] be a vector of independent and identically distributed Bernoulli(1 / 2) random variables, which are the input to a memoryless <b>binary</b> <b>symmetric</b> <b>channel,</b> with the error probability in the interval 0 ≤ p ≤ 1 / 2 and Y=[Y_ 1 [...] . Y_n] the corresponding output. Let f:{ 0, 1 }^n →{ 0, 1 } be an n-dimensional Boolean function. Then, MI(f(X),Y) ≤ 1 -H(p). Our proof employs Karamata's theorem, concepts from probability theory, transformations of random variables and vectors and algebraic manipulations. Comment: Withdrawn due to rejection by the IEEE Transactions on Information Theory; the reason for rejection was limited originality {{and scope of the}} proposed resul...|$|R
