1994|4579|Public
25|$|Therefore, the {{variance}} of the mean {{of a large number}} of standardized variables is approximately equal to their average correlation. This makes clear that the <b>sample</b> <b>mean</b> of correlated variables does not generally converge to the population mean, even though the law of large numbers states that the <b>sample</b> <b>mean</b> will converge for independent variables.|$|E
25|$|Since each {{observation}} has expectation λ so {{does this}} <b>sample</b> <b>mean.</b> Therefore, the maximum likelihood estimate is an unbiased estimator of λ. It {{is also an}} efficient estimator, i.e. its estimation variance achieves the Cramér–Rao lower bound (CRLB). Hence it is minimum-variance unbiased. Also it can be proved that the sum (and hence the <b>sample</b> <b>mean</b> {{as it is a}} one-to-one function of the sum) is a complete and sufficient statistic for λ.|$|E
25|$|Consider now a {{function}} of the unknown parameter: an estimator is a statistic used to estimate such function. Commonly used estimators include <b>sample</b> <b>mean,</b> unbiased sample variance and sample covariance.|$|E
50|$|The Newman-Keuls method {{employs a}} {{stepwise}} approach when comparing <b>sample</b> <b>means.</b> Prior to any <b>mean</b> comparison, all <b>sample</b> <b>means</b> are rank-ordered in ascending or descending order, thereby producing an ordered range (p) of <b>sample</b> <b>means.</b> A comparison is then {{made between the}} largest and smallest <b>sample</b> <b>means</b> within the largest range. Assuming that the largest range is four means (or p = 4), {{a significant difference between}} the largest and smallest means as revealed by the Newman-Keuls method would result in a rejection of the null hypothesis for that specific range of means. The next largest comparison of two <b>sample</b> <b>means</b> would then be made within a smaller range of three means (or p = 3). Unless there is no significant differences between two <b>sample</b> <b>means</b> within any given range, this stepwise comparison of <b>sample</b> <b>means</b> will continue until a final comparison is made with the smallest range of just two means. If there is no significant difference between the two <b>sample</b> <b>means,</b> then all the null hypotheses within that range would be retained and no further comparisons within smaller ranges are necessary.|$|R
50|$|Produces biased {{estimates}} of grade and tonnage above an ore waste cut-off. Which {{is called the}} volume variance relationship i.e. the variability of the grade distribution depends on the volume of samples. Large volume <b>samples</b> <b>mean</b> small variability whereas small volume <b>samples</b> <b>mean</b> large variability.|$|R
5000|$|A {{stepwise}} multiple comparisons procedure used {{to identify}} <b>sample</b> <b>means</b> that are significantly different from each other. It is used often as a post hoc test whenever {{a significant difference between}} three or more <b>sample</b> <b>means</b> has been revealed by an analysis of variance (ANOVA) ...|$|R
25|$|For {{a finite}} {{population}}, the population mean of a property {{is equal to}} the arithmetic mean of the given property while considering every member of the population. For example, the population mean height {{is equal to the}} sum of the heights of every individual divided {{by the total number of}} individuals. The <b>sample</b> <b>mean</b> may differ from the population mean, especially for small samples. The law of large numbers dictates that the larger the size of the sample, the more likely it is that the <b>sample</b> <b>mean</b> will be close to the population mean.|$|E
25|$|This simple {{combination}} is possible because the <b>sample</b> <b>mean</b> and sample variance {{of the normal}} distribution are independent statistics; this is only true for the normal distribution, and in fact characterizes the normal distribution.|$|E
25|$|Standard {{deviation}} {{refers to}} the extent to which individual observations in a sample differ from a central value, such as the sample or population mean, while Standard error refers to an estimate of difference between <b>sample</b> <b>mean</b> and population mean.|$|E
30|$|Tables  4 and 5 {{present the}} summary {{results of the}} {{simulations}} based on 1000 repetitions under the given true values. In these tables, the first column shows {{the parameters of the}} model and second column shows the true values of the parameters. Tables  4 and  5 give the <b>sample</b> <b>means</b> of the MLEs of parameters obtained by the EM algorithm. For all of the sets, the <b>sample</b> <b>means</b> of the estimated parameters are close to the corresponding true values of the parameters. If the percent of censored observations decrease (i.e., if number of failures increase), the <b>sample</b> <b>means</b> of the MLEs become more closers to the true values for all most all sets, as expected. Similarly, the <b>sample</b> <b>means</b> of the MLEs become more closers to the true values for increasing sample sizes.|$|R
5000|$|... #Caption: This figure {{demonstrates}} the central limit theorem. The <b>sample</b> <b>means</b> are generated using a random number generator, which draws numbers between 0 and 100 from a uniform probability distribution. It illustrates that increasing sample sizes {{result in the}} 500 measured <b>sample</b> <b>means</b> being more closely distributed about the population mean (50 in this case). It also compares the observed distributions with the distributions that would be expected for a normalized Gaussian distribution, and shows the chi-squared values that quantify the goodness of the fit (the fit is good if the reduced chi-squared value is less than or approximately equal to one). The input into the normalized Gaussian function is the <b>mean</b> of <b>sample</b> <b>means</b> (~50) and the <b>mean</b> <b>sample</b> standard deviation divided by the square root of the sample size (~28.87/), which is called the standard deviation of the mean (since {{it refers to the}} spread of <b>sample</b> <b>means).</b>|$|R
5000|$|... #Subtitle level 2: Measures {{based on}} {{something}} more than <b>sample</b> <b>means</b> ...|$|R
25|$|That is, the {{variance}} of the mean decreases when n increases. This formula for {{the variance}} of the mean {{is used in the}} definition of the standard error of the <b>sample</b> <b>mean,</b> which is used in the central limit theorem.|$|E
25|$|In {{symmetric}} unimodal distributions, such as {{the normal}} distribution, the mean (if defined), median and mode all coincide. For samples, if {{it is known that}} they are drawn from a symmetric distribution, the <b>sample</b> <b>mean</b> can be used as an estimate of the population mode.|$|E
25|$|Often, {{since the}} {{population}} variance is an unknown parameter, {{it is estimated}} by the mean sum of squares; when this estimated value is used, {{the distribution of the}} <b>sample</b> <b>mean</b> is no longer a normal distribution but rather a Student's t distribution with n1 degrees of freedom.|$|E
5000|$|The two-pass {{algorithm}} first computes the <b>sample</b> <b>means,</b> {{and then}} the covariance: ...|$|R
5000|$|In practice, the Z-factor is {{estimated}} from the <b>sample</b> <b>means</b> and <b>sample</b> standard deviations ...|$|R
50|$|The Tukey method {{uses the}} studentized range {{distribution}}.Suppose {{that we take}} a sample of size n from each of k populations with the same normal distribution N(μ, σ) and suppose that ''''min is the smallest of these <b>sample</b> <b>means</b> and ''''max is the largest of these <b>sample</b> <b>means,</b> and suppose S2 is the pooled sample variance from these samples. Then the following random variable has a Studentized range distribution.|$|R
25|$|For example, {{in terms}} of efficiency, given a sample of a normally-distributed {{numerical}} parameter, the arithmetic mean (average) for the population can be estimated with maximum efficiency by computing the <b>sample</b> <b>mean</b> – adding {{all the members of}} the sample and dividing by the number of members.|$|E
25|$|Firstly, if the omniscient mean {{is unknown}} (and is {{computed}} as the <b>sample</b> <b>mean),</b> then the sample variance is a biased estimator: it underestimates the variance {{by a factor}} of (n−1) / n; correcting by this factor (dividing by n−1 instead of n) is called Bessel's correction. The resulting estimator is unbiased, and is called the (corrected) sample variance or unbiased sample variance. For example, when n=1 the variance of a single observation about the <b>sample</b> <b>mean</b> (itself) is obviously zero regardless of the population variance. If the mean is determined in some other way than from the same samples used to estimate the variance then this bias does not arise and the variance can safely be estimated as that of the samples about the (independently known) mean.|$|E
25|$|L-estimators {{are often}} much more robust than maximally {{efficient}} conventional methods – the median is maximally statistically resistant, having a 50% breakdown point, and the X% trimmed mid-range has an X% breakdown point, while the <b>sample</b> <b>mean</b> (which is maximally efficient) is minimally robust, breaking {{down for a}} single outlier.|$|E
40|$|In t-test, the {{difference}} between 2 <b>sample</b> <b>means</b> are tested for Significance. In ANOVA {{the difference}}s between means of more than 2 samples are tested for significance. This is done by examining the variation within the whole groups of <b>sample</b> <b>means.</b> It consists of a comparison between 2 estimates of the overall variation (of the complete set of measurements included in the analyses), one estimate being based on the variance of <b>sample</b> <b>means</b> about the grand mean. The other based on the variance of the individual measurements about their treatment means. The first estimate is called treatment variance. The second estimate is called error variance. If the null hypothesis is true, the ratio of these estimates would approximate 1. If, on the other hand, the <b>sample</b> <b>means</b> estimates differ from the population or group means then the ratio would exceed 1, In practice, this ratio is calculated as F {{and the level of}} probability of obtaining such a ratio is determined if the null hypothesis were to be true...|$|R
30|$|We {{compute the}} <b>sample</b> <b>means</b> (SMs) and mean squared errors (MSEs) of the {{estimates}} for the both Cases (i) and (ii).|$|R
3000|$|... {{where all}} the {{right-hand}} side variables are <b>sample</b> <b>means,</b> and the α, β, φ and γ are OLS estimated coefficient vectors.|$|R
25|$|The {{midpoint}} of the distribution (a+b)/2 is both the mean and the median of the uniform distribution. Although both the <b>sample</b> <b>mean</b> and the sample median are unbiased estimators of the midpoint, neither is as efficient as the sample mid-range, i.e. the arithmetic mean of the sample maximum and the sample minimum, which is the UMVU estimator of the midpoint (and also the maximum likelihood estimate).|$|E
25|$|The sample extrema can be {{used for}} a simple {{normality}} test, specifically of kurtosis: one computes the t-statistic of the sample maximum and minimum (subtracts <b>sample</b> <b>mean</b> and divides by the sample standard deviation), and if they are unusually large for the sample size (as per the three sigma rule and table therein, or more precisely a Student's t-distribution), then the kurtosis of the sample distribution deviates significantly from that of the normal distribution.|$|E
25|$|In most larger {{samplings}} of data, {{some data}} points will be {{further away from}} the <b>sample</b> <b>mean</b> than what is deemed reasonable. This can be due to incidental systematic error or flaws in the theory that generated an assumed family of probability distributions, {{or it may be}} that some observations are far {{from the center of the}} data. Outlier points can therefore indicate faulty data, erroneous procedures, or areas where a certain theory might not be valid. However, in large samples, a small number of outliers is to be expected (and not due to any anomalous condition).|$|E
3000|$|... {{where the}} θ’s are OLS {{estimated}} coefficients {{and all the}} right-hand side variables are <b>sample</b> <b>means</b> 8. Note that θ [...]...|$|R
40|$|This article {{develops}} nonparametric inference {{procedures for}} estimation and testing problems for means on manifolds. A {{central limit theorem}} for Frechet <b>sample</b> <b>means</b> is derived leading to an asymptotic distribution theory of intrinsic <b>sample</b> <b>means</b> on Riemannian manifolds. Central limit theorems are also obtained for extrinsic <b>sample</b> <b>means</b> w. r. t. an arbitrary embedding of a differentiable manifold in a Euclidean space. Bootstrap methods particularly suitable for these problems are presented. Applications are given to distributions on the sphere S^d (directional spaces), real projective space RP^N- 1 (axial spaces), complex projective space CP^k- 2 (planar shape spaces) w. r. t. Veronese-Whitney embeddings and a three-dimensional shape space Σ_ 3 ^ 4. Comment: Published at [URL] in the Annals of Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|R
2500|$|... as n {{tends to}} infinity, for any fixed [...] Therefore, the {{sequence}} Tn of <b>sample</b> <b>means</b> is consistent {{for the population}} meanμ.|$|R
25|$|Two {{hypothesis}} {{tests are}} particularly widely used. First, {{one wants to}} know if the estimated regression equation is any better than simply predicting that all values of the response variable equal its <b>sample</b> <b>mean</b> (if not, it is said to have no explanatory power). The null hypothesis of no explanatory value of the estimated regression is tested using an F-test. If the calculated F-value is found to be large enough to exceed its critical value for the pre-chosen level of significance, the null hypothesis is rejected and the alternative hypothesis, that the regression has explanatory power, is accepted. Otherwise, the null hypothesis of no explanatory power is accepted.|$|E
25|$|In principle, Monte Carlo {{methods can}} be used to solve any problem having a {{probabilistic}} interpretation. By the law of large numbers, integrals described by the expected value of some random variable can be approximated by taking the empirical mean (a.k.a. the <b>sample</b> <b>mean)</b> of independent samples of the variable. When the probability distribution of the variable is parametrized, mathematicians often use a Markov chain Monte Carlo (MCMC) sampler. The central idea is to design a judicious Markov chain model with a prescribed stationary probability distribution. That is, in the limit, the samples being generated by the MCMC method will be samples from the desired (target) distribution. By the ergodic theorem, the stationary distribution is approximated by the empirical measures of the random states of the MCMC sampler.|$|E
25|$|For {{practical}} purposes, different {{measures of}} location and dispersion are often compared {{on the basis}} of how well the corresponding population values can be estimated from a sample of data. The median, estimated using the sample median, has good properties in this regard. While it is not usually optimal if a given population distribution is assumed, its properties are always reasonably good. For example, a comparison of the efficiency of candidate estimators shows that the <b>sample</b> <b>mean</b> is more statistically efficient than the sample median when data are uncontaminated by data from heavy-tailed distributions or from mixtures of distributions, but less efficient otherwise, and that the efficiency of the sample median is higher than that {{for a wide range of}} distributions. More specifically, the median has a 64% efficiency compared to the minimum-variance mean (for large normal samples), which is to say the variance of the median will be ~50% greater than the variance of the mean—see asymptotic efficiency and references therein.|$|E
5000|$|... where [...] "i.i.d" [...] are {{independent}} and identically distributed random variables and N denotes the normal distribution. The two <b>sample</b> <b>means</b> are ...|$|R
3000|$|..., where i[*]is the {{treatment}} number and j[*]identifies the replicate. <b>Sample</b> <b>means</b> and <b>sample</b> standard deviations were used on estimated coefficients of variation [...]...|$|R
2500|$|The {{difference}} between the two <b>sample</b> <b>means,</b> each denoted by , which appears in the numerator for all the two-sample testing approaches discussed above, is ...|$|R
