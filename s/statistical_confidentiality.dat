32|21|Public
5000|$|For cases {{which are}} either complex or of general {{interest}} {{in the view of}} the Commission or the Member State concerned, the Commission (Eurostat) shall take a decision after consultation of the CMFB. The Commission (Eurostat) shall make decisions public, together with the opinion of the CMFB, without prejudice to the provisions relating to <b>statistical</b> <b>confidentiality</b> of Regulation (EC) No 322/97.|$|E
50|$|The {{agency was}} {{originally}} established as the National Statistical Service of Greece (Εθνική Στατιστική Υπηρεσία Ελλάδος) in 1956 by Legislative Decree 3627/1956. In 1986, Presidential Decree 224/1986 it {{was transformed into}} the General Secretariat of the National Statistical Service of Greece and {{became part of the}} Ministry of National Economy. Law 2392/1996 provided for the arrangement of issues concerning the access of the General Secretariat of the National Statistical Service of Greece to administrative sources and files, as well as <b>statistical</b> <b>confidentiality</b> issues.|$|E
50|$|Its work {{is subject}} to the Spanish Personal Data Protection Act 15/1999, which {{guarantees}} that Eustat does not facilitate any information that can identify or individualise the suppliers of the information, who are protected by <b>statistical</b> <b>confidentiality.</b> It coordinates two advisory bodies, the Basque Statistics Commission, where the Departments of the Basque Government, the provincial councils and the local councils as producers of statistics are represented, and the Basque Statistics Council, where all social agents and representatives of society also participate.|$|E
50|$|The Office of the Commissioner sets {{policy and}} {{standards}} for the center and oversees its operation, thus ensuring that <b>statistical</b> quality and <b>confidentiality</b> are maintained.|$|R
2500|$|The {{organisation}} has <b>statistical</b> excellence, integrity, <b>confidentiality</b> {{and data}} security, leading, connecting, and communicating as its values. It says on its website: [...] "By following these values, {{we aim to}} deliver accessible, relevant, and timely statistics for all New Zealanders.|$|R
50|$|Abowd's other {{research}} interests include network models for integrated labor market data; <b>statistical</b> methods for <b>confidentiality</b> protection of microdata, including the creation of synthetic data; international comparisons of labor market outcomes; executive compensation; bargaining and other wage-setting institutions; and the econometric tools of labor market analysis.|$|R
50|$|All data {{collected}} by the national statistical office must protect the privacy of individual respondents, whether persons or businesses. But on the contrary, government units such as institutions cannot invoke <b>statistical</b> <b>confidentiality.</b> All respondents have to be informed about the purpose and legal basis of the survey and especially about the confidentiality measures. The statistical office should not release any information that could identify an individual or group without prior consent. After data collection, replies should go back directly to the statistical producer, without involving any intermediary. Data processing implies that filled-in paper and electronic form with full names should be destroyed.|$|E
40|$|A {{microdata}} file is the set {{of answers}} obtained through a survey. For household surveys or population censuses, files of this type are hierarchical if the data on all the persons in the households sampled or enumerated are present. The statistical agencies ensure that this data remain anonymous by eliminating all possibility of identification. A number of countries produce hierarchical files of microdata from their censuses. Canada, however, does not. As a matter of fact, Canada has disseminated microdata files since 1971 but these files have always been designed without incorporating the complete hierarchy of households. We are assessing here the protection of <b>statistical</b> <b>confidentiality</b> for the 2001 Census, that is, we are assessing whether it is easy or very difficult to identify individuals from data alone. We use two measurements of <b>statistical</b> <b>confidentiality</b> protection: the conditional probability of uniqueness and the conditional probability of exact matches. We apply these two measurements {{to a set of}} records from the 1996 census for various groups of variables. On the basis of the results obtained, we conclude that publication of a public use hierarchical file significantly reduces the protection of <b>statistical</b> <b>confidentiality...</b>|$|E
40|$|Shortly {{after the}} USA Patriot Act was adopted and became {{law in the}} United States in October 2001 in {{response}} to the terrorist attacks of 9 / 11, Margo Anderson and I wrote a paper, “NCES and the Patriot Act ” (Seltzer and Anderson, 2002), that described how one section of that complex Act set aside the <b>statistical</b> <b>confidentiality</b> protections that hitherto had protected responses to statistical inquiries of the National Center for Education Statistics (NCES). The NCES is that part of the decentralized US federal statistical system located in the Department of Education and charged with gathering, compiling, and disseminating a wide variety of data pertaining to education at all levels. For a broader discussion of <b>statistical</b> <b>confidentiality</b> in the United States, see Anderso...|$|E
40|$|It is {{important}} to underline that since the nineties, National Institute of Statistics permanently relies in its activity {{on a set of}} fundamental principles included in important normative documents. We draw attention on the following stipulation of 1992 : “To carry out its attributions, the National Institute of Statistics has to provide, during the whole statistical research the registration and publication, protection measures for natural or legal persons interests which could be touched by revealing data regarding the patrimony and activity, according to the principles of <b>statistical</b> data <b>confidentiality</b> and to legal norms on classified information” (Government Ordinance no. 9 / 1992 regarding official statistics, approved with modifications and completions by the Law no. 11 / 1994, republished in Official Journal no. 763 /October 18, 2002) ...|$|R
40|$|Abstract. In {{recent work}} on <b>statistical</b> methods for <b>confidentiality</b> and {{disclosure}} limitation, Dobra and Fienberg (2000, 2003) and Dobra (2002) have generalized Bonferroni-Fréchet-Hoeffding bounds for cell entries in k-way contingency tables given marginal totals. In this paper, we consider extensions of their approach focused on {{upper and lower}} bounds for cell entries given arbitrary sets of marginals and conditionals. We give a complete characterization of the two-way table problem and discuss some implications to statistical disclosure limitation. In particular, we employ tools from computational algebra to describe the locus of all possible tables under the given constraints and discuss how this additional knowledge affects the disclosure...|$|R
40|$|To protect <b>confidentiality,</b> <b>statistical</b> {{agencies}} typically alter data before releasing {{them to the}} public. Ideally, although {{generally not}} done, the agency also provides a way for secondary data analysts to assess the quality of inferences obtained with the released data. Quality measures can help secondary data analysts to identify inaccurate conclusions resulting from the disclosure limitation procedures, as well as have confidence in accurate conclusions. We propose a framework for an interactive, web-based system that analysts can query for measures of inferential quality. As we illustrate, agencies seeking to build such systems must consider the additional disclosure risks from releasing quality measures. We suggest some avenues of research on limiting these risks. ...|$|R
40|$|The {{municipal}} register of inhabitants is a document which {{belongs to the}} respective City Council, fundamental to {{the management of the}} municipal population for the important consequences of the inclusion in it and by the large amount of information that it compulsorily collects. These characteristics make it become a source of information very requested by different agents, among them must be especially noted the rest of Public Administrations. The Supreme Court has flatly rejected that City Councils may refuse to give data included in the municipal population register to other Administrations that claim them in a legitimate exercise of its own powers, by application of the institution of <b>statistical</b> <b>confidentiality.</b> This article will attempt to analyze whether, indeed, the <b>statistical</b> <b>confidentiality</b> is absolutely irrelevant to the information included in {{municipal register}}s of inhabitants. </span...|$|E
40|$|Abstract. The {{roots of}} the modern concept of <b>{{statistical}}</b> <b>confidentiality</b> in the US federal statistical system can be traced directly back to {{the late nineteenth century}} efforts of statisticians to ensure full and accurate responses by businesses to statistical inquiries. Officials argued that such confidentiality guarantees were needed to ensure that the providers of enterprise and establishment data could be confident that the statistical agencies could not be forced to share their responses with others, such as regulatory or tax authorities, congressional investigators, prying journalists, and competitors, who might use this information {{to the detriment of the}} data provider. Nevertheless, over the years, the principle of <b>statistical</b> <b>confidentiality</b> with respect to information provided by businesses in statistical inquiries has been repeatedly challenged by other executive branch departments, independent regulatory agencies, the courts, Congress, and members of the public, with quite varied results. The paper uses the published record and archival research to examine the history of challenges to <b>statistical</b> <b>confidentiality,</b> and the responses of the statistical agencies, the federal statistical system as a whole, including the office of the chief statistician in OMB (and its predecessors), executive department and independent non-statistical agencies, the courts, and Congress as well as representatives of the business community. Long-term trends and the implications for maintaining and strengthening the confidentiality protections for establishment- and enterprise-level business data provided to federal agencies for statistical purposes are discussed...|$|E
40|$|The {{paper will}} {{introduce}} into the topic, describing {{the position of}} Statistical Disclosure Control (SDC) in the statistical production process and explaining {{the need for a}} standard tool for SDC in the European Statistical System and the benefits that can be expected. It will focus on the description of current best practice methods and tools in the field, explaining how they have been studied and were identified as such. The paper will outline in which way technology transfer for <b>statistical</b> <b>confidentiality</b> can be promoted by integration of these methods and tools into one software package, focussing on what is needed to create standard software for <b>statistical</b> <b>confidentiality.</b> It is a special objective of this paper, to explain, how the use of such a software within the European Statistical System will be promoted, as to hopefully achieve a successful, widespread transfer of technology...|$|E
40|$|One {{approach}} to limiting disclosure risk in public-use microdata is to release multiply-imputed, partially synthetic data sets. These are data on actual respondents, but with condential data replaced by multiply-imputed synthetic values. When imputing confidential values, a mis-specified model can invalidate inferences, because {{the distribution of}} synthetic data {{is determined by the}} model used to generate them. We present a practical method to generate synthetic values when the imputer has only limited information about the true data generating process. We combine a simple imputation model (such as regression) with a series of density-based transformations to pre- serve the distribution of the condential data, up to sampling error, on specied subdomains. We demonstrate through simulation and a large scale application that our approach preserves important statistical properties of the condential data, including higher moments, with low disclosure risk. <b>statistical</b> disclosure limitation, <b>confidentiality,</b> privacy, multiple imputation, partially synthetic data...|$|R
40|$|Cell {{suppression}} and audit {{programs have}} been used at the Census Bureau for many years for insuring the confidentiality of establishments that contribute data that are used for building economic magnitude data tables. Since the 1987 Economic Census, the suppression programs were based on network flow models which work well for 2 D tables but which have some drawbacks for higher dimensional tables. Linear programming (LP) based models now {{appear to be a}} practical option for higher dimensional tables and they do not have these same drawbacks. This paper describes work {{over the last two years}} in implementing these LP models, as well as some of the mathematical reasons for preferring the LP based models. Practical aspects of these programs are discussed; e. g., calculating capacities, refinement runs, backtracking, linked tables, frozen cells, and rounded data. A description of earlier work is also included, as are goals for future research. Key words: <b>statistical</b> disclosure control, <b>confidentiality,</b> cell suppression, audit programs, economic magnitude data, linear programming models, CPLE...|$|R
5000|$|The {{organisation}} has <b>statistical</b> excellence, integrity, <b>confidentiality</b> {{and data}} security, leading, connecting, and communicating as its values. It says on its website: [...] "By following these values, {{we aim to}} deliver accessible, relevant, and timely statistics for all New Zealanders.Confidentiality {{is not only a}} policy chosen by the department, but required by law. A section of the Statistics Act 1975 reads [...] "No information from an individual schedule is to be separately published or disclosed 37(3), except as authorised by the Statistics Act (the Act permits others to see information from an individual schedule, but only when it is in a form that prevents identification of the respondent concerned, and then only under strict security conditions). This means government can not abuse personal information from a census by revealing (or threatening to reveal it) for dubious purposes. For example a particular person's income responses can not be handed over to a tax collection agency.|$|R
40|$|A natural {{strategy}} {{to protect the}} confidentiality of individual data is to aggregate them at the lowest possible level. Some studies realised in Eurostat on this topic will be presented: properties of classifications in clusters of fixed sizes, micro-aggregation as a generic method to protect the confidentiality of individual data, application to the Community Innovation Survey. The work performed in Eurostat will be put in line with other projects conducted at European level {{on the topic of}} <b>statistical</b> <b>confidentiality...</b>|$|E
40|$|The safe {{dissemination}} of statistical tabular data {{is one of}} the main concerns of National Statistical Institutes (NSIs). Although each cell of the tables is made up of the aggregated information of several individuals, the <b>statistical</b> <b>confidentiality</b> can be violated. NSIs must guarantee that no individual information can be derived from the released tables. One widely used type of methods to reduce the disclosure risk is based on the perturbation of the cell values. We consider a new controlled perturbation method which, given a set of tables to be protected, finds the closest safe ones - thus reducing the information loss while preserving confidentiality. This approach means solving a quadratic optimization problem with a much larger number of variables than constraints. Real instances can provide problems with millions of variables. We show that interior-point methods are an effective choice for that model, and, also, that specialized algorithms which exploit the problem structure can be faster than state-of-the art general solvers. Computational results are presented for instances of up to 1000000 variables. Copyright Springer-Verlag Berlin/Heidelberg 2005 Interior-point methods, Quadratic Programming, Large-scale programming, <b>Statistical</b> <b>confidentiality,</b> Controlled perturbation methods,...|$|E
40|$|Confidentiality {{and data}} {{protection}} are the counterparts of the obligation for response and the cornerstone for confidence building between the national statistical {{services and the}} respondents. On this common principle the various national statistical systems among the 15 (and other) European states have been established. There is great concern at national and international level on <b>statistical</b> <b>confidentiality.</b> The respect of privacy has led national authorities to develop ad-hoc legislation which, inter alia, prevents national statistical institutes transmitting confidential data to Eurostat...|$|E
40|$|Objectives: To {{describe}} patient referral patterns at {{a family}} practice clinic. Design: A descriptive study. Subjects and Methods: The study was conducted at Family Practice Center of Aga Khan University Hospital, Karachi, Pakistan, in July 2003. A structured questionnaire was used. Patients were interviewed and their medical records were reviewed. The study investigators conducted the interview and review. Ethical requirements were ensured including written informed consent administration and assurance with regard to <b>confidentiality.</b> <b>Statistical</b> Package for Social Sciences (SPSS) software was used for data analysis. Results: Fifty patients and their records were reviewed. The majority was married and included more women than men, {{with a mean age}} of 35. 5 years. Most of the study subjects were well educated and were either housewives, in service or students. The majority of the referrals were to Obstetrics 2 ̆ 6 Gynecology, Dental and Ophthalmology clinics. The referral form, which explained the reasons for referral was filled for all subjects. Ongoing problem was managed and return appointment given in forty (80...|$|R
40|$|AbstractBased on some {{relevant}} references, {{this article}} presents some important tools to evaluate the managing diversity in cross-cultural project teams. The approach used {{is based on a}} market survey results. For data collections were used four questionnaires. The four questionnaires are used in order to characterize: intercultural competence, communication style, the emotional intelligence level and the potential role that a person could play when he/she is part of a team. In the research were used five variables: age, nationality, residence (urban/rural), high school graduate and profile. The context of the research was defined by a collaborative and cross-cultural project team {{that took place in the}} University of Limoges (France) in 2013. All the questionnaires were applied on a sample of 125 subjects. All subjects worked in collaborative activities or cross-cultural projects. From the beginning, all the subjects were informed about the research context and the data <b>confidentiality.</b> <b>Statistical</b> processing will be performed with the specific version of Microsoft Office Excel 2003 and Sphinx Plus 2 - Lexica Edition - V 5...|$|R
40|$|Traditional <b>statistical</b> {{methods for}} <b>confidentiality</b> {{protection}} of <b>statistical</b> databases {{do not scale}} well to deal with GWAS (genome-wide association studies) databases espe-cially in terms of guarantees regarding protection from linkage to external information. The more recent concept of differential privacy, introduced by the cryptographic com-munity, is an approach which provides a rigorous definition of privacy with meaningful privacy guarantees {{in the presence of}} arbitrary external information, although the guar-antees come at a serious price in terms of data utility. Building on such notions, we propose new methods to release aggregate GWAS data without compromising an in-dividual’s privacy. We present methods for releasing differentially private minor allele frequencies, chi-square statistics and p-values. We compare these approaches on sim-ulated data and on a GWAS study of canine hair length involving 685 dogs. We also propose a privacy-preserving method for finding genome-wide associations based on a differentially-private approach to penalized logistic regression. Key Words: chi-squared statistics; contingency tables; differential privacy; genome-wide association studies (GWAS); logistic regression; p-values; single nucleotide polymorphism (SNP). ...|$|R
40|$|When {{publishing}} data containing o#cial statistics, a need {{to preserve}} <b>statistical</b> <b>confidentiality</b> arises. Statistical disclosure of individuals' data must be prevented. There is a wide choice of techniques to achieve this anonymization: data perturbation, data suppression, etc. In this paper, we tackle the problem of using anonymized data to compute exact statistics; the goal is for a classified level (statistical institute) {{to be able to}} retrieve statistics computed by an unclassified level (external contractor) on disclosure-protected macrodata. Our approach is based on privacy homomorphisms, especially on a recent one...|$|E
40|$|When {{publishing}} contingency tables {{which contain}} o#cial statistics, {{a need to}} preserve <b>statistical</b> <b>confidentiality</b> arises. Statistical disclosure of individual units must be prevented. There is a wide choice of techniques to achieve this anonymization: cell suppression, cell perturbation, etc. In this paper, we tackle the problem of using anonymized data to compute exact statistics; our approach is based on privacy homomorphisms, which are encryption transformations such that the decryption of a function of ciphers is a (possibly di#erent) function of the corresponding clear messages. A new privacy homomorphism is presented and combined with some anonymization techniques, {{in order for a}} classified level to retrieve exact statistics from statistics computed on disclosure-protected data at an unclassified level...|$|E
40|$|In {{this paper}} {{we will give}} an {{overview}} of the 5 framework CASC (Computational Aspects of <b>Statistical</b> <b>Confidentiality)</b> project and concentrate on the ARGUS software. This CASC-project {{can be seen as a}} follow up of the 4 Framework SDC-project. However, the main emphasis is more on building practical tools. The further development of the ARGUS-software will play a central role in this project. Besides this software development, several research topics have been included in the CASC-project. These research topics, both for the disclosure control of microdata as well as tabular data, aim at obtaining practical results that might be implemented in future version of ARGUS and find its way to the end-users...|$|E
40|$|Recent {{advances}} in technology dramatically increase the volume of data that statistical agencies can gather and disseminate. The improved accessibility translates into {{a higher risk of}} identifying individuals from public microdata, and therefore increases the importance of the evaluation of disclosure risk and confidentiality control. This dissertation addresses three related but distinct research questions in <b>statistical</b> data <b>confidentiality.</b> The first study concerns the evaluation of disclosure risk for microdata when an intruder attempts to identify survey respondents by linking data records with a large external commercial data file based on a set of common variables. The dependence of disclosure risk to the commercial data coverage, the accuracy of the common identification information, and the amount of identification information to which an intruder accesses, is discussed theoretically and empirically tested using an experiment. The second study presents a practical implementation of fully-imputed synthetic data approach for a large, complex longitudinal survey as means of protecting confidentiality, following the initial proposal by Rubin (1993) and Little (1993). The imputation uses separate semiparametric algorithms for continuous, binary and categorical variables. A new combining rule of synthetic data inference is proposed to account for the uncertainty due to simultaneously imputing item-missing data and generating synthetic data. The loss of data utility is evaluated via the use of a propensity score approach in addition to three information loss metrics. The third study extends this fully-synthetic data approach to cope with situations where small area statistics are essential important. This research is the first in the statistical disclosure control literature to consider small area statistics. The goal is to create synthetic data with enough geographical details to permit small area analyses, which otherwise is impossible because such geographical identifiers are usually suppressed due to disclosure control. A Bayesian framework for appropriate small area models is proposed to generate synthetic microdata from the predictive posterior distributions. Two simulation studies and one empirical illustration are used to evaluate this approach...|$|R
40|$|When intense {{redaction}} {{is needed}} to protect the confidentiality of data subjects' identities and sensitive attributes, statistical agencies can use synthetic data approaches. To create synthetic data, the agency replaces identifying or sensitive values with draws from statistical models estimated from the confidential data. Many agencies are reluctant to implement this idea because (i) {{the quality of the}} generated data depends strongly {{on the quality of the}} underlying models, and (ii) developing effective synthesis models can be a labor-intensive and difficult task. Recently, there have been suggestions that agencies use nonparametric methods from the machine learning literature to generate synthetic data. These methods can estimate non-linear relationships that might otherwise be missed and can be run with minimal tuning, thus considerably reducing burdens on the agency. Four synthesizers based on machine learning algorithms-classification and regression trees, bagging, random forests, and support vector machines-are evaluated in terms of their potential to preserve analytical validity while reducing disclosure risks. The evaluation is based on a repeated sampling simulation with a subset of the 2002 Uganda census public use sample data. The simulation suggests that synthesizers based on regression trees can result in synthetic datasets that provide reliable estimates and low disclosure risks, and that these synthesizers can be implemented easily by <b>statistical</b> agencies. Census <b>Confidentiality</b> Disclosure Imputation Microdata Synthetic...|$|R
40|$|Cell {{suppression}} is {{a widely}} used technique for protecting sensitive information in statistical data presented in tabular form. Previous works on the subject mainly concentrate on 2 - and 3 -dimensional tables whose entries are subject to marginal totals. In this paper we {{address the problem of}} protecting sensitive data in a statistical table whose entries are linked by a generic system of linear constraints. This very general setting covers, among others, k-dimensional tables with marginals as well as the so-called hierarchical and linked tables that are very often used nowadays for disseminating statistical data. In particular, we address the optimization problem known in the literature as the (secondary) Cell Suppression Problem, in which the information loss due to suppression has to be minimized. We introduce a new integer linear programming model and outline an enumerative algorithm for its exact solution. The algorithm can also be used as a heuristic procedure to find near-optimal solutions. Extensive computational results on a test-bed of 1, 160 real-world and randomly generated instances are presented, showing the effectiveness of the approach. In particular, we were able to solve to proven optimality 4 -dimensional tables with marginals as well as linked tables of reasonable size (to our knowledge, tables of this kind were never solved optimally by previous authors). <b>Statistical</b> Disclosure Control, <b>Confidentiality,</b> Cell Suppression, Integer Linear Programming, Tabular Data, Branch-and-Cut Algorithms...|$|R
40|$|Data {{shuffling}} is {{a recently}} proposed technique for masking numerical data where the confidential values are shuffled between records while maintaining all monotonic {{relationships between the}} variables in the data set. Data shuffling {{is based on the}} multivariate normal copula which assumes that there is no tail dependence in the data set. In many practical situations, however, tail dependence plays a crucial role in decision making. Hence, it is desirable that the data masking procedure be capable of preserving tail dependence when present. In this study, we provide a new data shuffling approach based on t copulas that is capable of maintaining tail dependence in the masked data in a large number of applications. <b>Statistical</b> <b>confidentiality</b> Copulas Data shuffling Disclosure risk Data dissemination...|$|E
40|$|Achieving {{data and}} {{information}} dissemination without arming anyone is a central task of any entity in charge of collecting data. In this article, the authors examine the literature on data and <b>statistical</b> <b>confidentiality.</b> Rather than comparing the theoretical properties of specific methods, they emphasize the main themes that emerge from the ongoing discussion among scientists regarding how best to achieve the appropriate balance between data protection, data utility, and data dissemination. They cover the literature on de-identification and reidentification methods with emphasis on health care data. The authors also discuss the benefits and limitations for the most common access methods. Although there is abundant theoretical and empirical research, their review reveals lack of consensus on fundamental questions for empirical practice: How to assess disclosure risk, how to choose among disclosure methods, how to assess reidentification risk, and how to measure utility loss. ...|$|E
40|$|Dissemination of {{statistics}} on the web raises opportunities as well as challenges to <b>statistical</b> <b>confidentiality.</b> Last year Statistics Norway (SN) opened its Statistics Bank (SBN) on the web giving users {{the opportunity to make}} their own tables by aggregating from detailed base tables at a disaggregated geographical level. The architecture of such web publication structures puts restrictions on which disclosure protection methods are relevant. In 2002, SN experimentally developed a rounding method to make it possible to disseminate frequency count tables from the Census 2001 in SBN in a safe manner and in accordance with the rules of the Norwegian Statistics Code. The experiment was successful enough to be applied to the census publication and the results inspire further development of the method. The paper describes the method and why existing methods and software was not used. Ideas for improvements are outlined...|$|E
40|$|Empirical {{research}} using micro data via remote access has been advocated in recent time by <b>statistical</b> offices since <b>confidentiality</b> is easier warranted for this approach. However, disclosure of single values and units cannot be completely avoided. Binary regressors (dummy variables) bear {{a high risk}} of disclosure, especially if their interactions are considered as it is done by definition in saturated models. However, contrary to views expressed in earlier publications the risk is only existing if besides parameter estimates also predicted values are reported to the researcher. The paper considers saturated specifications of the most popular linear and nonlinear microeconometric models and shows that in all cases the disclosure risk is high if some design points are represented by a (very) small number of observations. For two of the models not belonging to the exponential family (probit model and negative binomial regression model) we show that the same estimates of the conditional expectations arise here although the parameter estimates are defined by a modified equation. In the last section we draw {{attention to the fact that}} interaction of binary regressors can be used to construct strategic dummy variableswhich lead to hight disclosure risk as shown, for example, in Bleninger et al. (2010) for the linear model. In this paper we extend the analysis to the set of established nonlinear models, in particular logit, probit and count data models...|$|R
40|$|AbstractThe {{protection}} of privacy of individual-level information in genome-wide association study (GWAS) databases {{has been a major}} concern of researchers following the publication of “an attack” on GWAS data by Homer et al. (2008). Traditional <b>statistical</b> methods for <b>confidentiality</b> and privacy {{protection of}} statistical databases do not scale well to deal with GWAS data, especially in terms of guarantees regarding protection from linkage to external information. The more recent concept of differential privacy, introduced by the cryptographic community, is an approach that provides a rigorous definition of privacy with meaningful privacy guarantees in the presence of arbitrary external information, although the guarantees may come at a serious price in terms of data utility. Building on such notions, Uhler et al. (2013) proposed new methods to release aggregate GWAS data without compromising an individual’s privacy. We extend the methods developed in Uhler et al. (2013) for releasing differentially-private χ 2 -statistics by allowing for arbitrary number of cases and controls, and for releasing differentially-private allelic test statistics. We also provide a new interpretation by assuming the controls’ data are known, which is a realistic assumption because some GWAS use publicly available data as controls. We assess the performance of the proposed methods through a risk-utility analysis on a real data set consisting of DNA samples collected by the Wellcome Trust Case Control Consortium and compare the methods with the differentially-private release mechanism proposed by Johnson and Shmatikov (2013) ...|$|R
40|$|AIM: Corneal {{diseases}} {{constitute a}} significant cause {{of visual impairment}} and blindness in the developing world. The aim {{of the study was}} to assess the knowledge and attitude of medical students on eye donation. METHODS: A cross-sectional descriptive study was conducted among first year medical students in the month of December 2012. Universal sampling was employed as the method of sampling. All first year students which were present at the time of study were included as the study subjects. Total sample size for the study was 138. A pre-tested semi-structured questionnaire was used for conducting the study which was administered to each of the study participants after obtaining their informed consent. Utmost care was taken to maintain privacy and <b>confidentiality.</b> <b>Statistical</b> analysis was done using SPSS version 17. Frequency distributions were calculated for all the variables. RESULTS: Majority of the medical students 130 (94. 2 %) were aware about eye donation. Television was the most common source of awareness identified for eye donation by 73 (56. 2 %) subjects. Lack of awareness was the most common reason cited by 47 (34. 1 %) students for restriction of eye donation. CONCLUSION: Though majority of the medical students were aware about eye donation there were many gaps identified in their knowledge which can be bridged only by building health promotion strategies to clear their misconceptions about eye donation [TAF Prev Med Bull 2014; 13 (4. 000) : 295 - 300...|$|R
