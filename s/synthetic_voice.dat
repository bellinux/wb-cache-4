132|109|Public
25|$|Other hurdles exist when {{allowing}} the robot to use voice for interacting with humans. For social reasons, <b>synthetic</b> <b>voice</b> proves suboptimal as a communication medium, making {{it necessary to}} develop the emotional component of robotic voice through various techniques.|$|E
2500|$|Williams' first {{solid-state}} machines {{produced in}} 1976 were prototype runs based on electromechanical games; Aztec (1976) and Grand Prix. [...] Williams continued to release new electromechanical pinball machines through October 1977, when they released their last, Wild Card. [...] From November 1977, Williams released solid-state pinball games exclusively, beginning {{with their first}} solid state production model Hot Tip (1977), which sold 4,903 units (the electromechanical version previously released in June sold 1,300 units). [...] From the late 1970s through the 1980s, Williams released numerous innovative pinball games, such as Gorgar (1979, the first pinball featuring a <b>synthetic</b> <b>voice),</b> Firepower (1980), Black Knight (1980), Jungle Lord (1981), Space Shuttle (1984), Comet (1985), High Speed (1986), Pin*Bot (1986), F-14 Tomcat (1987), Cyclone (1988) and Taxi (1988).|$|E
5000|$|NFL player Steve Gleason had {{his voice}} cloned by CereProc {{following}} his diagnosis with MND. Gleason appeared in Microsoft's Super Bowl XLVIII commercial praising {{the power of}} technology, using his <b>synthetic</b> <b>voice</b> to narrate.|$|E
40|$|Speech {{synthesis}} {{models are}} typically built from a corpus of speech that has accurate transcriptions. However, {{many of the}} languages of the world {{do not have a}} standardized writing system. This paper is an initial attempt at building <b>synthetic</b> <b>voices</b> for such languages. It may seem useless to develop a text-to-speech system when there is no text available. But we will discuss some well defined use cases where we need these models. We will present our method to build <b>synthetic</b> <b>voices</b> from only speech data. We will present experimental results and oracle studies that show that we can automatically devise an artificial writing system for these languages, and build <b>synthetic</b> <b>voices</b> that are understandable and usable...|$|R
40|$|Conventional <b>synthetic</b> <b>voices</b> can {{synthesise}} neutral {{read aloud}} speech well. But, to make synthetic speech {{more suitable for}} {{a wider range of}} applications, the voices need to express more than just the word identity. We need to develop voices that can partake in a conversation and express, e. g. agreement, disagreement, hesitation, in a natural and believable manner. In speech synthesis there are currently two dominating frameworks: unit selection and HMM-based speech synthesis. Both frameworks utilise recordings of human speech to build <b>synthetic</b> <b>voices.</b> Despite the fact that the content of the recordings determines the segmental and prosodic phenomena that can be synthesised, surprisingly little research has been made on utilising the corpus to extend the limited behaviour of conventional <b>synthetic</b> <b>voices.</b> In this thesis we will show how natural sounding conversational characteristics can be added to both unit selection and HMM-based <b>synthetic</b> <b>voices,</b> by adding speech from a spontaneous conversation to the voices. We recorded a spontaneous conversation, and by manually transcribing and selecting utterances we obtained approximately two thousand utterances from it. These conversational utterances were rich in conversational speech phenomena, but they lacked the general coverage that allows unit selection and HMM-based synthesis techniques to synthesise high quality speech. Therefore we investigated a number of blending approaches in the <b>synthetic</b> <b>voices,</b> where the conversational utterances were augmented with conventional read aloud speech. The <b>synthetic</b> <b>voices</b> that contained conversational speech were contrasted with conventional voices without conversational speech. The perceptual evaluations showed that the conversational voices were generally perceived by listeners as having a more conversational style than the conventional voices. This conversational style was largely due to the conversational voices’ ability to synthesise utterances that contained conversational speech phenomena in a more natural manner than the conventional voices. Additionally, we conducted an experiment that showed that natural sounding conversational characteristics in synthetic speech can convey pragmatic information, in our case an impression of certainty or uncertainty, about a topic to a listener. The conclusion drawn is that the limited behaviour of conventional <b>synthetic</b> <b>voices</b> can be enriched by utilising conversational speech in both unit selection and HMM-based speech synthesis...|$|R
5000|$|Flite, Festival Speech Synthesis System and in {{particular}} kal_diphone (Kevin A Lenzo) made from his voice, and FestVox for building <b>synthetic</b> <b>voices</b> ...|$|R
5000|$|Design {{elements}} should {{match in}} human realism. A robot may look uncanny when human and nonhuman elements are mixed. For example, both a robot with a <b>synthetic</b> <b>voice</b> or {{a human being}} with a human voice {{have been found to}} be less eerie than a robot with a human voice or a human being with a <b>synthetic</b> <b>voice.</b> For a robot to give a more positive impression, its degree of human realism in appearance should also match its degree of human realism in behavior. If an animated character looks more human than its movement, this gives a negative impression. Human neuroimaging studies also indicate matching appearance and motion kinematics are important.|$|E
50|$|Other hurdles exist when {{allowing}} the robot to use voice for interacting with humans. For social reasons, <b>synthetic</b> <b>voice</b> proves suboptimal as a communication medium, making {{it necessary to}} develop the emotional component of robotic voice through various techniques.|$|E
50|$|In 2005 Dolphin worked {{alongside}} BT and the National Library for the Blind {{to develop}} a prototype <b>synthetic</b> <b>voice</b> application enabling books, magazines and newspapers to be converted into audio format, thus enabling them to be read by visually impaired computer users.|$|E
50|$|The company {{primarily}} produces <b>synthetic</b> <b>voices</b> {{to be used}} in telephony systems, mobile applications, desktop applications, {{and with}} other TTS software such as open-source Festival.|$|R
40|$|With {{most human}} {{languages}} having less than 1 million speakers {{it is unlikely}} that standard commercial systems will be able to justify supporting the vast majority of so-called "minority " languages. In our continuing task of providing tools for building <b>synthetic</b> <b>voices</b> in currently unsupported languages, this paper describes a number of experiments in building <b>synthetic</b> <b>voices</b> without requiring specific phonetic knowledge of the target languages. Even when a language is well studied defining an appropriate phoneme set is never easy. The work presented here shows the adequacy of unit selection synthesis techniques when no explicit phoneme set is available...|$|R
40|$|This paper {{illustrates}} {{the importance of}} various cognitive factors involved in perceiving and comprehending synthetic speech. It includes findings drawn from the relative psychological and psycholinguistic literature together with experimental results obtained at the Fondazione Ugo Bordoni laboratory. Overall, it is shown that listening to and comprehending <b>synthetic</b> <b>voices</b> is more difficult than with a natural voice. However, and more importantly, this difficulty can and does decrease with the subjects' exposure to said <b>synthetic</b> <b>voices.</b> Furthermore, greater workload demands are associated with synthetic speech and subjects listening to synthetic passages are required {{to pay more attention}} than those listening to natural passages. ...|$|R
50|$|A visitor {{walking to}} the left of the Theme Center on the Avenue of Patriots would visit the Communications and Business Systems focal exhibits. At the AT&T Pavilion the Voder, a mechanized, <b>synthetic</b> <b>voice,</b> spoke to attendees, foretelling the {{widespread}} use of electronic voices decades later.|$|E
5000|$|... "A soaPOPera for Laptops/ Mac Minis", 1997-2005(with Peter Sinclair) Four {{computers}} {{talk to one}} another, using <b>synthetic</b> <b>voice</b> {{and voice}} recognition software. During performances the computers are mounted on robot vehicles. They {{interact with each other}} and the human performers. When a guitar is played the computers sing along. In installation format the computers gossip with each other.|$|E
5000|$|Level 3 Failures: red warnings, {{situations}} that require immediate crew action and that place the flight in danger. For example, an engine fire {{or loss of}} cabin pressure. They are enunciated with a red master warning light, a warning (red) ECAM message and a continuous repetitive chime or a specific sound or a <b>synthetic</b> <b>voice.</b> The chime can be silenced by pressing the master warning push button.|$|E
5000|$|Festvox is a {{suite of}} tools by Alan W. Black and Kevin Lenzo for {{building}} <b>synthetic</b> <b>voices</b> for Festival. It includes a step-by-step tutorial with examples in document called [...] "Building Synthetic Voices".|$|R
50|$|The Festvox project aims to {{make the}} {{building}} of new <b>synthetic</b> <b>voices</b> more systematic and better documented, {{making it possible for}} anyone to build a new voice. It is distributed under a free software license similar to the MIT License.|$|R
40|$|This article {{presents}} an experiment {{that aims to}} determine whether {{blind and visually impaired}} people would accept the implementation of text-to-speech in the audio description of dubbed feature films in the Catalan context. A user study was conducted with 67 blind and partially sighted people who assessed two <b>synthetic</b> <b>voices</b> when applied to audio description, as compared to two natural voices. All of the voices had been previously selected in a preliminary test. The analysis of the data (both quantitative and qualitative) concludes that most participants accept Catalan text-to-speech audio description as an alternative solution to the standard human-voiced audio description. However, natural voices obtain statistically higher scores than <b>synthetic</b> <b>voices</b> and are still the preferred solution...|$|R
5000|$|Once {{her voice}} {{circuits}} are activated, The Machine provides spoken {{feedback to the}} player {{on his or her}} shots. The Machines voice is provided by Chicago-based singer Stephanie Rogers. [...] Due to the sexual overtones in some of her speech, the game includes a [...] "modesty" [...] setting that prevents some clips from being played. The original <b>synthetic</b> <b>voice</b> of Pin-Bot is also featured in the game.|$|E
50|$|In 1977 the NFB {{directed}} the final field {{trials of the}} first reading machine, developed by Ray Kurzweil. The machine weighed 80 pounds and cost $50,000. The machine used 50 bits per word and could store 750,000 bits of information. It used a camera to scan 15 characters per second and was programmed with the rules that govern spoken English. From this it produced the word with a <b>synthetic</b> <b>voice.</b>|$|E
5000|$|In 1979 Sharp {{released}} the world first quartz-based talking clock, the Talking Time CT-660E (German version CT-660G). Its silver transistor-radio-like case contained complex LSI circuitry with 3 SMD ICs (likely clock CPU, speech CPU and sound IC), producing a Speak&Spell-like <b>synthetic</b> <b>voice.</b> At the front rim {{was a small}} LCD. The alarm spoke the time and also had a melody [...] "Boccherini's Minuet"; after 5 minutes the alarm repeated with the words [...] "Please hurry!". It also had stopwatch and countdown timer modes. The tiny controls to turn off alarm or set functions are hard to reach under a small bottom lid.|$|E
30|$|Building a new {{general-purpose}} (non-limited domain) unit selection {{voice in}} a new language from scratch includes a huge overhead of data preparation, which includes preparing phonetically balanced sentences, recording them from a professional speaker in various speaking styles and emotions in a noise-free environment, and manually segmenting or correcting the automatic segmentation errors. All of it is time consuming, laborious, and expensive, and it restricts rapid building of <b>synthetic</b> <b>voices.</b> A free database such as CMU ARCTIC [4] has largely helped to rapidly build <b>synthetic</b> <b>voices</b> in the English language. But CMU ARCTIC is a small database, contains only a few speakers data, and is not prosodically rich (contains short declarative utterances only). Today, (1) {{a large amount of}} audio data has become available on the web in the form of audiobooks, podcasts, video lectures, video blogs, news bulletins, etc, and (2) thanks to technology, we can effortlessly record and store large amounts of high-quality single speaker audio such as lecture, impromptu, or read speech. Unlike CMU ARCTIC, these data are rich in prosody and provide a plethora of voices to choose from, and their use can significantly ease the overhead of data preparation thus allowing to rapidly build general-purpose natural-sounding <b>synthetic</b> <b>voices.</b>|$|R
40|$|This paper {{introduces}} the VoiceText text-to-speech system developed by Voiceware. By means of corpus based concatenative speech synthesis technique, we built high quality <b>synthetic</b> <b>voices</b> using the dataset {{provided for the}} Blizzard challenge 2007. The evaluation results show that VoiceText achieved high performances in both naturalness and intelligibility of synthesized speech. 1...|$|R
40|$|The ModelTalker speech {{synthesis}} system creates individualized <b>synthetic</b> <b>voices</b> for augmentative communication devices. To emulate {{the voice of}} an individual, that person records an inventory of words and phrases. The recorded inventory is then converted to a speech database {{that is used by}} ModelTalker. The system presently can work with augmentative communicatio...|$|R
5000|$|Before {{long the}} shine wore off. In 1988, Slonimsky wrote: Like many a futuristic contraption, the Rhythmicon was {{wonderful}} in every respect, {{except that it}} did not work. It was not until forty years later that an electronic instrument with similar specifications was constructed at Stanford University. It could do everything that Cowell and Theremin had wanted it to do and more, but it lacked the emotional quality essential to music. It sounded sterile, antiseptic, lifeless [...] - [...] like a robot with a <b>synthetic</b> <b>voice.</b> Cowell soon left the Rhythmicon behind to pursue other interests and it was all but forgotten for many years.|$|E
5000|$|Williams' first {{solid-state}} machines {{produced in}} 1976 were prototype runs based on electromechanical games; Aztec (1976) and Grand Prix. [...] Williams continued to release new electromechanical pinball machines through October 1977, when they released their last, Wild Card. [...] From November 1977, Williams released solid-state pinball games exclusively, beginning {{with their first}} solid state production model Hot Tip (1977), which sold 4,903 units (the electromechanical version previously released in June sold 1,300 units). [...] From the late 1970s through the 1980s, Williams released numerous innovative pinball games, such as Gorgar (1979, the first pinball featuring a <b>synthetic</b> <b>voice),</b> Firepower (1980), Black Knight (1980), Jungle Lord (1981), Space Shuttle (1984), Comet (1985), High Speed (1986), Pin*Bot (1986), F-14 Tomcat (1987), Cyclone (1988) and Taxi (1988).|$|E
5000|$|... "Automaton" [...] {{was written}} and {{produced}} by Jamiroquai members Matt Johnson and Jay Kay. Kay also provided vocals for the track. It is a disco, funk, synthpop, and dance-pop song that has drawn several comparisons to {{the works of}} French electronic music duo Daft Punk. The song features an instrumental that contains synthesisers which have been compared with those from the soundtrack for the 1982 film Tron, as well as vocals that feature a vocoder effect. Near {{the end of the}} song, Kay engages in a rap. Both Billboard and PopMatters compared his rap verse to the work of Afrika Bambaataa. [...] "Automaton" [...] was described as a [...] "sci-fi wonderland of vocoders and electronic whirs" [...] by Ben Kaye of Consequence of Sound. Daniel Kreps of Rolling Stone described the song's progression as [...] "propulsive verses that suddenly make way for the track's booming chorus before abruptly turning into a Daft Punk-like bridge where a <b>synthetic</b> <b>voice</b> chants 'I'm automaton.'" ...|$|E
40|$|Three {{real-time}} gesture controlled vocal {{instruments are}} presented. They {{are based on}} a time domain (LF) and a spectral domain (CALM) model of the glottal pulse signal. Gestural control is able to add expression to the <b>synthetic</b> <b>voices,</b> enabling simulation of various vocal behaviors. Expressive vocal instruments are demonstrated for musical and research purposes. 1...|$|R
40|$|In this paper, we {{describe}} the design and collection of corpora for diphone synthesis, the voice building process, and our experience {{in the creation of}} a new, publically available database of ten diphone sets of one American English speaker for the Festival Speech Synthesis System [3], using the FestVox document and tools [1]. In support of our goal to make the tools and techniques available for anyone to build their own <b>synthetic</b> <b>voices,</b> we have generalized and streamlined the tasks involved from what were once arcane anecdotes, half-written one-off scripts, and partial descriptions, to detailed, complete instructions that others have followed with good results. 1. INTRODUCTION The FestVox [1] document is a growing, publically available resource that contains tools, data, and text about building complete <b>synthetic</b> <b>voices</b> in English and other languages. That work covers everything from building text analyzers, lexicons, prosodic models as well as various waveform synthesis technique [...] ...|$|R
40|$|In {{this project}} a text-to-speech (TTS) HMM-based speech system (HTS) {{has been used}} to create {{emotional}} synthetic speech in Spanish. Nowadays the <b>synthetic</b> <b>voices</b> have high quality, but this is not enough, they must be able to capture the natural expressiveness of the human speech. Giving this expressiveness to the <b>synthetic</b> <b>voices</b> will lead to a much more natural voice, that is the goal of these systems. To achieve this, both male and female voices will be used and two different techniques will be applied: dependent models and average voice models with adaptation. In this TTS system diffeerent vocoders can be used. For this project GlottHMM has been used and then three perceptual test have been carried out to compare it with STRAIGHT vocoder. The results of the perceptual tests shows that STRAIGHT is very robust and that GlottHMM is not yet at its level regarding the emotional speech synthesis...|$|R
5000|$|Building on the {{recommendations}} of the University of Padua, by applying the technique of so-called diphones (the union of a consonant and a vowel, 150 in total for the Italian) the group created the first speech synthesizer with high intelligibility in 1975 [...] it was called MUSA (MUltichannel Speaking Automaton), which demonstrated what was possible with the technology of the time. The results achieved in those years were condensed into an audio disc at 45 rpm, with thousands of copies produced and spread through the mass communication media. It was mainly the Italian version of the song Frère Jacques carried out in polyphony with more singing voices (MUSA could manage up to 8 synthesis channels in parallel). The evolution of this prototype, with {{the increase in the number}} of diphones (about 1000), the refinement of the tools of linguistic analysis and better waveform management led to a marked improvement of the <b>synthetic</b> <b>voice.</b> This led to the creation of the integrated circuit [...] "voice synthesizer" [...] developed internally in CSELT which was added to the SGS (catalog as Zilog's Z80 microprocessor's peripheral (with the code M8950).|$|E
5000|$|Although both {{pilots were}} based in Zürich and the CVR picks up Lutz's query to Loehrer about Loehrer's {{familiarity}} with [...] "the 28 approach", which Loehrer confirmed he had, Lutz put the plane into an overly-steep descent that brought flight 3597 to MDA far too soon. When Loehrer reported the plane reaching 100 feet above MDA, the CVR records Lutz asking Loehrer, [...] "Do we have ground contact?" [...] Loehrer hesitated before replying [...] "Yes". However, flight simulators programmed {{with the time}} of day, terrain, and weather Lutz was facing at that time allowed investigators to determine that the only ground Lutz or Loehrer {{could see was the}} ground of the hilly terrain over which the plane was flying. Upon reaching MDA of 2400 ft, Lutz declared that he had [...] "ground contact" [...] and would continue on, then deliberately descended the plane below the minimum descent altitude (MDA) without having the required visual contact with either the approach lights or the runway, a major piloting error that ultimately led directly to the crash. The fact that Loehrer made no attempt to prevent the continuation of the flight below the minimum descent altitude also directly contributed to the crash. Lutz made an additional error by not monitoring his Distance Measuring Equipment (DME) as he made his approach; the CVR recorded Lutz's running narrative on nearly every move he made in the cockpit, but did not record any readout of the DME after a check, verified by Loehrer, at [...] from runway 28. Moments before the crash, Lutz's running commentary indicated to investigators that Lutz must have thought he was at or near [...] from runway 28 because he said [...] "Someone said he saw the runway late here". Instead, Lutz was over [...] from the runway, and could not possibly have seen the runway due to the presence of a hill, below the MDA of 2400 ft, that would have obscured his view. It was into this hill that Flight 3597 eventually crashed. Just before the crash, the <b>synthetic</b> <b>voice</b> of the ground proximity warning system (GPWS) announced the radio altimeter reading 500 feet above ground. Immediately thereafter Lutz exclaimed [...] "*****, two miles he said, he sees the runway". A few seconds later Lutz said [...] "Two thousand" [...] and then one second later the <b>synthetic</b> <b>voice</b> gave the [...] "minimums" [...] GPWS message, which was triggered by the radio altimeter reading at 300 feet. Even though Lutz finally realized that his inability to see the runway meant he needed to initiate a missed approach maneuver (called a [...] "Go-around"), his call for the go-around came too late; the plane's engines were not able to spool up fast enough to generate sufficient thrust to climb above the hill that had been obstructing his view, and it crashed into the hilltop at 22:06 CET.|$|E
5000|$|A kinect {{camera is}} used to move the data objects around in space. The work carries on from earlier net.art works that {{challenge}} the web browser {{and the idea of}} framing and layout. The work also challenges the corporatization of the internet. I wrote an html parser to get around the MoMA’s web page layout and grab the current exhibitions images. These are made public on the internet. I wanted to lay them out in my own design. This questions the browser and web page metaphor in general. It’s a hangover from print media and it’s the importing of a static media form into a dynamic information space. I chose to allow the viewer to immerse themselves in the information space and interact with the information through their body movement and eyes. The viewer also hears the RSS feed from an art blog Hyperallergic being read by a <b>synthetic</b> <b>voice.</b> I laid out a live weather feed {{in the form of a}} text circle that forms a ring outside of the ring of images produce from MoMA. The Kinect camera’s infrared video is mapped onto the cylinder in the center putting the viewer in the work. Sofy Yuditskaya collaborated with me on the code to bring this work to fruition. She took my basic code and refined it and added her own touches. That’s the thing about writing code, you often get stuck and need someone to suggest a different approach or to help finish the project. This work is called Post Browser and is an alternative approach to information access.|$|E
40|$|Spontaneous {{conversational}} speech has many characteristics {{that are currently}} not well modelled in unit selection and HMM-based speech synthesis. But {{in order to build}} <b>synthetic</b> <b>voices</b> more suitable for interaction we need data that exhibits more conversational characteristics than the generally used read aloud sentences. In this paper we will show how carefully selected utterances from a spontaneous conversation was instrumental for building an HMM-based <b>synthetic</b> <b>voices</b> with more natural sounding conversational characteristics than a voice based on carefully read aloud sentences. We also investigated a style blending technique as a solution to the inherent problem of phonetic coverage in spontaneous speech data. But the lack of an appropriate representation of spontaneous speech phenomena probably contributed to results showing that we could not yet compete with the speech quality achieved for grammatical sentences. Index Terms: HMM, speech synthesis, spontaneous, conversation, lexical fillers, filled pause...|$|R
40|$|Recently, {{the use of}} phoneme class-conditional probabilities as {{features}} (posterior features) for template-based ASR {{has been}} proposed. These features {{have been found to}} generalize well to unseen data and yield better systems than standard spectral-based features. In this paper, motivated by the high quality of current text-to-speech systems and the robustness of posterior features toward undesired variability, we investigate the use of synthetic speech to generate reference templates. The use of synthetic speech in template-based ASR not only allows to ad-dress the issue of in-domain data collection but also expansion of vocabulary. Using 75 - and 600 -word task-independent and speaker-independent setup on Phonebook database, we investi-gate different <b>synthetic</b> <b>voices</b> produced by the Festival HTS-based synthesizer trained on CMU ARCTIC databases. Our study shows that synthetic speech templates can yield perfor-mance comparable to the natural speech templates, especially with <b>synthetic</b> <b>voices</b> that have high intelligibility...|$|R
40|$|Security systems {{relying on}} voice {{identification}} can {{be threatened by}} human <b>voice</b> imitation or <b>synthetic</b> <b>voices.</b> As voice conversion {{can be seen as}} a sort of voice imitation, this paper analyses the performance of an automatic speaker identification system by using converted voices in order to know how vulnerable such systems are to this kind of disguise. The experiments are conducted by using intra-gender and cross-gender conversions between two males and two females. The results show that, in general terms, the system is more robust to intra-gender converted voices than to cross-gender ones...|$|R
