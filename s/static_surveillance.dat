19|19|Public
5000|$|Sampath {{is noted}} {{to be the}} first CEC to have taken {{far-reaching}} steps in expenditure management. Among these steps include <b>static</b> <b>surveillance</b> squads to monitor election expenditure of candidates, organizing expenditure observers during elections and maintaining shadow accounts for candidates. Importantly, Sampath issued the [...] "Transparency guidelines" [...] for political parties, thereby making it mandatory for them to file semi-annual returns of their balance sheets & and income statements. These steps go a long way towards ensuring free and fair elections in India.|$|E
40|$|We {{propose a}} {{non-parametric}} model for pedestrian motion based on Gaussian Process regression, in which trajectory data are modelled by regressing relative motion against current position. We {{show how the}} underlying model can be learned in an unsupervised fashion, demonstrating this on two databases collected from <b>static</b> <b>surveillance</b> cameras. We furthermore exemplify the use of model for prediction, comparing the recently proposed GP-Bayesfilters with a Monte Carlo method. We illustrate the benefit of this approach for long term motion prediction where parametric models such as Kalman Filters would perform poorly. 1...|$|E
40|$|This thesis {{addresses}} the automatic calibration of two <b>static</b> <b>surveillance</b> cameras in a manmade world with orthogonal and parallel structures {{and a common}} ground plane. An approach is taken where the calibration of the interior orientation, the undistortion of the lens and the calibration of a camera’s rotation to the world perform before calibrating the camera centers, which allows methods that work in slightly overlapping as in non-overlapping views. We present a new incremental calibration composed of Expectation Maximization and Simulated Annealing that uses the uncertainties of noisy line segments to process a video stream instead of a single image. The advantage of video is that orthogonal and parallel edg...|$|E
5000|$|Working to the ISTAR (Intelligence, Surveillance, Target Acquisition and Reconnaissance) {{group in}} each of the brigades, the STA Patrols Troop are subject matter experts on <b>Static</b> Covert <b>Surveillance</b> (SCS) and {{complement}} the reconnaissance activity conducted by other Ground Manned Reconnaissance (GMR) forces in the Brigade. The patrols' training includes: ...|$|R
40|$|Abstract — Human-robot {{interaction}} approaches like face detection, face recognition, pedestrian detection {{are widely}} known in robotics field; however often they lead to performance problems. Additionally, false positive and false negative problems are commonly associated to bad illumination and strong featured images. Moreover background segmentation approaches are frequently used {{to solve this}} problem on <b>static</b> camera <b>surveillance.</b> Though all these approaches are unable to effectively deal with the constant background changes that certainly happens when the camera sensor is installed on a mobile robot. Hence, in this work we propose a stereo vision dynamic background segmentation solution to this problem. I...|$|R
30|$|Use of node {{mobility}} {{in mobile}} sensor networks for relocation after initial random placement was previously suggested in [11, 12]. However, in their models, nodes only make a one-time movement {{to achieve a}} better (uniform) coverage. Using mobile nodes as data collection points (sinks) in sensor networks was studied by [13 – 15]. Liu et al. in [7] showed that the coverage can be improved by a mobile sensor network with continuous mobility over the time, compared to that with a <b>static</b> network. <b>Surveillance</b> coverage of mobile sensor networks under Brownian motion random node mobility model was addressed in [16]. Managing mobile node mobility in target tracking applications in mobile sensor networks is addressed in [17].|$|R
40|$|The {{capability}} to track individuals in CCTV cameras {{is important for}} e. g. surveillance applications at large areas such as train stations, airports and shopping centers. However, it is laborious to track and trace people over multiple cameras. In this paper, we present a system for real-time tracking and fast interactive retrieval of persons in video streams from multiple <b>static</b> <b>surveillance</b> cameras. This system is demonstrated in a shopping mall, where the cameras are positioned without overlapping fields-of-view and have different lighting conditions. The {{results show that the}} system allows an operator to find the origin or destination of a person more efficiently. The misses are reduced with 37 %, which is a significant improvement...|$|E
40|$|A {{method for}} {{estimating}} {{the dimensions of}} non-delimited free parking areas by using a <b>static</b> <b>surveillance</b> camera is proposed. The proposed method is specially designed to tackle the main challenges of urban scenarios (multiple moving objects, outdoor illumination conditions and occlusions between vehicles) with no training. The core of this work is the temporal analysis of the video frames to detect the occupancy variation of the parking areas. Two techniques are combined: background subtraction using a mixture of Gaussians to detect and track vehicles {{and the creation of}} a transience map to detect the parking and leaving of vehicles. The authors demonstrate that the proposed method yields satisfactory estimates on three real scenarios while being a low computational cost solution that can be applied in any kind of parking area covered by a single camera...|$|E
40|$|This paper {{presents}} {{a system to}} transmit the information from a <b>static</b> <b>surveillance</b> camera in an adaptive way, from low to higher bit-rate, based on the on-line generation of descriptions. The proposed system {{is based on a}} server/client model: the server is placed in the surveillance area and the client is placed in a user side. The server analyzes the video sequence to detect the regions of activity (motion analysis) and the corresponding descriptions (mainly MPEG- 7 moving regions) are generated together with the textures of moving regions and the associated background image. Depending on the available bandwidth, different levels of transmission are specified, ranging from just sending the descriptions generated to a transmission with all the associated images corresponding to the moving objects and background. Index Terms − Surveillance, MPEG- 7, video analysis, object extraction, adaptive video transmission. 1...|$|E
40|$|International audienceThe {{well known}} HOG (Histogram of Oriented Gradients) of Dalal and Triggs is {{commonly}} used for pedestrian detection from 2 d moving embedded cameras (driving assistance) or <b>static</b> cameras (video <b>surveillance).</b> In this paper we show how to use and improve the HOG detector in the UAV context. In order to increase the elevation angular robustness we propose to use a more appropriate training dataset and sliding windows. We show results on synthetic images...|$|R
40|$|Abstract — Interactive mobile robots require object/subject {{detection}} in very visually complex environments. In {{the field}} of computer vision, specially when applied to robotics, several approaches like face detection, face recognition and pedestrian detection often {{have to deal with}} issues associated to bad illumination and strong featured background. These issues imply lack of performance because human detection algorithms will frequently process the whole image searching for features. Also, background segmentation approaches are commonly used to solve this problem on <b>static</b> camera <b>surveillance.</b> However all these approaches are unable to effectively deal with the constant background changes that certainly happen when the camera sensor is installed on a mobile robot. Hence, in this work we propose a Horopter based Dynamic Background Segmentation solution to this problem. Results show that our approach, significantly enhanced tracking, and consequently improved movement classification towards interaction. I...|$|R
40|$|We {{propose a}} novel {{framework}} for large-scale scene understanding in <b>static</b> camera <b>surveillance.</b> Our techniques combine fast rank- 1 constrained robust PCA {{to compute the}} foreground, with non-parametric Bayesian models for inference. Clusters are extracted in foreground patterns using a joint multinomial+Gaussian Dirichlet process model (DPM). Since the multinomial distribution is normalized, the Gaussian mixture distinguishes between similar spatial patterns but different activity levels (eg. car vs bike). We propose a modification of the decayed MCMC technique for incremental inference, providing the ability to discover theoretically unlimited patterns in unbounded video streams. A promising by-product of our framework is online, abnormal activity detection. A benchmark video and two surveillance videos, with the longest being 140 hours long are used in our experiments. The patterns discovered are as informative as existing scene understanding algorithms. However, unlike existing work, we achieve near real-time execution and encouraging performance in abnormal activity detection...|$|R
40|$|International audienceIn {{surveillance}} applications human {{operators are}} either {{confronted with a}} high cognitive load or monotonic time periods where the operator's attention rapidly decreases. Therefore, automatic high-level interpretation of image sequences gains increasing importance in assisting human operators. We present a generic hierarchical system that generates high-level logic-based situation descriptions in various domains. The system consists of two components. First, a vision component provides 3 D spatial and temporal information about objects in scenes. Second, the situation recognition component uses knowledge encoded in Situation Graph Trees and a fuzzy graph traversal allowing exhaustive situation awareness. The system is tested with real video data comprising persons, their actions, and interactions. In order to show the domain independence we used recorded data from moving vehicles and <b>static</b> <b>surveillance</b> cameras. The {{results show that the}} system is usable with multi modal data and can easily be modified and extended...|$|E
40|$|This paper {{presents}} the results of analysing the effect of different motion segmentation techniques in a system that transmits the information captured by a <b>static</b> <b>surveillance</b> camera in an adaptative way based on the on-line generation of descriptions and their descriptions at different levels of detail. The video sequences are analyzed to detect the regions of activity (motion analysis) and to differentiate them from the background, and the corresponding descriptions (mainly MPEG- 7 moving regions) are generated together with the textures of the moving regions and the associated background image. Depending on the available bandwidth, different levels of transmission are specified, ranging from just sending the descriptions generated to a transmission with all the associated images corresponding to the moving objects and background. We study the effect of three motion segmentation algorithms in several aspects such as accurate segmentation, size of the descriptions generated, computational efficiency and reconstructed data quality. ...|$|E
40|$|A novel {{approach}} to extract target motion descriptors in multi-camera video surveillance systems is presented. Using two <b>static</b> <b>surveillance</b> cameras with partially overlapped {{field of view}} (FOV), control points (unique points from each camera) are identified in regions of interest (ROI) from both cameras footage. The control points within the ROI are matched for correspondence and a meshed Euclidean distance based signature is computed. A depth map is estimated using disparity of each control pair and the ROI is graded into number of regions {{with the help of}} relative depth information of the control points. The graded regions of different depths will help calculate accurately the pace of the moving target and also its 3 D location. The advantage of estimating a depth map for background static control points over depth map of the target itself is its accuracy and robustness to outliers. The performance of the algorithm is evaluated in the paper using several test sequences. Implementation issues of the algorithm onto the TI DaVinci DM 6446 platform are considered in the paper. 1...|$|E
40|$|Abstract. The {{performance}} of a detector depends much on its training dataset and drops significantly when the detector is applied to a new scene due to the large variations between the source training dataset and the target scene. In order to bridge this appearance gap, we pro-pose a deep model to automatically learn scene-specific features and visual patterns in <b>static</b> video <b>surveillance</b> without any manual labels from the target scene. It jointly learns a scene-specific classifier {{and the distribution of}} the target samples. Both tasks share multi-scale feature representations with both discriminative and representative power. We also propose a cluster layer in the deep model that utilizes the scene-specific visual patterns for pedestrian detection. Our specifically designed objective function not only incorporates the confidence scores of target training samples but also automatically weights the importance of source training samples by fitting the marginal distributions of target samples. It significantly improves the detection rates at 1 FPPI by 10 % compared with the state-of-the-art domain adaptation methods on MIT Traffic Dataset and CUHK Square Dataset. ...|$|R
5000|$|An STA Patrol {{comprises}} {{a team of}} four/six specialist soldiers, {{the role}} of STA Patrols is to conduct high risk <b>static</b> covert <b>surveillance</b> at long range and {{in close proximity to}} the enemy. The patrols are trained and equipped both to collect highly granular information and intelligence and to deliver joint effects at range; be they kinetic (all patrols contain personnel trained in the delivery of precision and indirect fires) or non-kinetic. A pre-requisite of service in the Patrols is successful completion of the STA Patrol Course and qualification as a Special Observer. Training emphasises mental and physical resilience and a high premium is placed on well developed self-reliance and self-discipline. Patrols are trained with a variety of skills to mitigate the dangers of operating in a high risk environment and/or isolated circumstances. Unlike most Army Reserve units, who are only required to train at up to sub-unit (company or squadron) level, the HAC is required to train as a regiment. [...] Its Permanent Staff Instructors are drawn from across the British Armed Forces.|$|R
40|$|This paper {{deals with}} the {{selection}} of relevant motion from multi-object movement. The proposed method {{is based on a}} multi-scale approach using features extracted from optical flow and global rarity quantification to compute bottom-up saliency maps. It shows good results from four objects to dense crowds with increasing performance. The results are convincing on synthetic videos, simple real video movements, a pedestrian database and they seem promising on very complex videos with dense crowds. This algorithm only uses motion features (direction and speed) but can be easily generalized to other dynamic or <b>static</b> features. Video <b>surveillance,</b> social signal processing and, in general, higher level scene understanding can benefit from this method. Index Terms — crowd analysis, social signa...|$|R
40|$|When people {{observe and}} {{interact}} with physical spaces, {{they are able to}} associate functionality to regions in the environment. Our goal is to automate dense functional understanding of large spaces by leveraging sparse activity demonstrations recorded from an ego-centric viewpoint. The method we describe enables functionality estimation in large scenes where people have behaved, as well as novel scenes where no behaviors are observed. Our method learns and predicts "Action Maps", which encode the ability for a user to perform activities at various locations. With the usage of an egocentric camera to observe human activities, our method scales with the size of the scene without the need for mounting multiple <b>static</b> <b>surveillance</b> cameras and is well-suited to the task of observing activities up-close. We demonstrate that by capturing appearance-based attributes of the environment and associating these attributes with activity demonstrations, our proposed mathematical framework allows for the prediction of Action Maps in new environments. Additionally, we offer a preliminary glance of the applicability of Action Maps by demonstrating a proof-of-concept application in which they are used in concert with activity detections to perform localization. Comment: To appear at CVPR 201...|$|E
40|$|Proceedings of the 26 th Picture Coding Symposium, PCS 2007, Lisbon, Portugal, November 2007 This paper {{presents}} {{a system to}} transmit the information from a <b>static</b> <b>surveillance</b> camera in an adaptive way, from low to higher bit-rate, based on the on-line generation of descriptions. The proposed system {{is based on a}} server/client model: the server is placed in the surveillance area and the client is placed in a user side. The server analyzes the video sequence to detect the regions of activity (motion analysis) and the corresponding descriptions (mainly MPEG- 7 moving regions) are generated together with the textures of moving regions and the associated background image. Depending on the available bandwidth, different levels of transmission are specified, ranging from just sending the descriptions generated to a transmission with all the associated images corresponding to the moving objects and background. This work is partially supported by Cátedra Infoglobal-UAM para Nuevas Tecnologías de video aplicadas a la seguridad. This work is also supported by the Ministerio de Ciencia y Tecnología of the Spanish Government under project TIN 2004 - 07860 (MEDUSA) and by the Comunidad de Madrid under project P-TIC- 0223 - 0505 (PROMULTIDIS) ...|$|E
40|$|Abstract — Real time moving object {{detection}} and tracking {{is one of}} the important research fields that have gained a lot of attention in the last few years. Cameras installed around us but there are no means to monitor all of them continuously. It is necessary to develop technologies that automatically process those images in order to detect problematic situations or unusual behavior of human. Design computer vision base automated video surveillance system addresses real-time observation of people within a busy environment leading to the description of their actions and interactions. Tracking is required for security, safety and site management. Object detection by background subtraction technique. Using single camera detect track human behavior. Background subtraction is the process of separating out the foreground objects from the background in a sequence of video frames. If human entity is cross the line design security in mall or public area the object is tracked. It is laborious to track and trace people over multiple cameras. In this paper, we present a system for real-time tracking and fast interactive retrieval of persons in video streams from multiple <b>static</b> <b>surveillance</b> cameras. The system when realizes the human entry, it is processed in a second and the alert is produced for the security purpose. Keywords—: Human {{detection and}} tracing, background subtraction, video streams...|$|E
40|$|We {{present a}} novel {{framework}} for learning patterns of motion and sizes {{of objects in}} <b>static</b> camera <b>surveillance.</b> The proposed method provides a new higher-level layer to the traditional surveillance pipeline for anomalous event detection and scene model feedback. Pixel level probability density functions (pdfs) of appearance {{have been used for}} background modelling in the past, but modelling pixel level pdfs of object speed and size from the tracks is novel. Each pdf is modelled as a multivariate Gaussian Mixture Model (GMM) of the motion (destination location & transition time) and the size (width & height) parameters of the objects at that location. Output of the tracking module is used to perform unsupervised EM-based learning of every GMM. We have successfully used the proposed scene model to detect local as well as global anomalies in object tracks. We also show the use of this scene model to improve object detection through pixel-level parameter feedback of the minimum object size and background learning rate. Most object path modelling approaches first cluster the tracks into major paths in the scene, which can be a source of error. We avoid this by building local pdfs that capture a variety of tracks which are passing through them. Qualitative and quantitative analysis of actual surveillance videos proved the effectiveness of the proposed approach. 1...|$|R
40|$|H. 264 /advanced video coding {{surveillance}} video encoders use the Skip mode {{specified by}} the standard to reduce bandwidth. They also use multiple frames as reference for motion-compensated prediction. In this paper, we propose two techniques to reduce the bandwidth and computational cost of <b>static</b> camera <b>surveillance</b> video encoders without affecting detection and recognition performance. A spatial sampler is proposed to sample pixels that are segmented using a Gaussian mixture model. Modified weight updates are derived for {{the parameters of the}} mixture model to reduce floating point computations. A storage pattern of the parameters in memory is also modified to improve cache performance. Skip selection is performed using the segmentation results of the sampled pixels. The second contribution is a low computational cost algorithm to choose the reference frames. The proposed reference frame selection algorithm reduces the cost of coding uncovered background regions. We also study the number of reference frames required to achieve good coding efficiency. Distortion over foreground pixels is measured to quantify the performance of the proposed techniques. Experimental results show bit rate savings of up to 94. 5 % over methods proposed in literature on video surveillance data sets. The proposed techniques also provide up to 74. 5 % reduction in compression complexity without increasing the distortion over the foreground regions in the video sequence...|$|R
40|$|This paper aims at {{breaking}} {{new ground}} in modeling and estimation of recording noise of surveillance video for {{further development of}} new techniques to restore video images. In order to tackle the video denoising problem with non-stationary image contents and various noise sources, a critical task is estimation of varieties of noise in video signals. The estimation {{is based on a}} new general integrated surveillance video noise model (GISVNM), which integrates all typical realistic noise models, including signal independent noise model and signal dependent model to model behaviors of Poisson, additive and impulse noises. In particular, the parameters of the Poison and Gaussian based noise models are estimated by using spatial-temporal noise characteristics of the <b>static</b> background of <b>surveillance</b> video, and the parameters of the impulse model are estimated by geometric properties based on spatial characteristics of the video. The experiments showed promising results obtained using the proposed techniques...|$|R
40|$|Studying {{free-standing}} conversational groups (FCGs) in unstructured {{social settings}} (e. g., cocktail party) is gratifying {{due to the}} wealth of information available at the group (mining social networks) and individual (recognizing native behavioral and personality traits) levels. However, analyzing social scenes involving FCGs is also highly challenging due to the difficulty in extracting behavioral cues such as target locations, their speaking activity and head/body pose due to crowdedness and presence of extreme occlusions. To this end, we propose SALSA, a novel dataset facilitating multimodal and Synergetic sociAL Scene Analysis, and make two main contributions to research on automated social interaction analysis: (1) SALSA records social interactions among 18 participants in a natural, indoor environment for over 60 minutes, under the poster presentation and cocktail party contexts presenting difficulties {{in the form of}} low-resolution images, lighting variations, numerous occlusions, reverberations and interfering sound sources; (2) To alleviate these problems we facilitate multimodal analysis by recording the social interplay using four <b>static</b> <b>surveillance</b> cameras and sociometric badges worn by each participant, comprising the microphone, accelerometer, bluetooth and infrared sensors. In addition to raw data, we also provide annotations concerning individuals' personality as well as their position, head, body orientation and F-formation information over the entire event duration. Through extensive experiments with state-of-the-art approaches, we show (a) the limitations of current methods and (b) how the recorded multiple cues synergetically aid automatic analysis of social interactions. SALSA is available at [URL] 14 pages, 11 figure...|$|E
40|$|Personal {{use of this}} {{material}} is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing {{this material}} for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. J. C. San Miguel, and J. M. Martínez, "On the effect of motion segmentation techniques in description based adaptive video transmission", in AVSS ' 07 : Proceedings of the 2007 IEEE Conference on Advanced Video and Signal Based Surveillance, 2007, p. 359 - 364 This paper {{presents the results of}} analysing the effect of different motion segmentation techniques in a system that transmits the information captured by a <b>static</b> <b>surveillance</b> camera in an adaptative way based on the on-line generation of descriptions and their descriptions at different levels of detail. The video sequences are analyzed to detect the regions of activity (motion analysis) and to differentiate them from the background, and the corresponding descriptions (mainly MPEG- 7 moving regions) are generated together with the textures of the moving regions and the associated background image. Depending on the available bandwidth, different levels of transmission are specified, ranging from just sending the descriptions generated to a transmission with all the associated images corresponding to the moving objects and background. We study the effect of three motion segmentation algorithms in several aspects such as accurate segmentation, size of the descriptions generated, computational efficiency and reconstructed data quality. This work is partially supported by Cátedra Infoglobal-UAM para Nuevas Tecnologías de video aplicadas a la seguridad. This work is also supported by the Ministerio de Ciencia y Tecnología of the Spanish Government under project TIN 2004 - 07860 (MEDUSA) and by the Comunidad de Madrid under project P-TIC- 0223 - 0505 (PROMULTIDIS) ...|$|E
40|$|Dynamically {{changing}} background (dynamic background) still {{presents a}} great challenge to many motion-based video surveillance systems. In the context of event detection, {{it is a major}} source of false alarms. There is a strong need from the security industry either to detect and suppress these false alarms, or dampen the effects of background changes, so as to increase the sensitivity to meaningful events of interest. In this paper, we restrict our focus {{to one of the most}} common causes of dynamic background changes: (1) that of swaying tree branches and (2) their shadows under windy conditions. Considering the ultimate goal in a video analytics pipeline, we formulate a new dynamic background detection problem as a signal processing alternative to the previously described but unreliable computer vision-based approaches. Within this new framework, we directly reduce the number of false alarms by testing if the detected events are due to characteristic background motions. In addition, we introduce a new data set suitable for the evaluation of dynamic background detection. It consists of real-world events detected by a commercial surveillance system from two <b>static</b> <b>surveillance</b> cameras. The research question we address is whether dynamic background can be detected reliably and efficiently using simple motion features and in the presence of similar but meaningful events, such as loitering. Inspired by the tree aerodynamics theory, we propose a novel method named local variation persistence (LVP), that captures the key characteristics of swaying motions. The method is posed as a convex optimization problem, whose variable is the local variation. We derive a computationally efficient algorithm for solving the optimization problem, the solution of which is then used to form a powerful detection statistic. On our newly collected data set, we demonstrate that the proposed LVP achieves excellent detection results and outperforms the best alternative adapted from existing art - n the dynamic background literature...|$|E
40|$|Abstract — Understanding and {{analysing}} {{video data}} from <b>static</b> or mobile <b>surveillance</b> cameras often requires {{knowledge of the}} scene and the camera placement. In this article, we provide a way to simplify the user’s task of understanding the scene by rendering the camera view as if observed from the user’s perspective by estimating his position using a real-time visual SLAM system. Augmenting the view {{is referred to as}} hidden view synthesis. Compared to previous work, the current approach improves by simplifying the setup and requiring minimal user input. This is achieved by building a map of the environment using a visual SLAM system and then registering the surveillance camera in this map. By exploiting the map, a different moving camera can render hidden views in real-time at 30 Hz. We discuss some of the challenges remaining for full automation. Results are shown in an indoor environment for surveillance applications and outdoors with application to improved safety in transport. I...|$|R
40|$|Abstract—The {{performance}} of a generic pedestrian detector may drop significantly when it is applied to a specific scene due to the mismatch between the source training set and samples from the target scene. We propose a new approach of automatically transferring a generic pedestrian detector to a scene-specific detector in <b>static</b> video <b>surveillance</b> without manually labeling samples from the target scene. The proposed transfer learning framework consists of four steps. (1) Through exploring the indegrees from target samples to source samples on a visual affinity graph, the source samples are weighted to match the distribution of target samples. (2) It explores a set of context cues to automatically select samples from the target scene, predicts their labels, and computes confidence scores to guide transfer learning. (3) The confidence scores propagate among target samples according to their underlying visual structures. (4) Target samples with higher confidence scores have larger influence on training scene-specific detectors. All these considerations are formulated under a single objective function called Confidence-Encoded SVM, which avoids hard thresholding on confidence scores. During test, only the appearance-based detector is used without context cues. The effectiveness is demonstrated through experiments on two video surveillance datasets. Compared with a generic detector, it improves the detection rates by 48 % and 36 % at one false positive per image (FPPI) on the two datasets respectively. The training process converges after one or two iterations on the datasets in experiments. Index Terms—Pedestrian detection, transfer learning, confidence-encoded SVM, domain adaptation, video surveillance F...|$|R
40|$|This paper {{presents}} an abandoned item and illegally parked vehicle detection method for single <b>static</b> camera video <b>surveillance</b> applications. By processing the input video at different frame rates, two backgrounds are constructed; one for short-term {{and another for}} long-term. Each of these backgrounds {{is defined as a}} mixture of Gaussian models, which are adapted using online Bayesian update. Two binary foreground maps are estimated by comparing the current frame with the backgrounds, and motion statics are aggregated in a likelihood image by applying a set of heuristics to the foreground maps. Likelihood image is then used to differentiate between the pixels that belong to moving objects, temporarily static regions and scene background. Depending on the application, the temporary static regions indicate abandoned items, illegally parked vehicles, objects removed from the scene, etc. The presented pixel-wise method does not require object tracking, thus its performance is not upper-bounded to error prone detection and correspondence tasks that usually fail for crowded scenes. It accurately segments objects even if they are fully occluded. It can also be effectively implemented on a parallel processing architecture...|$|R
40|$|UnrestrictedOur goal is {{to detect}} and to track {{multiple}} moving vehicles observed from <b>static</b> <b>surveillance</b> cameras, which are usually placed on poles or buildings. Methods of background subtraction are widely used {{in these kinds of}} conditions. But to extract vehicle information from motion foreground, common difficulties, such as noise foreground, shadow, scene occlusion, blob merge and blob split, have to be solved. By using vehicle shape models, in addition to camera calibration and ground plane knowledge, the proposed methods can detect, track and classify moving vehicles in the presence of all these difficulties.; Two methods are proposed in this thesis to deal with related problems. The first method uses dynamic background model to extract the motion foreground. The models of camera and vehicle are used to reduce the foreground noise. Spatial and temporal constraints are applied to handle blob split, and object color appearance is used to track each vehicle when multiple vehicles are merged together. Evaluation on a large dataset by a third party shows that this method works robustly under many conditions.; The second method focuses on challenging tracking situations where vehicle inter-occlusion is prevalent and persistent. In this case, each foreground blob can contain multiple vehicles. Simple one-to-one correspondence between the foreground blobs and vehicles does not apply any more. Segmentation of the merged vehicles is a difficult problem. This proposed method works in the framework of Markov chain Monte Carlo (MCMC) approach. By sampling in the multi-vehicle configuration space, the method searches for the set of vehicle parameters, that best explains the foreground. Several bottom-up detections are utilized with top-down analysis to guide the sampling in an effective way.; The goal of this work is to infer the trajectory of each individual vehicle. Because of the approximation of vehicle models and the limitation of the likelihood function, the multi-vehicle configuration with the highest probability may not always be the correct segmentation. By exploring the spatial and temporal constraints across the image sequences, a tracking method is proposed to reduce the errors on single frame vehicle detection...|$|E
40|$|Indiana University-Purdue University Indianapolis (IUPUI) Surveillance videos are {{recorded}} continually and the retrieval of such videos currently still relies on human operators. Automatic retrieval has not reached a satisfactory accuracy. As an intermediate representation, this work develops multiple original temporal profiles of video to convey accurate temporal {{information in the}} video while keeping certain spatial characteristics. These are effective methods to visualizes surveillance video contents efficiently in a 2 D temporal image, suitable for indexing and retrieving a large video database. We are aiming to provide a compact index that is intuitive and preserves {{most of the information}} in the video in order to avoid browsing extensive video clips frame by frame. By considering some of the properties of <b>static</b> <b>surveillance</b> videos, we aim at accentuating the temporal dimension in our visualization. We have introduced our framework as three unique methods that visualize different aspects of a surveillance video, plus an extension to non-static surveillance videos. In our first method "Localized Temporal Profile", by knowing that most surveillance videos are monitoring specific locations, we try to emphasize the other dimension, time, in our solution. we focus on describing all the events only in critical locations of the video. In our next method "Multi-Position Temporal Profile", we generate an all-inclusive profile that covers all the events in the video field of view. In our last method "Motion Temporal Profile" we perform in-depth analysis of scene motion and try to handle targets with non-uniform, non-translational motion in our temporal profile. We then further extend our framework by loosening the constraint that the video is static and including cameras with smooth panning motion as such videos are widely used in practice. By performing motion analysis on the camera, we stabilize the camera to create a panorama-like effect for the video, allowing us to utilize all of the aforementioned methods. The resulting profiles allows temporal indexing to each video frame, and contains all spatial information in a continuous manner. It also shows the actions and progress of events in the temporal profile. Flexible browsing and effective manipulation of videos can be achieved using the resulting video profiles...|$|E
40|$|Surveillance videos are {{recorded}} continually and the retrieval of such videos currently still relies on human operators. Automatic retrieval has not reached a satisfactory accuracy. As an intermediate representation, this work develops multiple original temporal profiles of video to convey accurate temporal {{information in the}} video while keeping certain spatial characteristics. These are effective methods to visualizes surveillance video contents efficiently in a 2 D temporal image, suitable for indexing and retrieving a large video database. We are aiming to provide a compact index that is intuitive and preserves {{most of the information}} in the video in order to avoid browsing extensive video clips frame by frame. ^ By considering some of the properties of <b>static</b> <b>surveillance</b> videos, we aim at accentuating the temporal dimension in our visualization. We have introduced our framework as three unique methods that visualize different aspects of a surveillance video, plus an extension to non-static surveillance videos. ^ In our first method 2 ̆ 2 Localized Temporal Profile 2 ̆ 2, by knowing that most surveillance videos are monitoring specific locations, we try to emphasize the other dimension, time, in our solution. we focus on describing all the events only in critical locations of the video. In our next method 2 ̆ 2 Multi-Position Temporal Profile 2 ̆ 2, we generate an all-inclusive profile that covers all the events in the video field of view. In our last method 2 ̆ 2 Motion Temporal Profile 2 ̆ 2 we perform in-depth analysis of scene motion and try to handle targets with non-uniform, non-translational motion in our temporal profile. We then further extend our framework by loosening the constraint that the video is static and including cameras with smooth panning motion as such videos are widely used in practice. By performing motion analysis on the camera, we stabilize the camera to create a panorama-like effect for the video, allowing us to utilize all of the aforementioned methods. The resulting profiles allows temporal indexing to each video frame, and contains all spatial information in a continuous manner. It also shows the actions and progress of events in the temporal profile. Flexible browsing and effective manipulation of videos can be achieved using the resulting video profiles. ...|$|E
40|$|Static intra-access {{pressure}} ratio does not correlate with access blood flow. BackgroundAccess flow (Qa) measurement is recommended by Kidney Disease Outcomes Quality Initiative (K/DOQI) as the preferred method for access <b>surveillance.</b> <b>Static</b> intra-access {{pressure ratio}} (SIAPR) measurement {{is the second}} surveillance method of choice. The purpose of this prospective multicenter {{study was to investigate}} the relationship between SIAPR and Qa and to examine the premise upon which SIAPR surveillance is based—namely, that high SIAPR is a surrogate for low Qa associated with hemodynamically significant stenosis. MethodsSIAPR and Qa (HD 01; Transonic Systems, Inc., Ithaca, NY, USA) were simultaneously measured monthly in 242 patients [146 prosthetic arteriovenous bridge grafts (AVG), 96 autogenous arteriovenous fistulas (AVF) ] from three centers. SIAPR was measured according to the K/DOQI protocol. ResultsThere was no correlation between Qa and venous or arterial SIAPR in AVGs (R 2 = 0. 0037 and R 2 = 0. 006, respectively, N = 730), or in AVFs (R 2 = 0. 0247 and R 2 = 0. 0329, respectively, N = 431). Of the high SIAPR measurements in AVGs, 81 % and 50 % were associated with Qa ≥ 600 and Qa ≥ 1000 mL/min, respectively. Of the AVGs studied, 41 % (60 / 146) had consistently high Qa ≥ 1000 mL/min. Seventy percent (42 / 60) of these high-Qa AVGs had at least two consecutive sessions with high SIAPR measurements, thereby meeting the K/DOQI SIAPR criteria for referral. In addition, 78 % (14 / 18) of new AVGs with Qa ≥ 1000 mL/min, and 86 % (6 / 7) of AVGs with the highest Qa (≥ 2000 mL/min), had high SIAPR. As a result, these high-Qa AVGs, which represented the best functioning AVGs by K/DOQI Qa standards, were erroneously targeted for referral based on SIAPR measurements. ConclusionSIAPR does not correlate with Qa or discriminate between high and low Qa. Therefore, because the utility of SIAPR surveillance for detection of clinically significant stenosis depends on a correlation with Qa, the current use of absolute K/DOQI SIAPR thresholds for intervention based on the presumption that such thresholds are indicative of low Qa is not justified, and should be discontinued. Studies need to be done to examine the utility of SIAPR for trend analysis...|$|R
40|$|This thesis targets Artificial Intelligence - a {{fundamental}} branch of Computer Engineering striving to provide human-like capabilities and intelligence {{to the computer}} systems. More specifically, it deals with computer vision, which has gained {{a lot of attention}} by researchers due to its wide applicability in day-to-day tasks involving view generation, synthesizing animations and videos from <b>static</b> images, <b>surveillance,</b> medical imaging, tracking, object recognition and classification etc. This thesis investigates the problem areas of image synthesis, object recognition and object categorization. The problem of generating images at novel, arbitrary and unconstrained viewpoints covering interpolation and extrapolation is investigated by operating on a sparse set of basis images of a real scene. This image generation methodology is further incorporated to develop models for object recognition and categorization.   First, an image synthesis strategy has been presented that generates virtual views at arbitrary points using interpolation and extrapolation from a sparse set of images. The traditional work on view synthesis using interpolation has been extended and {{it has been shown that}} view extrapolation can be done as easily as interpolation. Moreover, certain scenarios have been identified like planar and/or multi-planar scenes and pure rotational camera motion for image capture that allow direct retrieval of the underlying mapping function between the images and hence leading to even more simplified image extrapolation. The major issues and factors affecting the accuracy of generation have been explored and suggestions are presented to improve the virtual view quality. Next, an approach is presented to generate a model for multi-view object recognition. A view centered model is generated using either a video sequence or a sparse set of images captured around the object following arbitrary and unconstrained camera trajectory. It does not require any prior knowledge of camera parameters and positioning or motion of object and/or camera. The model thus generated is quite dense with a lot of redundant images. Thus the virtual view generation strategy is applied to identify the redundant images and remove them. This results in a model that is computationally economical in terms of space and time. Next, for testing or recognition, the model is used in conjunction with a video sequence which provides information of multiple views of the object and thus increases the confidence measure of results. The model is robust in that it captures the topological structure of the objects from multiple viewpoints allowing the use of a video iv sequence rather than a single test image for object recognition. No constraint hasbeen placed on camera and/or object motion while capturing the video. Next, an approach for video-based multi-view object classification is presented. For each object instance of a particular category, a neighborhood graph-based model is generated using the set of input images which are arranged in a manner that highlights the underlying topological structure. Again, no constraint is placed on the motion and placement of the object and camera during image capture. Moreover no prior knowledge of positioning or parameters of camera is desired. The view synthesis algorithm is used to identify the redundant images in the model and remove them to give a computationally economical model in terms of space and training time. The independent graphs of the different instances of the object category are then merged by automatically identifying the corresponding viewpoints across them. The strength of this approach is that it allows object categorization from multiple viewpoints while eliminating the need of manual alignment of common viewing angles across object instances. Another strength is that the video sequences have been used for object classification, instead of images, which increases precision of result...|$|R
40|$|Thesis (Ph. D.) [...] University of Washington, 2015 - 12 We propose an {{automatic}} system which dynamically tracks video objects (human and vehicle) and create their 3 -D visualization from big visual data. Big visual data implies that all videos are collected from either <b>static</b> or mobile <b>surveillance</b> cameras. Our {{goal is to}} track a video object within such a surveillance camera network. To achieve this goal, several tracking scenarios must be carefully dealt with. In this work, we focus on tracking under a single static camera, tracking under a single moving camera, and tracking across multiple moving cameras. In the case of tracking under a single static camera, our proposed work is mainly based on constrained multiple-kernel tracking framework. For human tracking, the system adopts a Kalman filter to predict and refine the tracking results. A pre-trained human detector is further applied to solve initial merging issues. For human tracking across multiple static cameras, a self-organized and scalable multiple-camera tracking system that tracks human across cameras with nonoverlapping views is proposed. For vehicle tracking, our proposed approach regards each patch of the 3 -D vehicle model as a kernel, and track the kernels under certain constraints facilitated with the 3 -D geometry of the vehicle model. Meanwhile, a kernel density estimator is designed to fit the 3 -D vehicle model during tracking. By elegant application of the constrained multiple-kernel (CMK) tracking facilitated with the 3 -D vehicle model, the vehicles are able to be tracked efficiently and located precisely. As for tracking under a single moving camera, we propose a robust moving platform based object tracking system, and apply to human tracking. Our work effectively integrates Visual Simultaneous Localization And Mapping, pedestrian detection, ground plane estimation, and kernel-based tracking techniques. The proposed system systematically detects the pedestrians from recorded video frames and tracks the pedestrians in the V-SLAM inferred 3 -D space via a tracking-by-detection scheme. In order to efficiently associate the detected pedestrian frame-by-frame, we propose a novel tracking framework, combining the CMK tracking and the estimated 3 -D (depth) information, to globally optimize the data association between consecutive frames. By {{taking advantage of the}} appearance model and 3 -D information, the proposed system not only achieves high effectiveness but also handles efficiently occlusion in the tracking. Based on the results of tracking under a single moving camera, we propose a new framework to track on-road pedestrians across multiple driving recorders. More specifically, we treat the problem as a multi-label classification task, determining whether a specific pedestrian belongs to one or several cameras’ field of views by considering the association likelihood of the tracked pedestrians. The likelihood is calculated based on the pedestrians’ motion cues and appearance features, which are necessarily transformed via brightness transfer functions obtained by some available spatially overlapping views for compensating for the diversity of the cameras. When a pedestrian is leaving a camera’s field of view, the proposed framework predicts and interpolates the possible moving trajectories facilitated by an open map service which can provide routing information. Moreover, based on the GPS locations, we can also reconstruct a 3 -D visualization on a 3 -D virtual real-world environment, so as to show the dynamic scenes of the recorded videos...|$|R
