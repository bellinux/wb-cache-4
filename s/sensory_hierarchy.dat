13|18|Public
5000|$|The <b>sensory</b> <b>hierarchy</b> induces {{a number}} of {{differences}} between the various layers. As one moves up the hierarchy, representations have increased: ...|$|E
50|$|The {{relationship}} between sensory and motor processing {{is an important}} aspect of the basic theory. It is proposed that the motor areas of the cortex consist of a behavioural hierarchy similar to the <b>sensory</b> <b>hierarchy,</b> with the lowest levels consisting of explicit motor commands to musculature and the highest levels corresponding to abstract prescriptions (e.g. 'resize the browser'). The sensory and motor hierarchies are tightly coupled, with behaviour giving rise to sensory expectations and sensory perceptions driving motor processes.|$|E
40|$|Deep neural {{networks}} as powerful tools {{are widely used}} in various domains. However, the nature of computations at each layer of the deep networks {{is far from being}} well understood. Increasing the interpretability of deep {{neural networks}} is thus important. Here, we construct a mean-field framework to understand how compact representations are developed across layers, not only in deterministic random deep networks but also in generative deep networks where network parameters are learned from input data. Our theory shows that the deep computation implements a dimensionality reduction while maintaining a finite level of weak correlations between neurons for possible feature extraction. This work may pave the way for understanding how a <b>sensory</b> <b>hierarchy</b> works in general. Comment: 5 pages, 3 figures, a physics explanation of decorrelation and dimensionality reduction is adde...|$|E
50|$|Besides these subjectively {{satisfying}} explanations, {{the framework}} {{also makes a}} number of testable predictions. For example, the important role that prediction plays throughout the <b>sensory</b> <b>hierarchies</b> calls for anticipatory neural activity in certain cells throughout sensory cortex. In addition, cells that 'name' certain invariants should remain active throughout the presence of those invariants, even if the underlying inputs change. The predicted patterns of bottom-up and top-down activity - with former being more complex when expectations are not met - may be detectable, for example by {{functional magnetic resonance imaging}} (fMRI).|$|R
50|$|In {{the center}} of the figure, each map has a range and {{resolution}} that is appropriate for path planning at its level. At each level, there are symbolic data structures and segmented images with labeled regions that describe entities, events, and situations that are relevant to decisions that must be made at that level. On the left is a <b>sensory</b> processing <b>hierarchy</b> that extracts information from the sensory data stream that is needed to keep the world model knowledge database current and accurate.|$|R
40|$|AbstractMRI, {{enabling}} in vivo {{analysis of}} cortical morphology, offers {{a powerful tool}} {{in the assessment of}} brain development and pathology. One of the most ubiquitous measures used—the thickness of the cortex—shows abnormalities in a number of diseases and conditions, but the functional and biological correlates of such alterations are unclear. If the functional connotations of structural MRI measures are to be understood, we must strive to clarify the relationship between measures such as cortical thickness and their cytoarchitectural determinants. We therefore sought to determine whether patterns of cortical thickness mirror a key motif of the cortex, specifically its structural hierarchical organisation. We delineated three <b>sensory</b> <b>hierarchies</b> (visual, somatosensory and auditory) in two species—macaque and human—and explored whether cortical thickness was correlated with specific cytoarchitectural characteristics. Importantly, we controlled for cortical folding which impacts upon thickness and may obscure regional differences. Our results suggest that an easily measurable macroscopic brain parameter, namely, cortical thickness, is systematically related to cytoarchitecture and to the structural hierarchical organisation of the cortex. We argue that the measurement of cortical thickness gradients may become an important way to develop our understanding of brain structure–function relationships. The identification of alterations in such gradients may complement the observation of regionally localised cortical thickness changes in our understanding of normal development and neuropsychiatric illnesses...|$|R
30|$|The {{investigation}} of structural and functional networks {{of the brain}} {{is at the heart}} of many initiatives in neuroscience; it is an essential component of the Human Brain Project ([URL] and it is the primary goal of the Human Connectome Project ([URL] Functional connectivity per se is defined in terms of quantitative measures of linked activity, computed from time series of regional brain activations. Indirect correlates of brain activity are mediated by metabolic changes as these can be traced from the regional consumption of radioactively labelled glucose in Positron Emission Tomography (PET) or the Blood Oxygenated level Dependent (BOLD) functional Magnetic Resonance Imaging (fMRI). Both methods provide indirect correlates of brain activity with time constants of many minutes for PET and a few seconds for fMRI. These changes are slow – orders of magnitude slower than the few millisecond transit time for the activity between areas. With improvements in the accuracy of these methods, it has become clearer that the foci of brain activity coincides with the cytoarchitectonic areas, with initial demonstrations emphasizing responses to well defined stimuli as these excite the early cytoarchitectonic areas in each <b>sensory</b> <b>hierarchy.</b>|$|E
40|$|Abstract. This {{contribution}} {{deals with}} preliminary investigations {{on the behavior}} of the cortical algorithm in probabilistic hierarchic analysis synthesis systems. Such a subsystem is one the key components of Cognitive Dynamic Systems or Cognitive User Interfaces respectively. Both systems are typically characterized by the cybernetic circle that describes the perception of the environment along the <b>sensory</b> <b>hierarchy,</b> the selection of an optimal response and action articulation on the environment along the motor hierarchy. Further, a cognitive system should be able to predict the consequences of its own actions. For this purpose an inner model of the communication participant and its simulation is required. Based on this assumption, the bidirectional flow of information in analysis synthesis systems may be justified. Even though the cortical algorithm is drawn to cascaded bidirectional HMMs (CBHMMs), in this study the impact of the bidirectional information processing has been investigated just for simple single layer bidirectional HMMs. The proposed experiment is based on Shannon’s channel model, at which synthetic source data are transmitted to the receiver- disturbed by Gaussian noise at different SNR. Finally, we compare the state recognition rate for all possible setups using single layer HMMs...|$|E
40|$|Many learned {{behaviors}} {{are thought to}} require the activity of high-level neurons that represent categories of complex signals, such as familiar faces or native speech sounds. How these complex, experience-dependent neural responses emerge within the brain’s circuitry is not well understood. The caudomedial mesopallium (CMM), a secondary auditory region in the songbird brain, contains neurons that respond to specific combinations of song components and respond preferentially to the songs that birds have learned to recognize. Here, we examine the transformation of these learned responses across a broader forebrain circuit that includes the caudolateral mesopallium (CLM), an auditory region that provides input to CMM. We recorded extracellular single-unit activity in CLM and CMM in European starlings trained to recognize sets of conspecific songs and compared multiple encoding properties of neurons between these regions. We find that the responses of CMM neurons are more selective between song components, convey more information about song components, and are more variable over repeated components than the responses of CLM neurons. While learning enhances neural encoding of song components in both regions, CMM neurons encode {{more information about the}} learned categories associated with songs than do CLM neurons. Collectively, these data suggest that CLM and CMM are part of a functional <b>sensory</b> <b>hierarchy</b> that is modified by learning to yield representations of natural vocal signals that are increasingly informative with respect to behavior...|$|E
40|$|This essay {{examines}} {{objects and}} images {{pertaining to the}} tactile experience of blind and visually-impaired people in nineteenth-century European culture, questioning {{the ways in which}} shifting <b>sensory</b> <b>hierarchies</b> (particularly the relationship between touch and vision) constituted blindness as disability. We focus on a theme that held particular social and cultural interest in nineteenth-century accounts of blindness: travel and geography. We contrast the writings and portraiture of James Holman, a British traveller with visual disabilities, with the pedagogic programmes that were developed in European institutions for blind people. Holman’s public persona optimistically pointed to ways of knowing the world that were not dependent upon vision. Yet when geographic touch entered the institution, it became subject to disciplinary control, replicating the conventions of sight and curtailing movement of the body through promotion of local, above global, knowledge. This comparative approach enables a critical examination of the tactile realm of blindness. We analyse medical-pedagogical ideas on impaired perception and sensory compensation that underpinned attempts to define the touch of the blind via instructive object-lessons, alongside heuristic ways of engaging with the world through touch. We distinguish between systems in which blind people were cast as either producers or recipients of sensory knowledge and detail the various ways in which this was enabling or disabling...|$|R
40|$|Generative {{models of}} neuroimaging and {{electrophysiological}} data present {{new opportunities for}} accessing hidden or latent brain states. Dynamic causal modeling (DCM) uses Bayesian model inversion and selection to infer the synaptic mechanisms underlying empirically observed brain responses. DCM for electrophysiological data, in particular, aims to estimate the relative strength of synaptic transmission at different cell types and via specific neurotransmitters. Here, we report a DCM validation study concerning inference on excitatory and inhibitory synaptic transmission, using different doses of a volatile anaesthetic agent (isoflurane) to parametrically modify excitatory and inhibitory synaptic processing while recording local field potentials (LFPs) from primary auditory cortex (A 1) and the posterior auditory field (PAF) in the auditory belt region in rodents. We test whether DCM can infer, from the LFP measurements, the expected drug-induced changes in synaptic transmission mediated via fast ionotropic receptors; i. e., excitatory (glutamatergic) AMPA and inhibitory GABA(A) receptors. Cross- and auto-spectra from the two regions were used to optimise three DCMs based on biologically plausible neural mass models and specific network architectures. Consistent with known extrinsic connectivity patterns in <b>sensory</b> <b>hierarchies,</b> we found that a model comprising forward connections from A 1 to PAF and backward connections from PAF to A 1 outperformed a model with forward connections from PAF to A 1 and backward connections from A 1 to PAF and a model with reciprocal lateral connections. The parameter estimates from the most plausible model indicated that the amplitude of fast glutamatergic excitatory postsynaptic potentials (EPSPs) and inhibitory postsynaptic potentials (IPSPs) behaved as predicted by previous neurophysiological studies. Specifically, with increasing levels of anaesthesia, glutamatergic EPSPs decreased linearly, whereas fast GABAergic IPSPs displayed a nonlinear (saturating) increase. The consistency of our model-based in vivo results with experimental in vitro results lends further validity to the capacity of DCM to infer on synaptic processes using macroscopic neurophysiological data...|$|R
40|$|The {{problem of}} robot control is {{approached}} from a systems standpoint where a complete control system must include {{all of the}} aspects involved in moving a robot, not just the algorithms in the classic controls literature. The NASA/NBS Standard Reference Model for Telerobot Control System Architecture (NASREM) provides the framework for a complete manipulator control system. It is composed of three hierarchies: task decomposition, world modeling, and sensory processing. The task decomposition hierarchy divides tasks into smaller and smaller subtasks. In order to achieve the desired decomposition, the task decomposition hierarchy must often access information stored in the world modeling hierarchy, which contains a workspace representation, object descriptions, robot models, etc. The <b>sensory</b> processing <b>hierarchy</b> constantly fils the world model with processed sensor information. In the process of buildin...|$|R
40|$|AbstractTo {{perceive}} a coherent environment, incomplete or overlapping visual forms must {{be integrated into}} meaningful coherent percepts, a process referred to as “Gestalt” formation or perceptual completion. Increasing evidence suggests that this process engages oscillatory neuronal activity in a distributed neuronal assembly. A separate line of evidence suggests that Gestalt formation requires top-down feedback from higher order brain regions to early visual cortex. Here we combine magnetoencephalography (MEG) and effective connectivity analysis in the frequency domain to specifically address the effective coupling between sources of oscillatory brain activity during Gestalt formation. We demonstrate that perceptual completion of two-tone “Mooney” faces induces increased gamma frequency band power (55 – 71 Hz) in human early visual, fusiform and parietal cortices. Within this distributed neuronal assembly fusiform and parietal gamma oscillators are coupled by forward and backward connectivity during Mooney face perception, indicating reciprocal influences of gamma activity between these higher order visual brain regions. Critically, gamma band oscillations in early visual cortex are modulated by top-down feedback connectivity from both fusiform and parietal cortices. Thus, we provide a mechanistic account of Gestalt perception in which gamma oscillations in feature sensitive and spatial attention-relevant brain regions reciprocally drive one another and convey global stimulus aspects to local processing units at low levels of the <b>sensory</b> <b>hierarchy</b> by top-down feedback. Our data therefore support the notion of inverse hierarchical processing within the visual system underlying awareness of coherent percepts...|$|E
40|$|The {{nonlinear}} {{nature of}} integration among cortical brain areas renders the effective connectivity between them inherently dynamic and context-sensitive. One emerging architectural principle of functional brain organization, which rests explicitly on these nonlinear interactions, is that neuronal responses expressed {{at any level}} in a <b>sensory</b> <b>hierarchy</b> reflect an interaction between (i) bottom up “driving” afferents from lower cortical areas and (ii) backwards “modulatory” inputs from higher areas that mediate top-down contextual effects. A compelling example is attentional modulation of responses in functionally specialized sensory areas. The aim of this work was to demonstrate that parietal regions may mediate selective attention to motion by modulating the effective connectivity from early visual cortex to the motion-sensitive area V 5 /MT. Using functional magnetic resonance imaging, and an analysis of effective connectivity based on nonlinear system identification, we found that backwards modulatory influences from the posterior parietal cortex are sufficient to account for a significant component of attentional modulation of V 5 /MT responses to “driving” inputs from V 2. By explicitly modeling interactions among inputs to V 5 /MT, {{we were able to}} make inferences about the influences of V 2 inputs and their concomitant activity-dependent modulation by parietal afferents. The latter effects embody dynamic changes in effective connectivity that may underlie attentional mechanisms. These results speak to the context-sensitive nature of functional integration in the brain and provide empirical evidence that attentional effects may be mediated by backwards connections, of a modulatory sort, in humans...|$|E
40|$|This {{presentation}} {{describes the}} operation of the Jamaican reggae dancehall sound system session as a popular cultural laboratory for understanding the biopolitics of sensation. The paper explores the sensorial apparatus (Agamben) or dispositif (Foucault) of the open-air street sound system session. This engineers the supra liminal excesses of sensorial experience in the biopolitical economies of pleasure that constitute thedancehall scene {{at the heart of the}} island’s popular culture. The set of equipment amplifies intensities and accentuates rhythmic patterning, thereby serving as a means of subjectification (Lazzarato, Guattari) under popular control, sustained on the margins of police and state hegemony. The sound system set of equipment produces tens of thousands of watts of music power that is subject to the engineers’ manipulation (a sonic knowledge developed over generations) to balance, fine-tune, spatialise and mix the audio frequencies. The sensory surfaces of the entire corporeal envelope of the bodies of the crowd operate in multi-sensory flux (Gibson), in parallel and synaesthetically, outside the conventional Aristotelian five-fold <b>sensory</b> <b>hierarchy,</b> as well as reciprocally (with leading and following dance moves) and antiphonically (between MC and crowd). The dancehall culture further configures the rhythmic dynamics of the session from dance choreography to nightly, weekly and seasonal patterns of activity. The biopolitics of this sensorial apparatus contrasts with that of Virtual Reality by configuring the actual embodied reality of the experience of affective intensities. In contrast with personalised digital monitoring technologies of the quantised self, those of the sound system configure a shared multi-sensory sociality...|$|E
40|$|Mammals rely on vision, audition, and {{olfaction}} to remotely sense stimuli {{in their}} environment. Determining how the mammalian brain uses this sensory information to recognize objects {{has been one}} of the major goals of psychology and neuroscience. Likewise, researchers in computer vision, machine audition, and machine olfaction have endeavored to discover good algorithms for stimulus classification. Almost 50 years ago, the neuroscientist Jerzy Konorski proposed a theoretical model in his final monograph in which competing sets of "gnostic" neurons sitting atop <b>sensory</b> processing <b>hierarchies</b> enabled stimuli to be robustly categorized, despite variations in their presentation. Much of what Konorski hypothesized has been remarkably accurate, and neurons with gnostic-like properties have been discovered in visual, aural, and olfactory brain regions. Surprisingly, there have not been any attempts to directly transform his theoretical model into a computational one. Here, I describe the first computational implementation of Konorski's theory. The model is not domain specific, and it surpasses the best machine learning algorithms on challenging image, music, and olfactory classification tasks, while also being simpler. My results suggest that criticisms of exemplar-based models of object recognition as being computationally intractable due to limited neural resources are unfounded...|$|R
40|$|NoPerceived time is {{inherently}} malleable. For example, adaptation to relatively long or short sensory events {{leads to a}} repulsive aftereffect such that subsequent events appear to be contracted or expanded (duration adaptation). Perceived visual duration can also be distorted via concurrent presentation of discrepant auditory durations (multisensory integration). The neural loci of both distortions remain unknown. In the current study we use a psychophysical approach to establish their relative positioning within the <b>sensory</b> processing <b>hierarchy.</b> We show that audiovisual integration induces marked distortions of perceived visual duration. We proceed to use these distorted durations as visual adapting stimuli yet find subsequent visual duration aftereffects {{to be consistent with}} physical rather than perceived visual duration. Conversely, the concurrent presentation of adapted auditory durations with nonadapted visual durations results in multisensory integration patterns consistent with perceived, rather than physical, auditory duration. These results demonstrate that recent sensory history modifies human duration perception prior to the combination of temporal information across sensory modalities and provides support for adaptation mechanisms mediated by duration selective neurons situated in early areas of the visual and auditory nervous system (Aubie, Sayegh, & Faure, 2012; Duysens, Schaafsma, & Orban, 1996; Leary, Edwards, & Rose, 2008) ...|$|R
40|$|AbstractWe {{examined}} {{the impact of}} variability in speech stimuli on improvement of general performance and on accessibility to low-level information {{as a function of}} practice. Listeners had to discriminate between two similar words in noise in two configurations that differed only in their low-level binaural information, which was either null or maximal. The difference in performance quantifies the use of binaural low-level information. These configurations were presented in three training protocols: in separate blocks; in a consistently interleaved manner; and in a randomly mixed manner. The first protocol enabled optimal use of the low-level binaural cues already at the first training session. The second, consistently interleaved protocol required more than one training session to reach the same performance. The final, mixed protocol did not enable optimal use of the low-level cues even after multi-session training. Interestingly, training with the first two protocols transferred to the mixed one. These results are in line with recent findings in the visual modality. In both modalities, the effects of variability on learning {{can be explained by the}} introduction of obstructions to a search mechanism going down along the <b>sensory</b> processing <b>hierarchy,</b> as suggested by the Reverse Hierarchy Theory...|$|R
40|$|Intellectual {{property}} (IP) {{scholars have}} long struggled {{to explain the}} boundaries of and differences between copyright and patent law. This Article proposes a novel explanation: copyright and patent can be fruitfully understood as establishing a dichotomy between the different human senses. Copyright has bracketed works addressed to the senses of sight and hearing, and it treats products appealing to touch, taste, and smell as functional and, thus, uncopyrightable. To the extent the latter receive IP protection, {{it is through the}} utility patent regime. The Article begins by establishing this descriptive proposition, and it shows how some of the most contested areas of IP (e. g., the useful articles doctrine in copyright law and design patents) involve breaches of this sensory dichotomy. Next, I argue that the sensory dichotomy in IP reflects the <b>sensory</b> <b>hierarchy</b> in traditional Western aesthetic theory. According to this tradition, sight and hearing are considered “high” senses capable of unconstrained aesthetic and cultural experiences. Touch, taste, and smell, by contrast, are considered “low” senses, because their connection to natural bodily needs constrains their aesthetic capacities. IP law’s treatment of the senses in copyright and patent law matches this hierarchy. In recent years, however, fundamental principles of Western aesthetic theory have been undermined by developments in cognitive neuroscience, evolutionary aesthetics, and haptic and culinary communication. This research suggests that sight and hearing are not as aesthetically unconstrained and functionless, nor are touch, taste, and smell as aesthetically constrained and functional as previously believed. Accordingly, I argue that IP law should treat appeals to the senses uniformly. Works that express or communicate ideas, emotions, or pleasures to any of the five senses {{in such a way that}} creates original works of authorship should be potentially copyrightable. The Article concludes with an analysis of this proposal’s effects on various creative fields, including tactile objects, fashion, culinary dishes, and yoga...|$|E
40|$|To {{increase}} {{fitness for}} survival, organisms not only passively react to environmental changes but also actively predict future events {{to prepare for}} potential hazards within their environment. Accumulating evidence indicates that the human brain is a remarkable predictive machine which constantly models causal relationships and predicts future events. This ‘predictive processing’ framework, a prediction-based form of Bayesian inference, states that the brain continuously generates and updates predictions about incoming sensory signals. This framework has been showing notable explanatory power in understanding the mechanisms behind both human behaviour and neurophysiological data and elegantly specifies the underlying computational principles of the neural system. However, even though predictive processing {{has the potential to}} provide a unified theory of the brain (Karl Friston, 2010), we still have a limited understanding about fundamental aspects of this model, such as how it deals with different types of information, learns statistical regularities and perhaps most fundamentally of all what its relationship to conscious experience is. This thesis aims to investigate the major gaps in our current understanding of the predictive processing framework via a series of studies. Study 1 investigated the fundamental relationship between unconscious statistical inference reflected by predictive processing and conscious access. It demonstrated that predictions that are in line with sensory evidence accelerate conscious access. Study 2 investigated how low level information within the <b>sensory</b> <b>hierarchy</b> is dealt with by predictive processing and regularity learning mechanisms through “perceptual echo” in which the cross-correlation between a sequence of randomly fluctuating luminance values and occipital electrophysiological (EEG) signals exhibits a long-lasting periodic (100 ms cycle) reverberation of the input stimulus. This study identified a new form of regularity learning and the results demonstrate that the perceptual echo may reflect an iterative learning process, governed by predictive processing. Study 3 investigated how supra-modal predictive processing is capable of learning regularities of temporal duration and also temporal predictions about future events. This study revealed a supramodal temporal prediction mechanism which processes auditory and visual temporal information and integrates information from the duration and rhythmic structures of events. Together these studies provide a global picture of predictive processing and regularity learning across differing types of predictive information...|$|E
40|$|Drawing upon Vivian Sobchack’s {{notions of}} “ultra-hearing” and “ultra-seeing” (a re- {{interpretation}} of Bachelard’s <b>sensory</b> <b>hierarchy),</b> this paper explores the possibility {{and nature of}} moving from the ‘work’ (in electroacoustic musical and contemporary choreographical senses respectively) to an intermedial conception of a multisensory and ‘live-digital’ ‘event’ which moves from, through and beyond the ‘concrete’, towards a rhythmical space that is, {{in the words of}} Don Ihde ‘a deliberate decentering of (the) dominant tradition(s) in order to discover what may be missing’. The authors’ investigation into the taxonomies between sonic worlds, image generation and movement making, in a recent and on-going collaboration, therefore engages in the imaginative potential of an ‘event’ (in Massumi’s terms) where the modulation of acousmatic sound, image and movement becomes enlivened beyond the fixed dimensions of each respective discipline. Moreover, besides exploring the potential connections between sound, image and the body, the authors have been searching for a way to create environments that stimulate poetic relationships beyond the normal compositional opportunities afforded by each of their respective areas. This has encouraged a situation were technological and technical processes leads one to respond to the qualities of things, which in turn has encouraged an epistemology of materiality – not fixed in form – but open to the nuances that flow between them. Indeed, in terms of the integral nature of ‘fixing’ movement and image qualities in determining the nature of the ‘live’ or ‘performative’, there have emerged some interesting connections with post-Schaefferian musical practice. As Hansen describes, “At the heart of this endeavor is a conviction that today’s microtemporal digital technologies do not simply impact human sensory experience from the outside, but rather materialize a potentiality that characterizes sensory experience from its very origin [...] . ”. In consequence, a central concern has been how each of the constituent parts of the unfolding ‘event’ can then remain in flux, or better said remain un-fixed (the term ‘event’ and not ‘work’ is used purposefully here to avoid the idea that what is created is a fixed piece of work) in order to tap into sensory experience proper. Parallels relating to affect, interpretation and meaningfulness between electrocoustic and live-digital dance practices and audience experiences will be drawn. Notions of intimacy, central to this ongoing work, will be explored in relation to the conference theme; that is to say, between ‘live’ and digital, between eye and ear, between movement and digital image, and between performance and intermedia ‘environment’ and audience. Given the centrality of space as well as temporality in this endeavor, the authors continue to explore where such practice should happen; it seems already not in the concert hall or traditional dance venue [...] ...|$|E
40|$|It {{is known}} that {{olfaction}} and vision can work in tandem to represent object identities. What is yet unclear is {{the stage of the}} <b>sensory</b> processing <b>hierarchy</b> at which the two types of inputs converge. Here we study this issue through a well established visual phenomenon termed binocular rivalry. We show that smelling an odor from one nostril significantly enhances the dominance time of the congruent visual image in the contralateral visual field, relative to that in the ipsilateral visual field. Moreover, such lateralization-based enhancement extends to category selective regions so that when two images of words and human body, respectively, are engaged in rivalry in the central visual field, smelling natural human body odor from the right nostril increases the dominance time of the body image compared with smelling it from the left nostril. Semantic congruency alone failed to produce this effect in a similar setting. These results, taking advantage of the anatomical and functional lateralizations in the olfactory and visual systems, highlight the functional dissociation of the two nostrils and provide strong evidence for an object-based early convergence of olfactory and visual inputs in sensory representations...|$|R
40|$|Sensory {{and motor}} cortices each contain {{multiple}} topographic maps with {{the structure of}} sensory organs (such as the retina or cochlea) mapped onto the cortical surface. These sensory maps are hierarchically organized. For example, visual field maps contain neurons that represent increasingly large parts of visual space with increasingly complex responses 1. Some visual neurons respond to stimuli with a particular numerosity — the number of objects in a set. We recently discovered a parietal topographic numerosity map in which neural numerosity preferences progress gradually across the cortical surface 2, analogous to sensory maps. Following this analogy, we hypothesized {{that there may be}} multiple numerosity maps. Numerosity perception is implicated in many cognitive functions, including foraging 3, multiple object tracking 4, dividing attention 5, decision-making 6 and mathematics 7 – 9. Here we use ultra-highfield (7 Tesla, 7 T) functional magnetic resonance imaging (fMRI) and neural-model-based analyses to reveal numerosityselective neural populations organized into six widely separated topographic maps in each hemisphere. Although we describe subtle differences between these maps, their properties are very similar, unlike in <b>sensory</b> map <b>hierarchies.</b> These maps are found in areas implicated in object recognition, motion perception, attention control, decision-making and mathematics. Multiple numerosity maps may allow interactions with these cognitive systems, suggesting a broad role for quantity processing in supporting many perceptual and cognitive functions...|$|R
40|$|Abstract—It {{may come}} as no {{surprise}} that a simple “planar unicycle ” (or “skate”) model can successfully follow a wall at high speed under PD-control. What would be surprising is that such a simple control mechanism may underly the control of one of Nature’s fastest terrestrial insects, the American cockroach. For this paper, we implemented the same controller (up to scale) believed to govern cockroach wall following to successfully control two models of the cockroach: a differential-drive mobile robot with a flexible artificial antenna and a lateral leg spring model with moving center of pressure as the control input. These physical and numerical experiments demonstrate the sufficiency of the cockroach’s putative controller in a real-world setting with unmodeled effects, and suggest how the nervous system might guide leg placement in response to <b>sensory</b> stimuli. This <b>hierarchy</b> of models may prove useful in generating prescriptive hypotheses for biological testing and hence elucidating the general principles that underly sensor-guided animal locomotion. I...|$|R
40|$|Correlated sensory inputs {{coursing}} {{along the}} individual <b>sensory</b> processing <b>hierarchies</b> arrive at multisensory convergence zones in cortex where inputs are processed in an integrative manner. The exact hierarchical level of multisensory convergence zones {{and the timing}} of their inputs are still under debate, although increasingly, evidence points to multisensory integration at very early sensory processing levels. The objective {{of the current study}} was to determine, both psychophysically and electrophysiologically, whether differential visual-somatosensory integration patterns exist for stimuli presented to the same versus opposite hemifields. Using high-density electrical mapping and complementary psychophysical data, we examined multisensory integrative processing for combinations of visual and somatosensory inputs presented to both left and right spatial locations. We assessed how early during sensory processing visual-somatosensory (VS) interactions were seen in the event-related potential and whether spatial alignment of the visual and somatosensory elements resulted in differential integration effects. Reaction times to all VS pairings were significantly faster than those to the unisensory conditions, regardless of spatial alignment, pointing to engagement of integrative multisensory processing in all conditions. In support, electrophysiological results revealed significant differences between multisensory simultaneous VS and summed V+S responses, regardless of the spatial alignment of the constituent inputs. Nonetheless, multisensory effects were earlier in the aligned conditions, and were found to be particularly robust in the case of right-sided inputs (beginning at just 55 ms). In contrast to previous work on audio-visual and audio-somatosensory inputs, the current work suggests a degree of spatial specificity to the earliest detectable multisensory integrative effects in response to visual-somatosensory pairings...|$|R
40|$|Self-supervised {{models of}} how the brain {{represents}} and categorises the causes of its sensory input {{can be divided into}} two classes: those that minimise the mutual information (i. e. redundancy) among evoked responses and those that minimise the prediction error. Although these models have similar goals, the way they are attained, and the functional architectures employed, can be fundamentally different. This review describes the two classes of models and their implications for the functional anatomy of <b>sensory</b> cortical <b>hierarchies</b> in the brain. We then consider how empirical evidence can be used to disambiguate between architectures that are sufficient for perceptual learning and synthesis. Most models of representational learning require prior assumptions about the distribution of sensory causes. Using the notion of empirical Bayes, we show that these assumptions are not necessary and that priors can be learned in a hierarchical context. Furthermore, we try to show that learning can be implemented in a biologically plausible way. The main point made in this review is that backward connections, mediating internal or generative models of how sensory inputs are caused, are essential if the process generating inputs cannot be inverted. Because these processes are dynamical in nature, sensory inputs correspond to a non-invertible nonlinear convolution of causes. This enforces an explicit parameterisation of generative models (i. e. backward connections) to enable approximate recognition and suggests that feedforward architectures, on their own, are not sufficient. Moreover, nonlinearities in generative models, that induce a dependence on backward connections, require these connections to be modulatory; so that estimated causes in higher cortical levels can interact t...|$|R
40|$|The {{capacity}} of animals to navigate through familiar or novel environments depends crucially on {{the integration of}} a disparate set of self motion cues. The study begins {{with one of the}} most simple, planar visual motion, and investigates the cortical organisation of motion sensitive areas. It finds evidence of columnar organisation in hMT+ and a large scale map in V 1. Chapter 3 extends this by using stimuli designed to emulate visual and auditory forward motion. It finds that participants are able to determine their direction with a precision close to that predicted by Bayesian integration. Predictions were made regarding neural processing through a modified divisive normalisation model, which was also used to fit the behavioural adaptation results. The integration of different modalities requires visual and auditory streams to combine at some stage within the <b>sensory</b> processing <b>hierarchy.</b> Previous research suggests the ventral intraparietal region (VIP) may be the seat of such integration. Chapter 4 tests whether VIP does combine these cues and whether the correlation between VIP and the unimodal regions changes depending on the coherence of unimodal stimuli. The presence of such modulation is predicted by some models, such as the divisive normalisation model. The processing of such egocentric self motion cues leads to the updating of allocentric representations, these are believed to be encoded by head direction cells and place cells. The experiment in chapter 5 uses a virtual reality stimulus during fMRI scanning to give participants the sense of moving and navigating. Their location in the virtual environment was decoded above chance from voxels in the hippocampus. No head direction signal was classified above chance from any of the three cortical regions investigated. We tentatively conclude that head direction is considerably more difficult to classify from the BOLD signal, possibly due to the homogeneous organisation of head direction cells...|$|R
40|$|Multimodal {{sensory input}} directs simple and complex {{behaviors}} in animals. Most research {{to date has}} been limited to studies of individual senses rather than multiple senses working together, leading to important advances in our comprehension of the sensory systems in isolation, but not their complementary and alternative roles in difficult behavioral tasks, such as feeding. In the marine environment, a prey item might emit an odor, create a hydrodynamic disturbance, such as from gill movements or swimming, be visible to the predator, produce a sound, and/or produce a weak electrical field. Therefore, {{the goal of this}} study was to investigate the integration of olfaction, mechanoreception by the lateral line system, vision, and electroreception in a marine animal. Sharks were chosen as a model organism in which to investigate multisensory integration because of their sensitivity and acuity, the presence of the same suite of sensory modalities in all species, the availability of experimental animals from different species, habitats and ecologies, and the rich literature on sharks 2 ̆ 7 prey capture behavior. Two approaches were used: controlled artificial stimuli, delivered to the animals, were used to determine the spatial and concentration characteristics of odor encounters that guide the initial orientation to an odor plume in the far field in a model elasmobranch, the smooth dogfish, Mustelus canis; and sensory deprivation was used to restrict the availability of natural cues emanating from live prey items in order to elucidate the complementary and alternating roles of the senses in detecting, tracking, orienting to, striking at, and ultimately capturing prey. In the latter experiments, three species of sharks from different ecological niches were investigated: benthic, suction-feeding nurse sharks (Ginglymostoma cirratum) that hunt nocturnally for fish; ram-biting bonnetheads (Sphyrna tiburo) that scoop crustaceans off the bottom of seagrass beds; and ram-feeding blacktip sharks (Carcharhinus limbatus) that rapidly chase down midwater teleost prey. In orienting to odor patches, bilateral time differences between the nares are more important than concentration differences, such that animals turn toward the side stimulated first, even with delayed pulses of higher concentration. This response would steer the shark into each oncoming odor patch, helping the animal maintain contact with an odor plume. Sensory deprivation experiments revealed similarities and differences among species in terms of which senses they choose to focus on for particular behaviors, likely as a result of differences in the environments that they hunt in, type of prey consumed, and foraging strategies used, as well as anatomical differences in the central nervous system and the sensory organs. In most cases, multiple senses can be used for the same behavioral task. Thus, sharks are capable of successfully capturing prey, even when the optimal sensory cues are unavailable, by switching to alternative sensory modalities, which indicates that feeding behavior is plastic. Nurse sharks rely primarily on olfaction for detection. Olfaction in combination with vision, the lateral line, or touch is required for tracking. Nurse sharks orient to prey using the lateral line, vision, or electroreception, but will not ingest food if olfaction is blocked. Capture is mediated by the electrosensory system or tactile cues. Bonnetheads normally detect prey using olfaction, rely on olfactory-based tracking until they are close to the prey, then vision to line up a strike, and finally electroreception to time the jaw movements for capture. They can detect, orient, and strike visually in the absence of olfactory cues. Blacktip sharks also detect prey using olfaction or vision. Olfaction is used in combination with vision or the lateral line system for tracking. Long-distance orientation and striking is visually mediated, but strike precision relies on lateral line cues and an increase in misses occurs when this system is blocked. In the absence of vision, short-range orientation and striking can occur using lateral line cues. Capture is mediated by electroreception or tactile cues. Collectively, these results were used to develop species-specific <b>sensory</b> <b>hierarchies</b> for shark feeding behavior in a captive environment, the first such hierarchies to cover a complete behavioral sequence in a vertebrate...|$|R
40|$|Opinion TINS- 1016; No. of Pages 11 <b>Sensory</b> and {{sensorimotor}} <b>hierarchies</b> Cortical layers {{are identified}} by cytoarchitecture, and fur-ther characterised by patterns of intrinsic axonal and dendritic arborisation [4]. Laminar distribution distin-guishes consistent types of extrinsic corticocortical connec-tion, classified as ascending, descending, and lateral [5]. These patterns are sufficiently conserved {{to identify a}} hierarchical organisation of areas in sensory systems (Box 1). Initial descriptions of sensorimotor hierarchies The principles of predictive coding A percept {{can be regarded as}} a hypothesis that explains sensory input [16, 17] – on occasion, an erroneous hypothe-sis, as demonstrated by classic illusions (Figure 1 A). The percept interprets sensory data, such that what we see is the inferred cause of the sensations, not merely an image of the data per se [18]. In Figure 1 A, the facial features have an ambiguous depth structure that is resolved by our past experience of convex faces. The ability to infer the cause of visual sensations (e. g., a face) rests on an internal, gener-ative model of how objects generate sensory data [19, 20]. Generative models are required to finesse the problem of sensory indeterminacy (e. g., ambiguity) that illusions aptly illustrate. A generative model also has a temporal aspect: velocity is not a property of an instantaneous scene or ‘snapshot’, but an attribute that integrates sensory evidence over 0166 - 2236 / $ – see front matte...|$|R
40|$|The {{nature of}} sensory {{organisation}} - how information is transferred or recognised as equivalent {{between the various}} senses and whether a dominance hierarchy exists between them - is a topic which has interested man {{since at least the}} time of the ancient Greeks. This thesis focuses on the relationship between the visual and haptic senses and how this relationship changes during development. While it was previously known that adults achieve a unified percept in the face of conflicting visual and haptic information by a process of visual capture, there have been conflicting reports in the research literature on why this situation obtains and on the reactions of young children to such a situation. Two methodologies, the cross-modal transfer paradigm and the conflict paradigm, have been widely used in the study of sensory organisation. The conflict paradigm, used to explore possible <b>sensory</b> dominance <b>hierarchies,</b> has been widely criticised. In the present study a methodology has been developed which meets these criticisms; it has been used to study the hierarchical organisation of vision and touch in subjects from early childhood to adulthood. visual bias has been shown to occur across all age groups in haptic judgements of size, shape and texture regardless {{of the nature of the}} visual distortion. In contrast to most previous researchers, visual bias has been demonstrated to decrease with age and to be inversely related to haptic accuracy. The observed decrease in visual bias with age is accounted for in terms of the concomitant increase in the ability of subjects both to recognise the unnatural nature of the tasks they are being asked to perform and to adopt a problem-solving approach to the tasks. Through this type of approach all possible sources of information are sought and critically evaluated. A theory has been proposed to account for the occurrence of visual bias. The theory proposes that haptic judgements involve the transduction of the haptic information to a visual representation and that it is on this representation that all comparisons and judgements are made. According to this theory the occurrence of visual bias arises from the difficulty of developing an accurate transduction in the presence of similar but different visual information. The theory predicts that performance on a haptic task would deteriorate more in the presence of a simultaneous visual task than an auditory task of equivalent difficulty. This prediction has been tested and confirmed. The proposed theory has been shown to account for the order of difficulty normally reported for inter and intra-modal visual-haptic tasks and also to have explanatory value for data reported from other research into intermodal organisation...|$|R

