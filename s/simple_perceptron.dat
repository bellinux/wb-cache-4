71|64|Public
5000|$|Artificial Neural {{networks}} library implements {{some common}} network architectures (multi-layer feed forward and distance networks) and learning algorithms (back propagation, delta rule, <b>simple</b> <b>perceptron,</b> evolutionary learning).|$|E
40|$|We study {{learning}} and generalisation {{ability of a}} specific two-layer feed-forward neural network and compare its properties {{to that of a}} <b>simple</b> <b>perceptron.</b> The input patterns are mapped nonlinearly onto a hidden layer, much larger than the input layer, and this mapping is either fixed or may result from an unsupervised learning process. Such preprocessing of initially uncorrelated random patterns results in the correlated patterns in the hidden layer. The hidden-to-output mapping of the network is performed by a <b>simple</b> <b>perceptron,</b> trained using a supervised learning process. We investigate the effects of the correlations on the {{learning and}} generalisation properties as opposed to those of a <b>simple</b> <b>perceptron</b> with uncorrelated patterns. As it turns out, this architecture has some advantages over a <b>simple</b> <b>perceptron...</b>|$|E
40|$|Abstract. A novel {{approach}} to estimate generalisation errors of the <b>simple</b> <b>perceptron</b> {{of the worst}} case is introduced. It {{is well known that}} the generaiisation error of the <b>simple</b> <b>perceptron</b> is of the form d # with an unknown constant d which depends only on the dimension of inputs, where t is the number of learned examples. Based upon extreme value theory in statistics we obtain an exact form of the generalisation error of the <b>simple</b> <b>perceptron.</b> The method introduced in this paper opens up new possibilities to consider generalisation errors of a class of neural networks. ...|$|E
40|$|Neural {{networks}} are organized in committees {{to improve the}} correctness of the decisions created by artificial neural networks (ANN’s). In the classification of human chromosomes, it is accustomed to use multilayer perceptrons with multiple (22 - 24) outputs. Because of the huge number of synaptic weights to be tuned, these classifiers cannot go beyond a level of 92 % overall correctness. In this study we represent a special organized committee of 462 <b>simple</b> <b>perceptrons</b> to improve the rate of correct classification of 22 types of human chromosomes. Each of these <b>simple</b> <b>perceptrons</b> is trained to distinguish between two types of chromosomes. When a new data is entered, the votes of these 462 <b>simple</b> <b>perceptrons</b> and additional 22 dummy perceptrons create a decision matrix of the size 22 × 22. By a special assembling of these votes we get {{a higher rate of}} correct classification of 22 types of human chromosomes...|$|R
40|$|Abstract. Learning {{behavior}} of <b>simple</b> <b>perceptrons</b> is analyzed for a teacher-student {{scenario in which}} output labels are provided by a teacher network {{for a set of}} possibly correlated input patterns, and such that teacher and student networks are of the same type. Our main concern is the effect of statistical correlations among the input patterns on learning performance. For this purpose, we extend to the teacher-student scenario a methodology for analyzing randomly labeled patterns recently developed in J. Phys. A: Math. Theor. 41, 324013 (2008). This methodology is used for analyzing situations in which orthogonality of the input patterns is enhanced in order to optimize the learning performance. PACS numbers: 02. 50. -r, 84. 35. +iLearning of correlated patterns by <b>simple</b> <b>perceptrons</b> 2 1...|$|R
40|$|Abstract. Several {{authors have}} {{theoretically}} determined distribution-free bounds on sample complexity. Formulas based on several learning paradigms have been presented. However, {{little is known}} on how these formulas perform and compare {{with each other in}} practice. To our knowledge, controlled experimental results using these formulas, and comparing of their behavior, have not so far been presented. The present paper represents a contribution to filling up this gap, providing experimentally controlled results on how <b>simple</b> <b>perceptrons</b> trained by gradient descent or by the support vector approach comply with these bounds in practice. ...|$|R
40|$|Contents Introduction 4 1. The Basics of Feed-Forward ANNs 4 2. The <b>Simple</b> <b>Perceptron</b> Model 5 A <b>Simple</b> <b>Perceptron</b> Learning the OR Logic Function 9 A <b>Simple</b> <b>Perceptron</b> Learning the AND Logic Function 11 A <b>Simple</b> <b>Perceptron</b> Trying to Learn the XOR Logic Function 14 3. The Multi-layer Perceptron Model (MLP) 16 MLP Learning the XOR Logic Function 17 4. Algorithm for Transforming Nonlinearly Separable Classes in Linearly Separable Classes 19 Conclusions 20 References 20 2 List of Tables and Figures Table 4 The {{verification}} of the algorithm correctness for the perceptron which {{has learned the}} AND logic function Table 7 The classification process of the three vectors resulted from re-coding and performed in the output layer Table 8 The re-coding of the 4 input vectors by adding a third common component (x 3) to the vectors in similar classes Figure 4 The network state before learning the OR logic function Figure 5 The graphic representation of the network state before learn...|$|E
40|$|The present paper {{presents}} a synthetic {{approach to the}} XOR operation structure combining the cognitive matter of different fields of scientific study in order to elect further utility of this operation for the inequalities measurement in Regional Science. The observation of structural similarities, between the <b>simple</b> <b>perceptron</b> pattern, in artificial neural networks (ANN), and an XOR binary logic gate, in Digital Electronics Theory authorizes these two models to be considered identical for inequalities detection, under the presumption {{that both of them}} can provide a solution for the XOR problem. The solution of the XOR problem for a <b>simple</b> <b>perceptron</b> pattern introduces a new inequalities index, aroused as a generalization of the XOR problem's solution for the ANN <b>simple</b> <b>perceptron</b> model. The new index appears to operate satisfactorily in Regional Science's inequalities research, providing a significant similar behavior in comparison with the Theil index and performing better in cases that the Theil index calculation presupposes transformation treatment...|$|E
40|$|We review past {{work for}} {{predicting}} and understanding HIV protease function, using both {{machine learning algorithms}} and other classification algorithms. We show that the best algorithm for solving the task is the <b>simple</b> <b>Perceptron,</b> which has never before been applied to this problem. The <b>simple</b> <b>Perceptron</b> is efficient because the peptide data set is linearly separable, a fact that previous researchers seem to have overlooked. We also discuss the issue of data set size {{in relation to the}} size of the feature space, and classifier bias and variance and how this relates to the HIV protease function prediction problem...|$|E
40|$|We study {{supervised}} learning and generalisation in coupled perceptrons trained on-line using two learning scenarios. In the first scenario {{the teacher and}} the student are independent networks and both are represented by an Ashkin-Teller perceptron. In the second scenario the student and the teacher are <b>simple</b> <b>perceptrons</b> but are coupled by an Ashkin-Teller type four-neuron interaction term. Expressions for the generalisation error and the learning curves are derived for various learning algorithms. The analytic results find excellent confirmation in numerical simulations. Comment: Latex, 21 pages, 9 figures, iop style files include...|$|R
40|$|Mining of data streams must balance three {{evaluation}} dimensions: accuracy, {{time and}} memory. Excellent accuracy on data streams has been obtained with Naive Bayes Hoeffding Trees—Hoeffding Trees with naive Bayes models at the leaf nodes—albeit with increased runtime compared to standard Hoeffding Trees. In this paper, {{we show that}} runtime can be reduced by replacing naive Bayes with perceptron classifiers, while maintaining highly competitive accuracy. We also show that accuracy can be increased even further by combining majority vote, naive Bayes, and perceptrons. We evaluate four perceptron-based learning strategies and compare them against appropriate baselines: <b>simple</b> <b>perceptrons,</b> Perceptron Hoeffding Trees, hybrid Naive Bayes Perceptron Trees, and bagged versions thereof. We implement a perceptron that uses the sigmoid activation function instead of the threshold activation function and optimizes the squared error, with one perceptron per class value. We test our methods by performing an evaluation study on synthetic and real-world datasets comprising up to ten million examples...|$|R
40|$|Learning {{behavior}} of <b>simple</b> <b>perceptrons</b> is analyzed for a teacher-student {{scenario in which}} output labels are provided by a teacher network {{for a set of}} possibly correlated input patterns, and such that teacher and student networks are of the same type. Our main concern is the effect of statistical correlations among the input patterns on learning performance. For this purpose, we extend to the teacher-student scenario a methodology for analyzing randomly labeled patterns recently developed in J. Phys. A: Math. Theor. 41, 324013 (2008). This methodology is used for analyzing situations in which orthogonality of the input patterns is enhanced in order to optimize the learning performance...|$|R
40|$|Feedforward {{neural network}} {{performances}} depend on finding optimal learning parameters. Among them, the learning rate {{is critical for}} both the convergence speed {{and the value of}} the learning error. This paper introduces a new method for learning rate adaptation, namely the "Anti [...] Oscillatory Dynamic Method". Its performances are evaluated on a vowel classification task, with various cost functions. A comparison with other strategies proves that our solution outperforms Moody's "Search then Converge" technique. Keywords: neural network, cost function, adaptive learning rate INTRODUCTION Artificial neural networks (ANNs) are nonlinear, parallel processing devices, with proven high performance signal processing capabilities. Each ANN is defined by the characteristics of the <b>simple</b> <b>perceptron,</b> the network topology, the cost function, the learning algorithm and the stability conditions [1, 3, 10, 16]. The <b>simple</b> <b>perceptron</b> (alternative names are: artificial neuron, node, neurode) is the b [...] ...|$|E
40|$|In this note, we {{revisit the}} {{algorithm}} of Har-Peled et. al. [HRZ 07] for computing a linear maximum margin classifier. Our presentation is self contained, and the algorithm itself is slightly simpler {{than the original}} algorithm. The algorithm itself is a <b>simple</b> <b>Perceptron</b> like iterative algorithm. For more details and background, the reader is referred to the original paper...|$|E
40|$|Rosenblatt's {{convergence}} theorem for the <b>simple</b> <b>perceptron</b> initiated much {{excitement about}} iterative weight modifying neural networks. However, this convergence only holds {{for the class}} of linearly separable functions, which is vanishingly small compared to arbitrary functions. With multilayer networks of nonlinear units it is possible, though not guaranteed, to solve arbitrary functions. Backpropagation is a metho...|$|E
40|$|Principles, techniques, and {{algorithms}} in {{machine learning}} {{from the point of}} view of statistical inference; representation, generalization, and model selection; and methods such as linear/additive models, active learning, boosting, support vector machines, hidden Markov models, and Bayesian networks. From the course home page: Course Description 6. 867 is an introductory course on machine learning which provides an overview of many techniques and algorithms in machine learning, beginning with topics such as <b>simple</b> <b>perceptrons</b> and ending up with more recent topics such as boosting, support vector machines, hidden Markov models, and Bayesian networks. The course gives the student the basic ideas and intuition behind modern machine learning methods as well as a bit more formal understanding of how and why they work. The underlying theme in the course is statistical inference as this provides the foundation for most of the methods covered...|$|R
40|$|We {{introduce}} {{a large family}} of Boltzmann machines that can be trained using standard gradient descent. The networks can have one or more layers of hidden units, with tree-like connectivity. We show how to implement the supervised learning algorithm for these Boltzmann machines exactly, without resort to simulated or mean-field annealing. The stochastic averages that yield the gradients in weight space are computed by the technique of decimation. We present results {{on the problems of}} N-bit parity and the detection of hidden symmetries. 1 Introduction Boltzmann machines (Ackley, Hinton, & Sejnowski, 1985) have several compelling virtues. Unlike <b>simple</b> <b>perceptrons,</b> they can solve problems that are not linearly separable. The learning rule, simple and locally based, lends itself to massive parallelism. The theory of Boltzmann learning, moreover, has a solid foundation in statistical mechanics. Unfortunately, Boltzmann machines [...] - as originally conceived [...] -also have some serious drawbacks [...] ...|$|R
40|$|In the {{framework}} of on-line learning, a learning machine might move around a teacher due to the differences in structures or output functions between {{the teacher and the}} learning machine. In this paper we analyze the generalization performance of a new student supervised by a moving machine. A model composed of a fixed true teacher, a moving teacher, and a student is treated theoretically using statistical mechanics, where the true teacher is a nonmonotonic perceptron and the others are <b>simple</b> <b>perceptrons.</b> Calculating the generalization errors numerically, we show that the generalization errors of a student can temporarily become smaller than that of a moving teacher, even if the student only uses examples from the moving teacher. However, the generalization error of the student eventually becomes the same value with that of the moving teacher. This behavior is qualitatively different from that of a linear model. Comment: 12 pages, 5 page...|$|R
40|$|Abstract. The storage {{capacity}} of an incremental learning algorithm for the parity machine, the Tilinglike Learning Algorithm, is analytically determined in {{the limit of}} a large number of hidden perceptrons. Different learning rules for the <b>simple</b> <b>perceptron</b> are investigated. The usual Gardner-Derrida one leads to a {{storage capacity}} close to the upper bound, which is independent of the learning algorithm considered. ...|$|E
40|$|The optical {{implementation}} of neural networks {{can be realized}} by storing the weights in holograms with {{a limited number of}} gray values. Motivated by this fact, we focused our investigation in this thesis on analyzing the dependence of the generalization and training errors of a <b>simple</b> <b>perceptron</b> with discrete weights, on the training set size, and on the number of allowed discrete values...|$|E
40|$|We {{study the}} first-order phase {{transition}} {{in the model}} of a <b>simple</b> <b>perceptron</b> with continuous weights and large, but finite value of the inputs. Making the analogy with the usual finite-size physical systems, we calculate the shift and the rounding exponents near the transition point. In {{the case of a}} general perceptron with larger variety of outputs, the analysis gives only bounds for the exponents...|$|E
40|$|This paper explores some {{algorithms}} for automatic quantization of real-valued datasets using thermometer {{codes for}} pattern classification applications. Experimental {{results indicate that}} a relatively simple randomized thermometer code generation technique can result in quantized datasets that when used to train <b>simple</b> <b>perceptrons,</b> can yield generalization on test data that is substantially better than that obtained with their unquantized counterparts. 1 Introduction Artificial neural networks offer a particularly attractive framework {{for the design of}} pattern classification and inductive knowledge acquisition systems {{for a number of reasons}} including their potential for parallelism and fault tolerance. A single threshold logic unit (TLU), also known as <b>perceptron,</b> is a <b>simple</b> neural network that can be trained to classify a set of input patterns into one of two classes. A TLU is an elementary processing unit that computes a function of the weighted sum of its inputs. Assuming that the [...] ...|$|R
40|$|We give {{a review}} of basic {{statistical}} and neural techniques for classification. Statistical techniques {{are based on the}} idea of estimating class-conditional likelihoods and using Bayes rule to convert these to posterior class probabilities whereas neural techniques estimate directly the posteriors. Statistical techniques include (i) Parametric (Gaussian) Bayes classifiers, (ii) Nonparametric kernel-based density estimators like k-nearest neighbor and Parzen windows, and (iii) mixtures of (Gaussian) densities (a special case of which is the Learning Vector Quantization). As neural classifiers, we include <b>simple</b> <b>perceptrons</b> and multilayer perceptrons with sigmoid and Gaussian hidden units. The neural and statistical techniques are quite similar in many respects and many approaches have been discovered independently twice, once in 1960 s by statisticians and once in 1980 s by the neural network researchers. One of the aims {{of this article is to}} make this link more apparent. We al [...] ...|$|R
40|$|This chapter gives a short {{introduction}} {{to the theory of}} neural networks in the context of supervised learning. <b>Simple</b> <b>perceptrons,</b> multilayer perceptrons, radial basis function networks, the backpropagation algorithm, and other topics are discussed. The problem of overfitting and generalization which is of eminent practical importance is emphasized. All theoretical concepts are illustrated by JAVA-applets which the reader can download from: http : ==diwww:epf l:ch=mantra= 1. 1 Introduction Over the last twenty years neural networks have found their way into numerous applications ranging from character recognition, plant optimization, to financial prediction; see e. g. [2]. Often the term 'neural network' is used is a rather broad sense which groups together different families of algorithms and methods. On the biological end of the spectrum, the term neural network is used to describe models of computation in single neurons or whole areas of the brain. In the following we focus on arti [...] ...|$|R
40|$|Various {{algorithms}} {{for constructing}} weight matrices for Hopfield-type associative memories are reviewed, including ones with much higher capacity than the basic model. These alternative algorithms either iteratively approximate the projection weight matrix or use <b>simple</b> <b>perceptron</b> learning. An experimental {{investigation of the}} performance of networks trained by these algorithms is presented, including measurements of capacity, training time and their ability to correct corrupted versions of the training patterns...|$|E
40|$|We {{study the}} first-order {{transition}} {{in the model}} of a <b>simple</b> <b>perceptron</b> with continuous weights and large, bit finite value of the inputs. Making the analogy with the usual finite-size physical systems, we calculate the shift and the rounding exponents near the transition point. In {{the case of a}} general perceptron with larger variety of inputs, the analysis only gives bounds for the exponents. Comment: 7 pages, 1 figure, Submitted to Phys. Lett. ...|$|E
40|$|We {{study the}} {{generalization}} {{ability of a}} <b>simple</b> <b>perceptron</b> which learns unlearnable rules. The rules are presented by a teacher perceptron with a non-monotonic transfer function. The student is trained in the on-line mode. The asymptotic behaviour of the generalization error is estimated under various conditions. Several learning strategies are proposed and improved to obtain the theoretical lower bound of the generalization error. Comment: LaTeX 20 pages using IOP LaTeX preprint style file, 14 figure...|$|E
40|$|Abstract –This 1 paper {{presents}} some apprilcations {{of neural}} {{networks in the}} microwave modeling. The applications are related to modeling of either passive or active structures and devices. Modeling is performed using not only <b>simple</b> multilayer <b>perceptron</b> network (MLP) but also advanced knowledge based neural network (KBNN) structures. Keywords–Neural network, modeling, microwave, microstrip gap, microwave cavity, microwave transistor, noise parameters, scattering parameters. I...|$|R
40|$|We {{present an}} {{extension}} of Freund and Schapire's AdaBoost algorithm that allows an input-dependent combination of the base hypotheses. A separate weak learner is used for determining the input-dependent weights of each hypothesis. The error function minimized by these additional weak learners is a margin cost function that {{has also been shown}} to be minimized by AdaBoost. The weak learners used for dynamically combining the base hypotheses are <b>simple</b> <b>perceptrons.</b> We compare our dynamic combination model with AdaBoost on a range of binary and multi-class classication problems. It is shown that the dynamic approach signicantly improves the results on most data sets when (rather weak) perceptron base hypotheses are used, while the dierence in performance is small when the base hypotheses are MLPs. 2 IDIAP{RR 99 - 09 1 Introduction Ensemble methods such as bagging [1] and boosting [2] operate by taking a base algorithm and invoking it repeatedly with dierent training sets. A main cha [...] ...|$|R
40|$|In {{this paper}} we {{introduce}} Nieme, 1 a machine learning library for large-scale classification, regression and ranking. Nieme {{relies on the}} framework of energy-based models (LeCun et al., 2006) which unifies several learning algorithms ranging from <b>simple</b> <b>perceptrons</b> to recent models such as the pegasos support vector machine or l 1 -regularized maximum entropy models. This framework also unifies batch and stochastic learning which are both seen as energy minimization problems. Nieme can hence {{be used in a}} wide range of situations, but is particularly interesting for large-scale learning tasks where both the examples and the features are processed incrementally. Being able to deal with new incoming features at any time within the learning process is another original feature of the Nieme toolbox. Nieme is released under the GPL license. It is efficiently implemented in C++, it works on Linux, Mac OS X and Windows and provides interfaces for C++, Java and Python...|$|R
40|$|Abstract In {{this article}} we {{study the effects of}} {{introducing}} structure in the input distribution of the data to be learnt by a <b>simple</b> <b>perceptron.</b> We determine the learning curves within the framework of Statistical Mechanics. Stepwise generalization occurs {{as a function of the}} number of examples when the distribution of patterns is highly anisotropic. Although extremely simple, the model seems to capture the relevant features of a class of Support Vector Machines which was recently shown to present this behavior...|$|E
40|$|Abstract: High {{capacity}} associative {{neural networks}} {{can be built}} from networks of perceptrons, trained using <b>simple</b> <b>perceptron</b> training. Such networks perform much better than those trained using the standard Hopfield one shot Hebbian learning. An experimental investigation into how such networks perform when the connection weights are not free to take any value is reported. The three restrictions investigated are: a symmetry constraint, a sign constraint and a dilution constraint. The selection of these constraints is motivated by both engineering and biological considerations. ...|$|E
40|$|The storage {{capacity}} of an incremental learning algorithm for the parity machine, the Tilinglike Learning Algorithm, is analytically determined in {{the limit of}} a large number of hidden perceptrons. Different learning rules for the <b>simple</b> <b>perceptron</b> are investigated. The usual Gardner-Derrida one leads to a {{storage capacity}} close to the upper bound, which is independent of the learning algorithm considered. Comment: Proceedings of the Conference Disordered and Complex Systems, King's College, London, July 2000. 6 pages, 1 figure, uses aipproc. st...|$|E
40|$|In {{this report}} {{we discuss the}} use of two simple {{classifiers}} to initialise the input-tohidden layer of a one-hidden-layer neural network. These classifiers divide the input space in convex regions that can be represented by membership functions. These functions are then {{used to determine the}} weights of the first layer of a feedforward network. Keywords and phrases: mapping decision trees onto neural networks, <b>simple</b> <b>perceptrons,</b> LVQ-networks, initialisation of feedforward networks. 1 Introduction In this report we discuss how two well-known classifiers can be used to initialise the first layer of a neural network. These classifiers divide the input space in convex regions. These regions are represented by so-called membership functions generated during training. Subsequently, the coefficients of these membership functions are used to initialise (the weights of) a neural network. This is called a mapping of the classifiers onto the neural net. The neural net is then further trained to i [...] ...|$|R
40|$|This paper reports an {{investigation}} of the relationship between learning and evolution in populations of backprop networks. The simulation environment, which has been used previously by Parisi, Nolfi and Cecconi (Parisi, Nolfi and Cecconi, 1991), consists of a two dimensional grid with a random sample of the vertices containing a reward for those animats that visit them. Each animat is a small backprop neural network with a limited lifetime. Initial weights are chosen using a genetic algorithm. Depending on the experiment, these weights {{may or may not be}} modified by backpropagation during the animats' lifetimes in response to their performance in the environment. We find no evidence that learning during animats' lifetimes has any beneficial effect on evolution for this particular task. We find that the task is learned more successfully by <b>simple</b> <b>perceptrons</b> than by multi-layer networks, which suggests that it is linearly separable. Finally, we offer an alternative, more simple, explana [...] ...|$|R
40|$|This paper {{shows how}} sparse, high-dimensional {{probability}} distributions could {{be represented by}} neurons with exponential compression. The representation is a novel application of compressive sensing to sparse probability distributions {{rather than to the}} usual sparse signals. The compressive measurements correspond to expected values of nonlinear functions of the probabilistically distributed variables. When these expected values are estimated by sampling, the quality of the compressed representation is limited only by the quality of sampling. Since the compression preserves the geometric structure of the space of sparse probability distributions, probabilistic computation can be performed in the compressed domain. Interestingly, functions satisfying the requirements of compressive sensing can be implemented as <b>simple</b> <b>perceptrons.</b> If we use <b>perceptrons</b> as a <b>simple</b> model of feedforward computation by neurons, these results show that the mean activity of {{a relatively small number of}} neurons can accurately represent a high-dimensional joint distribution implicitly, even without accounting for any noise correlations. This comprises a novel hypothesis for how neurons could encode probabilities in the brain. Comment: 9 pages, 4 figure...|$|R
