36|288|Public
5000|$|Eight-way <b>set</b> <b>associative</b> <b>cache,</b> {{a common}} choice for later {{implementations}} ...|$|E
50|$|<b>Set</b> <b>associative</b> <b>cache</b> is a {{trade-off}} between Direct mapped cache and Fully associative cache.|$|E
50|$|The <b>Set</b> <b>associative</b> <b>cache</b> can be {{imagined}} as a (n*m) matrix. The cache {{is divided into}} ‘n’ sets and each set contains ‘m’ cache lines. A memory block is first mapped onto a set and then placed into any cache line of the set.|$|E
5000|$|The 64 Kib L1 Cache of the WinChip 2 used a 32 KB 2-way <b>set</b> <b>associative</b> code <b>cache</b> and a 32 KB 4-way <b>set</b> <b>associative</b> data <b>cache.</b>|$|R
40|$|Simulation time {{is often}} the {{bottleneck}} in the cache design process. In this paper, algorithms for the efficient simulation of direct mapped and <b>set</b> <b>associative</b> <b>caches</b> are presented. Two classes of direct mapped caches are considered: fixed line size caches and fixed size caches. A binomial tree representation of the caches in each class is introduced. The fixed line size class is considered for <b>set</b> <b>associative</b> <b>caches.</b> A generalization of the binomial tree data structure is introduced and the fixed line size class of <b>set</b> <b>associative</b> <b>caches</b> is represented using the generalized binomial tree. Algorithms are developed that use the data structures to determine miss ratios for the caches in each class. Analytical and empirical comparisons of the algorithms to previously published algorithms such as all-associativity and forest simulation are presented. Analytically it is shown that the new algorithms always perform better than earlier algorithms. Empirically, the new algorithms are shown to [...] ...|$|R
50|$|For this example, a direct-mapped 16 KB data cache which returns doubleword (8-byte) aligned {{values is}} assumed. Each {{line of the}} SRAM is 8 bytes, and there are 2048 lines, {{addressed}} by Addr13:3. The sum addressed SRAM idea applies equally well to <b>set</b> <b>associative</b> <b>caches.</b>|$|R
50|$|Since {{conflict}} misses can {{be attributed}} to the lack of sufficient associativity, increasing the associativity to a certain extent (8‐way associativity almost as effective as fully‐associative) decreases the amount of conflict misses, however, such an approach increases the cache access time and consumes a lot more power than a <b>set</b> <b>associative</b> <b>cache.</b>|$|E
40|$|Set {{associative}} caches have fixed ways. Entire cache {{is enabled}} during cache operation. This paper proposes cache architecture mapping cache line to fixed cache way of mapped set. The address is mapped to set as in conventional <b>set</b> <b>associative</b> <b>cache.</b> The tag {{value of the}} mapped line is divided into blocks of size of number of cache ways. The average of maximum and minimum frequency of this division is the mapped way. The proposed model is simulated with SPEC 2 K benchmarks. The average memory access time degradation of 3. 8 % is seen over traditional <b>set</b> <b>associative</b> <b>cache.</b> The energy saving of 49 % is observed in proposed model...|$|E
40|$|Abstract — A 3 D {{computer}} graphics (3 DCG) library with energy-efficient cache architecture is implemented for mobile multimedia systems. The developed library {{is based on}} fixed-point arithmetic for low energy consumption. To achieve high performance, the library is optimized at both of the assembly and the algorithm levels on 3 different Advanced RISC Machines (ARM) processors. In order to enhance energy-efficiency as well as performance furthermore, a series of simulations have been performed on application programs with various cache configurations. We find that 2 -way <b>set</b> <b>associative</b> <b>cache</b> consumes low energy with negligible performance degradation. In this cache system, optimized library can achieve 66. 1 % performance improvement and 25. 3 % energy saving in average compared with the conventional 4 -way <b>set</b> <b>associative</b> <b>cache</b> system. Software and hardware co-optimization achieves 67 K polygons/sec with low energy consumption. We verified the graphics library with proposed cache architecture by implementing a mobile graphics LSI. I...|$|E
40|$|Cache {{ways are}} fixed in {{traditional}} caches. This paper proposes an algorithm to have variable {{number of ways}} in <b>set</b> <b>associative</b> <b>caches.</b> The cache {{is assumed to be}} fast registers. A free register to a set is allocated on cold miss. The least recently used policy is used to replace a way in case of no free registers. This algorithm results in variable number of ways for mapped sets. Simulations were performed with SPEC 2 K benchmarks on the proposed model. An improvement of 3 % is seen in average memory access time...|$|R
30|$|For {{comparison}} purposes, {{we chose}} a base cache hierarchy configuration {{defined by the}} 8 -kByte, 4 -way <b>set</b> <b>associative</b> level one <b>cache</b> with a 64 -Byte line size, and a 64 -kByte, 4 -way <b>set</b> <b>associative</b> level two <b>cache</b> with a 64 -Byte line size—a fairly common configuration.|$|R
5000|$|L1 {{cache size}} 16 KB {{write-back}} 4-way <b>set</b> <b>associative</b> unified I/D <b>cache.</b>|$|R
40|$|We Report On The Design Of Efficient Cache Controller Suitable For Use In FPGA-Based Processors. Semiconductor Memory Which Can Operate At Speeds Comparable With The Operation Of The Processor Exists; It Is Not Economical To Provide All The Main Memory With Very High Speed Semiconductor Memory. The Problem Can Be Alleviated By Introducing A Small Block Of High Speed Memory Called A Cache Between The Main Memory And The Processor. Set-Associative Mapping Compromise Between A Fully Associative Cache And A Direct Mapped Cache, As It Increases Speed. With Reference To <b>Set</b> <b>Associative</b> <b>Cache</b> Memory We Have Designed Cache Controller. Spatial Locality Of Reference Is Used For Tracking Cache Miss Induced In Cache Memory. In Order To Increase Speed, Less Power Consumption And Tracking Of Cache Miss In 4 -Way <b>Set</b> <b>Associative</b> <b>Cache</b> Memory, FPGA Cache Controller Will Proposed By This Research Work. We Believe That Our Design Work Achieves Less Circuit Complexity, Less Power Consumption And High Speed In Terms Of FPGA Resource Usag...|$|E
40|$|Caches may consume {{half of a}} microprocessor’s {{total power}} and cache misses incur {{accessing}} off-chip memory, which is both time consuming and energy costly. Therefore, minimizing cache power consumption and reducing cache misses are important to reduce total energy consumption of embedded systems. Direct mapped caches consume much less power than that of same sized set associative caches but with a poor hit rate on average. Through experiments, we observe that memory space of direct mapped instruction caches is not used efficiently in most embedded applications. We design an efficient cache – a configurable instruction cache that can be tuned to utilize the cache sets efficiently for a particular application such that cache memory is exploited more efficiently by index remapping. Experiments on 11 benchmarks drawn from Mediabench show that the efficient cache achieves almost the same miss rate as a conventional two-way <b>set</b> <b>associative</b> <b>cache</b> on average and with total memory-access energy savings of 30 % compared with a conventional two-way <b>set</b> <b>associative</b> <b>cache...</b>|$|E
40|$|A {{probabilistic}} model {{to estimate the}} number of misses on a <b>set</b> <b>associative</b> <b>cache</b> with an LRU replacement algorithm is introduced. Such type of modeling {{has been done by}} our group in previous works for sparse matrices with an uniform distribution of the non zero elements. In this paper we present new results focusing in different types of distributions that usually appear in some well-known real matrices suites, such as Harwell-Boeing or NEP...|$|E
50|$|The {{instruction}} cache is external and supports {{a capacity of}} 256 KB to 4 MB. Instructions are pre-decoded before they enter the cache by adding five bits to each instruction. These bits {{reduce the amount of}} time required to decode the instruction later in the pipeline. The {{instruction cache}} is direct-mapped to avoid the complexity of <b>set</b> <b>associative</b> <b>caches</b> and is accessed via a 148-bit bus. The tags for the cache are also external. It is built from synchronous SRAMs (SSRAMs).|$|R
40|$|Performance tuning becomes harder as {{computer}} technology advances. One {{of the factors}} is the increasing complexity of memory hierarchies. Most modern machines now use at least one level of cache memory. To reduce execution stalls, cache misses must be very low. Software techniques used to improve locality have been developped for numerical codes, such as loop blocking and copying. Unfortunately, the behavior of direct mapped and <b>set</b> <b>associative</b> <b>caches</b> is still erratic when large numerical data is accessed. Execution time can vary drasticly for the same loop kernel depending on uncontrolled factors such as array leading size. The only software method available to improve execution time stability is the copying of frequently used data, which is costly in execution time. Users are not usually cache organisation experts. They {{are not aware of}} such phenomena, and have no control over it. In this paper, we show that the recently proposed 4 -way skewed <b>associative</b> <b>cache</b> yields very stable e [...] ...|$|R
40|$|As {{processors}} become faster, {{memory hierarchy}} becomes a serious bottleneck. In recent years memory speeds {{have failed to}} keep up with processor speeds and the gap has been steadily increasing. Cache performance has become a critical parameter in system performance. Victim caches are small fully <b>associative</b> <b>caches</b> placed between a cache and its refill path. This results in the misses served by the victim caches, having only a very small miss penalty, typically one cycle, as opposed to several cycles for main memory. Small victim caches from one to five line sizes are sufficient to significantly improve the effective cache hit rate. This improvement however is sub-linear {{in the size of the}} victim cache and can be poorer for <b>set</b> <b>associative</b> <b>caches.</b> We propose to do a quantitative comparison of set-associative caches and direct-mapped caches together with a victim cache. In particular we aim to find the "associativity " of a direct-mapped cache with a victim cache. This would [...] ...|$|R
40|$|In this study, the {{performance}} of an e-learning environment is analyzed and evaluated in terms of average network traffic and upload/download rates under various cache memory organizations. In particular, we study the influence of three cache organizations, namely fully associative, direct, and set associative caches. As {{a result of this}} work, we recommend the <b>set</b> <b>associative</b> <b>cache</b> memory organization with the LFU replacement policy, as this led to optimal performance in e-learning environments with the highest hit ratio and upload/download rates...|$|E
40|$|Current cache design {{supports}} only {{fixed line}} size. In this report, we propose an adaptive line size cache where line size can dynamically change {{based on a}} locality prediction mechanism. Overall better utilization of spatial and temporal localities is achieved by this novel cache design. Experiments on SPEC 95 benchmarks show that this cache design can significantly reduce cache miss rates. 2 -level adaptive line size cache can achieve average 1. 7 speedup over tradition 2 -level cache with L 1 line size 32 B and L 2 line size 64 B. And 128 k direct-mapped L 2 cache outperforms 512 K 2 -way <b>set</b> <b>associative</b> <b>cache...</b>|$|E
40|$|The precise {{determination}} of worst-case execution times (WCETs) for programs is mostly being performed on fully linked executables, since all needed {{information is available}} and all machine parameters influencing cache performance are available to the analysis. This paper describes how to perform a component-wise prediction of the instruction cache behavior guaranteeing conservative results compared {{to an analysis of}} a fully linked executable. This proves the correctness of the method based on a previous proof of correctness of the analysis of fully linked executables. The analysis is described for a general A-way <b>set</b> <b>associative</b> <b>cache.</b> The only assumption is that the replacement strategy is LRU...|$|E
40|$|Caching is a {{technique}} first used by memory management to reduce bus traffic and latency of data access. Web traffic has increased tremendously {{since the beginning of}} the 1990 s. With the significant increase of Web traffic, caching techniques are applied to Web caching to reduce network traffic, user perceived latency, and server load by caching the documents in local proxies. In this paper, analization of both advantages and disadvantages of some current Web cache replacement algorithms including lowest relative value algorithm, least weighted usage algorithm and least unified-value (LUV) algorithm is done. Based on our analysis, we proposed a new algorithm, called least grade replacement (LGR), which takes recency, frequency, perfect-history, and document size into account for Web cache optimization. The optimal recency coefficients were determined by using 2 - and 4 -way <b>set</b> <b>associative</b> <b>caches.</b> The cache size was varied from 32 k to 256 k in the simulation. The simulation results showed that the new algorithm (LGR) is better than LRU and LFU in terms of hit ratio (BR) and byte hit ratio (BHR) ...|$|R
5000|$|... 32-way <b>set</b> <b>associative</b> L3 victim <b>cache</b> sized {{at least}} 2 MB, shared between {{processing}} cores {{on a single}} die (each with 512 K of independent exclusive L2 cache), with a sharing-aware replacement policy.|$|R
50|$|The {{original}} Pentium 4 processor {{also had}} an eight-way <b>set</b> <b>associative</b> L2 integrated <b>cache</b> 256 KB in size, with 128-byte cache blocks. This implies 17 + 8 + 7 = 32, and hence 17 bits for the tag field.|$|R
40|$|In this paper, {{we propose}} an {{approach}} to estimate the Worst Case Response Time (WCRT) of tasks in a preemptive multi-tasking single-processor real-time system with a <b>set</b> <b>associative</b> <b>cache.</b> The approach focuses on analyzing the cache reload overhead caused by preemptions. We combine inter-task cache eviction behavior analysis and path analysis of the preempted task to reduce, in our analysis, the estimate {{of the number of}} cache lines that can possibly be evicted by the preempting task (thus requiring a reload by the preempted task). A mobile robot application which contains three tasks is used to test our approach. The experimental results show that our approach can tighten the WCRT estimate by up to 73 % over prior state-of-the-art. 1...|$|E
40|$|The cache {{hierarchy}} {{prevalent in}} todays high performance processors {{has to be}} taken into account in order to design algorithms which perform well in practice. We start from the empirical observation that external memory algorithms often turn out to be good algorithms for cached memory. This is not self evident since caches have a fixed and quite restrictive algorithm choosing the content of the cache. We investigate the impact of this restriction for the frequently occurring case of access to multiple sequences. We show that any access pattern to k = Θ(M=B) sequential data streams can be efficiently supported on an a-way <b>set</b> <b>associative</b> <b>cache</b> with capacity M and line size B. The bounds are tight up to lower order terms...|$|E
40|$|Based on {{the system}} request or {{application}} request a set of word is loaded on the cache memory. When the system is switched off the cache history gets abscond. The performance of CPU {{is based on the}} factors such as cache hit, write through cache, write back, cache memory mapping technique, CPU speed, bandwidth, cache memory size etc., Some of the standard cache addresses mapping techniques are set associative, associative and direct mapping technique. This paper proposes a novel idea of <b>set</b> <b>associative</b> <b>cache</b> address mapping using linear equation. The standard set associative mapping is remapped with linear set associative for to secure the data in a non sequential portion by having the standard mapping execution time. This is mainly focus on to design the cache enhancement and improvement...|$|E
40|$|We {{present a}} {{combined}} architectural and circuit technique {{for reducing the}} energy dissipation of microprocessor memory structures. This approach exploits the subarray partitioning of high speed memories and varying application requirements to dynamically disable partitions during appropriate execution periods. When applied to 4 -way <b>set</b> <b>associative</b> <b>caches,</b> trading off a 2 % performance degradation yields a combined 40 % reduction in L 1 Dcache and L 2 cache energy dissipation. 1. INTRODUCTION The continuing microprocessor performance gains afforded by advances in semiconductor technology have come {{at the cost of}} increased power consumption. Each new high performance microprocessor generation brings additional on-chip functionality, and thus an increase in switching capacitance, as well as increased clock speeds over the previous generation. For example, both transistor count and clock speed have roughly doubled in the three years separating the Alpha 21164 microprocessor [6, 11] and the [...] ...|$|R
40|$|This paper {{presents}} a technique for eliminating redundant cache-tag and cache-way accesses to reduce power consumption. The basic {{idea is to}} keep {{a small number of}} Most Recently Used (MRU) addresses in a Memory Address Buffer (MAB) and to omit redundant tag and way accesses when there is a MAB-hit. Since the approach keeps only tag and set-index values in the MAB, the energy and area overheads are relatively small even for a MAB with a large number of entries. Furthermore, the approach does not sacrifice the performance. In other words, neither the cycle time nor the number of executed cycles increases. The proposed technique has been applied to Fujitsu VLIW processor (FR-V) and its power saving has been estimated using NanoSim. Experiments for 32 kB 2 -way <b>set</b> <b>associative</b> <b>caches</b> show the power consumption of I-cache and D-cache can be reduced by 40 % and 50 %, respectively. ...|$|R
40|$|It {{has long}} been empirically {{observed}} that the cache miss rate decreased as a power law of cache size, where the power was approximately- 1 / 2. In this paper, we examine the dependence of the cache miss rate on cache size both theoretically and through simulation. By combining the observed time dependence of the cache reference pattern with a statistical treatment of cache entry replacement, we predict that the cache miss rate should vary with cache size as an inverse power law for a first level cache. The exponent in the power law {{is directly related to}} the time dependence of cache references, and lies between- 0. 3 to- 0. 7. Results are presented for both direct mapped and <b>set</b> <b>associative</b> <b>caches,</b> and for various levels of the cache hierarchy. Our results demonstrate that the dependence of cache miss rate on cache size arises from the temporal dependence of the cache access pattern. 1...|$|R
40|$|In this paper, {{we propose}} a cache design that {{provides}} the same miss rate as a two-way <b>set</b> <b>associative</b> <b>cache,</b> but with a access time closer to a direct-mapped cache. As with other designs, a traditional direct-mapped cache is conceptually partitioned into multiple banks, and the blocks in each set are probed, or examined, sequentially. Other designs either probe the set in a fixed order or add extra delay in the access path for all accesses. We use prediction sources to guide the cache examination, {{reducing the amount of}} searching and thus the average access latency. A variety of accurate prediction sources are considered, with some being available in early pipeline stages. We feel that our design offers the same or better performance and is easier to implement than previous designs...|$|E
40|$|Increasing {{levels of}} {{microprocessor}} power dissipation call for new approaches at the architectural level that save energy by better matching of on-chip resources to application requirements. Selective cache ways provides {{the ability to}} disable {{a subset of the}} ways in a <b>set</b> <b>associative</b> <b>cache</b> during periods of modest cache activity, while the full cache may remain operational for more cache-intensive periods. Because this approach leverages the subarray partitioning that is already present for performance reasons, only minor changes to a conventional cache are required, and therefore, full-speed cache operation can be maintained. Furthermore, the tradeoff between performance and energy is flexible, and can be dynamically tailored to meet changing application and machine environmental conditions. We show that trading off a small performance degradation for energy savings can produce a significant reduction in cache energy dissipation using this approach. 1. Introduction Contin [...] ...|$|E
40|$|In this paper, we {{investigate}} {{the problem of}} inter-task cache interference in preemptive multi-tasking real-time systems. A prioritized cache is used to reduce cache conflicts among tasks by partitioning the cache. Cache partitions are assigned to tasks according to their priorities. We extend a known tool, SYMTA, in order to estimate the Worst Case Execution Time of each task executing on a uniprocessor with a unified prioritized L 1 cache. Furthermore, we apply a formal timing analysis approach to estimate the Worst Case Response Time (WCRT) of each task using the prioritized cache. Our WCRT analysis handles nested preemptions. WCRT using a prioritized cache is compared to using a conventional <b>set</b> <b>associative</b> <b>cache</b> {{of the same size}} and associativity. Our experiments show that the WCRT estimate can be reduced up to 26 % when a prioritized cache is used...|$|E
40|$|Submitted {{on behalf}} of EDAA ([URL] audienceThis paper {{presents}} a technique for eliminating redundant cache-tag and cache-way accesses to reduce power consumption. The basic idea is to keep {{a small number of}} Most Recently Used (MRU) addresses in a Memory Address Buffer (MAB) and to omit redundant tag and way accesses when there is a MAB-hit. Since the approach keeps only tag and set-index values in the MAB, the energy and area overheads are relatively small even for a MAB with a large number of entries. Furthermore, the approach does not sacrifice the performance. In other words, neither the cycle time nor the number of executed cycles increases. The proposed technique has been applied to Fujitsu VLIW processor (FR-V) and its power saving has been estimated using NanoSim. Experiments for 32 kB 2 -way <b>set</b> <b>associative</b> <b>caches</b> show the power consumption of I-cache and D-cache can be reduced by 40 % and 50 %, respectively...|$|R
40|$|This thesis {{proposes a}} new mechanism, called Store Buffers, for {{implementing}} single cycle store instructions in a pipelined processor. Single cycle store instructions {{are difficult to}} implement because {{in most cases the}} tag check must be performed before the data can be written into the data cache. Store buffers allow a store instruction to read the cache tag as it. passes through the pipe while keeping the store instruction data buffered in a backup register until the data cache is free. This strategy guarantees single cycle store execution without increasing the hit access time or degrading the performance of the data cache for simple direct-mapped caches, as well as for more complex <b>set</b> <b>associative</b> and write-back <b>caches.</b> As larger caches are incorporated on-chip, the speed of store instructions becomes an increasingly important part of the overall performance. The first part of the thesis describes the design and implementation of store buffers in write through, write-back, direct-mapped and <b>set</b> <b>associative</b> <b>caches.</b> The second part describes the implementation and simulation of store buffers in a 6 -stage pipeline with a direct mapped write-through pipelined cache. The performance of this method is compared to other cache write techniques. Preliminary results show that store buffers perform better than other store strategies under high IO latencies and cache thrashing. With as few as three buffers, they significantly reduce the number of cycles per instruction. by Radhika Nagpal. Thesis (B. S. and M. S.) [...] Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1994. Includes bibliographical references (p. 87) ...|$|R
40|$|Improving cache {{performance}} requires understanding cache behavior. However, measuring cache {{performance for}} one or two data input sets provides little insight into how cache behavior varies across all data input sets and all cache configurations. This paper uses locality analysis to generate a parameterized model of program cache behavior. Given a cache size and associativity, this model predicts the miss rate for arbitrary data input set sizes. This model also identifies critical data input sizes where cache behavior exhibits marked changes. Experiments show this technique is within 2 percent of the hit rate for <b>set</b> <b>associative</b> <b>caches</b> on a <b>set</b> of floating-point and integer programs using array and pointer-based data structures. Building on the new model, this paper presents an interactive visualization tool that uses a three-dimensional plot to show miss rate changes across program data sizes and cache sizes and its use in evaluating compiler transformations. Other uses of this visualization tool include assisting machine and benchmark-set design. The tool can be accessed on the Web a...|$|R
