350|325|Public
50|$|A root word {{can change}} its stress when affixes are added, because affixes carry their own {{inherent}} stress. /'sadi/ ‘one’ (penultimate) /ma'sadi/ ‘united’ (penultimate) /fagmasadi'un/ ‘unity’ (final) /namasadi'an/ ‘agreement’ (final)In affective <b>speech</b> (<b>utterances</b> {{in which the}} speaker wishes to convey emotion), lengthening may change stress:/na'taw/ ‘what?’ may become /:na:taw/ when said with rising pitch on the first syllable and low pitch on the second. This indicates acute surprise.|$|E
5000|$|In child-directed <b>speech,</b> <b>utterances</b> {{have several}} {{additional}} features. For example, the phonology in child-directed speech is different: Utterances are spoken more slowly, with longer pauses in between utterances, higher pitches, etc. The lexis and semantics differ, and a speaker uses words suited for children, [...] "doggie" [...] instead of [...] "dog," [...] for example. The grammar is simpler, repetitive, with less use of verbs and adjectives. There {{is a greater}} use of one word utterances and the pragmatics uses supportive language like expansions and re-casting.|$|E
5000|$|The {{subjects}} in the above experiment were not asked to report the full dream scenario during which the utterances were made. Excluded, therefore, from the analysis was any consideration of [...] "pragmatic competence," [...] the [...] "knowledge of condition and manner or appropriate use his or her linguistic ability in conformity with various purposes," [...] which forms part of the broader field of language psychology (rather than formal psycholinguistics). Dream utterances plus their dream scenario contexts were gathered in a second experiment, again following carefully defined protocols, this time using the telephone elicitation technique. (Volunteer subjects were awakened on random nights at random hours, on average no {{more than once a}} week.) 77 dream reports from 33 subjects were tape-recorded and transcribed. These contained 92 dream <b>speech</b> <b>utterances</b> elicited verbatim from the subjects immediately after provoked awakening (to minimize deterioration in memory recall) plus an additional 113 utterances (81 in direct verbatim form, 32 in indirect form) contained in the subsequent telling of the entire dream. 40 percent of the utterances were said by the dreamer; 60 percent by someone else in the dream scenario, usually addressed to the dreamer (a reversal of the proportions in the linguistic performance experiment).|$|E
30|$|Step 1. Decode {{the input}} <b>speech</b> <b>utterance</b> with an LVCSR decoder, {{collecting}} phone back-tracking information {{in both the}} word-based phone alignment and bag-of-phone methods. Generate an initial phone-level transcription of the <b>speech</b> <b>utterance</b> {{and a bag of}} phone hypotheses.|$|R
40|$|An {{automatic}} tool {{to reduce}} foreign-accent is described and evaluated. An unaccented <b>speech</b> <b>utterance</b> {{was used to}} improve three prosodic features of a corresponding foreign-accented utterance. The duration, pitch and intensity of the foreign-accented <b>speech</b> <b>utterance</b> were modified using DTW (Dynamic Time Warping), WSOLA (Waveform Similarity Overlap Add), and other automatic speech processing algorithms. The modified <b>speech</b> <b>utterance</b> was then evaluated to determine the perceived foreign accent compared to the original. Fifteen native speakers of American English {{took part in the}} perceptual test to rate the degree of foreign-accent in Korean-accented American English. The results show that the modified Korean-accented utterances were perceived to have a lower degree of foreign-accent than the original Korean-accented utterances. 1...|$|R
3000|$|... of the <b>speech</b> <b>utterance</b> X. N-best {{list for}} its {{simplicity}} is widely used, and N-best rescoring in LVCSR is reviewed here.|$|R
3000|$|Pulse {{positions}} {{representing the}} glottal closures are {{extracted from the}} <b>speech</b> <b>utterances.</b> Each pulse position [...]...|$|E
40|$|A novel {{method to}} extract the {{reverberation}} time from reverberated <b>speech</b> <b>utterances</b> is presented. In this study, <b>speech</b> <b>utterances</b> are restricted to pronounced digits; uncontrolled discourse is not considered. The reverberation times considered are wide band, within the frequency range of <b>speech</b> <b>utterances.</b> A multilayer feed forward neural network is trained on speech examples with known reverberation times generated by a room simulator. The speech signals are preprocessed by calculating short-term rms values. A second decision-based neural network is added to improve {{the reliability of the}} predictions. In the retrieve phase, the trained neural networks extract room reverberation times from speech signals picked up in the rooms to an accuracy of 0. 1 s. This provides an alternative to traditional measurement methods and facilitates the occupied measurement of room reverberation times...|$|E
30|$|In {{the first}} mode, “clean-condition {{training}}”, the training data consist of 8440 clean <b>speech</b> <b>utterances</b> from 55 female and 55 male adults.|$|E
40|$|In our {{previous}} works, a Switching Linear Gaussian Hidden Markov Model (SLGHMM) and its segmental derivative, SSLGHMM, were proposed {{to cast the}} problem of modeling a noisy <b>speech</b> <b>utterance</b> in robust automatic speech recognition by a well-designed dynamic Bayesian network. An important issue of SSLGHMM is how to specify a switching state value for each frame of feature vector in a given <b>speech</b> <b>utterance.</b> In this paper, we propose several approaches for addressing this issue and compare their performance on Aurora 3 connected digit recognition tasks...|$|R
40|$|The {{problem of}} speaker {{identification}} {{is an area}} with many different applications. The most practical use {{can be found in}} applications dealing with security, surveillance, and automatic transcription in a multi-speaker environment. Speaker identification is a difficult task and the task has several different approaches. The state of the art for speaker identification techniques include Dynamic Time Warped (DTW) template matching, Hidden Markov Modeling (HMM), and codebook schemes based on Vector Quantization (VQ). This paper emphasizes on text dependent speaker identification, which deals with detecting a particular speaker from a known population. The system reads the <b>speech</b> <b>utterance.</b> System identifies the user by comparing the codebook of <b>speech</b> <b>utterance</b> with those of the stored in the database and lists, which contain the most likely speakers, could have given that <b>speech</b> <b>utterance.</b> The vector quantization approach will be proposed, due to ease of implementation and high accuracy...|$|R
30|$|Step 1. Decode {{the input}} <b>speech</b> <b>utterance</b> with an LVCSR decoder, {{collecting}} phone back-tracking information using {{the bag of}} phones approach. Use this to generate a bag of phone hypotheses.|$|R
40|$|This paper {{describes}} a technique for automatically generating multiple levels of linguistic annotation for a corpus of <b>speech</b> <b>utterances.</b> Using a training corpus of multilevel annotations, a corresponding finite-state representation is automatically constructed by grammatical inference. This finite-state description is then {{employed as a}} knowledge component to automatically generate a new multilevel annotation for an unseen utterance. The approach is evaluated on a small corpus of English <b>speech</b> <b>utterances</b> annotated over four levels of phonological description. ...|$|E
30|$|Apart {{from these}} {{parameter}} computations on sustained vowels, using complete <b>speech</b> <b>utterances</b> cepstral peak prominence measures and speaking rates were computed and analysed.|$|E
3000|$|Sighing {{speech was}} mostly {{observed}} in the interjections [...] "aah" [...] and [...] "haa", but was also observed along with <b>speech</b> <b>utterances</b> like in [...] "ah, arigatoo" [...] ("oh, thanks") (e.g., Figure 8 (h)).|$|E
30|$|Step 1. Decode {{the input}} <b>speech</b> <b>utterance</b> with an LVCSR decoder, {{collecting}} phone back-tracking information using word-based phone alignment {{to generate a}} word-based phone lattice. Left and right triphone contexts in the generated lattice are removed.|$|R
40|$|The use {{of single}} time-instance features, where entire <b>speech</b> <b>utterance</b> {{is used for}} feature computation, is not {{accurate}} and adequate in capturing the time localized information of short-time transient distortions and their distinction from plosive sounds of speech, particularly degraded by impulsive noise. Hence, the importance of estimating features at multiple time-instances is sought. In this, only active speech segments of degraded speech are used for features computation at multiple time-instances on per frame basis. Here, active speech means both voiced and unvoiced frames except silence. The features of different combinations of multiple contiguous active speech segments are computed and called multiple time-instances features. The joint GMM training has been done using these features along with the subjective MOS of the corresponding <b>speech</b> <b>utterance</b> to obtain the parameters of GMM. These parameters of GMM and multiple time-instances features of test speech are used to compute the objective MOS values of different combinations of multiple contiguous active speech segments. The overall objective MOS of the test <b>speech</b> <b>utterance</b> is obtained by assigning equal weight to the objective MOS values of the different combinations of multiple contiguous active speech segments. This algorithm outperforms the Recommendation ITU-T P. 563 and recently published algorithms...|$|R
40|$|The {{results in}} these thesis show that text {{independent}} speaker recognition in a <b>speech</b> <b>utterance</b> composed of several speakers {{based on a}} coarse average of the cepstrum coefficients will not give a satisfactory result. Though I find {{that it could be}} used with success in simpler speaker verification applications...|$|R
40|$|We {{present a}} {{technique}} to automatically discover the (word-sized) phone patterns that are present in <b>speech</b> <b>utterances.</b> These patterns are learnt from a set of phone lattices generated from the utter-ances. Just like children acquiring language, our system does not have prior information on what the meaningful patterns are. By applying the non-negative matrix factorisation algorithm to a fixed-length high-dimensional vector representation of the <b>speech</b> <b>utterances,</b> a decomposition in terms of additive units is obtained. We illustrate that these units correspond to words {{in case of a}} small vocabulary task. Our result also raises questions about whether explicit segmentation and clustering are needed in an unsupervised learning context...|$|E
40|$|Some glottal {{analysis}} approaches {{based upon}} linear prediction or complex cepstrum approaches have been {{proved to be}} effective to estimate glottal source from real <b>speech</b> <b>utterances.</b> We propose a new approach employing both an all-pole odd-order linear prediction to provide a coarse estimation and phase decomposition based causality/anti-causality separation to generate further refinements. The obtained measures show that this method improved performance in terms of reducing source-filter separation in estimation of glottal flow pulses (GFP). No glottal model fitting is required by this method, thus it has wide and flexible adaptation to retain fidelity of speakers's vocal features with computationally affordable resource. The method is evaluated on real <b>speech</b> <b>utterances</b> to validate it...|$|E
30|$|Two sets of {{experiments}} were performed {{pertaining to the}} matching codebook being CD- 1 or CD- 2. The first set consisted of test utterances generated by adding noise to ten clean ‘whispered’ <b>speech</b> <b>utterances</b> from the same speaker as in generation of the CD- 1 codebook. Similarly, the second set {{of experiments}} had test utterances generated using ten clean ‘normal’ <b>speech</b> <b>utterances</b> from the same speaker as in CD- 2, convolved with the same recorded impulse response as used in training CD- 2 to constitute the context match scenario for CD- 2. In both sets of experiments, the test utterances considered were different from those used in the training of the codebooks. The noisy test utterances were generated as described {{in the beginning of the}} section.|$|E
40|$|This paper aims {{to examine}} the {{problems}} and tasks of early intervention for infants with developmental disabilities through questionnaires given to nursing teachers in Na ha city. The questionnaires consisted of following items, such as ability of the activities of daily lives, <b>speech</b> <b>utterance,</b> social relationship, body and fingers movement, and hand-in-hand behavior in emotional relations with other children. Results were as follows About 70 % of disabled children on the activities of daily life behavior were well and normal. As of <b>speech</b> <b>utterance,</b> the half of them was normal, but the others were recognized non <b>speech</b> and <b>utterance.</b> Social relationship {{was similar to the}} speech, that is, the half was normal and the rest was non. Body and fingers movement was mostly no problems found except children with cerebral palys. Behavior of hand in and was also good condition in nearly 70 % children but other 30 % were needed to give taking much care of developing emotional relationship with other children. More concrete discussions are stated in the pape...|$|R
40|$|This paper {{presents}} {{a method for}} blind estimation of reverberation times in reverberant enclosures. The proposed algorithm {{is based on a}} statistical model of short-term log-energy sequences for echo-free speech. Given a <b>speech</b> <b>utterance</b> recorded in a reverberant room, it computes a Maximum Likelihood estimate of the room full-band reverberation time. The estimation method is shown to require little data and to perform satisfactorily. The method has been successfully applied to robust automatic speech recognition in reverberant environments by model selection. For this application, the reverberation time is first estimated from the reverberated <b>speech</b> <b>utterance</b> to be recognized. The estimation is then used to select the best acoustic model out of a library of models trained in various artificial reverberant conditions. Speech recognition experiments in simulated and real reverberant environments show the efficiency of our approach which outperforms standard channel normalization techniques...|$|R
40|$|This {{communication}} {{presents a}} new method for {{automatic speech recognition}} in reverber-ant environments. Our approach consists {{in the selection of}} the best acoustic model out of a library of models trained on artificially reverberated speech databases corresponding to various reverberant conditions. Given a <b>speech</b> <b>utterance</b> recorded within a reverberant room, a Maximum Likelihood estimate of the fullband room reverberation time is computed using a statistical model for short-term log-energy sequences of anechoic speech. The estimated rever-beration time is then used to select the best acoustic model, i. e., the model trained on a speech database most closely matching the estimated reverberation time, which serves to recognize the reverberated <b>speech</b> <b>utterance.</b> The proposed model selection approach is shown to improve significantly recognition accuracy for a connected digit task in both simulated and real reverberant environments, outperforming standard channel normalization techniques...|$|R
40|$|This paper {{introduces}} {{a new approach}} to ranking <b>speech</b> <b>utterances</b> by a system’s confidence that they contain a spoken word. Multiple alternate pronunciations, or degradations, of a query word’s phoneme sequence are hypothesized and incorporated into the ranking function. We consider two methods for hypothesizing these degradations, the best of which is constructed using factored phrasebased statistical machine translation. We show that this approach is able to significantly improve upon a state-of-the-art baseline technique in an evaluation on held-out speech. We evaluate our systems using three different methods for indexing the <b>speech</b> <b>utterances</b> (using phoneme, phoneme multigram, and word recognition), and find that degradation modeling shows particular promise for locating out-of-vocabulary words when the underlying indexing system is constructed with standard word-based speech recognition. ...|$|E
40|$|A {{method for}} {{performing}} speaker recognition comprises: estimating respective uncertainties of acoustic coverage of respective <b>speech</b> <b>utterance(s)</b> by {{first and second}} speakers, the acoustic coverage representing respective sounds used by the speakers when speaking; representing the respective uncertainties of acoustic coverage {{in a manner that}} allows for efficient memory usage by discarding dependencies between uncertainties of different sounds for the speakers; representing the respective uncertainties of acoustic coverage in a manner that allows for efficient computation by representing an inverse of the respective uncertainties of acoustic coverage and then discarding the dependencies between the uncertainties of different sounds for the speakers; and computing a score between the <b>speech</b> <b>utterance(s)</b> by the speakers in a manner that leverages the respective uncertainties of the acoustic coverage during the comparison, the score being indicative of a likelihood that the speakers are the same speaker...|$|E
30|$|A voice-tag (or name-tag) {{application}} converts human <b>speech</b> <b>utterances</b> into {{an abstract}} representation {{which is then}} utilized to recognize (or classify) speech in subsequent uses. The voice-tag application is the first widely deployed speech recognition application that uses technologies like Dynamic Time Warping (DTW) or HMMs in embedded platforms such as in mobile devices.|$|E
40|$|Abstract. This {{communication}} {{presents a}} new method for {{automatic speech recognition}} in reverberant environments. Our approach consists {{in the selection of}} the best acoustic model out of a library of models trained on artificially reverberated speech databases corresponding to various reverberant conditions. Given a <b>speech</b> <b>utterance</b> recorded within a reverberant room, a Maximum Likelihood estimate of the fullband room reverberation time is computed using a statistical model for short-term log-energy sequences of anechoic speech. The estimated reverberation time is then used to select the best acoustic model, i. e., the model trained on a speech database most closely matching the estimated reverberation time, which serves to recognize the reverberated <b>speech</b> <b>utterance.</b> The proposed model selection approach is shown to improve significantly recognition accuracy for a connected digit task in both simulated and real reverberant environments, outperforming standard channel normalization techniques. Keywords: room reverberation, maximum likelihood estimation, automatic speech recognition 1...|$|R
40|$|This paper {{proposes a}} method for {{detecting}} begin and end points of a <b>speech</b> <b>utterance</b> using the knowledge of Vowel Onset Points (VOPs). VOP {{is defined as the}} instant at which the onset of vowel takes place. An algorithm for VOP detection in continuous speech is discussed. VOP helps in overcoming the difficulties present in coming up with multiple thresholds followed in most of the existing begin-end detection algorithms. The VOP of the first vowel is used as an anchor point for further analysis to detect the begin of the <b>speech</b> <b>utterance.</b> Similarly, the VOP of the last vowel is used as an anchor point for detecting the end point. The performance of the proposed beginend detection algorithm is compared with the existing energy-based approach by conducting textdependent speaker verification experiments. The speaker verification system using the knowledge of VOP for begin-end detection shows a significant improvement in the performance...|$|R
40|$|Abstract—The {{speech signal}} conveys {{information}} about {{the identity of the}} speaker. The area of speaker identification is concerned with extracting the identity of the person speaking the <b>utterance.</b> As <b>speech</b> interaction with computers becomes more pervasive in activities such as the telephone, financial transactions and information retrieval from speech databases, the utility of automatically identifying a speaker is based solely on vocal characteristic. This paper emphasizes on text dependent speaker identification, which deals with detecting a particular speaker from a known population. The system prompts the user to provide <b>speech</b> <b>utterance.</b> System identifies the user by comparing the codebook of <b>speech</b> <b>utterance</b> with those of the stored in the database and lists, which contain the most likely speakers, could have given that <b>speech</b> <b>utterance.</b> The <b>speech</b> signal is recorded for N speakers further the features are extracted. Feature extraction is done by means of LPC coefficients, calculating AMDF, and DFT. The neural network is trained by applying these features as input parameters. The features are stored in templates for further comparison. The features for the speaker who has to be identified are extracted and compared with the stored templates using Back Propogation Algorithm. Here, the trained network corresponds to the output; the input is the extracted features of the speaker to be identified. The network does the weight adjustment and the best match is found to identify the speaker. The number of epochs required to get the target decides the network performance. Keywords—Average Mean Distance function...|$|R
40|$|In a {{multimodal}} conversation, user referring patterns {{could be}} complex, involving multiple referring expressions from <b>speech</b> <b>utterances</b> and multiple gestures. To resolve those references, multimodal integration based on semantic constraints is insufficient. In this paper, we describe a graph-based probabilistic approach that simultaneously combines both semantic and temporal constraints {{to achieve a}} high performance. ...|$|E
30|$|In {{the first}} two columns of the Table 8, the {{experimental}} results with acoustic models adapted to {{the male and female}} gender of speaker evaluated on the whole test data set are presented. The next two columns show the performance of language models in combination of gender-dependent acoustic models evaluated on the test <b>speech</b> <b>utterances</b> per gender.|$|E
40|$|This paper {{presents}} and discusses examples of mechanical inference problems {{which must be}} solved in order to construct effective mechanical speech understanding systems. The examples are taken from incremental simulations of a prototype speech understanding system which will use syntactic, semantic, and pragmatic information as well as acoustical and phonological information to mechanically &quot;understand &quot; continuous <b>speech</b> <b>utterances...</b>|$|E
30|$|The DAE is {{fine-tuned}} using reverberant {{speech as}} the input and clean speech as the target. The input frames and the output frames {{for the training}} were adjusted to be time-aligned in the multi-condition training-data generation process. The last portions of reverberant <b>speech</b> <b>utterance</b> files exceeding {{the length of the}} clean speech were trimmed to equalize the lengths of input and output.|$|R
40|$|We {{describe}} a new method for phoneme sequence recognition given a <b>speech</b> <b>utterance,</b> {{which is not}} based on the HMM. In contrast to HMM-based approaches, our method uses a discriminative kernel-based training procedure in which the learning process is tailored to the goal of minimizing the Levenshtein distance between the predicted phoneme sequence and the correct sequence. The phoneme sequence predictor is devised by mapping the <b>speech</b> <b>utterance</b> along with a proposed phoneme sequence to a vectorspace endowed with an inner-product that is realized by a Mercer kernel. Building on large margin techniques for predicting whole sequences, we are able to devise a learning algorithm which distills to separating the correct phoneme sequence from all other sequences. We {{describe a}}n iterative algorithm for learning the phoneme sequence recognizer and further describe an efficient implementation of it. We present initial encouraging experimental results with the TIMIT and compare the proposed method to an HMM-based approach. Index Terms: speech recognition, phoneme recognition, acoustic modeling, support vector machines...|$|R
40|$|We {{propose a}} new {{paradigm}} for aligning a phoneme sequence of a <b>speech</b> <b>utterance</b> with its acoustical signal counterpart. In contrast to common HMM-based approaches, our method employs a discriminative learning procedure in which the learning phase is tightly coupled with the alignment task at hand. The alignment function we devise is based on mapping the input acousticsymbolic representations of the <b>speech</b> <b>utterance</b> along with the target alignment into an abstract vector space. We suggest a specific mapping into the abstract vector-space which utilizes standard speech features (e. g. spectral distances) as well as confidence outputs of a framewise phoneme classifier. Building on techniques used for large margin methods for predicting whole sequences, our alignment function distills to a classifier in the abstract vector-space which separates correct alignments from incorrect ones. We describe a simple iterative algorithm for learning the alignment function and discuss its formal properties. Experiments with the TIMIT corpus show that our method outperforms the current state-of-the-art approaches. 1...|$|R
