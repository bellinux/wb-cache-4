42|790|Public
50|$|Samples for Proteomics {{contain a}} myriad of peptide {{sequence}}s, the sequence of interest may be highly represented or of low abundance. However, for successful MS analysis the peptide should be enriched within the <b>sample.</b> <b>Reduction</b> of sample complexity is achieved through selective enrichment using affinity chromatography techniques. This involves targeting a peptide with a distinguishing feature like a biotin label or a post translational modification. Interesting methods have been developed that {{include the use of}} antibodies, lectins to capture glycoproteins, immobilized metal ions to capture phosphorylated peptides and suicide enzyme substrates to capture specific enzymes. Here, chemical biologists can develop reagents to interact with substrates, specifically and tightly, to profile a targeted functional group on a proteome scale. Development of new enrichment strategies is needed in areas like non-ser/thr/tyr phosphorylation sites and other post translational modifications. Other methods of decomplexing samples relies on upstream chromatographic separations.|$|E
40|$|This paper {{presents}} a new <b>sample</b> <b>reduction</b> algorithm, <b>sample</b> <b>reduction</b> by data structure analysis (SR-DSA), for SVMs {{to improve their}} scalability. SR-DSA utilizes data structure information in determining which data points are not useful in learning the separating plane and could be removed. As this algorithm is performed before SVMs training, it avoids the problem suffered by most <b>sample</b> <b>reduction</b> methods whose choices of samples heavily depend on repeatedly training of SVMs. Experiments on both synthetic and real world datasets have shown that SR-DSA is capable of {{reducing the number of}} samples as well as the time for SVMs training while maintaining high testing accuracy. Department of ComputingRefereed conference pape...|$|E
40|$|International Telemetering Conference Proceedings / October 28 - 31, 1985 / Riviera Hotel, Las Vegas, NevadaFor space {{monitoring}} systems, {{it is necessary}} to compress the transmitted data to minimize the power and bandwidth requirements. Although there are many data compression techniques, here we will only discuss the <b>sample</b> <b>reduction</b> technique because it is the best candidate for cases when the signal contains many redundant samples and the resolution requirement for the reconstructed signal is very high. In this paper, we introduce a new code to further reduce the compressed signal data derived from the <b>sample</b> <b>reduction</b> technique. The main advantage of this approach is that we can minimize the transmission data rate without requiring a priori knowledge of the signal distribution property. In addition, it can also reduce the buffer size requirement. Examples are given for clarification and discussion...|$|E
40|$|Abstract We surview {{variants}} and {{extensions of}} the LLL-algorithm of Lenstra, Lenstra Lovász, extensions to quadratic indefinite forms and to faster and stronger reduction algorithms. The LLL-algorithm with Householder orthogonalisation in floating-point arithmetic is very efficient and highly accurate. We surview approximations of the shortest lattice vector by feasible lattice reduction, in particular by block reduction, primal-dual <b>reduction</b> and random <b>sampling</b> <b>reduction.</b> Segment reduction performs LLL-reduction in high dimension mostly working with a few local coordinates. Key words: LLL-reduction, Householder orthogonalisation, floating-point arithmetic, block reduction, segment <b>reduction,</b> primal-dual <b>reduction,</b> <b>sampling</b> <b>reduction,</b> reduction of indefinite quadratic forms. ...|$|R
40|$|We {{propose a}} {{practical}} <b>sampling</b> <b>reduction</b> algorithm for lattice bases based on work by Schnorr [1] {{as well as}} two even more effective generalizations. We report the empirical behaviour of these algorithms. We describe how <b>Sampling</b> <b>Reduction</b> allows to stage lattice attacks against the NTRU cryptosystem with smaller BKZ parameters than before and conclude that therefore the recommeded NTRU security parameters offer ≤ 74 Bit security...|$|R
40|$|In {{this paper}} we propose a {{practical}} lattice based <b>reduction</b> by <b>sampling</b> to avoid any dependence on Schnorr's Geometric Series Assumption. It is a generalization of Schnorr's RSR algorithm. It is also well defined for bases where this algorithm is not applicable. It demonstrates that the <b>sampling</b> <b>reduction</b> can significantly reduce the length of the base vectors. We also propose a practical <b>sampling</b> <b>reduction</b> algorithm for lattice bases based on work by Schnorr. We report the empirical behaviour of these algorithms...|$|R
40|$|ABSTRACT: The study {{aimed to}} i) {{quantify}} the measurement {{uncertainty in the}} physical tests of rice and beans for a hypothetical defect, ii) verify whether homogenization and <b>sample</b> <b>reduction</b> in the physical classification tests of rice and beans is effective to reduce the measurement uncertainty {{of the process and}} iii) determine whether the increase in size of beans sample increases accuracy and reduces measurement uncertainty in a significant way. Hypothetical defects in rice and beans with different damage levels were simulated according to the testing methodology determined by the Normative Ruling of each product. The homogenization and <b>sample</b> <b>reduction</b> in the physical classification of rice and beans are not effective, transferring to the final test result a high measurement uncertainty. The sample size indicated by the Normative Ruling did not allow an appropriate homogenization and should be increased...|$|E
40|$|In {{the lead}} {{optimisation}} of drug candidates, the first discovery pharmacokinetic (PK) in vivo study {{is of great}} importance to provide an initial assessment of the drug PK parameters. Data from in vivo PK studies is generally used to give feedback to chemists to improve {{the properties of the}} lead compound series, and also to calculate the initial doses in further effect studies. As the throughput in lead optimisation in drug discovery is constantly increasing, methods for high throughput bioanalysis and <b>sample</b> <b>reduction</b> are of great interest. This study presents an evaluation and a method for cassette analysis (compounds incubated separately followed by combining each time point for multiple compounds) of discovery drug compounds from PK in vivo studies in rat using high performance liquid chromatography / mass spectrometry (HPLC-MS/MS) and ultra performance liquid chromatography / mass spectrometry (UPLC-MS/MS) for quantification. Strategies for <b>sample</b> <b>reduction,</b> fast chromatography and challenges in ionization suppression using different formulation solutions are addressed...|$|E
3000|$|With the {{addition}} of QCs supported on alumina, fading and ultimate leaching of the dark yellow color due to phenolate ions occurred, and brown color of 4 -ap appeared. 4 -nitrophenolate ion peak at 400 nm got reduced and within 10 min, a new peak around 295 nm appeared due to 4 -ap [42, 43]. The spectrum of 4 -ap was verified {{with that of a}} standard <b>sample.</b> <b>Reduction</b> can be visualized with the color change, and it was almost complete which was authenticated by the optical absorbance value of 4 -ap. Excess amount of reductant NaBH 4 was used, and therefore, a pseudo-first-order rate equation may be considered. As a result of adsorption of 4 -np and BH 4 [...]...|$|E
40|$|In 2003, Schnorr {{presented}} a novel lattice basis reduction method named Random <b>Sampling</b> <b>Reduction</b> (RSR). He concluded that RSR improves the shortest vector approximation factor achievable in fixed {{time to the}} fourth root compared with previous methods. Unfortunately, RSR requires two assumptions that we cannot expect to hold in general. We propose Simple <b>Sampling</b> <b>Reduction</b> (SSR) that turns RSR into a practical algorithm and alternative algorithms that estimate the probability of finding a short vector by sampling. We demonstrate that SSR can improve reduction results and propose various generalizations of SSR that yield more short basis vectors or shorter reduction times. We also propose a quantum variant of SSR that yields a quadratic speedup. We show several cryptographic applications where an attacker can gain an advantage {{by the use of}} <b>Sampling</b> <b>Reduction.</b> Finally, we outline the design of LaRed, a flexible and interactively usable C++ framework and library for lattice reduction that implements our algorithms besides the well known LLL and BKZ algorithms...|$|R
30|$|After {{nanoparticle}} {{flooding in}} all <b>samples,</b> <b>reduction</b> in pore area of big pores was {{much higher than}} small pores. This meant that nanoparticles got attached or adsorbed to the pore walls. In addition, because of the density difference between moving nanoparticles and carrying fluid, nanoparticles could be retained in the pores due to gravity settling.|$|R
40|$|Abstract. We surview {{variants}} and {{extensions of}} the LLL-algorithm of Lenstra, Lenstra Lovász, extensions to quadratic and polynomial forms and to faster and stronger reduction algorithms. The LLL-algorithm with Householder orthogonalisation in floating-point arithmetic is very efficient and highly accurate. We surview approximations of the shortest lattice vector by feasible lattice reduction, in particular by block reduction, primal-dual <b>reduction</b> and random <b>sampling</b> <b>reduction.</b> Segment reduction performs LLL-reduction in high dimension using a few local coordinates. Keywords. LLL-reduction, Householder orthogonalisation, floating-point arithmetic, block reduction, segment reduction, quadratic forms, polynomial forms. ...|$|R
40|$|Background: Total {{homocysteine}} (tHcy) {{has emerged}} as an important independent risk factor for cardiovascular disease. Analytical methods are needed to accommodate the high testing volumes for tHcy and provide rapid turnaround. Methods: We developed liquid chromatography electrospray tandem mass spectrometry (LC-MS/MS) method based on the analysis of 100 �L of either plasma or urine with homocystine-d 8 (2 nmol) added as internal standard. After <b>sample</b> <b>reduction</b> and deproteinization, the analysis was performed in the multiple reaction monitoring mode in which tHcy and Hcy-d 4 were detected through {{the transition from the}} precursor to the product ion (m/z 136 to m/z 90 and m/z 140 to m/z 94, respectively). The retention time of tHcy and Hcy-d 4 was 1. 5 mi...|$|E
40|$|Abstract. The {{objective}} {{of this study is}} to develop a simple, environmentally benign alternate analytical method for simultaneous estimation of iron oxidation states. This proposed method of iron speciation is based on 1, 10 -phenanthroline (phen) modified redox potential of transition metal ions. In a pre-step ex-cess cerium(IV) oxidizes iron(II) in sample to iron(III). Initial back titration of un-reacted cerium(IV) with cobalt(II) in presence of phen gives amount of iron(II) in <b>sample,</b> <b>reduction</b> of iron(III) with cobalt(II) is then used to estimate total iron as iron(III). The potentiometric titration method has been successfully test-ed for determinations of iron(II) and total iron in synthetic and natural samples and represents a distinc-tively green alternative to other reported protocols of iron speciation analysis. (doi: 10. 5562 /cca 2167...|$|E
40|$|Abstract. This paper {{deals with}} {{dimensionality}} and sample length re-duction {{applied to the}} tasks of exploratory data analysis. Proposed tech-nique relies on distance preserving linear transformation of given dataset to the lower dimensionality feature space. Coefficients of feature transfor-mation matrix are found using Fast Simulated Annealing- an algorithm inspired by physical annealing of solids. Furthermore the elimination or weighting of data elements which, as an effect of above mentioned trans-formation, were moved significantly {{from the rest of}} the dataset can be performed. Presented method was positively verified in routines of clus-tering, classification and outlier detection. It ensures proper efficiency of those procedures in compact feature space and with reduced data sample length at the same time. Key words: dimensionality reduction, <b>sample</b> <b>reduction,</b> linear trans-formation, fast simulated annealing, cluster analysis, classification, out-lier detection...|$|E
5000|$|The {{control for}} <b>sample</b> rate <b>reduction</b> (a.k.a. [...] "downsampling" [...] or [...] "averaging") is {{sometimes}} shown in Hz {{for a new}} sample rate, or as a <b>reduction</b> factor. <b>Sample</b> rate <b>reduction</b> is sometimes shown instead {{as the number of}} consecutive samples to average together to create a new sample. A value of 20 reduces the sample rate to 1/20th of its original rate.|$|R
50|$|A typical bitcrusher uses {{two methods}} to reduce audio fidelity: <b>sample</b> rate <b>reduction</b> and {{resolution}} reduction.|$|R
3000|$|... may be {{included}} after the <b>sampling</b> rate <b>reduction</b> stage. Such a filter stage increases the attenuation around f [...]...|$|R
30|$|Participants of {{this study}} were freshmen B.A. {{students}} of English language and literature at a state university in the southeastern of Iran. They were two intact classes (one control and one treatment) whose age ranged from 18 through to 38  years. The initial number of the students was 50; after data collection and scoring, some <b>sample</b> <b>reduction</b> occurred as a result of which a total of 40 participants (19 in the treatment group and 21 in the control group) remained for further analysis (16 males and 24 females). Both groups received the treatments in their reading comprehension courses 1 and 2 but with different instructors, methodologies, and reading course books. On the basis of a proficiency test results, the English level of the sample was found to be homogeneous (M[*]=[*] 60 out of 90, SD[*]=[*] 2.3, p[*]>[*]. 05) and about intermediate.|$|E
40|$|International audienceBaTiO and BaLaTiO {{nanoceramics}} showing colossal permittivity {{values have}} been characterized. While starting powders are of cubic symmetry, X-Ray and Neutron Diffraction techniques, and Raman Spectroscopy measurements {{show that the}} one-step processed ceramics obtained by Spark Plasma Sintering (SPS) contain cubic and tetragonal phases. The rather large oxygen deficiency determined in such ceramics by Electron Micro Probe Analysis and Electron Energy Loss Spectroscopy analyzes is explained {{by the presence of}} Ti, as evidenced by X-ray Photoelectron Spectroscopy measurements. Transmission Electron Microscopy and High Resolution Transmission Electron Microscopy show that these ceramics contain 50 - 300 nm grains which have single-domains, while grain boundaries are of the nanometer scale. Colossal permittivity values measured in our dense nanoceramics are explained by a charge hopping mechanism and interfacial polarization {{of a large number of}} polarons generated after <b>sample</b> <b>reduction</b> in SPS apparatus...|$|E
40|$|Ceramics {{based on}} SrTiO 3 are of growing {{interest}} as thermoelectric materials {{because of their}} high-temperature stability and non-toxicity. Substitution of La and Nb into the perovskite structure provides opportunities to control both the microstructure and properties. Ceramic solid solutions of (1 − x) SrTiO 3 − (x) La 1 / 3 NbO 3 were prepared by the mixed oxide route, using compositional steps of x = 0. 1. Pressed pellets were sintered at temperatures of 1573 K to 1723 K in air. Addition of aliovalent ions (La 3 +, Nb 5 +) on the A/B sites (Sr 2 +, Ti 4 +) led to A-Site cation deficiency in the stoichiometric compositions and other defect structures which increased carrier concentration. A maximum ZT of 0. 004 was obtained for the x = 0. 2 stoichiometric sample, although much higher ZT values are possible by <b>sample</b> <b>reduction...</b>|$|E
3000|$|Previous {{research}} on clinical trial adaptation by <b>sample</b> size <b>reduction</b> has universally {{focused on the}} question of when such reduction should be performed. In contrast, no consideration has been given to the question of which specific samples should be removed from the trial and which should be retained when <b>sample</b> size <b>reduction</b> is performed. Indeed the current practice is to remove a random subset of samples. More formally, if the sample size before adaptation is n, the samples are {x [...]...|$|R
30|$|More {{sophisticated}} multitap structures {{could be}} used and have also been proposed {{with reference to the}} standard structure [22]. Since they operate in the frequency domain and at twice the multicarrier symbol rate, they introduce an additional delay proportional to the number of taps. We consider the single-tap equalizer in both structures since it maintains limited the overall latency of the transceiver. It may appear that the FS equalizer be equivalent to a multitap sub-channel equalizer following the standard structure and therefore that the considered comparison be unfair. However, they are not equivalent for two reasons: (a) because the delay introduced by the two structures is different and obtaining the minimum delay is important in a transceiver, like the OFDM/OQAM one, with an already larger delay in comparison with the OFDM system; (b) the PPN-FFT scheme performs equalization after <b>sampling</b> rate <b>reduction</b> which introduces an interpolation operation. The distinction of the two structures in terms of <b>sampling</b> rate <b>reduction</b> {{lies in the fact that}} the FS structure performs equalization in its internal behavior and therefore before <b>sampling</b> rate <b>reduction</b> while the PPN structure performs equalization after <b>sampling</b> rate <b>reduction</b> and consequently needs to use the single-tap equalizer to not increase the transceiver delay.|$|R
40|$|Auditory evoked {{potentials}} (AEP) can be simultaneously recorded on-line as {{a succession}} of 11 waves, through a single input channel of a mini-computer. Since the response waves differ widely in frequency, a computing routine has been developed to display the whole response pattern in a single picture. Based upon a non-linear <b>samples</b> <b>reduction</b> of the digitized response, this routine allows a logarithmic transformation of the time axis. The method improves {{the identification of the}} AEP components and provides an objective estimate of the central auditory pathway for both neurophysiological and neuroclinical studies...|$|R
40|$|BaTiO 3 −x and Ba 0. 95 La 0. 05 TiO 3 −x {{nanoceramics}} showing colossal permittivity {{values have}} been characterized. While starting powders are of cubic symmetry, X-ray and Neutron Diffraction techniques and Raman Spectroscopy measurements {{show that the}} one-step processed ceramics obtained by Spark Plasma Sintering (SPS) contain cubic and tetragonal phases. Rather large oxygen deficiency determined in such ceramics by Electron Micro Probe analysis and Electron Energy Loss Spectroscopy analyzes is explained {{by the presence of}} Ti 3 +, as evidenced by X-ray Photoelectron Spectroscopy measurements. Transmission Electron Microscopy and High Resolution Transmission Electron Microscopy show that these ceramics contain 50 – 300 nm grains, which have single-domains, while grain boundaries are of nanometer scale. Colossal permittivity values measured in our dense nanoceramics are explained by a charge hopping mechanism and an interfacial polarization {{of a large number of}} polarons generated after <b>sample</b> <b>reduction</b> in SPS apparatus...|$|E
40|$|High-throughput {{three-dimensional}} cryogenic imaging {{of thick}} biological specimens is valuable for identifying biologically- or pathologically-relevant features of interest, especially for subsequent correlative studies. Unfortunately, high-resolution imaging techniques at cryogenic conditions often require <b>sample</b> <b>reduction</b> through sequential physical milling or sectioning for sufficient penetration to generate each {{image of the}} 3 -D stack. This study represents the first demonstration of using ptychographic hard X-ray tomography at cryogenic temperatures for imaging thick biological tissue in a chemically-fixed, frozen-hydrated state without heavy metal staining and organic solvents. Applied to mammalian brain, this label-free cryogenic imaging method allows visualization of myelinated axons and sub-cellular features such as age-related pigmented cellular inclusions at a spatial resolution of ~ 100 nanometers and thicknesses approaching 100 microns. Because our approach does not require dehydration, staining or reduction of the sample, we introduce the possibility for subsequent analysis of the same tissue using orthogonal approaches {{that are expected to}} yield direct complementary insight to the biological features of interest...|$|E
40|$|This report {{proposes a}} general axiomatic pyramid {{decomposition}} scheme for signal analysis and synthesis. This scheme comprises the following ingredients: (i) the pyramid {{consists of a}} (finite or infinite) number of levels such that the information content decreases towards higher levels; (ii) each step towards a higher level is constituted by an (information-reducing) analysis operator, whereas each step towards a lower level is modeled by an (information-preserving) synthesis operator. One basic assumption is necessary: synthesis followed by analysis yields the identity operator, meaning that no information is lost by these two consecutive steps. In this report, several examples are described of linear as well as nonlinear (e. g., morphological) pyramid decomposition schemes. Some of these examples are known from the literature (Laplacian pyramid, morphological granulometries, skeleton decomposition) {{and some of them}} are new (morphological Haar pyramid, median pyramid). Furthermore, the report makes a distinction between single-scale and multiscale decomposition schemes (i. e. without or with <b>sample</b> <b>reduction)</b> ...|$|E
5000|$|To {{solve the}} exact version of SVP under the Euclidean norm, several {{different}} approaches are known, {{which can be}} split into two classes: algorithms requiring superexponential time (...) and [...] memory, and algorithms requiring both exponential time and space (...) in the lattice dimension. The former class of algorithms most notably includes lattice enumeration and random <b>sampling</b> <b>reduction,</b> while the latter includes lattice sieving, computing the Voronoi cell of the lattice, and discrete Gaussian sampling. An open problem is whether algorithms for solving exact SVP exist running in single exponential time (...) and requiring memory scaling polynomially in the lattice dimension.|$|R
40|$|The {{addition}} of flexible, general implementations of geometrical splitting and Russian Roulette, in combination called geometrical importance <b>sampling,</b> for variance <b>reduction</b> {{and of a}} scoring system, for controlling the sampling, are described. The efficiency of the variance reduction implementation is measured in a simulation of a typical benchmark experiment for neutron shielding. Using geometrical importance <b>sampling</b> a <b>reduction</b> of the computing time of a factor 89 compared to the analog calculation, for obtaining a neutron flux with a certain precision, was achieved for the benchmark application...|$|R
40|$|Although {{microarray}} {{technology has}} become the most common method for studying global gene expression, a plethora of technical factors across the experiment contribute to the variable of genome gene expression profiling using peripheral whole blood. A practical platform needs to be established in order to obtain reliable and reproducible data to meet clinical requirements for biomarker study. We applied peripheral whole blood <b>samples</b> with globin <b>reduction</b> and performed genome-wide transcriptome analysis using Illumina BeadChips. Real-time PCR was subsequently used to evaluate the quality of array data and elucidate the mode in which hemoglobin interferes in gene expression profiling. We demonstrated that, when applied in the context of standard microarray processing procedures, globin reduction results in a consistent and significant increase in the quality of beadarray data. When compared to their pre-globin reduction counterparts, post-globin <b>reduction</b> <b>samples</b> show improved detection statistics, lowered variance and increased sensitivity. More importantly, gender gene separation is remarkably clearer in post-globin <b>reduction</b> <b>samples</b> than in pre-globin <b>reduction</b> <b>samples.</b> Our study suggests that the poor data obtained from pre-globin <b>reduction</b> <b>samples</b> {{is the result of the}} high concentration of hemoglobin derived from red blood cells either interfering with target mRNA binding or giving the pseudo binding background signal. We therefore recommend the combination of performing globin mRNA reduction in peripheral whole blood samples and hybridizing on Illumina BeadChips as the practical approach for biomarker study...|$|R
40|$|We {{address the}} problem of multi-class {{classification}} in the case where the number of classes is very large. We propose a double sampling strategy on top of a multi-class to binary reduction strategy, which transforms the original multi-class problem into a binary classification problem over pairs of examples. The aim of the sampling strategy is to overcome the curse of long-tailed class distributions exhibited in majority of large-scale multi-class classification problems and {{to reduce the number of}} pairs of examples in the expanded data. We show that this strategy does not alter the consistency of the empirical risk minimization principle defined over the double <b>sample</b> <b>reduction.</b> Experiments are carried out on DMOZ and Wikipedia collections with 10, 000 to 100, 000 classes where we show the efficiency of the proposed approach in terms of training and prediction time, memory consumption, and predictive performance with respect to state-of-the-art approaches. Comment: 16 pages, 3 figure...|$|E
40|$|The popular {{method of}} {{enumerating}} the primes is the Sieve of Eratosthenes. It can be programmed very neatly in a lazy functional language, but runs rather slowly. A little-known alternative method is the Wheel Sieve, originally formulated as a fast imperative algorithm for obtaining all primes up {{to a given}} limit, assuming destructive access to a bit-array. This article describes functional variants of the wheel sieve that enumerate all primes as a lazy list. 2 A standard solution Few readers of this journal will be unfamiliar with the following program to enumerate the primes using The Sieve of Eratosthenes: primes = sieve [2 [...] ] sieve (p:xs) = p: sieve [x | x 0] This little program, or something very like it, {{has been part of}} the stock-in-trade of lazy list-processing for over twenty years (Turner, 1975). For all its dainty appearance, however, the program makes brutal demands on the reduction machine. Suppose we compute pk, the kth prime. <b>Sample</b> <b>reduction</b> countsy are...|$|E
40|$|The {{definitive}} {{version is}} available at: [URL] In their recent paper, Kissling & Carl (2008) recommended the spatial error simultaneous autorregresive model (SARerr) over {{ordinary least squares}} (OLS) for modelling species distribution. We compared these models with the generalized least squares model (GLS) and a variant of SAR (SARvario). GLS and SARvario are superior to standard implementations of SAR because the spatial covariance structure is described by a semivariogram model. Innovation: We used the complete datasets employed by Kissling & Carl (2008), with strong spatial autocorrelation, and two datasets in which the spatial structure was degraded by <b>sample</b> <b>reduction</b> and grid coarsening. GLS performed consistently better than OLS, SARerr and SARvario in all datasets, {{especially in terms of}} goodness of fit. SARvario was marginally better than SARerr in the degraded datasets. Main conclusions: GLS was more reliable than SAR-based models, so its use is recommended when dealing with spatially autocorrelated data. CGL 2005 - 01625 /BOS (CICYT) Peer reviewe...|$|E
40|$|We {{present a}} new {{contrast}} enhancing color to grayscale conversion algorithm which works in real-time. It incorporates novel techniques for image <b>sampling</b> and dimensionality <b>reduction,</b> <b>sampling</b> color differences by Gaussian pairing and analyzing color differences by predominant component analysis. In {{addition to its}} speed and simplicity, the algorithm has the advantages of continuous mapping, global consistency, and grayscale preservation, as well as predictable luminance, saturation, and hue ordering properties...|$|R
3000|$|Choice (2) in Table 2 {{provides}} a statistics for task B on <b>samples</b> without dimension <b>reduction.</b> The statistics s [...]...|$|R
40|$|We {{propose a}} new lattice {{reduction}} method. Our algorithm approximates shortest lattice vectors up to a factor and {{makes use of}} Grover's quantum search algorithm. The proposed method has the expected running time O(n A). That is about the square root of the running time O(n A) of Schnorr's recent random <b>sampling</b> <b>reduction</b> which in turn improved the running time to the fourth root of previously known algorithms. Our result demonstrates that the availability of quantum computers will a#ect not only the security of cryptosystems based on integer factorization or discrete logarithms, but also of lattice based cryptosystems. Rough estimates based on our asymptotic improvements and experiments reported in [HPS 98] suggest that the NTRU security parameter needed to be increased from 503 to 1277 if su#ciently large quantum computer were available nowadays...|$|R
