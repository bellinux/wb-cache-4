0|1400|Public
5000|$|Mendi + Keith Obadike present Crosstalk: American <b>Speech</b> <b>Music</b> (various artists), (2008, Bridge) ...|$|R
50|$|The National Youth Day is {{observed}} all over India at schools and colleges—with processions, <b>speeches,</b> recitations, <b>music,</b> youth conventions, seminars, Yogasana presentation, competitions in essay-writing, recitations, <b>speeches,</b> <b>music,</b> sports, {{and other programs}} on 12 January every year.|$|R
40|$|Characterizing <b>music</b> <b>dynamics</b> for {{improvisation}} characterizing <b>music</b> <b>dynamics</b> {{is important}} for music generation, music analogies, music retrieval, rmisic impro-visation etc. Though there has beeri some work on mod-eling music, {{most of them do}} not consider the temporal or dynamical aspects of music. In this paper we show {{that it is possible to}} characterize <b>music</b> <b>dynamics</b> using linear prediction. We provide a technique for music improvisariori using this model. The improvisation scheme is then applied to the generation of backgroundmrisic for videos. 1...|$|R
50|$|Crosstalk: American <b>Speech</b> <b>Music</b> is a {{compilation}} album of speech-based music by various composers, poets, visual artists and DJs.|$|R
50|$|Some of {{his musical}} {{analyses}} were published {{along with other}} works in Langage, musique, poésie <b>Speech,</b> <b>music,</b> poetry (1972).|$|R
5000|$|An audio {{amplifier}} and loudspeaker, which turns electrical signals into sound waves (<b>speech,</b> <b>music,</b> and other sounds) {{to accompany the}} images.|$|R
40|$|This work {{is devoted}} to the problem of {{automatic}} <b>speech</b> and <b>music</b> discrim-ination. As we will see here, <b>speech</b> and <b>music</b> signals have quite distinctive features. However, the efficient distinction between <b>speech</b> and <b>music</b> is still an open problem. This problem arises when it is necessary to extract speec...|$|R
5000|$|The Thursday {{dinner on}} campus is a formal event {{involving}} <b>speeches,</b> <b>music,</b> fine arts or entertainment. Student attendance at this meal is mandatory.|$|R
40|$|Dynamics {{is one of}} the {{principal}} means of expressivity in Western classical music. Still, preceding research on room acoustics has mostly neglected the contribution of <b>music</b> <b>dynamics</b> to the acoustic perception. This study investigates how the different concert hall acoustics influence the perception of varying <b>music</b> <b>dynamics.</b> An anechoic orchestra signal, containing a step in <b>music</b> <b>dynamics,</b> was rendered in the measured acoustics of six concert halls at three seats in each. Spatial sound was reproduced through a loudspeaker array. By paired comparison, naive subjects selected the stimuli that they considered to change more during the music. Furthermore, the subjects described their foremost perceptual criteria for each selection. The most distinct perceptual factors differentiating the rendering of <b>music</b> <b>dynamics</b> between halls include the dynamic range, and varying width of sound and reverberance. The results confirm the hypothesis that the concert halls render the performed <b>music</b> <b>dynamics</b> differently, and with various perceptual aspects. The analysis against objective room acoustic parameters suggests that the perceived dynamic contrasts are pronounced by acoustics that provide stronger sound and more binaural incoherence by a lateral sound field. Concert halls that enhance the dynamics have been found earlier to elicit high subjective preference. Peer reviewe...|$|R
5000|$|In 2008 {{his work}} [...] "Morning Blues for Yvan" [...] was {{featured}} on the compilation album Crosstalk: American <b>Speech</b> <b>Music</b> (Bridge Records) produced by Mendi + Keith Obadike.|$|R
25|$|RadioVague in {{conjunction}} with the now defunct CableRadio broadcast <b>speeches,</b> <b>music</b> and interviews from the event to the internet throughout the day using a satellite uplink provided by Psand.net.|$|R
50|$|Music {{education}} {{for young children}} is an educational program introducing children in a playful manner to singing, <b>speech,</b> <b>music,</b> motion and organology. It is a subarea of music education.|$|R
50|$|RadioVague in {{conjunction}} with the now defunct CableRadio broadcast <b>speeches,</b> <b>music</b> and interviews from the event to the internet throughout the day using a satellite uplink provided by Psand.net.|$|R
2500|$|Every year on Midsummer Eve, {{the park}} is a {{rallying}} point for {{thousands of people who}} attend community singing, <b>speeches,</b> <b>music</b> and a [...] "witch"-burning bonfire at the lakeside in front of the palace.|$|R
50|$|The courses include Mathematics, Science, Social Science, English, Sanskrit, Hindi and Marathi. However, {{apart from}} {{academic}} courses, the school gives {{special attention to}} Self-expression arts such as Drama, Sculpture, Painting, Elocution and <b>Speech,</b> <b>Music</b> and Dance.|$|R
40|$|The {{problem of}} <b>speech,</b> <b>music</b> and music with songs {{discrimination}} in telephony with handsets variability is {{addressed in this}} paper. Two systems are proposed. The first system uses three Gaussian Mixture Models (GMM) for <b>speech,</b> <b>music</b> and songs respectively. Each GMM comprises 8 Gaussians trained on very short sessions. Twenty six speakers (13 females, 13 males) have been randomly chosen from the SPIDRE corpus. The music were obtained from a large set of data and comprises various styles. For 138 minutes of testing time, a speech discrimination score of 97. 9 % is obtained when no channel normalization is used. These performance are obtained for a relatively short analysis frame (32 ms sliding window, buffering of 100 ms). When using channel normalization, an important score reduction (on the order of 10 to 20 %) is observed. The second system has been designed for applications requiring shorter processing times along with shorter training sessions. It {{is based on an}} empirical transformation of the #MFCC that enhances the dynamical evolution of tonality. It yields in average an acceptable discrimination rate of 90 % (speech- /music) and 84 % (<b>speech,</b> <b>music</b> and songs with music) ...|$|R
30|$|This paper studies a novel audio segmentation-by-classification {{approach}} {{based on}} factor analysis. The proposed technique compensates the within-class variability by using class-dependent factor loading matrices and obtains the scores by computing the log-likelihood ratio {{for the class}} model to a non-class model over fixed-length windows. Afterwards, these scores are smoothed to yield longer contiguous segments of the same class by means of different back-end systems. Unlike previous solutions, our proposal does not make use of specific acoustic features and does not need a hierarchical structure. The proposed method is applied to segment and classify audios coming from TV shows into five different acoustic classes: <b>speech,</b> <b>music,</b> <b>speech</b> with <b>music,</b> <b>speech</b> with noise, and others. The technique is compared to a hierarchical system with specific acoustic features achieving a significant error reduction.|$|R
30|$|In this work, we have {{presented}} a hierarchical scheme for classifying audio signals {{into three categories}} namely <b>speech,</b> <b>music</b> without voice (instrumental), music with voice (song). In the first stage, we classify the signals as <b>speech</b> or <b>music</b> and subsequently music is further classified as instrumental and song. Audio texture that has been derived based on ZCR and STE co-occurrence matrices can successfully discriminate <b>speech</b> and <b>music.</b> Thus, audio texture acts as an effective way for summarizing frame level features. It {{has been shown that}} MFCC based features are well suited in classifying the music signal as instrumental and song. RANSAC has been utilized as the classifier and it is capable of handling the wide variation in data. Experimental result indicates the effectiveness of the proposed methodology.|$|R
40|$|Long playing {{record to}} {{accompany}} El Rodeo, 1958, the University of Southern California's student yearbook. Sounds {{from the campus}} during the 1957 - 1958 school year. Excerpts from <b>speeches,</b> <b>music</b> events, athletic events, and other campus activities; Gary Brandt, narrator. Produced by the Department of Telecommunication...|$|R
50|$|It is a {{generalization}} {{and refinement}} of Fourier analysis, {{for the case}} when the signal frequency characteristics are varying with time. Since many signals of interest - such as <b>speech,</b> <b>music,</b> images, and medical signals - have changing frequency characteristics, time-frequency analysis has broad scope of applications.|$|R
50|$|AIDS Walk Austin {{began as}} From All Walks of Life in 1988 and is ASA's oldest {{continuous}} fundraiser. The AIDS Walk is a 5k walk through downtown Austin, TX and includes <b>speeches,</b> <b>music</b> and remembrance. In recent years, {{it has also}} featured panels from the AIDS Memorial Quilt.|$|R
3000|$|..., which, respectively, {{characterize}} amplitude {{and frequency}} modulations. We {{say that a}} sinusoid representation is slowly varying if these derivatives have small absolute values. Slowly varying sinusoids include many signals in <b>speech,</b> <b>music,</b> and telecommunications that we technically handle as sinusoids [4, 6, 7], and are the central elements of sinusoid modelling.|$|R
50|$|Divine Savior Holy Angels {{provides}} honors-level {{and advanced}} placement courses, {{as well as}} electives in visual arts, computer science, physical education, journalism, <b>speech,</b> <b>music,</b> dance, and theater arts. Languages offered are Spanish, French, and Latin. Students are required to complete theology courses all four years, plus overnight retreats and community service.|$|R
50|$|In 2007 his {{composition}} For Irving Lippel, {{performed by}} Jeffrey Irving and Daniel Lippel {{appeared on the}} album Sustenance (New Focus Recordings) and in 2008 an excerpt of his work Life Studies was included on the compilation album Crosstalk: American <b>Speech</b> <b>Music</b> (Bridge Records) produced by Mendi + Keith Obadike.|$|R
30|$|This {{special issue}} on {{intelligent}} audio, <b>speech,</b> and <b>music</b> processing (IASMP) consists of 13 papers that reflect a {{diverse range of}} disciplines in <b>speech,</b> audio, and <b>music</b> processing. These papers are grouped under analysis, communication, and interaction areas.|$|R
30|$|The {{algorithm}} {{consists of}} two stages. The first stage is a supervised learning phase, based on a statistical approach. In this phase training data is collected from <b>speech</b> and <b>music</b> signals separately, and after processing and feature extraction, optimal separation thresholds between <b>speech</b> and <b>music</b> are set for each analyzed feature separately.|$|R
40|$|<b>Speech</b> and <b>music</b> {{are both}} based on sounds which are uttered and received, {{and that can}} be {{organized}} in order to make “sense”. This study makes a detailed comparison between <b>speech</b> and <b>music,</b> also referring to considerations and researches made in the past centuries in the field of musicology, linguistics, literature, music psychology...|$|R
40|$|While the {{technology}} for mining text documents in large databases {{could be said to}} be relatively mature, the same cannot be said for mining other important data types such as <b>speech,</b> <b>music,</b> images and video. Yet these forms of multimedia data are becoming increasingly prevalent on the internet and intranets as bandwidt...|$|R
50|$|Something Understood is {{a weekly}} radio {{programme}} on BBC Radio 4 {{which deals with}} topics of religion, spirituality, and the larger questions of human life, and takes a particular spiritual theme, exploring it through <b>speech,</b> <b>music,</b> prose, and poetry. It is broadcast early on Sunday mornings with a repeat late on Sunday evening.|$|R
40|$|A {{system for}} {{classification}} of audio signals containing <b>speech,</b> <b>music,</b> noise and silence is proposed. Appropriate subband processing is {{applied for the}} characterization of each sound source. The algorithra operates in four steps to classify {{the contents of a}} given audio signal. The acoustical parameters and statistical measures {{to be used in the}} classification process are obtained via an off-line training procedure. The starting and finishing instants of the acoustical events are labelled in the silence/onset detection stages. Acoustical parameters of the given signal are extracted, analysis of variance and classification using the LBG algorithm is performed by generating codebooks of acoustical vectors. Experimental work is carded out on a database containing <b>speech,</b> <b>music,</b> noise and silence. The experiments demonstrate that the system achieves 88 % classification success on the average when the sound sources are non-simultaneous...|$|R
40|$|The aim of {{this study}} was to {{determine}} if duration-related stress in <b>speech</b> and <b>music</b> is processed in a similar way in the brain. To this end, we tested 20 adults for their abstract mismatch negativity (MMN) event-related potentials to two duration-related stress patterns: stress on the first syllable or note (long-short), and stress on the second syllable or note (short-long). A significant MMN was elicited for both <b>speech</b> and <b>music</b> except for the short-long speech stimulus. The long-short stimuli elicited larger MMN amplitudes for <b>speech</b> and <b>music</b> compared to short-long stimuli. An extra negativity-the late discriminative negativity (LDN) -was observed only for music. The larger MMN amplitude for long-short stimuli might be due to the familiarity of the stress pattern in <b>speech</b> and <b>music.</b> The presence of LDN for music may reflect greater long-term memory transfer for music stimuli. 11 page(s...|$|R
40|$|Soundtracks of {{multimedia}} files are information rich, from which much content-related metadata can be extracted. There is a pressing demand for automated classification, identification and information mining of audio content. A {{segment of the}} audio soundtrack can be either <b>speech,</b> <b>music,</b> event sounds {{or a combination of}} them. There exist many individual algorithms for the recognition and analysis of <b>speech,</b> <b>music</b> or event sounds, allowing for embedded information to be retrieved in a semantic fashion. A systematic review shows that a universal system that is optimised to extract the maximum amount of information for further text mining and inference does not exist. Mainstream algorithms typically work with a single class of sound, e. g. <b>speech,</b> <b>music</b> or even sounds and classification methods are predominantly exclusive (detects one class at a time) and losing much of information when two or three classes are overlapped. A universal open architecture for audio content and scene analysis has been proposed by the authors. To mitigate information losses in overlapped content, non-exclusive segmentation approaches were adopted. This paper is presented from one possible implementation deploying the universal open architecture as a paradigm to show how the universal open architecture can integrate existing methods and workflow but maximise extractable semantic information. In the current work, overlapped content is identified and segmented from carefully tailored feature spaces and a family of decision trees are used to generate a content score. Results show that the developed system, when compared with well established audio content analysers, can identify and thus extract information from much more <b>speech</b> and <b>music</b> segments. The full paper will discuss the methods, detail the results and illustrate how the system works...|$|R
40|$|Many cues convey emotion {{similarly}} in <b>speech</b> and <b>music.</b> Researchers {{have established}} that acoustic cues such as pitch height, tempo, and intensity carry important emotional information in both domains. In this investigation, {{we examined the}} emotional significance of melodic and rhythmic contrasts between successive syllables or tones in <b>speech</b> and <b>music,</b> referred to as Melodic Interval Variability (MIV) and the normalized Pairwise Variability Index (nPVI). The spoken stimuli were 96 tokens expressing the emotions of irritation, fear, happiness, sadness, tenderness and no emotion. The music stimuli were 96 phrases, played with or without performance expression and composed {{with the intention of}} communicating similar emotions (anger, fear, happiness, sadness, tenderness and no emotion). The results showed that <b>speech,</b> but not <b>music,</b> was characterized by changes in MIV as a function of intended emotion. However, both <b>speech</b> and <b>music,</b> showed similar changes in nPVI. Emotional portrayals, both spoken and musical, had higher nPVI values than stimuli convey-ing “no emotion”. The results suggest that MIV may function differently in <b>speech</b> and <b>music,</b> but that there may be similarities between the two domains with respect to nPVI...|$|R
40|$|The paper {{presents}} a novel classification and segmentation scheme for MP 3 and AAC audio in the compressed domain. The input audio is split into <b>speech,</b> <b>music</b> and silent segments using {{features such as}} total energy, band energy ratio, pause rate, subband centroid and fundamental frequency. Simulation results show {{the efficiency of the}} proposed algorithm. ...|$|R
40|$|Liturgical {{differences}} between Catholic and Reformed celebrations in Switzerland were studied by a temporal analysis of 16 worships. The importance of <b>speech,</b> <b>music,</b> and silence and {{the participation of}} the priest, assembly and organist are analysed. Data from 190 churches show that Reformed churches present a smaller average volume, a denser occupation of space and a shorter reverberation time than Catholic churches. The geometrical and acoustical characteristics of Reformed churches favour <b>speech</b> intelligibility whereas <b>music</b> is advantaged in Catholic churches, which agrees with the observed liturgical differences...|$|R
3000|$|Second, {{supervised}} classification {{can be used}} to classify each database signal into a predefined class, for instance, to <b>speech,</b> <b>music,</b> and environmental sounds. Supervised classification in general has been widely studied, and audio classifiers typically employ neural networks [4] or hidden Markov models (HMMs) [5] on frame-wise features. In general audio classification, extracting features in short ([...] [...]...|$|R
