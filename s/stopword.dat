108|119|Public
5000|$|The Topic-based Vector Space Model (TVSM) (literature: http://www.logos-verlag.de/cgi-bin/engbuchmid?isbn=0514&lng=eng&id=) {{extends the}} vector space model of {{information}} retrieval by removing the constraint that the term-vectors be orthogonal. The assumption of orthogonal terms is incorrect regarding natural languages which causes problems with synonyms and strong related terms. This facilitates the use of <b>stopword</b> lists, stemming and thesaurus in TVSM.In contrast to the generalized vector space model the TVSM {{does not depend on}} concurrence-based similarities between terms.|$|E
5000|$|Text mining {{is based}} on a {{statistical}} analysis of word recurrence in a corpus. An algorithm extracts words and expressions from title, summary and claims and gathers them by declension. [...] "And" [...] and [...] "if" [...] are labeled as non-information bearing words and are stored in the <b>stopword</b> list. Stoplists can be specialized in order to create an accurate analysis. Next, the algorithm ranks the words by weight, according to their frequency in the patent's corpus and the document frequency containing this word. The score for each word is calculated using a formula such as: ...|$|E
40|$|Words in a {{document}} that are frequently occurring but meaningless in terms of Information Retrieval (IR) are called stopwords. It is repeatedly claimed that stopwords do not contribute towards the context or information of the documents {{and they should be}} removed during indexing as well as before querying by an IR system. However, the use of a single fixed <b>stopword</b> list across different document collections could be detrimental to the retrieval effectiveness. This paper presents different methods in deriving a <b>stopword</b> list automatically for a given collection and evaluates the results using four different standard TREC collections. In particular, a new approach, called term-based random sampling, is introduced based on the Kullback-Leibler divergence measure. This approach determines how informative a term is and hence enables us to derive a <b>stopword</b> list automatically. This new approach is then compared to various classical approaches based on Zipf’s law, which we used as our baselines here. Results show that the <b>stopword</b> lists derived by the methods inspired by Zipf’s law are reliable but very expensive to carry out. On the other hand, the computational effort taken to derive the <b>stopword</b> lists using the new approach was minimal compared to the baseline approaches, while achieving a comparable performance. Finally, we show that a more effective <b>stopword</b> list can be obtained by merging the classical <b>stopword</b> list with the <b>stopword</b> lists generated by either the baselines or the new proposed approach...|$|E
40|$|This diploma thesis focuses {{its point}} on {{automatization}} of <b>stopwords</b> generation as one method of pre-processing a textual documents. It analyses an influence of <b>stopwords</b> removal to {{a result of}} data mining tasks (classification and clustering). First the text mining techniques and frequently used algorithms are described. Methods of creating domain specific lists of <b>stopwords</b> are described to detail. In the end the results of large collections of text files testing and implementation methods are presented and discussed...|$|R
30|$|We {{observe that}} {{removing}} <b>stopwords</b> lowers {{the performance of}} the classifier. Most frequently used words in English such as “be”, “has”, “if”, “and”, and “on”, carry no information, and therefore, removal of <b>stopwords</b> is a common technique to improve performance. However, from our experiments, removal of such functional words would result in the loss of vital features like “should”, “more”, “could”, “would” and “have” and this leads to inaccuracy, as shown in Table 6. To answer RQ 4, the <b>stopwords</b> are essential for the suggestion extraction task.|$|R
30|$|Preprocessing involves: tokenization, {{removing}} <b>stopwords</b> and stemming.|$|R
40|$|Most {{information}} retrieval systems use <b>stopword</b> lists and stemming algorithms. However, {{we have found}} that recognizing singular and plural nouns, verb forms, negation, and prepositions can produce dramatically different text classification results. We present results from text classification experiments that compare relevancy signatures, which use local linguistic context, with corresponding indexing terms that do not. In two different domains, relevancy signatures produced better results than the simple indexing terms. These experiments suggest that <b>stopword</b> lists and stemming algorithms may remove or conflate many words {{that could be used to}} create more effective indexing terms. Introduction Most {{information retrieval}} systems use a <b>stopword</b> list to prevent common words from being used as indexing terms. Highly frequent words, such as determiners and prepositions, are not considered to be content words because they appear in virtually every document. <b>Stopword</b> lists are almost univer [...] ...|$|E
40|$|International audienceWhile {{the use of}} patent mapping tools is growing, the 'black-box' systems {{involved}} do {{not generally}} allow the user to interfere further than the preliminary retrieval of documents. Except for one thing: the <b>stopword</b> list, i. e. the list of 'noise' words to be ignored, which can be modified to one's liking and dramatically impacts the final output and analysis. This paper invokes information science and computer science to provide clues for {{a better understanding of}} the <b>stopword</b> lists' origin and purpose, and how they fit in the mapping algorithm. Further, it stresses the need for <b>stopword</b> lists that depend on the document corpus analyzed. Thus, the analyst is invited to add and remove stopwords — or even, in order to avoid inherent biases, to use algorithms that can automatically create ad hoc <b>stopword</b> lists...|$|E
40|$|In {{the last}} few years, {{electronic}} documents have been {{the main source of}} data in many research areas like Web Mining, Information Retrieval, Artificial Intelligence, Natural Language Processing etc. Text Processing plays a vital role for processing structured or unstructured data from the web. Preprocessing is the main step in any text processing systems. One significant preprocessing technique is the elimination of functional words, also known as stopwords, which affects the performance of text processing tasks. An efficient <b>stopword</b> removal technique is required in all text processing tasks. In this paper, we are proposing a <b>stopword</b> removal algorithm for Hindi Language which is using the concept of a Deterministic Finite Automata (DFA). A large number of available works on <b>stopword</b> removal techniques are based on dictionary containing <b>stopword</b> lists. Then pattern matching technique is applied and the matched patterns, which is a <b>stopword,</b> is removed from the document. It is a time consuming task as searching process takes a long time. This makes the method inefficient and very expensive. In comparison of that, our algorithm has been tested on 200 documents and achieved 99 % accuracy and also time efficient...|$|E
30|$|Eliminate <b>stopwords</b> (i.e., {{the most}} common words in a language).|$|R
30|$|The {{preponderance}} of <b>stopwords</b> in Table 12 brings about {{one final question}} regarding {{the extent of their}} influence. We tested for this by removing all but <b>stopwords</b> from all computer filtered chats and running SVM over these data. Results (F 1, 0.58) seem to indicate that <b>stopwords</b> alone could be used to some extent for predator detection, which would outperform a number of other tested approaches in PAN 2012. How meaningful such a result is, and what it can tell us about the conversations in general and the machine learning approaches in particular, remains to be understood.|$|R
5000|$|Advanced functionality: faceted search, clustering, filters, snippets, synonyms, <b>stopwords,</b> highlighting, categorization, “find similar”, {{automatic}} thumbnail screenshot inclusion, boost/reduce relevance, ...|$|R
40|$|In {{this brief}} communication, we {{evaluate}} {{the use of}} two <b>stopword</b> lists for the English language (one comprising 571 words and another with 9) and compare them with a search approach accounting for all word forms. We show that through implementing the original Okapi form or certain ones derived from the Divergence from Randomness (DFR) paradigm, significantly lower performance levels may result when using short or no <b>stopword</b> lists. For other DFR models and a revised Okapi implementation, performance differences between approaches using short or long <b>stopword</b> lists or no list at all are usually not statistically significant. Similar conclusions can be drawn when using other natural languages such as French, Hindi, or Persian...|$|E
30|$|<b>Stopword</b> Removal. Stopwords {{are some}} of the most common words like “the”, “these”, “a”. Preprocessor removes stopwords because they offer no {{meaningful}} help in further analysis.|$|E
40|$|We {{present a}} topic {{boundary}} detection method that searches for connections between sequences of utterances in multi party dialogues. The connections are established based on word identity. We compare our method to a state-of-the art automatic topic boundary detection method {{that was also}} used on multi party dialogues. We checked various methods of preprocessing of the data, including stemming, lemmatization and <b>stopword</b> filtering with a text-based as well as speech-based <b>stopword</b> lists. Using standard evaluation methods we found that our method outperformed the state-of-the art method. 1...|$|E
40|$|We {{investigate}} {{the effects of}} lexicon size and <b>stopwords</b> on Chinese information retrieval using our method of short-word segmentation based on simple language usage rules and statistics. These rules allow us to employ a small lexicon of only 2, 175 entries and provide quite admirable retrieval results. It is noticed that accurate segmentation is not essential for good retrieval. Larger lexicons can lead to incremental improvements. The presence of <b>stopwords</b> do not contribute much noise to IR. Their remova...|$|R
3000|$|RQ 4 —NLP techniques: What is {{the impact}} of <b>stopwords</b> on the {{accuracy}} of the classifications? (“Statistical classifier results and analysis” section) [...]...|$|R
30|$|While {{also the}} {{organization}} names are case insensitive, {{this does not}} apply to the acronyms, which have to match also the case. This is to avoid to match “WHO” to the pronoun “who”. This measure is necessary because we do not perform standard <b>stopwording</b> to the text, meaning that we do not remove syntactic words (like “the”, “and”, etc.) from the webpage text. The reason is that organization names contain <b>stopwords,</b> and we would miss matches if we removed them.|$|R
40|$|Sentiment {{classification}} over Twitter {{is usually}} {{affected by the}} noisy nature (abbreviations, irregular forms) of tweets data. A popular procedure to reduce the noise of textual data is to remove stopwords by using pre-compiled <b>stopword</b> lists or more sophisticated methods for dynamic <b>stopword</b> identification. However, the effectiveness of removing stopwords {{in the context of}} Twitter sentiment classification has been debated in the last few years. In this paper we investigate whether removing stopwords helps or hampers the effectiveness of Twitter sentiment classification methods. To this end, we apply six different <b>stopword</b> identification methods to Twitter data from six different datasets and observe how removing stopwords affects two well-known supervised sentiment classification methods. We assess the impact of removing stopwords by observing fluctuations on the level of data sparsity, the size of the classifier’s feature space and its classification performance. Our results show that using pre-compiled lists of stopwords negatively impacts the performance of Twitter sentiment classification approaches. On the other hand, the dynamic generation of <b>stopword</b> lists, by removing those infrequent terms appearing only once in the corpus, appears to be the optimal method to maintaining a high classification performance while reducing the data sparsity and substantially shrinking the feature space...|$|E
40|$|User queries {{to search}} engines are {{observed}} to predominantly contain inflected content words but lack stopwords and capitalization. Thus, they often resemble natural language queries after case folding and <b>stopword</b> removal. Query recovery aims {{to generate a}} linguistically well-formed query from a given user query as input to provide natural language processing tasks and cross-language information retrieval (CLIR). The evaluation of query translation shows that translation scores (NIST and BLEU) decrease after case folding, <b>stopword</b> removal, and stemming. A baseline method for query recovery reconstructs capitalization and stopwords, which considerably increases translation scores and significantly increases mean average precision for a standard CLIR task. Categories and Subject Descriptors...|$|E
40|$|Abstract. For {{our first}} {{participation}} in CLEF retrieval tasks, our first {{objective was to}} define a general <b>stopword</b> list for various European languages (namely, French, Italian, German and Spanish) and also to suggest simple and efficient stemming procedures for them. Our second aim was to suggest a combined approach that might be implemented {{in order to facilitate}} effective access to multilingual collections. 1. Monolingual indexing and search Most European languages (including French, Italian, Spanish, German) share many of the same characteristics as does the language of Shakespeare (e. g., word boundaries marked in a conventional manner, variant word forms generated by adding suffixes {{to the end of a}} root, etc.). Any adaptation of indexing or search strategies thus means the elaboration of general <b>stopword</b> lists and fast stemming procedures. <b>Stopword</b> lists contain nonsignificant words that are removed from a document or a request before the indexing process is begun. Stemming procedures try to remove inflectional and derivational suffixes in order to conflate word variants into the same stem or root. This first chapter will deal with these issues and is organized as follows: Section 1. 1 contains an overview of our five test collections while Section 1. 2 describes our general approach to building <b>stopword</b> lists and stemmers for use with languages other than English. Section 1. 3 depicts the Okapi probabilistic model together with the description of the runs submitted by us in the monolingual track. 1. 1. Overview of the test-collections The corpora used in our experiments included newspapers such as the Los Angeles Times, Le Monde (French) ...|$|E
3000|$|... 11 <b>Stopwords,</b> which {{constitute}} a stoplist, are basically functional words (for instance: prepositions, articles, conjunctions, etc), which present high frequency in the corpora and no terminological value.|$|R
40|$|Phrase searching in text indexes Compare {{different}} approaches to perform phrase searching, and consider a new approach whereas bigrams is considered as index term. This master thesis focus at the challenges within phrase searching in large text indexes, and to assess alternative approaches to cope with such indexes. This goal was achieved by performing an experiment, based on the theory of using bigrams consisting of <b>stopwords</b> as additional index terms. Realizing the characteristics within inverted index structures, we utilized <b>stopwords</b> as indicators for severe long posting lists. The characteristics of <b>stopwords</b> proved valuable, and they were collected based on a already established index for {{a subset of the}} TREC GOV 2 collection. In alternative approaches we outlined two 9 ̆ 3 state of the art 9 ̆ 4 index structures, speciﬁcally designed to cope with phrase searching challenges. The ﬁrst structure - nextword index - followed a modiﬁcation of the inverted index structure. The second structure - phrase index - utilized the inverted structure in using complete phrases as index terms. Our bigram index focused on the same manipulation of the inverted index structure as the phrase index, using bigrams of words to rastically cut posting lists lengths. This was one of our main goals, as we identiﬁed <b>stopwords</b> posting list lengths {{to be one of the}} primary challenges with phrase searching in inverted index structures. Using <b>stopwords</b> to create and select bigrams proved successful to enhance phrase searching, as response times substantially improved. We conclude that our bigram index provides a signiﬁcant performance in crease in terms of query evaluation time, and outperforms the standard inverted index within phrase searching...|$|R
30|$|<b>Stopwords</b> {{are common}} English {{words such as}} “the”, “am”, and “their” which do not {{influence}} the semantics of the review. Removing them can reduce noise. Informative terms like “bug” or “add” will become more influential, which might improve the accuracy of document classifiers. However, some keywords that are commonly defined as <b>stopwords</b> can be relevant for the review classification. For instance, the terms “should” and “must” might indicate a feature request, “did”, “while” and “because” a feature description, “but”, “before” and “now” a bug report and “very”, “too”, “up” and “down” a rating.|$|R
40|$|Abstract. In {{this paper}} we propose a {{semantic}} approach to automatically identify and remove stopwords from Twitter data. Unlike most existing approaches, which rely on outdated and context-insensitive <b>stopword</b> lists, our proposed approach considers the contextual semantics and sentiment of words in order to measure their discrimination power. Evaluation results on 6 Twitter datasets show that, removing our semantically identified stopwords from tweets, increases the binary sentiment classification performance over the classic pre-complied <b>stopword</b> list by 0. 42 % and 0. 94 % in accuracy and F-measure respectively. Also, our approach reduces the sentiment classifier’s feature space by 48. 34 % and the dataset sparsity by 1. 17 %, on average, compared to the classic method...|$|E
40|$|Abstract: The study uses {{grounded}} theory approach to develop different categories of stopwords {{leading to the}} creation of a <b>stopword</b> list for email-based data. The finding of the study will contribute in better understanding of email as data and developing better algorithms which could automatically remove specific category of stopwords. Résumé: Cette étude se base sur la théorie à base empirique pour développer différentes catégories de mots vides qui seront utilisés pour créer une liste aux fins d'analyse des données issues de courriels. Les résultats permettront une meilleure compréhension des courriels comme source de données et la création de meilleurs algorithmes de suppression automatique de catégories précises de mots vides. Text mining typically involves a variety of decisions including algorithm selection, parameter setting (Keogh et al., 2004; Xu and Wunsch, 2005) and the creation of a <b>stopword</b> list. However, the latter is often treated as a tedious but necessary part of the text mining process and is rarely the subject of systematic investigation, unlike, say the validity or utility of the results of the mining process, or of the mining algorithm. As a result, <b>stopword</b> list creation can receive little attention. It is tempting to just re-use...|$|E
40|$|We {{describe}} the LACS submission to the Search sub-task of the Search and Hyperlinking Task at MediaEval 2014. Our experiments investigate how different retrieval models interact with word stemming and <b>stopword</b> removal. On the development data, we segment the subtitle and Automatic Speech Recognition (ASR) transcripts into fixed length time units, {{and examine the}} effect of different retrieval models. We find that stemming provides consistent improvement; <b>stopword</b> removal is more sensitive to the retrieval models on the subtitles. These manipulations do not contribute to stable improvement on the ASR transcripts. Our experiments on test data focus on the subtitle. The gap in performance for different retrieval models is much less compared to the development data. We achieved 0. 477 MAP on the test data. 1...|$|E
30|$|The main {{objective}} of E-TERMOS {{is to make}} the creation of terminological products possible, whether they are for academic research or promotion purposes, by means of the (semi) automation of the stages of the terminological work. The goal of the automatic term extraction in this project is to obtain candidate terms from the corpora of the specificity in question. In order to perform the extraction, firstly, it is possible to choose the size of the gram to be used, which may be from 2 to 7. Then, it is possible to remove <b>stopwords</b> {{with the use of a}} list of provided <b>stopwords,</b> which is a result of the work of Teline [31]. After the removal of the <b>stopwords,</b> the terms are extracted with the support of the statistical and/or linguistic knowledge. According to the author, the incorporation of statistical measures (log likelihood ratio, mutual information, and Dice’s coefficient) from the NSP package, linguistic (to be defined) and hybrid (union of statistical and linguistic knowledge) are to be included. Nowadays, the simple frequency statistical measure is available.|$|R
3000|$|Ability {{to handle}} the ambiguities of natural languages, thanks to <b>stopwords</b> (words {{filtered}} out during the processing of text), stemming (ability to detect words derived from a common root), synonyms detection, and controlled vocabularies such as thesauri and taxonomies [...]...|$|R
5000|$|Text {{processing}} {{support for}} SBCS and UTF-8 encodings, <b>stopwords,</b> indexing of words known not {{to appear in}} the database ("hitless"), stemming, word forms, tokenizing exceptions, and [...] "blended characters" [...] (dual-indexing as both a real character and a word separator).|$|R
40|$|This paper {{presents}} {{the outcomes of}} initial Greek Web searching experimentation. The effects of localization support and standard Information Retrieval techniques such as term normalization, <b>stopword</b> removal and simple stemming are studied in international and local search engines. Finally, evaluation points and conclusions are discussed. 1...|$|E
40|$|For {{our first}} {{participation}} in CLEF retrieval tasks, our first {{objective was to}} define a general <b>stopword</b> list for various European languages (namely, French, Italian, German and Spanish) and also to suggest simple and efficient stemming procedures for them. Our second aim was to suggest a combined approach that might be implemented {{in order to facilitate}} effective access to multilingual collections. 1. Monolingual indexing and search Most European languages (including French, Italian, Spanish, German) share many of the same characteristics as does the language of Shakespeare (e. g., word boundaries marked in a conventional manner, variant word forms generated by adding suffixes {{to the end of a}} root, etc.). Any adaptation of indexing or search strategies thus means the elaboration of general <b>stopword</b> lists and fast stemming procedures. <b>Stopword</b> lists contain nonsignificant words that are removed from a document or a request before the indexing process is begun. Stemming procedures try to remove inflectional and derivational suffixes in order to conflate word variants into the same stem or root. This first chapter will deal with these issues and is organized as follows: Section 1. 1 contains an overview of our five test collections while Section 1. 2 describes our general approach to building <b>stopword</b> lists and stemmers for use with languages other than English. Section 1. 3 depicts the Okapi probabilistic model together with the description of the runs submitted by us in the monolingual track. 1. 1. Overview of the test-collections The corpora used in our experiments included newspapers such as the Los Angeles Times, Le Monde (French), La Stampa (Italian), Der Spiegel and Frankfurter Rundschau (German) and EFE (Spanish) and various news items edited by the Swiss news a [...] ...|$|E
30|$|In addition, the {{description}} of a keyword may {{not belong to the}} researcher’s list of stopwords. A pre-defined list of stopwords for English and Brazilian Portuguese languages, obtained from well-known public <b>stopword</b> repositories [34], is automatically registered for each new researcher. If the researcher is sure about the inclusion of a keyword that appears in his/her list of stopwords, he/she may update his/her list of stopwords. Stopwords are words that have no intrinsic meaning, and hence, they are not suitable for identification of specific concepts [35], such as software concerns. The usage of <b>stopword</b> lists in the ObasCId-Tool is useful because (i) it serves as a guide for the registration of more appropriated keywords and (ii) it may avoid the incidence of several false positives during concern identification process.|$|E
40|$|We {{illustrate}} {{the utility of}} generative models {{for the purpose of}} stylometry – the science of author attribution. Though content words provide semantic handles and intuitively relate to author-styles, they are usually associated with a large vocabu-lary and are not consistent across corpora. On the contrary, <b>stopwords</b> are limited in number and do not suffer from the above mentioned issues and yet seem to retain abstract signatures of author style. We explore the use of Latent Dirichlet Allocation on <b>stopwords</b> and show that the resulting topic distributions provide robust handles to classify authors and help perform authorship attributions. In ad-dition to this, we also observe that they are effective in identifying the gender of the authors. ...|$|R
40|$|<b>Stopwords</b> are meaningless, {{non-significant}} {{terms that}} frequently {{occur in a}} document. They should be removed, like a noise. Traditionally, two different approaches of building a stoplist have been used: the former considers the most frequent terms looking at a language (e. g., english stoplist), the other includes the most occurring terms in a document collection. In several tasks, e. g., text classification and clustering, documents are typically grouped into categories. We propose a novel approach aimed at automatically identifying specific <b>stopwords</b> for each category. The proposal relies on two unbiased metrics that allow to analyze the informative content of each term; one measures the discriminant capability and the latter measures the characteristic capability. For each term, the former {{is expected to be}} high in accordance with the ability to distinguish a category against others, whereas the latter is expected to be high according to how the term is frequent and common over all categories. A preliminary study and experiments have been performed, pointing out our insight. Results confirm that, for each domain, the metrics easily identify specific stoplist wich include classical and category-dependent <b>stopwords...</b>|$|R
40|$|Abstract. In this article, we {{investigate}} {{the structure of}} Croatian linguistic co-occurrence networks. We examine the change of network structure properties by systematically vary-ing the co-occurrence window sizes, the corpus sizes and removing <b>stopwords.</b> In a co-occurrence window of size n we establish {{a link between the}} current word and n − 1 subsequent words. The results point out that the increase of the co-occurrence window size is followed by a decrease in diameter, average path shortening and expectedly con-densing the average clustering coefficient. The same can be noticed for the removal of the <b>stopwords.</b> Finally, since the size of texts is reflected in the network properties, our results suggest that the corpus influence can be reduced by increasing the co-occurrence window size...|$|R
