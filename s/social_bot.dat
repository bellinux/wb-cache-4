11|51|Public
5000|$|A <b>social</b> <b>bot</b> (also: socialbot or socbot) is a {{particular}} type of chatterbot that is employed in social media networks to automatically generate messages (e.g. tweets) or in general advocate certain ideas, support campaigns, and public relations either by acting as a [...] "follower" [...] or even as a fake account that gathers followers itself. In this respect, social bots can be said to have passed the Turing test.|$|E
5000|$|The Kirchner {{government}} hired {{people to}} write in blogs, social networks such as Facebook and Twitter, internet forums and other web pages of public access. Known as [...] "Blogueros K" [...] or [...] "Cyber K", they were financed by the Chief of the Cabinet of Ministers. Their interventions are usually disruptive, and focused on discrediting the opponents with insults and cyberbullying. An investigation from the TV program Periodismo para todos revealed a network of <b>social</b> <b>bot</b> registered in Twitter, posting messages of advocacy of the Kirchners. According to the investigation, 400 users committed identity fraud, using profile photos of other users. All those accounts had similar URLs, similar contents, similar posts, and published posts {{in the same time}} of the day. This network of users produced nearly 6,000 messages by month and 200 by day. This ammount of messages helped to establish [...] "trending topics", the most popular topics of the day in Twitter. Ministers Nilda Garré and Juan Manuel Abal Medina shared many messages of those fake accounts, to further increase their popularity. The program also interviewed some people whose photos were used for the Twitter accounts, and confirmed that those accounts did not belong to them.|$|E
40|$|In the past, online social {{networks}} (OSN) like Facebook and Twitter became powerful instruments for communication and networking. Unfortunately, {{they have also}} become a welcome target for socialbot attacks. Therefore, a deep {{understanding of the nature}} of such attacks is important to protect the Eco-System of OSNs. In this extended abstract we propose a categorization scheme of <b>social</b> <b>bot</b> attacks that aims at providing an overview of the state of the art of techniques in this emerging field. Finally, we demonstrate the usefulness of our categorization scheme by characterizing recent socialbot attacks according to our categorization scheme. Author Keywords socialbots; attack; Taxonomy; categorization scheme...|$|E
40|$|While most online {{social media}} {{accounts}} {{are controlled by}} humans, these platforms also host automated agents called <b>social</b> <b>bots</b> or sybil accounts. Recent literature reported on cases of <b>social</b> <b>bots</b> imitating humans to manipulate discussions, alter the popularity of users, pollute content and spread misinformation, and even perform terrorist propaganda and recruitment actions. Here we present BotOrNot, a publicly-available service that leverages more than one thousand features to evaluate {{the extent to which}} a Twitter account exhibits similarity to the known characteristics of <b>social</b> <b>bots.</b> Since its release in May 2014, BotOrNot has served over one million requests via our website and APIs. Comment: 2 pages, 2 figures, WWW Developers Da...|$|R
50|$|<b>Social</b> <b>bots</b> {{appear to}} have played a {{significant}} role in the United States presidential election, 2016 and their history appears to go back at least to the United States midterm elections, 2010. Twitterbots are already well-known examples, but corresponding autonomous agents on Facebook and elsewhere have also been observed. Nowadays, <b>social</b> <b>bots</b> can generate convincing internet personas that are well capable of influencing real people, although they are not always reliable.|$|R
50|$|German {{chancellor}} Angela Merkel {{has issued}} the Bundestag {{to deal with}} the possibilities of political manipulation by <b>social</b> <b>bots</b> or fake news.|$|R
40|$|El objetivo de este trabajo es el de presentar el robot AIToy, la versión educativa del robot social con emociones AISoy. La exposición del estudio está dividida en tres partes: primera, crítica al sistema educativo tradicional; segunda, presentación a nivel {{software}} y hardware de AISoy; tercera, breve introducción al nuevo paradigma educativo en cuya filosofía se sustenta AIToy y, cuarta, descripción de la plataforma educativa y del sistema de diálogo de AIToy. The {{main goal}} {{of this paper is}} to introduce the educative bot AIToy, the new educative version of the emotional and <b>social</b> <b>bot</b> AISoy. This article is divided as following: first, critics to the traditional educative system are exposed; second, the main characteristics of AISoy’s bot are described; third, we introduce the new educative paradigm in which AIToy’s bot is based on and, finally, we show the AIToy’s educative platform and its dialogue system. ES...|$|E
40|$|Social botnets {{have become}} an {{important}} phenomenon on social media. There are {{many ways in which}} social bots can disrupt or influence online discourse, such as, spam hashtags, scam twitter users, and astroturfing. In this paper we considered one specific social botnet in Twitter to understand how it grows over time, how the content of tweets by the social botnet differ from regular users in the same dataset, and lastly, how the social botnet may have influenced the relevant discussions. Our analysis is based on a qualitative coding for approximately 3000 tweets in Arabic and English from the Syrian <b>social</b> <b>bot</b> that was active for 35 weeks on Twitter before it was shutdown. We find that the growth, behavior and content of this particular botnet did not specifically align with common conceptions of botnets. Further we identify interesting aspects of the botnet that distinguish it from regular users. Comment: 13 pages, 4 figures, Presented at the ACM conference on Computer-Supported Cooperative Work and Social Computing (CSCW 2016...|$|E
40|$|This thesis have enlightened {{the topic}} of social bots and how to {{indicate}} their presence. As a practical {{part of the work}} in this thesis, a program for indicating social bots on Twitter were developed. The presented solution have both a technological and a biological aspect. The technological aspect is the indicator program. The program analyze an account to look for indications of <b>social</b> <b>bot</b> activity. The different indicators used in the program were inspired by several earlier studies on social bots. As a result of analyzing data available through the Twitter API, some of the indicators are also based on new ideas. The program uses analyses of several different features and behaviors. By implementing several analyses, the program should be theoretically able to detect social bots with different behaviors. Instead of having a detection tool, the program analyze an account to look for indications of <b>social</b> <b>bot</b> activity. If any indications are present, the analyzed account is to be further inspected. This is where the biological aspect comes in. Further analyses can be performed by either a dedicated person or by crowdsourcing. Crowdsourcing is a promising approach of analyses. It is effective and spreads the workload over several people. The biological aspect of the solution is included because humans can detect small inconsistencies that is hard to define in algorithms. The indicator program works in near real-time. Right before analysis, account information and tweets from the account is downloaded. Right after account and tweet data is retrieved, the analyses are performed. The analyses are performed in about one to four seconds (url-analysis excluded). Having a fast-working indication program can be crucial for fast detection of social bots on Twitter. By having fast indication of social bots in place, the impact of the social bots cause is minimized. Social bots have become very advanced in the recent years. And they continue to develop. Advancements in fields such as AI, makes detection of social bots more difficult. By continuing to develop methods for detecting them, we can be ready for the next types of social bots. To stay ahead of new social bots that come out, we also need to research new approaches for detecting them...|$|E
5000|$|Bots (short for robots) are {{automated}} {{programs that}} run over the internet. [...] There are {{many forms of}} bots with varying behaviors. The bots most relevant to social media marketing are chatbots and <b>social</b> <b>bots.</b> Chatbots and <b>social</b> <b>bots</b> are programmed to mimic natural human interactions such as liking, commenting, following, and unfollowing on social media platforms. The ability of these <b>bots</b> to automate <b>social</b> media marketing needs has created a large demand {{and the establishment of}} a new industry of bot providers.|$|R
50|$|<b>Social</b> <b>bots,</b> {{besides being}} able to produce {{messages}} autonomously, also share many traits with spambots {{with respect to their}} tendency to infiltrate large user groups.|$|R
40|$|This paper {{gives an}} {{overview}} of impersonation bots that generate output in one, or possibly, multiple modalities. We also discuss rapidly advancing areas of machine learning and artificial intelligence {{that could lead to}} frighteningly powerful new multi-modal <b>social</b> <b>bots.</b> Our main conclusion is that most commonly known bots are one dimensional (i. e., chatterbot), and far from deceiving serious interrogators. However, using recent advances in machine learning, it is possible to unleash incredibly powerful, human-like armies of <b>social</b> <b>bots,</b> in potentially well coordinated campaigns of deception and influence. Comment: 2 figure...|$|R
40|$|Computational {{propaganda}} deploys {{social or}} political bots {{to try to}} shape, steer and manipulate online public discussions and influence decisions. Collective behaviour of populations of social bots has not been yet widely studied, though understanding of collective patterns arising from interactions between bots would aid <b>social</b> <b>bot</b> detection. Here we show that there are significant differences in collective behaviour between population of bots and population of humans as detected from their Twitter activity. Using a large dataset of tweets we have collected during the UK EU referendum campaign, we separated users into population of bots and population of humans based {{on the length of}} sequences of their high-frequency tweeting activity. We show that while pairwise correlations between users are weak they co-exist with collective correlated states, however the statistics of correlations and co-spiking probability differ in both populations. Our results demonstrate that populations of social bots and human users in social media exhibit collective properties similar to the ones found in social and biological systems placed near a critical point...|$|E
40|$|We present Edina, the University of Edinburgh's <b>social</b> <b>bot</b> for the Amazon Alexa Prize competition. Edina is a {{conversational}} agent whose responses utilize data {{harvested from}} Amazon Mechanical Turk (AMT) through an innovative new technique we call self-dialogues. These are conversations {{in which a}} single AMT Worker plays both participants in a dialogue. Such dialogues are surprisingly natural, efficient to collect and reflective of relevant and/or trending topics. These self-dialogues provide training data for a generative neural network {{as well as a}} basis for soft rules used by a matching score component. Each match of a soft rule against a user utterance is associated with a confidence score which we show is strongly indicative of reply quality, allowing this component to self-censor and be effectively integrated with other components. Edina's full architecture features a rule-based system backing off to a matching score, backing off to a generative neural network. Our hybrid data-driven methodology thus addresses both coverage limitations of a strictly rule-based approach and the lack of guarantees of a strictly machine-learning approach. Comment: 10 pages; submitted to the 1 st Proceedings of the Alexa Priz...|$|E
40|$|Social bots are {{automatic}} or semi-automatic {{computer programs}} that mimic humans and/or human behavior in online social networks. Social bots can attack users (targets) in online social networks {{to pursue a}} variety of latent goals, such as to spread information or to influence targets. Without a deep {{understanding of the nature}} of such attacks or the susceptibility of users, the potential of social media as an instrument for facilitating discourse or democratic processes is in jeopardy. In this paper, we study data from the <b>Social</b> <b>Bot</b> Challenge 2011 - an experiment conducted by the WebEcologyProject during 2011 - in which three teams implemented a number of social bots that aimed to influence user behavior on Twitter. Using this data, we aim to develop models to (i) identify susceptible users among a set of targets and (ii) predict users ’ level of susceptibility. We explore the predictiveness of three different groups of features (network, behavioral and linguistic features) for these tasks. Our results suggest that susceptible users tend to use Twitter for a conversational purpose and tend to be more open and social since they communicate with many different users, use more social words and show more affection than non-susceptible users...|$|E
40|$|As {{social media}} has {{permeated}} {{large parts of}} the population it simultaneously has become a way to reach many people e. g. with political messages. One way to efficiently reach those people is the application of automated computer programs that aim to simulate human behaviour - so called <b>social</b> <b>bots.</b> These bots are thought to be able to potentially influence users' opinion about a topic. To gain insight in the use of these bots {{in the run-up to the}} German Bundestag elections, we collected a dataset from Twitter consisting of tweets regarding a German state election in May 2017. The strategies and influence of <b>social</b> <b>bots</b> were analysed based on relevant features and network visualization. 61 <b>social</b> <b>bots</b> were identified. Possibly due to the concentration on German language as well as the elections regionality, identified bots showed no signs of collective political strategies and low to none influence. Implications are discussed. Comment: Accepted for publication in the Proceedings of the Australasian Conference on Information Systems, 201...|$|R
40|$|The Turing test {{aimed to}} {{recognize}} {{the behavior of a}} human from that of a computer algorithm. Such challenge is more relevant than ever in today's social media context, where limited attention and technology constrain the expressive power of humans, while incentives abound to develop software agents mimicking humans. These <b>social</b> <b>bots</b> interact, often unnoticed, with real people in social media ecosystems, but their abundance is uncertain. While many bots are benign, one can design harmful bots with the goals of persuading, smearing, or deceiving. Here we discuss the characteristics of modern, sophisticated <b>social</b> <b>bots,</b> and how their presence can endanger online ecosystems and our society. We then review current efforts to detect <b>social</b> <b>bots</b> on Twitter. Features related to content, network, sentiment, and temporal patterns of activity are imitated by bots {{but at the same time}} can help discriminate synthetic behaviors from human ones, yielding signatures of engineered social tampering. Comment: Check [URL] for the final version; 'Bot or Not?' is available at: [URL]...|$|R
40|$|Abstract—Online social {{networks}} (OSNs) are increasingly threatened by <b>social</b> <b>bots</b> which are software-controlled OSN accounts that mimic human users with malicious intentions. A social botnet {{refers to a}} group of <b>social</b> <b>bots</b> under the control of a single botmaster, which collaborate to conduct malicious behavior, {{while at the same time}} mimicking the interactions among normal OSN users to reduce their individual risk of being detected. We demonstrate the effectiveness and advantages of exploiting a social botnet for spam distribution and digital-influence manipulation through real experiments on Twitter and also trace-driven simulations. Our results can help understand the potentially detrimental effects of social botnets and help OSNs improve their bot(net) detection systems. I...|$|R
40|$|With the {{increasing}} importance of online social networks such as Twitter or Facebook, {{a new breed}} of computer programs, so-called social bots, emerged. Social bots are automatic or semi-automatic computer programs that mimic humans and/or human behavior in online social networks. Social bots can attack users in online social networks to pursue a variety of latent goals, such as to spread information or to influence targets. Without a deep {{understanding of the nature of}} such attacks or the susceptibility of users, the potential of social media as an instrument for facilitating discourse or democratic processes is in jeopardy. In this paper, we study data from the <b>Social</b> <b>Bot</b> Challenge 2011 - an experiment conducted by the WebEcologyProject during 2011 - in which three teams implemented a number of social bots that aimed to influence user behavior on Twitter. Using this data, we aim to develop models to (i) identify susceptible users among a set of targets and (ii) predict users’ level of susceptibility. We explore the predictiveness of three different groups of features (network, behavioral and linguistic features) for these tasks. Our results suggest that susceptible users tend to use Twitter for a conversational purpose and tend to be more open and social since they communicate with many different users, use more social words and show more affection than non-susceptible users...|$|E
40|$|Users with {{anomalous}} {{behaviors in}} online communication systems (e. g. email and social medial platforms) are potential threats to society. Automated anomaly detection based on advanced machine learning techniques {{has been developed}} to combat this issue; challenges remain, though, due {{to the difficulty of}} obtaining proper ground truth for model training and evaluation. Therefore, substantial human judgment on the automated analysis results is often required to better adjust the performance of anomaly detection. Unfortunately, techniques that allow users to understand the analysis results more efficiently, to make a confident judgment about anomalies, and to explore data in their context, are still lacking. In this paper, we propose a novel visual analysis system, TargetVue, which detects anomalous users via an unsupervised learning model and visualizes the behaviors of suspicious users in behavior-rich context through novel visualization designs and multiple coordinated contextual views. Particularly, TargetVue incorporates three new ego-centric glyphs to visually summarize a user's behaviors which effectively present the user's communication activities, features, and social interactions. An efficient layout method is proposed to place these glyphs on a triangle grid, which captures similarities among users and facilitates comparisons of behaviors of different users. We demonstrate the power of TargetVue through its application in a <b>social</b> <b>bot</b> detection challenge using Twitter data, a case study based on email records, and an interview with expert users. Our evaluation shows that TargetVue is beneficial to the detection of users with anomalous communication behaviors...|$|E
40|$|Online social {{networks}} (OSNs) are increasingly threatened by <b>social</b> <b>bots</b> which are software-controlled OSN accounts that mimic human users with malicious intentions. A social botnet {{refers to a}} group of <b>social</b> <b>bots</b> under the control of a single botmaster, which collaborate to conduct malicious behavior while mimicking the interactions among normal OSN users to reduce their individual risk of being detected. We demonstrate the effectiveness and advantages of exploiting a social botnet for spam distribution and digital-influence manipulation through real experiments on Twitter and also trace-driven simulations. We also propose the corresponding countermeasures and evaluate their effectiveness. Our results can help understand the potentially detrimental effects of social botnets and help OSNs improve their bot(net) detection systems. Comment: 14 pages, revision from TDS...|$|R
40|$|The Turing test {{asked whether}} one could {{recognize}} {{the behavior of}} a human from that of a computer al-gorithm. Today this question has suddenly become very relevant in the context of social media, where text constraints limit the expressive power of humans, and real incentives abound to develop human-mimicking software agents called <b>social</b> <b>bots.</b> These elusive entities wildly populate social media ecosystems, often go-ing unnoticed among the population of real people. Bots can be benign or harmful, aiming at persuading, smearing, or deceiving. Here we discuss the characteristics of modern, sophisticated <b>social</b> <b>bots,</b> and how their presence can endanger online ecosystems and our society. We then discuss current efforts aimed at detection of <b>social</b> <b>bots</b> in Twitter. Characteristics related to content, network, sentiment, and temporal pat-terns of activity are imitated by bots {{but at the same time}} can help discriminate synthetic behaviors from human ones, yielding signatures of engineered social tampering. The rise of the machines Bots (short for software robots) have been around since the early days of computers: one compelling example is that of chatbots, algorithms designed to hold a conversa-tion with a human, as envisioned by Alan Turing in the 1950 s [Turing 1950]. The dream of designing a computer algorithm that passes the Turing test has driven ar...|$|R
40|$|The massive {{spread of}} digital {{misinformation}} {{has been identified}} as a major global risk and has been alleged to influence elections and threaten democracies. Communication, cognitive, social, and computer scientists are engaged in efforts to study the complex causes for the viral diffusion of misinformation online and to develop solutions, while search and social media platforms are beginning to deploy countermeasures. However, to date, these efforts have been mainly informed by anecdotal evidence rather than systematic data. Here we analyze 14 million messages spreading 400 thousand claims on Twitter during and following the 2016 U. S. presidential campaign and election. We find evidence that <b>social</b> <b>bots</b> play a disproportionate role in spreading and repeating misinformation. Automated accounts are particularly active in amplifying misinformation in the very early spreading moments, before a claim goes viral. Bots target users with many followers through replies and mentions, and may disguise their geographic locations. Humans are vulnerable to this manipulation, retweeting bots who post misinformation. Successful sources of false and misleading claims are heavily supported by <b>social</b> <b>bots.</b> These results suggest that curbing <b>social</b> <b>bots</b> may be an effective strategy for mitigating the spread of online misinformation. Comment: 27 pages, 13 figures, 2 table...|$|R
50|$|Such {{manipulation}} may {{be conducted}} {{for purposes of}} propaganda, discreditation, harming corporate or political competitors, improving personal or brand reputation or plain trolling among other things. To accomplish these objectives, online influencers, hired professionals and/or software − typically Internet <b>bots</b> such as <b>social</b> <b>bots,</b> votebots and clickbots − may be used.|$|R
50|$|Ghost followers, also {{referred}} to as ghosts and ghost accounts or lurkers, are users on social media platforms who remain inactive or do not engage in activity. They register on platforms such as Twitter and Instagram. These users follow active members, but do not partake in liking, commenting, messaging, and posting. These accounts may be created by people or by <b>social</b> <b>bots.</b>|$|R
40|$|<b>Social</b> <b>bots</b> {{can affect}} online {{communication}} among humans. We study this phenomenon {{by focusing on}} #YaMeCanse, the most active protest hashtag {{in the history of}} Twitter in Mexico. Accounts using the hashtag are classified using the BotOrNot bot detection tool. Our preliminary analysis suggests that bots {{played a critical role in}} disrupting online communication about the protest movement. Comment: 10 page...|$|R
40|$|<b>Social</b> <b>bots</b> are {{currently}} regarded an influential but also somewhat mysterious factor in public discourse and opinion making. They {{are considered to}} be capable of massively distributing propaganda in social and online media and their application is even suspected to be partly responsible for recent election results. Astonishingly, the term `Social Bot' is not well defined and different scientific disciplines use divergent definitions. This work starts with a balanced definition attempt, before providing an overview of how <b>social</b> <b>bots</b> actually work (taking the example of Twitter) and what their current technical limitations are. Despite recent research progress in Deep Learning and Big Data, there are many activities bots cannot handle well. We then discuss how bot capabilities can be extended and controlled by integrating humans into the process and reason that this is currently the most promising way to go in order to realize effective interactions with other humans. Comment: 36 pages, 13 figure...|$|R
5000|$|The first {{generation}} of bots could sometimes be distinguished from real users by their often superhuman capacities to post messages around the clock (and at massive rates). Later developments have succeeded in imprinting more [...] "human" [...] activity and behavioural patterns in the agent. To unambiguously detect <b>social</b> <b>bots</b> as what they are, a variety of criteria must be applied together using pattern detection techniques, some of which are: ...|$|R
50|$|Menczer's {{group has}} focused on {{analysis}} and modeling of how memes and misinformation spread through social media in domains such as the Occupy movement and political elections, and how to combat astroturfing and detect <b>social</b> <b>bots.</b> Menczer and colleagues have also advanced the understanding of information virality, {{and in particular the}} effect of the competition for our finite attention and the prediction of what memes will go viral based on the structure of early diffusion networks.|$|R
30|$|There are {{at least}} three {{questions}} about information campaigns that present scientific challenges: what, how, and who. The first concerns the subtle notion of trustworthiness of information, ranging from verified facts [12], to rumors and exaggerated, biased, unverified or fabricated news [4, 7, 13]. The second considers the tools employed for the propaganda. Again, the spectrum is wide: from a known brand that openly promotes its products by targeting users who have shown interest, to the adoption of <b>social</b> <b>bots,</b> trolls and fake or manipulated accounts that pose as humans [5, 14 – 16]. The third question relates to the (possibly concealed) entities behind the promotion efforts and the transparency of their goals. Even before these question can be explored, one would {{need to be able to}} identify an information campaign in social media. But discriminating such campaigns from grassroots conversations poses both theoretical and practical challenges. Even the very definition of ‘campaign’ is conceptually difficult, as it entangles the nature of the content (e.g., product or news), purpose of the source (e.g., deception, recruiting), strategies of dissemination (e.g., promotion or orchestration), different dynamics of user engagement (e.g., the aforementioned <b>social</b> <b>bots),</b> and so on.|$|R
40|$|Online Social Networks (OSNs) play an {{important}} role for internet users to carry out their daily activities like content sharing, news reading, posting messages, product reviews and discussing events etc. At the same time, various kinds of spammers are also equally attracted towards these OSNs. These cyber criminals including sexual predators, online fraudsters, advertising campaigners, catfishes, and <b>social</b> <b>bots</b> etc. exploit the network of trust by various means especially by creating fake profiles to spread their content and carry out scams. All these malicious identities are very harmful for both the users as well as the service providers. From the OSN service provider point of view, fake profiles affect the overall reputation of the network in addition to the loss of bandwidth. To spot out these malicious users, huge manpower effort and more sophisticated automated methods are needed. In this paper, various types of OSN threat generators like compromised profiles, cloned profiles and online <b>bots</b> (spam <b>bots,</b> <b>social</b> <b>bots,</b> like bots and influential bots) have been classified. An attempt is made to present several categories of features that have been used to train classifiers in order to identify a fake profile. Different data crawling approaches along with some existing data sources for fake profile detection have been identified. A refresher on existing cyber laws to curb social media based cyber crimes with their limitations is also presented. Comment: 31 pages, 8 figure...|$|R
5000|$|The use of <b>social</b> <b>bots</b> and chatbots {{has created}} an {{analytical}} crisis in the marketing industry. Companies use <b>social</b> and chat <b>bots</b> to automate their social marketing that appears to consumers and other companies to be real interaction. The ability for bots to mimic human interaction {{makes it difficult for}} marketers and data analysts to differentiate between human interactions and automated bot interactions; having implications for quality of data. Companies continue to use bots to automate their social media interactions although the same bots are negatively affecting their marketing data causing a [...] "digital cannibalism" [...] in social media marketing. Additionally, bots violate the terms of use on many social mediums such as Instagram. This can result in profiles being taken down and banned.|$|R
5000|$|The term [...] "ChatterBot" [...] was {{originally}} coined by Michael Mauldin (creator {{of the first}} Verbot, Julia) in 1994 to describe these conversational programs. Today, chatbots are part of virtual assistants such as Google Assistant, and are accessed via many organizations' apps, websites, and on instant messaging platforms such as Facebook Messenger. [...] Non-assistant applications include chatbots used for entertainment purposes, for research, and <b>social</b> <b>bots</b> which promote a particular product, candidate, or issue. Actually Semantycs of Full On Net, emulates human behavior and interact with databases and ERP like SAP.|$|R
40|$|Bots {{have been}} playing {{a crucial role in}} online {{platform}} ecosystems, as efficient and automatic tools to generate content and diffuse information to the social media human population. In this chapter, we will discuss the role of <b>social</b> <b>bots</b> in content spreading dynamics in social media. In particular, we will first investigate some differences between diffusion dynamics of content generated by bots, as opposed to humans, in the context of political communication, then study the characteristics of bots behind the diffusion dynamics of social media spam campaigns. Comment: Chapter of the book "Spreading Dynamics in Social Systems" edited by Y. Y. Ahn and Sune Lehman...|$|R
40|$|In {{order to}} build {{dialogue}} systems to tackle the ambitious task of holding social conversations, we argue {{that we need a}} data driven approach that includes insight into human conversational chit chat, and which incorporates different natural language processing modules. Our strategy is to analyze and index large corpora of social media data, including Twitter conversations, online debates, dialogues between friends, and blog posts, and then to couple this data retrieval with modules that perform tasks such as sentiment and style analysis, topic modeling, and summarization. We aim for personal assistants that can learn more nuanced human language, and to grow from task-oriented agents to more personable <b>social</b> <b>bots.</b> Comment: IWSDS 201...|$|R
40|$|Online social {{networks}} (OSN) like Twitter or Facebook are popular and powerful since they allow reaching millions of users online. They {{are also a}} popular target for socialbot attacks. Without a deep understanding {{of the impact of}} such attacks, the potential of online {{social networks}} as an instrument for facilitating discourse or democratic processes is in jeopardy. In this extended abstract we present insights from a live lab experiment in which <b>social</b> <b>bots</b> aimed at manipulating the social graph of an online social network, in our case Twitter. We explored the link creation behavior between targeted human users and our results suggest that socialbots may indeed have the ability to shape and influence the social graph in online social networks. However, our results also show that external factors may {{play an important role in}} the creation of social links in OSNs...|$|R
