1062|1204|Public
5|$|The rise of {{consumer}} GPUs {{has led to}} support for compute kernels, either in graphics APIs (referred to as compute <b>shaders),</b> in dedicated APIs (such as OpenCL), or in other language extensions.|$|E
5|$|At the time, floating-point {{division}} was generally expensive compared to multiplication; the fast inverse square root algorithm bypassed the division step, giving it its performance advantage. Quake III Arena, a first-person shooter video game, used the fast inverse square root algorithm to accelerate graphics computation, but the algorithm {{has since been}} implemented in some dedicated hardware vertex <b>shaders</b> using field-programmable gate arrays (FPGA).|$|E
5|$|Graphically, F.E.A.R. uses normal mapping and {{parallax}} mapping to give textures a {{more realistic}} appearance; the latter is used to {{give the appearance of}} depth to flat bullet hole sprites on walls. Volumetric lighting and lightmapping are included {{with the addition of a}} per-pixel lighting model, allowing complex lighting effects to be developed. Vertex, pixel and high-level <b>shaders,</b> including a host of additional special effects, are also featured in Jupiter EX.|$|E
5000|$|Maximum <b>shader</b> {{operations}}: 48 billion <b>shader</b> operations {{per second}} (3 <b>shader</b> pipelines × 16 processors × 2 ALUs × 500 MHz) ...|$|R
50|$|<b>Shader</b> Model 4.0 is {{a feature}} of DirectX 10, which has been {{released}} with Windows Vista. <b>Shader</b> Model 4.0 allows 128-bit HDR rendering, as opposed to 64-bit HDR in <b>Shader</b> Model 3.0 (although this is theoretically possible under <b>Shader</b> Model 3.0).|$|R
40|$|Abstract. <b>Shader</b> {{plays an}} {{important}} role in DirectX 11, and whatever things are rendered by <b>Shader.</b> It is great significance to study virtual simulation technique based on <b>Shader.</b> Firstly, the common <b>shader</b> core is introduced by this paper. Secondly, the virtual simulation system architecture based on <b>shader</b> is studied. Finally, a virtual simulation system is implemented based on DirectX 11 in VS 2010, and achieves better results...|$|R
5|$|Deathrow is {{a sports}} video game {{developed}} by Southend Interactive {{and published by}} Ubisoft Entertainment. It was released as an Xbox gaming system exclusive on 18 October 2002 in Europe and four days later in North America. Deathrow development began in May 1999 as an online PC (player computer) game. In early 2001, Southend transitioned to an Xbox console release, which let the team use pixel <b>shaders,</b> bump mapped textures, and specular lighting. Deathrow was built on an in-house 3D game engine and was Southend's first full release.|$|E
5|$|ILM was {{tasked with}} imagining what the {{immediate}} assimilation of an Enterprise crewmember would look like. Jaeger {{came up with}} a set of cables that sprang from the Borg's knuckles and buried themselves in the crewmember's neck. Wormlike tubes would course through the victim's body and mechanical devices break the skin. The entire transformation was created using computer-generated imagery. The wormlike geometry was animated over the actor's face, then blended in {{with the addition of a}} skin texture over the animation. The gradual change in skin tone was simulated with <b>shaders.</b>|$|E
5|$|Rendering was {{executed}} in Mental Ray, using numerous custom-made <b>shaders</b> to produce lifelike lighting, water and landscapes. Wireframes {{also served as}} a guide to model the backgrounds. In total Riven has over three hours of video and almost five thousand images; rendering was a major bottleneck in production despite the use of 18 dedicated workstations. Some scenes consisted {{of tens of thousands of}} individual models and textures and more than a hundred virtual light sources. Loading a single island model could take two hours. Runtime animation effects were created by Mark DeForest, to add flying insects and simple water ripples.|$|E
5000|$|Multi-way {{programmable}} parallel floating-point <b>shader</b> pipelines, independent pixel/vertex <b>shader</b> architecture ...|$|R
5000|$|<b>Shader</b> (2012, reissued in 2016 as <b>Shader</b> Complete) (as Sacred Tapestry) ...|$|R
50|$|Support include macro, custom <b>shader</b> code block, dynamic {{definition}} in <b>shader</b> compiling.|$|R
5|$|The head of {{the project}} was Yoshihisa Hashimoto, Square Enix's Chief Technology Officer, who had moved over to the company from Sonic Team in 2009 and became {{involved}} with development in 2011. Other key Square Enix staff members working on Luminous Studio include Takeshi Nozue, Akira Iwata and Hiroshi Iwasaki. While ground work was being laid for Luminous, members of the team traveled to look at engine technology being developed by IO Interactive, Crystal Dynamics and Eidos Montréal, western video game developers who became subsidiaries after the company bought out Eidos Interactive. Square Enix's western subsidiaries shared information about game engine development from their experience developing the CDC and Glacier 2 game engines and shared their source code with the Luminous Studio team. During 2012, one third of the final development team was from western subsidiaries of the company. Luminous was developed based on high-end DirectX 11 technology. While designed for eighth-generation video games, it was said to also be compatible with any console and hardware that could handle <b>shaders,</b> such as PlayStation 3 and Xbox 360. Its compatibility with Nintendo's seventh-gen hardware such as the Wii and Nintendo 3DS was doubted, as those consoles did not support <b>shaders.</b> During this early stage, they were looking into the possibility of adjusting the engine for use on Wii U. The company were hoping to promote Luminous as a kind of brand, showing off the logo and tech demo when they were ready.|$|E
5|$|Uncharted {{uses the}} Cell {{microprocessor}} to generate dozens of layered character animations to portray realistic expressions and fluid movements, which allow for responsive player control. The PlayStation 3's graphics processing unit, the RSX Reality Synthesizer, employed several functions to provide graphical details that helped immerse the player {{into the game}} world: lighting models, pixel <b>shaders,</b> dynamic real-time shadowing, and advanced water simulation. The new hardware allowed for processes which the team had never used in PlayStation 2 game development and required them to quickly familiarize with the new techniques; for example, parallel processing and pixel <b>shaders.</b> While Blu-ray afforded greater storage space, the team became concerned with running out of room several times—Uncharted used more and bigger textures than previous games, and included several languages on the disc. Gameplay elements requiring motion sensing, such as throwing grenades and walking across beams, were implemented {{to take advantage of}} the Sixaxis controller. A new PlayStation 3 controller, the DualShock 3, was unveiled at the 2007 Tokyo Game Show, and featured force feedback vibration. Uncharted was also on display at the show with demonstrations that implemented limited support for vibration.|$|E
5|$|As {{with each}} season since Big Brother 6, {{the program was}} filmed at CBS Studios in Studio City, California. The {{production}} team was located in the second story of the House which included the story department, audio department and the switchers and <b>shaders.</b> The House was equipped with 52 cameras and 80 microphones to record the participants. The art department that created the competitions for the program was located outside the House. The House theme was eco-friendly and modern California living was released on June 29 during media day, where select {{members of the press}} were invited to spend 12 hours inside the House. Official pictures of the House interior were released by CBS on the same day, showing the living room, bedrooms, kitchen, bathroom, lounge room and backyard. The living room featured chipboard walls with fake plants along the side.|$|E
5000|$|... #Caption: A normal <b>shader</b> (left) and a NPR <b>shader</b> using cel-shading (right) ...|$|R
5000|$|Complex <b>shader</b> effects {{began their}} {{days with the}} release of <b>Shader</b> Model 1.0 with DirectX 8. <b>Shader</b> Model 1.0 {{illuminated}} 3D worlds with what is called standard lighting. Standard lighting, however, had two problems: ...|$|R
50|$|While heavily {{based upon}} the {{previous}} generation, this line included extensions to the <b>Shader</b> Model 2 feature-set. <b>Shader</b> Model 2b, the specification ATI and Microsoft defined with this generation, offered somewhat more <b>shader</b> program flexibility.|$|R
5|$|First Contact was {{the last}} film to feature a {{physical}} model of the Enterprise. For the ship's dramatic introduction, the effects team combined motion control shots of the Enterprise model with a computer-generated background. Sequence supervisor Dennis Turner, who had created Generations energy ribbon and specialized in creating natural phenomena, was charged with creating the star cluster, modeled after the Eagle Nebula. The nebular columns and solid areas were modeled with basic wireframe geometry, with surface <b>shaders</b> applied to make {{the edges of the}} nebula glow. A particle render that ILM had devised for the earlier tornado film Twister was used to create a turbulent look within the nebula. Once the shots of the Enterprise had been captured, Turner inserted the ship into the computer-generated background and altered its position until the images matched up.|$|E
5|$|Max Payne 2 {{uses the}} same game engine as the one used in Max Payne, but with several {{significant}} upgrades. Even though the game only supports DirectX 8.1, the graphics in Max Payne 2 mimic those generated by DirectX 9 by making optimal use of effects such as reflection, refraction, <b>shaders,</b> and ghosting. The developers considered one particular scene in which effects are used well: When Max has lucid dreams, the screen appears fuzzy and out of focus. Since Max Payne, the polygon count (the number of polygons rendered per frame) has been increased, which smooths out the edges of character models. In addition, characters have a much greater range of expressions. Previously, Max had only one expression available; in Max Payne 2, he often smirks and moves his eyebrows to react to different scenarios.|$|E
5|$|The game's water effects {{received}} substantial upgrades {{because of}} the large role naval battles play in The Battle for Middle-earth II. The developers endeavored to make the surface of oceans and lakes look realistic by using techniques similar to those applied in films when creating computer-generated ocean water. The digital water simulates deep ocean water by reflecting its surroundings on the surface, and wave technology was used to create large waves along coastlines to immerse the player in the game experience. Lost towns, corals, and fish were added underwater to add to the effect. Water was chosen as the first graphical component of The Battle for Middle-earth II to take advantage of DirectX 9 programmable <b>shaders.</b> These additions were part of an overall Electronic Arts strategy to continue the Lord of the Rings experience that began with the trilogy film series.|$|E
5000|$|Config Core Rankine, Curie: Vertex <b>Shader</b> : Pixel <b>Shader</b> : TMUs : ROPs ...|$|R
50|$|Although the R420-based {{chips are}} {{fundamentally}} similar to R300-based cores, ATI did tweak {{and enhance the}} pixel <b>shader</b> units for more flexibility. A new pixel <b>shader</b> version (PS2.b) allowed slightly greater <b>shader</b> program flexibility than plain PS2.0, but was still shy of full PS3.0 capabilities. This new revision to PS2.0 increased {{the maximum number of}} instructions and registers available to pixel <b>shader</b> programs.|$|R
50|$|On August 9, 2004, Microsoft updated DirectX {{once more}} to DirectX 9.0c. This also exposed the <b>Shader</b> Model 3.0 profile for {{high-level}} <b>shader</b> language (HLSL). <b>Shader</b> Model 3.0's lighting precision has a minimum of 32 bits as opposed to 2.0's 8-bit minimum. Also all lighting-precision calculations are now floating-point based. NVIDIA states that contrast ratios using <b>Shader</b> Model 3.0 can {{be as high as}} 65535:1 using 32-bit lighting precision. At first, HDRR was only possible on video cards capable of Shader-Model-3.0 effects, but software developers soon added compatibility for <b>Shader</b> Model 2.0. As a side note, when referred to as <b>Shader</b> Model 3.0 HDR, HDRR is really done by FP16 blending. FP16 blending is not part of <b>Shader</b> Model 3.0, but is supported mostly by cards also capable of <b>Shader</b> Model 3.0 (exceptions include the GeForce 6200 series). FP16 blending {{can be used as a}} faster way to render HDR in video games.|$|R
5|$|The {{graphics}} engine used {{to power the}} game was christened the Diesel Engine. Grin developed this engine for flexibility and scalability, allowing the engine to be easily upgraded with new features. Based on DirectX, this meant the engine could be used across Windows and Xbox platforms. Grin worked closely with NVIDIA to incorporate then new technologies such as pixel and vertex <b>shaders</b> to render complex scenes. Ballistics was marketed by NVIDIA as a flagship title for their new series of GeForce 3 graphics cards. The game came bundled with various versions of the GeForce 3, with distribution of the bundled game handled by Interplay OEM. Later versions of the Diesel Engine would be used in Grin games such as the Windows version of Tom Clancy's Ghost Recon Advanced Warfighter.|$|E
5|$|To {{experience}} {{firsthand the}} processes mod-makers {{would have to}} go through with the new engine, Valve ported Half-Life (dubbed Half-Life: Source) and Counter-Strike to their new Source engine. Half-Life: Source is a straight port, lacking any new content or the Blue Shift High Definition pack. However, it does take advantage of vertex and pixel <b>shaders</b> for more realistic water effects, as well as Half-Life 2's realistic physics engine. They also added several other features from Half-Life 2, including improved dynamic lightmaps, vertex maps, ragdolls, and a shadowmap system with cleaner, higher resolution, specular texture and normal maps, as well as utilization of the render-to-texture soft shadows found in Half-Life 2s Source engine, along with 3D skybox replacements in place of the old 16-bit color prerendered bitmap skies. The Half-Life port possesses many of the Source engine's graphical strengths as well as control weaknesses that have been noted in the Source engine. Later updates added a field of view options, support for OS X and Linux, an optional high-definition texture pack, among other improvements. Half-Life: Source is available with special editions of Half-Life 2, or separately on Steam.|$|E
5|$|For the game's characters, the {{developers}} {{intended to keep}} the comical looks from the previous games, but they decided to update their appearances with realistic materials and textures {{to take advantage of}} the more powerful level of graphics capabilities of seventh-generation consoles. The designers first drew character sketches to determine the characters' looks and clothing, creating a main and an alternate outfit for each character, which were then made into reference artwork. Afterward, low-level meshes for each character were created, with details added with Zbrush, resulting in characters that, before being scaled down for the game environment, had more than six million polygons. Textures and pixel <b>shaders</b> were added using the team's previous work on Tony Hawk's Project 8 to match the style of earlier Guitar Hero games. Unlike the Tony Hawk games, each character in the game was given a unique skeleton to match their variations in sizes and shapes, allowing the animators to create unique moves for individual characters. Guitar Hero III: Legends of Rock introduces three new characters modeled after real-life musicians. Slash, Tom Morello, and Bret Michaels, each performing one or more songs from their past recordings. All three were brought into the game using motion capture from the Motion Analysis Corporation.|$|E
50|$|Phenomena {{consist of}} one or more <b>shader</b> trees (DAG). A {{phenomenon}} looks like regular <b>shader</b> to the user, and in fact may be a regular <b>shader,</b> but generally it will contain a link to a <b>shader</b> DAG, which may include the introduction or modification of geometry, introduction of lenses, environments, and compile options. The idea of a Phenomenon is to package elements and hide complexity.|$|R
50|$|SP - <b>Shader</b> Processor (Unified <b>Shader,</b> CUDA Core), SFU - Special Function Unit, SM - Streaming Multiprocessor.|$|R
50|$|R600 core {{includes}} 64 <b>shader</b> clusters, while RV610 and RV630 cores have 8 and 24 <b>shader</b> clusters respectively.|$|R
5|$|The goal of Lost Coast was to {{demonstrate}} the new high-dynamic-range rendering implemented into the Source game engine. Valve first attempted to implement high-dynamic-range rendering in Source in late 2003. The first method stored textures in RGBA color space, allowing for multisample anti-aliasing and pixel <b>shaders</b> to be used, but this prevented alpha mapping and fog effects from working properly, as well as making textures appear sharp and jagged. The second method involved saving two versions of a texture: one with regular data, {{and the other with}} overbrightening data. However, this technique did not allow for multisample anti-aliasing and consumed twice as much video card memory, making it infeasible. The third method, shown at the E3 convention in 2005, used floating point data to define the RGB color space, allowing for reasonably efficient storage of the high-dynamic-range data. However, this method also did not allow for multisample anti-aliasing, and was only compatible with Nvidia video cards, leaving ATI cards unable to run high dynamic range. The fourth and final method compromised between the second and third methods, using overbrightening textures sparingly and allowing ATI cards to render HDR in a different way to the Nvidia ones while nearly producing the same end result.|$|E
5|$|Tomb Raider: Underworld {{continued}} the plot line established in Legend. Crystal Dynamics used {{new technology to}} improve the character for seventh generation consoles, focusing on improving realism. The dirt accumulation and water cleansing mechanic from Legend was altered to be a real-time mechanic that can involve the entire game environment. To achieve a more natural appearance, the developers added spherical harmonics to provide indirect lighting to in-game objects like Lara Croft. Crystal Dynamics made the character model more complex and detailed than previous instalments, featuring more texture layers that determine the appearance of shadows and reflective light on it, and using skeletal animation to portray believable movement. The number of polygons in the model increased to 32,000. The developers enhanced Croft's facial model by {{increasing the number of}} polygons, bones used in the animation skeleton, and graphical <b>shaders</b> in the face to add more detail and expressive capabilities. The hair was created as a real-time cloth simulation to further add realism to its shape and movement. The developers kept Croft's hair tied back because they felt a real person would not want it flying around while performing dangerous manoeuvres. The character's body size was increased and breast size reduced to portray more realistic proportions.|$|E
25|$|Shader storage buffer objects, {{allowing}} <b>shaders</b> to {{read and}} write buffer objects like image load/store from 4.2, but through the language rather than function calls.|$|E
40|$|In this paper, {{we present}} a rapid {{prototyping}} framework for GPU-based volume rendering. Therefore, we propose a dynamic <b>shader</b> pipeline based on the SuperShader concept and illustrate the design decisions. Also, important requirements {{for the development of}} our system are presented. In our approach, we break down the rendering <b>shader</b> into areas containing code for different computations, which are defined as freely combinable, modularized <b>shader</b> blocks. Hence, high-level changes of the rendering configuration result in the implicit modification of the underlying <b>shader</b> pipeline. Furthermore, the prototyping system allows inserting custom <b>shader</b> code between <b>shader</b> blocks of the pipeline at run-time. A suitable user interface is available within the prototyping environment to allow intuitive modification of the <b>shader</b> pipeline. Thus, appropriate solutions for visualization problems can be interactively developed. We demonstrate the usage and the usefulness of our framework with implementations of dynamic rendering effects for medical applications...|$|R
30|$|Tegra K 1 {{contains}} a recent GPGPU that adopts a unified <b>shader</b> architecture (Fig. 4). Unified shading architecture hardware {{is composed of}} an array of computing units (192 CUDA cores) which are capable of handling any type of shading tasks instead of dedicated vertex and fragment processor as in old GPUs. All computing units have the same characteristics. They can run either a fragment <b>shader</b> or a vertex <b>shader.</b> With a heavy vertex workload, we could allocate most computing units to run a vertex <b>shader.</b> In {{the case of a}} low vertex workload and a heavy fragment load, more computing units could be allocated to run fragment <b>shader.</b> In our work, we allocate more processing units to run the fragment <b>shader</b> to perform the desired parallel processing.|$|R
50|$|Bryan <b>Shader</b> has a {{daughter}} named Sarah <b>Shader</b> who is currently attending MIT and studying Computer Science and perhaps math.|$|R
