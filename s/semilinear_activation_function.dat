1|4139|Public
40|$|Abstract. We {{consider}} {{the problem of}} efficient approximate learning by multilayered feedforward circuits subject to two objective functions. First, we {{consider the}} objective to maximize the ratio of correctly classified points compared to the training set size (e. g., see [3, 5]). We show that for single hidden layer threshold circuits with n hidden nodes and varying input dimension, approximation of this ratio within a relative error c/n 3, for some positive constant c,is NP-hard even {{if the number of}} examples is limited with respect to n. For architectures with two hidden nodes (e. g., as in [6]), approximating the objective within some fixed factor is NP-hard even if any sigmoid-like activation function in the hidden layer and ε-separation of the output [19] is considered, or if the <b>semilinear</b> <b>activation</b> <b>function</b> substitutes the threshold function. Next, we consider the objective to minimize the failure ratio [2]. We show that it is NP-hard to approximate the failure ratio within every constant larger than 1 for a multilayered threshold circuit provided the input biases are zero. Furthermore, even weak approximation of this objective is almost NP-hard. ...|$|E
40|$|We propose an {{adaptive}} <b>activation</b> <b>function</b> of neural network classifier for isolated handwritten digits that undergo basic transformations. The utilized network is a backpropagation network with sigmoid and arctangent <b>activation</b> <b>functions.</b> The performance of network with both <b>activation</b> <b>functions</b> is compared. The {{results show that}} the network applying {{an adaptive}} <b>activation</b> <b>function</b> between layers converged much faster compared to non-adaptive <b>activation</b> <b>functions</b> with 50...|$|R
5000|$|The <b>activation</b> <b>function</b> [...] is {{non-linear}} and differentiable. A {{commonly used}} <b>activation</b> <b>function</b> is the logistic function: ...|$|R
40|$|Abstract This paper {{proposes a}} {{simultaneous}} learning algorithm for both <b>activation</b> <b>functions</b> and connection weights. The <b>activation</b> <b>function</b> {{is composed of}} several basic functions, such as sigmoidal function, Gaussian function and so on. In order to avoid local minima, the <b>activation</b> <b>functions</b> are controlled and randomly disturbed every some epochs. The <b>activation</b> <b>functions</b> are automatically optimized for each application. Probability and speed of learning are higher than the conventionals...|$|R
40|$|This article {{deals with}} <b>activation</b> <b>functions</b> for Cloud model’s type of neural networks. Cloud model {{offers a new}} {{perspective}} on qualitative–quantitative reasoning and represent the natural way of expressing the fuzziness and randomness close to human thinking. 1 Cloud <b>Activation</b> <b>Function</b> The <b>activation</b> <b>function</b> is a mechanism which activates the information in hidden layer neurons and transforms it into the desired output. Dozens of <b>activation</b> <b>functions</b> are used in neural networks. The most similar to the Cloud model <b>activation</b> <b>function</b> is Radial Basis Function used in RBF networks, especially the Gaussian one: where c is the vector of centers. φ...|$|R
3000|$|An <b>activation</b> <b>function</b> {{is used to}} {{calculate}} the output of the neural network. The learning procedure of the neural network requires the differentiation of the <b>activation</b> <b>function</b> to renew the weights value. Therefore, the <b>activation</b> <b>function</b> has to be differentiable. A sigmoid function, having an [...] " [...] [...]...|$|R
5000|$|Nonlinear - When the <b>activation</b> <b>function</b> is non-linear, then a two-layer {{neural network}} can be {{proven to be}} a {{universal}} function approximator. The identity <b>activation</b> <b>function</b> does not satisfy this property. When multiple layers use the identity <b>activation</b> <b>function,</b> the entire network is equivalent to a single-layer model.|$|R
3000|$|... (x) is a {{non-linear}} <b>activation</b> <b>function.</b> Theory {{proves that}} feedforward networks embedding a sigmoidal <b>activation</b> <b>function,</b> sigm(r) = (1 + e−r [...]...|$|R
30|$|In biologically {{inspired}} neural networks, the <b>activation</b> <b>function</b> {{is usually}} an abstraction representing {{the rate of}} action potential firing in the cell. Non-monotonic functions can be more suitable than other <b>activation</b> <b>functions.</b> In many electronic circuits, the input-output functions of amplifiers may be neither monotonically increasing nor continuously differentiable. The constants are positive, negative or zero in the above assumption. So, the <b>activation</b> <b>function</b> may be non-monotonic and more widespread than usual sigmoid functions and Lipschitz functions. Such conditions are discourteous in qualifying the lower and upper bounds of the <b>activation</b> <b>functions.</b> Therefore, by using the LMI-based technique, the generalized <b>activation</b> <b>function</b> is considered to reduce the possible conservatism.|$|R
3000|$|... 2) <b>Activation</b> <b>Function</b> and Optimal Hidden Node Range: The {{number of}} hidden layer nodes {{and the choice}} of <b>activation</b> <b>functions</b> {{also need to be}} {{adjusted}} in the training process of each single learning unit. For an <b>activation</b> <b>function,</b> the ELM computation accuracy can only be maximized within a specific hidden node range. In the test, the Sigmoid and Sine functions are chosen as the candidate <b>activation</b> <b>functions,</b> and the optimal hidden node range for those two functions is [150, 250].|$|R
40|$|It is {{well known}} that the {{behaviour}} of a neural network built with classical summing neurons, as in a multilayer perceptron, widely depends on the <b>activation</b> <b>functions</b> of the involved neurons. Many authors have proposed the use of <b>activation</b> <b>functions</b> with some free parameters which should allow one {{to reduce the size of}} the network, trading connection complexity with <b>activation</b> <b>function</b> complexity. Since many implementations of neural network are based on digital hardware, performing the selected <b>activation</b> <b>function</b> through a lookup-table (LUT), it could be interesting to study neural networks whose neurons have adaptable LUT-based <b>activation</b> <b>functions.</b> In this way, after learning, the neurons will present arbitrary <b>activation</b> <b>functions</b> which can also be efficiently implemented with digital technologies. In this paper a preliminary study of the adaptive LUT-based neuron (L-neuron) is presented, together with some experimental results on canonical problems...|$|R
40|$|The {{influence}} of the <b>activation</b> <b>function</b> on fault tolerance property of the feedforward neural networks is empirically investigated. The simulation {{results show that the}} <b>activation</b> <b>function</b> largely influences the fault tolerance and the generalization property of neural networks. The neural networks with symmetric sigmoid <b>activation</b> <b>function</b> is largely fault tolerant than the networks with asymmetric sigmoid function. The close relation between the fault tolerance and the the generalization property was not observed and the networks with asymmetric <b>activation</b> <b>function</b> slightly generalize better than the network with the symmetric <b>activation</b> <b>function.</b> An XOR-like problem that allows a practical investigation of the fault tolerance property of the networks with different <b>activation</b> <b>functions</b> is presented. Then the results are evaluated on character recognition problem on which the generalization ability is investigated. 1 Introduction Feedforward neural networks (NNs), trained with ba [...] ...|$|R
40|$|This paper {{shows that}} the {{performance}} of the Hopfield network for solving optimization problems can be improved by using a new <b>activation</b> (output) <b>function.</b> The effects of the <b>activation</b> <b>function</b> on {{the performance of the}} Hopfield network are analyzed. It is shown that the sigmoid <b>activation</b> <b>function</b> in the Hopfield network is sensitive to noise of neurons. The reason is that the sigmoid function is most sensitive in the range where noise is most predominant. A new <b>activation</b> <b>function</b> that is more robust against noise is proposed. The new <b>activation</b> <b>function</b> has the capability of amplifying the signals between neurons while suppressing noise. The performance of the new <b>activation</b> <b>function</b> is evaluated through simulation. Compared with the sigmoid <b>function,</b> the new <b>activation</b> <b>function</b> reduces the error rate of tour length by 30. 6 % and increases the percentage of valid tours by 38. 6 % during simulation on 200 randomly generated city distributions of the 10 -city traveling salesman problem...|$|R
30|$|The final {{parameter}} to find is the <b>activation</b> <b>function,</b> {{the choice}} of <b>activation</b> <b>function</b> is an important design issue, it is {{a vital part of}} neural network providing nonlinear mapping potential and help achieving fast convergence and good generalization performance. To choose the right <b>activation</b> <b>function</b> we a asses the performance of the three most used <b>activation</b> <b>functions</b> in FFNN architecture, the radial basis function (RBF), the tangent sigmoid function (Tansig) and the logistic sigmoid function (Logsig), we do so for each developed model.|$|R
40|$|This paper {{shows on}} {{developing}} agent based modelling for represent {{the performance of}} doing logic programming in Hopfield network by using a new <b>activation</b> <b>function.</b> The effects of the <b>activation</b> <b>function</b> {{on the performance of}} the neuro-symbolic integration are analyzed mathematically and compared with the existing method. Computer simulations are carried out to validate the effectiveness on the new <b>activation</b> <b>function.</b> The resuls obtained showed that the new <b>activation</b> <b>function</b> outperform the existing method in doing logic programming in Hopfield network. The models developed by agent based modelling also support this theory...|$|R
40|$|We {{investigate}} {{the existence and}} dynamical behaviors of multiple equilibria for competitive neural networks with a class of general Mexican-hat-type <b>activation</b> <b>functions.</b> The Mexican-hat-type <b>activation</b> <b>functions</b> are not monotonously increasing, {{and the structure of}} neural networks with Mexican-hat-type <b>activation</b> <b>functions</b> is totally different from those with sigmoidal <b>activation</b> <b>functions</b> or nondecreasing saturated <b>activation</b> <b>functions,</b> which have been employed extensively in previous multistability papers. By tracking the dynamics of each state component and applying fixed point theorem and analysis method, some sufficient conditions are presented to study the multistability and instability, including the total number of equilibria, their locations, and local stability and instability. The obtained results extend and improve the very recent works. Two illustrative examples with their simulations are given to verify the theoretical analysis...|$|R
40|$|Abstract. A {{special class}} of {{recurrent}} neural network, termed Zhang neu-ral network (ZNN) {{depicted in the}} implicit dynamics, has recently been proposed for online solution of time-varying matrix square roots. Such a ZNN model can be constructed by using monotonically-increasing odd ac-tivation functions to obtain the theoretical time-varying matrix square roots in an error-free manner. Different choices of <b>activation</b> <b>function</b> arrays may lead to different performance of the ZNN model. Generally speaking, ZNN model using hyperbolic sine <b>activation</b> <b>functions</b> may achieve better per-formance, as compared with those using other <b>activation</b> <b>functions.</b> In this paper, to pursue the superior convergence and robustness proper-ties, hyperbolic sine <b>activation</b> <b>functions</b> are applied to the ZNN model for online solution of time-varying matrix square roots. Theoretical analysis and computer-simulation results further demonstrate the superior perfor-mance of the ZNN model using hyperbolic sine <b>activation</b> <b>functions</b> {{in the context of}} large model-implementation errors, in comparison with that us-ing linear <b>activation</b> <b>functions...</b>|$|R
40|$|The {{choice of}} <b>activation</b> <b>functions</b> in deep {{networks}} {{has a significant}} effect on the training dynamics and task performance. Currently, the most successful and widely-used <b>activation</b> <b>function</b> is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new <b>activation</b> <b>functions.</b> Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel <b>activation</b> <b>functions.</b> We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered <b>activation</b> <b>function.</b> Our experiments show that the best discovered <b>activation</b> <b>function,</b> f(x) = x ·sigmoid(β x), which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top- 1 classification accuracy on ImageNet by 0. 9 % for Mobile NASNet-A and 0. 6 % for Inception-ResNet-v 2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network. Comment: Updated version of "Swish: a Self-Gated <b>Activation</b> <b>Function...</b>|$|R
40|$|Texto completo. Acesso restrito. p. 6438 – 6446 The use {{of neural}} network models for time series {{forecasting}} has been motivated by experimental results that indicate high capacity for function approximation with good accuracy. Generally, these models use <b>activation</b> <b>functions</b> with fixed parameters. However, {{it is known}} that the choice of <b>activation</b> <b>function</b> strongly influences the complexity and neural network performance and that a limited number of <b>activation</b> <b>functions</b> has been used in general. We describe the use of an asymmetric <b>activation</b> <b>functions</b> family with free parameter for neural networks. We prove that the <b>activation</b> <b>functions</b> family defined, satisfies the requirements of the universal approximation theorem We present a methodology for global optimization of the <b>activation</b> <b>functions</b> family with free parameter and the connections between the processing units of the neural network. The main idea is to optimize, simultaneously, the weights and <b>activation</b> <b>function</b> used in a Multilayer Perceptron (MLP), through an approach that combines the advantages of simulated annealing, tabu search and a local learning algorithm. We have chosen two local learning algorithms: the backpropagation with momentum (BPM) and Levenberg–Marquardt (LM). The overall purpose is to improve performance in time series forecasting...|$|R
40|$|This paper {{studies the}} Lp {{approximation}} capabilities of sum-of-product (SOPNN) and sigma-pi-sigma (SPSNN) neural networks. It is {{proved that the}} set of functions that are generated by the SOPNN with its <b>activation</b> <b>function</b> in Lploc(R) is dense in Lp(K) for any compact set K ⊂ RN, {{if and only if}} the <b>activation</b> <b>function</b> is not a polynomial almost everywhere. It is also shown that if the <b>activation</b> <b>function</b> of the SPSNN is in L∞loc(R), then the functions generated by the SPSNN are dense in Lp(K) if and only if the <b>activation</b> <b>function</b> is not a constant (a. e.) ...|$|R
40|$|In {{this paper}} a new neural network architecture, {{based on an}} {{adaptive}} <b>activation</b> <b>function,</b> called generalized sigmoidal neural network (GSNN), is proposed. The <b>activation</b> <b>functions</b> are usually sigmoidal but other functions, also depending on some free parameters, have been studied and applied. Most approaches tend to use relatively simple functions (as adaptive sigmoids), primarily due to computational complexity and difficulties hardware realization. The proposed adaptive <b>activation</b> <b>function,</b> built as a piecewise approximation with suitable cubic splines, can have arbitrary shape and allows to reduce the overall size of the neural networks, trading connection complexity with <b>activation</b> <b>function</b> complexity...|$|R
40|$|Department of Electrical EngineeringIn this thesis, we {{investigate}} {{the performance of}} various <b>activation</b> <b>functions</b> of deep convolutional neural networks (DCNNs) and propose new <b>activation</b> <b>functions.</b> First, we propose twofold parametric ReLU. We observed that time complexity of S-shaped ReLU is relatively huge due to the computation of forward and backward-pass propagation. Thus we removed translation parameters of S-shaped ReLU and design twofold parametric ReLU. Second, inspired by just noticeable difference of the Weber's law, we reflect the property that subjective sensation {{is proportional to the}} logarithm of image intensity. We formulate an <b>activation</b> <b>function</b> by modifying the logarithm function which is used only on the first layer of DCNNs. Experimental results show that the performances of the proposed <b>activation</b> <b>functions</b> are better than that of the existing <b>activation</b> <b>functions.</b> clos...|$|R
40|$|In Artificial Intelligence {{classification}} is {{a process}} of identifying classes of a different entities on the basis information provided from the dataset. Extreme Learning Machine (ELM) is one of the efficient classifiers. ELM is formed by interconnected layers. Each layer has many nodes (neurons). The input layer communicates with hidden layer with random weight and produces output layer with the help of <b>activation</b> <b>function</b> (transfer <b>function).</b> <b>Activation</b> <b>functions</b> are non-linear <b>functions</b> and different <b>activation</b> <b>functions</b> may produce different output on same dataset. Not every <b>activation</b> <b>function</b> is suited for every type classification problem. This paper shows the variation of average test accuracy with various <b>activation</b> <b>functions.</b> Along with it also has been shown that how much performance varied due to selection of random bias parameter between input and hidden layer of ELM. General Term...|$|R
40|$|Common {{nonlinear}} <b>activation</b> <b>functions</b> used in {{neural networks}} can cause training difficulties {{due to the}} saturation behavior of the <b>activation</b> <b>function,</b> which may hide dependencies that are not visible to vanilla-SGD (using first order gradients only). Gating mechanisms that use softly saturating <b>activation</b> <b>functions</b> to emulate the discrete switching of digital logic circuits are good examples of this. We propose to exploit the injection of appropriate noise so that the gradients may flow easily, even if the noiseless application of the <b>activation</b> <b>function</b> would yield zero gradient. Large noise will dominate the noise-free gradient and allow stochastic gradient descent toexplore more. By adding noise only to the problematic parts of the <b>activation</b> <b>function,</b> we allow the optimization procedure to explore {{the boundary between the}} degenerate (saturating) and the well-behaved parts of the <b>activation</b> <b>function.</b> We also establish connections to simulated annealing, when the amount of noise is annealed down, making it easier to optimize hard objective functions. We find experimentally that replacing such saturating <b>activation</b> <b>functions</b> by noisy variants helps training in many contexts, yielding state-of-the-art or competitive results on different datasets and task, especially when training seems to be the most difficult, e. g., when curriculum learning is necessary to obtain good results...|$|R
5000|$|Approximates {{identity}} {{near the}} origin - When <b>activation</b> <b>functions</b> have this property, the neural network will learn efficiently when its weights are initialized with small random values. When the <b>activation</b> <b>function</b> does not approximate identity near the origin, special {{care must be}} used when initializing the weights. In the table below, <b>activation</b> <b>functions</b> where [...] and [...] and [...] is continuous at 0 are indicated as having this property.|$|R
40|$|Artificial Neural Network (ANN) is {{beginning}} {{little by little}} to replace the task of an expert, even with the ANN can be a tool to replace a doctor. One of kind of ANN is Backpropagation networks, this network {{can be used to}} training programs {{in order to be able}} to recognize whether it is pig or cow wave spectra. To determine the output in Backpropagation training required suitable <b>activation</b> <b>functions.</b> Therefore, in this research will be compared to some of the <b>activation</b> <b>function</b> that can be used in training. <b>Activation</b> <b>functions</b> will be tested with the ratio test to determine the interval convergence. After tested with the ratio test it was found that the <b>activation</b> <b>function</b> was the best <b>activation</b> <b>function</b> to use the Backpropagation network training, because it has a weight range that can meet the methods used in the determination of weights. When tested with the data, the <b>activation</b> <b>function</b> is able to recognize correctly all trial datas. Expected in future research to examine the weight that makes the interval training to achieve fast convergence and the error bit...|$|R
40|$|In the neuroevolution literature, {{research}} has primarily focused on evolving {{the number of}} nodes, connections, and weights in artificial neural networks. Few {{attempts have been made}} to evolve <b>activation</b> <b>functions.</b> Research in evolving <b>activation</b> <b>functions</b> has mainly focused on evolving function parameters, and developing heterogeneous networks by selecting from a fixed pool of <b>activation</b> <b>functions.</b> This paper introduces a novel technique for evolving heterogeneous artificial neural networks through combinatorially generating piecewise <b>activation</b> <b>functions</b> to enhance expressive power. I demonstrate this technique on NeuroEvolution of Augmenting Topologies using ArcTan and Sigmoid, and show that it outperforms the original algorithm on non-Markovian double pole balancing. This technique expands the landscape of unconventional <b>activation</b> <b>functions</b> by demonstrating that they are competitive with canonical choices, and introduces a purview for further exploration of automatic model selection for artificial neural networks...|$|R
40|$|This paper {{presents}} a digital VLSI {{implementation of a}} feed-forward neural network classifier based on the saturating linear <b>activation</b> <b>function.</b> The architecture consists of one-hidden layer performing the weighted sum followed by a saturating linear <b>activation</b> <b>function.</b> The hardware implementation of such a network {{presents a}} significant advantage in terms of circuit complexity as compared to a network based on a sigmoid <b>activation</b> <b>function,</b> but without compromising the classification performance. Simulation results on two benchmark problems show that feedforward neural networks with the saturating linearity perform as well as networks with the sigmoid <b>activation</b> <b>function.</b> The architecture can also handle variable precision resulting in a higher computational resources at lower precision...|$|R
30|$|Since the <b>activation</b> <b>functions</b> in [1] are bounded, {{while the}} <b>activation</b> <b>functions</b> in Example  1 are not bounded, hence the global {{exponential}} stability of periodic solutions for Example  1 cannot be verified by the result in [1].|$|R
40|$|In a {{previous}} work Pollack {{showed that a}} particular type of heterogeneous processor network is Turing universal. Siegelmann and Sontag (1991) showed the universality of homogeneous networks of first-order neurons having piecewise-linear <b>activation</b> <b>functions.</b> Their result was generalized by Kilian and Siegelmann (1996) to include various sigmoidal <b>activation</b> <b>functions.</b> Here we focus on a type of high-order neurons called switch-affine neurons, with piecewise-linear <b>activation</b> <b>functions,</b> and prove that nine such neurons suffice for simulatin...|$|R
40|$|In this study, <b>activation</b> <b>functions</b> of all {{layers of}} the {{multilayered}} feedforward neural network have been determined by using genetic algorithm. The main criteria that show {{the efficiency of the}} neural network is to approximate to the desired output with the same number nodes and connection weights. One of the important parameter to determine this performance is to choose a proper <b>activation</b> <b>function.</b> In the classical neural network designing, a network is designed by choosing one of the generally known <b>activation</b> <b>function.</b> In the presented study, a table has been generated for the <b>activation</b> <b>functions.</b> The ideal <b>activation</b> <b>function</b> for each node has been chosen from this table by using the genetic algorithm. Two dimensional regression problem clusters has been used to compare the performance of the classical static neural network and the genetic algorithm based neural network. Test results reveal that the proposed method has a high level approximation capacity...|$|R
40|$|The <b>activation</b> <b>function</b> used to {{transform}} the activation level of a unit (neuron) into an output signal. There {{are a number of}} common <b>activation</b> <b>functions</b> in use with artificial neural networks (ANN). The most common choice of <b>activation</b> <b>functions</b> for multi layered perceptron (MLP) is used as transfer functions in research and engineering. Among the reasons for this popularity are its boundedness in the unit interval, the function’s and its derivative’s fast computability, and a number of amenable mathematical properties in the realm of approximation theory. However, considering the huge variety of problem domains MLP is applied in, it is intriguing to suspect that specific problems call for single or a set of specific <b>activation</b> <b>functions.</b> The aim {{of this study is to}} analyze the performance of generalized MLP architectures which has back-propagation algorithm using various different <b>activation</b> <b>functions</b> for the neurons of hidden and output layers. Fo...|$|R
30|$|Throughout {{this paper}} we {{considered}} a linear <b>activation</b> <b>function.</b> What happens if a different <b>activation</b> <b>function</b> is chosen? Do we observe {{the same type}} of dynamics? In this appendix we briefly investigate two other activation functions: an exponential and a quadratic.|$|R
40|$|Objective: Implementation of {{multilayer}} {{neural network}} (MLNN) with sigmoid <b>activation</b> <b>function</b> for the diagnosis of hepatitis disease. Methods: Artificial neural networks (ANNs) are efficient tools currently in common use for medical diagnosis. In hardware based architectures <b>activation</b> <b>functions</b> {{play an important role}} in ANN behavior. Sigmoid function is the most frequently used <b>activation</b> <b>function</b> because of its smooth response. Thus, sigmoid function and its close approximations were implemented as <b>activation</b> <b>function.</b> The dataset is taken from the UCI machine learning database. Results: For the diagnosis of hepatitis disease, MLNN structure was implemented and Levenberg Morquardt (LM) algorithm was used for learning. Our method of classifying hepatitis disease produced an accuracy of 91. 9...|$|R
40|$|Network size {{of neural}} {{networks}} is {{highly dependent on}} <b>activation</b> <b>functions.</b> A trainable <b>activation</b> <b>function</b> has been proposed, which consists of a linear combination of some basic <b>functions.</b> The <b>activation</b> <b>functions</b> and the connection weights are simultaneously trained. An 8 bit parity problem can be solved by using a single output unit and no hidden unit. In this paper, we expand this model to multilayer neural networks. Furthermore, nonlinear functions are used at the unit inputs in order to realize more flexible transfer <b>functions.</b> The previous <b>activation</b> <b>functions</b> and the new nonlinear functions are also simultaneously trained. More complex pattern classification problems can be solved with {{a small number of}} units and fast convergence...|$|R
40|$|This brief {{proposes a}} general {{framework}} of the nonlinear recurrent neural network for solving online the generalized linear matrix equation (GLME) with global convergence property. If the linear <b>activation</b> <b>function</b> is utilized, the neural state matrix of the nonlinear recurrent neural network can globally and exponentially converge to the unique theoretical solution of GLME. Additionally, {{as compared with the}} case of using the linear <b>activation</b> <b>function,</b> two specific types of nonlinear <b>activation</b> <b>functions</b> are proposed for the general nonlinear recurrent neural network model to achieve superior convergence. Illustrative examples are shown to demonstrate the efficacy of the general nonlinear recurrent neural network model and its superior convergence when activated by the aforementioned nonlinear <b>activation</b> <b>functions...</b>|$|R
