3|1598|Public
40|$|The use of Charles Ragin’s Qualitative Comparative Analysis (QCA) is {{increasing}} {{in the social}} sciences. However, some of its characteristics, especially those of its fuzzy set variant, are still not well understood by users. QCA, a <b>set</b> <b>theoretic</b> <b>method,</b> aims to describe, in a Boolean form, the configurations of conditions that are necessary and/or sufficient for some outcome. The calibration of set memberships is a central feature. We discuss how two alternative calibrations of a condition affect the assessment of consistency with sufficiency. Using first an abstract example and then an empirical one from the sociology of education, we explain why “stricter” calibration of conditions results in higher consistency with sufficiency. We demonstrate that conventional truth table analysis is not an ideal way to compare the analytic consequences of alternative calibrations and therefore employ an alternative which allows a more direct comparison of consistency indices while keeping comparative configurational contexts intact...|$|E
40|$|Validation {{is one of}} {{the most}} {{important}} steps in developing a reliable simulation model. It evaluates whether or not the model forms a representation of the simulated system accurate enough to satisfy the goals of the modelling study. The methods that are currently available for model validation are binary in nature in the sense that they only allow either to accept or reject the validity of the model. In this paper, we develop a new, fuzzy <b>set</b> <b>theoretic</b> <b>method</b> that allows to express degrees of model validity and that is hence continuous in nature. The method employs a fuzzy-neural machine learning algorithm and makes use of a new concept in fuzzy set theory known as resemblance relations. By a computational experiment, we demonstrate how our method can be used to discriminate more from less valid simulation models of a particular manufacturing process. status: publishe...|$|E
40|$|A {{recorded}} signal frequently {{results from}} the mixture of many signals from several classifiable sources. Knowledge of {{the contribution of the}} underlying sources to the recorded signal is valuable in several applications, such as remote sensing. Such mixtures may be analyzed using finite mixture models. Historically, finite mixture models decompose a density as the sum of a finite number of component densities. Current methods for estimating the contribution of each component assume a parametric form for the mixture components. Furthermore, these methods assume a collection of samples from the mixture are observed rather than an aggregate representation of the samples, such as a histogram. This work introduces a method to address the many practical cases where parametric mixture models are insufficient to describe the mixture components. The observed mixture is assumed to occur in an aggregate representation of samples. Thus, the mixture components are represented as finite-length signals or vectors. The proposed method incorporates the first and second order statistics of the mixture components obtained from previously collected samples of the mixture components. The new method is based on the <b>set</b> <b>theoretic</b> <b>method</b> of successive projections onto convex sets (POCS). The set theoretic approach defines a set of feasible solutions as the intersection of sets consistent with the prior knowledge of a desirable solution. POCS is an iterative procedure used to find a point in the set of feasible solutions. This work considers several sets describing the finite mixture model, including a new model set generalizing a set based on the error-in-variables model. To illustrate the viability of the new method, comparisons are made with the expectation-maximization (EM) algorithm for mixtures with parametric components. Simulations of mixture with nonparametric components emphasize the advantages of the new method, since no other methods address mixtures with nonparametric components. The new method is applied to the problem of resolving hyperspectral data representing the mixture of several component spectra...|$|E
30|$|Motivated by {{the above}} {{mentioned}} works, we establish a vectorial form of Ekeland-type variational principle for multivalued bioperator whose domain {{is a complete}} metric space and its range is a subset of a locally convex Hausdorff topological space by using the <b>set</b> <b>theoretic</b> <b>methods.</b> We also consider Caristi-Kirk fixed point theorem in a more general setting and our techniques allow us to improve and to extend their results in [2, 6, 7].|$|R
30|$|Recent {{advances}} in multiple target tracking [38, 39] {{have resulted in}} random <b>set</b> <b>theoretic</b> <b>methods</b> [40] and in [41], an instance of such methods, namely a cardinalized probability hypothesis density (CPHD) filter [42] was presented for multiple ground target tracking. An example, with two groups of targets with four single targets in each group, is given. Track extraction is shown to be faster if the road information is used with the same road network model and observation model (GMTI) as in [31].|$|R
40|$|Abstract. Methods {{originating}} in theoreticar computer science for shoiving that certain decision probrems are Np-complete {{have also been}} used to show that certain compactness theorems are equivalent in ZF set theory to the Boolean Prime ldear rheorem (BpI). conversely, {{there is some evidence}} that <b>set</b> <b>theoretic</b> <b>methods</b> connnected with research on BPI may prove useful in computer science. we survey what is known and then look at some new exampres and exprore the underlying reasons for the successful application of quite similar methods to soive&quot;different problems. ...|$|R
40|$|This paper {{analyzes}} the robust control correction synthesis problem for constrained discrete-time control systems. The robust control correction synthesis problem is first introduced and discussed {{in a general}} non-linear compact setting under two interpretations on the uncertainty leading to inf-sup and sup-inf control correction syntheses. The solution of the corresponding robust control correction synthesis problem is obtained by utilizing <b>set</b> <b>theoretic</b> <b>methods.</b> In addition, motivated by numerical considerations, this paper also discusses the problem formulated in the linear convex compact setting and comments on plausible computational procedures...|$|R
40|$|This paper {{analyzes}} the control correction synthesis problem, introduced and discussed {{in a general}} non-linear compact setting, for constrained discrete-time control systems. In addition, motivated by computational considerations and a practical application, this paper examines, in more detail, the problem formulated in linear convex compact and linear piecewise convex compact settings. The solution of the corresponding control correction synthesis problem is obtained by utilizing <b>set</b> <b>theoretic</b> <b>methods,</b> in particular, by employing set invariance concepts. The control correction strategy is illustrated by an example which represents a simplified version of a practical case study...|$|R
40|$|International audienceIn this study, <b>set</b> <b>theoretic</b> <b>methods</b> {{are used}} to design a {{fault-tolerant}} scheme for a multisensor control application. The basic principle is {{the separation of the}} invariant sets for the estimations of the state and tracking error under healthy and faulty functioning. The fault scenario assumes abrupt changes of the observation equations. The main contribution of this paper is the introduction of controlled invariant sets in the fault detection mechanism. The control action is chosen in order to guarantee the closed-loop positive invariance of a candidate region when the exogenous signals (additive disturbances, noise and reference/set-points) are bounded...|$|R
40|$|This paper, {{whose purpose}} is both {{substantive}} and methodological, focuses on changes over a nine year period, drawing {{on data from}} two British birth cohorts (individuals born in 1958 and 1970), and, substantively, employs <b>set</b> <b>theoretic</b> <b>methods</b> to explore {{the extent to which}} an upward shift in qualifications achieved led to any reduction in the roles class and gender played in the achievement of professional, managerial and technical (PMT) social class destinations in early adulthood. Our methodological purpose is to illustrate how a counterfactual modelling approach can be used together with Ragin's <b>set</b> <b>theoretic</b> <b>methods</b> to provide an alternative way of analysing relationships in this area. We draw on earlier work exploring the extent to which educational achievement was 'meritocratic' with respect to ability for these cohorts (Author 1, 2005, 2006). Our configurational account of the causal pathways to various class destinations is set against the background of a simple model of 'meritocracy' (allocation to available class positions by qualifications alone taking account of the empirical marginal distributions). This model allows us to specify, counterfactually, what qualifications would have represented necessary and sufficient conditions in our modelled meritocracy for reaching the PMT class. By comparison of these conditions with the empirically derived necessary and sufficient conditions for achieving these outcomes (using Ragin et al's fs/QCA software) we show that while allocation processes were far from meritocratic in both cohorts, there were some changes in the way both class and gender combined with qualifications as conditions for destinations. We also show that Ragin's configurational methods, focussing on holistically-conceived cases and conjunctural causation rather than on the net effects of independent variables, provide a useful analytic technique for capturing relationships in this field. Boolean Methods, QCA, Social Class, Gender, Education, Meritocracy, Counterfactual Models...|$|R
40|$|Image {{understanding}} applications must {{be capable}} of integrating uncertain information {{from a variety of}} sources. Fuzzy set theory is especially suited to provide methods to deal with and to fuse uncertain and ambiguous information arising in computer vision. We introduce a general framework, called `active fusion', that actively selects and combines information in order to arrive at a reliable result at reasonable costs. An active fusion module is designed followed by an outline of how to implement such a framework using fuzzy <b>set</b> <b>theoretic</b> <b>methods.</b> The realization of a fusion / control unit of such an active fusion module for the efficient control of complicated processes in image understanding seems feasible. The presented framework will be implemented to carry out experiments in active object recognition. Keywords: image understanding, active information fusion, fuzzy sets. 1 Motivation Information fusion deals with the integration of information from several different sources. [...] ...|$|R
40|$|Ragin’s Qualitative Comparative Analysis (QCA) {{and related}} <b>set</b> <b>theoretic</b> <b>methods</b> are {{increasingly}} popular. This {{is a welcome}} development, since it encourages systematic configurational analyses of social phenomena. One downside of this growth in popularity is a tendency for more researchers to use the approach in a formulaic manner—something made possible, and more likely, by the availability of free software. We wish to see QCA employed, as Ragin intended, in a self-critical manner. For this to happen, researchers need to understand more {{of what is going}} on behind the results generated by the available software packages. One important aspect of <b>set</b> <b>theoretic</b> analyses of sufficiency and necessity is the effect that the distribution of cases in a dataset can have on results. We explore this issue in a number of ways. We begin by exploring how both deterministic and nondeterministic data-generating processes are reflected in the analyses of populations differing in only the weights of types of cases. We show how and why weights matter in causal analyses that focus on necessity and also, where models are not fully specified, sufficiency. We then draw on this discussion to show that a recent textbook discussion of hidden necessary conditions is weakened as a result of its neglect of weighting issues. Finally, having shown that case weights raise a number of difficulties for <b>set</b> <b>theoretic</b> analyses, we offer suggestions, drawing on two imagined population datasets concerning health outcomes, for mitigating their effect...|$|R
40|$|The {{context for}} {{this paper is}} the ongoing debate {{concerning}} the relative merits, {{for the analysis of}} quantitative data, of, on the one hand, variable-analytic correlational methods, and, on the other, the case-based <b>set</b> <b>theoretic</b> <b>methods</b> developed by Charles Ragin. While correlational approaches, based in linear algebra, typically use regression to establish the net effects of several “independent” variables on an outcome, the <b>set</b> <b>theoretic</b> approach analyses, more holistically, the conjunctions of factors sufficient and/or necessary for an outcome to occur. Here, in order to bring out key differences between the approaches, we focus our attention on the basic building blocks of the two approaches: respectively, the concept of linear correlation and the concept of a sufficient and/or necessary condition. We initially use invented data (for ability, educational achievement, and social class) to simulate what is at stake in this methodological debate and we then employ data taken from the British National Child Development Study to explore the structuring of the relationship between respondents‟ early measured ability and later educational achievement across various configurations of parental and grandparental class origin and sex. The substantive idea informing the analysis, derived from Boudon‟s work, is that, for respondents from higher class origins, ability will tend to be sufficient but not necessary for later educational achievement while, for lower class respondents, ability will tend to be necessary but not sufficient. We compare correlational analyses, controlling for class and gender, with fuzzy set analyses to show that <b>set</b> <b>theoretic</b> indices can better capture these varying relationships than correlational measures. In conclusion, we briefly consider how our demonstration of some of the advantages of the <b>set</b> <b>theoretic</b> approach for modelling empirical relationships might be related to the debate concerning the relation between observed regularities and causal mechanisms...|$|R
40|$|Corporate {{governance}} in transition economies {{does not fit}} in the dominant normative models. China embodies institutional tensions between an inherited system of political governance and new laws transplanted from Western countries that empower external shareholders on capital markets. The two empirical studies in this dissertation apply <b>set</b> <b>theoretic</b> <b>methods</b> on large samples of Chinese listed firms to uncover the causal complexity involved in corporate governance problems, focusing on the complementarities, functional equivalence and causal asymmetry. The first study analyses the configurations that facilitate and deter the most salient governance problem: the diversion of cash flow from the firm by controlling shareholders through tunneling. The second study analyses the diversity of governance forms in successful firms, including politically embedded firms, those that rely on outsider control systems resembling the Anglo-American model, as well as creative hybrid forms. The dissertation shows that, even in transition economies, property rights matter for allocating decision making rights between large and minority shareholders. Also, political connections matter, {{but not as much}} as is commonly assumed since many private firms operate profitably without any political ties. Finally, the thesis commands caution regarding the role of independent directors, who not only fail to provide effective monitoring on insiders, but often facilitate collusion easing tunneling behavior...|$|R
40|$|Dedicated to {{the memory}} of my friend and {{colleague}} Stelios Pichorides Problems concerning the uniqueness of an expansion of a function in a trigonometric series have a long and fascinating history, starting back in the 19 th Century with the work of Riemann, Heine and Cantor. The origins of set theory are closely connected with this subject, as it was Cantor’s research into the nature of exceptional sets for such uniqueness problems that led him to the creation of set theory. And the earliest application of one of Cantor’s fundamental concepts, that of ordinal numbers and transfinite induction, can be glimpsed in his last work on this subject. The {{purpose of this paper is}} to give a basic introduction to the application of <b>set</b> <b>theoretic</b> <b>methods</b> to problems concerning uniqueness for trigonometric series. It is written in the style of informal lecture notes for a course or seminar on this subject and, in particular, contains several exercises. The treatment is as elementary as possible and only assumes some familiarity with the most basic results of general topology, measure theory, functional analysis, and descriptive set theory. Standard references to facts that are used without proof are given in the appropriate places...|$|R
40|$|There are 5 {{formulas}} in {{the language}} of the Turing degrees, D, with, _ and ^, that define the relations x 00 y 00, x 00 = y 00 and so x 2 L 2 (y) = fx yjx 00 = y 00 g in any jump ideal containing 0 (!). There are also 6 & 6 and 8 formulas that de…ne the relations w = x 00 and w = x 0, respectively, in any such ideal I. In the language with just the quantifier complexity of each of these definitions increases by one. On the other hand, no 2 or 2 formula {{in the language}} with just de…nes L 2 or x 2 L 2 (y). Our arguments and constructions are purely degree theoretic without any appeals to absoluteness considerations, <b>set</b> <b>theoretic</b> <b>methods</b> or coding of models of arithmetic. As a corollary, we see that every automorphism of I is …xed on every degree above 0 00 and every relation on I that is invariant under double jump or joining with 0 00 is definable over I if and only if it is definable in second order arithmetic with set quantification ranging over sets whose degrees are in I. Similar direct coding arguments show that every hyperjump ideal I is rigid and biinterpretable with second order arithmetic with set quantification ranging over sets with hyperdegrees in I. Analogous results hold for various coarser degree structures...|$|R
40|$|Abstract The climate {{policies}} of developed states vary from small {{greenhouse gas emissions}} reduction targets to the formulation of highly ambitious, legally-binding objectives. As such, the research question of this thesis is ‘What explains variation amongst developed states’ climate policies?’ This thesis seeks to explain variation in climate policy ambition in the twenty-three developed states of the UNFCCC Annex II between 2006 and 2010. This investigation employs a nested design approach. It commences with a critical evaluation of the existing literature on environmental and climate policy to identify potential independent variables. Fuzzy set Qualitative Comparative Analysis then tests four hypotheses, {{in order to find}} the patterns that influence climate policy in the twenty-three states and select case studies. Austria, Finland, Germany and Sweden are selected as case studies, as their climate policies are not explained by the medium-n analysis. The four states also share very similar scores for each of the conditions being tested, but differ regarding the outcome. From here, semi-structured elite interviews with forty policy-makers and analyses of primary sources are employed as part of a small-n analysis on the four case studies. The concept of ‘path dependence’ is employed to facilitate an understanding of the long-term processes involved in climate policy development. Three main arguments are made in this thesis. Firstly, the combination of left-wing government and membership of the European Union is sufficient to result in ambitious climate policy, while non-membership of the European Union is sufficient to result in ‘not ambitious’ climate policy. Secondly, states which developed renewable electricity policies according to the principles of Ecological Modernisation formulated pioneering climate policy. Thirdly, states that produced nuclear power, but also sought to phase out the energy source, formulated more ambitious climate policy than states that did not produce any nuclear power, or sought to expand the energy source. This thesis furthers the understanding of climate policy variation, adds to the burgeoning field of <b>set</b> <b>theoretic</b> <b>methods,</b> and provides more nuanced explanations of how Ecological Modernisation and nuclear energy can influence climate change policy. ...|$|R
40|$|Some {{remarks on}} the problem of {{knowledge}} representation and processing, as recognized in connection with the use of computers in the scientific research work, emphasizes the relevance of these problems for the studies on both the theory of languages and the expert system. A consideration of the common traits in the recent history of these studies, with reference to the use of computers on texts in natural language motivates the introduction of <b>set</b> <b>theoretic</b> and algebraic <b>methods,</b> suitable for applications in the analysis and in the automatic treatment of languages, based on the concept of model sets and on relational structures suggested from the connections between syntax and semantics evidenced in some example of sub-languages corresponding to theories of different classes of physical phenomena. Some details of these methods are evidenced, which have already successfully used or whose applications appears suggestive of interesting development...|$|R
40|$|Thesis (Ph. D.) [...] University of Washington, 2015 High Performance (HP) {{buildings}} are rapidly growing phenomena in Architecture Engineering and Construction industry, addressing many criteria affecting the buildings’ {{design and construction}} such as sustainability, functionality and cost-effectiveness. Responding to all these criteria, however, requires a transitioning from the traditional design process towards the whole system approach in which team members can effectively collaborate to analyze the tradeoffs between various interdependent systems and products, {{and be able to}} optimize the building as a whole. Construction, engineering and management (CEM) scholars have identified effective elements to facilitate such transition. Fostering Integrated Project Teams (IPT), and implementation of Building Information Modeling (BIM) are two of the fundamental elements presented. However, there is still a gap within the literature, in terms of contextualizing these elements, considering the causal complexities embedded in delivery of HP projects. Using Fuzzy sets Qualitative Comparative Analysis approach, this study presents a framework for analyzing interdependencies within contractual, organizational and social elements that foster IPT practices and BIM implementation within HP projects. The proposed framework is used to construct three major typologies of HP projects with superior reduction in their energy use: “information driven”, “process driven”, and “organizationally driven” projects. Comparison among the fundamental differences among the three typologies shows that formation of trust and approaches to learning and innovation has different drivers in each typology: information technology such as BIM, and inter-organizational scope understanding are the driving forces in Information Driven Projects. Process Driven Projects, however, depend on contractual settings and early involvement of the construction team, while Organizationally Driven Projects rely on an experienced architectural firm and their already established collaborative work practices. The study also found that in addition to the exclusive elements that facilitate design and construction of HP buildings in each typology, several elements are necessary to be present in all HP projects. Such elements include setting ambitious environmental goals, owner engagement in the design process, close working relationship among architects and engineers, and frequent inter-organizational meetings. The findings of this study provide a platform for CEM scholars to investigate complementarities among various contractual, organizational, and social elements facilitating design and construction of HP projects. In addition, Identifying similarities between their projects with the proposed typologies, practitioners will be able to better strategize and make informed decisions about incorporating new IPT and BIM work processes within their projects. Finally, this project can be served as an example for implementation of configurational <b>set</b> <b>theoretic</b> <b>methods</b> such as fsQCA within CEM domain, helping to bridge the sharp divide that currently exists between large N quantitative and small N qualitative studies on construction projects...|$|R
40|$|If (X,d) is a metric space {{then the}} map f X→ X is defined to be a weak {{contraction}} if d(f(x),f(y)) <d(x,y) for all x,y∈ X, x≠ y. We determine the simplest non-closed sets X⊆R^n {{in the sense}} of descriptive <b>set</b> <b>theoretic</b> complexity such that every weak contraction f X→ X is constant. In order to do so, we prove that there exists a non-closed F_σ set F⊆R such that every weak contraction f F→ F is constant. Similarly, there exists a non-closed G_δ set G⊆R such that every weak contraction f G→ G is constant. These answer questions of M. Elekes. We use measure <b>theoretic</b> <b>methods,</b> first of all the concept of generalized Hausdorff measure. Comment: 10 page...|$|R
40|$|This paper {{looks at}} the {{application}} of <b>set</b> <b>theoretic</b> estimation techniques to geophysical tomographic imaging. Conventional imaging methods {{try to find a}} solution that satisfies an objective function chosen by the user. <b>Set</b> <b>theoretic</b> estimation finds a solution that is consistent with the measured data and all prior knowledge. Several methods currently in use in geotomography (ART, SIRT and POCS) fit within the framework of <b>set</b> <b>theoretic</b> estimation. This paper looks at extending these <b>methods</b> using <b>set</b> <b>theoretic</b> approaches...|$|R
30|$|In [34], {{the proof}} {{relies on a}} {{stochastic}} averaging theory. Due to the <b>set</b> <b>theoretic</b> nature of the correlated equilibira, the convergence analysis is carried out through a differential inclusion, which is the <b>set</b> <b>theoretic</b> extension of a differential equation.|$|R
50|$|<b>Set</b> <b>theoretic</b> {{programming}} is a programming paradigm based on mathematical set theory. One {{example of a}} programming language based on this paradigm is SETL. The goal of <b>set</b> <b>theoretic</b> {{programming is}} to improve programmer speed and productivity significantly, and also enhance program clarity and readability.|$|R
40|$|Consensus {{theory is}} adopted {{as a means}} of {{classifying}} geographic data from multiple sources. The foundations and usefulness of different consensus <b>theoretic</b> <b>methods</b> are discussed in conjunction with pattern recognition. Weight selections for different data sources are considered and modeling of non-Gaussian data is investigated. The application of consensus theory in pattern recognition is tested on two data sets: 1) multisource remote sensing and geographic data and 2) very-high-dimensional remote sensing data. The results obtained using consensus <b>theoretic</b> <b>methods</b> are found to compare favorably with those obtained using well-known pattern recognition <b>methods.</b> The consensus <b>theoretic</b> <b>methods</b> can be applied in cases where the Gaussian maximum likelihood method cannot. Also, the consensus <b>theoretic</b> <b>methods</b> are computationally less demanding than the Gaussian maximum likelihood method and provide a means for weighting data sources differently...|$|R
5000|$|Gemstone started (as Servio Logic) {{to build}} a <b>set</b> <b>theoretic</b> model data base machine.|$|R
5000|$|... a {{question}} whose potential answers partition the relevant possibilities (in the <b>set</b> <b>theoretic</b> sense), ...|$|R
40|$|This {{article is}} {{concerned}} with reflection principles {{in the context of}} Cantor’s conception of the <b>set</b> <b>theoretic</b> universe. We argue that within a Cantorian conception of the <b>set</b> <b>theoretic</b> universe reflection principles can be formulated that confer intrinsic plausibility to strong axioms of infinity. How can I talk to you, I have no words [...] . Virgin Prunes, I am God...|$|R
5000|$|Forcing in {{recursion}} {{theory is}} {{a modification of}} Paul Cohen's original <b>set</b> <b>theoretic</b> technique of forcing {{to deal with the}} effective concerns in recursion theory. Conceptually the two techniques are quite similar, in both one attempts to build generic objects (intuitively objects that are somehow 'typical') by meeting dense sets. Also both techniques are elegantly described as a relation (customarily denoted [...] ) between 'conditions' and sentences. However, where <b>set</b> <b>theoretic</b> forcing is usually interested in creating objects that meet every dense set of conditions in the ground model, recursion theoretic forcing only aims to meet dense sets that are arithmetically or hyperarithmetically definable. Therefore, some of the more difficult machinery used in <b>set</b> <b>theoretic</b> forcing can be eliminated or substantially simplified when defining forcing in recursion theory. But while the machinery may be somewhat different recursion <b>theoretic</b> and <b>set</b> <b>theoretic</b> forcing are properly regarded as an application of the same technique to different classes of formulas.|$|R
40|$|Working in {{the context}} of Projective Determinacy (PD), we {{introduce}} and study in this paper a countable ∏^ 1 _(2 n+ 1) set of reals Q_(2 n+l) and an associated real y^ 0 _(2 n+l) for each n ≥ 0 (real means element of ω^ω in this paper). Our theory has analytical (descriptive <b>set</b> <b>theoretic)</b> as well as <b>set</b> <b>theoretic</b> aspects, strongly interrelated with each other...|$|R
40|$|Abstract. The aim of {{this paper}} is to {{initiate}} a study of the jet bundles on the grassmannian over a field of characteristic zero using higher direct images of G-linearized sheaves, Lie <b>theoretic</b> <b>methods,</b> enveloping algebra <b>theoretic</b> <b>methods</b> and generalized Verma modules. We also classify any jet bundle on an arbitrary homogeneous space in terms of representations of semi simple Lie algebras. Content...|$|R
40|$|For {{the family}} of graded lattice ideals of {{dimension}} 1, we establish a complete intersection criterion in algebraic and geometric terms. In positive characteristic, it is shown that all ideals of this family are binomial <b>set</b> <b>theoretic</b> complete intersections. In characteristic zero, we show that an arbitrary lattice ideal which is a binomial <b>set</b> <b>theoretic</b> complete intersection is a complete intersection. Comment: Internat. J. Algebra Comput., to appea...|$|R
40|$|In this correspondence, the {{restoration}} of a signal degraded by a stochastic impulse response is formulated as a problem with uncertainties in both the measurements and the impulse response. The method of total least squares, and variants thereof, are effective techniques for solving this class of problems. However, unlike <b>set</b> <b>theoretic</b> estimation schemes, these methods do not allow the incorporation of other a priori information in the estimate. In this correspondence, two new sets motivated by total least squares are introduced for <b>set</b> <b>theoretic</b> estimation. The convexity of these sets is established and the projection operators onto these sets are given. Through simulations, {{the advantages of the}} new technique over conventional and older <b>set</b> <b>theoretic</b> schemes for restoration are demonstrated...|$|R
5000|$|... the <b>set</b> <b>theoretic</b> Boolean operations: union K ∪ L, {{intersection}} K ∩ L, and complement , hence also relative complement K-L.|$|R
40|$|Abstract. The main {{objective}} {{of this paper is}} to highlight the role of the <b>set</b> <b>theoretic</b> analysis in the model predictive control synthesis. In particular, the <b>set</b> <b>theoretic</b> analysis is invoked to: (i) indicate the fragility of the model predictive control synthesis with respect to variations of the terminal constraint set and the terminal cost function and (ii) discuss a simple, tube based, robust model predictive control synthesis method for a class of nonlinear systems...|$|R
40|$|We {{study the}} problem of localizing a mobile robot with an onboard-device making angular {{measurements}} on the location of known but undistinguishable land-marks. Noiiel algorithms are proposed for I. efficient posture initialization based on a simple linear solution and for 2. recursive posture estimation. Derived in a <b>set</b> <b>theoretic</b> framework, the algorithms cope with nonwhite, nongaussian noise and deterministic errors. Experiments with the <b>set</b> <b>theoretic</b> posture estimator demonstrate its simplicity and effectiveness in real-world applications. ...|$|R
40|$|Abstract. Acquiring soil’s {{critical}} load {{is very important}} when designing soil foundation and soil subgrade with overloading method. Mohr-Coulomb strength theory is combined with slip-line field theory, then a <b>theoretic</b> <b>method</b> which is practicable to analyze {{critical load}} of soil come out. Analyzed theoretically and compared with others, the <b>theoretic</b> <b>method</b> is applied to describe the relationship between internal friction angel, internal cohesion, δ, lateral pressure and critical load, which express that this <b>theoretic</b> <b>method</b> is rational. Numerical analysis is adopted which {{make it clear that}} soil subgrade’s yield zone is small when it is under the critical pressure, which approve that this theory is very close to lower limit of soil’s critical load. The theoretical method can provide useful information to engineering designers and researchers...|$|R
