206|311|Public
5000|$|Journal of <b>Statistical</b> <b>Computation</b> and Simulation is {{indexed in}} the {{following}} services: ...|$|E
50|$|Al-Hussaini, E. and Osman, M. On {{median of}} finite mixture. Journal of <b>Statistical</b> <b>Computation</b> and Simulation 1997; 58: 121-142.|$|E
50|$|Journal of <b>Statistical</b> <b>Computation</b> and Simulation is a peer-reviewed {{scientific}} {{journal that}} publishes papers related to computational statistics. It {{is published by}} Taylor & Francis in English.|$|E
30|$|All <b>statistical</b> <b>computations</b> were {{calculated}} using the R statistical package, software version 3.3. 3.|$|R
30|$|All <b>statistical</b> <b>computations</b> were {{performed}} with SPSS software (Statistical Package for the Social Sciences, SPSS, Version 12, Chicago, IL, USA).|$|R
40|$|In {{this paper}} we show how Perl, an {{expressive}} and extensible high-level programming language, with network and ob ject-oriented programming support, {{can be used}} in processing data for statistics and statistical computing. The paper is organized in two parts. In Part I, we introduce the Perl programming language, with particular emphasis on the features that distinguish it from conventional languages. Then, using practical examples, we demonstrate how Perl's distinguishing features make it particularly well suited to perform labor intensive and sophisticated tasks ranging from the preparation of data to the writing of statistical reports. In Part II we show how Perl can be extended to perform <b>statistical</b> <b>computations</b> using modules and by "embedding" specialized statistical applications. We provide example on how Perl can be used to do simple statistical analyses, perform complex <b>statistical</b> <b>computations</b> involving matrix algebra and numerical optimization, and make <b>statistical</b> <b>computations</b> more easily reproducible. We also investigate the numerical and statistical reliability of various Perl statistical modules. Important computing issues such as ease of use, speed of calculation, and efficient memory usage, are also considered. ...|$|R
5000|$|Standard SQL {{interface}} with many analytics capabilities built-in, such as time series gap filling/interpolation, event-based windowing and sessionization, pattern matching, event series joins, <b>statistical</b> <b>computation</b> (e.g., regression analysis), and geospatial analysis.|$|E
50|$|Although used {{mainly by}} statisticians and other {{practitioners}} requiring an environment for <b>statistical</b> <b>computation</b> and software development, R can also {{operate as a}} general matrix calculation toolbox - with performance benchmarks comparable to GNU Octave or MATLAB. Arrays are stored in column-major order.|$|E
50|$|CNB {{announced}} that an error {{was found in}} the <b>statistical</b> <b>computation</b> of drug abuser arrest statistics for the period 2008 to 2010 after migrating to a new IT system. The amended and updated data reflects higher numbers of drug abusers for that period. CNB stressed that the mistake in the computation did not affect its enforcement efforts.|$|E
30|$|All {{mathematical}} and <b>statistical</b> <b>computations</b> {{were made}} using Excel 2007 (Microsoft Office) and OriginPro 8.5. 0 SR 1 (OriginLab Corporation, USA). Data {{were reported as}} mean ± SD. Student t test was used for comparison of the developed method with standard method.|$|R
40|$|In {{the last}} decade, open source {{programming}} technology {{is widely used}} among statisticians for developing a new statistical software and data analysis. This is R software environment and the main objective {{of this paper is}} to underline the importance of R for <b>statistical</b> <b>computations,</b> data analysis, visualization and applications in various fields. Regarding to this, the paper is primarily intended for people already familiar with common statistical concepts. Thus the statistical methods used to illustrate the R performance are not explained in detail. The main intention is to offer an overview to get started, to motivate beginners by illustrating the flexibility of R, and to show how simply it enables the user to carry out <b>statistical</b> <b>computations.</b> ...|$|R
30|$|In this study, three multivariate techniques, {{hierarchical}} {{cluster analysis}} (CA), discriminant analysis (DA) and principal component analysis/factor analysis (PCA/FA), {{were employed to}} analyze the temporal variations of the selected parameters. All mathematical and <b>statistical</b> <b>computations</b> were performed using Microsoft Excel and SPSS 19.0.|$|R
5000|$|Qualitative content {{analysis}} is [...] "a systematic, replicable technique for compressing many words of text into fewer content categories based on explicit rules of coding". It often involves building and applying a [...] "concept dictionary" [...] or fixed vocabulary of terms {{on the basis}} of which words are extracted from the textual data for concording or <b>statistical</b> <b>computation.</b>|$|E
5000|$|This {{applications}} allows {{visitors to}} keep track of their collectibles, books, records and DVDs. Users can share their collections. Recommendations can be generated based on user ratings, using <b>statistical</b> <b>computation</b> and network theory. Some sites offer a buddy system, as well as virtual [...] "check outs" [...] of items for borrowing among friends. Folksonomy or tagging is implemented on most of these sites.|$|E
50|$|More {{sophisticated}} numeracy skills include {{understanding of}} ratio concepts (notably fractions, proportions, percentages, and probabilities), and knowing {{when and how}} to perform multistep operations. Two categories of skills are included at the higher levels: the analytical skills (the ability to understand numerical information, such as required to interpret graphs and charts) and the statistical skills (the ability to apply higher probabilistic and <b>statistical</b> <b>computation,</b> such as conditional probabilities).|$|E
40|$|In this paper, a new {{graduate}} program in higher education, known as Computing Systems Engineering (CSE), for universities are proposed. This programconsists of several {{theoretical and practical}} courses to improve the skills of Bsc-graduates in Mathematics, Statistics, Industrial Engineering, Electrical Engineering, Computer Engineering, Information Technology Engineering as well as Financial Engineering, Economic, Accounting and other related subjects. The mission of CSE program in to enhance knowledge and capabilities of Bsc-Graduate mentioned for development, analysis, implementation, management and evaluation of processes, <b>statistical</b> methods, <b>computation</b> and processing of computing systems in organizations. In this paper, three branches for CSE program, including <b>Statistical</b> <b>Computations,</b> Mathematical Computations and Information Systems are designed and 32 academic-units for each one are suggested. Keywords: Master Program, System Engineering, Computer, Mathematics, Statistic...|$|R
30|$|Then {{the daily}} SOC and {{arriving}} moment information of each PrEV can be obtained. Centralizing the information, and taking 5  min as step, <b>statistical</b> <b>computations</b> of the PrEVs plug-in and plug-off are dynamically performed. Finally, combined with corresponding charging power and scenarios, the total charging load of the PrEV cluster is obtained.|$|R
30|$|Variables are {{expressed}} as mean ± 1 SD. Proportions were compared using the Fisher exact test for independent samples and the McNemar chi-square test for matched pairs. All statistical tests were two sided at a 5 % level of significance. All <b>statistical</b> <b>computations</b> were performed using SAS software version 9.2 (SAS Institute Inc, Cary, NC, USA).|$|R
50|$|Zaman's recent {{research}} {{has been in the}} field of pseudo-random number generation which is now widely used in modern computing needs. He has also published papers on generalisations of Markov chains, and on using computers to solve various theoretical problems in mathematics and statistics. His publications have been in various journals including: The Annals of Probability, Mathematics of Computation, Journal of Applied Probability, and Journal of <b>Statistical</b> <b>Computation.</b>|$|E
50|$|In 1964 McFadden {{joined the}} faculty of UC Berkeley, {{focusing}} his research on choice behavior {{and the problem of}} linking economic theory and measurement. In 1974 he introduced Conditional logit analysis. In 1975 McFadden won the John Bates Clark Medal. In 1977 he moved to the Massachusetts Institute of Technology. In 1981 {{he was elected to the}} National Academy of Sciences. He returned to Berkeley in 1991, founding the Econometrics Laboratory, which is devoted to <b>statistical</b> <b>computation</b> for economics applications. He remains its director. He is a trustee of the Economists for Peace and Security. In 2000 he won the Erwin Plein Nemmers Prize in Economics.|$|E
50|$|Anastasi mostly applied {{existing}} {{methods to}} {{individual and group}} ability testing, as well as self-report inventories and measuring interests and attitudes. She followed the methodological principles of norms, reliability, validity, and item analysis. The essay “Psychological Testing: Basic Concepts and Common Misconceptions,” encapsulates Anastasi’s methodological positions. Anastasi stressed that, in order to evaluate any psychometric test, the tester must be knowledgeable of the main features of the tests, particularly as they apply to norms, validity, and reliability. Her approach to standard scores and standard deviation was one in which she believed that understanding statistical concepts was essential to understanding the meaning of <b>statistical</b> <b>computation.</b>|$|E
3000|$|All {{programs}} used in {{this study}} were developed using C++ gcc 4.5 (copyright 2010 Free Software Foundation) on a Linux platform (Ubuntu, Canonical USA Inc., Lexington, MA, USA), and all <b>statistical</b> <b>computations</b> and user interface to access segmentation, registration, and quantification algorithms were processed in Matlab (copyright 2010 MathWorks Inc., Natick, MA, USA). All the programs were executed on an Intel [...]...|$|R
40|$|Wilkinson Tests are {{entry-level}} {{tests for}} assessing the numerical accuracy of <b>statistical</b> <b>computations</b> that have been applied to statistical software packages. Some software developers whose products have failed these tests have corrected deficiencies in subsequent versions. Thus, these tests have had a meliorative impact {{on the state of}} statistical software. These same tests are applied to several econometrics packages. Many deficiencies are noted. ...|$|R
40|$|According to a dual-mechanism hypothesis, {{although}} <b>statistical</b> <b>computations</b> {{based on}} nonadjacent transitional probabilities may suffice for speech segmentation, an additional rule-following mechanism {{is required in}} order to extract structural information out of the linguistic stream. We present a neural network study that shows how statistics alone can support the discovery of structural regularities, beyond the segmentation of speech, disconfirming the dual-mechanism hypothesis...|$|R
5000|$|Adaptive QoS routing is a cross-layer {{optimization}} {{adaptive routing}} mechanism. The cross-layer mechanism provides up-to-date local QoS {{information for the}} adaptive routing algorithm, by considering the impacts of node mobility and lower-layer link performance. The multiple QoS requirements are satisfied by adaptively using forward error correction and multipath routing mechanisms, based on the current network status. The complete routing mechanism includes three parts: (1) a modified dynamic source routing algorithm that handles route discovery and the collection of QoS related parameters; (2) a local <b>statistical</b> <b>computation</b> and link monitoring function located in each node; and (3) an integrateddecision-making system to calculate the number of routing paths, coding parity length, and traffic distribution rates.|$|E
5000|$|Jeffrey Seth [...] "Jeff" [...] Rosenthal, FRSC, FIMS (born October 13, 1967, Scarborough, Ontario) is {{an award-winning}} Canadian {{statistician}} and author. He {{is a professor}} in the University of Toronto's Department of Statistics, cross-appointed with Department of Mathematics. He has written numerous research papers about the theory of Markov chain Monte Carlo and other <b>statistical</b> <b>computation</b> algorithms, many joint with Gareth O. Roberts. He received the CRM-SSC Prize in 2006, the COPSS Presidents' Award in 2007, the Statistical Society of Canada Gold Medal in 2013, and a Faculty of Arts & Science Outstanding Teaching Award in 1998. He was elected a Fellow of the Institute of Mathematical Statistics in 2005, and of the Royal Society of Canada in 2012.|$|E
50|$|Programming with Big Data in R (pbdR) is {{a series}} of R {{packages}} and an environment for statistical computing with Big Data by using high-performance <b>statistical</b> <b>computation.</b> The pbdR uses the same programming language as R with S3/S4 classes and methods which is used among statisticians and data miners for developing statistical software. The significant difference between pbdR and R code is that pbdR mainly focuses on distributed memory systems, where data are distributed across several processors and analyzed in a batch mode, while communications between processors are based on MPI that is easily used in large high-performance computing (HPC) systems. R system mainly focuses on single multi-core machines for data analysis via an interactive mode such as GUI interface.|$|E
40|$|The Wilkinson Tests, {{entry-level}} {{tests for}} assessing the numerical accuracy of <b>statistical</b> <b>computations,</b> have been applied to statistical software packages. Some software developers, having failed these tests, have corrected deficiencies in subsequent versions. Thus these tests have had a meliorative impact {{on the state of}} statistical software. These same tests are applied to several econometrics packages. Many deficiencies are noted...|$|R
40|$|According to Peña et al. (2002), <b>statistical</b> <b>computations</b> {{based on}} nonadjacent {{transitional}} probabilities {{of the sort}} that are exploited in speech segmentation cannot be used in order to induce existing grammatical regularities in the speech stream. In their view, statistics are insufficient to support the discovery of the underlying grammatical regularities. In this note I argue that a single statistical mechanism can account for the data Peña et al. report...|$|R
40|$|SAS/STAT 9 {{delivers}} {{a wealth of}} tools and function-ality for statistical modeling and data analysis. Areas covered by new or enhanced software include mul-tiple imputation, conditional logistic regression, ro-bust regression, general linear models for propor-tional hazards, regression diagnostics, survey data analysis, power and sample size analysis, <b>statistical</b> distance <b>computations,</b> <b>statistical</b> graphics, and par-allelization...|$|R
50|$|To {{implement}} {{an adaptive}} multipath routing scheme, three functions distributed {{in different parts}} of the network are needed. First, a modified dynamic source routing function is needed. It handles route discovery and collecting the local QoS-related information along the selectedroutes. Second, there is a local <b>statistical</b> <b>computation</b> and link monitoring function located in each node. This function is used to support the above routing function. It will manageand build the local routing information in each node, which includes a QoS-related table. The third function will be in charge of the final decision-making process. The adaptiverouting parameters are derived from the decision-making algorithm based on the QoS constraints. They are the number N of selected paths, parity length k of the FEC, code and the set {R} of the traffic distribution rates on each path. With these functions, adaptive multipath QoS routing is implemented.|$|E
5000|$|All {{academics}} in the UK {{could apply}} for a free account on the NES. The science and research ranged from the permeation of drugs through a membrane {{to the welfare of}} ethnic minority groups in the United Kingdom. Other applications included modelling the coastal oceans, modelling of HIV mutations, research into cameras for imaging patients during cancer treatments and simulating galaxy formation. As an example use of NES, [...] "The motivation, methodology and implementation of an e-Social Science pilot demonstrator project entitled: Grid Enabled Micro-econometric Data Analysis (GEMEDA). This used the NES to investigate a policy relevant social science issue: the welfare of ethnic minority groups in the United Kingdom. The underlying problem is that of a statistical analysis that uses quantitative data from more than one source. The application of grid technology to this problem allows one to integrate elements of the required empirical modelling process: data extraction, data transfer, <b>statistical</b> <b>computation</b> and results presentation, {{in a manner that is}} transparent to a casual user".|$|E
5000|$|By the 18th century, {{the term}} [...] "statistics" [...] {{designated}} the systematic collection of demographic and economic data by states. For {{at least two}} millennia, these data were mainly tabulations of human and material resources that might be taxed or put to military use. In the early 19th century, collection intensified, {{and the meaning of}} [...] "statistics" [...] broadened to include the discipline concerned with the collection, summary, and analysis of data. Today, data is collected and statistics are computed and widely distributed in government, business, most of the sciences and sports, and even for many pastimes. Electronic computers have expedited more elaborate <b>statistical</b> <b>computation</b> even as they have facilitated the collection and aggregation of data. A single data analyst may have available a set of data-files with millions of records, each with dozens or hundreds of separate measurements. These were collected over time from computer activity (for example, a stock exchange) or from computerized sensors, point-of-sale registers, and so on. Computers then produce simple, accurate summaries, and allow more tedious analyses, such as those that require inverting a large matrix or perform hundreds of steps of iteration, that would never be attempted by hand. Faster computing has allowed statisticians to develop [...] "computer-intensive" [...] methods which may look at all permutations, or use randomization to look at 10,000 permutations of a problem, to estimate answers that are not easy to quantify by theory alone..|$|E
40|$|Opsis is a hydrologic {{computer}} application that performs database administration, data visualisation and hydrologic and <b>statistical</b> <b>computations.</b> It is developed in an object-oriented environment {{with a wide}} use of graphical tools. It establishes a representation of hydrologic data series as objects with standard characteristics, and a protocol that hydrologic procedures should follow when acting on objects. It is connected to a large hydrologic database and {{has been used by}} several Greek organisations...|$|R
40|$|In {{this paper}} we discuss a general {{approach}} to design and implementation of <b>statistical</b> <b>computations.</b> We use three previous articles as examples to illustrate difficulties which arise in this kind of application and methods which may be used to solve them. A common theme in these articles is univariate and multivariate generalised Pareto distributions. However, the discussed problems are of a rather general nature and demonstrate some typical tasks in applied statistical research...|$|R
40|$|We {{describe}} output perturbation {{techniques that}} allow for a provable, rigorous sense of individual privacy. Examples where the techniques are effective span from basic <b>statistical</b> <b>computations</b> to sophisticated machine learning algorithms. Keywords: Private query processing, output perturbation. Rapidly increasing volumes of sensitive individual information are maintained by governments, statistical agencies and private enterprises, the latter making them increasingly ubiquitous as electronic collection and archiving evolves. The potential social benefits from analyzing these databases are...|$|R
