41|0|Public
50|$|With Gail Carpenter, Grossberg {{developed}} the {{adaptive resonance theory}} (ART). ART is a cognitive and neural theory of how the brain can quickly learn, and stably remember and recognize, objects and events in a changing world. ART proposed a solution of the <b>stability-plasticity</b> dilemma; namely, how a brain or machine can learn quickly about new objects and events without just as quickly being forced to forget previously learned, but still useful, memories. ART predicts how learned top-down expectations focus attention on expected combinations of features, leading to a synchronous resonance that can drive fast learning. ART also predicts how large enough mismatches between bottom-up feature patterns and top-down expectations can drive a memory search, or hypothesis testing, for recognition categories with which to better learn to classify the world. ART thus defines a type of self-organizing production system. ART was practically demonstrated through the ART family of classifiers (e.g., ART 1, ART 2, ART 2A, ART 3, ARTMAP, fuzzy ARTMAP, ART eMAP, distributed ARTMAP), developed with Gail Carpenter, which {{has been used in}} large-scale applications in engineering and technology where fast, yet stable, incrementally learned classification and prediction are needed.|$|E
40|$|Abstract — Handling catastrophic forgetting is an {{interesting}} and challenging topic in modeling the memory mechanisms {{of the human brain}} using machine learning models. From a more general point of view, catastrophic forgetting reflects the <b>stability-plasticity</b> dilemma, {{which is one of the}} several dilemmastobeaddressedinlearningsystems:toretainthe stored memory while learning new information. Different to the existing approaches, we introduce a Pareto-optimality based multi-objective learning framework for alleviating catastrophic learning. Compared to the single-objective learning methods, multi-objective evolutionary learning with the help of pseudorehearsal is shown to be more promising in dealing with the <b>stability-plasticity</b> dilemma. I...|$|E
40|$|Handling catastrophic forgetting is an {{interesting}} and challenging topic in modeling the memory mechanisms {{of the human brain}} using machine learning models. From a more general point of view, catastrophic forgetting reflects the <b>stability-plasticity</b> dilemma, {{which is one of the}} several dilemmas to be addressed in learning systems: to retain the stored memory while learning new information. Different to the existing approaches, we introduce a Pareto-optimality based multi-objective learning framework for alleviating catastrophic learning. Compared to the single-objective learning methods, multi-objective evolutionary learning with the help of pseudorehearsal is shown to be more promising in dealing with the <b>stability-plasticity</b> dilemma. © 2006 IEEE...|$|E
40|$|Abstract. We {{present a}} {{category}} learning vector quantization (cLVQ) approach for incremental and life-long learning of multiple visual categories where {{we focus on}} approaching the <b>stability-plasticity</b> dilemma. To achieve the life-long learning ability an incremental learning vector quantization approach is combined with a category-specific feature selection method in a novel way to allow several metrical “views ” on the representation space for the same cLVQ nodes. ...|$|E
40|$|In this paper, the {{processing}} of sonar signals {{has been carried out}} using Minimal Resource Allocation Network (MRAN), Probabilistic Neural Network (PNN) and Fuzzy Artmap (FAM) in differentiation of commonly encountered features in indoor environments. The <b>stability-plasticity</b> behaviors of all three networks have been investigated. The experimental result shows that MRAN possesses lower network complexity but experiences higher plasticity in comparison to PNN and FAM. The study also shows that MRAN performance is superior in terms of on-line learning than PNN and FAM...|$|E
40|$|Abstract—We {{present an}} {{architecture}} for incremental online learning in high-dimensional feature spaces {{and apply it}} on a mobile robot. The model is based on learning vector quantization, approaching the <b>stability-plasticity</b> problem of incremental learn-ing by adaptive insertions of representative vectors. We employ a cost-function-based learning vector quantization approach and introduce a new insertion strategy optimizing a cost-function based on a subset of samples. We demonstrate this model within a real-time application for a mobile robot scenario, where we perform interactive real-time learning of visual categories. I...|$|E
40|$|In this paper, {{a hybrid}} model {{consisting}} of the fuzzy ARTMAP (FAM) neural network and the {{classification and regression tree}} (CART) is formulated. FAM is useful for tackling the <b>stability–plasticity</b> dilemma pertaining to data-based learning systems, while CART is useful for depicting its learned knowledge explicitly in a tree structure. By combining the benefits of both models, FAM–CART is capable of learning data samples stably and, at the same time, explaining its predictions with a set of decision rules. In other words, FAM–CART possesses two important properties of an intelligent system, i. e., learning in a stable manner (by overcoming the <b>stability–plasticity</b> dilemma) and extracting useful explanatory rules (by overcoming the opaqueness issue). To evaluate the usefulness of FAM–CART, six benchmark medical data sets from the UCI repository of machine learning and a real-world medical data classification problem are used for evaluation. For performance comparison, a number of performance metrics which include accuracy, specificity, sensitivity, and the area under the receiver operation characteristic curve are computed. The results are quantified with statistical indicators and compared with those reported in the literature. The outcomes positively indicate that FAM–CART is effective for undertaking data classification tasks. In addition to producing good results, it provides justifications of the predictions {{in the form of a}} decision tree so that domain users can easily understand the predictions, therefore making it a useful decision support tool...|$|E
40|$|Stochastic {{learning}} {{solves the}} <b>stability-plasticity</b> problem (Fusi et al., 2000 a) but raises new {{issues related to}} the generation of the proper noise driving the synaptic dynamics. Here we show that a simple, fully deterministic, spike-driven synaptic device can make use of the network generated vari- ability in the neuronal activity to drive the required stochastic mechanism. Randomness emerges naturally from the interaction of deterministic neu- rons, and no extra source of noise is needed. Learning and forgetting rates of the network can be easily controlled by changing the statistics of the spike trains without changing any inherent parameter of the synaptic dynamics. ...|$|E
40|$|This paper {{presents}} {{a system for}} long-term SLAM (simultaneous localization and mapping) by mobile service robots and its experimental evaluation in a real dynamic environment. To deal with the <b>stability-plasticity</b> dilemma (the trade-off between adaptation to new patterns and preservation of old patterns), the environment is represented at multiple timescales simultaneously (5 in our experiments). A sample-based representation is proposed, where older memories fade at different rates depending on the timescale, and robust statistics are used to interpret the samples. The dynamics of this representation are analysed in a five week experiment, measuring the relative influence of short- and long-term memories over time, and further demonstrating the robustness of the approach. ...|$|E
40|$|We {{propose a}} novel method for {{establishing}} correspon-dences on deformable objects for single-target object track-ing. The key ingredient is a dissimilarity measure be-tween correspondences {{that takes into}} account their geo-metric compatibility, allowing us to separate inlier corre-spondences from outliers. We employ both static correspon-dences from the initial appearance of the object as well as adaptive correspondences from the previous frame to ad-dress the <b>stability-plasticity</b> dilemma. The geometric dis-similarity measure enables us to also disambiguate key-points that are difficult to match. Based on these ideas we build a keypoint-based tracker that outputs rotated bound-ing boxes. We demonstrate in a rigorous empirical analysis that this tracker outperforms {{the state of the art}} on a dataset of 77 sequences. 1...|$|E
40|$|Abstract- We have {{previously}} introduced Learn++, an ensemble based incremental learning algorithm for acquiring new knowledge from data that later become available, even when such data introduce new classes. In this paper, we describe a modification to this algorithm, where the voting weights of the classifiers are updated dynamically {{based on the}} location of the test input in the feature space. The new algorithm provides improved performance, stronger immunity to catastrophic forgetting and finer balance to the <b>stability-plasticity</b> dilemma than its predecessor, particularly when new classes are introduced. The modified algorithm and its performance, as compared to Adaboost. M 1 and the original Learn++, on real and benchmark datasets are presented. 0 - 7803 - 9048 - 2 / 05 /$ 20. 00 © 2005 IEE...|$|E
40|$|Abstract—One of the {{greatest}} challenges of life-long learning architectures is how to efficiently and reliably cope with the <b>stability-plasticity</b> dilemma. We propose an extension of a flexible system combining a static offline classifier and an incremental online classifier that is well suited for life-long learning scenarios. The pre-trained offline classifier preserves ground knowledge that should be respected during training, while the online classifier enables learning of new or specific information encountered during use. The combination is realised by a dynamic classifier selection strategy based on confidences of both ingredients. We report exemplary results of this architecture for the case of learning vector quantization (LVQ) for several data sets, thereby including an extensive comparison to alternative state of the art algorithms for incremental learning such as incremental generalised LVQ and the support vector machine. I...|$|E
40|$|In this paper, the {{processing}} of sonar signals {{has been carried out}} using a Minimal Resource Allocation Network (MRAN) in identification of commonly encountered features in indoor environments. The <b>stability-plasticity</b> behaviors of the network have been investigated. From previous observations, the experimental results show that MRAN possesses lower network complexity but experiences higher plasticity, and is unstable. A novel approach is proposed to solve these problems in MRAN and has also been experimentally proven that the network generalizes faster at a lower number of neurons (nodes) compared to the original MRAN. This new approach has been applied as a preprocessing tool to equip the network with certain information about the data to be used in training the network later. With this initial 'guidance', the network predicts extremely well in both sequential and random learning...|$|E
40|$|We {{propose a}} {{hierarchical}} self-organizing neural network ("HiGS") with adaptive architecture and simple topological organization. This network combines features of Fritzke's Growing Cell Structures and traditional hierarchical clustering algorithms. The height and {{width of the}} tree structure depend on the user-specified level of error desired, and the weights in upper layers of the network do not change in later phases of the learning algorithm. Parameters such as node deletion rate are adaptively modified by the learning algorithm. 1. Introduction Connectionist learning systems often face the <b>stability-plasticity</b> dilemma [2]. Most unsupervised neural network learning algorithms are stable {{with respect to their}} topology and plastic with respect to weight vector adaptations; such is the case in Kohonen's topology-preserving self-organizing map (SOM) [4]. An exception is Fritzke's Growing Cell Structures (GCS) network [1], which is much more plastic in that nodes may be inserted [...] ...|$|E
40|$|In {{this work}} we propose a random field {{approach}} to unsupervised machine learning, classifier training and pattern classification. The proposed method treats each sample as a random field {{and attempts to}} assign an optimal cluster label to it so as to partition the samples into clusters without a priori knowledge {{about the number of}} clusters and the initial centroids. To start with, the algorithm assigns each sample a unique cluster label, making it a singleton cluster. Subsequently, to update the cluster label, the similarity between the sample in question and the samples in a voting pool and their labels are involved. The clusters progressively form without the user specifying their initial centroids, as interaction among the samples continues. Due to its flexibility and adaptability, the proposed algorithm can be easily adjusted for on-line learning and is able to cope with the <b>stability-plasticity</b> dilemma. ...|$|E
40|$|Abstract — Fundamental to {{the problem}} of lifelong machine {{learning}} is how to consolidate the knowledge of a learned task within a long-term memory structure (domain knowledge) without the loss of prior knowledge. We investigate the effect of curriculum, ie. the order in which tasks are learned, on the consolidation of task knowledge. Relevant background material on knowledge transfer and consolidation using multiple task learning (MTL) neural networks is reviewed. A large MTL network is used as the long-term memory structure and task rehearsal overcomes the <b>stability-plasticity</b> problem and the loss of prior knowledge. Experimental results demonstrate that curriculum has an important effect on the accuracy of consolidated knowledge particularly for the first few tasks that are learned. The results also suggest that, for given set of tasks and training examples, the mean accuracy of consolidated domain knowledge converges to the same level regardless of the curriculum. I...|$|E
40|$|This paper {{presents}} a neural network based artificial vision system able to analyse {{the image of}} a car given by a camera, locate the registration plate and recognise the registration number of the car. The paper describes in detail various practical problems encountered in implementing this particular application and the solutions used to solve them. The main features of the system presented are: controlled <b>stability-plasticity</b> behaviour, controlled reliability threshold, both off-line and on-line learning, self assessment of the output reliability and high reliability based on high level multiple feedback. The system has been designed using a modular approach which allows easy upgrading and/or substituting of various sub-modules thus making it potentially suitable in a large range of vision applications. The OCR engine was designed as an interchangeable plug-in module. This allows the user to choose an OCR engine which is suited to the particular application and to upgrade it easil [...] ...|$|E
40|$|Abstract Stability and {{plasticity}} {{in learning}} systems are both equally essential, but achieving stability and plasticity simultaneously is difficult. Adaptive resonance theory (ART) neural networks {{are known for}} their plastic and stable learning of categories, hence providing an answer to the so called <b>stability-plasticity</b> dilemma. However, it has been demonstrated recently that contrary to general belief, ART stability is not possible with infinite streaming data. In this paper, we present an improved stabilization strategy for ART neural networks that does not suffer from this problem and that produces a soft-clustering solution as a positive side effect. Experimental results in a task of text clustering demonstrate that the new stabilization strategy works well, but with a slight loss in clustering quality compared to the traditional approach. For real-life intelligent applications in which infinite streaming data is generated, the stable and soft-clustering solution obtained with our approach more than outweighs the small loss in quality...|$|E
40|$|An {{autonomous}} robot {{need not be}} given {{all the details of}} the environment in which it is going to act: it can acquire them by direct interaction. One approach to learn by interaction is reinforcement learning, though, the robot has also to be able to autonomously categorize the input data it receives from the environment, deal with the <b>stability-plasticity</b> dilemma, and learn very rapidly. In this paper we present a digital artificial brain architecture capable of dealing with such problems. Furthermore, we present its use for controlling a mobile {{autonomous robot}} in an obstacle avoidance task in a real arena. Keywords. Artificial neural networks, mobile autonomous robots, neurocontrol. 1 Introduction Programming an autonomous robot so that it reliably acts in an unknown or a dynamic environment is a difficult thing to do. This is due to missing information during programming, the dynamic nature of the environment and the inherent noise in the robot's sensors and actuators [1]. One com [...] ...|$|E
40|$|Supervised Competitive Learning (SCL) assembles {{a set of}} {{learning}} modules into a supervised learning system to address the <b>stability-plasticity</b> dilemma. Each learning module acts as a similarity detector for a prototype, and includes prototype resetting (akin to that of ART) to respond to new prototypes. SCL has usually employed backpropagation networks as the learning modules. It has been tested with two feature abstractors: about 30 energy-based features, and a combination of energy-based and graphical features (about 60). About 75 subjects have been involved. In recent testing (15 college students), SCL recognized 99 % (energy features only) of test digits, 91 % (energy) and 96. 6 % (energy/graphical) of test letters, and 85 % of test gestures (energy/graphical). SCL has also been tested with fuzzy sets as learning modules for recognizing handwritten digits and handwritten gestures, recognizing 97 % of test digits, and 91 % of test gestures. 1 Assistant Professor, Department of Mathematic [...] ...|$|E
40|$|SCL assembles {{a set of}} {{learning}} modules into a supervised learning system to address the <b>stability-plasticity</b> dilemma. Each learning module acts as a similarity detector for a prototype, and includes prototype resetting (akin to that of ART) to respond to new prototypes. Here (Part I) we report SCL results using backpropagation networks as the learning modules. We used two feature extractors: about 30 energy-based features, and a combination of energy-based and graphical features (about 60). SCL recognized 98 % (energy) and 99 % (energy/graphical) of test digits, and 91 % (energy) and 96 % (energy/graphical) of test letters. In the accompanying paper (Part II), we report the results of SCL using fuzzy sets as learning modules for recognizing handwritten digits. 1. Introduction When an adaptive learning system such as a backpropagation (BP) net is used to encode input patterns from an evolving environment, it suffers the the stabilityplasticity dilemma formulated by Grossberg [Grossberg 1 [...] ...|$|E
40|$|A neural {{cognitive}} architecture {{would be}} an architecture based on simulated neurons, that provided a set of mechanisms for all cognitive behaviour. Moreover, this would be compatible with biological neural behaviour. As a result, such architectures can both {{form the basis of}} a fully-fledged AI and help to explain how cognition emerges from a collection of neurons in the human brain. The development of such a neural cognitive architecture is in its infancy, but a protoarchitecture in the form of behaving agents entirely based on simulated neurons is described. These agents take natural language commands, view the environment, plan and act. The development of these agents has led to a series of questions {{that need to be addressed}} to advance the development of neural cognitive architectures. These questions include long posed ones where progress has been made, such as the binding and symbol grounding problems; issues about biological architectures including neural models and brain topology; issues of emergent behaviour such as short and long-term Cell Assembly dynamics; and issues of learning such as the <b>stability-plasticity</b> dilemma. These questions can act as a road map for the development of neural cognitive architectures and AIs based on them...|$|E
40|$|Problem statement: In {{this study}} a {{self-generation}} ART- 1 neural network {{that is an}} efficient algorithm that emulates the self-organizing pattern recognition developed to avoid the <b>stability-plasticity</b> dilemma in competitive networks learning, is presented for Latin alphabet recognition to use in a vision system for road sings recognition. Approach: The first step of our approach deals with the training process where a set of input vectors are presented sequentially to the preprocessor to specify the inputs for the networks. Secondly {{the value of the}} mean squared error was used to measure the candidate for the output in the recognition phase. Thirdly to move down the large error-surface created by delta rule during the search phase the gradient-descent is used by changing each value of the weights by an amount that is proportional to the negative of the sigmoid function slope. Results: In the simulation test our system can self organize in real time producing stable recognition while getting inputs pattern beyond those originally stored. It can preserve its previously learned knowledge while keeping its ability to learn new patterns. Conclusions: The result suggests that the proposed system is pertinent to be put in practical use...|$|E
40|$|An {{incremental}} learning algorithm based on weighted majority voting of an ensemble of classifiers is introduced for supervised neural networks, where the voting weights are updated dynamically {{based on the}} current test input of unknown class. The algorithm’s dynamic voting weight update feature is an enhancement to our previously introduced {{incremental learning}} algorithm, Learn++. The algorithm is capable of incrementally learning new information from additional datasets that may later become available, even when the new datasets include instances from additional classes that were not previously seen. Furthermore, the algorithm retains formerly acquired knowledge without requiring access to datasets used earlier, attaining a delicate balance on the <b>stability-plasticity</b> dilemma. The algorithm creates additional ensembles of classifiers based on an iteratively updated distribution function on the training data that favors training with increasingly difficult to learn, previously not learned and/or unseen instances. The final classification is made by weighted majority voting of all classifier outputs in the ensemble, where the voting weights are determined dynamically during actual testing, based on the estimated performance of each classifier on the current test data instance. We present the algorithm in its entirety, {{as well as its}} promising simulation results on two real world applications...|$|E
40|$|Abstract. This paper {{describes}} the Hi-NOON neural simulator, originally {{conceived as a}} general-purpose, object-oriented software system for the simulation of small systems of biological neurons, {{as an aid to}} the study of links between neurophysiology and behaviour in lower animals. As such, the artificial neurons employed are spiking in nature: to effect an appropriate compromise between computational complexity and biological realism, modeling was at the transmembrane potential level of abstraction. Further, since real neural systems incorporate different types of neurons specialized to somewhat different functions, the software was written to accommodate a non-homogeneous population of neurons. Hi-NOON has been used in animat (cricket phonotaxis) and biologically-based robot studies. In particular, it was employed to implement the nervous system of our ARBIB robot. A simple model of synaptogenesis has been added so improving the stability of its learning {{in the light of the}} <b>stability-plasticity</b> dilemma, and as a mechanism for long-term memory. The efficacy of the simulator is illustrated with respect to some recent applications to situated systems studies. Now that Hi-NOON has been expanded to simulate large nervous systems in a concurrent environment, it can be applied to humanoid robotics in the future. ...|$|E
30|$|For {{long-term}} visual tracking task, {{many factors}} {{may affect the}} performance of the trackers, such as illumination variation, occlusion, scale changing and disappearance/reappearance, and the DCF-based tracker may imply poor performance under these situations. In this paper, we will focus on the scale estimation issue of DCF-based visual tracking. It {{is one of the most}} important factors in long-term visual tracking and our works also confirmed it. The nature for this is that, for the long-term visual tracking of discriminative model-based method, a big well-known issue is the <b>stability–plasticity</b> dilemma [22, 23]. That is, if we use some stable samples, such as the target assigned in the first frame, to train the classifier, then the tracker is unlikely to drifting and more robust to occlusions. However, if the target appearance variation is not taken into account in this case, the tracker is doomed to work not well in long-term visual tracking process. Furthermore, the capability of accurately retrieving the target scale is beneficial in many tracking applications. Here, we first make a short summary on the existing scale estimation method, and then, experiment comparisons among these methods have been conducted to get a deep insight on this issue. And some problems on the benchmark dataset are discussed.|$|E
40|$|Neural machine {{learning}} methods, such as deep neural networks (DNN), have achieved remarkable {{success in a}} number of complex data processing tasks. These methods have arguably had their strongest impact on tasks such as image and audio processing - data processing domains in which humans have long held clear advantages over conventional algorithms. In contrast to biological neural systems, which are capable of learning continuously, deep artificial networks have a limited ability for incorporating new information in an already trained network. As a result, methods for continuous learning are potentially highly impactful in enabling the application of deep networks to dynamic data sets. Here, inspired by the process of adult neurogenesis in the hippocampus, we explore the potential for adding new neurons to deep layers of artificial neural networks in order to facilitate their acquisition of novel information while preserving previously trained data representations. Our results on the MNIST handwritten digit dataset and the NIST SD 19 dataset, which includes lower and upper case letters and digits, demonstrate that neurogenesis is well suited for addressing the <b>stability-plasticity</b> dilemma that has long challenged adaptive {{machine learning}} algorithms. Comment: 8 pages, 8 figures, Accepted to 2017 International Joint Conference on Neural Networks (IJCNN 2017...|$|E
40|$|Abstract: Problem statement: In {{this study}} a {{self-generation}} ART- 1 neural network {{that is an}} efficient algorithm that emulates the self-organizing pattern recognition developed to avoid the <b>stability-plasticity</b> dilemma in competitive networks learning, is presented for Latin alphabet recognition to use in a vision system for road sings recognition. Approach: The first step of our approach deals with the training process where a set of input vectors are presented sequentially to the preprocessor to specify the inputs for the networks. Secondly {{the value of the}} mean squared error was used to measure the candidate for the output in the recognition phase. Thirdly to move down the large error-surface created by delta rule during the search phase the gradient-descent is used by changing each value of the weights by an amount that is proportional to the negative of the sigmoid function slope. Results: In the simulation test our system can self organize in real time producing stable recognition while getting inputs pattern beyond those originally stored. It can preserve its previously learned knowledge while keeping its ability to learn new patterns. Conclusions: The result suggests that the proposed system is pertinent to be put in practical use. Keywords: ART- 1, binary input vector, character recognition, Latin alphabe...|$|E
40|$|The very aim {{of every}} brain-computer {{interface}} (BCI) is to translate stimulated brain activity into a relevant computer command. This highly depends on error free processing methods and systematic regression or classification. Classification process {{is carried out}} for predicting the categories of test data. The brain processes multiple functions simultaneously resulting in a complex EEG (Electroencephalogram) pattern. Further, {{the characteristics of the}} P 300 component are difficult to be determined a priori especially when the signals are analysed on single trial basis. Hence it is essential for a strong classifier which can able to characterise the presence of P 300 component which is evoked during the target stimuli in the EEG. In this paper, we used the robust classification model Fuzzy ARTMAP to craft an efficient non-invasive P 300 based BCI from single trial visual evoked potential (VEP) signals. Fuzzy ARTMAP is an ART network for the association of analog pattern in supervised mode and is capable of overcoming the <b>stability-Plasticity</b> dilemma [1]. In this experiment, the VEP signals extracted during individual trials from able and severely disable bodied subjects are classified using the feature signal pattern. The high accuracy obtained as classification percentages validates the suitability of our proposed Fuzzy ARTMAP classification for single trial approach in BCI...|$|E
40|$|The {{application}} of cognitive mechanisms to support knowledge acquisition is, from {{our point of}} view, crucial for making the resulting models coherent, efficient, credible, easy to use and understandable. In particular, there are two characteristic features of intelligence that are essential for knowledge development: forgetting and consolidation. Both {{plays an important role in}} knowledge bases and learning systems to avoid possible information overflow and redundancy, and in order to preserve and strengthen important or frequently used rules and remove (or forget) useless ones. We present an incremental, long-life view of knowledge acquisition which tries to improve task after task by determining what to keep, what to consolidate and what to forget, overcoming The <b>Stability-Plasticity</b> dilemma. In order to do that, we rate rules by introducing several metrics through the first adaptation, to our knowledge, of the Minimum Message Length (MML) principle to a coverage graph, a hierarchical assessment structure which treats evidence and rules in a unified way. The metrics are not only used to forget some of the worst rules, but also to set a consolidation process to promote those selected rules to the knowledge base, which is also mirrored by a demotion system. We evaluate the framework with a series of tasks in a chess rule learning domain...|$|E
40|$|Adaptive {{resonance}} architectures are {{neural networks}} that self-organize stable pattern recognition codes in real-time {{in response to}} arbitrary sequences of input patterns. This article introduces ART 2, a class of adaptive resonance architectures which rapidly self-organize pattern recognition categories in response to arbitrary sequences of either analog or binary input patterns. In order to cope with arbitrary sequences of analog input patterns, ART 2 architectures embody solutions {{to a number of}} design principles, such as the <b>stability-plasticity</b> tradeoff, the search-direct access tradeoff, and the match-reset tradeoff. In these architectures. top-down learned expectation and matching mechanisms are critical in self-stabilizing the code learning process. A parallel search scheme updates itself adaptively as the learning process unfolds, and realizes a form of real-time hypothesis discovery, testing, learning, and recognition. After learning selfstabilizes, the search process is automatically disengaged. Thereafter input patterns directly access their recognition codes without any search. Thus recognition time for familiar inputs does not increase with the complexity of the learned code. A novel input pattern can directly access a category if it shares invariant properties with the set of familiar exemplars of that category. A parameter called the attentional [...] -igilance parameter determines how fine the categories will be. If vigilance increases (decreases) due to en [...] -ironmental feedback, then the system automatically searches for and learns finer (coarser) recognition categories. Gai...|$|E
40|$|Fuzzy {{information}} granules indicate sufficiently interpretable fuzzy {{sets for}} achieving {{a high level}} of human cognitive abstraction. Furthermore, granularity, complexity, and accuracy are associated with fuzzy information granules. Measuring granularity is a promising means of verifying the effectiveness of the fuzzy granular model. Higher granularity indicates fine partitions, whereas coarser partitions suggest lower granularity. Therefore, accuracy is directly proportional to the granularity, such that, the higher the granularity, the more accurate and more complex the model is. Consequently, the granularity-simplicity tradeoff is also a significant criterion in considering the interpretability-accuracy tradeoff. This paper thoroughly reviews diverse ideas to understand the fuzzy information granule and addresses a sensible compromise between interpretability-accuracy and granularity-simplicity. Those requirements contradict each other, thus certain conceptual and mathematical considerations are necessary in designing a granular framework. Moreover, a double axis taxonomy is introduced in this paper: “complexity-based granularity versus semantic-based granularity” (which considers granularity measures) and “granular partition level versus granular rule base level” (regarding knowledge base stages). However, several constraints should be considered in designing a granular framework such as the granularity-accuracy dilemma, the overfitting/underfitting situation, the granular rule base level conflict, the interpretability constraint threshold, the <b>stability-plasticity</b> dilemma, and the parameter optimization. This paper primarily aims to present a conceptual framework to better understand existing methods, as well as how these methods can inspire future research...|$|E
40|$|Electronic neuromorphic devices with on-chip, on-line {{learning}} {{should be able}} to modify quickly the synaptic couplings to acquire information about new patterns to be stored (synaptic plasticity) and, at the same time, preserve this information on very long time scales (synaptic stability). Here, we illustrate the electronic implementation of a simple solution to this <b>stability-plasticity</b> problem, recently proposed and studied in various contexts. It is based on the observation that reducing the analog depth of the synapses to the extreme (bistable synapses) does not necessarily disrupt the performance of the device as an associative memory, provided that 1) the number of neurons is large enough; 2) the transitions between stable synaptic states are stochastic; and 3) learning is slow. The drastic reduction of the analog depth of the synaptic variable also makes this solution appealing {{from the point of view}} of electronic implementation and offers a simple methodological alternative to the technological solution based on floating gates. We describe the full custom analog very large-scale integration (VLSI) realization of a small network of integrate-and-fire neurons connected by bistable deterministic plastic synapses which can implement the idea of stochastic learning. In the absence of stimuli, the memory is preserved indefinitely. During the stimulation the synapse undergoes quick temporary changes through the activities of the pre- and postsynaptic neurons; those changes stochastically result in a long-term modification of the synaptic efficacy. The intentionally disordered pattern of connectivity allows the system to generate a randomness suited to drive the stochastic selection mechanism. We check by a suitable stimulation protocol that the stochastic synaptic pla [...] ...|$|E
40|$|Abstract—This paper {{describes}} a new supervised fusion (hybrid) electrocardiogram (ECG) classification solution {{consisting of a}} new QRS complex geometrical feature extraction {{as well as a}} new version of the learning vector quantization (LVQ) classification algorithm aimed for overcoming the <b>stability-plasticity</b> dilemma. Toward this objective, after detection and delineation of the major events of ECG signal via an appropriate algorithm, each QRS region and also its corresponding discrete wavelet transform (DWT) are supposed as virtual images and each of them is divided into eight polar sectors. Then, the curve length of each excerpted segment is calculated and is used as the element of the feature space. To increase the robustness of the proposed classification algorithm versus noise, artifacts and arrhythmic outliers, a fusion structure consisting of five different classifiers namely as Support Vector Machine (SVM), Modified Learning Vector Quantization (MLVQ) and three Multi Layer Perceptron-Back Propagation (MLP–BP) neural networks with different topologies were designed and implemented. The new proposed algorithm was applied to all 48 MIT–BIH Arrhythmia Database records (within–record analysis) and the discrimination power of the classifier in isolation of different beat types of each record was assessed and as the result, the average accuracy value Acc= 98. 51 % was obtained. Also, the proposed method was applied to 6 number of arrhythmias (Normal, LBBB, RBBB, PVC, APB, PB) belonging to 20 different records of the aforementioned database (between– record analysis) and the average value of Acc= 95. 6 % was achieved. To evaluate performance quality of the new proposed hybrid learning machine, the obtained results were compared with similar peer– reviewed studies in this area...|$|E
40|$|A spiking {{neural network}} model is {{described}} for learning to discriminate among spatial patterns in an unsupervised manner. The network anatomy consists of source neurons that are activated by external inputs, a reservoir that resembles a generic cortical layer with an excitatory-inhibitory (EI) network and a sink layer of neurons for readout. Synaptic plasticity {{in the form of}} STDP is imposed on all the excitatory and inhibitory synapses at all times. While long-term excitatory STDP enables sparse and efficient learning of the salient features in inputs, inhibitory STDP enables this learning to be stable by establishing a balance between excitatory and inhibitory currents at each neuron in the network. The synaptic weights between source and reservoir neurons form a basis set for the input patterns. The neural trajectories generated in the reservoir due to input stimulation and lateral connections between reservoir neurons can be readout by the sink layer neurons. This activity is used for adaptation of synapses between reservoir and sink layer neurons. A new measure called the discriminability index (DI) is introduced to compute if the network can discriminate between old patterns already presented in an initial training session. The DI is also used to compute if the network adapts to new patterns without losing its ability to discriminate among old patterns. The final outcome is that the network is able to correctly discriminate between all patterns – both old and new. This result holds as long as inhibitory synapses employ STDP to continuously enable current balance in the network. The results suggest a possible direction for future investigation into how spiking neural networks could address the <b>stability-plasticity</b> question despite having continuous synaptic plasticity...|$|E
40|$|AbstractIn {{the last}} decade, machine {{learning}} (ML) techniques {{have been used}} for developing classifiers for automatic brain tumour diagnosis. However, the development of these ML models rely on a unique training set and learning stops once this set has been processed. Training these classifiers requires a representative amount of data, but the gathering, preprocess, and validation of samples is expensive and time-consuming. Therefore, for a classical, non-incremental approach to ML, it is necessary to wait long enough to collect all the required data. In contrast, an incremental learning approach may allow us to build an initial classifier with a smaller number of samples and update it incrementally when new data are collected. In this study, an incremental learning algorithm for Gaussian Discriminant Analysis (iGDA) based on the Graybill and Deal weighted combination of estimators is introduced. Each time a new set of data becomes available, a new estimation is carried out and a combination with a previous estimation is performed. iGDA does not require access to the previously used data and is able to include new classes that were not in the original analysis, thus allowing the customization of the models to the distribution of data at a particular clinical center. An evaluation using five benchmark databases has been used to evaluate the behaviour of the iGDA algorithm in terms of <b>stability–plasticity,</b> class inclusion and order effect. Finally, the iGDA algorithm has been applied to automatic brain tumour classification with magnetic resonance spectroscopy, and compared with two state-of-the-art incremental algorithms. The empirical results obtained show the ability of the algorithm to learn in an incremental fashion, improving the performance of the models when new information is available, and converging in the course of time. Furthermore, the algorithm shows a negligible instance and concept order effect, avoiding the bias that such effects could introduce...|$|E
