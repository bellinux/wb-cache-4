4|30|Public
50|$|The NorGrid {{project was}} {{initially}} {{established as a}} <b>subactivity</b> of the Norwegian infrastructure project Notur for high-performance computing. In March 2007, NorGrid was recognized by the Research Council of Norway as a separate initiative with its own funding.|$|E
40|$|The {{mixture of}} {{production}} and ad-hoc aspects in organizational work is a ubiquitous phenomenon which potentially occurs on all levels of detail in a workflow. In this paper I first describe how this problem is addressed in the Object-oriented Activity Support Model. The model supports the manual modifications of the <b>subactivity</b> tree in collaboration with a workflow engine, and it supports the mixture of activities with and without an execution scheme. Then I describe in more detail how the HieraStates mechanism for specifying and running execution schemes supports schemes which allow several degrees of flexibility during execution of an activity. These schemes provide support for activities where the ad-hoc and production aspects cannot be easily separated into different substeps. Throughout the paper {{the example of a}} purchase order is used to illustrate the approach. 1 Introduction Workflows involving both production and ad-hoc aspects are characteristic for larger projects which fol [...] ...|$|E
40|$|A method {{combining}} {{machine learning}} and regression analysis to auto-matically and intelligently update predictive models {{used in the}} Kansas Department of Transportation’s (KDOT’s) internal management system is presented. The predictive models used by KDOT consist of planning factors (mathematical functions) and base quantities (constants). The duration of a functional unit (defined as a <b>subactivity)</b> {{is determined by the}} product of a planning factor and its base quantity. The availability of a large data base on projects executed over the past decade provided the opportunity to develop an automated process updating predictive mod-els based on extracting information from historical data through machine learning. To perform the entire task of updating the predictive models, the learning process consists of three stages. The first stage derives the numerical relationship between the duration of a functional unit and the project attributes recorded in the data base. The second stage finds the functional units with similar behavior—that is, identifies functiona...|$|E
40|$|A {{letter report}} {{issued by the}} General Accounting Office with an {{abstract}} that begins "Pursuant to a congressional request, GAO evaluated the military services' and the Department of Defense's (DOD) fiscal year (FY) 1994 - 1998 operation and maintenance (O&M) budget requests, focusing on: (1) identifying {{the differences between the}} amounts Congress initially designated for O&M <b>subactivities,</b> and those DOD reported as obligated for the same subactivities; (2) identifying those O&M <b>subactivities</b> where DOD obligated funds differently than recommended by Congress in each year of the 5 -year period GAO examined; and (3) assessing information available to Congress to track DOD's movement of funds among O&M <b>subactivities.</b> ...|$|R
5000|$|Activities can be {{decomposed}} into <b>subactivities,</b> {{until at}} the bottom we find atomic actions.|$|R
50|$|A {{structure}} chart is a top-down modular design tool, constructed of squares representing the different modules in the system, and lines that connect them. The lines represent the connection and or ownership between activities and <b>subactivities</b> {{as they are}} used in organization charts.|$|R
40|$|I {{compute the}} cohomology of a non-commutative complex {{underlying}} {{the notion of}} the gauge field on the fuzzy sphere. 1. Noncommutative geometry is a well established mathematical discipline with a surprising and nontrivial impact on quantum field theory in general and the standard model in particular [1]. There is a <b>subactivity</b> in that vast subject which aims to replace the field theoretical models on the standard smooth manifolds by its counterparts defined on suitable noncommutative deformations of those manifolds [2, 4, 3]. The resulting noncommutative models usually respect all symmetries or supersymmetries of the commutative theories but they have an important advantage of possessing only a finite (though large) number of degrees of freedom. Recently such structures have emerged also {{in the context of the}} matrix model of M-theory [5, 6, 7]. The basic idea of the approach is as follows: One considers an Euclidean space-time which is taken to be compact for convenience. This spacetime gets equipped with a symplectic structure. A quantization of this symplectic structure gives an algebra of quantum observables which is to be taken as the definition of the non-commutative manifold. The compactness results in the finiteness of that noncommutative algebra of observables. The important feature of the formalism is that the Hamiltonian vector fields on the classical manifold survive the deformation. They are generated by the quantized Hamiltonians via the commutators. Finally, also the integration over the symplectic manifolds gets replaced in the deformed picture by the operation of taking the trace over the matrix algebra. Having at hand the deformed notions of algebra, Hamiltonian vector fields and integration we can construct the field theoretical actions for the models involving the scalar fields on the deformed noncommutative manifold. As an example, consider a Riemann sphere as a spacetime of an Euclidean field theory. The crucial observation is that S 2 is naturally a symplectic manifold; the symplectic form ω is up to a normalization just the round volume form on the sphere. Using the standard complex coordinate z on the Riemann sphere, we have ω = −...|$|E
40|$|Documents ::::::::::::::::::::::::: 40 6. 1. 1 Operations on Abstract Documents ::::::::::::: 41 6. 1. 2 The States of Abstract Documents ::::::::::::: 42 6. 2 Document History :::::::::::::::::::::::::: 43 6. 2. 1 Modeling Document Operations by <b>Subactivities</b> :::::: 43 6. 2. 2 The <b>Subactivities</b> for the Document Methods ::::::: 44 6. 3 DocumentVersions :::::::::::::::::::::::::: 44 6. 3. 1 De#nition of DocumentVersions ::::::::::::::: 44 6. 3. 2 Operations for DocumentVersions ::::::::::::: 45 6. 3. 3 Named Versions :::::::::::::::::::::::: 45 6. 4 Document Structure ::::::::::::::::::::::::: 46 6. 4. 1 Simple Documents :::::::::::::::::::::: 46 6. 4. 2 Structured Documents :::::::::::::::::::: 47 ii 6. 4. 3 Homogeneously Structured Documents ::::::::::: 48 6. 4. 4 Combination with Version Management ::::::::::: 49 6. 5 Document Content :::::::::::::::::::::::::: 50 6. 5. 1 Internal Documents :::::::::::::::::::::: 51 6. 5. 2 Filed Documents ::::::::::::::::::::::: 52 6. 5. 3 Filed Documents [...] ...|$|R
40|$|This paper {{describes}} {{a method for}} detection, tracking and recognition of lower arm and hand movements from color video sequences using a linguistic approach driven by motion analysis and clustering techniques. The novelty of our method comes from (i) automatic arm detection, without any manual initialization, foreground or background modeling, (ii) gesture representation {{at different levels of}} abstraction using a linguistic approach based upon signal to symbolic mappings, and (iii) robust matching for gesture recognition using the weighted largest common sequence (of symbols). Learning Vector Quantization abstracts the affine motion parameters as morphological primitive units, i. e., ”letters”, clustering techniques derive sequence of letters as ”words ” for both <b>subactivities</b> and the transition occurring between them, and finally the arm activities are recognized in terms of sequences of some <b>subactivities.</b> Using activity cycles from six kinds of arm movements, e. g., (slow and fast pounding), striking, swing, swirl, and stirring, which were not available during training, the performance achieved is perfect- 100 %- if one allows, as it should be the case for invariance purposes, to recognize slow and fast pounding video sequences as one and the same type of activity...|$|R
40|$|For FY 2013, NOAA {{requests}} a net {{decrease of}} $ 7, 697, 000 and 14 FTE below the FY 2013 base level {{for a total}} of $ 478, 066, 000 and 1, 224 FTE for the National Ocean Service after a technical transfer of $ 5, 116, 000 and 4 FTE to the National Marine Fisheries Service. This includes $ 3, 907, 000 and 0 FTE in inflationary adjustments. BASE JUSTIFICATION FOR FY 2013 : The National Ocean Service (NOS) has three <b>subactivities</b> under the Operations, Research an...|$|R
40|$|Abstract. This paper {{analyzes}} stochastic {{performance of}} a distributed global activity, composed of sub-activities sequenced serially, probabilistically, or concurrently. We provide general formulas with which we calculate the {{performance of a}} composite activity based on the performance of the constituent sub-activities and the control structure. To do this, we model each sub-activity as a Partially Ordered Specification (POS), where each sub-activity is characterized by independent input events, dependent output events and the stochastic minimum delays between these events. This technique allows two or more <b>subactivities</b> to be combined hierarchically. Proofs of correctness for these formulas are given and a simple example is discussed throughout the paper...|$|R
40|$|Many computer-supported {{applications}} are of long duration and consist of multiple steps that are exe-cuted over possibly heterogeneous servers. Such activ-ities have weaker atomicity requirements than trans-actions. Previously, we illustrated {{how to organize}} the execution of such activities using triggers and transac-tions. In this paper, we describe an execution model in. which activities may consist recursively of steps that may be <b>subactivities</b> or transactions. The model defines precisely the semantics of activities: commu-nication between steps and the failure semantics of activities including compensation and exception han-dling. The model also supports querying the status of activities. We also propose an implementation of the model using recoverable queues for reliably chaining the steps according to the semantics of the model. ...|$|R
40|$|Abstract. The use {{of norms}} is a {{well-known}} technique of co-ordination in multi-agent systems (MAS) adopted from human societies. A normative position is the “social burden ” associated with individual agents, that is, their obligations, permissions and prohibitions. Compound activities may be regulated by means of normative positions. However, conflicts may appear among normative positions of activities and <b>subactivities.</b> Recently several computational approaches have appeared to make norms operational in MAS {{but they do not}} cope with compound activities. In this paper, we propose an algorithm to determine the set of applicable normative positions, i. e., the largest set of normative positions without conflicts in the state of an activity, and propagate them to the sub-activities. ...|$|R
40|$|A {{workflow}} is a long-duration multi-step activity. The {{task of the}} workflow {{manager is}} to automate the execution and {{the control of the}} workflow activities. In this paper we are interested in workflows that execute under the control of various processing stations that may be located at different nodes of a distributed system. The stations may be autonomous and only partially automated. We present the design and a proposed implementation of a new model for workflow management. The model presented is modular in the sense that modification of a <b>subactivities</b> of the workflow does not necessarily require changes to the workflow specification. Furthermore, the model preserves the autonomy of the individual office environments and does not require them to modify the means they use to process the individual steps of the activity...|$|R
40|$|The {{increasing}} {{size and}} complexity of many software systems demand {{a greater emphasis on}} capturing and maintaining knowledge at many different levels within the software development process. This knowledge includes descriptions of the hardware and software components and their behavior, external and internal design specifications, and support for system testing. The knowledge-based software engineering (KBSE) research paradigm is concerned with systems that use formally represented knowledge, with associated inference procedures, to support the various <b>subactivities</b> of software development. As they grow in scale, KBSE systems must balance expressivity and inferential power with the real demands of knowledge base construction, maintenance, performance and comprehensibility. Description Logics (DL's) possess several features [...] a terminological orientation, a formal semantics and efficient reasoning procedures [...] which offer an effective tradeoff of these factors. We discuss three KBSE [...] ...|$|R
40|$|A {{computer}} framework {{feasible for}} developing parallel systems {{according to the}} Bulk Synchronous Parallel (BSP) computing model is described: Switched Interconnection of Parallel Processors (Swipp). Demanding applications {{can be described as}} directed graphs where the interdependent subtasks constitute the nodes. The tasks are predistributed by a system master, Computer Executive Engine (CEE), to a set of heterogeneous computing nodes. Each computing node has a preprogrammed secondary control processor attached for performing communication and runtime tasks, thus allowing primary processors of various kinds and programming styles. Synchronization of a bulk of <b>subactivities</b> is done in locksteps by the CEE. Basic features are modelled by the Ptolemy framework and prototype modules are being implemented. I. INTRODUCTION The performance of sequential single CPU processors has been steadily increasing over the last decades due to circuit technology improvements. The von Neumann model of comp [...] ...|$|R
40|$|Most {{object-oriented}} {{languages are}} strong on reusability or on strong-typing, but weak on concurrency. In {{response to this}} gap, we are developing Hybrid, an object-oriented language in which objects are the active entities. Objects in Hybrid are organized into domains, and concurrent executions into activities. All object communications are based on remote procedure-calls. Unstructured sends and accepts are forbidden. To this the mechanisms of delegation and delay queues are added to enable switching and triggering of activities. Concurrent <b>subactivities</b> and atomic actions are provided for compactness and simplicity. We show how solutions to many important concurrent problems, such a pipelining, constraint management and "administration" can be compactly expressed using these mechanisms. 1. Introduction. The idea of applying object-oriented approaches to the programming of concurrent systems has been gaining popularity in recent years. Some of the object-oriented languages that have [...] ...|$|R
40|$|How {{much time}} do {{consumers}} predict they will spend on using a product or service when they have control over the usage time? We propose that their predicted consumption time is systematically influenced by the valence and {{the representation of the}} target event. In three studies, we show that consumers predict spending more time on a pleasant event when it is unpacked into several <b>subactivities</b> and spending less time on an unpleasant event when it is unpacked. We also investigate the underlying mechanism and demonstrate that (1) people have a lay belief that they spend more (less) time on more (less) pleasant events and (2) unpacking increases the intensity of predicted consumption experience. We further show that these changes in time predictions influence consumption decisions and address alternative explanations, including mood, mood regulation, and attention. In closing, we discuss theoretical and managerial implications. ...|$|R
40|$|Efficient and {{effective}} studying of scientific papers {{is an important}} part of software engineering education. Moreover, it contributes to the knowledge necessary to carry out software research and development. However, we experienced numerous problems related to the many <b>subactivities</b> of studying scientific papers. It turns out that many of them are analogous to the problems occurring in the software development process, e. g., coordination, data capturing, communication, validation and scheduling. We have addressed them by applying principles and techniques known from software process engineering. A comprehensive process model has been developed, enacted, improved and validated. The process model describes literature search, paper selection, reading, and group discussion of papers, as well as recording, validation, and retrieval of the data we captured from the selected papers. The focus of this paper lies on the discussion part of the model. A model of this sub-process describes the s [...] ...|$|R
40|$|Testimony {{issued by}} the General Accounting Office with an {{abstract}} that begins "Accurate cost information is crucial for proper program management. Such information {{is especially important for}} the Bureau of Land Management's (BLM) Mining Law Administration Program (MLAP) because this program partially funded through mining fees that Congress has earmarked only for mining law administration operations. Some labor costs and several contracts and services were improperly charged to MLAP, causing other <b>subactivities</b> to benefit from funds intended for MLAP operations. Therefore, fewer funds have been available for actual MLAP operations. Although BLM has tried to make correcting adjustments for some of these improper charges, it has not established specific guidance or procedures to prevent improper charging of MLAP funds from recurring. Until additional procedures for MLAP are developed and effectively implemented, Congress and program managers can only place limited reliance on the accuracy of MLAP cost information. ...|$|R
25|$|Conventional {{temporal}} probabilistic models such as {{the hidden}} Markov model (HMM) and conditional random fields (CRF) model directly model {{the correlations between the}} activities and the observed sensor data. In recent years, increasing evidence has supported the use of hierarchical models which take into account the rich hierarchical structure that exists in human behavioral data. The core idea here is that the model does not directly correlate the activities with the sensor data, but instead breaks the activity into sub-activities (sometimes referred to as actions) and models the underlying correlations accordingly. An example could be the activity of preparing spaghetti, which can be broken down into the <b>subactivities</b> or actions of cutting vegetables, frying the vegetables in a pan and serving it on a plate. Examples of such a hierarchical model are Layered Hidden Markov Models (LHMMs) and the hierarchical hidden Markov model (HHMM), which have been shown to significantly outperform its non-hierarchical counterpart in activity recognition.|$|R
40|$|We {{propose a}} system that can {{recognize}} daily human activities with a Kinect-style depth camera. Our system utilizes a set of view-invariant features and the hidden state conditional random field (HCRF) model to recognize human activities from the 3 D body pose stream provided by MS Kinect API or OpenNI. Many high-level daily activities {{can be regarded as}} having a hierarchical structure where multiple <b>subactivities</b> are performed sequentially or iteratively. In order to model effectively these high-level daily activities, we utilized a multiclass HCRF model, which is a kind of probabilistic graphical models. In addition, in order to get view-invariant, but more informative features, we extract joint angles from the subject’s skeleton model and then perform the feature transformation to obtain three different types of features regarding motion, structure, and hand positions. Through various experiments using two different datasets, KAD- 30 and CAD- 60, the high performance of our system is verified...|$|R
40|$|Active {{objects are}} concurrent, active {{entities}} {{based on the}} object-oriented paradigm. We present a model for understanding active objects based on the remote procedure call, and {{on the notion of}} activities, which capture a single-thread flow of control between objects. We also present simple mechanisms for creating activities, interleaving and delaying activities, and for constructing atomic actions and concurrent <b>subactivities.</b> We show how these mechanisms can be used to capture very general forms of triggering. Our model for active objects, and the mechanisms for manipulating activities are embedded in Hybrid, a concurrent, object-oriented language. The model is also useful for understanding and dealing with deadlock in such systems. 1 Introduction. The object-oriented approach is an increasingly popular approach for enhancing reusability of code through abstraction, instantiation, inheritance, and homogeneity [Nier 86 Nyga 86 Stef 85]: Objects are persistent entities with a well-defined interface for handling requests, and a hidden realization. Objects with the same interface are instances of types (or “classes”), and may inherit propertie...|$|R
40|$|District {{central office}} administrators {{increasingly}} face policy demands to use “evidence ” in their decision making. These demands {{up the ante}} on education policy researchers and policy makers to better understand what evidence use in district central offices entails and the conditions that may support it. To that end, the authors conducted {{a comprehensive review of}} research literature on evidence use in district central offices, finding that the process of evidence use is complex, spanning multiple <b>subactivities</b> and requiring administrators to make sense of evidence and its implications for central office operations. These activities have significant political dimensions and involve the use of “local knowledge ” as a key evidence source. Evidence use is shaped by features of the evidence itself and various organizational and institutional factors. Policy shapes evidence use, but other factors mediate its impact. The authors conclude with implications for future policy and research on central office evidence-based decision making. Keywords: policy implementation; school district; central office; evidence...|$|R
40|$|Permutation {{modeling}} {{is challenging}} {{because of the}} combinatorial nature of the problem. However, such modeling is often required in many real-world applications, including activity recognition where <b>subactivities</b> are often permuted and partially ordered. This paper introduces a novel Hidden Permutation Model (HPM) that can learn the partial ordering constraints in permuted state sequences. The HPM is parameterized as an exponential family distribution and is flexible {{so that it can}} encode constraints via different feature functions. A chain-flipping Metropolis-Hastings Markov chain Monte Carlo (MCMC) is employed for inference to overcome the O(n!) complexity. Gradient-based maximum likelihood parameter learning is presented for two cases when the permutation is known and when it is hidden. The HPM is evaluated using both simulated and real data from a location-based activity recognition domain. Experimental results indicate that the HPM performs far better than other baseline models, including the naive Bayes classifier, the HMM classifier, and Kirshner 2 ̆ 7 s multinomial permutation model. Our presented HPM is generic and can potentially be utilized in any problem where the modeling of permuted states from noisy data is needed...|$|R
40|$|MBA Professional ReportThe {{purpose of}} this MBA project is to {{identify}} the actual cost savings when a Performance Based Logistics (PBL) contract, with incentives, is awarded to replace a specified maintenance echelon for critical components or subcomponents of systems. We will examine the impact on system availability {{to determine if a}} proposed investment will be practical. The entire logistics flow and maintenance processes, to include all associated <b>subactivities,</b> will be analyzed. In order to do this, we must initially identify all costs of operating the current organic maintenance echelon, which will become the PBL baseline. This will be used to determine the value added of any incremental percentage change in readiness or elimination of organic maintenance echelon(s). We propose to develop a simulation based decision support tool to assist Program Managers (PM) with issues of valuing options for the improvement of system availability, and making appropriate investment options. Ultimately, this project will determine the financial viability and practicality of implementing a PBL incentive contract. US Navy (USN) author...|$|R
40|$|E cient and e ective {{studying}} of scienti c papers is {{an important}} part of software engineering education. Moreover, it contributes to the knowledge necessary to carry out software research and development. However, we experienced numerous problems related to the many <b>subactivities</b> of studying scienti c papers. It turns out that many of them are analogous to the problems occurring in the software development process, e. g., coordination, data capturing, communication, validation and scheduling. We have addressed them by applying principles and techniques known from software process engineering. A comprehensive process model has been developed, enacted, improved and validated. The process model describes literature search, paper selection, reading, and group discussion of papers, as well as recording, validation, and retrieval of the data we captured from the selected papers. The focus of this paper lies on the discussion part of the model. A model of this sub-process describes the systematic classi cation, analysis, and evaluation of the papers. It is used to guide the group discussion, and it helps to ensure that pertinent information from the discussion is retained inan annotated database...|$|R
40|$|The {{links between}} the {{theoretical}} issues influencing the structure of construction project organizations are discussed. The impact of the environment of a construction project and the technological sophistication of the project are considered {{in terms of how}} these factors shape project organizations. The environment is variously assayed for its complexity, its dynamism and its hostility. The technology used in projects is assessed by its level of certainty (whether it is well understood), its complexity and the level of interdependence between <b>subactivities</b> in the project. The variables are used to formulate hypotheses concerning their impact upon the structuring of construction projects and these are studied in 18 case studies. The research has been developed within an interpretive (phenomenological) paradigm. The findings suggest that complex environments lead to greater decentralization of authority, mainly by delegation. In the dimension of technology, complexity led to a wider use of liaison devices on projects with a greater number of technical functional specialists being used by projects. As projects become more technically interdependent then informality and flexibility are the principal mechanisms of project control. link_to_subscribed_fulltex...|$|R
40|$|A {{letter report}} {{issued by the}} General Accounting Office with an {{abstract}} that begins "The Bureau of Land Management's (BLM) Mining Law Administration Program (MLAP) is responsible for managing the environmentally responsible exploration and development of locatable minerals on public lands. The program is funded through mining fees collected from the holders of unpatented mining claims and sites and by appropriations {{to the extent that}} fees are inadequate to fund the program. Congress and program managers need accurate cost information in order to make informed program and budgeting decisions. However, GAO found that BLM's financial records did not accurately reflect the true costs of its programs because the costs of labor and a number of contracts and services costs were charged to MLAP and not to the appropriate program. As a result, other <b>subactivities</b> benefited from the charging of these improper costs and fewer funds have been available for actual MLAP operations. BLM has taken steps to make correcting adjustments for improper charges to MLAP contracts and services; however, additional adjustments are needed to correct for labor costs that were improperly charged to MLAP. Until these adjustments for improperly charged labor are made, Congress and program managers can place only a limited reliance on the accuracy of MLAP cost information. ...|$|R
40|$|A {{letter report}} {{issued by the}} General Accounting Office with an {{abstract}} that begins "Congress has {{expressed concern about the}} extent to which the Department of Defense has moved funds that directly affect military readiness, such as those that finance training, to pay for other <b>subactivities</b> within its operation and maintenance (O&M) account, such as real property maintenance and base operations. This report reviews the (1) Army's obligation of O&M division training funds and (2) readiness of the Army's divisions. GAO found that the Army continued to use division training funds for purposes other than training during fiscal year 2000. However, the reduced funding did not interfere with the Army's planned training events or exercises. The Army's tank units also reported that, despite the reduced funding and their failure to meet their tank mileage performance goal, their readiness remained high. Specifically, many tank units reported that they could be fully trained for their wartime mission within a short time period. Units that reported that they would need more time to become fully trained generally cited personnel issues rather than the lack of training funds as the reason. Even so, starting in fiscal year 2001, the Army has taken action to restrict moving training funds by exempting unit training funds from any Army headquarters' adjustments and requiring prior approval before Army commands move any training funds. ...|$|R
40|$|The {{simian virus}} 40 (SV 40) large tumor antigen (T antigen) under its natural {{regulatory}} elements induces choroid plexus papillomas in transgenic mice. Because these tumors develop focally after several months, {{it has been}} suggested that secondary cellular alterations are required to induce a tumor in this tissue. In contrast to SV 40, the related lymphotropic papovavirus early region induces rapid nonfocal choroid plexus neoplasia in transgenic mice. Here, using hybrid gene constructs, we showed that T antigen from either virus in in fact sufficient to induce these tumors. Their abilities to induce proliferative abnormalities in other tissues, such as kidney and thymus, were also indistinguishable. Differences in the rate of choroid plexus tumorigenesis reflected differences in the control regions of the two viruses, rather than differences in T antigen per se. Under SV 40 regulation, expression was limited to a fraction of the choroid plexus cells prior to the formation of focal tumors. When SV 40 T antigen was placed under lymphotropic papovavirus control, in contrast, expression was generally uniform in the choroid plexus and rapid expansion of the tissue ensued. We found a direct relationship between T-antigen expression, morphological transformation, and proliferation of the choroid plexus epithelial cells. Analysis of mosaic transgenic mice indicated further that T antigen exerts its mitogenic effect cell autonomously. These studies form the foundation for elucidating the role of various T-antigen <b>subactivities</b> in tumorigenesis...|$|R
40|$|District {{central office}} administrators {{increasingly}} face policy demands to use “evidence ” in their decision making. These demands {{up the ante}} on education policy researchers and policy makers to better understand what evidence use in district central offices entails and the conditions that may support it. To that end, the authors conducted {{a comprehensive review of}} research literature on evidence use in district central offices, finding that the process of evidence use is complex, spanning multiple <b>subactivities</b> and requiring administrators to make sense of evidence and its implications for central office operations. These activities have significant political dimensions and involve the use of “local knowledge ” as a key evidence source. Evidence use is shaped by features of the evidence itself and various organizational and institutional factors. Policy shapes evidence use, but other factors mediate its impact. The authors conclude with implications for future policy and research on central office evidence-based decision making. Keywords: policy implementation; school district; central office; evidence-based decision making Contemporary federal and state policies increasingly demand that schooldistrict central offices use “evidence”—variously defined—to ground their educational improvement efforts. In the 1980 s and 1990 s, for example, the standards-based reform movement initiated a call for school district central offices to assess student performance against federal, state, and local standards and use findings to guide their decisions about how to expand students’ opportunities to learn. Currently, No Child Left Behind (NCLB) requires that all programs funded under this initiative stem from “evidence ” and “scien-tifically based research ” and that they otherwise be “data-driven ” (NCLB Act, 2002). These requirements affect a wide range of federal, state, and distric...|$|R

