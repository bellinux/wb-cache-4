24|0|Public
50|$|A {{partitioned}} {{data set}} (PDS) is a data set containing multiple members, each of which holds a separate <b>sub-data</b> set, similar to a directory in other types of file systems. This type of data set {{is often used to}} hold executable programs (load modules), source program libraries (especially Assembler macro definitions), and Job Control Language. A PDS may be compared to a Zip file or COM Structured Storage.|$|E
3000|$|... [...]. It {{is shown}} in [28] that the {{diversity}} gain of i-th detected <b>sub-data</b> stream layer of V-BLAST scheme is (N[*]−[*]i[*]+[*] 1). Thus, [...]...|$|E
3000|$|... where, as in Eq.  1, indexes 1 and 2 {{refer to}} the two sub-models {{computed}} from the two ASM-V <b>sub-data</b> sets, and index 0 refers to the original ASM-V candidate model.|$|E
30|$|It can be {{seen from}} (51) that the {{received}} i-th <b>sub-data</b> stream is composed of STBC layer symbols, AWGN noise and the potential propagation error from V-BLAST layer. The equivalent noise is the combination of the last two parts.|$|E
40|$|Genetic {{programming}} (GP) is {{an evolutionary}} technique and is gaining attention {{for its ability}} to learn the underlying data relationships and express them in a mathematical manner. Although GP uses the same principles as genetic algorithms, it is a symbolic approach to program induction; i. e., it involves the discovery of a highly fit computer program from the space of computer programs that produces a desired output when presented with a particular input. We have successfully applied the GP paradigm for the n-category pattern classification problem. The ability of the GP classifier to learn the data distributions depends upon the number of classes and the spatial spread of data. As the number of classes increases, it increases the difficulty for the GP classifier to resolve between classes. So, {{there is a need to}} partition the feature space and identify sub-spaces with reduced number of classes. The basic objective is to divide the feature space into sub-spaces and hence the data set that contains representative samples of n classes into <b>sub-data</b> sets corresponding to the sub-spaces of the feature space, so that some of the <b>sub-data</b> sets/spaces can have data belonging to only p-classes (p<n). The GP classifier is then evolved independently for the <b>sub-data</b> sets/spaces of the feature space. The GP classifier becomes simpler for some of the <b>sub-data</b> sets/spaces as only p classes are present. It also results in localized learning as the GP classifier has to learn the data distribution in only a sub-space of the feature space rather than in the entire feature space. In this paper, we are integrating the GP classifier with feature space partitioning (FSP) for localized learning to improve pattern classification...|$|E
40|$|For an {{efficient}} traffic policy {{more and better}} statistical data then now available are needed. These data need to cover {{a large number of}} aspects regarding: trip production, quality, characteristics of rod users, road and vehicles. To facilitate this Registration System of Traffic Elements (integraal Verkeerselementen Registratiesysteem INVERS) is necessary. The system which is described here is a compilation of <b>sub-data</b> bases of the Integral Traffic Accident Registration system...|$|E
40|$|Diffraction images {{collected}} on dectris Pilatus. Format of images in *. cbf. Images collected with Mesh and Collect strategy at ESRF. Thaumatin : contain 6 sets of exposure reflecting 6 increasing Dose (from 1 to 6) Cubic Insulin : contain 3 sets of <b>sub-data</b> sets : Before_ 1 and Before_ 2 (means before UV irradiation). and After : (after UV irradiation). For full information refer to material and method in : (submit, will be update...|$|E
30|$|As {{its name}} {{suggests}} the IRI Data Library is organized as a library; {{a collection of}} both locally held and remotely held data sets, designed to make the data more accessible for the library’s users. Data sets in the library come from many different sources and many different “data cultures” in many different formats. By “data set” we mean a collection of data organized as multidimensional dependent variables, independent variables, and <b>sub-data</b> sets, along with the metadata (particularly metadata on purpose and use) {{that makes it possible}} to interpret the data in a meaningful manner.|$|E
40|$|The {{measured}} data of {{global solar radiation}} on a horizontal surface, {{as well as the}} number of sunshine hours, mean daily ambient temperature, maximum and minimum ambient temperatures, relative humidity and amount of cloud cover, for Jeddah (latitude 21 Â° 42 ' 37 ''N, longitude 39 Â° 11 ' 12 ''E), Saudi Arabia for the period 1996 - 2006 are analyzed. The data are divided into two sets. The <b>sub-data</b> set 1 (1996 - 2004) are employed to develop empirical correlations between the monthly average of daily global solar radiation fraction (H/H 0) and various meteorological parameters. The nonlinear Angström type model developed by Sen and the trigonometric function model proposed by Bulut and Büyükalaca are also evaluated. New empirical constants for these two models have been obtained for Jeddah. The <b>sub-data</b> set 2 (2005, 2006) are then used to evaluate the derived correlations. Comparisons between measured and calculated values of H have been performed. It is indicated that, the Sen and Bulut and Büyükalaca models satisfactorily describe the horizontal global solar radiation for Jeddah. All the proposed correlations are found to be able to predict the annual average of daily global solar radiation with excellent accuracy. Therefore, the long term performance of solar energy devices can be estimated. Global solar radiation Regression analysis Meteorological data...|$|E
40|$|Integrating {{analysis}} of the benthic palaeoecological record with multivariate ordination techniques represents a powerful synergy able to provide an improved characterization of coastal depositional facies in a sequence stratigraphical perspective. Through quantitative {{analysis of}} benthic foraminifer, ostracod and mollusc associations from the postglacial succession of Core M 3 (Arno coastal plain, Tuscany, Italy), and application of detrended correspondence analysis (DCA) to the mollusc <b>sub-data</b> set, we offer a refined picture of stratigraphical variations in faunal content from a paralic depositional setting, and reconstruct the palaeoenvironmental gradients that account for such variations. Despite distinct ecological behaviours, and taphonomic and sedimentological constraints, a strong ecological control on meio- and macrofaunal biofacies and taxa turnover is documented across the study succession. Amongst all possible mechanisms that {{may play a role}} in shaping' fossil distribution, the ecological signal driven by salinity represents the most prominent factor controlling the composition of fossil associations in the cored succession. Molluscs can even provide outstanding quantitative estimates of palaeosalinity along the sampled core. When plotted stratigraphically, the three fossil <b>sub-data</b> sets show consistent patterns of vertical evolution that enable prompt identification of the key surfaces for sequence stratigraphical interpretation in otherwise lithologically indistinguishable deposits. The concomitant maximum richness of species with strong marine affinity, paralleled by the highest DCA salinity estimates, allows recognition of the maximum flooding zone, dated to approximate to 7. 7 cal. ka BP, within a homogeneous succession of outer lagoon clays. These clays are sandwiched between early transgressive, swamp to inner lagoon deposits and overlying prograding coastal-alluvial plain facies...|$|E
40|$|Recent {{advances}} in macromolecular crystallography {{have made it}} practical to rapidly collect hundreds of <b>sub-data</b> sets consisting of small oscillations of incomplete data. This approach, generally referred to as serial crystallography, has many uses, including an increased effective dose per data set, the collection of data from crystals without harvesting (in situ data collection) and studies of dynamic events such as catalytic reactions. However, selecting which data sets from this type of experiment should be merged can be challenging and new methods are required. Here, it is shown that a genetic algorithm {{can be used for}} this purpose, and five case studies are presented in which the merging statistics are significantly improved compared with conventional merging of all data...|$|E
40|$|The {{measured}} data of global and {{diffuse solar radiation}} on a horizontal surface, the number of bright sunshine hours, mean daily ambient temperature, maximum and minimum ambient temperatures, relative humidity and amount of cloud cover for Jeddah (lat. 21 Â° 42 ' 37 ''N, long. 39 Â° 11 ' 12 ''E), Saudi Arabia, during the period (1996 - 2007) are analyzed. The monthly averages of daily values for these meteorological variables have been calculated. The data are then divided into two sets. The <b>sub-data</b> set I (1996 - 2004) are employed to develop empirical correlations between the monthly average of daily global solar radiation fraction (H/H 0) and the various weather parameters. The <b>sub-data</b> set II (2005 - 2007) are then {{used to evaluate the}} derived correlations. Furthermore, the total solar radiation on horizontal surfaces is separated into the beam and diffuses components. Empirical correlations for estimating the diffuse solar radiation incident on horizontal surfaces have been proposed. The total solar radiation incident on a tilted surface facing south Ht with different tilt angles is then calculated using both Liu and Jordan isotropic model and Klucher's anisotropic model. It is inferred that the isotropic model is able to estimate Ht more accurate than the anisotropic one. At the optimum tilt angle, the maximum value of Ht is obtained as ~ 36 (MJ/m 2 Â day) during January. Comparisons with 22 Â years average data of NASA SSE Model showed that the proposed correlations are able to predict the total annual energy on horizontal and tilted surfaces in Jeddah with a reasonable accuracy. It is also found that at Jeddah, the solar energy devices have to be tilted to face south with a tilt angle equals the latitude of the place in order to achieve the best performance all year round. Solar radiation Sunshine hours Meteorological parameters Regression analysis Tilted surfaces...|$|E
40|$|This thesis {{presents}} structural separation logic, a novel program reasoning {{approach for}} software that manipulates both standard heaps and structured data such as lists and trees. Structural separation logic builds upon existing work in both separation logic and context logic. It considers data abstractly, {{much as it}} is exposed by library interfaces, ignoring implementation details. We provide a programming language that works over structural heaps, which are similar to standard heaps but allow data to be stored in an abstract form. We introduce abstract heaps, which extend structural heaps to enable local reasoning about abstract data. Such data can be split up with structural addresses. Structural addresses allow <b>sub-data</b> (e. g. a sub-tree within a tree) to be abstractly allocated, promoting the <b>sub-data</b> to an abstract heap cell. This cell can be analysed in isolation, then re-joined with the original data. We show how the tight footprints this allows can be refined further with promises, which enable abstract heap cells to retain information about the context from which they were allocated. We prove that our approach is sound with respect to a standard Hoare logic. We study two large examples. Firstly, we present an axiomatic semantics for the Docu- ment Object Model in structural separation logic. We demonstrate how structural separa- tion logic allows abstract reasoning about the DOM tree using tighter footprints than were possible in previous work. Secondly, we give a novel presentation of the POSIX file system library. We identify a subset of the large POSIX standard that focuses on the file system, including commands that manipulate both the file heap and the directory structure. Axioms for this system are given using structural separation logic. As file system resources are typically identified by paths, we use promises to give tight footprints to commands, so that that they do not require all the resource needed to explain paths being used. We demonstrate our reasoning using a software installer example. Open Acces...|$|E
40|$|The {{concept of}} {{predictive}} maintenance, whose application became every day {{more and more}} diffused was born some years ago. The fundamental idea on which the predictive maintenance is based on is the monitoring of specific parameters that can supply useful information on the system state of health. In the presented application, vibrational levels represent one of these parameters and relative continuous monitoring is proposed. As a drawback of this approach, the availability of monitoring devices and their correct installation is needed, even if many times not availablr for cost or installation reasons. To avoid such limitations, the present work present an Artificial Neaural Network based approach {{for the management of}} “virtual” sensors whose data are derived from a limited set of <b>sub-data.</b> The presented application will show interesting results obtained with reference to a traction converter system {{as an example of the}} proposed technique...|$|E
40|$|Thesis (M. Sc. (Computer Science)) [...] North-West University, Potchefstroom Campus, 2007. The aim of {{this study}} is to {{investigate}} whether agile system development methodologies (ASDMs) are suitable for the development of data warehouses. To reach this aim, a literature study was conducted on the relatively settled ASDMs by firstly defining a system development methodology (SDM) and an ASDM. Each ASDM explanation contains the identified key factors, unique process model, and method of use. The seven ASDMs investigated in this study, are: Dynamic System Development Methodology (DSDM), Scrum, Extreme Prograrr~ming (XP), Feature Driven Development (FDD), Crystal ASDMs - especially Crystal Clear (CC), Adaptive Software Development (ASD), and Lean Development (LD). In addition, a literature study is conducted on the data warehouse approaches of lnmon (1996) and Kimball et a/. (1998). Each data warehouse approach is explained using the architecture, lifecycle and four distinct phases within the lifecycle. The four distinct phases include: collecting requirements, data modelling, data staging, and data access and deployment. After this was done, lnmon and Kimball's approaches were compared. After studying the ASDMs and data warehousing approaches, theoretical deductions were made regarding the suitability of ASDMs in data warehouse development. General deductions (including the applicability of agile processes) for all ASDMs as well as unique deductions for each of the seven ASDMs mentioned above were formulated in theory. The theoretical deductions lead to the limitation of the empirical section of the study to the suitability of ASDMs within the,framework of Kimball's approach. Theoretical deductions were empirically tested by conducting an interpretive experiment where seven data warehouse development teams used an assigned ASDM to develop a data warehouse. The data warehouse consisted of one data mart. Each team was expected to develop their data mart incrementally, one <b>sub-data</b> mart at a time. Every <b>sub-data</b> mart was developed iteratively to form the data mart. The data mart was then deployed as a whole (including everything from collecting requirements to report generation) to the users. The findings of the study are a combination of the theoretical deductions and interpretive results (propositions) of the interpretive experiment conducted. These findings indicate that ASDMs are suitable to develop data warehouses in a constantly changing environment. Master...|$|E
40|$|An {{apparatus}} {{for generating}} a quality-scalable video data stream (36) is described which comprises means (42) for coding a video signal (18) using block-wise transformation to obtain transform blocks (146, 148) of transformation coefficient values {{for a picture}} (140) of the video signal, a predetermined scan order (154, 156, 164, 166) with possible scan positions being defined among the transformation coefficient values within the transform blocks so that in each transform block, for each possible scan position, {{at least one of}} the transformation coefficient values within the respective transform block belongs to the respective possible scan position; and means (44) for forming, for each of a plurality of quality layers, a video <b>sub-data</b> stream (30; 28, 30) containing scan range information indicating a sub-set of the possible scan positions, and transform coefficient information on transformation coefficient values belonging to the sub-set of possible scan positions such that the sub-set of each quality layer comprises at least one possible scan position not comprised by the sub-set of any other of the plurality of quality layers...|$|E
40|$|A new wavelet based feature {{parameter}} {{have been}} developed to represent the characteristics of PD activities, i. e. the wavelet decomposition energy of PD pulses measured from non-conventional ultra wide bandwidth PD sensors such as capacitive couplers (CC) or high frequency current transformers (HFCT). The generated feature vectors can contain different dimensions depending on the length of recorded pulses. These high dimensional feature vectors can then be processed using Principal Component Analysis (PCA) to map the data into a three dimensional space whilst the first three most significant components representing the feature vector are preserved. In the three dimensional mapped space, an automatic Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm is then applied to classify the data cluster(s) produced by the PCA. As the procedure is undertaken in a three dimensional space, the obtained clustering results can be easily assessed. The classified PD <b>sub-data</b> sets are then reconstructed in the time domain as phase-resolved patterns to facilitate PD source type identification. The proposed approach has been successfully applied to PD data measured from electrical machines and power cables where measurements were undertaken in different laboratories...|$|E
40|$|Size-segregated aerosol {{samples were}} {{collected}} using six stages High Volume Cascade Impactor. Aerosol mass and water soluble ions concentrations were determined. The Hybrid Single-Particle Lagrangian Integrated Trajectory (HYSPLIT) model was used to study the origin of air masses arriving to Belgrade in the investigated period. The obtained results of aerosol mass and water-soluble ion concentrations have been divided into six <b>sub-data</b> sets based on air mass categories. The highest average mass concentration of the fine mode (Dp ≤ 0. 49 μm) was found for air masses coming to Belgrade from the southeast and northwest directions, and of the coarse mode (3. 0 < Dp ≤ 7. 2 μm) for air masses arriving from the northwest direction. The highest concentrations of SO 42 – {{were found in the}} fine particles transported to the investigated area by air masses from southeast direction. The analysis of contribution of marine aerosol components (Na+ and Cl-), the Correlation and Cluster Analysis indicated the influence of marine aerosol on urban aerosol of the central Balkans coming from the Western Mediterranean and northern direction. NH 4 + and SO 42 - and K+ dominated in the fine mode for all air mass categories. PCA demonstrated the dominant impact of secondary aerosol formation processes on urban aerosols...|$|E
40|$|Background: Stereoelectroencephalography (SEEG) methodology, {{originally}} developed by Talairach and Bancaud, is progressively gaining popularity for the presurgical invasive evaluation of drug-resistant epilepsies. Objective: To describe recent SEEG methodological implementations {{carried out in}} our center, to evaluate safety, and to analyze in vivo application accuracy in a consecutive series of 500 procedures {{with a total of}} 6496 implanted electrodes. Methods: Four hundred nineteen procedures were performed with the traditional 2 -step surgical workflow, which was modified for the subsequent 81 procedures. The new workflow entailed acquisition of brain 3 -dimensional angiography and magnetic resonance imaging in frameless and markerless conditions, advanced multimodal planning, and robot-assisted implantation. Quantitative analysis for in vivo entry point and target point localization error was performed on a <b>sub-data</b> set of 118 procedures (1567 electrodes). Results: The methodology allowed successful implantation in all cases. Major complication rate was 12 of 500 (2. 4 %), including 1 death for indirect morbidity. Median entry point localization error was 1. 43 mm (interquartile range, 0. 91 - 2. 21 mm) with the traditional workflow and 0. 78 mm (interquartile range, 0. 49 - 1. 08 mm) with the new one (P < 2. 2 × 10). Median target point localization errors were 2. 69 mm (interquartile range, 1. 89 - 3. 67 mm) and 1. 77 mm (interquartile range, 1. 25 - 2. 51 mm; P < 2. 2 × 10), respectively. Conclusion: SEEG is a safe and accurate procedure for the invasive assessment of the epileptogenic zone. Traditional Talairach methodology, implemented by multimodal planning and robot-assisted surgery, allows direct electrical recording from superficial and deep-seated brain structures, providing essential information in the most complex cases of drug-resistant epilepsy. Abbreviations: DSA, digital subtraction angiographyEP, entry pointEPLE, entry point localization errorEZ, epileptogenic zoneSEEG, stereoelectroencephalographyTP, target pointTPLE, target point localization error. Copyright © 2012 by the Congress of Neurological Surgeons...|$|E
40|$|International audience[1] This paper {{presents}} {{a compilation of}} intensity data covering the past 10 millennia (ArcheoInt). This compilation, which upgrades the one of Korte et al. (2005), contains 3648 data and incorporates additional intensity and directional data sets. A large majority of these data ($ 87 %) were acquired on archeological artifacts, and the remaining $ 13 % correspond to data obtained from volcanic products. The present compilation also includes important metadata for evaluating the intensity data quality and providing a foundation to guide improved selection criteria. We show that $ 50 % of the data set fulfill reasonable reliability standards which {{take into account the}} anisotropic nature of most studied objects (potsherds), the stability of the magnetization, and the data dispersion. The temporal and geographical distributions of this <b>sub–data</b> set {{are similar to those of}} the main data set, with $ 72 % of the data dated from the past three millennia and $ 76 % obtained from western Eurasia. Approximately half of the selected intensity data are associated with at least an inclination value. To constrain the axial and full dipole evolution over the past three millennia requires that we avoid any overrepresentation of the western Eurasian data. We introduce a first-order regional weighting scheme based on the definition of eight widely distributed regions of 30 ° width within which the selected data are numerous enough. The regional curves of virtual axial dipole moments (VADM) and of mixed VADM-virtual dipole moments (VDM) averaged over sliding windows of 200 years and 500 years testify for strong contributions from either equatorial dipole or nondipole components. The computation of global VADM and mixed VADM/VDM variation curves, assuming an equal weight for each region, yields a dipole evolution marked by a distinct minimum around 0 B. C. /A. D. followed by a maximum around the third-fourth century A. D. A second minimum is present around the eighth century A. D. This variation pattern is compatible with the one deduced from earlier, mor...|$|E
40|$|Gas-phase {{concentrations}} of semi-volatile organic compounds (SVOCs) were calculated from gas/particle (G/P) partitioning theory using their measured particle-phase concentrations. The particle-phase {{data were obtained}} from an existing filter measurement campaign (27 January 2003 – 2 October 2005) {{as a part of}} the Denver Aerosol Sources and Health (DASH) study, including 970 observations of 71 SVOCs (Xie et al., 2013). In each compound class of SVOCs, the lighter species (e. g. docosane in n alkanes, fluoranthene in PAHs) had higher total concentrations (gas + particle phase) and lower particle-phase fractions. The total SVOC concentrations were analyzed using positive matrix factorization (PMF). Then the results were compared with source apportionment results where only particle-phase SVOC concentrations were used (particle only-based study; Xie et al., 2013). For the particle only-based PMF analysis, the factors primarily associated with primary or secondary sources (n alkane, EC/sterane and inorganic ion factors) exhibit similar contribution time series (r = 0. 92 – 0. 98) with their corresponding factors (n alkane, sterane and nitrate + sulfate factors) in the current work. Three other factors (light n alkane/PAH, PAH and summer/odd n alkane factors) are linked with pollution sources influenced by atmospheric processes (e. g. G/P partitioning, photochemical reaction), and were less correlated (r = 0. 69 – 0. 84) with their corresponding factors (light SVOC, PAH and bulk carbon factors) in the current work, suggesting that the source apportionment results derived from particle-only SVOC data could be affected by atmospheric processes. PMF analysis was also performed on three temperature-stratified subsets of the total SVOC data, representing ambient sampling during cold (daily average temperature 20 °C) periods. Unlike the particle only-based study, in this work the factor characterized by the low molecular weight (MW) compounds (light SVOC factor) exhibited strong correlations (r = 0. 82 – 0. 98) between the full data set and each <b>sub-data</b> set solution, indicating that the impacts of G/P partitioning on receptor-based source apportionment could be eliminated by using total SVOC concentrations...|$|E
40|$|The aim of {{this study}} was to predict the beef carcass and LM (thoracis part) {{characteristics}} and the sensory properties of the LM from rearing factors applied during the fattening period. Individual data from 995 animals (688 young bulls and 307 cull cows) in 15 experiments were used to establish prediction models. The data concerned rearing factors (13 variables), carcass characteristics (5 variables), LM characteristics (2 variables), and LM sensory properties (3 variables). In this study, 8 prediction models were established: dressing percentage and the proportions of fat tissue and muscle in the carcass to characterize the beef carcass; cross-sectional area of fibers (mean fiber area) and isocitrate dehydrogenase activity to characterize the LM; and, finally, overall tenderness, juiciness, and flavor intensity scores to characterize the LM sensory properties. A random effect was considered in each model: the breed for the prediction models for the carcass and LM characteristics and the trained taste panel for the prediction of the meat sensory properties. To evaluate the quality of prediction models, 3 criteria were measured: robustness, accuracy, and precision. The model was robust when the root mean square errors of prediction of calibration and validation <b>sub-data</b> sets were near to one another. Except for the mean fiber area model, the obtained predicted models were robust. The prediction models were considered to have a high accuracy when the mean prediction error (MPE) was ≤ 0. 10 and to have a high precision when the was the closest to 1. The prediction of the characteristics of the carcass from the rearing factors had a high precision (> 0. 70) and a high prediction accuracy (MPE 0. 10). Only the flavor intensity of the beef score could be satisfactorily predicted from the rearing factors with high precision (= 0. 72) and accuracy (MPE = 0. 10). All the prediction models displayed different effects of the rearing factors according to animal categories (young bulls or cull cows). In consequence, these prediction models display the necessary adaption of rearing factors during the fattening period according to animal categories to optimize the carcass traits according to animal categories...|$|E
40|$|The Denver Aerosol Sources and Health (DASH) study aims to {{identify}} and quantify the sources of PM 2. 5 {{that are related to}} negative health outcomes. The positive matrix factorization (PMF), a multivariate receptor model, was used as the primary tool for source apportionment of PM 2. 5 based on particulate speciation data. However, several questions need to be addressed on the receptor-based source apportionment of PM 2. 5. In DASH study, 24 -h PM 2. 5 samples were collected at one centrally located site in Denver. This {{raises the question of whether}} the heterogeneity in PM 2. 5 sources or source contributions across the urban area might lead to biased health effects estimation. In this work, PM 2. 5 samples were collected at four urban sites in Denver for one year. The carbonaceous speciation data were used as inputs for PMF analysis. The results showed that the four sampling sites have consistent source profiles and similar source distribution of elemental carbon (EC) and organic carbon (OC). The speciation of PM 2. 5 in the DASH study includes inorganic ions, EC and OC, organic molecular markers (OMMs) and water soluble elements (WSEs). To evaluate the utility of different speciation data sets for source apportionment of bulk PM 2. 5 species, different combinations of source tracers with bulk PM 2. 5 species were applied for PMF analysis. The results suggested that OMMs were better source tracers for EC and OC than WSEs. However, OMMs are mostly semi-volatile organic compounds (SVOCs), and their particle-phase fractions are impacted by gas/particle (G/P) partitioning. In this work, a 32 -month series of PM 2. 5 speciation data was available for PMF analysis. The influence of G/P partitioning was identified by the comparison of PMF analysis of the full data set versus temperature-stratified <b>sub-data</b> sets. With the prediction of gas-phase SVOC concentrations by an equilibrium absorption model, the PMF analysis using total SVOC (gas + particle phase) data set showed consistent results between the full data set and temperature-stratified sub sets. A 1 -year field study of both gas- and particle-phase SVOCs was conducted to verify the gas-phase SVOCs prediction. The observed G/P partitioning of SVOCs was reasonably consistent with that predicted by an equilibrium absorption model...|$|E
40|$|This {{study was}} carried out to compare the fasting plasma glucose (FPG) and 2 -h plasma glucose (2 -h PG) {{criteria}} for diabetes {{with regard to their}} relation to stroke mortality and the incidence of ischemic and hemorrhagic stroke. In addition, the age-and gender difference in the incidence of coronary heart disease (CHD) and stroke and their relation with known cardiovascular disease risk factors and diabetes mellitus was examined. The study was a <b>sub-data</b> analysis of the Diabetes Epidemiology: Collaborative analysis Of Diagnostic criteria in Europe (DECODE) study including 25 181 individuals, 11 844 (47 %) men and 13 345 (53 %) women aged 25 to 90 years, from 14 European cohorts. In individuals without a history of diabetes elevated 2 -h post-challenge glucose was a better predictor of stroke mortality than elevated fasting glucose in men, whereas the latter was better than the former in women. Elevated FPG and 2 -h PG levels were associated with an increased risk of ischemic stroke incidence. 2 -h PG contributed to the risk more strongly than FPG. No relationship between hyperglycemia and the risk of hemorrhagic stroke was found. The risk of CHD and ischemic stroke incidence increased with age in both genders, but was higher in all age groups in men than in women. The gender difference was, however, more marked for CHD than for ischemic stroke. Age, smoking and diabetes contributed to the development of both CHD and ischemic stroke. Elevated cholesterol levels predicted CHD only, whereas elevated blood pressure was a risk predictor for the incidence of ischemic stroke. The CHD and ischemic stroke risk was higher in men than in women with and without diabetes, however, the gender difference diminished for CHD but enlarged for ischemic stroke in diabetic individuals. The known risk factors including diabetes contributed differently to the risk of CHD and ischemic stroke in women and in men. Hyperglycemia defined by FPG or 2 -h PG increases the risk of ischemic stroke in individuals without diabetes. FPG better predicts stroke mortality in women and 2 -h PG in men. The risk of acute CHD and ischemic stroke is higher in men than in women in all ages, but such gender difference is more marked for CHD than for ischemic stroke. CHD risk is higher in men than in women, but the difference is reduced in diabetic population. Diabetes, however, increases stroke risk more in men than in women in all ages. Ei saatavill...|$|E

