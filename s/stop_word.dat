107|474|Public
50|$|During the Pit <b>Stop,</b> <b>word</b> of a {{sandstorm}} in the Sahara {{forced the}} teams {{to move to}} Gabès to start the leg for safety. At {{the start of the}} leg, the teams traveled to the Palace Hotel in Tunis for their next clue.|$|E
5000|$|In 2008, the New York Times {{reported}} that Johns Hopkins University had instructed the POPLINE search engine {{to ignore the}} search term [...] "abortion", as it ignores search terms such as [...] "a" [...] and [...] "the", in February 2008 in response to pressure from the United States Agency for International Development. Making the term a <b>stop</b> <b>word</b> removed the ability of users to search for this common term for a reproductive health issue.|$|E
40|$|Abstract: In modern {{information}} retrieval systems, effective indexing {{can be achieved}} by removal of stop words. Till now many <b>stop</b> <b>word</b> lists have been developed for English language. However, no standard <b>stop</b> <b>word</b> list has been constructed for Chinese language yet. With the fast development of {{information retrieval}} in Chinese language, exploring Chinese <b>stop</b> <b>word</b> lists becomes critical. In this paper, to save the time and release the burden of manual <b>stop</b> <b>word</b> selection, we propose an automatic aggregated methodology based on statistical and information models for extraction of a <b>stop</b> <b>word</b> list in Chinese language. Result analysis shows that our stop list is comparable with a general English <b>stop</b> <b>word</b> list, and our list is much more general than other Chinese stop lists as well. Our <b>stop</b> <b>word</b> extraction algorithm is a promising technique, which saves the time for manual generation and constructs a standard. It could be applied into other languages in the future. Key-Words: <b>stop</b> <b>word</b> list, statistical modeling, information theory...|$|E
30|$|<b>Stops</b> <b>words</b> {{are words}} {{that do not}} contain {{important}} significance for building the model. Some example <b>stop</b> <b>words</b> include the, at, like, etc. We remove <b>stop</b> <b>words</b> from all the tokenized email text.|$|R
30|$|Removal of <b>stop</b> <b>words</b> <b>Stop</b> <b>words</b> {{are very}} common and {{high-frequency}} words. This process {{carried out by}} removing frequently used <b>stop</b> <b>words</b> (prepositions, irrelevant words, special character, ASCII code), new line, extra white spaces etc. to enhance the performance of feature selection technique.|$|R
30|$|Research {{agrees that}} unigrams with removal of <b>stop</b> <b>words</b> and {{stemming}} {{provide the best}} results for LSA [14]. Nevertheless, when we use bigrams, the <b>stop</b> <b>words</b> take {{on the role of}} “function words”; as so, we opted to count the <b>stop</b> <b>words.</b>|$|R
40|$|In modern {{information}} retrieval systems, effective indexing {{can be achieved}} by removal of stop words. Till now many <b>stop</b> <b>word</b> lists have been developed for English language. However, no standard <b>stop</b> <b>word</b> list has been constructed for Chinese language yet. With the fast development of {{information retrieval}} in Chinese language, exploring the evaluation of Chinese <b>stop</b> <b>word</b> lists becomes critical. In this paper, to save the time and release the burden of manual comparison, we propose a novel <b>stop</b> <b>word</b> list evaluation method with a mutual information-based Chinese segmentation methodology. Experiments have been conducted on training texts taken from a recent international Chinese segmentation competition. Results show that effective <b>stop</b> <b>word</b> lists can improve the accuracy of Chinese segmentation significantly. 1...|$|E
3000|$|<b>Stop</b> <b>word</b> removal The stop {{words are}} used {{frequently}} in natural language. These include ‘is’, ‘to’, ‘for’, ‘an’, ‘are’, ‘in’ and, ‘at’. The <b>stop</b> <b>word</b> elimination plays {{a pivotal role}} for dimensionality reduction of the text for further analysis. It assists {{in the identification of}} the remaining key words in the natural language becomes easy, and subsequent analysis can be performed efficiently. A list compiled by Savoy (2005), contains vast collection of stop words. The <b>stop</b> <b>word</b> elimination process start with the selection of words and ends by discarding such words from the text. In this work, we propose python-based algorithm for stop words removal process shown as follows: [...]...|$|E
30|$|The {{pre-processing}} module is used {{to clean}} the noisy text by applying different steps, such as tokenization, <b>stop</b> <b>word</b> removal, lemmatization, co-reference resolution, spell correction, and case-conversion.|$|E
50|$|In computing, <b>stop</b> <b>words</b> {{are words}} which are {{filtered}} out {{before or after}} processing of natural language data (text). Though <b>stop</b> <b>words</b> usually refer to the most common words in a language, {{there is no single}} universal list of <b>stop</b> <b>words</b> used by all natural language processing tools, and indeed not all tools even use such a list. Some tools specifically avoid removing these <b>stop</b> <b>words</b> to support phrase search.|$|R
30|$|Removing <b>stops</b> <b>words</b> {{and small}} words.|$|R
30|$|Tweet: we {{represent}} a tweet as bag of words. We removed all the <b>stop</b> <b>words</b> (based {{on the standard}} INQUERY stop list). The final representation is a clean tweet without <b>stop</b> <b>words</b> or useless words. We used t as a symbol for tweet object.|$|R
40|$|We use the Okapi {{retrieval}} system {{to conduct the}} email discussion search. The following issues are investigated. First, we {{make use of the}} thread structure in the emails to re-rank the documents retrieved by Okapi. We would like to see whether such post-processing of the retrieval result can boost the retrieval performance. Second, in terms of query formulation, we investigate whether the use of only title in a topic achieves better or worse results than the inclusion of other fields such as description and narrative. Third, we investigate whether stemming and <b>stop</b> <b>word</b> removal {{play an important role in}} the email search. Our conclusion includes that (1) re-ranking documents using a straightforward method that considers the thread structure can make a small improvement to the retrieval performance, (2) formulating the query using all the fields in a topic achieves the best result, and (3) the use of stemming and <b>stop</b> <b>word</b> removal can improve the performance, but the degree of improvement depends on the stemming method and the <b>stop</b> <b>word</b> list used. ...|$|E
40|$|Pairwise {{similarity}} judgement {{correlations between}} humans and Latent Semantic Analysis (LSA) were explored {{on a set of}} 50 news documents. LSA is a modern and commonly used technique for automatic determination of document similarity. LSA users must choose local and global weighting schemes, the number of factors to be retained, <b>stop</b> <b>word</b> lists and whether to background. Global weighting schemes had more effect than local weighting schemes. Use of a <b>stop</b> <b>word</b> list almost always improved performance. Introduction of a background set of similar documents increased larger correlations and reduced smaller ones. The correlations ranged between approximately 0 and 0. 6 depending on the LSA settings indicating the importance of correct settings. The low maximum correlation indicates that information presentation schemes based on LSA may often be at variance with visualisation...|$|E
40|$|The {{primary purpose}} of an {{information}} retrieval system is to retrieve all the relevant documents, which {{are relevant to the}} user query. The Latent Semantic Indexing (LSI) based ad hoc document retrieval task investigates the performance of retrieval systems that search a static set of documents using new questions/queries. Performance of LSI has been tested for several smaller datasets (e. g., MED, CISI abstracts etc) however, LSI has not been tested for a large dataset. In this research, we concentrated on the performance of LSI on large dataset. <b>Stop</b> <b>word</b> list and term weighting schemes are two key parameters in the area of information retrieval. We investigated the performance of LSI by using three different set of <b>stop</b> <b>word</b> lists and, also, without removing the stop words from the test collection. We also applied three different term-weighting (raw term frequency, log-entropy, and tf-idf) schemes to measure retrieval performance of LSI. We observed that, firstly, for a LSI based ad hoc information retrieval system, a tailored <b>stop</b> <b>word</b> list must be assembled for every unique large dataset. Secondly, the use of tf-idf term weighting scheme shows better retrieval performance than log-entropy and raw term frequency weighting schemes even when the test collection became large. [...] P. ii. The original print copy of this thesis may be available here: [URL]...|$|E
50|$|<b>Stop</b> <b>Words,</b> Ormond, Vic.: Hybrid Publishers, 2011. 87pp.|$|R
3000|$|F(t) denotes {{whether the}} word t {{exists in the}} word bag or not, and is mainly decided {{according}} to the part of speech and <b>stopping</b> <b>words.</b> If the word t is a <b>stopping</b> <b>word</b> and {{in the part of}} speech that does not belong to verb, noun, and adjective, L [...]...|$|R
50|$|This is {{different}} from harmless but useless words that are called <b>stop</b> <b>words.</b>|$|R
40|$|This paper {{describes}} {{an approach to}} find automatically new technological ideas in textual information. On the basis of (Thorleuchter (2008)) the existing theoretical algorithm is enlarged in consideration of text mining approaches like stemming, term frequency etc. (Ferber (2003)) and "creativity technique" approaches from literature (Dean et al. (2001)). The aim of the new algorithm is to find ideas by using a general <b>stop</b> <b>word</b> list, because up to now the existing approach {{is based on the}} inefficient usage of a (domain) specific <b>stop</b> <b>word</b> list specific created for the analyzed text. This new approach is evaluated with non-proprietary data and it is realized as web-based application, named "Technological Idea Miner" {{that can be used for}} further testing and evaluation. The presentation of the identified ideas will be displayed in consideration of cognitive research knowledge like described in (Puppe et al. (2003)) ...|$|E
3000|$|Calibration was {{performed}} during {{the running of}} the experiment: it was feasible to use an approach that found the best possible values for each parameter, for the best accuracy. On preprocessing, variations were considered for the entries of A: a) Counting all the stop words b) Removing all the stop words c) Removing all the <b>stop</b> <b>word</b> plus a stemming process [...]...|$|E
40|$|Abstract — Document {{clustering}} {{is automatically}} group related document into cluster. In this clustering frame work focus on {{correlations between the}} documents in the local patches are maximized while {{the correlations between the}} documents outside these patches are minimized simultaneously. The proposed systems are adopts both supervised and unsupervised constraints to demonstrate the effectiveness of the proposed algorithm in this framework. The novel proposed classical K-Mean clustering algorithm applied for data preprocessing in <b>stop</b> <b>word</b> removal, stemming and synonym word replacement to apply semantic similarity between words in the documents. In addition, content can be retrieved from text files, HTML pages as well as XML pages. Tags are eliminated from HTML files. Attribute name and values are taken as normal paragraph words in XML files and then preprocessing (<b>stop</b> <b>word</b> removal, stemming and synonym word replacement) is applied. In addition, TEXT, HTML and XML documents are cluster using cosine similarity model implement these research works...|$|E
30|$|The mainly medical key phrases are extracted, and the <b>stop</b> <b>words</b> {{are removed}} from a given medical topic.|$|R
40|$|Abstract — Many {{words in}} {{documents}} recur very frequently but are essentially meaningless {{as they are}} used to join words together in a sentence. It is commonly understood that <b>stop</b> <b>words</b> do not contribute to the context or content of textual documents. Due to their high frequency of occurrence, their presence in text mining presents an obstacle {{to the understanding of the}} content in the documents. To eliminate the bias effects, most text mining software or approaches make use of <b>stop</b> <b>words</b> list to identify and remove those words. However, the development of such top words list is difficult and inconsistent between textual sources. This problem is further aggravated by sources such as Twitter which are highly repetitive or similar in nature. In this paper, we will be examining the original work using term frequency, inverse document frequency and term adjacency for developing a <b>stop</b> <b>words</b> list for the Twitter data source. We propose a new technique using combinatorial values as an alternative measure to effectively list out <b>stop</b> <b>words.</b> Keywords- <b>Stop</b> words; Text mining; RAKE; ELFS; Twitter. I...|$|R
40|$|Many {{words in}} {{documents}} recur very frequently but are essentially meaningless {{as they are}} used to join words together in a sentence. It is commonly understood that <b>stop</b> <b>words</b> do not contribute to the context or content of textual documents. Due to their high frequency of occurrence, their presence in text mining presents an obstacle {{to the understanding of the}} content in the documents. To eliminate the bias effects, most text mining software or approaches make use of <b>stop</b> <b>words</b> list to identify and remove those words. However, the development of such top words list is difficult and inconsistent between textual sources. This problem is further aggravated by sources such as Twitter which are highly repetitive or similar in nature. In this paper, we will be examining the original work using term frequency, inverse document frequency and term adjacency for developing a <b>stop</b> <b>words</b> list for the Twitter data source. We propose a new technique using combinatorial values as an alternative measure to effectively list out <b>stop</b> <b>words...</b>|$|R
40|$|Due to {{the great}} {{variation}} of biological names in biomedical text, appropriate tok-enization is an important preprocessing step for biomedical information retrieval. De-spite its importance, {{there has been little}} study on the evaluation of various tokeniza-tion strategies for biomedical text. In this work, we conducted a careful, systematic evaluation of a set of tokenization heuristics on all the available TREC biomedical text collections for ad hoc document retrieval, using two representative retrieval methods and a pseudo relevance feedback method. We also studied the effect of stemming and <b>stop</b> <b>word</b> removal on the retrieval performance. As expected, our experiment results show that tokenization can significantly affect the retrieval accuracy; appropriate to-kenization can improve the performance by up to 96 %, measured by mean average precision (MAP). In particular, it is shown that different query types require different tokenization heuristics, stemming is effective only for certain queries, and <b>stop</b> <b>word</b> removal in general does not improve the retrieval performance on biomedical text...|$|E
40|$|In this work, we {{investigate}} possible benefits of {{natural language processing}} tools, as means to support automated text categorization. Our corpus consists of a small collection of categorized Danish web pages {{in the fields of}} art, architecture, and design. The natural language processing techniques we examine are <b>stop</b> <b>word</b> removal, removal of functional words, and lemmatization. The tools are based on a <b>stop</b> <b>word</b> list, a part-of-speech tagger and a dictionary. We evaluate effects on a string matching classifier and a support vector machine. The classification accuracy increases when using the lemmata, either in addition to or replacing the original inflected words in the documents. Positive effects are seen on both precision and recall. In absence lemmatization, the removal of stop words increases classifier performance, although not as much. Results are valid both for support vector machine, and string matching categorization. Acknowledgments First of all, I would like to thank my main supervisor Pierre Nugues, fo...|$|E
30|$|In the {{preprocessing}} procedure, each {{document in}} the document set implements vocabulary, analyzes words, and filters numbers, hyphens, and punctuations. Using a <b>stop</b> <b>word</b> list removes function words to leave useful words such as noun and verb [16]. Extracting stem words and removing the prefix and postfix improve the accuracy of retrieval. Finally, determining certain words as an index element expresses literature content conception.|$|E
30|$|Pre-processing This {{technique}} {{is required to}} remove noisy, inconsistent and incomplete information by considering tokenization, <b>stop</b> <b>words</b> removal, stemming method.|$|R
50|$|As in {{the example}} above, it is {{possible}} to display the output as natural text (re-applying inflexion, adding <b>stop</b> <b>words).</b>|$|R
40|$|AbstractData {{pre-processing}} {{presents the}} most time consuming phase in {{the whole process of}} knowledge discovery. The complexity of data pre-processing depends on the data sources used. The aim of this work is to determine to what extent it is necessary to carry out the time consuming data pre-processing in the process of discovering sequential patterns in e-documents. We used the transaction/sequence model for text representation and sequence rule analysis as a method of modelling. We compare four datasets of different quality obtained from texts and pre-processed in different ways: data with identified the paragraph sequences, data with identified the sentence sequences, data with identified the paragraph sequences without <b>stop</b> <b>words</b> and data with identified the sentence sequences without <b>stop</b> <b>words.</b> We try {{to assess the impact of}} these advanced techniques of data pre-processing on the quantity and quality of the extracted rules. The results confirm some initial assumptions, but they also show that the <b>stop</b> <b>words</b> removal has a substantial impact on the quantity and quality of extracted rules in case of paragraph sequence identification. Contrary, in case of sentence sequence identification, removing the <b>stop</b> <b>words</b> has not any significant impact on the quantity and quality of extracted rules...|$|R
40|$|Abstract This paper {{describes}} {{our approach}} to the Detailed Analysis subtask of the PAN 2012 competition. Our experiments deal with monolingual plagiarism cases, only. We use a simple set-based algorithm, that employs Dice’s coefficient as a similarity measure. Furthermore we employ basic strategies from Information Retrieval and Natural Language Processing for <b>stop</b> <b>word</b> removal and language detection. We achieved the 7 th place with an overall PlagDet score of 0. 35. ...|$|E
40|$|This paper {{presents}} a dictionary-based query translation and construction method for CLIR. The framework focuses on solving morphological and lexical problems in CLIR generally between many language pairs. An application {{based on this}} method, the extendable UTACLIR system, capable of performing query translations between several source and target language pairs, with a relatively easy interface to change and add new external resources like morphological normalisers, dictionaries and <b>stop</b> <b>word</b> lists will be available free to academic users. ...|$|E
30|$|Initially, text {{documents}} {{which have}} been collected from various sources were accumulated in a database. Then, pre-processing {{was carried out by}} considering the various stages like: tagging by means of Stanford POS tagger tool, <b>stop</b> <b>word</b> removal and stemming, based on Porter Stemmer algorithm and morphological capabilities of WordNet. The above preprocessing is common for both existing and proposed algorithms considered in this study. Then the documents are represented as VSM. These documents are clustered using Bisecting K-means algorithm which generates K number of clusters.|$|E
40|$|To perform {{document}} classification algorithmically, documents need to {{be represented}} such that it is understandable to the machine learning classifier. The report discusses {{the different types of}} feature vectors through which document can be represented and later classified. The project aims at comparing the Binary, Count and TfIdf feature vectors and their impact on document classification. To test how well each of the three mentioned feature vectors perform, we used the 20 -newsgroup dataset and converted the documents to all the three feature vectors. For each feature vector representation, we trained the Naïve Bayes classifier and then tested the generated classifier on test documents. In our results, we found that TfIdf performed 4 % better than Count vectorizer and 6 % better than Binary vectorizer if <b>stop</b> <b>words</b> are removed. If <b>stop</b> <b>words</b> are not removed, then TfIdf performed 6 % better than Binary vectorizer and 11 % better than Count vectorizer. Also, Count vectorizer performs better than Binary vectorizer, if <b>stop</b> <b>words</b> are removed by 2 % but lags behind by 5 % if <b>stop</b> <b>words</b> are not removed. Thus, we can conclude that TfIdf should be the preferred vectorizer for document representation and classification...|$|R
30|$|Porter stemmer [36] {{has been}} applied to both {{collections}} and <b>stop</b> <b>words</b> [37] and words with less than three characters are also removed.|$|R
5000|$|STAIRS {{provided}} good search {{performance by}} indexing every {{word in a}} document except user-selectable <b>stop</b> <b>words,</b> usually common words such as [...] "and" [...] or [...] "the." ...|$|R
