5|10000|Public
40|$|This article {{describes}} and illustrates use of random-effects regression models (RRM) {{to examine the}} natural history of smoking from adolescence to adulthood. For longitudinal data analysis, RRM are useful because they allow for the presence of missing data, time-varying or invariant covariates, and subjects measured at different timepoints. Thus, a key advantage of RRM is that it can accomodate "unbalanced" longitudinal data, where a sample of subjects are not all measured at each and every timepoint. Also, variants of RRM have been developed to model dichotomous and ordinal outcomes, which are common in substance use research. A pattern-mixture approach (Little, 1995) can also be accommodated within RRM to further handle and describe the influence of missing data in longitudinal <b>studies.</b> <b>For</b> <b>this</b> <b>approach,</b> subjects are first divided into groups depending on their missingdata pattern, and then variables based on these groups are used as model covariates. Researchers are then able to examine the effect of missing-data patterns on the outcome(s) of interest. In this article we will illustrate these methods using an example from a study examining smoking status from early adolescence to young adulthood...|$|E
40|$|Random-effects {{regression}} models {{have become increasingly}} popular for analysis of longitudinal data. A key advantage of the random-effects approach {{is that it can}} be applied when subjects are not measured at the same number of timepoints. In this article we describe use of random-effects pattern-mixture models to further handle and describe the influence of missing data in longitudinal <b>studies.</b> <b>For</b> <b>this</b> <b>approach,</b> subjects are first divided into groups depending on their missing-data pattern and then variables based on these groups are used as model covariates. In this way, researchers are able to examine the effect of missing-data patterns on the outcome (or outcomes) of interest. Furthermore, overall estimates can be obtained by averaging over the missing-data patterns. A psychiatric clinical trials data set is used to illustrate the random-effects pattern-mixture approach to longitudinal data analysis with missing data. Longitudinal studies occupy an important role in psychological and psychiatric research. In these studies the same individuals are repeatedly measured on a number of important variables over a series of timepoints. As an example, a longitudinal design is often used to determine whether a particular therapeutic agent can produce changes in clinical status over the course of an illness. Another application for the longitudinal study is to assess potential indicators of a change, in the subject's clinical status; for example, the assessment of whether drug plasma level measurements indicate clinical outcome. Even in well-controlled situations, missing data invariably occur in longitudinal studies. Subjects can b...|$|E
40|$|Social {{neuroscience}} {{benefits from}} the experimental manipulation of neuronal activity. One possible manipulation, neurofeedback, is an operant conditioning-based technique in which individuals sense, interact with, and manage their own physiological and mental states. Neurofeedback {{has been applied to}} a wide variety of psychiatric illnesses, as well as to treat sub-clinical symptoms, and even to enhance performance in healthy populations. Despite growing interest, there persists a level of distrust and/or bias in the medical and research communities in the USA toward neurofeedback and other functional interventions. As a result, neurofeedback has been largely ignored, or disregarded within social neuroscience. We propose a systematic, empirically-based approach for assessing the effectiveness, and utility of neurofeedback. To that end, we use the term perturbative physiologic plasticity to suggest that biological systems function as an integrated whole that can be perturbed and guided, either directly or indirectly, into different physiological states. When the intention is to normalize the system, e. g., via neurofeedback, we describe it as self-directed neuroplasticity, whose outcome is persistent functional, structural, and behavioral changes. We argue that changes in physiological, neuropsychological, behavioral, interpersonal, and societal functioning following neurofeedback can serve as objective indices and as the metrics necessary for assessing levels of efficacy. In this chapter, we examine the effects of neurofeedback on functional connectivity in a few clinical disorders as case <b>studies</b> <b>for</b> <b>this</b> <b>approach.</b> We believe this broader perspective will open new avenues of investigation, especially within social neuroscience, to further elucidate the mechanisms and effectiveness of these types of interventions, and their relevance to basic research...|$|E
40|$|We {{propose a}} new {{distributed}} algorithm for computing a truncated Newton method, where the main diagonal of the Hessian is computed using belief propagation. As a case <b>study</b> <b>for</b> <b>this</b> <b>approach,</b> {{we examine the}} sensor selection problem, a Boolean convex optimization problem. We form two distributed algorithms. The first algorithm is a distributed version of the interior point method by Joshi and Boyd, and the second algorithm is {{an order of magnitude}} faster approximation. As an example application we discuss distributed anomaly detection in networks. We demonstrate the applicability of our solution using both synthetic data and real traffic logs collected from the Abilene Internet backbone. Comment: Submitted for publicatio...|$|R
30|$|Recently complex {{approximation}} operators {{have been}} <b>studied</b> intensively. <b>For</b> <b>this</b> <b>approach,</b> {{we refer to}} the book of Gal [1], where he considers approximation properties of several complex operators such as Bernstein, q-Bernstein, Favard-Szasz-Mirakjan, Baskakov and some others. Also we refer to the useful book of Aral, Gupta and Agarwal [2] who consider many applications of q-calculus in approximation theory. Now, {{for the construction of}} the new operators, we give some notations on q-analysis [3, 4].|$|R
40|$|Objectives: Real-time {{surveillance}} {{systems are}} valuable for timely response {{to public health}} emergencies. It has been challenging to leverage existing surveillance systems {{in state and local}} communities, and, using a centralized architecture, add new data sources and analytical capacity. Because this centralized model has proven to be difficult to maintain and enhance, the US Centers for Disease Control and Prevention (CDC) has been examining the ability to use a federated model based on secure web services architecture, with data stewardship remaining with the data provider. Methods: As a case <b>study</b> <b>for</b> <b>this</b> <b>approach,</b> the American Association of Poison Control Centers and the CDC extended an existing data warehouse via a secure web service, and shared aggregate clinical effects and case counts data by geographic region and time period. To visualize these data, CDC developed a web browser-based interface, Quicksilver, which leveraged the Google Maps API and Flot, a javascript plotting library. Results: Two iterations of the NPDS web service were completed in 12 weeks. The visualization client, Quicksilver, was developed in four months...|$|R
40|$|In {{the last}} decade, optogenetics has revolutionised the neurosciences. The technique, {{which allows for}} {{cell-type}} specific excitation and inhibition of neurons in the brain of freely moving rodents, {{has been used to}} tighten the links of causality between neural activity and behaviour. Optogenetics is also enabling an unprecedented characterisation of circuits and their dysfunction in a number of brain diseases, above all those conditions that are not caused by neurodegeneration. Notable {{progress has been made in}} addiction, depression and obsessive-compulsive disorders, as well as other anxiety disorders. By extension, the technique has also been used to propose blueprints for innovative rational treatment of these diseases. The goal is to design manipulations that disrupt pathological circuit function or restore normal activity. This can be achieved by targeting specific projections in order to apply specific stimulation protocols validated by ex-vivo analysis of the mechanisms underlying the dysfunction. In a number of cases, specific forms of pathological synaptic plasticity have been implicated. For example, addictive drugs via strong increase of dopamine trigger a myriad of alterations of glutamate and γ-aminobutyric acid transmission, also called drug-evoked synaptic plasticity. This opens the way to the design of optogenetic reversal protocols, which might restore normal transmission with the hope to abolish the pathological behaviour. Several proof of principle <b>studies</b> <b>for</b> <b>this</b> <b>approach</b> have recently been published. However, for many reasons, optogenetics will not be translatable to human applications in the near future. Here, we argue that an intermediate step is novel deep brain stimulation (DBS) protocols that emulate successful optogenetic approaches in animal models. We provide a roadmap for a translational path to rational, optogenetically inspired DBS protocols to refine existing approaches and expand to novel indications...|$|E
40|$|Win-Yan Chan, Jonathan M Cohen, Jeremy S Brown Centre for Inflammation and Tissue Repair, UCL Respiratory, Division of Medicine, University College London, London, UK Abstract: Pathogens {{that can}} colonize the upper {{respiratory}} tract include Streptococcus pneumoniae, Hemophilus influenzae, Neisseria meningitidis, Moraxella catarrhalis, and Staphylococcus aureus. While these pathogens commonly asymptomatically colonize the nasopharynx of healthy adults, disease progression may occur in some individuals. In addition to these respiratory pathogens, there are {{a large number of}} commensal species also found in the {{upper respiratory tract}} which only very rarely cause disease, creating a complex community of bacterial species in the nasopharynx. This review addresses the novel, potential strategies that utilize the interactions between both homologous and heterologous species in the nasopharynx to vaccinate individuals against pathogenic bacteria. These strategies include the mechanisms employed by colonizing bacteria to regulate the presence of other species in the nasopharynx and the effect that colonization of the nasopharynx has on the host immune response. Interventional strategies investigated so far include the introduction of nonpathogenic bacteria to the nasopharynx to immunize against a closely related species, controlled colonization using both wild-type and attenuated species, and the use of other nonpathogenic colonizers to express antigens from potential pathogens. All these approaches harness the ability of the colonization to induce a mucosal immune response that can protect against future infection. In this review, S. pneumoniae and N. meningitidis colonization are used as case <b>studies</b> <b>for</b> <b>this</b> <b>approach</b> as the immunological effects of colonization have been widely studied in animal and human models. Colonization-based strategies have great potential, and, in particular, the attenuated strain approach has produced some encouraging data in animal models. However, the strategy for attenuating virulence must be stringent and caused by highly stable mutations that are unlikely to revert. In addition, the consequences of artificial administration of genetically modified bacteria to the nasopharynx on the usual host microbiome are unknown and would need to be monitored carefully. Keywords: Streptococcus pneumoniae, colonization, adaptive immunity, antibody, protein antigen, capsular antigen, Neisseria sp...|$|E
40|$|In this work, we {{introduce}} EdgeMaps as a {{new method}} for integrating the visualization of explicit and implicit data relations. Explicit relations are specific connections between entities already present in a given dataset, while implicit relations are derived from multidimensional data based on shared properties and similarity measures. Many datasets include both explicit and implicit relations, which are often difficult to represent together in information visualizations. Node-link diagrams typically focus on explicit data connections, while not incorporating implicit similarities between entities. Multi-dimensional scaling considers similarities between items, however, explicit links between nodes are not displayed. In contrast, EdgeMaps visualize both implicit and explicit relations by combining and complementing spatialization and graph drawing techniques. As a case <b>study</b> <b>for</b> <b>this</b> <b>approach</b> we chose a dataset of philosophers, their interests, influences, and birthdates. By introducing the limitation of activating only one node at a time, interesting visual patterns emerge that resemble the aesthetics of fireworks and waves. We argue that the interactive exploration of these patterns enables the viewer to grasp {{the structure of a}} graph better than complex graph visualizations...|$|R
40|$|The {{cerebellar}} anlage {{is subject}} to a multifactorial process, with intrinsic and extrinsic factors, to acquire a complex adult morphological structure. The spectacular cortical development provokes dramatic changes in shape {{and size of the}} initial structure, that course with the formation of folia and fissures. The study of these transformations from a global point of view is the goal of the present <b>study.</b> <b>For</b> <b>this</b> <b>approach,</b> midsagittal sections of chick embryonic cerebella from stages HH 36 to HH 44 were used to obtain sets of landmarks analyzed by geometric morphometrics. This tool exploits not only the morphology per se, but the spatial relationships of anatomical structures in a quantitative and visual way. Preliminary results indicate that changes in the cerebellum are mainly observed at the transition between stages HH 37 -HH 38, HH 39 -HH 40 and HH 40 -HH 41. The observed changes are summarized: 1) - the cavum ventricularis is progressively reduced to be converted into a cleft; 2) - centrifugal expansion of the folia, specially from HH 40 onwards; 3) - fixation of the base of the fissures to the cerebellum core during the centrifugal expansion of the folia. No data (2008...|$|R
30|$|Zhang et al. (2012) and Bashir et al. (2013) {{reported}} that the Cd concentration was also increased in VIT knockdown rice. Therefore, <b>this</b> <b>approach</b> should be avoided in Cd-contaminated soils. Although further <b>studies</b> are required <b>for</b> <b>this</b> <b>approach,</b> likewise approach 5 and 6, approach 7 may also be applied {{in combination with other}} approaches to further increase Fe concentrations in polished seeds.|$|R
40|$|ObjectivesReal-time {{surveillance}} {{systems are}} valuable for timely response {{to public health}} emergencies. It has been challenging to leverage existing surveillance systems {{in state and local}} communities, and, using a centralized architecture, add new data sources and analytical capacity. Because this centralized model has proven to be difficult to maintain and enhance, the US Centers for Disease Control and Prevention (CDC) has been examining the ability to use a federated model based on secure web services architecture, with data stewardship remaining with the data provider. MethodsAs a case <b>study</b> <b>for</b> <b>this</b> <b>approach,</b> the American Association of Poison Control Centers and the CDC extended an existing data warehouse via a secure web service, and shared aggregate clinical effects and case counts data by geographic region and time period. To visualize these data, CDC developed a web browser-based interface, Quicksilver, which leveraged the Google Maps API and Flot, a javascript plotting library. ResultsTwo iterations of the NPDS web service were completed in 12 weeks. The visualization client, Quicksilver, was developed in four months. DiscussionThis implementation of web services combined with a visualization client represents incremental positive progress in transitioning national data sources like BioSense and NPDS to a federated data exchange model. ConclusionQuicksilver effectively demonstrates how the use of secure web services in conjunction with a lightweight, rapidly deployed visualization client can easily integrate isolated data sources for biosurveillance...|$|R
40|$|Understanding {{sources of}} {{variability}} in the archaeological record through the study of ceramic record formation {{is a prerequisite for}} inferring prehistoric human behavior. This study presents a program of investigation that: (1) provides analytical procedures for evaluating the representativeness of data sets so that they may be used to build reliable inferences concerning the past, and (2) provides a methodology for discovering behaviors associated with the occupation and abandonment of a settlement. Chodistaas Ruin (A. D. 1263 - 1290 s), an 18 -room pueblo located in the Grasshopper Region of Arizona, provides an ideal case <b>study</b> <b>for</b> illustrating <b>this</b> <b>approach</b> to {{variability in the}} archaeological record...|$|R
40|$|The {{engineering}} of supervisory controllers {{for large}} and complex cyber-physical systems requires dedicated engineering support. The Compositional Interchange Format language and toolset {{have been developed}} <b>for</b> <b>this</b> purpose. We highlight a model-based engineering framework for the engineering of supervisory controllers and explain how the CIF language and accompanying tools {{can be used for}} typical activities in that framework such as modeling, supervisory control synthesis, simulation-based validation, verification, and visualization, real-time testing, and code generation. We mention a number of case <b>studies</b> <b>for</b> which <b>this</b> <b>approach</b> was used in the recent past. We discuss future developments on the level of language and tools as well as research results that may be integrated in the longer term...|$|R
40|$|Abstract- In an Enterprise environment, {{installation}} and upgrade of software {{is a time}} consuming and nonautomated process. Traditionally end-users download {{the latest version of}} the software and manually update their own computers. There are few commercial software management products available in the market, which automate this process. This paper presents a method to achieve the same using Mobile Agent Technology. This paper presents a case <b>study</b> <b>for</b> applicability of <b>this</b> <b>approach</b> in a grid scenario...|$|R
40|$|The Semantic Web seeks {{integrate}} {{data from}} many different sources. Since different sources often use different names for the same object, we need to map between these names. We first consider the use of keys to do this mapping and discuss some of the associated problems. We introduce the concept of bootstrapping from some shared names to more shared names and discuss some conditions under which this process is guaranteed to be correct. We describe a probabilistic approach to matching and propose approximations {{to address the issue}} of requiring a combinatorially large number of joint probabilities. We report on empirical <b>studies</b> <b>for</b> validating <b>this</b> <b>approach</b> in two interesting domains. Finally, we discuss the implications of better matching techniques for privacy. 1. BACKGROUND The ease with which web sites could link to each other doubtles...|$|R
40|$|In a {{traditional}} information retrieval system, {{it is assumed}} that queries can be posed about any topic. In reality, a large fraction of web queries are posed about {{a relatively small number of}} topics, like products, entertainment, current events, and so on. One way of exploiting this sort of regularity in web search is to build, from the information found on the web, comprehensive databases about specific topics. An appropriate interface to such a database can support complex structured queries which are impossible to answer with traditional topic-independent query methods. Here we discuss three case <b>studies</b> <b>for</b> <b>this</b> "data-centric" <b>approach</b> to web search. A common theme in this discussion is the need for very robust methods for finding relevant information, extracting data from pages, and integrating information taken from multiple sources, and the importance of statistical learning methods as a tool for creating such robust methods. 1 Introduction In {{a traditional}} information retri [...] ...|$|R
40|$|Service-Oriented Architectures (SOAs) {{have become}} an {{important}} cornerstone {{of the development of}} enterprise-scale software applications. Although a range of domain-specific languages and standards are available for dealing with such architectures, model-driven approaches starting from models written in an established modelling language like UML and including the ability for model transformation (in particular, for code generation) are still in their infancy. In this paper, we show (1) how our UML-based domain-specific language for working with SOA artefacts, UML 4 SOA, can be used for modelling service orchestrations, and (2) how to exploit so-designed models in the MDD 4 SOA approach to generate code in multiple languages, among them BPEL and WSDL, Java, and the formal language Jolie. We use a case <b>study</b> <b>for</b> illustrating <b>this</b> <b>approach.</b> Our main contributions are an easy-to-use, conservative extension to the UML 2 for modelling service orchestrations on a high level of abstraction, and a fully automated, model-driven approach for transforming these orchestrations down to code. 1...|$|R
40|$|A {{genome-wide}} association study (GWAS) identifies {{regions of}} the genome that likely affect the variable state of a phenotype of interest. These regions can then be studied with population genetic methods to make inferences about the evolutionary history of the trait. There are increasing opportunities to use GWAS results – even from clinically-motivated <b>studies</b> – <b>for</b> tests of classic anthropological hypotheses. One such example, presented here as a case <b>study</b> <b>for</b> <b>this</b> <b>approach,</b> involves tooth development variation related to dental crowding. Specifically, more than 10 % of humans fail to develop one or more permanent third molars (M 3 agenesis). M 3 presence/absence variation within human populations has a significant genetic component (heritability estimate h 2 = 0. 47). The evolutionary significance of M 3 agenesis {{has a long history}} of anthropological speculation. First, the modern frequency of M 3 agenesis could reflect a relaxation of selection pressure to retain larger and more teeth following the origins of cooking and other food-softening behaviors (i. e., the genetic drift hypothesis, or classically, the “probable mutation effect”). Alternatively, commensurate with increasing hominin brain size and facial shortening, M 3 agenesis may have conferred an adaptive fitness advantage if the risk of M 3 impaction and potential health complications was reduced (i. e., the positive selection hypothesis). A recent GWAS identified 70 genetic loci that may play a role in human M 3 presence/absence variation. To begin evaluating the contrasting evolutionary scenarios for M 3 agenesis, we used the integrated haplotype score (iHS) statistic to test whether those 70 genetic regions are enriched for genomic signatures of recent positive selection. None of our findings are inconsistent with the null hypothesis of genetic drift to explain the high prevalence of human M 3 agenesis. This result might suggest that M 3 impaction rates for modern humans don’t accurately retrodict those of the pre-agricultural past. Alternatively, the absence of support for the positive selection hypothesis could reflect a lack of power; this analysis should be repeated following the completion of more comprehensive GWAS analyses for human M 3 agenesis...|$|R
40|$|Investigations {{concerning}} the new, glycosidated phospholipid analog Inositol-C 2 -PAF {{were conducted in}} <b>this</b> <b>study.</b> <b>For</b> <b>this</b> <b>approach</b> cells of human skin and blood were used. The platelet-activating factor, PAF, is a potent mediator of allergic and inflammatory reactions. Clinical studies point to application possibilities of modified PAF-analogs as means of treating hyperproliferative skin diseases and infections caused by protozoans. Within {{the scope of this}} doctoral thesis it was shown, that Inositol-C 2 -PAF leads to a selective inhibition of cell proliferation of HaCaT- and SCC- 25 -cells at non-toxic concentrations. The anti-proliferative effect follows a time and concentration dependency and lies within the lower micromolar range. In contrast Inositol-C 2 -PAF does not inhibit the proliferation of human fibroblasts and peripheral blood cells. For keratinocytes and carcinoma cells it could be shown that the anti-proliferative effect was not solely due to the induction of apoptosis but that Inositol-C 2 -PAF positively effected epithelial cell differentiation. This positive effect could be demonstrated by the up-regulation of the expression and activity of markers of the terminal differentiation. Furthermore, Inositol-C 2 -PAF strengthens cell-matrix adhesion via the induction of beta 1 -integrin clusters and the up-regulation of the expression of the alpha 6 beta 4 -integrin. This leads to a reorganisation of the cytoskeleton that is characterised by more cortical F-actin fibres. Incubation with Inositol-C 2 -PAF leads to a reduced migration of keratinocytes in both transwell-assays and during wound-healing studies. Using appropriate cDNA constucts and pharmacological inhibitors it could be shown that this inhibition was most likely due to the activation of the small GTPase Cdc 42 and the disturbance of the crosstalk between the small GTPases Cdc 42, Rac 1 and RhoA. Inositol-C 2 -PAF does neither influence the surface expression of stimulated or unstimulated T-cell markers nor the cytokine secretion, although it leads to pro-inflammatory changes within monocytes. Beyond, nanoemulsions containing Inositol-C 2 -PAF were prepared. These nanoemulsions are characterised by a homogenous distribution of particles, with a mean diameter of about 100 nm. Penetration studies using human whole-skin showed that Inositol-C 2 -PAF was metabolically stable and penetrated into 300 &# 956;m depth of the skin...|$|R
40|$|Several {{weaknesses}} {{have been}} identified to the programming-first approach often used in introductory computer science courses. Despite these weaknesses, programming {{continues to be the}} central focus in CS 1 <b>for</b> many institutions. <b>This</b> paper proposes a concept-first approach that can be integrated into existing programming-first curriculum. The approach is based on three principles: a) drawing from the students 2 ̆ 7 everyday experiences to introduce new ideas and skills; b) allowing students time to acquire a foundation in these concepts before introducing a high-level programming language; and c) separating fundamental concepts from language syntax. A feasibility <b>study</b> <b>for</b> integrating <b>this</b> new <b>approach</b> in a CS 1 class at USU is described...|$|R
40|$|AbstractThe {{principle}} of self-consistency has been employed to estimate regression quantile with randomly censored response. The asymptotic <b>studies</b> <b>for</b> <b>this</b> type of <b>approach</b> was established only recently, {{partly due to}} the complex forms of the current self-consistent estimators of censored regression quantiles. Of interest, how the self-consistent estimation of censored regression quantiles is connected to the alternative martingale-based approach still remains uncovered. In this paper, we propose a new formulation of self-consistent censored regression quantiles based on stochastic integral equations. The proposed representation of censored regression quantiles entails a clearly defined estimation procedure. More importantly, it greatly simplifies the theoretical investigations. We establish the large sample equivalence between the proposed self-consistent estimators and the existing estimator derived from martingale-based estimating equations. The connection between the new self-consistent estimation approach and the available self-consistent algorithms is also elaborated...|$|R
40|$|Query {{performance}} strongly {{depends on}} finding an execution plan that touches as few superfluous tuples as possible. The access structures deployed <b>for</b> <b>this</b> purpose, however, are non-discriminative. They assume every {{subset of the}} domain being indexed is equally important, and their structures cause a high maintenance overhead during updates. <b>This</b> <b>approach</b> often fails in decision support or scientific environments where index selection represents a weak compromise amongst many plausible plans. An alternative route, explored here, is to continuously adapt the database organization by making reorganization {{an integral part of}} the query evaluation process. Every query is first analyzed for its contribution to break the database into multiple pieces, such that both the required subset is easily retrieved and subsequent queries may benefit from the new partitioning structure. To <b>study</b> the potentials <b>for</b> <b>this</b> <b>approach,</b> we developed a small representative multi-query benchmark and ran experiments against several open-source DBMSs. The results obtained are indicative for a significant reduction in system complexity with clear performance benefits. ...|$|R
40|$|This paper {{introduces}} a socio-ecological {{analysis of the}} artisanal fisheries system on Easter Island (27 ° 07 'S, 109 ° 22 'W) through the identification and interaction of stakeholders. It also comprises a structural analysis of the system aiming to identify any key issues and then propose research and development programs for multiple fisheries that will contribute to their sustainable development. The methodology is divided into four stages: i) identification of issues with stakeholder (fishers, government workers and expert scientists) participation, ii) analysis of a structural matrix consisting of a direct causality study of the symmetric, structural and binary matrix based on socio-ecological issues allowing the calculation {{of the level of}} influence and dependence of each issue, iii) identification of key issues with an influence/dependency diagram, and iv) proposal of research and development programs according to the needs and opportunities identified in the previous stages. The Easter Island artisanal fishing system is used as a case <b>study</b> <b>for</b> <b>this</b> methodological <b>approach.</b> Thus, the fishers identified 108 issues, which were then grouped by similarity, reducing the number to 27 global issues, of which seven were identified as key. Surveyed local and central government workers and expert scientists identified 7, 2 and 5 issues, respectively. Finally, research and development programs are proposed that will encourage a series of changes to the fisheries situation on the island, in order to resolve issues and promote their sustainable development...|$|R
40|$|In this {{presentation}} {{the effects}} of an altered teaching methodology, in which the „student as producer ‟ approach was adopted, are outlined. Currently, many students exist as knowledge consumers; however, Neary and Winn (2009) have suggested the positive effect on students learning through the inclusion of research-like activities {{at the core of}} the undergraduate curriculum; the students act as „producers‟ of knowledge. In this presentation a third year pharmaceutical technology class were the case <b>study</b> group <b>for</b> <b>this</b> teaching <b>approach,</b> and the module focussed on pharmaceutical manufacture quality systems and legislation. Group work formed an integral part of class time as part of the altered teaching methodology. This presentation will describe the engaging and creative activities which allowed reduction in class notes and minimal didactic teaching. Students investigated individual learning styles and individualised their learning experience based on suggested techniques suitable to their style. The epistemic process of wondering, critiquing, collaboration, visualisation and connection in both class activities and the aligned continua...|$|R
40|$|Computer Assisted Instruction (CAI) {{has shown}} {{increased}} popularity recently {{and there are}} many <b>studies</b> showing promise <b>for</b> <b>this</b> <b>approach</b> <b>for</b> children with Autism Spectrum Disorders (ASD). However, there are no between-subject studies to date assessing the efficacy of CAI with this population. In this study, 47 preschool and K- 1 students in ASD classrooms participated from Los Angeles Unified School District. TeachTown: Basics, a CAI program which also includes supplementary off-computer activities, was implemented over 3 months for approximately 20 minutes per day on the computer and 20 minutes per day in supple-mentary TeachTown: Basics activities. Compared to the students in the control group, the TeachTown: Basics students showed more improvement overall on language and cognitive outcome measures. In addition, students who used TeachTown: Basics demonstrated significant progress overall in the software and those students who used the program for more time demonstrated larger gains within the software and in outcome measures. Although not conclusive, these findings offer possi-bilities for the use of CAI for remediating many deficits for children with ASD and other special needs. In addition, CAI may offer solution...|$|R
40|$|It is {{well-known}} {{that the existence}} of transversally intersecting separatrices of hyperbolic periodic solutions leads, in a typical situation, to complicated and irregular dynamics. Therefore, {{in the case of a}} two-dimensional mapping or a three-dimensional flow, with this transversality property, there is no non-trivial analytic or meromorphic first integral, i. e., a function constant along each trajectory of the system under consideration. Additional robust conditions are obtained and discussed that guarantee the absence of such an integral in the manydimensional case, regardless of the finite dimension in question (the strongest analytic non-integrability). These conditions guarantee also the absence of any non-trivial analytic one-parameter symmetry group, and, more generally, analytic or meromorphic vector fields generating a local symmetry, i. e., a local phase flow commuting with the system under consideration. Furthermore, the analytic centralizer of the system is discrete in the compact-open topology. A differential-topological structure of the invariant set of “quasi-random motions” is <b>studied</b> <b>for</b> <b>this</b> purpose. The <b>approach</b> used is essentially geometrical. Some related topics are also discussed...|$|R
40|$|Genomic selection, enabled by whole genome {{prediction}} (WGP) methods, is revolutionizing plant breeding. Existing WGP {{methods have}} been shown to deliver accurate predictions in the most common settings, such as prediction of across environment performance for traits with additive gene effects. However, prediction of traits with non-additive gene effects and prediction of genotype by environment interaction (G×E), continues to be challenging. Previous attempts to increase prediction accuracy for these particularly difficult tasks employed prediction methods that are purely statistical in nature. Augmenting the statistical methods with biological knowledge has been largely overlooked thus far. Crop growth models (CGMs) attempt to represent the impact of functional relationships between plant physiology and the environment in the formation of yield and similar output traits of interest. Thus, they can explain the impact of G×E and certain types of non-additive gene effects on the expressed phenotype. Approximate Bayesian computation (ABC), a novel and powerful computational procedure, allows the incorporation of CGMs directly into the estimation of whole genome marker effects in WGP. Here we provide a proof of concept <b>study</b> <b>for</b> <b>this</b> novel <b>approach</b> and demonstrate its use with synthetic data sets. We show that <b>this</b> novel <b>approach</b> can be considerably more accurate than the benchmark WGP method GBLUP in predicting performance in environments represented in the estimation set as well as in previously unobserved environments for traits determined by non-additive gene effects. We conclude that this proof of concept demonstrates that using ABC for incorporating biological knowledge in the form of CGMs into WGP is a very promising and novel approach to improving prediction accuracy for some of the most challenging scenarios in plant breeding and applied genetics...|$|R
40|$|Substantial {{progress}} has been made in studying water mist fire suppression systems (WMFSSs) in the past decade. As the one-zone model is commonly used <b>for</b> <b>studying</b> WMFSSs, <b>this</b> <b>approach</b> will be reviewed in this paper. This model is based on the experimental observation of water mists mixing with fire gas in a protected space and is developed <b>for</b> <b>studying</b> obstructed fire extinguishment in a compartment. The room is taken to be a homogeneous zone and the interactions between the fire-induced flow and the discharging water mists are <b>studied.</b> Conservation equations <b>for</b> the droplet phase and gas species in a fire compartment are solved to calculate the parameters describing the fire extinguishment. The effects of fire size, ventilation, pre-burning time, droplet size and discharging rate can be studied. The continuous discharge mode and cycling discharge mode of operating a WMFSS are described. The limitations of the one-zone modelling approach are also discussed. It is suggested that more experimental fire tests be carried out in order to explore the one-zone model further for practical design of the system. Department of Building Services Engineerin...|$|R
40|$|This paper {{explores the}} use of flow length and travel time as a {{pre-processing}} step for incorporating spatial precipitation information into Artificial Neural Network (ANN) models used for river flow forecasting. Spatially distributed precipitation is commonly required when modelling large basins, and it is usually incorporated in distributed physically-based hydrological modelling approaches. However, these modelling approaches are recognised to be quite complex and expensive, especially due to the data collection of multiple inputs and parameters, which vary in space and time. On the other hand, ANN models for flow forecasting are frequently developed only with precipitation and discharge as inputs, usually without taking into consideration the spatial variability of precipitation. Full inclusion of spatially distributed inputs into ANN models still leads to a complex computational process that may not give acceptable results. Therefore, here we present {{an analysis of the}} flow length and travel time as a basis for pre-processing remotely sensed (satellite) rainfall data. This pre-processed rainfall is used together with local stream flow measurements of previous days as input to ANN models. The case <b>study</b> <b>for</b> <b>this</b> modelling <b>approach</b> is the Ganges river basin. A comparative analysis of multiple ANN models with different hydrological pre-processing is presented. The ANN showed its ability to forecast discharges 3 -days ahead with an acceptable accuracy. Within this forecast horizon, the influence of the pre-processed rainfall is marginal, because of dominant influence of strongly auto-correlated discharge inputs. For forecast horizons of 7 to 10 days, the influence of the pre-processed rainfall is noticeable, although the overall model performance deteriorates. The incorporation of remote sensing data of spatially distributed precipitation information as pre-processing step showed to be a promising alternative for the setting-up of ANN models for river flow forecasting...|$|R
40|$|An {{experimental}} {{study has been}} conducted to evaluate and compare the use of fumigated diesel fuel or gasoline as supplementary fuels for a naturally-aspirated, four-stroke diesel engine with a swirl-combustion chamber. The supplementary diesel fuel or gasoline is introduced together with the aspirated air (fumigation) in various proportions {{with respect to the}} main diesel fuel, which is injected in the usual manner. The influence of fuel/feed ratios (supplementary or main feed), for a large range of loads, has been examined on fuel consumption, pressure diagrams, exhaust smokiness and exhaust-gas emissions (nitrogen oxides, hydrocarbons and carbon monoxide). Knocking limits have been determined. The differences in the measured performance and exhaust-emission parameters from baseline engine operation, when using either supplementary diesel fuel or gasoline fumigated in the intake air, are determined and compared. Our <b>study</b> shows promise <b>for</b> <b>this</b> <b>approach</b> and indicates that above approximately 60 % of maximum load, there is high smoke reduction with only a slight change in specific fuel consumption, when using either one of the supplementary fumigated fuels. Examination of gaseous pollutant levels shows involved relations with respect to load and fuel proportions. Theoretical aspects of the supplementary fuel-mode (fumigation) of combustion are used to explain the observed engine behaviour...|$|R
40|$|This {{article is}} an {{exploratory}} {{case study of}} the École Hôtelière de Lausanne (EHL) and its teaching model for entrepreneurship education, drawing on the conceptual framework developed by Fayolle and Gailly (2008). In view of its history and favourable environment for entrepreneurship, the École Hôtelière de Lausanne (EHL) appears as a relevant field of <b>study</b> <b>for</b> <b>this</b> initial descriptive <b>approach.</b> The study shows that the ontological dimension of entrepreneurship teaching at EHL is homogeneous and shared by all the actors involved. On a didactic level, no obvious contradiction was noted between the various stakeholders’ points of view. However, the objectives, assessment criteria, course contents and pedagogical methods were found to differ depending on the entrepreneurship courses taught. Based on the conceptual framework developed by Fayolle and Gailly (2008), the overall teaching model nevertheless appeared coherent and structured. Implications <b>for</b> future research: <b>This</b> article {{can lead to a}} broader study of entrepreneurship teaching models for the hospitality industry. Similar studies could be conducted on a representative sample of European hotel schools to establish if there is a specific teaching model for entrepreneurship education in European Hospitality schools. More generally, this article could lead to expanding our knowledge of the teaching models in use for entrepreneurship education...|$|R
40|$|International audienceEnvironmental {{protection}} {{becomes a}} global challenge currently. Green roof {{is one of}} the innovativeconcepts to face this battle. An increase in its use is noticed in urban areas worldwide. But a question arises: what are the environmental consequences of the green roofs’ life cycle? In this paper, the environmental performance of two complete systems of lighter and heavier green roofs implemented in a global south low-income country are analyzed and compared {{in order to determine the}} potential impacts of both types of green roof systems. For proposing solutions aiming at reducing environmental loads of green roofs, Life-Cycle Assessment (LCA) approach was used in the present <b>study.</b> <b>For</b> <b>this</b> purpose, the <b>approach</b> consists of the following phases: definition of the objective, life cycle inventory, characterization of impacts, and interpretation of results. LCA calculations were done with the help of OpenLCA software. Results show that, non treated materials and / or imported ones are more environmentally impactful. Hence, it is profitable to reduce the use of cement, gravel, virgin plastics, and soil as well as imported materials whose transport is done by plane. In addition, use of natural fertilizer for amending the growth substrate and water from well for watering the green roof, is also recommended...|$|R
40|$|The aim of {{this study}} was to {{characterize}} the texture of a set of processed cheeses by consumers taking diversity in perception, handling the product and vocabulary into account. Hence, a free-text comment methodology using a personal approach was investigated to answer the following questions: Which are the terms frequently generated by consumers? Which are the main characteristics of products based on consumer descriptions? Are we able to identify the texture specificities of a product or a group of products? A set of 20 products representing the texture diversity in the processed cheese market were studied. The term frequencies were <b>studied</b> <b>for</b> the entire product set. In total, 550 different terms were generated, among which, 9 terms Sticky, Shiny, Yellow, Smooth, Compact, Hard, Spreadable, Creamy and Easy to spread were the most frequently elicited by the consumers. In addition, the product profiles were obtained on the basis of the products' main characteristics. Specific terms and differences in frequencies were then <b>studied</b> <b>for</b> each product. <b>This</b> <b>approach</b> made it possible to describe the products by means of the terms given by consumers. In particular, common characteristics among some products raise the question of the relationship between product description and formulation factors...|$|R
40|$|This paper {{explores the}} {{complexity}} of researching networked learning and tutoring on two levels. Firstly, on the theoretical level, we argue {{that the nature of}} praxis in networked environments (that is, learning and tutoring) is so complex that no single theoretical model, among those currently available, is a sufficiently powerful, descriptively, rhetorically, inferentially or in its application to real contexts, to provide a framework for a research agenda that takes into account the key aspects of human agency. Furthermore, we argue that this complexity of praxis requires a multi-method approach to empirical investigation, in order that theory and praxis may converse, with both being enriched by these investigations. Secondly, on an empirical level, and as an example that draws upon our theoretical argument about complexity, we present the findings of a multi-method analysis of the learning and tutoring processes occurring in an on-line community of professionals engaged in a Master’s Programme in E-Learning. This investigation is informed by two mainstream theoretical perspectives on learning, and employs computer-assisted content analysis and critical event recall as complementary methodologies. This study reveals the differentiated nature of participants’ learning, even within a highly structured collaborative learning environment, identifies some of the key functions and roles of participants, and provides an indication of the value of such multi-method <b>studies.</b> Future prospects <b>for</b> <b>this</b> <b>approach</b> to research in the field are considere...|$|R
40|$|Ribosomal {{profiling}} is {{a promising}} approach with increasing popularity <b>for</b> <b>studying</b> translation. <b>This</b> <b>approach</b> enables monitoring the ribosomal density along genes at {{a resolution of}} single nucleotides. In this study, we focused on ribosomal density profiles of mouse embryonic stem cells. Our analysis suggests, for the first time, that even in mammals such as M. musculus the elongation speed is significantly and directly affected by determinants of the coding sequence such as: 1) the adaptation of codons to the tRNA pool; 2) the local mRNA folding of the coding sequence; 3) the local charge of amino acids encoded in the codon sequence. In addition, our analyses suggest that in general, the translation velocity of ribosomes is slower {{at the beginning of}} the coding sequence and tends to increase downstream. Finally, a comparison of these data to the expected biophysical behavior of translation suggests that it suffers from some unknown biases. Specifically, the ribosomal flux measured on the experimental data increases along the coding sequence; however, according to any biophysical model of ribosomal movement lacking internal initiation sites, the flux is expected to remain constant or decrease. Thus, developing experimental and/or statistical methods for understanding, detecting and dealing with such biases is of high importance...|$|R
