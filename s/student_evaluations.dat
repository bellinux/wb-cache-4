792|2212|Public
25|$|Evaluations {{are based}} {{primarily}} on written exams (marks are based on two midterms and one end of term exam) and percentage grades appear on transcripts. Practical exams are solely based on an objective structured clinical examination (OSCE) type of standardized exams. In the final year, the school of medicine invites external examiners (from the United States, Britain, among others) to participate in <b>student</b> <b>evaluations</b> and {{to benefit from the}} experience of others.|$|E
500|$|During the Republican presidential {{primaries}} of 2016, {{opponents of}} Trump's candidacy used Trump University to criticize him. Mitt Romney said in early March: [...] "Donald Trump is a phony, a fraud. His promises are as worthless as {{a degree from}} Trump University." [...] Senators Ted Cruz and Marco Rubio raised the subject during televised debates in February and March. One debate moderator, Megyn Kelly of Fox News, pursued the issue at length. Trump responded that Trump University was [...] "a small business" [...] and <b>student</b> <b>evaluations</b> were overwhelmingly positive. He said lawsuits were a routine part of business and that he wins most of them. Of one of the class action suits he said: [...] "It's something I could have settled many times. I could settle it right now for very little money, but I don't {{want to do it}} out of principle." [...] Hillary Clinton used the Trump University allegations against Trump in speeches and campaign ads.|$|E
2500|$|A {{spokeswoman}} for CSULB, {{said that at}} least two classes a year taught by all professors—including MacDonald—have <b>student</b> <b>evaluations,</b> and that some of the questions on those evaluations are open-ended, allowing students to raise any issue. [...] "Nothing has come through" [...] to suggest bias in class, she said. [...] "We don't see it." [...] Jonathan Knight, who handles academic freedom issues for the American Association of University Professors said if there are no indications that MacDonald shares his views in class, [...] "I don't see a basis for an investigation" [...] into what goes on in his courses.|$|E
40|$|This {{dissertation}} {{covers the}} development of a <b>student</b> <b>evaluation</b> questionnaire web application for the Department of Computer Science of the University of Sheffield. The author named the application as Universal <b>Student</b> <b>Evaluation</b> Questionnaire because it provides a universal solution for universities' <b>student</b> <b>evaluation</b> statistics...|$|R
40|$|<b>Student</b> <b>evaluation</b> of {{teaching}} is commonplace in many universities {{and may be}} the predominant input into the performance evaluation of staff and organisational units. This article used publicly available <b>student</b> <b>evaluation</b> {{of teaching}} data to present examples of where institutional responses to evaluation processes appeared to be educationally ineffective and where {{the pursuit of the}} ‘right’ <b>student</b> <b>evaluation</b> results appears to have been mistakenly equated with the aim of improved teaching and learning. If the vast resources devoted to <b>student</b> <b>evaluation</b> of teaching are to be effective, then the data produced by <b>student</b> <b>evaluation</b> systems must lead to real and sustainable improvements in teaching quality and student learning, rather than becoming an end in itself...|$|R
40|$|In this paper, {{we present}} a web-based {{intelligent}} education system to help students and tutors {{in the context of}} an AI course. We concentrate on the knowledge management and <b>student</b> <b>evaluation</b> aspects of the system. Knowledge management mainly refers to test questions construction and management. <b>Student</b> <b>evaluation</b> refers to the evaluation of the knowledge level of a student with regards to taught concepts. A rule-based expert system helps in <b>student</b> <b>evaluation.</b> A number of statistics provided by the system give valuable information to the tutor. KEY WORDS Web-based education, <b>student</b> <b>evaluation,</b> expert systems, knowledge management. 1...|$|R
2500|$|Each {{publication}} {{presents and}} elaborates {{a set of}} standards {{for use in a}} variety of educational settings. The standards provide guidelines for designing, implementing, assessing and improving the identified form of evaluation. Each of the standards has been placed in one of four fundamental categories to promote educational evaluations that are proper, useful, feasible, and accurate. [...] In these sets of standards, validity and reliability considerations are covered under the accuracy topic. [...] For example, the student accuracy standards help ensure that <b>student</b> <b>evaluations</b> will provide sound, accurate, and credible information about student learning and performance.|$|E
2500|$|The Bill and Melinda Gates Foundation is {{sponsoring}} a multi-year study of value-added modeling with their Measures of Effective Teaching program. [...] Initial results, released in December 2010, indicate that both value-added modeling and student perception of several key teacher traits, such as {{control of the}} classroom and challenging students with rigorous work, correctly identify effective teachers. [...] The study about <b>student</b> <b>evaluations</b> was done by Ronald Ferguson. [...] The study also discovered that teachers who teach to the test are much less effective, and have significantly lower value-added modeling scores, than teachers who promote a deep conceptual understanding of the full curriculum. Reanalysis of the MET report’s results conducted by Jesse Rothstein, an economist and professor at University of California, Berkeley, dispute some of these interpretations, however. Rothstein argues that the analyses in the report do not support the conclusions, and that [...] "interpreted correctly... undermine rather than validate value-added-based approaches to teacher evaluation.” [...] More recent work from the MET project, however, validates the use of value added approaches.|$|E
5000|$|... #Subtitle level 3: RateMyProfessors.com versus formal in-class <b>student</b> <b>evaluations</b> ...|$|E
40|$|This paper {{describes}} {{the implementation of}} online <b>student</b> <b>evaluation</b> of instruction in the colleges of engineering of two national universities. The paper reviews the history of development of the online processes, and the benefits and challenges of using Internet technology for <b>student</b> <b>evaluation</b> of course instruction...|$|R
50|$|From the {{experiment}} of Dr. Fox effect, the expressiveness {{of a teacher}} when delivering lecture material can affect <b>student</b> <b>evaluation</b> of the teacher. Other than that, individual differences among teachers such as personality, popularity, lecture fluency, non verbal behavior, and attractiveness can also {{have an effect on}} <b>student</b> <b>evaluation</b> of teachers.|$|R
40|$|In higher education, <b>student</b> <b>evaluation</b> of {{instruction}} provides data that serve {{a variety of}} purposes including the revision of courses and programs, improvement {{of instruction}}, institutional accreditation, and tenure decisions about faculty (Dunkin & Barnes, 1986). When instruction is delivered online, <b>student</b> <b>evaluation</b> becomes notably more complicated, as issues of technology and pedagog...|$|R
5000|$|... #Subtitle level 2: Effects of {{instructor}} characteristics on <b>student</b> <b>evaluations</b> ...|$|E
5000|$|The Utility {{standards}} {{promote the}} design and implementation of informative, timely, and useful <b>student</b> <b>evaluations.</b>|$|E
5000|$|The Feasibility {{standards}} {{help ensure}} that <b>student</b> <b>evaluations</b> are practical; viable; cost-effective; and culturally, socially, and politically appropriate.|$|E
5000|$|Ronald Ferguson (economist), a {{researcher}} who studied <b>student</b> <b>evaluation</b> of teachers ...|$|R
40|$|Purpose: The paper aims to {{disseminate}} solutions to common problems in <b>student</b> <b>evaluation</b> processes. It proposes that <b>student</b> <b>evaluation</b> {{can be applied}} to quality assurance and improving learning and teaching. The paper presents solutions in the areas of: presenting outcomes as performance indicators, constructing appropriate surveys, improving response rates, reporting student feedback to students and student engagement as a feature of university quality assurance. Design/methodology/approach: The research approach of this paper is comparative case study, allowing in-depth exploration of multiple perspectives and practices at seven Australian universities. Process and outcome data were rigorously collected, analysed, compared and contrasted. Findings: The paper provides empirical evidence for <b>student</b> <b>evaluation</b> as an instrument of learning and teaching data analysis for quality improvement. It suggests that collecting data about student engagement and the student experience will yield more useful data about student learning. Furthermore, findings indicate that students benefit from more authentic inclusion in the evaluation process and outcomes. Research limitations/implications: Because of the chosen research approach, the research results may lack generalisability. Therefore, researchers are encouraged to test the proposed propositions further and apply to their own university contexts. Practical implications: The paper includes recommendations at the institution- and sector-wide levels to effectively use <b>student</b> <b>evaluation</b> as a university performance indicator and as a tool of change. Originality/value: This paper fulfils an identified need to examine <b>student</b> <b>evaluation</b> processes across institutions and focuses on the role of <b>student</b> <b>evaluation</b> in quality assurance...|$|R
40|$|Purpose – The paper aims to {{disseminate}} solutions to common problems in <b>student</b> <b>evaluation</b> processes. It proposes that <b>student</b> <b>evaluation</b> {{can be applied}} to quality assurance and improving learning and teaching. The paper presents solutions in the areas of: presenting outcomes as performance indicators, constructing appropriate surveys, improving response rates, reporting student feedback to students and student engagement as a feature of university quality assurance. Design/methodology/approach – The research approach of this paper is comparative case study, allowing in-depth exploration of multiple perspectives and practices at seven Australian universities. Process and outcome data were rigorously collected, analysed, compared and contrasted. Findings – The paper provides empirical evidence for <b>student</b> <b>evaluation</b> as an instrument of learning and teaching data analysis for quality improvement. It suggests that collecting data about student engagement and the student experience will yield more useful data about student learning. Furthermore, findings indicate that students benefit from more authentic inclusion in the evaluation process and outcomes. Research limitations/implications – Because of the chosen research approach, the research results may lack generalisability. Therefore, researchers are encouraged to test the proposed propositions further and apply to their own university contexts. Practical implications – The paper includes recommendations at the institution- and sector-wide levels to effectively use <b>student</b> <b>evaluation</b> as a university performance indicator and as a tool of change. Originality/value – This paper fulfils an identified need to examine <b>student</b> <b>evaluation</b> processes across institutions and focuses on the role of <b>student</b> <b>evaluation</b> in quality assurance...|$|R
5000|$|Harold T. White Prize for {{introductory}} {{teaching in}} Harvard Physics Dept., based on professor recommendations and <b>student</b> <b>evaluations.</b> (May 1997) ...|$|E
5000|$|The Accuracy {{standards}} {{help ensure}} that <b>student</b> <b>evaluations</b> will provide sound, accurate, and credible information about student learning and performance.|$|E
5000|$|Harvard University Certificate of Distinction in Teaching, {{given by}} Derek Bok Center for Teaching, Harvard Univ., based on <b>student</b> <b>evaluations.</b> (May 1997) ...|$|E
40|$|Israel. • Educational {{activities}} (a) Courses taught 1. Decision Support Systems – graduate course – Ben-Gurion University, 2011. 2. Introduction to Probability – undergraduate course – Ben-Gurion University, 2010. Average <b>student</b> <b>evaluation</b> 4. 3 / 5 3. Regression Analysis – undergraduate course- Ben-Gurion University, 2010, 2009. Average <b>student</b> <b>evaluation</b> 4. 4 / 5 4. Decision Making Models –undergraduate course – Ben-Gurion University, 2009...|$|R
40|$|In {{this paper}} we {{analyzed}} {{the process of}} <b>student</b> <b>evaluation</b> from “Spiru Haret” University. The process under consideration occurs according to a specific Procedure – Process of <b>student</b> <b>evaluation</b> from the Manual of Quality Assurance Procedures, “Spiru Haret” University, Edition 1, 2012. The goal of this procedure, mentioned in the Manual, is to present the <b>student</b> <b>evaluation</b> procedure by using the Blackboard educational platform and other evaluation techniques of quality learning, based on materials developed by teachers of “Spiru Haret” University, as well as corresponding responsibilities, {{in order to increase}} the learning process quality and the exigency degree in the examination process, as well as students’ satisfaction measured by accumulated competences. We appreciate that the purpose of this procedure is first and foremost to ensure transparency and objectivity in exam passing decision. After identifying the weaknesses with the “cause - effect” chart, we have sought to improve <b>student</b> <b>evaluation</b> process using PDCA (Plan-Do-Check-Act) method, resulting in the design of a new assessment flowchart...|$|R
40|$|Research {{indicates}} that administering university <b>student</b> <b>evaluation</b> of teaching electronically rather than via paper-based surveys increases {{the quality and}} timeliness of the feedback thereby making a stronger contribution to teaching and learning enhancement. The documented drawback of electronic <b>student</b> <b>evaluation</b> is the response rate, which is significantly lower than paper-based surveys. This study documents a pilot project whereby electronic <b>student</b> <b>evaluation</b> of teaching was administered for one semester in units of study in three of four of the University’s faculties. The outcomes confirmed similar studies’ results. Whereas most studies are written from the academic and/or administrator point of view, the unique contribution of this study, co-authored by a graduate student, is that {{students were asked to}} evaluate the evaluation through an online forum, focus groups, and through the student association. The feedback conveyed a clear and consistent message that students prefer electronic <b>student</b> <b>evaluation</b> of teaching because of enhanced anonymity and convenience and less time pressure...|$|R
5000|$|... {{documents}} {{relating to}} the care of students, as well as any <b>student</b> <b>evaluations</b> containing verbal information on the personal qualities of the student (30) ...|$|E
5000|$|The Propriety {{standards}} {{help ensure}} that <b>student</b> <b>evaluations</b> are conducted lawfully, ethically, and {{with regard to the}} rights of students and other persons affected by student evaluation.|$|E
50|$|Trenton Central High School was {{the focus}} of a {{research}} study aimed at preventing obesity in students, in which <b>student</b> <b>evaluations</b> of the results {{played a major role in}} interpretation of the outcomes.|$|E
40|$|Method Results Number of Exams Students Take Kind of Exams <b>Students</b> Take <b>Evaluation</b> of Course Exams Depending on Test Format Students 2 ̆ 7 Best and Worst Testing Experiences Best Test Experiences Worst Test Experiences Conclusion Implications for Instructional Development Centers TRC <b>Student</b> <b>Evaluation</b> of Testin...|$|R
40|$|There are two {{different}} positions about <b>student</b> <b>evaluation</b> process. Its defenders argue it is an objective and trustworthy mechanism, and its critics consider it is a biased process that does not fulfil its objective. This study analyses the evaluations {{of a group of}} 187 full-time professors of the UPR-Bayamón during eight different semesters in the periods of 1998 - 1999 and 2003 - 2004. Results indicate that evaluation is influenced by professor, student and class characteristics, and they are consistent {{with the fact that the}} process could be biased, because there is a significant relation between the <b>student</b> <b>evaluation</b> of the professor and the class grade expected. For instance, professors can “buy” better evaluations promoting higher expecting grades to the students, and stimulate the phenomenon of “grade inflation”. <b>student</b> <b>evaluation,</b> premium, penalties...|$|R
40|$|The thesis surveys <b>student</b> <b>evaluation</b> of {{university}} teachers by questionnaires. Its target {{is to understand}} more exactly how both participating sides, ie. teachers and students, see the problems. The empirical part of the thesis surveys the situation at the three faculties of Charles University - the Faculty of Philosophy & Arts, the Faculty of Social Sciences and the Catholic Theology Faculty - by depths interveiws. Those interviews made possible to get relatively detailed {{information that can be}} applied to preparation for <b>student</b> <b>evaluation.</b> It can also bring better understanding and acceptance of <b>student</b> <b>evaluation</b> by teachers and students, which is a precondition of the use of the results by faculties, as well as the sufficient participation of <b>students</b> in the <b>evaluation</b> {{in such a way that}} the research could brind relevant data. The thesis has got two parts. The theoretical part summarizes knowledge referring to the status {{of university}} teachers and their evaluation, the empirical part contains the description of the research methodologies, inroduces the valorization of the data obtained in several evaluations that had taken place, and summarizes the information obtained by the depths interviews. The most important conclusion of the thesis is that students and teachers consider <b>student</b> <b>evaluation</b> useful [...] ...|$|R
50|$|The {{results showed}} that the professors who instilled a deeper meaning of the {{material}} appeared worse on the initial exam and evaluations, {{but in the long run}} received better academic standing. This finding calls into question the validity of <b>student</b> <b>evaluations.</b>|$|E
50|$|<b>Student</b> <b>evaluations</b> are a {{controversial}} method of assessing academic achievement. Recent studies have correlated high student evaluation of instructors with high grades rather than mastery of content. Studies have {{also noted that}} students understanding of assessment criteria can lead to enhanced learning experiences.|$|E
5000|$|The Joint Committee on Standards for Educational Evaluation {{published}} {{three sets}} of standards for educational evaluations. The Personnel Evaluation Standards [...] was published in 1988, The Program Evaluation Standards (2nd edition) [...] was published in 1994, and The <b>Student</b> <b>Evaluations</b> Standards [...] was published in 2003.|$|E
40|$|This paper compares <b>student</b> <b>evaluation</b> of {{teaching}} {{on the web}} site MeinProf. de to the standard evaluation procedures at German universities. While MeinProf offers the advantage of a broad, publicly available database, a number of theoretical and econometric problems emerge. Many of these problems are, however, pervasive in the <b>student</b> <b>evaluation</b> {{of teaching}} as a whole. We demonstrate that this seriously hampers empirical work on the relationship of teaching and research. Lehrevaluation; Lehre versus Forschung; Lehrqualitaet...|$|R
40|$|Responding to <b>student</b> needs <b>Student</b> <b>evaluation</b> and {{feedback}} toolkit To run this CD-ROM please insert {{it into your}} CD drive. For PC users, the CD-ROM will open in your internet browser window. For Apple Mac users, please double click the CD icon on the desktop, then double click on the file called index. html and the CD will open in your internet browser. Once open, you can navigate the <b>Student</b> <b>Evaluation</b> and Feedback Toolkit from th...|$|R
40|$|Activities of Faculty Development (FD) in the Graduate School of Dental Medicine and School of Dental Medicine of Hokkaido University {{consist of}} lectures on FD, {{workshops}} on FD and <b>student</b> <b>evaluation</b> of undergraduate lectures. Basically, the lectures on FD are held 4 times every year. The speakers are selected {{not only from}} the faculty of Hokkaido University but also from outside the university. A workshop on FD is held once every year. Usually about 30 faculty members of the Graduate School of Dental Medicine participate in the workshop that takes place for two days in a hotel out of campus. The <b>student</b> <b>evaluation</b> of undergraduate lectures is conducted twice every year; i. e., for the lectures in the first semester and the second semester. The lectures and workshop on FD began in 2000, and the <b>student</b> <b>evaluation</b> of undergraduate lectures began in 1997. Many faculty members have participated in the lectures and workshops on FD and made efforts to develop their competence. The <b>student</b> <b>evaluation</b> of undergraduate lectures showed a gain in the score. However, there are a few faculty members who have a negative view of FD in practice...|$|R
