3096|2490|Public
500|$|The {{geophysical}} and archaeological {{study of}} {{the area around the}} stone feature as well as the soil <b>sampling</b> <b>strategy</b> and the close observation of the 1792-93 maps of the area suggest that the stone layout cannot be the garden of Delahaye. [...] This study also indicated that the location of the garden shown on the maps is in a dry and rocky environment which does not fit he description of the French ... The stone layout found in 2002 is probably an uncompleted structure associated with the late 19th and early 20th century development of the area. [...] It is obviously {{one of the few remaining}} witnesses of this part of the history of Coal Pit Bight and needs to be protected and further researched.|$|E
2500|$|The {{relative}} {{risk of death}} due to the 2003 invasion and occupation was estimated by comparing mortality in the 17.8 months after the invasion with the 14.6 months preceding it. The authors stated, [...] "Making conservative assumptions, we think that about 100,000 excess deaths, or more have happened since the 2003 invasion of Iraq." [...] Among such [...] "conservative assumptions" [...] is the exclusion of data from Fallujah in many of its findings. Since interpreting {{the results of the}} study would be complicated by the inclusion of an outlier cluster in Fallujah, where heavy fighting caused far more casualties than elsewhere in Iraq, the study focused mainly on the results that excluded the Fallujah cluster. [...] While the authors argued that the Fallujah cluster's inclusion could be justified as a normal part of the <b>sampling</b> <b>strategy</b> (the authors noted that other [...] "hotspots" [...] like Najaf had not ended up being surveyed), and the authors presented two sets of results in some cases (one set including the Fallujah data and one not), the article, and most press coverage of the article, stresses the data that excluded the Fallujah cluster.|$|E
50|$|The {{adaptive}} <b>sampling</b> <b>strategy</b> dramatically {{reduces the}} rendering time for high-quality rendering - the higher quality and/or size of data-set, the more significant {{advantage over the}} regular/even <b>sampling</b> <b>strategy.</b> However, adaptive ray casting upon a projection plane and adaptive sampling along each individual ray do not map well to the SIMD architecture of modern GPU. Multi-core CPUs, however, are a perfect fit for this technique, making them suitable for interactive ultra-high quality volumetric rendering.|$|E
40|$|We propose {{deterministic}} <b>sampling</b> <b>strategies</b> for compressive imaging {{based on}} Delsarte-Goethals frames. We show that these <b>sampling</b> <b>strategies</b> result in multi-scale measurements {{which can be}} related to the 2 D Haar wavelet transform. We demonstrate the effectiveness of our proposed strategies through numerical experiments...|$|R
30|$|Select 24 plots {{using one}} of the <b>sampling</b> <b>strategies</b> under consideration.|$|R
30|$|On {{the basis}} of the {{information}} in these three articles, the conclusion of the special issue evaluates the feasibility of cross-country <b>sampling</b> <b>strategies</b> which yield high quality samples of immigrant minorities. The article by Hans-Jürgen Andreß and Romana Careja focuses on probability samples and the use of population registers, as these are the standard for quality samples, while other <b>sampling</b> <b>strategies</b> are only briefly touched upon (Andreß & Careja, 2018). The analysis shows that even with only six European countries an identical register-based sampling design is difficult. The authors propose that, by focusing on sampling immigrant minorities in cities, researchers can better implement <b>sampling</b> <b>strategies</b> which result in comparable samples.|$|R
5000|$|There {{are many}} {{variants}} {{on the basic}} PRM method, some quite sophisticated, that vary the <b>sampling</b> <b>strategy</b> and connection strategy to achieve faster performance. See e.g. [...] for a discussion.|$|E
5000|$|Lagrangian {{techniques}} {{are based on}} parameterizing the contour according to some <b>sampling</b> <b>strategy</b> and then evolve each element according to image and internal terms. Such {{techniques are}} fast and efficient, however the original [...] "purely parametric" [...] formulation (due to Kass, Witkin and Terzopoulos in 1987 and known as [...] "snakes"), is generally criticized for its limitations regarding the choice of <b>sampling</b> <b>strategy,</b> the internal geometric properties of the curve, topology changes (curve splitting and merging), addressing problems in higher dimensions, etc.. Nowadays, efficient [...] "discretized" [...] formulations {{have been developed to}} address these limitations while maintaining high efficiency. In both cases, energy minimization is generally conducted using a steepest-gradient descent, whereby derivatives are computed using, e.g., finite differences.|$|E
50|$|Lawrence has {{particular}} {{contributions in}} the development of sequence alignment algorithms, which is approaching the modif finding problem by integrating the Bayesian statistics and Gibbs <b>sampling</b> <b>strategy.</b> In his seminal paper published in Science in 1993, the first application of the statistical technique Gibbs sampling to the problem of multiple sequence alignment was described and clearly illustrated.|$|E
40|$|Mobile sensing {{has been}} {{recently}} proposed for sampling spatial fields, where mobile sensors record the field along various paths for reconstruction. Classical and contemporary sampling typically {{assumes that the}} sampling locations are approximately known. This work explores multiple <b>sampling</b> <b>strategies</b> along random paths to sample and reconstruct a two dimensional bandlimited field. Extensive simulations are carried out, with insights from sensing matrices and their properties, to evaluate the <b>sampling</b> <b>strategies.</b> Their performance is measured by evaluating the stability of field reconstruction from field samples. The effect of location unawareness on some <b>sampling</b> <b>strategies</b> is also evaluated by simulations. Comment: 6 pages, 2 figures; submitted to ICASSP 201...|$|R
40|$|Visual {{analytics}} {{and interactive}} machine learning both try to leverage the complementary strengths {{of humans and}} machines to solve complex data exploitation tasks. These fields overlap most significantly when training is involved: the visualization or machine learning tool improves over time by exploiting observations of the human-computer interaction. This paper focuses on {{one aspect of the}} human-computer interaction that we call user-driven <b>sampling</b> <b>strategies.</b> Unlike relevance feedback and active learning <b>sampling</b> <b>strategies,</b> where the computer selects which data to label at each iteration, we investigate situations where the user selects which data is to be labeled at each iteration. User-driven <b>sampling</b> <b>strategies</b> can emerge in many visual analytics applications but they have not been fully developed in machine learning. User-driven <b>sampling</b> <b>strategies</b> suggest new theoretical and practical research questions for both visualization science and machine learning. In this paper we identify and quantify the potential benefits of these strategies in a practical image analysis application. We find user-driven <b>sampling</b> <b>strategies</b> can sometimes provide significant performance gains by steering tools towards local minima that have lower error than tools trained with all of the data. In preliminary experiments we find these performance gains are particularly pronounced when the user is experienced with the tool and application domain...|$|R
40|$|The {{purpose of}} this paper is to provide a {{typology}} of sampling designs for qualitative researchers. We introduce the following sampling strategies: (a) parallel sampling designs, which represent a body of <b>sampling</b> <b>strategies</b> that facilitate credible comparisons of two or more different subgroups that are extracted from the same levels of study; (b) nested sampling designs, which are <b>sampling</b> <b>strategies</b> that facilitate credible comparisons of two or more members of the same subgroup, wherein one or more members of the subgroup represent a sub-sample of the full sample; and (c) multilevel sampling designs, which represent <b>sampling</b> <b>strategies</b> that facilitate credible comparisons of two or more subgroups that are extracted from different level s of study...|$|R
50|$|Filmer-Sankey's {{investigation}} was twofold. First, he undertook {{a thorough investigation}} into the documents pertaining to previous excavations at the site, through which his team ascertained that although the ship burial was the most notable feature of the site, the cemetery primarily contained cremation burials, and was therefore best compared with the Norfolk cemetery of Spong Hill. This accomplished, the secondary task of developing a <b>sampling</b> <b>strategy</b> had to be devised. The use of fieldwalking and geophysical survey had already proved unsuccessful, {{and so it was}} decided that excavation would be used as the primary method of investigation. In 1985, fourteen 3×3 trenches were opened, but only two cremation urns, both damaged by ploughing, were uncovered. One of these trenches was subsequently enlarged to 6×6 metres, revealing both two further funerary urns and an inhumation burial. This discovery meant that the excavators had to rethink their <b>sampling</b> <b>strategy</b> and wider approach to the site.|$|E
5000|$|In statistics, ignorability is {{a feature}} of an {{experiment}} design whereby the {{method of data collection}} (and the nature of missing data) do not depend on the missing data. A missing data mechanism such as a treatment assignment or survey <b>sampling</b> <b>strategy</b> is [...] "ignorable" [...] if the missing data matrix, which indicates which variables are observed or missing, is independent of the missing data conditional on the observed data.|$|E
5000|$|Studies can be {{designed}} to observe intra-host or inter-host interactions. Bacterial phylodynamic studies usually focus on inter-host interactions with samples from many different hosts in a specific geographical location or several different geographical locations. [...] The {{most important part of}} a study design is how to organize the <b>sampling</b> <b>strategy.</b> [...] For example, the number of sampled time points, the sampling interval, and the number of sequences per time point are crucial to phylodynamic analysis. [...] Sampling bias causes problems when looking at a diverse taxological samples. [...] For example, sampling from a limited geographical location may impact effective population size.|$|E
5000|$|... (with Michelle Hegmon) The sample size-richness relation: The {{relevance}} of research questions, <b>sampling</b> <b>strategies,</b> and behavioral variation. American Antiquity 58:489-496.|$|R
40|$|In this paper, we have {{proposed}} to use {{two classes of}} <b>sampling</b> <b>strategies</b> based on the modified ratio estimator using the standard deviation and the coefficient of skewness of the auxiliary variable by Singh (2003) for estimating the population mean (total) of the study variable in a finite population. The properties of the proposed <b>sampling</b> <b>strategies</b> are studied and some concluding remarks are given. Also, an empirical study is included as an illustration...|$|R
40|$|We {{explore the}} {{possibilities}} of obtaining compression in video through modified <b>sampling</b> <b>strategies</b> using multichannel imaging systems. The redundancies in video streams are exploited through compressive sampling schemes to achieve low power and low complexity video sensors. The <b>sampling</b> <b>strategies</b> {{as well as the}} associated reconstruction algorithms are discussed. These compressive sampling schemes could be implemented in the focal plane readout hardware resulting in drastic reduction in data bandwidth and computational complexity...|$|R
50|$|Several {{countries}} {{have used a}} system which is known as short form/long form. This is a <b>sampling</b> <b>strategy</b> which randomly chooses a proportion of people to send a more detailed questionnaire to (the long form). Everyone receives the short form questions. Thereby more data are collected but not imposing a burden on the whole population. This also reduces {{the burden on the}} statistical office. Indeed, in the UK all residents were required to fill in the whole form but only a 10% sample were coded and analysed in detail, until 2001. New technology means that all data are now scanned and processed. Recently there has been controversy in Canada about the cessation of the long form with the head, Munir Sheikh resigning.|$|E
50|$|There is some {{argument}} over the <b>sampling</b> <b>strategy</b> {{to be employed}} in trial trenching, especially in evaluating sites that are intended for development. Issues such as the effectiveness of certain trench layouts or {{the percentage of the}} site to be dug (normally around 5% at present) are widely discussed. Whether an effective picture of past human activity on a site can be truly estimated through this methods is widely debated. Development can destroy buried archaeology forever and a reliable evaluation methodology is very important. Whilst it is difficult to quantify the number of false negative results there have certainly been examples of evaluations suggesting a relatively limited amount of past activity which has had to be upwardly revised during the excavation.|$|E
50|$|Cultural contexts: The most {{important}} consideration when designing a <b>sampling</b> <b>strategy</b> for a cultural context is {{to fit the}} sampling design to the research objectives. For example, if {{the objective of the}} study is to identify activity areas, it may be ideal to sample using a grid system. If the objective is to identify foodstuffs, it may be more beneficial to focus on areas where food processing and consumption took place. It is always beneficial to sample ubiquitously throughout the site, because it is always possible to select a smaller portion of the samples for analysis from a larger collection. Samples should be collected and labeled in individual plastic bags. It is not necessary to freeze the samples, or treat them in any special way because silica is not subject to decay by microorganisms.|$|E
5000|$|... #Caption: Pulse-Doppler signal {{processing}} begins with samples taken between multiple transmit pulses. <b>Sample</b> <b>strategy</b> expanded for one transmit pulse is shown.|$|R
40|$|The Bayesian method (BM) can use {{previous}} {{information for}} the optimization of dosage regimen. However, Bayes' law remains true when the parameters are obtained from the infinite population. Therefore a bias might exist in the previous information and affect BM predictive performance. To overcome this shortcoming, the blood drug concentration of a patient {{can be used to}} individualize his pharmacokinetic parameters. Until now, at least two <b>sampling</b> <b>strategies,</b> i. e. steady-state and non-steady-state <b>sampling</b> <b>strategies,</b> have been developed to individualize and predict blood drug concentration. In the present study we used five sampling strategies: (1) all samples; (2) post-infusion samples; (3) during-infusion samples; (4) samples within 95 % confidence interval/interquartile range of a steady-state concentration; (5) the sample of the mean/median at the mid-time-point of a steady-state to individualize and predict blood cyclosporine concentrations in haematological patients with multidrug resistance. We investigated the effects of different <b>sampling</b> <b>strategies</b> on BM and the nonlinear least squared method (NLLSM) predictive performances. The results showed that BM predictive performance was better than NLLSM. But the results did not prove that the steady-state <b>sampling</b> <b>strategies</b> were superior to the non-steady-state ones...|$|R
3000|$|Methods 6 – 11 {{belonged to}} {{different}} stratified <b>sampling</b> <b>strategies.</b> The stratified <b>sampling</b> method divides the population into subpopulations of size n [...]...|$|R
5000|$|The NIOSH Nanomaterial Exposure Assessment Technique (NEAT 2.0) is a <b>sampling</b> <b>strategy</b> to {{determine}} exposure potential for engineered nanomaterials. It includes filter-based and area samples, {{as well as}} a comprehensive assessment of emissions at processes and job tasks to better understand peak emission periods. Evaluation of worker practices, ventilation efficacy, and other engineering exposure control systems and risk management strategies serve to allow for a comprehensive exposure assessment. [...] The NIOSH Manual of Analytical Methods includes guidance on electron microscopy of filter samples of carbon nanotubes and nanofibers, and additionally some NIOSH methods developed for other chemicals can be used for off-line analysis of nanomaterials, including their morphology and geometry, elemental carbon content (relevant for carbon-based nanomaterials), and elemental makeup. [...] Efforts to create reference materials are ongoing.|$|E
5000|$|In 2006 {{an archaeological}} survey of this site and others {{relating}} to the d'Entrecasteaux expedition concluded that:The geophysical and archaeological study of {{the area around the}} stone feature as well as the soil <b>sampling</b> <b>strategy</b> and the close observation of the 1792-93 maps of the area suggest that the stone layout cannot be the garden of Delahaye. This study also indicated that the location of the garden shown on the maps is in a dry and rocky environment which does not fit he description of the French ... The stone layout found in 2002 is probably an uncompleted structure associated with the late 19th and early 20th century development of the area. It is obviously {{one of the few remaining}} witnesses of this part of the history of Coal Pit Bight and needs to be protected and further researched.|$|E
50|$|Migration {{of living}} animals and {{settling}} particle-attached organisms {{can lead to}} an uneven distributions of biota at different locations of the world. When small organisms {{find their way into}} a ballast tank, the foreign organism or animal can upset the balance of the local habitat. When a local habitat is changed, it can interfere with the natural habitat and potentially damage the existing animal life. Vessel workers check the ballast tank for living organisms ≥50 μm in discrete segments of the drain, it also represents the level of sedimentary of different rock or soil in the tank. Throughout the sample collection, concentrations of organisms and marine life varied in result in the drain segments, patterns also varied in level of stratification in other trials. To have the best <b>sampling</b> <b>strategy</b> for stratified tanks, is to collect various time-integrated samples spaced evenly throughout each discharge.|$|E
30|$|For more {{detailed}} information on the studies and <b>sampling</b> <b>strategies,</b> see Allmendinger et al. (2011), Antoni et al. (2010) and Aßmann et al. (2011).|$|R
30|$|Using data {{collected}} from 20 destructively sampled trees, we evaluated 11 different <b>sampling</b> <b>strategies</b> using six evaluation statistics: bias, relative bias, {{root mean square error}} (RMSE), relative RMSE, amount of biomass sampled, and relative biomass sampled. We also evaluated the performance of the selected <b>sampling</b> <b>strategies</b> when different numbers of branches (3, 6, 9, and 12) are selected from each tree. Tree specific log linear model with branch diameter and branch length as covariates was used to obtain individual branch biomass.|$|R
40|$|Sequential Monte Carlo {{methods are}} {{powerful}} algorithms to sample from sequences of complex probability distributions. They are mainly {{based on a}} combination of importance sampling and resampling techniques. The efficiency of these methods depends crucially on the <b>sampling</b> <b>strategies</b> adopted. In this paper, we present an extended importance sampling framework which allows more freedom than standard techniques to impute random samples. This makes it possible to develop efficient and original <b>sampling</b> <b>strategies.</b> Applications to optimal filtering problems illustrate this approach...|$|R
50|$|Regardless of type, all mollusk shells {{should be}} {{collected}} using a standardized stratigraphic <b>sampling</b> <b>strategy.</b> Utilizing {{this type of}} strategy avoids the problem of ignoring the collection of smaller shells, an issue that can result from hand-picking. Once samples are collected, they must be sent to a laboratory to be air-dried. A standard weight is then collected for each sample. Each standardized sample is then placed in a plastic bowl (labeled with stratigraphic information) and covered with hot water. Shells float {{to the top and}} are skimmed off into a set of sieves, which separate the shells by size. After shells are removed from the soil, the soil should be covered in a solution of 70% hot water and 30% hydrogen peroxide. Once the mixture fizzes, it is passed through a set of sieves. Both the soil and shells are then placed in the drying oven. After cooling, shells are fully prepared for analysis and can be extracted from the sieves.|$|E
5000|$|In 2009, TNTP {{published}} The Widget Effect: Our National Failure to Acknowledge and Act Upon Teacher Effectiveness. The report, which surveyed over 15,000 {{teachers and}} 1,300 principals in 12 school districts, {{concluded that the}} U.S. public education system treats teachers as interchangeable parts, rather than individual professionals. American Federation of Teachers (AFT) President Randi Weingarten provided a public statement of support for report, saying it “points {{the way to a}} credible, fair, accurate and effective teacher evaluation system that would improve teaching and learning.” [...] The National Education Policy Center (NEPC) review of The Widget Effect praised the overall quality of the report but said, [...] "'it is unclear ... how and why particular districts were selected, and whether they represent the range of teacher evaluation practices being implemented in school districts and states across the United States.' Omissions in the report's description of its methodology (e.g., <b>sampling</b> <b>strategy</b> and survey response rates) and its sample lead to questions about the generalizability of the findings." ...|$|E
5000|$|After {{subjects}} were identified, both {{quantitative and qualitative}} analyses of the three samples were conducted, and graphic representations of the data were constructed and marked for relevant traits. Descriptive statistical comparisons were made for the entire dataset {{as well as for the}} foreign and prostitute subgroups. Inferential statistics were also used to determine whether the distributions for age and the time it took for a field worker to locate a nominee (speed) were significant and whether the respective snowballs were drawn from populations with the same distributions. The second question was seen as especially appropriate for and [...] "ascending" [...] <b>sampling</b> <b>strategy</b> because it cannot be assumed that each snowball is drawn from the same population when only an [...] "imperfect sampling frame" [...] composed of a [...] "special list" [...] compiled by nominees, is available. Because it has been seen as especially appropriate for small samples (as little as three cases) for which population parameters are unknown and cannot be confidently assumed, the nonparametric Kolmogorov-Smirnov (KS) test was used. Two-tailed KS tests were performed on the pooled data of the three samples (one-sample test) and on the between-snowballs (subgroups) data (two-samples test).|$|E
40|$|This paper {{introduces}} a filterbank interpretation of various <b>sampling</b> <b>strategies,</b> {{which leads to}} efficient interpolation and reconstruction methods. An identity, which {{is referred to as}} the Interpolation Identity, is developed and is used to obtain particularly efficient discrete-time systems for interpolation of generalized samples as well as a class of nonuniform samples, to uniform Nyquist samples, either for further processing in that form or for conversion to continuous time. The Interpolation Identity also leads to new <b>sampling</b> <b>strategies</b> including an extension of Papoulis’ generalized sampling expansion...|$|R
40|$|Non-marine {{molluscs}} {{are often}} forgotten {{when it comes}} to environmental assessment studies. This is mainly {{due to the lack of}} knowledge of this zoological group, of the scattered data that has been collected in an independent way by different authors using different <b>sampling</b> <b>strategies</b> and/or methods. However many continental molluscs are recognized as being vulnerable and a fair number are registered on protected species’ lists. This paper presents and analyses the different <b>sampling</b> <b>strategies</b> (schemes and methods) used for the study of freshwater and land Molluscs in order to define better approaches...|$|R
30|$|However, in many fields, {{including}} immigrants, {{the statistical}} figures are not harmonised between Germany and the Netherlands. One important {{reason is the}} lack of identical sampling frames. In this paper we will address the challenges of drawing samples based on immigrants living in the Netherlands and Germany. Before the <b>sampling</b> <b>strategies</b> will be discussed, we will outline the various concepts referring to immigrants, and their implementation. It will be demonstrated that in both countries traditional randomized sampling techniques among immigrants necessitate an adjustment of the population definition as many immigrants are not (yet) registered and consequently are not found in sampling frames. Furthermore, the two countries differ in the possibilities to use population registers for sampling designs and link registers enabling harmonised <b>sampling</b> <b>strategies.</b> We will start with a short overview of the immigrants, followed by the concepts and sampling techniques used. In the conclusions, we will provide an answer to which extent harmonised <b>sampling</b> <b>strategies</b> are feasible and the consequences for survey research.|$|R
