122|0|Public
25|$|There are {{two major}} {{prerequisites}} for brain morphometry: First, the brain features of interest must be measurable, and second, statistical methods {{have to be in}} place to compare the measurements quantitatively. Shape feature comparisons form the basis of Linnaean taxonomy, and even in cases of convergent evolution or brain disorders, they still provide a wealth of information {{about the nature of the}} processes involved. Shape comparisons have long been constrained to simple and mainly volume- or <b>slice-based</b> measures but profited enormously from the digital revolution, as now all sorts of shapes in any number of dimensions can be handled numerically.|$|E
2500|$|With the {{exception}} of the usually <b>slice-based</b> histology of the brain, neuroimaging data are generally stored as matrices of voxels. The most popular morphometric method, thus, is known as Voxel-based morphometry (VBM; cf. [...] ). Yet as an imaging voxel is not a biologically meaningful unit, other approaches have been developed that potentially bear a closer correspondence to biological structures: Deformation-based morphometry (DBM), surface-based morphometry (SBM) and fiber tracking based on diffusion-weighted imaging (DTI or DSI). All four are usually performed based on Magnetic Resonance (MR) imaging data, with the former three commonly using T1-weighted (e.g. Magnetization Prepared Rapid Gradient Echo, MP-RAGE) and sometimes T2-weighted pulse sequences, while DTI/DSI use diffusion-weighted ones. However, recent evaluation of morphometry algorithms/software demonstrates inconsistency among several of them. This renders a need for systematic and quantitative validation and evaluation of the field.|$|E
50|$|There are {{two major}} {{prerequisites}} for brain morphometry: First, the brain features of interest must be measurable, and second, statistical methods {{have to be in}} place to compare the measurements quantitatively. Shape feature comparisons form the basis of Linnaean taxonomy, and even in cases of convergent evolution or brain disorders, they still provide a wealth of information {{about the nature of the}} processes involved. Shape comparisons have long been constrained to simple and mainly volume- or <b>slice-based</b> measures but profited enormously from the digital revolution, as now all sorts of shapes in any number of dimensions can be handled numerically.|$|E
40|$|Background. <b>Slice-based</b> {{cohesion}} metrics leverage program {{slices with}} respect to the output variables of a module to quantify the strength of functional relatedness of the elements within the module. Although <b>slice-based</b> cohesion metrics have been proposed for many years, few empirical studies have been conducted to examine their actual usefulness in predicting fault-proneness. Objective. We aim to provide an in-depth understanding of the ability of <b>slice-based</b> cohesion metrics in effort-aware post-release fault-proneness prediction, i. e. their effectiveness in helping practitioners find post-release faults when taking into account the effort needed to test or inspect the code. Method. We use the most commonly used code and process metrics, including size, structural complexity, Halstead's software science, and code churn metrics, as the baseline metrics. First, we employ principal component analysis to analyze the relationships between <b>slice-based</b> cohesion metrics and the baseline metrics. Then, we use univariate prediction models to investigate the correlations between <b>slice-based</b> cohesion metrics and post-release fault-proneness. Finally, we build multivariate prediction models to examine the effectiveness of <b>slice-based</b> cohesion metrics in effort-aware post-release fault-proneness prediction when used alone or used together with the baseline code and process metrics. Results. Based on open-source software systems, our results show that: 1) <b>slice-based</b> cohesion metrics are not redundant {{with respect to}} the baseline code and process metrics; 2) most <b>slice-based</b> cohesion metrics are significantly negatively related to post-release fault-proneness; 3) <b>slice-based</b> cohesion metrics in general do not outperform the baseline metrics when predicting post-release fault-proneness; and 4) when used with the baseline metrics together, however, <b>slice-based</b> cohesion metrics can produce a statistically significant and practically important improvement of the effectiveness in effort-aware post-release fault-proneness prediction. Conclusion. <b>Slice-based</b> cohesion metrics are complementary to the most commonly used code and process metrics and are of practical value in the context of effort-aware post-release fault-proneness prediction. Department of Computin...|$|E
40|$|This report {{provides}} an overview of <b>slice-based</b> software metrics. It brings together information about the development of the metrics from Weiser???s original idea that program slices may be used in the measurement of program complexity, with alternative <b>slice-based</b> measures proposed by other researchers. In particular, it details two aspects of <b>slice-based</b> metric calculation not covered elsewhere in the literature: output variables and worked examples of the calculations. First, output variables are explained, their use explored and standard reference terms and usage proposed. Calculating <b>slice-based</b> metrics requires a clear understanding of ???output variables??? because they form the basis for extracting the program slices on which the calculations depend. This report includes a survey of the variation in the definition of output variables used by different research groups and suggests standard terms of reference for these variables. Our study identifies four elements which are combined in the definition of output variables. These are the function return value, modified global variables, modified reference parameters and variables printed or otherwise output by the module. Second, <b>slice-based</b> metric calculations are explained with the aid of worked examples, to assist newcomers to the field. Step-by-step calculations of <b>slice-based</b> cohesion and coupling metrics based on the vertices output by the static analysis tool CodeSurfer (R) are presented and compared with line-based calculations...|$|E
40|$|Complementation of Büchi automata, {{required}} for checking automata containment, is of major {{theoretical and practical}} interest in formal verification. We consider two recent approaches to complementation. The first is the rank-based approach of Kupferman and Vardi, which operates over a DAG that embodies all runs of the automaton. This approach {{is based on the}} observation that the vertices of this DAG can be ranked in a certain way, termed an odd ranking, iff all runs are rejecting. The second is the <b>slice-based</b> approach of Kähler and Wilke. This approach tracks levels of "split trees" - run trees in which only essential information about the history of each run is maintained. While the <b>slice-based</b> construction is conceptually simple, the complementing automata it generates are exponentially larger than those of the recent rank-based construction of Schewe, and it suffers from the difficulty of symbolically encoding levels of split trees. In this work we reformulate the <b>slice-based</b> approach in terms of run DAGs and preorders over states. In doing so, we begin to draw parallels between the rank-based and <b>slice-based</b> approaches. Through deeper analysis of the <b>slice-based</b> approach, we strongly restrict the nondeterminism it generates. We are then able to employ the <b>slice-based</b> approach to provide a new odd ranking, called a retrospective ranking, that is different from the one provided by Kupferman and Vardi. This new ranking allows us to construct a deterministic-in-the-limit rank-based automaton with a highly restricted transition function. Further, by phrasing the <b>slice-based</b> approach in terms of ranks, our approach affords a simple symbolic encoding and achieves the tight bound of Schewe's constructio...|$|E
40|$|Volumetric, <b>slice-based,</b> 3 -D atlases are {{invaluable}} {{tools for}} understanding complex cortical convolutions. We present a simple scheme to convert a <b>slice-based</b> atlas to a conceptual surface atlas that {{is easier to}} visualize and understand. The key idea is to unfold each slice into a one-dimensional vector, and concatenate a succession of these vectors – while maintaining as much spatial contiguity as possible – into a 2 -D matrix. We illustrate our methodology using a coronal slicebased atlas of the Rhesus Monkey cortex. The conceptual surface-based atlases provide a useful complement to <b>slice-based</b> atlases {{for the purposes of}} indexing and browsing...|$|E
40|$|Seoul National Univ. This paper {{presents}} a fully automatic segmentation method for the virtual bronchoscopy, which requires the segmentation of a trachea and left/right bronchi. A trachea region is automatically detected, based on medical knowledge and DICOM header information, and its center point is selected as a seed {{point for the}} <b>slice-based</b> 3 D Seeded Region Growing(SRG). The <b>slice-based</b> 3 D SRG grows a region by collecting its homogeneous neighbors in a single slice, and makes seed points at adjacent slices. The seed points grow at their own slices, recursively. While growing, a region size constraint between adjacent slices is preserved to prevent from leakage problem of conventional SRG methods. The <b>slice-based</b> 3 D SRG can segment a complex tubular shaped organ whose center axis can be represented by a high order curve. Segmentation results are presented using CT slice image data sets. ...|$|E
40|$|This work {{explores the}} parallelization of <b>slice-based</b> {{lightweight}} Keccak implementations. The functionality of Xilinx FPGAs {{to use a}} single slice as a 32 -bit Shift Register Lookup table (SRL) was recently used by Winderickx et al. [1] to implement Keccak. This implementation resulted in the smallest Keccak implementation, given that a custom interface was used. To enhance the implementation, we explore the parallelization of the datapath. Four datapath widths are used: 25, 50, 100 and 200 bits. The implementations with a datapath width of 25 and 50 bits give better area and throughput results than other <b>slice-based</b> lightweight Keccak implementations; larger datapath widths result in an inefficient usage of the SRL functionality. Furthermore, the custom interface with datapath widths larger than 50 bits is not compatible with 64 -bit microprocessors and is therefore unusable in practical scenarios. The standard-compliant implementations perform worse than other <b>slice-based</b> lightweight Keccak implementations for all datapath widths...|$|E
40|$|This brief {{compares the}} use of multiplierless and DSP <b>slice-based</b> {{cross-correlation}} for IEEE 802. 16 d orthogonal frequency division multiplexing (OFDM) timing synchronization on Xilinx Virtex- 6 and Spartan- 6 field programmable gate arrays (FPGAs). The natural approach, given the availability of embedded DSP blocks on these FPGAs, would be to implement standard multiplier-based cross-correlation. However, this can consume {{a significant number of}} DSP blocks, which may not fit on low-power devices. Hence, we compare a DSP 48 E 1 <b>slice-based</b> design to four different quantizations of multiplierless correlation in terms of resource utilization and power consumption. OFDM timing synchronization accuracy is evaluated for each system at different signal-to-noise ratios. Results show that even relatively coarse multiplierless coefficient quantization can yield accurate timing synchronization, and does so at high clock speeds. Multiplierless designs enjoy reduced power consumption over the DSP 48 E 1 <b>Slice-based</b> design, and can be used where DSP Slice resources are insufficient, such as on low-power FPGA devices...|$|E
40|$|This paper {{demonstrates}} that existing <b>slice-based</b> measures can reasonably be mapped {{to the field}} of state-based specification languages. By making use of Z specifications this contribution renews the idea of slice-profiles and derives coupling and cohesion measures for them. The measures are then assessed by taking a critical look at their sensitiveness in respect to modifications on the specification source. The presented study shows that <b>slice-based</b> coupling and cohesion measures have the potential to be used as quality indicators for specifications as they reflect the changes in the structure of a specification as accustomed from their program-related pendants. ...|$|E
40|$|Abstract — This brief {{compares the}} use of multiplierless and DSP <b>slice-based</b> {{cross-correlation}} for IEEE 802. 16 d orthogonal frequency division multiplexing (OFDM) timing synchronization on Xilinx Virtex- 6 and Spartan- 6 field programmable gate arrays (FPGAs). The natural approach, given the availability of embedded DSP blocks on these FPGAs, would be to implement standard multiplier-based cross-correlation. However, this can consume {{a significant number of}} DSP blocks, which may not fit on low-power devices. Hence, we compare a DSP 48 E 1 <b>slice-based</b> design to four different quantizations of multiplierless correlation in terms of resource utilization and power consumption. OFDM timing synchronization accuracy is evaluated for each system at different signal-to-noise ratios. Results show that even relatively coarse multiplierless coefficient quantization can yield accurate timing synchronization, and does so at high clock speeds. Multiplierless designs enjoy reduced power consumption over the DSP 48 E 1 <b>Slice-based</b> design, and can be used where DSP Slice resources are insufficient, such as on low-power FPGA devices. Index Terms — Correlation, cognitive radio, field-programmable gate arrays (FPGA), IEEE 802. 16 standards, orthogonal frequency division multiplexing (OFDM). I...|$|E
40|$|<b>Slice-based</b> {{visualizations}} of CT and MRI {{data are}} frequently used for diagnosis, intervention planning and intraoperative navigation since they allow a precise analysis and localization. We present new techniques {{to enhance the}} visualization of cross sectional medical image data. Our work is focussed on intervention planning and intraoperative navigation. We address the following problems of <b>slice-based</b> visualization in these areas: {{the lack of a}} graphical overview on the positions of anatomic structures, the localization of a target structure and the display of safety zones around pathologic structures. To improve the overview, we introduce LIFTCHARTs, attached as vertical bars to a <b>slice-based</b> visualization. For localizing target structures, we introduce halos. These techniques restrict the occlusion of the original data to a minimum and avoid any modification of the original data. To demonstrate the usability of these visualization techniques, we show two application scenarios in which the techniques come into operation. Categories and Subject Descriptors (according to ACM CCS) : I. 3. 3 [Computer Graphics]: Picture/Image Generation—Display algorithms; J. 3 [Life And Medical Sciences]: Medical information systems 1...|$|E
40|$|Büchi {{complementation}} {{has been}} studied for five decades since the formalism was introduced in 1960. Known complementation constructions can be classified into Ramsey-based, determinization-based, rank-based, and <b>slice-based</b> approaches. For the performance of these approaches, {{there have been several}} complexity analyses but very few experimental results. What especially lacks is a comparative experiment on all the four approaches to see how they perform in practice. In this paper, we review the state of Büchi complementation, propose several optimization heuristics, and perform comparative experimentation on the four approaches. The experimental results show that the determinization-based Safra-Piterman construction outperforms the other three and our heuristics substantially improve the Safra-Piterman construction and the <b>slice-based</b> construction...|$|E
40|$|This paper {{presents}} a new data structure, <b>Slice-based</b> Binary Shell (SBS), for efficient manipulation and rendering of binary volume data. Since SBS stores only surface voxels with selected {{attributes of the}} voxels in a <b>slice-based</b> data structure that allows {{direct access to the}} voxels, it shows high storage and computational efficiency. This efficiency becomes more prominent when representing multiple binary objects. We also present an efficient rendering algorithm for SBS. The algorithm, based on the shear-warp technique, provides high-speed interactive rendering for binary volumes of many objects on a PC with no specialized hardware. Key words: Binary volume rendering- Surface voxel- Spatial data structure – Medical imaging...|$|E
40|$|Software {{reengineering}} is {{a costly}} endeavor, {{due in part}} to the ambiguity of where to focus reengineering effort. Coupling and Cohesion metrics, particularly quantitative cohesion metrics, have the potential to aid in this identification and to measure progress. The most extensive work on such metrics is with <b>slice-based</b> cohesion metrics. While their use of semantic dependence information should make them an excellent choice for cohesion measurement, their wide spread use has been impeded in part by a lack of empirical study. Recent advances in software tools make, for the first time, a large-scale empirical study of slicebased cohesion and coupling metrics possible. Four results from such a study are presented. First, “head-to-head ” qualitative and quantitative comparisons of the metrics identify which metrics provide similar views of a program and which provide unique views of a program. This study includes statistical analysis showing that <b>slice-based</b> metrics are not proxies for simple size-based metrics such as lines of code. Second, two longitudinal studies show that <b>slice-based</b> metrics quantify the deterioration of a program as it ages. This serves to validate the metrics: the metrics quantify the degradation that exists during development; turning this around, the metrics can be used to measure the progress of a reengineering effort. Third, base-line values for slicebased metrics are provided. These values act as targets for reengineering efforts with modules having values outside the expected range being the most in need of attention. Finally, <b>slice-based</b> coupling is correlated and compared with slice based cohesion...|$|E
40|$|Abstract: The paper {{shows how}} AutoCAD {{functions}} can be utilized to perform 3 D transformations well-known in literature- i. e. {{the so called}} <b>slice-based</b> transformations such as: twisting, bending, tapering, shear transformations to allow automatic modeling of the classic architectural objects, e. g. twisted columns, or Saint Gilles surface...|$|E
40|$|A current goal {{in volume}} {{graphics}} is a volume rendering algorithm that provides an elegant and controllable tradeoff between image quality and rendering speed. In this report we propose a <b>slice-based</b> volume rendering algorithm which attempts {{to address this}} goal. We describe both the basic algorithm and several ways that it can operate in an incremental and an adaptive manner...|$|E
40|$|Complementation of Büchi automata {{has been}} studied for over five decades since the {{formalism}} was introduced in 1960. Known complementation constructions can be classified into Ramsey-based, determinization-based, rank-based, and <b>slice-based</b> approaches. Regarding the performance of these approaches, {{there have been several}} complexity analyses but very few experimental results. What especially lacks is a comparative experiment on all of the four approaches to see how they perform in practice. In this paper, we review the four approaches, propose several optimization heuristics, and perform comparative experimentation on four representative constructions that are considered the most efficient in each approach. The experimental results show that (1) the determinization-based Safra-Piterman construction outperforms the other three in producing smaller complements and finishing more tasks in the allocated time and (2) the proposed heuristics substantially improve the Safra-Piterman and the <b>slice-based</b> constructions. Comment: 28 pages, 4 figures, a preliminary version of this paper appeared in the Proceedings of the 15 th International Conference on Implementation and Application of Automata (CIAA...|$|E
40|$|Abstract. Dynamic contrast-enhanced {{magnetic}} resonance imaging of the breast is acquired {{for the detection of}} breast cancer. To rate a tumor to be benign or malignant, radiologists evaluate the tumor’s morphology and its enhancement kinetics. We present a new multi planar reformatting (MPR) view, the File-Card-Browser View, to improve and complete the standard axial <b>slice-based</b> evaluation. We tested our technique with a tumor set containing 20 cases and present first results. ...|$|E
40|$|Cohesion {{was first}} {{developed}} to predict properties of implementations created from a given design. Unfortunately, cohesion, as originally defined, {{could not be}} objectively assessed, while more recently developed objective cohesion measures depend on code-level information. We show that association-based and <b>slice-based</b> approaches {{can be used to}} measure cohesion using only design-level information. Our design-level cohesion measures are formally defined, can be readily implemented, and can support software design, maintenance, and restructuring...|$|E
40|$|Radiologists {{from all}} {{application}} areas {{are trained to}} read <b>slice-based</b> visualizations of 3 D medical image data. Despite the numerous examples of sophisticated three-dimensional renderings, especially all variants of direct volume rendering, such methods are often considered not very useful by radiologists who prefer <b>slice-based</b> visual-ization. Just recently there have been attempts to bridge this gap between 2 D and 3 D renderings. These attempts include specialized techniques for volume picking that re-sult in repositioning slices. In this paper, we present a new volume picking tech-nique that, in contrast to previous work, does not require pre-segmented data or metadata. The positions picked by our method are solely based on the data itself, the transfer function and, most importantly, {{on the way the}} volumet-ric rendering is perceived by viewers. To demonstrate the usefulness of the proposed method we apply it for auto-matically repositioning slices in an abdominal MRI scan, a data set from a flow simulation {{and a number of other}} volumetric scalar fields. Furthermore we discuss how the method can be implemented in combination with various different volumetric rendering techniques. ...|$|E
40|$|Abstract Software {{reconstruction}} is {{a costly}} endeavor, due inpart to {{the ambiguity of}} where to focus reengineering effort. Cohesion metrics, and particularly quantitativecohesion metrics, {{have the potential to}} aid in this identification and to measure progress. The most extensivework on such metrics is with <b>slice-based</b> cohesion metrics. While their use of semantic dependence informa-tion should make them an excellent choice for cohesion measurement, their wide spread use has been impededby a lack of empirical study...|$|E
40|$|This paper {{presents}} SlotComposer, {{a software}} tool for automated synthesis flow and communication infrastructure generation for partially reconfigurable hardware modules. SlotComposer inserts <b>slice-based</b> bus macros between adjacent modules or if specified connects partially reconfigurable modules to the Reconfigurable Multiple Bus (RMB) communication infrastructure. At {{the same time}} it optimises the usage and placement of bus macros and instantiates all intermediate communication signals and generates all the necessary constraint files to synthesise the project using Xilinx Partial Reconfiguration Flow tool...|$|E
40|$|<b>Slice-based</b> metrics for {{cohesion}} {{have been}} defined and examined for years. However, if a module with low cohesion has been identified, the metrics cannot help the maintainer {{to restructure the}} module to improve the cohesion. This work presents statement-level cohesion metrics based on slices and chops. When visualized, the statement-level cohesion metrics can show which parts of a module have a low cohesion and thus help the maintainer to identify the parts that should be restructured. 1...|$|E
40|$|In {{this work}} {{we present a}} {{flexible}} framework for GPU-based volume rendering. The framework {{is based on a}} single pass volume raycasting approach and is easily extensible in terms of new shader functionality. We demonstrate the flexibility of our system by means of a number of high-quality standard and non-standard volume rendering techniques. Our implementation shows a promising performance in a number of benchmarks while producing images of higher accuracy than obtained by standard pre-integrated <b>slice-based</b> volume rendering...|$|E
40|$|AbstractThis paper {{presents}} a mechanistic model for prediction cutting forces in bull-nose end milling of curved surfaces. Firstly, a mechanistic cutting force model for bull-nose end mill is established. Secondly, the machining process characteristics for four-axis milling of ring-shaped casings are discussed. A <b>slice-based</b> method is proposed for prediction the cutter-workpiece engagements. Then, the cutting forces {{with three different}} lead angles are predicted and compared with the corresponding experimental values. The results show the feasibility and effectiveness of the presented prediction approach...|$|E
40|$|A fast display {{method for}} {{volumetric}} data sets is presented, which involves a <b>slice-based</b> method for extracting the potentially visible voxels {{to represent the}} visible surfaces. For a given viewing direction, the number of visible voxels can be trimmed further by culling most of the voxels not visible from that direction. The entire 3 D array of voxels is also present for invasive operations and direct access to interior structures. This approach has been integrated on a low-cost graphic engine as an interactive system for craniofacial surgical planning that is currently in clinical use...|$|E
40|$|Abstract. A monomodal <b>slice-based</b> {{displacement}} {{analysis of the}} spinal cord for three-dimensional computer tomography imaging in radiation therapy planning is presented. In total, 21 head and neck cases with tumor indications close to the spinal cord are studied and evaluated. Two-dimensional cross-correlation is applied to propagate manually segmented contours of the spinal cord from a high-resolution planning CT to subsequently acquired control CTs. The method and the fully automatic implementation {{turned out to be}} reliable and robust. A very few manual corrections on the resulting contours remained necessary in single transversal slices. ...|$|E
40|$|In this paper, {{we examine}} the {{performance}} of the early z-culling feature on current high-end commodity graphics cards and present an isosurface-aided hardware acceleration algorithm for <b>slice-based</b> volume rendering (iSBVR) to maximize its utilization. We analyze the computational models for early z-culling of the texture based volume rendering. We demonstrate that the performance improves with two to four times speedup against an original straightforward SBVR on an ATI 9800 pro display board. As volumetric shaders become increasingly complex, the advantages of fast z-culling will become even more pronounced. ...|$|E
40|$|In this paper, {{we propose}} and compare two video <b>slice-based</b> discard schemes, namely adaptive-PSD and Adaptive-ESD, for the {{transmission}} of MPEG video streams over ATM best effort services. The schemes perform adaptive and selective cell burst discard {{at the level of}} MPEG video slices and intelligently adjusts drop policies to switch buffer occupancy and video cell payload types. In comparison to previous approaches, the performance evaluation have shown a significant reduction of the bad throughput crossing the network and a better protection of critical Intra- and Predictive-coded pictures at both cell and video slice levels...|$|E
40|$|Although {{attribute}} grammars {{are commonly}} used for compiler construction, little investigation has been conducted on debugging attribute grammars. The paper proposes two types of systematic debugging methods, an algorithmic debugging and <b>slice-based</b> debugging, both tailored for attribute grammars. By means of query-based interaction with the developer, our debugging methods effectively narrow the potential bug space in the attribute grammar description and eventually identify the incorrect attribution rule. We have incorporated this technology in our visual debugging tool called Aki. Comment: In M. Ducasse (ed), proceedings of the Fourth International Workshop on Automated Debugging (AADEBUG 2000), August 2000, Munich. cs. SE/ 001003...|$|E
40|$|This {{material}} {{is presented to}} ensure timely dissemination of scholarly and technical work. Copyright and all rights therein are retained by authors or by other copyright holders. All persons copying this information are expected {{to adhere to the}} terms and constraints invoked by each author's copyright. In most cases, these works may not be reposted without the explicit permission of the copyright holder. " ???Copyright IEEE. Personal use of this {{material is}} permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works must be obtained from the IEEE. ???In this paper, we investigate the Barcode open-source system (OSS) using one of Weiser's original <b>slice-based</b> metrics (Tightness) as a basis. In previous work, low numerical values of this <b>slice-based</b> metric were found to indicate fault-free (as opposed to fault-prone) functions. In the same work, we deliberately excluded from our analysis a category comprising 221 of the 775 observations representing 'inconclusive' log reports extracted from the OSS change logs. These represented OSS change log descriptions where it was not entirely clear whether a fault had occurred or not in a function and, for that reason, could not reasonably be incorporated into our analysis. In this paper we present a methodology through which we can draw conclusions about that category of report...|$|E
40|$|The {{adoption}} of a flexible grid will benefit the network design and control plane of future optical networks by providing increased adaptability of spectral resources to heterogeneous network conditions. Unfortunately, this flexibility is gained {{at the cost of}} significant additional complexity in the network design and control. In this paper, we consider the optimization of routing and spectrum allocation in FlexiGrid Ring Networks and explore the trade-off between network cost (in terms of spectrum and transponder utilization) and problem complexity (in terms of number of variables/constraints and computational time). Such tradeoffs are investigated under multiple assumptions in terms of traffic grooming, regeneration, and modulation/baud rate assignment capabilities and contrasted to the case of Fixed grid. We show how in presence of traffic grooming the additional complexity due to the flexible grid has a minor impact on problem complexity. Similarly, in all the considered scenarios, regeneration and modulation/baud rate assignment do not relevantly impact problem complexity. We also consider two possible alternative Integer Linear Programming models: the <b>slice-based</b> and the channel-based approach. The former handles each slice individually, whereas the latter uses precomputed subsets of contiguous slices of different bandwidths. Both models are solved under several different network settings. Complexity comparison of the ILP models shows that the <b>slice-based</b> approach provides better performance than the channel-based approach, and that the performance gap between the two models increases with introduction of additional flexibility and dimensions...|$|E
40|$|Scientists, {{engineers}} and physicians {{are used to}} analyze 3 D data with <b>slice-based</b> visualizations. Radiologists for example are trained to read slices of medical imaging data. Despite the numerous examples of sophisticated 3 D rendering techniques, domain experts, who still prefer <b>slice-based</b> visualization do not consider these to be very useful. Since 3 D renderings {{have the advantage of}} providing an overview at a glance, while 2 D depictions better serve detailed analyses, it is of general interest to better combine these methods. Recently there have been attempts to bridge this gap between 2 D and 3 D renderings. These attempts include specialized techniques for volume picking in medical imaging data that result in repositioning slices. In this paper, we present a new volume picking technique called WYSIWYP ("what {{you see is what you}} pick") that, in contrast to previous work, does not require pre-segmented data or metadata and thus is more generally applicable. The positions picked by our method are solely based on the data itself, the transfer function, and the way the volumetric rendering is perceived by the user. To demonstrate the utility of the proposed method, we apply it to automated positioning of slices in volumetric scalar fields from various application areas. Finally, we present results of a user study in which 3 D locations selected by users are compared to those resulting from WYSIWYP. The user study confirms our claim that the resulting positions correlate well with those perceived by the use...|$|E
40|$|Most hybrid 3 D {{segmentation}} methods either heuristically couple {{the respective}} algorithm or combine a true 3 D with a 2 D algorithm due to computational considerations. In this {{paper we propose}} a new probabilistic framework for 3 D image segmentation that combines tightly linked region- and shape-based constraints. Regionbased label constraints are modeled by a 3 D Markov random field, and are tightly coupled to shape-based constraints of a 3 D Deformable Model. The full 3 D nature of the combined model leads to a robust smooth surface segmentation that outperforms the single constraint, <b>slice-based</b> as well as the loosely coupled 3 D methods. 1...|$|E
