16|173|Public
5000|$|SurroundCast - a {{wireless}} rear or side speaker amplifier system. It {{consists of a}} transmitter with <b>speaker</b> <b>level</b> inputs and an amplified receiver module with 60 watts of digital power and <b>speaker</b> <b>level</b> outputs. The system allows you to set up any brand of surround speakers {{without the need for}} front to back of room wires. The system has a low latency (15 ms) and is a quick and easy installation. No programming, no fuss. Just great audio.Note: SurroundCast is not compatible with any other Soundcast wireless products ...|$|E
50|$|Nishimachi International School (西町インターナショナルスクール), {{established}} in 1949, is a co-educational, non-sectarian, private K-9 day school {{located in the}} Azabu area of Tokyo, Japan. The main language of instruction is in English. Japanese is taught to all students every day from beginner to native <b>speaker</b> <b>level.</b>|$|E
50|$|It {{is common}} to use both a DI signal and a {{microphone}} {{in front of the}} speaker cabinet or combo amp, in both live sound and recording settings. One method is to connect a bass guitar amplifier's <b>speaker</b> <b>level</b> output (via a pad, to attenuate the signal) to a DI and then run it to one channel of the mixing console, and run a miked guitar speaker cabinet signal into another channel of the mixing console. Another method is to connect a DI between the guitar and the amplifier. The DI signal and mic'd guitar speaker can then be selectively blended, with the DI providing a more immediate, present, bright, un-equalized sound, and the microphone providing a more 'live' sound, with instrument amplifier and speaker enclosure characteristics and some room ambience (natural reverb).|$|E
40|$|We {{present an}} {{experiment}} aimed at understanding how to optimally use acoustic and prosodic information to predict a <b>speaker’s</b> <b>level</b> of certainty. With a corpus of utterances {{where we can}} isolate a single word or phrase {{that is responsible for}} the <b>speaker’s</b> <b>level</b> of certainty we use different sets of sub-utterance prosodic features to train models for predicting an utterance’s perceived level of certainty. Our results suggest that using prosodic features of the word or phrase responsible for the level of certainty and of its surrounding context improves the prediction accuracy without increasing the total number of features when compared to using only features taken from the utterance as a whole. ...|$|R
30|$|For example, in this {{scenario}} when the ambient noise is high, the SCM increases the <b>speaker’s</b> sound <b>level.</b> On the other hand, if the ambient noise is at average or low, the module decreases the <b>speaker’s</b> sound <b>level.</b>|$|R
50|$|The {{following}} {{subjects are}} offered at Stage 2 level: Accounting, Australian and International Politics, Biology, Business and Enterprise, Chemistry, Chinese (Background <b>Speakers</b> <b>Level),</b> Classical Studies, Drama, Economics, English as a Second Language Studies, English Communications, English Studies, French (Continuers), Geography, Information Technology Studies, Japanese (Continuers), Legal Studies, Mathematical Applications, Mathematical Methods, Mathematical Studies, Specialist Mathematics, Modern History, Physics, Psychology.|$|R
30|$|Another {{use case}} similar to Section 3.1 was {{achieved}} {{in order to}} manage the device’s <b>speaker</b> <b>level</b> depending on the ambient noise. As {{in the case of}} brightness management, we have two main modules. The first one is the Ambient Noise Collector (ANC) and is measuring the ambient noise and shares its to the second module Sound Control Module (SCM) which will adapt the <b>speaker</b> <b>level</b> accordingly.|$|E
40|$|Objective. To {{systematically}} {{evaluate the}} noise generated by toys targeted {{for children and}} to compare the results {{over the course of}} 4 consecutive holiday shopping seasons. Study Design. Experimental study. Setting. Academic medical center. Subjects and Methods. During 2008 - 2011, more than 200 toys marketed for children older than 6 months were screened for loudness. The toys with sound output of more than 80 dBA at <b>speaker</b> <b>level</b> were retested in a soundproof audiometry booth. The generated sound amplitude of each toy was measured at <b>speaker</b> <b>level</b> and at 30 cm away from the speaker. Results. Ninety different toys were analyzed. The mean (SD) noise amplitude was 100 (8) dBA (range, 80 - 121 dBA) at the <b>speaker</b> <b>level</b> and 80 (11) dBA (range, 60 - 109 dBA) at 30 cm away from the speaker. Eighty-eight (98...|$|E
40|$|We propose in {{this paper}} an {{automatic}} system to detect sigmatism from the speech signal. Sigmatism occurs when the tongue is positioned incorrectly during articulation of sibilant phones like /s / and /z/. For our task we extracted various sets of features from speech: Mel frequency cepstral coefficients, energies in specific bandwidths of the spectral envelope, and the so-called supervectors, which are the parameters of an adapted speaker model. We then trained several classifiers on a speech database of German adults simulating three different types of sigmatism. Recognition results were calculated at a phone, word and <b>speaker</b> <b>level</b> for both the simulated database and for a database of pathological speakers. For the simulated database, we achieved recognition rates of up to 86 %, 87 % and 94 % at a phone, word and <b>speaker</b> <b>level.</b> The best classifier was then integrated {{as part of a}} Java applet that allows patients to record their own speech, either by pronouncing isolated phones, a specific word or a list of words, and provides them with a feedback whether the sibilant phones are being correctly pronounced...|$|E
5000|$|Depending on {{the level}} of formality, the <b>speaker's</b> {{education}} <b>level,</b> etc., various grammatical changes may occur in ways that echo the colloquial variants: ...|$|R
5000|$|The grammar or grammars of {{contemporary}} varieties of Arabic {{are a different}} question. Said M. Badawi, an expert on Arabic grammar, divided Arabic grammar into five different types based on the <b>speaker's</b> <b>level</b> of literacy {{and the degree to}} which the speaker deviated from Classical Arabic. Badawi's five types of grammar from the most colloquial to the most formal are Illiterate Spoken Arabic ( [...] ), Semi-literate Spoken Arabic ( [...] ), Educated Spoken Arabic ( [...] ), Modern Standard Arabic ( [...] ), and Classical Arabic ( [...] ).|$|R
50|$|Nobel Prize Keynote <b>Speakers,</b> Highest <b>Level</b> of Representation {{from the}} UN and The World Bank, Italian Authorities, High Profile Public Figures, Ministries and Authorities from Developing Countries, Prime International University Representatives and Academicians.|$|R
40|$|Presents a {{microphone}} array system {{for use in}} handsfree mobile telephone equipment. The array {{is based on a}} fast and efficient “on-site” and “self-calibration” scheme. The performance in suppressing the interior car cabin noise and the far-end speech is approximately 17 dB, respectively, while maintaining the near-end <b>speaker</b> <b>level.</b> The near-end signal is almost undistorted. The performance of two different algorithms, normalized least-mean-square (NLMS) and fully connected backpropagation supervised neural network (MLP-NN) are evaluated. The proposed microphone array calibration scheme can also be used in other situations such as speech recognition device...|$|E
40|$|The paper investigates {{automatic}} {{rating of}} non-native children’s pronunciation. We have designed {{a set of}} 28 pronunciation-features; when classification is performed in high-dimensional feature space best recognition-results can be achieved. Dif-ferent measures to evaluate inter-rater agreement and the ma-chine score are proposed. In the European project Pf-Star data of native and non-native children has been recorded; the Ger-man children reading English texts have been graded by 13 – 14 raters. When classifying 5 sentence-level marks {{the result can be}} interpreted as 73 % correct. Looking at a longer context, recognition becomes more robust. On the <b>speaker</b> <b>level</b> error and correlation is comparable with some of the human raters. 1...|$|E
40|$|The Aero-Physics Branch at NASA Ames Research Center {{utilizes}} a 32 - by 48 -inch subsonic {{wind tunnel}} for aerodynamics research. The feasibility of acquiring acoustic measurements with a phased microphone array was recently explored. Acoustic characterization of the wind tunnel was carried out with a floor-mounted 24 -element array and two ceiling-mounted speakers. The minimum <b>speaker</b> <b>level</b> for accurate level measurement was evaluated for various tunnel speeds up to a Mach number of 0. 15 and streamwise speaker locations. A variety of post-processing procedures, including conventional beamforming and deconvolutional processing such as TIDY, were used. The speaker measurements, with and without flow, were used to compare actual versus simulated in-flow speaker calibrations. Data for wind-off speaker sound and wind-on tunnel background noise were found valuable for predicting sound levels for which the speakers were detectable when the wind was on. Speaker sources were detectable 2 - 10 dB below the peak background noise level with conventional data processing. The effectiveness of background noise cross-spectral matrix subtraction was assessed and found to improve the detectability of test sound sources by approximately 10 dB over a wide frequency range...|$|E
50|$|Native <b>speakers</b> pass <b>levels</b> 10 through 7 at {{better than}} an 80% rate, whereas level 1 is so {{difficult}} that fewer than two thousand people take it each time it is offered, and fewer than 15% of those pass.|$|R
40|$|We {{present a}} project aimed at {{understanding}} the acoustic and prosodic correlates {{of confidence and}} uncertainty in spoken lan-guage. We elicited speech produced under varying levels of cer-tainty and performed perceptual and statistical analyses on the speech data to determine which prosodic features (e. g., pitch, energy, timing) {{are associated with a}} <b>speaker’s</b> <b>level</b> of certainty and where these prosodic manifestations occur relative to the lo-cation of the word or phrase that the speaker is confident or un-certain about. Our findings suggest that prosodic manifestations of confidence and uncertainty occur both in the local region that causes the uncertainty as well as in its surrounding context. Index Terms: prosody, human speech perception, emotion de-tection, paralinguistic cue...|$|R
50|$|On {{the rear}} of speaker cabinet, VMPS <b>speakers</b> {{included}} <b>level</b> controls to attenuate the sound level of the mid-range and treble (tweeter). These level controls were intended {{to be used in}} concert with PR mass tuning to achieve the desired overall tonal balance.|$|R
40|$|Recent work in {{automatic}} {{recognition of}} conversational telephone speech (CTS) has achieved accuracy levels comparable to human transcribers, {{although there is}} some debate how to precisely quantify human performance on this task, using the NIST 2000 CTS evaluation set. This raises the question what systematic differences, if any, may be found differentiating human from machine transcription errors. In this paper we approach this question by comparing the output of our most accurate CTS recognition system {{to that of a}} standard speech transcription vendor pipeline. We find that the most frequent substitution, deletion and insertion error types of both outputs show a high degree of overlap. The only notable exception is that the automatic recognizer tends to confuse filled pauses ("uh") and backchannel acknowledgments ("uhhuh"). Humans tend not to make this error, presumably due to the distinctive and opposing pragmatic functions attached to these words. Furthermore, we quantify the correlation between human and machine errors at the <b>speaker</b> <b>level,</b> and investigate the effect of speaker overlap between training and test data. Finally, we report on an informal "Turing test" asking humans to discriminate between automatic and human transcription error cases...|$|E
40|$|A speech-in-noise test {{which uses}} digit {{triplets}} in steady-state speech noise was developed. The test measures primarily the auditory, or bottom-up, speech recognition abilities in noise. Digit triplets were formed by concatenating single digits spoken {{by a male}} <b>speaker.</b> <b>Level</b> corrections were made to individual digits to create a set of homogeneous digit triplets with steep speech recognition functions. The test measures the speech reception threshold (SRT) in long-term average speech-spectrum noise via a 1 -up, 1 -down adaptive procedure with a measurement error of 0. 7 dB. One training list is needed for naive listeners. No further learning effects were observed in 24 subsequent SRT measurements. The test was validated by comparing results on the test with results on the standard sentences-in-noise test. To avoid the confounding of hearing loss, age, and linguistic skills, these measurements were performed in normal-hearing subjects with simulated hearing loss. The signals were spectrally smeared and/or low-pass filtered at varying cutoff frequencies. After correction for measurement error the correlation coefficient between SRTs measured with both tests equaled 0. 96. Finally, the feasibility of the test was approved in a study where reference SRT values were gathered in a representative set of 1386 listeners over 60 years of age. © 2013 Acoustical Society of America...|$|E
40|$|OBJECTIVE : The {{objective}} {{of this study was}} to develop and validate a smartphone-based digits-in-noise hearing test for South African English. DESIGN : Single digits (0 - 9) were recorded and spoken by a first language English female <b>speaker.</b> <b>Level</b> corrections were applied to create a set of homogeneous digits with steep speech recognition functions. A smartphone application was created to utilize 120 digit-triplets in noise as test material. An adaptive test procedure determined the speech reception threshold (SRT). Experiments were performed to determine headphones effects on the SRT and to establish normative data. STUDY SAMPLE : Participants consisted of 40 normal-hearing subjects with thresholds[*]≤ 15 [*]dB across the frequency spectrum (250 - 8000 [*]Hz) and 186 subjects with normal-hearing in both ears, or normal-hearing in the better ear. RESULTS : The results show steep speech recognition functions with a slope of 20 %/dB for digit-triplets presented in noise using the smartphone application. The results of five headphone types indicate that the smartphone-based hearing test is reliable and can be conducted using standard Android smartphone headphones or clinical headphones. CONCLUSION : A digits-in-noise hearing test was developed and validated for South Africa. The mean SRT and speech recognition functions correspond to previous developed telephone-based digits-in-noise tests. [URL] Pathology and AudiologyElectrical, Electronic and Computer Engineerin...|$|E
30|$|We {{address the}} problem of {{inferring}} a <b>speaker's</b> <b>level</b> of certainty based on prosodic information in the speech signal, which has application in speech-based dialogue systems. We show that using phrase-level prosodic features centered around the phrases causing uncertainty, in addition to utterance-level prosodic features, improves our model's level of certainty classification. In addition, our models can be used to predict which phrase a person is uncertain about. These results rely on a novel method for eliciting utterances of varying levels of certainty that allows us to compare the utility of contextually-based feature sets. We elicit level of certainty ratings from both the speakers themselves and a panel of listeners, finding that there is often a mismatch between speakers' internal states and their perceived states, and highlighting the importance of this distinction.|$|R
40|$|When {{answering}} factual questions, speakers can signal {{whether they}} are uncertain about the correctness of their answer using prosodic cues such as fillers (“uh”), a rising intonation contour or a marked facial expression. It {{has been shown that}} on the basis of such cues, observers can make adequate estimates about the <b>speaker’s</b> <b>level</b> of confidence, but it is unclear which of these cues have the largest impact on perception. To find the relative strength of the three aforementioned cues, a novel perception experiment was performed in which answers were artificially manipulated {{in such a way that}} all possible combinations of the cues of interest could be judged by participants. Results showed that while all three factors had a significant influence on the perception results, this effect was by far the largest for facial expressions. 1...|$|R
40|$|We {{address the}} problem of {{inferring}} a <b>speaker's</b> <b>level</b> of certainty based on prosodic information in the speech signal, which has application in speech-based dialogue systems. We show that using phrase-level prosodic features centered around the phrases causing uncertainty, in addition to utterance-level prosodic features, improves our model's level of certainty classification. In addition, our models can be used to predict which phrase a person is uncertain about. These results rely on a novel method for eliciting utterances of varying levels of certainty that allows us to compare the utility of contextually-based feature sets. We elicit level of certainty ratings from both the speakers themselves and a panel of listeners, finding that there is often a mismatch between speakers' internal states and their perceived states, and highlighting the importance of this distinction. Comment: 11 page...|$|R
40|$|Immersion {{education}} has been researched for over 40 years, and its pros and cons in terms of second language acquisition have been disclosed; receptive skills and fluency can reach close to the native <b>speaker</b> <b>level,</b> but productive skills their accuracy cannot with years of instruction in the second language. In order to improve immersion students’ productive skills and their accuracy level, incorporation of form-focused instruction with content instruction was introduced. However, little has been found about how immersion teachers actually implement form-focused instruction in their classrooms, especially in the Asian context. ^ This study reveals how first and fifth grade English immersion teachers in Japan incorporate form-focused instruction in mathematics and science lessons. The central question is whether (1) teachers’ beliefs toward the importance of target language teaching, (2) teachers’ beliefs toward child second language acquisition, and (3) students’ cognitive development {{have an impact on}} the amount and types of form-focused instruction that the teachers implement in their classroom. The results show that very little focus was placed on the target language teaching in the content classes, and only the teachers’ beliefs toward child second language acquisition had a slight influence on how much form-focused instruction they implement in their lessons. ...|$|E
40|$|When multivariate pattern {{decoding}} {{is applied}} to fMRI studies entailing more than two experimental conditions, a most common approach is to transform the multiclass classification problem {{into a series of}} binary problems. Furthermore, for decoding analyses, classification accuracy is often the only outcome reported although the topology of activation patterns in the high-dimensional features space may provide additional insights into underlying brain representations. Here we propose to decode and visualize voxel patterns of fMRI datasets consisting of multiple conditions with a supervised variant of self-organizing maps (SSOMs). Using simulations and real fMRI data, we evaluated the performance of our SSOM-based approach. Specifically, the analysis of simulated fMRI data with varying signal-to-noise and contrast-to-noise ratio suggested that SSOMs perform better than a k-nearest-neighbor classifier for medium and large numbers of features (i. e. 250 to 1000 or more voxels) and similar to support vector machines (SVMs) for small and medium numbers of features (i. e. 100 to 600 voxels). However, for a larger number of features (> 800 voxels), SSOMs performed worse than SVMs. When applied to a challenging 3 -class fMRI classification problem with datasets collected to examine the neural representation of three human voices at individual <b>speaker</b> <b>level,</b> the SSOM-based algorithm was able to decode speaker identity from auditory cortical activation patterns. Classification performances were similar between SSOMs and other decoding algorithms; however, the ability to visualize decoding models and underlying data topology of SSOMs promotes a more comprehensive understanding of classification outcomes. We further illustrated this visualization ability of SSOMs with a re-analysis of a dataset examining the representation of visual categories in the ventral visual cortex (Haxby et al., 2001). This analysis showed that SSOMs could retrieve and visualize topography and neighborhood relations of the brain representation of eight visual categories. We conclude that SSOMs are particularly suited for decoding datasets consisting of more than two classes and are optimally combined with approaches that reduce the number of voxels used for classification (e. g. region-of-interest or searchlight approaches) ...|$|E
40|$|This {{dissertation}} {{proposes a}} standardized {{approach to the}} analysis of contact-induced language change and applies the model to an examination of Faetar, a language which has developed in a situation of contact between Italian and Francoprovencal. The model incorporates factors which {{have been shown to be}} significant in determining contact-induced change and variation and indicates how these factors can be analyzed at the individual <b>speaker</b> <b>level.</b> Once a uniform approach to the analysis is taken, progress toward a model of contact-induced language change will be more rapid. This approach permits testing of claims regarding the relationship between social context and types of language change. ^ The language examined is Faetar, an unwritten and virtually unstudied Francoprovencal dialect which has been spoken in a village in southern Italy for the past 600 years. Because the language has been in contact with Italian as well as the colloquial dialects of the area, it has undergone many changes and is no longer mutually intelligible with Francoprovencal. Unlike many dialects spoken in isolated communities, Faetar does not appear to be dying as the language is held in high regard by its speakers. They recognize the prestige of this marker of their distinctness from other southern Italians. The question arises, however, of just how distinct the language is. In its centuries of contact with Italian, Faetar has changed in many ways. These contact-induced changes, and methods of analyzing them, are the focus of this dissertation. ^ Four phenomena that show the effects of Italian and Apulian dialects on Faetar are examined. Quantitative analysis of each pattern within a variationist framework, using both elicited and naturally occurring speech data, is presented. The variables examined are the appearance of word-medial and word-initial geminates, post-tonic deletion, and lexical choice. Degree of change at the lexical, phonological, and morphological levels is compared. ^ A detailed description of these phenomena augments the scarce data available for this uncodified language variety, contributing to the reconstruction of Francoprovencal as it was spoken some 600 years ago, as well as aiding the preservation of Faetar. ...|$|E
50|$|With {{permission}} {{from both of}} his neighbors, he put on various displays each year at his home. Typical light shows ran between 6 p.m. and 10 p.m. each night. Viewers heard the music on an FM broadcast in their cars. This kept the <b>speaker</b> noise <b>level</b> down for his neighbors.|$|R
40|$|International audienceWe {{analyzed}} the realizations of French voiced fricatives /,Ʒ/ by German non-native and French native speakers, in final position of an accentual group, {{a position where}} German fricatives are devoiced. Three <b>speaker</b> <b>levels</b> (from beginners to advanced speakers) and different boundary types (depending on whether the fricative {{is followed by a}} pause, a schwa, or is directly followed by the first phoneme of the subsequent group), are considered. A set of cues, among which periodicity, fricative duration, and intensity in low frequencies, is used for voicing analysis. Results show that German realizations vary significantly with language, speakers’ level and boundary type, and argue in favor of an influence of L 1 (German) final devoicing on non-native realizations. We discuss these effects as well as the influence of orthography (the presence of a schwa at the end of words) ...|$|R
50|$|Kenneth Savitsky and Thomas Gilovich {{performed}} two {{experiments on}} public speaking anxiety {{in relation to}} the illusion of transparency. The first focused on the speaker's perception of his or her anxiety levels versus an observer's perception of the <b>speaker's</b> anxiety <b>levels.</b> The results were as expected: the speaker judged himself or herself more harshly than the observer did.|$|R
40|$|The task of {{automatic}} speaker recognition, wherein a system verifies or determines a speaker's identity using {{a sample of}} speech, has been studied for a few decades. In that time, {{a great deal of}} {{progress has been made in}} improving the accuracy of the system's decisions, through the use of more successful machine learning algorithms, and the application of channel compensation techniques and other methodologies aimed at addressing sources of errors such as noise or data mismatch. In general, errors can be expected to have one or more causes, involving both intrinsic and extrinsic factors. Extrinsic factors correspond to external influences, including reverberation, noise, and channel or microphone effects. Intrinsic factors relate inherently to the speaker himself, and include sex, age, dialect, accent, emotion, speaking style, and other voice characteristics. This dissertation focuses on the relatively unexplored issue of dependence of system errors on intrinsic speaker characteristics. In particular, I investigate the phenomenon that some speakers within a given population have a tendency to cause a large proportion of errors, and explore ways of finding such speakers. There are two main components to this thesis. In the first, I establish the dependence of system performance on speakers, building upon and expanding previous work demonstrating the existence of speakers with tendencies to cause false alarm or false rejection errors. To this end, I explore two different data sets: one that is an older collection of telephone channel conversational speech, and one that is a more recent collection of conversational speech recorded on a variety of channels, including the telephone, as well as various types of microphones. Furthermore, in addition to considering a traditional speaker recognition system approach, for the second data set I utilize the outputs of a more contemporary approach that is better able to handle variations in channel. The results of such analysis repeatedly show variations in behavior across speakers, both for true speaker and impostor speaker cases. Variation occurs both at the level of speech utterances, wherein a given speaker's performance can depend on which of his speech utterances is used, as well as on the <b>speaker</b> <b>level,</b> wherein some speakers have overall tendencies to cause false rejection or false alarm errors. Additionally, lamb-ish speaker behavior (where the speaker tends to produce false alarms as the target) is correlated with wolf-ish behavior (where the speaker tends to produce false alarms as the impostor). On the more recent data set, 50 % of the false rejection and false alarm errors are caused by only 15 - 25 % of the speakers. The second component of this thesis investigates a straightforward approach to predict speakers that will be difficult for a system to correctly recognize. I use a variety of features to calculate feature statistics that are then used to compute a measure of similarity between speaker pairs. By ranking these similarity measures for a set of impostor speaker pairs, I determine those speaker pairs that are easy for a system to distinguish and those that are difficult-to-distinguish. A variety of these simple distance measures could successfully select both easy- and difficult-to-distinguish speaker pairs, as evaluated by differences in detection cost and false alarm probability across a large number of systems. Of those tested, the best feature-measure at finding the most and least difficult-to-distinguish speaker pairs was the Euclidean distance between vectors of the mean first, second, and third formant frequencies. Even greater success was attained by the Kullback-Liebler (KL) divergence between pairs of speaker-specific GMMs. Furthermore, an examination of the smallest and biggest distances (as computed by the KL divergence) revealed individual speaker tendencies to consistently fall among the most (or least) difficult-to-distinguish speaker pairs. I then develop an approach for finding those individual speakers who will be difficult for the system, using a set of feature statistics calculated over regions of speech. In particular, a support vector machine (SVM) classifier is trained to distinguish between difficult and easy speaker examples, in order to produce an overall measure of speaker difficulty as a target or impostor. The resulting precision and recall measures were over 0. 8 for difficult impostor speaker detection, and over 0. 7 for difficult target speaker detection. Depending on the application, the detection threshold can be tuned to improve precision, recall, or specificity in order to best suit the needs of a particular task. The same approach can be taken with single conversation sides, as with a set of conversation sides corresponding to the same speaker, since the input feature statistics can be calculated over any number of speech samples...|$|E
40|$|Abstract. This paper {{presents}} {{a method for}} the valuation of discourses from different linguistic perspectives: lexical, syntactic and semantic. We describe a platform (Discourse Analysis Tool – DAT) which integrates a range of language processing tools {{with the intent to}} build complex characterisations of the political discourse. The idea behind this construction is that the vocabulary and the clause structure of the sentence betray the <b>speaker’s</b> <b>level</b> of culture, while the semantic classes mentioned in a speech characterises the speaker’s orientation. When the object of study is the political discourse, an investigation on these dimensions could put in evidence features influencing the electing public. The method is intended to help both political speakers to improve their discourse abilities, by comparing their speeches with those of personalities of the public life in the past, and the public at large by evidencing hidden aspects of the linguistic and intellectual abilities of their candidates...|$|R
50|$|Catalan {{courses for}} adults are divided into: Beginners’ level (aimed at non-Catalan <b>speakers),</b> Intermediate <b>level</b> {{intended}} {{for people with}} a good command of the oral language and aimed at developing comprehension, speaking and writing skills, andAdvanced level. Specific training is designed for groups such a shopkeepers and local police or professional groups with their own specialised language such as legal language.|$|R
40|$|We {{present and}} {{evaluate}} {{a novel approach}} towards automatically detecting a <b>speaker’s</b> <b>level</b> of dominance in a meeting scenario. Since previous studies reveal that audio {{appears to be the}} most important modality for dominance recognition, we focus on the analysis of the speech signals recorded in multiparty meetings. Unlike recently published techniques which concentrate on frame-level hidden Markov modeling, we propose a recognition framework operating on segmental data and investigate context modeling on three different levels to explore possible performance gains. First, we apply a set of statistical functionals to capture large-scale feature-level context within a speech segment. Second, we consider bidirectional Long Short-Term Memory recurrent neural networks for long-range temporal context modeling between segments. Finally, we evaluate the benefit of situational context incorporation by simultaneously modeling speech of all meeting participants. Overall, our approach leads to a remarkable increase of recognition accuracy when compared to hidden Markov modeling. Index Terms: dominance recognition, meeting analysis, Long Short-Term Memory, audio feature extractio...|$|R
40|$|This study {{examines}} the occurrence {{and use of}} recasts in adult native-speaker/nonnative-speaker interactions in a nonclassroom setting. The study focuses on native-speaker recasts in three types of negotiations: one-signal negotiated interactions, extended negotiated interactions (Pica, 1988), and nonnegotiated interactions, and on re-casts in response to nonnative <b>speaker</b> <b>levels</b> of grammati-cality (single vs. multiple errors) resulting from four conversation tasks. The results show that recasts occur in different patterns than those reported in earlier research, but that these recasts are nevertheless used at rates con-sistent with previous research. These findings raise ques-tions concerning current criteria for determining the Susan M. Braidi, Department of Foreign Languages. I {{would like to thank}} the following people who have helped in the prepa-ration of this paper: Jennifer Thomas for help with data coding, James Dybdahl for data transcription, Pam Erramuzpe for research assistance, Dan Chilko and Magdalena Niewiadomska-Bugaj for their assistance with the statistical analysis, Frank W Medley, Jr., for comments on previous drafts, and the students for their participation in the study. This research was funde...|$|R
40|$|Computer-assisted {{language}} learning (CALL) {{is a form}} of computer-based assisted learning used in teaching to facilitate the {{language learning}} process. One major aspect of CALL for spoken language learning is the automatic assessment of pronunciation quality. It greatly relies on speech recognition technology to provide gradings for the pronunciation quality of the given input speech. This paper introduces a phone verification approach which allows the detection of mispronunciations at phone level. The detection thresholds can be determined based on the equal error rate (EER) metric using a database containing only native speech. In addition this approach also allows aggregation of assessment scores at sentence and <b>speaker</b> <b>levels</b> by computing the average phone rejection rates. This paper compares three different methods of generating goodness of pronunciation confidence scores. In addition, this paper also examines both unsupervised versus supervised adaptation techniques to improve the verification performance. Experimental results are reported based on the EER metric. APSIPA ASC 2009 : Asia-Pacific Signal and Information Processing Association, 2009 Annual Summit and Conference. 4 - 7 October 2009. Sapporo, Japan. Poster session: Automatic Speech Recognition (6 October 2009) ...|$|R
