775|1853|Public
5|$|The Economist Intelligence Unit, the {{business}} information {{arm of the}} Economist Group, gathered results from two internet questionnaires, one of business schools and one of their students and recent graduates, and used them to rate business schools located all over the world. Information provided by the schools made up 80% of the ranking, with student and alumni responses accounting for only 20%. Factors in the evaluation included faculty:student ratio, GMAT scores of incoming students, student body diversity, foreign languages offered, percentage of graduates finding jobs within three months after graduation, percentage of graduates finding jobs through the school's career service, graduates' salaries and the comparison of pre-enrollment and post-graduation salaries, and student/alumni evaluations of the program, facilities, services, and alumni network. Results were tabulated using a <b>smoothing</b> <b>method</b> incorporating the three previous years' results. The organization used strict data provision thresholds, {{with the result that}} some highly regarded schools were omitted from the list of 100 ranked schools.|$|E
500|$|Interactive {{graphics}} {{and other features}} were added in 1991 with version 2.0. Version 2 was twice the size as the original, though it was still delivered on a floppy disk. It required 2 MB of memory and came with 700 pages of documentation. Support for Microsoft Windows was added with version 3.1 in 1994. [...] Rewritten with Version 4 and released in 2002, JMP could import data from {{a wider variety of}} data sources and added support for surface plots. Version 4 also added time series forecasting and new smoothing models, such as the seasonal <b>smoothing</b> <b>method,</b> called Winter's Method, and ARIMA (Autoregressive Integrated Moving Average). It was also the first version to support JSL, JMP Scripting Language.|$|E
50|$|Generalized {{additive}} {{model is a}} <b>smoothing</b> <b>method</b> for multiple predictors that allows for non-parametric predictions.|$|E
40|$|Abstract We {{present an}} {{introduction}} to a class of <b>smoothing</b> <b>methods</b> for complementarity problems and their applications. We first discuss the features that characterize the <b>smoothing</b> <b>methods</b> for cornplemen-tarity problems. We then outline the algorithms and convergence analysis. We finally give a brief view of <b>smoothing</b> <b>methods</b> for variational inequalities, semi-infinite programs, constrained optimization problems and mathematical programming with equilibrium constraints...|$|R
40|$|Abstract We {{present an}} {{introduction}} to a class of <b>smoothing</b> <b>methods</b> for complementarity problems and their applications. We first discuss the features that characterize the <b>smoothing</b> <b>methods</b> for complementarity problems. We then outline the algorithms and convergence analysis. We finally give a brief view of <b>smoothing</b> <b>methods</b> for variational inequalities, semi-infinite programs, constrained optimization problems and mathematical programming with equilibrium constraints. Key Words: <b>smoothing</b> <b>methods,</b> nonsmooth equations, complementarity problems, P 0 function. 1. Introduction <b>Smoothing</b> <b>methods</b> {{have been developed for}} solving many important optimization problems including complementarity problems [3, 5, 8, 10, 13, 16, 33, 35, 37, 47, 48, 54, 57], variational inequalities [2, 17, 19, 30, 49], optimal control problems [39], semi-infinite programs [55], mathematical programs with equilibrium constraints [28, 34] and constrained optimization problems [1]. A feature of these problems is t [...] ...|$|R
40|$|We {{consider}} {{a class of}} <b>smoothing</b> <b>methods</b> for minimization problems where the feasible set is convex but the objective function is not convex, not differentiable and perhaps not even locally Lipschitz at the solutions. Such optimization problems arise from wide applications including image restoration, signal reconstruction, variable selection, optimal control, stochastic equilibrium and spherical approximations. In this paper, we focus on <b>smoothing</b> <b>methods</b> for solving such optimization problems, which use {{the structure of the}} minimization problems and composition of smoothing functions for the plus function (x) +. Many existing optimization algorithms and codes can be used in the inner iteration of the <b>smoothing</b> <b>methods.</b> We present properties of the smoothing functions and the gradient consistency of subdifferential associated with a smoothing function. Moreover, we describe how to update the smoothing parameter in the outer iteration of the <b>smoothing</b> <b>methods</b> to guarantee convergence of the <b>smoothing</b> <b>methods</b> to a stationary point of the original minimization problem. Department of Applied Mathematic...|$|R
5000|$|... 1.Using {{moving-average}} <b>smoothing</b> <b>method</b> {{to estimate}} the trend-cycle for all periods. In the monthly data, use 12-month centered moving average is appropriate to be applied {{to estimate the}} trend-cycle component.|$|E
50|$|For every {{exponential}} <b>smoothing</b> <b>method</b> we {{also need}} to choose the value for the smoothing parameters. For simple exponential smoothing, there is only one smoothing parameter (α), but for the methods that follow there is usually more than one smoothing parameter.|$|E
5000|$|The unknown {{parameters}} {{and the initial}} values for any exponential <b>smoothing</b> <b>method</b> can be estimated by minimizing the SSE. The errors are specified as [...] for t=1,…,T (the one-step-ahead within-sample forecast errors). Hence we find {{the values of the}} unknown {{parameters and}} the initial values that minimize ...|$|E
40|$|Recent work on {{language}} models for information retrieval {{has shown that}} smoothing language models is crucial for achieving good retrieval performance. Many different effective <b>smoothing</b> <b>methods</b> have been proposed, which mostly implement various heuristics to exploit corpus structures. In this paper, we propose a general and unified optimization framework for smoothing language models on graph structures. This framework not only provides a unified formulation of the existing smoothing heuristics, but {{also serves as a}} road map for systematically exploring <b>smoothing</b> <b>methods</b> for language models. We follow this road map and derive several different instantiations of the framework. Some of the instantiations lead to novel <b>smoothing</b> <b>methods.</b> Empirical results show that all such instantiations are effective with some outperforming {{the state of the art}} <b>smoothing</b> <b>methods...</b>|$|R
40|$|Abstract—In certain contexts, maximum entropy (ME) mod-eling can {{be viewed}} as maximum {{likelihood}} (ML) training for exponential models, and like other ML methods is prone to overfitting of training data. Several <b>smoothing</b> <b>methods</b> for ME models have been proposed to address this problem, but previous results do not make it clear how these <b>smoothing</b> <b>methods</b> compare with <b>smoothing</b> <b>methods</b> for other types of related models. In this work, we survey previous work in ME smoothing and compare the performance of several of these algorithms with conventional techniques for smoothing-gram language models. Because of the mature body of research in-gram model smoothing and the close connection between ME and conventional-gram models, this domain is well-suited to gauge the performance of ME <b>smoothing</b> <b>methods.</b> Over a large number of data sets, we find that fuzzy ME smoothing performs as well as or better than all other algorithms under consideration. We contrast this <b>method</b> with previous-gram <b>smoothing</b> <b>methods</b> to explain its superior performance. Index Terms—Exponential models, language modeling, max-imum entropy, minimum divergence,-gram models, smoothing...|$|R
40|$|This paper {{describes}} {{an approach to}} text classification using language models. This approach is {{a natural extension of}} the traditional Naïve Bayes classifier, in which we replace the Laplace smoothing by some more sophisticated <b>smoothing</b> <b>methods.</b> In this paper, we tested four <b>smoothing</b> <b>methods</b> commonly used in information retrieval. Our experimental results show that using a language model, we are able to obtain better performance than traditional Naïve Bayes classifier. In addition, we also introduce into the existing <b>smoothing</b> <b>methods</b> an additional factor of smoothing scale according to the amount of training data of the class, and this allows us to further improve the classification performance...|$|R
5000|$|NPMR is {{essentially}} a smoothing technique that can be cross-validated and applied in a predictive way. Many other smoothing techniques are well known, for example smoothing splines and wavelets. The optimal choice of a <b>smoothing</b> <b>method</b> depends on the specific application.Nonparametric regression models always fits for larger data ...|$|E
50|$|There {{are cases}} where the {{smoothing}} parameters may be chosen in a subjective manner — the forecaster specifies {{the value of the}} smoothing parameters based on previous experience. However, a more robust and objective way to obtain values for the unknown parameters included in any exponential <b>smoothing</b> <b>method</b> is to estimate them from the observed data.|$|E
5000|$|In other words, as {{time passes}} the {{smoothed}} statistic st becomes the weighted average of a greater and greater number of the past observations xt−n, and the weights assigned to previous observations are in general proportional {{to the terms of}} the geometric progression {1, (1 − α), (1 − α)2, (1 − α)3, ...}. A geometric progression is the discrete version of an exponential function, so this is where the name for this <b>smoothing</b> <b>method</b> originated according to Statistics lore.|$|E
40|$|Kernel <b>smoothing</b> <b>methods</b> {{can be used}} {{to better}} visualise data that are {{available}} over geography. The visualisations created by these methods allow users to observe trends in the data which may not be apparent from numerical summaries or point data. Crime data in the UK has recently been made available to the public at post code level, with visualisations of the amount of recorded events at each postcode for a given area available online. In this paper we apply existing kernel <b>smoothing</b> <b>methods</b> to this data, to assess the use of kernel <b>smoothing</b> <b>methods</b> as applied to open access data. Kernel <b>smoothing</b> <b>methods</b> are applied to crime data from the greater London metropolitan area, using methods freely available in R. The kernel smoothers used are demonstrated to provide effective and useful visualisations of the data, and it is proposed that these methods should be more widely applied in data visualisations in the future...|$|R
40|$|Abstract In {{this paper}} {{we present a}} class of sq~lared <b>smoothing</b> Newton <b>methods</b> for the box {{constrained}} variational inequality problem. This class of squared <b>smoothing</b> Newton <b>methods</b> is a regularized version {{of the class of}} <b>smoothing</b> Newton <b>methods</b> proposed in [25]. We tested all the test problem collections of GAMSLIB and MCPLIB wit,h all available starting points. Nurrlerical results indicate t,hat these squared <b>smoothing</b> Newton <b>methods</b> are extremely robust and promising. Key Words variational inequality problem, smoothing approximation, <b>smoothing</b> Newton <b>method,</b> regularizatior ~ method, convergence. ...|$|R
40|$|Classification {{learning}} {{is a type of}} supervised machine learning technique that uses a classification model (e. g. decision tree) to predict unknown class labels for previously unseen instances. In many applications it can be very useful to additionally obtain class probabilities for the different class labels. Decision trees that yield these probabilities are also called probability estimation trees (PETs). Smoothing is a technique used to improve the probability estimates. There are several existing <b>smoothing</b> <b>methods,</b> such as the Laplace correction, M-Estimate smoothing and M-Branch smoothing. Smoothing does not just apply to PETs. In the field of text compression, PPM in particular, <b>smoothing</b> <b>methods</b> play a important role. This thesis migrates <b>smoothing</b> <b>methods</b> from text compression to PETs. The newly migrated methods in PETs are compared with the best of the existing <b>smoothing</b> <b>methods</b> considered in this thesis under different experiment setups. Unpruned, pruned and bagged trees are considered in the experiments. The main finding is that the PPM-based methods yield the best probability estimate when used with bagged trees, but not when used with individual (pruned or unpruned) trees...|$|R
50|$|In 1966 Strutinsky made a {{breakthrough}} concerning {{the problem of}} incorporating shell effects into nuclear deformation energies {{higher than those of}} the liquid drop model (LDM). For this problem he devised an averaging method, now known as the Strutinsky <b>smoothing</b> <b>method.</b> At a 1969 Symposium in Lysekil, Sweden, he presented the results of applying his shell-correction method to calculating fission barriers, giving a physical explanation of the fission isomer — this was an experimental fact which had not yet been explained theoretically. The Strutinsky energy theorem and Strutinsky shell-corrections are applicable to various many-fermion systems, such as metal clusters and semiconductor quantum dots.|$|E
5000|$|Interactive {{graphics}} {{and other features}} were added in 1991 with version 2.0. Version 2 was twice the size as the original, though it was still delivered on a floppy disk. It required 2 MB of memory and came with 700 pages of documentation. Support for Microsoft Windows was added with version 3.1 in 1994. [...] Rewritten with Version 4 and released in 2002, JMP could import data from {{a wider variety of}} data sources and added support for surface plots. Version 4 also added time series forecasting and new smoothing models, such as the seasonal <b>smoothing</b> <b>method,</b> called Winter's Method, and ARIMA (Autoregressive Integrated Moving Average). It was also the first version to support JSL, JMP Scripting Language.|$|E
5000|$|An {{example is}} {{detailed}} {{in the pictures}} to the left. The goal {{of the project was}} to determine (among other things) whether or not the predictor, number of years in the major leagues (baseball,) had an effect on the response, salary, a player made. An initial scatter plot of the data indicates that there is heteroscedasticity in the data as the variance is not constant at each level of the predictor. Because we can visually detect the non-constant variance, it useful now to plot , and look to see if the shape is indicative of any known distribution. One can estimate [...] and [...] using a general <b>smoothing</b> <b>method.</b> The plot of the non-parametric smoothed variance function can give the researcher an idea {{of the relationship between the}} variance and the mean. The picture to the right indicates a quadratic relationship between the mean and the variance. As we saw above, the Gamma variance function is quadratic in the mean.|$|E
40|$|AbstractWe study local {{convergence}} of <b>smoothing</b> quasi-Newton <b>methods</b> for solving {{a system of}} nonsmooth (nondifferentiable) equations in Rn. The feature of <b>smoothing</b> quasi-Newton <b>methods</b> {{is to use a}} smooth function to approximate the nonsmooth mapping and update the quasi-Newton matrix at each step. Convergence results are given under directional derivative consistence property. Without differentiability we establish a Dennis-Moré-type superlinear convergence theorem for <b>smoothing</b> quasi-Newton <b>methods</b> and we prove linear {{convergence of}} the <b>smoothing</b> Broyden <b>method.</b> Furthermore, we propose a superlinear convergent <b>smoothing</b> Newton-Broyden <b>method</b> without using the generalized Jacobian and the semismooth assumption. We illustrate the smoothing approach on box constrained variational inequalities...|$|R
40|$|We {{consider}} the single, double and triple exponential <b>smoothing</b> <b>methods,</b> in which additive trend and seasonality may be included. These are compared {{among themselves and}} also with last-value forecasting. This is done using a theoretical model and data from an actual time series. We observe that, double or triple exponential <b>smoothing</b> <b>methods</b> may give more accurate forecasts than last-value or single exponential smoothing forecasts sometimes. But, often, {{there may not be}} much difference among such methods...|$|R
40|$|A {{combination}} of image processing algorithms {{can be used}} to detect and extract certain objects from images. Image noise may obscure the objects in the images and must rst be removed using image denoising and <b>smoothing</b> <b>methods</b> before the extraction of image objects can e ciently be attempted. Contour detection algorithms {{can be used to}} identify and extract objects from denoised images. The type of image, noise and noise intensity will determine the e ectiveness of the denoising and <b>smoothing</b> <b>methods.</b> The {{combination of}} the aforementioned will also determine the e ciency of the contour detection algorithms. In this dissertation we give some background on basic mathematical morphology techniques and we see how those techniques are used in various image smoothers and contour detection techniques. We develop our own smoother using some ideas from other well-known smoothers and compare the performance of the di erent <b>smoothing</b> <b>methods.</b> Various contour detection algorithms are investigated and we put the combination of the <b>smoothing</b> <b>methods</b> and the contour detection algorithms to the test by comparing how e ciently they detect and extract objects from images under various conditions. Dissertation (MSc) [...] University of Pretoria, 2015. tm 2015 StatisticsMScUnrestricte...|$|R
50|$|In {{the finite}} element method, an input domain with a complex {{geometry}} is partitioned into elements with simpler shapes; for instance, two-dimensional domains (either subsets of the Euclidean plane or surfaces in three dimensions) are often partitioned into triangles. It {{is important for}} the convergence of the finite element methods that these elements be well shaped; in the case of triangles, often elements that are nearly equilateral triangles are preferred. Lloyd's algorithm can be used to smooth a mesh generated by some other algorithm, moving its vertices and changing the connection pattern among its elements in order to produce triangles that are more closely equilateral. These applications typically use a smaller number of iterations of Lloyd's algorithm, stopping it to convergence, in order to preserve other features of the mesh such as differences in element size {{in different parts of the}} mesh. In contrast to a different <b>smoothing</b> <b>method,</b> Laplacian smoothing (in which mesh vertices are moved to the average of their neighbors' positions), Lloyd's algorithm can change the topology of the mesh, leading to more nearly equilateral elements as well as avoiding the problems with tangling that can arise with Laplacian smoothing. However, Laplacian smoothing can be applied more generally to meshes with non-triangular elements.|$|E
40|$|A new {{iterative}} <b>smoothing</b> <b>method</b> {{based on}} the extended Kalman filter is introduced to smooth noisy chaotic time series. Two examples are given to illustrate the <b>smoothing</b> <b>method.</b> The <b>smoothing</b> <b>method</b> is then employed as a noise prior to identification and prediction. Three different prediction methods are introduced and the prediction performance is compared using three nonlinear examples. Superior predictive performance is obtained by the prediction method that employs the pre-processing step on the data...|$|E
40|$|ABSTRACT. This paper {{proposes a}} <b>smoothing</b> <b>method</b> {{for the general}} n-dimensional max function, based on a {{recursive}} extension of smoothing functions for the 2 -dimensional max function. A theo-retical framework is introduced, and some applications are discussed. Finally, a numerical comparison with a well-known <b>smoothing</b> <b>method</b> is presented...|$|E
3000|$|... [...]), {{turn to be}} Gaussian {{and thus}} {{we are able to}} use {{computationally}} light Gaussian <b>smoothing</b> <b>methods</b> to infer the states of the system.|$|R
50|$|In statistics, several scatterplot <b>smoothing</b> <b>methods</b> are {{available}} to fit a function through the points of a scatterplot to best represent {{the relationship between the}} variables.|$|R
40|$|We {{provide a}} new {{approach}} to automatic business forecasting based on an extended range of exponential <b>smoothing</b> <b>methods.</b> Each method in our taxonomy of exponential <b>smoothing</b> <b>methods</b> can be shown to be equivalent to the forecasts obtained from a state space model. This allows (1) the easy calculation of the likelihood, the AIC and other model selection criteria; (2) the computation of prediction intervals for each method; and (3) random simulation from the underlying state space model. We demonstrate the methods by applying them to the data from the M-competition and the M 3 -competition...|$|R
40|$|This paper gives a {{definition}} for the convexity of B-spline surfaces {{and points out}} the conditions on which the convexity depends. A back shift <b>smoothing</b> <b>method</b> is introduced. This method is built {{on the basis of}} the convexity conditions. Application of this <b>smoothing</b> <b>method</b> gives a strictly convex curv...|$|E
40|$|The file {{attached}} to this record is the authors final peer reviewed version. The version of record can be found by following the DOI link. To resolve the conﬂict between our desire for a good smoothing eﬀect and desire to give additional weight to the recent change, a grey accumulating generation operator that can smooth the random interference of data is introduced into the double exponential <b>smoothing</b> <b>method.</b> The results of practical numerical examples have demonstrated that the proposed grey double exponential <b>smoothing</b> <b>method</b> outperforms the traditional double exponential <b>smoothing</b> <b>method</b> in forecasting problems...|$|E
40|$|A {{widespread}} forecasting method within {{supply chain}} models is the exponential <b>smoothing</b> <b>method.</b> The {{use of a}} particular forecasting method affects the costs of a supply chain. To improve {{the efficiency of the}} supply chain costs, this paper introduces the theory of wavelets. An application of this theory to the field of forecasting is wavelet denoising. Results obtained by the exponential <b>smoothing</b> <b>method</b> are compared to the results obtained by wavelet denoising. This comparison is supported by simulation experiments which include incorporation of forecasting algorithms within supply chain models. Different series of simulated data are used for testing these two methods and it is shown that wavelet denoising has an edge over the exponential <b>smoothing</b> <b>method</b> cost-wise. Supply chain management Inventory costs Penalty costs Forecasting methods Exponential <b>smoothing</b> <b>method</b> Wavelet denoising...|$|E
40|$|Nonparametric <b>smoothing</b> <b>methods</b> are {{increasingly}} used since computing power is easier available. Analyzing samples by these methods usually {{is done by}} varying the smoothing parameter. This may lead to {{an enormous amount of}} calculations which make an interactive analysis impossible. In this paper we demonstrate how the computational burden can be reduced by discretization methods. We first focus on density estimation and density derivatives estimation since most nonparametric <b>smoothing</b> <b>methods</b> involve the estimation of such quantities. The ideas derived in this context are then employed [or Average Derivative Estimation. ...|$|R
40|$|In this paper, {{we propose}} {{a new and}} {{powerful}} mesh/soup denoising technique. Our approach is inspired by recent non-local image denoising schemes and naturally extends bilateral mesh <b>smoothing</b> <b>methods.</b> The main idea behind the approach is very simple. A new position of vertex $P$ of a noisy mesh is obtained as a weighted mean of mesh vertices $Q$ with nonlinear weights reflecting a similarity between local neighborhoods of $P$ and $Q$. We demonstrated that our technique outperforms recent state-of-the-art <b>smoothing</b> <b>methods.</b> We also suggest a new approach for comparing different mesh/soup denoising methods...|$|R
3000|$|To {{localize}} immersed sources, we {{proposed to}} compare two methods: classical methods, {{based on a}} spatial analysis of the spatial covariance matrix of the data to estimate the DOA [5], and the proposed method, based on a spatio-temporal analysis which first estimates TDOA from the frequential covariance matrix of the data on each sensor and then estimates the DOA and range of the sources. To decorrelate the signals, <b>smoothing</b> <b>methods</b> are used. Spatial <b>smoothing</b> for classical <b>methods</b> [18] and frequential smoothing for the proposed methods are used. The <b>smoothing</b> <b>methods</b> in spatial (respectively, frequential) domain require {{that the number of}} sensors N (respectively, the number M of frequencies) must be {{greater than or equal to}} [...]...|$|R
