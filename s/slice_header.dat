6|7|Public
40|$|This paper {{considers}} {{the problem of}} data recovery and reconstruction in erroneous MPEG- 2 video sequences. The basic resynchronization point in MPEG- 2 video bitstream is the <b>slice</b> <b>header,</b> a slice usually denoting a full row of macroblocks. When an error occurs, {{the rest of the}} damaged slice is lost up to the next slice. In order to improve the efficiency of conventional error concealment schemes, we propose to exploit the error detection information to force the decoding of errorfree bits immediately after a lost area, before reaching either the next resynchronization point or the next erroneous area. This early resynchronization is achieved by trying to decode variable length codes until some macroblocks are recognized. In order to retrieve differentially coded data such as DC coefficients and macroblocks positions, a specific algorithm has been designed which uses not only available AC coefficients and neighbouring data, but also differential values decoded from the earlyresynchronize [...] ...|$|E
40|$|Weighted {{prediction}} (WP) is {{an efficient}} video coding tool that was introduced since {{the establishment of}} the H. 264 /AVC video coding standard, for compensating the temporal illumination change in motion estimation and compensation. WP parameters, including a multiplicative weight and an additive offset for each reference frame, are required to be estimated and transmitted to the decoder by <b>slice</b> <b>header.</b> These parameters cause extra bits in the coded video bitstream. High efficiency video coding (HEVC) provides WP parameter prediction to reduce the overhead. Therefore, WP parameter prediction is crucial to research works or applications, which are related to WP. Prior art has been suggested to further improve the WP parameter prediction by implicit prediction of image characteristics and derivation of parameters. By exploiting both temporal and interlayer redundancies, we propose three WP parameter prediction algorithms, enhanced implicit WP parameter, enhanced direct WP parameter derivation, and interlayer WP parameter, to further improve the coding efficiency of HEVC. Results show that our proposed algorithms can achieve up to 5. 83 % and 5. 23 % bitrate reduction compared to the conventional scalable HEVC in the base layer for SNR scalability and 2 x spatial scalability, respectively. Department of Electronic and Information Engineering 2016 - 2017 > Academic research: refereed > Publication in refereed journalbcr...|$|E
40|$|Commercial {{chip design}} {{verification}} {{is a complex}} activity involving many abstraction levels (such as architectural, register transfer, gate, switch, circuit, fabrication), many different aspects of design (such as timing, speed, functional, power, reliability and manufacturability) and many different design styles (such as ASIC, full custom, semi-custom, memory, cores, and asynchronous). In this paper, functional coverage analysis of verification of RTL (Register Transfer Level) design of H. 264 CAVLD (context-based adaptive variable length decoding) <b>slice</b> <b>header</b> decoder using SystemVerilog implementation of OVM (open verification methodology) is presented. The methodology used for verification is OVM which has gathered very positive press coverage, including awards from magazines and industry organizations. There {{is no doubt that}} the OVM is one of the biggest stories in recent EDA (electronic design automation) history. The SystemVerilog language is at the heart of the OVM which inherited features from Verilog HDL, VHDL, C, C++ and adopted by IEEE as hardware description and verification language in 2005. The verification environment developed in OVM provides multiple levels of reuse, both within projects and between projects. Emphasis is put onto the actual usage of the verification components and functional coverage. The whole verification is done using SystemVerilog hardware description and verification language. We are using QuestaSim 6. 6 b for simulation...|$|E
50|$|There are no {{restrictions}} on how goroutines access shared data, making race conditions possible. Specifically, unless a program explicitly synchronizes via channels or other means, writes from one goroutine might be partly, entirely, {{or not at}} all visible to another, often with no guarantees about ordering of writes. Furthermore, Go's internal data structures like interface values, <b>slice</b> <b>headers,</b> hash tables, and string headers are not immune to race conditions, so type and memory safety can be violated in multithreaded programs that modify shared instances of those types without synchronization.|$|R
30|$|The {{purpose of}} rate control is to compute QP for all frames within the {{allowable}} rates. With FMO enabled, {{the effect on}} the rate control is the increased number of header bits because of PPS and <b>slice</b> <b>headers,</b> and higher buffer levels because of loss of coding efficiency as compared to not using FMO. The proposed improvements to the frame layer rate control of H. 264 /AVC are improved bit allocation by modifying the target bit using the frame complexity measure, enhancement of the existing MAD complexity measure, a new header bits model and adjustment of QP with FMO considerations.|$|R
40|$|We {{address a}} new error-resilient scheme for broadcast-quality {{interactive}} MPEG- 2 video streams to be transmitted over lossy packet networks. A new scene-complexity adaptive mechanism, namely Adaptive MPEG- 2 Information Structuring and Protection (AMISP) is introduced. AMISP lies on an information structuring scheme which modulates {{the number of}} resynchronization points (i. e., <b>slice</b> <b>headers</b> and intra-coded macroblocks) {{in order to maximize}} the perceived video quality. The video quality the end-user experiences depends both {{on the quality of the}} compressed video before transmission and on the degradation due to packet loss. Therefore, the structuring scheme constantly determines the best compromise between the rate allocated to encoding pure video information and the rate aiming at reducing the sensitivity to packet loss. It is then extended with a Forward Error Correction (FEC) based protection algorithm to become AMISP. AMISP triggers the insertion of FEC packets in the MPEG- 2 video pa [...] ...|$|R
40|$|Abstract- Weighted {{prediction}} (WP) {{is a tool}} {{to compensate}} In section II, we would first review some weight paramethe brightness difference in video sequences with brightness ters determination methods and later discuss the challenges variations. In this paper, some weight parameter determination in hardware implementation in section III. We will present methods are surveyed, and a weighted prediction algorithm together with the hardware architecture design is proposed. The our scheme and the execution flow together with hardware main idea of the algorithm is {{to limit the number}} of weight architecture in section IV. Simulation results and conclusion parameters transmitted by quantizing the parameter into levels will be given in section V and VI respectively. and using only offset as the parameter. As a result, the extra parameters sent in each <b>slice</b> <b>header</b> is thus limited by the number II. SURVEY OF WEIGHTED PREDICTION METHODS of levels, and the parameter determination process requires much less computations. By further utilizing sub-sampling of brightness The proces igted prediction in hvbrid videocoding levels and estimated offset sum, a simplified architecture is also can be divided into three parts. Survey and analysis on the proposed. Simulation result shows that the later architecture methods proposed by some papers is discussed below. achieves a coding gain of about 0. 5 dB over weighted predictio...|$|E
30|$|The H. 264 video decoder {{application}} {{is taken as}} main use case application. It is a high quality video compression algorithm relying on several efficient strategies extracting spatial (within a frame) and temporal dependencies (between frames). This {{application is}} characterized by a flexible coding, high compression and high quality resolution. Moreover, it is a promising standard for embedded devices. The main steps of the H. 264 decoding process consist in the following: First, a compressed bit stream coming from the Network application layer (NAL), which formats the representation of the video and provides header information in a manner appropriate for conveyance by particular transport layers, is received at the input of the decoder. Then, the entropy decoded bloc begins with decoding the <b>slice</b> <b>header</b> where each slice consists of one or more 1616 macroblocks, and then it decodes the other parameters. The decoded data are entropy and sorted to produce a set of quantized coefficients. These coefficients are then inverse quantized and inverse transformed. Thereafter, the data obtained are added to the predicted data from the previous frames depending upon the header information. Finally the original block is obtained after the de-blocking filter to compensate the block artifacts effect.The H. 264 video decoder application can be broken down into various tasks sets corresponding to different types of parallelization. In our experiments, we use the slices version, one of the task models of H. 264 proposed by Thales Group, France[21] in the context of French national project Pherma[22].|$|E
40|$|With low {{bit rate}} scenarios, a hybrid video coder (e. g. AVC/H. 264) tends to {{allocate}} greater portion of bits for motion vectors, while saving bits on residual errors. According to this fact, a coding scheme with non-normative global motion models in combination with conventional local motion vectors is proposed, which describes the motion of a frame by the affine motion parameter sets drawn by motion segmentation of the luminance channel. The motion segmentation task is capable of adapting the number of motion objects to the video contents. 6 -D affine model sets are driven by linear regression from the scalable block-based motion fields estimated by the existent MPEG encoder. In cases {{that the number of}} motion objects exceeds a certain threshold, the global affine models are disabled. Otherwise the 4 scaling factors of the affine models are compressed by a vector quantizer, designed with a unique cache memory for efficient searching and coding. The affine motion information is written in the <b>slice</b> <b>header</b> as a syntax. The global motion information is used for compensating those macroblocks whose Lagrange cost is minimized by the AFFINE mode. The rate-distortion cost is computed by a modified Lagrange equation, which takes into consideration the perceptual discrimination of human vision in different areas. Besides increasing the coding efficiency, the global affine model manifests the following two features that refine the compressed video quality. i) When the number of slices per frame is more than 1, the global affine motion model can enhance the error-resilience of the video stream, because the affine motion parameters are duplicated in the headers of different slices over the same frame. ii) The global motion model predicts a frame by warping the whole reference frame and this helps to decrease blocking artifacts in the compensation frame. Ph. D. Committee Chair: Jackson, Joel; Committee Member: anderson, david; Committee Member: fritz, hermann; Committee Member: Mersereau, Russel; Committee Member: Yezzi, Anthon...|$|E
40|$|This work {{addresses}} the optimization of TV-resolution MPEG- 2 video streams to be transmitted over lossy packet networks. This paper introduces a new scene-complexity adaptive mechanism, namely the Adaptive MPEG- 2 Information Structuring (AMIS) mechanism. AMIS adaptively modulates {{the number of}} resynchronization points (i. e the <b>slice</b> <b>headers</b> and intra-coded macroblocks) {{in order to maximize}} the perceived video quality assuming it is aware of the packet loss probability and the error concealment technique implemented in the decoder. The perceived video quality depends both on the encoding quality and the degradation due to data loss. Therefore, AMIS constantly determines the best compromise between the rate allocated to pure video information and the rate aiming at reducing the sensitivity to packet loss. Results show that the proposed algorithm behaves much better than the traditional MPEG- 2 encoding scheme in terms of perceived video quality under the same traffic constraints. Keyword [...] ...|$|R
30|$|The use of {{flexible}} macroblock ordering (FMO) in H. 264 /AVC improves error resiliency {{at the expense}} of reduced coding efficiency with added overhead bits for <b>slice</b> <b>headers</b> and signalling. The trade-off is most severe at low bit rates, where header bits occupy {{a significant portion of the}} total bit budget. To better manage the rate and improve coding efficiency, we propose enhancements to the H. 264 /AVC frame layer rate control, which take into consideration the effects of using FMO for video transmission. In this article, we propose a new header bits model, an enhanced frame complexity measure, a bit allocation and a quantization parameter adjustment scheme. Simulation results show that the proposed improvements achieve better visual quality compared with the JM 9.2 frame layer rate control with FMO enabled using a different number of slice groups. Using FMO as an error resilient tool with better rate management is suitable in applications that have limited bandwidth and in error prone environments such as video transmission for mobile terminals.|$|R
30|$|The main {{advantage}} of using FMO {{is the ability}} to contain the spatial propagation of error within the slice boundary. Since each slice is designed to be decodable independently of other slices, using FMO allows the encoder and decoder to resynchronize their states at the slice boundary in the event that there is an error in the bit stream. Using FMO also provides a way to spread the erroneous MBs within the frame and take {{advantage of}} the spatial locations of the successfully decoded MBs for better error concealment. However, using FMO for added error resiliency has some trade-offs in coding efficiency. Coding efficiency is reduced because of the restriction of intra prediction across slice boundaries. The motion vector prediction is affected because of having constrained or dispersed search space. The context adaptive variable length coding/context adaptive arithmetic coding entropy coding is also reset {{at the beginning of each}} slice. Using FMO also adds overhead bits because of <b>slice</b> <b>headers</b> and PPS bits. If the MB-to-slice group map, also referred to as an MB address map or an MBA map, is changed in every frame, then a PPS header has to be constructed and inserted in the bit stream.|$|R

