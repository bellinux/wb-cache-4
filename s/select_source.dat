26|2501|Public
5000|$|Mandeep Singh Sodhi, CEO of <b>Select</b> <b>Source</b> International, President and CEO of Datawave Inc, President and CEO of Jobma.com ...|$|E
50|$|Water {{management}} engineering {{developments in}} the past century have degraded these wetlands through the construction on artificial embankments. These constructions may be classified as dykes, bunds, levees, weirs, barrages and dams but serve the single purpose of concentrating water into a <b>select</b> <b>source</b> or area. Wetland water sources that were once spread slowly over a large, shallow area are pooled into deep, concentrated locations. Loss of wetland floodplains results in more severe and damaging flooding. Catastrophic human impact in the Mississippi River floodplains was seen in death of several hundred individuals during a levee breach in New Orleans caused by Hurricane Katrina. Ecological catastrophic events from human-made embankments have been noticed along the Yangtze River floodplains since the middle of the river has become prone to more frequent and damaging flooding. Some of these events include the loss of riparian vegetation, a 30% loss of the vegetation cover throughout the river's basin, a doubling of the percentage of the land affected by soil erosion, and a reduction in reservoir capacity through siltation build-up in floodplain lakes.|$|E
30|$|<b>Select</b> <b>source</b> {{image and}} find the largest human region in each source image.|$|E
50|$|<b>Selected</b> <b>sources</b> such as {{scientific}} papers, etc.|$|R
50|$|Communities in Canada: <b>Selected</b> <b>Sources.</b> Toronto: McClelland, 1970.|$|R
2500|$|See also First Crusade – <b>selected</b> <b>sources</b> {{and further}} reading ...|$|R
3000|$|For {{a set of}} {{candidate}} cities T = {T_k |k = 1, 2, [...]...,m}, if the ith city Ti is the target city, {{the goal of the}} source city selection step is to <b>select</b> <b>source</b> cities from the remaining m[*]−[*] 1 cities. Thus, the number of possible combinations is 2 m− 1. Exhaustive search on such a large number {{of candidate}} combinations is impossible. In order to overcome this problem, source cities are selected in the following way.|$|E
40|$|Descriptors of foreign-language reading ability, {{such as the}} U. S. Inter-agency Language Roundtable (ILR) reading {{levels and}} the Common European Framework of References for Languages (CEFR), are some-times uses to <b>select</b> <b>source</b> {{passages}} for official and professional transla-tion tests. However, {{little research has been}} done on whether these de-scriptors correspond to translation difficulties. This paper examines er-rors made in a small set of Japanese-to-English tests from the American Translators Association Certification Examination to determine whether the errors could be predicted from the reading level descriptors...|$|E
40|$|A novel {{image fusion}} {{algorithm}} based on bandelet transform is proposed. Bandelet transform {{can take advantage}} of the geometrical regularity of image structure and represent sharp image transitions such as edges efficiently in image fusion. For reconstructing the fused image, the maximum rule is used to <b>select</b> <b>source</b> images’ geometric flow and bandelet coefficients. Experimental results indicate that the bandelet-based fusion algorithm represents the edge and detailed information well and outperforms the wavelet-based and Laplacian pyramid-based fusion algorithms, especially when the abundant texture and edges are contained in the source images. Navigation Science Foundation (No. 05 F 07001) and the National Natural Science Foundation of China (No. 60472081) ...|$|E
30|$|The {{cooperative}} {{communications between}} the relay and the <b>selected</b> <b>source</b> begin.|$|R
5000|$|Differentiating (filtering and <b>selecting</b> <b>sources</b> {{based on}} {{judgement}} {{of quality and}} relevance) ...|$|R
5000|$|Josephinism. <b>Selected</b> <b>sources</b> on {{the history}} of theresianisch-Josephine reforms, Wiss. Book Company, Darmstadt, 1995, [...]|$|R
40|$|Transfer {{learning}} significantly accelerates the {{reinforcement learning}} process by exploiting relevant knowledge from previous experiences. The problem of optimally selecting source policies during {{the learning process}} is of great importance yet challenging. There has been little theoretical analysis of this problem. In this paper, we develop an optimal online method to <b>select</b> <b>source</b> policies for reinforcement learning. This method formulates online source policy selection as a multi-armed bandit problem and augments Q-learning with policy reuse. We provide theoretical guarantees of the optimal selection process and convergence to the optimal policy. In addition, we conduct experiments on a grid-based robot navigation domain to demonstrate its efficiency and robustness by comparing to the state-of-the-art transfer learning method...|$|E
40|$|A {{system for}} {{generating}} visualizations of routes in 3 D environments is presented. From {{the input of}} a 3 D triangle mesh, the approximate height of a navigating agent, {{and the number of}} floors, the system generates a segmentation of the environment into rooms and a roadmap for routing using a version of the watershed algorithm. A user may then interactively <b>select</b> <b>source</b> and destination rooms, and a shortest-path algorithm on the roadmap discovers the optimal route. Finally, the system selects viewing parameters by optimizing a score function of the view to produce a single static image of the path in an exploded view. The resulting visualization of the route displays simultaneously internal structural features of the building at each relevant floor while maintaining path visibility and clarity of important navigational cues such as changes in orientation. Keywords...|$|E
40|$|An {{important}} issue in ad-hoc on-demand distance vector (AODV) routing protocols is route failure caused by node mobility in the MANETs. The AODV requires a new route discovery procedure whenever a route breaks and these frequent route discoveries increase transmission delays and routing overhead. The present study proposes a new method for AODVs using a genetic algorithm {{to improve the}} route recovery mechanism. When failure occurs in a route, the proposed method (GAAODV) makes decisions regarding the QOS parameter to <b>select</b> <b>source</b> or local repair. The task of the genetic algorithm is to find an appropriate combination of weights to optimize end-to-end delay. This paper evaluates the metrics of routing overhead, average end-to-end delay, and packet delivery ratio. Comparison of the new algorithm and AODV (RFC 3561) using a NS- 2 simulator shows that GAAODV obtains better results for the QOS parameters...|$|E
30|$|Considering the tariffs, each {{power plant}} in each period adopts pricing {{strategy}} for the <b>selected</b> <b>source.</b>|$|R
5000|$|Historians on History (in Bulgarian, Sofia, 1988), <b>Selected</b> <b>Sources</b> for Balkan History (in Bulgarian, Sofia, 1977) ...|$|R
5000|$|Ross, Carl Radicalism in Minnesota, 1900-1960: {{a survey}} of <b>selected</b> <b>sources</b> (Minnesota Historical Society Press. 1994) ...|$|R
40|$|Cross-correlating {{one month}} of ambient seismic noise {{recorded}} at USArray stations in California yields hundreds of short period surfacewave group-speed measurements on inter-station paths. We used these measurements to construct tomographic {{images of the}} principal geological units beneath California, with low-speed anomalies corresponding to the main sedimentary basins and high-speed anomalies corresponding to the igneous cores of the major mountain ranges. This method can improve the resolution and fidelity of crustal images obtained from surface wave analyses. The aim of ambitious new deployments of seismic arrays, such as the PASSCAL and USArray programs (1), {{is to improve the}} resolution of images of Earth’s interior by adding more instruments to regional- and continental-scale seismic networks. Traditional obser- 1 vational methods cannot fully exploit emerging array data because they are based on seismic waves emitted from earthquakes, which emanate from <b>select</b> <b>source</b> regions predominantl...|$|E
40|$|Testing {{electronics}} for {{vulnerability to}} radio frequency (RF) radiation is time-consuming, {{due to the}} large number of source variables of interest. One typically searches for the minimum electric field that causes upset, as a function of center frequency, pulse width, pulse repetition frequency, number of pulses, and bandwidth. It is impossible to test all combinations of all the variables, so one must intelligently select the source parameters most likely to expose the greatest vulnerability. To <b>select</b> <b>source</b> parameters, we propose using standard techniques from minimization theory. Within a space of two or more variables, we search for the combination that upsets the system at the lowest power or field level. We investigated the vulnerability of media converters (MCs) to pulsed RF fields. We tested these devices by pinging a remote computer, and observing the field levels at which the pings failed to return. ...|$|E
40|$|Unit {{value or}} point value {{transfers}} from individual source studies remain {{the oldest and}} most common form of benefit transfer. Although practitioners generally recommend benefit function transfers, these are not always possible. Where unit value transfers are to be performed, appropriate protocols must be followed to <b>select</b> <b>source</b> studies, transfer values, and perform necessary value adjustments. This chapter demonstrates the processes and challenges involved in the implementation of unit value transfers, using case studies of environmental values in a peri-urban community on the east coast of Australia where key ecosystems ranged from coastal beaches to inland forests. Key issues in evaluating the potential for benefit transfer included the availability and quality of source studies, the extent of overlap between source studies and the target site, the need for different forms of adjustment to account for variations in scope and scale, and the limitations to unit value transfers...|$|E
30|$|After {{receiving}} {{all sources}} confirmations, the relay informs one <b>selected</b> <b>source</b> whose choice {{can make the}} relay to maximize its revenue.|$|R
2500|$|J. Hamilton, Bernard Hamilton, and Yuri Stoyanov. Christian Dualist Heresies in the Byzantine World, C. 650-C. 1450: <b>Selected</b> <b>Sources</b> (New York 1998) ...|$|R
5000|$|Lambert of Hersfeld: Annalen. Wissenschaftliche Buchgesellschaft, Darmstadt, 1957. (<b>Selected</b> <b>sources</b> on German {{history of}} the Middle Ages. Freiherr vom Stein {{memorial}} edition, 13) ...|$|R
40|$|A novel {{image fusion}} {{algorithm}} based on bandelet transform is proposed. Bandelet transform {{can take advantage}} of the geometrical regularity of image structure and represent sharp image transitions such as edges efficiently in image fusion. For reconstructing the fused image, the maximum rule is used to <b>select</b> <b>source</b> images ’ geometric flow and bandelet coefficients. Experimental results indicate that the bandelet-based fusion algorithm represents the edge and detailed information well and outperforms the wavelet-based and Laplacian pyramid-based fusion algorithms, especially when the abundant texture and edges are contained in the source images. OCIS codes: 100. 0100, 100. 7410, 350. 2660, 350. 6980. Image fusion is the combination of two or more different images to form a new image by using a certain algorithm[1]. The combination of sensory data from mul-tiple sensors can provide more reliable and accurate in-formation. It forms a rapidly developing research area in remote sensing, medical image processing, and com-puter vision[2 − 4]. Most of these approaches were base...|$|E
40|$|A {{common goal}} for {{transfer}} learning {{research is to}} show that a learner can solve a source task and then leverage the learned knowledge to solve a target task faster than if it had learned the target task directly. A more difficult goal is to reduce the total training time so that learning the source task and target task is faster than learning only the target task. This paper addresses the second goal by proposing a transfer hierarchy for 2 -player games. Such a hierarchy orders games in terms of relative solution difficulty and can be used to <b>select</b> <b>source</b> tasks that are faster to learn than a given target task. We empirically test transfer between two types of tasks in the General Game Playing domain, the testbed for an international competition developed at Stanford. Our results show that transferring learned search heuristics from tasks {{in different parts of the}} hierarchy can significantly speed up search even when the source and target tasks differ along a number of important dimensions...|$|E
40|$|In {{the recent}} years the NFC with the {{combination}} of smart devices has widened the utilization range of NFC. It is said it will replace the credit card in electronic payment so security {{is one of the}} area that is to be checked. Currently the NFC security requires user’s public key with the fixed value which contain the message. The attacker can create a new profile of the user by using their public key and thus the privacy of the user is compromised. In this work, network environment can be generated using 50 nodes. User can <b>select</b> <b>source</b> and destination node and then simulation can be done on the basis of proposed algorithm. The conditional privacy protection based on multiple pseudonyms is used to solve the issues generated in NFC environment. This work is to optimize the above environment by using optimization algorithm. Genetic algorithm which is artificial intelligence based algorithm is used in this to reduce the storage requirement, average delay, packet drop ratio and improve the network’s throughput...|$|E
5000|$|Lambert of Hersfeld: Annalen, Darmstadt 1957. (= <b>selected</b> <b>sources</b> {{about the}} German {{history of the}} Middle Ages. Freiherr vom Stein - {{memorial}} edition; 13) ...|$|R
5000|$|Search {{enhancement}} {{system with}} {{information from a}} <b>selected</b> <b>source</b> (United States Patent 7,640,232; December 29, 2009, Co-inventor with Edmund J. Fish while at AOL).|$|R
25|$|Both {{styles of}} auger bits were {{manufactured}} by several companies throughout the early- and mid-20th century, {{and are still}} available new from <b>select</b> <b>sources</b> today.|$|R
40|$|It is a {{well-known}} fact that medieval scribes often used several manuscripts as their sources {{in order to produce}} a new copy of a text; this is because every source manuscript can contain errors or be damaged. Because scribes did not attempt and were not able to <b>select</b> <b>source</b> manuscripts belonging to the same textual group, the new copy might reflect more than one textual tradition. Translators from Greek into Church Slavonic apparently had the same problems with their sources as scribes did. Moreover, translators had even more difficult problems due to itacism and the numerous abbreviations used in Byzantine manuscripts. However, so far scholars have provided no evidence for the use of multiple Greek manuscripts for translations into Church Slavonic. In this article a few instances of contaminated readings (conflations) from the Old Russian translation of the Life of St.  Andrew the Fool are cited. They reflect variant readings from different Greek manuscripts and seem to prove that the translator worked from at least two Greek manuscript sources, which enabled him to choose the wording he considered to be the best for any given passage...|$|E
30|$|Since the {{variation}} pattern of load during holidays {{is different than}} that of non-holidays, forecasting holiday load is a challenging task. With a focus on this problem, we propose a learning framework based on weighted knowledge transfer for daily peak load forecasting during holidays. First, we <b>select</b> <b>source</b> cities which can provide extra hidden knowledge to improve the forecast accuracy of the load of the target city. Then, all the instances which are from source cities and the target city will be weighted and trained by the improved weighted transfer learning algorithm {{which is based on}} the TrAdaBoost algorithm and can decrease negative transfer. We evaluate our method with the classical support vector machine method and a method based on knowledge transfer on a real data set, which includes eleven cities from Guangdong province to illustrate the performance of the method. To solve the problem of limited historical holiday load data, we transfer the data from nearby cities {{based on the fact that}} nearby cities in Guangdong province have a similar economic development level and similar load variation pattern. The results of comparative experiments show that the forecasting framework proposed by this paper outperforms these methods in terms of mean absolute percent error and mean absolute scaled error.|$|E
40|$|Documentaries present static {{images in}} an {{engaging}} manner by panning across them in 3 D. This is normally a laborious process involving hours of manual rotoscoping. Our system automates the process, if given an image with depth. Overview Our system takes an image plus depth information. We segment this into layers, {{which are used}} to synthesise new views of the scene and create effects that rely on occluded image and depth data, such as camera pans and depth-of-field. Hard Segmentation We use a hard segmentation technique based on GrabCut 1. Where the original GrabCut uses colour pixels, that is pixels in 3 D RGB space, we extend it to depth by considering pixels in 4 D RGBZ space; this gives a cleaner segmentation where objects at different depths have similar colour. Inpainting Criminisi’s exemplar-based inpainting Once the foreground object is extracted, the area behind it is invalidated and filled in using exemplar-based inpainting 3, extended to use and fill depth information. We <b>select</b> <b>source</b> patches by finding patches which are: Result The output of our system {{is a set of}} layers, with each layer being an image with depth. These layers can then be rendered easily, for example as meshes, using standard methods. Effects, such as depth-of-field, can be applied as they would be to normal 3 D geometry, without occlusion issues or rubber sheet effects. Input image and depth map Foremost object selection GrabCut RGB segmentation Our RGBZ segmentatio...|$|E
500|$|Herman of Reichenau: Chronicle. In: Eleventh-Century Germany: The Swabian Chronicles (<b>selected</b> <b>sources</b> {{translated}} and annotated with {{an introduction}} by I. S. Robinson) (2008); Manchester University Press; [...]|$|R
3000|$|... [...]. As {{the number}} of layers is limited by layered source encoder and {{transmission}} bandwidth, we can try some available value of <b>selected</b> <b>source</b> layers L [...]...|$|R
5000|$|Herman of Reichenau: Chronicle. In: Eleventh-century Germany: The Swabian Chronicles (<b>selected</b> <b>sources</b> {{translated}} and annotated with {{an introduction}} by I. S. Robinson) (2008); Manchester University Press; [...]|$|R
