21|86|Public
50|$|Each 64-bit Cyclops64 chip (processor) {{will run}} at 500 {{megahertz}} and contain 80 processors. Each processor {{will have two}} thread units and a floating point unit. A thread unit is an in-order 64-bit RISC core with 32 kB <b>scratch</b> <b>pad</b> <b>memory,</b> using a 60-instruction subset of the Power Architecture instruction set. Five processors share a 32 kB instruction cache.|$|E
50|$|On-chip caches uses static RAM that consume {{power in}} the range of 25% to 50% of the total chip power and {{occupies}} about 50% of the total chip area. Scratchpad memory occupies lesser area than on-chip caches. This will typically reduce the energy consumption of the memory unit, because less area implies reduction in the total switched capacitance. Current embedded processors particularly in the area of multimedia applications and graphic controllers have on-chip scratch pad memories. In cache memory systems, the mapping of program elements is done during run time, whereas in <b>scratch</b> <b>pad</b> <b>memory</b> systems this is done either by the user or automatically by the compiler using suitable algorithm.|$|E
40|$|Abstract — In {{an effort}} to make {{processors}} more power efficient <b>scratch</b> <b>pad</b> <b>memory</b> (SPM) have been proposed instead of caches, which can consume majority of processor power. However, application mapping on SPMs remain a challenge. We propose a dynamic SPM management scheme for program stack data for processor power reduction. As opposed to previous efforts, our solution does not mandate any hardware changes, does not need profile information, and SPM size at compile-time, and seamlessly integrates support for recursive functions. Our technique manages stack frames on SPM using a <b>scratch</b> <b>pad</b> <b>memory</b> manager (SPMM), integrated into the application binary by the compiler. Our experiments on benchmarks from MiBench [15] show average energy savings of 37 % along with a performance improvement of 18 %. I...|$|E
30|$|A {{closely related}} field of {{interest}} lies in the optimization of <b>scratch</b> <b>pad</b> <b>memories</b> (SPM), which are an efficient replacement for caches in embedded systems {{used in conjunction with}} external memory [14]. To identify the optimal selection of address ranges to be mapped to the SPM, ILP models have been developed [4, 6] and dynamic programming has been used [13]; all of which only consider a single fixed size SPM and cannot be used to optimize the SPM’s sub-organization.|$|R
40|$|Abstract—In this paper, {{we present}} a runtime memory {{allocation}} algorithm, that aims to substantially reduce the overhead caused by shared-memory accesses by allocating memory directly in the local <b>scratch</b> <b>pad</b> <b>memories.</b> We target a heterogeneous platform, with a complex memory hierarchy. Using special instrumentation, we determine what memory areas are used in functions that could run on different processing elements, like, for example a reconfigurable logic array. Based on profile information, the programmer annotates some functions as candidates for accelerated execution. Then, an algorithm decides the best allocation, {{taking into account the}} various processing elements and special <b>scratch</b> <b>pad</b> <b>memories</b> of the the heterogeneous platform. Tests are performed on our prototype platform, a Virtex ML 410 with Linux operating system, containing a PowerPC processor and a Xilinx FPGA, implementing the MOLEN programming paradigm. We test the algorithm using both state of the art H. 264 video encoder as well as other synthetic applications. The performance improvement for the H. 264 application is 14 % compared to the software only version while the overhead is less than 1 % of the application execution time. This improvement is the optimal improvement that can be obtained by optimizing the memory allocation. For the synthetic applications the results are within 5 % of the optimum. 1 I...|$|R
40|$|<b>Scratch</b> <b>Pad</b> <b>Memories</b> (SPMs) have {{received}} considerable attention lately as on-chip memory building blocks. The main characteristic that distinguishes an SPM from a conventional cache memory {{is that the}} data flow is controlled by software. The main focus {{of this paper is}} the management of an SPM space shared by multiple applications that can potentially share data. The proposed approach has three major components; a compiler analysis phase, a runtime space partitioner, and a local partitioning phase. Our experimental results show that the proposed approach leads to minimum completion time among all alternate memory partitioning schemes tested. Copyright © 2009, Inderscience Publishers...|$|R
40|$|In {{multimedia}} {{and other}} streaming applications {{a significant portion}} of energy is spent on data transfers. Exploiting data reuse opportunities in the application, we can reduce this energy by making copies of frequently used data in a small local memory and replacing speed and power inefficient transfers from main off-chip memory by more efficient local data transfers. In this paper we present an automated approach for analyzing these opportunities in a program that allows modification of the program to use custom <b>scratch</b> <b>pad</b> <b>memory</b> configurations comprising a hierarchical set of buffers for local storage of frequently reused data. Using our approach we are able to reduce energy consumption of the memory subsystem when using a <b>scratch</b> <b>pad</b> <b>memory</b> by a factor of two on averag...|$|E
40|$|The pursuit {{for higher}} {{performance}} and higher power-efficiency in computing {{has led to}} the evolution of multi-core processor architectures. Early multi-core processors primarily used the shared memory multi-processing paradigm. However, the conventional shared memory architecture, due to its limited scalability becomes a performance bottleneck. Newer architectures like the IBM Cell with 10 cores have adopted new memory architectures to truly enable the peak computing performance available. In order to achieve higher performance, it is necessary to re-design not only the bus topology, but also the memory hierarchy. The distributed memory model used in non-uniform memory access (NUMA) architectures is becoming popular in these modern processors. Conventional on-chip memory like caches have been replaced by a low power, low area alternative called <b>scratch</b> <b>pad</b> <b>memory</b> (SPM). Caches perform the data and code transfers in hardware in an automated fashion. Unlike caches, the transfers in SPM need to be explicitly managed by the compiler. In order to achieve a power-efficient operation, it is important to map the most frequently used objects onto the SPM. In this thesis, a dynamic <b>scratch</b> <b>pad</b> <b>memory</b> management scheme is proposed for program stack data with the objective of processor power reduction. As opposed to previous efforts, this technique does not need the SPM size at compile-time, does not mandate any hardware changes, does not need profile information and seamlessly integrates support for recursive functions. This solution manages stack frames on SPM using a software <b>scratch</b> <b>pad</b> <b>memory</b> manager (SPMM), integrated into the application binary by the compiler. The experiments on benchmarks from MiBench suite show average energy savings of 37 % along with a performance improvement of 18 %...|$|E
40|$|A dynamic <b>scratch</b> <b>pad</b> <b>memory</b> (SPM) {{management}} scheme for program stack data {{with the objective}} of processor power reduction is presented. Basic technique does not need the SPM size at compile time, does not mandate any hardware changes, does not need profile information, and seamlessly integrates support for recursive functions. Stack frames are managed using a software SPM manager, integrated into the application binary, and shows average energy savings of 32 % along with a performance improvement of 13 %, on benchmarks from MiBench. SPM management can be further optimized and made pointer safe, by knowing the SPM size. close 4...|$|E
40|$|Abstract. The {{design of}} future {{high-performance}} embedded systems is hampered by two problems: First, the required hardware needs {{more energy than}} is available from batteries. Second, current cache-based approaches for bridging the increasing speed gap between processors and memories cannot guarantee predictable real-time behavior. A contribution to solving both problems is made in this paper which describes a comprehensive set of algorithms {{that can be applied}} at design time in order to maximally exploit <b>scratch</b> <b>pad</b> <b>memories</b> (SPMs). We show that both the energy consumption as well as the computed worst case execution time (WCET) can be reduced by up to to 80 % and 48 %, respectively, by establishing a strong link between the memory architecture and the compiler...|$|R
40|$|The {{concept of}} Parallel Vector (<b>scratch</b> <b>pad)</b> <b>Memories</b> (PVM) was {{introduced}} as one solution for Parallel Computing in DSP, which can provides parallel memory addressing efficiently with minimum latency. The parallel programming more efficient {{by using the}} parallel addressing generator for parallel vector memory (PVM) proposed in this thesis. However, without hiding complexities by cache, the cost of programming is high. To minimize the programming cost, automatic parallel memory address generation is needed to hide the complexities of memory access. This thesis investigates methods for implementing conflict-free vector addressing algorithms on a parallel hardware structure. In particular, match vector addressing requirements extracted from the behaviour model to a prepared parallel memory addressing template, in order to supply data in parallel from the main memory to the on-chip vector memory. According to the template and usage of the main and on-chip parallel vector memory, models for data pre-allocation and permutation in <b>scratch</b> <b>pad</b> <b>memories</b> of ASIP can be decided and configured. By exposing the parallel memory access of source code, the memory access flow graph (MFG) will be generated. Then MFG will be used combined with hardware information to match templates in the template library. When it is matched with one template, suited permutation equation will be gained, and the permutation table that include target addresses for data pre-allocation and permutation is created. Thus {{it is possible to}} automatically generate memory address for parallel memory accesses. A tool for achieving the goal mentioned above is created, Permutator, which is implemented in C++ combined with XML. Memory access coding template is selected, as a result that permutation formulas are specified. And then PVM address table could be generated to make the data pre-allocation, so that efficient parallel memory access is possible. The result shows that the memory access complexities is hiden by using Permutator, so that the programming cost is reduced. It works well in the context that each algorithm with its related hardware information is corresponding to a template case, so that extra memory cost is eliminated...|$|R
40|$|In today's {{embedded}} applications {{a significant}} portion of energy is spent in the memory subsystem. Several approaches have been proposed to minimize this energy, including the use of <b>scratch</b> <b>pad</b> <b>memories,</b> with many based on static analysis of a program. However, often {{it is not possible to}} perform static analysis and optimization of a program's memory access behavior unless the program is specifically written for this purpose. In this paper we introduce the FORAY model of a program that permits aggressive analysis of the application's memory behavior that further enables such optimizations since it consists of 'for' loops and array accesses which are easily analyzable. We present FORAY-GEN: an automated profile-based approach for extraction of the FORAY model from the original program. We also demonstrate how FORAY-GEN enhances applicability of other memory subsystem optimization approaches, resulting in an average of two times increase in the number of memory references that can be analyzed by existing static approaches. Comment: Submitted on behalf of EDAA ([URL]...|$|R
40|$|In {{this paper}} we {{address the problem of}} on-chip memory {{selection}} for computationally intensive applications, by proposing <b>scratch</b> <b>pad</b> <b>memory</b> as an alternative to cache. Area and energy for different scratch pad and cache sizes are computed using the CACTI tool while performance was evaluated using the trace results of the simulator. The target processor chosen for evaluation was AT 91 M 40400. The results clearly establish scratchpad memory as a low power alternative in most situations with an average energy reduction of 40 %. Further the average area-time reduction for the scratchpad memory was 46 % of the cache memory. 1...|$|E
40|$|Abstract One of {{the most}} {{critical}} components that de-termine the success of an MPSoC based architecture is its on-chip memory. <b>Scratch</b> <b>Pad</b> <b>Memory</b> (SPM) is increasingly being applied to substitute cache as the on-chip memory of embedded MPSoCs due to its superior chip area, power consumption and timing predictabil-ity. SPM can be organized as a Virtually Shared SPM (VS-SPM) architecture that takes advantage of both shared and private SPM. However, making effective use of the VS-SPM architecture strongly depends on two inter-dependent problems: variable partitioning and task scheduling. In this paper, we decouple these two problems and solve them in phase-ordered manner. We propose two variable partitioning heuristics base...|$|E
40|$|In {{this paper}} we {{address the problem of}} on-chip mem-ory {{selection}} for computationally intensive applications, by proposing <b>scratch</b> <b>pad</b> <b>memory</b> as an alternative to cache. Area and energy for different scratch pad and cache sizes are computed using the CACTI tool while performance was evaluated using the trace results of the simulator. The tar-get processor chosen for evaluation was AT 91 M 40400. The results clearly establish scratehpad memory as a low power alternative in most situations with an average energy re-duction of J 0 %. Further the average area-time reduction for the seratchpad memory was 46 % of the cache memory. 1 1. In t roduct ion The salient feature of portable devices is light weight and low power consumption. Applications n multimedia, video processing, speech processing, DSP applications and wire-less communication require efficient memory design since on chip memory- occupies more than 50 % of the total chip area [1]. This will typically reduce the energy consumption of the memory unit, because l ss area implies reduction in the total switched capacitance. On chip caches using static RAM consume power in the range of 25 % to,~ 5 % of the total chip power [2]. Recently, interest has been fo-cussed on having on chip <b>scratch</b> <b>pad</b> <b>memory</b> to reduce the power and improve performance. On the other hand, they can replace caches only if they axe supported by an effec-tive compiler. Current embedded processors particularly in the area of multimedia applications and graphic ontrollers ZThis project is supported under DST-DAAD grant...|$|E
40|$|Limited Local Memory (LLM) {{architectures}} {{are popular}} scalable memory multi-core architectures {{in which each}} core has a local software-controlled memory, e. g., the IBM Cell processor. While similar to the <b>scratch</b> <b>pad</b> <b>memories</b> (SPMs) in embedded systems, local memory architecture is different: instead of being {{in addition to the}} cached memory hier-archy, local memories {{are a part of the}} only memory hierarchy of the core. Consequently, different schemes of managing local memory are needed. The existing circular stack man-agement is a promising approach, but is only for extremely embedded applications, with constraints on the maximum stack size, and limited use of pointers. This research presents a generalized approach to manage the stack data of any application in a constant space on the local memory. The experimental results on benchmarks from MiBench running on the IBM Cell processor in Sony Playstation 3, show gains in programmability, and generalization without much loss of performance. ii...|$|R
40|$|Submitted {{on behalf}} of EDAA ([URL] audienceIn today's {{embedded}} applications {{a significant portion of}} energy is spent in the memory subsystem. Several approaches have been proposed to minimize this energy, including the use of <b>scratch</b> <b>pad</b> <b>memories,</b> with many based on static analysis of a program. However, often {{it is not possible to}} perform static analysis and optimization of a program's memory access behavior unless the program is specifically written for this purpose. In this paper we introduce the FORAY model of a program that permits aggressive analysis of the application's memory behavior that further enables such optimizations since it consists of 'for' loops and array accesses which are easily analyzable. We present FORAY-GEN: an automated profile-based approach for extraction of the FORAY model from the original program. We also demonstrate how FORAY-GEN enhances applicability of other memory subsystem optimization approaches, resulting in an average of two times increase in the number of memory references that can be analyzed by existing static approaches...|$|R
40|$|Abstract. The Cell Broadband Engine Architecture {{is a new}} {{heterogeneous}} multi-core architecture {{targeted at}} compute-intensive workloads. The architecture of the Cell BE has several features that are unique in high-performance general-purpose processors, such as static instruction scheduling, extensive support for vectorization, <b>scratch</b> <b>pad</b> <b>memories,</b> explicit programming of DMAs, mailbox communication, multiple processor cores, etc. It is necessary to make explicit use of these features to obtain high performance. Yet, little work reports on how to apply them and how much each of them contributes to performance. This paper presents our experiences with programming the Cell BE architecture. Our test application is Clustal W, a bio-informatics program for multiple sequence alignment. We report on how we apply the unique features of the Cell BE to Clustal W and how important each is to obtain high performance. By making extensive use of vectorization and by parallelizing the application across all cores, we speedup the pairwise alignment phase of Clustal W with a factor of 51. 2 over PPU (superscalar) execution. The progressive alignment phase is sped up {{by a factor of}} 5. 7 over PPU execution, resulting in an overall speedup by 9. 1. ...|$|R
40|$|The initial {{control and}} {{programming}} philosophies of the RELAPSE are discussed. A block diagram showing {{the relationship of}} the Arithmetic Units (composed of Stages and Bit Processors), to the Functional Units, and other components of the RELAPSE is used to guide this discussion. The latest version of the Bit Processor design is presented. Included is a detailed discussion of the Bit Processor's new <b>scratch</b> <b>pad</b> <b>memory</b> component. The section also clarifies the usage of the Bit Processor's processing registers, and Input/Output functions. The final design phase of the Arithmetic Unit is underway by a study of the Proposed IEEE Floating Point Standard. The decisions on conformation to this standard will be used as inputs into the finalization of the designs of the Bit Processor, Stage, and Arithmetic Units of the RELAPSE...|$|E
40|$|Abstract—Limited Local Memory (LLM) {{architectures}} are power-efficient, scalable memory multi-core architectures, {{in which}} cores have a scratch-pad like local memory that is software controlled. Any data transfers between the main {{memory and the}} local memory must be explicitly present as Direct Memory Access (DMA) commands in the application. Stack data management of the cores is an important problem in LLM architecture, and our previous work outlined a promising scheme for that [1]. In this paper, we improve the previous approach, and now can i) manage limitless stack data, ii) increase the applicability of stack management, and iii) perform stack management with smaller footprint on the local memory. We demonstrate these by executing benchmarks from the MiBench suite on the IBM Cell processor. Index Terms—Stack, local memory, <b>scratch</b> <b>pad</b> <b>memory,</b> embedded systems, multi-core processor, IBM Cell, MPI I...|$|E
40|$|In {{this report}} we {{evaluate}} {{the options for}} low power on-chip memories during system design and configuration. Specifically, we compare the use of scratch pad memories with that of cache {{on the basis of}} performance, area and energy. The target architecture used in our experiments is the AT 91 M 40400 microcontroller containing an ARM 7 TDMI core. A packing algorithm is used to map the memory objects of the benchmarks to the scratch pad. Area and energy for different scratch pad and cache sizes are computed using the CACTI tool while performance is derived using the trace results of the ARMulator. We observe area and performance improvements by using a <b>scratch</b> <b>pad</b> <b>memory.</b> For example, for bubble sort there is a performance improvement of 18 % from a hardware which needs 34 % less area. The scratch pad also needs less energy per access, due to the absence of tag comparison. ...|$|E
40|$|The Cell Broadband Engine (BE) Architecture {{is a new}} {{heterogeneous}} multi-core architecture {{targeted at}} compute-intensive workloads. The architecture of the Cell BE has several features that are unique in high-performance general-purpose processors, most notably the extensive support for vectorization, <b>scratch</b> <b>pad</b> <b>memories</b> and explicit programming of direct memory accesses (DMAs) and mailbox communication. While these features strongly increase programming complexity, it is generally claimed that significant speedups {{can be obtained by}} using Cell BE processors. This paper presents our experiences with using the Cell BE architecture to accelerate Clustal W, a bio-informatics program for multiple sequence alignment. We report on how we apply the unique features of the Cell BE to Clustal W and how important each is in obtaining high performance. By making extensive use of vectorization and by parallelizing the application across all cores, we demonstrate a speedup of 24. 4 times when using 16 synergistic processor units on a QS 21 Cell Blade compared to single-thread execution on the power processing unit. As the Cell BE exploits a large number of slim cores, our highly optimized implementation is just 3. 8 times faster than a 3 -thread version running on an Intel Core 2 Duo, as the latter processor exploits a small number of fat cores...|$|R
40|$|Abstract — The dual {{effects of}} larger die sizes and {{technology}} scaling, combined with aggressive voltage scaling for power reduction, increase the error rates for on-chip memories. Traditional on-chip memory reliability techniques (e. g., ECC) incur significant power and performance overheads. In this paper, we propose a low-power-and-performance-overhead Embedded RAID (E-RAID) strategy and present Embedded RAIDs-on-Chip (E-RoC), a distributed dynamically managed reliable memory subsystem. E-RoC achieves reliability through redundancy by optimizing RAID-like policies tuned for on-chip distributed memories. We achieve on-chip reliability of memories {{through the use}} of distributed dynamic <b>scratch</b> <b>pad</b> allocatable <b>memories</b> (DSPAMs) and their allocation policies. We exploit aggressive voltage scaling to reduce power consumption overheads due to parallel DSPAM accesses, and rely on the E-RoC manager to automatically handle any resulting voltage-scaling-induced errors. Our experimental results on multimedia benchmarks show that E-RoC’s fully distributed redundant reliable memory subsystem reduces power consumption by up to 85 % and latency up to 61 % over traditional reliability approaches that use parity/cyclic hybrids for error checking and correction. I...|$|R
40|$|Gao, Guang R. Over {{the past}} decade {{computer}} architectures have drastically evolved to circumnavigate prevailing physical limitations in chip technology. Energy consumption and heat expenditure have become the predominant concerns for architects and chip manufacturers. Previously anticipated trends such as frequency scaling, deep execution pipelines, and fully consistent caches in future many-core systems have been deemed unsustainable. Current architectures are exhibiting new trends including simpler pipelines, lower frequencies, and <b>scratch</b> <b>pad</b> <b>memories.</b> Moreover, these architectures have an ever increasing number of cores. Many predict future architectures to contain thousands of heterogeneous cores on a single die. With these radical shifts in architectures, current execution models are struggling to adequately scale in performance and newer metrics like energy consumption. The shortcomings of current models have caused some to look back to fine-grained execution models designed for parallelism like dataflow and EARTH. Using these models as inspiration, the Codelet execution model is an event-driven, fine-grained model designed to exploit parallelism while providing efficient mechanism for locality. In the following, we present the Delaware Asynchronous RunTime System (DARTS), an implementation of the Codelet model. DARTS is a faithful implementation of the Codelet model, providing a vehicle to reason and further develop codelet ideas. It provides two levels of parallelism, event-driven codelets permitting fine-grained parallelism and invoked threaded procedures which ensures locality. Furthermore, the DARTS runtime is built on a reconfigurable abstract machine allowing DARTS to provide performance portability across both architectures and applications. In addition, we provide an in depth analysis of DARTS and its underlying model running on off-the-shelf hardware. Utilizing two x 86 machines (both Intel and AMD), we explore the overheads of the codelet model and its implementation using micro benchmarks. Furthermore, we demonstrate DARTS' performance for two benchmarks, matrix multiply and breadth first search. Leveraging these results, we aim to establish the Codelet model as a promising execution model for future many-core architectures via an efficient and well-designed runtime. University of Delaware, Department of Electrical and Computer EngineeringM. E. E...|$|R
40|$|Recent {{advances}} in circuit and process technologies have pushed non-volatile memory technologies {{into a new}} era. These technologies exhibit appealing properties such as low power consumption, non-volatility, shock-resistivity, and high density. However, there are challenges to which we need answers in the road of applying non-volatile memories as main memory in computer systems. First, non-volatile memories have limited number of write/erase cycles compared with DRAM memory. Second, write activities on non-volatile memory are more expensive than DRAM memory in terms of energy consumption and access latency. Both challenges will benefit from reduction of the write activities on the nonvolatile memory. In this paper, we target embedded Chip Multiprocessors (CMPs) with <b>Scratch</b> <b>Pad</b> <b>Memory</b> (SPM) and non-volatile main memory. We introduce data migration and recomputation techniques {{to reduce the number}} of write activities on non-volatile memories. Experimental results show that the proposed methods can reduce the number of writes by 59. 41 % on average, which means that the non-volatile memory can last 2. 8 times as long as before. Meanwhile, the finish time of programs is reduced by 31. 81 % on average...|$|E
40|$|Abstract—Scratch pad {{memories}} (SPM) {{are attractive}} alternatives for caches on multicore systems since caches are relatively expensive {{in terms of}} area and energy consumption. The key to effectively utilizing SPMs on multicore systems is the data placement algorithm. In this paper, two polynomial time algorithms, regional data placement for multicore (RDPM) and regional data placement for multicore with duplication (RDPM-DUP), have been proposed to generate near-optimal data placement with minimum total cost. There is only one copy for each data in RDPM, while RDPM-DUP allows data duplication. Experimental {{results show that the}} proposed RDPM algorithm alone can reduce the time cost of memory accesses by 32. 68 % on average compared with existing algorithms. With data duplication, the RDPM-DUP algorithm further reduces the time cost by 40. 87 %. In terms of energy consumption, the proposed RDPM algorithm with exclusive copy can reduce the total cost by 33. 47 % on average. When RDPM-DUP is applied, the improvement increases up to 38. 15 % on average. Index Terms—Data duplication, data placement, embedded systems, multicore, <b>scratch</b> <b>pad</b> <b>memory.</b> I...|$|E
40|$|Abstract — The memory {{subsystem}} {{is a major}} contributor to the performance, power, and area of complex SoCs used in feature rich multimedia products. Hence, memory architecture of the embedded DSP is complex and usually custom designed with multiple banks of single-ported or dual ported on-chip <b>scratch</b> <b>pad</b> <b>memory</b> and multiple banks of off-chip memory. Building software for such large complex memories with many of the software components as individually optimized software IPs is a big challenge. In order to obtain good performance and a reduction in memory stalls, the data buffers of the application need to be placed carefully in different types of memory. In this paper we present a unified framework (MODLEX) that combines different data layout optimizations to address the complex DSP memory architectures. Our method models the data layout problem as multi-objective Genetic Algorithm (GA) with performance and power being the objectives and presents a set of solution points which is attractive from a platform design viewpoint. While most of the work in the literature assumes that performance and power are non-conflicting objectives, our work demonstrates that there is significant trade-off (up to 70 %) that is possible between power and performance. I...|$|E
5000|$|<b>Scratch</b> <b>Pad</b> - {{here the}} user can store notes; they are saved {{automatically}} ...|$|R
25|$|Schaefer, Daniel. From <b>scratch</b> <b>pads</b> and dreams: A ten year {{history of}} the University of North Florida, University of North Florida, 1982.|$|R
5000|$|Now we roll {{a random}} number k from 1 to 8—let's make it 3—and strike out the kth (i.e. third) number (3, of course) on the <b>scratch</b> <b>pad</b> and {{write it down}} as the result: ...|$|R
40|$|Many {{embedded}} array-intensive applications have irregular access {{patterns that}} are not amenable to static analysis for extraction of access patterns, and thus prevent efficient use of a <b>Scratch</b> <b>Pad</b> <b>Memory</b> (SPM) hierarchy for performance and power improvement. We present a profiling based strategy that generates a memory access trace {{which can be used}} to identify data elements with fine granularity that can profitably be placed in the SPMs to maximize performance and energy gains. We developed an entire toolchain that allows incorporation of the code required to profitably move data to SPMs; visualization of the extracted access pattern after profiling; and evaluation/exploration of the generated application code to steer mapping of data to the SPM to yield performance and energy benefits. We present a heuristic approach that efficiently exploits the SPM using the profiler-driven access pattern behaviors. Experimental results on EEMBC and other industrial codes obtained with our framework show that we are able to achieve 36 % energy reduction and reduce execution time by up to 22 % compared to a cache based system. Categories and Subject Descriptors D. 3. 3 [Programming Languages]: Language Constructs and Features—dynamic storage management; D. 3. 4 [Programming Languages]...|$|E
40|$|Our Multi Processor System on Chip (MPSoC) {{template}} provides processing tiles {{that are}} connected via a network on chip. A processing tile contains a processing unit and a <b>Scratch</b> <b>Pad</b> <b>Memory</b> (SPM). This paper presents the Omphale tool that performs {{the first step}} in mapping a job, represented by a task graph, to such an MPSoC, given the SPM sizes as constraints. Furthermore a memory tile is introduced. The result of Omphale is a Cyclo Static DataFlow (CSDF) model and a task graph where tasks communicate via sliding windows that are located in circular buffers. The CSDF model is used to determine the size of the buffers and the communication pattern of the data. A buffer must fit in the SPM of the processing unit that is reading from it, such that low latency access is realized with a minimized number of stall cycles. If a task and its buffer exceed the size of the SPM, the task is examined for additional parallelism or the circular buffer is partly located in a memory tile. This results in an extended task graph that satisfies the SPM size constraints...|$|E
40|$|Embedded {{multimedia}} applications {{consist of}} regular and irregular memory access patterns. Particularly, irregular pattern are not amenable to static analysis for extraction of access patterns, and thus prevent {{efficient use of}} a <b>Scratch</b> <b>Pad</b> <b>Memory</b> (SPM) hierarchy for performance and energy improvements. To resolve this, we present a compiler strategy to optimize data layout in regular/irregular multimedia applications running on embedded multiprocessor environments. The goal is to maximize the amount of accesses to the SPM over the entire system {{which leads to a}} reduction in the energy consumption of the system. This is achieved by optimizing data placement of application-wide reused data so that it resides in the SPMs of processing elements. Specifically, our scheme is based on a profiling that generates a memory access footprint. The memory access footprint is used to identify data elements with fine granularity that can profitably be placed in the SPMs to maximize performance and energy gains. We present a heuristic approach that efficiently exploits the SPMs using memory access footprint. Our experimental results show that our approach is able to reduce energy consumption by 30 % and improve performance by 18 % over cache based memory subsystems for various multimedia applications...|$|E
5000|$|Initially, {{register}} #2 contains [...] "2". Registers #0, #1 and #3 {{are empty}} (contain [...] "0"). Register #0 remains unchanged throughout calculations {{because it is}} used for the unconditional jump. Register #1 is a <b>scratch</b> <b>pad.</b> The program begins with instruction 1.|$|R
50|$|Wiredset, {{a digital}} {{marketing}} agency, {{created the first}} version of Trendrr called Infofilter in 2006. Infofilter pulled time-series data and pushed it into graphs and charts, without the interactive features such as mash-ups and <b>scratch</b> <b>pad.</b> The reporting function was manual, and access to the platform was deemed private.|$|R
40|$|Abstract—Exploiting runtime {{memory access}} traces {{can be a}} {{complementary}} approach to compiler optimizations for the energy reduction in memory hierarchy. This is particularly important for emerging multimedia applications since they usually have input-sensitive runtime behavior which results in dynamic and/or irregular memory access patterns. These types of applications are normally hard to optimize by static compiler optimizations. The reason is that their behavior stays unknown until runtime and may even change during computation. To tackle this problem, we propose an integrated approach of software [compiler and operating system (OS) ] and hardware (data access record table) techniques to exploit data reusability of multimedia applications in Multiprocessor Systems on Chip. Guided by compiler analysis for generating <b>scratch</b> <b>pad</b> data layouts and hardware components for tracking dynamic <b>memory</b> accesses, the <b>scratch</b> <b>pad</b> dat...|$|R
