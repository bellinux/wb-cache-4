0|1864|Public
5000|$|... the syntactical calque, with <b>syntactical</b> <b>functions</b> or {{constructions of}} the source {{language}} being imitated in the target language.|$|R
50|$|Within a sentence, interjections can {{function}} as attributes, verbal equivalents, {{or they can}} be used as filler, which has no <b>syntactical</b> <b>function</b> at all.|$|R
40|$|A {{predicate}} of {{a passive}} sentence in Japanese can be filled in by a transitive,ditransitif, and intransitive verb. Passive sentences with intransitive verbs as the predicates commonly express oppositive meaning. Linguists commonly classify {{this type of}} passive sentence regarding only to its <b>syntactical</b> <b>function</b> and category. This study however investigates passive intransitive sentences regarding not only their <b>syntactical</b> <b>function</b> and category, but also their semantic construction. The study finds five types of syntactical construction of passive intransitive (I~V). Each type of the passive is distinguished by the semantic information of its modifiers...|$|R
50|$|Romanian {{adjectives}} {{determine the}} quality of things. They can only fulfill the <b>syntactical</b> <b>functions</b> of attribute and of adjectival complement, which in Romanian is called nume predicativ (nominal predicative).|$|R
5000|$|In linguistics, an apo koinou {{construction}} [...] is a {{blend of}} two clauses through a lexical word which has two <b>syntactical</b> <b>functions,</b> one in each of the blended clauses. The clauses are connected asyndentically.|$|R
5000|$|In German orthography, nouns {{and most}} <b>words</b> with the <b>syntactical</b> <b>function</b> of nouns are capitalised {{to make it}} easier for readers to {{determine}} the <b>function</b> of a <b>word</b> within a sentence ( [...] - [...] "On Friday I went shopping."; [...] - [...] "One day he finally showed up.") This convention is almost unique to German today (shared perhaps only by the closely related Luxembourgish language and several insular dialects of the North Frisian language), but it was historically common in other languages such as Danish (which abolished the capitalization of nouns in 1948) and English.|$|R
50|$|Tae Daeng, as {{with most}} of the Tai Language family, employ a Subject-Verb-Order word order and {{because of the lack of}} inflections upon verbs, <b>syntactical</b> <b>functions</b> are largely derived from word order and prepositions. Particles are highly {{adaptive}} and can usually be found at the end of a sentence in order to emphasize, question, command or indicate a level of familiarity or respect.|$|R
40|$|In {{this thesis}} the onomatopoeias and mimeses in Korean are analyzed. The {{question}} of a classification of onomatopoeia and mimeses as some part of speech is discussed after the definition of onomatopoeias and mimeses. In Korean, onomatopoeia and mimeses are mostly considered as adverbs, while in Czech as interjections. The last approach {{is to create a}} new part of speech of onomatopoeias and mimeses only. There are four specific conditions of Korean onomatopoeias and mimeses, which is a sound symbolism, reduplications, a specific word formation and typical <b>syntactical</b> <b>functions</b> and a limited compatibility. At least two of the conditions should meet to classify a word to be onomatopoeia or mimesis in Korean. Afterwards the question of quantity of onomatopoeia and mimeses in Korean is presented on an excerption from Korean and Czech dictionary. It was observed, that the quantity of Korean onomatopoeia and mimeses is based expecially on its variability. The main formal features of Korean onomatopoeias and mimeses are presented with several examples. The sound symbolism of vowels and consonants, the various kinds of reduplication, the word forming processes and the <b>syntactical</b> <b>functions</b> are discussed respectively. In the other part the question of distribution of onomatopoeias and mimeses in Korean compare to [...] ...|$|R
50|$|Words {{that are}} not <b>function</b> <b>words</b> are called content words (or open class words or lexical words or autosemantic words): these include nouns, verbs, adjectives, and most adverbs, {{although}} some adverbs are <b>function</b> <b>words</b> (e.g., then and why). Dictionaries define the specific meanings of content words, but can only describe the general usages of <b>function</b> <b>words.</b> By contrast, grammars describe the use of <b>function</b> <b>words</b> in detail, but treat lexical words in general terms only.|$|R
40|$|A {{large number}} of {{auditory}} studies explored the role of <b>function</b> <b>words</b> in syntactic processing, but few researched <b>function</b> <b>words</b> in written input. The present study probed the role of <b>function</b> <b>words</b> in word skipping, sentence compacting, chunking preference and the detection mechanism of grammatical incongruence by means of number estimation across 4 syntactic conditions (grammatical sentences, scrambled sentences, sentences with agreement errors and sentences with structural errors), 3 sentence lengths (6 or 7 words, 8 or 9 words, 10 or 11 words) and 3 ratios of <b>function</b> <b>words</b> and content words. We find that appropriate usage of <b>function</b> <b>words</b> highly facilitates syntactic analysis though <b>function</b> <b>words</b> are always skipped in proficient reading. Adjacent <b>function</b> <b>words</b> and content words in grammatical sentences {{are more likely to}} be processed as chunks, and this effect of chunking make sentences significantly more compact than scrambled sentences. In addition, the detection mechanism of grammatical incongruence is attributed to resolving the conflicts with prediction...|$|R
50|$|<b>Function</b> <b>words</b> {{might be}} prepositions, pronouns, {{auxiliary}} verbs, conjunctions, grammatical articles or particles, {{all of which}} belong {{to the group of}} closed-class words. Interjections are sometimes considered <b>function</b> <b>words</b> but they belong to the group of open-class <b>words.</b> <b>Function</b> <b>words</b> might or might not be inflected or might have affixes.|$|R
40|$|This study {{investigates the}} <b>function</b> <b>word</b> deficits in aphasic {{patients}} and, in particular, agrammatic Broca's aphasics. Several {{explanations for the}} <b>function</b> <b>word</b> problem are addressed including a <b>function</b> <b>word</b> vocabulary deficit theory, a general syntactic deficit theory, and an abstract word deficit theory. Subjects {{with varying degrees of}} agrammatism were tested on a variety of production, comprehension, and reading, and syntactic tests which isolate semantic and syntactic aspects of both <b>function</b> and content <b>words</b> in order to better define the nature of the <b>function</b> <b>word</b> deficits in agrammatism...|$|R
50|$|All {{words can}} be {{classified}} as either content or <b>function</b> <b>words,</b> {{although it is not}} always easy to make the distinction. With only around 150 <b>function</b> <b>words,</b> 99.9% of words in the English language are content words. Although small in numbers, <b>function</b> <b>words</b> are used at a disproportionately higher rate and make up about 50% of any English text. This is due to the conventional patterns of words usage which bind <b>function</b> <b>words</b> to content words almost every time they are used, creating an interdependence between the two word groups.|$|R
50|$|Below {{are rules}} {{about the number}} of n’s in <b>function</b> <b>words,</b> which do not decline nor conjugate. Various <b>function</b> <b>words</b> which {{indicate}} movement end with -an and never -ann.|$|R
40|$|International audienceAuthorship {{attribution}} is {{the task}} of identifying {{the author of a}} given document. Various style markers have been proposed in the literature to deal with the authorship attribution task. Frequencies of <b>function</b> <b>words</b> {{have been shown to be}} very reliable and effective for this task. However, despite the fact that they are state-of-the-art, they basically rely on the invalid bag-of-words assumption, which stipulates that text is a set of independent words. In this contribution, we present a comparative study on using two different types of style marker based on <b>function</b> <b>words</b> for authorship attribution. We compare the effectiveness of using sequential rules of <b>function</b> <b>words</b> as style marker that do not relay on the bag-of-words assumption to that of the frequency of <b>function</b> <b>words</b> which does. Our results show that the frequencies of <b>function</b> <b>words</b> outperform the sequential rules...|$|R
40|$|One of {{the main}} issues in a word {{alignment}} task is the difficulty of handling <b>function</b> <b>words</b> {{that do not have}} direct translations which we call unique <b>function</b> <b>words.</b> They are often aligned to some words in the other language incorrectly. This is prominent in language pairs with very different sentence structures. In this paper, we propose a novel approach for handling unique <b>function</b> <b>words.</b> The proposed model monolingually derives unique <b>function</b> <b>words</b> from bilin-gually generated treelet pairs. The monolingual derivation prevents incorrect alignments for unique <b>function</b> <b>words.</b> The derivation probabilities are estimated from a large monolingual corpus, which is much easier to acquire than a parallel corpus. Also, the proposed alignment model uses semantic-head dependency trees where dependency relations between words be-come similar in each language. Experimental results on an English-Japanese corpus show that the proposed model achieves better alignment and translation quality compared with the baseline models...|$|R
40|$|The {{generation}} of precise and comprehensible translations {{is still a}} challenge in the patent and scientific domain. In particular, <b>function</b> <b>words</b> are often poorly translated in standard machine translation systems, particularly across language pairs with greatly differing syntax. In this paper we exploit the target-side structure in tree-to-tree machine translation to post-edit <b>function</b> <b>words</b> automatically using a tree-based <b>function</b> <b>word</b> language model. We show that a significant improvement in human evaluation can be achieved with our proposed method. ...|$|R
40|$|In {{statistical}} word alignment for machine translation, <b>function</b> <b>words</b> usually cause poor aligning performance {{because they}} do not have clear correspondence between different languages. This paper proposes a novel approach to improve word alignment by pruning alignments of <b>function</b> <b>words</b> from an existing alignment model with high precision and recall. Based on monolingual and bilingual frequency characteristics, a language-independent <b>function</b> <b>word</b> recognition algorithm is first proposed. Then a group of carefully defined syntactic structures combined with content word alignments are used for further <b>function</b> <b>word</b> alignment pruning. The experimental results show that the proposed approach improves both the quality of word alignment and the performance of statistical machine translation on Chinese-to-English, Germanto-English and French-to-English language pairs. ...|$|R
5000|$|As the accents were (and are) {{not shown}} on a Torah scroll, {{it was found}} {{necessary}} to have a person making hand signals to the reader to show the tune, as in the Byzantine system of neumes. This system of cheironomy survives in some communities to the present day, notably in Italy. It is speculated that both the shapes {{and the names of}} some of the accents (e.g. tifcha, literally [...] "hand-breadth") may refer to the hand signals rather than to the <b>syntactical</b> <b>functions</b> or melodies denoted by them. Today in most communities there is no system of hand signals and the reader learns the melody of each reading in advance.|$|R
5000|$|A German modal {{particle}} serves no necessary <b>syntactical</b> <b>function,</b> but {{expresses the}} speakers {{attitude towards the}} utterance. Modal particles include ja, halt, doch, aber, denn, schon and others. Some of these also appear in non-particle forms. Aber, for example, is also the conjunction but. In Er ist Amerikaner, aber er spricht gut Deutsch, [...] "He is American, but he speaks German well," [...] aber is a conjunction connecting two sentences. But in Er spricht aber gut Deutsch!, the aber is a particle, with the sentence perhaps best translated as [...] "What good German he speaks!" [...] The particles appear more often in relaxed spoken and casually written registers of German.|$|R
5000|$|There {{are other}} special constructions that also {{take place in}} certain {{specific}} environments. For example, quantificatives and nouns can be in apposition to other nouns that independent subjects or objects: ʔuhkʔo'nisɛ'mǎn, ho't ʔaku'hpanʔuhkɛ'nì [...] "He assembled all (of) his people, it is sad" [...] < [...] "he assembled, it is said, his people, all." [...] Additionally, a possessive [...] can serve in the same <b>syntactical</b> <b>functions</b> that a noun can. For example, ta'čɔhak ʔu'rǐhč, hi'yuhɔ'nì [...] "The chief's house was (made of) grass" [...] (ta'čɔhaku [...] "the chief", possessor noun, + ʔu'rihči [...] "his house", alienably possessed noun, the combination serving as independent subject).|$|R
40|$|We {{describe}} a simple improvement to n-gram language models where we estimate the distribution over closed-class (<b>function)</b> <b>words</b> {{separately from the}} conditional distribution of open-class <b>words</b> given <b>function</b> <b>words.</b> In English, <b>function</b> <b>words</b> account for about 30 % of written language, and also form a natural skeleton for most sentences. By factoring a language model into a <b>function</b> <b>word</b> model and a conditional model over open-class <b>words</b> given <b>function</b> <b>words,</b> we largely avoid the problem of sparse training data in the first phase, and localize the need for sophisticated smoothing techniques primarily to the second conditional model. We test our factored approach on the Brown and Wall Street Journal corpora and observe a 3. 5 % to 25. 2 % improvement in perplexity over standard methods, depending on the particular smoothing method and test set used. Compared to other proposals for improving n-gram language models, our factorization {{has the advantage of}} inherent simplicity and efficiency, and improves generalization between data sets...|$|R
40|$|In a {{regression}} study of conversational speech, {{we show that}} frequency, contextual predictability and repetition have separate contributions to word duration, despite their substantial correlations. Moreover, content- and function-word durations are affected differently by their frequency and predictability. Content words are shorter when more frequent, and shorter when repeated, while <b>function</b> <b>words</b> are not so affected. <b>Function</b> <b>words</b> have shorter pronunciations, after controlling for frequency and predictability. While both content and <b>function</b> <b>words</b> are strongly affected by predictability from the word following them, sensitivity to predictability from the preceding word is largely limited to very frequent <b>function</b> <b>words.</b> The results {{support the view that}} content and <b>function</b> <b>words</b> are accessed differently in production. We suggest a lexical-access-based model of our results, in which frequency or repetition lead to shorter or longer word durations by causing faster or slower lexical access, mediated by a general mechanism that coordinates the pace of higher-level planning and the execution of the articulatory plan...|$|R
40|$|In {{the present}} paper, we propose the {{effective}} usage of <b>function</b> <b>words</b> to generate generalized translation rules for forest-based translation. Given aligned forest-string pairs, we extract composed tree-to-string translation rules {{that account for}} multiple interpretations of both aligned and unaligned target <b>function</b> <b>words.</b> In order to constrain the exhaustive attachments of <b>function</b> <b>words,</b> we limit to bind them to the nearby syntactic chunks yielded by a target dependency parser. Therefore, the proposed approach can not only capture source-tree-to-target-chunk correspondences but can also use forest structures that compactly encode an exponential number of parse trees to properly generate target <b>function</b> <b>words</b> during decoding. Extensive experiments involving large-scale English-to-Japanese translation revealed a significant improvement of 1. 8 points in BLEU score, as compared with a strong forest-to-string baseline system. ...|$|R
5000|$|... lèih/làih (v. come, {{sometimes}} <b>function</b> <b>word)</b> Standard Chinese: ...|$|R
40|$|In this paper, we {{show some}} {{properties}} of <b>function</b> <b>words</b> in dependency trees. <b>Function</b> <b>words</b> are grammatical words, such as articles, prepositions, pronouns, conjunctions, or auxiliary verbs. These words are often short and very frequent in texts and therefore {{many of them}} can be easily recognized. We formulate a hypothesis that <b>function</b> <b>words</b> tend to have a fixed number of dependents and we prove this hypothesis on treebanks. Using this hypothesis, we are able to improve unsupervised dependency parsing and outperform previously published state-of-the-art results for many languages...|$|R
5000|$|Grammatical words, as a class, {{can have}} {{distinct}} phonological properties from content words. Grammatical words sometimes {{do not make}} full use of all the sounds in a language. For example, {{in some of the}} Khoisan languages, most content words begin with clicks, but very few <b>function</b> <b>words</b> do. In English, very few <b>words</b> other than <b>function</b> <b>words</b> begin with voiced th [...] (see Pronunciation of English th); English <b>function</b> <b>words</b> may have fewer than three letters 'I', 'an', 'in' while non-function words usually have three or more 'eye', 'Ann', 'inn' (see three letter rule).|$|R
40|$|Inspired by {{experimental}} psychological findings {{suggesting that}} <b>function</b> <b>words</b> play a special role in word learning, {{we make a}} simple modification to an Adaptor Grammar based Bayesian word segmentation model {{to allow it to}} learn sequences of monosyllabic "function words" at the beginnings and endings of collocations of (possibly multi-syllabic) words. This modification improves unsupervised word segmentation on the standard Bernstein- Ratner (1987) corpus of child-directed English by more than 4 % token f-score compared to a model identical except that it does not special-case "function words, setting a new state-of-the-art of 92. 4 % token f-score. Our <b>function</b> <b>word</b> model assumes that <b>function</b> <b>words</b> appear at the left periphery, and while this is true of languages such as English, it is not true universally. We show that a learner can use Bayesian model selection to determine the location of <b>function</b> <b>words</b> in their language, even though the input to the model only consists of unsegmented sequences of phones. Thus our computational models support the hypothesis that <b>function</b> <b>words</b> play a special role in word learning. 11 page(s...|$|R
5000|$|Other <b>function</b> <b>words</b> (...) are {{separated}} from other words, including: ...|$|R
5000|$|Unstressed: {{unstressed}} syllables of polysyllabic words; monosyllabic <b>function</b> <b>words.</b>|$|R
2500|$|As the accents were (and are) {{not shown}} on a Torah scroll, {{it was found}} {{necessary}} to have a person making hand signals to the reader to show the tune, as in the Byzantine system of neumes. [...] This system of cheironomy survives in some communities to the present day, notably in Italy. [...] It is speculated that both the shapes {{and the names of}} some of the accents (e.g. tifcha, literally [...] "hand-breadth") may refer to the hand signals rather than to the <b>syntactical</b> <b>functions</b> or melodies denoted by them. [...] Today in most communities there is no system of hand signals and the reader learns the melody of each reading in advance.|$|R
50|$|Below {{are some}} Mantauran Rukai <b>function</b> <b>words</b> from Zeitoun (2007).|$|R
5000|$|... ge (genitive, {{similar to}} 's; {{sometimes}} <b>function</b> <b>word)</b> Standard Chinese: ,, ...|$|R
40|$|<b>Function</b> <b>words</b> are lexical {{units with}} {{generally}} lit-tle semantic weight that often {{play a role}} of “gram-matical ” elements in a sentence, introducing or modifying content words. These include prepo-sitions (with), determiners (some), pronouns (she) and conjunctions (furthermore). Complex <b>function</b> <b>words</b> are <b>function</b> <b>words</b> made up of several to-kens, like complex prepositions (in front of), de-terminers (a lot of) and conjunctions (as long as). This abstract discusses the representation and detection of ADV+que constructions, a type of complex conjunction in French. These construc-tions are formed by adverbs like bien (well) or ainsi (likewise) followed by subordinative con-junction que (which) ...|$|R
50|$|The list of <b>function</b> <b>words</b> {{below is}} sourced from Adelaar (1997).|$|R
5000|$|<b>Function</b> <b>words</b> help in modifying meaning {{considered}} the following sentence - ...|$|R
