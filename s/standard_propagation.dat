46|235|Public
40|$|Abstract — The {{quality of}} {{coverage}} of any radio network design {{depends on the}} accuracy of the propagation model employed during planning and initial deployment. For efficient radio network design, the propagation models are estimated from signal strength measurement taken in the area of interest. In this paper, the suitability of Okumura-Hata model, COST 231 -Hata model and <b>Standard</b> <b>Propagation</b> Model for radio coverage prediction o...|$|E
30|$|Derived {{quantities}} {{such as the}} b/a ratios in Tables  3, 4 as well as {{the areas}} of the unit cells in Tables  5, 6 acquired precision measures by the <b>standard</b> <b>propagation</b> law of estimates, i.e., the sum of absolute values of the partial differentials at the extracted values times 50 % of the last digit of the numerical result (significant number) that are associated with the extracted lattice parameters.|$|E
40|$|The MTE (mixture of {{truncated}} exponentials) model allows to {{deal with}} Bayesian networks containing discrete and continuous variables simultaneously. One {{of the features of}} this model is that <b>standard</b> <b>propagation</b> algorithms can be applied. In this paper, we study the problem of estimating these models from data. We propose an iterative algorithm based on least squares approximation. The performance of the algorithm is tested both with artificial and actual data...|$|E
40|$|In this paper, we {{characterize}} all graphs G {{with extreme}} k-power propagation time |G|- 1 or |G|- 2 for k≥ 1, and |G|- 3 for k≥ 2. We determine all trees T whose 1 -power propagation time (also called power <b>propagation</b> time or <b>standard</b> power <b>propagation</b> time) is |T|- 3. Partial characterizations of graphs with k-power propagation time equal to 1 are also established. Finally, {{we consider the}} effects of edge subdivisions and edge contractions on the <b>standard</b> power <b>propagation</b> time of a graph, and give an upper bound on {{the sum of the}} <b>standard</b> power <b>propagation</b> time of a graph and its complement...|$|R
40|$|The {{purpose of}} this study is to compare the {{performance}} of neural network using Krzyzak algorithm and <b>standard</b> back <b>propagation</b> algorithm in forecasting domain. To implement this study a timber data set, which represents a non-seasonal time series data, is used. The performance is measured based on the accuracies, which is, quantified by root mean square error and learning speed for convergence. The results show that by using a small value of learning rate, Krzyzak algorithm is better than <b>standard</b> back <b>propagation</b> algorithm for medium and long term forecasting...|$|R
30|$|In [9], the LDPC code is decoded by the layered belief {{propagation}} (LBP) algorithm {{as defined}} in [23]. LBP is {{a variation of the}} <b>standard</b> belief <b>propagation</b> [1] which is designed for architecture aware (AA) LDPC codes with quasi-cyclic properties. It is shown that with LBP, the decoding of AA LDPC codes is improved by two times in the number of iterations required for a given error rate [24]. However, we have used <b>standard</b> belief <b>propagation</b> decoding to reproduce the results of [9] and to analyze the performance of the proposed LDPC codes in co-operative relay networks. To get the equivalent performance of LBP using <b>standard</b> belief <b>propagation</b> decoding, the number of decoding iterations is set to twice the number of iterations defined in [9]. In [9], ten decoding iterations at the relay and ten decoding iterations at the destination are used. We use 20 decoding iterations at both the relay and the destination.|$|R
40|$|Abstract Results from PAMELA and ATIC {{indicate}} that the Kaluza-Klein type dark matter particles could be the annihilation source of the observed excess of electrons and positrons. Assuming {{the existence of a}} nearby black hole with 10 000 – 100000 solar masses and a point source boost algorithm, we apply the <b>standard</b> <b>propagation</b> model and find that the results fit the data well. Key words: dark matter — black hole physics — cosmic rays...|$|E
40|$|We {{present a}} constraint-based model and a {{planning}} engine for manufacturing process planning. By exploiting the expressive power of constraint programming (CP), all relevant, sometimes con- icting pieces of domain knowledge are represented. The planner applies <b>standard</b> <b>propagation</b> techniques together with customized search to nd cost-optimal solutions in presence of hard, soft and conditional constraints. The planning engine {{was built on}} an existing general-purpose CP system and was validated in the domains of machining and of sheet metal bending. ...|$|E
40|$|Possibilistic {{networks}} are important tools {{for dealing with}} uncertain pieces of information. For multiplyconnected networks, {{it is well known}} that the inference process is a hard problem. This paper studies a new representation of possibilistic networks, called hybrid possibilistic networks. The uncertainty is no longer represented by local conditional possibility distributions, but by their compact representations which are possibilistic knowledge bases. We show that the inference algorithm in hybrid networks is strictly more efficient than the ones of <b>standard</b> <b>propagation</b> algorithm...|$|E
30|$|The first code {{considered}} is an n= 4095 and R= 0.82 quasi-regular code [21]. The error {{floor of}} this code {{when using the}} <b>standard</b> belief <b>propagation</b> decoder is shown in Fig.  3.|$|R
40|$|We {{present a}} new type of {{constructive}} algorithm for incremental learning. The algorithm overcomes many of the problems associated with <b>standard</b> back <b>propagation</b> such as speed and optimum network size. We investigate the ability of the network to learn and test the resulting generalisation of the network...|$|R
30|$|To {{determine}} whether asymptomatic tissue culture material contains fungal microorganisms, 30 different Pinus radiata D.Don genotypes from four {{different types of}} tissue culture material were analysed {{for the presence of}} fungal DNA or mycelium. In addition, thirteen fungal and oomycete isolates were cultured on <b>standard</b> tissue <b>propagation</b> media.|$|R
40|$|Recent {{studies have}} shown that {{ultrasonic}} propagation along chain-like structures can give very interesting properties. It is now known that non-linear propagation can take place along these chains, and this leads to some fascinating possibilities. This is because propagation {{is in the form of}} solitary waves. These have much different properties to <b>standard</b> <b>propagation</b> along a wire for example, because the propagation is non-liner, i. e. it is a function of driving amplitude. Such materials were studied at ultrasonic frequencies of 1 MHz and above, and could provide a new approach to transducer design...|$|E
40|$|An {{improved}} scheme {{has been}} developed to solve the <b>standard</b> <b>propagation</b> equations (SPE) 1 for Erbium doped fiber sources (EDFS). 2 The scheme uses Rosenbrock algorithm based on implicit differentiation methods. 3 The profiles of Forward Amplified Spontaneous Emission (FASE), Backward Amplified Spontaneous Emission (BASE) and Pump power along the fiber are presented and compared {{with the results of}} conventional fixed step and embedded Runge-Kutta methods, specifically RK 45. 3 The proposed multi-step method is stable even when designing EDFS for fiber lengths at which the above methods fail to converge. Keywords: EDFA,RK 45, Rosenbrock, Amplified Spontaneous Emission 1...|$|E
40|$|Abstract—We study social {{influence}} from a topic modeling perspective. We introduce novel topic-aware influence-driven propagation models that experimentally result {{to be more}} accurate in describing real-world cascades than the <b>standard</b> <b>propagation</b> models studied in the literature. In particular, we first propose simple topic-aware extensions of the well-known Independent Cascade and Linear Threshold models. Next, we propose a different approach explicitly modeling authoritativeness, influence and relevance under a topic-aware perspective. We devise methods to learn {{the parameters of the}} models from a dataset of past propagations. Our experimentation confirms the high accuracy of the proposed models and learning schemes. I...|$|E
30|$|The <b>standard</b> belief <b>propagation</b> (BP) {{decoding}} algorithm {{consists of}} initialization, check-node updating, symbol-node updating, stopping criterion test, and output steps [20]. In the proposed decoding algorithms, the initialization, stopping criterion test, and output steps {{remain the same}} as that in the standard BP algorithm, and therefore omitted here.|$|R
40|$|In {{dispersion}} managed {{high bit}} rate systems, the importance of correctly choosing the pulse launch position is investigated. Using this technique, error free transmission has been achieved of a 40 Gbit/s 231 - 1 nonlinear RZ PRBS over 1160 km in a dispersion compensated <b>standard</b> fiber <b>propagation</b> experiment with a 75 km standard fiber span...|$|R
40|$|This {{paper is}} {{a review of}} studies on effects of {{nutrients}} on biological productivity and efforts made so far at restoration of nutrients in lakes. It is to provide {{an understanding of the}} basis scientific process accruing in lakes, therefore of prime importance in maintaining water quality <b>standards</b> for <b>propagation</b> of effective lake managemen...|$|R
40|$|The {{implications}} of abundance measurements of cosmic-ray nuclei heavier than Fe are examined {{in light of}} <b>standard</b> <b>propagation</b> models and nucleosynthetic theory. Suggestions that the cosmic-ray source abundances differ significantly from solar system abundances are not well supported by the data without assuming ad hoc models of preferential acceleration. The solar system abundances of Pb and Bi are split into r-, standard s-, and cyclic s-process components to assess the contributions of these nucleosynthetic processes to the cosmic-ray source. The apparent deficiency of Pb seen in the HEAO 3 HNE data might indicate an absence of Pb from the recycling s-process...|$|E
40|$|Mixtures of {{truncated}} exponentials (MTEs) are {{a powerful}} alternative to discretisation {{when working with}} hybrid Bayesian networks. One {{of the features of}} the MTE model is that <b>standard</b> <b>propagation</b> algorithms can be used. However, the complexity of the process is too high and therefore approximate methods, which tradeoff complexity for accuracy, become necessary. In this paper we propose an approximate propagation algorithm for MTE networks which is based on the Penniless propagation method already known for discrete variables. We also consider how to use Markov Chain Monte Carlo to carry out the probability propagation. The performance of the proposed methods is analysed in a series of experiments with random networks...|$|E
40|$|AbstractPossibilistic {{networks}} and possibilistic logic are two standard frameworks {{of interest for}} representing uncertain pieces of knowledge. Possibilistic networks exhibit relationships between variables while possibilistic logic ranks logical formulas according to their level of certainty. For multiply connected networks, it is well-known that the inference process is a hard problem. This paper studies a new representation of possibilistic networks called hybrid possibilistic networks. It results from combining the two semantically equivalent types of standard representation. We first present a propagation algorithm through hybrid possibilistic networks. This inference algorithm on hybrid networks is strictly more efficient (and confirmed by experimental studies) than the one of <b>standard</b> <b>propagation</b> algorithm...|$|E
40|$|It {{is shown}} that in {{strongly}} dispersion managed {{high bit rate}} systems {{it is important to}} correctly choose the pulse launch position. Using this technique error free transmission has been achieved of a 40 Gbit/s 231 - 1 nonlinear RZ PRBS over 1160 km in a dispersion compensated <b>standard</b> fibre <b>propagation</b> experiment with a 75 km standard fibre spa...|$|R
40|$|Abstract. This work {{describes}} {{a method of}} approximating matrix permanents efficiently using belief propagation. We formulate a probability distribution whose partition function is exactly the permanent, then use Bethe free energy to approximate this partition function. After deriving some speedups to <b>standard</b> belief <b>propagation,</b> the resulting algorithm requires (n 2) time per iteration. Finally, we demonstrate the advantages of using this approximation. ...|$|R
5000|$|Spacecraft orbits are {{computed}} by {{the numerical}} {{integration of the}} equation of motion. For this the gravitational force, i.e. the gradient of the potential, must be computed. Efficient recursive algorithms {{have been designed to}} compute the gravitational force for any [...] and [...] (the max degree of zonal and tesseral terms) and such algorithms are used in <b>standard</b> orbit <b>propagation</b> software.|$|R
40|$|High {{resolution}} {{satellite observations}} of the galactic cosmic rays (approximately 100 - 300 MeV/amu) have yielded determinations of near-earth isotopic compositions, which are presented: (C- 13) /(C- 12) = 0. 070 + or - 0. 006; (Si- 29) /(Si- 28) = 0. 109 (+ 0. 024, - 0. 014); and (Si- 30) /(Si- 28) = 0. 084 (+ 0. 020, - 0. 014). Using a <b>standard</b> <b>propagation</b> model, source values greater than solar are derived {{for each of these}} ratios. The (C- 13) excess {{may be the result of}} cross section uncertainties, but the (Si- 29) and (Si- 30) appear to reflect real differences between the galactic cosmic-ray source and the solar system...|$|E
40|$|In this paper, {{we present}} an {{improved}} receiver architecture for sweep-spread-carrier modulation, a spread-spectrum technique proposed to effectively contrast {{the effects of}} time dispersion over multipath propagation channels in underwater acoustic wireless links. The proposed structure is capable of {{taking advantage of the}} energy received from all propagation paths rather than only from the strongest path, as envisaged in the pioneering paper introducing this modulation technique. A hardware version of the modem was implemented in the laboratory and its behavior was assessed and compared, using <b>standard</b> <b>propagation</b> models, to that exhibited by the traditional single-path-based scheme in terms of bit error rate. Results are presented showing that gains of a few decibels can be achieved in signal-to-noise-plus-interference ratio. Issues relevant to carrier/symbol synchronization, channel estimation, and sensitivity to Doppler distortion are also addressed...|$|E
40|$|We {{have studied}} the {{variation}} of e^+ and p̅ top of the atmosphere spectra due to the parameters uncertainties of the Milky Way geometry, propagation models and cross sections. We have used the B/C data and Galprop code for propagation analysis. We have also derived the uncertainty bands for subFe/Fe ratio, H and He. Finally we have considered a neutralino induced component in the mSUGRA framework. We conclude {{that even in the}} case of diffusion convection model for the <b>standard</b> <b>propagation,</b> where secondary spectra gives a good fit of the data, SUSY contribution can not be excluded up to now due to the overall uncertainties. Comment: 8 pages, 11 figures, proceedings for the workshop on "Science with the New Generation of High Energy Gamma-ray Experiments", June 2004, Bari (Italy); added reference...|$|E
40|$|The {{problem of}} low complexity, close to optimal, channel {{decoding}} of linear codes with short to moderate block length is considered. It is shown that deep learning methods {{can be used}} to improve a <b>standard</b> belief <b>propagation</b> decoder, despite the large example space. Similar improvements are obtained for the min-sum algorithm. It is also shown that tying the parameters of the decoders across iterations, so as to form a recurrent neural network architecture, can be implemented with comparable results. The advantage is that significantly less parameters are required. We also introduce a recurrent neural decoder architecture based on the method of successive relaxation. Improvements over <b>standard</b> belief <b>propagation</b> are also observed on sparser Tanner graph representations of the codes. Furthermore, we demonstrate that the neural belief propagation decoder {{can be used to}} improve the performance, or alternatively reduce the computational complexity, of a close to optimal decoder of short BCH codes. Comment: Accepted To IEEE Journal Of Selected Topics In Signal Processin...|$|R
30|$|Fungal DNA was {{detected}} in all samples tested. No fungal or bacterial microorganisms {{were able to}} be grown {{from any of the}} tissue culture material tested. However, confocal microscopy showed the presence of filaments that could have been fungal mycelium. Growth of thirteen fungal and oomycete isolates on <b>standard</b> tissue <b>propagation</b> media indicated these media can severely limit the growth of some of these microorganisms.|$|R
40|$|International audienceIn this paper, we {{investigate}} a minimum time problem for controlled non-autonomous differential systems, with dynamics {{depending on the}} final time. The minimal time function associated to this problem does not satisfy the dynamic programming principle. However, we will prove that it is related to a <b>standard</b> front <b>propagation</b> problem via the reachability function. Two simple numerical examples are given to illustrate our approach...|$|R
40|$|Standard {{radar clutter}} {{reflectivity}} models {{such as the}} GIT sea clutter model [1, 2] and the Billingsley land clutter model [3, 4] are based upon many measurements, in part to remove the effects of varying propagation environment, in order to predict radar reflectivity for <b>standard</b> <b>propagation.</b> It {{has been known for}} some time that radars on ships operating in regions near land are often subject to clutter amplitudes that vary significantly from these models, and this is due to anomalous propagation, or ducting [5, 6]. This paper describes the Littoral Clutter Model (LCM) developed by Naval Surface Warfare Center Dahlgren Division and its validation against recorded radar data. LCM includes propagation modeling and is an extension of both the GIT sea clutter, and Billingsley land clutter models. The model compares favorably to measured land and sea clutter amplitudes in the presence of ducting...|$|E
40|$|One of the {{enablers}} for new {{consumer electronics}} based products {{to be accepted}} in to the market is the availability of inexpensive, flexible and multi-standard chipsets and services. DVB-T, the principal standard for terrestrial broadcast of digital video in Europe, has been extremely successful in leading to governments reconsidering their targets for analogue television broadcast switch-off. To enable one further small step in creating increasingly cost effective chipsets, the ODFM deterministic equalizer has been presented before with its application to DVB-T. This paper discusses the test set-up of a DVB-T compliant baseband simulation that includes the deterministic equalizer and DVB-T <b>standard</b> <b>propagation</b> channels. This is then followed by a presentation of the found inner and outer Bit Error Rate (BER) results using various modulation levels, coding rates and propagation channels in order to ascertain the actual performance of the deterministic equalizer(1) ...|$|E
40|$|Abstract- Statistical {{properties}} of electromagnetic environment in wireless networks affecting its performance and safety are studied. A statistical method to evaluate {{risks to the}} general public due to electromagnetic radiation in wireless networks as well as their performance is proposed. The analysis is based on the <b>standard</b> <b>propagation</b> channel model, a Poisson model of random spatial distribution of transmitters, and a threshold-based model of the victim receptor behaviour (radio receiver or human body). The distribution of dominant interference level is derived and analysed under various network and system configurations. The aggregate interference is dominated by the nearest one. The outage probability is used as a measure of not only the wireless link quality-of-service, but also of environmental risks induced by electromagnetic radiation. The maximum acceptable interference levels for reliable link performance and for low environmental risks are shown to be surprisingly similar. I...|$|E
40|$|The {{purpose of}} this chapter is two-fold: (1) {{to make the case}} that a <b>standard</b> {{backward}} <b>propagation</b> artificial neural network (ANN) can be used as a general model of the information processing activities of the firm, and (2) to present a synthesis of Barr and Saraceno (BS) (2002, 2004, 2005), who offer various models of the firm as an artificial neural network. neural networks, information processing, firm learning, agent-based...|$|R
40|$|Abstract- In this paper, {{computational}} {{data mining}} methodology {{was used to}} predict four major stock market indexes. Two learning algorithms including Linear Regression and Neural Network <b>Standard</b> Back <b>Propagation</b> (SBP) were tested and compared. The models were trained from two years of historical data from January 2006 to December 2007 in order to predict the major stock prices indexes in the United States, Europe, China and Hong Kong. The performance of these prediction models was evaluated using two widely used statistical metrics. The comparison showed that using Neural Network <b>Standard</b> Back <b>Propagation</b> algorithm resulted in better prediction accuracy then using linear regression algorithm. In addition, traditional knowledge shows that a longer training period with more training data could help to build a more accurate prediction model. However, as the stock market in China has been highly fluctuating {{in the past two}} years, this paper shows that data collected from a closer and shorter period could help to reduce the prediction error for such highly speculated fast changing environment...|$|R
40|$|This study {{highlights}} on {{the subject}} of weight initialization in multi-layer feed-forward networks. Training data is analyzed and the notion of criti- cal point is introduced for determining the initial weights for the input to hidden layer synaptic con- nections. The proposed method has been applied to artificial data. The experimental results show that the proposed method takes almost 1 / 2 of the train- ing time required for <b>standard</b> back <b>propagation...</b>|$|R
