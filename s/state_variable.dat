3257|9126|Public
25|$|This kind of evidence, of {{independence}} of sequence of stages, {{combined with the}} above-mentioned evidence, {{of independence}} of qualitative kind of work, would show {{the existence of an}} important <b>state</b> <b>variable</b> that corresponds with adiabatic work, but not that such a <b>state</b> <b>variable</b> represented a conserved quantity. For the latter, another step of evidence is needed, which {{may be related to the}} concept of reversibility, as mentioned below.|$|E
25|$|Sensible heat, in {{contrast}} to latent heat, is heat transferred to a thermodynamic system that effects a change of temperature in the system, while some other specified <b>state</b> <b>variable</b> or variables are left unchanged.|$|E
25|$|This {{statement}} {{helps to}} define temperature {{but it does}} not, by itself, complete the definition. An empirical temperature is a numerical scale for the hotness of a thermodynamic system. Such hotness may be defined as existing on a one-dimensional manifold, stretching between hot and cold. Sometimes the zeroth law is stated to include {{the existence of a}} unique universal hotness manifold, and of numerical scales on it, so as to provide a complete definition of empirical temperature. To be suitable for empirical thermometry, a material must have a monotonic relation between hotness and some easily measured <b>state</b> <b>variable,</b> such as pressure or volume, when all other relevant coordinates are fixed. An exceptionally suitable system is the ideal gas, which can provide a temperature scale that matches the absolute Kelvin scale. The Kelvin scale is defined {{on the basis of the}} second law of thermodynamics.|$|E
50|$|The {{suitable}} {{relationship that}} defines non-equilibrium thermodynamic <b>state</b> <b>variables</b> is as follows. On occasions {{when the system}} {{happens to be in}} states that are sufficiently close to thermodynamic equilibrium, non-equilibrium <b>state</b> <b>variables</b> are such that they can be measured locally with sufficient accuracy by the same techniques as are used to measure thermodynamic <b>state</b> <b>variables,</b> or by corresponding time and space derivatives, including fluxes of matter and energy. In general, non-equilibrium thermodynamic systems are spatially and temporally non-uniform, but their non-uniformity still has a sufficient degree of smoothness to support the existence of suitable time and space derivatives of non-equilibrium <b>state</b> <b>variables.</b> Because of the spatial non-uniformity, non-equilibrium <b>state</b> <b>variables</b> that correspond to extensive thermodynamic <b>state</b> <b>variables</b> have to be defined as spatial densities of the corresponding extensive equilibrium <b>state</b> <b>variables.</b> On occasions when the system is sufficiently close to thermodynamic equilibrium, intensive non-equilibrium <b>state</b> <b>variables,</b> for example temperature and pressure, correspond closely with equilibrium <b>state</b> <b>variables.</b> It is necessary that measuring probes be small enough, and rapidly enough responding, to capture relevant non-uniformity. Further, the non-equilibrium <b>state</b> <b>variables</b> are required to be mathematically functionally related to one another in ways that suitably resemble corresponding relations between equilibrium thermodynamic <b>state</b> <b>variables.</b> In reality, these requirements are very demanding, and it may be difficult or practically, or even theoretically, impossible to satisfy them. This is part of why non-equilibrium thermodynamics is a work in progress.|$|R
50|$|The {{minimum number}} of <b>state</b> <b>variables</b> {{required}} to describe a system equals {{the order of the}} differential equation; more <b>state</b> <b>variables</b> than the minimum can be defined. For example, a second order system can be defined by two(minimal realization) or more <b>state</b> <b>variables.</b>|$|R
40|$|This {{paper is}} devoted to {{questions}} of improved <b>state</b> <b>variables</b> filters synthesis for discrete dynamic objects based on correct problem statement of the filtration considering influence of entry conditions mismatch of object <b>state</b> <b>variables</b> and the filter on filtration quality. Corresponding statement filtration problem and a method of improved <b>state</b> <b>variables</b> filters synthesis is resulted. Results of experimental researches which prove, that efficiency of <b>state</b> <b>variables</b> estimations received by the developed filter is resulted, it is essential above efficiency standard Kalman?s estimations. ? ?????? ??????????????? ??????? ??????? ???????? ?????????? ????????? ?????????? ???????????? ???????? ?? ?????? ?????????? ?????????? ?????? ??????????, ??????????? ??????? ??????????????? ????????? ??????? ?????????? ????????? ??????? ? ??????? ?? ???????? ??????????. ????????? ??????????????? ?????????? ?????? ?????????? ? ????? ??????? ?????????? ???????? ?????????? ?????????. ????????? ?????????? ????????????????? ????????????, ??????? ??????????, ??? ????????????? ?????? ?????????? ?????????, ?????????? ????????????? ????????, ??????????? ???? ????????????? ??????????? ???????????? ??????...|$|R
25|$|Since gas {{molecules}} {{can move}} freely within a container, their mass is normally characterized by density. Density {{is the amount}} of mass per unit volume of a substance, or the inverse of specific volume. For gases, the density can vary over a wide range because the particles are free to move closer together when constrained by pressure or volume. This variation of density is referred to as compressibility. Like pressure and temperature, density is a <b>state</b> <b>variable</b> of a gas and the change in density during any process is governed by the laws of thermodynamics. For a static gas, the density is the same throughout the entire container. Density is therefore a scalar quantity. It can be shown by kinetic theory that the density is inversely proportional {{to the size of the}} container in which a fixed mass of gas is confined. In this case of a fixed mass, the density decreases as the volume increases.|$|E
25|$|The {{internal}} state {{variables are}} the smallest possible subset of system variables that can represent {{the entire state}} of the system at any given time. The minimum number of state variables required to represent a given system, , is usually equal to the order of the system's defining differential equation. If the system is represented in transfer function form, the minimum number of state variables is equal to the order of the transfer function's denominator after it has been reduced to a proper fraction. It is important to understand that converting a state-space realization to a transfer function form may lose some internal information about the system, and may provide a description of a system which is stable, when the state-space realization is unstable at certain points. In electric circuits, the number of state variables is often, though not always, the same as the number of energy storage elements in the circuit such as capacitors and inductors. The state variables defined must be linearly independent, i.e., no <b>state</b> <b>variable</b> can be written as a linear combination of the other state variables or the system {{will not be able to}} be solved.|$|E
2500|$|... with [...] {{the volume}} of the system, which is a <b>state</b> <b>variable.</b> In general, for {{homogeneous}} systems, ...|$|E
50|$|A {{slightly}} weaker notion than controllability {{is that of}} stabilizability. A {{system is}} said to be stabilizable when all uncontrollable <b>state</b> <b>variables</b> can be made to have stable dynamics. Thus, even though some of the <b>state</b> <b>variables</b> cannot be controlled (as determined by the controllability test above) all the <b>state</b> <b>variables</b> will still remain bounded during the system's behavior.|$|R
40|$|This paper proposes lazy group sifting for dynamic <b>variable</b> {{reordering}} during <b>state</b> traversal. The proposed method relaxes {{the idea}} of pairwise grouping of present <b>state</b> <b>variables</b> and their corresponding next <b>state</b> <b>variables.</b> This is done to produce better variable orderings during image computation without causing BDD size blowup in the substitution of next <b>state</b> <b>variables</b> with present <b>state</b> <b>variables</b> {{at the end of}} image computation. Experimental results show that our approach is more robust in state traversal than the approaches that either unconditionally group variable pairs or never group them. ...|$|R
50|$|The most {{commonly}} used languages for representing planning domains and specific planning problems, such as STRIPS and PDDL for Classical Planning, are based on <b>state</b> <b>variables.</b> Each possible <b>state</b> {{of the world is}} an assignment of values to the <b>state</b> <b>variables,</b> and actions determine how the values of the <b>state</b> <b>variables</b> change when that action is taken. Since a set of <b>state</b> <b>variables</b> induce a <b>state</b> space that has a size that is exponential in the set, planning, similarly to many other computational problems, suffers from the curse of dimensionality and the combinatorial explosion.|$|R
2500|$|Usually {{transfer}} {{between a}} {{system and its}} surroundings applies to transfer of a <b>state</b> <b>variable,</b> and obeys a balance law, that the amount lost by the donor system {{is equal to the}} amount gained by the receptor system. Heat is not a <b>state</b> <b>variable.</b> For his 1947 definition of [...] "heat transfer" [...] for discrete open systems, the author Prigogine carefully explains at some length that his definition of it does not obey a balance law. He describes this as paradoxical.|$|E
2500|$|Historically, many energy {{units for}} {{measurement}} of heat have been used. The standards-based {{unit in the}} International System of Units (SI) is the joule (J). Heat is measured by {{its effect on the}} states of interacting bodies, for example, by the amount of ice melted or a change in temperature. [...] The quantification of heat via the temperature change of a body is called calorimetry, and is widely used in practice. [...] In calorimetry, sensible heat is defined with respect to a specific chosen <b>state</b> <b>variable</b> of the system, such as pressure or volume. Sensible heat causes a change of the temperature of the system while leaving the chosen <b>state</b> <b>variable</b> unchanged. Heat transfer that occurs at a constant system temperature but changes the <b>state</b> <b>variable</b> is called latent heat with respect to the variable. For infinitesimal changes, the total incremental heat transfer is then the sum of the latent and sensible heat.|$|E
2500|$|Some {{languages}} support {{breaking out}} of nested loops; in theory circles, these are called multi-level breaks. One common use example is searching a multi-dimensional table. This can be done either via multilevel breaks (break out of N levels), as in bash and PHP, or via labeled breaks (break out and continue at given label), as in Java and Perl. Alternatives to multilevel breaks include single breaks, together with a <b>state</b> <b>variable</b> which is tested to break out another level; exceptions, which are caught at the level being broken out to; placing the nested loops in a function and using return to effect termination of the entire nested loop; or using a label and a goto statement. C does not include a multilevel break, and the usual alternative {{is to use a}} goto to implement a labeled break. Python does not have a multilevel break or continue – this was proposed in , and rejected on the basis that the added complexity was not worth the rare legitimate use.Python-3000] Announcing PEP 3136], Guido van Rossum ...|$|E
50|$|Multi-code state {{assignment}} technique implements priority encoding by restraining redundant states. Thus {{state can}} be encoded using fewer <b>state</b> <b>variables</b> (bits). Further, flip-flops corresponding to those absent <b>state</b> <b>variables</b> can be clock-gated.|$|R
40|$|Multibeta asset pricing {{models are}} {{examined}} using proxies for economic <b>state</b> <b>variables</b> in {{a framework that}} exploits time-varying expected returns to estimate conditional betas. Examples include multiple consumption-beta models and models where asset returns proxy for the <b>state</b> <b>variables.</b> When the <b>state</b> <b>variables</b> are not specified, the tests indicate two or three time-varying expected risk premiums in the sample of quarterly asset returns. Conditional betas relative to consumption generate less striking evidence against the model than betas relative to asset returns, but both the consumption and the market variables fail to proxy for the <b>state</b> <b>variables.</b> Copyright 1990 by American Finance Association. ...|$|R
40|$|We {{develop a}} new {{estimation}} methodology for dynamic optimization models with unobserved <b>state</b> <b>variables</b> Our approach is semiparametric {{in the sense of}} not requiring explicit parametric assumptions to be made concerning the distribution of these unobserved <b>state</b> <b>variables</b> We propose a two-step pairwise-difference estimator which exploits two common features of dynamic optimization problems: (1) the weak monotonicity of the agent's decision (policy) function in the unobserved <b>state</b> <b>variables</b> conditional on the observed state variables; and (2) the state-contingent nature of optimal decision-making which implies that conditional on the observed <b>state</b> <b>variables</b> the variation in observed choices across agents must be due to randomness in the unobserved <b>state</b> <b>variables</b> across agents We apply our estimator to a model of dynamic competitive equilibrium in the market for milk production quota in Ontario Canad...|$|R
2500|$|That {{important}} <b>state</b> <b>variable</b> {{was first}} recognized and denoted [...] by Clausius in 1850, {{but he did}} not then name it, and he defined it in terms not only of work but also of heat transfer in the same process. It was also independently recognized in 1850 by Rankine, who also denoted it [...] and in 1851 by Kelvin who then called it [...] "mechanical energy", and later [...] "intrinsic energy". In 1865, after some hestitation, Clausius began calling his state function [...] "energy". In 1882 it was named as the internal energy by Helmholtz. If only adiabatic processes were of interest, and heat could be ignored, the concept of internal energy would hardly arise or be needed. The relevant physics would be largely covered by the concept of potential energy, as was intended in the 1847 paper of Helmholtz on the principle of conservation of energy, though that did not deal with forces that cannot be described by a potential, and thus did not fully justify the principle. Moreover, that paper was critical of the early work of Joule that had by then been performed. A great merit of the internal energy concept is that it frees thermodynamics from a restriction to cyclic processes, and allows a treatment in terms of thermodynamic states.|$|E
2500|$|This {{definition}} {{rests on}} the physical assumption that there are readily available walls permeable only to heat. In his detailed definition of a wall permeable only to heat, Carathéodory includes several ideas. The non-deformation <b>state</b> <b>variable</b> of a closed system is represented as a real number. A state of thermal equilibrium between two closed systems connected by a wall permeable only to heat means that a certain mathematical relation holds between the state variables, including the respective non-deformation variables, of those two systems (that particular mathematical relation is regarded by Buchdahl as a preferred statement of the zeroth law of thermodynamics). Also, referring to thermal contact equilibrium, [...] "whenever each of the systems [...] and [...] is made to reach equilibrium with a third system [...] under identical conditions, the systems [...] and [...] are in mutual equilibrium." [...] It {{may be viewed as}} a re-statement of the principle stated by Maxwell in the words: [...] "All heat is of the same kind." [...] This physical idea is also expressed by Bailyn as a possible version of the zeroth law of thermodynamics: [...] "All diathermal walls are equivalent." [...] Thus the present definition of thermodynamic temperature {{rests on the}} zeroth law of thermodynamics. Explicitly, this present definition of thermodynamic temperature also rests on the first law of thermodynamics, for the determination of amounts of energy transferred as heat.|$|E
2500|$|It {{has been}} argued that the current {{challenges}} facing risk assessments can be addressed with TKTD modeling. [...] TKTD models were derived in response to a couple of factors. [...] One is the lack of time being considered as a factor in toxicity and risk assessment. [...] Some of the earliest developed TKTD models, such as the Critical Body Residue (CBR) model and Critical Target Occupation (CTO) model, have considered time as a factor but a criticism has been that they are for very specific circumstances such as reversibly acting toxicants or irreversibly acting toxicants. [...] Further extrapolation of the CTO and CBR models are DEBtox, which can model sublethal endpoints, and hazard versions of the CTO, which takes into account stochastic death as opposed to individual tolerance. [...] Another significant step to developing TKTD models was the incorporation of a <b>state</b> <b>variable</b> for damage. [...] By using damage as a toxicodynamic state-variable, modeling intermediate recovery rates can be accomplished for toxicants that act reversibly with their targets, without the assumptions of instant recovery (CBR model) or irreversible interactions (CTO model). TKTD models that incorporate damage are the Damage Assessment Model (DAM) and the Threshold Damage Model (TDM). For what may seem like straightforward endpoints, a variety of different TKTD approaches exist. [...] A review of the assumptions and hypotheses of each was previously published {{in the creation of a}} general unified threshold model of survival (GUTS).|$|E
40|$|We {{estimate}} {{the parameters of}} pricing kernels that depend on both aggregate wealth and <b>state</b> <b>variables</b> that describe the investment opportunity set, using FTSE 100 and S&P 500 index option returns as the returns to be priced. The coefficients of the <b>state</b> <b>variables</b> are highly significant and remarkably consistent across specifications of the pricing kernel, and across the two markets. The results provide further evidence that, consistent with Merton’s (1973) Intertemporal Capital Asset Pricing Model, <b>state</b> <b>variables</b> in addition to market risk The failure of simple complete markets option pricing models of the Black-Scholes (1973) type points to the importance in option pricing of <b>state</b> <b>variables</b> other than the underlying asset price. Despite increasing evidence that <b>state</b> <b>variables</b> other than the market index are important for pricing bot...|$|R
30|$|Some <b>state</b> <b>variables</b> of WTs have {{significant}} impacts on EOMs of synchronous system, while others affect it less. To simplify the calculation, the <b>state</b> <b>variables</b> having little relation with EOMs can be ignored.|$|R
5000|$|State {{assignment}} assigns <b>state</b> <b>variables</b> to each <b>state</b> {{based on}} the data gathered {{in the first two}} steps in order to minimize the switching activity. There are three algorithm to assign <b>state</b> <b>variables,</b> ...|$|R
5000|$|Coupled forms: Gold Rader (normal), <b>State</b> <b>Variable</b> (Chamberlin), Kingsbury, Modified <b>State</b> <b>Variable,</b> Zölzer, Modified Zölzer ...|$|E
5000|$|Where the [...] symbol on the <b>state</b> <b>variable</b> in the {{postcondition}} {{indicates the}} value of the <b>state</b> <b>variable</b> before execution of the operation.|$|E
50|$|This can be decoded {{properly}} {{by paying}} attention to the leap second <b>state</b> <b>variable,</b> which unambiguously indicates whether the leap has been performed yet. The <b>state</b> <b>variable</b> change is synchronous with the leap.|$|E
40|$|We {{describe}} a new method to model gene expression from time-course gene expression data. The modelling is {{in terms of}} state-space descriptions of linear systems. A cell can be considered to be a system where the behaviours (responses) of the cell depend completely on the current internal state plus any external inputs. The gene expression levels in the cell provide information about the behaviours of the cell. In previously proposed methods, genes were viewed as internal <b>state</b> <b>variables</b> of a cellular system and their expression levels were the values of the internal <b>state</b> <b>variables.</b> This viewpoint has suffered from the underestimation of the model parameters. Instead, we view genes as the observation variables, whose expression values depend on the current internal <b>state</b> <b>variables</b> and any external input. Factor analysis is used to identify the internal <b>state</b> <b>variables,</b> and Bayesian Information Criterion (BIC) is used to determine the number of the internal <b>state</b> <b>variables.</b> By building dynamic equations of the internal <b>state</b> <b>variables</b> and the relationships between the internal <b>state</b> <b>variables</b> and the observation variables (gene expression profiles), we get state-space descriptions of gene expression model. In the present method, model parameters may be unambiguously identified from time-course gene expression data. We apply the method to two time-course gene expression datasets to illustrate it. 1...|$|R
30|$|Hinkelmann et al.’s (2011) ODD {{extension}} maps each agent’s {{state into}} {{a set of}} <b>state</b> <b>variables.</b> A set of time-indexed polynomial discrete dynamical equations that update the values of each agent’s <b>state</b> <b>variables</b> are then defined {{in such a way}} as to allow the <b>state</b> <b>variables</b> to form an algebraic field. They then use the rich body of existing algebraic techniques to prove system-level properties of two simple demonstration models.|$|R
40|$|Gradient {{matching}} with Gaussian processes is {{a promising}} tool for learning parameters of ordinary differential equations (ODE's). The essence of gradient matching is {{to model the}} prior over <b>state</b> <b>variables</b> as a Gaussian process which implies that the joint distribution given the ODE's and GP kernels is also Gaussian distributed. The state-derivatives are integrated out analytically since they are modelled as latent <b>variables.</b> However, the <b>state</b> <b>variables</b> themselves are also latent variables because they are contaminated by noise. Previous work sampled the <b>state</b> <b>variables</b> since integrating them out is not analytically tractable. In this paper we use mean-field approximation to establish tight variational lower bounds that decouple <b>state</b> <b>variables</b> and are therefore, {{in contrast to the}} integral over <b>state</b> <b>variables,</b> analytically tractable and even concave for a restricted family of ODE's, including nonlinear and periodic ODE's. Such variational lower bounds facilitate "hill climbing" to determine the maximum a posteriori estimate of ODE parameters. An additional advantage of our approach over sampling methods is the determination of a proxy to the intractable posterior distribution over <b>state</b> <b>variables</b> given observations and the ODE's...|$|R
5000|$|Analog-inspired forms such as Sallen-key and <b>state</b> <b>variable</b> filters ...|$|E
5000|$|Sandia Doctoral Study Program (1993-1995) {{internal}} <b>state</b> <b>variable</b> plasticity ...|$|E
50|$|To relate {{this concept}} to programming, {{this means that}} instead of {{recording}} the event history in a multitude of variables, flags, and convoluted logic, you rely mainly on just one <b>state</b> <b>variable</b> that can assume {{only a limited number}} of a priori determined values (e.g., two values in case of the keyboard). The value of the <b>state</b> <b>variable</b> defines the current state of the system at any given time. The concept of state reduces the problem of identifying the execution context in the code to testing just the <b>state</b> <b>variable</b> instead of many variables, thus eliminating a lot of conditional logic.|$|E
40|$|In {{this paper}} {{we present a}} {{technique}} for deriving real-time programs from a formal specification of the requirements of real-time systems. The requirements of the real-time system are specified as a DC formula over continuous <b>state</b> <b>variables.</b> We approximate continuous <b>state</b> <b>variables</b> by discrete ones and formalise the relationship between them. Then a discrete design is formulated as a DC formula over discrete <b>state</b> <b>variables,</b> and proved to be correct w. r. t. the continuous specification of the system under some assumptions about the behaviour {{of the environment and}} the relationship between continuous <b>state</b> <b>variables</b> and discrete <b>state</b> <b>variables.</b> Then a real-time control program is derived from the discrete design. We extend the Hoare triples to embed the real-time behaviour of program and develop a set of compositional rules for the design and the verification of real-time programs...|$|R
40|$|The paper {{presents}} a syntactical approach to designing real-time distributed systems {{that can handle}} both continuous time and discrete time models in a uniform logical framework. We approximate continuous <b>state</b> <b>variables</b> by discrete ones and formalise the relationship between them. The requirements of a real-time system are specified as a formula over continuous <b>state</b> <b>variables.</b> Then we derive a discrete design of a digital controller as formula over discrete <b>state</b> <b>variables</b> that makes the system to satisfy the continuous specification under some assumptions about the behaviour {{of the environment and}} the relationship between continuous <b>state</b> <b>variables</b> and discrete <b>state</b> <b>variables.</b> We provide rules useful for refining and verifying the correctness of a design syntactically. Keywords: Continuous specification, discrete design, real-time distributed systems, verification. 1. Introduction A real-time system {{can be thought of as}} a plant in permanent interaction with its environment. Howev [...] ...|$|R
30|$|The term {{state of}} the system, rigorously defined through the <b>state</b> <b>variables</b> of the system, is used {{extensively}} in discussing and modelling/simulation of systems. These <b>state</b> <b>variables</b> are chosen according {{to the nature of}} the system.|$|R
