17|10000|Public
40|$|To achieve high {{throughput}} in wireless networks {{a partial}} parallel LDPC decoder is proposed in this paper. For fullyparallel decoders, it suffers from large hardware complexity {{caused by a}} large <b>set</b> <b>of</b> <b>processing</b> <b>units</b> and complex interconnections. In wireless networks coding complexity and routing congestion can be reduced by designing the decoder with partially-parallel architecture. The partially-parallel architecture with Split Row algorithm reduces the total global wire length by about 26 % without any hardware overhead and increasing the throughput by 60 % and 71 % in wireless networks...|$|E
40|$|There is {{a wealth}} of {{literature}} on distributed algorithms and data structures. Standard models used in the research community are synchronous or asynchronous shared memory or network models. The shared memory model is basically a generalization of the von Neumann model from one processing unit to multiple processing units or processes acting on a single, linear addressable memory. In the network model, there is no shared memory. Every processing unit has its own, private memory, and the processing units are connected by a network of (usually) bidirectional communication links that allow the processing units to exchange messages. The <b>set</b> <b>of</b> <b>processing</b> <b>units</b> is usually considered to be fixed though processing units may fail and recover according to some stochastic or adversarial model. With the rise of very large distributed systems such as peer-to-peer systems, these models are not appropriate any more. For example, the <b>set</b> <b>of</b> <b>processing</b> <b>units</b> can be highly dynamic and {{there may not be}} any mutual trust relationships between the units. This creates fundamental problems, such as keeping the (honest) units in a single connected component, that the previous models cannot address in their basic form. We show how to extend the network model so that we have a model that is powerful enough to design algorithms and data structures that are provably robust even against massive adversaria...|$|E
40|$|A {{new digital}} VLSI {{architecture}} {{has been presented}} {{for the implementation of}} discrete-time multilayer CNNs. At functional level, the architecture is organized as 12 layers of 64 × 64 cells, which interact as specified by a set of 3 -D generalized templates. At structural level, the application of cloning templates occurs in a <b>set</b> <b>of</b> <b>processing</b> <b>units</b> programmed by instruction masks, generated {{on the basis of the}} algorithm to be emulated. It is demonstrated that this architecture is applicable to multilayer algorithms for visual processing, and also to standard CNNs, including those that use sequences of templates or that work in parallel. Simulations evidence the high efficiency of this implementation...|$|E
40|$|Stochastic {{evolutionary}} {{growth and}} pattern formation models are {{treated in a}} unified way in terms of algorithmic models of nonlinear dynamic systems with feedback built <b>of</b> a standard <b>set</b> <b>of</b> signal <b>processing</b> <b>units.</b> A number <b>of</b> concrete models is described and illustrated by numerous examples of artificially generated patterns that closely imitate wide variety of patterns found in the nature...|$|R
40|$|We are {{considering}} the following assignment problem. A finite <b>set</b> T <b>of</b> tasks th, h= 1, [...] .,n, {{has to be}} assigned to a finite <b>set</b> P <b>of</b> <b>processing</b> <b>units</b> pj, j= 1, [...] .,m. Each task th has to be assigned to exactly one <b>processing</b> <b>unit,</b> however, more than one <b>processing</b> <b>unit</b> is able to process th. Each task th has, moreover, a different “preference ” for being assigned to a particular <b>processing</b> <b>unit...</b>|$|R
50|$|The RH-32 was a radiation-hardened 32-bit {{microprocessor}} chipset {{developed by}} the USAF Rome Laboratories for the Ballistic Missile Defense Agency, and produced by Honeywell (later, TRW) for Aerospace applications. It achieves a throughput of 20 MIPS. It was a three-chip <b>set,</b> consisting <b>of</b> Central <b>Processing</b> <b>Unit,</b> Floating Point Unit, and Cache Memory.|$|R
40|$|International audienceLet us {{consider}} an upper bounded number of data streams {{to be processed}} by a Divisible Load application. The total workload is unknown and the available speeds for communicating and computing can be poorly a priori estimated. This paper presents a resource selection method that aims at maximizing the throughput of this processing. From a <b>set</b> <b>of</b> <b>processing</b> <b>units</b> linked by a network, this method consists in forming an optimal set of master-workers clusters. Results of simulations are presented to assess the efficiency of this method experimentally. Before focusing on the proposed resource selection method, the paper reminds the underlying adaptive scheduling method on which it relies...|$|E
30|$|NN [13] {{are those}} systems modeled {{based on the}} human brain working. As the human brain {{consists}} of millions of neurons that are interconnected by synapses, a neural network {{is a set of}} connected input or output units in which each connection has a weight associated with it. The network learns in the learning phase by adjusting the weights so as to be able to predict the correct class label of the input. An artificial neural network consists of connected <b>set</b> <b>of</b> <b>processing</b> <b>units.</b> The connections have weights that determine how one unit will affect another. Subsets of such units act as input and output nodes, and the remaining nodes constitute the hidden layer. By assigning activation to each of the input node and allowing them to propagate through the hidden layer nodes to the output nodes, neural network performs a functional mapping from input values to output values.|$|E
40|$|Parallel {{computation}} requires splitting a job among a <b>set</b> <b>of</b> <b>processing</b> <b>units</b> called workers. The computation {{is generally}} {{performed by a}} set of one or more master workers that split the workload into chunks and distribute them to a set of slave workers. In this setting, communication among workers can be problematic and/or time consum-ing. Tree search algorithms are particularly suited for being applied in a parallel fashion, as different nodes can be processed by different workers in parallel. In this paper we propose a simple mechanism to convert a se-quential tree-search code into a parallel one. In the new paradigm, called SelfSplit, each worker is able to autonomously determine, without any communication with the other workers, the job parts it has to process. Computational results are reported, showing that SelfSplit can achieve an almost linear speedup for hard Constraint Programming applications, even when 64 workers are considered...|$|E
40|$|International audienceThe {{computation}} of {{the distance}} function is a crucial and limiting element in many applications <b>of</b> image <b>processing.</b> This is particularly true for the PDE-based methods, where the distance is used to compute various geometric properties of the travelling curve. Massive Marchingais a parallel algorithm computing the distance function by propagating the solution from the sources and permitting simultaneous spreading of componentlabels in the influence zones. Its hardware implementation is conceivable as no sorted data structures are used. The feasibility is demonstrated here on a <b>set</b> <b>of</b> parallely-operating <b>Processing</b> <b>Units</b> arranged in a linear array. The text concludes by {{a study of the}} accuracy and the implementation cost...|$|R
40|$|The {{analysis}} {{of the problem of}} distribution of intelligence and a novel technique for dynamic task allocation and reconfiguration in a distributed system has been proposed. A formalization of task allocation problem was proposed and a system that was able to automatically download and run logical modules onto physical processors was described. A model was proposed to decompose logical surveillance functionalities into a <b>set</b> <b>of</b> modulues and to optimally allocate such modules among a <b>set</b> <b>of</b> physical <b>processing</b> <b>units</b> by minimizing a functional cost. The example of working system that provides a complete distributed computing environment infrastructure characterized by an highly scalable model, to handle data...|$|R
40|$|Heterogeneous and {{asymmetric}} computing {{systems are}} com-posed by a <b>set</b> <b>of</b> different <b>processing</b> <b>units,</b> {{each with its}} own unique performance and energy characteristics. Still, the majority of current network packet processing frameworks targets only a single device (the CPU or some accelerator), leaving other processing resources idle. In this paper, we propose an adaptive scheduling approach that supports het-erogeneous and asymmetric hardware, tailored for network packet processing applications. Our scheduler is able to re-spond quickly to dynamic performance fluctuations that oc-cur at real-time, such as traffic bursts, application overloads and system changes. The experimental results show that our system is able to match the peak throughput <b>of</b> a diverse <b>set</b> <b>of</b> packet <b>processing</b> workloads, while consuming up to 3. 5 x less energy...|$|R
40|$|Abstract. The Self Distributing Virtual Machine (SDVM) is a middle-ware {{concept to}} form a {{parallel}} computing machine consisting of a any <b>set</b> <b>of</b> <b>processing</b> <b>units,</b> such as functional units in a processor or FPGA, processing units in a multiprocessor chip, or computers in a computer cluster. Its structure and functionality is biologically inspired aiming towards forming a combined workforce of independent units (”sites”), each acting on {{the same set of}} simple rules. The SDVM supports growing and shrinking the cluster at runtime as well as heterogeneous clusters. It uses the work-stealing principle to dynamically distribute the workload among all sites. The SDVM’s en-ergy management targets the health of all sites by adjusting their power states according to workload and temperature. Dynamic reassignment of the current workload facilitates a new energy policy which focuses on increasing the reliability of each site. This paper presents the structure and the functionality of the SDVM. ...|$|E
40|$|A {{biorefinery}} involving internal stream reuse {{and recycling}} (including products and co-products) {{should result in}} better biomass resource utilisation, leading to a system with increased efficiency, flexibility, profitability and sustainability. To benefit from those advantages, process integration methodologies need {{to be applied to}} understand, analyse and design highly integrated biorefineries. A bioethanol integration approach based on mass pinch analysis is presented in this work for the analysis and design of product exchange networks formed in biorefinery pathways featuring a <b>set</b> <b>of</b> <b>processing</b> <b>units</b> (sources and demands) producing or utilising bioethanol. The method is useful to identify system debottleneck opportunities and alternatives for bioethanol network integration that improve utilisation efficiency in biorefineries with added value co-products. This is demonstrated by a case study using a biorefinery producing bioethanol from wheat with arabinoxylan (AX) co-production using bioethanol for AX precipitation. The final integrated bioethanol network design allowed the reduction of bioethanol product utilisation by 94 %, avoiding significant revenue losses. © 2012 Elsevier Ltd...|$|E
40|$|The Self Distributing Virtual Machine (SDVM) is a {{middleware}} {{concept to}} form a parallel computing machine consisting of a any <b>set</b> <b>of</b> <b>processing</b> <b>units,</b> such as functional units in a processor or FPGA, processing units in a multiprocessor chip, or computers in a computer cluster. Its structure and functionality is biologically inspired aiming towards forming a combined workforce of independent units (”sites”), each acting on {{the same set of}} simple rules. The SDVM supports growing and shrinking the cluster at runtime as well as heterogeneous clusters. It uses the work-stealing principle to dynamically distribute the workload among all sites. The SDVM’s energy management targets the health of all sites by adjusting their power states according to workload and temperature. Dynamic reassignment of the current workload facilitates a new energy policy which focuses on increasing the reliability of each site. This paper presents the structure and the functionality of the SDVM. 1 st IFIP International Conference on Biologically Inspired Cooperative Computing - Mechatronics and Computer Cluster...|$|E
40|$|International audienceTo fully {{tap into}} the {{potential}} of today heterogeneous machines, offloading parts of an application on accelerators is no longer sufficient. The real challenge is to build systems where the application would permanently spread across the entire machine, that is, where parallel tasks would be dynamically scheduled over the full <b>set</b> <b>of</b> available <b>processing</b> <b>units.</b> In this paper we present SOCL, an OpenCL implementation that improves and simplifies the programming experience on heterogeneous architectures. SOCL enables applications to dynamically dispatch computation kernels over processing devices so as to maximize their utilization. OpenCL applications can incrementally make use of light extensions to automatically schedule kernels {{in a controlled manner}} on multi-device architectures. We demonstrate the relevance of our approach by experimenting with several OpenCL applications on a range of heterogeneous architectures. We show that performance portability is enhanced by using SOCL extensions...|$|R
40|$|Abstract. In {{this paper}} PAPRICA, a massively {{parallel}} coprocessor {{devoted to the}} analysis of bitmapped images is presented considering first the computational model, then the architecture and its implementation, and finally the performance analysis. The main goal of the project was to develop a subsystem to be attached to a standard workstation and to operate as a specialized processing module in dedicated systems. The computational model is strongly related to the concepts of mathematical morphology, and therefore the instruction <b>set</b> <b>of</b> the <b>processing</b> <b>units</b> implements basic morphological transformations. Moreover, the specific processor virtualization mechanism allows to handle and process multiresolution data sets. The actual implementation consists of a mesh of 256 single bit <b>processing</b> <b>units</b> operating in a SIMD style and is based on a <b>set</b> <b>of</b> custom VLSI circuits. The architecture comprises specific hardware extensions that significantly improved performances in real-time applications. 1...|$|R
40|$|Document-Based Integrated Management (DBIM) {{is a novel}} way of {{consolidating}} {{enterprise network}} and system man-agement. We present an approach to DBIM that utilizes the autonomic capabilities <b>of</b> a <b>set</b> <b>of</b> distributed Document <b>Processing</b> <b>Units</b> (DPU), which constitute a document pro-cessing plane within the enterprise network. Our approach is explained in detail using a scenario from the performance and fault management domain. We also explore the connec-tion of DBIM with current research topics in the autonomic systems field, and the relationships to other popular inte-grated management approaches, like Policy-Based Manage...|$|R
40|$|Abstract — The fully {{parallel}} LDPC decoding architecture {{can achieve}} high decoding throughput, but it suffers from large hardware complexity {{caused by a}} large <b>set</b> <b>of</b> <b>processing</b> <b>units</b> and complex interconnections. A practical solution of area-efficient decoders {{is to use the}} partially parallel architecture in which a PU is shared for a several rows or columns. It is important in the partially parallel architecture to determine the rows or columns to be processed in a PU and their processing order. The dependencies between rows and columns should be considered to minimize the overall processing time by overlapping the decoding operations. This paper proposes an efficient scheduling algorithm that can be applied to general LDPC codes, which is based on the concept of the matrix permutation. Experimental results show that the proposed scheduling achieves a higher decoding rate, leading to a reduction of 25 % processing time on the average. A 1024 -bit rate- 1 / 2 LDPC decoder employing the proposed scheduling algorithm provides almost 1 Gbps decoding throughput and occupies one-fifth area compared to the fully parallel decoder. I...|$|E
40|$|Abstract – The fully {{parallel}} LDPC decoding architecture {{can achieve}} high decoding throughput, but it suffers from large hardware complexity {{caused by a}} large <b>set</b> <b>of</b> <b>processing</b> <b>units</b> and complex interconnections. A practical solution to achieve area-efficient decoders {{is to use the}} folded architecture in which a PU is time-multiplexed for several row or column operations. In the folded architecture, the rows or columns to be processed in a PU and their processing order should be carefully determined by analyzing their dependencies among rows and columns to enable overlapped processing that leads to increased performance. Finding an optimum grouping of rows and columns for each PU, however, takes considerable amount of time, since the number of rows and columns can be several thousands. This paper proposes an efficient scheduling algorithm that can be applied to general LDPC codes, which is based on the concept of the matrix permutation. Experimental results show that the proposed scheduling achieves a reduction of 36. 7 % processing time, on the average, for various LDPC codes, leading to a higher decoding rate...|$|E
40|$|We {{describe}} {{a new technique}} {{that can be used}} to derandomize a number of randomized algorithms for routing and sorting on meshes. We demonstrate the power of this technique by deriving improved deterministic algorithms for a variety of routing and sorting problems. Our main results are an optimal algorithm for k-k routing on multi-dimensional meshes, a permutation routing algorithm with running time 2 n+o(n) and queue size 5, and an optimal algorithm for 1 - 1 sorting. 1 Introduction One of the main problems in the simulation of idealistic parallel computers by realistic ones is the problem of message routing through the sparse network of links connecting a <b>set</b> <b>of</b> <b>processing</b> <b>units</b> (PUs) among each other. In this paper, we consider the case of the n Θ n mesh, in which n 2 PUs are connected by a regular twodimensional grid of bidirectional communication links. There may also be additional wrap-around connections between the two PUs at opposite ends of each row and each column of t [...] ...|$|E
40|$|We outline {{our plans}} for {{incorporating}} a Neural Network Prototyping Package into the IRAF environment. The package we are developing {{will allow the}} user to choose between different types of networks and to specify {{the details of the}} particular architecture chosen. Neural networks consist of a highly interconnected <b>set</b> <b>of</b> simple <b>processing</b> <b>units.</b> The strengths <b>of</b> the connections between units are determined by weights which are adaptively set as the network 'learns'. In some cases, learning can be a separate phase of the user cycle of the network while in other cases the network learns continuously. Neural networks {{have been found to be}} very useful in pattern recognition and image processing applications. They can form very general 'decision boundaries' to differentiate between objects in pattern space and they can be used for associative recall of patterns based on partial cures and for adaptive filtering. We discuss the different architectures we plan to use and give examples of what they can do...|$|R
40|$|Abstract—In {{the last}} decade, {{the field of}} {{microprocessor}} architecture has seen the rise of multicore processors, which consist of the interconnection <b>of</b> a <b>set</b> <b>of</b> independent <b>processing</b> <b>units</b> or cores in the same chip. As the number of cores per multiprocessor increases, the bandwidth and energy requirements for their interconnection networks grow exponentially and {{it is expected that}} conventional on-chip wires {{will not be able to}} meet such demands. Alternatively, nanophotonics has been regarded as a strong candidate for chip communication since it could provide high bandwidth with low area and energy footprints. However, issues such as the unavailability of efficient on-chip light sources or the difficulty of implementing all-optical buffering or header processing hinder the development of scalable photonic on-chip networks. In this paper, the area and laser power of several photonic on-chip network proposals is analytically modeled and its scalability is evaluated. Also, a graphene-based hybrid wireless/optical-wired approach is presented, aiming at enabling end-to-end photonic on-chip networks to scale beyond thousands of cores...|$|R
40|$|Abstract: To fully {{tap into}} the {{potential}} of today’s heterogeneous machines, offloading parts of an application on accelerators is not sufficient. The real challenge is to build systems where the application would permanently spread across the entire machine, that is, where parallel tasks would be dynamically scheduled over the full <b>set</b> <b>of</b> available <b>processing</b> <b>units.</b> In this report we present SOCL, an OpenCL implementation that improves and simplifies the programming experience on heterogeneous architectures. SOCL enables applications to dynamically dispatch computation kernels over processing devices so as to maximize their utilization. OpenCL applications can incrementally make use of light extensions to automatically schedule kernels {{in a controlled manner}} on multi-device architectures. A preliminary automatic granularity adaptation extension is also provided. We demonstrate the relevance of our approach by experimenting with several OpenCL applications on a range of representative heterogeneous architectures. We show that performance portability is enhanced by using SOCL extensions. Key-words...|$|R
40|$|We {{consider}} {{the design process}} of VLSI systems dedicated to the real-time implementation of cooperative algorithms whose functionalities can be characterized by multi-layer ensembles of simple elements which interact locally. These algorithms are related, even though not exclusively, {{to the implementation of}} various tasks in low-level machine vision. The starting point in the design process is the formulation of the sequential algorithm that computes the behavior of the system. Algorithmic transformations are performed to expose the parallelism originally present in the task. Given the description in terms of parallel loops, we partition the system and organize it as a <b>set</b> <b>of</b> <b>processing</b> <b>units.</b> The architectural structure of these units takes properly into account the algorithmic constraints on precision both in data representation and computation. The program flow implemented by our programmable architectural solution (ASIP) is an iterative sequence of multiply-and-accumulate operations performed in parallel. The programmability concerns both the structure/coefficients of the algorithm - depending on the specific application - and its computational parameters. The architecture's main blocks are described in VHDL and synthesized as a semi-custom chip, using standard tools. Following this procedure, we designed an ASIP core for performing real-time texture-based image segregation...|$|E
40|$|Applications that query, {{analyze and}} {{manipulate}} very large data sets have become important consumers of resources. With {{the current trend}} toward collectively using heterogeneous collections of disparate machines (the Grid) for a single application, techniques used for tightly coupled, homogeneous machines are not sufficient. Recent research on programming models for developing applications in the Grid has proposed component-based models as a viable approach, in which an application is composed of multiple interacting computational objects. We have been developing a framework, called filter-stream programming, for building data-intensive applications in a distributed environment. In this model, the processing structure of an application is represented as a <b>set</b> <b>of</b> <b>processing</b> <b>units,</b> referred to as filters. In earlier work, we studied the effects of filter placement across heterogeneous host machines {{on the performance of}} the application. In this paper, we develop the problem of scheduling instances of a filter group running on the same set of hosts. A filter group is a set of filters collectively performing a computation for an application. In particular, we seek the answer to the following question: should a new instance be created, or an existing one reused? We experimentally investigate the effects of instantiating multiple filter groups on performance under varying application characteristics. (Cross-referenced as UMIACS-TR- 2001 - 06...|$|E
40|$|S. Joordens and D. Besner (1994) {{described}} {{an attempt to}} simulate a semantic ambiguity advantage in lexical decision using a connectionist model (Masson, 1991) {{that was based on}} a Hopfield (1982) network. The question of the validity of the ambiguity advantage is briefly considered, and the assumptions behind the simulation results reported by Joordens and Besner are critically examined. The model used by Joordens and Besner is compared with other connectionist models, and alternative methods of simulating lexical decisions with this class of models are discussed. It is concluded that further empirical evidence is required and that a number of modeling alternatives need to be explored before strong conclusions can be made about the validity of the semantic ambiguity advantage and about the best way to model the effect. Representing and processing ambiguous words is a challenge for distributed memory models, which are also known as parallel distributed processing or connectionist models. This class of models represents lexical knowledge in weights associated with links that connect a <b>set</b> <b>of</b> <b>processing</b> <b>units</b> to one another and instantiates a known word by evoking its unique pattern of activation across the processing units. The instantiation of a word as a pattern of activation across an entire collection of units contrasts with the classic view of lexical representation in which each word (or word meaning) is represented by a single unit or node in a network (e. g. ...|$|E
40|$|In {{the last}} decade, {{the field of}} {{microprocessor}} architecture has seen the rise of multicore processors, which consist of the interconnection <b>of</b> a <b>set</b> <b>of</b> independent <b>processing</b> <b>units</b> or cores in the same chip. As the number of cores per multiprocessor increases, the bandwidth and energy requirements for their interconnection networks grow exponentially and {{it is expected that}} conventional on-chip wires {{will not be able to}} meet such demands. Alternatively, nanophotonics has been regarded as a strong candidate for chip communication since it could provide high bandwidth with low area and energy footprints. However, issues such as the unavailability of efficient on-chip light sources or the difficulty of implementing all-optical buffering or header processing hinder the development of scalable photonic on-chip networks. In this paper, the area and laser power of several photonic on-chip network proposals is analytically modeled and its scalability is evaluated. Also, a graphene-based hybrid wireless/optical-wired approach is presented, aiming at enabling end-to-end photonic on-chip networks to scale beyond thousands of coresPeer ReviewedPostprint (published version...|$|R
40|$|The paper {{describes}} two complementary and integrable approaches, a probabilistic {{one and a}} deterministic one, {{based on}} classic and advanced modelling techniques for safety analysis of complex computer based systems. The probabilistic approach is based on classical and innovative probabilistic analysis methods. The deterministic approach is based on formal verification methods. Such approaches are applied to the gas turbine control system of ICARO co generative plant, in operation at ENEA CR Casaccia. The main {{difference between the two}} approaches, behind the underlining different theories, is that the probabilistic one addresses the control system by itself, as the <b>set</b> <b>of</b> sensors, <b>processing</b> <b>units</b> and actuators, while the deterministic one also includes the behaviour of the equipment under control which interacts with the control system. The final aim of the research, documented in this paper, is to explore an innovative method which put the probabilistic and deterministic approaches in a strong relation to overcome the drawbacks of their isolated, selective and fragmented use which can lead to inconsistencies in the evaluation results. 1...|$|R
40|$|International audienceTo fully {{tap into}} the {{potential}} of heterogeneous machines composed of multicore processors and multiple accelerators, simple offloading approaches in which the main trunk of the application runs on regular cores while only specific parts are offloaded on accelerators are not sufficient. The real challenge is to build systems where the application would permanently spread across the entire machine, that is, where parallel tasks would be dynamically scheduled over the full <b>set</b> <b>of</b> available <b>processing</b> <b>units.</b> To face this challenge, we previously proposed StarPU, a runtime system capable of scheduling tasks over multicore machines equipped with GPU accelerators. StarPU uses a software virtual shared memory (VSM) that provides a high-level programming interface and automates data transfers between <b>processing</b> <b>units</b> so as to enable a dynamic scheduling of tasks. We now present how we have extended StarPU to minimize the cost <b>of</b> transfers between <b>processing</b> <b>units</b> in order to efficiently cope with multi-GPU hardware configurations. To this end, our runtime system implements data prefetching based on asynchronous data transfers, and uses data transfer cost prediction to influence the decisions taken by the task scheduler. We demonstrate the relevance of our approach by benchmarking two parallel numerical algorithms using our runtime system. We obtain significant speedups and high efficiency over multicore machines equipped with multiple accelerators. We also evaluate the behaviour of these applications over clusters featuring multiple GPUs per node, showing how our runtime system can combine with MPI...|$|R
40|$|International audienceThe main {{objective}} {{of this paper is}} to introduce the concept of Reconfigurable Graphic Coding and its validation under the form of a Functional Units (FU) library. The heterogeneity of data for 3 D graphics objects representation requires the adaptability of the compression schemas to various types of content. While such adaptation can be relatively easy to support in software implementations, the same is much more difficult to implement in hardware. Although compression schemas inherently share the same data processing chain, the components forming it may vary with respect to the number and type of components to encode, data range and correlation type. Based on the analysis of the state of the art on 3 D graphics compression approaches, we propose a <b>set</b> <b>of</b> <b>processing</b> <b>units.</b> We show how this set can be configured/connected into a network, including hardware networks, to obtain reference decoders. Moreover, the network can be reconfigured at runtime, based on information that is provided with the encoded object. This modular concept of functional units, allows optimized management of computation (such as identification of parallelizable functions or functions that are suitable for acceleration) relative to the hardware architecture (CPU, GPU, FPGA, etc.). In the four decoders presented, at least half of the FUs are being reused at least once. The results were performed by generating and compiling C code from RVC-CAL code and comparing the results with the MPEG reference software implementation. The FUs described in this paper were standardized by MPEG as part of the ISO/IEC 23001 - 4...|$|E
40|$|Multicore {{machines}} {{equipped with}} accelerators {{are becoming increasingly}} popular. The TOP 500 -leading RoadRunner machine {{is probably the most}} famous example of a parallel computer mixing IBM Cell Broadband Engines and AMD opteron processors. Other architectures, featuring GPU accelerators, are expected to appear in the near future. To fully tap into the potential of these hybrid machines, pure offloading approaches, in which the main core of the application runs on regular processors and offloads specific parts on accelerators, are not sufficient. The real challenge is to build systems where the application would permanently spread across the entire machine, that is, where parallel tasks would be dynamically scheduled over the full <b>set</b> <b>of</b> available <b>processing</b> <b>units.</b> To face this challenge, we propose a new runtime system capable of scheduling tasks over heterogeneous, accelerator-based machines. Our system features a software virtual shared memory that provides a weak consistency model. The system keeps track of data copies within accelerator embedded-memories and features a data-prefetching engine. Such facilities, together with a database of self-tuned per-task performance models, can be used to greatly improve the quality of scheduling policies in this context. We demonstrate the relevance of our approach by benchmarking various parallel numerical kernel implementations over our runtime system. We obtain significant speedups and a very high efficiency on various typical workloads over multicore machines equipped with multiple accelerators...|$|R
40|$|During {{the past}} few years, several studies have drawn {{attention}} {{to the role of}} distributional characteristics in the segmentation of speech into word units. In the present chapter, we examine the role of distributional constraints in the acquisition <b>of</b> sub-syllabic language <b>processing</b> <b>units.</b> In particular, we suggest that specific <b>processing</b> <b>units</b> such as the rime might emerge from the distributional properties of sequences of vowels and consonants. We then summarize recent work showing that speakers are indeed sensitive to statistical distributions between vocalic and consonantal segments within the syllable. Finally, a simulation study using the Parser model (Perruchet & Vinter, 1998) indicates how probabilistic constraints can influence the acquisition of representational units. We conclude that what can be viewed as <b>processing</b> <b>units</b> probably represents only {{the tip of the iceberg}} of broader knowledge about statistical properties of language and that the acquisition of such representational units is, at least partially, shaped by general learning processes. 2 In most contemporary models, language recognition and use are supposed to hinge on a <b>set</b> <b>of</b> intermediate representational <b>processing</b> <b>units</b> <b>of</b> various sizes. For example, it has been claimed that syllables constitute <b>processing</b> <b>units</b> in auditory word recognition, at least for syllable-timed languages (e. g., Mehler, Dommergues, Frauenfelder, & Segui, 1981...|$|R
40|$|As {{distributed}} algorithms {{may involve}} {{a large amount}} of data describing local state information and complex interactions between elements, it is often very di cult to achieve anunderstanding of their control ow (and performance behaviour) only from a pseudo-code description or from data streams (e. g. execution traces). This paper is on our project which constitutes a systematic e ort to explore the use of visualisation and animation aids to illuminate the key ideas of distributed protocols and algorithms. In particular, it involves: development of a database of distributed protocols and concurrent data objects implementations� availability of an animation program for each entry of the database (i. e. each protocol or concurrent object implementation), which, given the (on- or o-line) trace of any execution, it animates {{it in a way that}} demonstrates the key ideas of the respective protocol or concurrent object implementation� development of a database of network descriptions. The integrated library, Lydian, will be of big help for educational purposes, such as for teaching distributed computing, computer networks, communication protocols, operating systems � students will be able to gain a direct impression of the behaviour of the protocol and teachers will be able to illustrate concepts that can otherwise be explained concisely only in a technical paper. For the protocols and data objects implementation part we use simulation platforms for network and multiprocessor systems. Since distributed protocols are designed to execute on any type of system that can be described as a <b>set</b> <b>of</b> interconnected <b>processing</b> <b>units,</b> the use <b>of</b> these platforms enables the implementation of a wide range of protocols. For the creation of network descriptions and their tra c behaviour a simple and nice graphical editor was used accompanied by a <b>set</b> <b>of</b> drawing algorithms. For the animation part we use a powerful toolkit, which, like the simulation platforms, is appropriate for many di erent architectures. ...|$|R
40|$|A {{document}} {{discusses the}} replication of the functionality of the onboard space-shuttle general-purpose computers (GPCs) in field-programmable gate arrays (FPGAs) and application-specific integrated circuits (ASICs). The {{purpose of the}} replication effort is to enable utilization of proven space-shuttle flight software and software-development facilities {{to the extent possible}} during development of software for flight computers {{for a new generation of}} launch vehicles derived from the space shuttles. The replication involves specifying the instruction <b>set</b> <b>of</b> the central <b>processing</b> <b>unit</b> and the input/output processor (IOP) of the space-shuttle GPC in a hardware description language (HDL). The HDL is synthesized to form a "core" processor in an FPGA or, less preferably, in an ASIC. The core processor can be used to create a flight-control card to be inserted into a new avionics computer. The IOP of the GPC as implemented in the core processor could be designed to support data-bus protocols other than that of a multiplexer interface adapter (MIA) used in the space shuttle. Hence, a computer containing the core processor could be tailored to communicate via the space-shuttle GPC bus and/or one or more other buses...|$|R
40|$|This paper {{deals with}} a {{synthesis}} of Pulsed Para-Neural Networks (PPNN) in a 3 -D Cellular Automata space. In its essence, PPNN is a <b>set</b> <b>of</b> simple <b>processing</b> <b>units</b> that change their states only in certain discrete moments of time denoted t, t+ 1, t+ 2, [...] . The inlets to and outlets from the units are located {{in such a way}} that a given unit may (but does not have to) send a pulse to and only to its nearest neighbors. There are three kinds <b>of</b> <b>processing</b> units: red cells, yellow cells, and blue cells. A red cell emits a pulse at t+ 2 if and only if the value of its counter at t+ 1 gets equal to or greater than 2, where the state of the counter for t+ 1 is the state for t plus the weighted sum of pulses incoming in t. Each weight is associated with one and only one inlet to the cell and may be equal to 1, 0 or – 1. Every red cell zeroes its counter when emitting a pulse. Every yellow cell emits a pulse in t+ 1 if and only if one and only one pulse entered it in t. Every blue cell emits a pulse in t+ 1 if a pulse entered it in t through its only inlet. A red cell works in the way approximating the behavior of a neural soma which cumulates excitation/inhibition in both time and space. A yellow cell represents a formal neuron whose inputs provide a pre-synaptic inhibition to all other inputs. Since a blue cell is a 1 -clock delay, a string of adjacent blue cells may model an axon. Despite the simplicity of the paradigm of computation of PPNN, a number of useful devices has been created in its framework. We present (1) an associative memory to be filed via reinforcement learning (what is remembered is a <b>set</b> <b>of</b> phases of pulses circulating in closed loops made of cells), and (2) a spiking neuron that non-linearly cumulates excitation and responds with a changeable frequency of produced pulses, (3) an adjustable timer an...|$|R
