20|3492|Public
2500|$|Bogart's {{roles were}} not only repetitive, but {{physically}} demanding and draining (studios were not yet air-conditioned), and his regimented, tightly <b>scheduled</b> <b>job</b> at Warners was anything but the indolent and [...] "peachy" [...] actor's life he hoped for. However, he was always professional and generally respected by other actors. He used these [...] "B movie" [...] years to start developing his enduring film persona—the wounded, stoical, cynical, charming, vulnerable, self-mocking loner with a code of honor.|$|E
2500|$|In a third case, travel records {{indicate}} Gacy {{was at a}} <b>scheduled</b> <b>job</b> site in Michigan at 6 a.m. on September 26, 1977—the day following the disappearance of a 19-year-old youth named John Mowery. Mowery was last seen leaving his mother's house at 10 p.m. on September 25. His roommate was an employee of PDM Contractors who had formerly lived with Gacy and had moved into Mowery's apartment less than one week before the youth's disappearance. Two witnesses have stated that this roommate had recommended to Mowery that he meet [...] "a man who is going out of town" [...] two days prior to the youth's disappearance.|$|E
5000|$|Here {{you would}} run {{some sort of}} <b>scheduled</b> <b>job</b> to {{automatically}} run the [...] "Export to BlogML" [...] function and save the output as a compressed backup file somewhere.|$|E
3000|$|... (s,σ), the {{parameter}} σ is build only on {{the extra}} utility of the <b>scheduled</b> <b>jobs,</b> but does not consider {{the utility of the}} tasks of the <b>scheduled</b> <b>jobs.</b> This can be improved by computing C [...]...|$|R
50|$|<b>Job</b> shop <b>scheduling,</b> {{a similar}} problem for <b>scheduling</b> <b>jobs</b> on machines. Some {{variants}} of multiprocessor <b>scheduling</b> and <b>job</b> shop <b>scheduling</b> are equivalent problems.|$|R
50|$|A {{very common}} method in {{embedded}} systems is to manually <b>schedule</b> <b>jobs.</b> This can for example {{be done in}} a time-multiplexed fashion. Sometimes the kernel is divided in three or more parts: Manual scheduling, preemptive and interrupt level. Exact methods for <b>scheduling</b> <b>jobs</b> are often proprietary.|$|R
50|$|This {{method of}} {{executing}} scripts keeps the console window busy until the script has finished its <b>scheduled</b> <b>job.</b> Users cannot {{interact with the}} console window until the script ends or until they interrupt it.|$|E
5000|$|... {{can be made}} to mail a user {{when done}} {{carrying}} out a <b>scheduled</b> <b>job</b> of theirs, can use more than one job queue, and can read a list of jobs to carry out from a file instead of standard input.|$|E
50|$|One of {{the main}} {{features}} of ARexx is the fact it could expand {{the capabilities of the}} AmigaOS by adding some procedures the OS lacked. For example, a simple ARexx program could be written to print a warning message on the screen of the monitor, or play an audio alert signal if a certain Amiga program stops, faults or has finished its <b>scheduled</b> <b>job.</b>|$|E
40|$|The {{problem of}} <b>scheduling</b> <b>jobs</b> with {{precedence}} constraints {{is a central}} problem in Scheduling Theory which arises in many industrial and scientific applications. In this paper we present a polynomial time approximation scheme for the problem of <b>scheduling</b> <b>jobs</b> with chain precedence constraints on a fixed number of uniformly related machines...|$|R
50|$|Cron {{process for}} <b>scheduling</b> <b>jobs</b> {{to run at}} a {{particular}} time.|$|R
5000|$|Cron {{process for}} <b>scheduling</b> <b>jobs</b> {{to run on}} a {{particular}} date.|$|R
5000|$|Bogart's {{roles were}} not only repetitive, but {{physically}} demanding and draining (studios were not yet air-conditioned), and his regimented, tightly <b>scheduled</b> <b>job</b> at Warners was anything but the indolent and [...] "peachy" [...] actor's life he hoped for. However, he was always professional and generally respected by other actors. He used these [...] "B movie" [...] years to start developing his enduring film persona—the wounded, stoical, cynical, charming, vulnerable, self-mocking loner with a code of honor.|$|E
5000|$|In a third case, travel records {{indicate}} Gacy {{was at a}} <b>scheduled</b> <b>job</b> site in Michigan at 6 a.m. on September 26, 1977—the day following the disappearance of a 19-year-old youth named John Mowery. Mowery was last seen leaving his mother's house at 10 p.m. on September 25. His roommate was an employee of PDM Contractors who had formerly lived with Gacy and had moved into Mowery's apartment less than one week before the youth's disappearance. Two witnesses have stated that this roommate had recommended to Mowery that he meet [...] "a man who is going out of town" [...] two days prior to the youth's disappearance.|$|E
5000|$|SymmetricDS is {{open source}} {{software}} for database and file synchronization with Multi-master replication, filtered synchronization, and transformation capabilities. [...] It is designed to scale for {{a large number of}} nodes, work across low-bandwidth connections, and withstand periods of network outage. [...] Data synchronization occurs asynchronously from a <b>scheduled</b> <b>job,</b> with data changes being sent over a push or pull operation. It uses standard web protocols (HTTP) and database technologies (JDBC) in order to support a wide range of platforms and maximize its interoperability. It includes support for Oracle, MySQL, MariaDB, PostgreSQL, Greenplum, SQL Server, SQL Server Azure, HSQLDB, H2, Derby, DB2, Firebird, Informix, Interbase, SQLite, Sybase ASE, Sybase ASA, MongoDB, Amazon_Redshift, and VoltDB databases.|$|E
30|$|We broadly divided task {{assignment}} strategies into <b>scheduling,</b> <b>job</b> board, and recommendation.|$|R
50|$|<b>Scheduled</b> <b>jobs</b> {{are managed}} by a web {{interface}} {{on a separate}} computer.|$|R
40|$|This paper {{describes}} an innovative interactive decision making tool for <b>scheduling</b> <b>jobs</b> in a flexible manufacturing environment. While the proposed tool provides computer integrated mechanism for <b>scheduling</b> <b>jobs</b> to the flexible manufacturing systems, it retains the human touch needed to augment computer analysis with human experience and judgement. A practical instance where this tool is used is also discussed...|$|R
5000|$|Through HTCondor, {{developed}} at the University of Wisconsin, DiaGrid harvests and manages computing cycles from idle or underused high-performance computing cluster nodes, servers, machines in campus computer and other labs, and office computers. Whenever a local user or <b>scheduled</b> <b>job</b> needs a given machine, the HTCondor job is stopped and automatically sent to another HTCondor node as soon as possible. While this [...] "opportunistic" [...] model limits {{the ability to do}} parallel processing and communications, a HTCondor pool can provide smaller, serial jobs vast numbers of cycles in a very short amount of time. HTCondor—and by extension, DiaGrid—is designed for high-throughput computing and is excellent for parameter sweeps, Monte Carlo simulation, or nearly any serial application. Some classes of parallel jobs (master-worker) may be run effectively via HTCondor as well.|$|E
50|$|Two {{years after}} Upcher had {{purchased}} the property work finally {{began on the}} new hall on the 2 July 1812. By this time Humphry Repton had begun {{to recover from the}} accident but he increasingly relied on his son for the day-to-day running of the contract. One of the first <b>scheduled</b> <b>job</b> was to construct a new track (Now known as the Back Drive) down to the coast road which would be necessary to transport all the building materials to the estate. The Gault bricks for the face brickwork of the hall where from Lincolnshire and were brought to Norfolk by sea. Other materials arrived by train to nearby Weybourne and Sheringham stations and hauled to site along the new road. Other building materials used on the new hall were reclamations from local sources. These included Oak retrieved from a wrecked ship at Blakeney and other timber from a local demolished granary. The Repton’s employed a clerk of works to oversee the job on the recommendation of Abbot Upcher. He had been the master of a local workhouse and although Upcher admired the man's diligence and enthusiastic attitude, this did not make up for his inexperience in the building trade. Upcher allowed the clerk to have the wooden arch centres which had been used to form the cellar, removed prematurely. This was done {{at a time when there}} had been torrential rain in the area for several days. Unsurprisingly the removal of the arch Centring's caused the collapse of the cellar ceiling throwing progress back quite considerably. The construction of the house ran simultaneously with the landscaping of the estate. This work also included a terrace garden on the north south of the hall which sat in the lee of the hill.|$|E
30|$|Consider {{the problem}} of {{scheduling}} identical jobs on a single machine, in which the jobs are processed in batches, with a setup time for each batch. The completion time of a job coincides with the completion time of the last <b>scheduled</b> <b>job</b> in its batch and all jobs in this batch have the same completion time. For a given number of jobs, we want to choose batch sizes so as to minimize {{the sum of the}} completion times of the jobs. There is a trade-off between keeping the number of setups incurred small, by having large batches, and keeping small the time each job waits for its batch to finish, by having small batches.|$|E
40|$|Abstract — Auction {{mechanisms}} {{have been}} proposed {{as a means to}} efficiently and fairly <b>schedule</b> <b>jobs</b> in high-performance computing environments. The Generalized Vickrey Auction has long been known to produce efficient allocations while exposing users to truth-revealing incentives, but the algorithms used to compute its payments can be computationally intractable. In this paper we present a novel implementation of the Generalized Vickrey Auction that uses dynamic programming to <b>schedule</b> <b>jobs</b> and compute payments in pseudo-polynomial time. Additionally, we have built a version of the PBS scheduler that uses this algorithm to <b>schedule</b> <b>jobs,</b> and in this paper we present the results of our tests using this scheduler. I...|$|R
50|$|Master Server: Controls all backup {{management}} tasks, catalog, <b>scheduling,</b> <b>job</b> execution, {{and distributed}} processing.|$|R
5000|$|Open-shop <b>scheduling,</b> <b>Job</b> Shop <b>Scheduling,</b> Flow Shop Scheduling Problem, {{optimization}} {{problems in}} computer science.|$|R
40|$|Processor allocation, {{the spatial}} {{assignment}} {{of a set}} of processors to each <b>scheduled</b> <b>job,</b> is an important issue in the drive to fully realize the performance potential of massively parallel supercomputers. With the advent of non-contiguous processor allocation strategies, the fragmentation problem has been solved and system utilization has improved. However, message-passing contention is the current performance bottleneck in fragmentation-free processor allocation. In this paper, we present MC, a new processor allocation strategy for mesh-connected parallel computers that (1) is fragmentation-free, (2) yields compact allocations and thus (3) is very successful at minimizing message-passing contention. Moreover, MC is inherently parallelizable, employing just the idle nodes and needing only limited communication. We test MC's performance using a message-passing simulator and workload traces from the San Diego Supercomputing Center. The results show that MC outperforms all other publishe [...] ...|$|E
40|$|Abstract. We {{consider}} online preemptive scheduling of {{jobs with}} fixed starting times revealed {{at those times}} on m uniformly related machines, {{with the goal of}} maximizing the total weight of completed jobs. Every job has a size and a weight associated with it. A newly released job must be either assigned to start running immediately on a machine or otherwise it is dropped. It is also possible to drop an already <b>scheduled</b> <b>job,</b> but only completed jobs contribute their weights to the profit of the algorithm. In the most general setting, no algorithm has bounded competitive ratio, and we consider a number of standard variants. We give a full classification of the variants into cases which admit constant competitive ratio (weighted and unweighted unit jobs, and C-benevolent instances, which is a wide class of instances containing proportional-weight jobs), and cases which admit only a linear competitive ratio (unweighted jobs and D-benevolent instances). In particular, we give a lower bound of m o...|$|E
40|$|Scheduled {{production}} system leads to avoiding stock accumulations, losses reduction, decreasing or even eliminating idol machines, {{and effort to}} better benefitting from machines for on time responding customer orders and supplying requested materials in suitable time. In flexible job-shop scheduling {{production system}}s, we could reduce time and costs by transferring and delivering operations on existing machines, that is, among NP-hard problems. The scheduling objective minimizes the maximal completion time of all the operations, which is denoted by Makespan. Different methods and algorithms have been presented for solving this problem. Having a reasonable scheduled production system has significant influence on improving effectiveness and attaining to organization goals. In this paper, new algorithm were proposed for flexible job-shop scheduling problem systems (FJSSP-GSPN) {{that is based on}} gravitational search algorithm (GSA). In the proposed method, the flexible job-shop scheduling problem systems was modeled by color Petri net and CPN tool and then a <b>scheduled</b> <b>job</b> was programmed by GSA algorithm. The experimental results showed that the proposed method has reasonable performance in comparison with other algorithms...|$|E
5000|$|... webcron is {{the term}} for a {{time-based}} job scheduler hosted on a web server. The name derives its roots from the phrase web server and the Unix daemon cron. A webcron solution enables users to <b>schedule</b> <b>jobs</b> to run within the web server environment on a web host that does not offer a shell account or other means of <b>scheduling</b> <b>jobs.</b>|$|R
50|$|With vWorkApp, a {{dispatcher}} {{can create}} and <b>schedule</b> <b>jobs</b> for mobile workers via an account on the app's website. After a dispatcher <b>schedules</b> a <b>job,</b> mobile workers receive real-time push technology notifications with job details on their smartphones.|$|R
40|$|Abstract. The {{problem of}} <b>scheduling</b> <b>jobs</b> with {{precedence}} constraints {{is a central}} problem in Scheduling Theory which arises in many industrial and scientific applications. In this paper we present a polynomial time approximation scheme for the problem of <b>scheduling</b> <b>jobs</b> with chain precedence constraints on a fixed number of uniformly related machines. Our algorithm works even if we allow “slow ” machines to remain idle...|$|R
40|$|This paper {{deals with}} the {{assignment}} of stations to operators into flexible manufacturing systems by minimizing the operators’ walking times over a given planning horizon. The manufacturing system is supposed to present a linear layout where the stations are aligned along a longitudinal axis. Stations are flexible (so that different jobs can be scheduled on the same station along the planning horizon) and may differ {{from each other in}} terms of degree of automation and, consequently, in terms of amount of human labor involved. Thus, two or more stations might share the same operator within the working shift and, consequently, the assignment of the optimal subset of stations to each operator is anything but trivial. Hence, a mixed-integer linear programming model is proposed which takes into account (and minimizes) the travel walking distances of the operators according to the assigned subset of stations. Other realistic constraints, such as the actual availability of the operators and the requirement stating that each <b>scheduled</b> <b>job</b> has to be assigned to a single operator for its entire duration, are included in the model. The validity of the model is proved by discussing a real-world case study from the plastic industry...|$|E
40|$|International audienceWe {{address the}} flow shop {{scheduling}} problem with two machines and time delays. We dispose {{of a set}} J={ 1, 2, [...] .,n} of n jobs that must be executed without preemption on two machines M 1 and M 2, which can only handle one operation at a time. Each job j has two operations. The first operation must be executed during p(1,j) time units on M 1. Then, lj time units are needed to transport job j to M 2, where the second operation has to be executed during p(2,j) time units. The objective {{is to find a}} feasible schedule on the two machines that minimizes the completion time of the last <b>scheduled</b> <b>job</b> on M 2. This problem is NP-hard in the strong sense even with unit-time operations. We present a comprehensive theoretical analysis of the different lower bounds of the literature and we propose new relaxation schemes. Then, we elucidate dominance relationships between them. Moreover, we investigate a linear programming-based lower bound that includes the implementation of a new dominance rule and a valid inequality. Finally, we present the result of an extensive computational study that was carried out on a set of 480 instances including new hard instances. Our new relaxation schemes outperform {{the state of the art}} lower bounds...|$|E
40|$|We {{consider}} online preemptive scheduling of {{jobs with}} fixed starting times revealed {{at those times}} on m uniformly related machines, {{with the goal of}} maximizing the total weight of completed jobs. Every job has a size and a weight associated with it. A newly released job must be either assigned to start running immediately on a machine or otherwise it is dropped. It is also possible to drop an already <b>scheduled</b> <b>job,</b> but only completed jobs contribute their weights to the profit of the algorithm. In the most general setting, no algorithm has bounded competitive ratio, and we consider a number of standard variants. We give a full classification of the variants into cases which admit constant competitive ratio (weighted and unweighted unit jobs, and C-benevolent instances, which is a wide class of instances containing proportional-weight jobs), and cases which admit only a linear competitive ratio (unweighted jobs and D-benevolent instances). In particular, we give a lower bound of m on the competitive ratio for scheduling unit weight jobs with varying sizes, which is tight. For unit size and weight we show that a natural greedy algorithm is 4 / 3 -competitive and optimal on m = 2 machines, while for large m, its competitive ratio is between 1. 56 and 2. Furthermore, no algorithm is better than 1. 5 -competitive. ...|$|E
5000|$|The DI Job Server executes, {{monitors}} and <b>schedules</b> <b>jobs</b> {{that have been}} created by using the Designer.|$|R
5000|$|G. Mosheiov. (1994). <b>Scheduling</b> <b>jobs</b> under simple linear deterioration. Computers and Operations Research, Vol. 21, No. 6, 653-659.|$|R
40|$|The {{problem of}} <b>scheduling</b> <b>jobs</b> with {{precedence}} constraints is studied. A polynomial time approximation scheme is presented {{for the case}} when the machines are identical and the precedence graph partitions the jobs into groups of fixed size. As an interesting particular case, this algorithm is a polynomial time approximation scheme for <b>scheduling</b> <b>jobs</b> with chain or tree precedence constraints on identical machines, when each chain or tree is of constant size. ...|$|R
