12|15|Public
40|$|The article {{shows that}} Rosa Luxemburg’s {{analysis}} of capitalist accumulation is framed within a ‘circuitist’ macroeconomic reading of capitalism as a monetary production economy. Authors examine Luxemburg’s {{reading of the}} enlarged reproduction scheme introduced in volume II of Marx’s Capital and summarize critiques of Luxemburg’s approach by Nikolai Bukharin, Michał Kalecki and Joan Robinson. Authors argue that Rosa Luxemburg cannot be reduced {{to the status of}} an ‘under-consumptionist’ but she presents a clear picture of the macro-monetary and <b>sequential</b> <b>working</b> of the capitalist process. </p...|$|E
40|$|This paper {{presents}} a study for a production scheduling {{problem in a}} textile company, specifically in the weaving preparation area. Basically, the processing orders {{can be considered as}} <b>sequential</b> <b>working</b> steps trough three operations (charging - weaving - discharging), and the goal is to minimize time variation and to avoid delays. The machine utilization should be as higher as possible due to short delivering deadlines. The production unit has got 4 of these weaving machines functioning at the same time. Four dispatching rules were tested in order to find the best solution. The optimization procedure highlighted some interesting issues that are discussed in this paper...|$|E
40|$|The {{present study}} {{examined}} the impact of engaging frontal-mediated working memory processes on implicit and explicit category learning. Two stimulus dimensions were relevant to categorization, but in some conditions, a third, irrelevant dimension was also presented. Results indicated that in both implicit and explicit conditions, {{the inclusion of the}} irrelevant dimension impaired performance by increasing the reliance on suboptimal unidimensional strategies. With three-dimensional stimuli, a striking dissociation was observed between implicit and explicit category learning when participants performed a <b>sequential</b> <b>working</b> memory task. With explicit category learning, performance was impaired further, and there was an increased use of suboptimal unidimensional strategies. However, with implicit category learning, the performance impairment decreased, and there was an increased use of optimal strategies. These findings demonstrate the paradoxical situation in which learning can be improved under sequential-task conditions and have important implications for training, decision making, and understanding interactive memory systems...|$|E
5000|$|... #Caption: An {{example of}} a <b>sequential</b> {{algorithm}} not <b>working</b> correctly concurrently: two nodes, i and i+1, being removed simultaneously result in node i+1 not being removed ...|$|R
50|$|In {{addition}} to work immediately {{relevant to the}} war effort, mathematicians involved with the panel also pursued problems of interest to them without contracts from outside organizations. Most notably, Abraham Wald developed the statistical technique of <b>sequential</b> analysis while <b>working</b> for AMP.|$|R
40|$|A {{model of}} {{prospective}} time-estimation is introduced which explains {{the interplay of}} working memory demands on duration estimation. The approach is integrated into a cognitive architecture and tested by estimating the duration of a task that varied coordinative and <b>sequential</b> demands on <b>working</b> memory. The comparison with experimental data shows that the model is able to simulate the influence of these demands on human time-estimation. Keywords: Time-estimation; Computational cognitive modeling; Cognitive architectures; Coordinative working memory...|$|R
40|$|Typical {{real time}} {{computer}} vision tasks require {{huge amount of}} processing power and time for handling real time computer vision applications. The nature of processing in a typical computer vision algorithm usually ranges from many small arithmetic operations (Fine Grain Parallelism) to symbolic operations (Coarse grain parallelism). The task become more complicate while considering image processing application due to large data sets and there processing. The existing processing system responds efficiently under <b>sequential</b> <b>working</b> and result in efficient output, but results in a slow operating system which results in a inefficient processing system under high speed image processing systems. Parallel processing founds {{to be the only}} solution to obtain the require processing speed for handling high-speed image processing applications. The existing image processing systems support usually only one suit of operations at once and fail to respond under multiple tasks. System taking single instruction or multiple instruction process operates using low level and high-level operations. Generally SIMD architecture is suitable under low level processing while MIMD architecture is suitable for high-level processing. This paper explores on modeling and simulation of parallel Imag...|$|E
40|$|Recent {{studies have}} {{indicated}} a further need to investigate the role of motivation in workingmemory (WM) training and that time perception affects motivation. We addressed whethersubjectively perceived time on task in reference to objective time on task could serve as animplicit measure of motivation, while controlling for individual differences in timeperception. Here, the relationship between different measures of time perception, WM, andmotivation was explored in healthy children. Fifty children in three natural groups (ages: 6 - 7, 8 - 9, 10 - 11) at a Swedish school participated. WM scores changed with age as expected. However, the absence of correlations between WM performance and intrinsic motivationwere inconsistent with previous findings, presumably due to the low statistical sensitivity. Nevertheless, time perception accuracy (r= 0. 318, p= 0. 043) and state motivation (r= 0. 434,p= 0. 005) correlated with performance on task interference, but not WM. With somereservations due to low sensitivity, time perception accuracy appears to be linked tocoordinative capacity required for shifting attention, but {{to a lesser degree}} <b>sequential</b> <b>working</b> memory capacity...|$|E
40|$|Auditory {{signals of}} speech are speaker-dependent, but {{representations}} of language meaning are speaker-independent. Such a transformation enables speech {{to be understood}} from different speakers. A neural model is presented that performs speaker normalization to generate a pitchindependent representation of speech sounds, while also preserving information about speaker identity. This speaker-invariant representation is categorized into unitized speech items, which input to <b>sequential</b> <b>working</b> memories whose distributed patterns can be categorized, or chunked, into syllable and word representations. The proposed model fits into an emerging model of auditory streaming and speech categorization. The auditory streaming and speaker normalization parts of the model both use multiple strip representations and asymmetric competitive circuits, thereby suggesting that these two circuits arose from similar neural designs. The normalized speech items are rapidly categorized and stably remembered by Adaptive Resonance Theory circuits. Simulations use synthesized steady-state vowels from the Peterson and Barney [J. Acoust. Soc. Am. 24, 175 - 184 (1952) ] vowel database and achieve accuracy rates similar to those achieved by human listeners. These results are compared to behavioral data and other speaker normalization models. National Science Foundation (SBE- 0354378); Office of Naval Research (N 00014 - 01 - 1 - 0624...|$|E
40|$|This paper {{presents}} students's {{fulfillment of}} an assignment that explored {{the concepts of}} digital architecture using rapid prototyping (RP) process. A point cloud was given to students, and different representational data were substantiated as real 3 D physical models. The presence of RP models and the <b>sequential</b> illustration of <b>working</b> steps in their reports revealed that the control of shapes often differed from what students perceived in VR worlds. The results thus confirm that physical models are useful for visualization {{as well as in}} design pedagogy...|$|R
40|$|International audienceBulk {{synchronous}} parallelism (BSP) {{offers an}} abstract and simple model of parallelism yet allows to take realistically {{into account the}} communication costs of parallel algorithms. BSP {{has been used in}} many application domains. BSPlib and its variants are programming libraries for the C language that support the BSP style. Bulk Synchronous Parallel ML (BSML) is a library for BSP programming with the functional language OCaml. It is based on an extension of the lambda-calculus by parallel operations on a data structure named parallel vector. BSML offers a global view of programs, i. e. BSML programs can be seen as <b>sequential</b> programs <b>working</b> on a parallel data structure (seq of par) while a BSPlib program is written in the SPMD style and understood as a parallel composition of communicating sequential programs (par of seq). The communication styles of BSML and BSPlib are also quite different...|$|R
40|$|Substantial {{evidence}} suggests that test anxiety is associated with poor performance in complex tasks. Based on the differentiation of coordinative and <b>sequential</b> demands on <b>working</b> memory (Mayr & Kliegl, 1993), two studies {{examined the effects of}} sequential demands on the relationship between test anxiety and cognitive performance. Both studies found that high sequential demands had beneficial effects on the speed and accuracy of the performance of test-anxious participants. It is suggested that the more frequent memory updates associated with high sequential demands may represent external processing aids that compensate for the restricted memory capacity of individuals with high test anxiet...|$|R
40|$|The aim of {{the present}} {{research}} {{was to investigate the}} effect of cross-cultural and age-related factors on self-referent metacognitive efficiency, psychological well-being, and mnestic performance in late adulthood. Ninety-three healthy adults recruited in individualistic northwest Italian and collectivistic Sardinian contexts were respectively assigned to the Old (i. e., 65 - 74 years) and Very Old (i. e., ≥ 75 years) groups and were individually administered a battery of well-being and metacognitive measures and working memory tasks. A series of MANOVAs was carried out on well-being and metacognitive measures and working memory tasks. Sardinians showed greater levels of perceived well-being, less marked psychological distress, and more preserved mnestic functions than the controls from the northwest Italian context. Moreover, participants from the Old group self-referred more coping strategies, emotional competencies, and personal satisfaction, and less depressive symptoms. Then, a hierarchical linear regressions where different socio-demographic, working memory metacognitive and social desirability measures were used as predictors of general psychological well-being shows that socio cultural context, social desirability, visuo-spatial <b>sequential</b> <b>working</b> memory and metamemory measures predict perceived well-being. Socio-cultural contexts emphasizing the positive social role of the elderly seem to promote psychological well-being, that is, life quality in late adulthoo...|$|E
40|$|Background: Negative {{pressure}} wound therapy (NPWT) is {{an established}} modality {{in the treatment}} of chronic wounds, open fractures, and post-operative wound problems. This method has not been widely used due to the high cost of equipment and consumables. This study demonstrates an indigenously developed apparatus which gives comparable results {{at a fraction of the}} cost. Readily available materials are used for the air-tight dressing. Materials and Methods: Equipment consists of suction apparatus with adjustable pressure valve set to a pressure 125 - 150 mmHg. An electronic timer switch with a <b>sequential</b> <b>working</b> time of 5 min and a standby time of 3 min provides the required intermittent negative pressure. Readily available materials such as polyvinyl alcohol sponge, suction drains and steridrapes were used to provide an air tight wound cover. Results: A total of 90 cases underwent 262 NPWT applications from 2009 to 2014. This series, comprised of 30 open fractures, 21 post-operative and 39 chronic wounds. The wound healing rate in our study was comparable to other published studies using NPWT. Conclusion: The addition of electronic timer switch will convert a suction apparatus into NPWT machine, and the results are equally effective compared to more expensive counter parts. The use of indigenous dressing materials reduces the cost significantly...|$|E
40|$|In {{the last}} few decades several {{concepts}} of Dynamical Systems Theory (DST) have guided psychologists, cognitive scientists, and neuroscientists to rethink about sensory motor behavior and embodied cognition. A critical step in the progress of DST application to the brain (supported by modern methods of brain imaging and multi-electrode recording techniques) has been the transfer of its initial success in motor behavior to mental function, i. e., perception, emotion, and cognition. Open questions from research in genetics, ecology, brain sciences, etc. have changed DST itself and lead to the discovery of a new dynamical phenomenon, i. e., reproducible and robust transients that are at the same time sensitive to informational signals. The goal of this review is to describe a new mathematical framework -heteroclinic sequential dynamics- to understand self-organized activity in the brain that can explain certain aspects of robust itinerant behavior. Specifically, we discuss a hierarchy of coarse-grain models of mental dynamics in the form of kinetic equations of modes. These modes compete for resources at three levels: (i) within the same modality, (ii) among different modalities from the same family (like perception), and (iii) among modalities from different families (like emotion and cognition). The analysis of the conditions for robustness, i. e., the structural stability of transient (sequential) dynamics, give us the possibility to explain phenomena like the finite capacity of our <b>sequential</b> <b>working</b> memory -a vital cognitive function-, and to find specific dynamical signatures -different kinds of instabilities- of several brain functions and mental diseases...|$|E
40|$|This article {{introduces}} multiprocessor scheduling algorithms {{based upon}} cellular automata. To design cellular automata corresponding {{to a given}} program graph a generic definition of program graph neighborhood is used, transparent to the various kinds, sizes and shapes of program graphs. The cellular automata-based scheduler works in two modes. In learning mode a genetic algorithm (GA) is applied to discover rules of cellular automata (CAs) suitable for solving instances of a scheduling problem. In operation mode discovered rules of cellular automata are able to find automatically an optimal or suboptimal solution of the scheduling problem for any initial allocation of a program graph in two-processor system graph. Discovered rules are typically suitable for <b>sequential</b> cellular Automata <b>working</b> as a scheduler. Experimental results concerning scheduling algorithms discovered {{in the context of}} cellular automata- based scheduling system are presented...|$|R
40|$|Scientists, {{engineers}} and other domain-experts have computational {{problems that are}} growing in size and complexity, thereby, increasing the demand for High Performance Computing (HPC). The demand for reduced time-to-solution is also increasing and simulations on high performance computers are being preferred over physical prototype development. Though HPC is gradually becoming indispensible for business growth, the programming challenges associated with HPC application development are a key bottleneck to embracing it on a massive scale. Current high-level approaches for generating HPC applications are either domain-dependent or do not leverage from existing applications. Message Passing Interface (MPI) {{is the most popular}} standard for writing parallel applications for distributed memory HPC platforms. The development of parallel applications using MPI often begins with <b>working</b> <b>sequential</b> applications that undergo major rewrites to incorporate appropriate calls to MPI routines. Writing efficient parallel applications using MPI is a complex task due to the extra burden on programmers...|$|R
40|$|Subcortical loops {{through the}} basal ganglia and the {{cerebellum}} form computationally powerful distributed processing modules (DPMs). This paper relates the computational {{features of a}} DPM's loop through the basal ganglia to experimental results for two kinds of natural action selection. First, functional imaging during a serial order recall task was used to study human brain activity during the selection of <b>sequential</b> actions from <b>working</b> memory. Second, microelectrode recordings from monkeys trained in a step-tracking task were used to study the natural selection of corrective submovements. Our DPM-based model assisted {{in the interpretation of}} puzzling data from both of these experiments. We come to posit that the many loops through the basal ganglia each regulate the embodiment of pattern formation in a given area of cerebral cortex. This operation serves to instantiate different kinds of action (or thought) mediated by different areas of cerebral cortex. We then use our findings to formulate a model of the aetiology of schizophrenia...|$|R
40|$|This {{paper is}} showing {{results from a}} joint {{research}} project "MICROFLOW", funded by the German Department for Education and Research (BMBF). Usual simultaneous reflow soldering processes like convection soldering or vapor phase soldering were optimized {{in the past for}} a minimum of temperature difference between small and large or heavy components on electronic assemblies. Especially the increasing demands for polymer electronics, for electrical-optical assemblies or high temperature electronics require are further development of soldering processes. Such a process should allow a direct heating of the solder joints up to soldering temperature and have to save all other components at the same time. Today this is possible only by application of <b>sequential</b> <b>working</b> selective soldering processes like hot bar soldering or laser soldering. But for a cost effective industrial application it is necessary to realize a selective and as well simultaneous soldering process, which is indeed not available at present. The wanted selective heating method for an effective simultaneous process is possible in principle by using of electromagnetic fields, when the energy is penetrating the assembly and is launching heat in certain regions, depending on specific material characteristics. Electromagnetic fields in the microwave frequency region are able to treat various sizes and shapes of assemblies with large capacity and with a high throughput. Indeed it is possible to heat conventional solder pastes only with very slowly and with a high microwave power density. It is possible to increase the launched heat considerable by mixing of additional materials, so called suszeptors, into the solder paste, which are absorbing microwave power with a high efficiency. A fundamental task for a save microwave application was the to guarantee the operators safety and to ensure the electromagnetic compatibility of printed circuit boards, integrated circuits and other components as well. This was possible b means of minimizing of volumetric microwave power, optimizing microwave frequency and above all the ensuring of field homogeneity...|$|E
40|$|Background: The {{increasing}} demand for acute care and restructuring of hospitals resulting in emergency department (ED) closures and fewer inpatient beds {{are reasons to}} improve ED efficiency. The approach towards the patient care process varies among doctors. The objective {{of this study was}} to determine variations in the patient care process and patient flow among emergency physicians (EP’s) and internists at the ED of Leiden University Medical Centre (LUMC), the Netherlands. Methods: An observational instrument was developed during a pilot study at the LUMC ED, following observations of activities performed by EP’s and internists. The instrument divides all different types of activities a clinician can perform on the ED into eight categories. Using the observational instrument, their activities were observed and registered for 10 separate days. Primary outcomes were defined as the time spend on the eight separate activity categories, the total length of stay (LOS) and the number of patients seen during an interval. Secondary outcomes were general observations of working routine features that determine patient flow at the ED. The obtained data were analyzed into SPSS. Results: Ten doctors were observed during a total of ± 36 hours in which 42 patients were seen. Although EP’s were observed for a shorter period of time than internists (13 : 48 vs. 22 : 10 hrs, - 38 %), they saw more patients (26 vs. 16, + 62 %). EP’s tended to spend a higher proportion of their time on patient contact than internists (27. 2 % vs. 17. 3 %, p = 0. 06). Both groups dedicated the highest proportion of their time to documentation (31. 5 % and 33. 4 %, p = 0. 75) and had little communication with ED nurses (3. 7 % and 2. 4 % p = 0. 57). The average LOS of internal patients was higher than that of EP’s patients (5. 25 ± sd 1 : 33 and 2. 26 ± sd 1 : 32 hours). Internists occupied more treatment rooms at the same time (2. 41 vs. 2. 08, p < 0. 00) and followed a more <b>sequential</b> <b>working</b> routine. Conclusions: This paper describes the determination of variations in the ED care process and patient flow among EP’s and internists by an observational instrument. A pilot study with the instrument showed variations in the patient care process and patient flow among the two groups at the LUMC ED...|$|E
40|$|A special {{development}} of solder pastes was demanded for the {{development of}} an innovative reflow soldering process by means of microwave heating. The presented results are showing a part of the joint research project ?MICROFLOW?, funded by the German Department for Education and Research (BMBF). Usual simultaneous reflow soldering processes like convection soldering or vapor phase soldering were optimized in the past for a minimum of temperature difference between small and large or heavy components on electronic assemblies. Especially the increasing demands for polymer electronics, for electrical-optical assemblies or high temperature electronics require are further development of soldering processes. Such a process should allow a direct heating of the solder joints up to soldering temperature and have to save all other components at the same time. Today this is possible only by application of <b>sequential</b> <b>working</b> selective soldering processes like hot bar soldering or laser soldering. But for a cost effective industrial application it is necessary to realize a selective and as well simultaneous soldering process, which is indeed not available at present. The wanted selective heating method for an effective simultaneous process is possible in principle by using of electromagnetic fields, when the energy is penetrating the assembly and is launching heat in certain regions, depending on specific material characteristics. Electromagnetic fields in the microwave frequency region are able to treat various sizes and shapes of assemblies with large capacity and with a high throughput. Indeed it is possible to heat conventional solder pastes only with very slowly and with a high microwave power density. It is possible to increase the launched heat considerable by mixing of additional materials, so called suszeptors, into the solder paste, which are absorbing microwave power with a high efficiency. Such suszeptors must have suitable polar or dielectric properties and can be added to the solder paste as a powder, solvent or fluid. In this way it is possible to accelerate the heating of the solder paste significant in comparison with the remaining assembly, whereby the demanded microwave energy can be minimized. Beside of this fundamental work, to generate a microwave suited heat source in the solder paste, the usual works of paste or flux development and qualification are also very important. First of all the wetting quality, especially for lead-free solders and no less the behaviors of flux residues, have to be investigated. Therefore the solder melting and wetting was investigated, including solder balling tests on different substrate materials. In the next step various corrosion tests like humidity storage and surface insulation measurements were carried out. The test results were leading to a goal-directed selection and optimization of flux systems for a new microwave suited solder paste...|$|E
40|$|Binary search trees (BSTs) with {{rotations}} {{can adapt}} to {{various kinds of}} structure in search sequences, achieving amortized access times substantially better than the Theta(log n) worst-case guarantee. Classical examples of structural properties include static optimality, <b>sequential</b> access, <b>working</b> set, key-independent optimality, and dynamic finger, {{all of which are}} now known to be achieved by the two famous online BST algorithms (Splay and Greedy). ( [...] .) In this paper, we introduce novel properties that explain the efficiency of sequences not captured by any of the previously known properties, and which provide new barriers to the dynamic optimality conjecture. We also establish connections between various properties, old and new. For instance, we show the following. (i) A tight bound of O(n log d) on the cost of Greedy for d-decomposable sequences. The result builds on the recent lazy finger result of Iacono and Langerman (SODA 2016). On the other hand, we show that lazy finger alone cannot explain the efficiency of pattern avoiding sequences even in some of the simplest cases. (ii) A hierarchy of bounds using multiple lazy fingers, addressing a recent question of Iacono and Langerman. (iii) The optimality of the Move-to-root heuristic in the key-independent setting introduced by Iacono (Algorithmica 2005). (iv) A new tool that allows combining any finite number of sound structural properties. As an application, we show an upper bound on the cost of a class of sequences that all known properties fail to capture. (v) The equivalence between two families of BST properties. The observation on which this connection is based was known before - we make it explicit, and apply it to classical BST properties. ( [...] . ...|$|R
40|$|Today’s {{processors}} {{exploit the}} fine grain data parallelism {{that exists in}} many applications via ILP design, vector processing, and SIMD instructions. Thus, future gains must come from chipmultiprocessors, which present developers with previously unimaginable computing resources. Programmers can use these resources for coarse-grain data-parallel computation or task parallelism. Given the extensive research history in coarse-grain data parallelism, we argue that the right approach is to invest research effort on task parallelism because it is currently poorly supported in programming languages, operating systems and performance analysis tools. Such an approach encourages refactoring <b>working</b> <b>sequential</b> applications into taskparallel, and in particular pipeline-parallel, applications. Thus, we join the minority chorus that believes the best strategy for developing parallel programs may be to evolve them from sequential implementations. There are challenges; future multi-core systems {{are likely to be}} heterogeneous and consist of many types of cores. Programmers need support in understanding and exploiting such systems. We believe that the systems community needs to focus on building complete toolchains that encompass all four stages of parallel program development for task parallelism: identification, implementation, verification, and runtime system support. This paper discusses this vision and our efforts in developing such a toolchain. ...|$|R
40|$|This draft aims {{to analyze}} {{the role of a}} {{governmental}} think tank on the policy-process on institutional reforms resulting in the Lisbon Treaty. On 1 st December 2009, the Lisbon Treaty entered into force. This latter has substantial impact on the European governance, largely due to central institutional changes. As outlined in the title, this draft analyzes the European governance through the Lisbon Treaty. Governance is generally defined as a co-production mode of decision-making among different types of actors. This governance will be analyzed through a specific actor willing to contribute to policy-making : the think tank, or research institute; defined as an organization generating policy-oriented research in an effort to enable policymakers to make informed decisions about public policy issues. Based on a theoretical framework on the influence of think tanks in Germany developed by Thunert, we hypothesize that strategies developed by the BEPA have a stronger visibility at the issue articulation stage; and a medium visibility at the policy formulation and policy implementation stages. This theoretical framework has been developed at a meso level. The purpose of this draft is to test it at a macro level: the EU level. The analysis of the governmental think tank will also enrich our understanding {{of the relationship between the}} BEPA and the European Commission. Do we face an instrumentalization by the European Commission of the BEPA? We might hypothesize that the European Commission uses the competencies developed by the BEPA in order to meet its own interests. Think tank could have an impact on three different stages (issue articulation, policy formulation and policy implementation). The model developed by Thunert could be transposed to the political process that led to the Lisbon Treaty; from the Laeken Declaration in 2001 to the signature of the Lisbon Treaty in 2007. On basis of a <b>sequential</b> analysis, <b>working</b> groups – were selected: the Amato group, the Convention on the Future of Europe; the Round table on “A sustainable project for tomorrow’s Europe”, also known as Strauss-Kahn report; and the Amato group...|$|R
40|$|Ion mass {{spectrometry}} {{was used to}} investigate discharges formed during high power impulse magnetron sputtering (HiPIMS) and direct current magnetron sputtering (DCMS) of a graphite target in Ar and Ar/N- 2 ambient. Ion energy distribution functions (IEDFs) were recorded in time-averaged and time-resolved mode for Ar+, C+, N- 2 (+), N+, and CxNy+ ions. An increase of N- 2 in the sputter gas (keeping the deposition pressure, pulse width, pulse frequency, and pulse energy constant) results for the HiPIMS discharge in {{a significant increase in}} C+, N+, and CN+ ion energies. Ar+, N- 2 (+), and C 2 N+ ion energies, in turn, did not considerably vary with the changes in working gas composition. The HiPIMS process showed higher ion energies and fluxes, particularly for C+ ions, compared to DCMS. The time evolution of the plasma species was analyzed for HiPIMS and revealed the <b>sequential</b> arrival of <b>working</b> gas ions, ions ejected from the target, and later during the pulse-on time molecular ions, in particular CN+ and C 2 N+. The formation of fullerene-like structured CNx thin films for both modes of magnetron sputtering is explained by ion mass-spectrometry results and demonstrated by transmission electron microscopy as well as diffraction. Funding Agencies|Hungarian Academy of Sciences||</p...|$|R
40|$|Psychiatric {{disorders}} are often caused by partial heterogeneous disinhibition in cognitive networks, controlling <b>sequential</b> and spatial <b>working</b> memory (SWM). Such dynamic connectivity changes {{suggest that the}} normal relationship between the neuronal components within the network deteriorates. As a result, competitive network dynamics is qualitatively altered. This dynamics defines the robust recall of the sequential information from memory and, thus, the SWM capacity. To understand pathological and non-pathological bifurcations of the sequential memory dynamics, here we investigate the model of recurrent inhibitory-excitatory brain networks with heterogeneous inhibition. We consider the ensemble of units with all-to-all inhibitory connections, in which the connection strengths are monotonically distributed at some interval. Based on computer experiments and studying the Lyapunov exponents, we observed and analyzed the new phenomenon - clustered sequential dynamics. The results are interpreted {{in the context of}} the winnerless competition principle. Accordingly, clustered sequential dynamics is represented in the phase space of the model by two weakly interacting quasi-attractors. One of them is similar to the sequential heteroclinic chain - the regular image of SWM, while the other is a quasi-chaotic attractor. Coexistence of these quasi-attractors means that the recall of the normal information sequence is intermittently interrupted by episodes with chaotic dynamics. We indicate potential dynamic ways for augmenting damaged working memory and other cognitive functions...|$|R
40|$|The {{development}} of geostatistics has been mostly acccomplished by application-oriented engineers {{in the past}} twenty years. The focus on concrete applications {{gave birth to a}} great many algorithms and computer programs designed to address very dierent issues, such as estimating or simulating a variable while possibly accounting for secondary information like seismic data, or integrat-ing geological and geometrical data. At the core of any geostatistical data integration methodology is a well-designed algorithm. Yet, despite their obvious dierences, all these algorithms share a lot of commonalities one should capitalize on when building a geostatistics pro-gramming library, lest the resulting library is poorly reusable and diÆcult to expand. Building on this observation, we design a comprehensive, yet exible and easily reusable library of geostatistics algorithms in C 1 The recent advent of the generic programming paradigm allows us to el-egantly express the commonalities of the geostatistical algorithms into com-puter code. Generic programming, also refered to as "programming with con-cepts", provides a high level of abstraction without loss of eÆciency. This last point is a major gain over object-oriented programming which often trades ef-ciency for abstraction. It is not enough for a numerical library to be reusable, it also has to be fast. Because generic programming is "programming with concepts", the es-sential step in the library design is the careful identication and thorough de nition of these concepts shared by most of the geostatistical algorithms. Building on these denitions, a generic and expandable code can be provided. To show the advantages of such a generic library, we use the G s TL to build two <b>sequential</b> simulation programs <b>working</b> on two very dierent types of grids: a surface with faults and an unstructured grid; without requiring any change to the G s TL code...|$|R

