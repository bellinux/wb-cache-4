1174|261|Public
25|$|Consider now a {{function}} of the unknown parameter: an estimator is a statistic used to estimate such function. Commonly used estimators include sample mean, unbiased sample variance and <b>sample</b> <b>covariance.</b>|$|E
25|$|The use of {{the term}} n−1 is called Bessel's correction, and it is also used in <b>sample</b> <b>covariance</b> and the sample {{standard}} deviation (the square root of variance). The square root is a concave function and thus introduces negative bias (by Jensen's inequality), which depends on the distribution, and thus the corrected sample standard deviation (using Bessel's correction) is biased. The unbiased estimation of standard deviation is a technically involved problem, though for the normal distribution using the term n−1.5 yields an almost unbiased estimator.|$|E
25|$|The eigendecomposition of a {{symmetric}} positive semidefinite (PSD) matrix yields an orthogonal {{basis of}} eigenvectors, {{each of which}} has a nonnegative eigenvalue. The orthogonal decomposition of a PSD matrix is used in multivariate analysis, where the <b>sample</b> <b>covariance</b> matrices are PSD. This orthogonal decomposition is called principal components analysis (PCA) in statistics. PCA studies linear relations among variables. PCA is performed on the covariance matrix or the correlation matrix (in which each variable is scaled to have its sample variance equal to one). For the covariance or correlation matrix, the eigenvectors correspond to principal components and the eigenvalues to the variance explained by the principal components. Principal component analysis of the correlation matrix provides an orthonormal eigen-basis for the space of the observed data: In this basis, the largest eigenvalues correspond to the principal components that are associated with most of the covariability among a number of observed data.|$|E
40|$|AbstractCovariances play a {{fundamental}} {{role in the}} theory of stationary processes and they can naturally be estimated by <b>sample</b> <b>covariances.</b> There is a well-developed asymptotic theory for <b>sample</b> <b>covariances</b> of linear processes. For nonlinear processes, however, many important problems on their asymptotic behaviors are still unanswered. The paper presents a systematic asymptotic theory for <b>sample</b> <b>covariances</b> of nonlinear time series. Our results are applied to the test of correlations...|$|R
40|$|Covariances play a {{fundamental}} {{role in the}} theory of stationary processes and they can naturally be estimated by <b>sample</b> <b>covariances.</b> There is a well-developed asymptotic theory for <b>sample</b> <b>covariances</b> of linear processes. For nonlinear processes, however, many important problems on their asymptotic behaviors are still unanswered. The paper presents a systematic asymptotic theory for <b>sample</b> <b>covariances</b> of nonlinear time series. Our results are applied to the test of correlations. Asymptotic normality Covariance Dependence Linear process Martingale Moderate deviation Nonlinear time series Stationary process Test of correlation...|$|R
5000|$|To {{justify the}} Potts model, {{it is often}} noted {{that it can be}} derived {{following}} a maximum entropy principle: For a given set of <b>sample</b> <b>covariances</b> and frequencies, the Potts model represents the distribution with the maximal Shannon entropy of all distributions reproducing those covariances and frequencies. For a multiple sequence alignment, the <b>sample</b> <b>covariances</b> are defined as ...|$|R
25|$|The Ensemble Kalman Filter (EnKF) is a Monte Carlo {{implementation}} of the Bayesian update problem: given a probability density function (pdf) {{of the state of}} the modeled system (the prior, called often the forecast in geosciences) and the data likelihood, the Bayes theorem is used to obtain the pdf after the data likelihood has been taken into account (the posterior, often called the analysis). This is called a Bayesian update. The Bayesian update is combined with advancing the model in time, incorporating new data from time to time. The original Kalman Filter assumes that all pdfs are Gaussian (the Gaussian assumption) and provides algebraic formulas for the change of the mean and the covariance matrix by the Bayesian update, as well as a formula for advancing the covariance matrix in time provided the system is linear. However, maintaining the covariance matrix is not feasible computationally for high-dimensional systems. For this reason, EnKFs were developed. EnKFs represent the distribution of the system state using a collection of state vectors, called an ensemble, and replace the covariance matrix by the <b>sample</b> <b>covariance</b> computed from the ensemble. The ensemble is operated with as if it were a random sample, but the ensemble members are really not independent – the EnKF ties them together. One advantage of EnKFs is that advancing the pdf in time is achieved by simply advancing each member of the ensemble. For a survey of EnKF and related data assimilation techniques, see G. Evensen.|$|E
2500|$|... where [...] is the (<b>sample)</b> <b>covariance</b> of the {{periodic}} returns {{on the two}} assets, or alternatively denoted as , [...] or [...]|$|E
2500|$|... where n is {{the sample}} size, [...] is the vector of column means and [...] is a [...] <b>sample</b> <b>covariance</b> matrix.|$|E
40|$|We {{consider}} {{the estimation of}} the parameters of a stationary random field on d-dimensional lattice by minimizing the classical Whittle approximation to the Gaussian log likelihood. If the usual biased <b>sample</b> <b>covariances</b> are used, the estimate is efficient only in one dimension. To remove this edge effect, we introduce data tapers and show that the resulting modified estimate is efficient also in two and three dimensions. This avoids {{the use of the}} unbiased <b>sample</b> <b>covariances</b> which are in general not positive-definit...|$|R
3000|$|Second, the {{ensemble}} can only transport limited information {{and provide a}} <b>sampling</b> <b>covariance</b> P̅_k|k, (7 b) or (8 b), of at most rank N− 1. Consequently, identically zero entries of P [...]...|$|R
40|$|Long Memory Stochastic {{volatility}} (LMSV) models capture two standardized {{features of}} financial data: the log-returns are uncorrelated, but their squares, or absolute values are (highly) dependent {{and they may}} have heavy tails. EGARCH and related models were introduced to model leverage, i. e. negative dependence between previous returns and future volatility. Limit theorems for partial sums, sample variance and <b>sample</b> <b>covariances</b> are basic tools to investigate the presence of long memory and heavy tails and their consequences. In this paper we extend the existing literature on the asymptotic behaviour of the partial sums and the <b>sample</b> <b>covariances</b> of long memory stochastic volatility models in the case of infinite variance. We also consider models with leverage, for which our results are entirely new in the infinite variance case. Depending on the nterplay between the tail behaviour and the intensity of dependence, wo types of convergence rates and limiting distributions can arise. In articular, we show that the asymptotic behaviour of partial sums is the same for both LMSV and models with leverage, whereas there is a crucial difference when <b>sample</b> <b>covariances</b> are considered...|$|R
2500|$|The EnKF is now {{obtained}} {{simply by}} replacing the state covariance [...] in Kalman gain matrix [...] by the <b>sample</b> <b>covariance</b> [...] computed from the ensemble members (called the ensemble covariance), that is: ...|$|E
2500|$|Note {{that since}} [...] is a {{covariance}} matrix, {{it is always}} positive semidefinite and usually positive definite, so the inverse above exists and the formula can be implemented by the [...] Cholesky decomposition. In, [...] is replaced by the <b>sample</b> <b>covariance</b> [...] where and the inverse {{is replaced by a}} pseudoinverse, computed using the Singular Value Decomposition (SVD) [...]|$|E
2500|$|The {{ensemble}} Kalman filter (EnKF) is a recursive filter {{suitable for}} {{problems with a}} large number of variables, such as discretizations of partial differential equations in geophysical models. The EnKF originated as a version of the Kalman filter for large problems (essentially, the covariance matrix is replaced by the <b>sample</b> <b>covariance),</b> and it is now an important data assimilation component of ensemble forecasting. EnKF is related to the particle filter (in this context, a particle is the same thing as ensemble member) but the EnKF makes the assumption that all probability distributions involved are Gaussian; [...] when it is applicable, it is much more efficient than the particle filter.|$|E
40|$|Advances in Applied Probability, 44 (4), 1113 - 1141, 2012 International audienceLong Memory Stochastic {{volatility}} (LMSV) models capture two standardized {{features of}} financial data: the log-returns are uncorrelated, but their squares, or absolute values are (highly) dependent {{and they may}} have heavy tails. EGARCH and related models were introduced to model leverage, i. e. negative dependence between previous returns and future volatility. Limit theorems for partial sums, sample variance and <b>sample</b> <b>covariances</b> are basic tools to investigate the presence of long memory and heavy tails and their consequences. In this paper we extend the existing literature on the asymptotic behaviour of the partial sums and the <b>sample</b> <b>covariances</b> of long memory stochastic volatility models in the case of infinite variance. We also consider models with leverage, for which our results are entirely new in the infinite variance case. Depending on the nterplay between the tail behaviour and the intensity of dependence, wo types of convergence rates and limiting distributions can arise. In articular, we show that the asymptotic behaviour of partial sums is the same for both LMSV and models with leverage, whereas there is a crucial difference when <b>sample</b> <b>covariances</b> are considered...|$|R
40|$|This paper {{proposes a}} new nonparametric {{approach}} to the estimation of the mean Doppler velocity (first spectral moment) and the spectral width (square root of the second spectral centered moment) of a zero-mean stationary complex Gaussian process immersed in independent additive white Gaussian noise. By assuming that the power spectral density of the underlying process is band li mi ted, the exact maxity li 923 :y od estimates of its spectral moments are derived. An estimate based on the <b>sample</b> <b>covariances</b> is also studied. Both methods are robust {{in the sense that}} they do not rely on any assumption concerning the power spectral density (besides being bandlimited). Under weak conditions, the estimates based on <b>sample</b> <b>covariances</b> are best asmptoti all normal...|$|R
40|$|This paper proposes an {{annealed}} {{particle swarm optimization}} based {{particle filter}} algorithm for articulated 3 D human body tracking. In our algorithm, a <b>sampling</b> <b>covariance</b> and an annealing factor are incorporated into the velocity updating equation of particle swarm optimization (PSO). The <b>sampling</b> <b>covariance</b> and the annealing factor are initiated with appropriate values {{at the beginning of}} the PSO iteration, and `annealing' is carried out at reasonable steps. Experiments with multi-camera walking sequences from the Brown dataset show that: 1) the proposed tracker can effectively alleviate the problem of inconsistency between the image likelihood and the true model; 2) the tracker is also robust to noise and body self-occlusion. Xiaoqin Zhang, Weiming Hu, Xiangyang Wang, Yu Kong, Nianhua Xie, Hanzi Wang, Haibin Ling, Steve Mayban...|$|R
5000|$|The {{sample mean}} and the <b>sample</b> <b>covariance</b> matrix are {{unbiased}} {{estimates of the}} mean and the covariance matrix of the random vector , a row vector whose jth element (j = 1, ..., K) {{is one of the}} random variables. The <b>sample</b> <b>covariance</b> matrix has [...] in the denominator rather than [...] due to a variant of Bessel's correction: In short, the <b>sample</b> <b>covariance</b> relies on the difference between each observation and the sample mean, but the sample mean is slightly correlated with each observation since it is defined in terms of all observations. If the population mean [...] is known, the analogous unbiased estimate ...|$|E
5000|$|The <b>sample</b> <b>covariance</b> matrix is a K-by-K matrix [...] with entries ...|$|E
50|$|The {{sample mean}} or {{empirical}} mean and the <b>sample</b> <b>covariance</b> are statistics computed from a collection (the sample) {{of data on}} one or more random variables.The sample mean and <b>sample</b> <b>covariance</b> are estimators of the population mean and population covariance, where the term population refers to the set from which the sample was taken.|$|E
40|$|Data {{collected}} on a rectangular lattice occur frequently {{in many areas}} such as field trials, geostatistics, remotely sensed data, and image analysis. Models for the spatial process often make simplifying assumptions, including axial symmetry and separability. We consider methods for testing these assumptions and compare tests based on <b>sample</b> <b>covariances,</b> tests based on the sample spectrum, and model-based tests...|$|R
40|$|Covariance {{matrices}} {{are important}} {{in many areas of}} neural modelling. In Hopfield networks they are used to form the weight matrix which controls the autoassociative properties of the network. In Gaussian processes, which {{have been shown to be}} the infinite neuron limit of many regularised feedforward neural networks, covariance matrices control the form of Bayesian prior distribution over function space. This thesis examines interesting modifications to the standard covariance matrix methods to increase functionality or efficiency of these neural techniques. Firstly the problem of adapting Gaussian process priors to perform regression on switching regimes is tackled. This involves the use of block covariance matrices and Gibbs sampling methods. Then the use of Toeplitz methods is proposed for Gaussian process regression where sampling positions can be chosen. A comparison is made between Hopfield weight matrices, and <b>sample</b> <b>covariances.</b> This allows work on <b>sample</b> <b>covariances</b> to be used [...] ...|$|R
40|$|We obtain uniform {{consistency}} {{results for}} kernel-weighted <b>sample</b> <b>covariances</b> in a nonstationary multiple regression framework {{that allows for}} both fixed design and random design coefficient variation. In the fixed design case these nonparametric <b>sample</b> <b>covariances</b> have different uniform asymptotic rates depending on direction, a result that differs fundamentally from the random design and stationary cases. The uniform asymptotic rates derived exceed the corresponding rates in the stationary case and confirm the existence of uniform super-consistency. The modelling framework and convergence rates allow for endogeneity and thus broaden the practical econometric import of these results. As a specific application, we establish uniform consistency of nonparametric kernel estimators of the coefficient functions in nonlinear cointegration models with time varying coefficients or functional coefficients, and provide sharp convergence rates. For the fixed design models, in particular, there are two uniform convergence rates that apply in two different directions, both rates exceeding the usual rate in the stationary case...|$|R
5000|$|Where [...] is the <b>sample</b> <b>covariance,</b> and [...] is the penalizing parameter.|$|E
50|$|In statistics, {{sometimes}} the covariance matrix of a multivariate random variable {{is not known}} but has to be estimated. Estimation of covariance matrices then deals {{with the question of}} how to approximate the actual covariance matrix on the basis of a sample from the multivariate distribution. Simple cases, where observations are complete, can be dealt with by using the <b>sample</b> <b>covariance</b> matrix. The <b>sample</b> <b>covariance</b> matrix (SCM) is an unbiased and efficient estimator of the covariance matrix if the space of covariance matrices is viewed as an extrinsic convex cone in Rp×p; however, measured using the intrinsic geometry of positive-definite matrices, the SCM is a biased and inefficient estimator. In addition, if the random variable has normal distribution, the <b>sample</b> <b>covariance</b> matrix has Wishart distribution and a slightly differently scaled version of it is the maximum likelihood estimate. Cases involving missing data require deeper considerations. Another issue is the robustness to outliers, to which <b>sample</b> <b>covariance</b> matrices are highly sensitive.|$|E
5000|$|... {{which is}} simply the <b>sample</b> <b>covariance</b> matrix. This is a biased {{estimator}} whose expectation is ...|$|E
40|$|This paper {{proposes a}} new nonparametric method for {{estimation}} of spectral moments of a zero-mean Gaussian process immersed in additive white Gaussian noise. Although {{the technique is}} valid for any order moment, particular attention {{is given to the}} mean Doppler (first moment) and to the spectral width (square root of the second spectral centered moment). By assuming that the power spectral density of the underlying process is bandlimited,themaximum likelihood estimates of its spectral moments are derived. A suboptimal estimate based on the <b>sample</b> <b>covariances</b> is also studied. Both methods are robust {{in the sense that they}} do not rely on any assumption concerning the power spectral density (besides being bandlimited). Under weak conditions, the set of estimates based on <b>sample</b> <b>covariances</b> are unbiased and strongly consistent. Compared with the classical pulse pair and the periodogram based estimates, the proposed methods exhibit better statistical properties for asymmetric spectra and/or spectra with large spectral widths, while involving a computational burden of the same order...|$|R
40|$|We {{consider}} {{recognition by}} Gaussian models, noise sensitivity from using <b>sample</b> <b>covariances</b> to estimate population covariances, and correction by combining Euclidean and Mahalanobis distances. 1 Introduction The Zernike moments uniquely describe functions {{on the unit}} disk, and can be extended to images [2] [3]. Their invariance properties make them attractive as descriptors for Optical Character Recognition. To investigate this belief, {{and the use of}} Gaussian models, the author implemented a straightforward system to recognize printed digits at a single orientation, size, resolution and typeface [6]. This system was very successful on clean data, but was expected to do poorly when the orientation of the digits was noisy [5]. Noting that the system fitted a Gaussian model to each digit, as described in [4, Section 4], it was suggested 1 that there may be oversensitivity to noise due to the use of <b>sample</b> <b>covariances</b> to estimate population covariances. We explore this hypothesis throug [...] ...|$|R
30|$|In Figure  1 c, we {{approximated}} color ellipsoids to each cluster using {{principal component}} analysis, where the sample mean and <b>sample</b> <b>covariances</b> were used. In Figure  1 b,c, the upper cluster {{is from the}} road and the lower cluster is from the tree trunk. Approximating the RGB clusters with an ellipsoidal shape does well in characterizing the three-dimensional density of the cluster of points.|$|R
5000|$|... and ψ(·) is the digamma function. The {{intrinsic}} bias of the <b>sample</b> <b>covariance</b> matrix equals ...|$|E
50|$|XTX {{itself can}} be {{recognised}} as proportional to the empirical <b>sample</b> <b>covariance</b> matrix of the dataset X.|$|E
5000|$|The <b>sample</b> <b>covariance</b> of N {{observations}} of K variables is the K-by-K matrix [...] with the entries ...|$|E
40|$|We {{investigate}} asymptotic {{properties of}} partial sums and <b>sample</b> <b>covariances</b> for lin-ear processes whose innovations are dependent. Central limit theorems and invariance principles are established under fairly mild conditions. Our results go beyond earlier ones by allowing a quite wide class of innovations which includes many important non-linear time series models. Applications to linear processes with GARCH innovations and other non-linear time series models are discussed. ...|$|R
40|$|Linkage disequilibrium among loci is an {{important}} parameter in explaining genotypic means and variances in animal and plant breeding populations. Joint haplotype frequencies and their <b>sampling</b> <b>covariance</b> matrix {{for any number of}} linked loci were derived for backcross populations derived from inbred lines. The predicted frequencies can be used to test whether the linkage disequilibrium observed between (marker) loci in backcross populations is as expected...|$|R
40|$|Covariances play a {{fundamental}} {{role in the}} theory of time series and they are critical quantities that are needed in both spectral and time domain analysis. Estimation of covariance matrices is needed in the construction of confidence regions for unknown parameters, hypothesis testing, principal component analysis, prediction, discriminant analysis among others. In this paper we consider both low- and high-dimensional covariance matrix estimation problems and present a review for asymptotic properties of <b>sample</b> <b>covariances</b> and covariance matrix estimates. In particular, we shall provide an asymptotic theory for estimates of high dimensional covariance matrices in time series, and a consistency result for covariance matrix estimates for estimated parameters. ...|$|R
