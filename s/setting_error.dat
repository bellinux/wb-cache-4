25|1450|Public
2500|$|At the onset, the NTSB {{determined}} that the captain had flown an unstable approach, descending steeply and {{too close to the}} runway. As a result, the captain would not have had sufficient time to correct problems as they arose. Furthermore, the plane's flaps were found to be in the up position, which is improper procedure. The NTSB could only speculate that either the pilots forgot to set the flaps or that they incorrectly retracted the flaps while attempting to go-around. The flap <b>setting</b> <b>error</b> could have contributed to a stall.|$|E
50|$|Altimeter setting error: The crew did {{not have}} the correct {{pressure}} set. Altimeter <b>setting</b> <b>error</b> is a problem mainly reported in the London TMA, 80% of the errors occur when the aircraft is in the climb, is above the transition altitude/level and the standard pressure setting isn't set.|$|E
5000|$|At the onset, the NTSB {{determined}} that the captain had flown an unstable approach, descending steeply and {{too close to the}} runway. As a result, the captain would not have had sufficient time to correct problems as they arose. Furthermore, the plane's flaps were found to be in the up position, which is improper procedure. The NTSB could only speculate that either the pilots forgot to set the flaps or that they incorrectly retracted the flaps while attempting to go-around. The flap <b>setting</b> <b>error</b> could have contributed to a stall. [...] Ultimately, the cause of the crash was not blamed on pilot error, although that was a contributing factor, but rather on maintenance issues at Executive Airlines. Pilots on prior flights with this aircraft had reported difficulties with the engine thrust, but few proper repairs were done. It seemed likely that one of the engines slipped to idle causing asymmetrical thrust and a loss of speed. The NTSB concluded that Executive Airlines' scheduled maintenance and inspections of the airplane were not performed in conformance with its approved maintenance program and that the manner in which required inspections of maintenance tasks were recorded and the subsequent approval of the airplane for return to service were not conducted in accordance with the proper maintenance practices.|$|E
5000|$|... : This pseudo-variable expands to {{the last}} <b>set</b> <b>error</b> level, a value between [...] "" [...] and [...] "" [...] (without leading zeros). External {{commands}} and some internal commands <b>set</b> <b>error</b> levels upon execution. See also the identically named pseudo-variable [...] under DR-DOS and the [...] command.|$|R
3000|$|... where e 0 is rate {{of error}} {{obtained}} from bootstrap sets {{not having the}} instance being predicted (test <b>set</b> <b>error)</b> and e [...]...|$|R
40|$|Abstract—The Chinese {{remainder}} theorem (CRT) {{has been}} recently generalized from determining a single integer from its remainders to determining multiple integers from their sets (residue sets) of remainders. In this letter, {{we consider the}} generalized CRT when the residue <b>sets</b> have <b>errors.</b> We first obtain a sufficient condition {{on the number of}} erroneous residue sets so that multiple integers still can be uniquely determined from their residue sets. We then propose a determination algorithm of multiple integers from their residue <b>sets</b> with <b>errors.</b> Finally, we apply the newly proposed algorithm to multiple frequency determination from multiple sensors with low sampling rates and show the effectiveness of the proposed algorithm with considering residue <b>set</b> <b>errors</b> over the one without considering residue <b>set</b> <b>errors.</b> Index Terms—Chinese remainder theorem (CRT), multiple frequency determination, remainder errors, sensor networks, undersampling. I...|$|R
40|$|In {{the process}} of form grinding, gear <b>setting</b> <b>error</b> was the main factor that {{influenced}} the form grinding accuracy; we proposed an effective method to improve form grinding accuracy that corrected the error by controlling the machine operations. Based on establishing the geometry model of form grinding and representing the gear setting errors as homogeneous coordinate, tooth mathematic model was obtained and simplified under the gear <b>setting</b> <b>error.</b> Then, according to the gear standard of ISO 1328 - 1 : 1997 and the ANSI/AGMA 2015 - 1 -A 01 : 2002, the relationship was investigated by changing the gear setting errors with respect to tooth profile deviation, helix deviation, and cumulative pitch deviation, respectively, under the condition of gear eccentricity error, gear inclination error, and gear resultant error. An error compensation method was proposed based on solving sensitivity coefficient matrix of <b>setting</b> <b>error</b> in a five-axis CNC form grinding machine; simulation and experimental results demonstrated that the method can effectively correct the gear <b>setting</b> <b>error,</b> as well as further improving the forming grinding accuracy...|$|E
3000|$|As {{the full}} search {{algorithm}} searches the lattice of N 2 LCC setting points and picks the best one, the final <b>setting</b> <b>error</b> {{is not only}} constraint on the CNR at the detector {{but also on the}} quantisation error due to the finite number of setting points. For high CNR values, the correct point will be picked with high probability, and the <b>setting</b> <b>error</b> is dominated by the quantisation error which is proportional to [...]...|$|E
40|$|Abstract. In a {{multiple}} probes scanning {{system for a}} high precision straightness measurement, the lateral <b>setting</b> <b>error</b> between probes is ignored generally due to the average effect {{in the range of}} the aperture of the probes. In this paper, we study a two displacement sensors scanning system. Firstly, the principle of two-point method which is based on the natural extension method is introduced. Then, the influence of the lateral <b>setting</b> <b>error</b> between two probes is analyzed quantificationally when the discrete Fourier transform algorithm is applied. It is shown that a relative evaluation error within the range of 1 % can be obtained if the lateral <b>setting</b> <b>error</b> is smaller than 1. 5 % of the sampling interval as long as the spatial wavelength of the evaluated profile is longer than the probe interval. The effectiveness {{of the results of the}} quantitative analysis is confirmed by computer simulations...|$|E
5000|$|GETKEY (DR DOS 6.0 {{and higher}} only) : Waits for key (or timeout) and <b>sets</b> <b>error</b> level to its ASCII code (or 1024 on timeout).|$|R
40|$|We {{present an}} {{analysis}} of how the generalization performance (expected test <b>set</b> <b>error)</b> relates to the expected training <b>set</b> <b>error</b> for nonlinear learning systems, such as multilayer perceptrons and radial basis functions. The principal result is the following relationship (computed to second order) between the expected test set and training set errors: hE test () i 0 hE train () i + 2 oe 2 eff p eff () n : (1) Here, n {{is the size of}} the training sample, oe 2 eff is the effective noise variance in the response variable(s), is a regularization or weight decay parameter, and p eff () is the effective number of parameters in the nonlinear model. The expectations h i of training set and test <b>set</b> <b>errors</b> are taken over possible training sets and training and test sets 0 respectively. The effective number of parameters p eff () usually differs from the true number of model parameters p for nonlinear or regularized models; this theoretical conclusion is supported by M [...] ...|$|R
40|$|Based on the {{observation}} that the unpredictable nature of conversational speech makes {{it almost impossible to}} reliably model sequential word constraints, the notion of word <b>set</b> <b>error</b> criteria is proposed for improved recognition of spontaneous dialogues. The single pass Adaptive Boosting (AB) algorithm enables the language model weights to be tuned using the word <b>set</b> <b>error</b> criteria. In the two pass version of the algorithm, the basic idea is to predict a set of words based on some a priori information, and perform a re-scoring pass wherein the probabilities of the words in the predicted word set are amplified or boosted in some manner. An adaptive gradient descent procedure for tuning the word boosting factor has been formulated which enables the boost factors to be incrementally adjusted to maximize accuracy of the speech recognition system outputs on held-out training data using the word <b>set</b> <b>error</b> criteria. Two novel models which predict the required word sets have been presented: u [...] ...|$|R
40|$|The {{stochastic}} {{simulation of}} an econometric model is {{an application of}} Monte Carlo methods. Deterministic simulation is performed <b>setting</b> <b>error</b> terms to zero. Stochastic simulation, on the contrary, {{takes into account the}} disturbance terms, solving the model after adding a vector of pseudo-random numbers drawn from a prespecified multivariate distribution. ...|$|E
40|$|The {{accuracy}} of positioning surfaces {{to be processed}} on automatic lines with pallet-fixtures essentially depends on the <b>setting</b> <b>error</b> of the pallet-fixtures with work-pieces in ready-to-work position. The applied methods for calculating the <b>setting</b> <b>error</b> do not give a complete picture of the possible coordinates of the point when in the pallet is displaced in different directions. The aim of the work was to determine an {{accuracy of}} the setting work-pieces on automatic lines with pallets-fixtures, using a computational and analytical method, to improve a manufacturing precision of parts. The currently used methods to calculate the <b>setting</b> <b>error</b> do not give a complete picture of the possible coordinates of {{the point of the}} pallet displacement in different directions. The paper offers a method of equivalent mechanism to determine all the variety of options for displacements in the horizontal plane with a diverse combination of angular and plane-parallel displacements. Using a four-bar linkage, as an equivalent mechanism, allows us to define a zone of the possible positions of any point of the work-piece pallet platform, as the zone bounded by the coupler curve. In case the gaps in the nodes of the two fixtures are equal the zone of possible positions of the point in the parallel displacement of the platform is determined by the circumference and at an angular displacement by the ellipse. The obtained analytical dependences allow us to determine the error at the stage of design with the certain gaps in the fixture nodes. The above method of calculation makes it possible to define a zone of the appropriate placement of the work-piece on its platform for the specified parameters of the pallet to meet conditions for ensuring the coordinate accuracy of the processed axes of holes. </p...|$|E
40|$|A new {{algorithm}} of {{parameter identification}} of equivalent circuit for electrical charge replacement is suggested. The approach {{is based on}} the solution of integral equation of the I type with respect to the function of indicial admittance, by which then determination of replacement circuit parameters is carried out. Application of smoothing splines and original regulating algorithm including kernel <b>setting</b> <b>error</b> of integration equation permits to obtain a stable algorithm of parameter identification. The investigation of algorithm shows high calculating efficiency and sufficient accuracy of parameter identification...|$|E
40|$|Recent {{work has}} shown that {{combining}} multiple versions of unstable classifiers such as trees or neural nets results in reduced test <b>set</b> <b>error.</b> One of the more effective is bagging (Breiman [1996 a]) Here, modified training sets are formed by resampling from the original training set, classifiers constructed using these training sets and then combined by voting. Freund and Schapire [1995, 1996] propose an algorithm the basis {{of which is to}} adaptively resample and combine (hence the acronym [...] arcing) so that the weights in the resampling are increased for those cases most often misclassified and the combining is done by weighted voting. Arcing is more successful than bagging in test <b>set</b> <b>error</b> reduction. We explore two arcing algorithms, compare them {{to each other and to}} bagging, and try to understand how arcing works. We introduce the definitions of bias and variance for a classifier as components of the test <b>set</b> <b>error.</b> Unstable classifiers can have low bias on a large range of data sets. Their problem is high variance. Combining multiple versions either through bagging or arcing reduces variance significantly * Partially supported by NSF Grant 1 - 444063 - 21445 1...|$|R
40|$|We {{present an}} {{analysis}} of how the generalization performance (expected test <b>set</b> <b>error)</b> relates to the expected training <b>set</b> <b>error</b> for nonlinear learning systems, such asmultilayer perceptrons and radial basis functions. The principal result is the following relationship (computed to second order) between the expected test set and training set errors: hEtest () i 0 hEtrain () i + 2 2 peff () eff n: (1) 2 Here, n {{is the size of}} the training sample, eff is the e ective noise variance in the response variable(s), is a regularization or weight decay parameter, and peff () istheeective number of parameters in the nonlinear model. The expectations hiof training set and test <b>set</b> <b>errors</b> are 0 taken over possible training sets and training and test sets respectively. The e ective number of parameters peff () usually di ers from the true number of model parameters p for nonlinear or regularized models� this theoretical conclusion is supported by Monte Carlo experiments. In addition to the surprising result that peff () 6 = p, we propose an estimate of (1) called the generalized prediction error (GP E) which generalizes well established estimates of prediction risk such asAkaike's FPE and AIC, Mallows CP, and Barron's PSE to the nonlinear setting. 1 1 GP E and pef f () were previously introduced in Moody (1991). ...|$|R
40|$|New local "hybrid" functionals {{proposed}} by V. V. Karasiev in [J. Chem. Phys. 118, 8567 (2003) ] are benchmarked against nonlocal hybrid functionals. Their performance is tested {{on the total}} and high occupied orbital energies, {{as well as the}} electric moments of selected diatomic molecules. The new functionals, along with the Hartree-Fock and non-hybrid functionals, are employed for finite-difference calculations, which are basis-independent. Basis <b>set</b> <b>errors</b> in the total energy and electric moments are calculated for the 6 - 311 G, 6 - 311 G++G(3 df, 3 pd) and AUG-cc-pVnZ (n= 3, 4, 6) basis sets used in conjunction with the Hartree-Fock and conventional density functional methods. A comparison between the results of the finite-difference local "hybrid" and basis set nonlocal hybrid functional shows that total energies of local and nonlocal hybrid functionals agree to within the basis <b>set</b> <b>error.</b> Discrepancies for multipole moments are larger in magnitude when compared to the basis <b>set</b> <b>errors,</b> but still reasonably small (smaller than errors produced by the 6 - 311 G basis set). Thus, we recommend using the new local "hybrid" functionals whenever the accuracy is expected to be sufficient, because they require a solution of just differential Kohn-Sham equations, instead of integro-differential ones in the case of hybrid functionals...|$|R
40|$|An error {{polynomial}} is defined, {{the coefficients}} of which indicate the difference at any instant between a {{system and a}} model of lower order approximating the system. It is shown how Markov parameters and time series proportionals of the model can be matched {{with those of the}} system by <b>setting</b> <b>error</b> polynomial coefficients to zero. Also discussed {{is the way in which}} the error between system and model can be considered as being a filtered form of an error input function specified by means of model parameter selection...|$|E
40|$|Recently, the {{analysis}} of quasi-Monte Carlo (QMC) sampling of integrands with singularities gained considerable interest. In this <b>setting</b> <b>error</b> bounds for QMC integration, in addition to discrepancy, include a measure how well the singularities are avoided by the utilized sequences. The article aims to generalize results for the corner avoidance of the classical Halton sequence to Halton sequences that start in an arbritrary point of the unit cube. In particular, it is shown that almost all (in Lebesgue sense) random-start Halton sequences exhibit the same corner avoidance property as the original Halton sequence...|$|E
40|$|Single point {{inclined}} axis grinding techniques, {{including the}} wheel setting, wheel-workpiece interference, error source determination and compensation approaches, were studied to fabricate small aspheric moulds of high profile accuracy. The interference of a cylindrical grinding wheel with the workpiece was analysed and {{the criteria for}} selection of wheel geometry for avoiding the interference was proposed. The grinding process was performed with compensation focused on two major error sources, wheel <b>setting</b> <b>error</b> and wheel wear. The grinding {{results showed that the}} compensation approach was efficient and the developed grinding process was capable to generate small aspheric concave surfaces on tungsten carbide material with a profile error of smaller than 200. nm in PV value after two to three compensation cycles...|$|E
5000|$|The Gauss-Markov {{assumptions}} {{concern the}} <b>set</b> of <b>error</b> random variables, : ...|$|R
40|$|Objectives: {{defining}} a well defined statistical framework What {{can we learn}} {{and how can we}} decide if our learning is effective? Efficient learning with many parameters Trade-off (generalization/and training <b>set</b> <b>error)</b> How to represent real world objects Objectives: {{defining a}} well defined statistical framework What can we learn and how can we decide if our learning is effective? Efficient learning with many parameters Trade-off (generalization/and training <b>set</b> <b>error)</b> How to represent real world objects PAC Learning Definition (1) Let c be the function (i. e. a concept) we want to learn Let h be the learned concept and x an instance (e. g. a person) error(h) = Prob [c(x) h(x) ] It would be useful if we could find: Pr(error(h) > ε) < δ Given a target error ε, the probability to make a larger error is less δ Definizione di PAC Learning (2) This methodology is called Probably Approximatel...|$|R
50|$|Regularization {{perspectives}} on support vector machines provide {{a way of}} interpreting support vector machines (SVMs) {{in the context of}} other machine learning algorithms. SVM algorithms categorize multidimensional data, with the goal of fitting the training set data well, but also avoiding overfitting, so that the solution generalizes to new data points. Regularization algorithms also aim to fit training set data and avoid overfitting. They do this by choosing a fitting function that has low error on the training set, but also is not too complicated, where complicated functions are functions with high norms in some function space. Specifically, Tikhonov regularization algorithms choose a function that minimize the sum of training <b>set</b> <b>error</b> plus the function's norm. The training <b>set</b> <b>error</b> can be calculated with different loss functions. For example, regularized least squares is a special case of Tikhonov regularization using the squared error loss as the loss function.|$|R
40|$|For {{smart grid}} development, {{one of the}} key {{expectations}} is that the data should be accessible to and readily interpreted by different applications. Presently, protection settings are represented using proprietary parameters and stored in various file formats. This makes it very difficult for computer applications to manipulate such data directly. This paper introduces a process that translates the proprietary protection setting data into IEC 61850 standardised format and saves the data as System Configuration description Language (SCL) files. A code generation process that allows rapid implementation of the translation process is proposed. Among various applications, the paper demonstrates how such a translation process and generated SCL files can facilitate the development of an intelligent system for protection <b>setting</b> <b>error</b> detection and validation. ...|$|E
40|$|Background. Presently known spektrum comparator, {{especially}} the higher categories, according to experts, {{who does not}} satisfy the requirements imposed on them, especially in dynamic range compared radiometric values and common errors of calibration of where the main proportion of the positioning error of the comparator with respect to radiators and <b>setting</b> <b>error</b> attenuation coefficient, this comparator is expedient to develop {{a wide range of}} optical transmittance. Objective. Metrological analysis of the original spektrum comparator wide-range on the basis of two optically coupled integrating spheres and development of recommendations for the design with the specified accuracy parameters. Results. The equations that allow to calculate and compare the metrological characteristics of the comparators. Conclusions. Metrological characteristics are analyzed for precision spektrum comparator wide-range radiometry. It is shown that significantly exceeds known analogues by using modern element base of the spektrum comparator. ???????? ??????????????? ?????? ????????????? ?????????????????? ?????????????????? ?? ???? ???? ?????????-??????????? ????????????? ???? ??? ???????????? ???????????, ???????????????? ??? ??????????????? ?????????????? ??????? ??????????? ??????????? ????????? ???????. ???????????????? ???????? ??????????? ??????????????????, ??????????? ???????????? ?? ?? ????????????? ? ?????? ?????????? ????...|$|E
40|$|The optics and the {{stability}} of the SPS-LHC transfer line TI 8 was studied with beam trajectories during its commissioning in October 2004. Steering magnet response measurements were used to analyze the quality of the steering magnets and of the beam position monitors. A simultaneous fit of the quadrupole strengths was used to search for setting or calibration errors. A large <b>setting</b> <b>error</b> of a quadrupole was identified with this technique, as well as a 1 % phase advance error in the vertical plane. Residual coupling between the planes was evaluated using high statistics samples of trajectories. The same high statistics sample were analysed using the Model Independent Analysis technique to understand possible sources of trajectory movements. The transfer line was found to be very stable and the dominant source of position jitter seems to be due to the ripple of the extraction septum...|$|E
40|$|Abstract Standard Genetic Programming {{operators}} are highly disruptive, with the concomitant risk {{that it may}} be difficult to converge to an optimal structure. The Tree Adjoining Grammar (TAG) formalism provides a more flexible Genetic Programming tree representation which supports a wide range of operators while retaining the advantages of tree-based representation. In particular, minimalchange point insertion and deletion operators may be defined. Previous work has shown that point insertion and deletion, used as local search operators, can dramatically reduce search effort in a range of standard problems, and can reduce the impact of Daida’s structural difficulty problem. Here, we evaluate the effect of local search with these operators on a real-World ecological time series modelling problem. For the same search effort, TAG-based GP with the local search operators generates solutions with significantly lower training <b>set</b> <b>error.</b> The results are equivocal on test <b>set</b> <b>error,</b> suggesting that parsimony control mechanisms will be required if improved generalisation is to be achieved...|$|R
40|$|In {{this paper}} we analyze the average {{behavior}} of the Bayes-optimal and Gibbs learning algorithms. We do this both for off-training-set error and conventional IID error (for which test sets overlap with training sets). For the IID case we provide a major extension {{to one of the}} better known results of [7]. We also show that expected IID test <b>set</b> <b>error</b> is a non-increasing function of training set size for either algorithm. On the other hand, as we show, the expected off training-set error for both learning algorithms can increase with training set size, for non-uniform sampling distributions. We characterize what relationship the sampling distribution must have with the prior for such an increase. We show in particular that for uniform sampling distributions and either algorithm, the expected off-training <b>set</b> <b>error</b> is a non-increasing function of training set size. For uniform sampling distributions, we also characterize the priors for which the expected error of the Bayes-optimal algo [...] ...|$|R
40|$|More than 30 {{years ago}} Nicholas Handy {{introduced}} the determinant full configuration interaction algorithm which provided the first exact benchmark calculations on molecules in small basis sets. Here I will discuss how advances in arbitrary-order, explicit, and local correlation methods now allow 'exact calculations' without basis <b>set</b> <b>error</b> {{both on the}} ground- and excited states of small molecules as well as molecular crystals. link_to_subscribed_fulltex...|$|R
40|$|The study {{deals with}} {{developing}} a new method to compensate the initial position error of diamond cutting tool in ultraprecision micromachining by use of a 5 -axis control ultraprecision machiningcenter. In the ultraprecision ma-chining, various kinds of errors have large influence on the machining accuracy although they are very small. In recent year, the demands is increasing to fabricate complicated microparts accurately and efficiently by means of multi-axis control ultraprecision machiningcenter recently. However, the accumulation of various kinds of errors due to {{the increasing number of}} motion axes and the setting difficulty of small diamond tools deteriorate the form accuracy of machined microparts. To solve these problems, the study proposes a method to precisely measure the tool radius and to correctly set the tool, and develops the initial tool <b>setting</b> <b>error</b> compensation system. From experimental results, it is found that the method proposed in the study is effective. 1...|$|E
40|$|We {{have devised}} a simple {{numerical}} technique to treat rugged data points that arise {{due to the}} insufficient gain <b>setting</b> <b>error</b> (or quantization error) of a digital instrument. This is a very wide spread problem that all experimentalists encounter some time or the other and {{they are forced to}} deal with it by suitable adjustments of instrument gains and other relevant parameters. But mostly this entails one to repeat the experiment,this may be inconvenient at the least. Here we prescribe a method that would actually attempt to smoothen the data set that is already so obtained. Our method is based on an entirely different algorithm that is not available anywhere else. This method mimics what one would do by intuitive visual inspection and not like the arcane digital filtering, spline fitting etc. that is available in the market. Nor does it depend on any instrumental parameter tweaking. This makes the program totally general purpose and also intellectually more satisfying. Comment: This paper has been withdrawn by the authors. 15 pages, 6 figures. Withdrawn pending wor...|$|E
30|$|Diverse magnetotelluric data (apparent resistivity, phase or {{geomagnetic}} transfer function) {{used during}} the inversion process provide different information on the model {{as a consequence of}} data error. The relative influence of these data constitutes a subject of interest on the inversion process. This influence can be evaluated from the error ratio between two types of data; thus, when phase and the logarithm of apparent resistivity are involved, the well-known ratio of one half is obtained. A new error ratio between the geomagnetic transfer function and the logarithm of apparent resistivity is presented. We deduced this ratio, which is bounded by one half of the amplitude of the geomagnetic transfer function. In order to verify this new ratio, we employed a technique based on the study of the RMS misfit, obtained after an intensive inversion computation whilst taking different error values for the different data. This technique was applied to synthetic and experimental data, and the results agree with the proposed value. This value should be taken into account for <b>setting</b> <b>error</b> floors when performing joint data inversion in order to obtain the same influence from the different data.|$|E
40|$|Development. Authors listed alphabetically. The {{opinions}} expressed {{here and}} errors made are {{the responsibility of}} the authors and not the Ministry of Social Development. We thank Martha Hill, Stephen Jenkins, Tim Maloney, Rebecca Martin, and Bryan Perry for very helpful comments. We also thank Sandra MacDonald and the team from Statistics New Zealand for their hard work on the data <b>set.</b> <b>Errors</b> and omissions remain our own...|$|R
40|$|Problems in {{briefing}} {{of relief}} by {{air traffic controllers}} are discussed, including problems that arise when duty positions are changed by controllers. Altimeter reading and <b>setting</b> <b>errors</b> as factors in aviation safety are discussed, including problems associated with altitude-including instruments. A sample of reports from pilots and controllers is included, covering the topics of ATIS broadcasts an clearance readback problems. A selection of Alert Bulletins, with their responses, is included...|$|R
40|$|Abstract. We {{focus on}} {{studying}} stochastic nonlinear complementarity problems (SNCP) and stochastic mathematical programs with equilibrium constraints (SMPEC). Instead of the NCP functions {{employed in the}} literature, we use the restricted NCP functions to define expected residual minimization formulations for SNCP and SMPEC. We then discuss level <b>set</b> conditions and <b>error</b> bounds of the new formulation. Numerical examples show that the new formulations have some desirable properties which the existing ones do not have. Key words. Stochastic complementarity problem, stochastic mathematical program with equilibrium constraints, NCP function, restricted NCP function, level <b>set,</b> <b>error</b> bound. 2000 Mathematics Subject Classification. 90 C 33, 90 C 30. ...|$|R
