777|290|Public
25|$|Several other web applications, such as weblog software, also {{incorporate}} forum features. WordPress comments at {{the bottom}} of a blog post allow for a <b>single-threaded</b> discussion of any given blog post. Slashcode, on the other hand, is far more complicated, allowing fully threaded discussions and incorporating a robust moderation and meta-moderation system as well as many of the profile features available to forum users.|$|E
25|$|With {{the return}} of Jobs, this {{rebranding}} to version 8 also allowed Apple to exploit a legal loophole to terminate third-party manufacturers' licenses to System 7 and effectively shut down the Macintosh clone market. Later, Mac OS 8.1 finally added the new filesystem and Mac OS 8.6 updated the nanokernel to handle limited support for preemptive tasks. Its interface is Multiprocessing Services 2.x and later, {{but there is no}} process separation and the system still uses cooperative multitasking between processes. Even a process that is Multiprocessing Services-aware still has a portion that runs in the Blue Box—a task that also runs all <b>single-threaded</b> programs and the only task that can run 68k code.|$|E
25|$|For years, {{processor}} makers delivered {{increases in}} clock rates and instruction-level parallelism, so that <b>single-threaded</b> code executed faster on newer processors with no modification. Now, to manage CPU power dissipation, processor makers favor multi-core chip designs, and software {{has to be}} written in a multi-threaded manner {{to take full advantage}} of the hardware. Many multi-threaded development paradigms introduce overhead, and will not see a linear increase in speed vs number of processors. This is particularly true while accessing shared or dependent resources, due to lock contention. This effect becomes more noticeable as the number of processors increases. There are cases where a roughly 45% increase in processor transistors has translated to roughly 10–20% increase in processing power.|$|E
50|$|In {{computer}} programming, <b>single-threading</b> is {{the processing}} of one command at a time. The opposite of <b>single-threading</b> is multithreading. While {{it has been suggested}} that the term <b>single-threading</b> is misleading, the term has been widely accepted within the functional programming community.|$|R
25|$|Most hobs are <b>single-thread</b> hobs, but double-, and triple-thread hobs {{increase}} production rates. The {{downside is that}} they are not as accurate as <b>single-thread</b> hobs.|$|R
40|$|Abstract. The {{purpose of}} this study was to {{determine}} the optimal thread design for an experimental cylinder implant. Three-dimensional finite element models with an implant of <b>single-thread,</b> double-thread, and triple-thread were created. TheMax EQV stresses in jaw bones andMax displacement of implant-abutment complex were compared. It was found that, under AX load, the Max EQV stresses in cortical bone and cancellous bone of double-thread implant increased by 10. 4 % and 9. 2 % compared with that of <b>single-thread</b> implant respectively. Under BL load, the Max EQV stresses in cortical bone of double-thread implant increased by 9. 1 % and Max EQV stresses in cancellous bone of triple-thread implant decreased by 14. 2 % compared with that of <b>single-thread</b> implant respectively. The results imply that <b>single-thread</b> implant shows the best stress transmission under axial load. <b>Single-thread</b> and triple-thread implant show the better stress transmission under buccolingual load. Different thread designs show similar effect on the osseointegrated implant stability...|$|R
25|$|<b>Single-threaded</b> {{microprocessors}} today waste many cycles {{while waiting}} to access memory, considerably limiting system performance. The use of multi-threading masks {{the effect of}} memory latency by increasing processor utilization. As one thread stalls, additional threads are instantly fed into the pipeline and executed, resulting in a significant gain in application throughput. Users can allocate dedicated processing bandwidth to real-time tasks resulting in a guaranteed Quality of Service (QoS). MIPS’ MT technology constantly monitors the progress of threads and dynamically takes corrective actions to meet or exceed the real-time requirements. A processor pipeline can achieve 80-90% utilization by switching threads during data-dependent stalls or cache misses. All of this leads to an improved mobile device user experience, as responsiveness is greatly increased.|$|E
25|$|Hobbing uses a {{hobbing machine}} with two skew spindles, one mounted {{with a blank}} {{workpiece}} {{and the other with}} the hob. The angle between the hob's spindle (axis) and the workpiece's spindle varies, {{depending on the type of}} product being produced. For example, if a spur gear is being produced, then the hob is angled equal to the helix angle of the hob; if a helical gear is being produced then the angle must be increased by the same amount as the helix angle of the helical gear. The two shafts are rotated at a proportional ratio, which determines the number of teeth on the blank; for example, for a <b>single-threaded</b> hob if the gear ratio is 40:1 the hob rotates 40 times to each turn of the blank, which produces 40 teeth in the blank. If the hob has multiple threads the speed ratio must be multiplied by the number of threads on the hob. The hob is then fed up into the workpiece until the correct tooth depth is obtained. Finally the hob is fed through the workpiece parallel to the blank's axis of rotation.|$|E
500|$|Windows 10 {{includes}} DirectX 12, alongside WDDM 2.0. Unveiled March 2014 at GDC, DirectX 12 aims {{to provide}} [...] "console-level efficiency" [...] with [...] "closer to the metal" [...] access to hardware resources, and reduced CPU and graphics driver overhead. Most {{of the performance}} improvements are achieved through low-level programming, which allow developers to use resources more efficiently and reduce <b>single-threaded</b> CPU bottlenecking caused by abstraction through higher level APIs. DirectX 12 will also feature support for vendor agnostic multi-GPU setups. WDDM 2.0 introduces a new virtual memory management and allocation system to reduce workload on the kernel-mode driver.|$|E
5000|$|Better floating-point <b>single-thread</b> {{performance}} (>5x improvement) ...|$|R
40|$|Simultaneous Multithreading (SMT) {{processors}} achieve high processor throughput at {{the expense}} of <b>single-thread</b> performance. This paper investigates resource allocation policies for SMT processors that preserve, as much as possible, the <b>single-thread</b> performance of designated "foreground" threads, while still permitting other "background" threads to share resources...|$|R
40|$|The {{detection}} of a property called 2 ̆ 2 <b>single-threading</b> 2 ̆ 2 in functional programs or denotational language definitions can be exploited {{to produce a}} more efficient implementation of the program or language, by allowing a program variable or semantic domain to be implemented by a global data structure. Earlier work by David A. Schmidt has given sufficient criteria for the {{detection of}} the <b>single-threading</b> property in [lambda]-calculus expressions. We extend that work by giving criteria for <b>single-threading</b> detection in combinator notations;Two sets of <b>single-threading</b> criteria are given: one for a particular combinator language, TML, and the other for a generalized combinator notation. In both cases, proofs are given which demonstrate the soundness of the criteria. We also discuss some reasons for the differences in the two sets of criteria;In order to evaluate the usefulness of <b>single-threading</b> detection and the associated globalization transformation in practice, a testbed implementation of the TML criteria is being programmed, using the PSI/DAOS compiler-generation system developed at Aalborg University. Some preliminary results from this work are discussed...|$|R
2500|$|Mac Mini models {{released}} in late 2014 used dual-core processors, which perform worse in multi-threaded workloads {{compared to the}} quad-core processors from the 2012 model. Meanwhile, <b>single-threaded</b> workload performance increased. Comparing the high ends of both releases, the 2012 model used a 4-core, 8-thread Intel Core , whereas the 2014 model used a 2-core, 4-thread Intel Core [...]|$|E
5000|$|... #Subtitle level 2: Advantages {{compared}} to <b>single-threaded</b> processors ...|$|E
5000|$|<b>Single-threaded,</b> asynchronous, {{non-blocking}} core {{with simple}} event-based API ...|$|E
50|$|Most hobs are <b>single-thread</b> hobs, but double-, and triple-thread hobs {{increase}} production rates. The {{downside is that}} they are not as accurate as <b>single-thread</b> hobs.Depending on type of gear teeth to be cut, there are custom made hobs and general purpose hobs. Custom made hobs are different from other hobs as they are suited to make gears with modified tooth profile. The tooth profile is modified to add strength and reduce size and gear noise.|$|R
40|$|Gene co-expression {{networks}} comprise {{one type}} of valuable biological networks. Many methods and tools have been published to construct gene co-expression networks; however, most of these tools and methods are inconvenient and time consuming for large datasets. We have developed a user-friendly, accelerated and optimized tool for constructing gene co-expression networks that can fully harness the parallel nature of GPU (Graphic Processing Unit) architectures. Genetic entropies were exploited to filter out genes with no or small expression changes in the raw data preprocessing step. Pearson correlation coefficients were then calculated. After that, we normalized these coefficients and employed the False Discovery Rate to control the multiple tests. At last, modules identification was conducted to construct the co-expression networks. All of these calculations were implemented on a GPU. We also compressed the coefficient matrix to save space. We compared {{the performance of the}} GPU implementation with those of multi-core CPU implementations with 16 CPU threads, <b>single-thread</b> C/C++ implementation and <b>single-thread</b> R implementation. Our results show that GPU implementation largely outperforms <b>single-thread</b> C/C++ implementation and <b>single-thread</b> R implementation, and GPU implementation outperforms multi-core CPU implementation when the number of genes increases. With the test dataset containing 16, 000 genes and 590 individuals, we can achieve greater than 63 times the speed using a GPU implementation compared with a <b>single-thread</b> R implementation when 50 percent of genes were filtered out and about 80 times the speed when no genes were filtered out...|$|R
40|$|We {{studied the}} dynamic {{instruction}} count reduction for a <b>single-thread,</b> vectorized and a multi-threaded, non-vectorized, MPEG- 4 video encoder. Results indicate a maximum {{improvement of the}} order of 88 % for 22 CPU contexts for the multi-threaded case whereas the <b>single-thread,</b> vectorized version demonstrates an 85 % improvement for a vector register file length of 24 bytes, over the scalar case. We present VLSI macrocells of a vector accelerator implementing a subset of the MPEG- 4 vector ISA and a 2 -way, parametric, bus-based, cache coherent, SoC multi-processor...|$|R
5000|$|... #Subtitle level 2: Disadvantages {{compared}} to <b>single-threaded</b> processors ...|$|E
5000|$|... #Subtitle level 4: Hypothetical Multi <b>Single-Threaded</b> Core (MuSiC) {{environment}} ...|$|E
5000|$|Provides {{multithreading}} support {{even when}} running on <b>single-threaded</b> operating systems ...|$|E
40|$|Simultaneous Multithreading (SMT) {{processors}} achieve high processor throughput at {{the expense}} of <b>single-thread</b> performance. This paper investigates resource allocation policies for SMT processors that preserve, as much as possible, the <b>single-thread</b> performance of designated "foreground" threads, while still permitting other "background" threads to share resources. Since background threads on such an SMT machine have a near-zero performance impact on foreground threads, we refer to the background threads as transparent threads. Transparent threads are ideal for performing low-priority or non-critical computations, with applications in process scheduling, subordinate multithreading, and online performance monitoring...|$|R
40|$|<b>Single-thread</b> performance, {{reliability}} and power efficiency are critical design challenges of future multicore systems. Although point solutions {{have been proposed}} to address these issues, a more fundamental change to the fabric of multicore systems is necessary to seamlessly combat these challenges. Towards this end, this paper proposes CoreGenesis, a dynamically adaptive multiprocessor fabric that blurs out individual core boundaries, and encourages resource sharing across cores for performance, fault tolerance and customized processing. Further, {{as a manifestation of}} this vision, the paper provides details of a unified performance-reliability solution that can assemble variable-width processors from a network of (potentially broken) pipeline stage-level resources. This design relies on interconnection flexibility, microarchitectural innovations, and compiler directed instruction steering, to merge pipeline resources for high <b>single-thread</b> performance. The same flexibility enables it to route around broken components, achieving sub-core level defect isolation. Together, the resulting fabric consists of a pool of pipeline stage-level resources that can be fluidly allocated for accelerating <b>single-thread</b> performance, throughput computing, or tolerating failures...|$|R
40|$|Current {{integration}} trends {{embrace the}} prosperity of single-chip multi-core processors. Although multi-core processors deliver significantly improved system throughput, <b>single-thread</b> performance is not addressed. In this paper, we propose a new execution paradigm that utilizes multi-cores on a single chip collaboratively to achieve high performance for <b>single-thread</b> memoryintensive workloads while maintaining the flexibility to support multithreaded applications. The proposed execution paradigm, dual-core execution, consists of two superscalar cores (a front and back processor) coupled with a queue. The front processor fetches and preprocesses instruction streams and retires processed instructions into the queue for the back processor to consume. The front processor executes instructions as usual except for cache-missing loads, which produce an invalid value instead of blocking the pipeline. As a result, the front processor runs far ahead to warm up the data caches and fix branch mispredictions for the back processor. In-flight instructions are distributed in the front processor, the queue, and the back processor, forming a very large instruction window for <b>single-thread</b> out-oforder execution. The proposed architecture incurs only minor hardware changes and does not require any large centralized structures such as large register files, issue queues, load/store queues, or reorder buffers. Experimental results show remarkable latency hiding capabilities of the proposed architecture, even outperforming more complex <b>single-thread</b> processors with much larger instruction windows than the front or back processor. 1...|$|R
50|$|Networking - <b>single-threaded</b> code {{associated}} with moving packets in networking applications.|$|E
5000|$|AutoBench 1.1 - <b>single-threaded</b> {{code for}} automotive, industrial, and {{general-purpose}} applications ...|$|E
5000|$|Improved thread {{scheduling}} and instruction prefetching to achieve higher <b>single-threaded</b> performance ...|$|E
50|$|The {{simulation}} and rendering {{processes of}} the engine are currently run on a <b>single-thread.</b> However, it is planned {{for there to be}} a multi-threaded release, but the publish date has not yet been determined.|$|R
5000|$|Linear pitch, p : Distance {{from any}} point on a thread to the {{corresponding}} {{point on the}} adjacent thread, measured parallel to the axis. For a <b>single-thread</b> worm, lead and linear pitch are the same.|$|R
5000|$|A simple <b>single-thread</b> cane {{can then}} {{be used to make}} more complex canes. A small bundle of <b>single-thread</b> canes can be heated until they fuse, or heated canes, laid parallel, can be picked up on the {{circumference}} of a hot cylinder of clear or colored glass. This bundle, treated just as the chunk of color in the description above, is cased in clear glass and pulled out, forming a vetro a fili [...] cane with multiple threads and perhaps a clear or solid color core. If the cane is twisted as it is pulled, the threads take a spiral shape called vetro a retorti [...] (twisted glass) or zanfirico.|$|R
5000|$|Epoch, a <b>single-threaded</b> Linux init system {{focused on}} {{simplicity}} and service management ...|$|E
5000|$|TSimpleServer - A <b>single-threaded</b> server using {{standard}} blocking I/O. Useful for testing.|$|E
50|$|Source for a <b>single-threaded</b> Linux {{version of}} C5.0 is {{available}} under the GPL.|$|E
50|$|A modern microserver {{typically}} offers medium-high {{performance at}} high packaging densities, allowing very small compute node form factors. This {{can result in}} high energy efficiency (operations per Watt), typically better than that of highest <b>single-thread</b> performance processors.|$|R
40|$|Quantifying {{landscape}} {{variations in}} sediment supply to {{streams and rivers}} is fundamental {{to our understanding of}} both denudational processes and stream channel morpho-dynamics. Previous studies have linked a variety of sediment supply proxies to climatic, topographic or geologic factors, but few have connected these directly to the characteristics of fluvial systems draining these landscapes. Using measurements of water and sediment fluxes for over 80 basins in the northern Rocky Mountains, USA, it is shown that the sediment supply signal is dominated by basin lithology, while exhibiting little correlation to factors such as relief, mean basin slope, and drainage density. Bankfull sediment concentrations (bed load and suspended load) increase as much 100 -fold as basin lithology becomes dominated by softer sedimentary and volcanic rocks, at the expense of more resistant lithologies. Downstream hydraulic geometry relations for <b>single-thread</b> reaches in these basins are remarkably similar and reasonably well predicted based on a channel forming Shields number; yet this simple model cannot capture the 2 - 3 order of magnitude range in sediment flux for a given discharge. In these streams the difference in the magnitude of bed load flux is modulated regionally by changes in bed armoring, resulting in a non-unique Shields number for a given channel configuration. As a result, <b>single-thread</b> reaches can absorb a wide range in sediment concentration, but at very high concentrations, bed surface, subsurface and bed load grain sizes converge and a transition from <b>single-thread</b> to braided channel patterns is commonly observed. A physically-based sediment concentration braiding – <b>single-thread</b> discriminant function is derived and tested using the empirical data, appropriately classifying 50 of 53 pattern types. Flow modeling shows that 2 -dimensional variability in flow properties in braided reaches may become equal to or dominate over changes in slope in response to high sediment supply. The resilience of <b>single-thread</b> channels to morphologic change thus reflects the degree to which textural changes can modulate variations in sediment supply, and a transition to a braided planform likely represents a dynamic equilibrium form in the face of high sediment supply...|$|R
40|$|Redundant {{threading}} architectures duplicate all {{instructions to}} detect and possibly recover from transient faults. Several lighter weight Partial Redundant Threading (PRT) architectures have been proposed recently. (i) Opportunistic Fault Tolerance duplicates instructions only during periods of poor <b>single-thread</b> performance. (ii) ReStore does not explicitly duplicate instructions and instead exploits mispredictions among highly confident branch predictions as symptoms of faults. (iii) Slipstream creates a reduced alternate thread by replacing many instructions with highly confident predictions. We explore PRT as a possible direction for achieving the fault tolerance of full duplication {{with the performance of}} <b>single-thread</b> execution. Opportunistic and ReStore yield partial coverage since they are restricted to using only partial duplication or only confiden...|$|R
