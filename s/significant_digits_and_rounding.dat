1|10000|Public
40|$|This guide {{provides}} information for {{an understanding of}} SI units, symbols, and prefixes; style and usage in documentation in both the US and in the international business community; conversion techniques; limits, fits, and tolerance data; and drawing and technical writing guidelines. Also provided is information of SI usage for specialized applications like data processing and computer programming, science, engineering, and construction. Related information in the appendixes include legislative documents, historical and biographical data, a list of metric documentation, rules for determining <b>significant</b> <b>digits</b> <b>and</b> <b>rounding,</b> conversion factors, shorthand notation, and a unit index...|$|E
50|$|A similar six-dot code by EIA had the top row as first, second <b>and</b> third <b>significant</b> <b>digits</b> <b>and</b> {{the bottom}} row as voltage rating (in {{hundreds}} of volts; no color indicated 500 volts), tolerance, and multiplier. A three-dot EIA code {{was used for}} 500 volt 20% tolerance capacitors, and the dots signified first <b>and</b> second <b>significant</b> <b>digits</b> <b>and</b> the multiplier. Such capacitors were common in vacuum tube equipment and in surplus for a generation after the war but are unavailable now.|$|R
2500|$|... where FSD is the First <b>Significant</b> <b>Digit</b> <b>and</b> [...] is {{the sample}} size. Morrow has {{determined}} the critical values for both these statistics, which are shown below: ...|$|R
25|$|In {{practice}} CRC calculations {{most closely}} resemble long division in binary, {{except that the}} subtractions involved do not borrow from more <b>significant</b> <b>digits,</b> <b>and</b> thus become exclusive or operations.|$|R
25|$|A {{significant}} figure is a digit {{in a number}} that adds to its precision. This includes all nonzero numbers, zeroes between <b>significant</b> <b>digits,</b> <b>and</b> zeroes indicated to be significant.|$|R
50|$|In {{practice}} CRC calculations {{most closely}} resemble long division in binary, {{except that the}} subtractions involved do not borrow from more <b>significant</b> <b>digits,</b> <b>and</b> thus become exclusive or operations.|$|R
50|$|The answer must {{be found}} one digit at a time {{starting}} at the least <b>significant</b> <b>digit</b> <b>and</b> moving left. The last calculation is on the leading zero of the multiplicand.|$|R
50|$|Unlike histograms, stem-and-leaf {{displays}} {{retain the}} original data {{to at least}} two <b>significant</b> <b>digits,</b> <b>and</b> put the data in order, thereby easing the move to order-based inference and non-parametric statistics.|$|R
50|$|For 5% {{precision}} SMD resistors {{usually are}} marked with their resistance values using three digits: two <b>significant</b> <b>digits</b> <b>and</b> a multiplier <b>digit.</b> These are quite often white lettering {{on a black}} background, but other colored backgrounds and lettering can be used.|$|R
30|$|Indicate {{the number}} of {{significant}} digitse for the means and SEM and {{the number of}} decimal places for the p-values to be displayed in the table. By default, 3 <b>significant</b> <b>digits</b> <b>and</b> 3 decimal places are used for means/SEM and p-values respectively.|$|R
5000|$|Two {{alternative}} tests specific to this law have been published: first, the max (m) statistic is given byand secondly, the distance (d) statistic is given bywhere FSD is the First <b>Significant</b> <b>Digit</b> <b>and</b> [...] is the sample size. Morrow has determined the critical values for both these statistics, which are shown below: ...|$|R
50|$|This gives from 15 to 17 <b>significant</b> decimal <b>digits</b> precision. If a decimal string with at most 15 <b>significant</b> <b>digits</b> is {{converted}} to IEEE 754 double-precision representation, and then converted back to a decimal string with {{the same number of}} digits, the final result should match the original string. If an IEEE 754 double-precision number {{is converted}} to a decimal string with at least 17 <b>significant</b> <b>digits,</b> <b>and</b> then converted back to double-precision representation, the final result must match the original number.|$|R
5000|$|Let x be a nonnegative floating-point {{number and}} assume that the active {{rounding}} attribute is round to nearest, ties to even, denoted RN. If ULP(x) is {{less than or equal}} to 1, then [...] Otherwise, [...] or , depending on the value of the least <b>significant</b> <b>digit</b> <b>and</b> the exponent of x. This is demonstrated in the following Haskell code typed at an interactive prompt: ...|$|R
5000|$|In {{a trivial}} sense, all {{positive}} integers are pandigital in unary (or tallying). In binary, all integers are pandigital except for 0 {{and numbers of}} the form [...] (the Mersenne numbers). The larger the base, the rarer pandigital numbers become, though one can always find runs of [...] consecutive pandigital numbers with redundant digits by writing all the digits of the base together (but not putting the zero first as the most <b>significant</b> <b>digit)</b> <b>and</b> adding x + 1 zeroes at the end as least <b>significant</b> <b>digits.</b>|$|R
40|$|AbstractThis paper {{reports the}} modular design of {{software}} library components which read, write, add, subtract, multiply, and divide real valued numbers with attention to <b>significant</b> <b>digits</b> <b>and</b> <b>digit</b> grouping. Additionally, {{an example is}} presented about how to use this module to make other library components to read, write, and manipulate data matrix files with attention to embedded prose documentation. These components {{can be used to}} apply literate programming to data files...|$|R
50|$|A {{significant}} figure is a digit {{in a number}} that adds to its precision. This includes all nonzero numbers, zeroes between <b>significant</b> <b>digits,</b> <b>and</b> zeroes indicated to be significant.Leading and trailing zeroes are not significant because they exist only to show {{the scale of the}} number. Therefore, 1,230,400 usually has five {{significant figure}}s: 1, 2, 3, 0, and 4; the final two zeroes serve only as placeholders and add no precision to the original number.|$|R
40|$|Technical {{proposals}} {{and recommendations for}} revising FORTRAN were studied and categorized. In the area of numerical precision, the proposals basically agree {{on a set of}} necessary parameters, although a wide range of nomenclature and specific function names are used. Environmental parameters identified include the following: (1) base of floating point representation, (2) largest positive real number, exponent and integer, (3) largest negative real number, exponent and integer, (4) number of <b>significant</b> <b>digits,</b> <b>and</b> (5) exponent bias...|$|R
40|$|AbstractIn residue number systems many {{arithmetic}} operations, like addition and multiplication, {{can be done}} {{in constant}} time. But, among others, division is more complex. This paper introduces a division algorithm which is in its complexity comparable to the conventional integer division algorithm. The algorithm exhibits nice properties, especially for long integer arithmetic as it does not involve numbers with many <b>significant</b> <b>digits,</b> <b>and</b> is thus superior to comparable algorithms published recently. The algorithm copes well with parallel implementations of residue number systems' arithmetic...|$|R
5000|$|Stemplot : A stemplot (or stem-and-leaf plot), in statistics, is {{a device}} for {{presenting}} quantitative {{data in a}} graphical format, similar to a histogram, to assist in visualizing {{the shape of a}} distribution. They evolved from Arthur Bowley's work in the early 1900s, and are useful tools in exploratory data analysis. Unlike histograms, stemplots retain the original data to at least two <b>significant</b> <b>digits,</b> <b>and</b> put the data in order, thereby easing the move to order-based inference and non-parametric statistics.|$|R
40|$|We {{discuss a}} common {{suspicion}} about reported financial data, in 10 industrial {{sectors of the}} 6 so called "main developing countries" over the time interval [2000 - 2014]. These data are examined through Benford's law first <b>significant</b> <b>digit</b> <b>and</b> through distribution distances tests. It is shown that several visually anomalous data {{have to be a}} priori removed. Thereafter, the distributions much better follow the first <b>digit</b> <b>significant</b> law, indicating the usefulness of a Benford's law test from the research starting line. The same holds true for distance tests. A few outliers are pointed out. Comment: 22 pages, 34 references, 4 figures, 7 tables; to be published in Physica...|$|R
40|$|We {{demonstrate}} how {{to compute the}} correlation function by using the tensor renormalization group for 2 D Ising model. From the correlation function around the critical temperature, we extract the correlation length and the spontaneous magnetization and then determine some critical exponents. Attractive feature of our method to extract the critical exponents is that numerical derivative, that causes a loss of <b>significant</b> <b>digits,</b> <b>and</b> additional simulation parameters, that demand more computational cost and complicated analysis, are not required. On the other hand, our approach requires an additional treatment of impurity tensors, but the additional cost is still tolerable...|$|R
40|$|Drawing from a large, diverse body of work, {{this survey}} {{presents}} a comprehensive and unified {{introduction to the}} mathematics underlying the prevalent logarithmic distribution of <b>significant</b> <b>digits</b> <b>and</b> significands, {{often referred to as}} Benford’s Law (BL) or, in a special case, as the First Digit Law. The invariance properties that characterize BL are developed in detail. Special attention is given to the emergence of BL {{in a wide variety of}} deterministic and random processes. Though mainly expository in nature, the article also provides strengthened versions of, and simplified proofs for, many key results in the literature. Numerous intriguing problems for future research arise naturally...|$|R
30|$|The {{error of}} k, i.e. the {{uncertainty}} of the slope of the regression line, is, due to the number of data points, smaller than the least <b>significant</b> specified <b>digit</b> <b>and</b> therefore negligible. However, the standard deviation of the individual measurements from the regression line is 0.09 μm, corresponding to 2.1 μN.|$|R
25|$|Formann {{provided}} an alternative explanation by directing {{attention to the}} interrelation between {{the distribution of the}} <b>significant</b> <b>digits</b> <b>and</b> the distribution of the observed variable. He showed in a simulation study that long right-tailed distributions of a random variable are compatible with the Newcomb-Benford law, and that for distributions of the ratio of two random variables the fit generally improves. For numbers drawn from certain distributions (IQ scores, human heights) the Law fails to hold because these variates obey a normal distribution which is known not to satisfy Benford's law, since normal distributions can't span several orders of magnitude and the mantissae of their logarithms will not be (even approximately) uniformly distributed.|$|R
50|$|Formann {{provided}} an alternative {{explanation for the}} Newcomb-Benford law - a formalisation of the remarkable observation that the frequencies with which the leading digits of numbers occur in large data sets are far away from being uniform (e.g., the leading digit 1 occurs in nearly one third of all cases). In addition to the prevailing explanations based on scale- and base invariance, Formann directed the attention to the interrelation between {{the distribution of the}} <b>significant</b> <b>digits</b> <b>and</b> the distribution of the observed variable. He showed in a simulation study that long right-tailed distributions of a random variable are compatible with the Newcomb-Benford law, and that for distributions of the ratio of two random variables the fit generally improves.|$|R
40|$|The file {{associated}} with this record is under embargo until 12 months after publication, {{in accordance with the}} publisher's self-archiving policy. The full text may be available through the publisher links provided above. We discuss a common suspicion about reported financial data, in 10 industrial sectors of the 6 so called “main developing countries” over the time interval [2000 – 2014]. These data are examined through Benford's law first <b>significant</b> <b>digit</b> <b>and</b> through distribution distances tests. It is shown that several visually anomalous data have to be a priori removed. Thereafter, the distributions much better follow the first <b>digit</b> <b>significant</b> law, indicating the usefulness of a Benford's law test from the research starting line. The same holds true for distance tests. A few outliers are pointed out. Peer-reviewedPost-prin...|$|R
5000|$|In computing, fixed float {{describes}} {{a method of}} representing real numbers {{in a way that}} number and decimal point value is stored at different location or bytes in a memory allocated to variable unlike floating point. In a typical 4 byte (on little endian platform) fixed float number lower(lsb) 2 bytes are used to store the decimal part of the number just like integer value. While upper 2 bytes are used to store the part of number before the decimal point. Floating point numbers are, in general, represented approximately to a fixed number of <b>significant</b> <b>digits</b> <b>and</b> scaled using an exponent. The base for the scaling is normally 2, 10 or 16. The typical number that can be represented exactly is of the form: ...|$|R
5000|$|Montgomery {{multiplication}} is {{an alternative}} algorithm that processed the multiplier [...] "backwards" [...] (least <b>significant</b> <b>digit</b> first) <b>and</b> uses the least <b>significant</b> <b>digit</b> of the accumulator to control {{whether or not the}} modulus should be added/subtracted. This avoids the need for carries to propagate. However, the algorithm is impractical for single modular multiplications, since two or three additional Montgomery steps have to be performed to convert the operands into a special form before processing and to convert the result back into conventional binary at the end.|$|R
40|$|AbstractLet z be a {{positive}} integer which is obtained {{as the product of}} several large integers each with a periodic digit behavior. We investigate the periodic behavior for the leading <b>digits</b> of z <b>and</b> for the least <b>significant</b> <b>digits</b> of z <b>and</b> further study the relations between the two periodic behaviors...|$|R
40|$|This paper {{introduces}} a digit-serial GF(2 m) multiplier {{for use in}} the polynomial basis. The multiplier works with the most <b>significant</b> <b>digit</b> first <b>and</b> is scalable to an arbitrary <b>digit</b> size <b>and</b> can be constructed for any GF(2 m). It is derived from a commonly used MSB first bit-serial multiplier, known as the standard shift-register multiplier. As the latency of the multiplier decreases when the digit size increases, it is possible to minimize the complexity when the computational needs are known. 1...|$|R
40|$|The {{ground-state}} {{energy of}} a system consisting of four identical bosons or fermions is calculated using the Yakubovsky differential equations which are formulated in configuration space. The solution is restricted to include s waves only. Spline approximation and orthogonal collocation reduce the Yakubovsky equations to a matrix equation which is solved using the Lanczos algorithm. Storage requirements are reduced by more than three orders of a magnitude by exploiting the tensor structure present in the equation. Some of the results obtained with these methods are presented. All calculations are done on a workstation. The calculated binding energies have more than five <b>significant</b> <b>digits,</b> <b>and</b> it is therefore expected that the exploitation of the tensor structure {{makes it possible to}} use the Yakubovsky differential equations for realistic ground-state energy calculations with a higher accuracy than is possible with other methods...|$|R
40|$|A {{detailed}} {{investigation of}} three different rounding rules for multiplication and division is presented, including statistical analyses via Monte-Carlo simulations {{as well as a}} math-ematical derivation. This work expands upon a previous study by Mulliss and Lee (1998), by making the more realistic assumption that the contributing uncertainties are statistically independent. With this assumption, it is shown that the so-called standard rounding rule fails over 60 % of the time, leading to a loss in precision. Two alternative rules are studied, and both are found to be significantly more accurate than the standard rule. One alterna-tive rule requires one extra <b>significant</b> <b>digit</b> beyond that predicted by the standard rule. The other requires one to count numbers whose leading digit is 5 or greater as having an extra <b>significant</b> <b>digit,</b> <b>and</b> then to apply the standard rule. Although the second alternative rule is slightly more accurate, the first is shown to be completely safe for data — never leading to a truncation of <b>digits</b> that contain <b>significant</b> information. Accordingly, we recommend the first alternative rule as the new standard. PACS numbers: 01. 30. Pp, 01. 55. +b, 02. 70. Uu I...|$|R
40|$|The {{model of}} self-avoiding lattice walks and the {{asymptotic}} analysis of power-series {{have been two}} of the major research themes of Tony Guttmann. In this paper we bring the two together and perform a new analysis of the generating functions {{for the number of}} square lattice self-avoiding walks and some of their metric properties such as the mean-square end-to-end distance. The critical point x_c for self-avoiding walks is known to a high degree of accuracy and we utilise this knowledge to undertake a new numerical analysis of the series using biased differential approximants. The new method is major advance in asymptotic power-series analysis in that it allows us to bias differential approximants to have a singularity of order q at x_c. When biasing at x_c with q≥ 2 the analysis yields a very accurate estimate for the critical exponent γ= 1. 3437500 (3) thus confirming the conjectured exact value γ= 43 / 32 to 8 <b>significant</b> <b>digits</b> <b>and</b> removing a long-standing minor discrepancy between exact and numerical results. The analysis of the mean-square end-to-end distance yields ν= 0. 7500002 (4) thus confirming the exact value ν= 3 / 4 to 7 <b>significant</b> <b>digits.</b> Comment: 14 pages, 3 figure...|$|R
50|$|Statistical {{distributions}} reveal trends {{based on}} how numbers are distributed. Common examples include histograms and box-and-whisker plots, which convey statistical features such as mean, median, and outliers. In addition to these common infographics, alternatives include stem-and-leaf plots, Q-Q plots, scatter plot matrices (SPLOM) and parallel coordinates. For assessing a collection of numbers and focusing on frequency distribution, stem-and-leaf plots can be helpful. The numbers are binned based on the first <b>significant</b> <b>digit,</b> <b>and</b> within each stack binned again based on the second <b>significant</b> <b>digit.</b> On the other hand, Q-Q plots compare two probability distributions by graphing quantiles against each other. This allows the viewer {{to see if the}} plot values are similar and if the two are linearly related. SPLOM is a technique that represents the relationships among multiple variables. It uses multiple scatter plots to represent a pairwise relation among variables. Another statistical distribution approach to visualize multivariate data is parallel coordinates. Rather than graphing every pair of variables in two dimensions, the data is repeatedly plotted on a parallel axis and corresponding points are then connected with a line. The advantage of parallel coordinates is that they are relatively compact, allowing many variables to be shown simultaneously.|$|R
500|$|... {{according}} to some chosen radix; then, {{the part of the}} key used for the th pass of the algorithm is the th digit in the positional notation for the full key, starting from the least <b>significant</b> <b>digit</b> <b>and</b> progressing to the most significant. For this algorithm to work correctly, the sorting algorithm used in each pass over the data must be stable: items with equal digits should not change positions with each other. For greatest efficiency, the radix should be chosen to be near the number of data items, [...] Additionally, using a power of two near [...] as the radix allows the keys for each pass to be computed quickly using only fast binary shift and mask operations. With these choices, [...] and with pigeonhole sort or counting sort as the base algorithm, the radix sorting algorithm can sort [...] data items having keys in the range from [...] to [...] in time [...]|$|R
40|$|The {{magnetic}} moment μ of a bound electron, generally {{expressed by the}} g-factor μ=−g μ B s ħ − 1 with μ B the Bohr magneton and s the electron’s spin, can be calculated by bound-state quantum electrodynamics (BS-QED) to very high precision. The recent ultra-precise experiment on hydrogen-like silicon determined this value to eleven <b>significant</b> <b>digits,</b> <b>and</b> thus allowed to rigorously probe the validity of BS-QED. Yet, the investigation {{of one of the}} most interesting contribution to the g-factor, the relativistic interaction between electron and nucleus, is limited by our knowledge of BS-QED effects. By comparing the g-factors of two isotopes, it is possible to cancel most of these contributions and sensitively probe nuclear effects. Here, we present calculations and experiments on the isotope dependence of the Zeeman effect in lithium-like calcium ions. The good agreement between the theoretical predicted recoil contribution and the high-precision g-factor measurements paves the way for a new generation of BS-QED tests...|$|R
