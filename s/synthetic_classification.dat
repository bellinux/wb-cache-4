12|42|Public
40|$|Based on the {{analysis}} of the latest scientific data, the paper suggests a <b>synthetic</b> <b>classification</b> of the known bioantioxidants, peculiarities of their structure and functioning. The second part of the review describes the sulfur-containing compounds, chelators of the metal ions of variable valency, compounds containing hydroxyl or amino group, and nanoparticles. </p...|$|E
40|$|An essay of <b>synthetic</b> <b>classification</b> {{of turkish}} climates. The {{purpose of the}} method ^geographical sampling, factor {{analysis}} and algorithms of numerical taxonomy — is to classify the turkish climates according to 34 criteria including seasonal rythms, monthly indexes of water balance, averages and extrems. The most discriminant factors are the winterly cold (even in that country) andthe «méditer» raneity» of the summer. The classification points more clearly at some important facts and relations between the climatic types of Turkey. Charre Joël, Dumolard Pierre. Essai de classification synthétique des climats de la Turquie. In: Méditerranée, deuxième série, tome 14, 3 - 1973. pp. 51 - 65...|$|E
40|$|This work {{deals with}} the {{delimitation}} of physical-geographical landscapes of {{the territory of the}} Tourist Circuit Chilpancingo Azul, at Guerrero's State central region, from geo-ecological conception for the physical-geographical <b>synthetic</b> <b>classification</b> of territorial units. This approach obtained territorial units and its hierarchical classification using a taxonomic system of localities, neighborhoods and boroughs (smallest categories). Taking into account the 1 : 100 000 scale different geographical units were determined: 3 localities, 31 neighborhoods and 177 boroughs. The characterization of these territorial units include lithologic constitution, geomorphological conditions (morphogenesis and morphometrics) and spatial distribution of major types of soils and vegetation and land uses in the territory...|$|E
40|$|The UDC is {{attractive}} to different stakeholders across the information sector because of its wide-spread application, large vocabulary and availability in an electronic format. Modern information retrieval systems have the need but also the capacity to support flexible and interactive retrieval systems. The role of classification in such systems is {{to serve as an}} underlying knowledge structure that provides systematic subject organisation and thus complements the search using natural language terms. There are, however, specific requirements that must be satisfied in order to make efficient use of classification and these are not well known outside the library domain and are poorly implemented in library systems. This is especially the case for <b>synthetic</b> <b>classifications,</b> such as UDC, because its elements are meant to be manipulated by the system to fulfill different functions (a flexible systematic display, browsing or search purposes). This report summarizes the most important functionalities of the UDC that need {{to be taken into account}} during the implementation process. Important issues about the relation between the UDC schedules in electronic form - UDC Master Reference File and a classification tool (an authority file) that may be built on it, are highlighted. A better understanding of the UDC system's functionality may improve or facilitate its implementation and lower the costs of system maintenance which may be relevant for both prospective users and legacy systems...|$|R
40|$|Various {{modifications}} of decision trees have been extensively {{used during the}} past years due to their high efficiency and interpretability. Tree node splitting based on relevant feature selection is a key step of decision tree learning, {{at the same time}} being their major shortcoming: the recursive nodes partitioning leads to geometric reduction of data quantity in the leaf nodes, which causes an excessive model complexity and data overfitting. In this paper, we present a novel architecture - a Decision Stream, - aimed to overcome this problem. Instead of building a tree structure during the learning process, we propose merging nodes from different branches based on their similarity that is estimated with two-sample test statistics, which leads to generation of a deep directed acyclic graph of decision rules that can consist of hundreds of levels. To evaluate the proposed solution, we test it on several common machine learning problems - credit scoring, twitter sentiment analysis, aircraft flight control, MNIST and CIFAR image <b>classification,</b> <b>synthetic</b> data <b>classification</b> and regression. Our experimental results reveal that the proposed approach significantly outperforms the standard decision tree learning methods on both regression and classification tasks, yielding a prediction error decrease up to 35 %...|$|R
40|$|Abstract: In the {{tradition}} of European phytosociology, delimitations of vegetation units such as associations are mostly {{based on data from}} small areas where more detailed vegetation sampling has been carried out. Such locally delimited vege-tation units are often accepted in large-scale <b>synthetic</b> <b>classifications,</b> e. g. national vegetation monographs, and tentatively assigned to a small geographical range, forming groups of similar (vicarious) vegetation units in different small areas. These vicarious units, however, often overlap in species composition and are difficult to recognize from each other. We demonstrate this issue using an example of the classification of dry grasslands (Festuco-Brometea) in the Czech Republic. The standard vegetation classification of the Czech Republic supposes that the majority of accepted associations (66 out of 68) have a restricted distribution in one of the two major regions, Bohemia or Moravia. We compared the classification into traditional associations with the numerical classification of 1440 phytosociological relevés from the Czech Republic, in order to test whether {{the tradition}}ally recognized associations with small geographical ranges are reflected in numerical classification. In various comparisons, the groups of relevés identified by numerical analysis occupied larger areas than the traditional associations. This suggests that with consistent use of total species composition as the vegetation classification criterion, the resulting classification will usually include more vegetation units with larger geographical ranges, while many of the traditional local associations will disappear. Key words: dry grasslands, ecological scale, vegetation surve...|$|R
40|$|Machine {{learning}} {{research in}} image-based computer aided diagnosis {{is a field}} characterised by rich models and relatively small datasets. In this regime, conventional statistical tests for cross validation results {{may no longer be}} optimal due to variability in training set quality. We present a principle by which existing statistical tests can be conservatively extended to make use of arbitrary numbers of repeated experiments. We apply this to the problems of interval estimation and pair wise comparison for the accuracy of classification algorithms, and test the resulting procedures on real and <b>synthetic</b> <b>classification</b> tasks. The interval coverages in the synthetic task are notably improved, and the comparison has both increased power and reduced type I error. Experiments in the ADNI dataset show that the low replicability of split-half based tests can be dramatically improved...|$|E
40|$|Rough {{set theory}} approximates given concept(s) using lower and upper sets of the concept(s). Given that the {{uncertainty}} in a data set {{is caused by}} noisy or incomplete data, this approach is not always desirable {{because it does not}} exercise opportunities to discover/generalize a valuable pattern that is polluted by noise or that is almost certain. This problem has been subject of numerous studies on developing rough approximation methods based on different definitions of positive (and boundary) regions. In this paper, we investigate and experimentally compare three classification methods, based on rough sets using upper, lower, and elementary set methods, in regard to the very nature of real-world data such as redundant, noisy, unknown, and incomplete data. The issues around ultra large and dynamic data is addressed seperately. Data sets come from machine learning database repository, the <b>Synthetic</b> <b>Classification</b> Data Sets (SCDS) program ([URL] melli/ [...] ...|$|E
40|$|The {{purpose of}} this study is to {{describe}} the separate stages of the transformation in light of the basic accompanying fiscal difficulties and to formulate general conclusions regarding the factors which substantially affect the state of public finances and the quality of fiscal policy in post-communist countries. The structure of the paper is as follows: Section 2 provides a <b>synthetic</b> <b>classification</b> of the post-communist economies according to their advancement in the transformation process; section 3 contains a proposed scheme for distinguishing successive stages of fiscal policy during the transformation period. In the following four sections, four such stages of fiscal policy are discussed. These are: initial destabilization, initial stabilization, secondary fiscal crisis, and finally, the restoration of fiscal potential. Section 8 is devoted to a group of countries which have not been able, so far, to achieve sustainable macroeconomic stabilization. Section 9 presents the issues concerning quasi-fiscal subsidies and the quasi- fiscal deficit. Section 10 contains a summary and concluding remarks. Poland, economic transition, fiscal crisis...|$|E
40|$|Adopting a Bayesian {{approach}} and sampling the network parameters from their posterior distribution {{is a rather}} novel and promising method for improving the generalisation performance of neural network predictors. The present empirical study applies this scheme {{to a set of}} different <b>synthetic</b> and real-world <b>classification</b> problems. The paper focuses on the dependence of the prediction results on the prior distribution of the network parameters and hyperparameters, and provides a critical evaluation of the automatic relevance determination (ARD) scheme for detecting irrelevant inputs...|$|R
40|$|In the {{detection}} {{of the existence of}} the early gamma response, subjective methods have been used. In this study, an automated gamma detection technique is developed based on the features obtained from the time - frequency representation of the EEG signal in the gamma frequency band. The technique easily discriminates the gamma response existing and non-existing cases for the generated <b>synthetic</b> data. The <b>classification</b> of the technique and that of the expert opinion coincide % 77 for real EEG data. © 2006 IEEE...|$|R
40|$|Abstract — Advance image {{classification}} system focuses on synthetic (e. g non-photographic) & Natural (e. g photographic) images. The classification of images based on semantic description is a challenging and important problem in automatic image identification. An algorithm for natural and <b>synthetic</b> image <b>classification</b> has been developed. Some features are {{extracted from the}} raw images data in order to exploit the difference of color pattern and spatial correlation of pixels in natural and synthetic images. These features have poor accuracy if used alone but when combined together forming a more complex and accurate global classifier, their precision can be boosted. Our {{image classification}} algorithm will use low-level image features such as Color map, edge map, energy level, Threshold ratio & nearest neighborhood classifier for classifying the image into synthetic and natural. Key Words:-Synthetic image, Natural image, Color map...|$|R
40|$|Given a {{classification}} task, {{what is the}} best way to teach the resulting boundary to a human? While machine learning techniques can provide excellent methods for finding the boundary, including the selection of examples in an online setting, they tell us little about how we would teach a human the same task. We propose to investigate the problem of ex-ample selection and presentation in the context of teaching humans, and explore a variety of mechanisms in the interests of finding what may work best. In particular, we begin with the baseline of random presentation and then examine com-binations of several mechanisms: the indication of an exam-ple’s relative difficulty, the use of the shaping heuristic from the cognitive science literature (moving from easier exam-ples to harder ones), and a novel kernel-based “coverage model ” of the subject’s mastery of the task. From our exper-iments on 54 human subjects learning and performing a pair of <b>synthetic</b> <b>classification</b> tasks via our teaching system, we found that we can achieve the greatest gains with a combina-tion of shaping and the coverage model...|$|E
40|$|In {{this paper}} we {{investigate}} {{the use of}} neuron [...] specific activation functions (AFs) within generalized multi [...] layer perceptrons (GMLP). We utilize the netGEN system not only to evolve the structure of an artificial neural network (ANN), but also {{to search for a}} set of AF templates which are assigned to specific neurons by evolution. This may be seen as a loose anlogy to neuron differentiation in biological neural networks (BNNs). While BNNs employ different neuron types in functionally different brain areas, neuron differentiation in ANNs might be useful to increase the adaptability to specific problems. The evolution of AF templates is based on evolving the control points of a cubic spline function, hence non [...] monotonous AFs of (nearly) arbitrary shape may be generated. We present a number of experiments evolving ANN structure and AF templates using the parallel netGEN system to train the evolved architectures. We compare the evolved cubic spline ANNs with evolved sigmoid ANNs on <b>synthetic</b> <b>classification</b> problems and a time series prediction task so as to assess the benefits of problem [...] adapted AF templates...|$|E
40|$|We {{consider}} {{a method for}} estimating classification performance of a model-based synthetic aperture radar (SAR) automatic target recognition system. Target classification is performed by comparing an unordered feature set extracted from a measured SAR image chip with an unordered feature set predicted from a hypothesized target class and pose. A Bayes likelihood metric that incorporates uncertainty in both the predicted and extracted feature vectors is used to compute the match score. Evaluation of the match likelihoods requires a correspondence between the unordered predicted and extracted feature sets. This is a bipartite graph matching problem with insertions and deletions; we show that the optimal match {{can be found in}} polynomial time. We extend the results in 1 to estimate classification performance for a ten-class SAR ATR problem. We {{consider a}} <b>synthetic</b> <b>classification</b> problem to validate the classifier and to address resolution and robustness questions in the likelihood scoring method. Specifically, we consider performance versus SAR resolution, performance degradation due to mismatch between the assumed and actual feature statistics, and performance impact of correlated feature attributes...|$|E
40|$|Abstract — In this paper, we {{extend the}} {{exponentially}} embedded family (EEF), {{a new approach}} to model order estimation and probability density function construction originally proposed by Kay in 2005, to multivariate pattern recognition. Specifically, a parametric classifier rule based on the EEF is developed, in which we construct a distribution for each class based on a reference distribution. The proposed method can address different types of classification problems in either a data-driven manner or a model-driven manner. In this paper, we demonstrate its effectiveness with examples of <b>synthetic</b> data <b>classification</b> and real-life data classification in a data-driven manner and the example of power quality disturbance classification in a model-driven manner. To evaluate the classification performance of our approach, the Monte-Carlo method is used in our experiments. The promising experimental results indicate many potential applications of the proposed method. Index Terms — Exponentially embedded family (EEF), multivariate Gaussian classification, parametric classification rule. I...|$|R
40|$|Hybrid random {{fields are}} a {{recently}} proposed graphical model for pseudo-likelihood estimation in discrete domains. In this paper, we develop a continuous {{version of the}} model for nonparametric density estimation. To this aim, Nadaraya-Watson kernel estimators are used to model the local conditional densities within hybrid random fields. First, we introduce a heuristic algorithm for tuning the kernel bandwidhts in the conditional density estimators. Second, we propose a novel method for initializing the structure learning algorithm originally employed for hybrid random fields, which was meant instead for discrete variables. In order to test {{the accuracy of the}} proposed technique, we use a number of <b>synthetic</b> pattern <b>classification</b> benchmarks, generated from random distributions featuring nonlinear correlations between the variables. As compared to state-of-the-art nonparametric and semiparametric learning techniques for probabilistic graphical models, kernel-based hybrid random fields regularly outperform each considered alternative in terms of recognition accuracy, while preserving the scalability properties (with respect to the number of variables) that originally motivated their introduction...|$|R
40|$|International audienceThe Functional Requirements for Bibliographic Records (FRBR), an {{emerging}} {{model in the}} bibliographic domain, provide interesting possibilities in terms of cataloguing, representation and semantic enrichment of bibliographic data. However, the automated transformation of existing catalogs to fit this model is a requirement towards a wide adoption of FRBR in libraries. The cultural heritage community proposed a notable amount of FRBRization tools and projects, thus {{making it difficult for}} practitioners to compare and evaluate them. In this paper, we propose a <b>synthetic</b> and relevant <b>classification</b> of the FRBRization techniques according to specific criteria of comparison such as model expressiveness or specific enhancements...|$|R
40|$|The {{most common}} (or even only) choice of {{activation}} functions (AFs) for multi-layer perceptrons (MLPs) {{widely used in}} research, engineering and business is the logistic function. Among {{the reasons for this}} popularity are its boundedness in the unit interval, the function's and its derivative's fast computability, and a number of amenable mathematical properties in the realm of approximation theory. However, considering the huge variety of problem domains MLPs are applied in, it is intriguing to suspect that specific problems call for specific activation functions. Biological neural networks with their enormous variety of neurons mastering a set of complex tasks may be considered to motivate this hypothesis. We present a number of experiments evolving structure and activation functions of generalized multi{layer perceptrons (GMLPs) using the parallel netGEN system to train the evolved architectures. For the evolution of activation functions we employ cubic splines and compare the evolved cubic spline ANNs with evolved sigmoid ANNs on <b>synthetic</b> <b>classification</b> problems which allow conclusions w. r. t. the shape of decision boundaries. Also, an interesting observation concerning Minsky's Paradox is reported...|$|E
40|$|Abstract—This paper {{proposes a}} {{complete}} framework of poste-rior probability support vector machines (PPSVMs) for weighted training samples using modified concepts of risks, linear separa-bility, margin, and optimal hyperplane. Within this framework, a new optimization problem for unbalanced classification problems is formulated {{and a new}} concept of support vectors established. Furthermore, a soft PPSVM with an interpretable parameter is obtained {{which is similar to}} the-SVM developed by Schölkopf et al., and an empirical method for determining the posterior proba-bility is proposed as a new approach to determine. The main ad-vantage of an PPSVM classifier lies in that fact that it is closer to the Bayes optimal without knowing the distributions. To validate the proposed method, two <b>synthetic</b> <b>classification</b> examples are used to illustrate the logical correctness of PPSVMs and their relation-ship to regular SVMs and Bayesian methods. Several other clas-sification experiments are conducted to demonstrate that the per-formance of PPSVMs is better than regular SVMs in some cases. Compared with fuzzy support vector machines (FSVMs), the pro-posed PPSVM is a natural and an analytical extension of regular SVMs based on the statistical learning theory. Index Terms—Bayesian decision theory, classification, margin, maximal margin algorithms,-SVM, posterior probability, sup...|$|E
40|$|International audienceThe {{study of}} problem {{difficulty}} {{is an open}} issue in Genetic Programming (GP). Thegoal of this work is to generate models that predict the expected performance of a GPbasedclassifier when it is applied to an unseen task. Classification problems aredescribed using domain-specific features, {{some of which are}} proposed in this work,and these features are given as input to the predictive models. These models arereferred to as predictors of expected performance (PEPs). We extend this approach byusing an ensemble of specialized predictors (SPEP), dividing classification problemsinto specified groups and choosing the corresponding SPEP. The proposed predictorsare trained using 2 D <b>synthetic</b> <b>classification</b> problems with balanced datasets. Themodels are then used to predict the performance of the GP classifier on unseen realworlddatasets that are multidimensional and imbalanced. Moreover, as we know, thiswork is the first to provide a performance prediction of the GP classifier on test data,while previous works focused on predicting training performance. Accurate predictivemodels are generated by posing a symbolic regression task and solving it with GP. These results are achieved by using highly descriptive features and including adimensionality reduction stage that simplifies the learning and testing process. Theproposed approach could be extended to other classification algorithms and used asthe basis of an expert system for algorithm selection...|$|E
40|$|This paper {{introduces}} a fast decision tree based feature classification system for hand gesture recognition and pose estimation. Training {{of the decision}} trees is performed using <b>synthetic</b> data and <b>classification</b> is performed on images of real hands. The presence of each finger is individually classified and gesture classification is performed by parts. The attributes used for training and classification are simple ratios between the foreground and background pixels of the hand silhouette. The system {{does not require the}} hand to be perfectly aligned to the camera or use any special markers or input gloves on the hand...|$|R
40|$|A {{classification}} algorithm, {{called the}} Linear Centralization Classifier (LCC), is introduced. The algorithm seeks {{to find a}} transformation that best maps instances from the feature space to a space where they concentrate towards {{the center of their}} own classes, while maximimizing the distance between class centers. We formulate the classifier as a quadratic program with quadratic constraints. We then simplify this formulation to a linear program that can be solved effectively using a linear programming solver (e. g., simplex-dual). We extend the formulation for LCC to enable the use of kernel functions for non-linear classification applications. We compare our method with two standard classification methods (support vector machine and linear discriminant analysis) and four state-of-the-art classification methods when they are applied to eight standard classification datasets. Our experimental results show that LCC is able to classify instances more accurately (based on the area under the receiver operating characteristic) in comparison to other tested methods on the chosen datasets. We also report the results for LCC with a particular kernel to solve for <b>synthetic</b> non-linear <b>classification</b> problems...|$|R
40|$|To {{the best}} of our knowledge, this paper {{presents}} the first large-scale study that tests whether network categories (e. g., social networks vs. web graphs) are distinguishable from one another (using both categories of real-world networks and <b>synthetic</b> graphs). A <b>classification</b> accuracy of 94. 2 % was achieved using a random forest classifier with both real and synthetic networks. This work makes two important findings. First, real-world networks from various domains have distinct structural properties that allow us to predict with high accuracy the category of an arbitrary network. Second, classifying synthetic networks is trivial as our models can easily distinguish between synthetic graphs and the real-world networks they are supposed to model...|$|R
5000|$|In 1902, he divided science into Theoretical and Practical. Theoretical Science {{consisted}} of Science of Discovery and Science of Review, {{the latter of}} which he also called [...] "Synthetic Philosophy", a name taken from {{the title of the}} vast work, written over many years, by Herbert Spencer. Then, in 1903, he made it a three-way division: Science of Discovery, Science of Review, and Practical Science. In 1903 he characterized Science of Review as:...arranging the results of discovery, beginning with digests, and going on to endeavor to form a philosophy of science. Such is the nature of Humboldt's Cosmos, of Comte's Philosophie positive, and of Spencer's <b>Synthetic</b> Philosophy. The <b>classification</b> of the sciences belongs to this department." ...|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedThis thesis demonstrates that Youth Attitude Tracking Study (YATS) {{data can be}} used to create a <b>synthetic</b> AFQT <b>classification</b> procedure for distinguishing high quality respondents. Unlike previous methods, the procedure does not rely on interest in the military to predict AFQT category. The estimates are based on an analysis of the YATS data matched with the Defense Manpower Data Center cohort data file using a binomial logistic regression model. The market segment analyzed is 17 to 21 year old males who are either high school graduates or prospective high school graduates. The dependent variable is whether or not a respondent would score above the fiftieth percentile on the Armed Forces Qualification Test. The explanatory variables reflect individual demographic, educational and labor market characteristics at the time of YATS interview. The YATS time frame is restricted to 1983 through 1985 in order to facilitate future bridging of YATS models with models estimated with similar time period data from the National Longitudinal Survey of Youth (NLSY). Additionally, the models may be used to provide estimates of AFQT quality for more recent YATS respondents. [URL] United States Marine Corp...|$|R
40|$|Classification {{for very}} large {{datasets}} has many practical applications in data mining. Techniques such as discretization and dataset sampling {{can be used}} to scale up decision tree classifiers to large datasets. Unfortunately, both of these techniques can cause a significant loss in accuracy. We present a novel decision tree classifier called CLOUDS, which samples the splitting points for numeric attributes followed by an estimation step to narrow the search space of the best split. CLOUDS reduces computation and I/O complexity substantially compared to state of the art classifiers, while maintaining the quality of the generated trees in terms of accuracy and tree size. We provide experimental results with a number of real and <b>synthetic</b> datasets. KEYWORDS: <b>Classification,</b> Decision Trees, Data Mining, Large Datasets, Sampling, Estimation, Gini index...|$|R
50|$|While the East African rift {{system is}} {{experiencing}} profound extension the transfer zones that link major extensional faults do not experience this extension. Transfer zones within the East African rift are most commonly in the overlapping stage but every stage has been observed. Here, transfer zones are generally high areas with complex internal fault geometries. These general high areas generally are conjugate divergent type zones {{and have been}} observed in the Tanganyika rift. These broad highs can have vast effect of drainage as they can potentially split basins. Synthetic relay ramp style transfer zones are frequently observed in Lake Malawi. Brief examples have been given but all types within the <b>synthetic</b> and conjugate <b>classification</b> have been observed in the East African rift system.|$|R
40|$|An {{approach}} to semi-supervised learning is proposed {{that is based}} on a Gaussian random field model. Labeled and unlabeled data are represented as vertices in a weighted graph, with edge weights encoding the similarity between instances. The learning problem is then formulated in terms of a Gaussian random field on this graph, where the mean of the field is characterized in terms of harmonic functions, and is efficiently obtained using matrix methods or belief propagation. The resulting learning algorithms have intimate connections with random walks, electric networks, and spectral graph theory. We discuss methods to incorporate class priors and the predictions of classifiers obtained by supervised learning. We also propose a method of parameter learning by entropy minimization, and show the algorithm’s ability to perform feature selection. Promising experimental results are presented for <b>synthetic</b> data, digit <b>classification,</b> and text classification tasks. 1...|$|R
40|$|Pleiotropy, {{the ability}} of a single mutant gene to cause {{multiple}} mutant phenotypes, is a relatively common but poorly understood phenomenon in biology. Perhaps the greatest challenge in the analysis of pleiotropic genes is determining whether phenotypes associated with a mutation result from the loss of a single function or of multiple functions encoded by the same gene. Here we estimate the degree of pleiotropy in yeast by measuring the phenotypes of 4710 mutants under 21 environmental conditions, finding that it is significantly higher than predicted by chance. We use a biclustering algorithm to group pleiotropic genes by common phenotype profiles. Comparisons of these clusters to biological process <b>classifications,</b> <b>synthetic</b> lethal interactions, and protein complex data support the hypothesis that this method can be used to genetically define cellular functions. Applying these functional classifications to pleiotropic genes, we are able to dissect phenotypes into groups associated with specific gene functions...|$|R
40|$|Many {{classification}} algorithms {{have been}} successfully deployed in security-sensitive applications including spam filters and intrusion detection systems. Under such adversarial environments, adversaries can generate exploratory attacks against the defender such as evasion and reverse engineering. In this paper, we discuss why reverse engineering attacks {{can be carried out}} quite efficiently against fixed classifiers, and investigate the use of randomization as a suitable strategy for mitigating their risk. In particular, we derive a semidefinite programming (SDP) formulation for learning a distribution of classifiers subject to the constraint that any single classifier picked at random from such distribution provides reliable predictions with a high probability. We analyze the tradeoff between variance of the distribution and its predictive accuracy, and establish that one can almost always incorporate randomization with large variance without incurring a loss in accuracy. In other words, the conventional approach of using a fixed classifier in adversarial environments is generally Pareto suboptimal. Finally, we validate such conclusions on both <b>synthetic</b> and real-world <b>classification</b> problems. Copyright 2014 ACM...|$|R
40|$|This chapter {{focuses on}} an {{important}} yet neglected aspect of material culture: {{the appearance of}} new, made-to-measure materials that have begun to colonise the physical world. Within the social and historical sciences we have barely begun to theorize materiality beyond the fetish, an approach that ignores the full import of the material composition of objects. Everything we hold and everything we see takes shape thanks to materials, {{but it is the}} social identities we construct for materials that lead to their selection or rejection in manufacturing and influences the adoption or refusal by consumers of the resulting products. Through an exploration of the development and social reception of the first <b>synthetic</b> materials, material <b>classification,</b> and the relevance of the prototype to design practice, together with a consideration of how fully made-to-measure materials relate to these phenomena, we aim to present the reader with an outline of crucial changes that are reshaping the way we conceive of and interact with the material world...|$|R
500|$|The {{classification}} {{was originally}} enumerative, {{meaning that it}} listed all of the classes explicitly in the schedules. Over time it added some aspects of a faceted classification scheme, allowing classifiers to construct a number by combining a class number for a topic with an entry from a separate table. Tables cover commonly used elements such as geographical and temporal aspects, language, and bibliographic forms. For example, a class number could be constructed using 330 for economics + [...]9 for geographic treatment + [...]04 for Europe to create the class 330.94 European economy. Or one could combine the class 973 for United States + [...]05 for periodical publications on the topic {{to arrive at the}} number 973.05 for periodicals concerning the United States generally. The classification also makes use of mnemonics in some areas, such that the number 5 represents the country Italy in classification numbers like 945 (history of Italy), 450 (Italian language), 195 (Italian philosophy). The combination of faceting and mnemonics makes the <b>classification</b> <b>synthetic</b> in nature, with meaning built into parts of the classification number.|$|R
40|$|Although, {{computational}} Grid {{has been}} initially developed to solve large-scale scientific research problems, it is extended for {{commercial and industrial}} applications. An interesting commercial application with a wide impact {{on a variety of}} fields, is 3 D rendering, In order to implement, however, 3 D rendering to a grid infrastructure, we should develop appropriate scheduling and resource allocation mechanisms so that the negotiated Quality of Service (QoS) requirements are met. Efficient scheduling schemes require modeling and prediction of rendering workload. This is addressed in this paper, based on a combined fuzzy classification and neural network model. Initially, appropriate descriptors are extracted to represent the <b>synthetic</b> world. Fuzzy <b>classification</b> is used for organizing rendering descriptor so that a reliable representation is accomplished which increases the prediction accuracy. Neural network performs workload prediction by modeling the non-linear input-output relationship between rendering descriptors and the respective computational complexity. To increase the prediction accuracy, a constructive algorithm is adopted in this paper to train the neural network so that network weights and size are simultaneously estimated...|$|R
5000|$|The {{classification}} {{was originally}} enumerative, {{meaning that it}} listed all of the classes explicitly in the schedules. Over time it added some aspects of a faceted classification scheme, allowing classifiers to construct a number by combining a class number for a topic with an entry from a separate table. Tables cover commonly used elements such as geographical and temporal aspects, language, and bibliographic forms. For example, a class number could be constructed using 330 for economics + [...]9 for geographic treatment + [...]04 for Europe to create the class 330.94 European economy. Or one could combine the class 973 for United States + [...]05 for periodical publications on the topic {{to arrive at the}} number 973.05 for periodicals concerning the United States generally. The classification also makes use of mnemonics in some areas, such that the number 5 represents the country Italy in classification numbers like 945 (history of Italy), 450 (Italian language), 195 (Italian philosophy). The combination of faceting and mnemonics makes the <b>classification</b> <b>synthetic</b> in nature, with meaning built into parts of the classification number.|$|R
40|$|Adopting a Bayesian {{approach}} and sampling the network parameters from their posterior distribution {{is a rather}} novel and promising method for improving the generalisation performance of neural network predictors. The present empirical study applies this scheme {{to a set of}} different <b>synthetic</b> and real-world <b>classification</b> problems. The paper focuses on the dependence of the prediction results on the prior distribution of the network parameters and hyperparameters, and provides a critical evaluation of the automatic relevance determination (ARD) scheme for detecting irrelevant inputs. 1 Introduction Consider a K-fold classification problem, where an m-dimensional feature vector x t is assigned to one of K classes fC 1; : : :; CK g indicated by a label vector y t = (y 1 t; : : :; y K t); y k t = 1 if x t 2 C k; y k t = 0 if x t 62 C k; ky t k = 1 : For a neural network (NN) with K softmax 1 units in the final layer, the network outputs f k (x t; w) 2 [0; 1] can be interp [...] ...|$|R
