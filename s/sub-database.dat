34|31|Public
5000|$|Other {{parts of}} speech tagging for <b>sub-database,</b> such as common nouns, numerals, numeral classifiers, {{different}} types of verbs, and of adjectives, pronouns, adverbs, prepositions, conjunctions, particles marking mood, onomatopoeia, interjection, etc.|$|E
40|$|Problem statement: The {{agricultural}} {{industry is a}} major user of energy. Energy is used directly for operating {{agricultural machinery and equipment}} on the farm and indirectly in the manufacturing of fertilizers and pesticides and processing of agricultural products off the farm. In order to reduce the cost of agricultural production, energy uses on the farm must be identified and optimized using modern tools. Approach: A comprehensive and easy to use computer program was developed for the purpose of determining the farm energy requirements with the aim of reducing costs and maximizing profit. The program includes a main database composed of nine sub-databases: Tractors <b>sub-database,</b> agricultural machinery <b>sub-database,</b> pumps <b>sub-database,</b> stationary engines <b>sub-database,</b> planting dates <b>sub-database,</b> soil <b>sub-database,</b> operating variables of farm operations <b>sub-database,</b> draft and power equations <b>sub-database</b> and water requirement <b>sub-database.</b> The program was designed with visual C++ language. Results: The program was tested by comparing its results with the manually calculated results. The results showed that the program worked properly. The developed program was also illustrated using an example farm to show the different stages of determining the required farm energy. Conclusion: The program can be used: To determine the farm energy requirements, to assess the current status of farms in terms of energy use efficiency, for future planning of modern farms and as an educational tool. It has many advantages including: Ease of use when dealing with input through interactive windows, ease of addition or deletion or updating of sub-databases, ease of exploring the program windows and the potential for further future development of any part of the program. The program is unique as it includes all the information in a database and has a multi dimensional uses including: Evaluation of an existing system, selecting new machinery based on an optimum energy use and future planning of farm operations...|$|E
30|$|FDB 1 -A, FDB 2 -A, FDB 3 -A, and FDB 4 -A, {{where each}} <b>sub-database</b> stores 140 {{fingerprint}} samples.|$|E
5000|$|... 9. Renminbi DatabaseThe Renminbi (RMB) {{database}} {{is focused}} on relevant materials towards RMB research. Content includes RMB reports from academic journals, books, research papers, policy reports, research meetings, and syllabus that are within the <b>sub-databases</b> of the China Economic Databases platform.|$|R
30|$|Where, B <b>sub-databases</b> {{contain the}} most {{difficult}} fingerprint images used for evaluating protection strength of the proposed scheme. We generated a login database of size 150 UID’s and PWD’s, and an adult bank database of size 450000 records using GNU-licensed open source data generator tool [56].|$|R
40|$|CrSP数据库是美国芝加哥大学开发的金融数据库。介绍了CrSP包含的各子数据库及其内容,并以CrSP自行开发的图形化界面平台CrSPSIfT和WrdS网络查询界面为例,介绍了CrSP数据库的检索方法及个性化功能的使用技巧。CRSP Database is a {{financial}} database {{developed by the}} University of Chicago in USA. The paper introduces the component <b>sub-databases</b> of CRSP,and takes graphic interface platform CRSPSift and online inquiring interface WRDS for examples to expound the retrieval methods of CRSP Database and use skills of its personalized functions...|$|R
30|$|FDB 1 -B, FDB 2 -B, FDB 3 -B, and FDB 4 -B, {{where each}} <b>sub-database</b> stores ten very {{difficult}} fingerprint samples.|$|E
3000|$|... where {{distance}}(x,y) is {{the distance}} between x and y. N(x,k) is the set of k nearest neighbors with respect to x. |N(x,y)| {{is the number of}} RSSI data in N(x,k).The pseudo-code of <b>sub-database</b> construction is shown in Table 1.|$|E
40|$|Most of {{the current}} feed {{evaluation}} systems used to estimate dairy cow requirements and formulate diets cannot predict the effects of dietary changes on milk composition, in particular on milk fat content (MFC) and milk fat yield (MFY) responses. Dietary changes alter dairy cow MFC and MFY through modifications in the supply of nutrients, which act as precursors or inhibitors of mammary fat synthesis. Taking into account the supply of nutrients could allow feed evaluation systems to predict milk composition responses to dietary changes. We described an empirical model based on nutrient flows to predict MFC and MFY responses to dietary changes. This model was built by coupling a set of published empirical equations estimating nutrient flows (acetate, propionate, butyrate, glucose and digestible protein) from dietary characteristics and the measured ruminal VFA proportions to another set of equations estimating the response of milk fat to the supply of these nutrients. The model was evaluated by comparing the predicted and observed MFC and MFY responses on three databases derived from published feeding studies in dairy cows. The databases compiled published trials involving {{a change in the}} forage-to-concentrate ratio (‘F:C ratio’ <b>sub-database),</b> a change in the carbohydrate source (‘CHO source’ <b>sub-database)</b> or a change in both F:C ratio and CHO source (‘ratio and source’ <b>sub-database).</b> Overall, the current version of the model did not accurately predict milk fat responses: the model did not enable to explain the MFC and MFY responses in the ‘CHO source’ <b>sub-database</b> (P> 0. 10 and R 2 = 0. 02); in the ‘F:C ratio’ <b>sub-database,</b> agreement between observed and predicted responses was better (the slopes were not different from unity and were respectively 0. 84 and 0. 94 for MFC and MFY), but R 2 were low (R 2 = 0. 36 for MFC and R 2 = 0. 43 for MFY). In the ‘ratio and source’ database, MFY was quite correctly predicted (the slope was 0. 93 and R 2 = 0. 42), but not the MFC responses (P> 0. 10 and R 2 = 0. 01). The mean errors of prediction for MFC were 2. 61, 3. 79 and 3. 55 g/kg, respectively in the ‘F:C ratio’, ‘CHO source’ and the ‘ratio and source’ sub-databases. The effects of several interfering factors related to cows, diets or experimental conditions were tested to explain differences between observed and predicted responses in each <b>sub-database.</b> They enabled to identify several potential sources of bias in the model: especially, bias in estimation of OM digestibility, bias in estimation of glucose effects, lack of the effects of fatty acids in the model. These constitute interesting ways to improve the model...|$|E
40|$|The KMDB/MutationView is a {{graphical}} {{database of}} mutations in human disease-causing genes and its current version consists of nine category-based <b>sub-databases</b> including diseases of eye, heart, ear, brain, cancer, syndrome, autoimmunity, muscle and blood. The KMDB/MutationView stores mutation data of 97 genes involved in 87 different disease and is accessible through [URL]...|$|R
50|$|A {{distributed}} database, however, is {{a database}} {{in which all}} the information is stored on multiple physical locations. Distributed databases are divided into two groups: homogeneous and heterogeneous. It relies on replication and duplication within its multiple <b>sub-databases</b> in order to maintain its records up to date. It is composed of multiple database files, all controlled by a central DBMS.|$|R
40|$|Automatic {{recognition}} of human motions has an increasing {{demand in the}} recent visionary world. However, with the registration of large number of motions from varying viewpoints, the necessity for an effective motion database for recognition has be-come a vital issue. In the context of motion database development, this paper proposes a directional database organization for human motion recognition. This organization parti-tions the motion database into several <b>sub-databases</b> {{on the basis of}} camera orientation. Separate feature spaces are constructed, and correspondingly directional <b>sub-databases</b> are built, leading to the constitution of the complete motion database. The directionally similar but semantically different motions are properly distinguished. To show the ro-bustness of the proposed organization for recognizing human motions, a set of motions captured from varying viewpoints is analyzed. An eigenspace representation is employed as a generic feature space that sufficiently characterizes the motion features. Motion His-tory Image (MHI) and Exclusive-OR (XOR) image representations are used as motion templates where MHI is found performing better than XOR image. The experimental re-sults show high-level of satisfactory performance and claim the signi_cant improvement over earlier developed systems...|$|R
30|$|In this section, {{we present}} results on {{fingerprint}} database FVC 2004 database. This database has four sub-databases: DB 1, DB 2, DB 3, and DB 4. Each <b>sub-database</b> consists of fingerprint impressions obtained from 100 non-habituated, cooperative subjects, and every subject {{was asked to}} provide eight impressions of the same finger.|$|E
40|$|In this paper, {{we propose}} two palmprint {{identification}} schemes using fusion strategy. In the first fusion scheme, firstly, the principal lines of test image is extracted, and matched {{with that of}} all training images. Secondly, those training images with large matching scores are selected to construct a small training <b>sub-database.</b> At last, the decision level fusion, combing matching scores of principal lines and Locality Preserving Projections features, has been made for final identification in small training <b>sub-database.</b> From another point of view, {{it can be seen}} that the fusion is restricted by the previous results of principal lines matching, so we call it as restricted fusion. The second fusion scheme is similar to the first one. Just the fusion order is changed. The results of experiments conducted on PolyU palmprint database show that the proposed schemes can achieve 100 % accurate recognition rate. Department of Civil and Environmental Engineerin...|$|E
3000|$|When {{we carry}} out our experiments, each <b>sub-database</b> {{is divided into two}} subsets called {{template}} and query databases. Images selected by using template selection methods constitute the template database and the remaining images of the finger constitute the query database. A maximum matching score is chosen from all scores between a query fingerprint and templates as final score. We perform a comparison among the following methods for the same template selection: [...]...|$|E
40|$|Abstract: Statistical {{analysis}} of protein-protein interfaces in {{a database of}} pure peptide crystals shows that {{the distribution of the}} contact area contains two components: a major exponential distribution and a minor flat distribution. Analysis of two <b>sub-databases</b> provides evidence that the two components represent specific and non-specific contacts, respectively. The probability of an interface with a given area being specific can be estimated. A scaled quantity (contact ratio) is introduced that is more useful than contact area for discriminating specific and non-specific contacts in protein crystals...|$|R
40|$|EPOS² is a kernel Software Engineering Environment. It has {{emphasis}} on Process Modelling, based on object-oriented principles and on cooperating transactions against a uniformly versioned EPOSDB. EPOS-PM {{relies on the}} SPELL process modelling language. This extends the underlying EPOSDB data model with full object-orientation, type-level properties, tasking, and meta-types to allow reflection. SPELL {{can be used to}} model, plan, execute, and evolve software activities; and associated products, tools, human roles, and projects. The SPELL types in a Process Schema can be customised through subtyping and evolved in project-specific, versioned <b>sub-databases...</b>|$|R
40|$|JASPAR is {{a popular}} open-access {{database}} for matrix models describing DNA-binding preferences for transcription factors and other DNA patterns. With its third major release, JASPAR has been expanded and equipped with additional functions aimed at both casual and power users. The heart of the JASPAR database—the JASPAR CORE sub-database—has increased by 12 % in size, and three new specialized <b>sub-databases</b> have been added. New functions include clustering of matrix models by similarity, generation of random matrices by sampling from selected sets of existing models and a language-independent Web Service applications programming interface for matrix retrieval. JASPAR is available at [URL]...|$|R
40|$|We {{developed}} a fast method to construct local sub-databases from the NCBI-nr database for the quick similarity search and annotation of huge metagenomic datasets based on BLAST-MEGAN approach. A three-step <b>sub-database</b> annotation pipeline (SAP) was further proposed {{to conduct the}} annotation {{in a much more}} time-efficient way which required far less computational capacity than the direct NCBI-nr database BLAST-MEGAN approach. The 1 (st) BLAST of SAP was conducted using the original metagenomic dataset against the constructed <b>sub-database</b> for a quick screening of candidate target sequences. Then, the candidate target sequences identified in the 1 (st) BLAST were subjected to the 2 (nd) BLAST against the whole NCBI-nr database. The BLAST results were finally annotated using MEGAN to filter out those mistakenly selected sequences in the 1 (st) BLAST to guarantee the accuracy of the results. Based on the tests conducted in this study, SAP achieved a speedup of ~ 150 - 385 times at the BLAST e-value of 1 e- 5, compared to the direct BLAST against NCBI-nr database. The annotation results of SAP are exactly in agreement with those of the direct NCBI-nr database BLAST-MEGAN approach, which is very time-consuming and computationally intensive. Selecting rigorous thresholds (e. g. e-value of 1 e- 10) would further accelerate SAP process. The SAP pipeline may also be coupled with novel similarity search tools (e. g. RAPsearch) other than BLAST to achieve even faster annotation of huge metagenomic datasets. Above all, this <b>sub-database</b> construction method and SAP pipeline provides a new time-efficient and convenient annotation similarity search strategy for laboratories without access to high performance computing facilities. SAP also offers a solution to high performance computing facilities for the processing of more similarity search tasks...|$|E
40|$|Abstract. Study {{automotive}} {{transmission shaft}} abnormal noise control with engineering database, {{and improve the}} abnormal noise control process using experimental analysis, CAE technical analysis, sound quality and other professional knowledge. Then analyze the process, and establish the appropriate <b>sub-database.</b> Database management system organizes the various modules and promotes exchange of the user and the database with Human-Computer interface. On this basis it reveals the necessity of combination of engineering problems and database. Besides, it is verified that the entire process is effective relying on a miniature car. 1...|$|E
40|$|International audienceA {{new method}} called the self-optimized {{prediction}} method (SOPM) {{has been developed}} to improve the success rate in the prediction of the secondary structure of proteins. This new method has been checked against an updated release of the Kabsch and Sander database, 'DATABASE. DSSP', comprising 239 protein chains. The first step of the SOPM is to build sub-databases of protein sequences and their known secondary structures drawn from 'DATABASE. DSSP' by (i) making binary comparisons of all protein sequences and (ii) {{taking into account the}} prediction of structural classes of proteins. The second step is to submit each protein of the <b>sub-database</b> to a secondary structure prediction using a predictive algorithm based on sequence similarity. The third step is to iteratively determine the predictive parameters that optimize the prediction quality on the whole <b>sub-database.</b> The last step is to apply the final parameters to the query sequence. This new method correctly predicts 69 % of amino acids for a three-state description of the secondary structure (alpha helix, beta sheet and coil) in the whole database (46, 011 amino acids). The correlation coefficients are C alpha = 0. 54, C beta = 0. 50 and Cc = 0. 48. Root mean square deviations of 10 % in the secondary structure content are obtained. Implications for the users are drawn so as to derive an accuracy at the amino acid level and provide the user with a guide for secondary structure prediction. The SOPM method is available by anonymous ftp to ibcp. fr...|$|E
40|$|The central {{hypothesis}} {{of this paper}} is that database design and systems design in general can be simplified considerably by tailoring the design methods to a suitable range of applications. Domain-specific knowledge can be incorporated into a specialized database architecture that leaves the designer with the task to specify only the application-specific parts. Based on an analysis of business constraints, we propose such an architecture for the domain of business transaction processing. The architecture offers several data and transaction management services, special-purpose <b>sub-databases,</b> and design checking rules to be used by the application designer. Two services, input management and audit and control services, are described in more detail. ...|$|R
40|$|A {{component}} with an optimized {{combination of}} different materials (including homogeneous and heterogeneous materials) in its different regions {{for a specific}} application is termed as a component made of a multiphase perfect material (CMMPM). For selecting suitable materials for CMMPMs in their material design, a material database, which covers homogeneous materials and different types of heterogeneous materials, is needed. This paper introduces the development of such a database, including the determination of its design requirements, the selection of its database management system, the creation of its database model, its development method, and the details of its <b>sub-databases</b> for composite materials, functionally gradient materials, and heterogeneous materials with a periodic microstructure, respectively. © 2007 Elsevier Ltd. All rights reserved. link_to_subscribed_fulltex...|$|R
5000|$|Jim Gray, {{a towering}} figure within the {{database}} community, analyzed multi-primary replication schemes under the transactional model and ultimately published a widely cited paper {{skeptical of the}} approach [...] "The Dangers of Replication and a Solution". In a nutshell, he argued that unless data splits in some natural way so that the database can be treated as n disjoint <b>sub-databases,</b> concurrency control conflicts will result in seriously degraded performance and the group of replicas will probably slow down {{as a function of}} n. Indeed, he suggests that the most common approaches are likely to result in degradation that scales as O(n³). His solution, which is to partition the data, is only viable in situations where data actually has a natural partitioning key.|$|R
40|$|In {{this paper}} an attempt {{has been made}} to develop a {{progressive}} partitioning and counting inference approach for mining association rules in temporal databases. A temporal database like a sales database is a set of transactions where each transaction T is a set of items in which each item contains an individual exhibition period. The existing models of association rule mining have problems in handling transactions {{due to a lack of}} consideration of the exhibition period of each individual item and lack of an equitable support counting basis for each item. As a remedy to this problem we propose an innovative algorithm PPCI that combines progressive partition approach with counting inference method to discover association rules in a temporal database. The basic idea of PPCI is to first segment the database into sub-databases in such a way that items in each <b>sub-database</b> will have either a common starting time or a common ending time. Then for each <b>sub-database,</b> PPCI progressively filters 1 -itemset with a cumulative filtering threshold based on vital partitioning characteristics. Algorithm PPCI is also designed to employ a filtering threshold in each partition to prune out those cumulatively infrequent 1 -itemsets early and it also uses counting inference approach to minimise as much as possible the number of pattern support counts performed when extracting frequent patterns. Explicitly the execution time of PPCI in order of magnitude is smaller than those required by the schemes which are directly extended from existing methods. Temporal association rule, exhibition period, frequent itemset, counting inference...|$|E
40|$|The {{content of}} the current 2003 version, GEISA/IASI- 03, of the computer-accessible spectroscopic database, GEISA (Gestion et Etude des Informations Spectroscopiques Atmosphériques: Management and Study of Atmospheric Spectroscopic Information) /IASI (Infrared Atmospheric Sounder Interferometer) is described. The GEISA/IASI- 03 system {{comprises}} three independent spectroscopic archives related with: an individual line transition spectroscopic parameters <b>sub-database</b> which contains 14 molecules, i. e. : H 2 O, CO 2, O 3, N 2 O, CO, CH 4, O 2, NO, SO 2, NO 2, HNO 3, OCS, C 2 H 2, N 2, representing 51 isotopomers and 702, 550 entries, in the spectral range 599 - 3001 cm- 1; an absorption cross-sections <b>sub-database</b> which comprehends 6, 572, 329 entries related to 6 molecules, i. e. : CFC- 11, CFC- 12, CFC- 14, HCFC- 22, N 2 O 5, CCl 4; a catalog on micro-physical and optical properties of atmospheric aerosols, mainly refractive indices. The modifications and improvements, made since former editions in terms of database content and management, are detailed. GEISA/IASI is elaborated {{with the purpose of}} assessing the IASI (Infrared Atmospheric Sounding Interferometer: [URL] measurements capabilities, within the ISSWG (IASI Sounding Science Working Group), in the frame of the CNES (Centre National d'Etudes Spatiales, France) /EUMETSAT (EUropean organization for the exploitation of METeorological SATellites) European Polar System (EPS) preparation. All the archived data can be handled though general and user friendly associated management software facilities, which are interfaced on the ARA (Atmospheric Radiation Analysis) /LMD (Laboratoire de Météorologie Dynamique) group web site at: [URL]...|$|E
40|$|It {{presents}} an SPFA(Standing for Segmented Progressive Filter Algorithm). The basic idea behind SPFA is to first segment the database into sub-databases {{in such a}} way that item in each <b>sub-database</b> will have either the common starting time or the common ending time. Then, for each sub-databse, SPFA progressively filters candidate 2 -itemsets with cumulative filtering thresholds either forward or backward in time. This feature allows SPFA of adopting the scan reduction technique by generating all candidate k-itemsets from candidate 2 -itemsets directly. The experimental results show that SPFA significantly outperforms other schemes which are extended from prior methods in terms of the execution time and scalability. The advantage of SPFA becomes even more prominent as the size of the database increases...|$|E
40|$|Abstract. We {{present a}} simple and {{intuitive}} extension GCWA G of the generalized closed world assumption (GCWA) from positive disjunctive deductive databases to general disjunctive deductive databases (with default negation). This semantics is {{defined in terms of}} unfounded sets and possesses an argumentation-theoretic characterization. We also provide a top-down procedure for GCWA G, which is sound and complete with respect to GCWA G. We investigate two query evaluation methods for GCWA G: database partition, and database splitting. The basic idea of these methods is to divide the original deductive database into several smaller <b>sub-databases</b> and the query evaluation in the original database is transformed into the problem of query evaluation in smaller or simplified components. We prove that these two methods of query evaluation are all sound with respect to GCWA G...|$|R
40|$|We {{present a}} simple and {{intuitive}} extension GCWA G of the generalized closed world assumption (GCWA) from positive disjunctive deductive databases to general disjunctive deductive databases (with default negation). This semantics is {{defined in terms of}} unfounded sets and possesses an argumentation-theoretic characterization. We also provide a top-down procedure for GCWA G, which is sound and complete with respect to GCWA G. We investigate two query evaluation methods for GCWA G : database partition, and database splitting. The basic idea of these methods is to divide the original deductive database into several smaller <b>sub-databases</b> and the query evaluation in the original database is transformed into the problem of query evaluation in smaller or simplified components. We prove that these two methods of query evaluation are all sound with respect to GCWA G. Keywords: disjunctive deductive databases, closed world assumption, semantics, query evaluation, argumentation 1...|$|R
40|$|Abstract—This paper {{presents}} a novel human skin color classification into skin color tones: White and Black. This is performed {{by developing a}} skin color classifier based on pixel-based classification using RGB model. Our proposed method is classified {{under the category of}} an explicitly defined skin region model. The skin classifier divides our database formed by some images from the FERET set of faces into two <b>sub-databases</b> according to the skin color. The skin color classification method is then applied on a face recognition technique by reducing the number of trained images in the matching process. The performance of the proposed human skin color classifier is evaluated perceptually. Experimental results showed that our proposed skin color classifier is able to classify a face into its possible skin color tone and reaches 87 % as hit rate. Keywords-Skin color classification; Face recognition; Pixel-based classification; RGB model...|$|R
40|$|Abstract: A {{distributed}} database {{system is}} a combination of <b>sub-database</b> separated over many sites communicated through a network. Deadlock {{is one of the most}} common problems that occur in distributed database implementation. Deadlock occurs when a multiple transaction locks the same data sources and every transaction waits for other to release. Deadlock detection and resolution is not easy in a distributed database system, because such system is composed of more than one site communicated to central database. The main objective of this study is to analyze several proposed algorithms to detect and resolve the deadlock in distributed database and propose a new technique to avoid the drawbacks of previous algorithms, besides enhancing the efficiency of detection and resolving the deadlock in distributed database...|$|E
40|$|The updated 2009 {{edition of}} the spectroscopic {{database}} GEISA (Gestionet Etudedes Informations Spectroscopiques Atmospheriques; Management and Study of Atmospheric Spectroscopic Information) is described in this paper. GEISA is a computer-accessible system comprising three independent sub-databases devoted, respectively, to: line parameters, infrared and ultraviolet/visible absorption cross-sections, microphysical and optical properties of atmospheric aerosols. In this edition, 50 molecules {{are involved in the}} line parameters <b>sub-database,</b> including 111 isotopologues, for a total of 3, 807, 997 entries, in the spectral range from 10 - 6 to 35, 877. 031 cm- 1. GEISA, continuously developed and maintained at LMD (Laboratoirede Meteorologie Dynamique, France) since 1976, is implemented on the IPSL/CNRS(France) ‘‘Ether’’ Products and Services Centre WEB site ([URL] where all archived spectroscopic data can be handled through general and user friendly associated managements of software facilities. More than 350 researchers are registered for online use of GEISA...|$|E
40|$|The {{problem of}} {{discovering}} association rules between items in a database is an emerging area of research. Its {{goal is to}} extract significant patterns or interesting rules from large databases. Recent studies of mining association rules have proposed a closure mechanism. It is no longer necessary to mine the set {{of all of the}} frequent itemsets and their association rules. Rather, it is sufficient to mine the frequent closed itemsets and their corresponding rules. In the past, a number of algorithms for mining frequent closed itemsets have been based on items. In this paper, we use the transaction itself for mining frequent closed itemsets. An efficient algorithm called FCILINK is proposed that is based on a link structure between transactions. A given database is scanned once and then a much smaller <b>sub-database</b> is scanned twice. Our experimental results show that our algorithm is faster than previously proposed methods. Furthermore, our approach is significantly more efficient for dense databases. Association rules, closed itemsets, link structure...|$|E
40|$|Discovery of {{sequential}} patterns {{is becoming}} increasingly useful and essential in many scientific and commercial domains. Enormous sizes of available datasets and possibly large number of mined patterns demand efficient, scalable, and parallel algorithms. Even though a number of algorithms {{have been developed to}} efficiently parallelize frequent pattern discovery algorithms that are based on the candidategeneration-and-counting framework, the problem of parallelizing the more efficient projection-based algorithms has received relatively little attention and existing parallel formulations have been targeted only toward shared-memory architectures. The irregular and unstructured nature of the task-graph generated by these algorithms and the fact that these tasks operate on overlapping <b>sub-databases</b> makes it challenging to efficiently parallelize these algorithms on scalable distributed-memory parallel computing architectures. In this paper we present and study a variety of distributed-memory parallel algorithms for a tree-projection-based frequent sequence discovery algorithm that are able to minimize the various overheads associate...|$|R
40|$|Currently {{working at}} Asahi Kasei Cooperation For {{the purpose of}} {{building}} speech synthesis system that can generate high-quality speech with wide range in prosody and realize fine prosody control, we propose new speech database constructing method. As a speech synthesis method, we select a hybrid system which consists of two part: speech unit selection and prosody modification part by STRAIGHT (vocoder type high quality analysis-synthesis method). Our viewpoint for designing database is to reduce amount of prosody modification. which causes quality deterioration. Hence, {{to make it possible}} to generate arbitrary prosody within permissible range of prosody modification, we designed 9 <b>sub-databases</b> those consist of same phonetic balanced text set with different prosody. In this paper, we report the designing method and general features of obtained databases. Listening tests focused on durational fearure were also conducted. The results show effectiveness of the method and the necessity to change unit selection cost according to speech rate. 1...|$|R
40|$|LREC 2002 : International Conference on Language Resources and Evaluation, May 29 - 31, 2002, Las Palmas, Canary Islands, Spain. For {{the purpose}} of {{building}} speech synthesis system that can generate high-quality speech with wide range in prosody and realize fine prosody control, we propose new speech database constructing method. As a speech synthesis method, we select a hybrid system which consists of two part : speech unit selection and prosody modification part by STRAIGHT (vocoder type high quality analysis-synthesis method). Our viewpoint for designing database is to reduce amount of prosody modification. which causes quality deterioration. Hence, {{to make it possible}} to generate arbitrary prosody within permissible range of prosody modification, we designed 9 <b>sub-databases</b> those consist of same phonetic balanced text set with different prosody. In this paper, we report the designing method and general features of obtained databases. Listening tests focused on durational fearure were also conducted. The results show effectiveness of the method and the necessity to change unit selection cost according to speech rate...|$|R
