134|10000|Public
5000|$|Partition [...] into [...] subsets [...] {{with each}} element of [...] {{belonging}} {{to one of}} the [...] with equal probability (<b>sampling</b> <b>with</b> <b>replacement)</b> ...|$|E
5000|$|From this perspective, {{the case}} labeled [...] "Surjective f" [...] is {{somewhat}} strange: Essentially, we keep <b>sampling</b> <b>with</b> <b>replacement</b> until we've chosen each item at least once. Then, we count how many choices we've made, and if it's not equal to N, {{throw out the}} entire set and repeat. This is vaguely comparable to the coupon collector's problem, where the process involves [...] "collecting" [...] (by <b>sampling</b> <b>with</b> <b>replacement)</b> a set of X coupons until each coupon has been seen at least once. Note that in all [...] "surjective" [...] cases, the number of sets of choices is zero unless N ≥ X.|$|E
5000|$|The {{conditional}} {{density of}} the object at the current time [...] is estimated as a weighted, time-indexed sample set [...] with weights [...] N is a parameter determining the number of sample sets chosen. A realization of [...] is obtained by <b>sampling</b> <b>with</b> <b>replacement</b> from the set [...] with probability equal to the corresponding element of [...]|$|E
5000|$|<b>Sample,</b> <b>with</b> <b>replacement,</b> [...] {{training}} {{examples from}} , call these , [...]|$|R
5000|$|<b>Sample</b> <b>with</b> <b>replacement</b> N {{times from}} the set [...] with {{probability}} [...] to generate a realization of [...]|$|R
2500|$|We take a [...] <b>sample</b> <b>with</b> <b>replacement</b> of n values y1,...,y'n {{from the}} population, where n<nbsp&N, and {{estimate}} the variance {{on the basis}} of this sample. Directly taking the variance of the sample data gives the average of the squared deviations: ...|$|R
50|$|In statistics, {{bootstrapping}} is any test or metric {{that relies}} on random <b>sampling</b> <b>with</b> <b>replacement.</b> Bootstrapping allows assigning measures of accuracy (defined in terms of bias, variance, confidence intervals, prediction error or some other such measure) to sample estimates. This technique allows estimation of the sampling distribution of almost any statistic using random sampling methods. Generally, it falls in the broader class of resampling methods.|$|E
5000|$|Given a {{standard}} training set [...] of size n, bagging generates m new training sets , each of size n′, by sampling from D uniformly and with replacement. By <b>sampling</b> <b>with</b> <b>replacement,</b> some observations may be repeated in each [...] If n′=n, then for large n the set [...] {{is expected to}} have the fraction (1 - 1/e) (≈63.2%) of the unique examples of D, the rest being duplicates. This kind of sample is known as a bootstrap sample. The m models are fitted using the above m bootstrap samples and combined by averaging the output (for regression) or voting (for classification).|$|E
5000|$|In small {{populations and}} often in large ones, such {{sampling}} is typically done [...] "without replacement", i.e., one deliberately avoids choosing {{any member of}} the population more than once. Although simple random sampling can be conducted with replacement instead, this is less common and would normally be described more fully as simple random sampling with replacement.Sampling done without replacement is no longer independent, but still satisfies exchangeability, hence many results still hold. Further, for a small sample from a large population, sampling without replacement is approximately the same as <b>sampling</b> <b>with</b> <b>replacement,</b> since the odds of choosing the same individual twice is low.|$|E
30|$|We {{produced}} {{logistic regression}} models for 1000 bootstrapped <b>samples,</b> <b>sampled</b> <b>with</b> <b>replacement,</b> from our dataset (Fox 2002). We averaged these models {{to produce a}} final validation model to compare the ORs, 95  % CIs, p values, and the AIC with our prediction model.|$|R
5000|$|The {{training}} algorithm for random forests {{applies the}} general technique of bootstrap aggregating, or bagging, to tree learners. Given a training set [...] = , ..., [...] with responses [...] = , ..., , bagging repeatedly (B times) selects a random <b>sample</b> <b>with</b> <b>replacement</b> {{of the training}} set and fits trees to these samples: ...|$|R
40|$|Equiprobable <b>samples</b> <b>with</b> <b>replacements</b> from Їnite Abelian {{groups are}} con- sidered. Limit theorems are proved {{describing}} convergence {{of the distribution}} of the number of ordered subsamples meeting speciЇed linear relations to Poisson distributions. Basing on these theorems we construct a goodness-of-Їt test which checks the hypothesis on the uniform distribution of sample elements...|$|R
5000|$|In the {{terminology}} below, {{the case of}} <b>sampling</b> <b>with</b> <b>replacement</b> is termed [...] "Any f", while the case of sampling without replacement is termed [...] "Injective f". Each box indicates how many different sets of choices there are, in a particular sampling scheme. The row labeled [...] "Distinct" [...] means that the ordering matters. For example, if we have ten items, of which we choose two, then the choice (4,7) is different from (7,4). On the other hand, the row labeled [...] "Sn orders" [...] means that ordering doesn't matter: Choice (4,7) and (7,4) are equivalent. (Another way {{to think of this}} is to sort each choice by the item number, and throw out any duplicates that result.) In terms of probability distributions, <b>sampling</b> <b>with</b> <b>replacement</b> where ordering matters is comparable to describing the joint distribution of N separate random variables, each with an X-fold categorical distribution. The case where ordering doesn't matter, however, is comparable to describing a single multinomial distribution of N draws from an X-fold category, where only the number seen of each category matters. The case where ordering doesn't matter and sampling is without replacement is comparable to a single multivariate hypergeometric distribution, and the fourth possibility {{does not seem to have}} a correspondence. Note that in all the [...] "injective" [...] cases (i.e. sampling without replacement), the number of sets of choices is zero unless N ≤ X. ("Comparable" [...] in the above cases means that each element of the sample space of the corresponding distribution corresponds to a separate set of choices, and hence the number in the appropriate box indicates the size of the sample space for the given distribution.) ...|$|E
5000|$|Note {{that in some}} sense, the Pólya urn {{model is}} the [...] "opposite" [...] of the model of {{sampling}} without replacement, where every time a particular value is observed, it {{is less likely to}} be observed again, whereas in a Pólya urn model, an observed value {{is more likely to be}} observed again. In both of these models, the act of measurement has an effect on the outcome of future measurements. (For comparison, when <b>sampling</b> <b>with</b> <b>replacement,</b> observation of a particular value has no effect on how likely it is to observe that value again.) Note also that in a Pólya urn model, successive acts of measurement over time have less and less effect on future measurements, whereas in sampling without replacement, the opposite is true: After a certain number of measurements of a particular value, that value will never be seen again.|$|E
50|$|Bootstrapping is a {{statistical}} method for estimating the sampling distribution of an estimator by <b>sampling</b> <b>with</b> <b>replacement</b> {{from the original}} sample, most often {{with the purpose of}} deriving robust estimates of standard errors and confidence intervals of a population parameter like a mean, median, proportion, odds ratio, correlation coefficient or regression coefficient. It may also be used for constructing hypothesis tests. It is often used as a robust alternative to inference based on parametric assumptions when those assumptions are in doubt, or where parametric inference is impossible or requires very complicated formulas for the calculation of standard errors. Bootstrapping techniques are also used in the updating-selection transitions of particle filters, genetic type algorithms and related Resample/Reconfiguration Monte Carlo methods used in computational physics and molecular chemistry. In this context, the bootstrap is used to replace sequentially empirical weighted probability measures by empirical measures. The bootstrap allows to replace the samples with low weights by copies of the samples with high weights.|$|E
40|$|Background: The {{appropriate}} {{handling of}} missing covariate data in prognostic modelling studies {{is yet to}} be conclusively determined. A resampling study was performed to investigate the effects of different missing data methods on the performance of a prognostic model. Methods: Observed data for 1000 cases were <b>sampled</b> <b>with</b> <b>replacement</b> from a large complete dataset of 7507 patients to obtain 500 replications. Five levels of missingness (ranging from 5...|$|R
30|$|Multivariate {{analysis}} was performed on whole population combining significant SNP in a stepwise multivariate logistic regression selecting variables according to the Akaike Information Criterion (AIC). The model was validated internally on 1, 000 random <b>samples</b> <b>with</b> <b>replacement</b> on the whole dataset. The percentage of times each variable was selected was extracted. Only those variables which were selected in > 80 % of models were retained. The final model was adjusted for ER status.|$|R
30|$|While we {{estimate}} the standard errors of the parameter estimates via the approximate information matrix {{as described in}} Section 4, the <b>sampling</b> distributions associated <b>with</b> λ and ν are known to possess skewness (Sellers and Shmueli 2013). Thus, an alternative approach is non-parametric bootstrapping. To compute parameter estimates and associated variation in this manner, one can (for example) randomly draw 1000 <b>samples</b> <b>with</b> <b>replacement</b> from the data using the boot package (Canty and Ripley 2015) in R (R Core Team 2017).|$|R
5000|$|Another way {{to think}} {{of some of the}} cases is in terms of sampling, in statistics. Imagine a {{population}} of X items (or people), of which we choose N. Two different schemes are normally described, known as [...] "sampling with replacement" [...] and [...] "sampling without replacement". In the former case (<b>sampling</b> <b>with</b> <b>replacement),</b> once we've chosen an item, we put it back in the population, so that we might choose it again. The result is that each choice is independent of all the other choices, and the set of samples is technically referred to as independent identically distributed. In the latter case, however, once we've chosen an item, we put it aside so that we can't choose it again. This means that the act of choosing an item has an effect on all the following choices (the particular item can't be seen again), so our choices are dependent on one another.|$|E
5000|$|Instead, we use bootstrap, {{specifically}} case resampling, {{to derive}} {{the distribution of}} [...] We first resample the data to obtain a bootstrap resample. An example of the first resample might look like this X1* [...] x2, x1, x10, x10, x3, x4, x6, x7, x1, x9. Note {{that there are some}} duplicates since a bootstrap resample comes from <b>sampling</b> <b>with</b> <b>replacement</b> from the data. Note also that the number of data points in a bootstrap resample is equal to the number of data points in our original observations. Then we compute the mean of this resample and obtain the first bootstrap mean: μ1*. We repeat this process to obtain the second resample X2* and compute the second bootstrap mean μ2*. If we repeat this 100 times, then we have μ1*, μ2*, …, μ100*. This represents an empirical bootstrap distribution of sample mean. From this empirical distribution, one can derive a bootstrap confidence interval for the purpose of hypothesis testing.|$|E
5000|$|As an example, assume we are {{interested}} in the average (or mean) height of people worldwide. We cannot measure {{all the people in the}} global population, so instead we sample only a tiny part of it, and measure that. Assume the sample is of size N; that is, we measure the heights of N individuals. From that single sample, only one estimate of the mean can be obtained. In order to reason about the population, we need some sense of the variability of the mean that we have computed. The simplest bootstrap method involves taking the original data set of N heights, and, using a computer, sampling from it to form a new sample (called a 'resample' or bootstrap sample) that is also of size N. The bootstrap sample is taken from the original by using <b>sampling</b> <b>with</b> <b>replacement</b> (e.g. we might 'resample' 5 times from 1,2,3,4,5 and get 2,5,4,4,1), so, assuming N is sufficiently large, for all practical purposes there is virtually zero probability that it will be identical to the original [...] "real" [...] sample. This process is repeated a large number of times (typically 1,000 or 10,000 times), and for each of these bootstrap samples we compute its mean (each of these are called bootstrap estimates). We now have a histogram of bootstrap means. This provides an estimate of the shape of the distribution of the mean from which we can answer questions about how much the mean varies. (The method here, described for the mean, can be applied to almost any other statistic or estimator.) ...|$|E
5000|$|If {{the members}} of the {{population}} come in three kinds, say [...] "blue" [...] "red" [...] and [...] "black", the number of red elements in a sample of given size will vary by sample and hence is a random variable whose distribution can be studied. That distribution depends on the numbers of red and black elements in the full population. For a simple random <b>sample</b> <b>with</b> <b>replacement,</b> the distribution is a binomial distribution. For a simple random sample without replacement, one obtains a hypergeometric distribution.|$|R
5000|$|The random {{subspace}} {{method is}} similar to bagging except that the features ("attributes", [...] "predictors", [...] "independent variables") are randomly <b>sampled,</b> <b>with</b> <b>replacement,</b> for each learner. Informally, this causes individual learners to not over-focus on features that appear highly predictive/descriptive in the training set, but fail to be as predictive for points outside that set. For this reason, random subspaces are an attractive choice for problems where the number of features is {{much larger than the}} number of training points, such as learning from fMRI data or gene expression data.|$|R
30|$|Prices were {{calculated}} and reported in European Euros (€). Since all costs were realised approximately 12  months after treatment, a correction for differential timing of economic costs was not performed. Direct costs {{were calculated}} {{using data from}} all of the patients included in the trial, whereas calculation of total costs was limited to those patients who completed the questionnaire. The CI around the mean costs of each treatment and around the difference in costs were obtained using a bootstrap sampling procedure [8]. For this purpose, 2000 random <b>samples</b> <b>with</b> <b>replacement</b> were drawn from the distribution of total costs in the two treatment groups.|$|R
5000|$|One of {{the reasons}} for {{interest}} in this particular rather elaborate urn model (i.e. with duplication and then replacement of each ball drawn) is that it provides an example in which the count (initially x black and y white) of balls in the urn is not concealed, which is able to approximate the correct updating of subjective probabilities appropriate to a different case in which the original urn content is concealed while ordinary <b>sampling</b> <b>with</b> <b>replacement</b> is conducted (without the Pólya ball-duplication). Because of the simple [...] "sampling with replacement" [...] scheme in this second case, the urn content is now static, but this greater simplicity is compensated for by the assumption that the urn content is now unknown to an observer. A Bayesian analysis of the observer's uncertainty about the urn's initial content can be made, using a particular choice of (conjugate) prior distribution. Specifically, suppose that an observer knows that the urn contains only identical balls, each coloured either black or white, but he does not know the absolute number of balls present, nor the proportion that are of each colour. Suppose that he holds prior beliefs about these unknowns: for him the probability distribution of the urn content is well approximated by some prior distribution for the total number of balls in the urn, and a beta prior distribution with parameters (x,y) for the initial proportion of these which are black, this proportion being (for him) considered approximately independent of the total number. Then the process of outcomes of a succession of draws from the urn (with replacement but without the duplication) has approximately the same probability law as does the above Pólya scheme in which the actual urn content was not hidden from him. The approximation error here relates to the fact that an urn containing a known finite number m of balls of course cannot have an exactly beta-distributed unknown proportion of black balls, since the domain of possible values for that proportion are confined to being multiples of , rather than having the full freedom to assume any value in the continuous unit interval, as would an exactly beta distributed proportion. This slightly informal account is provided for reason of motivation, and can be made more mathematically precise.|$|E
30|$|There are two {{challenges}} in generalizing the random-sampling technique of [7], namely, (i) <b>sampling</b> <b>with</b> <b>replacement</b> cannot be used, and (ii) weights {{must be part}} of the sampling process.|$|E
40|$|For {{the mean}} of a finite population, a bounded risk {{estimation}} problem is considered for both the situations Where the population variance mayor may not be known. In this context, three popular (equal probability) sampling strategies are considered. These are the analogues of (i) simple random <b>sampling</b> <b>with</b> <b>replacement,</b> mean per unit estimation, (ii) simple random <b>sampling</b> <b>with</b> <b>replacement,</b> mean per distinct unit estimation, and (iii) simple random sampling without replacement, mean per unit estimation. It {{is well known that}} in the conventional fixedsample size scheme, (iii) fares better than (ii) and (ii) better than (i) ...|$|E
30|$|Receiver {{operating}} characteristic (ROC) {{analysis was performed}} {{in order to determine}} the best thresholds for PCT (or the combination of PCT + lactate after log transformation) which would be predictive of the outcome. Because of the possible impact of sample size on threshold value, a bootstrap analysis (1000 random <b>samples</b> <b>with</b> <b>replacement)</b> was performed to obtain a calculation of the optimal threshold of PCT and its 95  % confidence interval [13]. We assessed the sensitivity and specificity, positive (PPV) and negative predictive value (NPV) (all with their 95  % confidence intervals [95  % CI], calculated with the Wilson’s score with correction of continuity) for thresholds.|$|R
40|$|Random forests perform bootstrap-aggregation by {{sampling}} {{the training}} <b>samples</b> <b>with</b> <b>replacement.</b> This enables {{the evaluation of}} out-of-bag error {{which serves as a}} internal cross-validation mechanism. Our motivation lies in using the unsampled training samples to improve each decision tree in the ensemble. We study the effect of using the out-of-bag samples to improve the generalization error first of the decision trees and second the random forest by post-pruning. A preliminary empirical study on four UCI repository datasets show consistent decrease {{in the size of the}} forests without considerable loss in accuracy. Comment: Previous version in proceedings of ISMM 201...|$|R
40|$|AbstractGiven a <b>sample</b> <b>with</b> <b>replacement</b> from {{a finite}} set M, we show simply how to {{generate}} a maximal sequence of functions of the sample, all uniform on M, such that these functions are pairwise independent. We also consider the problem of generating a sequence of k-wise independent functions of the sample. To this end {{we set up a}} geometrical framework in which this problem is intimately related to that of finding, in a projective geometry of dimension r, a set of points such that no one subset with k points belongs to a hyperplane of dimension k − 1...|$|R
3000|$|... has {{the same}} number of tuples as D that are sampled with {{replacement}} from D. By <b>sampling</b> <b>with</b> <b>replacement,</b> it means that some of the original tuples of D may not be included in D [...]...|$|E
3000|$|... nThis serves {{much the}} same purpose as the <b>sampling</b> <b>with</b> <b>replacement</b> used in random forests. A smaller sample is {{adequate}} because when sampling without replacement, no case is selected more than once; there are no “duplicates.” [...]...|$|E
30|$|As {{described}} here, {{this process}} involves <b>sampling</b> <b>with</b> <b>replacement.</b> If, {{as is often}} the case, sampling without replacement is required, then no individual is permitted to enter the sample more than once. This may then require some additional plot selection to achieve the desired sample size.|$|E
5000|$|The Uniform Convergence Theorem states, roughly, that if [...] is [...] "simple" [...] and we draw <b>samples</b> {{independently}} (<b>with</b> <b>replacement)</b> from [...] {{according to}} any distribution , then with high probability, the empirical frequency will {{be close to}} its expected value, which is the theoretical probability.|$|R
30|$|The {{non-parametric}} {{bootstrap method}} {{is a special}} case of Monte Carlo method used for obtaining the distribution of residues of θ which can {{be representative of the}} population. The idea behind the bootstrap method is that the calculated residues can be an estimate of the population, so the distribution of the residues can be obtained by drawing many <b>samples</b> <b>with</b> <b>replacement</b> from the calculated residues. For the Monte Carlo method, however, it creates the distribution of residues of θ with a theoretical (i.e., normal) distribution. From this aspect, the bootstrap method is more empirically based and the Monte Carlo method is more theoretically based.|$|R
40|$|An {{analytic}} wavelet transform, {{based on}} Hilbert wavelet pairs, {{is applied to}} bivariate time-varying spectral estimation for neurophysiological time series. Under the as-sumption of an underlying block stationary process, both single-trial and ensemble studies are amenable to this method. A bootstrap procedure, which <b>samples</b> <b>with</b> <b>replacement</b> blocks centered around the events of interest, is proposed to identify time points for which the event-averaged magnitude squared coherence is non-zero. Clinical data sets are {{used to compare the}} wavelet-based technique with the classi-cal Fourier-based spectral measures and highlight its ability to detect time-varying coherence and phase properties. Key words: Coherence, electromyographic activity, local field potential, phase spectrum, wavelet packet transfor...|$|R
