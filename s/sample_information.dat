497|2523|Public
25|$|Focusing {{the light}} beam {{to a point}} {{on the surface of}} the sample under test, and {{recombining}} the reflected light with the reference will yield an interferogram with <b>sample</b> <b>information</b> corresponding to a single A-scan (Z axis only). Scanning of the sample can be accomplished by either scanning the light on the sample, or by moving the sample under test. A linear scan will yield a two-dimensional data set corresponding to a cross-sectional image (X-Z axes scan), whereas an area scan achieves a three-dimensional data set corresponding to a volumetric image (X-Y-Z axes scan), also called full-field OCT.|$|E
25|$|The {{process of}} {{collecting}} data from samples using the flow cytometer is termed 'acquisition'. Acquisition is mediated {{by a computer}} physically connected to the flow cytometer, and the software which handles the digital interface with the cytometer. The software is capable of adjusting parameters (e.g., voltage, compensation) for the sample being tested, and also assists in displaying initial <b>sample</b> <b>information</b> while acquiring sample data to ensure that parameters are set correctly. Early flow cytometers were, in general, experimental devices, but technological advances have enabled widespread applications {{for use in a}} variety of both clinical and research purposes. Due to these developments, a considerable market for instrumentation, analysis software, as well as the reagents used in acquisition such as fluorescently labeled antibodies has developed.|$|E
2500|$|For our <b>sample</b> <b>information</b> system, LEM2 will {{induce the}} {{following}} rules: ...|$|E
50|$|<b>Sampling</b> <b>information</b> for Amplified {{adapted from}} The-Breaks.|$|R
40|$|<b>Sampling</b> <b>information</b> : Explanation note: <b>Information</b> for <b>sampling</b> of {{the five}} microendemic Carpathian species (Pedicia lobifera, Pedicia staryi, Pedicia apusenica, Pedicia carpianica, Pedicia costobocica, Pedicia roxolanica), {{containing}} {{the name of the}} species, the date of collection, with localities, mountains, locations and geographic coordinate...|$|R
40|$|This paper {{combines}} frontier {{functions and}} switching regressions. This allows economic agents to operate under different efficiency 'regimes,' thus relaxing {{the assumption that}} all observations are drawn from the same distribution of inefficiency. The 'switch' is based on <b>sample</b> separation <b>information</b> that is treated first as perfect, then as imperfect (or noisy). Available <b>sample</b> separation <b>information</b> suggests an observation's regime, however the information may not be accurate. By comparing results across alternative specifications of <b>sample</b> separation <b>information</b> as perfect and noisy, this approach provides evidence {{on the quality of}} the <b>sample</b> separation <b>information.</b> The technique's usefulness is demonstrated via an application to constrained labor supply. Copyright 1995 by Economics Department of the University of Pennsylvania and the Osaka University Institute of Social and Economic Research Association. ...|$|R
2500|$|This is {{done through}} the {{calculation}} shown below, where [...] is the likelihood function. This assesses {{the probability of}} the observed data [...] arising from the hypothesis [...] is the assigned prior probability or initial belief about the hypothesis; the denominator [...] is formed by the integrating or summing of [...] {{is known as the}} posterior which is the recalculated probability, or updated belief about the hypothesis. It {{is a result of the}} prior beliefs as well as <b>sample</b> <b>information.</b> The posterior is a conditional distribution as the result of collecting or in consideration of new relevant data.|$|E
2500|$|Due to {{the lack}} of a {{sufficient}} budget, only a few hundred copies of The College Kicked-Out were initially pressed up. [...] The record was available only on compact disc and the original copies—complete with liner notes that featured both <b>sample</b> <b>information</b> and phony album credits (for example, sample clearance services are credited to a Hugh Jazfynes, a pun on hip-hop artists having to pay [...] "huge-ass fines" [...] should they get sued for failure to clear samples)—are very rare and are now considered collector's items. [...] The College Kicked-Out has since been pirated in the wake of Danny!'s recent success—namely, his signing to Definitive Jux, Interscope Records and later Okayplayer—making legitimate copies that much more difficult to authenticate.|$|E
2500|$|While for typical {{unimodal}} distributions (with {{centrally located}} modes, inflexion points at {{both sides of}} the mode, and longer tails) (with Beta(α,β) such that [...] ) it is known that the sample mean (as an estimate of location) is not as robust as the sample median, the opposite is the case for uniform or [...] "U-shaped" [...] bimodal distributions (with Beta(α,β) such that [...] ), with the modes located at the ends of the distribution. [...] As Mosteller and Tukey remark ( [...] p.207) [...] "the average of the two extreme observations uses all the <b>sample</b> <b>information.</b> This illustrates how, for short-tailed distributions, the extreme observations should get more weight." [...] By contrast, it follows that the median of [...] "U-shaped" [...] bimodal distributions with modes {{at the edge of the}} distribution (with Beta(α,β) such that [...] ) is not robust, as the sample median drops the extreme sample observations from consideration. [...] A practical application of this occurs for example for random walks, since the probability for the time of the last visit to the origin in a random walk is distributed as the arcsine distribution Beta(1/2,1/2): the mean of a number of realizations of a random walk is a much more robust estimator than the median (which is an inappropriate sample measure estimate in this case).|$|E
40|$|Classical scrapie, {{a federally}} {{reportable}} disease in Canada, is a fatal neurodegenerative disease of sheep and goats. In order to inform future scrapie eradication programs for Canada, a study estimating the national prevalence of scrapie was conducted from Nov 2010 to Dec 2012; seven cases were detected among 11, 702 sheep. The prevalence {{at the individual}} level is estimated to be 0. 06 % (CI from 0. 03 % to 0. 12 %); at the farm-level it is estimated to be 0. 22 % (CI from 0. 11 % to 0. 45 %). A <b>sampling</b> <b>information</b> index was developed which measures the available <b>sampling</b> <b>information</b> at the Census Division (CD) level. A choropleth map is used to show the spatial distribution of this index. CDs with a low information index value cluster in the West Coast, th...|$|R
40|$|A {{switching}} regression {{approach with}} imperfect <b>sample</b> separation <b>information</b> {{is used to}} determine convergence clubs. Regime classification allows one to determine which countries belong to the related convergence clubs. Initial per capita GDP {{does not provide a}} perfect <b>sample</b> separation <b>information,</b> but the regimes classification obtained from an endogenous switching model provides unusual results. ...|$|R
40|$|We {{consider}} consensus {{problems of}} first-order multiagent systems with <b>sampled</b> <b>information</b> and noisy measurements. A distributed stochastic approximation type algorithm is employed to attenuate the measurement noises. We provide {{conditions under which}} almost sure strong consensus is guaranteed for fixed and switching directed network topologies. Simulation results are provided to illustrate the theoretical results...|$|R
50|$|<b>Sample</b> <b>information</b> for Train of Thought {{adapted from}} TheBreaks.|$|E
5000|$|For our <b>sample</b> <b>information</b> system, LEM2 will {{induce the}} {{following}} rules: ...|$|E
5000|$|Samples or {{materials}} (published values): Ranges {{and tables}} of analytical data with the corresponding metadata are provided {{for all the}} reference materials together with <b>sample</b> <b>information</b> such as origin, availability, and reference.|$|E
40|$|Researchers {{have become}} {{increasingly}} interested in estimating mixtures of stochastic frontiers. Mester (1993), Caudill (1993), and Polachek and Yoon (1987), for example, estimate stochastic frontier models for different regimes, assuming <b>sample</b> separation <b>information</b> is given. Building on earlier work by Lee and Porter (1984), Douglas, Conway, and Ferrier (1995) estimate a stochastic frontier switching regression model {{in the presence of}} noisy <b>sample</b> separation <b>information.</b> The {{purpose of this paper is}} to extend earlier work by estimating a mixture of stochastic frontiers assuming no <b>sample</b> separation <b>information.</b> This case is more likely to occur in practice than even noisy <b>sample</b> separation <b>information.</b> In order to estimate a mixture of stochastic frontiers with no <b>sample</b> separation <b>information,</b> an EM algorithm to obtain maximum likelihood estimates is developed. The algorithm is used to estimate a mixture of stochastic (cost) frontiers using data on U. S. savings and loans for the years 1986, 1987, and 1988. Statistical evidence is found supporting the existence of a mixture of stochastic frontiers. Copyright Springer-Verlag Berlin Heidelberg 2003 Key words: Mixture model, Stochastic frontier, efficiency, JEL: C 24, C 81, D 24,...|$|R
40|$|A {{measure of}} {{probability}} changes in probit and logit dichotomous models is proposed based of the {{efficient use of}} <b>sampling</b> <b>information,</b> specifically using {{the average of the}} derivatives, as opposed to the traditional practice of calculating the derivative at the mean value. The two strategies are compared using a simulation exercise which reveals their respective robustness. ...|$|R
40|$|The {{traditional}} {{evaluation of}} a new product proposal is a determination of the break-even volume, with no consideration of the uncertain nature of the data. In this paper <b>sampling</b> <b>information</b> is used to reduce uncertainty. The optimal number of customers to be sampled is determined for a proposal formulated {{in terms of an}} expected market share. ...|$|R
50|$|The {{divergence}} of {{the indicator}} and the Laplacian of the indicator (or of the characteristic function, as the indicator is also known) {{have been used}} as the <b>sample</b> <b>information</b> from which surfaces can be reconstructed.|$|E
50|$|This {{special case}} is how {{expected}} value of perfect information and expected value of <b>sample</b> <b>information</b> are calculated where risk neutrality is implicitly assumed. For cases where decision-maker is risk averse or risk seeking, this simple calculation does not necessary yield correct result, and iterative calculation {{is the only}} way to ensure correctness.|$|E
5000|$|Expected {{value of}} <b>sample</b> <b>information</b> (EVSI) is a {{relaxation}} {{of the expected}} value of perfect information (EVPI) metric, which encodes the increase of utility that would be obtained {{if one were to}} learn the true underlying state, [...] Essentially EVPI indicates the value of perfect information, while EVSI indicates the value of some limited and incomplete information.|$|E
40|$|The {{information}} matrix based on ranked set sampling (RSS) has been derived {{and it has}} been shown to be the sum of two semi-positive matrices, one of which is the {{information matrix}} based on simple random sampling (SRS). This decomposition demonstrates that parametric inference based on RSS is generally more efficient than its SRS counterpart. Ranked set <b>sampling</b> <b>Information</b> matrix Errors in ranking...|$|R
40|$|This {{project is}} {{concerned}} with the development of a coal database providing information to the public and private research sectors on 30 coal <b>samples.</b> The <b>information</b> from these <b>samples,</b> along with <b>information</b> previously collected on 26 samples, will be distributed to DOE contractors performing coal research...|$|R
30|$|Property {{rights of}} <b>samples</b> and <b>information,</b> will often require {{explicit}} transfer of rights by the donor.|$|R
5000|$|VoI is {{sometimes}} distinguished into value of perfect information, also called value of clairvoyance (VoC), {{and value of}} imperfect information. They {{are closely related to}} the widely known expected value of perfect information and expected value of <b>sample</b> <b>information.</b> Note that VoI is not necessarily equal to [...] "value of decision situation with perfect information" [...] - [...] "value of current decision situation" [...] as commonly understood.|$|E
50|$|In {{decision}} theory, {{the expected}} value of <b>sample</b> <b>information</b> (EVSI) is the expected increase in utility that a decision-maker could obtain from {{gaining access to}} a sample of additional observations before making a decision. The additional information obtained from the sample may allow them to make a more informed, and thus better, decision, thus resulting in an increase in expected utility. EVSI attempts to estimate what this improvement would be before seeing actual sample data; hence, EVSI is a form of what is known as preposterior analysis.|$|E
50|$|Focusing {{the light}} beam {{to a point}} {{on the surface of}} the sample under test, and {{recombining}} the reflected light with the reference will yield an interferogram with <b>sample</b> <b>information</b> corresponding to a single A-scan (Z axis only). Scanning of the sample can be accomplished by either scanning the light on the sample, or by moving the sample under test. A linear scan will yield a two-dimensional data set corresponding to a cross-sectional image (X-Z axes scan), whereas an area scan achieves a three-dimensional data set corresponding to a volumetric image (X-Y-Z axes scan), also called full-field OCT.|$|E
40|$|A Monte Carlo {{procedure}} {{was developed to}} simulate turbulent boundary layer wall pressure fluctuations. The approach utilizes much of the newly available conditional <b>sampling</b> <b>information</b> to construct the required distribution functions. Various disturbance wave forms were examined, {{as well as the}} effect of frequency-dependent decay. Good agreement between the simulation and experimental data was achieved for root mean square pressure level, power spectrum, and space time correlation...|$|R
30|$|Although the {{description}} of core samples is always the most reliable data in sedimentary facies, due to {{the limited number of}} core <b>samples,</b> <b>information</b> on sedimentary facies based on core samples are localized and fragmented. For this reason, well-log data are used to provide more information of sedimentary facies along the well. This will generate more reliable values along the well to control the model results.|$|R
40|$|This paper {{deals with}} the {{estimation}} of current population mean under non-response in two-occasion successive <b>sampling.</b> <b>Information</b> on a dynamic auxiliary variable has been used and efficient estimation procedures have been suggested which are capable in minimizing {{the negative impact of}} non-response when it occurred on current occasion in two-occasion successive sampling. Properties of the proposed estimation procedures have been studied and suitable recommendations are made...|$|R
5000|$|This is {{done through}} the {{calculation}} shown below, where [...] is the likelihood function. This assesses {{the probability of}} the observed data [...] arising from the hypothesis [...] is the assigned prior probability or initial belief about the hypothesis; the denominator [...] is formed by the integrating or summing of [...] {{is known as the}} posterior which is the recalculated probability, or updated belief about the hypothesis. It {{is a result of the}} prior beliefs as well as <b>sample</b> <b>information.</b> The posterior is a conditional distribution as the result of collecting or in consideration of new relevant data.|$|E
50|$|In {{addition}} to scanning across the <b>sample,</b> <b>information</b> on the electronic structure {{at a given}} location in the sample {{can be obtained by}} sweeping voltage and measuring current at a specific location. This type of measurement is called scanning tunneling spectroscopy (STS) and typically results in a plot of the local density of states as a function of energy within the sample. The advantage of STM over other measurements of the density of states lies in its ability to make extremely local measurements: for example, the density of states at an impurity site can be compared to the density of states far from impurities.|$|E
5000|$|The Apollo 17 Lunar <b>Sample</b> <b>Information</b> Catalog, 1973, {{describes}} it as a [...] "holocrystalline, equigranular basalt containing some poikilitic plagioclase". Its total weight before being broken up was 2957 grams. The sliced off 1.1 gram moon chip segments from lunar basalt 70017 consists of 30 percent plagioclase, 59 percent pyroxene, 10 percent ilmenite and 1 percent olivine. The Apollo 17 [...] "Lunar basalt 70017" [...] is described by technicians as a coarse-grained high-Ti basalt. It {{has been described as}} moon soil with a crystallization age around 3.7 billion years and an exposure age of about 220 million years. All surfaces of the blocky angular shaped stone are rough and jagged.|$|E
5000|$|For a <b>sample</b> recipe <b>information,</b> the HTML code {{rendered}} by {{both the}} above code samples looks like: ...|$|R
40|$|Soft soil {{is often}} {{described}} as an anisotropic heterogeneous material. Standard soil investigation mainly involves vertically retrieved <b>samples.</b> <b>Information</b> on the parameters working in the horizontal direction remains scarce. Principally each soil property like permeability, stiffness or strength might show anisotropic behaviour. This thesis focuses on anisotropy in stiffness and the possibility to use conventional laboratory measurement techniques to determine the level of anisotropy in stiffness. Civil Engineering and Geoscience...|$|R
40|$|In many perceptual and {{cognitive}} decision-making problems, humans <b>sample</b> multiple noisy <b>information</b> sources serially, and integrate the <b>sampled</b> <b>information</b> {{to make an}} overall decision. We derive the optimal decision procedure for two-alternative choice tasks in which the different options are sampled one at a time, sources vary {{in the quality of}} the information they provide, and the available time is fixed. To maximize accuracy, the optimal observer allocates time to <b>sampling</b> different <b>information</b> sources in proportion to their noise levels. We tested human observers in a corresponding perceptual decision-making task. Observers compared the direction of two random dot motion patterns that were triggered only when fixated. Observers allocated more time to the noisier pattern, in a manner that correlated with their sensory uncertainty about the direction of the patterns. There were several differences between the optimal observer predictions and human behaviour. These differences point to a number of other factors, beyond the quality of the currently availabl...|$|R
