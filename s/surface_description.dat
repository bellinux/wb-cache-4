181|370|Public
25|$|Geometric matching/ shape {{complementarity}} methods {{describe the}} protein and ligand {{as a set}} of features that make them dockable. These features may include molecular surface / complementary surface descriptors. In this case, the receptor’s molecular surface is described in terms of its solvent-accessible surface area and the ligand’s molecular surface is described in terms of its matching <b>surface</b> <b>description.</b> The complementarity between the two surfaces amounts to the shape matching description that may help finding the complementary pose of docking the target and the ligand molecules. Another approach is to describe the hydrophobic features of the protein using turns in the main-chain atoms. Yet another approach is to use a Fourier shape descriptor technique. Whereas the shape complementarity based approaches are typically fast and robust, they cannot usually model the movements or dynamic changes in the ligand/ protein conformations accurately, although recent developments allow these methods to investigate ligand flexibility. Shape complementarity methods can quickly scan through several thousand ligands {{in a matter of seconds}} and actually figure out whether they can bind at the protein’s active site, and are usually scalable to even protein-protein interactions. They are also much more amenable to pharmacophore based approaches, since they use geometric descriptions of the ligands to find optimal binding.|$|E
50|$|In the 20th century, {{the term}} {{topography}} {{started to be}} used to describe <b>surface</b> <b>description</b> in other fields where mapping in a broader sense is used, particularly in medical fields such as neurology.|$|E
50|$|OpenCRG is a {{complete}} open source project including a tool-suite for the creation, modification and evaluation of road surfaces, and an open file format specification CRG (curved regular grid). Its objective is to standardize a detailed road <b>surface</b> <b>description</b> {{and it may be}} used for applications like tire-, vibration- or driving-simulation.|$|E
50|$|OpenCRG handles any {{arbitrary}} {{scalar data}} versus a reference grid. These are typically elevation data or friction coefficients. By this, different applications like tire- or vibration simulations {{take advantage of}} the open <b>surface</b> <b>descriptions</b> format OpenCRG.|$|R
40|$|Stability of {{time domain}} {{integral}} equation (TDIE) solvers has remained an elusive goal for many years. Advancement {{of this research}} has largely progressed on four fronts: (1) Exact integration, (2) Lubich quadrature, (3) smooth temporal basis functions, and (4) Space-time separation of convolutions with the retarded potential. The latter method was explored in [Pray et al. IEEE TAP 2012]. This method's efficacy in stabilizing solutions to the time domain electric field integral equation (TD-EFIE) was demonstrated on first order <b>surface</b> <b>descriptions</b> (flat elements) in tandem with 0 th order functions as the temporal basis. In this work, we develop the methodology necessary to extend to higher order <b>surface</b> <b>descriptions</b> {{as well as to}} enable its use with higher order temporal basis functions. These higher order temporal basis functions are used in a Galerkin framework. A number of results that demonstrate convergence, stability, and applicability are presented. Comment: 8 pages, 12 figure...|$|R
40|$|Orthophotography - and photo-textured 3 D surface models, {{in general}} - are most {{important}} photogrammetric products in heritage conservation. However, it is now common knowledge that conventional orthorectification software accepts only <b>surface</b> <b>descriptions</b> obtained via 2 D triangulation and cannot handle the question of image visibility. Ignoring multiple surface elevations and image oc- clusions of the complex surface shapes, typically met i...|$|R
50|$|Considering {{applications}} with {{a demand}} to road <b>surface</b> <b>description,</b> OpenDRIVE refers to special data structures holding these surface data. OpenCRG the microscopic brother is available {{taking care of}} the provision and evaluation of road surface descriptions. An implementation of OpenCRG into the OpenDRIVE file format specification has already been established in January 2008.|$|E
5000|$|Literary minimalism is {{characterized}} by an economy with words and a focus on <b>surface</b> <b>description.</b> Minimalist writers eschew adverbs and prefer allowing context to dictate meaning. Readers are expected {{to take an active}} role in creating the story, to [...] "choose sides" [...] based on oblique hints and innuendo, rather than react to directions from the writer.|$|E
50|$|Sometimes {{considered}} {{a variety of}} literary minimalism, dirty realism is characterized by an economy with words and a focus on <b>surface</b> <b>description.</b> Writers working within the genre tend to avoid adverbs, extended metaphor and internal monologue, instead allowing objects and context to dictate meaning. Characters are shown in ordinary, unremarkable occupations, and often a lack of resources and money that creates an internal desperation.|$|E
40|$|The basic photogrammetric {{deliverable}} in {{heritage conservation}} is orthophotography (and other suitable raster projections) - closely followed today {{by a growing}} demand for photo-textured 3 D surface models. The fundamental limitation of conventional photogram- metric software is twofold: it can handle neither fully 3 D <b>surface</b> <b>descriptions</b> nor the question of image visibility. As a consequence, software which ignores both surface an...|$|R
40|$|The Modelica MultiBody {{library is}} {{extended}} with collision handling. It is demonstrated {{how to use}} this new feature. Different implementations are explained based on parametric surfaces, on surfaces described by algebraic constraints, and on <b>surface</b> <b>descriptions</b> by primitives and triangles using the collision package SOLID 3. 5. Furthermore, the response calculation by a resultant contact force and torque is discussed...|$|R
40|$|Abstract—Stability of {{time domain}} {{integral}} equation (TDIE) solvers has remained an elusive goal for many years. Advance-ment {{of this research}} has largely progressed on four fronts: (1) Exact integration, (2) Lubich quadrature, (3) smooth temporal basis functions, and (4) Space-time separation of convolutions with the retarded potential. The latter method was explored in [1]. This method’s efficacy in stabilizing solutions to the time domain electric field integral equation (TD-EFIE) was demonstrated on first order <b>surface</b> <b>descriptions</b> (flat elements) in tandem with 0 th order functions as the temporal basis. In this work, we develop the methodology necessary to extend to higher order <b>surface</b> <b>descriptions</b> {{as well as to}} enable its use with higher order temporal basis functions. These higher order temporal basis functions are used in a Galerkin framework. A number of results that demonstrate convergence, stability, and applicability are presented. Index Terms—Time-domain integral equations, higher order temporal basis, stability, time-domain analysis, marching-on-in-time, space-time Galerkin method. I...|$|R
5000|$|Literary minimalism can be {{characterized}} as a focus on a <b>surface</b> <b>description</b> where readers are expected to {{take an active role in}} the creation of a story. The characters in minimalist stories and novels tend to be unexceptional. Generally, the short stories are [...] "slice of life" [...] stories. Minimalism, the opposite of maximalism, is a representation of only the most basic and necessary pieces, specific by economy with words. Minimalist authors hesitate to use adjectives, adverbs, or meaningless details. Instead of providing every minute detail, the author provides a general context and then allows the reader's imagination to shape the story. Among those categorized as postmodernist, literary minimalism is most commonly associated with Jon Fosse and especially Samuel Beckett.|$|E
50|$|Geometric matching/ shape {{complementarity}} methods {{describe the}} protein and ligand {{as a set}} of features that make them dockable. These features may include molecular surface / complementary surface descriptors. In this case, the receptor’s molecular surface is described in terms of its solvent-accessible surface area and the ligand’s molecular surface is described in terms of its matching <b>surface</b> <b>description.</b> The complementarity between the two surfaces amounts to the shape matching description that may help finding the complementary pose of docking the target and the ligand molecules. Another approach is to describe the hydrophobic features of the protein using turns in the main-chain atoms. Yet another approach is to use a Fourier shape descriptor technique. Whereas the shape complementarity based approaches are typically fast and robust, they cannot usually model the movements or dynamic changes in the ligand/ protein conformations accurately, although recent developments allow these methods to investigate ligand flexibility. Shape complementarity methods can quickly scan through several thousand ligands {{in a matter of seconds}} and actually figure out whether they can bind at the protein’s active site, and are usually scalable to even protein-protein interactions. They are also much more amenable to pharmacophore based approaches, since they use geometric descriptions of the ligands to find optimal binding.|$|E
50|$|MIKE 3 is a {{generalised}} {{mathematical modelling}} system {{designed for a}} wide range of applications in areas such as oceanography,coastal regions and estuaries and lakes. The system is fully three-dimensional solving the momentum equation and continuity equationsin the three Cartesian directions.MIKE 3 simulates unsteady flow taking into account density variations, bathymetry and external forcing such as meteorology, tidal elevations, currents and other hydrographic conditions. MIKE 3 can be applied to oceanographic studies,coastal circulation studies, water pollution studies,environmental impact assessment studies,heat and salt recirculation studies and sedimentation studies. MIKE 3 is composed of three fundamental modules: The hydrodynamic (HD) module, the turbulence module and the advection-dispersion (AD) module. Various features such as free <b>surface</b> <b>description,</b> laminar flow description and density variations are optionally invoked within the three fundamental modules. A number of application modules have been implemented and can be invoked optionally. These are advection-dispersion of conservative or linearly decaying substances, a water quality (WQ)module describing BOD-DO relations, nutrients and hygienic problems, a eutrophication (EU) module simulating algae growth and primary production, and a mud transport (MT) module simulating transport along with erosion and deposition of cohesive material. A Lagrangian-based particle (PA) module can also be invoked for simulating e.g. tracers, sediment transport or the spreading and decay of E-Coli bacteria. The modelling system is based on the conservation of mass and momentum in three dimensions of a Newtonian fluid. The flow is decomposed into mean quantities and turbulent fluctuations. The closure problem is solved through the Boussinesq eddy viscosity concept relating the Reynold stresses to the mean velocity field. To handle density variations, the equations for conservation of salinity and temperature are included. An equation of state constitutes the relation between the density and the variations in salinity and temperature and ñ if the MT calculations are invoked ñ mud concentration. In the hydrodynamic module, the prognostic variables are the velocity components in the three directions and the fluid pressure. The model equations are discretised in an implicit, finite difference scheme on a staggered grid and solved non-iteratively by use of the alternating directions' implicit technique. A phase and amplification analysis neglecting effects of viscosity, convective terms, rotation, density variations, etc. has been performed. Under these circumstances, the finite difference scheme is unconditionally stable.|$|E
40|$|The shape from shading (SfS) {{problem in}} {{computer}} vision requires complete {{knowledge of the}} image conditions under which an image is created to produce <b>surface</b> <b>descriptions.</b> Almost all the methods rely on modeling the image formation process view and invert it mathematically which in turn places constraints on imaging conditions. These methods make constraining assumptions on camera model (orthographic projections), light source (singlepoint source at infinity) and reflectance model (Lambertian). In this paper, we present a general framework for solving the SfS problem under general imaging conditions by using powerful image synthesis techniques from computer graphics and thus moving the complexity of producing <b>surface</b> <b>descriptions</b> from analysis to the synthesis side. The technique relies on iterative synthesis of images from object descriptions {{in order to minimize}} an error function. The technique is illustrated in detail for quadric surfaces, with the ellipsoid as the specific example. To extend the method to general surfaces, the surfaces are modeled under the Bézier framework. The proposed framework is found to be very general with the capability to accommodate widely varying reflectance models and light source types...|$|R
40|$|Noble metal-coated {{core-shell}} nanoparticles {{have been}} applied to a suite of catalytic applications, {{with the aim of}} decreasing the noble metal loading while ideally improving their performance. The chemistry and therefore activity at the surface of these materials are intimately related to the accurate description of the core-shell interface. Using density functional theory, we developed a procedure to obtain realistic <b>surface</b> topology <b>descriptions</b> at the heterometallic junction. This procedure was applied to a topical series of catalysts: Ti 0. 1 W 0. 9 C coated in atomically thin monolayers of noble metals. Our quantum chemical calculations provide access to both relevant <b>surface</b> <b>descriptions</b> of these materials and also rationalize several experimental observations. Our general procedure paves the way for the rationalization and prediction of next-generation heterometallic catalysts...|$|R
40|$|Normal meshes are new {{fundamental}} <b>surface</b> <b>descriptions</b> {{inspired by}} differential geometry. A normal mesh is a multiresolution mesh where each level {{can be written}} as a normal offset from a coarser version. Hence the mesh can be stored with a single float per vertex. We present an algorithm to approximate any surface arbitrarily closely with a normal semi-regular mesh. Normal meshes are useful in numerous applications such as compression, filtering, rendering, texturing, and modeling...|$|R
40|$|We {{present an}} {{efficient}} algorithm that computes a manifold triangular mesh from {{a set of}} unorganized sample points in 3. The algorithm builds on the observation made by several researchers that the Gabriel graph of the sample points provides a good <b>surface</b> <b>description.</b> However, this <b>surface</b> <b>description</b> is only one-dimensional. We associate {{the edges of the}} Gabriel graph with index 1 critical points of a dynamical system induced by the sample points. Exploiting also the information contained in the critical points of index 2 provides a two-dimensional <b>surface</b> <b>description</b> which can be easily turned into a manifold. 1...|$|E
40|$|Abstract. This paper {{describes}} {{a new technique}} for free-form object segmentation from a single arbitrary-viewed range image. The aim is to derive a <b>surface</b> <b>description</b> of objects that may vary in shape and complexity without any restriction {{on the type of}} surfaces on the object. We propose a surface representation scheme that uses edge information to built a <b>surface</b> <b>description</b> using algebraic implicit surfaces. The proposed technique, not only reduces the number of used patches, but also preserves surface-depth and orientation continuity. This is done by propagating and blending piecewise hermite interpolation surfaces. The system has been tested on several synthetic and real range images and the experimental results have shown that the system can produce reliable <b>surface</b> <b>description</b> of a variety of free-form objects. ...|$|E
40|$|This thesis {{describes}} a new method for localising anthropometric landmark points on 3 D face scans. The points are localised by fitting a sparse shape model {{to a set}} of candidate landmarks. The candidates are found using a feature detector that is designed using a data driven methodology, this approach also informs the choice of landmarks for the shape model. The fitting procedure is developed to be robust to missing landmark data and spurious candidates. The feature detector and landmark choice is determined by the performance of different local surface descriptions on the face. A number of criteria are defined for a good landmark point and good feature detector. These inform a framework for measuring the performance of various surface descriptions and the choice of parameter values in the <b>surface</b> <b>description</b> generation. Two types of <b>surface</b> <b>description</b> are tested: curvature and spin images. These descriptions, in many ways, represent many aspects of the two most common approaches to local <b>surface</b> <b>description.</b> Using the data driven design process for <b>surface</b> <b>description</b> and landmark choice, a feature detector is developed using spin images. As spin images are a rich <b>surface</b> <b>description,</b> we are able to perform detection and candidate landmark labelling in a single step. A feature detector is developed based on linear discriminant analysis (LDA). This is compared to a simpler detector used in the landmark and <b>surface</b> <b>description</b> selection process. A sparse shape model is constructed using ground truth landmark data. This sparse shape model contains only the landmark point locations and relative positional variation. To localise landmarks, this model is fitted to the candidate landmarks using a RANSAC style algorithm and a novel model fitting algorithm. The results of landmark localisation show that the shape model approach is beneficial over template alignment approaches. Even with heavily contaminated candidate data, we are able to achieve good localisation for most landmarks...|$|E
40|$|This work {{deals with}} {{computer}} analysis of textured <b>surfaces.</b> <b>Descriptions</b> of textures are formalized from natural language descriptions. Local texture descriptions are {{obtained from the}} directional and non-directional components of the Fourier transform power spectrum. Analytic expressions are derived for orientation, contrast, size, spacing, and in periodic cases, the locations of texture elements. The local descriptions are defined over windows of varying sizes. Key Words Computer vision, computer description of texture, texture elements, spatial organization of textured regions...|$|R
40|$|The {{purpose of}} this paper is to present a survey of rigid {{registration}} (also called matching) methods applicable to <b>surface</b> <b>descriptions.</b> As features are often used for the registration task, standard feature extraction approaches are described in addition. In order to give the reader a framework for his present registration problem, this report divides the matching task into three major parts (feature extraction, similarity metrics and search strategies). In each of them the reader has to decide between several possibilities, whose relations are in particularly pointed out. ...|$|R
40|$|This paper {{describes}} the initial {{phase of the}} development of a nondestructive, multisensor approach for detecting, quantifying and monitoring degradation of organic coatings applied ot aluminum <b>surfaces.</b> <b>Descriptions</b> of the purposes and chemical composition of layered coatings used on aircraft structures are provided. The discussion then concentrates on ultrasonic thickness measurements. One is the well-established pulse/echo scanning acoustic microscopy and, as a proposed alternative, contiuous acoustic waves measurements with a probe in contact to the sample. Advantages and disadvantages of the two methods and their potential as in field applications are discussed...|$|R
40|$|In this paper, we {{summarize}} {{our initial}} experiences in designing head-worn displays with free-form optical surfaces. Typical optical surfaces implemented in raytrace codes today are functions mapping two dimensional vectors to real numbers. The majority of optical designs {{to date have}} relied on conic sections and polynomials as the functions of choice. The choice of conic sections is justified since conic sections are stigmatic surfaces under certain imaging geometries. The choice of polynomials {{from the point of}} view of <b>surface</b> <b>description</b> can be challenged. The advantage of using polynomials is that the wavefront aberration function is typically expanded in polynomials. Therefore, a polynomial <b>surface</b> <b>description</b> may link a designer’s understanding of wavefront aberrations and the <b>surface</b> <b>description.</b> The limitations of using multivariate polynomials are described by a theorem due to Mairhuber and Curtis from approximation theory. In our recent work, we proposed and applied radial basis functions to represent optical surfaces as an alternative to multivariate polynomials. We compare the polynomial descriptions to radial basis functions using th...|$|E
40|$|Short PapersInternational audienceWe propose here a {{sculpture}} metaphor for rapid shape prototyping. This metaphor makes the underlying <b>surface</b> <b>description</b> transparent for the user, enabling him to {{focus only on}} the shape being modeled. Our approach is based on implicit surfaces deﬁned as iso-surfaces over a discrete ﬁeld...|$|E
30|$|The {{frictional}} force {{will be changed}} depending on surface slipperiness, and the floor reaction force will be changed depending on the surface hardness of the floor. This means that the information of surface hardness and slipperiness for <b>surface</b> <b>description</b> can be acquired by measuring the {{frictional force}} and floor reaction force.|$|E
40|$|Subject of Research. The paper {{deals with}} the {{problems}} of higher order aspherical surfaces approximation using different equation types. The objects of research are two types of equations for higher order aspherical <b>surfaces</b> <b>description</b> used in different software for optical systems design (SАRО, OPAL, ZEMAX, CODE-V, etc.) and dependent on z-coordinate or on a radial coordinate on the surface. Conversion from one type of equations to another is considered in view of application in different software for optical systems design. Methods. The subject matter of the method lies in usage of mean square method approximation for recalculation of high-order aspherical surface. Iterative algorithm for recalculation is presented giving the possibility to recalculate coefficients for different types of equations with required accuracy. Recommendations are given for choosing recalculation parameters such as the number of result equation coefficients, the number of points for recalculation and point allocation on a surface. Main Results. Example of recalculation for aspherical surface and accuracy estimation, including result aberration comparison between initial surface and recalculated surface are presented. The example has shown that required accuracy of surface representation was obtained. Practical Relevance. This technique is usable for recalculation of higher order aspherical surfaces in various types of software for optical systems design and also for research of optimal higher order aspherical <b>surfaces</b> <b>description...</b>|$|R
40|$|The basic photogrammetric {{deliverable}} in {{heritage conservation}} is orthophotography (and other suitable raster projections) – closely followed today {{by a growing}} demand for photo-textured 3 D surface models. The fundamental limitation of conventional photogrammetric software is twofold: it can handle neither fully 3 D <b>surface</b> <b>descriptions</b> nor the question of image visibility. As a consequence, software which ignores both surface and image occlusions is clearly inadequate for the complex surface topography encountered, as a rule, in conservation or restoration tasks; geometric accuracy and good visual quality are then possible only {{at the cost of}} tiresome human interaction, especially in the phase of surface modeling. However, laser scanning and powerful modeling/editing software allow today fast and accurate collection of vast numbers of surface points and the creation of reliable 3 D meshes. Close-range photogrammetry is obviously expected to extend its horizon by taking full advantage of this new possibility. Here an approach is presented for the automated generation of orthoimages and perspective views from fully 3 D <b>surface</b> <b>descriptions</b> derived from laser scanning. Initially, the algorithm detects surface occlusions for the novel view. Next – in contrast to conventional photogrammetric software which requires an operator to define individual original images as the source for image content – all available images participate in a view-independent texturing of the new image. Thus, following a bundle adjustment, all surface triangles are back-projected onto all initial images to establish visibilities. Texture “blending ” is realised via an appropriate weighting scheme...|$|R
40|$|Deformable objects play an {{important}} role in many applications, such as animation and simulation. Effective computation with deformable surfaces can be achieved through the use of dynamic meshes. In this paper, we introduce a framework for constructing and maintaining a timevarying adapted mesh structure that conforms to the underlying deformable surface. The adaptation function employs error metrics based on stochastic sampling. Our scheme combines normal and tangential geometric correction with refinement and simplification resolution control. Furthermore, it applies to both parametric and implicit <b>surface</b> <b>descriptions.</b> As the result, we obtain a simple and efficient general scheme that can be used for a wide range of computations. 1...|$|R
30|$|As {{the basis}} for {{segmentation}} of SPECT images, Fourier surfaces have the appealing property of being intrinsically resistant to noise. For SPECT, high-frequency modulation in the <b>surface</b> <b>description</b> is likely to reflect noise in the image rather than a true feature of the object considered. If only a few Fourier orders are used, only slowly varying solutions are achievable.|$|E
40|$|This paper {{presents}} a new solution in the roughness description based on 2, 5 D map. Three parameters for rough <b>surface</b> <b>description</b> were proposed. The research {{was performed to}} verify which parameter provides the best combination of time calculation and accuracy of the terrain roughness reconstruction. Prepared map {{may be used for}} mobile robot path planning...|$|E
40|$|Research {{that creates}} {{techniques}} to automatically obtain dimensional geometric parameters from the nonuniform B-spline <b>surface</b> <b>description</b> {{of an object}} is presented. These techniques have been implemented successfully in the aircraft design software, ACSYNT, a computer-aided design system for conceptual aircraft design created at Virginia Tech and NASA Ames. The techniques created and implemented in this research are also of significance to general-purpose design...|$|E
30|$|In this paper, we {{proposed}} a wiping motion {{that makes it possible}} to measure the surface condition as surface hardness and slipperiness. Experiments showed that wiping motion works effectively for surface state measurement, and {{proposed a}}n index for <b>surface</b> state <b>description.</b>|$|R
40|$|We {{present a}} new {{approach}} to combine two approaches to three-dimensional reconstruction: silhouette-based and correspondence-based approaches. The two approaches have complementary costs and benefits. Silhouette-based approaches deliver volumetric descriptions which often have very few outliers, but they cannot reconstruct concave surfaces. Correspondence-based approaches give <b>surface</b> <b>descriptions</b> with sub-pixel accuracy, but their search range either allows outliers or falls short of the correct match. We show that {{a combination of the two}} can deliver fine-grained accuracy with few outliers. Our specific implementation uses the silhouette reconstruction as prior data to center and bound a stereo search process. We explore the different performance characteristics of the combination and its two component methods qualitatively and quantitatively using real imagery...|$|R
30|$|In this paper, {{we propose}} a {{relatively}} general surface state measurement method {{independent of the}} robot movement form. In addition, we compare the measured value with the physical properties of the object and investigate the quantitative index used for the <b>surface</b> state <b>description.</b>|$|R
