2|22|Public
40|$|The wavelet filter cascade {{method for}} <b>subband</b> <b>encoding</b> of finite-difference time-domain {{simulation}} is presented. This method implements computational bandwidth compression via a filter bank comprised of discrete wavelet transforms which maps simulation data or measured data into a compressed representation with little degradation of signal integrity. This letter discusses {{the use of}} the WFC technique to augment the FDTD method in particular but may be extended to other time domain methods. Simulation results are compared to standard FDTD results for accuracy and signal fidelity, with reduction in problem size summarized...|$|E
40|$|The raw {{size of a}} high-dynamic-range (HDR) image {{brings about}} {{problems}} in storage and transmission. Many bytes are wasted in data redundancy and perceptually unimportant information. To address this problem, researchers have proposed some preliminary algorithms to compress the data, like RGBE/XYZE, OpenEXR, LogLuv, and so on. 1 These algorithms mostly use lossless compression strategies, so they are not capable of providing significant compression. However, lossy compression alternatives have started becoming available, such as Ward and Simmons ’ 2 <b>subband</b> <b>encoding</b> and Mantiuk et al. ’s 3 perception-motivated encoding (see the “Related Work ” sidebar). HDR images can have a dynamic range of more than four orders of magnitude while conventional 8 -bi...|$|E
40|$|An {{efficient}} {{image compression}} algorithm based on energy clustering and zero-quadtree representation (ECZQR) in the wavelet transform domain is proposed. In embedded coding, zeros within each <b>subband</b> are <b>encoded</b> {{in the framework}} of quadtree representation instead of zerotree representation. To use large rectangular blocks to represent zeros, it first uses morphological dilation to extract the arbitrarily shaped clusters of significant coefficients within each <b>subband.</b> The proposed <b>encoding</b> method results in less distortion in the decoded image than the line-by-line encoding method. Experimental results show that the algorithm is among the most efficient wavelet image compression algorithms. link_to_subscribed_fulltex...|$|R
40|$|In this paper, we {{investigate}} {{the performances of}} Gaussian modeling and linear prediction tools for error detection and concealment in the transmission of still images. We consider the transmission of <b>subband</b> <b>encoded</b> images through two types of channels. We model the residual correlation between subband coefficients by considering them as jointly Gaussian variables. The first transmission medium considered is a packet-oriented channel, where some packets are lost during transmission. The problem is to estimate the values of missing coefficients. In this case, particular {{care must be taken}} while evaluating correlation matrices from incomplete data. The other system considered is based on a discrete memoryless noisy channel affecting the data being transmitted. The challenge is here first to determine the locations of the errors-which is done through hypotheses tests-and then to replace them by estimates based on their neighbors. The reconstruction via linear prediction is shown to give better results than median filtering based reconstruction. Error detection through this Gaussian model also shows promising results, in particular when channel statistics are taken into account in a joint source-channel decoding framework...|$|R
30|$|G[*]=[*] 2. The overall {{encoding}} {{time for}} any GOP size {{can be estimated}} by combining the running time for the key frames and the WZ frames. From the table, one {{can see that the}} proposed system has a reduced run time when measured against the H. 264 intra codec. The running time for the proposed system increases as the rate increases (larger RD points), since more <b>subbands</b> are <b>encoded.</b> A rate increase for a given subband also increases the size of the encoding matrix, which leads to a run-time increase.|$|R
40|$|Abstract—The {{algorithm}} {{represents the}} DCT coefficients to concentrate signal energy and proposes combination and dictator {{to eliminate the}} correlation in the same level <b>subband</b> for <b>encoding</b> the DCT-based images. This work adopts DCT and modifies the SPIHT algorithm to encode DCT coefficients. The proposed algorithm also provides the enhancement function in low bit rate {{in order to improve}} the perceptual quality. Experimental results indicate that the proposed technique improves the quality of the reconstructed image in terms of both PSNR and the perceptual results close to JPEG 2000 at the same bit rate. Keywords—JPEG 2000, enhancement filte...|$|R
40|$|This work adopts DCT and modifies the SPIHT {{algorithm}} {{to encode}} DCT coefficients. The algorithm represents the DCT coefficients to concentrate signal energy and proposes combination and dictator {{to eliminate the}} correlation in the same level <b>subband</b> for <b>encoding</b> the DCT-based images. The proposed algorithm also provides the deblocking function in low bit rate {{in order to improve}} the perceptual quality. This work contribution is that the coding complexity of the proposed algorithm for DCT coefficients is just close to JPEG but the performance is higher than JPEG 2000. Experimental results indicate that the proposed technique improves the quality of the reconstructed image in terms of both PSNR and the perceptual results close to JPEG 2000 at the same bit rate. 1...|$|R
40|$|A novel {{compressed}} domain automatic music summarization {{approach is}} presented in this paper. The proposed method works directly in the compressed domain. Only the <b>encoded</b> <b>subband</b> samples are extracted and processed for characterizing music content and discovering the music structure. The experimental results and the evaluation by a subjective study {{have shown that the}} summarization based on MPEG- 1 Layer 3 (MP 3) music is comparable to the summarization based on uncompressed PCM music samples. 1...|$|R
40|$|A {{technique}} to extract pitch information directly from au-dio files encoded using MPEG/Audio coding standard is described. The technique {{works in the}} compressed domain and requires the MPEG/Audio file to be decoded only par-tially for extracting the <b>encoded</b> <b>subband</b> samples. The pa-per first proposes a method for extracting wavelet coeffi-cients from the subband samples. The pitch interval is then estimated from the time interval between two successive maxima of the wavelet coefficients. It is shown that the computational complexity for compressed domain pitch ex-traction is less than 7 % of the computations required for decoding the subband samples and finding the pitch. 1...|$|R
40|$|A {{technique}} to peform speech recognition directly from audio files encoded using the MPEG/Audio coding standard is described. The technique {{works in the}} compressed domain and {{does not require the}} MPEG/Audio file to be decompressed. Only the <b>encoded</b> <b>subband</b> sam-ples are extracted and processed for training and recognition. The underlying speech recognition engine used is based on the Hidden Markov model. The technique is applicable to layers I and II of MPEG/Audio, and training under one layer can be used to recognize the other. Results based on the recognition of a speaker-dependent, small vocabulary, and continuously spoken sentences shows accuracy as high as 99 % using this technique...|$|R
40|$|We propose ‘Contour Code’, a novel {{representation}} and binary hash table encoding for multispectral palmprint recognition. We first present a reliable technique for {{the extraction of}} a region of interest (ROI) from palm images acquired with non-contact sensors. The Contour Code representation is then derived from the Nonsubsampled Contourlet Transform. A uniscale pyramidal filter is convolved with the ROI followed by {{the application of a}} directional filter bank. The dominant directional subband establishes the orientation at each pixel and the index corresponding to this <b>subband</b> is <b>encoded</b> in the Contour Code representation. Unlike existing representations which extract orientation features directly from the palm images, the Contour Code uses a two stage filtering to extract robust orientation features. The Contour Code is binarized into an efficient hash table structure that only requires indexing and summation operations for simultaneous one-to-many matching with an embedded score level fusion of multiple bands. We quantitatively evaluate the accuracy of the ROI extraction by comparison with a manually produced ground truth. Multispectral palmprint verification results on the PolyU and CASIA databases show that the Contour Code achieves an EER reduction upto 50 %, compared to state-of-the-art methods. 1...|$|R
40|$|A low {{bit rate}} three {{dimensional}} decomposition algorithm for video compression with simple computational complexity is presented. This algorithm performs the temporal decomposition of a video sequence in a more efficient way by using 4 -tap short symmetric kernel filter (Haar filters) with decimation factor of 4 : 1 instead of 2 : 1 used in the classical 3 D-wavelet algorithms. The pyramid coding decomposition concept is then used for the spatial domain. The main goal {{of this paper is}} to design a simple encoding algorithm with a very high performance. Local adaptive vector quantization (LAVQ) is used to encode some of the spatial subbands. The codebook of LAVQ is simple and robust to the motion which occurs in the video sequences and which seldom captures from a single training sequence. The other <b>subbands</b> are <b>encoded</b> using the very simple coding algorithm called absolute moment block truncation code (AMBTC). The AMBTC is used for the bands that are highly correlated and with no motion or sparks information. Experimental results demonstrate a significant improvement in performance over earlier published algorithms. It gives an excellent image quality at PSNR on the average of 36. 9 dB and at a {{low bit rate}} of 0. 13 bpp for Miss America sequence. Finally, the coding and decoding of the proposed algorithm are of comparable and relatively low complexity and is well suited to parallel implementation...|$|R
40|$|Abstract—Most {{existing}} multiview video coding (MVC) {{techniques are}} based on the traditional hybrid DCT-based video coding schemes, e. g., MPEG- 2, MPEG- 4 and H. 264. They neither fully exploit the redundancy among different views nor provide an easy way of implementation for scalabilities. In this paper, we propose an MVC scheme based on wavelet, which can provide temporal, spatial, SNR as well as view scalabilities. To the best of our knowledge, wavelets have not been used for MVC in the literature before. In particular, we consider a multiview video as a 2 D matrix with 1 D along the temporal axis, 1 D along the view axis, and each element in the 2 D matrix represents a video frame. We first apply 1 D wavelet decomposition to the 2 D matrix along the temporal axis with motion compensation, and then along the view axis with disparity compensation. After that, 2 D spatial wavelet decomposition is applied to each element in the matrix. Finally, all the <b>subbands</b> are <b>encoded</b> by 3 D-ESCOT with rate-distortion optimization. Compared with traditional 3 D wavelet coding, our proposed wavelet-based MVC scheme applies one more dimensional wavelet transform along the view direction to exploit the redundancy between adjacent views. Some preliminary experiments are carried out using standard multiview video sequences and the efficiency of the proposed system is confirmed. I...|$|R
40|$|We {{present a}} wavelet image coder {{based on an}} {{explicit}} model of the conditional statistical relationships between coefficients in different subbands. In particular, we construct a parameterized model for the conditional probability of a coefficient given coefficients at a coarser scale. <b>Subband</b> coefficients are <b>encoded</b> one bitplane at a time using a non-adaptive arithmetic encoder. The overall ordering of bitplanes {{is determined by the}} ratio of their encoded variance to compressed size. We show rate-distortion comparisons of the coder to first and second-order theoretical entropy bounds and the EZW coder [1]. The coder is inherently embedded, and should prove useful in applications requiring progressive transmission. Orthonormal wavelet decompositions have proven to be extremely effective for image compression [2, 3, 4, 5, 1]. We believe there are several statistical reasons for this success. Similar to the Fourier transform, wavelets are quite good at decorrelating the second-order sta [...] ...|$|R
40|$|An {{improved}} video subband coding {{technique is}} presented. It {{is based on}} a spatially and spectrally localizing subband analysis, followed by scalar quantization and direct arithmetic coding. The quantization and coding parameters are adapted on the basis of local energy of the subband pixels. The proposed technique automatically achieves near-optimal rate allocation with respect to mean-square error (or alternatively, weighted mean-square error). In addition, because arithmetic coding requires no alphabet extension, the quantized subband pixels can be encoded in an arbitrary order without affecting bit rate. This makes it possible to obtain a good statistical estimate of the quantization resolution required to achieve a certain overall bit rate, based on a relatively small random sample of the subband pixels and their corresponding energies. The proposed technique requires that estimates of the local energy of the <b>subband</b> pixels be <b>encoded</b> and sent to the receiver as side-information. A [...] ...|$|R
40|$|We {{develop a}} {{probability}} model for natural images, based on empirical observation of their statistics in the wavelet transform domain. Pairs of wavelet coefficients, corresponding to basis functions at adjacent spatial locations, orientations, and scales, {{are found to}} be non-Gaussian in both their marginal and joint statistical properties. Specifically, their marginals are heavy-tailed, and although they are typically decorrelated, their magnitudes are highly correlated. We propose a Markov model that explains these dependencies using a linear predictor for magnitude coupled with both multiplicative and additive uncertainties, and show that it accounts for the statistics {{of a wide variety of}} images including photographic images, graphical images, and medical images. In order to directly demonstrate the power of the model, we construct an image coder called EPWIC (embedded predictive wavelet image coder), in which <b>subband</b> coefficients are <b>encoded</b> one bitplane at a time using a nonadapti [...] ...|$|R
40|$|In {{the first}} part of this paper, under the {{assumption}} of noiseless transmission, we develop two entropy-coded subband image coding schemes. The difference between these schemes is the procedure used for encoding the lowest frequency subband: predictive coding is used in one system and transform coding in the other. Other <b>subbands</b> are <b>encoded</b> using zero-memory quantization. After a careful study of subband statistics, the quantization parameters the corresponding Huffman codes and the bit allocation among subbands are all optimized. It is shown that both schemes perform considerably better than the scheme developed by Woods and O'Neil [2]. Roughly speaking, these new schemes perform the same as that in [2] at half the encoding rate. In the second part of the paper, after demonstrating the unacceptable sensitivity of these schemes to transmission noise, we will develop a combined source/channel coding scheme in which rate-compatible convolutional codes are used to provide protection agains channel noise. A packetization scheme to prevent infinite error propagation is used and an algorithm for optimal assignment of bits between the source and channel encoders of different subbands is developed. We will show the in the presence of channel noise, these channeloptimized schemes offer dramatic performance improvements ove' the schemes designed based on a noiseless channel assumption; they also perform better than that of [2] even in th absence of channel noise. Finally, the robustness of the proposed schemes against channel mismatch will be studied...|$|R
40|$|This paper {{presents}} a representation for video with application to coding. In recent years, scalability of image compression systems has gained in popularity. Our approach is scalable {{in the normal}} sense, but instead of allowing overall image frame quality to degrade with lower bandwidth, salient portions of the image frame maintain high quality while less important portions are allowed to degrade. The judgment of importance is made by a region [...] of [...] interest (ROI) finder. The use of prioritized ROI's help to eliminate redundancy and can be organized under a motion compensation ruling. 1. INTRODUCTION Classic progressive compression techniques perform compression from coarse to fine frequencies of entire <b>subbands</b> [5]. When <b>encoding</b> the <b>subbands</b> of higher frequency, because of their size {{it is especially important}} to encode efficiently while retaining critical details of the scene. Coding schemes such as [2], approach this problem by modeling the whole scene. Adding scalability to such [...] ...|$|R
40|$|We {{develop a}} {{statistical}} characterization of natural {{images in the}} wavelet transform domain. This characterization describes the joint statistics between pairs of coefficients corresponding to basis functions at adjacent spatial locations, orientations, and scales. We observe that the raw coefficients are nearly decorrelated, but their magnitudes are highly correlated. A linear magnitude predictor coupled with both multiplicative and additive uncertainties accounts for the joint coefficient statistics {{of a wide variety}} of images including photographic images, graphical images, and medical images. In order to directly demonstrate the power of this model, we construct an image coder called EPWIC (Embedded Predictive Wavelet Image Coder), in which <b>subband</b> coefficients are <b>encoded</b> one bitplane at a time using a non-adaptive arithmetic encoder that utilizes conditional probabilities calculated from the model. Bitplanes are ordered using a greedy algorithm that considers the MSE reduction per encoded bit. The decoder uses the statistical model to predict coefficient values based on the bits it has received. Despite the simplicity of the model, the rate-distortion performance of the coder is roughly comparable to the best image coders in the literature...|$|R
40|$|Abstract — In {{this paper}} we {{investigate}} {{the utilization of}} visual saliency maps for ROI-based video coding of video-telephony applications. Visually salient areas indicated in the saliency map are considered as ROIs. These areas are automatically detected using an algorithm for visual attention (VA) which builds on the bottom-up approach proposed by Itti et al. A top-down channel emulating the visual search for human faces performed by humans has been added, while orientation, intensity and color conspicuity maps are computed within a unified multi-resolution framework based on wavelet <b>subband</b> analysis. Priority <b>encoding,</b> for experimentation purposes, is utilized in a simple manner: Frame areas outside the priority regions are blurred using a smoothing filter and then passed to the video encoder. This leads to better compression of both Intra-coded (I) frames (more DCT coefficients are zeroed in the DCT-quantization step) and Inter coded (P,B) frames (lower prediction error). In more sophisticated approaches, priority encoding could be incorporated by varying the quality factor of the DCT quantization table. Extended experiments concerning both static images as well as low-quality video show the compression efficiency of the proposed method. The comparisons are made against standard JPEG and MPEG- 1 encoding respectively...|$|R
40|$|Digital {{images are}} {{replacing}} analogue images such as photographs and x-rays {{in many different}} fields. Compression of these digital images is desirable for efficient storage and transmission. Subband coding has proved an effective method of image compression. This thesis investigates subband analysis structures and filters which are optimised for still image compression. Among other results it is shown that a high coding gain, based on a typical image model, and good spatial localisation are desirable filter bank characteristics for subband image coding. Assuming a high bit rate {{it is well known}} that the Karhunen-Loeve Uansform (KLT) is the optimum orthogonal block Uansform in terms of a coding gain metric. It is shown further that the KLT is the optimum invertible block Uansform using the unified coding gain. The coding gain metric is examined under a rate constraint. It is shown that for highly correlated sources increased low frequency subband resolution is required for optimum performance at low rates as compared to high rates: a result that is corroborated using a practical subband coder. Subband filters (CQF 2 ̆ 7 s) that globally maximise the coding gain for all two-band perfect reconstruction orthogonal filter banks are derived. Various characteristics of these filters are predicted using a new theorem on the zeros of an eigenvector of a symmetric Toeplitz matrix corresponding to the minimum (maximum) eigenvalue. These filters are shown to enjoy the three properties of the KLT : namely maximum coding gain, minimum basis restriction error, and subband decorrelation. It is also shown that there is some freedom to select different impulse responses. The design of maximum gain filters is extended to include filters consUained to certain subspaces. For example maximum gain wavelets may be designed. A modified two-climensional discrete wavelet Uansform (DWT) is proposed based on a typical image model. A generic <b>subband</b> quantisation and <b>encoding</b> method suitable for any subband structure is inuoduced. This method is essentially a generalisation of the JPEG quantisation and encoding method and has good spatial adaptation properties. Using the generic <b>subband</b> quantisation and <b>encoding</b> method various <b>subband</b> analysis structures and filters are compared for still image compression. The best orthogonal filters, Daubechies wavelets and m a x i m u m gain filters designed using an i m a g e source model, and the discrete cosine transform (DCT) perform in a similar manner. The filters with the minimum spatial (time) width perform better than other impulse responses in a mean square error sense and exhibit significantly less ringing. The performance of cosine modulated filter banks, with poorer spatial resolution, is slightly inferior to the DCT. Preliminary investigations show that biorthogonal filters, with a smaller spatial width and higher coding gain, can outperform the best orthogonal filters, especially at low rates. These biorthogonal filters also exhibit minimal ringing. Finally, the modified D W T is shown to be superior to the D W T for head and shoulders type images...|$|R
40|$|A context-modeling sub-algorithm {{has been}} {{developed}} {{as part of an}} algorithm that effects three-dimensional (3 D) wavelet-based compression of hyperspectral image data. The context-modeling subalgorithm, hereafter denoted the context modeler, provides estimates of probability distributions of wavelet-transformed data being encoded. These estimates are utilized by an entropy coding subalgorithm that is another major component of the compression algorithm. The estimates make it possible to compress the image data more effectively than would otherwise be possible. The following background discussion is prerequisite to a meaningful summary of the context modeler. This discussion is presented relative to ICER- 3 D, which is the name attached to a particular compression algorithm and the software that implements it. The ICER- 3 D software is summarized briefly in the preceding article, ICER- 3 D Hyperspectral Image Compression Software (NPO- 43238). Some aspects of this algorithm were previously described, in a slightly more general context than the ICER- 3 D software, in "Improving 3 D Wavelet-Based Compression of Hyperspectral Images" (NPO- 41381), NASA Tech Briefs, Vol. 33, No. 3 (March 2009), page 7 a. In turn, ICER- 3 D is a product of generalization of ICER, another previously reported algorithm and computer program that can perform both lossless and lossy wavelet-based compression and decompression of gray-scale-image data. In ICER- 3 D, hyperspectral image data are decomposed using a 3 D discrete wavelet transform (DWT). Following wavelet decomposition, mean values are subtracted from spatial planes of spatially low-pass <b>subbands</b> prior to <b>encoding.</b> The resulting data are converted to sign-magnitude form and compressed. In ICER- 3 D, compression is progressive, in that compressed information is ordered so that as more of the compressed data stream is received, successive reconstructions of the hyperspectral image data are of successively higher overall fidelity...|$|R
40|$|This thesis {{addresses}} {{the key issues}} of image dataset compression, especially cubic-panorama image dataset compression, used in image-based virtual environment navigation to develop effective and efficient compression techniques and schemes. To my knowledge, {{this is the first}} work to be published to investigate and design compression schemes for cubic-panorama image datasets. A spatially consistent representation of cubic-panorama image datasets is proposed. With this spatially consistent representation, unrestricted search for displacement vectors as well as matching reference blocks is extended beyond side image boundaries in all directions. A block padding algorithm for constructing reference blocks is presented for displacement estimation and compensation. Optimized matching reference blocks are obtained to reduce prediction errors and improve compression efficiency. Superior coding performance is achieved with the spatially consistent representation compared with the generic planar representation of cubic-panorama image datasets. For cubic-panorama image dataset compression, a scalable lifted wavelet-based coding scheme with displacement compensation is developed. This scheme is based on the framework of lifted wavelet transforms with cross-image displacement-compensated enhancement combined with embedded entropy coding. The wavelet transforms efficiently generate hierarchically structured decomposition coefficients and provide the potential of spatially scalable coding required by image dataset compression. The lifting operations put wavelet analysis and synthesis into fast memory-saving in-place computations. The displacement compensation significantly improves the wavelet transform coding efficiency across basis images. A new embedded entropy coding approach named independent block with layered data partition (IBLDP) is proposed. It combines embedded independent coefficient block coding with layered bitplane data partitioning. To provide the required spatial resolution scalability and spatial random access, no inter-subband and inter-block dependencies are exploited. Each <b>subband</b> is <b>encoded</b> independent of other subbands. With the compact layered bitplane data representation and the simplified coding structure, IBLDP features reduced computational and implemental complexity and hence is more suitable for the efficient and interactive image rendering application. For cubic-panorama image dataset compression, a specific random access mechanism is designed. A new hierarchical data structure is proposed to accommodate the random access mechanism. The corresponding bit-stream syntax is formed to support this new data structure. Multi-level index tables are embedded in the bit-stream to easily facilitate the spatial image random access. A proper compromise between the coding efficiency and the random access flexibility is reached. A novel global displacement estimation and compensation approach with scaled block-depth estimation is developed. Based on a six-parameter perspective projection model, a displacement-compensated image prediction algorithm is presented, taking into consideration all kinds of camera motions. A unique block-based scaled depth estimation technique is proposed for image prediction. The displacement-compensated predicting images are generated by using global model parameters combined with the estimated scaled block-depth map. More accurate predicting images with less-data presentations are obtained compared with the traditional BMA. The proposed approach is more efficient when applied for cubic-panorama image dataset compression. Experimental results of the proposed techniques and schemes applied to encoding the testing cubic-panorama image datasets are demonstrated. Superior coding performances are achieved over those of the corresponding comparable techniques and schemes applied for cubic-panorama image dataset compression...|$|R
40|$|Two {{methods of}} {{increasing}} the effectiveness of three-dimensional (3 D) wavelet-based compression of hyperspectral images have been developed. (As used here, images signifies both images and digital data representing images.) The methods are oriented toward reducing or eliminating detrimental effects of a phenomenon, referred to as spectral ringing, that is described below. In 3 D wavelet-based compression, an image is represented by a multiresolution wavelet decomposition consisting of several subbands obtained by applying wavelet transforms in the two spatial dimensions corresponding to the two spatial coordinate axes of the image plane, and by applying wavelet transforms in the spectral dimension. Spectral ringing is named after the more familiar spatial ringing (spurious spatial oscillations) {{that can be seen}} parallel to and near edges in ordinary images reconstructed from compressed data. These ringing phenomena are attributable to effects of quantization. In hyperspectral data, the individual spectral bands play the role of edges, causing spurious oscillations to occur in the spectral dimension. In the absence of such corrective measures as the present two methods, spectral ringing can manifest itself as systematic biases in some reconstructed spectral bands and can reduce the effectiveness of compression of spatially-low-pass subbands. One of the two methods is denoted mean subtraction. The basic idea of this method is to subtract mean values from spatial planes of spatially low-pass <b>subbands</b> prior to <b>encoding,</b> because (a) such spatial planes often have mean values that are far from zero and (b) zero-mean data are better suited for compression by methods that are effective for subbands of two-dimensional (2 D) images. In this method, after the 3 D wavelet decomposition is performed, mean values are computed for and subtracted from each spatial plane of each spatially-low-pass subband. The resulting data are converted to sign-magnitude form and compressed {{in a manner similar to}} that of a baseline hyperspectral- image-compression method. The mean values are encoded in the compressed bit stream and added back to the data at the appropriate decompression step. The overhead incurred by encoding the mean values only a few bits per spectral band is negligible with respect to the huge size of a typical hyperspectral data set. The other method is denoted modified decomposition. This method is so named because it involves a modified version of a commonly used multiresolution wavelet decomposition, known in the art as the 3 D Mallat decomposition, in which (a) the first of multiple stages of a 3 D wavelet transform is applied to the entire dataset and (b) subsequent stages are applied only to the horizontally-, vertically-, and spectrally-low-pass subband from the preceding stage. In the modified decomposition, in stages after the first, not only is the spatially-low-pass, spectrally-low-pass subband further decomposed, but also spatially-low-pass, spectrally-high-pass subbands are further decomposed spatially. Either method can be used alone to improve the quality of a reconstructed image (see figure). Alternatively, the two methods can be combined by first performing modified decomposition, then subtracting the mean values from spatial planes of spatially-low-pass subbands...|$|R

