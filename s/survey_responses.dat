3325|3124|Public
5|$|Translation {{problems}} are {{compounded by the}} women's confusion over which type of FGM they experienced, or even whether they experienced it. Several {{studies have suggested that}} <b>survey</b> <b>responses</b> are unreliable. A 2003 study in Ghana found that in 1995 four percent said they had not undergone FGM, but in 2000 said they had, while 11 percent switched in the other direction. In Tanzania in 2005, 66 percent reported FGM, but a medical exam found that 73 percent had undergone it. In Sudan in 2006, a significant percentage of infibulated women and girls reported a less severe type.|$|E
5|$|The Financial Times {{poll was}} the result of over 10,000 {{respondents}} to nearly 23000 electronic questionnaires of alumni from 155 qualifying business schools. The survey began in July 2006 and all internationally accredited programs that are at least five years old and that have produced at least 30 graduates {{in each of the last}} three years were solicited. 113 of the 155 had at least 20 respondents and at least a 20 percent response rate. The questionnaire used twenty criteria in three main areas. The poll actually presents all twenty criteria to the reader. Eight criteria are based on alumni responses; eleven criteria are based on business school responses, and the final criterion is based on a research index produced by the Financial Times. The <b>survey</b> <b>responses</b> are audited by KPMG.|$|E
25|$|Financial Times uses <b>survey</b> <b>responses</b> from alumni who {{graduated}} three {{years prior to}} the ranking and information from business schools. Salary and employment statistics are weighted heavily.|$|E
40|$|Abstract Our data {{collection}} procedure for young adult (YA) follow-up surveys precedes the survey with {{a request to}} the parent for locator information on the YA. We tested how providing a token prepaid cash incentive to their parents would affect both parent response and subse-quent (i. e., “downstream”) <b>survey</b> <b>response</b> from the YAs. Parents {{were randomly assigned to}} one of three incentive conditions: $ 0 (N = 97), $ 1 (N = 98), and $ 2 (N = 97). We found strong evidence for a parent incen-tive effect on parent response during the parent mail/phone sequence, and mild evidence at the end of the parent effort. We found no parent incen-tive effect on final YA <b>survey</b> <b>response.</b> We did, however, find an effect on early YA <b>survey</b> <b>response.</b> For example, at 30 days following the ini-tial survey mailing, YA <b>survey</b> <b>response</b> was 46. 4 percent, 61. 2 percent, and 58. 8 percent for those whose parents received $ 0, $ 1, and $ 2, respec-tively (p =. 03). Also, we found that giving $ 1 or $ 2 to parents increased the speed of YA <b>survey</b> <b>response</b> by 34 percent (p =. 02). We found no evidence for a differential parent incentive effect on the speed of <b>survey</b> <b>response</b> between female and male YAs. The increase in the YA <b>survey</b> <b>response</b> speed imparted by a $ 1 or $ 2 incentive to parents can more tha...|$|R
40|$|Research has {{examined}} the benefits of having an engaged workforce. One way of measuring employee engagement is through organizational <b>surveys.</b> <b>Survey</b> <b>response</b> rate represents voluntary participation behavior and may be an outcome of employee engagement. However, research has not examined survey participation behavior as an outcome of employee engagement or factors that might influence this relationship. Thus, {{the purpose of the}} present study was to examine the relationship between dimensions of employee engagement (vigor, dedication, and absorption) and <b>survey</b> <b>response</b> rate at the aggregate level and to examine how this relationship varied as a function of percentage of union membership. Analyses were conducted for small and large groups because they had different work environments. The researcher found that there was a significant positive relationship between aggregate employee engagement levels and <b>survey</b> <b>response</b> rate. Moreover, this relationship was stronger for large groups than small groups, suggesting that group size influences the relationship between aggregate employee engagement and <b>survey</b> <b>response</b> rate. Regarding the moderating effect, union membership influenced the relationship between dedication and <b>survey</b> <b>response</b> rate only in small groups. These findings suggest that in small groups, the percentage of union membership in a group influenced the relationship between a strong sense of pride and personal accomplishment and <b>survey</b> <b>response</b> rate...|$|R
30|$|Follow-up of each <b>survey</b> <b>response</b> with an {{open-ended}} interview.|$|R
25|$|Norman’s 1999 study {{provides}} quantitative analysis of <b>survey</b> <b>responses</b> from 44 self-identified lesbian, gay and bisexual individuals using the Brighton and Hove (UK) public libraries. The survey {{was aimed at}} identifying five aspects of the libraries’ LGB users, including demographics, the effect of centralizing a collection, whether LGB individuals use bibliographies to find reading as well as reasons for using and perceptions of library service. Results of the survey were analyzed using SPSS.|$|E
25|$|Sentiment {{analysis}} (sometimes {{known as}} opinion mining or emotion AI) {{refers to the}} use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and <b>survey</b> <b>responses,</b> online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine.|$|E
25|$|A 2012 report {{produced}} by the Cato Institute found {{that the cost of}} the loans is overstated, and that payday lenders offer a product traditional lenders simply refuse to offer. However, the report is based on 40 <b>survey</b> <b>responses</b> collected at a payday storefront location. The report's author, Victor Stango, was {{on the board of the}} Consumer Credit Research Foundation (CCRF) until 2015, an organization funded by payday lenders, and received $18,000 in payments from CCRF in 2013.|$|E
5000|$|... #Subtitle level 2: Contradictory {{theories}} of <b>survey</b> <b>response</b> instability ...|$|R
40|$|Personalizing {{correspondence}} {{has often}} shown to significantly increase <b>survey</b> <b>response</b> rates in mail surveys. This study experimentally tests whether personalization of email invitations acts correspondingly to web <b>survey</b> <b>response</b> rates. Also, it is investigated whether personalization influences data quality. The {{results of the}} study, using a large student sample, show that personalization significantly increases the web <b>survey</b> <b>response</b> rate by 8. 6 percentage points. The data quality {{does not appear to}} be affected in any major way by personalizing the email invitations. However, the analyses do show that respondents of the personalization condition tend to respond with more social desirability bias to sensitive questions. Therefore, it is concluded that personalization has positive effects on the <b>survey</b> <b>response</b> rate, but one should carefully consider whether or not to personalize when a survey on sensitive topics is conducted. status: publishe...|$|R
30|$|These {{differences}} are unsurprising given typical <b>survey</b> <b>response</b> patterns with, for example, women and retirees {{more likely to}} answer surveys.|$|R
25|$|On September 14, 2007, ORB (Opinion Research Business), an {{independent}} UK based polling agency, published {{an estimate of}} the total casualties of the Iraq war. The figure suggested by ORB, which was based on <b>survey</b> <b>responses</b> from 1,499 adults, stands at 1,220,580 deaths, with a margin of error of 2.5%. This estimate, although conducted independently, and using a different polling methodology, is consistent with the Lancet findings if accounting for the additional 14 months covered by the ORB poll.|$|E
2500|$|The company uses {{artificial}} intelligence, {{natural language}} processing, and machine learning to derive insights from unstructured data such as contact center interactions, chatbot and live chat transcripts, product reviews, open-ended <b>survey</b> <b>responses,</b> and email. Luminoso's software identifies and quantifies patterns and relationships in text-based data, including domain-specific or creative language. Rather than human-powered keyword searches of data, the software automates taxonomy creation around concepts, allowing related {{words and phrases}} to be dynamically generated and tracked.|$|E
2500|$|... September 19, 2002. Section 713 of the Gramm-Leach-Bliley Act of 1999 (Public Law 106-102) {{directs the}} Board of Governors of the Federal Reserve System {{to study and}} report to the Congress on the default rates, {{delinquency}} rates, and profitability of lending activities undertaken in conformance with the Community Reinvestment Act of 1977 (CRA). The Board asked the 500 largest retail banking organizations to voluntarily complete a comprehensive survey focusing on their CRA-related lending activities and prepared a report summarizing <b>survey</b> <b>responses.</b> The Board was directed to make the report and supporting data {{available to the public}} (linked above).|$|E
40|$|This paper {{examines}} current {{theories of}} <b>survey</b> <b>response</b> behaviour, namely social exchange, cognitive dissonance, self-perception and commitment/involvement using a two-phased approach. A laboratory-type experiment (administered as a survey) and a field experiment {{were conducted to}} examine the relationship between survey participation and the major <b>survey</b> <b>response</b> theories that have been proposed to explain that participation and mode of survey data collection. The results suggest that there is a significant association between the <b>survey</b> <b>response</b> theories and <b>survey</b> participation. Exchange theory appears to be the basis of the most prevalent appeal followed by commitment/involvement, cognitive dissonance and self-perception, respectively. A higher response rate was found for personal interview followed by telephone interview and then by mail <b>survey.</b> However, the <b>response</b> rate of the field experiment was much smaller than the results obtained from a laboratory-type experiment with simulated survey appeals, a not totally unexpected finding...|$|R
40|$|Response to the American Time Use Survey (ATUS) has {{averaged}} 56 {{percent since the}} survey began in 2003. This {{is lower than the}} 80 percent rate that is recommended by the United States Office of Federal Statistical Policy and Standards. Low <b>survey</b> <b>response</b> to the ATUS has generated concern that the data may not be representative of the population they are supposed to represent and has spurred numerous efforts to understand and improve <b>response</b> to the <b>survey.</b> This paper summarizes some of the efforts that have been undertaken to improve and understand response to the ATUS in its first decade. Key Words: <b>survey</b> <b>response,</b> response rate...|$|R
50|$|Particular {{effort is}} made to achieve monthly <b>survey</b> <b>response</b> rates of around 80%, {{ensuring}} that an accurate picture of business conditions is recorded over time.|$|R
2500|$|Niche College Rankings is an American college ranking {{site that}} {{incorporates}} analysis of college based statistics and reviews. Niche also features A-F rankings for K-12 schools and neighborhoods or districts. Niche's rankings are updated every year. This is shown as they first developed college rankings by major {{as well as}} rankings and graded Report Cards for community colleges and trade schools. Niche provides its own grading system that applies a Bayesian method. [...] In 2017, Niche provides several rankings in each category, [...] "Best Colleges," [...] "Best by Major," [...] "Best by State," [...] "Admissions," [...] "Campus Life," [...] "Student," [...] and [...] "Academics". Niche collects more than 100 million college reviews and <b>survey</b> <b>responses</b> as well as comprehensive data such as U.S Department of Education. Niche also incorporates data from the new College Scorecard Data that was introduced in 2015 by the Obama Administration under the U.S Department of Education.|$|E
2500|$|The use {{of union}} data would {{probably}} prevail regardless of assumptions, since {{the data are}} collected through voluntary surveys. Since the responses are provided voluntarily, and since the response requires {{a substantial amount of}} work to understand and complete, it {{is in the interest of}} employers with high wage workforces and high overhead to respond. By answering the request for data when employers with lower wage workforces and low overhead do not, they pull the prevailing wage determination in their favor. For smaller employers and employers who do not participate in federal contracting, it is not worth the cost to complete the surveys. Furthermore, it is in the interest of local unions to respond to the surveys, since a predetermination of wage significantly below the union wage would allow non-union employers to bid successfully on contracts. Thus, the <b>survey</b> <b>responses</b> tend to be biased upwards towards collective bargaining agreement wage levels. This source of bias was noted in the DOL Office of the Inspector General report: “A past audit observed that the methods used by WH to obtain survey data allowed bias to be introduced into wage surveys. Statistical sampling of employers was not done. Only data from employers and third parties who volunteered to participate in the surveys were considered. Consequently, data that could have influenced survey results may have been omitted. Also, employers and third parties who may have had a stake in the outcome of wage decisions were afforded an opportunity to submit erroneous data that may have influenced the survey results.” ...|$|E
50|$|Race of {{interviewer}} effects {{occur when}} the race of an interview influences <b>survey</b> <b>responses.</b>|$|E
30|$|Survey {{information}} was obtained {{on a single}} sheet of paper using a 10 -cm visual analog scale (Appendix). There was a 100 % <b>survey</b> <b>response</b> rate.|$|R
50|$|Philip Converse has {{attributed}} <b>survey</b> <b>response</b> instability to respondents lacking meaningful beliefs, {{while others}} have chalked it up to measurement errors and vague language in surveys.|$|R
40|$|The Florida Youth Tobacco Survey (FYTS) was {{administered}} {{in the spring}} of 2008 to 38, 510 middle school students and 40, 283 high school students in 744 public schools throughout the state. The <b>survey</b> <b>response</b> rate for middle schools was 79 %, and the <b>survey</b> <b>response</b> rate for high schools was 73 %. The FYTS has been conducted annually since 1998. The data presented in this fact sheet are weighted to represent the entire population of public {{middle and high school students}} in Florida...|$|R
50|$|Survey {{response}} {{effects are}} variations in <b>survey</b> <b>responses</b> {{that result from}} seemingly inconsequential aspects of survey design and administration. Susceptibility to these effects varies depending on the stability of one’s beliefs. Those without a strong attitude on an issue, for instance, would more be more prone to survey response effects than those strongly for or against the issue. These effects can be broadly grouped as consistency or contrast effects. Consistency effects are effects that lead to <b>survey</b> <b>responses</b> that agree, {{not to be confused}} with the identically-named term used to refer to the phenomenon in which respondents intentionally try to get their <b>survey</b> <b>responses</b> to agree one another. Contrast effects on the other hand, lead to opposing responses.|$|E
5000|$|Scholars {{typically}} receive {{thousands of}} dollars in scholarship money. The following data represents <b>survey</b> <b>responses</b> from 2010 Governor's Scholars who graduated high school in 2011: ...|$|E
50|$|In {{addition}} to its eight specialized units, IMRB also has Abacus field offices {{that are responsible for}} the recruitment, administration and quality control of <b>survey</b> <b>responses.</b>|$|E
30|$|An Excel file {{containing}} the observer video scoring workbook and automated MISTO <b>survey</b> <b>response</b> sheet is available as an Additional file {{associated with this}} article (Additional file  1).|$|R
3000|$|... 4 In a {{comparison}} of the survey sample with the population of all establishments in Germany, Bossler et al. (2017) do not detect any meaningful selectivity in the <b>survey</b> <b>response.</b>|$|R
40|$|Over {{the past}} few decades, college student <b>survey</b> <b>response</b> rates have been declining. This is a {{problematic}} trend because student survey data are used extensively in endeavors such as accreditation, institutional improvement, and scholarly research. While low <b>survey</b> <b>response</b> rates are not necessarily a problem, they will be if they impact the representativeness of survey samples. Unfortunately, the limited literature on student survey nonresponse suggests that nonresponse is usually not random, though for college students {{little is known about}} the type of student, institutional, or administrative characteristics that promote student <b>survey</b> <b>response.</b> The purpose of this study was to examine predictors of college student <b>survey</b> <b>response,</b> in a comprehensive model that takes into account both student and institutional factors. Drawing on sociological, organizational, and psychological theories, a conceptual model of student- and institution-level influences on <b>survey</b> <b>response</b> was developed and tested using national longitudinal surveys administered by the Higher Education Research Institute (HERI) to first-time, full-time students enrolling at four-year institutions in the falls of 2003, 2004 and 2005. The study utilized hierarchical generalized linear modeling (HGLM) to examine predictors of longitudinal survey nonresponse one and four years after matriculation, for all students as well as for groups disaggregated by gender and self-identified race/ethnicity (White, Black/African American, Latino/a, and Asian American). Results revealed that a key group of response predictors was consistent across aggregated and disaggregated groups of students, one and four years after college entry. For virtually all students, a small set of student-level characteristics (most notably high school achievement, gender, personality, and self-rated likelihood of transfer) strongly predicted response propensities, indicating that students' entering characteristics have an enduring impact on their <b>survey</b> <b>response</b> likelihoods over the entire course of college. Institution-level results revealed that students were far less likely to respond to web surveys and mail surveys than they were to paper surveys handed out in person; survey incentives showed mixed effects. Institutional size was a consistent predictor across all students and surveys, while institutional survey climate significantly impacted response propensities for seniors only. Findings are discussed in terms of their implications for both researchers and practitioners...|$|R
5000|$|Financial Times uses <b>survey</b> <b>responses</b> from alumni who {{graduated}} three {{years prior to}} the ranking and information from business schools. Salary and employment statistics are weighted heavily.|$|E
5000|$|In 2014, SendGrid made The Denver Post Top Workplaces List, {{recognized}} as a top workplace based solely on <b>survey</b> <b>responses</b> about the workplace completed by SendGrid employees. (Denver Post Top Workplace) ...|$|E
50|$|In {{the summer}} of 2009, the NCSC {{commenced}} {{a survey of the}} usage of e-filing in state courts across the country, including U.S. Territories. The <b>survey</b> <b>responses</b> were published in the 2009 NCSC Court E-filing Survey.|$|E
40|$|Response {{rates are}} one {{indicator}} of a survey's data quality, {{as a great}} deal of importance has been placed on the mail <b>survey's</b> <b>response</b> rate. However, a telephone <b>survey's</b> <b>response</b> rate usually is not reported. Even if one is reported, the numbers used in the calculation are rarely defined making the response rate interpretation unclear. Using a recent telephone survey of Pennsylvania dairy managers, this paper demonstrates how telephone survey data should be reported. Essentially, every research report should include a discussion of how the survey was conducted, a disposition table, and well-defined formulas used to calculate response rates. Research Methods/ Statistical Methods,...|$|R
30|$|An {{electronic}} Internet-based <b>survey</b> (<b>survey</b> <b>response</b> rate[*]=[*] 66  %) {{was used}} to characterize a community cohort of MAs residing in Eastern Massachusetts, USA. Clinical and lifestyle factors associated with prevalent CV disease were determined using logistic regression.|$|R
40|$|Abstract A {{meta-analysis}} of prior studies of techniques de-signed to induce mail <b>survey</b> <b>response</b> rates was conducted. Re-search encompassing 184 effects (study outcomes) in 115 studies (articles) for 17 predictors of {{response rate was}} examined. The average effect size across all manipulations was r =. 065, indicat-ing an average increase of about 6. 5 percent in response rates for manipulations. Effect sizes for specific predictors and two potential moderators of effects were examined. Results indicated that repeated contacts {{in the form of}} preliminary notification and follow-ups, appeals, inclusion of a return envelope, postage, and monetary incentives, were effective in increasing survey re-sponse rates. Significant effect sizes for the predictors ranged from an increase in response of 2 percent to 31 percent. Implica-tions of the results for the conduct of mail surveys and future research on mail <b>survey</b> <b>response</b> behavior are discussed. Researchers have amassed myriad techniques to stimulate mail <b>survey</b> <b>response</b> rates, reduce item omission, speed up response, and reduce response bias. Examples include the use of preliminary notification, follow-up, sponsorship, appeals, postage, personalization, incentives, anonymity, prior commitment, and techniques affecting questionnaire Correspondence regarding {{this article should be addressed}} to FRANCIS I. YAMMARINO...|$|R
