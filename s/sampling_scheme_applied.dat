5|9711|Public
30|$|Our {{microdata}} at {{the firm}} level come from the Survey on Business Strategies (Encuesta sobre Estrategias Empresariales, ESEE). This is an annual survey on {{a representative sample of}} Spanish firms in 18 manufacturing sectors which has the advantage of providing the key variables required to compute TFP and temp-to-perm conversion rates (see below). The available sample period is 1991 – 2005. Firms were chosen in the base year according to a <b>sampling</b> <b>scheme</b> <b>applied</b> to each industry in the manufacturing sector where weights depend on their size category. While all manufacturing firms with more than 200 employees are surveyed and their participation rate in the survey reaches approximately 70 %, smaller firms with 10 – 200 employees are surveyed according to a random sampling scheme with a participation rate close to 5 %.|$|E
40|$|Sampling {{scheme to}} {{quantify}} nitrogen leaching risk in grazed grassland: which scheme for which accuracy? In grazed grasslands, {{the heterogeneity of}} urine patches distribution needs adapted sampling scheme in order to evaluate N-NO 3 - leaching risks. In such context, {{the aim of this}} paper is to define the accuracy that could be expected from the <b>sampling</b> <b>scheme</b> <b>applied</b> in this agro-ecosystem (one average sample including 30 core samples taken in the 0 - 30 cm soil layer) and the sampling scheme to be applied in order to reach a pre-defined level of standard error. The data bases mobilized were mainly 78 grasslands sampled in 2004 or in 2005 in 24 dairy farms and 8 grasslands sampled during four years (2004 to 2007) in the experimental site in Gembloux. Our results underlined the significant increase of the standard deviation with the concentration mean. So, the mean explained 63 and 49 % of the standard deviation increase observed, respectively, in 2004 and 2005. In such context, the <b>sampling</b> <b>scheme</b> <b>applied</b> in the Sustainable Nitrogen Management Program (one composite sample of 30 core samples taken in the 0 - 30 cm soil layer) allowed to reach a precision of 10 kg N-NO 3 -. ha- 1 in 35 % of the grazed grasslands sampled. To shift to three composite samples per hectare allows reaching such an accuracy in 90 % of the parcels showing an average nitrite nitrogen content lower than 20 kg N-NO 3 -. ha- 1. For highest values, the samples number necessary to reach such a precision level increases quickly. In such context and in order to identify false positive parcels in audited farms, an important tolerance margin is defined around the threshold value. This value is defined yearly in farms integrating good practices in terms of nitrogen resources management. The use of alternatives indicators to evaluate N-NO 3 -. ha- 1 leaching risk is also evaluated...|$|E
40|$|I {{describe}} a simple procedure for investigating the convergence properties of Markov Chain Monte Carlo sampling schemes. The procedure employs multiple runs from a sampler, {{using the same}} random deviates for each run. When the sample paths from all sequences converge, {{it is argued that}} approximate equilibrium conditions hold. The procedure also provides a simple diagnostic for detecting modes in multimodal posteriors. Several examples of the procedure are provided. In Ising models, the relation between the correlation parameter and the convergence rate of rudimentary Gibbs samplers is investigated. In another example, the effects of multiple modes on the convergence of coupled paths are explored using mixtures of bivariate normal distributions. The technique is also used to evaluate the convergence properties of a Gibbs <b>sampling</b> <b>scheme</b> <b>applied</b> to a model for rat growth rates (Gelfand et al 1990). Acknowledgements I would like to thank Steve MacEachern, Julian Besag, Donald Rubin, A [...] ...|$|E
40|$|PURPOSE: To combine global cardiac {{function}} imaging with compressed sensing (CS) {{in order}} to reduce scan time and to validate this technique in normal mouse hearts and in a murine model of chronic myocardial infarction. MATERIALS AND METHODS: To determine the maximally achievable acceleration factor, fully acquired cine data, obtained in sham and chronically infarcted (MI) mouse hearts were 2 - 4 -fold undersampled retrospectively, followed by CS reconstruction and blinded image segmentation. Subsequently, dedicated CS <b>sampling</b> <b>schemes</b> were implemented at a preclinical 9. 4 T magnetic resonance imaging (MRI) system, and 2 - and 3 -fold undersampled cine data were acquired in normal mouse hearts with high temporal and spatial resolution. RESULTS: The retrospective analysis demonstrated that an undersampling factor of three is feasible without impairing accuracy of cardiac functional parameters. Dedicated CS <b>sampling</b> <b>schemes</b> <b>applied</b> prospectively to normal mouse hearts yielded comparable left-ventricular functional parameters, and intra- and interobserver variability between fully and 3 -fold undersampled data. CONCLUSION: This study introduces and validates an alternative means to speed up experimental cine-MRI without the need for expensive hardware...|$|R
40|$|We {{propose a}} class of {{strongly}} efficient rare event simulation estimators for random walks and compound Poisson processes with a regularly varying increment/jump-size distribution in a general large deviations regime. Our estimator {{is based on an}} importance sampling strategy that hinges on the heavy-tailed sample path large deviations result recently established in Rhee, Blanchet, and Zwart (2016). The new estimators are straightforward to implement and can be used to systematically evaluate the probability {{of a wide range of}} rare events with bounded relative error. They are "universal" in the sense that a single importance <b>sampling</b> <b>scheme</b> <b>applies</b> to a very general class of rare events that arise in heavy-tailed systems. In particular, our estimators can deal with rare events that are caused by multiple big jumps (therefore, beyond the usual principle of a single big jump) as well as multidimensional processes such as the buffer content process of a queueing network. We illustrate the versatility of our approach with several applications that arise in the context of mathematical finance, actuarial science, and queueing theory...|$|R
40|$|For {{high volume}} data streams and large data warehouses, {{sampling}} {{is used for}} efficient approximate answers to aggregate queries over selected subsets. Mathematically, {{we are dealing with}} a set of weighted items and want to support queries to arbitrary subset sums. With unit weights, we can compute subset sizes which together with the previous sums provide the subset averages. The question addressed here is which <b>sampling</b> <b>scheme</b> we should use to get the most accurate subset sum estimates. We present a simple theorem on the variance of subset sum estimation and use it to prove variance optimality and near-optimality of subset sum estimation with different known <b>sampling</b> <b>schemes.</b> This variance is measured as the average over all subsets of any given size. By optimal we mean there is no set of input weights for which any <b>sampling</b> <b>scheme</b> can have a better average variance. Such powerful results can never be established experimentally. The results of this paper are derived mathematically. For example, we show that appropriately weighted systematic sampling is simultaneously optimal for all subset sizes. More standard schemes such as uniform sampling and probability-proportional-to-size sampling with replacement can be arbitrarily bad. Knowing the variance optimality of different <b>sampling</b> <b>schemes</b> can help deciding which <b>sampling</b> <b>scheme</b> to <b>apply</b> in a given context. ...|$|R
40|$|Urban {{areas are}} {{continuously}} expanding today, extending {{their influence on}} an increasingly large proportion of woods and trees located in or nearby urban and urbanizing areas, the socalled urban forests. Although these forests {{have the potential for}} significantly improving the quality the urban environment and the well-being of the urban population, data to quantify the extent and characteristics of urban forests are still lacking or fragmentary on a large scale. In this regard, an expansion of the domain of multipurpose forest inventories like National Forest Inventories (NFIs) towards urban forests would be required. To this end, it would be convenient to exploit the same <b>sampling</b> <b>scheme</b> <b>applied</b> in NFIs to assess the basic features of urban forests. This paper considers approximately unbiased estimators of abundance and coverage of urban forests, together with estimators of the corresponding variances, which can be achieved from the first phase of most largescale forest inventories. A simulation study is carried out in order to check the performance of the considered estimators under various situations involving the spatial distribution of the urban forests over the study area. An application is worked out on the data from the Italian NFI. L'articolo è disponibile sul sito dell'editore www. springer. co...|$|E
40|$|Since 2000, five {{different}} serotypes (1, 2, 4, 9, 16) of Bluetongue virus (BTV), {{a member of}} the genus Orbivirus within the family Reoviridae, have been actively circulating in Italy in animal species which are sensitive to the infection. Several regions in Southern and Central Italy have been involved in this epidemic. From 2006 to date, serious epidemic waves of a different serotype of BTV (serotype 8) have also occurred in Central-Northern Europe. In 2007, more than 12, 000 outbreaks were recorded in Belgium, the Netherlands, France, Luxembourg and Germany. Every year, Italy, particularly the Veneto Region, imports several hundred thousands cattle for the restocking of feed-lots from these countries. In particular, in 2007 the Veneto Region imported about 420, 000 cattle from France. From 26 October 2007 to 26 February 2008, 33 of these animals, introduced to 15 different meat-cattle holdings in the Region, were found to be seropositive to BTV- 8 (30 of these bovines were also PCR positive). This new epidemiological situation has focussed the attention on strengthening the BT surveillance system in this Region, in order to early detect any introduction of BTV- 8 and put proper sanitary measures into place to limit the possible spread of infection. The aim {{of the present study was}} to identify the geographical areas most exposed to the risk of virus introduction and spread in Veneto, in order to target monitoring activities in those areas. An Exploratory Spatial Data Analysis was carried out using MCM (Multiple Criteria Method) system. The analysis took into consideration the <b>sampling</b> <b>scheme</b> <b>applied</b> in BT surveillance programme and BT risk factors. Data on importation of cattle from areas at risk where active virus circulation was detected, density of dairy and meat cattle, and the presence and density of vectors. The application of MCM system allowed the identification of three different areas exposed to a higher risk of BTV- 8 introduction...|$|E
40|$|Model {{comparison}} is discussed from an information theoretic point of view. In particular the posterior predictive entropy {{is related to}} the target yielding DIC and modifications thereof. The adequacy of criteria for posterior predictive model {{comparison is}} also investigated depending on the comparison to be made. In particular variable selection as a special problem of model choice is formalized in different ways according to whether the comparison is a comparison across models or within an encompassing model and whether a joint or conditional <b>sampling</b> <b>scheme</b> is <b>applied.</b> DIC has been devised for comparisons across models. Its use in variable selection and that of other criteria is illustrated for a simulated data set. Key words: posterior predictive entropy, mutual information, model comparison, hypothesis testing Running head: DIC...|$|R
40|$|Although a nonparametric {{regression}} model {{allows us to}} obtain a graphical display {{of the relationship between}} the response variable and the independent variables, the exact form of the regression function is not evident. This weakness can be overcome by a parametric model since the relationship between the dependent and the independent variables is specified mathematically. The objective of this paper is to introduce a new graphical approach, called the shift function plot, with which a hypothesis test is constructed to evaluate the goodness of fit of a parametric {{regression model}}. Under multi-stage stratified <b>sampling</b> <b>schemes,</b> we <b>apply</b> the shift function plots to survey data. Asymptotic properties of the survey estimator of the shift function are established. An empirical example from the 1990 Ontario Health Survey is used to illustrate the application of the shift function...|$|R
40|$|Abstract. The Hammersley and Halton point sets, two {{well known}} low {{discrepancy}} sequences, {{have been used}} for quasi-Monte Carlo integration in previous research. A deterministic formula generates a uniformly distributed and stochastic-looking sampling pattern, at low computational cost. The Halton point set is also useful for incremental sampling. In this paper, we discuss detailed implementation issues and our experience of choosing suitable bases of the point sets, not just on the 2 D plane, but also on a spherical surface. The <b>sampling</b> <b>scheme</b> is also <b>applied</b> to ray tracing, with a significant improvement in error. ...|$|R
40|$|Determinantal point {{processes}} (DPPs) are random {{point processes}} well-suited for modeling repulsion. In machine learning, {{the focus of}} DPP-based models has been on diverse subset selection from a discrete and finite base set. This discrete setting admits an efficient sampling algorithm based on the eigendecomposition of the defining kernel matrix. Recently, there has been growing interest in using DPPs defined on continuous spaces. While the discrete-DPP sampler extends formally to the continuous case, computationally, the steps required are not tractable in general. In this paper, we present two efficient DPP <b>sampling</b> <b>schemes</b> that <b>apply</b> {{to a wide range}} of kernel functions: one based on low rank approximations via Nyström and random Fourier feature techniques and another based on Gibbs sampling. We demonstrate the utility of continuous DPPs in repulsive mixture modeling and synthesizing human poses spanning activity spaces. ...|$|R
40|$|Abstract—In {{this paper}} we discuss the continuationratio model for ordinal data. This {{particular}} type of model is to model the probability of one particular category given the categories proceeding this one. It can be shown that estimation of the continuationratio model parameters can be done efficiently by using the techniques in fitting the binary data models. In this way, {{one does not have}} to estimate the cut-point parameters as in the cumulative probability models. A Bayesian approach with the use of Gibbs sampler is adopted in this paper. The adaptive rejection sampling method proposed by Gilks and Wild is used. The adaptive rejection sampling (ARS) algorithm is an efficient and direct method to sample from complicated log-concave densities often found in many Gibbs <b>sampling</b> <b>scheme.</b> We <b>applied</b> this model to analyse data obtained from experiments about quality of telephone connection conducted by British Telecom (BT) Laboratory. The final results are satisfactory...|$|R
40|$|A {{common goal}} in {{microarray}} experiments {{is to identify}} genes that are differentially expressed among two or more biological conditions. There is currently no standard methodology for detecting differential expression in time course studies. However, {{it is clear that}} monitoring the behavior of gene expression over time is important and will be a common experimental design in the future. Here we present a general statistical significance method for detecting temporal differential expression that {{can be applied to the}} typical types of comparisons and <b>sampling</b> <b>schemes.</b> We <b>apply</b> this method to two studies that we have carried out on humans. The goal of one study is to identify genes showing temporal differential expression between controls and endotoxin-treated individuals, and the other is to identify genes that show aging effects in the kidney. Genes identified in both studies corroborate previous findings and also provide novel insights. This methodology has been implemented in the freely distributed EDGE software package. aging, differential expression, endotoxin, expression arrays, time series, q-values,...|$|R
40|$|Dendroclimatologists often {{approach}} {{field work}} {{with the intent of}} reconstructing a particular climate variable (e. g. temperature, streamflow, precipitation). Although guidelines exist for species and site selection, isolating the signal of interest is difficult in areas with complex terrain or a lack of ideal sites. In this case study, I suggest climatological techniques for a more efficient <b>sampling</b> <b>scheme</b> and <b>apply</b> these techniques to identify criteria for selecting sites sensitive to winter precipitation in the north-central Rocky Mountains. These techniques include examining factors influencing the regional response of tree growth to climate by utilizing the International Tree-Ring Databank (ITRDB), using eigenvector analyses to identify modes of variability between sites, and delineating climate regions based on the variable of interest through climate regionalization. Results suggest that low- or mid-elevation Pseudotsuga menziesii sites should be targeted for maximizing the winter precipitation signal in the case study area. The season of precipitation impacting growth was found to be a major component of the overall variability between sites...|$|R
40|$|Increasingly, data is {{published}} {{in form of}} semantic graphs. The most notable example is the Linked Open Data (LOD) initiative where {{an increasing number of}} data sources are published in the Semantic Web’s Resource Description Framework and where the various data sources are linked to reference one another. In this paper we apply machine learning to semantic graph data and argue that scalability and robustness can be achieved via an urn-based statistical <b>sampling</b> <b>scheme.</b> We <b>apply</b> the urn model to the SUNS framework which is based on multivariate prediction. We argue that multivariate prediction approaches are most suitable for dealing with the resulting high-dimensional sparse data matrix. Within the statistical framework, the approach scales up to large domains and is able to deal with highly sparse relationship data. We summarize experimental results using a friend-of-a-friend data set and a data set derived from DBpedia. In more detail, we describe novel experiments on disease gene prioritization using LOD data sources. The experiments confirm the ease-of-use, the scalability and the good performance of the approach...|$|R
40|$|Quantile and {{quantile}} effect {{functions are}} important tools for descriptive and inferential analysis {{due to their}} natural and intuitive interpretation. Existing inference methods for these functions {{do not apply to}} discrete and mixed continuous-discrete random variables. This paper offers a simple, practical construction of the simultaneous confidence bands for quantile and quantile effect functions. It is based on a natural transformation of simultaneous confidence bands for distribution functions, which are readily available for many problems. The construction is generic and does not depend {{on the nature of the}} underlying problem. It works in conjunction with parametric, semiparametric, and nonparametric modeling strategies and does not depend on the <b>sampling</b> <b>scheme.</b> We <b>apply</b> our method to characterize the distributional impact of insurance coverage on health care utilization and obtain the distributional decomposition of the racial test score gap. Our analysis generates new, interesting empirical findings, and complements previous analyses that focused on mean effects only. In both applications, the outcomes of interest are discrete rendering existing inference methods invalid for obtaining uniform confidence bands for quantile and quantile effects functions. Comment: 30 pages, 6 figure...|$|R
40|$|Abstract. Increasingly, data is {{published}} {{in the form of}} semantic graphs. The most notable example is the Linked Open Data (LOD) initiative where an increasing number of data sources are {{published in the}} Semantic Web’s Resource Description Framework and where the various data sources are linked to reference one another. In this paper we apply machine learning to semantic graph data and argue that scalability and robustness can be achieved via an urn-based statistical <b>sampling</b> <b>scheme.</b> We <b>apply</b> the urn model to the SUNS framework which is based on multivariate prediction. We argue that multivariate prediction approaches are most suitable for dealing with the resulting high-dimensional sparse data matrix. Within the statistical framework, the approach scales up to large domains and is able to deal with highly sparse relationship data. We summarize experimental results using a friend-of-a-friend data set and a data set derived from DBpedia. In more detail, we describe novel experiments on disease gene prioritization using LOD data sources. The experiments confirm the ease-of-use, the scalability and the good performance of the approach...|$|R
40|$|This {{paper is}} {{turned to the}} {{advanced}} Monte Carlo methods for realistic image creation. It offers a new stratified approach for solving the rendering equation. We consider the numerical solution of the rendering equation by separation of integration domain. The hemispherical integration domain is symmetrically separated into 16 parts. First 9 sub-domains are equal size of orthogonal spherical triangles. They are symmetric each to other and grouped with a common vertex around the normal vector to the surface. The hemispherical integration domain is completed with more 8 sub-domains of equal size spherical quadrangles, also symmetric each to other. All sub-domains have fixed vertices and computable parameters. The bijections of unit square into an orthogonal spherical triangle and into a spherical quadrangle are derived and used to generate sampling points. Then, the symmetric <b>sampling</b> <b>scheme</b> is <b>applied</b> to generate the sampling points distributed over the hemispherical integration domain. The necessary transformations are made and the stratified Monte Carlo estimator is presented. The rate of convergence is obtained and one {{can see that the}} algorithm is of super-convergent type...|$|R
40|$|Electron {{tomography}} usually {{suffers from}} so called missing wedge artifacts caused by limited tilt angle range. An equally sloped tomography (EST) acquisition scheme (which {{should be called}} the linogram <b>sampling</b> <b>scheme)</b> was recently <b>applied</b> to achieve 2. 4 -angstrom resolution. On the other hand, a compressive sensing-inspired reconstruction algorithm, known as adaptive dictionary based statistical iterative reconstruction (ADSIR), has been reported for x-ray computed tomography. In this paper, we evaluate the EST, ADSIR and an ordered-subset simultaneous algebraic reconstruction technique (OS-SART), and compare the ES and equally angled (EA) data acquisition modes. Our results show that OS-SART is comparable to EST, and the ADSIR outperforms EST and OS-SART. Furthermore, the equally sloped projection data acquisition mode has no advantage over the conventional equally angled mode in the context...|$|R
40|$|Phylogenetics is {{the study}} of the {{evolutionary}} relationship between species. Inference of phylogeny relies heavily on statistical models that have been extended and refined tremendously over the past years into very complex hierarchical models. Paper I introduces probabilistic graphical models to statistical phylogenetics and elaborates on the potential advantages a unified graphical model representation could have for the community, e. g., by facilitating communication and improving reproducibility of statistical analyses of phylogeny and evolution. Once the phylogeny is reconstructed it is possible to infer the rates of diversification (speciation and extinction). In this thesis I extend the birth-death process model, {{so that it can be}} applied to incompletely sampled phylogenies, that is, phylogenies of only a subsample of the presently living species from one group. Previous work only considered the case when every species had the same probability to be included and here I examine two alternative sampling schemes: diversified taxon sampling and cluster sampling. Paper II introduces these <b>sampling</b> <b>schemes</b> under a constant rate birth-death process and gives the probability density for reconstructed phylogenies. These models are extended in Paper IV to time-dependent diversification rates, again, under different <b>sampling</b> <b>schemes</b> and <b>applied</b> to empirical phylogenies. Paper III focuses on fast and unbiased simulations of reconstructed phylogenies. The efficiency is achieved by deriving the analytical distribution and density function of the speciation times in the reconstructed phylogeny. At the time of the doctoral defense, the following papers were unpublished and had a status as follows: Paper 1 : Manuscript. Paper 4 : Accepted. </p...|$|R
40|$|Geostatistical {{techniques}} {{were used to}} assess the spatial patterns of spores of arbuscular mycorrhizal fungi (AMF) in soils from two contrasting plant communities: a salt marsh containing only arbuscular mycorrhizal and non-mycorrhizal plants in a distinct clumped distribution pattern and a maquis with different types of mycorrhiza where most plants were relatively randomly distributed. Also evaluated was the relationship between the spatial distribution of spores and AM plant distribution and soil properties. A nested <b>sampling</b> <b>scheme</b> was <b>applied</b> in both sites with sample cores taken from nested grids. Spores of AMF and soil characteristics (organic matter and moisture) were quantified in each core, and core sample location was related to plant location. Semivariograms for spore density indicated strong spatial autocorrelation and a patchy distribution within both sites for all AM fungal genera found. However, the patch size differed between the two plant communities and AM fungal genera. In the salt marsh, AM fungal spore distribution was correlated with distance to AM plants and projected stand area of AM plants. In maquis, spatial AM fungal spore distribution was correlated with organic matter. These results suggest that spore distribution of AMF varied between the two plant communities according to plant distribution and soil properties...|$|R
40|$|Discrete-time hidden Markov {{models are}} a broadly useful class of latent-variable models with {{applications}} {{in areas such}} as speech recognition, bioinformatics, and climate data analysis. It is common in practice to introduce temporal non-homogeneity into such models by making the transition probabilities dependent on time-varying exogenous input variables via a multinomial logistic parametrization. We extend such models to introduce additional non-homogeneity into the emission distribution using a generalized linear model (GLM), with data augmentation for sampling-based inference. However, the presence of the logistic function in the state transition model significantly complicates parameter inference for the overall model, particularly in a Bayesian context. To address this we extend the recently-proposed Polya-Gamma data augmentation approach to handle non-homogeneous hidden Markov models (NHMMs), allowing the development of an efficient Markov chain Monte Carlo (MCMC) <b>sampling</b> <b>scheme.</b> We <b>apply</b> our model and inference scheme to 30 years of daily rainfall in India, leading to a number of insights into rainfall-related phenomena in the region. Our proposed approach allows for fully Bayesian analysis of relatively complex NHMMs on a scale that was not possible with previous methods. Software implementing the methods described in the paper is available via the R package NHMM. Comment: 40 pages, 26 figure...|$|R
40|$|In this paper, we {{introduce}} a new Poisson mixture model for count panel data where the underlying Poisson process intensity is determined endogenously by consumer latent utility maximization over a set of choice alternatives. This formulation accommodates the choice and count in a single random utility framework with desirable theoretical properties. Individual heterogeneity is introduced through a random coefficient scheme with a flexible semiparametric distribution. We deal with the analytical intractability of the resulting mixture by recasting the model as an embedding of infinite sequences of scaled moments of the mixing distribution, and newly derive their cumulant representations along with bounds on their rate of numerical convergence. We further develop an efficient recursive algorithm for fast evaluation of the model likelihood within a Bayesian Gibbs <b>sampling</b> <b>scheme.</b> We <b>apply</b> our model to a recent household panel of supermarket visit counts. We estimate the nonparametric density of three key variables of interest–price, driving distance, and their interaction–while controlling {{for a range of}} consumer demographic characteristics. We use this econometric framework to assess the opportunity cost of time and analyze the interaction between store choice, trip frequency, search intensity, and household and store characteristics. We also conduct a counterfactual welfare experiment and compute the compensating variation for a 10 %– 30 % increase in Walmart prices...|$|R
40|$|Although visual {{surveillance}} {{has emerged}} as an effective technolody for public security, privacy has become an issue of great concern in the transmission and distribution of surveillance videos. For example, personal facial images should not be browsed without permission. To cope with this issue, face image scrambling {{has emerged as}} a simple solution for privacy- related applications. Consequently, online facial biometric verification needs to be carried out in the scrambled domain thus bringing a new challenge to face classification. In this paper, we investigate face verification issues in the scrambled domain and propose a novel scheme to handle this challenge. In our proposed method, to make feature extraction from scrambled face images robust, a biased random subspace <b>sampling</b> <b>scheme</b> is <b>applied</b> to construct fuzzy decision trees from randomly selected features, and fuzzy forest decision using fuzzy memberships is then obtained from combining all fuzzy tree decisions. In our experiment, we first estimated the optimal parameters for the construction of the random forest, and then applied the optimized model to the benchmark tests using three publically available face datasets. The experimental results validated that our proposed scheme can robustly cope with the challenging tests in the scrambled domain, and achieved an improved accuracy over all tests, making our method a promising candidate for the emerging privacy-related facial biometric applications...|$|R
40|$|Soil {{organic carbon}} (SOC) change {{influences}} the life-cycle assessment (LCA) calculations for globally traded bio-based products. Broad {{agreement on the}} importance of SOC measurement stands in contrast with inconsistent measurement methods. This paper focuses on published SOC research on lands managed for maize (Zea mays L.) in the U. S. and sugarcane (Saccharum officinarum L.) in Brazil. A literature review found that reported SOC measurement protocols reflect different sampling strategies, measurement techniques, and laboratory analysis methods. Variability in sampling techniques (pits versus core samples), depths, increments for analysis, and analytical procedures (wet oxidation versus dry combustion) can influence reported SOC values. To improve consistency and comparability in future SOC studies, the authors recommend that: (a) the methods applied for each step in SOC studies be documented; (b) a defined protocol for soil pits or coring be applied; (c) samples be analyzed at 10 cm intervals for the full rooting depth and at 20 cm intervals below rooting until reaching 100 cm; (d) stratified <b>sampling</b> <b>schemes</b> be <b>applied</b> where possible to reflect variability across study sites; (e) standard laboratory techniques be used to differentiate among labile and stable SOC fractions; and (f) more long-term, diachronic approaches be used to assess SOC change. We conclude with suggestions for future research to further improve the comparability of SOC measurements across sites and nations...|$|R
40|$|The reduced density {{matrix of}} {{excitons}} coupled to a phonon bath at a finite temper-ature is studied using the path integral Monte Carlo method. Appropriate choices of estimators and importance <b>sampling</b> <b>schemes</b> {{are crucial to}} the performance of the Monte Carlo simulation. We show that by choosing the population-normalized estimator for the reduced density matrix, an efficient and physically-meaningful sam-pling function can be obtained. In addition, the nonadiabatic phonon probability density is obtained as a byproduct during the sampling procedure. For importance sampling, we adopted the Metropolis-adjusted Langevin algorithm. The analytic ex-pression for the gradient of the target probability density function associated with the population-normalized estimator cannot be obtained in closed form without a matrix power series. An approximated gradient that can be efficiently calculated is explored to achieve better computational scaling and efficiency. Application to a simple one-dimensional model system from the previous literature confirms the cor-rectness of the method developed in this manuscript. The displaced harmonic model system within the single exciton manifold shows the numerically exact temperature dependence of the coherence and population of the excitonic system. The <b>sampling</b> <b>scheme</b> can be <b>applied</b> to an arbitrary anharmonic environment, such as multichro-mophoric systems embedded in the protein complex. The result of this study is expected to stimulate further development of real time propagation methods that satisfy the detailed balance condition for exciton populations. 1 a...|$|R
40|$|The International Conference for Harmonization (ICH) has {{released}} regulatory guidelines for Pharmaceutical Development. In the document ICH Q 8, The Design Space {{of a process}} is presented as the set of factor settings providing satisfactory results. However, ICH Q 8 does not propose any practical methodology to define, derive and compute Design Space. In parallel, in the last decades, {{it has been observed}} that the diversity and the quality of analytical methods have evolved exponentially allowing substantial gains in selectivity and sensitivity. However, there is still a lack for a rationale towards the development of robust separation methods in a systematic way. Applying ICH Q 8 to analytical methods provides a methodology for predicting a region of the space of factors in which results will be reliable. Combining design of experiments and Bayesian standard multivariate regression, an identified form of the predictive distribution of a new response vector has been identified and used, under non-informative as well as informative prior distributions of the parameters. From the responses and their predictive distribution, various critical quality attributes can be easily derived. This Bayesian framework was then extended to the multi-criteria setting to estimate the predictive probability that several critical quality attributes will be jointly achieved in the future use of an analytical method. An example based on a high-performance liquid chromatography (HPLC) method is given. For this example, a constrained <b>sampling</b> <b>scheme</b> was <b>applied</b> to ensure the modeled responses have desirable properties. Peer reviewe...|$|R
40|$|Studies on liver macrophages have elucidated their {{key roles}} in immunological, fibrotic and {{regenerative}} responses, and shown that macrophages {{are not a}} homogeneous population. In the rat, two sets of liver macrophages coexist, identified by ED 1 and ED 2 antibodies. Those sets have different quantitative responses in liver injuries and may have different tasks throughout the injury and recovery phases. Nevertheless, the total number (N), number per gram (N g− 1) and proportion of those macrophages {{in relation to other}} liver cells has never been quantified using design-based stereology. Thus, we combined immunocytochemistry with those tools to produce an unbiased estimate of the N of ED 1 + and of ED 2 + cells. A smooth fractionator <b>sampling</b> <b>scheme</b> was <b>applied</b> to the liver of five male Wistar rats (3 months old), to obtain systematic uniform random sections (30 µm thick); these were immunostained with the monoclonal antibodies: ED 1, a pan-macrophagic marker; and ED 2, which identifies the completely differentiated macrophages, i. e. Kupffer cells. The N of ED 1 + cells was 340 × 106, estimated with a coefficient of error (CE) of 0. 04, and that of ED 2 + cells was 283 × 106, with a CE of 0. 05. These figures correspond to 10. 7 % and 8. 9 %, respectively, of the total liver cells. The new data constitute reference values for correlative inferences. Also, the methodological strategy, by its accuracy and precision, is valuable for future investigations on the liver cell composition in various models of disease, and especially for studying the more subtle variations that occur during the injury and recovery phases...|$|R
40|$|This thesis investigates and {{develops}} a {{stratified random sampling}} design for sediments in an offshore oil field environment. The sampling area was partitioned equally into 16 Zones, stratification {{were based on the}} near field and far field areas, and the number of samples in each Zone was chosen by proportional allocation, i. e. proportional to the available appropriate area (far field). Measurement techniques applied to the samples included laser granulometry, ultraviolet fluorescence, gas chromatography using mass selective detection or flame ionisation detection and elemental analysis. The total PAH concentrations (2 - to 6 -ring parent and alkylated PAHs, including the 16 US EPA PAHs) in sediments were relatively low (< 100 pg kg" dry weight). The PAH concentrations, Forties crude oil equivalent and diesel oil equivalent concentrations were generally higher in sediment of fine grain size and higher organic carbon loading. PAH distributions and concentration ratios Indicated a predominantly pyrolytic input, being dominated by the heavier, more persistent, 5 - and 6 -ring compounds, and with a high proportion of parent PAHs. The nalkane profiles of a number of the sediments contained small, high boiling, UCMs, indicative of weathered oil arising from a limited petrogenic input. Spatial structure analysis shows the existence of a trend in the variogram, and also the spatial pattern in the contour maps of the parameters measured, shows that the regionalized variable exhibited non-stationarity and were non-ergodic. The stratified random <b>sampling</b> <b>scheme</b> showed significant advantages over a classical grid <b>sampling</b> <b>scheme</b> when <b>applied</b> to the same area. Specifically, the stratified random sampling design gave much more reliable mean concentrations for all the parameters, achieving a much lower variance than the grid sampling. A further composite random <b>sampling</b> <b>scheme</b> was designed for sediments in the near-shore. The aim is to estimate a within-stratum mean value for each of the chosen measurement parameters with more thorough coverage (better representation), better precision and less variance at lower analytical cost. This scheme was trialed in two near-shore environments, the Clyde Estuary and the Firth of Forth. The results show {{no significant differences between the}} mean and distribution profile of the individual samples and the composite samples for all the parameters measured. This work utilised the best modern chemical analytical methods for the quantification of a range of hydrocarbon species, and utilised the results in a modem risk-based approach to environmental assessment. The new stratified random sampling design has been accepted for use in the national marine monitoring programme (NMMP) In the United Kingdom. EThOS - Electronic Theses Online ServiceFederal Republic of Nigeria, Petroleum Technology Development FundGBUnited Kingdo...|$|R
40|$|The aim of {{this study}} was to {{investigate}} sporulation dynamics of arbuscular mycorrhizal fungal (AMF) communities from agroecosystems differing in land use intensity in long-term experimental microcosms. These were set up with characteristic grassland plants (Lolium perenne, Trifolium pratense, Plantago lanceolata), and inoculated with soils from several grasslands and arable lands subjected to crop rotation or continuous monocropping. The microcosms were maintained under ambient light and temperature conditions over 3 years. A novel, localized <b>sampling</b> <b>scheme</b> was <b>applied</b> for attaining exclusively the newly formed spores at bimonthly intervals. Overall, 39 AMF species were detected by morphological spore identification. Some species were recovered from all sites, others exclusively from arable lands, or grasslands, or from all sites except under maize monocropping. Clear seasonal and successional AMF sporulation dynamics were revealed, implying different life strategies of different AMF species. A first group of Glomus spp., including G. mosseae, sporulated rapidly during the first season. A second group, including G. constrictum and G. fasciculatum, sporulated late in the first season and replaced the first group during subsequent seasons. A large third group, including G. invermaium, G. macrocarpum and G. sinuosum, sporulated much later, in the second or third season. Acaulospora, Archaeospora and Ambispora spp. sporulated mainly during spring and early summer, Scutellospora and Cetraspora spp. only in fall. While in the microcosms derived from arable lands, cumulative species numbers did not increase anymore after 2 years, the numbers still increased significantly in the microcosms from the grasslands indicating longer lasting periods of sporulation cycles. Remarkably, the arable land under organic farming produced the highest AMF species richness, even higher than the grasslands. In conclusion, AMF communities from distinct agro-ecosystems differed in species composition and seasonal and successional sporulation dynamics...|$|R
40|$|International audienceThe symmetry-adapted Monte Carlo <b>sampling</b> <b>scheme</b> is <b>applied</b> for the {{ab initio}} study of two mineralsystems, namely the calcite {{structured}} compound Ca 0. 75 Mg 0. 25 CO 3 and soda-melilite (Na,Ca) AlSi 2 O 7. Itis shown how an {{extensive use of}} symmetry, from the sampling of atomic configurations up to thequantum-mechanical calculation, makes feasible the investigation of large configuration spaces. As forthe sampling, we describe an effective procedure to specifically target low-energy configurations onthe potential energy surface of supercells of virtually any size. It {{is based on the}} suggestion that a correlationbetween symmetry and energy of the configurations exists according to which atomic distributionsof minimum and maximum energy are likely to have some spatial symmetry. This hypothesis is verifiedempirically and leads to a significant alleviation of the original problem by virtue of the possibility of tailoringthe symmetry-adapted Monte Carlo to select only symmetric configurations. The latter are alsofound to display a probability distribution {{similar to that of the}} entire set of configurations, thus providing,eventually, a suitable ab initio reference for the parameterization of model Hamiltonians. The moststable configuration so identified is used as pivot for the selection of new configurations having the sameatomic distribution but for the exchange of a couple of atoms. These are called ‘‘neighbors” to highlightboth their structural and energetic proximity to the pivot. We illustrate how, by collecting neighbors ofconfigurations of increasing energy, the description of the system can be progressively and deterministicallyimproved up to convergence of the calculated average properties, whatever the temperature. The same scheme works when moving to a supercell larger than the initial one (but of equivalent symmetry) since it is shown that stable structures remain so at any volume...|$|R
40|$|Access to {{web-scale}} corpora {{is gradually}} bringing robust automatic knowledge base creation and extension within reach. To exploit these large unannotated [...] -and {{extremely difficult to}} annotate [...] -corpora, unsupervised machine learning methods are required. Probabilistic models of text have recently found some success as such a tool, but scalability remains an obstacle in their application, with standard approaches relying on <b>sampling</b> <b>schemes</b> that {{are known to be}} difficult to scale. In this report, we therefore present an empirical assessment of the sublinear time sparse stochastic variational inference (SSVI) <b>scheme</b> <b>applied</b> to RelLDA. We demonstrate that online inference leads to relatively strong qualitative results but also identify some of its pathologies [...] -and those of the model [...] -which will need to be overcome if SSVI is to be used for large-scale relation extraction...|$|R
40|$|Abstract. A {{central limit theorem}} for the {{realized}} volatility {{based on}} a gen-eral stochastic <b>sampling</b> <b>scheme</b> is proved. The asymptotic distribution de-pends on the <b>sampling</b> <b>scheme,</b> which is written explicitly {{in terms of the}} asymptotic skewness and kurtosis of returns. The conditions for the central limit theorem to hold are examined for several concrete examples of schemes. A lower bound for mean-squared error is attained by a specific <b>sampling</b> <b>scheme.</b> More efficient <b>sampling</b> <b>schemes</b> for the Euler-Maruyama approximation than the usual equidistant scheme are given as an application. 1...|$|R
40|$|This paper targets the {{simulation}} of continuous-time Markov chain models of fault-tolerant systems with deferred repair. We start by stating sufficient {{conditions for a}} given importance <b>sampling</b> <b>scheme</b> to satisfy the bounded relative error property. Using those sufficient conditions, {{it is noted that}} many previously proposed importance <b>sampling</b> <b>schemes</b> such as failure biasing and balanced failure biasing satisfy that property. Then, we adapt the importance <b>sampling</b> <b>schemes</b> failure transition distance biasing and balanced failure transition distance biasing so as to develop new importance <b>sampling</b> <b>schemes</b> which can be implemented with moderate effort {{and at the same time}} can be proved to be more efficient for balanced systems than the simpler failure biasing and balanced failure biasing schemes. The increased efficiency for balanced and unbalanced systems of the new adapted importance <b>sampling</b> <b>schemes</b> is illustrated using examples. Preprin...|$|R
40|$|We first review {{existing}} sequential {{methods for}} estimating a binomial proportion. Afterward, we propose {{a new family}} of group sequential <b>sampling</b> <b>schemes</b> for estimating a binomial proportion with prescribed margin of error and confidence level. In particular, we establish the uniform controllability of coverage probability and the asymptotic optimality for such a family of <b>sampling</b> <b>schemes.</b> Our theoretical results establish {{the possibility that the}} parameters of this family of <b>sampling</b> <b>schemes</b> can be determined so that the prescribed level of confidence is guaranteed with little waste of samples. Analytic bounds for the cumulative distribution functions and expectations of sample numbers are derived. Moreover, we discuss the inherent connection of various <b>sampling</b> <b>schemes.</b> Numerical issues are addressed for improving the accuracy and efficiency of computation. Computational experiments are conducted for comparing <b>sampling</b> <b>schemes.</b> Illustrative examples are given for applications in clinical trials. Comment: 38 pages, 9 figure...|$|R
