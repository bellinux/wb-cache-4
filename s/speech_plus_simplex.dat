0|66|Public
500|$|Palmer, Beverly Wilson and Holly Byers Ochoa, eds. The Selected Papers of Thaddeus Stevens 2 vol (1998), 900pp; his <b>speeches</b> <b>plus</b> {{letters to}} and from Stevens ...|$|R
30|$|A {{comparison}} of the change scores for different classes within the same condition showed {{that there were no}} significant differences between classes, (speech alone: F(2, 42)[*]=[*] 0.29, p[*]=[*] 0.746, <b>speech</b> <b>plus</b> gesture: F(2, 48)[*]=[*] 1.05, p[*]=[*] 0.357).|$|R
30|$|A {{multiple}} regression analysis with change score {{as the dependent variable}} and condition (speech alone or <b>speech</b> <b>plus</b> gesture), gender, age, ethnicity, and primary language as independent variables showed a significant effect only for condition, β[*]=[*] 0.28, t(82)[*]=[*] 2.61, p[*]=[*] 0.01.|$|R
3000|$|The soft {{thresholding}} can {{be viewed}} as setting the components of the noise to zero and performing a magnitude subtraction on the <b>speech</b> <b>plus</b> noise components. It is evident that the soft thresholding eliminates the time-frequency discontinuity resulting in smoother signals, but it yields the estimated coefficients that are the WP coefficients [...]...|$|R
40|$|Speech {{intelligibility}} in adverse situations, such as reverberation and noise, is conserved {{until the}} degradations reach certain thresholds. Psychoacoustic studies {{have described the}} properties of speech {{that lead to the}} conservation of its intelligibility under those circumstances. The neural mechanisms that underlie the robustness of intelligibility in these situations, however, are not yet well understood. Here, the cortical representations of speech in reverberation and <b>speech</b> <b>plus</b> noise in reverberation are studied by measuring the cortical responses of human subjects using magnetoencephalography (MEG) while they listened to continuous speech narratives. It was hypothesized that the neural processing of speech in reverberation and <b>speech</b> <b>plus</b> noise in reverberation would follow a lack of cortical synchronization as function of the degradations. Encoding models show, however, that the neural encoding of speech in reverberation follow a different mechanism than that of speech in noise. On the other hand, in the absence of noise, it is possible to reconstruct with high accuracy the envelope of reverberant speech, thus demonstrating that the reverberant speech is well encoded by the brain...|$|R
50|$|Although {{he grew up}} in {{a family}} of health care specialists, he was hired as a 17-year-old high school junior by Coast Federal Savings & Loan of Los Angeles (now merged with Bank of the West) to deliver {{patriotic}} speeches to rotary clubs, churches, veterans’ groups and other organizations. He was paid $50 a <b>speech</b> <b>plus</b> gas mileage and $1 for every audience member. He often earned hundreds of dollars each week.|$|R
50|$|Each event {{begins with}} a keynote <b>speech</b> <b>plus</b> an {{educational}} crash course taught by a notable member of the Canadian technology industry. Attendees are then provided with reference materials {{to assist them in}} {{the second phase of the}} schedule: building a website from scratch. By the end of the seven-hour schedule, it is the conference's goal to have removed the attendees' intimidation associated with programming, and to give them the confidence and resources needed to continue their web development learning if they choose to do so.|$|R
40|$|THE "VOICED MAIL" {{system of}} MIT's Architecture Machine Group allows users of a {{research}} computer system to access their electronic mail through a text-to-speech synthesizer (<b>Speech</b> <b>Plus's</b> Prose 2000) and a touchtone telephone. In an environment of heavy on-line mail usage, this system has gained acceptance by a community of about 30 subscribers needing to both read and transmit messages. To use Voiced Mail, a user calls in and gives a unique identifier (home phone number) and a password by touchtones, much like using an automatic bank teller. Mail messages are sorted by source...|$|R
5000|$|Originally, extemporaneous {{commentary}} topics {{were more}} general and {{more focused on}} opinions than extemporaneous topics were, making extemp commentary an almost [...] "halfway point" [...] between impromptu speaking and normal extemp. However, the success of those who integrated more sources into their <b>speeches,</b> <b>plus</b> the trending of the topics to mimic extemp's, has blurred the line between extemporaneous and extemporaneous commentary. In some cases extemp commentary has a shorter prep time, and shorter speeches. The only definitive difference between extemp and extemp commentary is the position in which the speeches are given (standing as opposed to sitting).|$|R
30|$|The {{dependent}} variable {{is the change}} in score between the pre- and post-test (post-test raw score minus pretest raw score). An independent t-test showed that the change score was significantly greater for students in the <b>speech</b> <b>plus</b> gesture condition (M[*]=[*] 2.20, sd[*]=[*] 2.14) than {{for those in the}} speech alone condition (M[*]=[*] 0.86, sd[*]=[*] 2.51), t(92)[*]=[*] 2.78, p[*]=[*] 0.007, d[*]=[*] 0.57. Although the degree of change in the speech alone condition was very small, a one-sample t-test showed that it was significantly greater than 0, t(42)[*]=[*] 2.25, p[*]=[*] 0.03. Thus, both versions of the video resulted in significant learning, but the amount of learning was greater when gesture was included.|$|R
40|$|The task of {{automatically}} segmenting {{an acoustic}} signal into categories (such as speech, speech over background music, or music) {{is an important}} step in the transcription process. We are attempting to improve the accuracy of such segmentation systems by incorporating suprasegmental and other temporal information into the frame-based classifiers typically used for this purpose. Two specific approaches are introduced here, one based on using frequency contours to improve the location of segment boundaries and one based on including temporal features directly into the frame-based classifier. Results indicate that improvement in classification accuracy can be achieved through the use of temporal information, particularly for the <b>speech</b> <b>plus</b> music class where methods using traditional features often give poor results. 1...|$|R
50|$|Each {{district}} runs an International <b>Speech</b> contest, <b>plus</b> up {{to three}} other contests out of a list of four (Humorous, Tall Tales, Table Topics, and Evaluation). All contest are governed by an official speech contest rulebook, updated each calendar year by Toastmasters International.|$|R
40|$|We ran an {{experiment}} focusing on cognitive implication of reverse speech segments. Nine durations of reverse <b>speech</b> <b>plus</b> a non-distorted control condition {{have been considered}} (varying between 20 ms and 180 ms) {{in order to test}} the pattern of intelligibility degradation in French. We observed an overall strong negative correlation between the degree of intelligibility and the size of reverse speech windows. These results appear to be very comparable to those obtained in English by Greenberg and Arai (2001), at least on the slope of intelligibility performance decrease. However, intelligibility loss in French is delayed by 20 ms. Apart from confirming the cognitive ability to restore reverse speech up to a certain point, our study revealed differences that could be interpreted as language specific...|$|R
5000|$|FDR Library holds [...] "Papers of Jerome Davis: 1912 - 1965," [...] which {{includes}} subject files <b>plus</b> <b>speech</b> and writing files ...|$|R
30|$|It is our {{contention}} that the gestures used in the instruction for ANOVA were representational in nature using iconic gestures (indexical points and illustrating gestures) to convey within- and between-groups variance by linking spatial information on the slide with accompanying speech. Thus, the likely role gesture played in this research was to link relevant representations of ANOVA (in the form of concrete examples, equations, and speech) {{as well as to}} encapsulate the basic notion of variance using spatial imagery (circling around the means of all the experimental groups to relay the notion of between-groups variance) as these types of gestures were used predominantly throughout the lecture. So {{it is not surprising that}} students showed improvement when tested on aspects of variance after watching the <b>speech</b> <b>plus</b> gesture video.|$|R
40|$|Gestures {{are usually}} {{looked at in}} {{isolation}} or from an intra-propositional perspective essentially tied to one speaker. The Bielefeld multi-modal Speech-And-Gesture-Alignment (SAGA) corpus has many interactive gestures relevant for the structure of dialogue (Rieser 2008, 2009). To describe them, a dialogue theory is needed which {{can serve as a}} speechgesture interface. PTT (Poesio and Traum 1997, Poesio and Rieser submitted a) can do this job in principle, how this can be achieved is the main topic of this paper. As a precondition, the empirical research procedure from systematic corpus annotation via gesture typology to a partial ontology for gestures is described. It is then explained how PTT is extended to provide an incremental modelling of <b>speech</b> <b>plus</b> gesture in an assertion-acknowledgement adjacency pair where grounding between dialogue participants is obtained through gesture. ...|$|R
50|$|The {{company went}} on to record other notable writers reading their own works, such as W. H. Auden, Robert Frost, T. S. Eliot, Ernest Hemingway, Gertrude Stein and many more. The label {{expanded}} further to encompass other types of spoken word recordings, including children's stories, <b>speeches,</b> <b>plus</b> English- and foreign-language classics. Theater performances were also staged for the label, starring either the Shakespeare Recording Society or the Theatre Recording Society, depending on the playwright. These performances included many famous actors and actresses, including Anthony Quayle, Claire Bloom, Richard Burton, Albert Finney, John Gielgud, Siobhán McKenna, Michael Redgrave, Vanessa Redgrave, Felix Aylmer, Paul Scofield, Alec McCowen, Donald Pleasence, Ralph Richardson, Max Adrian and Maggie Smith among others. Other notable readers for the label included Vincent Price, Basil Rathbone, and Louis Jourdan.|$|R
40|$|At the Institute of Phonetic Sciences (IFA) we have {{collected}} a corpus of spoken Dutch of 4 male and 4 female speakers, containing conversational {{as well as}} read <b>speech,</b> <b>plus</b> sentences, words and syllables taken from the transcribed conversation text, and then spoken in isolation. This pertains to about 5. 5 hours of speech. All this material is segmented and labeled at the phoneme level. This information plus all meta data are stored in a database which makes all material highly accessible through SQL. Actually all information is freely available under the GNU General Public License for interested parties. This material will also be used in INTAS project 915, in which a comparison will be made of phonetic properties in Dutch, Finnish and Russian. As an initial result we will present some durational and spectral data of full and reduced phoneme realizations...|$|R
40|$|This paper {{presents}} {{application of}} minimum phase whitening filter for speech enhancement technique to two-microphone beamforming structure. The method uses switching technique for updating the estimate alternately between frontend minimum phase whitening filter during <b>speech</b> <b>plus</b> noise period and rear-end adaptive finite impulse response (FIR) filter during noise alone period. For application of speech beamforming technique, innovations-based minimum phase whitening filter is applied {{in front of}} sum and subtract (SS) function {{with the use of}} one sample delay filter in rear-end primary input. A test result shows that the proposed approach by alternately switching minimum phase whitening filter and adaptive FIR filter provides an improved performance of 5. 9 dB and 18. 03 dB in signal-to-noise ratio (SNR) in stationary noise and non-stationary noise, respectively than adaptive Griffiths and Jim (G-J) beamformer by alternately switching two adaptive FIR filters...|$|R
30|$|The <b>speech</b> <b>plus</b> gesture {{video was}} shown in three classes {{with a total of}} 51 {{students}}. The speech alone video {{was shown in}} three classes with a total of 43 students. The two groups did not differ in gender, age, ethnicity, or whether English was their primary language. All testing took place during the first 20  min of the regular class time. After students signed the Informed Consent form, they were asked to answer the ten items on the ANOVA pretest and were given about 5  min to do so. Then the video was shown to the entire class. Immediately after the video was shown, students were asked to take the same ten-item test (the “post-test”). Testing was done at the time in the semester after students had learned some basic inferential statistics, including the t-test, but before they started to learn about ANOVA.|$|R
5000|$|The Papers of Woodrow Wilson {{edited by}} Arthur S. Link {{complete}} in 69 vol, at major academic libraries. Annotated edition {{of all of}} WW's letters, <b>speeches</b> and writings <b>plus</b> many letters written to him ...|$|R
30|$|Each video {{included}} the same PowerPoint slide show {{accompanied by a}} lecture given by a tenure-track full-time professor who has taught this class frequently. The audio portion of the lecture was embedded within the PowerPoint and was thus identical between the two conditions. To create the video, the slide show was projected on to a screen in a classroom at NEIU. The instructor stood {{to the left of}} the screen and lip-synced the audio part of the lecture, which was actually played through the projection system. In one version of the video (speech alone), the speaker stood still and did not move her arms at all but alternated between facing out to the audience and looking toward the PowerPoint slides. In the other version (<b>speech</b> <b>plus</b> gesture), the instructor produced scripted gestures to highlight and draw links between relevant information in the slides and conceptual elements of ANOVA. For example, gestures were designed to illustrate the different types of variance underlying the ANOVA formula. The gestures used were similar to those this instructor naturally uses when lecturing on this topic.|$|R
40|$|Detection {{of speech}} in noisy {{recordings}} is challenging, {{especially when the}} noise does not follow the usual whiteness, stationarity and high signal-to-noise ratio assumptions. A robust speech detector can affect significantly the performance of several speech-processing tasks, such as endpoint detection, segmentation and finally recognition, if we deal with real life data as opposed to laboratory or controlled environment recordings. The detector proposed in this paper {{is based on a}} Gaussianity test that employs third-order cumulants of the data to decide on the binary hypotheses of noise only versus <b>speech</b> <b>plus</b> noise. <b>Speech</b> intervals are detected by exploiting the third-order information present in the speech signal. The detector can tolerate a large family of additive noises thanks to its third-order statistics basis. The sample-adaptive and decision feedback variations proposed here provide the detector with tracking ability with respect to both the time variations of speech and the possible non-stationarity of noise. Experiments carried out using real data recorded in a moving car interior show satisfactory performance of the proposed algorithms down to - 6 dB signal-to-noise ratio...|$|R
40|$|International audienceThis paper {{presents}} weighted {{approaches for}} integrated {{active noise control}} and noise reduction in hearing aids. The unweighted integrated active noise control and noise reduction scheme introduced in previous work does not allow to trade-off between the active noise control and the noise reduction. In some circumstances it will however be useful to emphasize one of the functional blocks. Changing the original optimisation problem to a constrained optimisation problem leads to a scheme based on a weighted mean squared error criterion that allows to focus either on the active noise control or on the noise reduction. It is similarly possible to derive a scheme that allows to focus either on reducing the speech distortion or on reducing the residual noise at the eardrum. In a single speech source scenario and {{when the number of}} sound sources (<b>speech</b> <b>plus</b> noise sources) is {{less than or equal to}} the number of microphones, it is possible to derive a simple formula for the output signal-to-noise ratio of the latter scheme. It can then be shown that this scheme delivers a constant signal-to-noise ratio at the eardrum for any weighting factor...|$|R
40|$|It {{is claimed}} here that {{experimental}} evidence about human speech processing and {{the richness of}} memory for linguistic material supports a distributed view of language where every speaker creates an idiosyncratic perspective on the linguistic conventions of the community. In such a system, words are not spelled in memory of speakers from uniform letter-like units (whether phones or phonemes), but rather from the rich auditory patterns of <b>speech</b> <b>plus</b> any coupled visual, somatosensory and motor patterns. The evidence is strong that people actually employ high-dimensional, spectro-temporal, auditory patterns to support speech production, speech perception and linguistic memory in real time. Abstract phonology (with its phonemes, distinctive features, syllable types, etc.) is actually a kind of social institution – a loose inventory of patterns that evolves over historical time in each human community as a structure with many symmetries and regularities in the community corpus. Linguistics studies the phonological (and grammatical) patterns of various communities of speakers. But linguists should not expect to find the descriptions they make to be explicitly represented in any individual speaker’s mind, much less in every mind in the community. The alphabet is actually a technology that has impose...|$|R
40|$|This {{study was}} a {{comparison}} of the effects of oral speech with total communication (<b>speech</b> <b>plus</b> sign language) training on the ability of mentally retarded children to repeat 4 -word sentences. Three children were chosen who used single words to communicate but who did not combine words into complete sentences. Three sentence pairs were trained, with each pair having one sentence trained using oral methods and an equivalent one trained using the total communication approach. Both training procedures involved chaining sentence parts, reinforcement, and prompting. Oral methods involved presenting vocal stimuli and requiring vocal responses whereas total communication methods involved presenting vocal and signed stimuli and requiring vocal and signed responses. For the initial sentence pair with each child, an alternating treatments design was used to determine the relative efficacy of the two language training approaches. This was repeated with a second and third sentence pair using a multiprobe technique within a multiple baseline design. Results pointed to the superiority of the total communication approach in facilitating sentence repetition. Possible explanations of these results are offered and the utility of the alternating treatments experimental design is discussed...|$|R
40|$|This paper {{describes}} {{methods that}} exploit stenographic transcripts of the German parliament {{to improve the}} acoustic models of a speech recognition system for this domain. The stenographic transcripts and the speech data {{are available on the}} Internet. Using data from the Internet makes it possible to avoid the costly process of the collection and annotation of a huge amount of data. The automatic data acquisition technique works using the stenographic transcripts and acoustic data from the German parliamentary <b>speeches</b> <b>plus</b> general acoustic models, trained on different data. The idea of this technique is to generate special finite state automata from the stenographic transcripts. These finite state automata simulate potential possible correspondences between the stenographic transcript and the spoken audio content, i. e. accurate transcript. The first step is the recognition of the speech data using finite state automaton as a language model. The next step is to find, to extract and to verify the match between sections of recognized words and actually spoken audio content. After this, the automatically extracted and verified data can be used for acoustic model training. Experiments show that for a given recognition task from the German Parliament domain the absolute decrease of the word error rate is 20 %. 1...|$|R
40|$|International audienceThis paper {{presents}} low-rank approximation based multichannel Wiener filter algorithms for {{noise reduction}} in <b>speech</b> <b>plus</b> noise scenarios, with application in cochlear implants. In a single speech source scenario, the frequency-domain autocorrelation matrix {{of the speech}} signal is often {{assumed to be a}} rank- 1 matrix, which then allows to derive different rank- 1 approximation based noise reduction filters. In practice, however, the rank of the autocorrelation matrix of the speech signal is usually greater than one. Firstly, the link between the different rank- 1 approximation based noise reduction filters and the original speech distortion weighted multichannel Wiener filter is investigated when the rank of the autocorrelation matrix of the speech signal is indeed greater than one. DRAFT 2 Secondly, in low input signal-to-noise-ratio scenarios, due to noise non-stationarity, the estimation of the autocorrelation matrix of the speech signal can be problematic and the noise reduction filters can deliver unpredictable noise reduction performance. An eigenvalue decomposition based filter and a generalized eigenvalue decomposition based filter are introduced that include a more robust rank- 1, or more generally rank-R, approximation of the autocorrelation matrix of the speech signal. These noise reduction filters are demonstrated to deliver a better noise reduction performance especially in low input signal-to-noise-ratio scenarios. The filters are especially usefull in cochlear implants, where more speech distortion and hence a more agressive noise reduction can be tolerated...|$|R
30|$|Speech {{enhancement}} {{is important}} in the field of speech communications and speech recognition. Many methods have been proposed in the literature (Loizou 2007). Spectral subtractive algorithms were among the first and are probably the simplest (Berouti et al. 1979; Boll 1979). They are {{based on the assumption that}} speech and noise are additive and thus the noisy speech signal can be enhanced by subtracting a noise estimate. Usually this is done in frequency domain using the magnitude of the short-time Fourier transform (STFT). For inverse transformation the phase of the noisy signal is considered. Statistical model-based methods provide a framework to find estimates of, e.g., the spectrum or magnitude spectrum of clean speech given the noisy speech spectrum (Ephraim and Malah 1984, 1985; McAulay and Malpass 1980). Subspace methods are based on the assumption that the clean signal only covers a subspace of the Euclidean space where the noisy speech signal exists (Ephraim and Van Trees 1995; Hu and Loizou 2003). Enhancement is performed by separating the noise subspace and the clean <b>speech</b> <b>plus</b> noise subspace and setting the components in the noise subspace to zero. Most speech enhancement algorithms make use of a noise estimate and their performance therefore heavily depends on the quality of the noise estimate. Poor noise estimates may lead to artifacts such as isolated peaks in the spectrum, which are perceived as tones of varying pitch and are known as musical noise (Berouti et al. 1979).|$|R
40|$|Systems for noise {{reduction}} {{are used in}} speech transmission and speech input systems. In the first case hand-set free telephone systems require {{noise reduction}} when used in a noisy environment like a car to improve the intellegibility for the customer whom the driver calls. If speech input systems are used in a noise environment the recognition rate degrades significantly {{so that it is}} not accepted by the customer. Systems for noise reduction either can be a separate module of a speech input system or are integrated into the speech input system. The pros and cons of both solutions are discussed. As far as the noise reduction is concerned several approaches are known which can be classified as noise suppression systems based on Wiener filter theory and as noise compensation systems using the least mean square algorithm for adaptation. Whereas in the first case only one input for <b>speech</b> <b>plus</b> noise is used the latter method requires one or more additional reference channels with correlate d noise signals and without any speech component. A more recent development takes adaptive beamforming for noise suppression into account. It is shown that in realistic applications e. g. a speech input system installed in a car, the recognition rate of the speech input system can be improved significantly if a proper setup of both, the speech recognition system and the noise reduction system are designed for the specific noise environment...|$|R
25|$|The House was {{supposed}} to break for the summer Thursday June 23, but remained open in an extended session due to the filibuster. The 103 NDP MPs had been taking it in turn to deliver 20 minute <b>speeches</b> – <b>plus</b> 10 minutes of questions and comments – in order to delay {{the passing of the}} bill. MPs are allowed to give such speeches each time a vote takes place, and many votes were needed before the bill could be passed. As the Conservative Party of Canada held a majority in the House, the bill passed. This was the longest filibuster since the 1999 Reform Party of Canada filibuster, on native treaty issues in British Columbia.|$|R
40|$|Objective: To assess {{cochlear}} implant (CI) outcomes, and factors affecting outcomes, {{for children with}} aplasia/hypoplasia of the cochlea nerve. We also developed a new grading system for the nerves of the internal auditory meatus (IAM) and cochlea nerve classification. Study Design: Retrospective patient review. Setting: Tertiary referral hospital and {{cochlear implant}} program. Patients: Children 0 to 16 years inclusive with a CI who had absent/hypoplastic cochlea nerve on magnetic resonance imaging (MRI). Intervention: Cochlear implant. Main Outcome Measures: MRI, trans-tympanic electrical auditory brainstem response, intraoperative electrical auditory brainstem response, Neural Response Telemetry, Categories of Auditory Perception score, Main mode of communication. Results: Fifty CI recipients (26 males and 24 females) were identified, 21 had bilateral CIs, 27 had developmental delay. MRI showed cochlea nerve aplasia in 64 ears, hypoplasia in 25 ears, and a normal nerve in 11 ears. Main mode of communication was analyzed for 41 children: 21 (51 %) used verbal language (15 speech alone, 5 <b>speech</b> <b>plus</b> some sign, 1 bilingual in speech and sign), and 20 (49 %) used sign language (10 sign alone, 9 sign <b>plus</b> some <b>speech,</b> 1 tactile sign). Seventy-three percent of children used some verbal language. Cochlea nerve aplasia/hypoplasia and developmental delay were both significant factors affecting main mode of communication. Categories of Auditory Performance scores were available for 59 CI ears; 47 % with CN Aplasia (IAM nerve grades 0 -III) and 89 % with CN hypoplasia (IAM nerve grade IV) achieved Categories of Auditory Performance scores of 5 to 7 (some verbal understanding) (p= 0. 003). Conclusion: Our results are encouraging and useful when counselling families regarding the likelihood of language outcomes and auditory understanding. 8 page(s...|$|R
40|$|WikiSpeech is {{a content}} {{management}} system for the web-based creation of speech databases {{for the development of}} spoken language technology and basic research. Its main features are full support for the typical recording, annotation and project administration workflow, easy editing of the <b>speech</b> content, <b>plus</b> a fully localizable user interface. For {{the creation of a new}} speech database, it is only necessary to open a new project within WikiSpeech, provide a link to any static project information pages and upload the prompt material to be presented to the speakers. Recordings and annotation are performed via the WWW in a platform independent manner on any Java compatible computer. WikiSpeech currently has been localized to four languages: German, English, Romanian and Russian...|$|R
40|$|The entire dissertation/thesis text is {{included}} in the research. pdf file; the official abstract appears in the short. pdf file (which also appears in the research. pdf); a non-technical general description, or public abstract, appears in the public. pdf file. Title from title screen of research. pdf file (viewed on November 26, 2007) Vita. Includes bibliographical references. Thesis (Ph. D.) University of Missouri-Columbia 2007. Dissertations, Academic [...] University of Missouri [...] Columbia [...] Psychology. In this study, a group of six adult men with post-stroke Broca's aphasia and a matched group of men with no neurological illness (NNI) completed an object description task in <b>speech</b> <b>plus</b> gesture, <b>speech</b> only, and gesture only conditions. Participants with aphasia were seen at monthly intervals for 6 months beginning at 1 - 2 months post-onset, and the Western Aphasia Battery (WAB) was administered at Times 1 and 6. Participants with aphasia demonstrated significant improvement in language over the 6 -month period. However, their speech was still significantly poorer than that of the NNI group, and their communication patterns differed in a number of ways. Gesture rate was significantly higher in early recovery than that for NNI adults. The majority of gestures produced by participants with aphasia were emblems, while the NNI group primarily made use of iconics. Participants with aphasia produced significantly fewer numbers of meaningful motor movements in pantomime gesture in early recovery. Substantial individual variability was apparent within the aphasia group. These findings are suggestive of re-organization in language and gesture during the 6 -month recovery period following cerebrovascular accident. They are discussed in terms of the integrated nature of processes underlying speech and gesture and potential clinical implications of gesture use as an index of language recovery in Broca's aphasia...|$|R
40|$|People with {{cochlear}} {{hearing loss}} have a reduced dynamic range of hearing, thus amplitude compression may provide adequate amplification of soft sounds without uncomfortable over-amplification of loud sounds caused by conventional linear amplification. Although compression is conceptually straightforward, there are various design parameters that may affect the intelligibility of speech, the quality of sounds, {{and the perception of}} background noise. Four combinations of the Dual Front-End automatic gain control (AGC) system were implemented: (1) The Dual Front-End with a Hold Timer developed in Stone et al. [8], which aimed at reducing pumping effects while maintaining a relatively fast release; (2) The Dual Front-End with the SNR Estimator, investigated by Martin et al. [4], designed to provide a varying release time constant depending on the SNR level; (3) The Dual Front-End with both the Hold Timer and the SNR Estimator; (4) The Dual Front-End by itself, without the Hold Timer or the SNR Estimator. A fifth system, composed of linear amplification and compression limiting, was implemented {{to be used as a}} reference condition. A variety of stimuli consisting of speech at different levels and <b>speech</b> <b>plus</b> environmental sounds were processed by the five systems and presented over headphones to three hearing-impaired subjects. Subjects rated the processed stimuli for intelligibility and quality. While no clear differences were found among the four compression systems, there were some major differences between the Dual Front-End systems and the Linear system. The direction of these differences varied with subject, and to a lesser degree, with stimulus condition. In addition, compression systems generally performed better in stimuli conditions with low SNRs, indicating that compression may be useful for suppression of background noise. by Ivan Aguayo. Thesis (M. Eng. and S. B.) [...] Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2001. Includes bibliographical references (leaves 90 - 91) ...|$|R
40|$|American {{quantitative}} sociolinguistics {{has drawn}} substantially {{on data from}} the African American speech community for its descriptive, theo-retical, and methodological development, but has given relatively little in return. Contributions from the speech community to sociolinguistics include the development of variable rules and frameworks for the anal-ysis of tense-aspect markers, social class, style, narratives, and <b>speech</b> events, <b>plus</b> research topics and employment for students and faculty. The contributions which sociolinguistics could make in return to the African American speech community- but has not done sufficiently-include the induction of African Americans into linguistics, the repre-sentation of African Americans in our writings, and involvement in courts, workplaces, and schools, especially {{with respect to the}} teaching of reading and the language arts. This last issue has surged to public attention following the Oakland School Board's "Ebonics " resolution...|$|R
