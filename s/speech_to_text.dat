122|10000|Public
25|$|Assistive {{technology}} {{in this area}} is broken down into low, mid, and high tech categories. Low tech encompasses equipment that is often low cost and does not include batteries or requires charging. Examples include adapted paper and pencil grips for writing or masks and color overlays for reading. Mid tech supports used in the school setting include the use of handheld spelling dictionaries and portable word processors used to keyboard writing. High tech supports involve the use of tablet devices and computers with accompanying software. Software supports for writing include the use of auditory feedback while keyboarding, word prediction for spelling, and <b>speech</b> <b>to</b> <b>text.</b> Supports for reading include the use of text to speech (TTS) software and font modification via access to digital text. Limited supports are available for math instruction and mostly consist of grid based software to allow younger students to keyboard equations and auditory feedback of more complex equations using MathML and Daisy.|$|E
500|$|On the , texting can be {{aided by}} the voice assistant, which {{converts}} <b>speech</b> <b>to</b> <b>text.</b> In addition to regular texting, messaging on the [...] supports iMessage, a specialized instant messaging program and service that allows unlimited texting to other Apple devices running iOS 5 or later. This supports the inclusion of content such as images and sound in text messages, integration with the device's voice-controlled software assistant, and read acknowledgements for sent messages. Input to the device comes from a keyboard displayed on the multi-touch screen or by voice-to text by speaking into the microphone. Entered text is supported by predictive and suggestion software; there is a multi-language spell-checker which recognises many regional accents of different languages.|$|E
2500|$|Using and {{implementing}} new technology such as <b>speech</b> <b>to</b> <b>text</b> software and Nintendo Wii video games ...|$|E
40|$|Visual {{features}} in printed documents improve text navigation. Visually disabled readers use Braille or <b>speech</b> <b>to</b> access <b>text.</b> These interfaces discard the visual features. Thus, text navigation is inecient. This paper describes two linguistic based methods for improving navigation. 1 Introduction Printed documents have many visual features for improving text navigation. For instance, the relevant {{parts of a}} document can be located by scanning the section headings. Font changes direct the reader's attention. Details such as citations can be skipped by ignoring text that are surrounded by brackets. Visually disabled readers use Braille or <b>speech</b> <b>to</b> access <b>text.</b> These alternative interfaces discards the visual information in documents. Thus, text navigation is inecient. This paper presents {{an overview of the}} problem and our implemented solutions. 2 Background 2. 1 Communication medium Braille represents letters and numerals with patterns of raised dots. This system has been implemente [...] ...|$|R
40|$|This {{thesis is}} about topic {{detection}} from spoken speech. The {{first part of}} the thesis deals with <b>speech</b> transcription <b>to</b> <b>text.</b> The thesis describes two different solutions of the topic detection - a machine learning based solution and an expert solution that composes a very precise query describing the document topic. Both methods are tested on a set of recordings and compared...|$|R
5000|$|Also MPEG-4 Part 3 audio objects, such as Audio Lossless Coding (ALS), Scalable Lossless Coding (SLS), MP3, MPEG-1 Audio Layer II (MP2), MPEG-1 Audio Layer I (MP1), CELP, HVXC (<b>speech),</b> TwinVQ, <b>Text</b> <b>To</b> <b>Speech</b> Interface (TTSI) and Structured Audio Orchestra Language (SAOL) ...|$|R
5000|$|<b>Speech</b> <b>to</b> <b>text</b> functionality, {{either for}} {{creation}} of offline subtitles, or for live respeaking ...|$|E
5000|$|In other countries, CART may be {{referred}} to as Palantype, Velotype, STTR (<b>speech</b> <b>to</b> <b>text</b> reporting).|$|E
5000|$|Using and {{implementing}} new technology such as <b>speech</b> <b>to</b> <b>text</b> software and Nintendo Wii video games ...|$|E
50|$|Scummbler is a {{compiler}} for SCUMM bytecode, for versions 3 to 5 of the SCUMM engine. It uses scripts decompiled {{from the}} original game files, retrieved {{using a combination of}} an unpacking tool like ScummPacker (also by the author of Scummbler), and the descumm tool from ScummVM. These scripts can be re-inserted into the original game files, making it useful for modifying existing games, such as for translation. Also available are an image encoder-decoder, and a tool to assist in mapping <b>speech</b> files <b>to</b> <b>text.</b>|$|R
40|$|Speech {{recognition}} {{is the process}} of converting <b>speech</b> signals <b>to</b> the <b>text.</b> Studies on speech recognition have increased very fast for the last twenty-five years. Most of these studies have used phoneme and word as speech recognition units. Namely, in phoneme based speech recognition systems, all phonemes in a language have been modelled by a speec...|$|R
40|$|Combination {{of speech}} {{recognition}} and handwriting {{recognition as a}} text entry way has been widely used on mobile phone products. However, users seldom use <b>speech</b> recognition <b>to</b> input <b>text.</b> It probably because we didn&# 39;t {{know what kind of}} situation was really suitable for using <b>speech</b> recognition <b>to</b> input <b>text</b> in daily life. The present study tried to answer what were typical scenarios and influencing factors for the combination of speech and handwriting in users&# 39; daily life. Three focus groups were conducted. Results suggested that environmental noise and social factors had an important influence on the usage of the speech recognition, while the attention demands and hands occupation limited the usage of the handwriting. Further, the speech-handwriting combination had special advantages in some scenarios, in terms of the entry speed, accuracy, and attention demands. And the combination way was less impacted by the environmental noises and social factors. These findings provided information for further filed studies and system design.  ...|$|R
50|$|Also called open captioning, or {{real-time}} stenography, {{or simply}} real-time captioning, Communication Access Real Time Captioning (CART) {{is the general}} name {{of the system that}} court reporters, closed captioners and voice writers, and others use to convert <b>speech</b> <b>to</b> <b>text.</b> A trained operator uses keyboard or stenography methods to transcribe spoken speech into written text. <b>Speech</b> <b>to</b> <b>text</b> software is used when voice writers provide CART.|$|E
5000|$|Information {{technology}} research, such as predictive Chinese text input for mobile phones, automatic <b>speech</b> <b>to</b> <b>text</b> conversion, {{opinion mining}} ...|$|E
50|$|HTML5 Audio is {{a subject}} of the HTML5 specification, {{incorporating}} audio input, playback, and synthesis, as well as <b>speech</b> <b>to</b> <b>text,</b> in the browser.|$|E
40|$|This paper {{provides}} an interface between the machine translation and speech synthesis system for converting English <b>speech</b> <b>to</b> Tamil <b>text</b> in English <b>to</b> Tamil <b>speech</b> <b>to</b> <b>speech</b> translation system. The speech translation system {{consists of three}} modules: automatic speech recognition, machine translation and <b>text</b> <b>to</b> <b>speech</b> synthesis. Many procedures for incorporation of speech recognition and machine translation have been projected. Still speech synthesis system {{has not yet been}} measured. In this paper, we focus on integration of machine translation and speech synthesis, and report a subjective evaluation to investigate the impact of speech synthesis, machine translation and the integration of machine translation and speech synthesis components. Here we implement a hybrid machine translation (combination of rule based and statistical machine translation) and concatenative syllable based speech synthesis technique. In order to retain the naturalness and intelligibility of synthesized speech Auto Associative Neural Network (AANN) prosody prediction is used in this work. The results of this system investigation demonstrate that the naturalness and intelligibility of the synthesized speech are strongly influenced by the fluency and correctness of the translated text...|$|R
5000|$|A pivotal {{point came}} with Bush’s September 11, 1990 [...] "Toward a New World Order" [...] <b>speech</b> (full <b>text)</b> <b>to</b> {{a joint session}} of Congress. This time it was Bush, not Gorbachev, whose idealism was {{compared}} to Woodrow Wilson, and to Franklin D. Roosevelt at the creation of the UN. Key points picked up in the press were: ...|$|R
40|$|This paper {{describes}} an {{acceptance test procedure}} for evaluating a spoken language translation system between Catalan and Spanish. The procedure consists of two independent tests. The first test was an utterance-oriented evaluation for determining how the use of speech benefits communication. This test allowed for comparing relative performance of the different system components, explicitly: source <b>text</b> <b>to</b> target <b>text,</b> source <b>text</b> <b>to</b> target <b>speech,</b> source <b>speech</b> <b>to</b> target <b>text,</b> and source <b>speech</b> <b>to</b> target <b>speech.</b> The second test was a task-oriented experiment for evaluating if users could achieve some predefined goals for a given task {{with the state of}} the technology. Eight subjects familiar with the technology and four subjects not familiar with the technology participated in the tests. From the results we can conclude that state of technology is getting closer to provide effective speech-to-speech translation systems but there is still lot {{of work to be done}} in this area. No significant differences in performance between users that are familiar with the technology and users that are not familiar with the technology was evidenced. This constitutes, as far as we know, the first evaluation of a Spoken Translation System that considers performance at both, the utterance level and the task level. 1...|$|R
5000|$|Speech {{analytics}} - used {{to monitor}} telephone conversations taking place between companies and customers, using phonetic analysis or <b>speech</b> <b>to</b> <b>text</b> to find keywords and phrases, classify call types and identify trends.|$|E
5000|$|... 2011-05 - v1.8.8.2: After a {{slightly}} flawed 1.8.8 not officially released, version 1.8.8.2 was released with new features like <b>speech</b> <b>to</b> <b>text,</b> online audio-recording, photo edition, SVG diagrams drawer, full-text indexing, certificates generation.|$|E
50|$|Interpreting -RAD {{provides}} {{high quality}} British Sign Language/English Interpreters, Deafblind Interpreters, Lipspeakers, Note-takers and <b>Speech</b> <b>to</b> <b>Text</b> Reporters.There {{is also an}} emergency service that is available 24 hours a day, 365 days a year.|$|E
40|$|Abstract — Speech is the {{important}} mode of communication and is the current research topic. The concentration is mostly focused on synthesis and analyzing part. Apart of synthesizing, <b>text</b> <b>to</b> <b>speech</b> system is developed. Speech synthesis is an artificial production of human <b>speech.</b> A <b>text</b> <b>to</b> <b>speech</b> system (TTS) is to convert an arbitrary text into speech. In India different languages have been spoken each being the mother tongue of {{tens of millions of}} people. In this paper,the <b>text</b> <b>to</b> <b>speech</b> system is primarily developed for Telugu, a Dravidian language predominantly spoken in Indian state of Andhra Pradesh. The important qualities expected from this system are naturalness and intelligibility. Telugu TTS can be developed using other synthesis methods like articulatory synthesis,formant synthesis and concatenative synthesis. This paper describes a development of a Telugu <b>text</b> <b>to</b> <b>speech</b> system using concatenative synthesis method on mobile based system OMAP 3530 (ARM Cortex A- 8 core) in Linux...|$|R
40|$|Adding an {{emotions}} using prosody manipulation {{method for}} Indonesian <b>text</b> <b>to</b> <b>speech</b> system. <b>Text</b> <b>To</b> <b>Speech</b> (TTS) {{is a system}} that can convert text in one language into speech, accordance with the reading of the text in the language used. The focus of this research is a natural sounding concept, the make "humanize" for the pronunciation of voice synthesis system <b>Text</b> <b>To</b> <b>Speech.</b> Humans have emotions / intonation that may affect the sound produced. The main requirement for the system used <b>Text</b> <b>To</b> <b>Speech</b> in this research is eSpeak, the database MBROLA using id 1, Human Speech Corpus database from a website that summarizes the words with the highest frequency (Most Common Words) used in a country. And there are 3 types of emotional / intonation designed base. There is a happy, angry and sad emotion. Method for develop the emotional filter is manipulate the relevant features of prosody (especially pitch and duration value) using a predetermined rate factor that has been established by analyzing the differences between the standard output <b>Text</b> <b>To</b> <b>Speech</b> and voice recording with emotional prosody / a particular intonation. The test results for the perception tests of Human Speech Corpus for happy emotion is 95 %, 96. 25 % for angry emotion and 98. 75 % for sad emotions. For perception test system carried by intelligibility and naturalness test. Intelligibility test for the accuracy of sound with the original sentence is 93. 3 %, and for clarity rate for each sentence is 62. 8 %. For naturalness, accuracy emotional election amounted to 75. 6 % for happy emotion, 73. 3 % for angry emotion, and 60 % for sad emotions. [...] - <b>Text</b> <b>To</b> <b>Speech</b> (TTS) merupakan suatu sistem yang dapat mengonversi teks dalam format suatu bahasa menjadi ucapan sesuai dengan pembacaan teks dalam bahasa yang digunakan. Comment: Keywords: <b>Text</b> <b>To</b> Speech; eSpeak; MBROLA; Human Speech Corpus; emotion; intonation; prosody manipulation; emosi; intonasi; manipulasi prosod...|$|R
40|$|We {{present a}} web-based tool for {{generating}} and editing pronunciation lexicons in multiple languages. The tool is implemented as a web application on Google App Engine {{and can be}} accessed remotely from a web browser. The client application displays to users a textual prompt and interface that reconfigures based on language and task. It lets users generate pronunciations via constrained phoneme selection, which allows users with no special training to provide phonemic transcriptions efficiently and accurately. Index Terms: pronunciation lexicons, <b>speech</b> recognition, <b>text</b> <b>to</b> <b>speech,</b> internationalizatio...|$|R
50|$|Signature offers nationally {{recognised}} qualifications in {{subjects such as}} British Sign Language and <b>speech</b> <b>to</b> <b>text</b> reporting, which over 30,000 learners study at more than 700 locations across the UK and Ireland each year.|$|E
5000|$|Medical, disabilities, {{many people}} have {{difficulty}} typing due to physical limitations such as repetitive strain injuries (RSI), muscular dystrophy, and many others. For example, people with difficulty hearing could use a system connected to their telephone to convert a caller's <b>speech</b> <b>to</b> <b>text.</b>|$|E
5000|$|Indic Computing means [...] "computing in Indic" [...] i.e. Indian Scripts and Languages. It {{involves}} developing {{software in}} Indic Scripts/languages, Input methods, Localization of computer applications, web development, Database Management, Spell checkers, <b>Speech</b> <b>to</b> <b>Text</b> and Text to Speech applications and OCR in Indian languages.|$|E
40|$|This paper {{consists}} {{of the study of}} three of the most successful and widely known authors of Chicano Literature: Miguel Mendez, Rolando Hinojosa-Smith and Rudolfo Anaya. The theoretical principles on which this analysis is based correspond to the fields of linguistics and narratology. The first part of the study deals with the language choice process related to each of the three writers. The second part of the analysis was carried out by transferring results obtained from the study of live <b>speech</b> <b>to</b> the literary <b>texts.</b> These two different levels of language were confronted in order to verify if any correspondence among them exists...|$|R
40|$|Introduction The {{explosive}} {{growth of the}} Internet and other sources of networked information have made automatic mediation of access to networked information sources an increasingly important problem. Much of this information is expressed as electronic text, and it is becoming practical to automatically convert some printed documents and recorded <b>speech</b> <b>to</b> electronic <b>text</b> as well. Thus, automated systems capable of detecting useful documents are finding widespread application. With even {{a small number of}} languages it can be inconvenient to issue the same query repeatedly in every language, so users who are able to read more than one language will likely prefer a multilingual text retrieval system over a collection of monolingual systems. And since reading ability in a language does not always imply fluent writing ability in that language, such users will likely find cross-language text retrieval particularly useful for languages in which they are less confident of their abili...|$|R
40|$|Abstract. The voice enabled web is a {{combination}} of XML based mark-up languages, <b>speech</b> recognition, <b>text</b> <b>to</b> <b>speech</b> (TTS) and web technologies. Key to the success of voice enabled web applications is the “naturalness ” of the interface. Users {{are much more likely to}} interact with a system they feel comfortable with and that responds in a human like way. This paper describes the deployment of TTS in commercial voice enabled web systems and considers whether the excessive usage of TTS can be detrimental to users perceptions of the system...|$|R
50|$|The speech engine {{itself is}} driven by the Microsoft Speech API (SAPI), version 4 and above. Microsoft SAPI {{provides}} a control panel for easily installing and switching between various available Text to Speech and <b>Speech</b> <b>to</b> <b>Text</b> engines, as well as voice training and scoring systems to improve the quality and accuracy of both engines.|$|E
5000|$|Speech translation: The Microsoft Translator API is an {{end-to-end}} REST based API {{that can}} be used to build applications, tools, or any solution requiring multi-languages speech translation. Speech to speech translation is available to or from any of the conversation languages, and <b>speech</b> <b>to</b> <b>text</b> translation is available from the conversation languages into any of the [...] supported language systems.|$|E
50|$|It is {{generally}} acknowledged that <b>speech</b> <b>to</b> <b>text</b> is possible, though recently Thomas Wilde, the new CEO of Everyzing, acknowledged that Everyzing works 70% {{of the time}} when there is music, ambient noise or more than one person speaking. If newscast style speaking (one person, speaking clearly, no ambient noise) is available, that can rise to 93%. (From the Web Video Summit, San Jose, CA, June 27, 2007).|$|E
40|$|Due to {{the highly}} {{communicative}} character of electronic commerce transactions, open-edi representation languages such as FLBC, take the speech act (operator) as their basic building block. The {{advantage of this}} approach is that the `deep structure' of electronic commerce transactions is addressed rather than the form. In this paper, we try to reveal higher-level units of speech acts which are materialized in so called meta-analysis patterns. We define various levels, from <b>speech</b> acts <b>to</b> <b>text.</b> Once patterns have been identified, they can be stored in an FLBC component library and be (re-) used effectively by business partners to speed up open-edi transactions. 1 Introduction The community that is using the world-wide web steadily grows, and has an estimated 20 - 40 million users (Bell and Gemmell, 1996). This trend offers (new) businesses the Porterian possibility to penetrate new markets and expand their activities by entering the electronic commerce. Electronic commerce can be interpr [...] ...|$|R
40|$|International audienceThis study {{focuses on}} {{alternative}} speech communication based on Cued Speech. Cued Speech is a visual mode of communication that uses hand shapes and placements {{in combination with}} the mouth movements of <b>speech</b> <b>to</b> make the phonemes of a spoken language look different from each other and clearly understandable to deaf and hearingimpaired people. Originally, the aim of Cued <b>Speech</b> was <b>to</b> overcome the problems of lip reading and thus enable deaf children and adults to wholly understand spoken language. In this study, however, we investigate the use of Cued Speech not only for perception, but also for speech production in the case of speech- or hearing-impaired individuals. The proposed method is based on hidden Markov model (HMM) automatic recognition. Automatic recognition of Cued <b>Speech</b> and conversion <b>to</b> <b>text,</b> audio, or synthesized Cued Speech can be served as an alternative speech communication method for individuals with speech or hearing impairments. This article presents vowel and consonant, and also isolated word recognition experiments for Cued Speech for French. The results obtained are promising and comparable to the results obtained when using audio signal...|$|R
40|$|Sundanese {{script is}} one revised {{regional}} Indonesia {{which is the}} work of the sundanese have orthographic peculiarities in terms of how the writing system by using a non-latin character as well as in terms of the unique pronunciation, therefore his presence should be conserved. One of its preservation efforts is to build an sundanese script recognition systems, which in the process is to identify a pattern of characters can make use of a technique of feature extraction and classification of characteristics, one of which was modified direction featured (MDF) which is the hallmark of the extraction method based on the shape of the patterns on the image, whereas the methods used for the process of classification of characteristics of an image is learning vector quntization (LVQ). This system will accept input in the image of sundanese script and image patterns revised its characteristics will be taken and entered into the database that will be used as training data, then done in the result of the extraction of characteristics are grouped into classes to the nearest value identified on a class derived from the test image, its function is to support the introduction of this aksara can be combined with <b>text</b> <b>to</b> <b>speech.</b> <b>Text</b> <b>to</b> <b>speech</b> system (TTS) is a system that can turn text into speech, so that the output can be showing the introduction of aksara sunda examples of their pronunciation. The level of accuracy of the test results of the 300 samples data of aksara sunda verified correctly between the suitability of image characters with names and their pronunciation is of 78. 67 %...|$|R
