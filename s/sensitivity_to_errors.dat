85|10000|Public
50|$|Langille et al {{tested the}} {{accuracy}} of this genome prediction step using leave-one-out cross validation on the input set of sequenced genomes. Additional tests examined <b>sensitivity</b> <b>to</b> <b>errors</b> in phylogenetic inference, lack of genomic data, and {{the accuracy of}} the confidence intervals on gene content.|$|E
50|$|Uses the {{piezoresistive effect}} of bonded or formed strain gauges to detect strain due to applied pressure, {{resistance}} increasing as pressure deforms the material. Common technology types are Silicon (Monocrystalline), Polysilicon Thin Film, Bonded Metal Foil, Thick Film, and Sputtered Thin Film. Generally, the strain gauges {{are connected to}} form a Wheatstone bridge circuit to maximize {{the output of the}} sensor and to reduce <b>sensitivity</b> <b>to</b> <b>errors.</b> This is the most commonly employed sensing technology for general purpose pressure measurement.|$|E
5000|$|His Ph.D {{work was}} with Prof. S.N.Roy {{in the area}} of factor analysis. This line of {{research}} (which {{can be seen in the}} monograph Roy, Gnanadesikanand Srivastava, 1971), was concerned with extracting information from experiments where a large number of input variables result in a largenumber of output variables. One important thrust of this work was how to deal with both structured variables (i.e. data where the variable varies across some range of values) and unstructured variables (variables like disease state or color where the order is essentially arbitrary). [...] A second thrust was evaluating whether the results of such analyses were dependent on small changes in the input variables, leading to an unrealistic <b>sensitivity</b> <b>to</b> <b>errors</b> of measurement.|$|E
30|$|This study {{demonstrates}} the potential utility of visualizing the ATB in the brain, enabling {{the determination of}} CBF with less <b>sensitivity</b> <b>to</b> <b>error</b> in input function.|$|R
40|$|It {{was found}} that error field {{threshold}} decreases for high β in NSTX, although the density correlation in conventional threshold scaling implies the threshold would increase since higher β plasmas in our study have higher plasma density. This greater <b>sensitivity</b> <b>to</b> <b>error</b> field in higher β plasmas is due <b>to</b> <b>error</b> field amplification by plasmas. When the effect of amplification is included with ideal plasma response calculations, the conventional density correlation can be restored and threshold scaling becomes more consistent with low β plasmas. However, it was {{also found that the}} threshold can be significantly changed depending on plasma rotation. When plasma rotation was reduced by non-resonant magnetic braking, the further increase of <b>sensitivity</b> <b>to</b> <b>error</b> field was observed...|$|R
5|$|The {{technique}} used to analyse the BFO values was validated against 87 aircraft {{with the same}} SATCOM equipment operating in the region {{around the time of}} Flight 370's disappearance and against 9 previous flights operated by the same aircraft (9M-MRO). The <b>sensitivity</b> <b>to</b> <b>error</b> was calculated during the early phase of Flight 370 when the aircraft's location, track, and ground speed were known. This resulted in an uncertainty of ±28° heading and ±9° of latitude.|$|R
40|$|This {{research}} shows that mating mindsets, which consumers commonly experience, can have a crucial impact on consumers' variety seeking tendencies. In two studies, we find that male consumers in a long-term mating mindset (versus a non-mating mindset) exhibit more (versus less) variety seeking behavior. For male consumers, the effects of mating mindsets on variety seeking are driven by <b>sensitivity</b> <b>to</b> <b>errors</b> of omission. However, for female consumers, variety seeking behavior and <b>sensitivity</b> <b>to</b> <b>errors</b> of omission {{were not significantly different}} across mating and non-mating conditions. In closing, the author discusses theoretical and managerial implications [...] by Yu-Wei Hu. Thesis: S. M. in Management Research, Massachusetts Institute of Technology, Sloan School of Management, 2014. Cataloged from PDF version of thesis. Includes bibliographical references (pages 49 - 53) ...|$|E
40|$|Since A. M. Turing’s paper {{proposing a}} {{mathematical}} basis for pattern formation in developing organisms many mathematical approaches {{have been proposed}} to model biological phenomenon. Continued laboratory study and recent improvements in measurement capabilities have provided an immense quantity of raw gene expression data. The level of data now available demands the development of well-characterized and tested computational tools. Thus, we have examined one mathematical model’s <b>sensitivity</b> <b>to</b> <b>errors</b> in estimating its ’ parameters. Errors in parameter estimation can arise from noise in the laboratory measurements and recasting of laboratory data. We elected to examine the rulebased mathematical model of Mjolsness et al for its ’ <b>sensitivity</b> <b>to</b> <b>errors</b> in estimated parameters. We have used the technique of sensitivity equations as generally applied in nonlinear systems analysis...|$|E
40|$|International audienceFault {{injection}} {{experiments to}} evaluate the <b>sensitivity</b> <b>to</b> <b>errors</b> induced by radiation, were performed on a SHARC Digital Signal Processor included in the instrument of a satellite project. To estimate the expected error rates in space environment, the flight software as well as different simple programs were exercised under simulated faults using the capabilities of a dedicated testbed...|$|E
30|$|In {{this section}} we report {{and discuss the}} results of the {{simulations}} of PRISM codec, in order <b>to</b> understand its <b>sensitivity</b> <b>to</b> transmission <b>errors</b> and its error resilience properties.|$|R
30|$|Robustness, which {{measures}} the beamformer’s <b>sensitivity</b> <b>to</b> random <b>errors,</b> {{is a very}} crucial point in superdirectivity theory. So {{it is essential to}} discuss the robustness of the high-order superdirectivity before our methods are presented.|$|R
40|$|A {{practical}} system for reading out from linear, multi-level, selectorless Resistive Random Access Memory (RRAM) arrays {{based on a}} Trans-Impedance Amplifier (TIA) approach is presented and studied. SPICE simulation of {{the core of the}} system is performed in order <b>to</b> extract predicted <b>sensitivity</b> <b>to</b> <b>error</b> factors such as non-zero TIA offsets and access resistance. A physical implementation of the system is then tested on a small, 12 × 12 reference array and measured results show its ability to decode absolute resistive states in the range of 1 k?- 220 k? within ? 11 % tolerance...|$|R
3000|$|To manage AC <b>sensitivity</b> <b>to</b> <b>errors,</b> {{authors of}} [6] {{proposed}} {{to use an}} extra symbol μ with probability ε > 0 to detect transmission errors. This symbol is introduced in the source alphabet but never transmitted. The forbidden symbol technique implies a reduction of the coding space {{by a factor of}} (1 -ε), thus, reduces compression efficiency. The amount of added rate redundancy is R [...]...|$|E
40|$|An {{increasing}} number of projects have examined the perceptual magnitude of visible artifacts in animated motion. These studies have been performed using a mix of character types, from detailed human models to abstract geometric objects such as spheres. We explore {{the extent to which}} character morphology influences user <b>sensitivity</b> <b>to</b> <b>errors</b> in a fixed set of ballistic motions replicated on three different character types. We find user sensitivity responds to changes in error type or magnitude in a similar manner regardless of character type, but that users display a higher sensitivity to some types of errors when these errors are displayed on more human-like characters. Further investigation of those error types suggests that being able to observe a period of preparatory motion before the onset of ballistic motion may be important. However, we found no evidence to suggest that a mismatch between the preparatory phase and the resulting ballistic motion was responsible for the higher <b>sensitivity</b> <b>to</b> <b>errors</b> that was observed for the most humanlike character. Categories and Subject Descriptors (according to ACM CCS) ...|$|E
40|$|Measurements {{are often}} {{provided}} {{in the presence of}} noise and uncertainties that require optimal filters to estimate processes with highest accuracy. The ultimate iterative unbiased finite impulse response (UFIR) filtering algorithm presented in this paper is more robust in real world than the Kalman filter. It completely ignores the noise statistics and initial values while demonstrating better accuracy under the mismodeling and temporary uncertainties and lower <b>sensitivity</b> <b>to</b> <b>errors</b> in the noise statistics...|$|E
40|$|We {{investigated}} two optical {{methods for}} characterizing submicron structures. Average errors {{of a few}} nanometers can {{be determined by the}} far-field diffraction metrology utilizing diffractive structures having enhanced <b>sensitivity</b> <b>to</b> fabrication <b>errors.</b> The scanning spot metrology is well suited for analyzing lithographic masks...|$|R
50|$|Applying error {{protection}} enables <b>error</b> correction up <b>to</b> {{a certain}} extent. Error correcting codes are usually applied equally {{to the whole}} payload. However, since different parts of an AAC payload show different <b>sensitivity</b> <b>to</b> transmission <b>errors,</b> {{this would not be}} a very efficient approach.|$|R
40|$|An {{accurate}} and e#cient model of human perception {{has been developed}} to control the placement of samples in a realistic image synthesis algorithm. Previous sampling techniques have sought <b>to</b> spread the <b>error</b> equally across the image plane. However, this approach neglects {{the fact that the}} renderings are intended to be displayed for a human observer. The human visual system has a varying <b>sensitivity</b> <b>to</b> <b>error</b> that is based upon the viewing context. This means that equivalent optical discrepancies can be very obvious in one situation and imperceptible in another. It is ultimately the perceptibility of this error that governs image quality and should be used as the basis of a sampling algorithm...|$|R
40|$|In this paper, {{we address}} the problem of {{enhancement}} of a noisy GARCH process using a particle filter. We compare our approach experimentally to a previously developed recursive estimation scheme. Simulations indicate that a significant gain in performance is obtained, at the cost of higher <b>sensitivity</b> <b>to</b> <b>errors</b> in the GARCH parameters. The proposed method allows tackling arbitrary driving noise distributions as well as arbitrary fidelity criteria. Index Terms — Particle filtering, GARCH, dynamic estimation. 1...|$|E
40|$|In last years, Simultaneous Localization And Mapping {{techniques}} monopolize {{research in}} mobile robot self-localization, {{especially in the}} civilian, Service Robotics domain. In this paper we argue that techniques derived from the industrial scenario, in particular beacon-based triangulation systems, {{should be taken into}} considerations even for civilian applications whenever we deem important to provide accurate, a priori specifications about the system behavior in a generic, yet untested environment. The paper provides an analytical expression of <b>sensitivity</b> <b>to</b> <b>errors</b> for triangulation depending on the system geometry...|$|E
40|$|Abstract. The {{condition}} of the latest versions of the ELMy H-mode and L-mode databases have been re-examined in view of their <b>sensitivity</b> <b>to</b> <b>errors</b> in the absorbed heating power and stored energy. It is shown that there is bias in the OLS regression {{for some of the}} variables. These short comings are overcome by the use of an error in variables technique and new scalings are derived. These give a very similar performance to existing scalings for ITER at the standard βn of 1. 8, but much improved performance at higher βn. 1...|$|E
40|$|Sea ice {{concentration}} {{has been}} retrieved in polar regions with satellite microwave radiometers for over 30 years. However, {{the question remains}} {{as to what is}} an optimal sea ice concentration retrieval method for climate monitoring. This paper presents some of the key results of an extensive algorithm inter-comparison and evaluation experiment. The skills of 30 sea ice algorithms were evaluated systematically over low and high sea ice concentrations. Evaluation criteria included standard deviation relative to independent validation data, performance in the presence of thin ice and melt ponds, and <b>sensitivity</b> <b>to</b> <b>error</b> sources with seasonal to inter-annual variations and potential climatic trends, such as atmospheric water vapour and water-surface roughening by wind. A selection of 13 algorithms is shown in the article to demonstrate the results. Based on the findings, a hybrid approach is suggested to retrieve sea ice concentration globally for climate monitoring purposes. This approach consists of a combination of two algorithms plus dynamic tie points implementation and atmospheric correction of input brightness temperatures. The method minimizes inter-sensor calibration discrepancies and <b>sensitivity</b> <b>to</b> the mentioned <b>error</b> sources...|$|R
30|$|The paper outline is as follows: Section 2 {{describes}} the system, measurement models, and state estimation. Simulations {{of the performance}} of the positioning scheme, its <b>sensitivity</b> <b>to</b> different <b>errors,</b> and the importance of the appearance of the trajectory are studied in Section 3. The paper ends with conclusions.|$|R
40|$|Bit flips {{provoked}} by radiation are a main concern for space applications. A fault injection experiment performed using a software simulator {{is described in}} this paper. Obtained results allow us to predict a low <b>sensitivity</b> <b>to</b> soft <b>errors</b> for the studied application, putting in evidence critical memory elements...|$|R
40|$|Quantum {{computing}} algorithms {{require that}} the quantum register be initially present in a superposition state. To achieve this, we consider the practical problem of creating a coherent superposition state of several qubits. Owing to considerations of quantum statistics, this requires that the entropy of the system go down. This, in turn, has two practical implications: (i) the initial state cannot be controlled; (ii) {{the temperature of the}} system must be reduced. These factors, in addition to decoherence and <b>sensitivity</b> <b>to</b> <b>errors,</b> must be considered in the implementation of quantum computers. Comment: 7 pages; the final published versio...|$|E
40|$|The goal of {{this thesis}} is to extend the {{analysis}} and the scope of first-order methods of smooth convex optimization. We consider three challenging difficulties: inexact first-order information, lack of smoothness and presence of linear constraints. When used with inexact information, we show that the Gradient Method (GM) is slow but robust, whereas the Fast Gradient Method (FGM) is fast but sensitive to errors. This trade-off between speed and <b>sensitivity</b> <b>to</b> <b>errors</b> is unavoidable: the faster a first-order method is, the worse its robustness must be. Between the existing methods, we develop a novel scheme, the Intermediate Gradient Method (IGM), which seeks an optimal compromise between speed and robustness and significantly accelerates the generation of accurate solutions. We also show how much strong convexity and stochastic first-order information can decrease the <b>sensitivity</b> <b>to</b> <b>errors</b> of first-order methods. When the objective function is not as smooth as desired, we show that first-order methods initially developed for smooth problems can still be applied. This result breaks the wall between smooth and nonsmooth optimization. In particular, FGM {{can be seen as}} a universal optimal first-order method. When linear constraints prevent the use of usual first-order methods, we propose a new approach, the double smoothing technique. We dualize the linear constraints, transform the dual function into a smooth strongly convex function and apply FGM. This technique efficiently generates nearly optimal and feasible primal solutions with accuracy guarantees. (FSA 3) [...] UCL, 201...|$|E
40|$|We {{describe}} {{a method that}} uses Optical Transition Radiation (OTR) screens to measure in a single shot the emittance of an electron beam with an energy greater than 100 MeV. Our setup consists of 4 OTR screens located near a beam waist. A fit of the 4 profiles allows {{the reconstruction of the}} Twiss parameters and hence a calculation of the beam emittance. This layout has been simulated using Mathematica to study its <b>sensitivity</b> <b>to</b> <b>errors</b> and using Geant 4 to include effects due to the scattering of the electrons in the screens. Some assumptions made in this document are conservative to allow for unexpected experimental issues...|$|E
40|$|Abstract — Electrical Impedance Tomography (EIT) calculates the {{internal}} conductivity distribution within a body from current simulation and voltage measurements {{on the body}} surface. Two main technical difficulties of EIT are its low spatial resolution and <b>sensitivity</b> <b>to</b> measurement <b>errors.</b> Image reconstruction using ℓ 1 norms allows addressing both difficulties, in comparison to traditional reconstruction using ℓ 2 norms. A ℓ 1 norm on the data residue term reduces the <b>sensitivity</b> <b>to</b> measurement <b>errors,</b> while the ℓ 1 norm on the image prior reduces edge blurring. This paper proposes and tests a general lagged diffusivity type iterative method for EIT reconstructions. ℓ 1 and ℓ 2 minimizations can be flexibly chosen on the data residue and/or image prior parts. Results show {{the flexibility of the}} algorithm and the merits of the ℓ 1 solution. I...|$|R
40|$|The Generalised Least Squares (GLS) {{approach}} to the estimation of Origin-Destination (OD) matrices permits the combination of survey and traffic count data {{in a way that}} allows for the relative accuracy of the two data sources. However, the procedure may result in estimates that infringe non-negativity or other constraints on the fitted values. A simple algorithm to solve the GLS problem subject to inequality constraints is presented and its convergence is proven. Expressions are derived for the variances and covariances of the fitted values both without active constraints. It is demonstrated that the imposition of inequality constraints can improve the accuracy of those fitted values directly and indirectly affected them, by reducing their <b>sensitivity</b> <b>to</b> <b>error</b> in the inputs. ...|$|R
40|$|A {{regularity}} of {{the predicted}} <b>sensitivities</b> <b>to</b> random and proportional dye concentration <b>errors</b> in regard <b>to</b> {{the position of}} target colour has been observed for the case of dyeing acrylic with basic dyes. The sensitivity of the recipe colour to random dye concentration errors is highest for light neutral target colours and is almost negligible for dark-shade recipes. Recipes for less saturated targets are slightly more sensitive than recipes for more saturated targets of equal lightness. The span of the <b>sensitivities</b> <b>to</b> weighing <b>error</b> of the recipes matching a given target varies with {{the position of the}} target in colour space. By contrast, the <b>sensitivity</b> of recipes <b>to</b> dye strength <b>error</b> is the highest at medium to low lightness for neutral and near neutral target shades. The span of sensitivities of the recipes for any such particular target is broad, with some having low <b>sensitivity</b> <b>to</b> strength <b>errors.</b> Recipes for the target colours at the "lighter" part of the gamut border were the least sensitive <b>to</b> strength <b>errors...</b>|$|R
40|$|Abstract. The aim of {{this paper}} is to analyze and design reduced order {{observers}} of the rotor flux of induction motors. The design requirements are: a) the convergence rate of the rotor flux estimation error; b) a low sensitivity to stator and rotor resistance variations; c) a low <b>sensitivity</b> <b>to</b> <b>errors</b> due to the implementation of the observers on microprocessor-based systems. It is shown that, in order to satisfy the requirements a) -c), it is sufficient to solve a constrained optimization problem according to a criterion in which these requirements appear explicitly. The implementation of the observer is discussed. The observer is tested by simulation and experiments. Copyright © 2005 IFA...|$|E
40|$|Artículos en revistasThis paper {{presents}} a rotor-speed estimator for induction motor drives which uses only stator {{voltage and current}} measurements. First of all, the identifiability of the rotor speed is analysed {{in relation to the}} operating point, giving a good insight into when and why rotor speed can be estimated. Secondly, a computation method is proposed for the rotor speed. Its <b>sensitivity</b> <b>to</b> <b>errors</b> in both model parameters and measurements is discussed in detail. In order to produce effective noise attenuation, a state-variable filter and a recursive total least-squares (TLS) algorithm are used. The estimator is validated by simulation and by field tests. info:eu-repo/semantics/publishedVersio...|$|E
40|$|The aim of {{this paper}} is to analyze and design reduced order {{observers}} of the rotor flux of induction motors. The design requirements are: a) the convergence rate of the rotor flux estimation error; b) a low sensitivity to stator and rotor resistance variations; c) a low <b>sensitivity</b> <b>to</b> <b>errors</b> due to the implementation of the observers on microprocessor-based systems. It is shown that, in order to satisfy the requirements a) -c), it is sufficient to solve a constrained optimization problem according to a criterion in which these requirements appear explicitly. The implementation of the observer is discussed. The observer is tested by simulation and experiments...|$|E
40|$|This paper {{presents}} and investigates {{a novel approach}} for constructing family of ISI-free pulses. We propose and discuss a family of new Nyquist pulses obtained from a linear combination of two ISI-free pulses produced by Nyquist filters characteristics. They show reduced <b>sensitivity</b> <b>to</b> timing <b>errors,</b> as compared with some recent pulses introduced in [6]...|$|R
40|$|Orbit {{determination}} accuracies attainable {{during a}} Mariner/Jupiter/Saturn encounter {{have been established}} by means of covariance analyses. Advanced earth-based multistation radiometric data, optical data consisting of star/satellite pictures provided by a narrow-angle TV camera on board the spacecraft, and batch sequential filtering methods were employed. An evaluation of sequential filter <b>sensitivities</b> <b>to</b> <b>errors</b> in modeling small nongravitational spacecraft accelerations is presented. Selected covariances developed in the orbit determination analysis are used in {{an investigation of the}} trajectory correction costs associated with a close Ganymede encounter. The 99 percentile Delta V contours in the Ganymede aiming plane are obtained for satellite encounters before or after Jupiter closed approach, and the sources of the dominant contributions to the Delta V costs are identified...|$|R
30|$|Identification of {{economically}} vulnerable {{buses and}} influential lines on LMP. Simulation {{studies show that}} the buses with the most <b>sensitivity</b> <b>to</b> topology <b>errors</b> are those at the ends of the union of the congested line and the erroneous line. This also implies that the erroneous line and/or the congested line are the most influential ones on LMP.|$|R
