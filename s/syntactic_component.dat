93|108|Public
25|$|Each logical system {{comes with}} both a <b>syntactic</b> <b>component,</b> which among other things determines the notion of provability, and a {{semantic}} component, which determines the notion of logical validity. The logically valid formulas of a system are sometimes called the theorems of the system, especially {{in the context of}} first-order logic where Gödel's completeness theorem establishes the equivalence of semantic and syntactic consequence. In other settings, such as linear logic, the syntactic consequence (provability) relation may be used to define the theorems of a system.|$|E
5000|$|The next {{complete}} <b>syntactic</b> <b>component</b> (s-expression) can be commented {{out with}} [...]|$|E
5000|$|... {{the status}} of the <b>syntactic</b> <b>component</b> of the grammar with respect to the other {{components}} (i.e. phonological and semantic).|$|E
5000|$|A list {{comprehension}} has {{the same}} <b>syntactic</b> <b>components</b> to represent generation of a list in order from an input list or iterator: ...|$|R
40|$|English {{system of}} nominalization is rapidly being enriched by the {{nominative}} structures of N+N(+N+…Nn) type. The most appropriate term {{for them is}} polynom. The nature of polynoms is rather lexical than <b>syntactic.</b> <b>Components</b> of polynoms may have different semantic relations. Polynoms have hierarchial structure, the core {{of which is the}} binom. In the future polynoms may turn into acronyms, blendings or complex nouns...|$|R
25|$|A study {{published}} in the conference PPIG evaluated the effects of syntax highlighting on the comprehension of short programs, finding that the presence of syntax highlighting significantly reduces the time taken for a programmer to internalise the semantics of a program. Additionally, data gathered from an eye-tracker during the study suggested that syntax highlighting enables programmers to pay less attention to standard <b>syntactic</b> <b>components</b> such as keywords.|$|R
5000|$|In Aspects, Chomsky {{summarized}} {{his proposed}} {{structure of a}} grammar in the following way: [...] "A grammar contains a <b>syntactic</b> <b>component,</b> a semantic component and a phonological component...The <b>syntactic</b> <b>component</b> consists of a base and a transformational component. The base, in turn, consists of a categorial subcomponent and a lexicon. The base generates deep structures. A deep structure enters the semantic component and receives a semantic interpretation; it is mapped by transformational rules into a surface structure, which is then given a phonetic interpretation {{by the rules of}} the phonological component." [...] In this grammar model, syntax is given a prominent, generative role, whereas phonology and semantics are assigned secondary, interpretive roles. This theory of grammar would later {{come to be known as}} the [...] "Standard Theory" [...] (ST).|$|E
50|$|Over time, {{syntactic}} {{change is}} the greatest modifier of a particular language. Massive changes - attributable either to creolization or to relexification - may occur both in syntax and in vocabulary. Syntactic change can also be purely language-internal, whether independent within the <b>syntactic</b> <b>component</b> or the eventual result of phonological or morphological change.|$|E
5000|$|The {{base in the}} <b>syntactic</b> <b>component</b> {{functions}} as follows: In the first step, a simple set of phrase structure rules generate tree diagrams (sometimes called Phrase Markers) consisting of nodes and branches, but with empty terminal nodes; these are called [...] "pre-lexical structures". In the second step, the empty terminal nodes are filled with complex symbols consisting of morphemes accompanied by syntactic and semantic features, supplied from the lexicon via lexical insertion rules. The resulting tree diagram is called a [...] "deep structure".|$|E
40|$|This paper {{presents}} {{the design and}} implementation of a finite-state syntactic grammar {{with the objective of}} extracting information about verb subcategorization instances from newspaper texts. After a partial parser has built basic syntactic units such as NPs, PPs, and sentential complements, a finite-state parser performs syntactic disambiguation and filtering of the results, in order to obtain a verb occurrence together with its associated <b>syntactic</b> <b>components.</b> ...|$|R
40|$|The {{thesis is}} about students’ {{perception}} of difficulty of algebraic equations in elementary school. The theoretical {{part of the}} thesis describes the process of metacognition and {{the function of the}} perception of difficulty of mathematics tasks in the learning process. Then, we present the semantic and <b>syntactic</b> <b>components</b> of mathematics tasks (an analogy to semantic and <b>syntactic</b> <b>components</b> of plain texts). In particular, we consider these components on tasks related to the solution of linear equations. The components serve as a tool for analyzing the difficulty of mathematics tasks. We consider also the results of TIMSS and NPZ studies related to the linear algebraic equations with one unknown and relate them to the components that influence their difficulty and are later used in the empirical part of the thesis. In the empirical part of the thesis we present a research on the factors that influence the students’ perception of the difficulty of algebraic equations. Students were asked to range the tasks within several triads of tasks according to the perceived difficulty of the tasks. The found perceived difficulty of the tasks is compared with students’ performance in the TIMSS and NPZ research and the published research by other authors. ...|$|R
40|$|This paper {{presents}} {{the design and}} implementation of a finite-state syntactic grammar of Basque {{that has been used}} with the objective of extracting information about verb subcategorization instances from newspaper texts. After a partial parser has built basic syntactic units such as noun phrases, prepositional phrases, and sentential complements, a finite-state parser performs syntactic disambiguation, determination of clause boundaries and filtering of the results, in order to obtain a verb occurrence together with its associated <b>syntactic</b> <b>components,</b> either complements or adjuncts. The set of occurrences for each verb is then filtered by statistical measures that distinguish arguments from adjuncts...|$|R
50|$|Each logical system {{comes with}} both a <b>syntactic</b> <b>component,</b> which among other things determines the notion of provability, and a {{semantic}} component, which determines the notion of logical validity. The logically valid formulas of a system are sometimes called the theorems of the system, especially {{in the context of}} first-order logic where Gödel's completeness theorem establishes the equivalence of semantic and syntactic consequence. In other settings, such as linear logic, the syntactic consequence (provability) relation may be used to define the theorems of a system.|$|E
50|$|During {{his years}} at RAND {{he worked on}} the machine {{translation}} of Russian technical literature into English and more generally on computational linguistics, a term that he created. The <b>syntactic</b> <b>component</b> of the RAND system was based on Lucien Tesnière's dependency grammar and Hays became its principal advocate in America. More than anyone else Hays is responsible for the realization that language processing should consist in the application of theoretically motivated grammars to specific texts by general algorithms. In 1967 Hays published the first textbook in computational linguistics, Introduction to Computational Linguistics. At his direction RAND assembled an annotated corpus of a million words of Russian text, and thus pioneered in what is now known as corpus linguistics.|$|E
40|$|The paper {{discusses}} {{various issues}} that regard the "word" {{as a unit}} of language. It is proposed that the difficulties in defining more complex, phrase-like constituents, as words can be overcome by adopting a mixed morphosyntactic approach which relegates some word formation to the <b>syntactic</b> <b>component</b> of the language...|$|E
40|$|Abstract. We {{propose a}} method of {{describing}} human activities from video images based on concept hierarchies of actions. Major difficulty in transforming video images into textual descriptions is how to bridge a semantic gap between them, which {{is also known as}} inverse Hollywood problem. In general, the concepts of events or actions of human can be classified by semantic primitives. By making correspondence between these concepts and the semantic features extracted from video images, appropriate <b>syntactic</b> <b>components</b> such as verbs, objects, etc. are determined and then translated into natural language sentences. We also demonstrate the performance of the proposed method by several experiments...|$|R
40|$|Dependency Grammar {{has been}} used by {{linguists}} as the basis of the <b>syntactic</b> <b>components</b> of their grammar formalisms. It has also been used in natural langauge parsing. In China, {{attempts have been made to}} use this grammar formalism to parse Chinese sentences using corpus-based techniques. This paper reviews the properties of Dependency Grammar as embodied in four axioms for the well-formedness conditions for dependency structures. It is shown that allowing multiple governors as done by some followers of this formalism is unnecessary. The practice of augmenting Dependency Grammar with functional labels is discussed in the light of building functional structures when the sentence is parsed. This will also facilitate semantic interpretion. ...|$|R
40|$|Understanding a {{block of}} {{handwritten}} text means mapping it into a semantic representation, We describe an approach to reading a I>lock of handwritten text when there arc certain loose constraints placed on the spatial layout and syntax 01 the tnt. Early recognition of primitives guides the location of <b>syntactic</b> <b>components.</b> A system to read handwritten postal addresses is described as an instance. The semantic representation {{in this case is}} a digit string (ZIP Code). Method, for segmenting a string of digits into components and for recognizing digits using a multiplicity of rcwgnit. ers are given. KC. I'words: Character rccognition: Handwriting recognition: Classifier systems. I...|$|R
40|$|Until {{well into}} the 1970 s {{research}} in generative syntax {{was based on the}} presumption that the <b>syntactic</b> <b>component</b> of a grammar should consist of ordered transformations that derive surface structures (and later logical forms) from deep structures (themselves generated by phrase structure rules) through intermediate structures. Though there wer...|$|E
40|$|In the derivational ‘crash ’ {{model of}} the Minimalist Program (Chomsky, 1995), the <b>syntactic</b> <b>{{component}}</b> feeds the phonological component though PF (and LF) convergence is ultimately responsible for overt movement. Strong features are illegitimate PF objects, hence they must be checked by movement {{in the course of}} the derivation and eliminated by the time th...|$|E
40|$|The {{purpose of}} this paper is t {{combinations}} of clitics in varie ordering of clitics is only deter after the <b>syntactic</b> <b>component,</b> nent. The alternative I propose in the syntax, as anticipated b some of the anomalies involve tics) while in others one eleme logical status or weak pronom Key words: clitic combination 1...|$|E
40|$|One {{central issue}} {{in the theory of}} clause types is whether force is {{represented}} in the syntax. Based on data from English, Italian, and Paduan, we examine this question focusing on a less well-studied clause type, exclamatives. We argue that there is no particular element in syntax responsible for introducing force. Rather, there are two fundamental <b>syntactic</b> <b>components</b> which identify a clause as exclamative, a factive and a Wh operator. These are crucial because they are responsible for two fundamental semantic properties characteristic of exclamatives, namely that they are factive and denote a set of alternative propositions. The force of exclamatives, which we characterize as ‘widening’, is derived indirectly, based on the semantic properties...|$|R
40|$|Object {{oriented}} databases (OODBs) {{are composed}} of semi-independent objects but must also provide {{for the maintenance of}} inter-object consistency, especially with respect to constraints arising from class hierarchies and inter-object references. Hence the problem to provide consistent generic update methods. We address the problem how to derive such methods from the structure of an OODB schema by the specification of generator macros for them. These generators are based on a strict mathematical formalization of OODB concepts including the possibility to represent <b>syntactic</b> <b>components</b> of the language as values within the language itself, which is known to form the basis of linguistic reflection. Moreover, the approach can be extended to the enforcement of user-defined integrity constraints that give rise to context sensitive macros turning each user-defined method into branches of its greatest consistent specialization...|$|R
40|$|In this thesis, {{morphological}} {{description of}} Turkish is encoded using the two-level model. This description {{is made up}} of the phonological component that contains the two-level morphophonemic rules, and the lexicon component which lists the lexical items and encodes the morphotactic constraints. The word grammar is expressed in tabular form. It includes the verbal and the nominal paradigm. Vowel and consonant harmony, epenthesis, reduplication, etc. are described in detail and coded in two-level notation. Loan-word phonology is modelled separately. The implementation makes use of Lexc/Twolc from Xerox. Mechanisms to integrate the morphological analyzer with the lexical and <b>syntactic</b> <b>components</b> are discussed, and a simple graphical user interface is provided. Work is underway to use this model in a classroom setting for teaching Turkish morphology to non-native speakers. Comment: 128 pages, postscript, MS Thesis in Dept of Computer Engineerin...|$|R
40|$|This study aims {{to develop}} {{techniques}} for the meaning {{acquisition of the}} NL phrases. Our approach is based on modeling meaning extraction by analyzing lexical component, <b>syntactic</b> <b>component</b> and semantic component of the phrases written or spoken in NL. To assure necessary semantic precision we adopt a double-level interpretation of the NL phrases using formal language with well defined semantics. This fact is essential for our approach...|$|E
40|$|This memo {{offers a}} working {{definition}} of the interface between the syntactic and semantic components of the Verbmobil demonstrator system. A subsequent version of the memo will address the outstanding issues sketched in Section 8. The <b>syntactic</b> <b>component</b> provides a HPSG-style representation of user utterances, derived from the speech recognition component. The semantic component builds a semantic representation from this syntactic representation. The <b>syntactic</b> <b>component</b> is realized by two systems: a system developed by Siemens and based on TUG (Trace and Unification Grammar) (Block and Schachtl, 1991); and one developed by IBM and based upon HPSG (Pollard and Sag, 1987; Pollard and Sag, 1994). The semantic construction component builds representations based upon a DRS-based semantic formalism (Bos et al., 1994). The purpose of the interface is to specify the information flow between these components: i. e. what common syntactic information is available to (and necessary for) the semantic component, and what semantic information is available to the <b>syntactic</b> <b>component.</b> The interface also addresses the syntax requirements arising from other components (such as transfer, dialogue and semantic evaluation). The memo is structured as follows. In Section 2, we give our motivation for a syntax semantic interface. In Section 3 we describe the structure {{and content of the}} daughters feature (dtrs) in a HPSG-style sign, and the set of macros which access syntactic information. In Section 4 a macro for accessing semantic information is defined and its use in constraining the syntactic analysis of date and time expressions is outlined. In Section 5 we describe the structure and content of lexical entries. Section 6 describes the analysis of specific phenomena; verb position, the treat [...] ...|$|E
40|$|When we {{pay close}} {{attention}} to the prosody of Wh-questions in Japanese, we discover many novel and interesting empirical puzzles that would require us to devise a much finer <b>syntactic</b> <b>component</b> of grammar. This paper addresses the issues that pose some problems to such an elaborated grammar, and offers solutions, making an appeal to the information structure and sentence processing involved in the interpretation of interrogative and focus constructions...|$|E
40|$|The higher-order {{structure}} of genes {{and other features}} of biological sequences can be described by means of formal grammars. These grammars can then be used by general-purpose parsers to detect and assemble such structures by means of syntactic pattern recognition. We describe a grammar and parser for eukaryotic protein-encoding genes, which by some measures is as effective as current connectionist and combinatorial algorithms in predicting gene structures for sequence database entries. Parameters on the grammar rules are optimized for several different species, and mixing experiments performed to determine the degree of species specificity and {{the relative importance of}} compositional, signal-based, and <b>syntactic</b> <b>components</b> in gene prediction. Introduction Formal language theory views languages as sets of strings over some alphabet, and specifies potentially infinite languages with concise sets of rules called grammars [10]. Grammars are an exceptionally well-studied methodology, fami [...] ...|$|R
40|$|The paper {{describes}} an experimental model of syntactic structure generation {{starting from the}} limited fragment of se-mantics that deals with the quantitative values of object parameters. To present the input information the basic semantic units of four types are proposed:&quot;object&quot;, &quot;parameter&quot;, &quot;function &quot; and &quot;constant&quot;. For the syntactic structure representation the system of <b>syntactic</b> <b>components</b> is used that combines {{the properties of the}} depen-dency and constituent systems: the syntac-tic components corresponding to wordforms and exocentric constituents are introduced and two basic subordinate relations (&quot;ac-tant &quot; and &quot;attributive&quot;) are claimed to be necessary. Special attention has been de-voted to problems of complex correspon-dence between the semantic units and lexi-cal-syntactic means, In the process of synthesis such sections of the model as the lexicon, the syntactic structure gene-ration rules, the set of syntactic restric-tions and morphological operators are uti-lized to generate the considerable enough subset of Russian parametric constructions. ...|$|R
40|$|The {{kinds of}} change in facial {{expression}} which can be produced by a sender's neural control of facial muscles, and, at the same time, can be visually perceived by a listener are described in discrete symbols. The linguistic information supplemented by the facial expressions in signing is categorized into syllabic, lexical and <b>syntactic</b> <b>components.</b> In order to control the facial expressions for this linguistic information, the timing and duration of the symbols are specified, and, by applying a set of concatenation rules, the control commands are generated. The adequacy of the symbolic descriptive system and its control rules have been examined through a computer synthesis of artificial facial expression, taking the example of transmitting syntactic information in traditional Japanese sign language. These facial expressions are combined with the symbolic descriptive system for hand shapes and arm actions and their control rules for the pictorial display of signing gestures...|$|R
40|$|Most Artificial Intelligence {{theories}} of longuoge either assume o <b>syntactic</b> <b>component</b> which serves OS “front end ” {{for the rest}} of the system, or else re-ject all attempts ot distinguishing modules within the comprehension system. In this paper we will present on alternotive which, while keeping modularity, will account for several puzzles for typical “syntax first ” theories. The maior addition to this theory is o “marker passing ” (or “spreading activation”) com...|$|E
40|$|This article {{intends to}} analyze and revise {{some aspects of the}} {{generative}} methodology when the latter is to be applied to describe correctly the organization of the <b>syntactic</b> <b>component</b> of a language. When considering a specific problem such as that of passivity, the analysis of the organizational principles of the standard (and extended) theory reveals the heterogeneity of the syntactic phenomena which are supposedly to be analysed by the same principles and an incapability to distinguish explicitly between relations based on meaning and on designation. Hence it is here claimed that the analysis of the same phenomena is made possible only if the general structure of the model be modified to include as underlying level of representation the type of relations proposed by Fillmore in his case grammar, and so be the procedures of categorization used with the adoption of the procedure of «partial matching » which as such does not require a rule or specific category for each and all of the syntactic phenomena. Upon such a modification, the possibility or convenience of a radical opposition between the levels of «competence» and «performace» becomes impracticable, as well as the claim to operativeness of the rules of the <b>syntactic</b> <b>component</b> either outside specific contextual situations or in consideration of other categorial alternatives...|$|E
40|$|This paper {{presents}} an {{implementation of a}} parser, which constructs syntax and semantics of a sentence in parallel. The parser consists of a <b>syntactic</b> <b>component</b> similar to PATR [...] 2 and a semantic component which uses Discourse Representation Theory (DRT). To construct the Discourse Representation Structures (DRS) a special algorithm based on [...] calculus is used ([5], [1], [6]). After adapting this algorithm to the parallel construction of syntax and semantics, the analysis of different phenomena in German like tempora, ambigous sentences, negation and verbs as valences of verbs is possible...|$|E
40|$|International audienceThis paper {{presents}} an overview on the IF toolset {{which is an}} environment for modelling and validation of heterogeneous real-time systems. The toolset is built upon a rich formalism, the IF notation, allowing structured automata-based system representations. Moreover, the IF notation is expressive enough to support real-time primitives and extensions of high-level modelling languages such as SDL and UML by means of structure preserving mappings. The core part of the IF toolset consists of a <b>syntactic</b> transformation <b>component</b> and an open exploration platform. The <b>syntactic</b> transformation <b>component</b> provides language level access to IF descriptions and {{has been used to}} implement static analysis and optimisation techniques. The exploration platform gives access to the graph of possible executions. It has been connected to different state-of-the-art model-checking and test-case generation tools. A methodology {{for the use of the}} toolset is presented at hand of a case study concerning the Ariane- 5 Flight Program for which both an SDL and a UML model have been validated...|$|R
40|$|AbstractModular {{structural}} {{operational semantics}} (MSOS) {{is a new}} framework that allows structural operational semantics (SOS) specifications to be made modular {{in the sense of}} not imposing the redefinition of transition rules, which is the case in SOS specifications, when an extension is made. Maude MSOS tool (MMT) is an executable environment for MSOS implemented in Full Maude as a realization of a semantics-preserving mapping between MSOS and rewriting logic (RWL). The modular SOS definition formalism (MSDF) is the specification language supported by MMT. MSDF syntax is quite close to MSOS mathematical notation and user-friendly by allowing several <b>syntactic</b> <b>components</b> to be left implicit. MMT joins the support for modularity with a user-friendly syntax together with the efficient execution and analysis of the Maude engine. We have used MMT in several different examples from programming languages semantics and concurrent systems. This paper reports on the development of MMT and its application to these two classes of specifications...|$|R
40|$|In {{this paper}} we {{describe}} {{an effort to}} construct a catalogue of syntactic data, exemplifying the major syntactic patterns of German. The purpose of the corpus is to support the diagnosis of errors in the <b>syntactic</b> <b>components</b> of natural language processing (NLP) systems. Two secondary aims are the evaluation of NLP systems components {{and the support of}} theoretical and empirical work on German syntax. The data consist of artificially and systematically constructed expressions, including also negative (ungrammatical) examples. The data are organized into a relational data base and annotated with some basic information about the phenomena illustrated and the internal structure of the sample sentences. The organization of the data supports selected systematic testing of specific areas of syntax, but also serves the purpose of a linguistic data base. The paper first gives some general motivation for the necessity of syntactic precision in some areas of NLP and discusses the potential contrib [...] ...|$|R
