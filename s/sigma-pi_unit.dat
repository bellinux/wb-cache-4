5|8|Public
50|$|Another way {{to solve}} {{nonlinear}} problems without using multiple layers is to use higher order networks (<b>sigma-pi</b> <b>unit).</b> In this type of network, each element in the input vector is extended with each pairwise combination of multiplied inputs (second order). This can be extended to an n-order network.|$|E
40|$|We {{investigate}} the computational {{power of a}} model for a spiking neuron in the Boolean domain by comparing it with traditional neuron models such as threshold gates (or McCulloch-Pitts neurons) and sigma-pi units (or polynomial threshold gates). In particular, we estimate the number of gates required to simulate a spiking neuron by a disjunction of threshold gates and we establish tight bounds for this threshold number. Furthermore, we analyze the degree of the polynomials that a <b>sigma-pi</b> <b>unit</b> must use for the simulation of a spiking neuron. We show that this degree cannot be bounded by any fixed value. Our results give evidence that the use of continuous time as a computational resource endows single-cell models with substantially larger computational capabilities...|$|E
40|$|This paper {{introduces}} a neural unit, {{similar to a}} <b>sigma-pi</b> <b>unit,</b> that can learn and generalize linearly inseparable binary input vectors. Learning effectively decides a higher-order polynomial suitable to the problem being trained. The unit generalizes {{in accordance with the}} relation specified by that polynomial, and hard problems like the parity problem are generalized easily. In training the neural unit, a gradient-descent based supervised learning algorithm is adopted. 1. INTRODUCTION The threshold mechanism in a McCulloch and Pitts neuron is not the only nonlinearity that {{plays an important role in}} information processing in the brain. Over the years, a substantial body of evidence has grown to support the presence of nonlinear synaptic connections and multiplicative-like operations. We refer the reader to the review article by Koch and Poggio [1]. The introduction of polynomial or sigma-pi units [2][3] have motivated the research to investigate the computational abilities of ne [...] ...|$|E
40|$|The HyperNet (Hypercube based neural Network) {{architecture}} uses higher-order sigmapi units {{which are}} realisable with conventional memory technology. Real-valued node {{input and output}} vectors promote automatic generalisation and allow the architecture to interface to analogue data. Hardware implementation of real-valued inputs as parallel stochastic bit streams provides a cost effective realisation of the architecture. An FPGA-based demonstration system is presented which uses on-chip reinforcement training. INTRODUCTION Conventional weighted-sum-of-inputs neurons can only perform linearly separable functions of their input vectors. Higher-order <b>sigma-pi</b> <b>units</b> can form any function monotonic of their inputs [Rumelhart 86]. Gurney has shown that HyperNet-like RAM-based nodes are functionally equivalent to <b>sigma-pi</b> <b>units</b> [Gurney 92]. The non-linear nature of sigma-pi neurons enables accurate function approximation using relatively few neurons and sparse interconnection schemes and lead [...] ...|$|R
40|$|A set of <b>sigma-pi</b> <b>units</b> {{randomly}} {{connected to}} two input vectors forms a disorganized type of hetero-associative memory related to convolution- and matrix-based associative memories. Associations are represented as patterns of activity rather than connection strengths. Decoding the associations requires another network of <b>sigma-pi</b> <b>units,</b> with connectivity {{dependent on the}} encoding network. Learning the connectivity of the decoding network involves setting n 3 parameters (where n {{is the size of}} the vectors), and can be accomplished in approximately 3 e n log n presentations of random patterns. This type of network stores information in activation values rather than in weight values, which makes the information accessible to further processing. This is essential for higher-level processing. The functionality of such random networks makes it more plausible that these types of associative networks could have arisen during the course of evolution. 1 Introduction The ability to reason abou [...] ...|$|R
40|$|Catalytic {{networks}} are abstracted from chemistry, and {{have recently been}} used to study cooperation in molecular evolution. Here catalytic {{networks are}} regarded as a connectionist model with <b>sigma-pi</b> <b>units</b> in a recurrent dynamics. The <b>sigma-pi</b> <b>units</b> are more sensitive to changes in a single input than the neuron models based on weighted sums. This paper partly presents previous work on the learning and generalisation in an association task. A particular type of architecture for catalytic networks is put forward here, based on a fixed-point analysis of several alternative components for possible architectures. The architecture of doubly-linked chains {{has been found to}} provide useful dynamics for association, while it allows for a range of different chain lengths. The chains can also branch, giving for example a tree structure. The nodes at the branching points perform logical AND or OR, depending on their connection parameters. The association works in both ways along each chai [...] ...|$|R
40|$|Computations by spiking neurons are {{performed}} using {{the timing of}} action potentials. We investigate the computational power of a simple model for such a spiking neuron in the Boolean domain by comparing it with traditional neuron models such as threshold gates (or McCulloch-Pitts neurons) and sigma-pi units (or polynomial threshold gates). In particular, we estimate the number of gates required to simulate a spiking neuron by a disjunction of threshold gates and we establish tight bounds for this threshold number. Furthermore, we analyze the degree of the polynomials that a <b>sigma-pi</b> <b>unit</b> must use for the simulation of a spiking neuron. We show that this degree cannot be bounded by any fixed value. Our results give evidence {{that the use of}} continuous time as a computational resource endows single-cell models with substantially larger computational capabilities. 1 Introduction Biological neurons communicate by sending spikes among themselves. A spike is a discrete event in continuous ti [...] ...|$|E
40|$|Due to {{copyright}} restrictions, {{the access}} to {{the full text of}} this article is only available via subscription. Boolean functions (BFs) can be represented by using polynomial functions when − 1 and + 1 are used represent True and False respectively. The coefficients of the representing polynomial can be obtained by exact interpolation given the truth table of the BF. A more parsimonious representation can be obtained with so called polynomial sign representation, where the exact interpolation is relaxed to allow the sign of the polynomial function to represent the BF value of True or False. This corresponds exactly to the higher order neuron or <b>sigma-pi</b> <b>unit</b> model of biological neurons. It is of interest to know what is the minimal set of monomials or input lines that is sufficient to represent a BF. In this study, we approach the problem by investigating the (small) subsets of monomials that cannot be absent as a whole from the representation of a given BF. With numerical investigations, we study low dimensional BFs and introduce a graph representation to visually describe the behavior of the two-element monomial subsets as to whether they cannot be absent from any sign representation. Finally, we prove that for any n-variable BF, any three-element monomial set cannot be absent as a whole if and only if all the pairs from that set has the same property. The results and direction taken in the study may lead to more efficient algorithms for finding higher order neuron representations with close-to-minimal input terms for Boolean functions...|$|E
40|$|A {{catalytic}} reaction {{network model}} {{is used for}} learning combinations of symbols. By variation of connection parameters, which are exponents here, the network architecture is modified such that it resembles {{the structure of the}} data. The advantage of learning the structure is the generalisation to unseen data, in an example of word recognition the unseen data is a word in a new font or case. The network consists of max-Π units whose output is integrated in a competitive dynamics. Such networks exhibit autocatalytic sets, which have been regarded as connectionist by Farmer [1]. Here learning is introduced into the catalytic network model and a general module is defined that can learn AND/OR combinations. Intermediate state values are employed when modules change their function. The resulting network structure and internal representations encourage applying this method to inference problems. 1 Introduction <b>Sigma-Pi</b> <b>units</b> [2] formally occur in population dynamic models of chemical react [...] ...|$|R
40|$|This {{document}} {{describes the}} data types and routines available in SNARL, a Simple Neural ARchitecture Library for C programmers. This document, along with SNARL source code and examples, is available by anonymous ftp from cs. utk. edu in the directory pub/levy/SNARL, {{and over the}} World Wide Web at [URL] I wrote SNARL {{because of the need}} on my part and others' for a small library of neural network routines providing <b>sigma-pi</b> <b>units</b> [2, 3] recurrence [1], and back-propagation in time [1]. SNARL is not a library for simple neural nets, but rather a simple library {{for a wide variety of}} neural nets: it allows you to ignore the computational support required to train test and such networks; instead, you can focus on the details of your particular application. I wrote SNARL from a dynamical-systems perspective [4], making it especially suitable for those wishing to explore the relationship between neural nets and dynamical systems. The best way to get started with SNARL would be to download all of the material in the ftp directories, and experiment with the sample programs in th...|$|R
40|$|Neural network {{research}} in machine learning {{grew out of}} theories from computational neuroscience from the 1960 s. While the class of affine-sigmoidal feature extractors has been studied extensively since the mid 1980 s, the computational neuroscience community has moved on to new models that are qualitatively different and more descriptive, without being substantially more computationally expensive. This paper brings a particular model proposed in (Rust et al., 2005) for low level neurons in the macaque visual system into a machine-learning context: we evaluate their model as an activation function (feature-extractor) for single-layer neural networks that perform image classification. The function we evaluate is somewhat similar to the higher-order processing units discussed in (Minsky & Papert, 1969) and the <b>Sigma-Pi</b> <b>units</b> described in (Rumelhart et al., 1986), but avoids the computational difficulties associated with these models by representing the second-order interaction weights with a low-rank positive semi-definite matrix, and avoids the learning difficulties associated with these models by using a gentler non-linearity than the logistic sigmoid. Remarkably good comparative results are obtained on three image classification tasks including 1. 4 % error on MNIST using a single-layer network. These results suggest that a single hidden layer neural network equipped with this neuron model can capture important patterns in the data that escape standard models such as sigmoid neural networks and support vector machines based on gaussian and polynomia...|$|R
40|$|In {{contrast}} to symbolic probkm-solving systems, neural network systems have many salient properties such as parallelism, lear. qability, and fault-tolerance [1, 2, 5]. But it seems difficult, if not impossible, {{to construct an}} expert system in a single gigantic neural network [3, 6]. In this paper we introduce a class of structured networks called YEN's (Voting and Election Network. !) in which several different types of neural learning networks are organized into a voting and election system. The distributed parallel cooperative inference mechanism of the generic VEN system is discussed, an election scheme based on a network of <b>sigma-pi</b> <b>units</b> is presented, and the experimental results and observations on a two-person board game are reported. The YEN arr:hitecture appears to {{provide a framework for}} constructing knowledge-based problem-solving systems using neural networks as building blocks, while still maintaining the strengths of the neural computing. 1 The Voting and Election Ivlodel The voting and election network (VEN) is a network whose components are neural learning networks that are organized into a voting and election system. The voting and election system consists of a distributor, one or more voters, and an elector (see Fig. l). The distributor distributes the input data among the voters. The voters cast their votes (hypotheses or constraints) for all candidates in parallel. It {{is the responsibility of the}} elector to determine the final solution that best satisfies the multiple hypotheses...|$|R
40|$|In a {{great variety}} of neuron models neural inputs are {{combined}} using the summing operation. We introduce the concept of multiplicative neural networks that contain units which multiply their inputs instead of summing them and, thus, allow inputs to interact nonlinearly. The class of multiplicative neural networks comprises such widely known and well studied network types as higher-order networks and product unit networks. We investigate the complexity of computing and learning for multiplicative neural networks. In particular, we derive upper and lower bounds on the Vapnik-Chervonenkis (VC) dimension and the pseudo dimension for various types of networks with multiplicative units. As the most general case, we consider feedforward networks consisting of product and sigmoidal units, showing that their pseudo dimension is bounded from above by a polynomial with the same order of magnitude as the currently best known bound for purely sigmoidal networks. Moreover, we show that this bound holds even in the case when the unit type, product or sigmoidal, may be learned. Crucial for these results are calculations of solution set components bounds for new network classes. As to lower bounds we construct product unit networks of fixed depth with superlinear VC dimension. For sigmoidal networks of higher order we establish polynomial bounds that, in contrast to previous results, do not involve any restriction of the network order. We further consider various classes of higher-order units, also known as <b>sigma-pi</b> <b>units,</b> that are characterized by connectivity constraints. In terms of these we derive some asymptotically tight bounds...|$|R

