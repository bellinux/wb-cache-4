22|29|Public
40|$|We {{compare the}} {{efficiency}} of two prominent techniques for simulation of complex systems: parallel tempering and Wang-Landau sampling. We show that both methods are of comparable efficiency but axe optimized for different platforms. Parallel tempering should be chosen on multi-processor system while Wang-Landau sampling is easier to implement on a <b>single-processor</b> <b>computer...</b>|$|E
40|$|We {{present an}} {{efficient}} strategy for mapping out the classical phase behavior of block copolymer systems using self-consistent field theory (SCFT). With our new algorithm, the complete solution of a classical block copolymer phase can be evaluated typically in {{a fraction of}} a second on a <b>single-processor</b> <b>computer,</b> even for highly segregated melts. This is accomplished by implementing the standard unit-cell approximation (UCA) for the cylindrical and spherical phases, and solving the resulting equations using a Bessel function expansion. Here the method is used to investigate blends of AB diblock copolymer and A homopolymer, concentrating on the situation where the two molecules are of similar size...|$|E
40|$|National audienceBecause of the developpement of {{materials}} science, {{there is a}} need to reduce the computational complexity of mechanical models. This paper aims to show that the Hyper Reduction method enables to reduce computational resources used for numerical simulations. Large mechanical models involving distributed nonlinearities require parallel computers to solve the governing equations related to these models. The proposed Hyper Reduction of such models provides reduced governing equations that enable simulations on a <b>single-processor</b> <b>computer.</b> This is achieved by using a reduced-basis and a selection of equilibrium equations of the detailed model. The use of a single processor during less time enables to save an amazing amount of the electrical energy during the numerical simulation...|$|E
40|$|The QU-GENE Computing Cluster (QCC) is a {{hardware}} and software solution to the automation and speedup of large QU-GENE (QUantitative GENEtics) simulation experiments {{that are designed to}} examine the properties of genetic models, particularly those that involve factorial combinations of treatment levels. QCC automates the management of the distribution of components of the simulation experiments among the networked <b>single-processor</b> <b>computers</b> to achieve the speedup...|$|R
40|$|Abstract: Real-time {{control of}} {{industrial}} processes {{has become an}} important issue in the recent years. Advances in hardware and software technologies allow the use of <b>single-processor</b> <b>computers</b> to perform real–time tasks. The paper first explains {{the basic elements of}} PC–based real–time control systems. Then, it describes the general operation of WinMechLab, a real–time single–processor platform for the simulation and control of mechanisms based on MS–Windows. Finally, we show the application of this software tool in a simple control task of a direct–drive didactic robot arm. 1...|$|R
40|$|We {{consider}} {{the problem of}} computing all Nash equilibria in bimatrix games (i. e., nonzero-sum two-player noncooperative games). Computing all Nash equilibria for large bimatrix games using <b>single-processor</b> <b>computers</b> is not feasible due to the exponential time required by the existing algorithms. We {{consider the}} use of parallel computing which allows us to solve larger games. We design and implement a parallel algorithm for computing all Nash Equilibria in bimatrix games. The algorithm computes all Nash equilibria by searching all possible supports of mixed strategies. We perform experiments on a cluster computing system to evaluate {{the performance of the}} parallel algorithm. 1...|$|R
40|$|The sudden {{shift from}} <b>single-processor</b> <b>computer</b> systems to many-processor {{parallel}} ones requires reinventing much of Computer Science (CS) : how to actually build and program the new parallel systems. CS urgently requires convergence to a robust parallel general-purpose platform that provides good performance and is {{easy enough to}} program by at least all CS majors. Unfortunately, lesser ease-ofprogramming objectives have eluded decades of parallel computing research. The idea of starting with an established easy parallel programming model and build an architecture for it has been treated as radical by vendors. This article advocates a more radical idea. Start with a minimalist stepping-stone: a simple abstraction that encapsulates the desired interface between programmers and system builders...|$|E
40|$|Abstract—This paper {{implements}} and analyzes {{a highway}} traffic-flow simulation based on continuum modeling of traffic dy-namics. A traffic-flow simulation {{was developed and}} mapped onto a parallel computer architecture. Two algorithms (the one-step and two-step algorithms) to solve the simulation equations were developed and implemented. Tests with real traffic data collected from the freeway network {{in the metropolitan area}} of Minneapolis, MN, were used to validate the accuracy and computation rate of the parallel simulation system (with 256 processors). The execution time for a 24 -h traffic-flow simulation over a 15. 5 -mi freeway, which takes 65. 7 min on a typical <b>single-processor</b> <b>computer,</b> took only 88. 51 s on the nCUBE 2 and only 2. 39 s on the CRAY T 3 E. The two-step algorithm, whose goal is to trade off extra computation for fewer interprocessor communications, was shown to save significantly on the communication time on both parallel computers. Index Terms—Communication latency hiding, real-time, simu-lation, traffic, two-step lax algorithm. I...|$|E
40|$|A {{method of}} {{wavefront}} sensing (more precisely {{characterized as a}} method of determining the deviation of a wavefront from a nominal figure) has been invented as an improved means of assessing the performance of an optical system as affected by such imperfections as misalignments, design errors, and fabrication errors. The method is implemented by software running on a <b>single-processor</b> <b>computer</b> that is connected, via a suitable interface, to the image sensor (typically, a charge-coupled device) in the system under test. The software collects a digitized single image from the image sensor. The image is displayed on a computer monitor. The software directly solves for the wavefront in a time interval of a fraction of a second. A picture of the wavefront is displayed. The solution process involves, among other things, fast Fourier transforms. It has been reported to the effect that some measure of the wavefront is decomposed into modes of the optical system under test, but it has not been reported whether this decomposition is postprocessing of the solution or part of the solution process...|$|E
40|$|We {{present a}} novel {{numerical}} method, based on high-frequency localization, {{for evaluation of}} electromagnetic-wave propagation through atmospheres exhibiting fully three-dimensional (height, range and cross-range) refractive index variations. This methodology, {{which is based on}} localization of Rytov-integration domains to small tubes around geometrical optics paths, can accurately solve three-dimensional propagation problems in orders-of-magnitude shorter computing times than other algorithms available presently. For example, the proposed approach can accurately produce solutions for propagation of ≈ 20 cm GPS signals across hundreds of kilometers of realistic, three-dimensional atmospheres in computing times on the order of 1 hour in a present-day single-processor workstation, a task for which other algorithms would require, in such <b>single-processor</b> <b>computers,</b> computing times on the order of several months...|$|R
40|$|Virtual {{observatories}} {{will give}} astronomers {{easy access to}} an unprecedented amount of data. Extracting scientific knowledge from these data will increasingly demand both efficient algorithms {{as well as the}} power of parallel computers. Nearly all efficient analyses of large astronomical datasets use trees as their fundamental data structure. Writing efficient tree-based techniques, a task that is time-consuming even on <b>single-processor</b> <b>computers,</b> is exceedingly cumbersome on massively parallel platforms (MPPs). Most applications that run on MPPs are simulation codes, since the expense of developing them is offset {{by the fact that they}} will be used for many years by many researchers. In contrast, data analysis codes change far more rapidly, are often unique to individual researchers, and therefore accommodate littl...|$|R
40|$|Today's PCs can {{directly}} manipulate numbers not {{longer than}} 64 bits because {{the size of}} the CPU registers and the data-path are limited. Consequently, arithmetic operations such as addition, can only be performed on numbers of that length. To solve the problem of computation on big-integer numbers, different algorithms were developed. However, these algorithms are considerably slow because they operate on individual bits; and are only designed to run over <b>single-processor</b> <b>computers.</b> In this paper, two algorithms for handling arithmetic addition on big-integer numbers are presented. The first algorithm is sequential while the second is parallel. Both algorithms, unlike existing ones, perform addition on blocks or tokens of 60 bits (18 digits), and thus boosting the execution time by a factor of 60...|$|R
40|$|As single-chip {{systems are}} {{predicted}} to soon contain over a billion transistors, design methodologies are evolving dramatically {{to account for}} the fast evolution of technologies and product properties. Novel methodologies feature the exploration of design alternatives early in development, the support for IPs, and early error detection – all with a decreasing time-to-market. In order to accommodate these product complexities and development needs, the modeling levels at which designers are working have quickly changed, as development at higher levels of abstraction allows for faster simulations of system models and earlier estimates of system performance while considering design trade-offs. Recent design advancements to exploit instruction-level parallelism on <b>single-processor</b> <b>computer</b> systems have become exceedingly complex, and modern applications are presenting an increasing potential to be partitioned and parallelized at the thread level. The new Single-Chip, Message-Passing (SCMP) parallel computer is a tightly coupled mesh of processing nodes that is designed to exploit thread-level parallelism as efficiently as possible. By minimizing the latency of communication among processors, memory access time, an...|$|E
40|$|Abstract—This article {{studies the}} {{usefulness}} of parallel process-ing in real-time traffic-flow simulation based on continuum mod-eling of traffic dynamics. Computational fluid dynamics (CFD’s) methods for solving simple macroscopic traffic-flow continuum models have been studied and efficiently implemented in traffic simulation codes (on serial computers) in the past. We designed a traffic-flow simulation code and mapped it onto a parallel computer architecture. This traffic simulation system is capable of simulating freeway traffic flow in real time. Tests with real traffic data collected from the freeway network {{in the metropolitan area}} of Minneapolis, MN, were used to validate the accuracy and computational rate of the parallel simulation system. The execution time for a 2 -h traffic-flow simulation of about 200 619 vehicles in an 18 -mi freeway, which takes 2. 35 min of computer time (on a <b>single-processor</b> <b>computer</b> simulator), took only 5. 25 s on the parallel traffic simulation system. This parallel system has a lot of potential for real-time traffic engineering applications. Index Terms — Freeway network, parallel, real time, traffic simulation...|$|E
40|$|Keywords: domain decomposition, Schur {{complement}} method, FETI-DP method, {{parallel computing}} Detailed and complex analyses of reactor vessels {{of nuclear power}} plants are very complicated and computationally demanding. This contribution deals with mechanical and thermal analyses of an existing reactor vessel made from prestressed concrete. The analyses has to described 33 years of construction and life of the vessel. The vessel has to be modelled as a threedimensional problem. With respect to symmetry, only one eighth of the vessel {{is used in the}} analysis. The specified analyses are very computationally demanding and they take several weeks on a modern <b>single-processor</b> <b>computer.</b> Therefore, the analyses are conducted in parallel and domain decomposition methods are applied. The Schur complement method is used in all analyses while the FETI-DP method is applied for some of them. Both methods are described in reference [1]. Very demanding analyses have been conducted on a cluster of PC’s. Application of the cluster speeds up the analyses significantly and solves some problems with sizes of output files...|$|E
50|$|Version 1.0 of the Linux kernel was {{released}} on 14 March 1994. This release of the Linux kernel only supported <b>single-processor</b> i386-based <b>computer</b> systems. Portability became a concern, and so version 1.2 (released 7 March 1995) gained support for computer systems using processors based on the Alpha, SPARC, and MIPS architectures.|$|R
40|$|This article {{presents}} {{the analysis and}} solution strategies employed in ParCYCLIC, a parallel nonlinear finite element program for the simulations of earthquake site response and liquefaction. In ParCYCLIC, finite elements are employed within an incremental plasticity coupled solid-fluid formulation. A constitutive model developed for the simulation of liquefaction-induced deformations is a main component of this analysis framework. Largescale experimental results for 3 -D geotechnical simulations are presented to demonstrate the performance of ParCYCLIC. It is shown that ParCYCLIC {{can be used to}} simulate large-scale problems, which would otherwise be infeasible using <b>single-processor</b> <b>computers</b> due to the limited memory sizes. Furthermore, the results show that the ParCYCLIC program is scalable to a large number of processors. Research continues to optimize the program to further reduce the total solution time and to apply the finite element program for large-scale simulation of ground-foundation interaction problem...|$|R
40|$|A {{parallel}} adaptive dynamic relaxation (ADR) algorithm {{has been}} developed for nonlinear structural analysis. This algorithm has minimal memory requirements, is easily parallelizable and scalable to many processors, and is generally very reliable and efficient for highly nonlinear problems. Performance evaluations on <b>single-processor</b> <b>computers</b> {{have shown that the}} ADR algorithm is reliable and highly vectorizable, and that it is competitive with direct solution methods for the highly nonlinear problems considered. The present algorithm is implemented on the 512 -processor Intel Touchstone DELTA system at Caltech, and it is designed to minimize the extent and frequency of interprocessor communication. The algorithm has been used to solve for the nonlinear static response of two and three dimensional hyperelastic systems involving contact. Impressive relative speedups have been achieved and demonstrate the high scalability of the ADR algorithm. For the class of problems addressed, the ADR algorithm represents a very promising approach for parallel-vector processing...|$|R
40|$|Atrial {{fibrillation}} is {{the most}} frequent arrhythmia, provoking discomfort, heart failure and arterial embolisms. The aim of this work {{is to develop a}} simplified anatomical computer model of human atria for the study of atrial arrhythmias and the understanding of electrical propagation mechanisms. With the model we propose, up to 40 s of real-time propagation have been simulated on a <b>single-processor</b> <b>computer.</b> The size and the electrophysiological properties of the simulated atria are within realistic values and information about anatomy has been taken into account in a three-dimensional structure. Besides normal sinus beat, pathological phenomena such as flutter and fibrillation have been induced using a programmed stimulation protocol. One important observation in our model is that atrial arrhythmias are a combination of functional and anatomical reentries and that the geometry plays an important role. This virtual atrium can reproduce electrophysiological observations made in humans but with the advantage of showing in great detail how arrhythmias are initiated and sustained. Such details are difficult or impossible to study in humans. This model will serve us as a tool to evaluate the impact of new therapeutic strategies and to improve them...|$|E
40|$|In this paper, {{we present}} the {{implementation}} and performance {{evaluation of a}} software-based H. 263 video encoder. The objective is to achieve real-time encoding rate on an ordinary <b>single-processor</b> <b>computer</b> such as a PC or a workstation. This requires optimizations at all design and implementation phases, including algorithmic enhancements, efficient implementations of all encoding modules, and taking advantage of certain architectural features of the machine. We enhance efficient algorithms for DCT and fast motion estimation such that their processing times are considerably reduced. We exploit various techniques to speedup the processing, including a number of compiler optimizations and removal of redundant operations. For exploiting the architectural features of the machine, we make use of low-level machine primitives such as Sun UltraSPARC’s Visual Instruction Set (VIS) and Intel’s Multimedia extension (MMX) which accelerate the computation in a single instruction stream multiple data stream (SIMD) fashion. Extensive benchmarking is carried out on a Sun UltraSPARC- 1 workstation and on two PCs (a 233 MHz Pentium II, and a 600 MHz Pentium III) to study {{the performance of the}} encoder. The effect of each type of optimization is determined for every coding mode of H. 263. Result...|$|E
40|$|Abstract. Traffic {{simulation}} is {{a valuable}} tool for the design and evaluation of road networks. Over the years, the level of detail to which urban and freeway traffic can be simulated has increased steadily, shifting from a merely qualitative macroscopic perspective to a very detailed microscopic view, where the behavior of individual vehicles is emulated realistically. With the improvement of behavioral models, however, the computational complexity has also steadily increased, {{as more and more}} aspects of real-life traffic have to be considered by the simulation environment. Despite the constant increase in computing power of modern personal computers, microscopic simulation stays computationally expensive, limiting the maximum network size than can be simulated on a <b>single-processor</b> <b>computer</b> in reasonable time. Parallelization can distribute the computing load from a single computer system to a cluster of several computing nodes. To this end, the exisiting simulation framework had to be adapted to allow for a distributed approach. As the simulation is ultimately targeted to be executed in real-time, incorporating real traffic data, only a spatial partition of the simulation was consid-ered, meaning the road network has to be partitioned into subnets of comparable complexity, to ensure a homogenous load balancing. The partition process must also ensure, that the divisio...|$|E
40|$|PROLOG, {{the most}} popular logic {{programming}} language, has been developed for <b>single-processor</b> <b>computers.</b> The implementations of sequential Prolog became efficient {{with the development of}} the Warren Abstract Machine (WAM) and are still improving. Today many parallel computers are commercially available and the question is how to utilize this opportunity to speed up the execution of Prolog programs. Our approach has been to study and develop efficient techniques for implementing OR-parallel systems for the full Prolog language on UMA and NUMA computers. Based on those techniques a high performance OR-parallel Prolog system has been designed and implemented on six different parallel computers. The system has a number of processes, called workers, consisting of two components: the engine, which does the actual Prolog work, and the scheduler. The schedulers, working together, divide the available work between the engines and support the sequential semantics of Prolog. We have extended th [...] ...|$|R
5000|$|The major {{downside}} of using softsynths {{can often be}} more latency (delay between playing the note and hearing the corresponding sound). Decreasing latency requires increasing the demand on the computer's processor. When the soft synthesizer is running as a plug-in for a host sequencer, both the soft synth and the sequencer are competing for processor time. Multi-processor computers can handle this better than <b>single-processor</b> <b>computers.</b> As the processor becomes overloaded, sonic artifacts such as [...] "clicks" [...] and [...] "pops" [...] can be heard during performance or playback. When the processor becomes completely overloaded, the host sequencer or computer can lock up or crash. Increasing buffer size helps, but also increases latency. However modern professional audio interfaces can frequently operate with extremely low latency, so in recent years this has become much {{less of a problem}} than {{in the early days of}} computer music.|$|R
50|$|PERPOS was {{developed}} for a line of Motorola 68000-based computers called the Power 5 series, which CCI developed. They were a line of multi-processor, fault-tolerant computers, code-named after the Great Lakes. The Power 5 line also included <b>single-processor</b> 68000-based <b>computers,</b> code-named after the Finger Lakes, running a regular Unix port called PERPOS-S, which was originally a Version 7-derived kernel with a System III-derived userland; the kernel was later modified to provide System III compatibility.|$|R
40|$|The sudden {{shift from}} <b>single-processor</b> <b>computer</b> systems to many-processor {{parallel}} computing systems requires reinventing much of Computer Science (CS) : how to actually build and program the new parallel systems. CS urgently requires convergence to a robust parallel general-purpose platform that provides good performance and {{is easy to}} program. Unfortunately, this same objective has eluded decades of parallel computing research. Now, continued delays and uncertainty could start affecting important sectors of the economy. This paper advocates a minimalist stepping-stone: settle first on a simple abstraction that encapsulates the new interface between programmers, on one hand, and system builders, on the other hand. This paper also makes several concrete suggestions: (i) the Immediate Concurrent Execution (ICE) abstraction {{as a candidate for}} the new abstraction, and (ii) the Explicit Multi-Threaded (XMT) general-purpose parallel platform, under development at the University of Maryland, as a possible embodiment of ICE. ICE and XMT build on a formidable body of knowledge, known as PRAM (for parallel random-access machine, or model) algorithmics, and a latent, though not widespread, familiarity with it. Ease-of-programming, strong speedups and other attractive properties of the approach suggest that we may be much better prepared for the challenges ahead than many realize. National Science Foundatio...|$|E
40|$|Particle Swarm Optimization (PSO) and Genetic Algorithms (GA) are usedto {{reduce the}} cost of a {{permanent}} magnet synchronous generator with concentratedwindings for tidal power applications. Reducing the cost of the electricalmachine is one way of making tidal energy more competitive compared to traditionalsources of electricity. Hybrid optimization combining PSO or GA with gradient based algorithmsseems to be suited for design of electrical machines. Results from optimizationwith Matlab indicate that hybrid GA performs better than Hybrid PSO forthis kind of optimization problems. Hybrid GA shows better convergence, lessvariance and shorter computation time than hybrid PSO. Hybrid GA managed to converge to an average cost of the generator that is 5. 2 % lower than what was reached by the hybrid PSO. Optimization results showa variance that is 98. 6 % lower for hybrid GA than it is for hybrid PSO. Movingfrom a pure GA optimization to the hybrid version reduced the average cost 31. 2 %. Parallel processing features are able to reduce the computation time of eachoptimization up to 97 % for large problems. The time it took to compute aGA problem with 2500 individuals was reduced from 12 hours to 21 minutesby switching from a <b>single-processor</b> <b>computer</b> to a computer with 48 processorcores. The run time for PSO with 400 particles and 100 iterations went from 18. 5 hours to 74 minutes, a 93 % reduction. </p...|$|E
40|$|A thread is an {{independent}} execution path, able to run simultaneously with other threads A C # program starts in a single thread created automatically by the CLR and operating system (the “main ” thread), and is made multithreaded by creating additional threads CLR assigns each thread its own memory stack so that local variables are kept separate Threads share data {{if they have a}} common reference to the same data C # programming lecture 8 : Threads 2 How threading works � Multithreading is managed internally by a thread scheduler, a function the CLR typically delegates to the operating system. � On a <b>single-processor</b> <b>computer,</b> a thread scheduler performs time-slicing – rapidly switching execution between each of the active threads � On a multi-processor computer, multithreading is implemented with a mixture of time-slicing and genuine concurrency – where different threads run code simultaneously on different CPUs. C # programming lecture 8 : Threads 3 Threads vs. Processes � All threads within a single application are logically contained within a process – the operating system unit in which an application runs � The key difference between threads and processes: � Processes are fully isolated from each other; � Threads share memory with other threads running in the same application C # programming lecture 8 : Threads 4 Why use concurrency? � Making use of multiprocessors � Driving slow devices, such as disks, networks, terminals and printers � Achieving timely response to the GUI’s users � Building a distributed syste...|$|E
40|$|Virtual {{observatories}} {{will give}} astronomers {{easy access to}} an unprecedented amount of data. Extracting scientic knowl-edge from these data will increasingly demand both efcient algorithms {{as well as the}} power of parallel computers. Such machines will range in size from small Beowulf clusters to large massively parallel platforms (MPPs) to collections of MPPs distributed across a Grid, such as the NSF TeraGrid facility. Nearly all efcient analyses of large astronomical datasets use trees as their fundamental data structure. Writing efcient tree-based techniques, a task that is time-consuming even on <b>single-processor</b> <b>computers,</b> is exceedingly cumbersome on parallel or grid-distributed resources. We have developed a library, Ntropy, that provides a exible, extensible, and easy-to-use way of developing tree-based data analysis algorithms for both serial and parallel platforms. Our experience has shown that not only does our library save development time, it also delivers an increase in serial performance. Furthermore, Ntropy makes it easy for an astronomer with little or no parallel programming experience to quickly scale their application to a distributed multiproces-sor environment. By minimizing development time for efcient and scalable data analysis, we enable wide-scale knowledge discovery on massive datasets...|$|R
40|$|The {{final version}} (3. 0) of the Montage {{software}} has been released. To recapitulate from previous NASA Tech Briefs articles about Montage: This software generates custom, science-grade mosaics of astronomical images on demand from input files that {{comply with the}} Flexible Image Transport System (FITS) standard and contain image data registered on projections that comply with the World Coordinate System (WCS) standards. This software can be executed on <b>single-processor</b> <b>computers,</b> multi-processor computers, and such networks of geographically dispersed computers as the National Science Foundation s TeraGrid or NASA s Information Power Grid. The primary advantage of running Montage in a grid environment is that computations can be done on a remote supercomputer for efficiency. Multiple computers at different sites {{can be used for}} different parts of a computation a significant advantage in cases of computations for large mosaics that demand more processor time than is available at any one site. Version 3. 0 incorporates several improvements over prior versions. The most significant improvement is that this version is accessible to scientists located anywhere, through operational Web services that provide access to data from several large astronomical surveys and construct mosaics on either local workstations or remote computational grids as needed...|$|R
40|$|Virtual {{observatories}} {{will give}} astronomers {{easy access to}} an unprecedented amount of data. Extracting scientific knowledge from these data will increasingly demand both efficient algorithms {{as well as the}} power of parallel computers. Nearly all efficient analyses of large astronomical datasets use trees as their fundamental data structure. Writing efficient tree-based techniques, a task that is time-consuming even on <b>single-processor</b> <b>computers,</b> is exceedingly cumbersome on massively parallel platforms (MPPs). Most applications that run on MPPs are simulation codes, since the expense of developing them is offset {{by the fact that they}} will be used for many years by many researchers. In contrast, data analysis codes change far more rapidly, are often unique to individual researchers, and therefore accommodate little reuse. Consequently, the economics of the current high-performance computing development paradigm for MPPs does not favor data analysis applica-tions. We have therefore built a library, called Ntropy, that provides a flexible, extensible, and easy-to-use way of developing tree-based data analysis algorithms for both serial and parallel platforms. Our experience has shown that not only does our library save development time, it also delivers an increase in serial performance. Furthermore, Ntropy makes it easy for an astronomer with little or no parallel programming experience to quickly scale their application to a distributed multiprocessor environment. By minimiz-ing development time for efcient and scalable data analysis, we enable wide-scale knowledge discovery on massive datasets...|$|R
40|$|Concurrent {{programming}} {{is a useful}} technique for structuring many important classes of applications such as interactive systems. This dissertation presents an approach to concurrent language design that provides {{a new form of}} linguistic support for constructing concurrent applications. This new approach treats synchronous operations as first-class values {{in a way that is}} analogous to the treatment of functions as first-class values in languages such as ML. The mechanism is set in the framework of the language Concurrent ML (CML), which is a concurrent extension of Standard ML. CML has a domain of first-class values, called events, that represent synchronous operations. Synchronous message passing operations are provided as the base-event values, and combinators are provided for constructing more complex events from other event values. This mechanism allows programmers to define new synchronization and communication abstractions that are first-class citizens, which gives programmers the flexibility to tailor their concurrency abstractions to their applications. The dissertation is organized into three technical parts. The first part describes the design and rationale of CML and shows how first-class synchronous operations can be used to implement many of the communication mechanisms found in other concurrent languages. The second part presents the formal operational semantics of first-class synchronous operations and proves that the polymorphic type system used by CML is sound. The third part addresses practical issues. It describes the use of CML in non-trivial applications, describes the implementation and performance of CML on a <b>single-processor</b> <b>computer,</b> and discusses issues related to the use and implementation of CML on a shared-memory multiprocessor...|$|E
40|$|Proceeding of: 2006 IEEE Nuclear Science Symposium Conference Record, San Diego, CA, Oct. 29 - Nov. 1, 2006 The goal of {{this work}} was the {{development}} of a lowcost micro-CT scanner, which could be used as an add-on in our previously developed PET systems for small-animals. The scanner design consists of a <b>single-processor</b> <b>computer</b> controlling a micro-focus X-ray tube and a flat panel detector, assembled in a common rotating gantry. The geometrical configuration was selected to achieve a spatial resolution of about 12 lp/mm with a field of view appropriate for small animals such as mice and rats. The radiated dose is controlled during the acquisition by two different elements: an aluminium filter and a tungsten shutter, attached to the X-ray source. The shutter is controlled by the computer in synchronism with the gantry rotation and the detector image integration. In order to achieve high performance with regards to per-animal screening time and cost, the acquisition protocol is able to take advantage from the highest frame rate of the detector also performing onthe-fly corrections for the detector raw data. These corrections include geometrical misalignments, sensor non-uniformities and defective elements, as well as conversion to attenuation images. An FDK reconstruction algorithm adapted to the specific conebeam geometry has been implemented. Symmetries are exploited to accelerate the algorithm and fast back-projection techniques have been developed for those protocols where high resolution is not a requirement. This work {{was supported in part by}} the Spanish Ministerio de Educación y Ciencia under Grant No. TEC 2004 - 07052 -C 02, la Comunidad de Madrid Grant No. GR/SAL/ 024104 CD Team, and the CENIT program of the Spanish Ministerio de Industria...|$|E
40|$|Abstract—In this paper, {{we present}} the {{optimization}} and performance {{evaluation of a}} software-based H. 263 video encoder. The objective is to maximize the encoding rate without losing the picture quality on an ordinary <b>single-processor</b> <b>computer</b> such as a PC or a workstation. This requires optimizations at all design and implementation phases, including algorithmic enhancements, efficient implementations of all encoding modules, and taking advantage of certain architectural features of the machine. We design efficient algorithms for DCT and fast motion estimation, and exploit various techniques {{to speed up the}} processing, including a number of compiler optimizations and removal of redundant operations. For exploiting the architectural features of the machine, we make use of low-level machine primitives such as Sun UltraSPARC’s visual instruction set and Intel’s multimedia extension, which accelerate the computation in a Single Instruction Stream Multiple Data Stream fashion. Extensive benchmarking is carried out on three platforms: a 167 -MHz Sun UltraSPARC- 1 workstation, a 233 -MHz Pentium II PC, and a 600 -MHz Pentium III PC. We examine the effect of each type of optimization for every coding mode of H. 263, highlighting the tradeoffs between quality and complexity. The results also allow us to make an interesting comparison between the workstation and the PCs. The encoder yields 45. 68 frames per second (frames/s) on the Pentium III PC, 18. 13 frames/s on the Pentium II PC, and 12. 17 frames/s on the workstation for QCIF resolution video with high perceptual quality at reasonable bit rates, which are sufficient for most of the general switched telephone networks based video telephony applications. The paper concludes by suggesting optimum coding options...|$|E
40|$|This paper {{presents}} a performance modeling methodology that is faster than traditional cycle-accurate simulation, {{more sophisticated than}} performance estimation based on system peak-performance metrics, and is shown to be effective on a class of High Performance Computing benchmarks. The method yields insight into the factors that affect performance on <b>single-processor</b> and parallel <b>computers...</b>|$|R
40|$|A {{parallel}} computer implementation (128 processors) of LEWICE, a NASA Lewis code {{used to predict}} the time-dependent ice accretion process for two-dimensional aerodynamic bodies of simple geometries, is described. Two-dimensional parallel droplet trajectory calculations are performed to demonstrate {{the potential benefits of}} applying parallel processing to ice accretion analysis. Parallel performance is evaluated {{as a function of the}} number of trajectories and the number of processors. For comparison, similar trajectory calculations are performed on <b>single-processor</b> Cray <b>computers,</b> and the best parallel results are found to be 33 and 23 times faster, respectively, than those of the Cray XMP and YMP...|$|R
40|$|The neural {{simulation}} tool NEST [1, www. nest-simulator. org] is a simulator for heterogeneous {{networks of}} point neurons or neurons {{with a small}} number of electrical compartments aiming at simulations of large neural systems. It is implemented in C++ and runs on a large range of architectures from <b>single-processor</b> desktop <b>computers</b> to large clusters and supercomputers with thousands of processor cores. With the example of the microcircuit model published by Potjans and Diesmann [2], we explain the basic modeling paradigm and features of the recently released version 2. 4 of NEST. The tutorial includes an introduction to the most important neuron and synapse models as well as the routines to set up and configure the network...|$|R
