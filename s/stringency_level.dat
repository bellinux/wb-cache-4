11|21|Public
40|$|Clean Development Mechanism (CDM) project {{developers}} have long {{complained about the}} complexities of project-specific baseline setting and the vagaries of additionality determination. In response to this, the CDM Executive Board took bold steps towards the standardization of CDM methodologies, culminating in the approval of guidelines {{for the establishment of}} performance standards in November 2011. The guidelines specify a performance standard <b>stringency</b> <b>level</b> for both baseline and additionality of 80...|$|E
40|$|Background: Many tools used {{to analyze}} microarrays in {{different}} conditions have been described. However, the integration of deregulated genes within coherent metabolic pathways is lacking. Currently no objective selection criterion based on biological functions exists to determine a threshold demonstrating that a gene is indeed differentially expressed. Methodology/Principal Findings: To improve transcriptomic analysis of microarrays, we propose a new statistical approach {{that takes into account}} biological parameters. We present an iterative method to optimise the selection of differentially expressed genes in two experimental conditions. The <b>stringency</b> <b>level</b> of gene selection was associated simultaneously with the p-value of expression variation and the occurrence rate parameter associated with the percentage of donors whose transcriptomic profile is similar. Our method intertwines <b>stringency</b> <b>level</b> settings, biological data and a knowledge database to highlight molecular interactions using networks and pathways. Analysis performed during iterations helped us to select the optimal threshold required for the most pertinent selection of differentially expressed genes. Conclusions/Significance: We have applied this approach to the well documented mechanism of human macrophage response to lipopolysaccharide stimulation. We thus verified that our method was able to determine with the highes...|$|E
30|$|Second, it is {{critical}} to establish and perfect the related policies and measures. This is the most important measure, chiefly because the China’s energy market has experienced a lack of more aggressive energy efficiency policies and more vigorous policy actions for decades. For example, the resource tax in China’s energy market has been comprehensively introduced. However, the <b>stringency</b> <b>level</b> of the resource tax has generally lagged behind that of developed countries. Furthermore, it is difficult to implement some energy market policies and measures in China.|$|E
40|$|The {{literature}} {{that deals with}} environmental policy and competitiveness has lar-gely approached these issues in a partial way. Authors who analyze the impact of environmental policy on competitiveness and location decisions of fi rms {{tend to focus on}} the impact of an exogenous change in environmental policy. Literature on the impact of the “openness of an economy ” on environmental policy stringency and policy competition typically assumes that 2 governments act strategically in their choice of environmental policy <b>stringency</b> <b>levels</b> in a world with 1 fi rm who decides where to locate. In this paper we review the literature on competitiveness and environmental policy. We also introduce the New Economic Geography lite-rature which enables to determine both the stringency of environmental policy as well as the location choice of fi rms...|$|R
40|$|Various {{types of}} policy {{instruments}} {{have been implemented}} to reduce local and global emissions, but the impact on innovation of different instruments has received less attention. This paper reviews empirical studies of the innovation impact of four main types of policy instruments in two high-emitting sectors. The conclusions are threefold. (1) Policy {{plays a key role}} for the development and diffusion of environmental innovation in the studied sectors. (2) Different types of instruments promote different types of innovations: general economic instruments has primarily encouraged incremental innovation, general regulatory instruments has enforced improvements based on modular innovation, and technology-specific instruments appears to have been needed to support the development and deployment of radically new technologies. (3) All types of policy instruments face challenges in design and implementation: understanding the selection impact of the chosen instruments, implementing increasing <b>stringency</b> <b>levels,</b> committing to an appropriate scale, and safeguarding policy stability...|$|R
40|$|Background: Genetic markers {{can be used}} to {{identify}} and verify the origin of individuals. Motivation for the inference of ancestry ranges from conservation genetics to forensic analysis. High density assays featuring Single Nucleotide Polymorphism (SNP) markers can be exploited to create a reduced panel containing the most informative markers for these purposes. The objectives of this study were to evaluate methods of marker selection and determine the minimum number of markers from the BovineSNP 50 BeadChip required to verify the origin of individuals in European cattle breeds. Delta, Wright’s FST, Weir & Cockerham’s FST and PCA methods for population differentiation were compared. The level of informativeness of each SNP was estimated from the breed specific allele frequencies. Individual assignment analysis was performed using the ranked informative markers. <b>Stringency</b> <b>levels</b> were applied by log-likelihood ratio to assess the confidence of the assignment test. Results: A 95 % assignment success rate for the 384 individually genotyped animals was achieved with < 80, < 100, < 140 and < 200 SNP markers (with increasing <b>stringency</b> threshold <b>levels)</b> across all the examined methods for marker selection. No further gain in power of assignment was achieved by sampling in excess of 200 SNP markers. The marker selection method that required the lowest number of SNP markers to verify the animal’s breed origin was Wright’s FST (60 to 140 SNPs depending on the chosen degree of confidence). Certain breeds required fewe...|$|R
40|$|International audienceBACKGROUND: Many tools used {{to analyze}} microarrays in {{different}} conditions have been described. However, the integration of deregulated genes within coherent metabolic pathways is lacking. Currently no objective selection criterion based on biological functions exists to determine a threshold demonstrating that a gene is indeed differentially expressed. METHODOLOGY/PRINCIPAL FINDINGS: To improve transcriptomic analysis of microarrays, we propose a new statistical approach {{that takes into account}} biological parameters. We present an iterative method to optimise the selection of differentially expressed genes in two experimental conditions. The <b>stringency</b> <b>level</b> of gene selection was associated simultaneously with the p-value of expression variation and the occurrence rate parameter associated with the percentage of donors whose transcriptomic profile is similar. Our method intertwines <b>stringency</b> <b>level</b> settings, biological data and a knowledge database to highlight molecular interactions using networks and pathways. Analysis performed during iterations helped us to select the optimal threshold required for the most pertinent selection of differentially expressed genes. CONCLUSIONS/SIGNIFICANCE: We have applied this approach to the well documented mechanism of human macrophage response to lipopolysaccharide stimulation. We thus verified that our method was able to determine with the highest degree of accuracy the best threshold for selecting genes that are truly differentially expressed...|$|E
40|$|Purpose – To {{construct}} an index (index of environmental sensitivity performance) {{to be used}} in a cross-country trade model in order to analyze the effect of various degrees of environmental stringency on the trade patterns, and especially on the export performance of the countries. Design/methodology/approach – The gravity model of trade is used in order to find the effects of environmental stringency on the variation in trade flows. Findings – The study shows that environmental stringency has an important impact on the export of the countries. The impact of the degree of environmental stringency on the exports is significantly negative suggesting an inverse relationship between export values and relative environmental sensitivity performance of the nations. Originality/value – This study supports the argument that the environmental <b>stringency</b> <b>level</b> differential between developing and developed nations is a crucial criteria in terms of explaining shifts in the trade patterns and international specialization of the countries. Environmental regulations, Export markets, International trade, Pollution...|$|E
40|$|Abstract—Accurate {{detection}} of orthologous proteins {{is a key}} aspect of comparative genomics. Orthologs in different species {{can be used to}} predict the function of uncontrived genes from model organisms as they retain the same biological function through the path of evolution. Orthologs can be inferred using phylogenetic, pair-wise similarity or synteny based methods. The study here describes a computational method for detecting orthologs of a protein. A phylogenetic tree based approach is used for identification of orthologous proteins. A Combination of species overlap algorithm and patristic distances is used for detecting orthologs of a protein from a set of FASTA sequences. Patristic distances have been used to drill the orthology predictions of any protein down to its closest orthologs. The approach gives a considerably good accuracy and has high specificity and precision. The use of Distance threshold allows controlling the <b>stringency</b> <b>level</b> of predictions so that the closeness and proximity between the protein of interest and its orthologs can be adjusted. Index Terms—Orthologs, comparative genomics...|$|E
40|$|AbstractA {{prediction}} {{of the mechanical}} behaviour based on the analysis of standardized requirements is essential to optimize the technical characteristics of materials used in cladding automatic-processes for nuclear reactor pressure-vessels, being crucial to prevent failures. The aim of this work {{is to develop a}} quantitative analysis of requirements of the AISI 304 and AISI 347 materials, and their German equivalents, DIN X 5 CrNi 18 - 10 and DIN X 6 CrNiNb 18 - 10, respectively, in order to predict the mechanical behavior. To this purpose, we have carried out prediction studies based on the estimation of ferrite-δ content and its relationship with hot-cracking described in the literature. Additionally, chemical and mechanical requirements have been evaluated by the application of a deterministic algorithm based on <b>Stringency</b> <b>Levels.</b> Results show that steel DIN X 6 CrNiNb 18 - 10 can be defined as the best option, because of its lowest hot cracking susceptibility and the high stringency of its requirements...|$|R
40|$|Environmental {{concerns}} and increasing <b>stringency</b> <b>levels</b> of environmental regulations, especially in developed countries, {{has given rise}} to the debate concerning pollution havens. The pollution haven hypothesis argues that differences in environmental policy stringency will give economic incentives to firms and industries to move their production to countries with more lenient environmental regulations. Even though there exists a vast strand of literature investigating this phenomena, their results are inconclusive. This thesis empirically studies weather difference in environmental policy stringency has a positive effect on foreign direct investment flows between the European OECD and BRIICS countries over the time-period 2003 to 2012. Using a fixed effect model, FDI is determined by the commonly used knowledge capital specification introduced by Markusen and Maskus (2002) in addition to the main independent variable proxied by a new indicator for environmental policy stringency. The results of this study shows a significant but weak support for the pollution haven hypothesis when considering a time-lag of the EPS-variable by one year...|$|R
40|$|DNA rearrangements {{caused by}} chromosometranslocations between band 11 q 23 and various {{chromosomes}} {{can be detected}} by a single probe, B 859, an 859 -base pair complementary DNA fragment derived from the human ALL-i gene. To try to understand why band 11 q 23 becomes a frequent target ofthe translocatlons,we have sequencedthe entire break point cluster region, a 8342 -base pair BamHI genomic fragment delin eated by 11859. We found eightAlu repeats located within this region in the same orientation as theALL- 1 gene. We have also analyzed the sequences of the breakpoints in 10 patIents with 6 different types of 11 q 23 aberra tion. In five patients the breaks coincided with Alassequences on chromo some 11, {{but not on the}} partner chromosomes. Also,sevenof the breaks occurred in the region delineated by exons 6 and 7, which is composed mainly ofAlu sequences. Ia three patients topoisomerase H recognition site-like sequences, at different <b>stringency</b> <b>levels,</b> were identified at th...|$|R
40|$|Jonathan Quong proposes the {{following}} “Stringency Principle” for proportionality in self-defense: “If a wrongful attacker threatens to violate a right with <b>stringency</b> <b>level</b> X, then {{the level of}} defensive force it is proportionate to impose on the attacker is equivalent to X. ” I adduce a counter-example that shows that this principle is wrong. Furthermore, Quong assumes that what determines the stringency of a person’s right is exclusively the amount of force that {{one would have to}} avert from someone else {{in order to have a}} necessity justification for one’s transgressing the right in order to avert said force. Yet, Quong provides no argument as to why, first, the stringency of a right should be measured exclusively with reference to permissible rights-infringement; and second, he provides no explanation as to why the permissibility of the rights-infringement should be established with reference to “someone else,” namely with reference to an “innocent person,” instead of with reference to the person against whom the right in question is actually being held: the aggressor. I argue that the latter option is certainly the more plausible one, but so amended the stringency principle will be unable to adjudicate any substantive questions about proportionality in self-defense. In particular, Quong’s account cannot “explain” – contrary to what Quong claims – the allegedly intuitive judgment that one must not kill in defense of property or in order to avoid minor injuries...|$|E
40|$|Hierarchical clustering. Taxonomic {{assignment}} of reads was performed using a preexisting database of SSU rDNA sequences from including XXX reference sequences generated by Sanger sequencing. Experimental amplicons (reads), sorted by abundance, were then concatenated with the reference extracted sequences sorted by decreasing length. All sequences, experimental and referential, were then clustered to 85 % identity using the global alignment clustering option of the uclust module from the usearch v 4. 0 software (Edgar, 2010). Each 85 % cluster was then reclustered {{at a higher}} <b>stringency</b> <b>level</b> (86 %) and so on (87 %, 88 %,.) in a hierarchical manner up to 100 % similarity. Each experimental sequence was then identified by the list of clusters to which it belonged at 85 % to 100 % levels. This information {{can be viewed as}} a matrix with the lines corresponding to different sequences and the columns corresponding to the cluster membership at each clustering level. Taxonomic assignment for a given read was performed by first looking if reference sequences clustered with the experimental sequence at the 100 % clustering level. If this was the case, the last common taxonomic name of the reference sequence(s) within the cluster was used to assign the environmental read. If not, the same procedure was applied to clusters from 99 % to 85 % similarity if necessary, until a cluster was found containing both the experimental read and reference sequence(s), in which case sequences were taxonomically assigned as described above...|$|E
40|$|Reversible protein {{phosphorylation}} {{is one of}} {{the most}} pervasive post-translational modifications, regulating diverse cellular processes in various organisms. High throughput experimental studies using mass spectrometry have identified many phosphorylation sites, primarily from eukaryotes. However, the vast majority of phosphorylation sites remain undiscovered, even in well studied systems. Because mass spectrometry-based experimental approaches for identifying phosphorylation events are costly, time-consuming, and biased toward abundant proteins and proteotypic peptides, in silico prediction of phosphorylation sites is potentially a useful alternative strategy for whole proteome annotation. Because of various limitations, current phosphorylation site prediction tools were not well designed for comprehensive assessment of proteomes. Here, we present a novel software tool, Musite, specifically designed for large scale predictions of both general and kinase-specific phosphorylation sites. We collected phosphoproteomics data in multiple organisms from several reliable sources and used them to train prediction models by a comprehensive machine-learning approach that integrates local sequence similarities to known phosphorylation sites, protein disorder scores, and amino acid frequencies. Application of Musite on several proteomes yielded tens of thousands of phosphorylation site predictions at a high <b>stringency</b> <b>level.</b> Cross-validation tests show that Musite achieves some improvement over existing tools in predicting general phosphorylation sites, and it is at least comparable with those for predicting kinase-specific phosphorylation sites. In Musite V 1. 0, we have trained general prediction models for six organisms and kinase-specific prediction models for 13 kinases or kinase families. Although the current pretrained models were not correlated with any particular cellular conditions, Musite provides a unique functionality for training customized prediction models (including condition-specific models) from users' own data. In addition, with its easily extensible open source application programming interface, Musite is aimed at being an open platform for community-based development of machine learning-based phosphorylation site prediction applications. Musite is available at [URL]...|$|E
40|$|The {{conventional}} wisdom {{on the evolution}} of copyright and what has shaped it has come under increasing strain in recent years. As technical innovation pushes for reforms, the results are increasingly subject to political debate and tension. Examining how copyright has evolved and what has driven the process is of key importance because of the economic importance of copyright to individual countries. In the light of this and to contribute to possible solutions, it is necessary to examine what or who has driven the process. To do this, the evolution of copyright polices has to be mapped in a comparative way. This thesis examines the evolution of copyright in Germany, the US, the UK and at an international level between 1880 and 2010. The analysis itself is split between the culture and stringency of policies. Culture refers to the overall approach to copyright while stringency covers the scope of protection. This approach is original because it allows for a comparison of copyright systems as neutrally as possible. The results are clearly quantifiable and more importantly the extent of evolutions is directly comparable. Furthermore, the nature of the data ensures that causal forces behind the pattern can be examined. This methodology will be applied to a number of propositions commonly found in the copyright literature. The focal point here will be on arguments of rising <b>stringency</b> <b>levels</b> over time and the cultural convergence between case studies. For these, the commonly argued causal forces, in particular technological innovation and the influence exercised by individual actors will be examined. The results show that neither the cultural or stringency evolutionary pattern nor the causal factors fully matches previous studies. First, the evolution of <b>stringency</b> <b>levels</b> has been more complex than previously argued. In addition, although there has been some degree of cultural convergence, this has not been caused by technology and even the influence of particular actors has been limited. In both cases, {{it is clear that the}} role of copyright exemptions has been under-theorised. (Data relating to the appendices were submitted as separate files which could not be uploaded to the repository. Please contact the author for more information...|$|R
40|$|In most mammals {{dispersal}} is male-biased {{and in many}} polygynous ungulates female philopatry and matrilineal grouping involve small-scale genetic structure. We have through sex-related {{differences in}} microsatellite allele distribution addressed sex-biased dispersal in a spatially expanding northern ungulate population. The Norwegian red deer population (Cervus elaphus atlanticus) has {{the last hundred years}} grown substantially and expanded spatially after a major decline from 300 to 100 years ago. Previous Bayesian analyses suggest a present division of genetic variation into five geographically separated subpopulations. Among these subpopulations the overall F st values were 0. 067 (SE= 0. 014) for males and 0. 094 (SE= 0. 017) for females. Pairwise F st values were significantly higher for females than males, demonstrating a stronger genetic structure among females, and that dispersal has been lower in females than males. Accordingly, a higher number of male than female first generation dispersers were identified among the five subpopulations using Bayesian assignment with prior population information, but significantly so only with relaxed <b>stringency</b> <b>levels</b> of assignment. The identified male-biased dispersal distances varied from 30 to 300 kilometers suggesting male biased dispersal on a large scale in red deer...|$|R
40|$|This paper {{demonstrates}} a new {{process that has}} been specifically designed {{for the support of}} the U. S. Department of Transportation’s (DOT’s) Corporate Average Fuel Economy (CAFE) standards. In developing the standards, DOT’s National Highway Traffic Safety Administration made use of the CAFE Compliance and Effects Modeling System (the "Volpe model " or the “CAFE model”), which was developed by DOT’s Volpe National Transportation Systems Center for the 2005 – 2007 CAFE rulemaking and has been continuously updated since. The model is the primary tool used by the agency to evaluate potential CAFE <b>stringency</b> <b>levels</b> by applying technologies incrementally to each manufacturer’s fleet until the requirements under consideration are met. The Volpe model relies on numerous technology-related and economic inputs, such as market forecasts, technology costs, and effectiveness estimates; these inputs are categorized by vehicle classification, technology synergies, phase-in rates, cost learning curve adjustments, and technology “decision trees. ” Part of the model’s function is to estimate CAFE improvements that a given manufacturer could achieve by applying additional technology to specific vehicles in its product line. A significant number of inputs to the Volpe decision-tree model are related to the effectiveness (fuel consumption reduction) of each fuel-saving technology...|$|R
40|$|Banks {{are often}} {{excluded}} in corporate finance research {{mainly because of}} the regulatory concerns. Compares to non-bank firms, banks are heavily regulated due to its special economic role of money and the uncertainty. Heavy regulation on banks could reduce the information asymmetry between the managers and investor by limiting the behaviour of banks {{at the time of the}} Seasoned Equity Offering (SEO), and by increasing the incentive for banks to avoid excessive risk-taking. Therefore, the market may be less likely to assume that bank issued securities signal information that the bank is overvalued compared to their non-bank counterparts. The objective of this thesis is therefore to examine commercial banks issued securities announcement effect. Three interrelated research questions are addressed in this thesis: 1) What is the difference in convertible bond announcement effect between banks and non-banks firm? 2) What is the difference in SEO announcement effect between banks and non-banks? 3) How do the stringency levels of bank regulation impact on the announcement effects of bank issued SEO? By using the U. S. convertible bond and SEO data from 1982 to 2012, I find that the bank issued a convertible bond and SEO announcement experience higher cumulative abnormal return than non-bank. This is consistent with the view that bank regulation reveals positive information about banks. Since banks are heavily regulated, the market is less likely to assume that the issuance of the convertible bond and SEO by banks signals information that is overvalued. These results are robust after controlling for a number of firm-, issue-, and market-specific characteristics. These results are robust by considering the different categories of non-bank industries by undertaking tests in relation to the differences in the CARS upon convertible bond/ SEO across industries, as well as the unbalanced sample between banks and non-banks by using the matched sample analysis. However, the relation between the <b>stringency</b> <b>level</b> of bank regulation and bank issued securities announcement effect may be nonlinear. As hypothesised, I find that bank regulation has an inverted U-shaped relation with the announcement effect of bank SEO by using the SEO data across 21 countries from 2001 to 2012. Under a less bank regulation environment, the market reacts more positively to the bank SEO announcement for an increase in the level of bank regulation. However, the bank SEO announcement effects become more negative if the bank regulation becomes too stringent. This inverted U-shaped relationship is robust after I use the exogenous cross-country, cross-year variation in the timing of the Basel II adoption as the instrument to assess the causal impact of bank regulation on SEO announcement effects. However, the stringency of regulation does not have a significant impact on the announcement effects of involuntary bank equity issuance...|$|E
40|$|Climate {{change has}} a severe {{impact on the}} Arctic region, causing sea ice, snow {{coverage}} and permafrost to decrease rapidly. These changes to the Arctic environment gradually increase access to the hydrocarbon resources below the Arctic Ocean seabed. However, there is hardly any regulation in the Arctic region to adequately protect its environment from increasing hydrocarbon activities. The aim of this thesis is therefore to make recommendations for an Arctic Regime for Offshore Hydrocarbon Activities (AROHA) which can effectively protect the Arctic environment from {{the negative effects of}} offshore hydrocarbon activities. The set up of this research entails examining policy systems and practices in order to learn and draw inspiration from them. Assessment criteria are established to assess the effectiveness of the current regime applicable to the Arctic and of several ‘comparable policy models’. These criteria are: <b>stringency,</b> <b>level</b> of collaboration, implementation and compliance. As a point of departure, a gap analysis based on the four criteria is performed for the Arctic Council Offshore Oil and Gas Guidelines, {{an important part of the}} current regime. This leads to a list of ‘options for improvement’. Next, four comparable policy models are assessed using the same criteria: the OSPAR Convention, CRAMRA, the Helsinki Convention, and two regional agreements between Arctic states. Based on the assessments of these policy models a list of ‘lessons learnt’ is established which provides for the elements and aspects that could make an AROHA strong or weak, effective or ineffective, successful or unsuccessful. Based on these lessons a proposal is developed for an AROHA. It has as objective to prevent and eliminate pollution from offshore hydrocarbon sources; to protect the Arctic maritime area; and to ensure that both present and future generations are entitled to the social and economic benefits arising from offshore activities. The proposed contracting parties are the Arctic coastal states; Canada, Denmark (Greenland), Norway, Russia, and the US. The agreement will roughly apply to the sea and seabed within national jurisdiction of the contracting parties, excluding a part of the OSPAR area. It is further proposed that a commission is established with the mandate to focus on regulating offshore hydrocarbon activities and protecting the marine environment. It can take legally binding decisions and recommendations and has far-reaching competences. A next step in this research is to investigate the feasibility of regime formation in the Arctic. For this, a list of fifteen factors which influence regime formation has been distilled from regime theories. An assessment of these factors reveals that the formation of an environmental protection regime for offshore hydrocarbon activities in the Arctic is an unlikely thing to happen at this moment. Nevertheless, several recommendations can be made to increase chances for the formation of an AROHA. Making the compliance mechanisms and institutional structure less intrusive on state sovereignty makes a regime more acceptable to the Arctic states. Feelings of sovereignty should be mitigated by setting clear delimitations for the continental shelf and by solving territorial disputes. Furthermore, the cooperation between the Arctic states should be maintained as should the Arctic Council. Research efforts should continue and the international community should keep investing in epistemic communities, which form an important link between scientists and policy makers. NGOs should keep pushing for regime formation in order to increase feelings of priority and political will among the Arctic states. Last, NGOs and the international community should emphasise that a large part of the Arctic Ocean is beyond state sovereignty. It is the common heritage of mankind and the Arctic states have obligations under the Law of the Sea to preserve the Arctic marine environment...|$|E
40|$|The {{biological}} sciences {{are becoming increasingly}} dependent on information science as they accrue large amounts of data that must be organized and analyzed {{in order to gain}} some new information or obtain results from the data. One such task is the physical mapping of genomes, which in this case, is creating a map that specifies the order of clones along a particular genome. Many genomes, especially grasses such as barley or wheat, contain retroposons and repetitive elements that make the task of building a physical map of the genome more difficult. The software project described in this thesis takes this into account and will allow these known problem segments of DNA to be filtered out before further calculations are done on the data. The algorithm’s flexibility allows genomes with different degrees of variation to be pieced together with varying <b>stringency</b> <b>levels</b> chosen by the user, allowing more accurate maps to be created, based on the available data. The software will order the clones of large genomes quickly and accurately, displaying a graphical representation of the clones depicting the overlap and order as determined by the algorithm...|$|R
40|$|Alternative {{splicing}} {{is involved}} in numerous cellular functions and is often disrupted and involved in disease. Previous research has identified methods to distinguish alternative conserved exons (ACEs) in human and mouse. However, the cellular machinery, the spliceosome, does not use comparative genomics to decide when to include and when to exclude an exon. Human RefSeq exons obtained from the University of California Santa Cruz (UCSC) genome browser were analyzed for tissue-specific skipping. Expressed sequence tags (ESTs) were aligned to exons and their tissue of origin and histology were identified. ACEs were also identified as {{a subset of the}} skipped exons. About 18 % of the exons were identified as tissue-specifically skipped in one of sixteen different tissues at four <b>stringency</b> <b>levels.</b> The different datasets were analyzed for both general features such as exon and intron length, splice site strength, base composition, conservation, modularity, and susceptibility to nonsense-mediated mRNA decay caused by skipping. Cis-element motifs that might bind protein factors that affect splicing were identified using overrepresentation analysis and conserved occurrence rate between human and mouse. (cont.) Tissue-specific skipped exons were then classified with both a decision-tree based classifier (Random ForestsTM) and a support vector machine. Classification results were better for tissue-specific skipped exons vs. constitutive exons than for tissue-specific skipped exons vs. exons skipped in other tissues. by Craig Jeremy Rothman. Thesis (M. Eng.) [...] Massachusetts Institute of Technology, Biological Engineering Division, 2007. Includes bibliographical references (p. 53 - 57) ...|$|R
40|$|Abstract Background Genetic markers {{can be used}} to {{identify}} and verify the origin of individuals. Motivation for the inference of ancestry ranges from conservation genetics to forensic analysis. High density assays featuring Single Nucleotide Polymorphism (SNP) markers can be exploited to create a reduced panel containing the most informative markers for these purposes. The objectives of this study were to evaluate methods of marker selection and determine the minimum number of markers from the BovineSNP 50 BeadChip required to verify the origin of individuals in European cattle breeds. Delta, Wright's F ST, Weir & Cockerham's F ST and PCA methods for population differentiation were compared. The level of informativeness of each SNP was estimated from the breed specific allele frequencies. Individual assignment analysis was performed using the ranked informative markers. <b>Stringency</b> <b>levels</b> were applied by log-likelihood ratio to assess the confidence of the assignment test. Results A 95 % assignment success rate for the 384 individually genotyped animals was achieved with ST (60 to 140 SNPs depending on the chosen degree of confidence). Certain breeds required fewer markers (95 % assignment success. The power of assignment success, and therefore the number of SNP markers required, is dependent on the levels of genetic heterogeneity and pool of samples considered. Conclusions While all SNP selection methods produced marker panels capable of breed identification, the power of assignment varied markedly among analysis methods. Thus, with effective exploration of available high density genetic markers, a diagnostic panel of highly informative markers can be produced. </p...|$|R
40|$|Epigenetic modifications, {{including}} DNA methylation and covalent {{modifications to}} histone tails, are major {{contributors to the}} regulation of gene expression. These changes are reversible, yet can be stably inherited, and may last for multiple generations without change to the underlying DNA sequence. Genomic imprinting results in expression {{from one of the}} two parental alleles and is one example of epigenetic control of gene expression. So far, 60 to 100 imprinted genes have been identified in the human and mouse genomes, respectively. Identification of additional imprinted genes has become increasingly important with the realization that imprinting defects are associated with complex disorders ranging from obesity to diabetes and behavioral disorders. Despite the importance imprinted genes play in human health, few studies have undertaken genome-wide searches for new imprinted genes. These have used empirical approaches, with some success. However, computational prediction of novel imprinted genes has recently come to the forefront. I have developed generalized linear models using data on a variety of sequence and epigenetic features within a training set of known imprinted genes. The resulting models were used to predict novel imprinted genes in the mouse genome. After imposing a stringency threshold, I compiled an initial candidate list of 155 genes. A subset of these genes was tested for evidence of imprinting using allele-specific restriction digests in either brain or placenta. Of the 10 genes tested in placenta, 2 showed evidence of maternal allele-specific expression. I also designed a custom microarray to test a total of 563 genes predicted as imprinted at lower <b>stringency</b> <b>levels.</b> Of these 563 genes, I experimentally tested 32 in placenta and 8 in brain, resulting in the identification of an additional 5 novel imprinted genes in placenta. This study is the first to demonstrate the utility of epigenetic marks in the prediction of imprinted genes. Furthermore, specific combinations of epigenetic marks were commonly found within particular regions relative to the transcriptional start sites of imprinted genes, implicating their placement and localization in the imprinting mechanism...|$|R
40|$|An AUG in {{an optimal}} {{nucleotide}} context {{is the preferred}} translation initiation site in eukaryotic cells. Interactions among translation initiation factors, including eIF 1 and eIF 5, govern start codon selection. Experiments described here showed that high intracellular eIF 5 <b>levels</b> reduced the <b>stringency</b> of start codon selection in human cells. In contrast, high intracellular eIF 1 <b>levels</b> increased <b>stringency.</b> High <b>levels</b> of eIF 5 induced translation of inhibitory upstream open reading frames (uORFs) in eIF 5 mRNA that initiate with AUG codons in conserved poor contexts. This resulted in reduced translation from the downstream eIF 5 start codon, indicating that eIF 5 autoregulates its own synthesis. As with eIF 1, which is also autoregulated through translation initiation, features contributing to eIF 5 autoregulation show deep evolutionary conservation. The results obtained {{provide the basis for}} a model in which auto- and cross-regulation of eIF 5 and eIF 1 translation establish a regulatory feedback loop that would stabilize the stringency of start codon selection...|$|R
40|$|The Drosophila {{vesicular}} monoamine transporter (DVMAT) {{regulates the}} loading and storage of monoaminergic transmitters in secretory vesicles, and proper localization to vesicles {{is required for}} the exocytotic release of neurotransmitters such as dopamine and serotonin. By genetically modifying the availability of vesicular monoamine transporters in specific monoamine neurons, I examine amine requirements for aspects of behavior and interactions between aminergic systems. I demonstrate that some behaviors rely predominantly on octopaminergic circuits with little apparent input from either serotonin or dopamine. In contrast, other behaviors can be rescued by expressing DVMAT in octopaminergic or dopaminergic neurons, suggesting potentially redundant circuits. Rescue of major aspects of adult locomotion and startle behavior required octopamine, but complementary roles were observed for serotoninand dopamine. Interestingly, adult circadian behavior could not be rescued by expression of DVMAT in a single subtype of aminergic neurons, but required at least two systems, suggesting the possibility of unexpected cooperative interactions. Studies using the temperature-sensitive GAL 80 inducible transgene system demonstrate that the temporal demand for DVMAT yields a large degree of flexibility with causes still left unclear. In order to understand the cellular mechanisms which regulate monoamine release, my work also examines motifs involved in trafficking vesicular monomamine transporters to the axon terminal and appropriately on synaptic vesicles during rounds of exo- and endocytosis. The C-terminus of DVMAT encodes a tyrosine-based motif (YXXï¿½) flanked between a di-leucine motif and downstream acidic residues that {{have been shown to be}} involved with endocytosis, sorting to synaptic vesicles and maturation of large dense core vesicles in vitro. I employ fast-capture, in vivo real-time imaging of pH-sensitive pHlourins to track wild-type and mutant DVMAT in intact circuits. My findings suggest that the C-terminus contains trafficking motifs that slow endocytosis kinetics and dampen the presynaptic neuron's ability to recruit vesicles to the plasma membrane. Moreover, the previously described Delta 3 deletion disrupts VMAT trafficking to the axon terminal. Combined with results showing that behaviors are differentially sensitive to mutations, these data are among the first to demonstrate that mislocalization of a synaptic protein may preferentially affect specific neuronal circuits and argue that behaviors require varying <b>stringency</b> <b>levels</b> for regulated neurotransmitter release...|$|R
40|$|The {{main purpose}} of this {{dissertation}} {{is to develop a}} process to improve actual policy-making procedures in terms of aviation environmental effects. This research work expands current practices with physics based publicly available models. The process herein proposed provides information regarding the interdependencies between the environmental effects of aircraft. These interdependencies are also tied to the actual physical parameters of the aircraft and the engine, making it more intuitive for decision-makers to understand the impacts to the vehicle due to different policy scenarios. These scenarios involve the use of fleet analysis tools in which the existing aircraft are used to predict the environmental effects of imposing new <b>stringency</b> <b>levels.</b> The aircraft used are reduced to a series of coefficients that represent their performance, in terms of flight characteristics, fuel burn, noise, and emissions. These coefficients are then utilized to model flight operations and calculate what the environmental impacts of those aircraft are. If a particular aircraft does not meet the stringency to be analyzed, a technology response is applied to it, in order to meet that stringency. Depending on the level of reduction needed, this technology response can {{have an effect on the}} fuel burn characteristic of the aircraft. The proposed alternative is to create a fleet of replacement aircraft to the current fleet that does not meet stringency. These replacement aircraft represent the achievable physical limits for state of the art systems. In addition, the replacement aircraft show the linkage between environmental effects and fundamental aircraft and engine characteristics, something that has been neglected in previous policy making procedures. Another aspect that has been ignored is the creation of the coefficients used for the fleet analyses. In current literature, a defined process for the creation of those coefficients does not exist, but this research work develops a process to do so and demonstrates that the characteristics of the aircraft can be propagated to the coefficients and to the fleet analysis tools. Ph. D. Committee Chair: Mavris, Dimitri N.; Committee Member: Kirby, Michelle R.; Committee Member: Schrage, Daniel P.; Committee Member: Senzig, David; Committee Member: Tai, Jimmy C. M...|$|R
40|$|The {{quality of}} care at nursing homes in the United States has been under intense {{scrutiny}} for several decades. Major policy reforms administered through the Centers for Medicare and Medicaid Services (CMS) and state agencies to strengthen regulatory enforcement and reduce asymmetry of information has produced mixed results. Prior research shows substantial differences in the enforcement of federal standards, resulting in significant variation in the number and severity of deficiency citations and their associated penalties (e. g. payment denials). With the rising importance to healthcare quality transparency, nursing homes are expected to correct their deficiencies in timely manner to maintain their quality reputation and minimize revenue loss from Medicare/Medicaid reimbursement denials. Numerous {{studies have examined the}} prevalence of and variations in quality deficiencies based on facility level and market level factors. However, a critical research gap exists in our understanding of the sources of variations in the duration taken to resolve quality deficiencies. ^ This study presents a retrospective ‘pooled’ cross-sectional analysis of 15, 649 CMS-certified nursing homes examining the association of select organizational and market factors with healthcare-related deficiency resolution time and Medicare payment denial duration. The Donabedian’s Structure-Process-Outcome framework was adapted to conceptualize the research model, and the data was analyzed using Heckman’s model in Stata. Data was obtained from Nursing Home Compare (June 2015 data release covering deficiencies and payment denials during 2012 - 2014) and Provider of Service files, and Census Bureau. The results show that nursing facilities on average have 31 deficiency citations (standard deviation SD= 18) of all categories. The average resolution time was about 35 days (SD= 18 days) for mild severity deficiencies and 37 days (SD= 26 days) for harmful deficiencies. Only 1, 227 facilities had payment denials of which 76 percent (n= 1065) had single denials, and the average denial duration was 31. 5 days (SD= 44 days). Regression results show that facilities have longer deficiency resolution time when operating in states with lower regulatory enforcement <b>stringency</b> <b>levels</b> and counties with higher market concentration. However, contrary to expectation, organizational factors like nonprofit status, five-star ratings and high staffing ratios were associated with longer response time to deficiencies. Whereas, the empirical evidence for payment denial duration was mixed. ^ In overall, the study provide new insights to policymakers and payer organizations on the need of focusing on select organizational and market factors in identifying targeted improvement initiatives. ...|$|R
40|$|This paper {{advances}} {{the measurement}} of nontariff measures (NTMs) by discussing a framework for how to compare regulations. We argue that relative differences in SPS regulations trigger the impact on trade flows between trading partner countries and specifically look at maximum residue levels (MRLs) for pesticides in a case study on Chilean fruit exports to the EU. In order to capture the relative differences and <b>stringency</b> in tolerance <b>levels</b> of trading partners, a simple indicator is constructed and applied in an econometric analysis. In comparison to existing indices of regulatory heterogeneity, the depth of information generated by our indicator severely compromises its coverage. Further development of our heterogeneity index will need to aim at including elements of process standards and conformity assessment procedures...|$|R
40|$|This thesis {{addresses}} {{the issue of}} rules of origin {{and their impact on}} trade flows. Four objectives are sought: i) to provide further evidence on the impact on trade of product-specific preferential rules of origin; ii) to develop a restrictiveness index based on empirical findings; iii) to open the path for the impact of the rules of origin on particular sectors other than textiles; and iv) to contribute with further evidence on regime-wide provisions. Literature on rules of origin is reviewed in Chapter 2. While theoretical literature establishes certain conditions under which rules of origin can increase welfare, empirical literature is unanimous about the negative effects they have on trade flows. Two main aspects stem from the review of the empirical literature. First, empirical literature on rules of origin remains still very limited in scope. Second, in order to proxy the stringency of the rules, traditional literature relies on restrictiveness indices based on an ex-ante observation rule. This rule depends on the authors’ appreciation, which can potentially be incorrect. Chapter 3 provides a broad explanation about the different type of product specific and regime-wide rules of origin. The framework {{to assess the impact of}} specific rules and regime-wide provisions on trade flows is developed in Chapter 4. The analysis is conducted using a gravity model of disaggregated panel data for four reporting countries and 16 FTA partners, controlling for reporter and partner fixed effects. In order to account for different ways of modeling specific rules of origin, four different methods are confronted. Data sources and explanations are also provided in this Chapter. Each of the methods is estimated for total trade flows, exports and imports, as a way to improve the validity of the estimates. The results, along some issues regarding the proper form of the specification are presented in Chapter 5. The results prove significant for every specification and suggest that regional value content type of rules, as well as self-certification procedures promote trade within the FTAs. Using the estimates from the previous chapter, an ex-post restrictiveness index is constructed in Chapter 6. This index is subsequently used to assess the stringency of the rules of origin by sector and by agreement <b>stringency</b> <b>levels.</b> One of the main differences of this index with past indices is the relatively high level of leniency it assigns to regional value content rules. The validity of the ex-post index is checked by estimating the impact of rules of origin on North-South trade as well as on agricultural, industrial and textile imports, finding support on the results. After analyzing the state of play of rules of origin in today’s world, policy recommendations are provided in Chapter 7. There is a practical unanimity on the need to reform the rules of origin as they currently stand. The possibility to choose across-the-board between a regional value content rule and current rules, coupled with self-certification procedures appears to address the concerns of researchers, industry and policy makers. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|Chapter 1 : Remanufacturing: Remanufacturing {{is a form}} of {{recycling}} where used durable goods are refurbished to a condition comparable to new products. With reduced energy and resource consumption, remanufactured goods are produced {{at a fraction of the}} original cost and with lower emissions of pollution. This paper presents a theoretical model of remanufacturing where a duopoly of original manufacturers produces a component of a final good. The component needing to be replaced creates an aftermarket. An environmental regulation assessing a minimum level of remanufacturability is also introduced The main results indicate that a social planner could use collusion of the firms on the level of remanufacturability as a substitute for environmental regulation. However, if an environmental regulation is to be implemented, collusion should be repressed since competition supports the public intervention better. One of the results also coincides with the Porter Hypothesis. Chapter 2 : Transboundary movements of waste: In a stylized model of international trade, a monopolist in the North exports second-hand products to a representative firm in the South to be reused as intermediate goods, with potential trade gains. The degree of reusability of waste products is a crucial choice variable in the North. This is because with a lack of international vigilance, non-reusable waste can be mixed illegally with the reusable waste. I explore the driving forces for the movement of illegal waste, paying particular attention to the role of local waste regulations, such as the EU's Waste Electrical and Electronic Equipment directive. Under mild conditions, it is shown that increased regulation stringency in the North leads its firm to reduce the degree of reusability of its products. As a result, the flow of non-reusable waste to the South increases, providing another channel for the Pollution Haven Hypothesis. Chapter 3 : Is Democracy good for the environment? The role of private mitigation efforts: We study the question posed in the title in the context of open economies where trade and welfare depend on the extent to which regulation permits the environment to be used as an input in production, and where individuals may privately mitigate the consequences of pollution at a cost. Governments may also manage the openness of the economy to trade directly as well as indirectly via environmental regulation. In this framework, we compare the degree of regulatory <b>stringency</b> and the <b>level</b> of pollution that emerge in the equilibria of a set of political regimes that range from autocratic to fully competitive or democratic. The answer to the question is not straightforward in this investigation because many well-off-citizens in democratic countries may prefer the higher gross incomes that come with freer trade and unregulated production...|$|R
40|$|In this report, {{we discuss}} current {{research}} on school siting and environmental health, specific school siting policies in states other than Michigan, federal school siting guidelines, and, lastly, {{we present a}} range of policy options for addressing environmental health issues {{as they relate to}} school siting in Michigan. As children can spend upwards of seven hours per day in school, the location and condition of their school can {{have a significant impact on}} their overall exposure to toxicants in their environment. Furthermore, school location can have an effect on what proportion of children are able to walk or bike to school, and, as such, can have profound impacts on healthy lifestyle choices (Miles, Adelaja, and Wyckoff, 2011). In particular, this report focuses on the environmental pollution burdens faced by children from sources external to the school, such as air pollution from motor-vehicle traffic or industrial sources, and soil and groundwater pollution from hazardous waste sites. Currently, there is no state regulation in Michigan that addresses school siting with respect to environmental quality considerations. At the state level, school siting policies and guidelines currently exist in twenty-six states (Fischbach, 2006). At the federal level, the Environmental Protection Agency (EPA) recently issued guidelines on school siting in October 2011 that they recommend local education agencies follow (U. S. EPA, 2011 a). The EPA guidelines serve an important role as a tool for improved child health, but because the EPA did not mandate or go into specifics about school siting rules due to the vast differences occurring from state to state, the guidelines can be broad and vague. The EPA’s national guidelines provide a basis for understanding key concerns and shortcomings of existing school siting policy, and how it may be used in state and local policies. So, in spite of the fact that these EPA guidelines are available, the presence of policies on school siting at the state level is still necessary to add more state-specific considerations to them. Literature Review We present a broad review of literature related to school siting to help frame the importance of the issue and lend context to the subjects in our Interview Synthesis and Policy Recommendations sections. This includes sections on health and environmental pollution as it relates to schools, school siting and environmental justice, the EPA school siting guidelines, school siting policies that exist in other states, and the current and historic atmosphere of school siting in Michigan. P a g e | 4 Methodology In addition to our review of relevant literature, we conducted a series of telephone interviews with state and federal government agency officials, public health researchers, school administrators, land-use experts, and non-governmental organization representatives. These stakeholders included informants from Michigan, other states, and at the national level. The aim of these interviews was to assess key issues with the status-quo siting process in Michigan, important benefits and drawbacks of policies and guidelines used by other states, and notable considerations to account for in crafting a set of policy recommendations for Michigan. Our interview process was approved by the University of Michigan Institutional Review Board. We used an informal, qualitative process to analyze the information we gathered in these interviews and used this information, along with information from a variety of literature sources, to craft a set of policy recommendations. Interview Synthesis We encountered a number of common themes throughout the course of our interviews. One such theme was the need to foster an environment of collaboration between local education agencies and local government organizations. We found that in the current siting environment, there is often disconnect and a lack of communication between local education agencies and local governments with regards to siting matters. Additionally, informants from many states acknowledged that costs not borne directly by local education agencies at the time of land acquisition and construction were frequently neglected in siting decisions. Schools built far from community centers serve as an example of how schools neglect the true costs of siting decisions. In Michigan, the county road commission frequently bears the cost of building infrastructure to the distant school site. Another example of this type of ‘hidden cost’ is the increasing incidence of symptomatic respiratory illness among students and faculty members in areas with more air pollution. A point of contention between informants was the relative benefits of a system of mandates as opposed to a system of guidelines to inform siting decisions. While some interviewees preferred the rigidity of mandates, expressing that school districts would likely not do much to comply with voluntary guidelines on siting, many interviewees preferred the flexibility of guidelines. Some claimed that mandates would face stiff political opposition, and might also generate resistance from local education agencies. In contrast, one superintendent exclaimed that school administrators would welcome such mandates if they made the job of siting easier, and expressed that the only difficulty in passing a mandate would be in getting it accepted by some portions of the state legislature. P a g e | 5 Policy Recommendations Our literature research and interviews uncovered numerous issues, hurdles, unique situations, and policy recommendations that encompassed a range of topics, as there are many interconnected factors which must be addressed in changing Michigan’s status-quo of school siting. To better understand all these factors and to better organize our research, we grouped the topics into broad themes and developed and crafted our policy recommendations around these themes. We anticipate that a school siting policy in Michigan may occur as a piecemeal process. Instead of having one overarching policy output, we separated the policy recommendation into themes to accommodate many political climates and <b>stringency</b> <b>levels</b> of the policies. These themes are: health, government, environment, community needs, and other (themes that could not be grouped into any of the other four groupings). Admittedly, these groups are broad and many topics overlap. Here is a brief explanation of our groupings and the thought process behind the grouping categories.  Health themes include: site acreage; pollution mitigation; prohibited sites; air quality; soil contamination; groundwater and regional pollutants. We grouped site acreage in with health because often schools that are located further away from city centers have larger site acreage. States with site minimum acreage requirements for schools (ex: one acre for every 100 students) discourage communities from building in town centers where walkability to school increases, thus increasing the health of the students from the increased physical activity.  Environment themes include: topography; seasonal variations; and rural, suburban, and urban siting. Rural, suburban, and urban siting factors were grouped into environment because these areas have different physical environments and environmental stressors.  Community themes include: historic preservation; conflict resolution between stakeholders; accounting for true-cost estimates; promoting public participation; and providing comment periods on siting plans. We grouped true-cost estimates into community, as opposed to government because often community members are not aware of what the true cost to build a school will be (i. e. how much will the construction plus addition infrastructure cost).  Government themes include: decision points; conflict resolution; and inter-agency coordination and communication. ‘Decision points’ refers to which organization has the final say in a school siting decision. Should it be the local school board, the school district, the local government, or a state agency that has the final decision for if and where a school should be built? Other themes include: inclusion of charter and parochial schools in a siting policy; transition strategies between the status quo and widespread use of a set of guidelines or a regulatory system related to siting; and site-specific considerations. These grouping could not fit P a g e | 6 in one specific category. Within all of these groupings, there were overarching themes that were addressed in each grouping. These themes include: the overall issues of transportation; site assessments and review processes; feedback on policy efficacy; outcomes of monitoring efforts; third party consultations; and safe routes to school program involvement Within these broad groupings of school siting considerations, we then developed separate “stringent,” “moderate,” and “lenient” policy recommendations for the ways that school districts could address each factor. Stringent policy recommendations often involve mandatory regulations and aim to minimize health risks associated with school sites to the greatest extent possible. However, these considerations may face funding questions and political barriers in the legislative process. Alternatively, lenient recommendations seek to create beneficial health outcomes, but through a system that makes few reforms to the status quo and requires very little financially. Our moderate recommendations offer a middle-ground between these two points. The policy recommendations found in our report are a synthesis of the Rhode Island Legal Institute’s foundational work, the EPA guidelines, interview feedback, lessons from siting policies implemented in other states, specific considerations for the state of Michigan, and, finally, contributions of new criteria for the school siting process. Ultimately, interested readers of this report can use the information and options that we provide to craft a well-reasoned policy based on the current climate of the state. A listing of complete policy recommendations can be found in Appendix I. Conclusion Michigan needs to enact a school siting policy that aims to promote the health and wellbeing of this state’s children. Since children are mandated to attend schools for a significant portion of their day, it is imperative that we foster a healthy environment in which they can thrive. Our policy recommendations provide a variety of avenues through which legislators can achieve this goal...|$|R

