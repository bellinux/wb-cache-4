0|6357|Public
40|$|Journalists rarely use the web as {{a source}} of data about the state of issues, debates and {{information}} flows in different societies. Liliana Bounegru looks at how media scholars have leveraged digital data <b>and</b> <b>algorithmic</b> <b>accountability.</b> In times of shrinking news budgets and staff cuts journalists can turn to such readily available sources of data as a way to understand public engagement with major issues. Scholars can support this process by making the datasets, tools and protocols developed during their work available to others...|$|R
40|$|Black-box medicine—the use of {{big data}} and {{sophisticated}} machine learning techniques for health-care applications—could {{be the future}} of personalized medicine. Black-box medicine promises {{to make it easier}} to diagnose rare diseases and conditions, identify the most promising treatments, and allocate scarce resources among different patients. But to succeed, it must overcome two separate, but related, problems: patient privacy <b>and</b> <b>algorithmic</b> <b>accountability.</b> Privacy is a problem because researchers need access to huge amounts of patient health information to generate useful medical predictions. And accountability is a problem because black-box algorithms must be verified by outsiders to ensure they are accurate and unbiased, but this means giving outsiders access to this health information. This article examines the tension between the twin goals of privacy and accountability and develops a framework for balancing that tension. It proposes three pillars for an effective system of privacy-preserving accountability: substantive limitations on the collection, use, and disclosure of patient information; independent gatekeepers regulating information sharing between those developing and verifying black-box algorithms; and information-security requirements to prevent unintentional disclosures of patient information. The article examines and draws on a similar debate in the field of clinical trials, where disclosing information from past trials can lead to new treatments but also threatens patient privacy...|$|R
5000|$|... "A.I. {{must have}} <b>algorithmic</b> <b>accountability</b> so that humans can undo {{unintended}} harm".|$|R
40|$|Farida Vis, Research Fellow in the Information School at the University of Sheffield, {{investigates the}} issue of trust in the debate about <b>algorithmic</b> <b>accountability,</b> arguing that we should instead focus on ‘trustworthiness’ and that {{now is the time}} for a {{considered}} debate about <b>algorithmic</b> governance <b>and</b> <b>accountability</b> frameworks...|$|R
50|$|Since DARPA's {{introduction}} of it's program in 2016, {{a number of}} initiatives have started {{to address the issue}} of <b>algorithmic</b> <b>accountability</b> <b>and</b> provide transparency concerning how technologies within this domain function.|$|R
40|$|Alison Powell, Assistant Professor at LSE, investigates how {{data and}} {{algorithms}} effect our daily lives, from negotiating public transport and booking restaurants, {{to the more}} serious issues of surveillance and privacy. She argues {{that there is a}} greater need for <b>algorithmic</b> <b>accountability</b> {{in order for us to}} understand its impact, both positive and negative, on not only our day-to-day lives, but also on citizenship and inequality in our societies...|$|R
50|$|Several {{papers have}} been {{published}} on these topics in 2016, the first of which, by Goodman / Flaxman, outlines {{the development of the}} right to explanation. Pasquale does not think the approach goes far enough, as he has stated in a blog entry at the London School of Economics (LSE). In fact at LSE there is a whole series on <b>Algorithmic</b> <b>Accountability</b> of which that was one entry in Feb. of 2016, and other notable ones were by Joshua Kroll and Mireille Hildebrandt.|$|R
40|$|In {{the debate}} on <b>algorithmic</b> <b>accountability,</b> <b>and</b> {{platform}} responsibility more specifically, {{the contribution of the}} social researcher is immense. In this set of posts, researchers reflect upon broad themes of control and agency — not only that which is faced by the data subject, but also by the researcher who relies on proprietary platforms to understand how these systems operate and interact with users. This research bears relevance to policy debates, because it provides evidence of ways in which automated systems shape consumer and citizens and look beyond conventional recommendations of transparency or openness...|$|R
40|$|This {{research}} received {{funding from}} the European Union Horizon 2020 Framework Programme and Seventh Framework Programme through the respective projects DANTE (Detecting and analysing terrorist-related online contents and financing activities), H 2020 - FCT- 2015 - 700367 and VALCRI (Visual Analytics for Sense-making in Criminal Intelligence Analysis), FP 7 -IP- 608142. n the hopes of making law enforcement more effective and efficient, police and intelligence analysts are increasingly relying on algorithms underpinning technology-based and data-driven policing. To achieve these objectives, algorithms must also be accurate, unbiased and just. In this paper, we examine how European data protection law regulates automated profiling and how this regulation impacts police and intelligence algorithms <b>and</b> <b>algorithmic</b> discrimination. In particular, we assess {{to what extent the}} regulatory frameworks address the challenges of <b>algorithmic</b> transparency <b>and</b> <b>accountability.</b> We argue that while the law regulates both algorithms and their discriminatory effects, the framework is insufficient in addressing the complex interactions that must take place between system developers, users, oversight and profiled individuals to fully guarantee <b>algorithmic</b> transparency <b>and</b> <b>accountability.</b> status: publishe...|$|R
40|$|The table <b>and</b> <b>algorithmic</b> {{method of}} {{calculation}} of polynomials based on preliminary coefficient processing is offered. Possibility of acceleration of calculation of polynomials in comparison with realization of the well-known table <b>and</b> <b>algorithmic</b> methods is shown. ????????? ????????-??????????????? ????? ?????????? ???????????, ?????????? ?? ??????????????? ????????? ?????????????. ???????? ??????????? ????????? ?????????? ??????????? ?? ????????? ? ??????????? ????????? ????????-??????????????? ???????...|$|R
40|$|There are two {{well-known}} computation {{methods for}} solving multi-digit subtraction items, namely mental <b>and</b> <b>algorithmic</b> computation. It has been contended that mental <b>and</b> <b>algorithmic</b> computation differentially rely on numerical magnitude processing, {{an assumption that}} has already been examined in children, but not yet in adults. Therefore, in this study, we examined how numerical magnitude processing was associated with mental <b>and</b> <b>algorithmic</b> computation, <b>and</b> whether this association with numerical magnitude processing was different for mental versus algorithmic computation. We also investigated whether the association between numerical magnitude processing <b>and</b> mental <b>and</b> <b>algorithmic</b> computation differed for measures of symbolic versus nonsymbolic numerical magnitude processing. Results showed that symbolic, and not nonsymbolic, numerical magnitude processing was associated with mental computation, but not with algorithmic computation. Additional analyses showed, however, that the size of this association with symbolic numerical magnitude processing was not significantly different for mental <b>and</b> <b>algorithmic</b> computation. We also tried to further clarify the association between numerical magnitude processing and complex calculation by also including relevant arithmetical subskills, i. e. arithmetic facts, needed for complex calculation that are also known to be dependent on numerical magnitude processing. Results showed that the associations between symbolic numerical magnitude processing <b>and</b> mental <b>and</b> <b>algorithmic</b> computation were fully explained by individual differences in elementary arithmetic fact knowledgestatus: publishe...|$|R
50|$|EXPRESS {{is similar}} to {{programming}} languages such as Pascal. Within a SCHEMA various datatypes can be defined together with structural constraints <b>and</b> <b>algorithmic</b> rules. A main feature of EXPRESS is the possibility to formally validate a population of datatypes - this is to check for all the structural <b>and</b> <b>algorithmic</b> rules.|$|R
50|$|Algo TV: TV channel {{delivers}} news regarding algorithms <b>and</b> <b>algorithmic</b> trading.|$|R
50|$|His latest {{research}} {{was in the}} area of computational <b>and</b> <b>algorithmic</b> algebraic geometry.|$|R
5000|$|... 1993: (with Ortrud R. Oellermann) Applied <b>and</b> <b>Algorithmic</b> Graph Theory, McGraw Hill [...]|$|R
5000|$|Thirty-three miniatures — Mathematical <b>and</b> <b>algorithmic</b> {{applications}} of linear algebra. AMS, 2010, [...]|$|R
50|$|TiVo Suggestions and Pandora have pioneered {{predictive}} consumer behavior {{based on}} history <b>and</b> <b>algorithmic</b> processing.|$|R
5000|$|For {{contributions}} to machine learning, artificial intelligence, <b>and</b> <b>algorithmic</b> game theory <b>and</b> computational social science.|$|R
5000|$|S-Net {{coordination}} {{language from}} the University of Hertfordshire, which provides separation of coordination <b>and</b> <b>algorithmic</b> programming ...|$|R
2500|$|Researcher, Andrew Tutt, {{argues that}} {{algorithms}} should be overseen by a specialist regulatory agency, similar to FDA. His academic work {{emphasizes that the}} rise of increasingly complex algorithms calls for the {{need to think about}} the effects of algorithms today. Due to the nature and complexity of algorithms, it will prove to be difficult to hold algorithms accountable under criminal law. Tutt recognizes that while some algorithms will be beneficial to help meet technological demand, others should not be used or sold if they fail to meet safety requirements. Thus, for Tutt, algorithms will require [...] "closer forms of federal uniformity, expert judgment, political independence, and pre-market review to prevent the introduction of unacceptably dangerous algorithms into the market". The issue of <b>algorithmic</b> <b>accountability</b> (the responsibility of algorithm designers to provide evidence of potential or realised harms) is of particular relevance in the field of dynamic and non-linearly programmed systems, e.g. artificial neural networks, deep learning, and genetic algorithms (see Explainable AI).|$|R
25|$|Hierarchical {{temporal}} {{memory is}} {{an approach that}} models some of the structural <b>and</b> <b>algorithmic</b> properties of the neocortex.|$|R
5000|$|Design: design {{games that}} have both good game-theoretical <b>and</b> <b>{{algorithmic}}</b> properties. This area is called algorithmic mechanism design ...|$|R
50|$|Staiger is {{an active}} {{researcher}} in combinatorics on words, automata theory, constructive dimension theory <b>and</b> <b>algorithmic</b> information theory.|$|R
50|$|The Electronium, {{created by}} Raymond Scott, {{is an early}} {{combined}} electronic synthesizer <b>and</b> <b>algorithmic</b> composition / generative music machine.|$|R
40|$|We propose {{methods for}} the {{evaluation}} software reliability parameter of the real-time processing systems at complex application software testing <b>and</b> <b>algorithmic</b> redundancy. A formulation {{of the problem of}} choice the software testing <b>and</b> <b>algorithmic</b> redundancy parameters for achieving reliability requirements for the realtime processing systems is given as a problem of the mixed (integer and continuous) programming with the nonlinear constraints. The problem is solved by brute-force and mesh adaptive direct search algorithms.  </p...|$|R
50|$|SuperCollider is an {{environment}} and programming language originally released in 1996 by James McCartney for real-time audio synthesis <b>and</b> <b>algorithmic</b> composition.|$|R
5000|$|The {{structure}} <b>and</b> <b>algorithmic</b> use of doubly chordal graphs {{is given}} by [...] These are graphs which are chordal and dually chordal.|$|R
3000|$|... {{mathematical}} <b>and</b> <b>algorithmic</b> modeling, {{from simple}} summation to linking different basic arithmetic operations and to operations, which demand {{the application of}} complex formulas.|$|R
50|$|His {{research}} {{career has}} been centered around the design of algorithms, together with work on computational complexity theory, cryptography, <b>and</b> <b>algorithmic</b> game theory.|$|R
40|$|Between {{computational}} <b>and</b> <b>algorithmic</b> 2 Marr’s {{levels of}} analysis – computational, <b>algorithmic,</b> <b>and</b> implementation – have served cognitive science {{well over the}} last 30 years. But the recent increase in {{the popularity of the}} computational level raises a new challenge: how do we begin to relate models at different levels of analysis? We propose {{that it is possible to}} define levels of analysis that lie between the computational <b>and</b> the <b>algorithmic,</b> providing a way to build a bridge between computational- and algorithmic-level models. The key idea is to push the notion of rationality, often used in defining computational-level models, deeper towards the algorithmic level. We offer a simple recipe for reverse-engineering the mind’s cognitive strategies by deriving optimal algorithms for a series of increasingly more realistic abstract computational architectures which we call “resource-rational analysis”. Between computational <b>and</b> <b>algorithmic</b> 3 Rational use of cognitive resources: Levels of analysis between the computational <b>and</b> the <b>algorithmic</b> Marr <b>and</b> Poggio (1977) proposed a key methodology of cognitive science: proceed b...|$|R
50|$|We {{introduce}} {{basic concepts}} <b>and</b> <b>algorithmic</b> questions studied in this area, and we mention some long-standing open problems. Then, we mention selected recent results.|$|R
30|$|Recent {{research}} in mobile ad hoc networks and {{wireless sensor networks}} raises a number of interesting, and difficult, theoretical <b>and</b> <b>algorithmic</b> issues. The objective of this special issue is to gather recent advances {{in the areas of}} wireless ad hoc and sensor networks, with a focus of theoretical <b>and</b> <b>algorithmic</b> aspect. In particular, it is devoted to distributed algorithms, randomized algorithms, analysis and modeling, optimizations, and theoretical methods in design and analysis of networking protocols for wireless ad hoc and sensor networks.|$|R
50|$|KeyKit is a {{graphical}} {{environment and}} programming language for MIDI synthesis <b>and</b> <b>algorithmic</b> composition. It was originally developed by Tim Thompson and released by AT&T.|$|R
50|$|Algorithmic {{learning}} theory is a mathematical framework for analyzing machine learning problems and algorithms. Synonyms include formal {{learning theory}} <b>and</b> <b>algorithmic</b> inductive inference. Algorithmic learning theory {{is different from}} statistical learning theory in {{that it does not}} make use of statistical assumptions <b>and</b> analysis. Both <b>algorithmic</b> <b>and</b> statistical learning theory are concerned with machine learning and can thus be viewed as branches of computational learning theory.|$|R
5000|$|Self {{replication}} A {{system with}} many units capable of self replication by collecting scattered building blocks will require solving {{many of the}} hardware <b>and</b> <b>algorithmic</b> challenges.|$|R
5000|$|... for {{fundamental}} contributions {{across a}} broad range of areas of operations research and management science, most notably in linear programming, combinatorial optimization, <b>and</b> <b>algorithmic</b> game theory.|$|R
