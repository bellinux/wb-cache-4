56|136|Public
25|$|As the {{prosodic}} {{unit of the}} dialect consists not of a lexical word {{but rather}} a minimal <b>syntactic</b> <b>phrase</b> known as bunsetsu, the high tone will shift, realigning itself to the ultimate or penultimate syllable, when morphemes, auxiliaries or grammatical particles such as が ga are appended at the end.|$|E
2500|$|Clitics do {{not always}} appear next to the word or phrase that they are {{associated}} with grammatically. They {{may be subject to}} global word order constraints that act on the entire sentence. Many Indo-European languages, for example, obey Wackernagel's law (named after Jacob Wackernagel), which requires clitics to appear in [...] "second position", after the first <b>syntactic</b> <b>phrase</b> or the first stressed word in a clause: ...|$|E
2500|$|Because prepositions in Russian {{act like}} clitics, the <b>syntactic</b> <b>phrase</b> {{composed}} of a preposition (most notably, the three that consist of just a single consonant: , , and [...] ) and a following word constitutes a phonological word that acts like a single grammatical word. For example, the phrase [...] ('with friends') is pronounced [...] In the syllable coda, suffixes that contain no vowels may increase the final consonant cluster of a syllable (e.g. [...] 'city of Noyabrsk' ||+ [...] || → [...] ), theoretically up to seven consonants: * [...] ('of monsterships'). There is usually an audible release between these consecutive consonants at word boundaries, the major exception being clusters of homorganic consonants.|$|E
50|$|Sentence suffixes occur {{after all}} other {{morphological}} processes and can occur on any morphological words or <b>syntactic</b> <b>phrases.</b> The nature of sentence suffixes allows {{for freedom of}} word order and creativity in sentence construction, useful to storytellers (Hardman, 2000).|$|R
50|$|As {{there is}} very little fusion {{involved}} in word formation, classical typology mostly applies to inflectional morphology. Depending on the preferred way of expressing non-inflectional notions, languages may be classified as synthetic (using word formation) or analytic (using <b>syntactic</b> <b>phrases).</b>|$|R
40|$|The {{question}} of the cohesiveness of words {{is central to the}} debate on the syntax-morphology interface. The traditional view that «words are built out of different structural elements and by different principles of composition than <b>syntactic</b> <b>phrases</b> » (Bresnan & Mchombo 1995 : 181) has been defended withi...|$|R
50|$|As the {{prosodic}} {{unit of the}} dialect consists not of a lexical word {{but rather}} a minimal <b>syntactic</b> <b>phrase</b> known as bunsetsu, the high tone will shift, realigning itself to the ultimate or penultimate syllable, when morphemes, auxiliaries or grammatical particles such as が ga are appended at the end.|$|E
5000|$|Clitics do {{not always}} appear next to the word or phrase that they are {{associated}} with grammatically. They {{may be subject to}} global word order constraints that act on the entire sentence. Many Indo-European languages, for example, obey Wackernagel's law (named after Jacob Wackernagel), which requires clitics to appear in [...] "second position", after the first <b>syntactic</b> <b>phrase</b> or the first stressed word in a clause: ...|$|E
50|$|Initial {{mutations}} {{were originally}} sandhi effects, and {{depended on the}} shape of the original final syllable in Primitive Irish. It was grammaticalised by the loss of final syllables in the transition from Primitive Irish to Old Irish. In Old Irish, the process was already grammatical to a large degree, and was limited to applying across words within a single <b>syntactic</b> <b>phrase</b> (e.g. between a noun and a modifying adjective, or between a preposition {{and the rest of the}} prepositional phrase). Initial mutations did not apply across phrase boundaries generally, but there are some instances where this does occur in the earliest Old Irish attestations.|$|E
40|$|Information Retrieval (IR) is an {{important}} application area of Natural Language Processing (NLP) where one encounters the genuine challenge of processing large quantities of unrestricted natural language text. While much effort {{has been made to}} apply NLP techniques to IR, very few NLP techniques have been evaluated on a document collection larger than several megabytes. Many NLP techniques are simply not efficient enough, and not robust enough, to handle a large amount of text. This paper proposes a new probabilistic model for noun phrase parsing, and reports on the application of such a parsing technique to enhance document indexing. The effectiveness of using <b>syntactic</b> <b>phrases</b> provided by the parser to supplement single words for indexing is evaluated with a 250 megabytes document collection. The experiment's resuits show that supplementing single words with <b>syntactic</b> <b>phrases</b> for indexing consistently and significantly improves retrieval performance...|$|R
40|$|This {{paper is}} an {{overview}} of ASCOF, a modular multilevel system for French-German translation. In ASCOF, the classical divisions of the translation process (analysis, transfer, synthesis) have been adopted. The analysis is realized by three phases: (1) the morphological analysis, (2) the identification of non-complex <b>syntactic</b> <b>phrases</b> and the macrostructure of the sentence, and (3) {{the determination of the}} structure of complex <b>syntactic</b> <b>phrases</b> and the <b>syntactic</b> functions, in which syntacfic and semantic criteria are used. Semantic criteria are stored in a semantic network. The syntax-oriented parts of the system interact with this semantic network during the identification of the syntacfic functions. The lexical transfer operates on the standardized output tree of the analysis. The structural transfer and the syntactic synthesis are achieved by transformational grammars; the morphological synthesis, at least, generates the word form of the target language (German...|$|R
40|$|The paper {{shows that}} catena represen-tation {{together}} with valence information {{can provide a}} good way of encoding Multiword Expressions (beyond idioms). It also discusses a strategy for mapping noun/verb compounds with their counter-part <b>syntactic</b> <b>phrases.</b> The data on Mul-tiword Expression comes from BulTree-Bank, while the data on compounds comes from a morphological dictionary of Bul-garian. ...|$|R
5000|$|Because prepositions in Russian {{act like}} clitics, the <b>syntactic</b> <b>phrase</b> {{composed}} of a preposition (most notably, the three that consist of just a single consonant: , , and [...] ) and a following word constitutes a phonological word that acts like a single grammatical word. For example, the phrase [...] ('with friends') is pronounced [...] In the syllable coda, suffixes that contain no vowels may increase the final consonant cluster of a syllable (e.g. [...] 'city of Noyabrsk' ||+ || → [...] ), theoretically up to seven consonants: * [...] ('of monsterships'). There is usually an audible release between these consecutive consonants at word boundaries, the major exception being clusters of homorganic consonants.|$|E
5000|$|Linguists Morris Halle and Samuel Jay Keyser {{developed}} the earliest theory of generative metrics — {{a set of}} rules that define those variations that are permissible (in their view) in English iambic pentameter. Essentially, the Halle-Keyser rules state that only [...] "stress maximum" [...] syllables are important in determining the meter. A stress maximum syllable is a stressed syllable surrounded on both sides by weak syllables in the same <b>syntactic</b> <b>phrase</b> and in the same verse line. In order to be a permissible line of iambic pentameter, no stress maxima can fall on a syllable that is designated as a weak syllable in the standard, unvaried iambic pentameter pattern. In the Donne line, the word God is not a maximum. That is because it is followed by a pause. Similarly the words you, mend, and bend are not maxima since they are each {{at the end of a}} line (as required for the rhyming of mend/bend and you/new.) Rewriting the Donne quatrain showing the stress maxima (denoted with an [...] "M") results in the following: ...|$|E
40|$|In {{this paper}} we {{describe}} an experiment with <b>syntactic</b> <b>phrase</b> indexing for Dutch texts. We compare different choices for combining terms to form head-modifier pairs {{and we also}} investigate {{the effect of adding}} none, one, or all constituent parts of the pair as a separate index term. The results of our experiments show that using head-modifier pairs as index terms can improve both recall and precision significantly but only if all constituent parts are also added separately. We found that using both Noun-Adjective and Noun-Noun head-modifier pairs produced the best results. Keywords Natural language processing; <b>syntactic</b> <b>phrase</b> indexing; head-modifier pairs; Dutch. 1 Introduction The work described in this paper is part of the UPLIFT project 1. UPLIFT investigates whether linguistic tools can improve and extend the functionality of vector space text retrieval systems. This paper describes an experiment with <b>syntactic</b> <b>phrase</b> indexing for Dutch texts. The basic idea behind phrase i [...] ...|$|E
50|$|In a {{study by}} Pate et al (2011), where a {{computational}} language model was presented, it was shown that acoustic cues can be helpful for determining syntactic structure when they are used with lexical information. Combining acoustic cues with lexical cues may usefully provide children with initial information about the place of <b>syntactic</b> <b>phrases</b> which supports the prosodic bootstrapping hypothesis.|$|R
50|$|The {{argument}} for prosodic bootstrapping {{was first introduced}} by Gleitman and Wanner (1982), who observed that infants might use prosodic cues (particularly acoustic cues) to discover underlying grammatical information about their native language. These cues (e.g intonation contour in a question phrase, lengthening a final segment) could aid infants in dividing the speech input into different lexical units, and furthermore aid in placing these units into <b>syntactic</b> <b>phrases</b> appropriate to the language.|$|R
50|$|Syntax in Jaqaru {{consists}} {{mainly of}} a system of sentence suffixes. These suffixes indicate sentence type (interrogative, declarative, etc.) Suffixes can and often do occur more than one per sentence, marking sentence type and creating complex constructions. Simply put, for sentences to be grammatical in Jaqaru, they must be inflected. Morphological words and <b>syntactic</b> <b>phrases</b> which do not contain a sentence suffix are judged by native speakers to be ungrammatical and for some, impossible to say (Hardman, 2000).|$|R
40|$|Term {{clustering}} and <b>syntactic</b> <b>phrase</b> formation are {{methods for}} transforming natural language text. Both have had only mixed success as strategies {{for improving the}} quality of text representations for document retrieval. Since the strengths of these methods are complementary, we have explored combining them to produce superior representations. In this paper WC discuss our implementation of a <b>syntactic</b> <b>phrase</b> generator, as well as our preliminary experiments with producing phrase clusters. These experiments show small improvements in retrieval effectiveness resulting from the use of phrase clusters, {{but it is clear that}} corpora much larger than standard information retrieval test colIections will be required to thoroughly evaluate the use of thin technique. ...|$|E
40|$|A {{parallel}} treebank {{consists of}} syntactically annotated sentences {{in two or}} more languages, taken from translated documents. These parallel sentences are linked through alignment. This paper explores the use of word n-gram alignment, computed for statistical machine translation, to create <b>syntactic</b> <b>phrase</b> alignment. We achieve a weighted F 0. 5 -score of over 65 %. ...|$|E
40|$|The {{study of}} clitics {{has been a}} fertile site for {{investigating}} matters of the syntax-phonology interface, arguably second only to phonological phrasing phenomena. For instance, a classi-cal puzzle has been the analysis of Wackernagel clitics, which appear in a second ‘position’ in the clause, either following the first <b>syntactic</b> <b>phrase</b> or first stressed word (Wackernage...|$|E
40|$|Particle verbs and prosody" This paper investigates a {{class of}} verb-particle {{combinations}} in Estonian and their status as particle verbs vs. <b>syntactic</b> <b>phrases.</b> The paper first gives {{a brief overview of}} the main difficulties with defining and classifying particle verbs in Estonian (the difference between noun-verb combinations and adverb-verb combinations; verbal particle (afiksaaladverb) as a separate word class in Estonian, etc.). The second part of the paper presents the results of a prosodic study. The study has two primary aims: 1. To ascertain whether we can use prosody to investigate the grammatical status of verb-particle combinations, and 2. to contribute to the discussion of the lexical vs. syntactic nature of compositional verb-particle combinations in Estonian. The results of the study suggest that complex predicates and <b>syntactics</b> <b>phrases</b> are prosodically different, and, consequently, that prosodic analysis {{can be used as a}} method to study the lexical vs. phrasal status of verb-particle combinations. As to the status of compositional verb-particle combinations, the results support their treatment as complex predicates rather than phrases. The results further suggest that elements that are intermediate between lexicon and syntax may also be in some sense located on a prosodic continuum...|$|R
40|$|This study {{presents}} {{a comparison between}} <b>syntactic</b> and prosodic <b>phrasing.</b> A parser is used to calculate the syntactic structures from the orthographic text the prosodic structures of which are given by means of ToBI label files. For the automatic evaluation the prosodic break indices " 3 " (intermediate phrase boundary) and " 4 " (intonation phrase boundary) are compared with the terminals extracted from the extensive syntactic structures generated by the parser. These terminals are assumed to be the carriers of the phrase boundaries. Keywords: syntax, parsing, prosody, phrase boundaries, prosodic labelling. 1. INTRODUCTION This study aims to show that there is a strong correspondence between <b>syntactic</b> <b>phrasing</b> and prosodic phrasing. It is based on the comparison of ToBI-labelled break indices and the output of a parser. There {{has been a lot of}} discussion about the correspondence between <b>syntactic</b> and prosodic <b>phrasing</b> ([9], [10], [11], [13]). Most of the criticism concerns syntactic ana [...] ...|$|R
25|$|Similar to the {{discussion}} above, clitics {{must be able to}} be distinguished from words. There {{have been a number of}} linguistic tests proposed to differentiate between the two categories. Some tests, specifically, are based upon the understanding that when comparing the two, clitics resemble affixes, while words resemble <b>syntactic</b> <b>phrases.</b> Clitics and words resemble different categories in the sense that they share certain properties with them. Six such tests are described below. These, of course, are not the only ways to differentiate between words and clitics.|$|R
40|$|A {{fundamental}} {{activity in}} programming language design is {{the association of}} a name to a <b>syntactic</b> <b>phrase.</b> This is called a definition binding, since the name defines the <b>syntactic</b> <b>phrase.</b> Bindings can also arise {{as a consequence of}} parameter transmission, via the association of a formal parameter name to an actual parameter, and are termed parameter bindings. Bindings can be understood in many different ways: we study two popular semantics of bindings, namely, eager (or call-by-value) semantics and lazy (or call-by-name) semantics. We validate Landin's correspondence principle, a benchmark of programming language design, by developing a categorical model for a higher-order, modular language. The model provides a uniform semantics of definition and parameter bindings. If the language satisfies the correspondence principle, the categorical model is a weakly symmetric, monoidal-closed category. This allows the understanding of definitions as records, their invocation as record indexing, [...] ...|$|E
40|$|Based on {{so-called}} pronoun-noun {{construction in}} Japanese, this article first establishes categorial distinction among nominal {{elements in the}} language. The distinction is connected to philosophical notions of Indexicality, direct reference, and extension- intension divide. It is claimed that these notions are represented quite straightforwardly by means of <b>syntactic</b> <b>phrase</b> structure. The result urges formal linguists {{to pay more attention}} to philosophical literature on language...|$|E
40|$|In {{this paper}} we {{introduce}} a semantic role labeling system constructed {{on top of}} the full syntactic analysis of text. The labeling problem is modeled using a rich set of lexical, syntactic, and semantic attributes and learned using one-versus-all AdaBoost classifiers. Our results indicate that even a simple approach that assumes that each semantic argument maps into exactly one <b>syntactic</b> <b>phrase</b> obtains encouraging performance, surpassing the best system that uses partial syntax by almost 6 %. ...|$|E
40|$|In Northern Bizkaian Basque (NBB), Intermediate Phrases (ips) align {{by default}} {{with the left}} edge of <b>syntactic</b> <b>phrases.</b> The main intonational cue of ips is partial pitch reset at their left edges. A minimal size {{constraint}} applies on ips occurring at the left edge of an Intonational Phrase (IP), requiring that they consist {{of at least two}} Accentual Phrases (APs). Following [9]’s idea that certain prominent positions demand augmentation, the NBB facts show that the left edge of an IP can also be a phonologically prominent position...|$|R
50|$|Similar to the {{discussion}} above, clitics {{must be able to}} be distinguished from words. There {{have been a number of}} linguistic tests proposed to differentiate between the two categories. Some tests, specifically, are based upon the understanding that when comparing the two, clitics resemble affixes, while words resemble <b>syntactic</b> <b>phrases.</b> Clitics and words resemble different categories in the sense that they share certain properties with them. Six such tests are described below. These, of course, are not the only ways to differentiate between words and clitics.|$|R
40|$|This work {{focuses on}} the {{relationship}} between prosodic and syntactic domains in order to investigate whether there is a preferred syntactic domain of tonal units and whether there is a preferred prosodic domain of syntactic constituents. Two independent analyses of phonetic prosodic boundaries of syntactic constituents and of syntactic structure of prosodic constituents in Italian spontaneous dialogues were carried out. On the one hand, our results confirmed data from previous studies on non-isomorphism of syntactic and prosodic constituents. On the other hand, new data are presented, which show that: 1) the coextension of prosodic and <b>syntactic</b> <b>phrasing</b> is favoured by specific <b>syntactic</b> structures, mainly <b>phrases</b> or minimal sentence structures; 2) informational and pragmatic factors as well as turn-taking frequency strongly constrain prosodic (and syntactic) phrasing; 3) the same acoustic-phonetic cues are involved in prosodic <b>phrasing</b> of different <b>syntactic</b> constituents...|$|R
40|$|We {{address the}} {{question}} of how syntactic and prosodic factors interact during the comprehension of spoken sentences. Previous studies using event-related brain potential measures have revealed that <b>syntactic</b> <b>phrase</b> structure violations elicit an early left anterior negativity followed by a late posterior positivity (P 600). We present recent experimental evidence showing that prosodic information can modulate these components and thus the syntactic processes they reflect. We conclude that the initiation of first-pass parsing processes is affected by the appropriateness of the prosodic realization of the preceding element...|$|E
40|$|The {{syntactic}} and prosodic {{information needed}} for auditory language comprehension is intertwined within the speech signal. Previous studies seeking to isolate automatic syntactic processes have reported an early left anterior negativity (ELAN) between 100 and 200 ms elicited by <b>syntactic</b> <b>phrase</b> structure violations. Although prosody was already well controlled in these studies, {{a change in}} the fundamental frequency (F 0) contour occurred together with the syntactic violation. The present magnetoencephalography study aimed to disentangle the influence of these two superimposed processes. Responses elicited by a <b>syntactic</b> <b>phrase</b> structure violation were compared to responses elicited by a prosodically incongruent change in the sentence's F 0 contour in order to estimate the contribution of a prosodic incongruency to the ELAN effect. While both violations elicited stronger superior temporal cortex activation than correct sentences in a 110 – 160 ms time window, the syntax violation effect was larger than the prosody violation effect and showed a left hemispheric bias which was absent for the prosodic violation. Furthermore, only syntactically incorrect sentences elicited an additional very early effect in a preceding time window. Thus, the syntax violation effect found in the current and also in previous studies cannot be attributed to the detection of an unexpected prosodic contour, but rather reflects difficulties in local phrase structure building...|$|E
40|$|Does the lexicon alone {{determine}} reference or does <b>syntactic</b> <b>phrase</b> structure {{contribute to}} this process as well? Researchers have proposed contrasting responses to this question. Some take young children’s frequent omissions of functional categories as evidence for diminished syntactic knowledge, arguing instead that children rely primarily on lexical categories {{in the early stages}} of language acquisition (e. g., Radford 1990, 1997, Tomasello 2000 a, 2000 b, 2002, Tsimpli, 1991). In contrast, others argue for early access to functional categories, even when these are absent in children’s speech (Boser, Lust, Santelmann, & Whitman 1992, Demuth 1994...|$|E
40|$|A {{parallel}} treebank {{consists of}} syntactically annotated sentences {{in two or}} more languages, taken from translated (i. e. parallel) documents. These parallel sentences are linked through alignment. Much {{work has been done}} on sentence and word alignment, but not as much on the intermediate level. This paper explores using n-gram alignment created for statistical machine translation based on GIZA++ word alignment. The n-grams are compared to the <b>syntactic</b> <b>phrases</b> of two parallel treebanks to create phrase alignment. The experiments show good results, even though the n-gram alignment is not very good, due to a small training material. ...|$|R
40|$|Certain Natural Language Processing (NLP) {{applications}} such as parsing and semantic processing require complete lexicons that provide subcategorization information for a word of interest, i. e. the necessary information about the set(s) of syntactic constituents the word must combine with, in order for its meaning to be fully expressed. Modem Greek presents high flexibility in the allowable orderings of its <b>syntactic</b> <b>phrases</b> as well as rich variety of syntactic constructions, which may function as arguments to verbs. In this paper, we describe a set of machine learning techniques used to automatically extract subcategorization frames of verbs from corpora...|$|R
40|$|We {{investigate}} {{the use of}} syntactically related pairs of words for the task of text classification. The set of all pairs of syntactically related words should intuitively provide a better description of what a document is about, than the set of proximity-based N-grams or selective <b>syntactic</b> <b>phrases.</b> We generate syntactically related word pairs using a dependency parser. We experimented with Support Vector Machines and Decision Tree learners on the 10 most frequent classes from the Reuters- 21578 corpus. Results show that syntactically related pairs of words produce better results in terms of accuracy and precision when used alone or combined with unigrams, compared to unigrams alone. ...|$|R
