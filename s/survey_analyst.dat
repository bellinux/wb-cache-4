6|73|Public
2500|$|There are {{a number}} of {{software}} extensions that can be added to ArcGIS for Desktop that provide added functionality, including 3D Analyst, Spatial Analyst, Network Analyst, <b>Survey</b> <b>Analyst,</b> Tracking Analyst, and Geostatistical Analyst. [...] Advanced map labeling is available with the Maplex extension, as an add-on to ArcView and ArcEditor and is bundled with ArcInfo. Numerous extensions have also been developed by third parties, such as the [...] spell-checker, [...] XTools and MAP2PDF for creating georeferenced pdfs (GeoPDF), ERDAS' Image Analysis and Stereo Analyst for ArcGIS, and ISM's PurVIEW, which converts Arc- desktops into precise stereo-viewing windows to work with geo-referenced stereoscopic image models for accurate geodatabase-direct editing or feature digitizing.|$|E
50|$|He did his {{undergraduate}} {{work at the}} City College of New York, {{where he}} received his B.S. in sociology with a minor in psychology in 1945. From 1958 to 1962 he was an {{associate director of the}} Psychological Corporation. He was Associate Manager of Marketing Communications Research for the advertising firm of McCann-Erickson, Inc., from 1952 to 1958, and a research associate at the Bureau of Social Science Research, the American University, Washington, D.C., from 1952 to 1956. Before going to the American University, he served as a senior <b>survey</b> <b>analyst</b> with the International Broadcasting Service of the U.S. Department of State (1950-1952); a study director with the Department of Scientific Research of the American Jewish Committee where, among other research projects, he worked on the Authoritarian Personality study and on the Research in Contemporary Cultures project with Margaret Mead (1947-1950), and as a research fellow, Department of Sociology, City College of New York (1946-1947). His research activity has focused mainly on social relations; attitudes and public opinion formation and change; communications; public health; and the sociology of politics. He was a frequent contributor to social research journals, publishing over 50 articles and authoring four books. He frequently lectured at conferences and seminars, and was elected to give the annual University Lecture at the University of Denver in 1968. He also served on numerous boards, commissions and governmental committees.|$|E
40|$|Tom Krenzke, Associate Director, Statistics Group, Westat. Clara Reschovsky, <b>Survey</b> <b>Analyst,</b> Metropolitan Washington Council of Governments. Practical {{insights}} {{to balance}} disclosure risk, data utility, {{and other issues}} relating to operational feasibility, budget, and timelines. Additional topics related to data training and documentation...|$|E
40|$|<b>Survey</b> <b>analysts</b> {{routinely}} ignore complex design {{factors such}} as clustering, stratification and weighting. • This results in biased estimates of standard errors and increased likelihood of Type I errors. • A substantive example is used to illustrate the problem and appropriate software applications are briefly reviewed...|$|R
40|$|Some <b>survey</b> <b>analysts</b> {{may wish}} to correct {{standard}} errors for weighting and clustering using statistical packages such as SUDAAN or Stata. This appendix proposes one method to prepare the data for such an analysis. We have included the following variables on each data file (continuing/new caregiver, separated caregiver, and focal child) ...|$|R
40|$|This paper aims at {{providing}} business <b>survey</b> <b>analysts</b> {{with simple}} econometric tools to quantify qualitative survey data. We extend {{the traditional and}} commonly applied method proposed by Carlson and Parkin (1975) to capture observable survey respondent heterogeneity. We also discuss specification tests. The empirical analysis is based on business survey data taken from the ZEW's 'Service Sector Business Survey', a quarterly business survey in the German business-related service sector carried out since 1994. [...] quantification technique,ordered probit,specification tests...|$|R
40|$|A score {{function}} associates {{a number}} to each item response. The function indicates {{the relative importance}} of allocatingmanual resources to review responses, thereby allowing the <b>survey</b> <b>analyst</b> to prioritise editingefforts. By applyingdifferent score functions to the Monthly Inquiry for the Distribution and Services Sector the article discusses how to measure the effectiveness of a score function, and what it is that makes this method effective at reducingmanual editing. One ®ndingis that the effectiveness of the technique is rather insensitive to the type of score function. In many situations it is useful to specify a threshold that splits the responses into two groups where manual editing is directed at responses with scores above the threshold. The article suggests a simple graphical tool that allows the analyst to assess the threshold. Key words: Selective editing; estimate-related; edit-related; threshold; progress graph. 1...|$|E
40|$|PERHAPS {{the most}} {{fundamental}} question in survey research is that posed {{in the title of}} Herbert Hyman's 1944 paper, "Do They Tell the Truth? " Implicit in Hyman's question are issues that should concern every <b>survey</b> <b>analyst.</b> These include the level of inaccuracy in survey data, the characteristics of inaccurate reporters, {{and the extent to which}} inaccuracy is respondent-specific as opposed to item-specific. The earliest large-scale effort to address these issues, the 1949 Denver Community Study, still ranks among the most extensive in-vestigations into the validity of survey data. At the end of the study's field period, record checks were conducted for 14 items asked during the interview. On the first issue, concerning levels of inconsistency between respondent claims and official records, Parry and Crossley (1950) reported that the discrepancies ranged from 2 percent (for an item on home ownership) to 38 percent (for an item on Community Abstract The assumption that reporting errors are uncorrelated across survey item...|$|E
5000|$|While he was {{an analyst}} at Credit Suisse, in the 2003 “Beat on the Street” <b>analyst</b> <b>survey,</b> he earned “top honors.” ...|$|R
50|$|Paraguay {{comprises}} a {{total of}} 40.6 million hectares of land. But based on soil <b>surveys,</b> <b>analysts</b> have estimated that only one-fifth of that area is appropriate for normal crop production. According to the 1981 agricultural census, 7 {{percent of the land}} was dedicated to crop production, 20 percent to forestry, 26 percent to livestock, and 47 percent to other purposes. Theseortant trends in Paraguayan agriculture was the increase in the percentage of land under cultivation, which had been only 2 percent in 1956. Livestock activity fluctuated greatly during the 1970s and 1980s but generally had increased, rising above the 22-percent land use reported in 1956. The improved utilization of agricultural resources resulted from increased colonization, favorable price movements for cash crops, further mechanization, and infrastructural improvements connecting produce with markets.|$|R
5000|$|Best on the Street, {{a similar}} <b>survey</b> of {{financial}} <b>analysts</b> by the Wall Street Journal {{that is used}} to rank the relative performance of the analysts.|$|R
40|$|Weight {{motivation}} {{is that many}} <b>survey</b> data <b>analysts</b> start from a modelling 1. Introduction approach using standard software and it seems desirable to develop means The {{question of how to}} take of taking account of sampling designs account of the sampling design when which adapt this approach naturally. modelling sample survey data has The broad approach will be to fi...|$|R
40|$|Ordered {{rating scales}} {{are one of}} the most {{frequently}} used question formats in large-scale <b>surveys.</b> <b>Analysts</b> of the responses to such questions often find themselves in need of describing the degree of agreement (concentration, consensus) of the answers to such questions. For that purpose they commonly use standard deviations of the response distributions, or measures based on these (such as the coefficient of consensus defined by Granberg and Holmberg, 1988), or the coefficient of variability, etc. This paper demonstrates that such measures are inappropriate for this purpose because they misrepresent what they are supposed to measure: the `peakedness' of a distribution. As an alternative a measure of agreement A is proposed. This measure is a weighted average of the degree of agreement that exists in the simple component parts – layers – into which any frequency distribution can be disaggregated, and for which agreement can be expressed in a straightforward and unequivocal way...|$|R
40|$|Pollsters {{seeking to}} advise {{political}} parties, <b>survey</b> <b>analysts</b> seeking to inform academic research, and journalists seeking {{to convey the}} gist of poll results and other survey findings to their audiences need to summarise their data. One method that has proved attractive, especially in Australia, involves an inspection of the marginal frequencies {{in relation to a}} particular dependent variable (say, "the swinging voter"'), the identification of the modal response for selected independent variables (demographic, psychographic, or any other), and the listing of these 10 create an identity for the "typical" case. This paper describes such moves, analyses the fallacy involved, and traces its real world consequences. For the media, consequences may include not only a massive distortion of the information being reported but, in applying the methods to their own industry, an overly narrow understanding of the backgrounds of the journalists who report the news, and the caricaturing by management of their own target audience. 9 page(s...|$|R
40|$|Service survey {{estimates}} sales on {{a calendar}} month basis. However, some "period reporters " report sales covering either a 4 -week or 5 -week period that rarely coincides with the exact beginning and ending dates of the calendar month. These period reports are adjusted to {{a calendar month}} basis using trading day weights computed {{as part of the}} seasonal adjustment process. This research examines whether the current methodology adequately accounts for the increased holiday sales that occur in November and December. During these months, <b>survey</b> <b>analysts</b> attempt to re-contact large period reporters to convert their original period report to a calendar basis. The companies that have both a period and a calendar report are used in this research to estimate the effects of unadjusted period reports, the current adjustment method and a simple adjustment {{based on the number of}} days contained in the period. The purpose is to measure the error caused by adjusting the period reports and discuss ways to improve measurement of the error...|$|R
30|$|Before the {{beginning}} of the <b>survey,</b> the <b>analysts</b> are introduced to the tool, its features and our motivation to build it. Subsequently, a JSON representation of a synthetic incident as described in Section 5.2 is shown. By using the JSON representation we are able to highlight the main problem with STIX-based intelligence, which is the low readability and accessibility of the format. Afterwards, the participants explore the incident freely and are asked to fill out the questionnaire.|$|R
40|$|The {{statements}} {{contained in}} this report are solely {{those of the authors}} and do not necessarily reflect the views or policies of the Health Care Financing Administration. The awardee assumes responsibility for the accuracy and completeness of the information contained {{in this report}}. iii ACKNOWLEDGMENTS We would like to thank a number of people for their contributions to this report. Kristin LaBounty and Cheryl Young located programs and program staff and carefully scored programs. <b>Survey</b> <b>analysts</b> John Mero, Sharon DeLeon, and Freda Milton tracked down programs and made hundreds of phone calls. Dexter Chu skillfully created a flexible, customized Access database for the Best Practices search. Jennifer Baskwell rapidly and accurately entered programs into the database. Knight Steel helped us with suggestions and comments throughout the study. Jennifer Schore provided insightful comments on earlier drafts of this report. Roy Grisham edited {{an earlier version of this}} report, and Walt Brower and Patricia Ciaccio carefully edited the current one. Cindy McClure expertly word-processed and produced this report under tight time pressure. Catherine Jansto, our project officer at HCFA provided ongoing advice and assistance. Finally, we wish t...|$|R
50|$|Best on the Street, earlier {{known as}} the All Star <b>Analysts</b> <b>Survey</b> is an annual survey-cum-contest of {{financial}} analysts in the United States conducted by the Wall Street Journal. Many financial firms participate in the survey and boast about the results on their websites.|$|R
40|$|As media theorists {{neglect the}} {{temporal}} {{aspects of the}} effect of television and <b>survey</b> <b>analysts</b> probe this medium in generalized fashion, a fundamental consequence of television is overlooked - the effects of a specific program on particular segments of society. Searching for a suitable program from which to explore this problem, I selected Hockey Night in Canada, the twice-weekly telecast of National Hockey League games which has persisted as the top Canadian television program for more than twelve years. [...] My initial hypothesis was that the effect of Hockey Night in Canada on the social and family lives of the white-collar workers in any city (especially St. John's) varied directly with their degree of interest in the program. The effects of the program influence their interpersonal relationships {{to such a degree that}} they are unaware of its total impact. [...] The lack of sociological research in this area left me with few guidelines on how to gather data bearing on my hypothesis. The method of questionnaire survey was ultimately selected as the most workable approach. The enduring problems of a new major professional hockey league and a historic series between Canada and Russia which may have affected viewers' response, I believe, have been successfully controlled...|$|R
40|$|Different {{cultures}} and states use their national definition of 'private household'. In the EU nearly each country has an own definition of household. These definitions {{correspond to the}} cultural and national structures of social life. The differences result in diverse household compositions and unequal sizes across European nations. Comparing household measures over countries <b>survey</b> <b>analysts</b> face several inconveniences. The composition of the surveyed household has direct impact on the respondents answer about the household size. With regard to the sociological variables 'total household income' and 'socio economic status' of the individual household members, {{the composition of the}} household and, therefore, the definition by means of which this composition is determined, is of central importance. In a first step we summarize definitions of household used in national surveys across Europe. Same dwelling, sharing economic resources, common housekeeping and family ties are the main and mostly used criteria. In a second step we discuss the possible combinations of these elements and the strategies of operationalization in social surveys. The third part illustrates the findings. We use ESS, ECHP and administrative micro data from official statistics. The country differences become obvious. Our conclusion is a revised fieldwork instrument measuring household in social surveys that increases data comparability across {{cultures and}} countries. " (author's abstract...|$|R
5000|$|Under CEQA, every {{agency in}} the state [...] "is {{encouraged}} to develop and publish thresholds of significance" [...] against which to compare {{the environmental impacts of}} projects. Such thresholds are to be published for public review and supported by substantial evidence before their adoption. A lead agency will normally consider the environmental impacts of a project to be significant if and only if they exceed established thresholds of significance. According to a 2001 survey, however, few agencies have actually developed thresholds of significance. The <b>survey's</b> <b>analysts</b> wondered, [...] "if most agencies are not developing thresholds and publishing them for public review, then what criteria are they using?". In absence of thresholds of significance developed independently by lead agencies, impact assessments apply the significant criteria detailed in Appendix G, Environmental Checklist, of the CEQA Statutes and Guidelines, which is produced by the California Office of Planning and Research (OPR). Lead agencies can also defer to authority agencies that publish their recommended guidelines for the resources they regulate. Examples of this include the California Department of Conservation has threshold tests for assessing impacts to agricultural resources using the LESA model, and the Bay Area Air Quality Management District has published guidelines for air quality impacts.|$|R
25|$|In April 2011, Netflix was {{expected}} to earn $1.07 {{a share in the}} first quarter of 2011 on revenue of $705.7 million, a huge increase compared to the year-earlier profit of 59¢ on revenue of $493.7 million, according to a <b>survey</b> of 25 <b>analysts</b> polled by FactSet Research.|$|R
40|$|The thesis {{focuses on}} requirements, {{which must be}} {{established}} when business intelligence is chosen, from user point of view. Business intelligence je decision support system {{and it is very}} important, that system must be ease of use and provide expected information for analyst and managers. During the process of choosing the software, they are the one, who provide requirements and expectation of the system. The thesis focuses on mapping those requirements and their importance using <b>survey</b> among <b>analyst</b> and managers, who uses business intelligence or want to use it...|$|R
40|$|The {{majority}} of data sets contain observations {{that do not}} conform to the structure followed {{by the rest of the}} data. These observations, known as outliers, can be found using a multitude of statistical and non-statistical methods. This paper highlights a generalized system built, specifically for price index <b>surveys,</b> which <b>analysts</b> can use to test different outlier detection methods. It also details the underlying theory involved. With the aid of an analytical user interface, analysts can test these statistical outlier detection methods and immediately see results which will help them reach a decision about the method that best suits their survey data...|$|R
5000|$|By early October, it was {{reported}} that the 2015 challenge had raised $500,000 as compared with the $115 million raised by the 2014 challenge. [...] The final figure {{was reported}} by the ALS Association in mid-October as being $1,000,000, with a <b>survey</b> by health <b>analysts</b> Treato showing that only 14% of donors from 2014 donated again in 2015.|$|R
5000|$|In 2004, several Roth Capital Partners {{research}} analysts received {{top honors}} in the Wall Street Journal [...] "Best on the Street Analyst Survey" [...] and the Forbes.com/StarMine- North American <b>Analyst</b> <b>Survey.</b> In addition, the firm raised over $1.6 billion in capital and achieved a #1 ranking {{for both the}} number of PIPE transactions and the total dollars placed.|$|R
40|$|The {{thesis is}} focused on contentment {{determination}} of Czech Republic Fire Brigade members. Basic ideas concerning work motivation and satisfaction are defined in theoretical part of the thesis followed by characterised work-satisfying factors. There are brief characteristics of <b>survey</b> object and <b>analyst</b> of research results in empirical part. In the final part of thesis are presented improvement suggestions for increasing satisfaction with job...|$|R
5000|$|In March 1945, Operation Starvation {{began in}} earnest, using 160 of LeMay's B-29 Superfortress bombers to attack Japan's inner zone. Almost {{half of the}} mines were the US-built Mark 25 model, {{carrying}} 1250 lbs of explosives and weighing about 2,000 lbs. Other mines used included the smaller 1,000 lb Mark 26. Fifteen B-29s were lost while 293 Japanese merchant ships were sunk or damaged. Twelve thousand aerial mines were laid, a significant barrier to Japan's access to outside resources. Prince Fumimaro Konoe said after the war that the aerial mining by B-29s had been [...] "equally {{as effective as the}} B-29 attacks on Japanese industry at the closing stages of the war when all food supplies and critical material were prevented from reaching the Japanese home islands." [...] The United States Strategic Bombing Survey (Pacific War) concluded {{that it would have been}} more efficient to combine the United States's effective anti-shipping submarine effort with land- and carrier-based air power to strike harder against merchant shipping and begin a more extensive aerial mining campaign earlier in the war. <b>Survey</b> <b>analysts</b> projected that this would have starved Japan, forcing an earlier end to the war. After the war, Dr. Johnson looked at the Japan inner zone shipping results, comparing the total economic cost of submarine-delivered mines versus air-dropped mines and found that, though 1 in 12 submarine mines connected with the enemy as opposed to 1 in 21 for aircraft mines, the aerial mining operation was about ten times less expensive per enemy ton sunk.|$|R
5000|$|She {{launched}} and is {{a regular}} contributor to the Cornerstone Journal of Sustainable Finance & Banking (JSFB),published monthly, which offers global perspectives on progress toward sustainable finance, banking and capitalism across regions and industry sectors. The JSFB was ranked as one of the “Most Innovative Research Products” in Extel’s 2014 Independent Research in Responsible Investment <b>Survey.</b> Over 1,000 <b>analysts,</b> portfolio managers and managers from 500 investment firms participate in the survey.|$|R
40|$|Mining SAR (structure-activity relationship) is {{a complex}} problem that is typical of those {{encountered}} mining data. A recent analysis of dopamine antagonist activity produced valuable models of the interactions between antagonists and dopamine receptors. The cascade model was used together with methods recently developed for datascape <b>surveys.</b> Two <b>analysts</b> collaborated in interpreting the rules generated, one of whom {{has had a long}} career in drug design with a pharmaceutical company. This paper briefly introduces the problem, the data source, how attributes were generated, and the tasks involved in rule interpretation. Some important hypotheses are described. We discuss how they are drawn from the rule expressions, what kind of data stimulated the analysts, and the necessary improvements to the method to facilitate the active response of analysts...|$|R
30|$|To {{validate}} our prototypical {{implementation of}} KAVAS {{and to provide}} first evidence of its usability and suitability to support knowledge conversion, we followed a two-fold research approach. An anonymous <b>analyst</b> <b>survey</b> validates the general suitability of the visualization approach for the addressed problem and eliminates usability issues of the interface. The survey is followed by expert interviews to confirm that KAVAS can facilitate knowledge conversions between domain experts and cyber threat intelligence.|$|R
5000|$|Industry {{analysts said}} {{the deal would}} combine Holcim's {{marketing}} strength with Lafarge's edge in innovation, while providing significant cost savings, but cautioned [...] "the road to merger clearance will be a long, complex and uncertain one." [...] Others said the deal could lead to further mergers within the industry and give competitors a chance to pick up assets at a bargain price. [...] Most <b>analysts</b> <b>surveyed</b> by Reuters felt the merger would be approved in the end.|$|R
40|$|Patent search {{tasks are}} {{difficult}} and challenging, often requiring expert patent analysts to spend hours, even days, sourcing relevant information. To aid {{them in this}} process, analysts use Information Retrieval systems and tools to cope with their retrieval tasks. With the growing interest in patent search, {{it is important to}} determine their requirements and expectations of the tools and systems that they employ. In this poster, we report a subset of the findings of a <b>survey</b> of patent <b>analysts</b> conducted to elicit their search requirements...|$|R
40|$|This paper {{evaluates the}} {{performance}} of the macroeconomic forecasts disclosed by three leading international organisations - the IMF, the European Commission and the OECD - and compares it with that of the mean forecasts of two <b>surveys</b> of private <b>analysts</b> - the Consensus Economics and The Economist. The publication of forecasts twice a year by international organisations always receives a great deal of public attention but the timely forecasts disclosed monthly by private institutions have been gaining increased visibility. The aim of this work is to help forecast users in answering the question of how much (little) confidence they should place in the alternative forecasts that are available at each moment. The evaluation covers real GDP growth and inflation projections for nine main advanced economies, over the period 1991 - 2009. Several evaluation criteria are used. The quantitative accuracy of forecasts is assessed and their unbiasedness and efficiency is tested. The directional accuracy of forecasts and the ability to predict economic recessions are also examined. The results suggest that the forecasting performance of the international organisations is broadly {{similar to that of the}} <b>surveys</b> of private <b>analysts.</b> By and large, current-year forecasts present desirable features and clearly outperform year-ahead forecasts for which evidence is more mixed both in terms of quantitative and qualitative accuracy. ...|$|R
40|$|A {{series of}} police {{practices}} and technology make up what today {{is known as}} crime analysis. Crime analysis can broadly {{be defined as the}} use of police knowledge and data to combat and solve crime. The current study seeks to illuminate the current status of crime analysis, and the measures being taken to gain legitimacy and recognition in the field of law enforcement. First, the historical backdrop of technology and police history will be established. Next, three inter-related research projects are used to frame patterns and practices of contemporary crime analysis. The first project examines police organizations’ adoption of community problem analysis. The second explores themes emerging from a list serve used by crime analysts for professional assistance and queries. Third, a <b>survey</b> of <b>analysts</b> from across New York State is used to describe the experience and training needs among contemporary crime analysts. The research findings are used to evaluate crime analysis as an emerging profession and suggest questions and avenues for future research...|$|R
40|$|While {{there is}} a wide {{consensus}} in using survey weights when estimating population parameters, {{it is not clear what}} to do when using survey data for analytic purposes (i. e. with the objective of making inference about model parameters). In the model-based framework (MB), under the hypothesis that the underlying model is correctly specified, using survey weights in regression analysis potentially involves a loss of efficiency. In a design-based perspective (DB), weighted estimates are both design consistent and can provide robustness to model mis-specification. In this paper, I suggest that the choice of using survey weights can be seen in a regression diagnostic set. The <b>survey</b> data <b>analyst</b> should check if the design information included in survey weights has some explanatory power in describing the model outcome. To accomplish this task a set of econometric tests is suggested, that could be supplemented by the analysis of model features under the two strategies. survey methods, model evaluation and testing...|$|R
