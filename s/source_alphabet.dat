67|103|Public
25|$|Justeson and Stephens {{proposed}} that this inherent vowel system in Brahmi and Kharoṣṭhī developed by transmission of a Semitic abjad through the recitation of its letter values. The {{idea is that}} learners of the <b>source</b> <b>alphabet</b> recite the sounds by combining the consonant with an unmarked vowel, e.g. /kə/,/kʰə/,/gə/, {{and in the process}} of borrowing into another language, these syllables are taken to be the sound values of the symbols. They also accepted the idea that Brahmi was based on a North Semitic model.|$|E
2500|$|Note: the [...] in [...] "-ary entropy" [...] is {{the number}} of {{different}} symbols of the ideal alphabet used as a standard yardstick to measure source alphabets. In information theory, two symbols are necessary and sufficient for an alphabet to encode information. Therefore, the default is to let [...] ("binary entropy"). Thus, the entropy of the <b>source</b> <b>alphabet,</b> with its given empiric probability distribution, is a number equal to the number (possibly fractional) of symbols of the [...] "ideal alphabet", with an optimal probability distribution, necessary to encode for each symbol of the <b>source</b> <b>alphabet.</b> Also note that [...] "optimal probability distribution" [...] here means a uniform distribution: a <b>source</b> <b>alphabet</b> with [...] symbols has the highest possible entropy (for an alphabet with [...] symbols) when the probability distribution of the alphabet is uniform. This optimal entropy turns out to be [...]|$|E
2500|$|This maximal entropy of [...] is {{effectively}} attained by a <b>source</b> <b>alphabet</b> having a uniform probability distribution: uncertainty is maximal when all possible events are equiprobable.|$|E
40|$|In this paper, {{approximation}} {{methods for}} binary polar code construction proposed by Tal and Vardy are extended to non-binary <b>source</b> <b>alphabets.</b> Additionally, a new approximation method that enables accurate polar code construction with less usage of computational resources is proposed. Efficiency {{and accuracy of}} proposed methods are supported analytically and numerically. © 2012 IEEE...|$|R
40|$|Novel coding schemes are {{introduced}} and relationships between optimal codes and Huffman codes are discussed. It is shown that, for finite <b>source</b> <b>alphabets,</b> the Huffman coding is the optimal coding, and conversely the optimal coding needs {{not to be}} the Huffman coding. It is also proven that there always exists the optimal coding for infinite <b>source</b> <b>alphabets.</b> We show that for every random variable with a countable infinite set of outcomes and finite entropy there exists an optimal code constructed from optimal codes for truncated versions of the random variable. And the average code word lengths of any sequence of optimal codes for the truncated versions converge to that of the optimal code. Furthermore, a case study of data compression is given. Comparing with the Huffman coding, the optimal coding is a more flexible compression method used not only for statistical modeling but also for dictionary schemes. ...|$|R
40|$|In this paper, {{lossless}} polar {{compression of}} g-ary memoryless {{sources in the}} noiseless setting is investigated. Polar compression scheme for binary memoryless sources, introduced by Cronie and Korada, is generalized to <b>sources</b> over prime-size <b>alphabets.</b> In {{order to reduce the}} average codeword length, a compression scheme based on successive cancellation list decoding is proposed. Also, a specific configuration for the compression of correlated sources is considered, and it is shown that the introduced polar compression schemes achieve the corner point of the admissible rate region. Based on this result, proposed compression schemes are extended to arbitrary finite <b>source</b> <b>alphabets</b> by using a layered approach. © 2013 IEEE...|$|R
2500|$|In {{general the}} -ary entropy of a source [...] with <b>source</b> <b>alphabet</b> [...] and {{discrete}} probability distribution [...] where [...] is {{the probability of}} [...] (say [...] is defined by: ...|$|E
2500|$|A <b>source</b> <b>alphabet</b> with {{non-uniform}} distribution {{will have}} less entropy than if those symbols had uniform distribution (i.e. the [...] "optimized alphabet"). This deficiency in entropy can be expressed as a ratio called efficiency: ...|$|E
2500|$|Choosing , [...] {{this implies}} that the entropy of a certain outcome is zero: [...] This implies that the {{efficiency}} of a <b>source</b> <b>alphabet</b> with [...] symbols can be defined simply as being equal to its -ary entropy. See also Redundancy (information theory).|$|E
40|$|Krichevsky {{has shown}} that the average {{redundancy}} rate of an adaptive block code for memoryless sources is R = n+t t+ 1 + O, where m is a cardinality of <b>source's</b> <b>alphabet,</b> n is a block size, and t is a size of a sample used to construct this code. This led him to a conclusion that using samples with t = n is sucient to make R = O, which is the order of the redundancy rate of block codes for a known source...|$|R
40|$|Abstract- We derive the {{rate-distortion}} {{region for}} the two-channel multiple description problem on stationary discrete ergodic and nonergodic <b>sources</b> with <b>alphabets</b> admitting an ergodic decomposition. The results {{do not provide}} a single-letter representation for the rate-distortion region on i. i. d. sources. I...|$|R
40|$|Existence of {{the optimal}} prefix codes {{is shown in}} this paper. Relationship between the optimal prefix code and the Huffman code is also discussed. We prove that all Huffman codes are optimal prefix codes and {{conversely}} optimal prefix codes need not be Huffman codes. Especially, the problem of whether the optimal prefix code has to be maximal is presented. Although for information <b>source</b> <b>alphabets</b> of being not greater than four letters we show that an optimal prefix code must be maximal, {{it remains to be}} an open problem in general. As seen from Huffman codes, optimal prefix codes are used not only for statistical modeling but also for dictionary methods. Moreover, it is obtained that the complexity of breaking an optimal prefix code is NP-complete from the viewpoint of computational difficulty...|$|R
50|$|In {{information}} theory and computer science, a code is usually considered as an algorithm which uniquely represents symbols from some <b>source</b> <b>alphabet,</b> by encoded strings, {{which may be}} in some other target alphabet. An extension of the code for representing sequences of symbols over the <b>source</b> <b>alphabet</b> is obtained by concatenating the encoded strings.|$|E
5000|$|Note: the [...] in [...] "-ary entropy" [...] is {{the number}} of {{different}} symbols of the ideal alphabet used as a standard yardstick to measure source alphabets. In information theory, two symbols are necessary and sufficient for an alphabet to encode information. Therefore, the default is to let [...] ("binary entropy"). Thus, the entropy of the <b>source</b> <b>alphabet,</b> with its given empiric probability distribution, is a number equal to the number (possibly fractional) of symbols of the [...] "ideal alphabet", with an optimal probability distribution, necessary to encode for each symbol of the <b>source</b> <b>alphabet.</b> Also note that [...] "optimal probability distribution" [...] here means a uniform distribution: a <b>source</b> <b>alphabet</b> with [...] symbols has the highest possible entropy (for an alphabet with [...] symbols) when the probability distribution of the alphabet is uniform. This optimal entropy turns out to be [...]|$|E
5000|$|This maximal entropy of [...] is {{effectively}} attained by a <b>source</b> <b>alphabet</b> having a uniform probability distribution: uncertainty is maximal when all possible events are equiprobable.|$|E
40|$|Ankara : The Department of Electrical and Electronics Engineering and the Graduate School of Engineering and Science of Bilkent University, 2013. Thesis (Master's) [...] Bilkent University, 2013. Includes bibliographical {{references}} leaves 60 - 62. In this study, lossless polar compression schemes {{are proposed}} for finite <b>source</b> <b>alphabets</b> in the noiseless setting. In the first part, lossless polar source coding scheme for binary memoryless sources introduced by Arıkan is extended to general prime-size alphabets. In {{addition to the}} conventional successive cancellation decoding (SC-D), successive cancellation list decoding (SCL-D) is utilized for improved performance at practical block-lengths. For code construction, greedy approximation method for density evolution, proposed by Tal and Vardy, is adapted to non-binary alphabets. In the second part, a variable-length, zero-error polar compression scheme for prime-size alphabets {{based on the work}} of Cronie and Korada is developed. It is shown numerically that this scheme provides rates close to minimum source coding rate at practical block-lengths under SC-D, while achieving the minimum source coding rate asymptotically in the block-length. For improved performance at practical block-lengths, a scheme based on SCL-D is developed. The proposed schemes are generalized to arbitrary finite <b>source</b> <b>alphabets</b> by using a multi-level approach. For practical applications, robustness of the zero-error source coding scheme with respect to uncertainty in source distribution is investigated. Based on this robustness investigation, it is shown that a class of prebuilt information sets can be used at practical block-lengths instead of constructing a specific information set for every source distribution. Since the compression schemes proposed in this thesis are not universal, probability distribution of a source must be known at the receiver for reconstruction. In the presence of source uncertainty, this requires the transmitter to inform the receiver about the source distribution. As a solution to this problem, a sequential quantization with scaling algorithm is proposed to transmit the probability distribution of the source together with the compressed word in an efficient way. Çaycı, SemihM. S...|$|R
40|$|Abstract—The {{foliation}} of {{a sphere}} {{in an even}} number of dimensions by flat tori {{can be used to}} construct discrete spherical codes and also homogeneous curves for transmitting a continuous <b>alphabet</b> <b>source</b> over an AWGN channel. In both cases the performance of the code is related to the packing density of specific lattices and their orthogonal sublattices. In the continuous case the packing density of curves relies also on the search for projection lattices with good packing density. We present here a survey of this topic including some recent research results and perspectives. Keywords—Spherical codes, group codes, flat torus, lattices, Gaussian channel, codes on graphs, continuous <b>alphabet</b> <b>source.</b> I...|$|R
3000|$|The {{considered}} system {{consists of}} a finite <b>alphabet</b> <b>source,</b> a classical arithmetic encoder, an Additive White Gaussian Noise (AWGN) channel and an arithmetic decoder. The source generates packets of L symbols s = (s 1,..., s [...]...|$|R
5000|$|In {{general the}} -ary entropy of a source [...] with <b>source</b> <b>alphabet</b> [...] and {{discrete}} probability distribution [...] where [...] is {{the probability of}} [...] (say [...] is defined by: ...|$|E
5000|$|A <b>source</b> <b>alphabet</b> with {{non-uniform}} distribution {{will have}} less entropy than if those symbols had uniform distribution (i.e. the [...] "optimized alphabet"). This deficiency in entropy can be expressed as a ratio called efficiency: ...|$|E
5000|$|Choosing , [...] {{this implies}} that the entropy of a certain outcome is zero: [...] This implies that the {{efficiency}} of a <b>source</b> <b>alphabet</b> with [...] symbols can be defined simply as being equal to its -ary entropy. See also Redundancy (information theory).|$|E
50|$|The Blahut-Arimoto algorithm, co-invented by Richard Blahut, is {{an elegant}} {{iterative}} technique for numerically obtaining rate-distortion functions of arbitrary finite input/output <b>alphabet</b> <b>sources</b> and much {{work has been}} done to extend it to more general problem instances.|$|R
40|$|Large <b>alphabet</b> <b>source</b> coding is a {{basic and}} well-studied problem in data compression. It has many {{applications}} such as compression of natural language text, speech and images. The classic perception of most commonly used methods is that a source is best described over an alphabet which {{is at least as}} large as the observed alphabet. In this work we challenge this approach and introduce a conceptual framework in which a large <b>alphabet</b> <b>source</b> is decomposed into "as statistically independent as possible" components. This decomposition allows us to apply entropy encoding to each component separately, while benefiting from their reduced alphabet size. We show that in many cases, such decomposition results in a sum of marginal entropies which is only slightly greater than the entropy of the source. Our suggested algorithm, based on a generalization of the Binary Independent Component Analysis, is applicable for a variety of large <b>alphabet</b> <b>source</b> coding setups. This includes the classical lossless compression, universal compression and high-dimensional vector quantization. In each of these setups, our suggested approach outperforms most commonly used methods. Moreover, our proposed framework is significantly easier to implement in most of these cases...|$|R
50|$|In {{information}} theory, {{given an}} unknown stationary <b>source</b> &pi; with <b>alphabet</b> A and a sample w from &pi;, the Krichevsky-Trofimov (KT) estimator produces an estimate &pi;i(w) of the probabilities of each symbol i &isin; A. This estimator is optimal {{in the sense}} that it minimizes the worst-case regret asymptotically.|$|R
5000|$|When {{the source}} {{language}} uses a fairly phonetic spelling system, a Cyrillization scheme may often be adopted that almost {{amounts to a}} transliteration, i.e. using a mapping scheme that simply maps each letter of the <b>source</b> <b>alphabet</b> to some letter of the destination alphabet, sometimes augmented by position-based rules. Among such schemes are several schemes universally accepted in Eastern Slavic languages: ...|$|E
5000|$|Before {{giving a}} mathematically precise definition, {{this is a}} brief example. The mapping is a code, whose <b>source</b> <b>{{alphabet}}</b> is the set [...] and whose target alphabet is the set [...] Using {{the extension of the}} code, the encoded string 0011001011 can be grouped into codewords as 0 011 0 01 011, and these in turn can be decoded to the sequence of source symbols acabc.|$|E
50|$|Justeson and Stephens {{proposed}} that this inherent vowel system in Brahmi and Kharosthi developed by transmission of a Semitic consonantal alphabet through the recitation of its letter values. The {{idea is that}} learners of the <b>source</b> <b>alphabet</b> recite the sounds by combining the consonant with an unmarked vowel, e.g. /kə/,/kʰə/,/gə/..., {{and in the process}} of borrowing into another language, these syllables are taken to be the sound values of the symbols. They also accepted the idea that Brahmi was based on a North Semitic model.|$|E
50|$|For {{the case}} of channel capacity, the {{algorithm}} was independently invented by Arimoto and Blahut. In {{the case of}} lossy compression, the corresponding algorithm was invented by Richard Blahut. The algorithm is most applicable {{to the case of}} arbitrary finite <b>alphabet</b> <b>sources.</b> Much work has been done to extend it to more general problem instances.|$|R
40|$|In {{this paper}} we provide {{sufficient}} conditions for lossy transmission of functions of correlated data over a multiple access channel (MAC). The conditions obtained {{can be shown}} as generalized version of Yamamoto’s result [28]. We also obtain efficient joint source-channel coding schemes for transmission of discrete and continuous <b>alphabet</b> <b>sources</b> to recover the function values...|$|R
40|$|We {{consider}} the classical multiterminal source coding problem subject to distortion constraints computed using the logarithmic loss distortion measure. We provide a single-letter {{description of the}} achievable rate distortion region for arbitrarily correlated <b>sources</b> with finite <b>alphabets.</b> In doing so, we also give the rate distortion region for the m-encoder CEO problem. Several applications and examples are given. ...|$|R
30|$|The {{benefit of}} Gaussian-distributed <b>source</b> <b>alphabet</b> for various {{communication}} channels is a well-studied information theoretic topic.|$|E
40|$|The Huffman coding {{algorithm}} is interpreted in the lattice of partitions of the <b>source</b> <b>alphabet.</b> Maximal chains in the partition lattice correspond to linear extensions of tree orders, and those among the chains that exhibit a simple greedy property correspond precisely to executions of the Huffman algorithm...|$|E
40|$|I present new {{algorithms}} for fixed-rate multiple {{description and}} multiresolution scalar quantizer design. The algorithms both run in time polynomial {{in the size}} of the <b>source</b> <b>alphabet</b> and guarantee globally optimal solutions. To the author’s knowledge, these are the first globally optimal design algorithms for multiple description and multiresolution quantizers...|$|E
40|$|We {{consider}} {{the problem of}} transmission of correlated discrete <b>alphabet</b> <b>sources</b> over a Gaussian Multiple Access Channel (GMAC). A distributed bit-to-Gaussian mapping is proposed which yields jointly Gaussian codewords. This can guarantee lossless transmission or lossy transmission with given distortions, if possible. The technique can be extended to the system with side information at the encoders and decoder...|$|R
40|$|Transliteration is {{the process}} of {{converting}} words from a given <b>source</b> language <b>alphabet</b> to a target language alphabet, in a way that best preserves the phonetic and orthographic aspects of the transliterated words. Even though an important effort has been made towards improving this process for many languages such as English, French and Chinese, little research work has been accomplished with regard to the Arabic language. In this work, an attention-based encoder-decoder system is proposed for the task of Machine Transliteration between the Arabic and English languages. Our experiments proved the efficiency of our proposal approach in comparison to some previous research developed in this area...|$|R
40|$|Abstract—We {{consider}} the two-encoder multiterminal source coding problem subject to distortion constraints computed under logarithmic loss. We provide a single-letter {{description of the}} achievable rate distortion region for arbitrarily correlated <b>sources</b> with finite <b>alphabets.</b> In doing so, we also give the rate distortion region for the CEO problem under logarithmic loss. Notably, the Berger-Tung inner bound is tight in both settings. I...|$|R
