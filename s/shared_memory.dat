7886|2133|Public
5|$|Concurrent {{programming}} languages, libraries, APIs, {{and parallel}} programming models (such as algorithmic skeletons) {{have been created}} for programming parallel computers. These can generally be divided into classes based on the assumptions they make about the underlying memory architecture—shared memory, distributed memory, or shared distributed memory. <b>Shared</b> <b>memory</b> programming languages communicate by manipulating <b>shared</b> <b>memory</b> variables. Distributed memory uses message passing. POSIX Threads and OpenMP {{are two of the}} most widely used <b>shared</b> <b>memory</b> APIs, whereas Message Passing Interface (MPI) is the most widely used message-passing system API. One concept used in programming parallel programs is the future concept, where one part of a program promises to deliver a required datum to another part of a program at some future time.|$|E
5|$|Main {{memory in}} a {{parallel}} computer is either <b>shared</b> <b>memory</b> (shared between all processing elements in a single address space), or distributed memory (in which each processing element has its own local address space). Distributed memory refers {{to the fact that}} the memory is logically distributed, but often implies that it is physically distributed as well. Distributed <b>shared</b> <b>memory</b> and memory virtualization combine the two approaches, where the processing element has its own local memory and access to the memory on non-local processors. Accesses to local memory are typically faster than accesses to non-local memory.|$|E
5|$|Computer {{architectures}} {{in which}} each element of main memory can be accessed with equal latency and bandwidth are known as uniform memory access (UMA) systems. Typically, {{that can be achieved}} only by a <b>shared</b> <b>memory</b> system, in which the memory is not physically distributed. A system that does not have this property is known as a non-uniform memory access (NUMA) architecture. Distributed memory systems have non-uniform memory access.|$|E
40|$|Gordon Elliot is {{interviewed about}} his life in Heart's Content. Gordon Elliott shares his {{experience}} of spending a Christmas in Heart's Content, {{one of his first}} experiences on a train; snowed-in between Carbonear and Heart's Content at the Government's "Halfway Point;" <b>shares</b> <b>memories</b> of Heart's Content as it appeared in 1911; compares Christmas traditions in England and Heart's Content; <b>shares</b> <b>memories</b> of Christmas mummering in Whitebourne; shares the story of 12 -day train journey from Northern Bite, Clarenville to Briggus Junction in 1912; was on the train with William Coaker; Elliot was posted in Trinity Bay at the time; <b>shares</b> <b>memories</b> of living in Rose Blanche and unexplained phenomenon in 1913; <b>shares</b> <b>memories</b> of Christmas on Change Islands; mummers; memories of St. John's in 1908; discussion of the Sir. Wilfred Grenfell and the mission. Recording ends...|$|R
40|$|Hiram Silk's series Looking Back episode {{features}} {{interviews with}} Eleanor Peyton, Edna MacClouter and Hannah Pillips. Interview with Eleanor Peyton, 86 years old, about her memories of Georgina Stirling, the opera singer; Georgina Stirling's famous sisters and family; {{her home and}} furnishings; hobbies; pets and her death in 1935. Eleanor Peyton <b>shares</b> <b>memories</b> of Christmas {{as a child in}} Twillingate; mummering; traditions; tea, food, games and gifts; Christmas Eve traditions.; Norma May Clouter <b>shares</b> <b>memories</b> of the twelve days of Christmas mummering; Christmas Eve dinner; Hannah Phllips <b>shares</b> <b>memories</b> of Christmas as a child in Salvage, Bonavista Bay...|$|R
2500|$|Kinniburgh, Moira and Burke, Fiona (1995). Kilbirnie and Glengarnock. <b>Shared</b> <b>Memories.</b> Kilbirnie Library[...]|$|R
5|$|Nevertheless, {{there are}} many {{scenarios}} where customized allocators are desirable. Some {{of the most common}} reasons for writing custom allocators include improving performance of allocations by using memory pools, and encapsulating access to different types of memory, like <b>shared</b> <b>memory</b> or garbage-collected memory. In particular, programs with many frequent allocations of small amounts of memory may benefit greatly from specialized allocators, both in terms of running time and memory footprint.|$|E
5|$|Computer systems {{make use}} of caches—small and fast {{memories}} located close to the processor which store temporary copies of memory values (nearby in both the physical and logical sense). Parallel computer systems have difficulties with caches that may store the same value {{in more than one}} location, with the possibility of incorrect program execution. These computers require a cache coherency system, which keeps track of cached values and strategically purges them, thus ensuring correct program execution. Bus snooping {{is one of the most}} common methods for keeping track of which values are being accessed (and thus should be purged). Designing large, high-performance cache coherence systems is a very difficult problem in computer architecture. As a result, <b>shared</b> <b>memory</b> computer architectures do not scale as well as distributed memory systems do.|$|E
25|$|The <b>Shared</b> <b>Memory</b> Net-Lib, on {{the other}} hand, manages {{connections}} between multiple instances of SQL Server that exist on one computer. It uses a <b>shared</b> <b>memory</b> area to communicate between the processes. This is inherently secure; {{there is no need}} for data encryption between instances of SQL Server that exist on one computer as the operating system does not allow any other process access to the instances' area of <b>shared</b> <b>memory.</b>|$|E
5000|$|Chrome is {{the leader}} of the {{artificial}} soldiers who <b>shared</b> <b>memories</b> with Chalce.|$|R
5000|$|Rust—for system programming, using message-passing with move semantics, <b>shared</b> {{immutable}} <b>memory,</b> and <b>shared</b> mutable <b>memory.</b>|$|R
5000|$|Strong spatial isolation: all {{partitions}} are {{executed in}} processor user mode, {{and do not}} <b>share</b> <b>memory.</b>|$|R
25|$|During the 1980s, as {{the demand}} for {{computing}} power increased, the trend to a much larger number of processors began, ushering {{in the age of}} massively parallel systems, with distributed memory and distributed file systems, given that <b>shared</b> <b>memory</b> architectures could not scale to a large number of processors. Hybrid approaches such as distributed <b>shared</b> <b>memory</b> also appeared after the early systems.|$|E
25|$|Moreover, a {{parallel}} algorithm {{can be implemented}} either in {{a parallel}} system (using <b>shared</b> <b>memory)</b> or in a distributed system (using message passing). The traditional boundary between parallel and distributed algorithms (choose a suitable network vs. run in any given network) does not lie {{in the same place}} as the boundary between parallel and distributed systems (<b>shared</b> <b>memory</b> vs. message passing).|$|E
25|$|In {{parallel}} computing, all processors {{may have}} access to a <b>shared</b> <b>memory</b> to exchange information between processors.|$|E
50|$|Hyung Park {{states that}} the nation is {{continuously}} revived, re-imagined, reconstituted, through <b>shared</b> <b>memories</b> among its citizens.|$|R
5000|$|Zuzu and Moshe <b>share</b> <b>memories</b> of toys {{they used}} to have; combing your hair can be fun (2001) ...|$|R
5000|$|... {{aim to be}} part of {{information}} ecology of personal and <b>shared</b> <b>memories</b> and tools rather than discrete standalone services ...|$|R
25|$|All {{processors}} {{have access}} to a <b>shared</b> <b>memory.</b> The algorithm designer chooses the program executed by each processor.|$|E
25|$|The {{other main}} issue was that early Macs lack a memory {{management}} unit (MMU), which precludes the possibility of several fundamental modern features. An MMU would provide memory protection to ensure that programs cannot accidentally overwrite other program's memory, and it would provision <b>shared</b> <b>memory.</b> Lacking <b>shared</b> <b>memory,</b> the API was instead written so the operating system and application shares all memory, which is what allows QuickDraw to examine the application's memory for settings like the line drawing mode or color.|$|E
25|$|One {{theoretical}} {{model is the}} parallel random access machines (PRAM) that are used. However, the classical PRAM model assumes synchronous access to the <b>shared</b> <b>memory.</b>|$|E
50|$|In {{the reunion}} show, the contestants look back, {{becoming}} emotional and even violent as they <b>share</b> <b>memories</b> of the experience.|$|R
5000|$|Olivera, Mercedes. (2009, October 17). Little Mexico photo exhibit {{given more}} time to <b>share</b> <b>memories.</b> The Dallas Morning News. http://www.dallasnews.com/sharedcontent/dws/dn/localnews/columnists/molivera /stories/DNolivera_17met.ART.State.Edition1.4c1d8f2.html ...|$|R
5000|$|Tauba Biterman (1918 - [...] ) - is a Holocaust {{survivor}} that dedicated {{her adult}} life {{to teaching and}} <b>sharing</b> <b>memories</b> of the Holocaust.|$|R
25|$|Other early {{supercomputers}} {{such as the}} Cray 1 and Cray 2 {{that appeared}} afterwards used {{a small number of}} fast processors that worked in harmony and were uniformly connected to the largest amount of <b>shared</b> <b>memory</b> that could be managed at the time.|$|E
25|$|The kernel and {{scheduling}} is distributed across the SPEs. Tasks are synchronized using mutexes or semaphores as {{in a conventional}} operating system. Ready-to-run tasks wait in a queue for an SPE to execute them. The SPEs use <b>shared</b> <b>memory</b> for all tasks in this configuration.|$|E
25|$|A {{model that}} {{is closer to}} the {{behavior}} of real-world multiprocessor machines and takes into account the use of machine instructions, such as Compare-and-swap (CAS), is that of asynchronous <b>shared</b> <b>memory.</b> There is a wide body of work on this model, a summary of which {{can be found in the}} literature.|$|E
50|$|There are buffer {{classes for}} all of Java's {{primitive}} types except , which can <b>share</b> <b>memory</b> with byte buffers and allow arbitrary interpretation of the underlying bytes.|$|R
40|$|This thesis {{addresses}} theoretical {{aspects of}} defining, implementing and programming with consistency conditions for distributed <b>shared</b> <b>memories.</b> We start by presenting {{a framework for}} defining consistency conditions from the programmer's point of view. We then show that weak conditions are too weak to solve some of the fundamental problems of concurrent programming. This provides a motivation for hybrid conditions. Following this, we present a formal definition of hybrid consistency, capturing the main features of several hybrid conditions. In these conditions, operations are classified as either weak or strong. Weak operations provide a semantics similar to weak conditions and can be implemented efficiently. Strong operations, on the other hand, provide a semantics similar to strong conditions which is necessary to maintain the expressiveness of real <b>shared</b> <b>memories</b> in distributed <b>shared</b> <b>memories.</b> Next, we develop two algorithms for implementing hybrid consistency in a message based s [...] ...|$|R
50|$|He meets Nanami Konoe, {{who lives}} next door. As time passes, their <b>shared</b> <b>memories</b> have accumulated. They {{think of their}} normal lives and {{relationship}} as precious things.|$|R
25|$|Since {{the late}} 1960s {{the growth in}} the power and {{proliferation}} of supercomputers has been dramatic, and the underlying architectural directions of these systems have taken significant turns. While the early supercomputers relied on a small number of closely connected processors that accessed <b>shared</b> <b>memory,</b> the supercomputers of the 21st century use over 100,000 processors connected by fast networks.|$|E
25|$|Since these {{formulas}} are matrix {{operations with}} dominant Level 3 operations, they {{are suitable for}} efficient implementation using software packages such as LAPACK (on serial and <b>shared</b> <b>memory</b> computers) and ScaLAPACK (on distributed memory computers). Instead of computing the inverse of a matrix and multiplying by it, it is much better (several times cheaper and also more accurate) to compute the Cholesky decomposition of the matrix and treat the multiplication by the inverse as solution of a linear system with many simultaneous right-hand sides.|$|E
25|$|As {{the number}} of {{processors}} increases, efficient interprocessor communication and synchronization on a supercomputer becomes a challenge. A number of approaches {{may be used to}} achieve this goal. For instance, in the early 1980s, in the Cray X-MP system, shared registers were used. In this approach, all processors had access to shared registers that did not move data back and forth but were only used for interprocessor communication and synchronization. However, inherent challenges in managing a large amount of <b>shared</b> <b>memory</b> among many processors resulted in a move to more distributed architectures.|$|E
50|$|In 2017, Humphreys {{was interviewed}} for The Bill Podcast, <b>sharing</b> <b>memories</b> from his time making The Bill {{as well as}} stories about his life and career in general.|$|R
50|$|The {{site was}} relaunched in March 2012, with the focus {{shifting}} from reuniting with school friends {{to being a}} place where people collect and <b>share</b> <b>memories</b> of the past.|$|R
40|$|Modern network {{processors}} (NPs) typically {{resemble a}} highly-multithreaded multiprocessor-on-a-chip, supporting {{a wide variety}} of mechanisms for on-chip storage and inter-task communication. NP applications are themselves composed of many threads that <b>share</b> <b>memory</b> and other resources, and synchronize and communicate frequently. In contrast, studies of new NP architectures and fea-tures are often performed by benchmarking a simulation model of the new NP using independent kernel programs that neither communicate nor <b>share</b> <b>memory.</b> In this paper we present a NP sim-ulation infrastructure that (i) uses realistic NP applications that are multithreaded, <b>share</b> <b>memory,</b> synchronize, and communicate; and (ii) automatically maps these applications to a variety of NP architectures and features. We use our infrastructure to evaluate threading and scaling, on-chip storage and communication, and to suggest future techniques for automated compilation for NPs. iii Acknowledgements First, I would like to thank my advisor, Gregory Steffan, for his con-structive comments throughout this project and especially for his pa...|$|R
