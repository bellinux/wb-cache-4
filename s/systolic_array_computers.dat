1|1062|Public
50|$|The WW-Warp and PC-Warp {{machines}} were <b>systolic</b> <b>array</b> <b>computers</b> with a linear array of ten or more cells, {{each of which}} is a programmable processor capable of performing 10 million single precision floating-point operations per second (10 MFLOPS). A 10-cell machine had a peak performance of 100 MFLOPS. The iWarp machines doubled this performance, delivering 20 MFLOPS single precision and supporting double precision floating point at half the performance.|$|E
40|$|Abstract: 2 ̆ 2 We {{describe}} a fast-propagation algorithm for a linear array of processors. Results of an {{implementation of this}} algorithm on Warp, a ten processor, programmable <b>systolic</b> <b>array</b> <b>computer,</b> are reviewed and compared with back-propagation implementations on other machines. Our current Warp simulator is about 8 times faster at simulating the NETtalk text-to-speech network than the fastest back-propagation simulator reported in the literature. This fast simulator on Warp is being used routinely in a road recognition experiment for robot navigation at Carnegie Mellon. Our results indicate that linear <b>systolic</b> <b>array</b> machines can be efficient neural network simulators. Planned extensions and improvements to our current algorithm are discussed. 2 ̆...|$|R
40|$|The Warp {{machine is}} a <b>systolic</b> <b>array</b> <b>computer</b> of linearly {{connected}} cells, {{each of which}} is a programmable processor capable of performing 10 million floating-point operations per second (10 MFLOPS). A typical Warp array includes 10 cells, thus having a peak computation rate of 100 MFLOPS. The Warp array can be extended to include more cells to accommodate applications capable of using the increased computational bandwidth. Warp is integrated as an attached processor into a UN host system. Programs for Warp are written in a high-level language supported by an optimizing compiler...|$|R
40|$|Abstract [...] This paper {{presents}} {{the design of}} a new bit-serial floating-point unit (FPU). It has been developed for the processors of the Instruction <b>Systolic</b> <b>Array</b> parallel <b>computer</b> model. In contrast to conventional bit-parallel FPUs the bit-serial approach requires different data formats. Our FPU uses an IEEE compliant internal floating point format that allows a fast least significant bit (LSB) -first arithmetic and can be efficiently implemented in hardware...|$|R
40|$|A {{high-performance}} systolic army computer called Warp {{has been}} designed and mnsuucted. The machine ha a synolic prry of 10 or more llncarly connected cells, {{each of which is}} a proptammable processor capable of performing 10 million floating-point operations per second (10 IMFLGPS). A lb-cell machine therefore ha a peak performance of 100 MFIBPS. Warp is integrated into a UNlX host system. Program develop-ment is supported by a compiler. llie fmt 10 -d machine became operational in 1986. Low-level vision processing for robot vehicles {{is one of the first}} applicatiotts of the machino This paper dcacribes the architecture and implementation of the Warp machine. and justiile-s and evaluates ome of the amhita? anal features with systan, software and applimtion considaa-thn 1. Int reduction Warp is a high-performance <b>systolic</b> <b>array</b> <b>computer</b> designed to provide computation power for signal, image and low-level vi& ptocmsing: the machine’s first applimtions are vision-based con-trol algorithms for robot vehicles, and image atul@s for large imege databases [3]. A full-aule Warp machine consists of a linear <b>systolic</b> <b>array</b> of 10 or more identical cells. each of which is a 10 MFLOPS programmable processor. The proccsor atnty is integrated in a powerful host system, which provides an adequate dam bandwidth to sustain the amty at full speed in the targeted applications, and a general purpose computing environmatt, cpefdlcally tmx. for application programa IIIS rcwarch aa rqpportd in part by Dafw Mvanccd Reaacb Fmjccu &cncy fDOl>l. monitored by tic Air Force Avionlm ti...|$|R
40|$|The {{use of a}} {{programming}} model {{which extends}} naturally from the underlying hardware, greatly eases the design and implementation of simulators, especially for those systems that resemble the hardware in the paradigm of computation. Given the characteristics of <b>systolic</b> <b>arrays,</b> SIMD <b>computers</b> which employ the data parallel programming model provide an ideal environment. In this paper, we present a <b>systolic</b> <b>array</b> simulator, a simulation tool written for the Connection Machine *(model CM 2), a SIMD machine with powerful interprocessor communication capabilities. Especially as recent advances have automated the design, {{there is a need}} for a verification environment to prototype <b>systolic</b> <b>arrays.</b> Primarily a simulation tool, the <b>systolic</b> <b>array</b> simulator also helps identify inefficiencies and motivates optimal design prior to implementation in either custom VLSI or DSP systems. Currently, we are updating the tool to allow the simulation of dynamic array reconfiguration algorithms under transient and permanent fault conditions. The simulator is also being ported to the CM 5. National Science FoundationKeck FoundationThinking Machines CorporationArmy Research OfficeUniv. of Minnesota Army High Performance Computing Research Cente...|$|R
40|$|The goal of {{this project}} was the {{feasibility}} study of a particular architecture of a digital signal processing machine operating in real time which could do in a pipeline fashion the computation of the fast Fourier transform (FFT) of a time-domain sampled complex digital data stream. The particular architecture makes use of simple identical processors (called inner product processors) in a linear organization called a <b>systolic</b> <b>array.</b> Through <b>computer</b> simulation the new architecture to compute the FFT with <b>systolic</b> <b>arrays</b> was proved to be viable, and computed the FFT correctly and with the predicted particulars of operation. Integrated circuits to compute the operations expected of the vital node of the systolic architecture were proven feasible, and even with a 2 micron VLSI technology can execute the required operations in the required time. Actual construction of the integrated circuits was successful in one variant (fixed point) and unsuccessful in the other (floating point) ...|$|R
40|$|This paper {{presents}} {{the design of}} a new multiplier architecture for normal integer multiplication of positive and negative numbers. It has been developed to increase the performance of algorithms for cryptographic and signal processing applications on implementations of the Instruction <b>Systolic</b> <b>Array</b> (ISA) parallel <b>computer</b> model [6, 7]. The multiplier operates least significant bit (LSB) -first. It is a modular bit-serial design which on the one hand can be efficiently implemented in hardware {{and on the other hand}} has the advantage that it can handle operands of arbitrary length...|$|R
40|$|Abstract-The Warp {{machine is}} a <b>systolic</b> <b>array</b> <b>computer</b> of {{accessed}} by a procedure {{call on the}} host, or through an linearly connected cells, {{each of which is}} a programmable interactive, programmable command interpreter called the processor capable of performing 10 million floating-point opera- Warp shell [8]. A high-level language called W 2 is used to tions per second (10 MFLOPS). A typical Warp array includes ten cells, thus having a peak computation rate of 100 MFLOPS. program Warp; the language is supported by an optimizing The Warp array can be extended to include more cells to compiler [121, [23] accommodate applications capable of using the increased compu- The Warp project started in 1984. A two-cell system was tational bandwidth. Warp is integrated as an attached processor completed in June 1985 at Carnegie Mellon. Construction of into a Unix host system. Programs for Warp are written in a two identical ten-cell prototype machines was contracted to high-level language supported by an optimizing compiler. The first ten-cell prototype was completed in February 1986 two industrial partners, GE and Honeywell. These prototypes delivery of production machines started in April 1987. Extensive were built from off-the-shelf parts on wire-wrapped boards. experimentation with both the prototype and production ma- The first prototype machine was delivered by GE in February chines has demonstrated that the Warp architecture is effective in 1986, and the Honeywell machine arrived at Carnegie Mellon the application domain of robot navigation as well as in other in June 1986. For a period of about a year starting from earl...|$|R
40|$|In {{this paper}} we are {{concerned}} with Dynamic Programming (DP) algorithms whose solution is given by a recurrence relation similar to that for the matrix parenthesization problem. Guibas, Kung and Thompson presented a <b>systolic</b> <b>array</b> algorithm for this problem that uses O(n 2) processing cells and solves the problem in O(n) time. In this paper, we present three different mappings of this systolic algorithm on a mesh connected parallel computer. The first two mappings use commonly known techniques for mapping <b>systolic</b> <b>arrays</b> to mesh <b>computers.</b> Both of them are able to obtain {{only a fraction of}} maximum possible performance. The primary reason for the poor performance of these formulations is that different nodes at different levels in the multistage graph in the DP formulation require different amounts of computation. Any adaptation has to take this into consideration and evenly distribute the work among the processors. Our third mapping balances the work load among processors and thus [...] ...|$|R
40|$|Abstract—Systolic arrays were orginally {{designed}} for per-mitting multiple computations for each memory access and thus speed up computing operations without increasing I/O requirements. Since its introduction, as people use and research it, {{a lot more}} meanings in the VLSI design area have been attached to it. The special purpose computers are no longer inflexible and unextendable, their design process {{become more and more}} automative. This report is divided into following parts: The introduction section talks about systolic arrays’s born reason, its utility and history, application. The second section an example to introduce <b>systolic</b> <b>arrays</b> was shown, then systolic arrays’s characteristics, what makes a <b>systolic</b> <b>array</b> are talked about. In the third section, I talked about the design philosophy(or methodlogy) of <b>systolic</b> <b>array,</b> talks about its advantages and disadvantages compare to traditional architecture. In the fourth section, I talked about programmable systolic architecture. In the end, a conclusion for the report was made. Index Terms—systolic <b>arrays,</b> <b>computer</b> architecture, unm ece 53...|$|R
40|$|We {{describe}} new architectures for {{the efficient}} computation of redundant manipulator kinematics (direct and inverse). By calculating {{the core of}} the problem in hardware, we can make full use of the redundancy by implementing more complex self-motion algorithms. A key component of our architecture is the calculation in the VLSI hardward of the Singular Value Decomposition of the manipulator Jacobian. Recent advances in VLSI have allowed the mapping of complex algorithms to hardware using <b>systolic</b> <b>arrays</b> with advanced <b>computer</b> arithmetic algorithms, such as the coordinate rotation (CORDIC) algorithms. We use CORDIC arithmetic in the novel design of our special-purpose VLSI array, which is used in computation of the Direct Kinematics Solution (DKS), the manipulator Jacobian, as well as the Jacobian Pseudoinverse. Application-specific (subtask-dependent) portions of the inverse kinematics are handled in parallel by a DSP processor which interfaces with the custom hardware and the host machine. The architecture and algorithm development is valid for general redundant manipulators and a wide range of processors currently available and under development commercially. National Science FoundationSandia National LaboratoryTexas Instrument...|$|R
40|$|We {{propose a}} <b>systolic</b> <b>array</b> for dynamic {{programming}} {{which is a}} technique for solving combinatorial optimization problems. We derive a <b>systolic</b> <b>array</b> for single-source shortest path problem, SA-SSSP, and then show that the <b>systolic</b> <b>array</b> serves as dynamic programming <b>systolic</b> <b>array</b> which is applicable to any dynamic programming problem by developing a <b>systolic</b> <b>array</b> for 0 - 1 knapsack problem, SA- 01 KS, with SA-SSSP for a basis...|$|R
40|$|Abstract- High-performance {{computation}} {{on a large}} {{array of}} cells {{has been an important}} feature of <b>systolic</b> <b>array.</b> To achieve even higher degree of concurrency, it is desirable to make cells of <b>systolic</b> <b>array</b> themselves <b>systolic</b> <b>array</b> as well. The architecture of <b>systolic</b> <b>array</b> with its cells consisting of another <b>systolic</b> <b>array</b> is to be called super-systolic array. In this paper we propose a scalable super-systolic array architecture which shows high-performance and can be adopted in the VLSI design including regular interconnection and functional primitives that are typical for a systolic architecture. I...|$|R
40|$|A {{systematic}} approach is presented for designing <b>systolic</b> <b>arrays</b> and their equivalent configurations for certain general classes of recursively formulated algorithms. A new method is also introduced {{to reduce the}} input bandwidth and storage requirements of the <b>systolic</b> <b>arrays</b> through the study of dependence among the input data. Many well known <b>systolic</b> <b>arrays</b> can be rederived and also many new <b>systolic</b> <b>arrays</b> can be discovered by this approach...|$|R
5000|$|<b>Systolic</b> <b>arrays</b> are {{arrays of}} DPUs which are {{connected}} to {{a small number of}} nearest neighbour DPUs in a mesh-like topology. DPUs perform a sequence of operations on data that flows between them. Because the traditional <b>systolic</b> <b>array</b> synthesis methods have been practiced by algebraic algorithms, only uniform arrays with only linear pipes can be obtained, so that the architectures are the same in all DPUs. The consequence is, that only applications with regular data dependencies can be implemented on classical <b>systolic</b> <b>arrays.</b> Like SIMD machines, clocked <b>systolic</b> <b>arrays</b> compute in [...] "lock-step" [...] with each processor undertaking alternate compute | communicatephases. But <b>systolic</b> <b>arrays</b> with asynchronous handshake between DPUs are called wavefront arrays.One well-known <b>systolic</b> <b>array</b> is Carnegie Mellon University's iWarp processor, which has been manufactured by Intel. An iWarp system has a linear array processor connected by data buses going in both directions.|$|R
40|$|A Doctoral Thesis. Submitted in partial {{fulfilment}} of {{the requirements}} for the award of Doctor of Philosophy at Loughborough University. The <b>systolic</b> <b>array</b> research was pioneered by H. T. Kung and C. E. Leiserson. <b>Systolic</b> <b>arrays</b> are special purpose synchronous architectures consisting of simple, regular and modular processors which are regularly interconnected to form an <b>array.</b> <b>Systolic</b> <b>arrays</b> are well suited for computational bound problems in Linear Algebra. In this thesis, the numerical problems, especially iterative algorithms are chosen and implemented on the linear <b>systolic</b> <b>array.</b> same. [Continues. ...|$|R
40|$|It is {{proposed}} that the derivation of least-square algorithms for <b>systolic</b> <b>array</b> processing is best accomplished via a geometrical approach. This approach generates a set of vector equations from which {{a large number of}} new and previously known <b>systolic</b> <b>arrays</b> can be obtained in a unified manner. The paper presents and qualitatively describes these vector equations and shows how one such <b>systolic</b> <b>array</b> follows...|$|R
40|$|Design of <b>systolic</b> <b>arrays</b> from {{a set of}} {{nonlinear}} and nonuniform recurrence equations is discussed. A systematic {{method for}} deriving a systolic design in such cases is presented. A novel architectural idea, termed a tagged <b>systolic</b> <b>array,</b> is introduced. The design methodology described broadens the class of algorithms amenable for tagged <b>systolic</b> <b>array</b> implementation. The methodology is illustrated by deriving a systolic design for the fast Fourier transform...|$|R
40|$|Parallel {{computing}} is {{a method}} of computation in which many calculations are carried out concurrently, operating {{on the principle that}} large problems can often be separated into smaller ones, which are then solved simultaneously. <b>Systolic</b> <b>array</b> is a specialized form of parallel computing which is an arrangement of processors in an array where data flows synchronously across the array between neighbours, usually with different data flowing in different directions. The simulation of <b>systolic</b> <b>array</b> for matrix multiplication is the practical application in order to evaluate the performance of <b>systolic</b> <b>array.</b> In this paper, a two-dimensional orthogonal <b>systolic</b> <b>array</b> for matrix multiplication is presented. Perl scripting language is used to simulate a two-dimensional orthogonal <b>systolic</b> <b>array</b> compared to conventional matrix multiplication in terms of average execution time. The comparison is made using matrices of size 5 xM versus Mx 5 which M ranges from 1 to 10, 10 to 100 and 100 to 1000. The orthogonal <b>systolic</b> <b>array</b> results show better average execution time when M is more than 30 compared to conventional matrix multiplication when the size of the matrix multiplication is increased...|$|R
40|$|It is {{our opinion}} that the system-on-chip, {{all-in-one}} approach to parallel digital signal processing combining the homogeneous processor array with large amounts of data memory, will be {{the core of future}} embedded digital signal processing applications. In this paper we present two radical approaches to efficient system-on-chip (SoC) design with homogeneous processor arrays for real-time applications. We propose two novel techniques to enhance the throughput of the SoC designed <b>systolic</b> <b>arrays.</b> A <b>systolic</b> <b>array</b> control mechanism based on very long instruction word (VLIW) principles is presented and a proposal is given for simultaneous execution of independent algorithm data sets, or even different algorithms, on the programmable <b>systolic</b> <b>array,</b> multithreaded <b>systolic</b> computation. Simulation results of our multithreading approach are based on a set of linear algebra algorithms. Keywords- System-on-Chip, processor <b>array,</b> <b>systolic</b> <b>array,</b> multithreading, VLIW, <b>systolic</b> <b>array</b> control. ...|$|R
40|$|This paper {{presents}} an architecture for programmable <b>systolic</b> <b>arrays</b> that provides simple and efficient systolic communication. The Brown <b>Systolic</b> <b>Array</b> is a linear {{implementation of this}} Systolic Shared Register architecture; a working 470 -processor prototype system performs 108 MOPS. A 32 -chip, 1504 -processor implementation could provide 5 GOPS of systolic co-processing power on a single board. Keywords: <b>systolic</b> <b>array,</b> parallel processing, VLSI, SIMD, sequence comparison. Introduction The <b>systolic</b> <b>array</b> {{is perhaps the most}} significant architectural development inspired by the revolution in VLSI fabrication technology. By pumping data through a regular network of hundreds or thousands of simple processing elements, computationally intensive problems can be solved many times faster on <b>systolic</b> <b>arrays</b> than on traditional machines. Since the introduction of the systolic paradigm, systolic co-processors have been proposed to solve a wide variety of problems. However, new VLSI single-pu [...] ...|$|R
40|$|A <b>systolic</b> <b>array</b> is {{presented}} to improve numerical approximations to integrals using Richardson's extrapolation procedure {{in the form of}} Romberg integration. Two designs are presented, the first is an intuitive linear <b>systolic</b> <b>array,</b> the second, a systolic ring using approximately 1 / 3 of the cells of the first <b>array.</b> Both <b>systolic</b> <b>arrays</b> have a computation time of 3 n cycles, which is a significant improvement on the O(n 2) steps required to construct the extrapolation table sequentially...|$|R
40|$|This brief {{presents}} a high-performance curve cryptographic processor for generalnew unified <b>systolic</b> <b>array</b> that efficiently implements addition,el, the control dependencies {{in the operation}} sequencestorage would stall the pipeline in the <b>systolic</b> <b>array.</b> These pipelineSynthesized in 0. 13 - µm standard-cell technology, the processorGF(p) systolic arithmetic unit. We propose a newdivision over GF(p). At the system level, between the <b>systolic</b> <b>array</b> and the separate storageby using two optimization methods. Synthesized inscalar multiplication for general curves over GF...|$|R
40|$|AbstractThe {{objective}} {{of this paper is}} to provide a systematic methodology for the design of space-time optimal pure planar <b>systolic</b> <b>arrays</b> for matrix multiplication. The procedure is based on data dependence approach. By the described procedure, we obtain ten different <b>systolic</b> <b>arrays</b> denoted as S 1 to S 10 classified into three classes according to interconnection patterns between the processing elements. Common properties of all <b>systolic</b> <b>array</b> designs are: each <b>systolic</b> <b>array</b> consists of n 2 processing elements, near-neighbour communications, and active execution time of 3 n − 2 time units. Compared to designs found in the literature, our procedure always leads to <b>systolic</b> <b>arrays</b> with optimal number of processing elements. The improvement in space domain is not achieved at the cost of execution time or PEs complexity. We present mathematically rigorous procedure which gives the exact ordering of input matrix elements at the beginning of the computation. Examples illustrating the methodology are shown...|$|R
40|$|In {{this paper}} the space-time mapping of the {{dependency}} matrix of an algorithm {{is used to}} study spatial properties of a <b>systolic</b> <b>array</b> implementation of a 3 -nested loop structure. Elementary expressions are developed for both the number of processing elements and {{the area of the}} array. These expressions involve only the space-time transformation and the lengths of the loops. As well, characterizations have been found for the form of the space-time transformation which produces a <b>systolic</b> <b>array</b> with the minimum number of processing elements, and one which has both the minimum number of processing elements and the smallest area. 1 Introduction The mapping of algorithms, structured as nested loops, into <b>systolic</b> <b>arrays</b> {{has been the focus of}} considerable research since the introduction of <b>systolic</b> <b>arrays</b> in 1979 [2, 3, 4, 6, 8]. Many of the results reported are concerned with both the possibility of implementing such an algorithm in a <b>systolic</b> <b>array,</b> and optimization of the implementation [...] ...|$|R
40|$|This paper {{explores the}} use of a dependently-typed design logic for {{describing}} and reasoning about <b>systolic</b> <b>arrays.</b> Taking triangular arrays as an example, it is shown how they may usefully be defined in various equivalent ways and how their behaviour may be related to that of the corresponding one-dimensional <b>systolic</b> <b>arrays.</b> This allows complex proofs of the properties of such (dynamic) arrays to be replaced with simpler proofs of the properties of (static) dependency graphs. As an example, the correctness of a systolic sorter is formally established. Overall it is shown that {{the use of}} dependent types for describing <b>systolic</b> <b>arrays</b> contributes significantly to the clarity and generality of their specifications. Keyword Codes: B. 6. 1; I. 2. 3; F. 3. 1 Keywords: Dependent types, higher-order logic, formal verification, <b>systolic</b> <b>arrays.</b> 1 Introduction <b>Systolic</b> <b>arrays</b> [8] are often of sufficient complexity that the use of formal methods to describe and reason about their behaviour can [...] ...|$|R
40|$|In this paper, we {{show that}} every <b>systolic</b> <b>array</b> executes a Regular Iterative Algorithm with a {{strongly}} separating hyperplane and conversely, that every such algorithm can be implemented on a <b>systolic</b> <b>array.</b> This characterization provides us with an unified framework for describing the contributions of other authors. It also exposes the relevance of many fundamental concepts that were introduced in the sixties by Hennie, Waite and Karp, Miller and Winograd, {{to the present day}} concern of <b>systolic</b> <b>array</b> design. 1...|$|R
50|$|<b>Systolic</b> <b>arrays</b> (< {{wavefront}} processors), {{were first}} described by H. T. Kung and Charles E. Leiserson, who published the first paper describing <b>systolic</b> <b>arrays</b> in 1978. However, the first machine {{known to have}} used a similar technique was the Colossus Mark II in 1944.|$|R
40|$|In {{this paper}} a {{bidirectional}} linear <b>systolic</b> <b>array</b> (BLSA) that computes minimum cost spanning tree (MCST) {{of a given}} graph is designed. We present a synthesis procedure, based on data dependencies and space-time transformations of index space, to design BLSA with optimal number of processing elements (PEs) for a given problem size. The execution time is minimized for that number of PEs. Explicit mathematical formulas for <b>systolic</b> <b>array</b> synthesis are derived. We compare performances of the BLSA with the unidirectional linear <b>systolic</b> <b>array</b> (ULSA). ...|$|R
3000|$|When {{the data}} of the first pixel have been written in the Write FIFO, the <b>systolic</b> <b>array</b> and the random {{generation}} module start working. Every clock cycle, a new pixel is read by the control unit {{and sent to the}} <b>systolic</b> <b>array.</b> In parallel, the [...]...|$|R
40|$|AbstractÐPortable image {{processing}} applications require an efficient, scalable platform with localized computing regions. This paper presents a newclass of area I/O systolic architecture {{to exploit the}} physical data locality of planar data streams by processing data where it falls. A synthesis technique using dependence graphs, data partitioning, and computation mapping is developed to handle planar data streams and to systematically design arrays with area I/O. Simulation {{results show that the}} use of area I/O provides a 16 times speedup over systems with perimeter I/O. Performance comparisons for a set of signal processing algorithms show that <b>systolic</b> <b>arrays</b> that consider planar data streams in the design process are up to three times faster than traditional <b>arrays.</b> Index TermsÐParallel <b>computer</b> architecture, <b>systolic</b> <b>arrays,</b> area I/O, design and performance evaluation. ...|$|R
40|$|Contract {{generated}} publications are compiled which {{describe the}} research {{activities for the}} reporting period. Study topics include: equivalent configurations of systolic arrays; least squares estimation algorithms with <b>systolic</b> <b>array</b> architectures; modeling and equilization of nonlinear bandlimited satellite channels; and least squares estimation and Kalman filtering by <b>systolic</b> <b>arrays...</b>|$|R
40|$|In this paper, {{we propose}} unified <b>systolic</b> <b>arrays</b> for {{computation}} of the 1 -D and 2 -D discrete cosine transform/discrete sine transform/discrete Hartley transform (DCT/DST/DHT). By decomposing the transforms into even- and odd-numbered frequency samples, the proposed architecture computes the 1 -D DCT/DST/DHT. Compared {{to the conventional}} methods the proposed <b>systolic</b> <b>arrays</b> exhibit advantages {{in terms of the}} number of PE's and latency. We generalize the proposed structure for computation of the 2 -D DCT/DST/DHT. The unified <b>systolic</b> <b>arrays</b> can be employed for computation of the inverse DCT/DST/DHT (IDCT/IDST/IDHT) ...|$|R
40|$|Reconfigurable <b>Systolic</b> <b>Arrays</b> are a {{generalization}} of <b>Systolic</b> <b>Arrays</b> where node operations and interconnections can be redefined even at run time. This flexibility increases {{the range of}} <b>systolic</b> <b>array’s</b> application, making {{the choice of the}} best systolic architecture to a given problem a critical task. In this work we investigate the specification and verification of such architectures using rewriting-logic, which provides a high level design framework for architectural exploration. In particular, we show how to use ELAN rewriting system to specify reconfigurable systems which can perform both arithmetic and symbolic computations. 1...|$|R
40|$|Abstract—A <b>systolic</b> <b>array</b> {{provides}} an alternative comput-ing paradigm to the von Neuman architecture. Though its hardware implementation has failed as a paradigm to design integrated circuits in the past, {{we are now}} discovering that the <b>systolic</b> <b>array</b> as a software virtualization layer can lead to an extremely scalable execution paradigm. To demonstrate this scalability, in this paper, we design and implement a 3 D virtual <b>systolic</b> <b>array</b> to compute a tile QR decomposition of a tall-and-skinny dense matrix. Our implementation {{is based on a}} state-of-the-art algorithm that factorizes a panel based on a tree-reduction. Using a runtime developed {{as a part of the}} Parallel Ultra Light <b>Systolic</b> <b>Array</b> Runtime (PULSAR) project, we demonstrate on a Cray-XT 5 machine how our virtual <b>systolic</b> <b>array</b> can be mapped to a large-scale machine and obtain excellent parallel performance. This is an important contribution since such a QR decomposition is used, for example, to compute a least squares solution of an overdetermined system, which arises in many scientific and engineering problems. Keywords-systolic array; QR decomposition; multithreading, message-passing, dataflow; runtime; I...|$|R
