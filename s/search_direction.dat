753|730|Public
5000|$|Smart Dynamic Search to {{automatically}} adjust <b>search</b> <b>direction</b> and step size.|$|E
50|$|The {{complete}} <b>search</b> <b>direction</b> {{is the sum}} of {{the predictor}} direction and the corrector direction.|$|E
5000|$|Now we {{only have}} to {{construct}} the new <b>search</b> <b>direction</b> by the Gram-Schmidt process, i.e., ...|$|E
3000|$|<b>Search</b> <b>Directions.</b> Generate <b>search</b> <b>directions</b> {{based on}} the APS strategy. If the current {{solution}} lies in a Semi-TR, then generate <b>search</b> <b>directions</b> to point outside the TR.|$|R
40|$|In the paper, {{aimed at}} the {{shortcoming}} of trust region method, we proposed a algorithm using negative curvature <b>direction</b> as its <b>searching</b> <b>direction.</b> The convergence of the algorithm was given. Furthermore, combing trust region method and curve-linear searching techniques, a trust region algorithm, using general curve-linear <b>searching</b> <b>direction,</b> was proposed. We proved its efficiency and feasibility. The algorithm has adjustability and can select or update its <b>searching</b> <b>direction</b> according to the iteration. This allows the algorithm that has the properties of curve-linear searching method and the global convergence of trust region method. Finally, we indicate that some <b>searching</b> <b>directions</b> of common methods can be as a special <b>searching</b> <b>direction</b> of the general method...|$|R
40|$|We {{propose a}} family of <b>search</b> <b>directions</b> based on primal-dual entropy {{in the context of}} {{interior}} point methods for linear programming. This new family contains previously proposed <b>search</b> <b>directions</b> in the context of primal-dual entropy. We analyze the new family of <b>search</b> <b>directions</b> by studying their primal-dual affine-scaling and constant-gap centering components. We then design primal-dual interior-point algorithms by utilizing our <b>search</b> <b>directions</b> in a homogeneous and self-dual framework. We present iteration complexity analysis of our algorithms and provide the results of computational experiments on NETLIB problems...|$|R
5000|$|Now, {{using this}} scalar , we can compute the next <b>search</b> <b>direction</b> p1 using the {{relationship}} ...|$|E
5000|$|... where [...] is {{the current}} best guess, [...] is a <b>search</b> <b>direction,</b> and [...] is the step length.|$|E
5000|$|The <b>search</b> <b>direction</b> pk at stage k {{is given}} by the {{solution}} of the analogue of the Newton equation ...|$|E
30|$|Compute {{the main}} <b>search</b> <b>directions.</b>|$|R
40|$|We {{consider}} {{a class of}} iterative numerical methods and introduce the notion of semiglobally, practically, strictly pseudogradient (SPSP) <b>search</b> <b>directions.</b> We demonstrate {{the relevance of the}} SPSP property in modelling a variety of optimization algorithms, including those subject to absolute and relative errors. We show that the attractors of iterative methods with SPSP <b>search</b> <b>directions</b> exhibit semiglobal, practical, asymptotic stability. Moreover, the SPSP property is robust in the sense that perturbations of SPSP <b>search</b> <b>directions</b> also have the SPSP property...|$|R
40|$|In {{this article}} we {{consider}} modified <b>search</b> <b>directions</b> in the endgame of interior point methods for linear programming. In this stage, the normal equations determining the <b>search</b> <b>directions</b> become ill-conditioned. The modified <b>search</b> <b>directions</b> are computered by solving perturbed systems in which the systems may be solved efficiently by the preconditioned conjugate gradient solver. We prove the convergence of the interior point methods using the modified <b>search</b> <b>directions</b> and show that each barrier problem is solved with a superlinear convergence rate. A variation of Cholesky factorization is presented for computing a better preconditioner when the normal equations are ill-conditioned. These ideas have been implemented successfully and the numerical {{results show that the}} algorithms enhance the performance of the preconditioned conjugate gradients-based interior point methods...|$|R
5000|$|This {{formulation}} {{is valid}} {{whether we are}} minimizing or maximizing. Note {{that if we are}} minimizing, the <b>search</b> <b>direction</b> would be the negative of z (since z is [...] "uphill"), and if we are maximizing, [...] should be negative definite rather than positive definite. We would typically do a backtracking line search in the <b>search</b> <b>direction</b> (any line search would be valid, but L-BFGS does not require exact line searches in order to converge).|$|E
5000|$|... : {{the current}} <b>search</b> <b>{{direction}}.</b> By convention, [...] {{is equal to}} 1 for the forward direction and 2 for the backward direction (Kwa 1989) ...|$|E
5000|$|Our {{next step}} in the process is to compute the scalar [...] that will {{eventually}} be used to determine the next <b>search</b> <b>direction</b> p1.|$|E
30|$|In this algorithm, we {{can take}} d^k<ε for some given {{precision}} as the stopping criterion. And we apply y^k and F_k to construct the <b>searching</b> <b>direction</b> d^k. The choice of a new <b>searching</b> <b>direction</b> leads to quite different in establishing the convergence result of Algorithm 3.1.|$|R
40|$|Abstract. Recently in [3] we {{have defined}} a new method for finding <b>search</b> <b>directions</b> for {{interior}} point methods (IPMs) in linear optimization (LO). Using one particular {{member of the}} new family of <b>search</b> <b>directions</b> we have developed a new primal-dual interior point algorithm for LO. We have proved that this short-update algorithm has also the O(n log...|$|R
40|$|I hereby {{declare that}} I am the sole {{author of this}} thesis. This is a true copy of the thesis, {{including}} any required final revisions, as accepted by my examiners. I understand that my thesis may be made electronically available to the public. ii We propose a family of <b>search</b> <b>directions</b> based on primal-dual entropy {{in the context of}} interior point methods for linear programming. This new family contains previously proposed <b>search</b> <b>directions</b> in the context of primal-dual entropy. We analyze the new family of <b>search</b> <b>directions</b> by studying their primal-dual affine-scaling and constant-gap centering components. We then design primal-dual interior-point algorithms by utilizing our <b>search</b> <b>directions</b> in a homogeneous and self-dual framework. We present iteration complexity analysis of our algorithms and provide the results of computational experiments on NETLIB problems. iii Acknowledgements I would like to express my deepest thank to my supervisor, Dr. Levent Tunçel. Without hi...|$|R
50|$|The predictor-corrector {{algorithm}} then first computes the affine scaling direction. Secondly, it {{solves the}} aggregated system {{to obtain the}} <b>search</b> <b>direction</b> of the current iteration.|$|E
5000|$|In (unconstrained) minimization, a {{backtracking}} line search, {{a search}} {{scheme based on}} the Armijo-Goldstein condition, is a line search method to determine the maximum amount to move along a given <b>search</b> <b>direction.</b> It involves starting with a relatively large estimate of the step size for movement along the <b>search</b> <b>direction,</b> and iteratively shrinking the step size (i.e., [...] "backtracking") until a decrease of the objective function is observed that adequately corresponds to the decrease that is expected, based on the local gradient of the objective function.|$|E
5000|$|... {{denotes the}} upper half of the {{solution}} vector matching the initial guess [...] for its lower half. We complete the initialization by choosing the first <b>search</b> <b>direction</b> ...|$|E
40|$|A novel primal-dual path-following interior-point {{algorithm}} for the Cartesian P*(k) -linear complementarity problem over symmetric cones is presented. The {{algorithm is}} based on a reformulation of the central path for finding the <b>search</b> <b>directions.</b> For a full Nesterov-Todd step feasible interior-point algorithm based on the new <b>search</b> <b>directions,</b> the complexity bound of the algorithm with small-update approach is the best-available bound. <br /...|$|R
5000|$|... where [...] so {{that the}} {{residuals}} and the <b>search</b> <b>directions</b> satisfy biorthogonality and biconjugacy, respectively, i.e., for , ...|$|R
40|$|In {{solving a}} linear system with {{iterative}} methods, one is usually {{confronted with the}} dilemma of having to choose between cheap, ineffcient iterates over sparse <b>search</b> <b>directions</b> (e. g., coordinate descent), or expensive iterates in well-chosen <b>search</b> <b>directions</b> (e. g., conjugate gradients). In this paper, we propose to interpolate between these two extremes, and show how to perform cheap iterations along nonsparse <b>search</b> <b>directions,</b> provided that these directions can be extracted from {{a new kind of}} sparse factorization. For example, if the <b>search</b> <b>directions</b> are the columns of a hierarchical matrix, then the cost of each iteration is typically logarithmic in the number of variables. Using some graph-Theoretical results on low-stretch spanning trees, we deduce as a special case a nearly linear time algorithm to approximate the minimal norm solution of a linear system Bx = b where B is the incidence matrix of a graph. We thereby can connect our results to recently proposed nearly linear time solvers for Laplacian systems, which emerge here as a particular application of our sparse matrix factorization...|$|R
5000|$|In {{practical}} implementations, {{a version}} of line search is performed to obtain the maximal step length {{that can be taken}} in the <b>search</b> <b>direction</b> without violating nonnegativity, [...]|$|E
50|$|Since this is {{the first}} iteration, we will use the {{residual}} vector r0 as our initial <b>search</b> <b>direction</b> p0; the method of selecting pk will change in further iterations.|$|E
5000|$|... where [...] and [...] are Lagrange multipliers. At an iterate , a basic {{sequential}} {{quadratic programming}} algorithm defines an appropriate <b>search</b> <b>direction</b> [...] {{as a solution}} to the quadratic programming subproblem ...|$|E
5000|$|In BiCG, the <b>search</b> <b>directions</b> [...] and [...] and the {{residuals}} [...] and [...] {{are updated}} {{using the following}} recurrence relations: ...|$|R
40|$|UTMOST IV {{currently}} includes 6 optimizers for parameter extraction. Selecting an optimizer {{can sometimes}} be confusing. This article reviews the optimizers and attempts to provide some guidelines in selecting an appropriate one. Optimization tasks are usually divided into two major categories: local and global. Local optimization assumes {{that there is a}} single minimum of a cost function which needs to be located. Locating this minimum is relatively simple – starting from some initial parameter values the optimizer successively finds <b>search</b> <b>directions</b> and tries to locate the point with lowest cost function along the <b>search</b> <b>directions.</b> The choice of good <b>search</b> <b>directions</b> depends on the initial parameter values and the methods the optimizer is using to calculate the directions. In contrast, a global optimization problem assumes that there may be multipl...|$|R
3000|$|... -property {{makes the}} {{analysis}} of the method far more complicated. Furthermore, we loose the orthogonality of the scaled <b>search</b> <b>directions</b> in the Cartesian [...]...|$|R
50|$|Features of the {{software}} include: quadratic approximations of the objective function whose second derivative matrices are updated {{by means of the}} BFGS formula, active sets technique, primal-dual quadratic programming procedure for calculation of the <b>search</b> <b>direction.</b>|$|E
5000|$|... so we {{must have}} [...] ，and the {{condition}} C3) ensures it. A natural choice would be [...] Condition C5) is a fairly stringent condition on the shape of it gives the <b>search</b> <b>direction</b> of the algorithm.|$|E
5000|$|Define {{the local}} {{slope of the}} {{function}} of [...] along the <b>search</b> <b>direction</b> [...] as [...] It is assumed that [...] is a unit vector in a direction in which some local decrease is possible, i.e., {{it is assumed that}} [...]|$|E
40|$|AbstractNorm-minimizing-type {{methods for}} solving large sparse linear systems with {{symmetric}} and indefinite coefficient matrices are considered. The Krylov subspace {{can be generated}} by either the Lanczos approach, such as the methods MINRES, GMRES and QMR, or by a conjugate-gradient approach. Here, we propose an algorithm based on the latter approach. Some relations among the <b>search</b> <b>directions</b> and the residuals, and how the <b>search</b> <b>directions</b> {{are related to the}} Krylov subspace are investigated. Numerical experiments are reported to verify the convergence properties...|$|R
40|$|A new derivative-free {{optimization}} {{method for}} unconstrained optimization of partially separable functions is presented. Using average curvature information computed from sampled function values the method generates an average Hessian-like matrix and uses its eigenvectors as new <b>search</b> <b>directions.</b> For partially separable functions, {{many of the}} entries of this matrix will be identically zero. The method is able to exploit this property {{and as a consequence}} update its <b>search</b> <b>directions</b> more often than if sparsity is not taken into account. Numerical results show that this is a more effective method for functions with a topography which requires frequent updating of <b>search</b> <b>directions</b> for rapid convergence. The method is an important extension of a method for nonseparable functions previously published by the authors. This new method allows for problems of larger dimension to be solved, and will in most cases be more efficient...|$|R
40|$|The Commutative Class of <b>search</b> <b>directions</b> for semidefinite {{programming}} is first proposed by Monteiro and Zhang [13]. In this paper, we investigate the corresponding class of <b>search</b> <b>directions</b> for linear programming over symmetric cones, {{which is a}} class of convex optimization problems including linear programming, second-order cone programming, and semidefinite programming as special cases. Complexity results are established for short, semi-long, and long step algorithms. We then propose a subclass of Commutative Class of <b>search</b> <b>directions</b> which has polynomial complexity even in semi-long and long step settings. The last subclass still contains the NT and HRVW/KSH/M directions. An explicit formula to calculate {{any member of the}} class is also given. Key words: Symmetric Cone, Primal-dual Interior-Point Method, Jordan Algebra, Polynomial Complexity A#liation: Department of Computer Science, The University of Electro-Communications 1 1. Introduction In this paper, we consider linear [...] ...|$|R
