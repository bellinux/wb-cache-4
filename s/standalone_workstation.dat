11|20|Public
25|$|There are two {{configurations}} of Btrieve for Windows NT/95, <b>standalone</b> <b>workstation</b> and client/server.|$|E
25|$|When {{using the}} <b>standalone</b> <b>workstation</b> {{configuration}} of Btrieve, all processing of records {{is done on}} the local workstation. The workstation relies on the underlying mechanisms of Windows to allow the MKDE (the W32MKDE.EXE program) to gain {{direct access to the}} database files, and uses lock files to deal with concurrency issues.|$|E
2500|$|On Windows 95, the MKDE {{interface}} (a Windows {{dynamic link}} library (DLL) called WBTRV32.DLL) determines what database access method is in use via the configuration file. If it detects both the client/server and workstation engines are installed on the same machine, it checks whether the target is set to workstation or server. If running on Windows NT and the server process NTMKDE.EXE is running along with the <b>standalone</b> <b>workstation</b> process W32MKDE.EXE it looks in the registry {{to determine if the}} target is a server or workstation. In both cases, if the MKDE interface is set to workstation, (the [...] "Standalone workstation" [...] configuration) it uses the MKDE (W32MKDE.EXE) to access the file directly. If it is set to server, the MKDE interface on the client uses a communications module (on Windows 95 this is W32BTICM.DLL, on Windows NT this is NTBTICM.DLL) that [...] "talks" [...] to the server. The server itself has its own matching communications module (again either W32BTICM.DLL or NTBTICM.DLL) that resides on the mapped drive. The server DLL communicates with the server MKDE (NTMKDE.EXE) which updates records, then sends a confirmation that the operation succeeded, back through the communications module to the client.|$|E
50|$|Terason systems use a Windows-based {{operating}} system for image processing, reporting, communication and system integration. Terason’s {{systems can be}} integrated into an existing communication infrastructure or can operate as <b>standalone</b> <b>workstations</b> within hospitals or at remote locations. uConnect™ is a remote diagnostic program from Terason that connects the consumer directly to a Terason support representative.|$|R
50|$|By {{the late}} 70s, it became {{possible}} for personal computers (such as the Apple II) to contain low-color framebuffers. However, {{it was not}} until the 1980s that a real revolution in the field was seen, and framebuffers capable of holding a standard video image were incorporated into <b>standalone</b> <b>workstations.</b> By the 90s, framebuffers eventually became the standard for all personal computers.|$|R
40|$|In {{order to}} bridge the digital divide while {{enriching}} the education of children, parents and teachers, this project builds upon the conceptual uniqueness of the Children's Science Center (CSC) in Rapid City, South Dakota to bring an interactive learning opportunity to area residents and augment interest in local paleontology and geology via the World Wide Web. This paper describes the integration process while focusing on the design and implementation of software developed using the Microsoft. NET framework for <b>standalone</b> <b>workstations.</b> Through the creation of these learning centers, the project combines educational resources, curricula and recent computer technology to provide a conducive venue for learning...|$|R
50|$|There are two {{configurations}} of Btrieve for Windows NT/95, <b>standalone</b> <b>workstation</b> and client/server.|$|E
5000|$|When {{using the}} <b>standalone</b> <b>workstation</b> {{configuration}} of Btrieve, all processing of records {{is done on}} the local workstation. The workstation relies on the underlying mechanisms of Windows to allow the MKDE (the [...] program) to gain {{direct access to the}} database files, and uses lock files to deal with concurrency issues.|$|E
5000|$|A CG {{system could}} also include 8" [...] floppy {{diskette}} drives, a {{disk operating system}} for storing graphics images, and a version of Microsoft BASIC. These allowed the CG {{to be used as}} a <b>standalone</b> <b>workstation,</b> able to generate images without being connected to a host machine. Later enhancements included a Color Lookup Table and arithmetic processing unit.|$|E
40|$|The IT (Information Technology) {{environment}} in today's laboratories is characterized as being highly distributed, heterogeneous, {{and in some}} instances extremely dynamic. Larger organizations have to deal with hundreds of different systems, ranging from <b>standalone</b> <b>workstations</b> and devices in laboratories to fully integrated LIMS (Laboratory Information Management System) and ERP (Enterprise Resource Planning) systems. An information system operating in such an environment must handle several emerging problems, such as heterogeneous hardware and software platforms, as well as distributed information sources and capabilities. It is also expected that the IT infrastructure scales well, easily integrates with legacy systems, allows resource sharing, and supports day-to-day operations such as information retrieval, data storage, validation, tracking, replication, and archival in a fully automated fashion. By using real-world examples, this presentation will illustrate how software agent technology can be used to manage the ever increasing IT complexity and user demands in the laboratory of the future...|$|R
40|$|Trace driven {{simulations}} {{are used}} to study the performance of several disk cache replacement policies for network fileservers. It is shown that locality based approaches, such as the common Least Recently Used (LRU) policy, which are known to work well on <b>standalone</b> disked <b>workstations</b> and at client workstations in distributed systems, are inappropriate at a fileserver. Quite simple frequency based approaches do better...|$|R
50|$|X11 Release 3 {{introduced}} display {{managers in}} October 1988 {{with the aim}} of supporting the standalone X terminals, just coming onto the market. Various display managers continue in routine use to provide a graphical login prompt on <b>standalone</b> computer <b>workstations</b> running X. X11R4 introduced the X Display Manager Control Protocol (XDMCP) in December 1989 to fix problems in the X11R3 implementation.|$|R
5000|$|SIMDIS {{provides}} {{either a}} 2-D or a 3-D {{display of the}} normally [...] "seen" [...] data such as platform position and orientation, {{as well as the}} [...] "unseen" [...] data such as the interactions of sensor systems with targets, countermeasures, and the environment. It includes custom tools for interactively analyzing and displaying data for equipment modes, spatial grids, ranges, angles, antenna patterns, line of sight and RF propagation. Capability for viewing time synchronized data from either a <b>standalone</b> <b>workstation</b> or multiple networked workstations is also provided.|$|E
5000|$|On Windows 95, the MKDE {{interface}} (a Windows {{dynamic link}} library (DLL) called [...] ) determines what database access method is in use via the configuration file. If it detects both the client/server and workstation engines are installed on the same machine, it checks whether the target is set to workstation or server. If running on Windows NT and the server process [...] is running along with the <b>standalone</b> <b>workstation</b> process [...] it looks in the registry {{to determine if the}} target is a server or workstation. In both cases, if the MKDE interface is set to workstation, (the [...] "Standalone workstation" [...] configuration) it uses the MKDE (...) to access the file directly. If it is set to server, the MKDE interface on the client uses a communications module (on Windows 95 this is , on Windows NT this is [...] ) that [...] "talks" [...] to the server. The server itself has its own matching communications module (again either [...] or [...] ) that resides on the mapped drive. The server DLL communicates with the server MKDE (...) which updates records, then sends a confirmation that the operation succeeded, back through the communications module to the client.|$|E
40|$|International audienceOBJECTIVE: This paper {{describes}} {{the methods used}} to create annotated deformable anatomic templates (DATs) and display them in a patient's axial 2 -dimensional and reformatted volume brain images. METHODS: A senior neuroradiologist annotated and manually segmented 1185 color-coded structures on axial magnetic resonance images of a normal template brain using domain knowledge from multiple medical specialties. Besides the visible structures, detailed pathways for vision, speech, cognition, and movement were charted. This was done by systematically joining visible anatomic anchor points and selecting the best fit based on comparisons with cadaver dissections and the constraints defined on the companion 2 -dimensional images. RESULTS: The DAT is commercially available for use on a picture archiving and communication system or as a <b>standalone</b> <b>workstation.</b> CONCLUSIONS: The DAT can quickly embed extensive, clinically useful functional neuroanatomic knowledge into the patient's brain images. Besides labeling visible structures, DAT displays clinically important, previously uncharted subdivisions of the fiber tracts...|$|E
50|$|In 1981, with a {{small number}} of BNR colleagues, Pearson left to found Orcatech Inc., one of the first {{companies}} specialising in the design and development of high resolution intelligent graphics workstations for the computer-aided engineering market. In the early 1980s, the availability of bit-slice and 16-bit microprocessors started to revolutionise high resolution computer graphics terminals which now increasingly became intelligent, semi-standalone and <b>standalone</b> <b>workstations.</b> Graphics and application processing were increasingly migrated to the intelligence in the workstation, rather than continuing to rely on central mainframe and mini-computers. The Orca 3000 was based on Motorola 68000 and AMD bit-slice processors and had Unix as its operating system. It was targeted squarely at the sophisticated end of the design engineering sector and included General Motors, Nortel, Boeing and Lockheed as clients. After developing a portfolio of successful workstation products and a rich portfolio of blue-chip clients, Orcatech went public on the Toronto Stock Exchange in 1983 at a valuation of over $100m, at that time one of the highest valued young technology companies ever to go public on the TSX.|$|R
2500|$|In {{the early}} 1980s, the {{availability}} of bit-slice and 16-bit microprocessors started to revolutionize high-resolution computer graphics terminals which now increasingly became intelligent, semi-standalone and <b>standalone</b> <b>workstations.</b> Graphics and application processing were increasingly migrated to the intelligence in the workstation, rather than continuing to rely on central mainframe and mini-computers. Typical of the early move to high-resolution computer graphics intelligent workstations for the computer-aided engineering market were the Orca 1000, 2000 and 3000 workstations, developed by Orcatech of Ottawa, a spin-off from Bell-Northern Research, and led by David Pearson, an early workstation pioneer. The Orca 3000 was based on Motorola 68000 and AMD bit-slice processors and had Unix as its operating system. It was targeted squarely at the sophisticated end of the design engineering sector. Artists and graphic designers {{began to see the}} personal computer, particularly the Commodore Amiga and Macintosh, as a serious design tool, one that could save time and draw more accurately than other methods. [...] The Macintosh remains a highly popular tool for computer graphics among graphic design studios and businesses. Modern computers, dating from the 1980s, often use graphical user interfaces (GUI) to present data and information with symbols, icons and pictures, rather than text. Graphics are one of the five key elements of multimedia technology.|$|R
40|$|A {{simulation}} framework {{has been}} developed for a large-scale, comprehensive, scaleable simulation of an Intelligent Transportation System (ITS). The simulator is designed for running on parallel computers and distributed (networked) computer systems, but can run on <b>standalone</b> <b>workstations</b> for smaller simulations. The simulator currently models instrumented smart vehicles with in-vehicle navigation units capable of optimal route planning and Traffic Management Centers (TMC). The TMC has probe vehicle tracking capabilities (display position and attributes of instrumented vehicles), and can provide two-way interaction with traffic to provide advisories and link times. Both the in-vehicle navigation module and the TMC feature detailed graphical user interfaces to support human-factors studies. Realistic modeling of variations of the posted driving speed are based on human factors studies that take into consideration weather, road conditions, driver personality and behavior, and vehicle type. The prototype {{has been developed}} on a distributed system of networked UNIX computers but is designed to run on parallel computers, such as ANL`s IBM SP- 2, for large-scale problems. A novel feature of the approach is that vehicles are represented by autonomous computer processes which exchange messages with other processes. The vehicles have a behavior model which governs route selection and driving behavior, and can react to external traffic events much like real vehicles. With this approach, the simulation is scaleable {{to take advantage of}} emerging massively parallel processor (MPP) systems...|$|R
40|$|The Internet {{technology}} has evolved very rapidly {{over the past few}} years. Due to its multimedia capability, the World Wide Web (WWW or Web) is the most popular and visible component of the Internet. It is being increasingly used to support various activities of the product development process. This paper is concerned with providing design for manufacture and assembly (DFMA) techniques on the Internet. An experiment is conducted to show how a well-known design for assembly (DFA) technique can be converted into a web-based version which is functionally equivalent to its version on a <b>standalone</b> <b>workstation.</b> Four important insights have been gained from the experiment. The web-based client and server architecture is found to be attractive for collaborative DFMA. The client-side web scripting can be exploited to develop generic frameworks for developing and applying different design for X (DFX) techniques, more importantly, in an integrated way. In addition, web-based DFX techniques provide more opportunities for integration with other decision-support systems such as Computer Aided Design (CAD), Computer Aided Process Planning (CAPP) and Computer Aided Production Management (CAPM) in the product realisation process. However, issues such as interactivity and security remain to be addressed. © 1999 Elsevier Science B. V. All rights reserved. link_to_subscribed_fulltex...|$|E
40|$|The Kepler {{mission is}} {{designed}} to continuously monitor up to 170, 000 stars at a 30 minute cadence for 3. 5 years searching for Earth-size planets. The data are processed at the Science Operations Center (SOC) at NASA Ames Research Center. Because of the large volume of data and the memory and CPU-intensive nature of the analysis, significant computing hardware is required. We have developed generic pipeline framework software {{that is used to}} distribute and synchronize the processing across a cluster of CPUs and to manage the resulting products. The framework is written in Java and is therefore platform-independent, and scales from a single, <b>standalone</b> <b>workstation</b> (for development and research on small data sets) to a full cluster of homogeneous or heterogeneous hardware with minimal configuration changes. A plug-in architecture provides customized control of the unit of work without the need to modify the framework itself. Distributed transaction services provide for atomic storage of pipeline products for a unit of work across a relational database and the custom Kepler DB. Generic parameter management and data accountability services are provided to record the parameter values, software versions, and other meta-data used for each pipeline execution. A graphical console allows for the configuration, execution, and monitoring of pipelines. An alert and metrics subsystem is used to monitor the health and performance of the pipeline. The framework was developed for the Kepler project based on Kepler requirements, but the framework itself is generic and could be used for a variety of applications where these features are needed...|$|E
40|$|The {{purpose of}} our work is the {{clinical}} validation of a Computer Aided Detection (CAD) system for the automatic identification of pulmonary nodules in chest Computed Tomography (CT) scans. Non-calcified pulmonary nodules are the early manifestation of lung cancers. Lung cancer {{is the leading cause}} of cancer-related death worldwide. Screening high risk individuals for lung cancer with low-dose CT scans is now being implemented in the United States and other countries are expected to follow soon. The detection of these pathological Regions Of Interest (ROIs) is a burden task for radiologists, mainly due to the high number of noisy images to be analysed. To support radiologists, researchers have started implementing CAD algorithms for the automatic identification of pathological ROIs. Several studies proved the positive impact of CADs as a support for radiologists in the detection, with sensitively benefit on the overall performance. Despite these very prominent results, CAD systems have not been spread in clinical routine yet. In fact, the standard approach to make CAD algorithms available in the clinical routine of health facilities, that is the deployment of <b>standalone</b> <b>workstations,</b> usually equipped with a vendor-dependent Graphic User Interface (GUI), presents several drawbacks, such as the high fixed cost of the software licenses and the dedicated hardware and the rapid obsolescence of both. Furthermore, the computational needs by CAD algorithms can be demanding, depending on their complexity, often requiring powerful and expensive hardware. The diffusion of Cloud Computing solutions, accessible via secure Web protocols, solves almost all the previous two issues. In addition, the Software as A Service (SaaS) approach provides the possibility of combining several CADs, with demonstrated benefits to the overall performance...|$|R
40|$|The {{combination}} of traditional microprocessors workstations and hardware-reconfigurable Field Programmable Gate Arrays (FPGAs) {{has developed a}} new class of workstations known as reconfigurable computers, with several examples demonstrating significant speedups compared to <b>standalone</b> PC <b>workstations</b> alone. Several platforms implement PC-FPGA communication using common PC peripheral interface buses such as PCI-X. A new approach from SRC Computers implements a high-speed communication interface that increases the throughput compared to PCI interfaces. This paper demonstrates an efficient high-throughput implementation of IDEA encryption using the SRC platform. SRC design choices that influence both throughput and area are evaluated. Detailed analyses of FPGA resource utilizations, data transfer and reconfiguration overheads for the SRC system are provided, and a comparison between SRC and a public domain software implementation of IDEA are given. 1...|$|R
40|$|Background 	 Open source {{software}} (such as OsiriX) and new, powerful mobile {{devices have}} become available, that allow {{to display a}} large amount of medical images from several imaging modalities without the need for dedicated <b>standalone</b> <b>workstations.</b> Our purpose was {{to evaluate the effectiveness of}} the iPad 2 ® as a mobile device for 2 D reading of chest CT datasets for the assessment of pulmonary nodules. Evaluation 	 We retrospectively reviewed 17 chest CT examinations for a total of 274 nodules sized between 2 mm and 27 mm. CTC images had been acquired using a 64 -row CT and were wirelessly imported in DICOM format on an iPad 2 ® 64 GB (Apple Inc, Cupertino, CA) running OsiriX HD® (www. osirix-viewer. com) from a Macintosh desktop computer (iMac® 3. 06 GHz) connected to our hospital PACS and running OsiriX 3. 9. Two experienced raters read CTC datasets independently on the iMac® and on the iPad 2 ®. Detection rate and segmental localization of lesions were recorded for each dataset, as well as the time needed for complete reading of each chest CT examination. All nodules detected on the iMac® were also identified on the iPad 2 ®, and their segmental localization was correctly assessed in 100 % of cases. Image reading time was comparable with both devices (4. 88 ± 2. 09 minutes for the iMac® vs 5. 21 ± 2. 45 minutes for the iPad 2 ®; p< 0. 05). Discussion 	 Our findings show that the iPad 2 ® can be successfully used for 2 D reading of chest CT datasets in patients with pulmonary nodules, as all lesions detected on the iMac® were also found and correctly localized on the iPad 2 ®. Image reading is relatively fast, supporting the hypothesis that the iPad 2 ® could be reliably used for preliminary visualization of lung nodules. The iPad 2 ® may also find a role for image sharing with non-radiology specialists and for teaching purposes...|$|R
50|$|From the AX model onwards RM {{computers}} were PC compatible. The 'X Series' was supplemented by the VX, using the new, 32-bit 80386 processor, marketed as a <b>standalone</b> CAD <b>workstation</b> or network fileserver. RM released M Series computers, primarily used as diskless network clients, using the 80286 and later 80386 processors. These used the Micro Channel architecture that {{featured in the}} IBM PS/2, which was faster than the standard ISA architecture, but failed to gain widespread acceptance. RM's fileserver platform became its 'E Series' computers, using the similarly short-lived EISA architecture and using a tower case to allow space for multiple hard disks. These fileservers ran Microsoft LAN Manager (on Microsoft OS/2) preconfigured with client Operating System files (Windows 3.0 and later 3.1) for remote booting and bundled with RM-developed tools for managing network users, client PCs and applications. This was sold as RM Net LM.|$|R
40|$|The {{integration}} of new media typesinto# open distributed systems presents {{a number of}} problems. In this paper we discuss these problemswithin# {{the context of an}} experimental distributed# multimedia system being designed at Lancaster. # Object management is highlighted as being avital# component of our system, and we outline anapproach# to management termed management byexception. # This approach is illustrated by considering the provision of persistence and location transparent invocation in our system. 1. Introduction Multimedia computing has emerged in thelast# few years as a major area of research. Thiswork# is motivated by the wide range of potential applications made feasible by combining informationsources# such as voice, graphics, hi-fi quality audio and video. Most of the work to date has centred onthe# <b>standalone</b> multimedia <b>workstation,</b> and a varietyof# software packages are now available in areas suchas# music composition, computer-aided learningand# interactive video. The comb [...] ...|$|R
40|$|Background 	 Open source {{software}} and new, powerful mobile devices have become available, that allow {{to display a}} large amount of medical images from several imaging modalities without the need for dedicated <b>standalone</b> <b>workstations.</b> Our purpose was {{to evaluate the effectiveness of}} 2 D image review of CT Colonography (CTC) datasets on the mobile device iPad 2 ® (Apple Inc, Cupertino, CA) with the open source software OsiriX HD. Evaluation 	 We retrospectively reviewed 23 CTC examinations performed in a colorectal cancer screening setting. CTC images had been acquired in the supine and prone position using a low radiation dose and a fecal tagging protocol based on oral administration of iodinated contrast material. All datasets were wirelessly imported in DICOM format on an iPad 2 ® 64 GB (Apple Inc, Cupertino, CA) running OsiriX HD® (www. osirix-viewer. com) from a Macintosh desktop computer (iMac® 3. 06 GHz) connected to our hospital PACS and running OsiriX 3. 9. Two experienced raters read CTC datasets independently on the iMac® and on the iPad 2 ®. Detection rate and segmental localization of lesions were recorded for each CTC dataset, as well as the time needed for complete reading of each CTC examination. Image quality was also visually assessed using a three-point scale (1 =poor, 2 =fair, 3 =good). All lesions detected on the iMac® were also identified on the iPad 2 ®, and their segmental localization was correctly assessed in 100 % of cases. Image quality was good with both devices, while image reading time was longer on the iPad 2 ® than on the iMac® (5. 32 ± 2. 16 vs 3. 51 ± 1. 58 minutes, respectively [mean±standard deviation], p< 0. 05). Discussion 	 Our findings show that the iPad 2 ® can be used for 2 D review of CTC, as all lesions detected on the iMac® were found with good image quality. The longer time needed for reading of CTC suggests that the iPad 2 ® is not optimized for primary reporting, but can be suitable for preliminary 2 D reading of CTC examinations. The iPad 2 ® may also find a role for image sharing and for teaching purposes. Limitations of 2 D reading should be overcome by the introduction of software with 3 D tools. CONCLUSION The iPad 2 ® can reliably be used for 2 D review of CTC datasets...|$|R
50|$|Other similar designs {{appeared}} {{soon thereafter}} from George Massenburg (in 1972) and Burgess McNeal from ITI corp. In May 1972 Massenburg introduced the term Parametric Equalization {{in a paper}} presented at the 42nd convention of the Audio Engineering Society. Most channel equalization on mixing consoles made from 1971 to the present day rely upon the designs of Flickinger, Massenburg and McNeal in either semi or fully parametric topology. In the late 1990s and in the 2000s, parametric equalizers became increasingly available as Digital Signal Processing (DSP) equipment, usually in the form of plug-ins for various digital audio <b>workstations.</b> <b>Standalone</b> outboard gear versions of DSP parametric equalizers were also quickly introduced after the software versions and are typically called Digital Parametric Equalizers.|$|R
40|$|With larger {{data sets}} and more {{sophisticated}} analyses, {{it is becoming increasingly}} common for neuroimaging researchers to push (or exceed) the limitations of <b>standalone</b> computer <b>workstations.</b> Nonetheless, although high-performance computing platforms such as clusters, grids and clouds are already in routine use by a small handful of neuroimaging researchers to increase their storage and/or computational power, the adoption of such resources by the broader neuroimaging community remains relatively uncommon. Therefore, the goal of the current manuscript is to: 1) inform prospective users about the similarities and differences between computing clusters, grids and clouds; 2) highlight their main advantages; 3) discuss when it may (and may not) be advisable to use them; 4) review some of their potential problems and barriers to access; and finally 5) give a few practical suggestions for how interested new users can start analyzing their neuroimaging data using cloud resources. Although the aim of cloud computing is to hide most of the complexity of the infrastructure management from end-users, we recognize that this can still be an intimidating area for cognitive neuroscientists, psychologists, neurologists, radiologists, and other neuroimaging researchers lacking a strong computational background. Therefore, with this in mind, we have aimed to provide a basic introduction to cloud computing in general (including some of the basic terminology, computer architectures, infrastructure and service models, etc.), a practical overview of the benefits and drawbacks, and a specific focus on how cloud resources can be used for various neuroimaging applications...|$|R
40|$|Integration of RIS and PACS {{services}} into {{a single}} solution has become a widespread reality in daily radiological practice, allowing substantial acceleration of workflow with greater ease of work compared with older generation film-based radiological activity. In particular, the fast and spectacular recent evolution of digital radiology (with special reference to cross-sectional imaging modalities, such as CT and MRI) has been paralleled {{by the development of}} integrated RIS [...] PACS systems with advanced image processing tools (either two- and/or three-dimensional) that were an exclusive task of costly dedicated workstations until a few years ago. This new scenario is likely to further improve productivity in the radiology department with reduction of the time needed for image interpretation and reporting, as well as to cut costs for the purchase of dedicated <b>standalone</b> image processing <b>workstations.</b> In this paper, a general description of typical integrated RIS [...] PACS architecture with image processing capabilities will be provided, and the main available image processing tools will be illustrated...|$|R
40|$|Cluster Computing {{addresses}} the latest results in these fields that support High Performance Distributed Computing (HPDC). In HPDC environments, parallel and/or distributed computing techniques {{are applied to}} the solution of computationally intensive applications across networks of computers. A cluster computing {{is a type of}} parallel or distributed computer system, which consists of a collection of interconnected stand-alone computers working together as a single integrated computing resource. The key components of a cluster include multiple <b>standalone</b> computers (PCs, <b>Workstations,</b> or SMPs), operating systems, high-performance interconnects, middleware, parallel programming environments, and applications. It assumes that the reader is familiar with the standard commodity hardware and software components such as stand-alone computers, operating systems such as Linux and Windows, and standard communication software such as TCP/IP. There are many applications which can benefit from parallelisation. Employing clusters of computers provides a method to utilise commodity components, minimising cost and and maximising longevity of the individual parts...|$|R

