2250|2904|Public
5|$|The Euclidean {{algorithm}} {{can be used}} {{to arrange}} the set of all positive rational numbers into an infinite binary <b>search</b> <b>tree,</b> called the Stern–Brocot tree.|$|E
5|$|There are {{numerous}} variations of binary search. In particular, fractional cascading speeds up binary {{searches for the}} same value in multiple arrays, efficiently solving a series of search problems in computational geometry and numerous other fields. Exponential search extends binary search to unbounded lists. The binary <b>search</b> <b>tree</b> and B-tree data structures are based on binary search.|$|E
5|$|A binary <b>search</b> <b>tree</b> is {{a binary}} tree data {{structure}} that works {{based on the}} principle of binary search. The records of the tree are arranged in sorted order, and each record in the tree can be searched using an algorithm similar to binary search, taking on average logarithmic time. Insertion and deletion also require on average logarithmic time in binary search trees. This can faster than the linear time insertion and deletion of sorted arrays, and binary trees retain the ability to perform all the operations possible on a sorted array, including range and approximate queries.|$|E
40|$|This paper defines {{constraint}} <b>search</b> <b>trees,</b> {{a general}} tree data structure for storing and accessing items with constraints as keys. Constraint <b>search</b> <b>trees</b> can mimic binary <b>search</b> <b>trees,</b> radix <b>search</b> <b>trees,</b> k-d trees, R-trees {{and many other}} kinds of <b>search</b> <b>trees.</b> The unifying framework of constraint <b>search</b> <b>trees</b> immediately suggests new kinds of spatial data indexing techniques, as well as showing how to generalize intersection queries in spatial data structures for arbitrary shapes defined by constraints. Constraint <b>search</b> <b>trees</b> provide a data structure for storing sets (or disjunctions) of constraints. Hence they provide a useful data structure for implementing constraint databases. We define efficient algorithms for constraint database operations such as constraint selection, join and subsumption applied to constraint <b>search</b> <b>trees.</b> 1 Introduction <b>Search</b> <b>trees</b> are a fundamental data structure of computer science, providing a way of storing collection of objects which allows efficien [...] ...|$|R
50|$|The {{running time}} of ternary <b>search</b> <b>trees</b> varies {{significantly}} with the input. Ternary <b>search</b> <b>trees</b> run best when given several similar strings, especially when those strings {{share a common}} prefix. Alternatively, ternary <b>search</b> <b>trees</b> are effective when storing {{a large number of}} relatively short strings (such as words in a dictionary).Running times for ternary <b>search</b> <b>trees</b> are similar to binary <b>search</b> <b>trees,</b> in that they typically run in logarithmic time, but can run in linear time in the degenerate (worst) case.|$|R
40|$|We {{present a}} new linear time {{heuristic}} for constructing binary <b>search</b> <b>trees.</b> The {{analysis of the}} algorithm, by establishing an upper bound {{on the cost of}} the produced binary <b>search</b> <b>trees,</b> permits to derive a limitation on the cost of optimal binary <b>search</b> <b>trees.</b> The obtained upper bound improve on previous results...|$|R
25|$|In {{applications}} of binary <b>search</b> <b>tree</b> data structures, {{it is rare}} for the values in the tree to be inserted without deletion in a random order, limiting the direct {{applications of}} random binary trees. However, algorithm designers have devised data structures that allow insertions and deletions to be performed in a binary <b>search</b> <b>tree,</b> at each step maintaining as an invariant the property that {{the shape of the}} tree is a random variable with the same distribution as a random binary <b>search</b> <b>tree.</b>|$|E
25|$|A {{contemporaneous}} algorithm of , although {{presented in}} different terms, {{can be viewed}} as being the same as the Bron–Kerbosch algorithm, as it generates the same recursive <b>search</b> <b>tree.</b>|$|E
25|$|The {{size of the}} <b>search</b> <b>tree</b> {{depends on}} the number of (children) tableau that can be {{generated}} from a given (parent) one. Reducing the number of such tableau therefore reduces the required search.|$|E
500|$|However, {{binary search}} is usually more {{efficient}} for searching as binary <b>search</b> <b>trees</b> {{will most likely}} be imperfectly balanced, resulting in slightly worse performance than binary search. This applies even to balanced binary <b>search</b> <b>trees,</b> binary <b>search</b> <b>trees</b> that balance their own nodes—as they rarely produce optimally-balanced trees—but to a lesser extent. Although unlikely, the tree may be severely imbalanced with few internal nodes with two children, resulting in the average and worst-case search time approaching [...] comparisons. Binary <b>search</b> <b>trees</b> take more space than sorted arrays.|$|R
40|$|Random binary <b>search</b> <b>trees,</b> b-ary <b>search</b> <b>trees,</b> median-of-(2 k+ 1) trees, quadtrees, simplex trees, tries, {{and digital}} <b>search</b> <b>trees</b> are special cases of random split trees. For these trees, we o#er a {{universal}} law {{of large numbers}} and a limit law for {{the depth of the}} last inserted point, as well as a law of large numbers for the height...|$|R
40|$|Constraint <b>search</b> <b>trees</b> are {{a generic}} {{approach}} to <b>search</b> <b>trees</b> where all operations are {{defined in terms}} of constraints. This abstract viewpoint makes clear the fundamental operations of <b>search</b> <b>trees</b> and immediately points to new possibilities for <b>search</b> <b>trees.</b> In this paper we present height-balanced constraint <b>search</b> <b>trees</b> (HCSTs), a general approach to building height-balanced index structures, and exemplify the approach with a new spatial index structure, the O-tree. An object in an O-tree is represented by constraints of the form ax i + bx j d where fa; bg f 1; 0; 1 g and x 1; : : :; xn are the dimensions of the spatial data. We define the basic operations to build and search HCSTs, as well as constraint joins. We illustrate these algorithms using O-trees showing how the algorithms can make use of the more accurate information in the O-tree nodes. Experiments compare the IO-performance of the 2 dimensional O-tree with the R-tree. 1. Introduction <b>Search</b> <b>trees</b> are a fundamental [...] ...|$|R
25|$|Every finite {{connected}} {{undirected graph}} {{has at least}} one Trémaux tree. One can construct such a tree by performing a depth-first search and connecting each vertex (other than the starting vertex of the search) to the earlier vertex from which it was discovered. The tree constructed in this way is known as a depth-first <b>search</b> <b>tree.</b> If uv is an arbitrary edge in the graph, and u is the earlier of the two vertices to be reached by the search, then v must belong to the subtree descending from u in the depth-first <b>search</b> <b>tree,</b> because the search will necessarily discover v while it is exploring this subtree, either from one of the other vertices in the subtree or, failing that, from u directly. Every finite Trémaux tree can be generated as a depth-first search tree: If T is a Trémaux tree of a finite graph, and a depth-first search explores the children in T of each vertex prior to exploring any other vertices, it will necessarily generate T as its depth-first <b>search</b> <b>tree.</b>|$|E
25|$|The {{backtracking}} {{depth-first search}} program, a slight improvement on the permutation method, constructs the <b>search</b> <b>tree</b> by considering one {{row of the}} board at a time, eliminating most nonsolution board positions {{at a very early}} stage in their construction.|$|E
25|$|In {{computer}} science and probability theory, a random binary tree is a binary tree selected at random from some probability distribution on binary trees. Two different distributions are commonly used: binary trees formed by inserting nodes {{one at a}} time according to a random permutation, and binary trees chosen from a uniform discrete distribution in which all distinct trees are equally likely. It is also possible to form other distributions, for instance by repeated splitting. Adding and removing nodes directly in a random binary tree will in general disrupt its random structure, but the treap and related randomized binary <b>search</b> <b>tree</b> data structures use the principle of binary trees formed from a random permutation in order to maintain a balanced binary <b>search</b> <b>tree</b> dynamically as nodes are inserted and deleted.|$|E
50|$|Hashtables {{can also}} be used in place of ternary <b>search</b> <b>trees</b> for mapping strings to values. However, hash maps also {{frequently}} use more memory than ternary <b>search</b> <b>trees</b> (but not as much as tries). Additionally, hash maps are typically slower at reporting a string that is not in the same data structure, because it must compare the entire string rather than just the first few characters. There is some evidence that shows ternary <b>search</b> <b>trees</b> running faster than hash maps. Additionally, hash maps do not allow for many of the uses of ternary <b>search</b> <b>trees,</b> such as near-neighbor lookups.|$|R
40|$|AbstractIn {{this paper}} we {{establish}} new bounds {{on the problem}} of constructing optimum binary <b>search</b> <b>trees</b> with zero-key access probabilities (with applications e. g. to point location problems). We present a linear-time heuristic for constructing such <b>search</b> <b>trees</b> so that their cost is within a factor of 1 + ε from the optimum cost, where ε is an arbitrary small positive constant. Furthermore, by using an interesting amortization argument, we give a simple and practical, linear-time implementation of a known greedy heuristics for such trees. The above results are obtained in a more general setting, namely in the context of minimum length triangulations of so-called semi-circular polygons. They are carried over to binary <b>search</b> <b>trees</b> by proving a duality between optimum (m − 1) -way <b>search</b> <b>trees</b> and minimum weight partitions of infinitely-flat semi-circular polygons into m-gons. With this duality we can also obtain better heuristics for minimum length partitions of polygons by using known algorithms for optimum <b>search</b> <b>trees...</b>|$|R
40|$|The Wiener index {{has been}} studied for simply {{generated}} random trees, non-plane unlabeled random trees and a huge subclass of random grid trees containing random binary <b>search</b> <b>trees,</b> random medianof-(2 k + 1) <b>search</b> <b>trees,</b> random m-ary <b>search</b> <b>trees,</b> random quadtrees, random simplex trees, etc. An important class of random trees for which the Wiener index was not studied so far are random digital trees. In this work, we close this gap. More precisely, we derive asymptotic expansions of moments of the Wiener index and show that a central limit law for the Wiener index holds. These results are obtained for digital <b>search</b> <b>trees</b> and bucket versions as well as tries and PATRICIA tries. Our findings answer in affirmative two questions posed by Neininger. ...|$|R
25|$|Even in {{countable}} graphs, a {{depth-first search}} might {{not succeed in}} eventually exploring the entire graph, and not every normal spanning tree can be generated by a depth-first search: to be a depth-first <b>search</b> <b>tree,</b> a countable normal spanning tree must have only one infinite path or one node with infinitely many children (and not both).|$|E
25|$|Searching {{is similar}} to searching a binary <b>search</b> <b>tree.</b> Starting at the root, the tree is recursively traversed from top to bottom. At each level, the search reduces its field of view to the child pointer (subtree) whose range {{includes}} the search value. A subtree's range {{is defined by the}} values, or keys, contained in its parent node. These limiting values are also known as separation values.|$|E
25|$|Although it is {{a binary}} tree (each vertex has two children), the Calkin–Wilf tree {{is not a}} binary search tree: its inorder does not {{coincide}} with the sorted order of its vertices. However, it {{is closely related to}} a different binary <b>search</b> <b>tree</b> on the same set of vertices, the Stern–Brocot tree: the vertices at each level of the two trees coincide, and are related to each other by a bit-reversal permutation.|$|E
5|$|Binary <b>search</b> <b>trees</b> lend {{themselves}} to fast searching in external memory stored in hard disks, as binary <b>search</b> <b>trees</b> can effectively be structured in filesystems. The B-tree generalizes this method of tree organization; B-trees are frequently used to organize long-term storage such as databases and filesystems.|$|R
40|$|AbstractWe {{present a}} {{detailed}} study of left–right-imbalance measures for random binary <b>search</b> <b>trees</b> under the random permutation model, i. e., where binary <b>search</b> <b>trees</b> are generated by random permutations of { 1, 2,…,n}. For random binary <b>search</b> <b>trees</b> of size n we study (i) {{the difference between the}} left and the right depth of a randomly chosen node, (ii) the difference between {{the left and the right}} depth of a specified node j=j(n), and (iii) the difference between the left and the right pathlength, and show for all three imbalance measures limiting distribution results...|$|R
50|$|Compared to hash tables, these {{structures}} have both advantages and weaknesses. The worst-case performance of self-balancing binary <b>search</b> <b>trees</b> is {{significantly better than}} that of a hash table, with a time complexity in big O notation of O(log n). This is in contrast to hash tables, whose worst-case performance involves all elements sharing a single bucket, resulting in O(n) time complexity. In addition, and like all binary <b>search</b> <b>trees,</b> self-balancing binary <b>search</b> <b>trees</b> keep their elements in order. Thus, traversing its elements follows a least-to-greatest pattern, whereas traversing a hash table can result in elements being in seemingly random order. However, hash tables have a much better average-case time complexity than self-balancing binary <b>search</b> <b>trees</b> of O(1), and their worst-case performance is highly unlikely when a good hash function is used.|$|R
25|$|An {{exhaustive}} search algorithm can {{solve the problem}} in time 2kn'O(1). Vertex cover is therefore fixed-parameter tractable, {{and if we are}} only interested in small k, we can solve the problem in polynomial time. One algorithmic technique that works here is called bounded <b>search</b> <b>tree</b> algorithm, and its idea is to repeatedly choose some vertex and recursively branch, with two cases at each step: place either the current vertex or all its neighbours into the vertex cover.|$|E
25|$|One case {{in which}} peek is not trivial is in an ordered list type (i.e., {{elements}} accessible in order) implemented by a self-balancing binary <b>search</b> <b>tree.</b> In this case find-min or find-max take O(log n) time, as does access to any other element. Making find-min or find-max take O(1) time {{can be done by}} caching the min or max values, but this adds overhead to the data structure and to the operations of adding or removing elements.|$|E
25|$|The {{benefit of}} alpha–beta pruning {{lies in the}} fact that {{branches}} of the <b>search</b> <b>tree</b> can be eliminated. This way, the search time can be limited to the 'more promising' subtree, and a deeper search can be performed in the same time. Like its predecessor, it belongs to the branch and bound class of algorithms. The optimization reduces the effective depth to slightly more than half that of simple minimax if the nodes are evaluated in an optimal or near optimal order (best choice for side on move ordered first at each node).|$|E
40|$|AbstractSearch <b>trees,</b> {{specially}} binary <b>search</b> <b>trees,</b> {{are very}} important data structures thatcontributed immensely to improved performance of different search algorithms. In this paper, we express certain parameters of <b>search</b> <b>trees</b> in terms of Stirling numbers. We also introduce two new inversion formulas relating Stirling numbers {{of the first and}} second kinds...|$|R
40|$|Abstract. We {{present a}} {{detailed}} study of left-right-imbalance measures for random binary <b>search</b> <b>trees</b> under the random permutation model, i. e., where binary <b>search</b> <b>trees</b> are generated by random permutations of { 1, 2, [...] ., n}. For random binary <b>search</b> <b>trees</b> of size n we study (i) {{the difference between the}} left and the right depth of a randomly chosen node, (ii) the difference between {{the left and the right}} depth of a specified node j = j(n), and (iii) the difference between the left and the right pathlength, and show for all three imbalance measures limiting distribution results. 1...|$|R
5000|$|A Note on the Horton-Strahler Number for Random Binary <b>Search</b> <b>Trees,</b> (1999) ...|$|R
25|$|Alpha–beta pruning is {{a search}} {{algorithm}} {{that seeks to}} decrease the number of nodes that are evaluated by the minimax algorithm in its <b>search</b> <b>tree.</b> It is an adversarial search algorithm used commonly for machine playing of two-player games (Tic-tac-toe, Chess, Go, etc.). It stops completely evaluating a move when at least one possibility {{has been found that}} proves the move to be worse than a previously examined move. Such moves need not be evaluated further. When applied to a standard minimax tree, it returns the same move as minimax would, but prunes away branches that cannot possibly influence the final decision.|$|E
25|$|In {{computer}} science, a trie, {{also called}} digital tree and sometimes radix tree or prefix tree (as {{they can be}} searched by prefixes), {{is a kind of}} search tree—an ordered tree data structure that is used to store a dynamic set or associative array where the keys are usually strings. Unlike a binary <b>search</b> <b>tree,</b> no node in the tree stores the key associated with that node; instead, its position in the tree defines the key with which it is associated. All the descendants of a node have a common prefix of the string associated with that node, and the root is associated with the empty string. Values are not necessarily associated with every node. Rather, values tend only to be associated with leaves, and with some inner nodes that correspond to keys of interest. For the space-optimized presentation of prefix tree, see compact prefix tree.|$|E
25|$|For any set {{of numbers}} (or, more generally, values from some total order), one may form a binary <b>search</b> <b>tree</b> {{in which each}} number is {{inserted}} in sequence as a leaf of the tree, without changing {{the structure of the}} previously inserted numbers. The position into which each number should be inserted is uniquely determined by a binary search in the tree formed by the previous numbers. For instance, if the three numbers (1,3,2) are inserted into a tree in that sequence, the number 1 will sit {{at the root of the}} tree, the number 3 will be placed as its right child, and the number 2 as the left child of the number 3. There are six different permutations of the numbers (1,2,3), but only five trees may be constructed from them. That is because the permutations (2,1,3) and (2,3,1) form the same tree.|$|E
50|$|Ternary {{trees are}} used to {{implement}} Ternary <b>search</b> <b>trees</b> and Ternary heaps.|$|R
5000|$|... #Caption: Binary <b>search</b> <b>trees</b> are <b>searched</b> {{using an}} {{algorithm}} similar to binary search.|$|R
40|$|Abstract We {{study the}} profile Xn,k of random <b>search</b> <b>trees</b> {{including}} binary <b>search</b> <b>trees</b> and m-ary <b>search</b> <b>trees.</b> Our main {{result is a}} functional limit theorem of the normalized profile Xn,k / E Xn,k for k = bff log nc in a certain range of ff. A central feature of the proof {{is the use of}} the contraction method to prove convergence in distribution of certain random analytic functions in a complex domain. This is based on a general theorem on the contraction method for random variables in an infinite dimensional Hilbert space. As part of the proof, we show that the Zolotarev metric is complete for a Hilbert space...|$|R
