297|95|Public
25|$|Binary newsgroups {{are only}} able to {{function}} reliably if there is sufficient storage allocated to a group to allow readers enough time to download all parts of a binary posting before it is flushed out of the group's <b>storage</b> <b>allocation.</b> This was at one time how posting of undesired content was countered; the newsgroup would be flooded with random garbage data posts, of sufficient quantity to push out all the content to be suppressed. This has been compensated by service providers allocating enough storage to retain everything posted each day, including such spam floods, without deleting anything.|$|E
25|$|Each {{news server}} {{generally}} allocates {{a certain amount}} of storage space for post content in each newsgroup. When this storage has been filled, each time a new post arrives, old posts are deleted {{to make room for the}} new content. If the network bandwidth available to a server is high but the <b>storage</b> <b>allocation</b> is small, it is possible for a huge flood of incoming content to overflow the allocation and push out everything that was in the group before it. If the flood is large enough, the beginning of the flood will begin to be deleted even before the last part of the flood has been posted.|$|E
25|$|In principle, the Life {{field is}} infinite, but {{computers}} have finite memory. This leads to problems when the active area encroaches {{on the border}} of the array. Programmers have used several strategies to address these problems. The simplest strategy is simply to assume that every cell outside the array is dead. This is easy to program, but leads to inaccurate results when the active area crosses the boundary. A more sophisticated trick is to consider the left and right edges of the field to be stitched together, and the top and bottom edges also, yielding a toroidal array. The result is that active areas that move across a field edge reappear at the opposite edge. Inaccuracy can still result if the pattern grows too large, but at least there are no pathological edge effects. Techniques of dynamic <b>storage</b> <b>allocation</b> may also be used, creating ever-larger arrays to hold growing patterns.|$|E
50|$|Norton Zone offered 5 GB {{of storage}} {{for free and}} larger <b>storage</b> <b>allocations</b> via subscription.|$|R
30|$|The map of {{the costs}} {{obtained}} with the simulation environment developed can be a decision support system to choose the best model evaluating the condition of production rate, preventive maintenance actions time required, and the buffer <b>storages</b> <b>allocation.</b>|$|R
40|$|Abstract—Container {{handling}} {{problems at}} container terminals are NP-hard problems. This paper presents an approach using discrete-event simulation modeling to optimize solution for <b>storage</b> space <b>allocation</b> problem, {{taking into account}} all various interrelated container terminal handling activities. The proposed approach is applied on a real case study data of container terminal at Alexandria port. The computational results show {{the effectiveness of the}} proposed model for optimization of <b>storage</b> space <b>allocation</b> in container terminal where 54 % reduction in containers handling time in port is achieved. Keywords—Container terminal, discrete-event simulation, optimization, <b>storage</b> space <b>allocation.</b> I...|$|R
5000|$|Support for {{multiple}} volume groups for optimal <b>storage</b> <b>allocation</b> ...|$|E
5000|$|... {{infinite}} store : The programmer is {{not responsible}} for <b>storage</b> <b>allocation</b> and can create as many data objects as needed.|$|E
5000|$|Tools or hooks for {{database}} design, {{application program}}ming, application program maintenance, database performance analysis and monitoring, database configuration monitoring, DBMS hardware configuration (a DBMS and related database may span computers, networks, and storage units) and related database mapping (especially for a distributed DBMS), <b>storage</b> <b>allocation</b> and database layout monitoring, storage migration, etc.|$|E
40|$|The current {{version of}} the {{computer}} program NONSAP for linear and nonlinear, static and dynamic finite element analysis is presented. The solution capabilities, the numerical techniques used, the finite element library, the logical construction {{of the program and}} <b>storage</b> <b>allocations</b> are discussed. The solutions of some sample problems considered during the development of the program are presented...|$|R
40|$|In this paper, {{we provide}} an {{overview}} of Logistical Runtime System (LoRS). LoRS is an integrated ensemble of tools and services that aggregate primitive (best effort, faulty) <b>storage</b> <b>allocations</b> to obtain strong properties such as capacity, performance, reliability, that Grid applications desire. The paper focuses on the design and implementation of LoRS, as well as the storage scheduling decisions that LoRS must make. ...|$|R
40|$|This paper {{introduces}} Multiagent Systems (MAS) <b>storage</b> resource <b>allocation</b> algorithms {{and methods}} on a Peer-to-Peer (P 2 P) system of computer storage resources. MAS Complex Adaptive Systems (CAS) based on squirrels behaviors are proposed and evaluated to produce emergent global behaviors that can solve the <b>storage</b> resource <b>allocation</b> {{problem in a}} distributed system of peers. Experimental results support the initial hypothesis that hoarding mechanisms found on squirrels behaviors efficiently allocate resources in distributed systems...|$|R
50|$|The ACM Transactions on Programming Languages and Systems (TOPLAS) is a {{bimonthly}} peer-reviewed scientific journal on programming languages {{published by}} the Association for Computing Machinery since 1979. The current editor-in-chief is Jens Palsberg. Its scope includes programming language design, implementation, and semantics of programming languages, compilers and interpreters, run-time systems, <b>storage</b> <b>allocation</b> and garbage collection, and formal specification, testing, and verification of software.|$|E
5000|$|There were (at least) two incarnations of Flex, {{implemented}} using hardware with writable microcode. The {{first was}} supplied by Logica to an RSRE design, {{and the second}} used an ICL PERQ. [...] The microcode alone was responsible for <b>storage</b> <b>allocation,</b> deallocation and garbage collection. This immediately precluded a whole class of errors arising from the misuse (deliberate or accidental) of pointers.|$|E
50|$|After {{designing}} a database for an application, {{the next stage}} is building the database. Typically, an appropriate general-purpose DBMS can be selected to be utilized for this purpose. A DBMS provides the needed user interfaces to be utilized by database administrators to define the needed application's data structures within the DBMS's respective data model. Other user interfaces are used to select needed DBMS parameters (like security related, <b>storage</b> <b>allocation</b> parameters, etc.).|$|E
40|$|In {{this work}} we present the Internet Backplane Protocol (IBP), a {{middleware}} created {{to allow the}} sharing of storage resources, implemented {{as part of the}} network fabric. IBP allows an application to control intermediate data staging operations explicitly. As IBP follows a very simple philosophy, very similar to the Internet Protocol, and the resulting semantic might be too weak for some applications, we introduce the exNode, a data structure that aggregates <b>storage</b> <b>allocations</b> on the Internet...|$|R
30|$|Instead of {{a linear}} mapping, Tronçon et al. {{proposed}} to compute an m-dimensional mapping window for each m-dimensional array [11]: {{the sides of}} a window were computed based on the maximal index difference in each dimension between array elements simultaneously alive. (The bounding-window mapping is also used in PPCG—a source-to-source compiler using polyhedral compilation techniques that extracts data-parallelism with a code generator for a modern graphics processing unit [12]). Darte et al. proposed a lattice-based mathematical framework for array mapping, establishing a correspondence between valid linear <b>storage</b> <b>allocations</b> and integer lattices called strictly admissible [13].|$|R
30|$|The {{equations}} for cache performance {{trends of}} private and public data requests {{are subject to the}} size of the available space, L, in the common cache. As such it is not possible to individually change the <b>storage</b> <b>allocations</b> for given sets of content without affecting other cached datasets. Hence, even though the shared cache configuration is simpler to implement and is less computationally expensive because of having all the cached datasets on a single global list for cache optimisation, the design does not permit flexible allocation of cache space that would grade various sets of content according their assigned QoS categories.|$|R
5000|$|Despite the name, FASTRAND was slow. The head {{positioning}} time was significant, so software allocated storage by tracks (1,792 words, 10,752 characters or 8,064 8 bit bytes) or [...] "positions", {{a group of}} 64 tracks (114,688 words, 688,128 characters or 510,096 8 bit bytes) which were under the heads at a single time. This <b>storage</b> <b>allocation</b> method remained on the 1100 series machines long after drums {{had been replaced by}} disks.|$|E
5000|$|The PL/I D compiler, using 16 {{kilobytes}} of memory, {{was developed}} by IBM Germany for the DOS/360 low end operating system. It implements {{a subset of the}} PL/I language requiring all strings and arrays to have fixed extents, thus simplifying the run-time environment. Reflecting the underlying operating system, it lacks dynamic <b>storage</b> <b>allocation</b> and the controlled storage class. [...] It was shipped within a year of PL/I F.|$|E
50|$|Unlike the {{database}} where global variable nodes must fit {{within a database}} block, local variable strings can grow to 1MB. The GT.M run-time provides dynamic <b>storage</b> <b>allocation</b> with garbage collection. The number of local variables {{and the number of}} nodes in local variables are limited only by storage available to the process. The default scope of a local variable is the lifetime of a process. Local variables created within routines using the New command have more limited scope.|$|E
40|$|Container {{handling}} {{problems at}} container terminals are NP-hard problems. This paper presents an approach using discrete-event simulation modeling to optimize solution for <b>storage</b> space <b>allocation</b> problem, {{taking into account}} all various interrelated container terminal handling activities. The proposed approach is applied on a real case study data of container terminal at Alexandria port. The computational results show {{the effectiveness of the}} proposed model for optimization of <b>storage</b> space <b>allocation</b> in container terminal where 54 % reduction in containers handling time in port is achieved. Comment: International Journal of Computer, Information, Systems and Control Engineering Vol: 9 No: 1, 201...|$|R
40|$|In {{this paper}} we {{consider}} distributed allocation problems with memory constraint limits. Firstly, we propose a tractable relaxation {{to the problem}} of optimal symmetric allocations from [1]. The approximated problem is based on the Q-error function, and its solution approaches the solution of the initial problem, as the number of storage nodes in the network grows. Secondly, exploiting this relaxation, we are able to formulate and to solve the problem for <b>storage</b> <b>allocations</b> for memory-limited DSS storing and arbitrary memory profiles. Finally, we discuss the extension to the case of multiple data objects, stored in the DSS. Comment: Submitted to IEEE GLOBECOM' 1...|$|R
40|$|International audienceIn {{this paper}} we {{consider}} distributed allocation problems with memory constraint limits. Firstly, we propose a tractable relaxation {{to the problem}} of optimal symmetric allocations, considered by Leong et al in 2012. The approximated problem is based on the Q-error function, and its solution approaches the solution of the initial problem, as the number of storage nodes in the network grows. Secondly, exploiting this relaxation, we are able to formulate and to solve the problem for <b>storage</b> <b>allocations</b> for a memory-limited distributed storage system (DSS), with an arbitrary memory profile. Finally, we discuss the extension to the case of storing multiple data objects in one DSS...|$|R
5000|$|External {{fragmentation}} {{arises when}} free memory is separated into small blocks and is interspersed by allocated memory. It is a weakness of certain <b>storage</b> <b>allocation</b> algorithms, when {{they fail to}} order memory used by programs efficiently. The result is that, although free storage is available, it is effectively unusable because it is divided into pieces that are too small individually to satisfy {{the demands of the}} application. The term [...] "external" [...] refers {{to the fact that the}} unusable storage is outside the allocated regions.|$|E
5000|$|Uniquely, the {{object-oriented}} paradigm involves {{dynamic memory}} allocation from heap storage for both object creation and message passing. A 1994 benchmark - [...] "Memory Allocation Costs in Large C and C++ Programs" [...] conducted by Digital Equipment Corporation {{on a variety}} of software, using an instruction-level profiling tool, measured how many instructions were required per dynamic <b>storage</b> <b>allocation.</b> The results showed that the lowest absolute number of instructions executed averaged around 50 but others reached as high as 611. See also [...] "Heap:Pleasures and pains" [...] by Murali R. Krishnan that states [...] "Heap implementations tend to stay general for all platforms, and hence have heavy overhead". The 1996 IBM paper [...] "Scalability of Dynamic <b>Storage</b> <b>Allocation</b> Algorithms" [...] by Arun Iyengar of IBM [...] demonstrates various dynamic storage algorithms and their respective instruction counts. Even the recommended MFLF I algorithm (H.S. Stone, RC 9674) shows instruction counts in a range between 200 and 400. The above pseudocode example does not include a realistic estimate of this memory allocation pathlength or the memory prefix overheads involved and the subsequent associated garbage collection overheads. Suggesting strongly that heap allocation is a nontrivial task, one open-source software microallocator, by game developer John W. Ratcliff, consists of nearly 1,000 lines of code.|$|E
50|$|Coral 66 is a {{general-purpose}} {{programming language}} based on ALGOL 60, with some features from Coral 64, JOVIAL, and FORTRAN. It includes structured record types (as in Pascal) and supports the packing of data into limited storage (also as in Pascal). Like Edinburgh IMP it allows embedded assembler, and also offers good run-time checking and diagnostics. It is specifically intended for real-time and embedded applications and {{for use on}} computers with limited processing power, including those limited to fixed point arithmetic and those without support for dynamic <b>storage</b> <b>allocation.</b>|$|E
40|$|Amazon's phenomenal {{sales growth}} {{and desire to}} {{maintain}} "Earth's Biggest Selection" have led {{to an increase in}} the diversity of product offerings that has resulted in a corresponding increase in complexity of Amazon's warehouse storage management. There is currently limited insight into the trade offs between the capital, fixed and variable costs of Amazon's storage related operational decisions, leading to inefficient warehouse <b>storage</b> type <b>allocations</b> and higher operational costs. The focus of this six-month LGO internship was to develop a cost model that takes into account all relevant costs to develop recommendations on warehouse <b>storage</b> type <b>allocations</b> for both existing and new fulfillment centers in Amazon's North America Fulfillment Center network. This thesis begins with an overview of Amazon and a description of their fulfillment center network. The overview is followed by a literature review of current warehouse design frameworks and storage optimization research. The following chapter analyzes the current inbound warehouse processes to identify what the relevant storage decisions are, where they are being made, and the current decision making process. Finally, through {{the development and implementation of}} a cost model and an analysis of key findings, the thesis provides recommendations for cost-optimized warehouse <b>storage</b> type <b>allocations.</b> The major recommendations are to replace floor pallet storage within existing fulfillment centers, increasing Non-Sortable product mix in select existing Sortable fulfillment centers, and optimized <b>storage</b> type <b>allocations</b> for new fulfillment centers. The expected scaled annual cost savings associated with these cost optimized warehouse <b>storage</b> type <b>allocations</b> within the existing fulfillment centers is 34 % across the entire network and 62 % for the select Sortable fulfillment center. The expected scaled annual cost savings associated with the optimized <b>storage</b> type <b>allocations</b> for the new fulfillment centers is 24 % per new Sortable building and 11 % per new Non-Sortable building. The methodology utilized within the cost model to compare fixed, variable and capital costs can be applied more broadly to assess the cost impact of different storage types in any warehousing design framework. by Amy Lee. Thesis (M. B. A.) [...] Massachusetts Institute of Technology, Sloan School of Management; and, (S. M.) [...] Massachusetts Institute of Technology, Engineering Systems Division; in conjunction with the Leaders for Global Operations Program at MIT, 2013. Cataloged from PDF version of thesis. Includes bibliographical references (p. 58) ...|$|R
40|$|With the {{increasing}} penetration of wind power, {{not only the}} uncertainties but also the correlation among the wind farms {{should be considered in}} the power system analysis. In this paper, Clayton-Copula method is developed to model the multiple correlated wind distribution and a new point estimation method (PEM) is proposed to discretize the multi-correlated wind distribution. Furthermore, combining the proposed modeling and discretizing method with Hybrid Multi-Objective Particle Swarm Optimization (HMOPSO), a comprehensive algorithm is explored to minimize the power system cost and the emissions by searching the best placements and sizes of energy storage system (ESS) considering wind power uncertainties in multi-correlated wind farms. In addition, the variations of load are also taken into account. The IEEE 57 -bus system is adopted to perform case studies using the proposed approach. The results clearly demonstrate the effectiveness of the proposed algorithm in determining the optimal <b>storage</b> <b>allocations</b> considering multi-correlated wind farms...|$|R
5000|$|File contiguity, {{because many}} {{filesystem}} architectures employ higher I/O speeds if transferring data on contiguous {{areas of the}} <b>storage,</b> whereas random <b>allocation</b> might prevent real-time or better loading performances.|$|R
5000|$|Thin provisioning, in a shared-storage environment, {{provides}} {{a method for}} optimizing utilization of available storage. It relies on on-demand allocation of blocks of data versus the traditional method of allocating all the blocks in advance. This methodology eliminates almost all whitespace which helps avoid the poor utilization rates, often as low as 10%, {{that occur in the}} traditional <b>storage</b> <b>allocation</b> method where large pools of storage capacity are allocated to individual servers but remain unused (not written to). This traditional model is often called [...] "fat" [...] or [...] "thick" [...] provisioning.|$|E
50|$|Binary newsgroups {{are only}} able to {{function}} reliably if there is sufficient storage allocated to a group to allow readers enough time to download all parts of a binary posting before it is flushed out of the group's <b>storage</b> <b>allocation.</b> This was at one time how posting of undesired content was countered; the newsgroup would be flooded with random garbage data posts, of sufficient quantity to push out all the content to be suppressed. This has been compensated by service providers allocating enough storage to retain everything posted each day, including such spam floods, without deleting anything.|$|E
5000|$|Dartmouth College {{developed}} two implementations of ALGOL 60 for the LGP-30. Dartmouth ALGOL 30 was a three-pass system (compiler, loader, and interpreter) {{that provided}} almost all features of ALGOL except those requiring run-time <b>storage</b> <b>allocation.</b> SCALP, a Self Contained Algol Processor, was a one-pass {{system for a}} small subset of ALGOL (no blocks other than the entire program), no procedure declarations, conditional statements but no conditional expressions, no constructs other than [...] in a [...] statement, no nested [...] declarations (nested calls are permitted), and no boolean variables and operators. As in ACT-III, every token had to be separated by an apostrophe.|$|E
40|$|International audienceWe study a {{deterministic}} {{storage yard}} management problem which jointly determines <b>storage</b> space <b>allocation</b> and yard crane assignment. The problem is first formulated as an integer grogram {{to minimize the}} total operational cost. To solve large-scaled problems, we then transform it to a set partitioning formulation and provide a framework of branch-and-price algorithm. Computational experiments to explain {{the performance of the}} solution approach are to be further conducted...|$|R
40|$|A common way of {{implementing}} multivariate polynomial multiplication and division is to represent polynomials as linked lists of terms sorted in a term ordering {{and to use}} repeated merging. This results in poor performance on large sparse polynomials. In this paper we use an auxiliary heap of pointers {{to reduce the number}} of monomial comparisons in the worst case while keeping the overall storage linear. We give two variations. In the first, the size of the heap is bounded by the number of terms in the quotient(s). In the second, which is new, the size is bounded by the number of terms in the divisor(s). We use dynamic arrays of terms rather than linked lists to reduce <b>storage</b> <b>allocations</b> and indirect memory references. We pack monomials in the array to reduce storage and to speed up monomial comparisons. We give a new packing for the graded reverse lexicographical ordering. We have implemented the heap algorithms in C with an interface to Maple. For comparison we have also implemented Yan’s “geobuckets” data structure. Our timings demonstrate that heaps of pointers are comparable in speed with geobuckets but use significantly less storage...|$|R
40|$|Abstract. A common way of {{implementing}} multivariate polynomial multiplication and division is to represent polynomials as a linked list of terms sorted in a term ordering {{and to use}} repeated merging. This results in poor performance on large sparse polynomials. In this paper we use an auxiliary heap of pointers {{to reduce the number}} of monomial comparisons in the worst case while keeping the overall storage linear. We give two variations. In the first, the size of the heap is bounded by the number of terms in the quotient(s). In the second, which is new, the size is bounded by the number of terms in the divisor(s). We use dynamic arrays of terms rather than linked lists of terms to reduce <b>storage</b> <b>allocations</b> and indirect memory references. We pack monomials in the array to reduce storage and speed up monomial comparisons. We give a new packing for the graded reverse lexicographical monomial ordering. We have implemented the heap algorithms in C with an interface to Maple. For comparison we have also implemented Yan’s “geobuckets” data structure. Our timings demonstrate that heaps of pointers are comparable in speed with geobuckets but use significantly less storage. ...|$|R
