1598|10000|Public
25|$|The Seldon Plan is {{statistical}} in nature. Future {{events are}} described as being probabilities. The variables, as discussed (see above) require {{a very large number}} of human beings, literally the population of the Galaxy, in order to reduce the ordinarily random events concerning human affairs to become amenable to <b>statistical</b> <b>modelling.</b>|$|E
25|$|However, {{a flaw in}} how it handled {{very high}} first innings scores (350+) became {{apparent}} from the 1999 Cricket World Cup match in Bristol between India and Kenya. Tony Lewis noticed {{that there was an}} inherent weakness in the formula that would give a noticeable advantage to the side chasing a total in excess of 350. A correction was built into the formula and the software, but was not fully adopted until 2004. One-day matches were achieving significantly higher scores than in previous decades, affecting the historical relationship between resources and runs. The second version uses more sophisticated <b>statistical</b> <b>modelling,</b> but does not use a single table of resource percentages. Instead, the percentages also vary with score, so a computer is required. Therefore, it loses some of the previous advantages of transparency and simplicity.|$|E
2500|$|However, these clone indices {{rely on a}} <b>statistical</b> <b>modelling</b> process. [...] Such indices {{have too}} short a history to state whether this {{approach}} will be considered successful.|$|E
50|$|A <b>statistical</b> <b>model</b> is {{a special}} class of {{mathematical}} model. What distinguishes a <b>statistical</b> <b>model</b> from other mathematical models is that a <b>statistical</b> <b>model</b> is non-deterministic. Thus, in a <b>statistical</b> <b>model</b> specified via mathematical equations, some of the variables do not have specific values, but instead have probability distributions; i.e. some of the variables are stochastic. In the example above, ε is a stochastic variable; without that variable, the model would be deterministic.|$|R
40|$|Cracks {{are one of}} {{the hidden}} dangers in {{concrete}} dams. The study on safety monitoring models of concrete dam cracks has always been difficult. Using the parametric <b>statistical</b> <b>model</b> of safety monitoring of cracks in concrete dams, {{with the help of the}} semi-parametric statistical theory, and considering the abnormal behaviors of these cracks, the semi-parametric <b>statistical</b> <b>model</b> of safety monitoring of concrete dam cracks is established to overcome the limitation of the parametric model in expressing the objective model. Previous projects show that the semi-parametric <b>statistical</b> <b>model</b> has a stronger fitting effect and has a better explanation for cracks in concrete dams than the parametric <b>statistical</b> <b>model.</b> However, when used for forecast, the forecast capability of the semi-parametric <b>statistical</b> <b>model</b> is equivalent to that of the parametric <b>statistical</b> <b>model.</b> The modeling of the semi-parametric <b>statistical</b> <b>model</b> is simple, has a reasonable principle, and has a strong practicality, with a good application prospect in the actual project...|$|R
50|$|The {{assumptions}} {{embodied by}} a <b>statistical</b> <b>model</b> describe {{a set of}} probability distributions, {{some of which are}} assumed to adequately approximate the distribution from which a particular data set is sampled. The probability distributions inherent in <b>statistical</b> <b>models</b> are what distinguishes <b>statistical</b> <b>models</b> from other, non-statistical, mathematical models.|$|R
2500|$|This {{makes the}} {{geometric}} mean the only correct mean when averaging normalized results, that is {{results that are}} presented as ratios to reference values. [...] This is relevant because the beta distribution is a suitable model for the random behavior of percentages and it is particularly suitable to the <b>statistical</b> <b>modelling</b> of proportions. [...] The geometric mean plays {{a central role in}} maximum likelihood estimation, see section [...] "Parameter estimation, maximum likelihood." [...] Actually, when performing maximum likelihood estimation, besides the geometric mean GX based on the random variable X, also another geometric mean appears naturally: the geometric mean based on the linear transformation ––, the mirror-image of X, denoted by G(1−X): ...|$|E
2500|$|Queensland {{represents}} the single bloodiest colonial frontier in Australia. Thus {{the records of}} Queensland document the most frequent reports of shootings and massacres of indigenous people, the three deadliest massacres on white settlers, the most disreputable frontier police force, and {{the highest number of}} white victims to frontier violence on record in any Australian colony. In 2009 professor Raymond Evans calculated the indigenous fatalities caused by the Queensland Native Police Force alone as no less than 24,000. In July 2014, Evans, in cooperation with the Danish historian Robert Ørsted-Jensen, presented the first-ever attempt to use <b>statistical</b> <b>modelling</b> and a database covering no less than 644 collisions gathered from primary sources, and ended up with total fatalities suffered during Queensland's frontier wars being no less than 66,680—with Aboriginal fatalities alone comprising no less than 65,180—whereas the hitherto commonly accepted minimum overall continental deaths [...] had previously been 20,000. The 66,680 covers Native Police and settler-inflicted fatalities on Aboriginal people, but also a calculated estimate for Aboriginal inflicted casualties on the invading forces of whites and their associates. The continental death toll of Europeans and associates has previously been roughly estimated as between 2,000 and 2,500, yet there is now evidence that Queensland alone accounted for an estimated 1,500 of these fatal frontier casualties.|$|E
50|$|<b>Statistical</b> <b>Modelling</b> is a {{bimonthly}} peer-reviewed scientific journal covering <b>statistical</b> <b>modelling.</b> It {{is published}} by SAGE Publications {{on behalf of the}} <b>Statistical</b> <b>Modelling</b> Society. The editors-in-chief are Brian D. Marx (Louisiana State University), Jeffrey Simonoff (New York University), and Arnošt Komárek (Charles University in Prague).|$|E
40|$|<b>Statistical</b> <b>model</b> {{checking}} {{is one of}} {{the powerful}} methods, used to analyze any large system. Test bed experiments are used for analysis of routing algorithms in computer network. For more deeper penetration, we use <b>statistical</b> <b>model</b> checking to analyze properties and performance of opportunistic networks. In order to do so, we link a <b>statistical</b> <b>model</b> checker to a discrete event simulator for opportunistic network. This linking allows <b>statistical</b> <b>model</b> checking of several opportunistic network properties and protocols...|$|R
40|$|This paper {{presents}} a novel automatic liver segmentation algorithm which combines <b>statistical</b> <b>models</b> with machine learning. In the approach, {{three kinds of}} <b>statistical</b> <b>models</b> are developed, including <b>statistical</b> pose <b>model</b> (SPM), <b>statistical</b> shape <b>model</b> (SSM), and <b>statistical</b> appearance <b>model</b> (SAM). The algorithm contains three major processes, including prior collecting, <b>statistical</b> <b>models</b> building, and shape detecting. In prior collecting, based on benchmark of liver segmentation, the prior information about the liver is collected, including its position, pose, shape, texture, and the statistical intensity distribution of surrounding area. To fully utilise the prior information for segmentation, the <b>statistical</b> <b>models</b> building will build a support vector machine (SVM) classifier and the three <b>statistical</b> <b>models.</b> The shape detecting process will model the segmentation {{as a process of}} model evolution to derive the liver shape. Experiment results of liver segmentation on CT images using the proposed method are presented with performance validation and discussion...|$|R
40|$|We {{describe}} {{a method for}} learning <b>statistical</b> <b>models</b> of images using a second-order hidden Markov mesh model. First, an image can be segmented {{in a way that}} best matches its <b>statistical</b> <b>model</b> by an approach related to the dynamic programming used for segmenting Markov chains. Second, given an image segmentation, a <b>statistical</b> <b>model</b> (3 D state transition matrix and observation distributions within states) can be estimated. These two steps are repeated until convergence to provide both a segmentation and a <b>statistical</b> <b>model</b> of the image. We propose a statistical distance measure between images based on the similarity of their <b>statistical</b> <b>models,</b> for classification and retrieval tasks. 1...|$|R
5000|$|Extreme climate events, <b>statistical</b> <b>modelling</b> {{of extreme}} {{precipitation}} ...|$|E
5000|$|... 2009 <b>Statistical</b> <b>modelling</b> of {{the catastrophic}} {{atmospheric}} precipitation. In: Dynamics and mathematical modelling of geophysical and hydrometerologycal processes.|$|E
5000|$|Massively-parallel {{computation}} {{on demand}} for sample-based <b>statistical</b> <b>modelling</b> and motion planning, task planning, multi-robot collaboration, scheduling and coordination of system； ...|$|E
40|$|AbstractThe {{traditional}} monitoring <b>statistical</b> <b>model</b> of {{arch dam}} deformation is established based on early average temperature {{as the temperature}} factors, but its extension forecast function is poorer. Based on {{the analysis of the}} shortcomings and causes of the traditional monitoring <b>statistical</b> <b>model,</b> this paper has studied a new deformation <b>statistical</b> <b>model</b> (called TMTD <b>statistical</b> <b>model).</b> In TMTD <b>statistical</b> <b>model,</b> the temperature factors use mean temperature Tm and linear temperature difference Td {{in the direction of the}} thickness of horizontal sections on arch dam. Tm and Td are obtained by theoretical calculation, so it can better reflect and forecast changes of arch dam temperature field. The contrast of project cases in terms of forecast effect between two models has been presented, and the contrast shows, TMTD <b>statistical</b> <b>model</b> has significant superiority in terms of extension forecast of arch dam deformation...|$|R
40|$|Sensor-based <b>statistical</b> <b>models</b> {{promise to}} support a variety of {{advances}} in human-computer interaction, but building applications that use them is currently difficult and potential advances go unexplored. We present Subtle, a toolkit that removes some of the obstacles to developing and deploying applications using sensor-based <b>statistical</b> <b>models</b> of human situations. Subtle provides an appropriate and extensible sensing library, continuous learning of personalized models, fully-automated high-level feature generation, and support for using learned models in deployed applications. By removing obstacles to developing and deploying sensor-based <b>statistical</b> <b>models,</b> Subtle {{makes it easier to}} explore the design space surrounding sensor-based <b>statistical</b> <b>models</b> of human situations. Subtle thus helps to move the focus of human-computer interaction research onto applications and datasets, instead of the difficulties of developing and deploying sensor-based <b>statistical</b> <b>models.</b> Author Keywords Toolkits, Subtle, sensor-based <b>statistical</b> <b>models,</b> machin...|$|R
50|$|All {{statistical}} hypothesis tests and all statistical estimators {{are derived from}} <b>statistical</b> <b>models.</b> More generally, <b>statistical</b> <b>models</b> {{are part of the}} foundation of statistical inference.|$|R
50|$|Previous knowledge. This {{seems obvious}} but {{standard}} <b>statistical</b> <b>modelling</b> techniques seldom if ever start from {{any reference to}} any earlier model.|$|E
50|$|However, these clone indices {{rely on a}} <b>statistical</b> <b>modelling</b> process. Such indices {{have too}} short a history to state whether this {{approach}} will be considered successful.|$|E
50|$|The best modern {{lossless}} compressors use probabilistic models, such as prediction by partial matching. The Burrows-Wheeler transform {{can also}} {{be viewed as an}} indirect form of <b>statistical</b> <b>modelling.</b>|$|E
40|$|We {{analyze the}} {{information}} geometry and the entropic dynamics of a 3 D Gaussian <b>statistical</b> <b>model</b> and compare our analysis {{to that of}} a 2 D Gaussian <b>statistical</b> <b>model</b> obtained from the higher-dimensional model via introduction of an additional information constraint that resembles the quantum mechanical canonical minimum uncertainty relation. We uncover that the chaoticity of the 2 D Gaussian <b>statistical</b> <b>model,</b> quantified by means of the Information Geometric Entropy (IGE), is softened with respect to the chaoticity of the 3 D Gaussian <b>statistical</b> <b>model...</b>|$|R
30|$|According {{to scatter}} characteristics, PolSAR image {{classification}} methods {{can be divided}} into two categories: the classification method based on <b>statistical</b> <b>model</b> and the polarimetric target decomposition method. The mathematical methods based on the statistical method of <b>statistical</b> <b>model</b> are <b>statistical</b> <b>modeling</b> and Bayes theory [1]. Whether the <b>statistical</b> <b>model</b> is correctly established or not determines the accuracy of the classification method. The polarimetric target decomposition method can classify the PolSAR image without the probability distribution of data [2], such as H/ɑ, H/A/ɑ decomposition [3], Pauli decomposition [4], and Krogager decomposition [5].|$|R
40|$|We {{study the}} {{information}} geometry and the entropic dynamics of a three- dimensional Gaussian <b>statistical</b> <b>model.</b> We then compare our analysis {{to that of}} a two- dimensional Gaussian <b>statistical</b> <b>model</b> obtained from the higher-dimensional model via introduction of an additional information constraint that resembles the quantum mechan- ical canonical minimum uncertainty relation. We show that the chaoticity (temporal com- plexity) of the two-dimensional Gaussian <b>statistical</b> <b>model,</b> quantiﬁed by means of the information geometric entropy (IGE) and the Jacobi vector ﬁeld intensity, is softened with respect to the chaoticity of the three-dimensional Gaussian <b>statistical</b> <b>model...</b>|$|R
5000|$|Leo Breiman {{distinguished}} two <b>statistical</b> <b>modelling</b> paradigms: {{data model}} and algorithmic model, wherein [...] "algorithmic model" [...] means {{more or less}} the machine learning algorithms like Random forest.|$|E
50|$|He {{obtained}} his BSc {{from the}} University of Sheffield and PhD from the University of Manchester. His thesis concerned the <b>statistical</b> <b>modelling</b> of metallic corrosion, {{and the application}} of extreme value theory.|$|E
50|$|Key {{organizational}} areas {{refer to}} “People, Process and Technology” and the subcomponents include alignment, architecture, data, data governance, delivery, development, measurement, program governance, scope, skills, sponsorship, <b>statistical</b> <b>modelling,</b> technology, value and visualization.|$|E
40|$|This paper {{discusses}} {{the use of}} <b>statistical</b> <b>models</b> for the problem of musical style imitation. <b>Statistical</b> <b>models</b> are created from extant pieces in a stylistic corpus, and have an objective goal which is to accurately classify new pieces. The process of music generation is equated {{with the problem of}} sampling from a <b>statistical</b> <b>model.</b> In principle {{there is no need to}} make the classical distinction between analytic and synthetic models of music. This paper presents several methods for sampling from an analytic <b>statistical</b> <b>model,</b> and proposes a new approach that maintains the intra opus pattern repetition within an extant piece. A major component of creativity is the adaptation of extant art works, and this is also an efficient way to sample pieces from complex <b>statistical</b> <b>models.</b> ...|$|R
40|$|AbstractA <b>statistical</b> <b>model</b> {{for global}} {{optimization}} is constructed generalizing some {{properties of the}} Wiener process to the multidimensional case. An approach {{to the construction of}} global optimization algorithms is developed using the proposed <b>statistical</b> <b>model.</b> The convergence of an algorithm based on the constructed <b>statistical</b> <b>model</b> and simplicial partitioning is proved. Several versions of the algorithm are implemented and investigated...|$|R
40|$|This paper {{presents}} {{a framework for}} the segmentation of anatomical structures in medical imagery by connected <b>statistical</b> <b>models.</b> The framework is based on three types of models: first, generic models which operate directly on image intensities, second, connecting models that impose restrictions on the spatial relationship of generic models, and third, a supervising model that re{{presents a}}n arbitrary number of generic and connecting models. In this paper, the <b>statistical</b> <b>model</b> of appearance is used as the generic <b>model,</b> whiles the <b>statistical</b> <b>model</b> of topology, obtained by applying principal component analysis (PCA) on aligned pose and shape parameters of the generic model, is used as the connecting model. The performance of such connected <b>statistical</b> <b>model</b> is demonstrated on anterior-posterior (AP) X-ray images of the hips and pelvis and compared to the modelling by one and six unconnected generic models. The most accurate and robust results were obtained by two-level hierarchical <b>modelling,</b> wherein connected <b>statistical</b> <b>models</b> were used first, followed by unconnected <b>statistical</b> <b>models...</b>|$|R
50|$|He {{is known}} for {{probabilistic}} and <b>statistical</b> <b>modelling.</b> Pearce published prolifically {{in the area of}} probabilistic and <b>statistical</b> <b>modelling</b> and analysis, with strong contributions being made in both theory and practice. His book with Dragomir addresses the fine points of the Hermite-Hadamard inequality and is published by Kluwer Academic Press. His applied interests included queuing theory, road traffic, telecommunications, and urban planning. With former student Bill Henderson, who followed him from Sheffield to Adelaide, he helped establish the successful Teletraffic Centre in the University of Adelaide. Publications are numerous and include three books, 23 book chapters, and over 300 research articles.|$|E
5000|$|The {{quantitative}} component {{draws on}} applied mathematics, computer science and <b>statistical</b> <b>modelling,</b> and emphasizes stochastic calculus, numerical methods and simulation techniques http://www.math.nyu.edu/financial_mathematics/content/02_financial/02; some programs also focus on econometrics / {{time series analysis}} http://www.cefims.ac.uk/cgi-bin/programmes.cgi?func=programme&id=42 http://www.few.eur.nl/few//index.cfm/site/Erasmus%20School%20of%20Economics/pageid/730795C2-C036-5AA1-D277EA76AEDFF510/index.cfm.|$|E
5000|$|Optimal {{matching}} {{is also a}} {{term used}} in <b>statistical</b> <b>modelling</b> of causal effects. In this context it refers to matching [...] "cases" [...] with [...] "controls", and is completely separate from the sequence-analytic sense.|$|E
40|$|The climate {{disruption}} of ENSO phenomena caused various disasters many {{place for the}} past four decades. Especially, there were drought and floods. The research was done to compare the predictive skills of <b>statistical</b> <b>models</b> in predicting seasonal ENSO. The verifying predictions used 3 metric. They were Pearson correlation, RMSE, and Euclidean distance. Verifying predictions of <b>statistical</b> <b>models</b> using observation data Ni??o 3. 4 and 6 <b>statistical</b> <b>models</b> i. e. Markov model, Constructed Analog (CA) model, CLIPER model, Canonical Correlation Analysis (CCA) model, Neural Network (NN) <b>model</b> and average <b>statistical</b> <b>model</b> of IRI (International Research Institute). The verifying predictions of the models for 10 years, 2006 until 2016 was found that the Constructed Analog (CA) <b>model</b> and average <b>statistical</b> <b>model</b> have a better predictions skills than other <b>statistical</b> <b>models</b> in predicting ENSO phenomenon. The improvement of the skills of different models was significant in the first 5 years and the last 5 years. It was happened only on Markov model, Neural Network (NN) <b>model,</b> and average <b>statistical</b> <b>model,</b> it showed to happen increase in the skills of the three models in the first 5 years to the last 5 years...|$|R
30|$|The {{proposed}} solution estimates the Alzheimer’s disease risk {{based on a}} <b>statistical</b> <b>model.</b> <b>Statistical</b> <b>models</b> for prediction can be discerned in three main classes: regression, classification, and neural networks [16].|$|R
3000|$|For CSALT, the <b>statistical</b> <b>model</b> {{includes}} the life distribution and the stress-life relationship; however, for SSALT and PSALT, the <b>statistical</b> <b>model</b> also {{includes the}} equivalent principle of stress level transition; [...]...|$|R
