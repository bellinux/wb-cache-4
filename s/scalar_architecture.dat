13|29|Public
50|$|A {{processor}} that executes every instruction {{one after}} the other (i.e., a non-pipelined <b>scalar</b> <b>architecture)</b> may use processor resources inefficiently, yielding potential poor performance. The performance can be improved by executing different substeps of sequential instructions simultaneously (termed pipelining), or even executing multiple instructions entirely simultaneously as in superscalar architectures. Further improvement can be achieved by executing instructions in an order different from that in which they occur in a program, termed out-of-order execution.|$|E
40|$|An {{architecture}} {{for improving}} computer per-formance is presented and discussed. The main {{feature of the}} architecture is {{a high degree of}} decoupling between operand access and execution. This results in an implementation which has two separate instruction streams that communicate via queues. A similar architecture has been previously proposed for array processors, but in that context the software is called on to do most of the coordination and synchronization between the instruction streams. This paper emphasizes implementation features that remove this burden from the programmer. Performance comparisons with a conventional <b>scalar</b> <b>architecture</b> are given, an...|$|E
40|$|Conjugate {{structure}} algebraic CELP (G. 729) is avoice codec that compresses {{speech signal}} based on modelparameter of human voice. This paper deals withimplementation of a speech-coding algorithm CS-ACELP usingITU-T’s G. 729 recommendation and optimize it for real-timeimplementation on a Very Long Instruction Word (VLIW) Digital Signal Processor (DSP) Central Processing Unit (CPU). Very long instruction word or VLIW {{refers to a}} CPUarchitecture designed {{to take advantage of}} instruction levelparallelism (ILP). A processor that executes every instructionone after the other (i. e. a non-pipelined <b>scalar</b> <b>architecture)</b> mayuse processor resources inefficiently, potentially leading to poorperformanc...|$|E
40|$|Geology and Geophysics, University of Minnesota (DGG/UMN) on {{analysis}} and visualization of large-scale simulation data of mantle convection. Our analysis revealed possible {{impacts of the}} post-perovskite phase transition on the dynamics in the Earth's lower mantle. Through the collaborative works, we also demonstrated that the ACuTEMan, a simulation code developed by SESG/ESC, shows an excellent performance on <b>scalar</b> <b>architectures</b> {{as well as on}} vector architectures like the Earth Simulator...|$|R
40|$|International audienceVirtual {{prototyping}} of MPSoCs requires fast processor simulation models. Dynamic binary {{translation is}} an efficient technology for instruction set simulation, {{but as it}} is basically used for effortless code migration, it targets mostly general purpose processors. As many heterogeneous MPSoCs include VLIW processors, we propose and detail in this paper a strategy to perform dynamic binary translation of VLIW codes on <b>scalar</b> <b>architectures</b> for simulation purposes. Our simulation experiments show {{that it is a}} few orders of magnitude faster than direct instruction interpretation, although the translator includes no optimization...|$|R
40|$|ISBN 978 - 1 - 4503 - 1426 - 8 International audienceVirtual {{prototyping}} of MPSoCs requires fast processor simulation models. Dynamic binary {{translation is}} an efficient technology for instruction set simulation, {{but as it}} is basically used for effortless code migration, it targets mostly general purpose processors. As many heterogeneous MPSoCs include VLIW processors, we propose and detail in this paper a strategy to perform dynamic binary translation of VLIW codes on <b>scalar</b> <b>architectures</b> for simulation purposes. Our simulation experiments show {{that it is a}} few orders of magnitude faster than direct instruction interpretation, although the translator includes no optimization...|$|R
40|$|Because {{dependencies}} {{limit the}} amount of parallelism in an instruction stream, a VLIW or superscalar processor typically cannot execute as many instructions each cycle as it has functional units. This otherwise wasted processing power can be harnessed in a multithreaded architecture, which exploits the independence of instructions from separate processes. If a multithreaded processor finds that the current process can not use a functional unit, it looks for an instruction from another ready process which can. This paper presents the results of an empirical study that measures the effectiveness on throughput of multithreaded processing when applied to a super <b>scalar</b> <b>architecture...</b>|$|E
40|$|Abstract Conjugate {{structure}} algebraic CELP (G. 729) is a voice codec that compresses {{speech signal}} based on model parameter of human voice. This paper deals with {{implementation of a}} speech-coding algorithm CS-ACELP using ITU- 7 ¶V * UHFRPPHQGDWLRQ DQG RSWLPL]H LW IRU UHDO-time implementation on a Very Long Instruction Word (VLIW) Digital Signal Processor (DSP) Central Processing Unit (CPU). Very long instruction word or VLIW refers to a CPU architecture designed {{to take advantage of}} instruction level parallelism (ILP). A processor that executes every instruction one after the other (i. e. a non-pipelined <b>scalar</b> <b>architecture)</b> may use processor resources inefficiently, potentially leading to poor performanc...|$|E
40|$|An {{architecture}} for high-performance scalar computation {{is proposed}} and discussed. The main {{feature of the}} architecture is {{a high degree of}} decoupling between operand access and execution. This results in an implementation that has two separate instruction streams that communicate via architectural queues. Performance comparisons with a conventional <b>scalar</b> <b>architecture</b> are given, and these show that significant performance gains can be realized. Single-instruction-stream versions, both physical and conceptual, are discussed, with the primary goal of minimizing the differences with conventional architectures. This allows known compilation and programming techniques to be used. Finally, the problem of deadlock in a decoupled system is discussed, and a deadlock prevention method is given...|$|E
40|$|Weapon {{designers}} have typically run large-scale, computationally intensive numerical simulations for missile and projectile simulations on high-end supercomputing architectures. Recently, the comparable sustained performanceto-price ratio of <b>scalar</b> microprocessor-based <b>architectures,</b> relative to vector processors, {{has resulted in}} their purchase and utilization by the scientific community. A modern distributed shared memory parallel supercomputing resource...|$|R
40|$|In this paper, {{we present}} a new {{architecture}} for highspeed pseudo vector processor based on a superscalar pipeline. Without using cache memory, the proposed architecture is able to overcome penalty of memory access latency by introducing register windows with register preloading and pipelined memory. One outstanding feature of the proposed architecture {{is that it is}} upward compatible with existing <b>scalar</b> <b>architectures.</b> Performance evaluation of the proposed architecture using the Livermore Loop Kernels shows over 6 times higher performance than a usual superscalar processor and 1. 2 times higher performance than a hypothetical extended model with cache prefetching technique with a memory access latency of 20 CPU clock cycles. List vectors are also effectively handled in a similar architecture. 1. Introduction Motivation Over the past several years, technological and architectural advances have realized dramatic improvements in microprocessor performance with peak performances becoming [...] ...|$|R
40|$|We {{present some}} {{theoretical}} and computational {{improvements to the}} single/double shift QR algorithm on vector machines. The improvement involves a lookahead algorithm that obtains vector performance better than the high multiplicity shift QR. The strategies presented are numerically {{similar to those found}} in such libraries as LAPACK and EISPACK. We provide tests to show improved timings against LAPACK, ESSL, and the vector version of EISPACK on an IBM 3090 /VF. We also provide theorems to show theoretical complexity improvements. 1 Introduction This paper examines the unsymmetric Schur decomposition for vector architectures. This method is also applicable for symmetric matrices and <b>scalar</b> <b>architectures,</b> but is most suited to finding the Schur decomposition on vector hardware. We improve the theoretical complexity of the QR approach, and reduce the overall timing. We do this by defining a lookahead or partial step. Our idea is to do the minimum amount of work possible, without ove [...] ...|$|R
40|$|The goal of {{this project}} was to study pipelined {{processor}} architectures along with instruction and data cache. Chosen pipelined architecture should be designed and implemented using VHDL language. Firstly, I decided to implement the subscalar architecture first, secondly, three versions of <b>scalar</b> <b>architecture.</b> For these architectures synthesis into FPGA was done and performance of these architectures was compared on chosen algorithm. In the next part of this thesis I designed and implemented instruction and data cache logic for both architectures. However I {{was not able to}} synthetise these caches. Last chapter of this thesis deals with the superscalar architecture, which is the architecture of nowadays...|$|E
40|$|With {{the advent}} of cleeply pipelined RISC proces-sors, static {{instruction}} scheduling by the compiler has become extremely important to obtain high processor performance. This {{is especially true for}} floating point code, since in general floating point operations have longer Mencies compared to integer operations. This paper suggests using a new algorithm called circular scheduling, to per-form software pipelining [...] Software pipelining has previously been investigated mostly for VLIW architectures. The algorithm clescribecl in this paper is shown to be quite effective for a <b>scalar</b> <b>architecture.</b> Register renaming, an idea that ori-ginates from dynamic instruction scheduling, is used in conjunction with this algorithm to aug-ment its performance. The techniques described here have been implemented as part of a com-mercial, production quality optimizing compiler for a RISC architecture. ‘rhc resulting perfor-mance improvement has verifiecl the feasibility and practicality of the techniques...|$|E
40|$|In this paper, {{we present}} a new <b>scalar</b> <b>architecture</b> for {{high-speed}} vector processing. Without using cache memory, the proposed architecture tolerates main memory access latency by introducing slide-windowed floating-point registers with data preloading feature and pipelined memory. The architecture can hold upward compatibility with existing scalar architectures. In the new architecture, software can control the window structure. This is the advantage compared with our previous work of registerwindows. Because of this advantage, registers are utilized more flexibly and computational efficiency is largely enhanced. Furthermore, this flexibility helps the compiler to generate efficient object codes easily. We have evaluated its performance on Livermore Fortran Kernels. The evaluation {{results show that the}} proposed architecture reduces the penalty of main memory access better than an ordinary scalar processor and a processor with cache prefetching. The proposed architecture with 64 regist [...] ...|$|E
40|$|International audienceMany of the {{recently}} announced integrated manycore architectures targeting specific applications embed several, if not many, very long instruction word (VLIW) processors. To start developing software while the hardware is still being designed, virtual prototypes of the full system are commonly used. Fast processor simulation is thus a requirement. To that aim, this paper introduces a strategy to perform dynamic binary translation (DBT) of VLIW codes on <b>scalar</b> <b>architectures.</b> We propose a high level simulation algorithm which takes into account VLIW oddities, such as explicit instruction parallelism, instructions with non unit register update latency, and delayed slots in branches. We present the implementation details of this algorithm within a DBT environment, as it raises many corner cases that are irrelevant in scalar DBT. Our experiments confirm that our solution is functionally correct, and show speedups of 1 and 2 orders of magnitude compared to raw instruction interpretation, even though no optimizations were performed on the code during and after translation...|$|R
40|$|Work {{toward a}} self-consistent plasma {{simulation}} using the DSMC (Direct Simulation Monte Carlo) method for {{examination of the}} flowfields of low-pressure high density plasma reactors is presented. Presently, DSMC simulations for these applications involve either treating the electrons as a fluid or imposing experimentally determined values for the electron number density profile. In either approach, the electrons themselves are not physically simulated. Self-consistent plasma DSMC simulations have been conducted for aerospace applications but at a severe computational cost {{due in part to}} the <b>scalar</b> <b>architectures</b> on which the codes were employed. The present work attempts to conduct such simulations at a more reasonable cost using a plasma version of the object-oriented parallel Cornell DSMC code, MONACO, on an IBM SP- 2. Due to availability of experimental data, the GEC reference cell is chosen to conduct preliminary investigations. An argon discharge is chosen to conduct preliminary investigations. An argon discharge is examined thus affording a simple chemistry set with eight gas-phase reactions and five species: Ar, Ar(+), Ar(*), Ar(sub 2), and e where Ar(*) is a metastable...|$|R
40|$|In recent years, Single Chip Multiprocessors {{have been}} gaining ground as {{alternatives}} to superscalar processor architectures. Comparisons [3] between CMPs and Superscalar Processors argue {{the case for}} chip multiprocessors through the results that super <b>scalar</b> <b>architectures</b> outperform CMPs of comparable die size and cost by only a small margin where coarse grained parallelism is not available. On the other hand, when coarse grained parallelism or a multiprogrammed workload is available, CMPs perform far better then super scalar processors. Implementing a simulator for a CMP such as Hydra provides {{an understanding of the}} issues involved in the design and implementation of such processors and also provides a testbed for exploring design alternatives. 2. Aims and Objectives The aim of the project is the implementation of a functional simulator for the Stanford Hydra Chip Multiprocessor using the Simics simulation framework. This includes specification of components, implementation of components as modules within Simics and testing to ensure that the system developed is functioning correctly. Project success will be judged by the ability of the system to execute speculative workloads, including misspeculations and squashes while maintaining program correctness...|$|R
40|$|Basic {{principles}} and design tradeoffs for control ol pipelined processors are first discussed. We concentrate on register-register architectures like the GRAY- 1 where pipeline control logic is localized {{to one or}} two pipeline stages and is referred to as &quot;instruction issue logic&quot;. Design tradeoffs are explored by giving designs for a variety of instruction issue methods that represent a range of complexity and sophistication. These vary from the original CRAY- 1 issue logic to a version of Tomasulo's algorithm, first used in the IBM 360 / 91 floating point unit. Also studied are Thornton's &quot;scoreboard &quot; algo-rithm used on the CDC 8600 and an algorithm we have devised. To provide a standard for comparison, all the issue methods are used to implement the GRAY- 1 <b>scalar</b> <b>architecture.</b> Then, using a simulation model and the Lawrence Livermore Loops compiled with the GRAY FOR-TRAN compiler, performance results for the various issue methods are given and discussed. 1...|$|E
40|$|In {{this paper}} we review main ideas {{mentioned}} {{in several other}} papers which talk about optimization techniques used by compilers. Here we focus on loop unrolling technique {{and its effect on}} power consumption, energy usage and also its impact on program speed up by achieving ILP (Instruction-level parallelism). Concentrating on superscalar processors, we discuss the idea of generalized loop unrolling presented by J. C. Hang and T. Leng and then we present a new method to traverse a linked list to get a better result of loop unrolling in that case. After that we mention the results of some experiments carried out on a Pentium 4 processor (as an instance of super <b>scalar</b> <b>architecture).</b> Furthermore, the results of some other experiments on supercomputer (the Alliat FX/ 2800 System) containing superscalar node processors would be mentioned. These experiments show that loop unrolling has a slight measurable effect on energy usage as well as power consumption. But it could be an effective way for program speed up. Comment: 4 pages, International Journal of Computer Science and Information Securit...|$|E
40|$|Cílem projektu bylo prostudovat zřetězené architektury procesorů, dále pak architektury instrukčních a datových cache. Vybraná zřetězená architektura měla být navržena včetně instrukční a datové cache a implementována v jazyce VHDL. Projekt jsem pojal tak, že jsem implementoval nejprve subskalární architekturu, poté tři verze skalární architektury. Byla provedena syntéza těchto architektur do FPGA a na zvoleném algoritmu porovnána jejich výkonnost. V další části práce jsem navrhl a implementoval instrukční i datovou cache pro obě architektury. Tyto cache se mi však už nepodařilo syntetizovat. Závěrečná kapitola této práce pojednává o superskalární architektuře, což je architektura používaná v dnešní době. The goal of {{this project}} was to study pipelined {{processor}} architectures along with instruction and data cache. Chosen pipelined architecture should be designed and implemented using VHDL language. Firstly, I decided to implement the subscalar architecture first, secondly, three versions of <b>scalar</b> <b>architecture.</b> For these architectures synthesis into FPGA was done and performance of these architectures was compared on chosen algorithm. In the next part of this thesis I designed and implemented instruction and data cache logic for both architectures. However I {{was not able to}} synthetise these caches. Last chapter of this thesis deals with the superscalar architecture, which is the architecture of nowadays. ...|$|E
40|$|Abstract—Koblitz curves are a {{class of}} {{computationally}} efficient elliptic curves where scalar multiplications can be accelerated using τNAF representations of scalars. However conversion from an integer scalar to a short τNAF is a costly operation. In this paper we improve the recently proposed scalar conversion scheme based on division by τ 2. We apply two levels of optimizations in the <b>scalar</b> conversion <b>architecture.</b> First we {{reduce the number of}} long integer subtractions during the scalar conversion. This optimization reduces the computation cost and also simplifies the critical paths present in the conversion architecture. Then we implement pipelines in the architecture. The pipeline splitting increases the operating frequency without increasing the number of cycles. We have provided detailed experimental results to support our claims made in this paper...|$|R
40|$|Thread-level and data-level {{parallel}} architectures {{have become}} the design of choice in many of today’s energy-efficient computing systems. However, these architectures put substantially higher requirements on the memory subsystem than <b>scalar</b> <b>architectures,</b> making memory latency and bandwidth critical in their overall efficiency. Data reuse exploration aims at reducing {{the pressure on the}} memory subsystem by exploiting the temporal locality in data accesses. In this paper, we investigate the effects on performance and energy from a data reuse methodology combined with parallelization and vectorization in multi- and many-core processors. As a test case, a full-search motion estimation kernel is evaluated on Intel® CoreTM i 7 - 4700 K (Haswell) and i 7 - 2600 K (Sandy Bridge) multi-core processors, as well as on an Intel® Xeon PhiTM many-core processor (Knights Landing) with Streaming Single Instruction Multiple Data (SIMD) Extensions (SSE) and Advanced Vector Extensions (AVX) instruction sets. Results using a single-threaded execution on the Haswell and Sandy Bridge systems show that performance and EDP (Energy Delay Product) can be improved through data reuse transformations on the scalar code by a factor of ≈ 3 × and ≈ 6 ×, respectively. Compared to scalar code without data reuse optimization, the SSE/AVX 2 version achieves ≈ 10 ×/ 17 × better performance and ≈ 92 ×/ 307 × better EDP, respectively. These results can be improved by 10 % to 15 % using data reuse techniques. Finally, the most optimized version using data reuse and AVX 512 achieves a speedup of ≈ 35 × and an EDP improvement of ≈ 1192 × on the Xeon Phi system. While single-threaded execution serves as a common reference point for all architectures to analyze the effects of data reuse on both scalar and vector codes, scalability with thread count is also discussed in the paper...|$|R
40|$|Image {{interpolation}} {{is widely}} used in many image processing applications, such as digital camera, mobile phone, tablet and display devices. Image interpolation is a method of estimating the new data points {{within the range of}} discrete set of known data points. Image interpolation can also be referred as image scaling, image resizing, image re-sampling and image zooming. This paper presents VLSI (Very Large Scale Integration) architecture of an area efficient image interpolation algorithm for any two dimensional (2 -D) image <b>scalar.</b> This <b>architecture</b> is implemented in FPGA (Field Programmable Gate Array) and the performance of this system is simulated using Xilinx system generator and synthesized using Xilinx ISE smulation tool. Various VLSI parameters such as combinational path delay, CPU time, memory usage, number of LUTs (Look Up Tables) are measured from the synthesis report...|$|R
40|$|An {{interrupt}} is precise if the saved process state {{corresponds with}} the sequential model of program execution where one instruction completes {{before the next}} begins. In a pipelined processor, precise interrupts are difficult to achieve because an instruction may be initiated before its predecessors have been completed. This paper describes and evaluates solutions to the precise interrupt problem in pipelined processors. The precise interrupt problem is first described. Then five solutions are discussed in detail. The first forces instructions to complete and modify the process state in architectural order. The other four allow instructions to complete in any order, but additional hardware is used so that a precise state can be restored when an interrupt occurs. All the methods {{are discussed in the}} context of a parallel pipeline struclure. Simulation results based on the CR. AY-IS <b>scalar</b> <b>architecture</b> are used to show that, at best, the first solution results in a performance degradation of about 16 ~. The remaining four solutions offer similar performance, and three of them result in as litlle as a 3 % performance loss. Several extensions, including virtual memory and linear pipeline structures, are briefly discussed...|$|E
40|$|This paper {{presents}} a 64 -bit fixed-point vector multiply-accumulator (MAC) architecture capable of supporting multiple precisions. The vector MAC can perform one 64 x 64, two 32 x 32, four 16 x 16 or eight 8 x 8 bit signed/unsigned multiply-accumulates using {{essentially the same}} hardware as a scalar 64 -bit MAC and with only a small increase in delay. The <b>scalar</b> MAC <b>architecture</b> is “vectorized ” by inserting mode-dependent multiplexing into the partial product generation and by inserting mode-dependent kills in the carry chain of the reduction tree and the final carry-propagate adder. This {{is an example of}} &quot;shared segmentation &quot; in which the existing scalar structure is segmented and then shared between vector modes. The vector MAC is area efficient and can be fully pipelined which makes it suitable for high-performance processors and possibly dynamically reconfigurable processors. I...|$|R
40|$|Copyright © 2003 IEEEThis paper {{presents}} a 64 -bit fixed-point vector multiply-accumulator (MAC) architecture capable of supporting multiple precisions. The vector MAC can perform one 64 x 64, two 32 x 32, four 16 x 16 or eight 8 x 8 bit signed/unsigned multiply-accumulates using {{essentially the same}} hardware as a scalar 64 -bit MAC and with only a small increase in delay. The <b>scalar</b> MAC <b>architecture</b> is "vectorized" by inserting mode-dependent multiplexing into the partial product generation and by inserting mode-dependent kills in the carry chain of the reduction tree and the final carry-propagate adder. This {{is an example of}} "shared segmentation" in which the existing scalar structure is segmented and then shared between vector modes. The vector MAC is area efficient and can be fully pipelined which makes it suitable for high-performance processors and possibly dynamically reconfigurable processors. Dimitri Tan, Albert Danysh and Michael Liebel...|$|R
40|$|This {{document}} {{describes the}} guidelines adopted for software {{development of the}} Community Land Model (CLM) {{and serves as a}} reference to the entire code base of the released version of the model. The version of the code described here is Version 3. 0 which was released in the summer of 2004. This document, the Community Land Model Version 3. 0 (CLM 3. 0) User's Guide (Vertenstein et al., 2004), the Technical Description of the Community Land Model (CLM) (Oleson et al., 2004), and the Community Land Model's Dynamic Global Vegetation Model (CLM-DGVM) : Technical Description and User's Guide (Levis et al., 2004) provide the developer, user, or researcher with details of implementation, instructions for using the model, a scientific description of the model, and a scientific description of the Dynamic Global Vegetation Model integrated with CLM respectively. The CLM is a single column (snow-soil-vegetation) biogeophysical model of the land surface which can be run serially (on a laptop or personal computer) or in parallel (using distributed or shared memory processors or both) on both vector and <b>scalar</b> computer <b>architectures.</b> Written in Fortran 90, CLM can be run offline (i. e., run in isolation using stored atmospheric forcing data), coupled to an atmospheric model (e. g., the Community Atmosphere Model (CAM)), or coupled to a climate system model (e. g., the Community Climate System Model Version 3 (CCSM 3)) through a flux coupler (e. g., Coupler 6 (CPL 6)). When coupled, CLM exchanges fluxes of energy, water, and momentum with the atmosphere. The horizontal land surface heterogeneity is represented by a nested subgrid hierarchy composed of gridcells, landunits, columns, and plant functional types (PFTs). This hierarchical representation is reflected in the data structures used by the model code. Biophysical processes are simulated for each subgrid unit (landunit, column, and PFT) independently, and prognostic variables are maintained for each subgrid unit. Vertical heterogeneity is represented by a single vegetation layer, 10 layers for soil, and up to five layers for snow, depending on the snow depth. For computational efficiency, gridcells are grouped into ''clumps'' which are divided in cyclic fashion among distributed memory processors. Additional parallel performance is obtained by distributing clumps of gridcells across shared memory processors on computer platforms that support hybrid Message Passing Interface (MPI) /OpenMP operation. Significant modifications to the source code have been made over the last year to support efficient operation on newer vector architectures, specifically the Earth Simulator in Japan and the Cray X 1 at Oak Ridge National Laboratory (Homan et al., 2004). These code modifications resulted in performance improvements even on the <b>scalar</b> <b>architectures</b> widely used for running CLM presently. To better support vectorized processing in the code, subgrid units (columns and PFTs) are grouped into ''filters'' based on their process-specific categorization. For example, filters (vectors of integers) referring to all snow, non-snow, lake, non-lake, and soil covered columns and PFTs within each clump are built and maintained when the model is run. Many loops within the scientific subroutines use these filters to indirectly address the process-appropriate subgrid units...|$|R
40|$|The higher {{computational}} {{complexity of}} an elliptic curve scalar point multiplication operation limits its implementation on general purpose processors. Dedicated hardware architectures {{are essential to}} reduce the computational time, which results in {{a substantial increase in}} the performance of associated cryptographic protocols. This paper presents a unified architecture to compute modular addition, subtraction, and multiplication operations over a finite field of large prime characteristic GF(p). Subsequently, dual instances of the unified architecture are utilized in the design of high speed elliptic curve <b>scalar</b> multiplier <b>architecture.</b> The proposed architecture is synthesized and implemented on several different Xilinx FPGA platforms for different field sizes. The proposed design computes a 192 -bit elliptic curve scalar multiplication in 2. 3 [*]ms on Virtex- 4 FPGA platform. It is 34 % faster and requires 40 % fewer clock cycles for elliptic curve scalar multiplication and consumes considerable fewer FPGA slices as compared to the other existing designs. The proposed design is also resistant to the timing and simple power analysis (SPA) attacks; therefore it is a good choice in the construction of fast and secure elliptic curve based cryptographic protocols...|$|R
40|$|Research on the {{high-performance}} {{implementation of}} nested data parallelism has, over time, covered {{a wide range}} of <b>architectures.</b> <b>Scalar</b> and vector processors as well as shared-memory and distributed memory machines were targeted. We are currently investigating methods to integrate this technology into a single portable compiler back-end. Essential to our approach are two program transformations, attening and calculational fusion, which even out irregular parallelism and increase locality of reference, respectively. We generate C code that makes use of a portable, light-weight, collective-communication library. First experiments on scalar, vector, and distributed-memory machines support the feasibility of the approach. 1 Introduction We discuss the design and code quality of a compilation system that implements nested data parallelism on {{a wide range of}} parallel machines; we strive for portable parallelism over machines containing scalar or vector processors and using sh [...] ...|$|R
40|$|A popular argument, {{generally}} {{attributed to}} Amdahl [1], is that vector and parallel architectures {{should not be}} carried to extremes because the scalar or serial portion of the code will eventually dominate. Since pipeline stages and extra processors obviously add hardware cost, a corollary to this argument is that the most cost-effective computer is one based on uniprocessor, <b>scalar</b> principles. For <b>architectures</b> that are both parallel and vector, the argument is compounded, making it appear that near-optimal performance on such architectures is a near-impossibility. A new argument is presented {{that is based on}} the assumption that program execution time, not problem size, is constant for various amounts of vectorization and parallelism. This has a dramatic effect on Amdahl’s argument, revealing that one can be much more optimistic about achieving high speedups on massively parallel and highly vectorized machines. The revised argument is supported by recent results of over 1000 times speedup on 1024 processors on several practical scientific applications [2]. 1...|$|R
40|$|Many {{scientific}} applications involve {{operations on}} sparse matrices. However, due to irregularities {{induced by the}} sparsity patterns, many operations on sparse matrices execute inefficiently on traditional <b>scalar</b> and vector <b>architectures.</b> To tackle this problem a scheme has been proposed consisting of two parts: (a) An extension to a vector architecture to support sparse matrix-vector multiplication using (b) a novel Blocked Based sparse matrix Compression Storage (BBCS) format. Within this context, {{in this paper we}} propose and describe a hardware mechanism for the extended vector architecture that performs the transposition A of a sparse matrix A using a hierarchical variation of the aforementioned sparse matrix compression format. The proposed Sparse matrix Transposition Mechanism (STM) is used as a Functional Unit for a vector processor and requires an s s word in-processor memory where s is the vector processor's section size. In this paper we provide a full description of the STM and show an expected performance increase of one order of magnitude...|$|R
40|$|Software defined radio (SDR) is a {{technical}} effort to use programmable hardware in wireless communication systems so that various protocols {{can be easily}} supported by software. However, using programmable hardware for SDR terminals has been unachievable because of their tight power budget and high demand on computation capability. The main theme of this thesis is to design a power efficient programmable baseband processor for the SDR. This thesis analyzed most contemporary wireless communication protocols both in system and algorithm levels. System level analysis is to see the interactions be-tween algorithms and the algorithm level analysis is to investigate the computation patterns of the algorithms comprising baseband operations. Based on the characterization results, this thesis proposes chip multiprocessor architecture, whose PEs have both parallel and <b>scalar</b> datapaths. Multiprocessor <b>architecture</b> is proposed to exploit the algorithm level parallelism. Both the par-allel and scalar datapaths are used because baseband processing {{is a combination of}} parallelizable and scalar computations. For additional enhancements, three nove...|$|R
40|$|Abstract—Many {{scientific}} applications involve {{operations on}} sparse matrices. However, due to irregularities {{induced by the}} sparsity patterns, many operations on sparse matrices execute inefficiently on traditional <b>scalar</b> and vector <b>architectures.</b> To tackle this problem a scheme has been proposed consisting of two parts: (a) An extension to a vector architecture to support sparse matrix-vector multiplication using (b) a novel Blocked Based sparse matrix Compression an×¢× Storage (BBCS) format. Within this context, {{in this paper we}} propose and describe a hardware mechanism for the extended vector architecture that performs the transposition�Ìof a sparse matrix�using a hierarchical variation of the aforementioned sparse matrix compression format. The proposed Sparse matrix Transposition Mechanism (STM) is used as a Functional Unit for a vector processor and requires word in-processor memory where×is the vector processor’s section size. In this paper we provide a full description of the STM and show an expected performance increase of one order of magnitude...|$|R
40|$|A normal Finite Element code {{typically}} {{takes about}} 30 - 40 % {{of the total}} time to cal-culate and assemble the global element matrices. The focus of this work is on efficient implementation of Finite Element subroutines which perform this task. An implementa-tion which {{takes into account the}} underlying hardware architecture’s features results in achieving good performance. Without these considerations {{it is not possible to}} achieve a good portion of the peak performance on any machine. The major factors influencing performance are hardware architecture, programming language and the compiler. To get a good performance on cache based machines, it is necessary to understand the memory hierarchy of these machines. The methods to gain performance on <b>scalar</b> and vector <b>architectures</b> are studied. The various features of C and Fortran programming languages which effect performance are looked into. The problems faced by the C compilers to optimize the code due to aliasing are examined in detail. Due to large differences in architectural designs, a code which runs efficiently on one machine need not necessarily perform well on the other...|$|R
40|$|Abstract: The binomial-tree {{model is}} a {{numerical}} method widely used in finance with a computational complexity which is quadratic {{with respect to the}} solution accuracy. The existing research has employed reconfigurable computing to provide faster solutions compared with general-purpose processors, but they require low-level manual design by a hardware engineer, and can only solve American options. This paper presents a formal mathematical framework that captures a large class of binomial-tree problems, and provides a systolic data-movement template that maps the framework into digital hardware. This paper also presents a fully automated design flow, which takes C-level user descriptions of binomial trees, with custom data types and tree operations, and automatically generates fully pipelined reconfigurable hardware solutions in field-programmable gate array (FPGA) bit-stream files. On a Xilinx Virtex- 7 xc 7 vx 980 t FPGA at a 100 -MHz clock frequency, we require 54 -μs latency to solve three 876 -step 32 -bit fixed-point American option binomial trees, with a pricing rate of 114 k trees/s. From the same device and in comparison to the existing solutions with equivalent FPGA technology, we always achieve better throughput. This ranges from 1. 4 x throughput compared with a hand-tuned register-transfer level systolic design, to 9. 1 x and 5. 6 x improvement with respect to <b>scalar</b> and vector <b>architectures,</b> respectively...|$|R
