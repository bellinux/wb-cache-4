4|6216|Public
40|$|The Sound Spatialization Framework is a C++ toolkit and {{development}} environment for providing advanced sound spatialization for virtual reality and multimedia applications. The Sound Spatialization Framework provides many powerful display and user-interface features {{not found in}} other sound spatialization software packages. It provides facilities that go beyond <b>simple</b> <b>sound</b> <b>source</b> spatialization: visualization and editing of the soundscape, multiple sinks, clustering of sound sources, monitoring and controlling resource management, support for various spatialization backends, and classes for MIDI animation and handling...|$|E
40|$|Today's {{aircraft}} noise calculation programs either use <b>simple</b> <b>sound</b> <b>source</b> descriptions with few input parameters or very sophisticated models with demanding input parameters. To {{fill the gap}} between these two approaches, a new model is proposed with the scope to find a reasonable comproise between its degree of detail and number of accessible flight parameters. By means of an extensive data exploration, the flight parameters with the strongest influence on the sound emission level were identified. These parameters were incorporated in two multiple linear regression models, one for airframe and one for engine sound sources. A new method allowed Fitting both source models to flyover measurements of the total aircraft. In total, models for ninteen combinations of aircraft and engine types were eastablished, which underlines the generality of the new source model for turbofan powered aircraft. Exemplary comparisons between measurements and model predictions for two aircraft types revealed, that the model is able to accurately reproduce directivity and spectra for diffrent fligth conditions...|$|E
40|$|Abstract. Knitted fabrics {{can be made}} {{to vibrate}} as a speaker by {{laminating}} it with Polyvinylidene fluoride (PVDF) strips to form curved arch laminated at the ends. The PVDF strip would vibrate due to the piezo electric effect when a 200 Vrms voltage is applied. A two-dimensional analytical prediction model is created based on the principles of stress and strain behaviour of a curved laminated arch. The model predicts the radial displacement of the arch with respect to the frequency of the excitation voltage of the PVDF strips. The Sound Propagation Level (SPL) at a particular point from the arch surface is modelled as a <b>simple</b> <b>sound</b> <b>source</b> in an infinite baffle. The acoustic pressure is linked with the radial displacement in the model. Simulation is carried out on structural properties of the fabric to discern the resonance region of the speaker vital for Active Noise Control to reduce automotive interior noise. 1...|$|E
40|$|International audienceThis paper {{presents}} the Nü framework. The {{objective of the}} framework is to provide composers with a tool to control web-based audio processes on spectators smartphones during live performances. Connecting their devices to a webpage broadcasted by the performer's laptop, spectators {{become part of the}} composition: from <b>simple</b> <b>sound</b> <b>sources</b> to active musicians. From a Max based interface, the performer can then control the behaviours of conceptual units, referred to as Nü modules, designed for live composition (distributed room reverb, granular synthesis, etc.). Each module is composed of a pair of JavaScript classes { one for the client, another for the server { relying on the Web Audio API for audio processing, and OSC messages for parameters control. Nü is an open source project based on the Soundworks framework...|$|R
40|$|This {{paper is}} {{concerned}} with the understanding of vibration characteristics of a box-type structure using finite element method as a tool. We found that mode shapes of the box structure can be classified according to their symmetrical properties, which are defined by the relative motion between the six plate panels constituting the box. The infinite number of modes of the box-type structure is divided into six groups. Each group has common features of symmetry and similar coupling mechanism between component panels and distribution of out-of-plane and in-plane components of vibration. Local and net-volume displacements associated with each mode can be correlated to the characteristics of the box as a <b>sound</b> <b>source.</b> Large volume displacement modes resembling the <b>simple</b> <b>sound</b> <b>sources</b> (e. g. monopole, dipole, etc) are identified among the low frequency modes. The distribution of in-plane and out-of-plane (including translational and rotational vibration at the box edges) vibration components in the modes of the box are also investigated to illustrate the energy transmission mechanism between the box panels...|$|R
40|$|Presented at 2 nd International Conference on Auditory Display (ICAD), Santa Fe, New Mexico, November 7 - 9, 1994. Using a <b>simple</b> {{model of}} <b>sound</b> <b>source</b> {{elevation}} judgment, an {{attempt was made}} to predict two aspects of listeners' localization behavior from measurements of the positions of the primary high-frequency notch in their head-related transfer functions. These characteristics were: (1) the scatter in elevation judgments and (2) possible biases in perceived elevation introduced by front-back and back-front reversals. Although significant differences were found among the notch-frequency patterns for individual subjects, the model was not capable of predicting differences in judgment behavior. This suggests that a simple model of elevation perception based on a single spectral notch frequency is inadequate...|$|R
40|$|Knitted {{elastomeric}} fabrics {{can be made}} {{to vibrate}} as a speaker by laminating them with polyvinylidene fluoride (PVDF) strips to form a curved arch clamped at the ends. The tightly knitted structure of the elastomeric knitted fabric, which is void of pores, makes it an ideal membrane suited for a speaker. The PVDF strip would vibrate due to the piezoelectric effect when a 200 Vrms voltage is applied. In this work a two-dimensional analytical prediction model is created based on the principles of stress and strain behaviour of a curved laminated arch. The model predicts the radial displacement of the arch with respect to the frequency of the excitation voltage of the PVDF strips. The sound propagation level (SPL) at a particular point from the arch surface is modelled as a <b>simple</b> <b>sound</b> <b>source</b> in an infinite baffle. The acoustic pressure is then linked with the radial displacement in the model. Simulation and experiments are then carried out on structural properties of the elastomeric knitted fabrics to discern the resonance region of the speaker vital for active noise control to reduce automotive interior noise...|$|E
40|$|All {{auditory}} {{sensory information}} is packaged {{in a pair}} of acoustical pressure waveforms, one at each ear. While there is obvious structure in these waveforms, that structure (temporal and spectral patterns) bears no simple relationship to the structure of the environmental objects that produced them. The properties of auditory objects and their layout in space must be derived completely from higher level processing of the peripheral input. This chapter begins with a discussion of the peculiarities of acoustical stimuli and how they are received by the human auditory system. A distinction is made between the ambient sound field and the effective stimulus to differentiate the perceptual distinctions among various <b>simple</b> classes of <b>sound</b> <b>sources</b> (ambient field) from the known perceptual consequences of the linear transformations of the <b>sound</b> wave from <b>source</b> to receiver (effective stimulus). Next, the definition of an auditory object is dealt with, specifically the question of how the various components of a sound stream become segregated into distinct auditory objects. The remainder of the chapter focuses on issues related to the spatial layout of auditory objects, both stationary and moving...|$|R
40|$|An audio {{localization}} test comparing {{accuracy of}} static and moving <b>sound</b> <b>sources</b> {{was carried out}} in a spatially immersive virtual environment, using loudspeaker array with vector based amplitude panning for virtual <b>sound</b> <b>sources.</b> Azimuth and elevation error in localization were measured with different sound signals. As was expected errors in azimuth localization accuracy were smaller than errors in elevation accuracy. There were more localization blur with virtual <b>sound</b> <b>sources</b> than <b>sound</b> <b>sources</b> reproduced directly from a single loudspeaker. Localization blur with moving <b>sound</b> <b>sources</b> were in the same level as with static panorated <b>sound</b> <b>sources.</b> Although the <b>sound</b> <b>sources</b> moved steadily, the measurements indicated that subjects perceived the changes in <b>sound</b> <b>source</b> location stepwise due to applied amplitude panning...|$|R
50|$|The {{binaural}} {{aspect of}} the cocktail party effect {{is related to the}} localization of <b>sound</b> <b>sources.</b> The auditory system is able to localize at least two <b>sound</b> <b>sources</b> and assign the correct characteristics to these sources simultaneously. As soon as the auditory system has localized a <b>sound</b> <b>source,</b> it can extract the signals of this <b>sound</b> <b>source</b> out of a mixture of interfering <b>sound</b> <b>sources.</b>|$|R
40|$|Sound power {{describes}} a <b>sound</b> <b>source</b> {{regardless of its}} environment and is useful in noise control applications, but can be cumbersome and time consuming to measure. Sound power levels can rank different <b>sound</b> <b>sources</b> and is often restricted in noise control legislation. An acoustic camera records a sound field with a microphone array. Due to properties of the array, and by using beamforming algorithms, an acoustic camera can separate sound from different directions. The acoustic camera measures sound pressure from a <b>sound</b> <b>source.</b> By assuming directivity properties the sound power of a <b>sound</b> <b>source</b> {{can be derived from}} the sound pressure. In this thesis an acoustic camera has been evaluated in order to determine sound power estimation performance and <b>sound</b> <b>source</b> separation ability. This is tested by six different measurement set-ups in an anechoic chamber. Two different <b>sound</b> <b>sources</b> are used in the trials: one reference <b>sound</b> <b>source</b> and one disturbing <b>sound</b> <b>source.</b> The reference <b>sound</b> <b>source</b> has a calibrated and documented sound power level to which the measurement results are compared. Measurements were performed at 1 to 5 m distance from the acoustic camera with both <b>sound</b> <b>sources.</b> The influence of a disturbing <b>sound</b> <b>source</b> on the reference <b>sound</b> <b>source</b> <b>sound</b> power level was measured with the <b>sound</b> <b>sources</b> separated 0. 65 m to 2. 6 m. The measurements show that the sound power level could at best be determined within 1 dB. The acoustic camera can separate different <b>sound</b> <b>sources</b> well. Influence from a disturbing <b>sound</b> <b>source,</b> 10 dB SPL stronger and distanced 1 m from a reference <b>sound</b> <b>source</b> was 2 to 3 dB for mid-frequency one-third octave bands at 5 m measurement distance. Measuring sound power with an acoustic camera is fast and mobile compared to room interaction methods and sound intensity measurements. The results of this thesis are useful when measuring sound power levels, especially for <b>sound</b> <b>sources</b> such as chimney outlets, wind power stations and big objects that can not be moved or do not fit in a room. Validerat; 20101217 (root...|$|R
5000|$|... #Caption: A Numark DM2002X Pro Master DJ mixer. This three channel mixer {{can have}} up to three input <b>sound</b> <b>sources.</b> The gain control knobs and {{equalization}} control knobs allow the volume and tone of each <b>sound</b> <b>source</b> to be adjusted. The vertical faders allow for further adjustment {{of the volume of}} each <b>sound</b> <b>source.</b> The horizontally-mounted crossfader enables the DJ to smoothly transition from a song on one <b>sound</b> <b>source</b> to a song from a different <b>sound</b> <b>source.</b>|$|R
40|$|Monopole <b>sound</b> <b>sources</b> (i. e. omni {{directional}} <b>sound</b> <b>sources</b> with a known volume velocity) {{are essential}} for reciprocal measurements used in vehicle interior panel noise contribution analysis. Until recently, these monopole <b>sound</b> <b>sources</b> use a <b>sound</b> pressure transducer sensor as a reference sensor. A novel monopole <b>sound</b> <b>source</b> principle is demonstrated that uses a Microflown (acoustic particle velocity) sensor as a reference sensor. As compared to a <b>sound</b> <b>source</b> that uses a sound pressure transducer as a reference sensor, the new <b>sound</b> <b>sources</b> demonstrated are relatively easy to calibrate, not sensitive to changes in ambient temperatures, and suitable to use {{in all sorts of}} acoustic environments. © 2008 SAE International...|$|R
40|$|Measuring the {{contribution}} of a particular <b>sound</b> <b>source</b> to the ambient sound level at an arbitrary location is impossible without some form of <b>sound</b> <b>source</b> separation. This made it difficult, if not impossible, to design automated systems that measure {{the contribution}} of a target sound to the ambient sound level. This paper introduces <b>sound</b> <b>source</b> separation technology {{that can be used}} to measure {{the contribution of}} a <b>sound</b> <b>source,</b> a passing plane, in environments where planes are not the dominant <b>sound</b> <b>source.</b> This <b>sound</b> <b>source</b> separation and classification technology, developed by Sound Intelligence makes it, in principle, possible to monitor the temporal development of any soundscape...|$|R
50|$|Applications of <b>sound</b> <b>source</b> {{localization}} include <b>sound</b> <b>source</b> separation, <b>sound</b> <b>source</b> tracking, {{and speech}} enhancement. Sonar uses <b>sound</b> <b>source</b> localization techniques {{to identify the}} location of a target. 3D sound localization is also used for effective human-robot interaction. With the increasing demand for robotic hearing, some applications of 3D sound localization such as human-machine interface, handicapped aid, and military applications, are being explored.|$|R
40|$|It is {{necessary}} {{that the number}} of the observation signals equals to the number of source signals, if independent component analysis is used to perform the <b>sound</b> <b>source</b> separation. It is difficult to perform <b>sound</b> <b>source</b> separation from a stereo music sound signal when the number of <b>sound</b> <b>sources</b> are more than two. We propose the technique to perform <b>sound</b> <b>source</b> separation from a stereo music sound signal {{that the number of}} <b>sound</b> <b>sources</b> is more than two using the frequency analysis and independent component analysis. 1...|$|R
50|$|As a {{consequence}} the auditory system seems {{only to be}} able to localize <b>sound</b> <b>sources</b> in reverberant environment at sound onsets or at bigger spectral changes. Then the direct sound of the <b>sound</b> <b>source</b> prevails at least in some frequency ranges and the direction of the <b>sound</b> <b>source</b> can be determined. Some milliseconds later, when the sound of the wall reflections arrives, a <b>sound</b> <b>source</b> localization seems no more to be possible. As long as no new localization is possible, the auditory systems seems to keep the last localized direction as perceived <b>sound</b> <b>source</b> direction.|$|R
50|$|<b>Sound</b> <b>sources</b> {{refer to}} the {{conversion}} of aerodynamic energy into acoustic energy. There are two main types of <b>sound</b> <b>sources</b> in the articulatory system: periodic (or more precisely semi-periodic) and aperiodic. A periodic <b>sound</b> <b>source</b> is vocal fold vibration produced at the glottis found in vowels and voiced consonants. A less common periodic <b>sound</b> <b>source</b> is the vibration of an oral articulator like the tongue found in alveolar trills. Aperiodic <b>sound</b> <b>sources</b> are the turbulent noise of fricative consonants and the short-noise burst of plosive releases produced in the oral cavity.|$|R
3000|$|The {{objective}} of this work is to estimate the multiple fixed <b>sound</b> <b>source</b> directions without a priori information of the <b>sound</b> <b>source</b> number and the environment. This work utilizes the time delay information and microphone array geometry to estimate the <b>sound</b> <b>source</b> directions [23]. A novel eigenstructure-based GCC (ES-GCC) method to estimate the time delay under a multi-source environment between two microphones is proposed. The theoretical proof of the ES-GCC method is given, and the experimental results show that it is robust in a noisy environment. As a result, the <b>sound</b> <b>source</b> direction and velocity {{can be obtained by}} solving the proposed linear equation model using the time delay information. Fundamentally, the <b>sound</b> <b>source</b> number should be known while estimating the <b>sound</b> <b>source</b> directions. Hence, the method which can estimate <b>sound</b> <b>source</b> number and directions simultaneously using the proposed adaptive [...]...|$|R
40|$|<b>Sound</b> <b>source</b> {{tracking}} is {{an important}} function for a robot operating in a daily environment, because the robot should recognize where a sound event such as speech, music and other environmental sounds originates from. This paper addresses <b>sound</b> <b>source</b> tracking by integrating a room and a robot microphone array. The room microphone array consists of 64 microphones attached to the walls. It provides 2 D (x-y) <b>sound</b> <b>source</b> localization based on a weighted delay-and-sum beamforming method. The robot microphone array consists of eight microphones installed on a robot head, and localizes multiple <b>sound</b> <b>sources</b> in azimuth. The localization results are integrated to track <b>sound</b> <b>sources</b> by using a particle filter for multiple <b>sound</b> <b>sources.</b> The experimental results show that particle filter based integration reduces localization errors and provides accurate and robust 2 D <b>sound</b> <b>source</b> tracking. 1...|$|R
40|$|This work {{explores the}} design and {{effectiveness}} of a robot that uses a combination of active sonar and a pseudo-random acoustic signal to navigate and reconstruct an unknown environment. The robot sends the pseudo-random signal into the environment and records the resulting response. This response is processed to gain information on the robot’s immediate surroundings. Previous work done in this area focused on the recording and processing aspect to determine {{the location of a}} <b>sound</b> <b>source</b> or multiple <b>sound</b> <b>sources.</b> We apply similar algorithms to localize what are known as virtual <b>sound</b> <b>sources.</b> Virtual <b>sound</b> <b>sources</b> are created when sound from a <b>sound</b> <b>source</b> reflects off of a surface, such as a wall or object, and are recorded by a receiver. The recorded reflected sound is commonly known as an echo. A virtual <b>sound</b> <b>source</b> is placed at the location where the sound incident to the receiver would have originated from had no reflection taken place. By placing the real <b>sound</b> <b>source</b> and receivers on the same platform, if we can accurately localize the generated virtual <b>sound</b> <b>sources,</b> we can compute the pat...|$|R
30|$|Two {{evaluation}} metrics were calculated: Number {{and location}} of sources For each session, a <b>sound</b> <b>source</b> was considered as ‘estimated’ if it was detected for at least 25 % of {{the duration of the}} session (30 s). If a source is estimated within a ± 15 ∘ range of the expected direction of an actual <b>sound</b> <b>source,</b> it is considered a true positive. If a <b>sound</b> <b>source</b> is estimated outside that range from an actual <b>sound</b> <b>source,</b> it is considered a false positive. If an actual <b>sound</b> <b>source</b> is not estimated during the experiment, it is considered a false negative. Using these metrics, the precision, recall, and F 1 scores [49] (Chapter 8) of the proposed system’s ability to detect <b>sound</b> <b>sources</b> are calculated. Average error Once estimated, an average absolute error is calculated for every <b>sound</b> <b>source</b> that is deemed true positive, from the direction it is actually located.|$|R
30|$|The methods above {{assume that}} the <b>sound</b> <b>source</b> number is known. But {{this may not be}} a {{realistic}} assumption because the environment usually contains various kinds of <b>sound</b> <b>sources.</b> Several eigenvalue-based methods have been proposed [20, 21] to estimate the <b>sound</b> <b>source</b> number. However, the eigenvalue distribution is sensitive to noise and reverberation. The work in [22] used the support vector machine (SVM) to classify the distribution with respect to the <b>sound</b> <b>source</b> number. However, it still requires a training stage for a robust result and the binary classification is inadequate when the <b>sound</b> <b>source</b> number is larger than two.|$|R
5000|$|Loudness: Distant <b>sound</b> <b>sources</b> {{have a lower}} {{loudness}} than close ones. This aspect can {{be evaluated}} especially for well-known <b>sound</b> <b>sources.</b>|$|R
40|$|<b>Sound</b> <b>source</b> {{occlusion}} {{occurs when}} the direct path from a <b>sound</b> <b>source</b> to a listener is blocked by an intervening object. Currently, {{a variety of methods}} exist for modeling <b>sound</b> <b>source</b> occlusion. These include finite element and boundary element methods, as well as methods based on time-domain models of edge diffraction. At present, the high computational requirements of these methods precludes their use in real-time environments. In the case of real-time geometric room acoustic methods (e. g. the image method, ray tracing), the model of sound propagation employed makes it difficult to incorporate wave-related effects such as occlusion. As a result, these methods generally do not incorporate <b>sound</b> <b>source</b> occlusion. The lack of a suitable <b>sound</b> <b>source</b> occlusion method means that developers of real-time virtual environments (such as computer games) have generally either ignored this phenomenon or used rudimentary and perceptually implausible approximations. A potential solution to this problem is the use of shadow algorithms from computer graphics. These algorithms can provide a way to efficiently simulate <b>sound</b> <b>source</b> occlusion in real-time and in a physically plausible manner. Two simulation prototypes are presented, one for fixed-position <b>sound</b> <b>sources</b> and another for moving <b>sound</b> <b>sources...</b>|$|R
40|$|AbstractA three {{dimensional}} {{imaging method}} using electromagnetic induction type <b>sound</b> <b>source</b> and amplitude correlation synthesis processing method was proposed {{in our previous}} work. Considering the directivity of the <b>sound</b> <b>source</b> and the nonlinearity of the processing method, this paper discusses the lateral detecting ability of the method by both numerical simulation and experimental testing. Three objects at identical depth are measured with varying lateral positions relating to the <b>sound</b> <b>source</b> and receiver array. The {{results show that the}} lateral detecting ability is determined mainly by the directivity of the <b>sound</b> <b>source</b> and the efficient detecting area is about 35 ∘ inside the spread angle of the <b>sound</b> <b>source...</b>|$|R
40|$|Abstract—Sound source {{localization}} is {{an important}} feature in robot audition. This work proposes a <b>sound</b> <b>source</b> number and directions estimation method by using the delay information of microphone array. An eigenstructure-based generalized cross correlation method is proposed to estimate time delay between microphones. Upon obtaining the time delay information, the <b>sound</b> <b>source</b> direction and velocity can be estimated by least square method. In multiple <b>sound</b> <b>source</b> case, the time delay combination among microphones is arranged such that the estimated sound speed value falls within an acceptable range. By accumulating the estimation results of <b>sound</b> <b>source</b> direction and using adaptive K-means++ algorithm, the <b>sound</b> <b>source</b> number and directions can be estimated. I...|$|R
3000|$|This paper {{assumes that}} the {{distance}} from source to the array is {{much larger than the}} array aperture, and (29) is used to solve the <b>sound</b> <b>source</b> direction estimation problem. If the number of <b>sound</b> <b>sources</b> is known, the <b>sound</b> <b>source</b> directions can be estimated by putting time delay vector [...]...|$|R
3000|$|This work {{explains}} a <b>sound</b> <b>source</b> number and directions estimation algorithm. The multiple source time delay vector combination {{problem can be}} solved by the proposed reasonable sound velocity-based method. By accumulating the estimated <b>sound</b> <b>source</b> angle, the <b>sound</b> <b>source</b> number and directions {{can be obtained by}} the proposed adaptive [...]...|$|R
40|$|<b>Sound</b> <b>source</b> {{characteristics}} {{may be one}} of {{the main}} causes of objective speech intelligibility metric inaccuracy. In this study, the influences of the <b>sound</b> <b>source</b> directivity and frequency response were investigated using three typical sound sources: an artificial mouth, a monitor speaker, and a dodecahedral <b>sound</b> <b>source.</b> The results show that, the simultaneous influences of directivity and frequency response on the objective speech intelligibility metric are significant, typically with a variation of 0. 147 in speech transmission index (STI); <b>sound</b> <b>source</b> directivity may also result in a noticeable difference in the objective speech intelligibility metric, typically with a variation of 0. 123 in STI. In comparison with <b>sound</b> <b>sources</b> with a high directivity index (DI), the measurement results for <b>sound</b> <b>sources</b> with a relatively low DI may be higher when background noise is high, and may be lower when background noise is low. The influence of <b>sound</b> <b>source</b> directivity may also depend on the room acoustic conditions, and at receiver position where reflections are abundant, the influence of <b>sound</b> <b>source</b> directivity may be more significant. Not applying frequency response equalisation resulted in large errors in the values being measured, which deviate from the real values of STI by up to 0. 172, depending on the original frequency response characteristics of the <b>sound</b> <b>sources</b> that are used...|$|R
40|$|A longstanding {{philosophical}} tradition {{holds that}} the primary objects of hearing are sounds rather than <b>sound</b> <b>sources.</b> In this case, we hear <b>sound</b> <b>sources</b> by—or in virtue of—hearing their sounds. This paper argues that, on the contrary, we {{have good reason to}} believe that the primary objects of hearing are <b>sound</b> <b>sources,</b> and that the relationship between a <b>sound</b> and its <b>source</b> is much like the relationship between a color and its bearer. Just as we see objects in seeing their colors, so we hear <b>sound</b> <b>sources</b> in hearing their sounds...|$|R
40|$|In this paper, {{we present}} an active {{audition}} system for humanoid robot “SIG the humanoid”. The audition {{system of the}} highly intelligent humanoid requires localization of <b>sound</b> <b>sources</b> and identification of meanings of the sound in the auditory scene. The active audition reported in this paper focuses on improved <b>sound</b> <b>source</b> tracking by integrating audition, vision, and motor movements. Given the multiple <b>sound</b> <b>sources</b> in the auditory scene, SIG actively moves its head to improve localization by aligning microphones orthogonal to the <b>sound</b> <b>source</b> and by capturing the possible <b>sound</b> <b>sources</b> by vision. However, such an active head movement inevitably creates motor noise. The system must adaptively cancel motor noise using motor control signals. The experimental result demonstrates that the active audition by integration of audition, vision, and motor control enables <b>sound</b> <b>source</b> tracking in variety of conditions...|$|R
30|$|The {{direction}} of the <b>sound</b> <b>source</b> can be estimated using the sound localization method described above. With one stationary microphone array, {{it is hard to}} estimate the <b>sound</b> <b>source</b> position. However, the home service robot can move around, which makes it possible to use triangulation to localize the <b>sound</b> <b>source.</b> Figure  4 shows an example of using triangulation to estimate the positions of two <b>sound</b> <b>sources.</b> If the robot can measure the sound direction at two different positions on the 2 D map, the sound position can be estimated by calculating the intersection of two lines pointing to the <b>sound</b> <b>sources</b> from the robot positions. This method may create a undesired intersection point like point P as shown in Fig.  4. However, this point moves when the robot measures at another position. Therefore, it can be eliminated given the assumption that the <b>sound</b> <b>sources</b> are stationary. With multiple steps, the robot can improve the accuracy of position estimation using the RANdom SAmple Consensus (RANSAC) algorithm [29]. The <b>sound</b> <b>source</b> position estimation algorithm is shown in Algorithm 1.|$|R
40|$|A {{flexible}} <b>sound</b> <b>source</b> {{is essential}} in a whole flexible system. It’s hard to integrate a conventional <b>sound</b> <b>source</b> based on a piezoelectric part into a whole flexible system. Moreover, the sound pressure from {{the back side of}} a <b>sound</b> <b>source</b> is usually weaker than that from the front side. With the help of direct laser writing (DLW) technology, the fabrication of a flexible 360 -degree thermal <b>sound</b> <b>source</b> becomes possible. A 650 -nm low-power laser was used to reduce the graphene oxide (GO). The stripped laser induced graphene thermal <b>sound</b> <b>source</b> was then attached to the surface of a cylindrical bottle so that it could emit sound in a 360 -degree direction. The sound pressure level and directivity of the <b>sound</b> <b>source</b> were tested, and the results were in good agreement with the theoretical results. Because of its 360 -degree sound field, high flexibility, high efficiency, low cost, and good reliability, the 360 -degree thermal acoustic <b>sound</b> <b>source</b> will be widely applied in consumer electronics, multi-media systems, and ultrasonic detection and imaging...|$|R
30|$|Conditions of {{experiment}} 2 : occlusion of the <b>sound</b> <b>source</b> This experiment {{was conducted to}} evaluate {{the effect of the}} occlusion of <b>sound</b> <b>source</b> on the localization accuracy. Cardboard box (approximate dimensions height: 1 m, width: 0.5 m for each) was placed as shown in Fig. 5 and it completely occlude <b>sound</b> <b>source</b> 1.|$|R
2500|$|If a <b>sound</b> <b>source</b> and two {{microphones}} {{are arranged}} in a straight line, with the <b>sound</b> <b>source</b> at one end, then the following can be measured: ...|$|R
