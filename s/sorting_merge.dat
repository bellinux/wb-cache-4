3|463|Public
50|$|The 70/15 {{was often}} used as a {{satellite}} processor for larger systems or used as an intelligent terminal for remote job entry. Typical applications of a satellite processor would include card-to-tape conversion, card/tape-to-printer report generation, tape-to-card punching, input pre-processing and verification, or tab-shop tasks like file <b>sorting,</b> <b>merge,</b> and data selection.|$|E
40|$|We {{implement}} a promising algorithm for sparse-matrix sparse-vector multiplication (SpMSpV) on the GPU. An efficient k-way merge {{lies at the}} heart of finding a fast parallel SpMSpV algorithm. We examine the scalability of three approaches—no <b>sorting,</b> <b>merge</b> sorting, and radix sorting—in solving this problem. For breadth-first search(BFS), we achieve a 1. 26 x speedup over state-of-the-art sparse-matrix dense-vector (SpMV) implementations. The algorithm seems generalizeable for single-source shortest path (SSSP) and sparse-matrix sparse-matrix multiplication, and other core graph primitives such as maximal independent set and bipartite matching...|$|E
40|$|Sorting is {{considered}} as a fundamental operation in computer science as it is used as an intermediate step in many operations. Sorting refers to the operation of arranging data in some given order such as increasing or decreasing, with numerical data, or alphabetically, with character data. An algorithm is any well-defined procedure or set of instructions, that takes some input {{in the form of}} some values, processes them and gives some values as output. Selection of best sorting algorithm for a particular problem depends upon problem definition. Merge Sort is the first that scales well to very large lists, because its worst-case running time is O(n log n). Together with its modest O(log n) space usage, Quick Sort {{is one of the most}} popular sorting algorithms and is available in many standard programming libraries. The most complex issue in Quick Sort is choosing a good pivot element; consistently poor choices of pivots can result in drastically slower O(n) performance, if at each step the median is chosen as the pivot then the algorithm works in O(n log n). Finding the median however, is an O(n) operation on unsorted lists and therefore exacts its own penalty with sorting. So, we are generating a new sorting algorithm while mixing the two <b>sorting</b> (<b>Merge</b> Sort and Quick Sort) to for best results...|$|E
40|$|Sorting is a {{commonly}} used operation in computer science. In {{addition to its}} main job of arranging lists or arrays in sequence, sorting is often also required to facilitate some other operation such as searching, merging and normalization or used as an intermediate operation in other operations. A sorting algorithm consists of comparison, swap, and assignment operations[1 - 3]. There are several simple and complex sorting algorithms that are being used in practical {{life as well as}} in computation such as Quick <b>sort,</b> Bubble <b>sort,</b> <b>Merge</b> <b>sort,</b> Bucket sort, Heap sort, Radix sort etc. But the application of these algorithms depends on the problem statement. This paper introduces MQ sort which combines the advantages of quick <b>sort</b> and <b>Merge</b> <b>sort.</b> The comparative analysis of performance and complexity of MQ sort is done against Quick <b>sort</b> and <b>Merge</b> <b>sort.</b> MQ sort significantly reduces complexity and provides better performance than Quick <b>sort,</b> <b>Merge</b> <b>sort...</b>|$|R
5000|$|Tibero ProSort is a {{solution}} that enables large amounts of data to be <b>sorted,</b> <b>merged</b> and converted.|$|R
40|$|The thesis {{presents}} {{the field of}} external sorting. In the thesis we describe and compare multiple sorting algorithms for external sorting based on their behavior, their advantages and disadvantages. The algorithms we compare are the straight multiway <b>merge</b> <b>sort,</b> balanced multiway <b>merge</b> <b>sort,</b> natural multiway <b>merge</b> <b>sort,</b> polyphase <b>merge</b> <b>sort,</b> cascade sort, distribution sort, funnel sort and two pre-sorting algorithms. The purpose of the thesis is to describe and present how the algorithms work in theory and in practice. We implemented the algorithms in the C programming language and then experimentally compared them on a personal computer with one external storage device...|$|R
25|$|A variant named binary <b>merge</b> <b>sort</b> uses {{a binary}} {{insertion}} sort to sort groups of 32 elements, {{followed by a}} final <b>sort</b> using <b>merge</b> <b>sort.</b> It combines the speed of insertion sort on small data sets {{with the speed of}} <b>merge</b> <b>sort</b> on large data sets.|$|R
5000|$|... #Caption: An {{example of}} <b>merge</b> <b>sort.</b> First divide the list into the {{smallest}} unit (1 element), then compare each element with the adjacent list to <b>sort</b> and <b>merge</b> the two adjacent lists. Finally {{all the elements}} are <b>sorted</b> and <b>merged.</b>|$|R
5000|$|The merge {{algorithm}} plays {{a critical}} role in the <b>merge</b> <b>sort</b> algorithm, a comparison-based <b>sorting</b> algorithm. Conceptually, <b>merge</b> <b>sort</b> algorithm consists of two steps: ...|$|R
40|$|Relational {{database}} systems use join queries {{to retrieve}} data from two relations. Several join methods {{can be used}} to execute these queries. This study investigated the effect of varying join selectivity factors on the performance of the join methods. Experiments using the ORACLE environment were set up to measure the performance of three join methods: nested loop join, <b>sort</b> <b>merge</b> join and hash join. The performance was measured in terms of total elapsed time, CPU time and the number of I/O reads. The study found that the hash join performs better than the nested loop and the <b>sort</b> <b>merge</b> under all varying conditions. The nested loop competes with the hash join at low join selectivity factor. The results also showed that the <b>sort</b> <b>merge</b> join method performs better than the nested loop when a predicate is applied to the inner table...|$|R
2500|$|The outer loop of block sort is {{identical}} to a bottom-up <b>merge</b> <b>sort,</b> where each level of the <b>sort</b> <b>merges</b> pairs of subarrays, [...] and , in sizes of 1, then 2, then 4, 8, 16, and so on, until both subarrays combined are the array itself.|$|R
40|$|We survey recent {{results on}} {{periodic}} algorithms. We {{focus on the}} problems of <b>sorting,</b> <b>merging</b> and permuting and concentrate on algorithms that have small constant periods. Keywords periodic network, compare-exchange operation, comparator, <b>sorting,</b> <b>merging,</b> permutation routing, switch, shift 1 Introduction We consider algorithms of a special kind designed for simple networks consisting of weak processing units. We concentrate on one feature, namely periodicity, that makes such algorithms easy to implement. We consider so called comparator networks that are typically used for tasks such as <b>sorting,</b> <b>merging</b> of <b>sorted</b> sequences and selection. Below we recall two alternative definitions of comparator networks: The model: Comparator networks are traditionally defined as consisting of n wires connected by comparators (see Fig. 1). The n input items move on the wires from left to 0 1 2 3 Fig. 1. A comparator network with 3 parallel steps right. At each time there is exactly one item on eac [...] ...|$|R
50|$|There {{are many}} {{well-known}} methods by which an array can be sorted, which include, {{but are not}} limited to: selection sort, bubble <b>sort,</b> insertion <b>sort,</b> <b>merge</b> <b>sort,</b> quicksort, heapsort, and counting sort. These sorting techniques have different algorithms associated with them, and there are therefore different advantages to using each method.|$|R
50|$|Other {{examples}} of adaptive sorting algorithms are adaptive heap <b>sort,</b> adaptive <b>merge</b> <b>sort,</b> patience sort, Shellsort, smoothsort, splaysort and Timsort.|$|R
50|$|In general, {{polyphase}} <b>merge</b> <b>sort</b> {{is better}} than ordinary <b>merge</b> <b>sort</b> when there are less than 8 files, while ordinary <b>merge</b> <b>sort</b> starts to become better at around 8 or more files.|$|R
50|$|The <b>Sort</b> <b>Merge</b> Generator was an {{application}} developed by Betty Holberton in 1951 for the Univac I {{and is one}} of the first examples of using a computer to create a computer program. The input to the application was a specification of files and the kind of <b>sort</b> and <b>merge</b> operations to use and the output would be machine code for performing the specification.|$|R
50|$|John von Neumann {{wrote the}} first array <b>sorting</b> program (<b>merge</b> <b>sort)</b> in 1945, when the first stored-program {{computer}} was still being built.|$|R
25|$|Practical general sorting {{algorithms}} {{are almost}} always based on an algorithm with average time complexity (and generally worst-case complexity) O(n log n), of which the most common are heap <b>sort,</b> <b>merge</b> <b>sort,</b> and quicksort. Each has advantages and drawbacks, with the most significant being that simple implementation of <b>merge</b> <b>sort</b> uses O(n) additional space, and simple implementation of quicksort has O(n2) worst-case complexity. These problems can be solved or ameliorated {{at the cost of}} a more complex algorithm.|$|R
40|$|Abstract – While <b>merge</b> <b>sort</b> is well-understood in {{parallel}} algorithms theory, relatively {{little is known}} of how to implement parallel <b>merge</b> <b>sort</b> with mainstream parallel programming platforms, such as OpenMP and MPI, and run it on mainstream SMP-based systems, such as multi-core computers and multi-core clusters. This is misfortunate because <b>merge</b> <b>sort</b> {{is not only a}} fast and stable sort algorithm, but it is also an easy to understand and popular representative of the rich class of divide-and-conquer methods; hence better understanding of <b>merge</b> <b>sort</b> parallelization can contribute to better understanding of divide-and-conquer parallelization in general. In this paper, we investigate three parallel merge-sorts: shared memory <b>merge</b> <b>sort</b> that runs on SMP systems with OpenMP; message-passing <b>merge</b> <b>sort</b> that runs on computer clusters with MPI; and combined hybrid <b>merge</b> <b>sort,</b> with both OpenMP and MPI, that runs on clustered SMPs. We have experimented with our parallel <b>merge</b> <b>sorts</b> on a dedicated Rocks SMP cluster and on a virtual SMP luster in the Amazon Elastic Compute Cloud. In our experiments, shared memory <b>merge</b> <b>sort</b> with OpenMP has achieved best speedup. We believe that we are the first ones to concurrently experiment with- and compare – shared memory, message passing, and hybrid <b>merge</b> <b>sort.</b> Our results can help in the parallelization of specific practical <b>merge</b> <b>sort</b> routines and, even more important, in the practical parallelization of other divide-and-conquer algorithms for mainstream SMP-based systems...|$|R
50|$|The {{classic example}} of {{recursion}} is in list-sorting algorithms such as <b>Merge</b> <b>Sort.</b> The <b>Merge</b> <b>Sort</b> recursive algorithm will first repeatedly divide the list into consecutive pairs; each pair is then ordered, then each consecutive pair of pairs, and so forth until {{the elements of the}} list are in the desired order.|$|R
50|$|A more {{sophisticated}} <b>merge</b> <b>sort</b> that optimizes tape (and disk) drive usage is the polyphase <b>merge</b> <b>sort.</b>|$|R
30|$|The {{bandwidth}} calculation step takes Θ(n) time. <b>Sorting</b> using <b>Merge</b> <b>sort</b> or Quicksort takes Θ(nlogn) and {{the final}} pairing takes Θ(n) time. Hence the time complexity of initial pairing ≈ Θ(nlogn).|$|R
40|$|We survey recent {{results on}} {{periodic}} algorithms. We {{focus on the}} problems of <b>sorting,</b> <b>merging</b> and permuting and concentrate on algorithms that have small constant periods. (orig.) Available from TIB Hannover: RR 6673 (47) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekSIGLEDEGerman...|$|R
40|$|Abstract – While <b>merge</b> <b>sort</b> is well-understood in {{parallel}} algorithms theory, relatively {{little is known}} of how to implement parallel <b>merge</b> <b>sort</b> with mainstream parallel programming platforms, such as OpenMP and MPI, and run it on mainstream SMP-based systems, such as multi-core computers and multi-core clusters. This is misfortunate because <b>merge</b> <b>sort</b> {{is not only a}} fast and stable sort algorithm, but it is also an easy to understand and popular representative of the rich class of divide-and-conquer methods; hence better understanding of <b>merge</b> <b>sort</b> parallelization can contribute to better understanding of divide-and-conquer parallelization in general. In this paper, we investigate three parallel merge-sorts: shared memory <b>merge</b> <b>sort</b> that runs on SMP systems with OpenMP; message-passing <b>merge</b> <b>sort</b> that runs on computer cluster...|$|R
40|$|In {{computer}} science,especially for the algorithmic {{theory of}} information,the research of sorting algorithm {{is very important}} thema and from computational experiments,the quick sort algorithm,which is discoverd by Hoare[2]is known as fastest ones. However,the worst complexity of this sorting method is very large and {{for the sort of}} special data,which include much same data,its performance is less than other <b>sorting</b> method,for example <b>merge</b> <b>sort.</b> So,in this note,first,we analyse the complexity of quick <b>sort</b> and <b>merge</b> <b>sort</b> form theoritical aspect and show that quick sort algorithm is faster than <b>merge</b> <b>sort</b> algorithm,when each data are different. Secondly,we introduce the hash sort algorithm,which can sort numbers very fast. The complexity of sorting n numbers is O(n) and it is smaller than O(n log n),which is the complexity of quick <b>sort</b> or <b>merge</b> <b>sort.</b> Strictly saying,hash sort algorithm is not a sorting algorithm,since it treats only numbers. In this note,we apply the idea of hash sort to general data and propose an algorithm that dividing given general data into the sets of same data. Finally,we use this technic to quick sort algorithm and propose a new sorting method whose worst complexity is faster than that of quick sort...|$|R
40|$|In this master's thesis we studied, {{implemented}} and compared sequential and parallel sorting algorithms. We implemented seven algorithms: bitonic sort, multistep bitonic sort, adaptive bitonic <b>sort,</b> <b>merge</b> <b>sort,</b> quicksort, radix sort and sample sort. Sequential algorithms were implemented on a {{central processing unit}} using C++, whereas parallel algorithms were implemented on a graphics processing unit using CUDA architecture. We improved the above mentioned implementations and adopted {{them to be able}} to sort input sequences of arbitrary length. We compared algorithms on six different input distributions, which consist of 32 -bit numbers, 32 -bit key-value pairs, 64 -bit numbers and 64 -bit key-value pairs. The results show that radix sort is the fastest sequential sorting algorithm, whereas radix <b>sort</b> and <b>merge</b> <b>sort</b> are the fastest parallel algorithms (depending on the input distribution). With parallel implementations we achieved speedups of up to 157 -times in comparison to sequential implementations...|$|R
50|$|In {{the worst}} case, <b>merge</b> <b>sort</b> does about 39% fewer {{comparisons}} than quicksort {{does in the}} average case. In terms of moves, <b>merge</b> <b>sort's</b> worst case complexity is O(n log n)—the same complexity as quicksort's best case, and <b>merge</b> <b>sort's</b> best case takes about half as many iterations as the worst case.|$|R
3000|$|... “Request” {{structures}} are then staged in a linked-list called request queue. The request queue allows I/O schedulers to <b>sort,</b> <b>merge</b> and coalesce the requests {{depending on the}} locations they access. Appendix (“Conclusion and future works” section) describes {{the relationships between the}} block I/O kernel data-structures used by the block layer to perform I/O operations.|$|R
2500|$|While {{there are}} {{a large number of}} sorting algorithms, in {{practical}} implementations a few algorithms predominate. Insertion sort is widely used for small data sets, while for large data sets an asymptotically efficient sort is used, primarily heap <b>sort,</b> <b>merge</b> <b>sort,</b> or quicksort. Efficient implementations generally use a hybrid algorithm, combining an asymptotically efficient algorithm for the overall sort with insertion sort for small lists {{at the bottom of a}} recursion. Highly tuned implementations use more sophisticated variants, such as Timsort (<b>merge</b> <b>sort,</b> insertion sort, and additional logic), used in Android, Java, and Python, and introsort (quicksort and heap sort), used (in variant forms) in some C++ sort implementations and in [...]NET.|$|R
50|$|Although {{heapsort}} has {{the same}} time bounds as <b>merge</b> <b>sort,</b> it requires only Θ(1) auxiliary space instead of <b>merge</b> <b>sort's</b> Θ(n). On typical modern architectures, efficient quicksort implementations generally outperform mergesort for sorting RAM-based arrays. On the other hand, <b>merge</b> <b>sort</b> is a stable sort and is more efficient at handling slow-to-access sequential media. <b>Merge</b> <b>sort</b> is often {{the best choice for}} sorting a linked list: in this situation it is relatively easy to implement a <b>merge</b> <b>sort</b> {{in such a way that}} it requires only Θ(1) extra space, and the slow random-access performance of a linked list makes some other algorithms (such as quicksort) perform poorly, and others (such as heapsort) completely impossible.|$|R
5000|$|While {{there are}} {{a large number of}} sorting algorithms, in {{practical}} implementations a few algorithms predominate. Insertion sort is widely used for small data sets, while for large data sets an asymptotically efficient sort is used, primarily heap <b>sort,</b> <b>merge</b> <b>sort,</b> or quicksort. Efficient implementations generally use a hybrid algorithm, combining an asymptotically efficient algorithm for the overall sort with insertion sort for small lists {{at the bottom of a}} recursion. Highly tuned implementations use more sophisticated variants, such as Timsort (<b>merge</b> <b>sort,</b> insertion sort, and additional logic), used in Android, Java, and Python, and introsort (quicksort and heap sort), used (in variant forms) in some C++ sort implementations and in [...]NET.|$|R
2500|$|Heapsort also {{competes with}} <b>merge</b> <b>sort,</b> {{which has the}} same time bounds. <b>Merge</b> <b>sort</b> {{requires}} [...] auxiliary space, but heapsort requires only a constant amount. Heapsort typically runs faster in practice on machines with small or slow data caches, and does not require as much external memory. On the other hand, <b>merge</b> <b>sort</b> has several advantages over heapsort: ...|$|R
40|$|A {{fast and}} robust ellipse-detection method based on <b>sorted</b> <b>merging</b> is {{proposed}} in this paper. This method first represents the edge bitmap approximately {{with a set}} of line segments and then gradually merges the line segments into elliptical arcs and ellipses. To achieve high accuracy, a <b>sorted</b> <b>merging</b> strategy is proposed: the merging degrees of line segments/elliptical arcs are estimated, and line segments/elliptical arcs are merged in descending order of the merging degrees, which significantly improves the merging accuracy. During the merging process, multiple properties of ellipses are utilized to filter line segment/elliptical arc pairs, making the method very efficient. In addition, an ellipse-fitting method is proposed that restricts the maximum ratio of the semimajor axis and the semiminor axis, further improving the merging accuracy. Experimental results indicate that the proposed method is robust to outliers, noise, and partial occlusion and is fast enough for real-time applications...|$|R
50|$|Based on the amortized {{analysis}} of splay trees, {{the worst case}} running time of splaysort, on an input with n data items, is O(n log n), matching the time bounds for efficient non-adaptive algorithms such as quicksort, heap <b>sort,</b> and <b>merge</b> <b>sort.</b>|$|R
50|$|Funnelsort {{is similar}} to <b>merge</b> <b>sort</b> in that some number of subarrays are recursively sorted, after which a merging step {{combines}} the subarrays into one <b>sorted</b> array. <b>Merging</b> is performed by a device called a k-merger, which is described in the section below.|$|R
50|$|Note {{that most}} {{worst-case}} sorting algorithms that do optimally {{well in the}} worst-case, notably heap <b>sort</b> and <b>merge</b> <b>sort,</b> do not take existing order within their input into account, although this deficiency is easily rectified {{in the case of}} <b>merge</b> <b>sort</b> by checking if left.last_item ≤ right.first_item, in which case a merge operation may be replaced by simple concatenation - a modification that is well within the scope of making an algorithm adaptive.|$|R
