2551|1310|Public
5|$|The European Nucleotide Archive handles {{large volumes}} of data which pose a {{significant}} storage challenge. As of 2012, the ENA's <b>storage</b> <b>requirements</b> continue to grow exponentially, with a doubling time of approximately 10 months. To manage this increase, the ENA selectively discards less-valuable sequencing platform data and implements advanced compression strategies. The CRAM reference-based compression toolkit was developed to help reduce ENA <b>storage</b> <b>requirements.</b>|$|E
5|$|There is {{no limit}} {{in the law}} on number of owned guns. The law {{specifies}} safe <b>storage</b> <b>requirements</b> for those owning more than two weapons or more than 500 rounds of ammunition. The safe <b>storage</b> <b>requirements</b> are further exacerbated for those owning more than 10 and more than 20 firearms.|$|E
5|$|There is {{currently}} no restriction on caliber size and no restriction on magazine capacity. However, special safe <b>storage</b> <b>requirements</b> apply for those {{having more than}} 500, 10,000 and 20,000 bullets.|$|E
40|$|Abstract: In this report, {{we study}} the exact and an {{approximate}} {{formulation of the}} general problem of onedimensional periodic task scheduling under <b>storage</b> <b>requirement,</b> irrespective of machine constraints. We rely on the SIRA theoretical framework that allows an optimisation of periodic <b>storage</b> <b>requirement</b> [18]. SIRA is based on inserting some storage dependence arcs (storage reuse arcs) labeled with reuse distances directly on the data dependence graph. In this new graph, {{we are able to}} bound the <b>storage</b> <b>requirement</b> measured as the exact number of necessary storage locations. The determination of storage and distance reuse is parametrised by the desired minimal scheduling period (respectively maximal execution throughput) {{as well as by the}} <b>storage</b> <b>requirement</b> constraints- either can be minimised while the other one is bounded, or alternatively, both are bounded [6, 14]. This report recalls our fundamental results on this problem, and proposes new experimental heuristics. We typically show how we can deal with some specific storage architectural constraints such as buffers and rotating storage facilities. Key-words: Task scheduling, <b>Storage</b> <b>requirement,</b> Periodic scheduling, Task parallelis...|$|R
40|$|In {{this paper}} we use machine {{learning}} algorithms like SVM, KNN and GIS to perform a behaviorcomparison on the web pages classifications problem, from the experiment {{we see in the}} SVM with smallnumber of negative documents to build the centroids has the smallest <b>storage</b> <b>requirement</b> and the least online test computation cost. But almost all GIS with different number of nearest neighbors have an evenhigher <b>storage</b> <b>requirement</b> and on line test computation cost than KNN. This suggests that some futurework should be done to try to reduce the <b>storage</b> <b>requirement</b> and on list test cost of GIS...|$|R
40|$|Abstract — A novel <b>storage</b> <b>requirement</b> {{estimation}} methodology {{is presented}} {{for use in}} the early system design phases when the data transfer ordering is only partly fixed. At that stage, none of the existing estimation tools are adequate, as they either assume a fully specified execution order or ignore it completely. This paper presents an algorithm for automated estimation of strict upper and lower bounds on the individual data dependency sizes in high level application code given a partially fixed execution ordering. In the overall estimation technique, this is followed by a detection of the maximally combined size of simultaneously alive dependencies, resulting in the overall <b>storage</b> <b>requirement</b> of the application. Using representative application demonstrators, we show how our techniques can effectively guide the designer to achieve a transformed specification with low <b>storage</b> <b>requirement...</b>|$|R
5|$|All {{versions}} of Perl do automatic data-typing and automatic memory management. The interpreter knows {{the type and}} <b>storage</b> <b>requirements</b> of every data object in the program; it allocates and frees storage for them as necessary using reference counting (so it cannot deallocate circular data structures without manual intervention). Legal type conversions — for example, conversions from number to string — are done automatically at run time; illegal type conversions are fatal errors.|$|E
25|$|Another {{challenge}} is {{the storage of}} this bulk data. Depending on the application there could be high data acquisition requirements which in turn lead to high <b>storage</b> <b>requirements.</b> Currently the internet is already responsible for 5% of the total energy generated and this consumption will increase significantly when we start utilizing applications with multiple embedded sensors.|$|E
25|$|The {{following}} example query is the snowflake schema {{equivalent of}} the star schema example code which returns {{the total number of}} units sold by brand and by country for 1997. Notice that the snowflake schema query requires many more joins than the star schema version in order to fulfill even a simple query. The benefit of using the snowflake schema in this example is that the <b>storage</b> <b>requirements</b> are lower since the snowflake schema eliminates many duplicate values from the dimensions themselves.|$|E
40|$|In this paper, {{we propose}} a novel <b>storage</b> <b>requirement</b> {{estimation}} methodology {{for use in}} the early system design phases when the data transfer ordering is only partially fixed. At that stage, none of the existing estimation tools are adequate, as they either assume a fully specified execution order or ignore it completely. Using representative application demonstrators, we show how our technique can effectively guide the designer to achieve a transformed specification with low <b>storage</b> <b>requirement.</b> 1...|$|R
5000|$|Note {{that each}} {{choice of the}} bit-subset {{selector}} imposes a <b>storage</b> <b>requirement</b> (C) that is exponential in the cardinality of the set of chosen bits.|$|R
40|$|This is a {{continuation}} work to SIRA (Sid-Ahmed-Ali Touati and Christine Eisenbeis. Early Periodic Register Allocation on ILP Processors. Parallel Processing Letters, Vol. 14, No. 2, June 2004. World Scientific.). We exetend that work with new heuristics and experimental results. In this report, we study the exact and an approximate formulation of the general problem of one-dimensional periodic task scheduling under <b>storage</b> <b>requirement,</b> irrespective of machine constraints. We rely on the SIRA theoretical framework that allows an optimisation of periodic <b>storage</b> <b>requirement</b> Touati:PPL: 04. SIRA is based on inserting some storage dependence arcs (storage reuse arcs) labeled with reuse distances directly on the data dependence graph. In this new graph, {{we are able to}} bound the <b>storage</b> <b>requirement</b> measured as the exact number of necessary storage locations. The determination of storage and distance reuse is parametrised by the desired minimal scheduling period (respectively maximal execution throughput) {{as well as by the}} <b>storage</b> <b>requirement</b> constraints - either can be minimised while the other one is bounded, or alternatively, both are bounded siralina 07,RR-INRIA-HAL- 00436348. This report recalls our fundamental results on this problem, and proposes new experimental heuristics. We typically show how we can deal with some specific storage architectural constraints such as buffers and rotating storage facilities...|$|R
25|$|Finding a modular {{multiplicative inverse}} has many {{applications}} in algorithms {{that rely on}} the theory of modular arithmetic. For instance, in cryptography the use of modular arithmetic permits some operations {{to be carried out}} more quickly and with less <b>storage</b> <b>requirements,</b> while other operations become more difficult. Both of these features can be used to advantage. In particular, in the RSA algorithm, encrypting and decrypting a message is done using a pair of numbers that are multiplicative inverses with respect to a carefully selected modulus. One of these numbers is made public and {{can be used in a}} rapid encryption procedure, while the other, used in the decryption procedure, is kept hidden. Determining the hidden number from the public number is considered to be computationally infeasible and this is what makes the system work to ensure privacy.|$|E
2500|$|... 1993 The Iñes Table – {{designed}} for a project in Grenoble – a multi-use piece of furniture accommodating a variety of working and <b>storage</b> <b>requirements.</b>|$|E
2500|$|... 2005 saw {{the opening}} of the Stage 6 Building at the Government Records Repository at Kingswood by The Hon Bob Debus, Attorney General and Minister for the Arts. This {{building}} is a [...] "state of the art" [...] facility equipped with geothermal air conditioning, high thermal mass insulation and an argon gas fire suppression system. In 2005 it was estimated that this building would fulfil the <b>storage</b> <b>requirements</b> of the NSW government for non-current records until 2011.|$|E
30|$|Low-rank property: The {{matrices}} Xk {{turn out}} to have low rank, and hence, the algorithm has minimum <b>storage</b> <b>requirement</b> since it only needs to keep principal factors in memory.|$|R
3000|$|... [*]bits. As {{shown in}} our {{simulation}} results in Figure 5, the maximum node {{degree in the}} proposed scheme is 7. However, in the probabilistic schemes, the <b>storage</b> <b>requirement</b> is [...]...|$|R
5000|$|A Range Query Tree with an {{underlying}} array of size n (padded to {{a power of}} two) has n leaves {{and a total of}} [...] nodes which requires O(n) <b>storage</b> <b>requirement.</b>|$|R
2500|$|The storage {{problem can}} be {{alleviated}} by an implementation technique called alphabet reduction, whereby the original strings are reinterpreted as longer strings over a smaller alphabet. E.g., a string of [...] bytes can alternatively {{be regarded as a}} string of [...] four-bit units and stored in a trie with sixteen pointers per node. Lookups need to visit twice as many nodes in the worst case, but the <b>storage</b> <b>requirements</b> go down by a factor of eight.|$|E
2500|$|The {{typesetting}} of Math in TeX is {{not without}} criticism, {{particularly with respect to}} technical details of the font metrics, which were designed in an era when significant attention was paid to <b>storage</b> <b>requirements.</b> This resulted in some [...] "hacks" [...] overloading some fields, which in turn required other [...] "hacks." [...] On an aesthetics level, the rendering of radicals has also been criticized. The OpenType math font specification largely borrows from TeX, but has some new features/enhancements.|$|E
2500|$|Salt caverns {{are usually}} {{much smaller than}} {{depleted}} gas reservoir and aquifer storage facilities. [...] A salt cavern facility may occupy only one one-hundredth of the area taken up by a depleted gas reservoir facility. Consequently, salt caverns cannot hold the large volumes of gas necessary to meet base load <b>storage</b> <b>requirements.</b> [...] Deliverability from salt caverns is, however, much higher than for either aquifers or depleted reservoirs. This allows the gas stored in a salt cavern to be withdrawn and replenished more readily and quickly. This quick cycle-time is useful in emergency situations or during short periods of unexpected demand surges.|$|E
5000|$|Storage - The {{amount of}} memory {{required}} for the processing of the lock mechanism. The <b>storage</b> <b>requirement</b> scales {{with the number of}} threads due to the increase {{in the size of the}} array can_serve.|$|R
40|$|Data {{storage is}} an {{important}} contributor to power dis-sipation, particularly in multi-media embedded systems. To achieve low power implementation for these systems, the <b>storage</b> <b>requirement</b> can be reduced by exploiting lim-ited data life-times and reuse of memory locations. This optimisation can only be performed quite late in the de-sign trajectory {{because most of the}} decisions on e. g. detailed memory access organisation should have been fixed. Thus an early and fast estimate of the <b>storage</b> <b>requirement</b> is crucial for the system designer. This pa-per proposes a grouping technique that identifies simul-taneously alive sets of data dependencies. The group-ing technique extends previous work on storage size re-quirement estimation for individual data dependencies, allowing the designer to obtain the global <b>storage</b> size <b>requirement</b> for an application. A prototype CAD tool has been developed and used to test the technique on real-life multi-media kernels. ...|$|R
40|$|Abstract—As network {{technologies}} advance, Scalable Video Coding (SVC) {{has become}} increasingly popular due to its universal multimedia access capability and competitive compression performance with the state-of-the-art single-layer video coding. However, it’s decoding delay, memory bandwidth and <b>storage</b> <b>requirement</b> are much larger than those of single-layer video coding due to various scalabilities. In this paper, we propose a novel display order instead of direct bitstream order oriented decoding method for the SVC decoder to solve above implementation problems. The analysis for hardware-oriented algorithm shows that the proposed decoding order can reduce the decoding delay, memory bandwidth and <b>storage</b> <b>requirement</b> significantly while is still applicable to various scalable requirements...|$|R
2500|$|Quality values {{account for}} {{about half of the}} {{required}} disk space in the FASTQ format (before compression), and therefore the compression of the quality values can significantly reduce <b>storage</b> <b>requirements</b> and speed up analysis and transmission of sequencing data. Both lossless and lossy compression are recently being considered in the literature. For example, the algorithm QualComp [...] performs lossy compression with a rate (number of bits per quality value) specified by the user. Based on rate-distortion theory results, it allocates the number of bits so as to minimize the MSE (mean squared error) between the original (uncompressed) and [...] the reconstructed (after compression) quality values. Other algorithms for compression of quality values include SCALCE [...] and Fastqz. Both are lossless compression algorithms that provide an optional controlled lossy transformation approach. For example, SCALCE reduces the alphabet size based on the observation that “neighboring” quality values are similar in general.|$|E
50|$|There is {{no limit}} {{in the law}} on number of owned guns. The law {{specifies}} safe <b>storage</b> <b>requirements</b> for those owning more than two weapons or more than 500 rounds of ammunition. The safe <b>storage</b> <b>requirements</b> are further exacerbated for those owning more than 10 and more than 20 firearms.|$|E
5000|$|Allocating system {{storage and}} {{planning}} future <b>storage</b> <b>requirements</b> for the database system ...|$|E
30|$|There {{are four}} sets of tests in this {{performance}} evaluation: the first set compares the <b>storage</b> <b>requirement</b> of three schemes. The second set analyzes labeling time. The third set examines the query performance and the last set investigates update performance.|$|R
40|$|International audienceIn this paper, {{we study}} an {{efficient}} approximate integer linear programming {{formulation of the}} general problem of one-dimensional periodic task scheduling under <b>storage</b> <b>requirement,</b> irrespective of machine constraints. We have already presented in [8] a theoretical framework that allows an optimal optimization of periodic <b>storage</b> <b>requirement</b> in a periodic schedule. This problem is used to optimise processor register usage in embedded systems. Our storage optimisation problem being NP-complete [7], solving an exact integer linear programming formulation as proposed in [8] is too expensive in practice. In this article, we give an efficient approximate model that allows fast resolution times while providing nearly optimal results. Our solution has been implemented and included inside a compiler for embedded processors...|$|R
40|$|<b>Storage</b> <b>requirement</b> and {{computational}} efficiency {{have always}} been challenges for the efficient implementation of discontinuous Galerkin (DG) methods for real life applications. In this paper, a fully implicit Jacobian-Free Newton-Krylov (JFNK) method is developed {{in the context of}} DG discretizations for the three-dimensional compressible Euler and Navier-Stokes equations. Compared with the Jacobian-based methods, the Jacobian-Free approach saves the storage for the Jacobian matrix which can be of great importance for DG methods. Three types of preconditioners are investigated in which the block diagonal preconditioner requires the least storage, while the block LU-SGS and ILU 0 preconditioners require more storage but are more computationally efficient. An implicit time-stepping strategy is adopted for the stability of the current solver, which is based upon a hexahedral spatial mesh, and the nonlinear solver package Kinsol is used to improve the computational efficiency and robustness. Numerical results demonstrate that the preconditioned JFNK-DG solver can substantially reduce the <b>storage</b> <b>requirement</b> compared with the Jacobian based method without significantly compromising accuracy or efficiency. Furthermore, as a good compromise between efficiency and <b>storage</b> <b>requirement,</b> the ILU 0 preconditioner shows the best choice of the preconditioners presented...|$|R
5000|$|ISO 11799:2003 Information and documentation—Document <b>storage</b> <b>requirements</b> for archive {{and library}} {{materials}} ...|$|E
50|$|The European Nucleotide Archive handles {{large volumes}} of data which pose a {{significant}} storage challenge. As of 2012, the ENA's <b>storage</b> <b>requirements</b> continue to grow exponentially, with a doubling time of approximately 10 months. To manage this increase, the ENA selectively discards less-valuable sequencing platform data and implements advanced compression strategies. The CRAM reference-based compression toolkit was developed to help reduce ENA <b>storage</b> <b>requirements.</b>|$|E
50|$|CIRT also {{maintains}} a computer centre, {{catering to the}} communication and data <b>storage</b> <b>requirements</b> of the Institute.|$|E
3000|$|... for {{the maximum}} {{activity}} descriptor. The activity images' binary nature significantly mitigates this large <b>storage</b> <b>requirement,</b> but a conventional high-level implementation still {{suffers from the}} number of required memory access operation. A solution {{can be found in the}} following writing of (14): [...]...|$|R
3000|$|... [(5 ∗(13 −i)+j) mod 45]. To be sure, 45 words {{represent}} {{an excess of}} storage relative to the minimum <b>storage</b> <b>requirement</b> of 38 words, but the advantage {{is that there is}} an easy-to-design function directing the mapping from the index space to the data storage.|$|R
3000|$|...x+i,y+j)) {{needs to}} be {{computed}} across the scan lines, leading to {{a significant increase in}} the <b>storage</b> <b>requirement.</b> As confirmed later in Section 3.4, the proposed multi-center bilateral filter operates more efficiently while achieving a similar stereo matching accuracy compared with the single-center case.|$|R
