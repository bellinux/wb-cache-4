2971|1282|Public
5|$|The World Organization to Investigate the Persecution of Falun Gong further called {{attention}} to portrayals of Wang Jindong on state-run television, claiming {{that the man who}} self-immolated on the square was not the same person who appeared in subsequent interviews with CCTV. It pointed to a voice analysis conducted by the <b>Speech</b> <b>Processing</b> Laboratory at National Taiwan University, which concluded that the voices did not match, and also noted that the hairline and facial proportions appeared to be different. These observations were used to advance the theory that the self-immolators were actors.|$|E
25|$|Neurocomputational <b>speech</b> <b>processing</b> is <b>speech</b> <b>processing</b> by {{artificial}} neural networks. Neural maps, mappings and pathways {{as described}} below, are model structures, i.e. important structures within artificial neural networks.|$|E
25|$|Neurocomputational {{models of}} <b>speech</b> <b>processing</b> are complex. They {{comprise}} {{at least a}} cognitive part, a motor part and a sensory part.|$|E
5000|$|<b>Speech</b> signal <b>{{processing}}</b> {{for processing}} and interpreting spoken words ...|$|R
30|$|As an {{important}} branch of signal <b>processing,</b> <b>speech</b> signal <b>processing</b> {{has achieved a}} considerable development in past decades. In addition, the application of CS theory {{to the field of}} <b>speech</b> signal <b>processing</b> is becoming a rising research focus. In[5, 6], the sparsity of the residual excitation is utilized to construct sparsifying matrices for voiced speech signals. However, in the aforementioned two literatures, the sparsifying matrix constructed using the impulse response for voiced speech is impractical for its dependence on the currently reconstructed signal itself. Therefore, a codebook of impulse response vectors generated from the training speech data is proposed as the sparsifying matrix in[5].|$|R
5000|$|P. Stoica and A. Nehorai, Music, Maximum {{likelihood}} and the Cramér-Rao bound. IEEE Trans. Acoustics, <b>Speech,</b> Signal <b>Processing,</b> vol. ASSP-37, 720-741, 1989.|$|R
25|$|The motor {{part of a}} neurocomputational {{model of}} <b>speech</b> <b>processing</b> starts with a phonemic {{representation}} of a speech item, activates a motor plan and ends with the articulation of that particular speech item (see also: articulatory phonetics).|$|E
25|$|The sensory {{part of a}} neurocomputational {{model of}} <b>speech</b> <b>processing</b> starts with an {{acoustic}} signal of a speech item (acoustic speech signal), generates an auditory representation for that signal and activates a phonemic representations for that speech item.|$|E
25|$|Verbal Comprehension is {{a fairly}} complex process, {{and it is not}} fully understood. From various studies and experiments, it has been found that the {{superior}} temporal sulcus activates when hearing human speech, and that <b>speech</b> <b>processing</b> seems to occur within Wernicke's area.|$|E
5000|$|... 2009: Johann-Philipp-Reis Award for Sebastian Möller in {{recognition}} of {{his work in the}} areas of <b>speech</b> signal <b>processing,</b> telecommunications and man-machine interaction.|$|R
30|$|However, {{despite many}} years of effort devoted to {{developing}} algorithms for <b>speech</b> signal <b>processing,</b> and despite the elaboration of automatic speech recognition and synthesis systems, {{our knowledge of the}} nature of the speech signal and the effects of pathologies is still limited. In spite of this, voice scientists and clinicians take profit of the simple models and methods developed by <b>speech</b> signal <b>processing</b> engineers to build up their own analysis methods for the assessment of disorders of voice (DoV).|$|R
40|$|To {{determine}} the neural mechanisms involved in vocal emotion processing, {{the current study}} employed {{functional magnetic resonance imaging}} (fMRI) to investigate the neural structures engaged in processing acoustic cues to infer emotional meaning. Two critical acoustic cues – pitch and speech rate – were systematically manipulated and presented in a discrimination task. Results confirmed that a bilateral network constituting frontal and temporal regions is engaged when discriminating vocal emotion expressions; however, we observed greater sensitivity to pitch cues in the right mid superior temporal gyrus/sulcus (STG/STS), whereas activation in both left and right mid STG/STS was observed for <b>speech</b> rate <b>processing.</b> Index terms: prosody, fMRI, pitch <b>processing,</b> <b>speech</b> rate <b>processing,</b> emotion comprehensio...|$|R
25|$|A neural {{representation}} {{within an}} {{artificial neural network}} is a temporarily activated (neural) state within a specific neural map. Each neural state is represented by a specific neural activation pattern. This activation pattern changes during <b>speech</b> <b>processing</b> (e.g. from syllable to syllable).|$|E
25|$|Neurocomputational <b>speech</b> <b>processing</b> is computer-simulation {{of speech}} {{production}} and speech perception {{by referring to}} the natural neuronal processes of speech production and speech perception, as they occur in the human nervous system (central nervous system and peripheral nervous system). This topic is based on neuroscience and computational neuroscience.|$|E
25|$|In {{terms of}} {{artificial}} intelligence the articulatory {{model can be}} called plant (i.e. the system, which {{is controlled by the}} brain); it represents a part of the embodiement of the neuronal <b>speech</b> <b>processing</b> system. The articulatory model generates sensory output which is the basis for generating feedback information for the DIVA model (see below: feedback control).|$|E
5000|$|He is the {{co-inventor}} of the Relative Spectral (RASTA) {{approach to}} <b>speech</b> signal <b>processing,</b> first {{described in a}} technical report published in 1991.|$|R
40|$|Abstract: Epidemiological {{studies suggest}} that {{lifetime}} prevalence of voice disorders is about 30 % for the general adult population. Moreover, vocal perfor-mance degradation may be amongst the earliest indi-cators of a neurodegenerative disease onset, such as Parkinson’s disease. Lacking alternative cost-effective biomarkers, biomedical <b>speech</b> signal <b>processing</b> has been gaining increasing impetus towards developing clinical decision support tools. Acoustic analysis of speech signals provides a convenient, automatic, accu-rate, robust, inexpensive, scalable approach assisting medical diagnosis and symptom severity monitoring. Nevertheless, the algorithmic tools developed for bio-medical <b>speech</b> signal <b>processing</b> are spread across different software platforms, hindering direct algo-rithmic comparisons and the further development of this impending field. This study brings many biomedi-cal <b>speech</b> signal <b>processing</b> algorithms together un-der the same software platform, and {{has led to the}} development of a practical free toolkit which can be accessed over the Internet using a simple application...|$|R
40|$|The two {{principal}} areas of {{natural language processing}} research in pragmatics are belief modelling and <b>speech</b> act <b>processing.</b> Belief modelling {{is the development of}} techniques to represent the mental attitudes of a dialogue participant. The latter approach, <b>speech</b> act <b>processing,</b> based on <b>speech</b> act theory, involves viewing dialogue in planning terms. Utterances in a dialogue are modelled as steps in a plan where understanding an utterance involves deriving the complete plan a speaker is attempting to achieve. However, previous speech act ba~sed approaches have been limited by a reliance upon relatively simplistic belief modelling techniques and their relationship to planning and plan recognition. In particular, such techniques assume precomputed nested belief structures. In this paper, we will present an approach to <b>speech</b> act <b>processing</b> based on novel belief modelling techniques where nested beliefs are propagated on demand...|$|R
25|$|DSP {{processor}} ICs {{are found}} in every type of modern electronic systems and products including, SDTV | HDTV sets, radios and mobile communication devices, Hi-Fi audio equipment, Dolby noise reduction algorithms, GSM mobile phones, mp3 multimedia players, camcorders and digital cameras, automobile control systems, noise cancelling headphones, digital spectrum analyzers, intelligent missile guidance, radar, GPS based cruise control systems, {{and all kinds of}} image processing, video processing, audio processing, and <b>speech</b> <b>processing</b> systems.|$|E
25|$|A severe {{problem of}} phonetic or {{sensorimotor}} models of <b>speech</b> <b>processing</b> (like DIVA or ACT) {{is that the}} development of the phonemic map during speech acquisition is not modeled. A possible solution of this problem could be a direct coupling of action repository and mental lexicon without explicitly introducing a phonemic map at the beginning of speech acquisition (even at the beginning of imitation training; see Kröger et al. 2011 PALADYN Journal of Behavioral Robotics).|$|E
25|$|This {{approach}} has been proposed {{as an alternative to}} the BOLD fMRI technique and used to detect visual response to photic stimulation, motor activation by finger tapping and activations in language areas during <b>speech</b> <b>processing.</b> Recently functional real-time single-voxel proton spectroscopy (fSVPS) has been proposed as a technique for real-time neurofeedback studies in magnetic fields of 7 tesla (7 T) and above. This approach could have potential advantages over BOLD fMRI and is the subject of current research.|$|E
40|$|In {{this paper}} an audio {{processing}} solution for video conference based aerobics is presented. The proposed solution leaves the workout music unaltered by separating {{it from the}} <b>speech</b> and <b>processing</b> each signal separately. The <b>speech</b> signal <b>processing</b> is also performed at a lower sample rate, which saves computational power. Real time evaluation of the system shows that high quality music {{as well as a}} good two-way communication is maintained during the aerobic session. Publiced in: Consumer Electronics (ICCE), 2010 Digest of Technical Papers International Conference on, [URL]...|$|R
5000|$|<b>Speech</b> & Language <b>Processing</b> for Assistive Technologies: SIGSLPAT ...|$|R
40|$|This diploma thesis {{deals with}} the <b>speech</b> signal <b>processing</b> and vowel {{analysis}} mainly to uncover differences in speech features depending on the emotional state of speaker. Created application ARePa for <b>speech</b> signal <b>processing</b> was developer in Matlab environment and contains Graphical User Interface (GUI) for better manipulation with ARePa and analysed records. The application includes a complete analysis of the speech signal and further comparison of current feature with feature values from database using histograms. Of course, the developer application allows the archivation of currently analysed records into database...|$|R
25|$|In 1949, José Enrique Moyal, who had derived it independently, {{recognized}} it as the quantum moment-generating functional, and thus {{as the basis of}} an elegant encoding of all quantum expectation values, and hence quantum mechanics, in phase space (cf. phase space formulation). It has applications in statistical mechanics, quantum chemistry, quantum optics, classical optics and signal analysis in diverse fields such as electrical engineering, seismology, time–frequency analysis for music signals, spectrograms in biology and <b>speech</b> <b>processing,</b> and engine design.|$|E
25|$|In 2003, LSTM {{started to}} become {{competitive}} with traditional speech recognizers. In 2007, the combination with CTC achieved first good results on speech data. In 2009, a CTC-trained LSTM {{was the first}} RNN to win pattern recognition contests, when it won several competitions in connected handwriting recognition. In 2014, Baidu used CTC-trained RNNs to break the Switchboard Hub5'00 speech recognition benchmark, without traditional <b>speech</b> <b>processing</b> methods. LSTM also improved large-vocabulary speech recognition, text-to-speech synthesis, for Google Android, and photo-real talking heads. In 2015, Google's speech recognition experienced a 49% improvement through CTC-trained LSTM.|$|E
25|$|Most {{language}} {{production and}} processing {{occur in the}} left hemisphere {{while the majority of}} the emotional processing and production of emotion in speech occurs in the right hemisphere. Schizophrenic patients usually have difficulty processing prosody. These patients also show a remarkable increase in lateralization towards the right hemisphere of both emotionally and non-emotional prosody rich speech. Also, a decrease in right-handedness led to an increase in the right hemisphere lateralization. This right hemisphere lateralization extends beyond prosody to many of aspects of language and <b>speech</b> <b>processing</b> in schizophrenic patients.|$|E
5000|$|... "The Computation of Line Spectral Frequencies Using Chebyshev Polynomials"/ P. Kabal and R. P. Ramachandran. IEEE Trans. Acoustics, <b>Speech,</b> Signal <b>Processing,</b> vol. 34, no. 6, pp. 1419-1426, Dec. 1986.|$|R
5000|$|Editor-in-Chief, IEEE Transactions on <b>Speech</b> and Audio <b>Processing,</b> 1996-2002 ...|$|R
5000|$|... #Article: International Conference on Acoustics, <b>Speech,</b> and Signal <b>Processing</b> ...|$|R
25|$|After the 1989 fall of communism, Jelinek helped {{establish}} scientific relationships, regularly visiting to lecture {{and helping to}} persuade IBM to establish a computing centre at Charles University. In 1993, he retired from IBM and went to Johns Hopkins University's Center for Language and <b>Speech</b> <b>Processing,</b> where he was director and Julian Sinclair Smith Professor of Electrical and Computer Engineering. He was still working {{there at the time}} of his death; Jelinek died of a heart attack at the close of an otherwise normal workday in mid-September 2010. He was survived by his wife, daughter and son, sister, stepsister, and three grandchildren: including Sophie Gold Jelinek.|$|E
25|$|Jelinek {{was born}} in Czechoslovakia just before the {{outbreak}} of World War II and emigrated {{with his family to}} the United States {{in the early years of}} the communist regime. He studied engineering at the Massachusetts Institute of Technology and taught for 10 years at Cornell University before being offered a job at IBM Research. In 1961, he married Czech screenwriter Milena Jelinek. At IBM, his team advanced approaches to computer speech recognition and machine translation. After IBM, he went to head the Center for Language and <b>Speech</b> <b>Processing</b> at Johns Hopkins University for 17 years, where he was still working on the day he died.|$|E
2500|$|<b>Speech</b> <b>{{processing}}</b> [...] {{study of}} speech signals and the processing methods of these signals. The signals are usually processed {{in a digital}} representation, so <b>speech</b> <b>processing</b> {{can be regarded as}} a special case of digital signal processing, applied to speech signal. Aspects of <b>speech</b> <b>processing</b> includes the acquisition, manipulation, storage, transfer and output of digital speech signals.|$|E
5000|$|IEEE/ACM Transactions on Audio, <b>Speech,</b> and Language <b>Processing</b> (TASLP) ...|$|R
5000|$|International Conference on Acoustics, <b>Speech,</b> and Signal <b>Processing</b> (ICASSP) ...|$|R
40|$|This is the {{accepted}} manuscript {{of a paper}} published in the 2014 IEEE International Conference on Acoustics, <b>Speech</b> and Signal <b>Processing</b> (ICASSP) (Acoustics, <b>Speech</b> and Signal <b>Processing</b> (ICASSP), 2014 IEEE International Conference on, Issue Date: 4 - 9 May 2014, Written by: Yoshioka, T.; Xie Chen; Gales, M. J. F.) ...|$|R
