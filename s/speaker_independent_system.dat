16|6426|Public
40|$|Colloque avec actes et comité de lecture. nationale. National audienceIn this paper, two {{adaptation}} schemes are presented: SMAP and SMLLR. Both methods {{update the}} parameters of the acoustic models of a <b>speaker</b> <b>independent</b> <b>system</b> in order to improve its performances for a new speaker. We experimented SMAP and SMLLR to HMMs of the ESPERE engine in the batch mode and in the unsupervised incremental mode. The HMMs were learned on the Resource Management corpus. Results of the batch adaptation show a greatest efficienty of SMAP. For the unsupervised incremental adaptation, SMLLR is more powerful than SMAP, according to the incremental scheme we choose...|$|E
40|$|Abstract. This paper investigates use of {{a machine}} learnt model for {{recognition}} of individually words spoken in Urdu language. Speech samples from many different speakers were utilized for modeling. Original time-domain samples are normalized and pre-processed by applying discrete Fourier transformation for speech feature extraction. In frequency domain, high degree of correlation was found for the same words spoken by different speakers. This helped produce models with high recognition accuracy. Details of model realization in MATLAB are included in this paper. Current work is being extended using linear predictive coding for efficient hardware implementation. Key words: Urdu speech processing, feature extraction, <b>speaker</b> <b>independent</b> <b>system,</b> machine learning, data pre-processing, modeling. ...|$|E
40|$|Application {{specific}} voice interfaces {{in local}} languages {{will go a}} long way in reaching the benefits of technology to rural India. A continuous speech recognition system in Hindi tailored to aid teaching Geometry in Primary schools is the goal of the work. This paper presents the preliminary work done towards that end. We have used the Mel Frequency Cepstral Coefficients as speech feature parameters and Hidden Markov Modeling to model the acoustic features. Hidden Markov Modeling Tool Kit — 3. 4 was used both for feature extraction and model generation. The Julius recognizer which is language independent was used for decoding. A <b>speaker</b> <b>independent</b> <b>system</b> is implemented and results are presented...|$|E
40|$|In Malaysia, many {{researchers}} focus on developing <b>speaker</b> <b>independent</b> <b>systems</b> for training or articulation therapy or to assist language learners {{to learn about}} Malay Language or Bahasa Malaysia. Accuracy, noise robustness and processing time are concerns when developing speech therapy systems. In this study, a Malay word pronunciation test application was developed using the first 3 format and fundamental frequencies {{in an effort to}} improve pronunciation in Malay. This application was developed using Matlab and uses a vowel recognition algorithm classified using MLP classification technique. The application was developed and tested on UUM undergraduate students. For vowel classification, when fundamental frequency was added, 3 -format feature vowel classification rate increased by 1. 55...|$|R
40|$|In this paper, we {{describe}} a new speaker verication approach, using a hybrid HMM/ANN system, and accommodating user customized passwords. This system is exploiting the high phonetic recognition rates usually achieved by HMM/ANN <b>speaker</b> <b>independent</b> <b>systems</b> to infer the HMM topology {{associated with the}} user specic password from a few utterances of that password. Dierent adaptation schemes are then compared to quickly adapt the <b>speaker</b> <b>independent</b> ANN parameters used for HMM inference into speaker dependent parameters used for speaker verication. Dierent scoring criteria, based on normalized accumulated posterior probabilities (previously used as condence measures in speech recognition) are also compared. Based on these improvements, our best system achieved false acceptance and false rejection rates of 8. 2 % and 3. 2 %, respectively, corresponding to an a posteriori threshold set to the minimum of the HTER (half total error rate), and in the worse case where all customers are using the same password...|$|R
40|$|Speech {{recognition}} {{is important for}} successful development of speech recognizers in most real world applications. While speaker dependent speech recognizers have achieved close to 100 % accuracy, the <b>speaker</b> <b>independent</b> speech recognition <b>systems</b> have poor accuracy not exceeding 75 %. In this paper we describe a two-module <b>speaker</b> <b>independent</b> speech recognition <b>system</b> for all-British English speech. The first module performs phoneme recognition using two-level neural networks. The second module executes word recognition from the string of phonemes employing Hidden Markov Model. The system was trained by British English speech consisting of 5000 words uttered by 100 speakers. The test samples comprised 2000 words spoken by {{a different set of}} 50 speakers. The recognition accuracy is found to be 98 % which is well above the previous results. Keywords: <b>speaker</b> <b>independent</b> speech recognition, Neural Network...|$|R
40|$|This {{dissertation}} {{addresses the}} independence of observations assumption which is typically made by today’s automatic speech recognition systems. This assumption ignores within-speaker correlations which are known to exist. The assumption clearly damages the recognition ability of standard speaker independent systems, as can seen by the severe drop in performance exhibited by systems between their speaker dependent mode and their speaker independent mode. The typical {{solution to this problem}} is to apply speaker adaptation to the models of the <b>speaker</b> <b>independent</b> <b>system.</b> This approach is examined in this thesis with the explicit goal of improving the rapid adaptation capabilities of the system by incorporating within-speaker correlation information into the adaptation process. This is achieved through the creation of an adaptation technique called reference speaker weighting and {{in the development of a}} speaker clustering technique called speaker cluster weighting. However, speake...|$|E
40|$|It has {{recently}} been shown that normalisation of vocal tract length can significantly increase recognition accuracy in speaker independent automatic speech recognition systems. An inherent difficulty with this technique is in automatically estimating the normalisation parameter from a new speaker's speech and previous techniques have typically relied on an exhaustive search to estimate this parameter. In this paper, we present a method of normalising utterances by a linear warping of the mel filter bank channels in which in which the normalisation parameter is estimated by fitting formant estimates to a probabilistic model. This method is fast, computitionally inexpensive and requires only {{a limited amount of}} data for estimation. It generates normalisations which are close to those which would be found by an exhaustive search. The normalisation is applied to a phoneme recognition task using the TIMIT database and results show a useful improvement over an un-normalised <b>speaker</b> <b>independent</b> <b>system...</b>|$|E
40|$|This paper {{proposes a}} {{differentiable}} pooling mechanism to perform model-based neural network speaker adaptation. The proposed tech-nique learns a speaker-dependent combination of activations within pools of hidden units, {{was shown to}} work well unsupervised, and does not require speaker-adaptive training. We have conducted a set of experiments on the TED talks data, as used in the IWSLT evalu-ations. Our {{results indicate that the}} approach can reduce word error rates (WERs) on standard IWSLT test sets by about 5 – 11 % relative compared to speaker-independent systems and was found comple-mentary to the recently proposed learning hidden units contribution (LHUC) approach, reducing WER by 6 – 13 % relative. Both methods were also found to work well when adapting with small amounts of unsupervised data – 10 seconds is able to decrease the WER by 5 % relative compared to the baseline <b>speaker</b> <b>independent</b> <b>system...</b>|$|E
40|$|Conventional <b>speaker</b> <b>independent</b> speech {{recognition}} <b>systems</b> are trained {{using data from}} many different speakers. Inter-speaker variability {{is a major problem}} because parametric representations of speech are highly speaker dependent. This paper describes a technique which allows speaker dependent parameters to be considered when building a <b>speaker</b> <b>independent</b> {{speech recognition}} <b>system.</b> The technique is based on utterance clustering, where subsets of the training data are formed and the variability within each subset minimized. Cluster dependent connectionist models are then used to estimate phone probabilities as part of a hybrid connectionist hidden Markov model based large vocabulary talker <b>independent</b> speech recognition <b>system.</b> The system has been evaluated on the ARPA Wall Street Journal continuous speech recognition task. 1. INTRODUCTION Speaker dependent speech recognition systems are generated using training utterances from a single speaker, resulting in a system tuned to a spec [...] ...|$|R
40|$|Phoneme {{recognition}} {{is important for}} successful development of speech recognizers in most real world applications. While speaker dependent phoneme recognizers have achieved close to 100 % accuracy, the <b>speaker</b> <b>independent</b> phoneme recognition <b>systems</b> have poor accuracy not exceeding 75 %. In this paper we describe a two-module <b>speaker</b> <b>independent</b> phoneme recognition <b>system</b> for all-Indian English speech. The first module performs classification of phonemes recognition using Probabilistic neural networks. The second module executes the recognized phonemes from the classified phonemes employing Recurrent Neural Networks. The system was trained by Indian English speech consisting of 1000 words uttered by 50 speakers. The test samples comprised 500 words spoken by {{a different set of}} 30 speakers. The recognition accuracy is found to be 98 % which is well above the previous results...|$|R
40|$|A new {{approach}} for speaker adaptation consisting of MLLR adaptation enriched {{by a special}} weighting scheme followed by MAP adaptation is presented. While the standard MLLR approach increases the error rate for the considered small amounts of adaptation data in on-line, unsupervised adaptation, our approach can reduce the error by up to 30 %. This result can further be improved by switching to MAP adaptation, yielding a final reduction in error rate of 38. 6 % compared to the <b>speaker</b> <b>independent</b> (SI) <b>system...</b>|$|R
40|$|In this paper, we {{introduced}} a new framework of speech recognizer based on HMM and neural net. Unlike the traditional hybrid system, the neural net {{was used as a}} post processor, which classify the speech data segmented by HMM recognizer. The purpose of this method is to improve the top-choice accuracy of HMM based speech recognition system in our lab. Major issues such as how to use the segmentation information of HMM in neural net, the structure of the neural net, the choice of the error metric for training neural net, and the determination of the training procedure are investigated within a set of experiments. In these experiments, we attempt to recognize 68 phoneme like units in continuous speech. Our results indicate that this is a potential method. About 20 % can be obtained to improve the recognition accuracy for multi-speaker system in syllable level, and 10 % for <b>speaker</b> <b>independent</b> <b>system.</b> I...|$|E
40|$|To build useful {{applications}} based on large vocabulary continuous speech recognition systems, such systems {{have to run}} in real time on common platforms. However, with most research focused on further reducing the recognition error rates, the topic of speed has been neglected {{in the development of}} speech recognition algorithms. I will present a <b>speaker</b> <b>independent</b> <b>system</b> that has been designed for fast speech recognition using vocabularies up to 65, 000 words. Using the approaches presented in this thesis, this recognizer can now run in real time, 200 times faster than the original evaluation system. Important progress was made on the following topics: Tradeoffs: {{a better understanding of the}} tradeoffs between the computational effort and the accuracy of the acoustic modeling provides a foundation to methodically develop faster algorithms. Algorithms: a number of new or improved algorithms were introduced and analyzed in this work, such as: ffl Lookaheads: Lookaheads provide early est [...] ...|$|E
40|$|This paper describes, how {{to perform}} speaker {{adaptation}} for a hybrid large vocabulary speech recognition system. The hybrid system {{is based on a}} Maximum Mutual Information Neural Network (MMINN), which is used as a Vector Quantizer (VQ) for a discrete HMM speech recognizer. The combination of MMINNs and HMMs has shown good performance on several large vocabulary speech recognition tasks like RM and WSJ. This paper now presents two approaches to perform speaker adaptation with this hybrid system. The first approach is a transformation of the feature space, which is performed by a neural network with maximum likelihood (ML) as objective function for the complete system, which means, that the parameters of the NN are estimated in order to match the HMM-parameters of the pre-trained <b>speaker</b> <b>independent</b> <b>system.</b> The second approach is to adapt the HMM parameters depending on the amount of training data available per HMM, using a regularization approach. Both approaches can be applied join [...] ...|$|E
40|$|We have {{proposed}} and evaluated {{a novel approach}} for online speaker adaptation of an acoustic model based on face recognition. Instead of traditionally used audio-based speaker identification we investigated the video modality for the task of speaker detection. A simulated on-line transcription created by a Large-Vocabulary Continuous Speech Recognition (LVCSR) system for online subtitling is evaluated utilizing <b>speaker</b> <b>independent</b> acoustic models, gender dependent models and models of particular speakers. In the experiment, the speaker dependent acoustic models were trained offline, and are switched online based on the decision of a face recognizer, which reducedWord Error Rate (WER) by 12 % relatively compared to <b>speaker</b> <b>independent</b> baseline <b>system...</b>|$|R
40|$|Last year National Electronics and Computer Technology (NECTEC) {{launched}} a speech corpus project {{for building a}} large-vocabulary <b>speaker</b> <b>independent,</b> continuous speechrecognition <b>system.</b> It is a cooperation project between NECTEC and universities with NECTEC as a host center. This paper gives details of the corpus including the sentence selection, the sentence distribution method and interesting statistics of the corpus...|$|R
40|$|We {{describe}} a telephone bandwidth <b>speaker</b> <b>independent</b> name recognition <b>system</b> for Saudi speakers. The acoustic models {{of this system}} are implemented using the Hidden Markov model toolkit (HTK) and developed based on the Saudi accented Arabic voice bank (SAAVB) database. The SAAVB database was designed for the construction and evaluation of <b>speaker</b> <b>independent</b> telephone speech recognition. It consists of 1033 speakers. Each speaker spoke 59 utterances. Our system has shown a word recognition rate of 79 % and a sentence recognition rat...|$|R
40|$|Automatic name dialing is a {{practical}} and interesting application of speech recognition on telephony systems. The IBM name recognition system is a large vocabulary, <b>speaker</b> <b>independent</b> <b>system</b> currently in use for reaching IBM employees in the United States. In this paper, we present some innovative algorithms that improve name recognition accuracy. Unlike transcription tasks, such as the Switchboard task, recognition of names poses {{a variety of different}} problems. Several of these problems arise from the fact that foreign names are hard to pronounce for speakers who are not familiar with the names and that there are no standardized methods for pronouncing proper names. Noise robustness is another very important factor as these calls are typically made in noisy environments, such as from a car, cafeteria, airport, etc. and over different kinds of cellular and land-line telephone channels. We have performed a systematic analysis of the speech recognition errors and tackled the issues separately with techniques ranging from weighted speaker clustering, massive adaptation, rapid and unsupervised adaptation methods to pronunciation modeling methods. We find that the decoding accuracy can be improved significantly (28 % relative) in this manner. 1...|$|E
40|$|We have {{designed}} a Turkish dictation system for Radiology applications. Turkish is an agglutinative language with free word order. These {{characteristics of the}} language result in the vocabulary explosion and {{the complexity of the}} N-gram language models in speech recognition. In order to alleviate this problem, we propose a task-specific, radiology, dictation system. Using words as recognition units, we achieve 87. 06 % recognition performance with a small vocabulary size in a <b>speaker</b> <b>independent</b> <b>system.</b> The most common reason of errors during the recognition is due to the pronunciation variations across speakers, and also due to the inaccurate modeling. In this paper, to overcome these problems, we proposed a pronunciation modeling technique in which variation is modeled at the lexicon level. The pronunciation variants are selected by learning the common mistakes of our speech recognition system. As a proof of the concept, firstly we apply this method to the isolated recognition of small vocabulary radiological words. Our preliminary results show that, 24. 74 % error rate reduction can be achieved for isolated word recognition. This idea can also be generalized to continuous speech recognition problem with a moderate vocabulary size. 1...|$|E
40|$|We {{analyze the}} {{performance}} of continuous speech recognition of a <b>speaker</b> <b>independent</b> <b>system</b> using Hidden Markov Model and Artificial Neural Network. Modern speech recognition systems use different combinations of the standard techniques over the basic approach to improve performance accuracy. One such combination which has gained more attention is the hybrid model. Our hybrid system for continuous speech recognition consists {{of a combination of}} Hidden Markov Model in the front end and the Neural Network with Radial basis function as the back end. The speech recognition process consists of the training phase and the recognition phase. The speech sentences are pre-processed and the features are extracted. The extracted feature vector is clustered into a model database by Hidden Markov Model and is trained by the Radial Basis Function Neural Network. During the recognition phase, the continuous sentence is pre-processed and its feature vector is modelled. This is compared with the database model which contains models stored during the training process. When a match occurs, the model is recognized and the recognition is made for the least error. From the recognized output the word error rate is computed, which is a measure of recognition performance of the hybrid model. The audio files of continuous sentences are taken from the TIMIT database. The performance of our hybrid HMM/RBFNN gives 65 % recognition rate...|$|E
40|$|Speech {{processing}} {{is developed}} {{as one of}} the paramount requisition region of digital signal processing. Different fields for research in speech processing are speech recognition, speaker identification, speech bland, speech coding etc. The objective of <b>Speaker</b> <b>Independent</b> Speech Recognition is to concentrate, describe and distinguish information about speech signal and methodology towards creating the speaker free speech recognition system. Extracted information will be valuable for the directing and working different electronic contraptions and hardware through the human voice proficiently. Feature extraction is the first venture for speech recognition. Numerous algorithms are recommended / created by the scientists for feature extraction. In this work, the cubic-log compression in Mel-Frequency Cepstrum Coefficient (MFCC) feature extraction system is utilized to concentrate the characteristics from speech sign for outlining a <b>speaker</b> <b>independent</b> <b>speaker</b> recognition <b>system.</b> Extracted features are used to train and test this system with the help of Vector Quantization approach...|$|R
40|$|This paper {{describes}} {{the acquisition of}} the multichannel multimodal database AV@CAR for automatic audio-visual speech recognition in cars. Automatic speech recognition (ASR) {{plays an important role}} inside vehicles to keep the driver away from distraction. It is also known that visual information (lip-reading) can improve accuracy in ASR under adverse conditions as those within a car. The corpus described here is intended to provide training and testing material for several classes of audiovisual speech recognizers including isolated word system, word-spotting <b>systems,</b> vocabulary <b>independent</b> <b>systems,</b> and <b>speaker</b> dependent or <b>speaker</b> <b>independent</b> <b>systems</b> {{for a wide range of}} applications. The audio database is composed of seven audio channels including, clean speech (captured using a close talk microphone), noisy speech from several microphones placed on the overhead of the cabin, noise only signal coming from the engine compartment and information about the speed of the car. For the video database, a small video camera sensible to the visible and the near infrared bands is placed on the windscreen and used to capture the face of the driver. This is done under different light conditions both during the day and at night. Additionally, the same individuals are recorded in laboratory, under controlled environment conditions to obtain noise free speech signals, 2 D images and 3 D + texture face models. 1...|$|R
40|$|Abstract—The paper {{describes}} a <b>speaker</b> <b>independent</b> segmentation <b>system</b> for breaking Arabic uttered sentences into its constituent syllables. The {{goal is to}} construct a database of acoustical Arabic syllables as a step towards a syllable-based Arabic speech verification/recognition system. The proposed technique segments the utterances based on maxima extraction from delta function of 1 st MFC coefficient. This method locates syllables boundaries by applying the template matching technique with reference utterances. The system was applied over a data set of 276 utterances to segment them into their 2544 constituent syllables. A segmentation success rate of about 91. 5 % was reached. Keywords—Arabic speech syllables; automatic segmentation; boundaries detection; delta-MFCC features I...|$|R
40|$|The {{objective}} of the work described here is to compare the isolated English language digit speech recognition using Hidden Markov Model for <b>speaker</b> <b>independent</b> <b>system.</b> Two different datasets were collected of audio recordings for the said comparison of isolated digits of English language. Speakers here read numeric digits 0 to 9 i. e. ZERO to NINE. One corpus is self recorded signals and other is standard CUAVE dataset (36 speakers, each uttered 10 words). The training and testing samples are separated for speaker dependent and speaker independent systems. The system has been implemented using the HMM toolkit i. e. HTK by training HMMs of the words making the vocabulary on the training data. Different HMMs for individual digits have been initialized and trained to have well modeled structure. The trained system was tested on training data as well as test data and results shown {{that most of the}} speech samples were correctly recognized. The system was tested for speaker independent and dependent way, to check the changes in the recognition rate. Further this can be used by developers and researchers interested in speech recognition for English language not only for isolated digits but also for other words of English language. If clean database is available, further this can be generalized to recognize words of any language. Continuous speech can also be recognized using study of this system...|$|E
40|$|A vowel {{recognition}} {{based on}} a pitch-synchronous signal processing is introduced in this paper. The investigation has been made within {{the development of a}} <b>speaker</b> <b>independent</b> <b>system</b> of automatic speech sounds identification. The length of a signal-processing window is equal to the pitch period. This makes the signal analysis more independent of a pitch value than in the case of using a fixed-window analysis. It is known that the most effective analysis could be made if a length of the analyzing window is divisible by the pitch period. Thus the smallest window, which provides the perfect effectiveness of the signal analysis, is used here. The patterns for vowels were generated with a help of the knowledge about the phonological system and phonetic rules of the Russian language. Conducted experiments have shown that phonetically-based patterns dictionary is not less effective for speaker-independent speech recognition than the one generated with a help of clustering analysis. The proposed vowels recognition method was tested on the following material: a set of isolated vowels manually extracted from phonetically representative text, read by a standard male speaker of Russian, and a set of isolated words, read by 10 male and 10 female speakers of Russian. Vowels were automatically extracted and then identified within a processing of {{the second part of the}} material. An average recognition accuracy of 85. 0 % was obtained. The achieved results seem to be quite successful. 1...|$|E
40|$|To {{increase}} the recognition rate of <b>speaker</b> <b>independent</b> <b>system</b> {{for a particular}} speaker {{with the use of}} relatively small amount of data, methods called recogniser adaptation to a new speaker are used. Study of these methods, design of a particular method and its corrections for the Czech language are the topics of this thesis. Let us note that the author does not know any work dealing with the recogniser adaptation to a new speaker for the Czech or other Slavic language. Outline of used methods for recogniser adaptation to a new speaker and their categorisation is presented in this thesis. The outline is based on thorough study of articles and paper from corresponding periodicals and world conferences. The task of recogniser adaptation to a new speaker, presented in this thesis, can be characterised as batch, supervised, text independent adaptation with the use of independent adaptation data set. For this purpose the method of maximum a posteriori (MAP) method is proposed. MAP method is based on the Bayesian approach. This method uses models from the SI base system and considers their parameters as a priori knowledge. Then, with the use of adaptation data set it tries to estimate the a posteriori information that is new model parameters, which are then better suited to the voice of a new speaker. Adaptation data set is much smaller than the amount of data needed for training of the base system. As the test results of the MAP method showed, the increase of the recognition rate reached did not match presumption. Analysis showed that the cause of this failure is undertraining of some parameters due to small amount of adaptation data for these parameters. The number of adapted parameters is too high and thus for some of them few or even no adaptation data are available. It is caused by rising number of basic phonetic units (triphones) and by bigger richness of the Czech language. For solution of this problem the correction of the MAP method - the parameter tying method - is proposed. This method uses high correlation between phonetically close parameter. Using this approach it is possible to replace undertrained model by estimates obtained using other parameter and knowledge about their mutual relationship. By implementation of this method better recognition results have been reached. These results can be considered as success and are comparable with the results presented in reference papers. Summary in EnglishAvailable from STL, Prague, CZ / NTK - National Technical LibrarySIGLECZCzech Republi...|$|E
40|$|Abstract. Most {{of current}} speech {{recognition}} systems {{are based on}} Hidden Markov Models assuming that speech features are sequence of stationary stochastic processes. However, there are certain speech attributes, such as background noise type or speaker voice color, {{that do not have}} stochastic character. This fact is often ignored, by designers of robust <b>speaker</b> <b>independent</b> recognition <b>system.</b> In this work, we investigate how the performance of a noisy speech recognition can be improved provided that we have prior knowledge about type and level of noise. Next, recognizer that is using separate models, each trained on a particular type and level of noise, is proposed for more appropriate modeling of speech. ...|$|R
40|$|This paper {{describes}} an improved input coding method for a text-to-phoneme (TTP) {{neural network model}} for <b>speaker</b> <b>independent</b> speech recognition <b>systems.</b> The code-book is self-organizing and is jointly optimized with the TTP model ensuring that the coding is optimal in terms of overall performance. The code-book {{is based on a}} set of single layer neural networks with shared weights. Experiments show that performance is increased com-pared to the NETTalk and NETSpeak models. 1...|$|R
40|$|In {{this paper}} we present an audio driven system capable of videorealistic {{synthesis}} of a speaker uttering novel phrases. The audio input signal requires no phonetic labelling and is <b>speaker</b> <b>independent.</b> The <b>system</b> requires {{only a small}} training set of video and produces fully co-articulated realistic facial synthesis. Natural mouth and face dynamics are learned in training to allow new facial poses, unseen in the training video, to be rendered. To improve specificity and synthesis quality {{the appearance of a}} speaker's mouth and face are modelled separately and combined to produce the final video. To achieve this we have developed a novel approach which utilizes a hierarchical and non-linear PCA model which couples speech and appearance...|$|R
40|$|Despite {{many years}} of {{research}}, Speech Recognition remains an active area of research in Artificial Intelligence. Currently, the most common commercial application of this technology on mobile devices uses a wireless client – server approach to meet the computational and memory demands of the speech recognition process. Unfortunately, such an approach is unlikely to remain viable when fully applied over the approximately 7. 22 Billion mobile phones currently in circulation. In this thesis we present an On – Device Speech recognition system. Such a system {{has the potential to}} completely eliminate the wireless client-server bottleneck. For the Voice Activity Detection part of this work, this thesis presents two novel algorithms used to detect speech activity within an audio signal. The first algorithm is based on the Log Linear Predictive Cepstral Coefficients Residual signal. These LLPCCRS feature vectors were then classified into voice signal and non-voice signal segments using a modified K-means clustering algorithm. This VAD algorithm is shown to provide a better performance as compared to a conventional energy frame analysis based approach. The second algorithm developed is based on the Linear Predictive Cepstral Coefficients. This algorithm uses the frames within the speech signal with the minimum and maximum standard deviation, as candidates for a linear cross correlation against the rest of the frames within the audio signal. The cross correlated frames are then classified using the same modified K-means clustering algorithm. The resulting output provides a cluster for Speech frames and another cluster for Non–speech frames. This novel application of the linear cross correlation technique to linear predictive cepstral coefficients feature vectors provides a fast computation method for use on the mobile platform; as shown by the results presented in this thesis. The Speech recognition part of this thesis presents two novel Neural Network approaches to mobile Speech recognition. Firstly, a recurrent neural networks architecture is developed to accommodate the output of the VAD stage. Specifically, an Echo State Network (ESN) is used for phoneme level recognition. The drawbacks and advantages of this method are explained further within the thesis. Secondly, a dynamic Multi-Layer Perceptron approach is developed. This builds on the drawbacks of the ESN and provides a dynamic way of handling speech signal length variabilities within its architecture. This novel Dynamic Multi-Layer Perceptron uses both the Linear Predictive Cepstral Coefficients (LPC) and the Mel Frequency Cepstral Coefficients (MFCC) as input features. A speaker dependent approach is presented using the Centre for spoken Language and Understanding (CSLU) database. The results show a very distinct behaviour from conventional speech recognition approaches because the LPC shows performance figures very close to the MFCC. A <b>speaker</b> <b>independent</b> <b>system,</b> using the standard TIMIT dataset, is then implemented on the dynamic MLP for further confirmation of this. In this mode of operation the MFCC outperforms the LPC. Finally, all the results, with emphasis on the computation time of both these novel neural network approaches are compared directly to a conventional hidden Markov model on the CSLU and TIMIT standard datasets...|$|E
40|$|This study {{addresses}} {{the lack of}} general and domain-specific text resources for Romanian Automatic Speech Recognition (ASR) systems. To overcome this problem, we propose to use Machine Translated (MT) text and the Web, as language modeling resources. The domain-specific ASR system built is eventually evaluated in terms of word error rate and significant improvements are being reported using MT-text and data extracted from the Web. The paper also describes and evaluates a diacritics restoration system for Romanian, which is mandatory to exploit Web Romanian data that comes generally without diacritics. With the methodology presented here, a decent large vocabulary, <b>speaker</b> <b>independent</b> ASR <b>system</b> for Romanian was obtained {{in a relatively short}} period of time. 1...|$|R
40|$|Abstract. The <b>speaker</b> <b>independent</b> speech {{recognition}} <b>system</b> built by Cedat 85 for the Italian language {{is presented in}} what follows. At the beginning, the system has been tailored and trained {{to be included in}} the parliamentary reporting process, one of the major activity of the company; then further development and adaptation activities allow the use of the system in many different fields, both as different in the addressed lexicon, and applied in different application areas like, as an example, the multimedia data mining...|$|R
40|$|The aim of {{this work}} {{is to improve the}} {{accuracy}} of our spoken broadcast transcription system in the task of Czech parliament speeches recognition. To achieve this goal, we propose several approaches for adaptation of both acoustic and language models of our system: a new two step unsupervised speaker adaptation strategy is presented to improve the former model while the latter one is created from a text corpus mixed properly from both general (2. 6 GB of Czech newspaper texts) and domain specific data (181 MB of parliament speeches). Our experimental results show that the combination of both adaptation approaches leads to near 30 % relative reduction of WER in comparison with the baseline <b>speaker</b> <b>independent</b> (SI) <b>system</b> operating with a general language model. 1...|$|R
40|$|In {{the modern}} world, Internet {{has become a}} popular place, people with speech hearing disabilities and search engines can't take part of speech content in podcast les. In order {{to solve the problem}} partially, the Sphinx decoders such as Sphinx- 3, Sphinx- 4 can be used to {{implement}} a Auto Transcript Generator application, by coupling already existing large acoustic model, language model and a existing dictionary, or by training your own large acoustic model, language model and creating your own dictionary to support continuous <b>speaker</b> <b>independent</b> speech recognition <b>system...</b>|$|R
40|$|Given two <b>independent</b> <b>speaker</b> {{verification}} <b>systems,</b> it {{is reasonable}} to expect some uncorrelated errors. If some trends in this behavior can be detected, one should be able to improve the performance of the combined system beyond that of the best individual system. In this paper, we use a non-linear combiner to combine results from two <b>independent</b> <b>speaker</b> verification <b>systems.</b> Experiments conducted on a subset of the 97 NIST Speaker Evaluation Task, show that the combination improves equal error rate (EER) by up to 9 % when the two systems have comparable performance. When one system outperforms the other, the performance of the combiner {{is similar to that of}} the best system. Experiments using simulated data show that the combination yields large improvement (21 %) in performance when the two systems outperform each other under different operating conditions. R'esum'e Donn'e deux syst`emes ind'ependants de v'erification de locuteur, ils devraient avoir quelques erreurs noncorr 'elative [...] ...|$|R
