6|18|Public
5000|$|Xetec's {{best selling}} product is its line of printer interfaces, {{which allows the}} use of many models of non-Commodore parallel-interface {{printers}} with Commodore computers. Some of the popular printers of that era that are supported include Canon, C-Itoh, Star Micronix, Epson, NEC, Okidata, and Panasonic. Early interface models (such as the [...] "Serial Printer Interface" [...] and [...] "Graphics Printer Interface") simply convert from Commodore's serial format to the more standard Centronics parallel interface, with only minimal ASCII conversions and graphic character printing. The Super Graphix Jr adds support for 50+ printers and [...] "Near Letter Quality", which is a technique of using multi-pass graphic printing to achieve higher quality text printing. The more sophisticated Super Graphix also adds an 8K data buffer, <b>screen</b> <b>dump</b> support, two user-loadable fonts (from a library of fonts on the included disk), and a font creation program. The Super Graphix Gold adds a 32K buffer, 4 fixed and 4 user-loadable fonts (from a library of fonts on disk), 10 font printing effects, picture printing, built-in <b>screen</b> <b>dump</b> programs, fast-serial support, and the rather unusual ability to interface a printer directly to a disk drive (for printing pictures and loading fonts directly from disk).|$|E
40|$|Abstract––In this paper, we {{introduce}} a Authentication Scheme for banking by using implicit password. As we know graphical password scheme suffered from shoulder-surfing and <b>screen</b> <b>dump</b> attacks. As {{we know for}} banking security is is {{the first line of}} defence against compromising confidentiality and integrity. Simply the username and password schemes are easy to implement. But that traditional scheme have been subjected to several attacks. Token and biometric based authentication systems were introduced for alternative to traditional scheme. However, they have not improved substantially to justify the investment. Keywords––security, usability, implicit authentication, behavior modeling, Mobile Banking. I...|$|E
40|$|Abstract — Authentication is an {{important}} process which assures the basic security goals, viz. confidentiality and integrity. Also, adequate authentication is {{the first line of}} defense for protecting any resource. It is important that the same authentication technique may not be used in every scenario. Though traditional login/password based schemes are easy to implement, they have been subjected to several attacks. As an alternative, token and biometric based authentication systems were introduced. However, they have not improved substantially to justify the investment. Thus, a variation to the login/password scheme, viz. graphical scheme was introduced. But it also suffered due to shoulder-surfing and <b>screen</b> <b>dump</b> attacks. We introduce a framework of our proposed (IPAS) Implicit Password Authentication System, which is immune to the common attacks suffered by other authentication schemes...|$|E
5000|$|Super-COMM [...] - [...] Sequential Systems [...] - [...] SSC compatible, {{built in}} term program in ROM, {{supported}} grappler <b>screen</b> <b>dumps</b> and graphics ...|$|R
5000|$|The Game Master (lead=yes) is a cheat {{cartridge}} {{designed for}} Konami's MSX games that {{was released on}} December 12, 1985. It allows player to start at different stages, adjust the number of lives, save the game's progress and high scores, make <b>screen</b> <b>dumps,</b> and play in slow motion, {{as well as other}} uses.|$|R
40|$|This book is {{intended}} for students and teachers of middle and secondary school mathematics. As well as detailed explanatory text, with many <b>screen</b> <b>dumps,</b> the book includes calculator exercises (with answers) and exploratory activities. Covers many aspects of mathematics including functions, graphs, equations, data analysis, simulation, sequences and series, iteration, recursion, matrices, complex numbers, programming and calculus. A chapter on data logging is included. Comprehensive index for quick reference...|$|R
30|$|The {{assessment}} {{began with}} straightforward questions {{based on a}} <b>screen</b> <b>dump</b> from the software for two reasons. The first was to confirm the students {{had picked up the}} basics of interpreting the information in the TinkerPlots format, and the second was to provide easy questions {{at the start of the}} assessment. The coding values for the instrument are given in the Additional file 1 with the questions asked, along with the rubric provided to teachers. Of interest is the total score obtained as it reflects the overall understanding of students in adapting to TinkerPlots as a context for reaching the learning objectives of the measurement activity. Further, the responses to questions 10 to 15, including a request for the definition of variation, throw light on the degree of application of ideas associated with variation in the classroom scenarios, as well as in a related context.|$|E
40|$|The {{design and}} {{implementation}} of a continuous media player for Unix workstations is described. The player can play synchronized digital video and audio read from a file server. The system architecture and results of preliminary performance experiments are presented. 1. Introduction Our goal {{is to develop a}} portable user interface and continuous media support library {{that can be used to}} implement a variety of multimedia applications (e. g., hypermedia systems, video conferencing, multimedia presentation systems, etc.). A key component of these applications is a continuous media (CM) player that can play scripts composed of one or more synchronized data streams. Example data streams are: digitized video or audio, animation sequences, image sequences, and text. The initial application we are implementing to test our abstractions is a video browser that allows a user to play high quality videos stored in a large database on a shared file server. Figure 1 shows a <b>screen</b> <b>dump</b> of the brows [...] ...|$|E
40|$|Wai-Tung Chung. Thesis (M. Phil.) [...] Chinese University of Hong Kong, 1994. Includes bibliographical {{references}} (leaves 102 - 104). Chapter 1. [...] - Introduction [...] - p. 1 Chapter 1. 1 [...] - Central Processing Unit innovationChapter 1. 2 [...] - Long Instruction Word computerChapter 1. 3 [...] - Prior attemptsChapter 2. [...] - The new architecture [...] - p. 11 Chapter 2. 1 [...] - The triple-instruction wordChapter 2. 2 [...] - Functional view of {{the architecture}}Chapter 2. 3 [...] - Inter-functional units synchronizationChapter 2. 4 [...] - Instruction set designChapter 2. 5 [...] - Special featuresChapter 3. [...] - Simulation of the architecture [...] - p. 39 Chapter 3. 1 [...] - Computer architecture simulationChapter 3. 2 [...] - The simulation language used: APLChapter 3. 3 [...] - Simulation environmentChapter 3. 4 [...] - Simulation designChapter 3. 5 [...] - The micro-architectureChapter 3. 6 [...] - Implementation detailsChapter 4. [...] - The supporting environment [...] - p. 53 Chapter 4. 1 [...] - The environmentChapter 4. 2 [...] - The Pseudo-machine configurationChapter 4. 3 [...] - Assembly language descriptionChapter 4. 4 [...] - Details of the utilitiesChapter 5. [...] - Evaluation [...] - p. 53 Chapter 5. 1 [...] - Case StudyChapter 5. 2 [...] - Results and comparisonChapter 5. 3 [...] - SummaryChapter 6. [...] - Discussion and conclusion [...] - p. 96 Chapter 6. 1 [...] - The triple-instruction computerChapter 6. 2 [...] - Use of APL for architectural simulationChapter 6. 3 [...] - Further considerationsChapter 7. [...] - References [...] - p. 81 Chapter 8. [...] - Appendix I: Program listing for the TIC simulatorChapter 9. [...] - Appendix II: <b>Screen</b> <b>dump</b> of the simulation run...|$|E
40|$|This {{submission}} supplements {{the manuscript}} entitled Evolutionary dynamics of mycorrhizal symbiosis in land plant diversification by Frida A. A. Feijen, Rutger A. Vos, Jorinde Nuytinck & Vincent S. F. T. Merckx. The contents of this submission are dating analysis results for rootings {{of the land}} plant topology. Contains the following files: 	*. log. gz BEAST logs 	*. trees. gz BEAST trees 	*. tiff <b>screen</b> <b>dumps</b> of tracer, showing the burn-in 	*. consensus. trees produced with treeannotator For more information: [URL]...|$|R
40|$|This thesis {{describes}} a system, Sysifos, that automates capturing and segmentation of <b>screen</b> <b>dumps</b> of web pages. The system builds {{a model of}} the spatial structure of a page based on the segmentation. The model is used when comparing two <b>screen</b> <b>dumps.</b> The system uses image analysis techniques to segment the page. The model is then compared to a model generated earlier. The model comparison is packaged in a form so that it may be used as a test oracle in standard Java testing frameworks, for example Junit or TestnG. The motivation for the development of Sysifos is that there are currently no established ways to automate testing of browser rendering, although browser related bugs are impor- tant. In this thesis one operation running 56 high volume websites was investigated, and it was found that browser related bugs represented around 13 % of all bugs needing developer attention. Sysifos was evaluated using a test set containing known errors. It found 100 % of the errors it was expected to ﬁnd, but reported one false positive. Test results are visualized using SVG. This thesis show that using a test oracle may be beneﬁcial when testing browser rendering of web pages. Currently, the image capturing service of Sysifos is not satisfactory according to speed and reliability, but the results of the evaluation indicate that Sysifos has the potential to become a valuable tool if the image capturing service can be improved...|$|R
40|$|We {{describe}} {{our experiences}} {{in developing a}} Computer Supported Collaborative Work (CSCW) prototype based only on Microsoft software. We undertook this experiment to gain insight into exactly how the various Microsoft infrastructural components and application packages {{could be used in}} developing collaborative systems. The paper contains a brief overview of the technologies we explored. Based on the knowledge gained during this first phase of the study, we decided upon the functional requirements for the prototype. Sometime during this period our coworkers began calling us "the Borg" after the Star Trek characters. We accepted this and developed our demonstration based on the Borg theme. We describe in detail the implementation of the prototype and show <b>screen</b> <b>dumps</b> of the running system. We make extensive use of the windows-browser integration to create a domain specific environment. A novel aspect of our system is how we provide "presence and awareness" by representing users (Borg) in [...] ...|$|R
40|$|Testing of X Windowapplications {{is often}} done using a low-levelapproach of synthesing X events, {{sending them to}} the application, and testing the result by {{comparing}} <b>screen</b> <b>dumps.</b> This paper describes a system that uses the object structure of Xt widgets to give a higher-levelsystem. It uses the language tcl as the object action description language. 1. Introduction In order to perform regression testing and to give automated demonstrations of X Windowapplications, it is neccessary to have some means of replaying X "input"events. That is, what would normally be user interaction with the application by mouse and keyboard must be simulated by some means. The X WindowSystem supplies a simple means {{to do this in}} that actual X events may be prepared and sent to an application using XSendEvent() [1]. However, this is a rather lowlevelapproach in some instances. It depends on windows being in particular locations and of particular sizes [2, 3]. Since such resources are often under the control [...] ...|$|R
40|$|Computer monitoring, {{capture and}} {{analysis}} of gas/liquid chromatograph traces N. Bulleid and J. Schofield 1 * The work described here is effectively the computerization of a gas/liquid chromatograph of the pen recorder type using a BBC microcomputer. The output from the g. I. c. can be cap-tured by the computer in accordance {{with a series of}} parameters. The sampling rate can be altered, as can an averaging facility to reduce noise, and a threshold level to eliminate the storage of irrelevent and space-consuming data. The unprocessed read-ings are initially stored on the computer's floppy disk, then later retrieved and cut into smaller sections to allow maximum resolu-tion for on-screen analysis. In the main analysis stage of the system all processing is shown graphically on the computer monitor at all stages. The principal steps in analysis involve the mathematical modelling and elimination of the solvent peak, the fixing of the retention time for each subsequent peak and its disentangling from any following peaks, and finally the calculation of the area under each individual peak. Several alternative methods are made available for disentangling peaks, which can be tried successively on a single peak then each printed out with comments for comparison later. All readings and results in table form and <b>screen</b> <b>dumps</b> of all trace images are available via a dot matrix printer...|$|R
40|$|SEDSTAT is an {{interactive}} computer program for processing and presentation of grain size data. The program utilizes graphic and moment methods for {{the calculation of}} grain size statistical parameters. The program has a menu driven format {{that can be easily}} understood by anyone with little, or no computer experience. Data storage and retrieval are the major features of SEDST AT. Another important feature of the program is the built in editor that prompts the user to enter and edit the unprocessed data of weight retained on each sieve and the sieve mesh size in Phi units. The program starts with receiving data from the keyboard or a disk data file; it then calculates the weight percentage, and cumulative weight percentage retained on each sieve. The gaussian (probability) interpolations and extrapolations are then performed to obtain the requisite critical percentiles that are used to calculate Folk and Ward graphical statistical parameters. Grouped moment statistical parameters are also calculated. Another function of the program is to construct two graphs; the first is a histogram of class weight percentage versus size class interval and the second is a probability plot of cumulative weight percentage versus size class interval. Below both of these graphs the parameters of grain size statistics are printed. A hard copy of the graphs may be obtained using a graphic printer or a <b>screen</b> <b>dumping</b> routine...|$|R
40|$|The AKPLOT routine was {{designed}} for engineers and scientists who use graphs {{as an integral part}} of their documentation. AKPLOT allows the user to generate a graph and edit its appearance on a CRT. This graph may undergo many interactive alterations before it is finally <b>screen</b> <b>dumped</b> to a printer for a hard copy plot. The finished AKPLOT graph may be stored in a file for future use. Features available in AKPLOT include: multiple curves on a single plot; combinations of linear and logarithmic scale axes; Lagrange interpolation of selected curves; shrink, expand, zoom, and tilt; ten different symbols and four different colors for curves; and three different grid types. AKPLOT enables the user to perform least squares fitting of all or selected curves with polynomials of up to 99 degrees and examine the least squares coefficients. The user must provide the data points to be plotted by one of two methods: 1) supplying an external file of X-Y values for all curves, or 2) computing the X-Y vectors by either placing BASIC code describing the relation in a designated section of the AKPLOT code or dynamically entering a one line function. Using either technique, the X-Y values are input to the computer only once, as the iterative graph edit loop bypasses the data input step for faster execution. AKPLOT is written in BASIC for interactive execution and has been implemented on an IBM PC series computer operating under DOS. AKPLOT requires a graphics board and a color monitor. This program was originally developed in 1986 and later revised in 1987...|$|R
40|$|Part of the Volume on Youth, Identity, and Digital Media This {{chapter is}} based on two claims, namely that digital media are {{fundamental}} in nurturing human competencies {{for the future and}} that children's leisured media practices are critical catalysts in that process. These claims are documented by results from a recent case study on children's content creation of digital animation. Based on these results, the chapter discusses some of the fundamental challenges posed to educational institutions if they are to nurture future-directed competences for all pupils. These challenges include pupils' understanding of knowledge, their attitudes to learning resources and contexts of use, and the distribution of power relations. Like 300 million other kids around the globe, every Dane under the age of 20 knows that the protagonist of The Little Mermaid is Ariel, a fiesty redhead who manages to shape her fate and fortune. This fact is noteworthy only because Danish author Hans Christian Andersen, the author of the orignal fairy tale, composed a tragic tale of loss and redemption. The narrative and experiential discrepancies raise fundamental questions {{about the ways in which}} global and local media products frame children's everyday culture and the ways in which media operate as identity markers in a variety of sociocultural contexts. Moreover, Disney's figures, like many other media elements, are routinely appropriated by children in their own, increasingly digitized, media productions, from simple drawings to blogs, <b>screen</b> <b>dumps,</b> and home pages. These practices raise important issues about the role played by digital forms of media production for children vis a vis the more conventional and widespread forms of media reception...|$|R
50|$|The regular {{discharge}} {{time for a}} tanker of 8 - 9 m³ is about 15 minutes (or 7-10 minutes to unload a tanker of 4000 liters). The outlet is typically 4 to 6 inch cm in diameter. The {{discharge time}} depends on {{the thickness of the}} sludge, the size of the outlet valve and hose, the amount of garbage in the fecal sludge and how often the driver has to stop to clean the <b>dump</b> <b>screen.</b>|$|R
40|$|During the PEP-II LER {{commissioning}} run in July 1998 {{the beam}} {{profile in the}} tune up dump profile monitor just before LER injection showed an anomalous parabolic shape. The sextupole component {{of the field of}} bend magnets B 2 and B 4 {{in the beginning of the}} south injection tunnel (SIT) was thought to be the cause of this. An off-line model of the B 2 /B 4 bend magnet field was created using DIMAD. Results of particle tracking simulated in DIMAD were compared with on-line lattice diagnostic data and observations of the beam profile on the tune up <b>dump</b> <b>screen...</b>|$|R
50|$|VGACAD was a misnomer, {{and meant}} VGA-Computer Assisted Drawing, rather than {{computer-aided}} design, as CAD is {{commonly referred to}} today. Its longevity was due to its color accuracy, speed, small size, and that its suite of small utilities often worked stand-alone. One called VGACAP, for 'capture', dumped video memory into a file that could later be converted to popular graphic image formats, later made commonplace when Microsoft Windows programmed the print <b>screen</b> key to <b>dump</b> graphics into the clipboard. However, VGACAP ran insulated apart from early versions of Windows, and thus could capture screens were applications prohibited such function.|$|R
3000|$|Four {{bacterial}} strains {{were isolated}} from the <b>screened</b> feather <b>dumping</b> site amongst which B. safensis LAU 13 (GenBank accession No KJ 461434) produced highest keratinolytic activity [16]. It is a rod-shaped, Gram-positive, spore-forming bacterium. The analysis based on 16 S rRNA sequence showed that this strain has 100  % sequence homology with B. safensis CBN- 8 (JQ 353775). Bacillussafensis was first identified in 2006 as a contaminant from spacecraft-assembly facilities (SAF) in USA from which it derived its specific epithet safensis [25]. Bacillus safensis are salt-tolerating plant growth promoting rhizobacteria (PGPR) [26, 27]. The productions of industrially important enzymes like β-galactosidase [28], endoinulinase [29], and lipases [30] by some strains of B. safensis have been reported. In addition, we recently established the first report of the production of keratinase by a strain of B. safensis [16]. However, there is no report on the utilisation of the bacterium for the synthesis of nanoparticles. Therefore, the production of nanoparticles using keratinase of B. safensis adds to the potential relevance of the bacterium for industrial applications.|$|R
50|$|The first {{screenshots}} {{were created}} {{with the first}} interactive computers around 1960. Through the 1980s, computer operating systems did not universally have built-in functionality for capturing screenshots. Sometimes text-only <b>screens</b> could be <b>dumped</b> to a text file, but the result would only capture {{the content of the}} screen, not the appearance, nor were graphics screens preservable this way. Some systems had a BSAVE command {{that could be used to}} capture the area of memory where screen data was stored, but this required access to a BASIC prompt. Systems with composite video output could be connected to a VCR, and entire screencasts preserved this way.|$|R
40|$|HomininSpace is {{an agent}} based {{modelling}} and simulation environment for moving hominin groups {{through a large}} scale geographical landscape. Changing carrying capacity in a reconstructed paleoclimate is the ultimate driving force behind dispersal in HomininSpace. Changing temperatures and precipitation levels influence the carrying capacity of the landscape, and {{are assumed to be}} the most influential parameter in the mobility of ancient hominins in the underlying model. This research combines for the first time an environmental reconstruction driven by the results from isotopic measurements with a year by year demographic model for Neandertal groups moving through North-west Europe. The Neandertals utilize the energy levels from the environment {{in the form of the}} meat from large herbivores. The aim is to assess conceptual models underlying the behavior of Middle Pleistocene hominins in fluctuating climatic conditions, including severe stress-inducing environments. The research contributes to understanding past hominin behaviors regarding mobility strategy, dispersal and occupation history within changing environments. Two major types of behavior driving movement were identified and are implemented in the simulations: a dynamic mobility and a static mobility. Dynamic mobility can be best described as hominins following their preferred habitat. Static mobility is an implementation of the source and sink model, where populations stay in the same area and suffer from local extinction when the climate deteriorates and are replenished from remote source locations when conditions improve. Simulations were run from 131 ky BP to 50 ky BP. For 14. 948 grid cells (148 x 101) in each of 81. 000 timesteps climatic parameters are reconstructed, including elevation, temperature (yearly average, warmest and coldest month values) and precipitation levels. From these values a (grid-based) environment is reconstructed through which groups of hominins move, driven by the inferred abundance of large herbivores, representing the energy levels stored in the local environment. For each simulation different parameters can be set through the user interface implementing different models and hypotheses about hominin behavior. Output of the simulation processes include density maps of hominin presence, density maps identifying areas where hominins died and statistical information on hominin groups including sizes, composition, foraging ranges, resource deficiencies, and ages. Simulations can be started, paused and restarted at any point in time. Results can further include a log file with the key characteristics of the simulation, debug information at a desired level, <b>screen</b> <b>dumps</b> in different formats and a playable movie from snapshots at indicated intervals. Movement patterns of the simulated hominins are matched against archaeological dating information on Neandertal material taken from the literature. This data is collected in a comprehensive database which includes site name and GPS location, material dated, date assigned including accuracy and dating method, reference to the literature, and a confidence level. The archaeological data are included as Checkpoints in Space and Time of which 75 individual sites are included. Simulation results are summarized in key values allowing assessment of the level of agreement between model and archaeology on different aspects...|$|R
40|$|This energy {{inventory}} and pinch {{analysis of the}} Preem, Lysekil refinery {{is a part of}} the Preem – Chalmers research cooperation and has been carried out by CIT Industriell Energi AB. The result in this report will be used as a basis for the research work at Chalmers. The aim with the project is to supply the researchers at Chalmers with energy data from the refinery in a form that is suitable for different types of pinch analysis. Furthermore, the aim is to make an analysis to establish the possible energy saving potentials in the refinery at various levels of process integration constraints. To be able to perform a pinch analysis, data for process streams has to be collected. This has been made using material received from Preem. Stream data has been extracted for all streams that have been identified on the process flow diagrams for all units of the refinery. Service areas and tank farm is not included. The stream data extraction is documented in a file. For each stream there is a calculation area with the information gathered to explain the choice of data used as stream data for the individual stream. Calculation of stream load is made by using known data of flow and physical data. If necessary data is not available from the <b>screen</b> <b>dumps,</b> data has been estimated. For the most important data, process engineers at Preem have been involved to give background information and assistance to find the best estimation possible. The refinery has a net heat demand of 409 MW (for the operation case studied) which is supplied by firing fuel gas. Steam is generated in the process by cooling process streams. One part of this steam (167 MW) is used in the process and the remainder(17 MW) is expanded in turbines and used for other purposes. The energy saving potential, i. e. the theoretical savings that are achievable depend on the constraints that are put on the heat exchanging between process streams in the refinery. Three levels have been analysed. A: There are no restrictions on the process streams that may be heat exchanged in the refinery. In this case the minimum heat demand is 199 MW giving a theoretical savings potential of 210 MW. B: All streams within each process unit can be exchanged with each other, but heat exchange between process units is not permitted. In this case the minimum heat demand of each process unit must be calculated. Some of the identified pinch violations are impossible to eliminate, due to process constraints, and the minimum heat demand is thus corrected to reflect this. The total savings potential, 140 MW, is calculated by adding the savings potential for the separate units. However only a part II of the steam generated above the pinch can be eliminated since it is used for heating purposes in other process units. Only the steam surplus can be considered a savings potential and the total potential is reduced to 117 MW. C: Heat exchange between process units is allowed for those streams which are heat exchanged with utility today (e. g., steam, air, cooling water). The heat exchange takes place with the aid of one or more utility system. However, it is not allowed to modify existing process to process heat exchangers to improve heat exchange between process units. The scope of the analysis is limited by only looking at the 5 largest process units. This group of units are using ~ 90 %, 363 MW, of the added external heat. If heat from the flue gases is recovered at a higher temperature it is possible to reduce the external heat demand with 26 MW to 337 MW...|$|R

