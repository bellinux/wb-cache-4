12|1093|Public
5000|$|JSR - Jump Subroutine - <b>Store</b> <b>contents</b> {{of program}} counter in stack, then jump to new address.|$|E
5000|$|Mainichi Store (まいにちいっしょSTORE): the Mainichi Store {{contains}} various {{content for}} purchase, items include several cosplay suits for Toro. A certain sum of Yen in the wallet and a proper [...] "license" [...] {{is needed to}} view and buy the Mainichi <b>Store</b> <b>contents.</b>|$|E
50|$|A DRM tool {{called the}} PlayStation Network Downloader is also {{required}} to download content from the PlayStation Store onto a PC. When downloading store content, an *.xpd file is first downloaded onto the user's computer and then opened by the PlayStation Network Downloader, which downloads the content {{directly to a}} PSP connected via a USB cable. This 'DRM' tool prevents the user from directly saving <b>store</b> <b>contents</b> onto their PC and illegally distributing the content. Currently, this software is also only compatible with certain Microsoft Windows operating systems.|$|E
50|$|A CMIS server <b>stores</b> <b>content,</b> {{and offers}} access via the CMIS protocol. Some servers also allow access via other protocols.|$|R
25|$|Frequently used <b>content</b> can be <b>stored</b> in <b>Content</b> <b>Store</b> {{for quick}} access.|$|R
5000|$|Collapsible panel - a {{panel that}} can compactly <b>store</b> <b>content</b> which is hidden or {{revealed}} by clicking the tab of the widget ...|$|R
5000|$|Based on his {{participant}} observation field work (he was {{employed as a}} physical therapist's assistant under {{a grant from the}} National Institute of Mental Health at a mental institution in Washington, D.C.), Goffman details his theory of the [...] "total institution" [...] (principally in the example he gives, as {{the title of the book}} indicates, mental institutions) and the process by which it takes efforts to maintain predictable and regular behavior on the part of both [...] "guard" [...] and [...] "captor," [...] suggesting that many of the features of such institutions serve the ritual function of ensuring that both classes of people know their function and social role, in other words of [...] "institutionalising" [...] them. Goffman concludes that adjusting the inmates to their role has at least as much importance as [...] "curing" [...] them. In the essay [...] "Notes on the Tinkering Trades," [...] Goffman concluded that the [...] "medicalization" [...] of mental illness and the various treatment modalities are offshoots of the 19th century and the Industrial Revolution and that the so-called [...] "medical model" [...] for treating patients was a variation on the way trades- and craftsmen of the late 19th century repaired clocks and other mechanical objects: in the confines of a shop or <b>store,</b> <b>contents</b> and routine of which remained a mystery to the customer.|$|E
40|$|After {{discussing}} the techniques employed in alpha-numeric V. D. U. s, and {{the style of}} storage used in map displays, the thesis goes on to describe & screen addressing system. This addressing system allows the display of detailed and varied designs on a raster-scan display, without recourse to the large stores required to implement the map technique. The addressing technique is used in a simple V. D. U, which, in order to minimise costs, uses a domestic television monitor as an output device. The display refresh store {{is made up of}} M. O. S. shift registers, and the <b>store</b> <b>contents</b> are organised under micro processor control. Though the unit described is primarily designed for the display of circuit diagrams it is easily adapted to the display {{of a wide range of}} graphic formats...|$|E
40|$|This paper {{studies the}} {{decentralized}} coded caching for a Fog Radio Access Network (F-RAN), whereby two edge-nodes (ENs) {{connected to a}} cloud server via fronthaul links with limited capacity are serving the requests of $K_r$ users. We consider all ENs and users are equipped with caches. A decentralized content placement is proposed to independently <b>store</b> <b>contents</b> at each network node during the off-peak hours. After that, we design a coded delivery scheme in order to deliver the user demands during the peak-hours under the objective of minimizing the normalized delivery time (NDT), which refers to the worst case delivery latency. An information-theoretic lower bound on the minimum NDT is derived for arbitrary number of ENs and users. We evaluate numerically {{the performance of the}} decentralized scheme. Additionally, we prove the approximate optimality of the decentralized scheme for a special case when the caches are only available at the ENs. Comment: To appear in IEEE ISIT 201...|$|E
5000|$|... #Caption: The Roku XD/S {{digital media}} players works with popular {{streaming}} media sites like Amazon.com and Netflix {{as well as}} locally <b>stored</b> <b>content</b> ...|$|R
25|$|Drupal runs on any {{computing}} {{platform that}} supports both a Web server capable of running PHP and a database to <b>store</b> <b>content</b> and configuration.|$|R
50|$|Some {{cross-platform}} <b>store</b> <b>content</b> such as emulated PlayStation {{games and}} demos {{can be transferred}} from the PlayStation 3 to the PlayStation Portable's Memory Stick via a USB connection.|$|R
40|$|Human colonic {{muscle tone}} varies diurnally and postprandially in {{predictable}} ways. Increased tone reduces {{the capacity of}} the colon to <b>store</b> <b>contents</b> after a meal, whereas increased distensibility (lesser tone) during sleep enlarges the storage capabilities and may slow transit. We tested the hypothesis that antidiarrhoeal drugs would also alter tone which, in turn, might reduce diarrhoea by facilitating the storage and salvage of fluids. Using a colonic barostat to create low pressure, isobaric colonic distension in healthy volunteers, we found that intravenous atropine (0. 01 mg/kg) relaxed the colon during fasting, reduced the postprandial increase in tone, and enhanced relaxation in the late (1 - 2 hour) postprandial period. Intravenous morphine (0. 1 mg/kg) caused variable effects soon after injection but, in fasting subjects, the descending colon relaxed 70 - 90 minutes after morphine. These changes in colonic motility were not always obvious by conventional manometric recording. Colonic distensibility is increased by antidiarrhoeal drugs and this effect may contribute to their efficacy in slowing colonic transit and augmenting absorption...|$|E
40|$|In today’s era of {{information}} technology, human computer interaction and interface design are critical factors {{for a successful}} retail business. In a virtual world, the purpose of store layout {{is to create an}} environment that fascinates customers, entices them to spend more time in the store and encourages them to purchase from the store. The accomplishment of a successful retail business relies on a quick response and the adoption of new technologies that simplify the customer’s shopping experience. Virtual stores {{play a vital role in}} enhancing the success of a retail business. Nowadays, digital signage is widely used to publicize different <b>store</b> <b>contents.</b> The aim of this paper is to select a distinct layout that can be used in digital signage. To enhance Customer Relationship Management (CRM), digital signage can be installed in various contexts, such as metro stations, shopping centers, airplane terminals, and so on. The proposed system scans the Quick Response (QR) code of a product, and the product then arrives at the user end. The tree, pipeline and guiding pathway layouts are employed in the context of digital signage. A questionnaire is used to determine customers’ opinions on the most efficient layout. The statistical results show that layout significantly affects customer’s behavior. Moreover, human computer interaction can help understand more about the customer’s interaction with digital signage...|$|E
40|$|Project Specification The {{objective}} of this project is to investigate {{the possibility of using}} persistent memory to store the EOS namespace. To this end, a hashtable amenable to transactional use is necessary. The hashtable needs to be benchmarked and integrated with the EOS source code. Furthermore, the hashtable should be validated on persistent memory. Abstract EOS[1] provides fast and reliable disk-only storage for data produced by the LHC experiments. In order to ensure fast access, EOS keeps a representation of the namespace in RAM along with a change-log file on disk. As a consequence, restoring the namespace from disk after a restart or a crash can take a relatively long time. Migration of the namespace to persistent memory (non-volatile RAM or NVRAM) will ensure persistence of data even {{in the event of a}} sudden power cut, as well as a considerable improvement on boot-up time. However, this migration requires that changes to the memory are done in a transactional fashion; in the event of a crash, we would like to recover with a consistent view of the namespace. We present a hashtable that can be used to <b>store</b> <b>contents</b> persistently and transactionally, for use with the Mnemosyne[2] toolchain. To benchmark and validate this hashtable, an extensible benchmarking tool was written. We show that, outside Mnemosyne, our hashtable has a performance similar to the implementation currently in use. We furthermore integrate our own hashtable into the EOS code base...|$|E
5000|$|Digital Media Server (DMS): <b>store</b> <b>content</b> {{and make}} it {{available}} to networked digital media players (DMP) and digital media renderers (DMR). Examples include PCs and network-attached storage (NAS) devices.|$|R
30|$|XMLDB: A {{database}} in XML {{format that}} holds all information {{needed by the}} following modules executed on smartphones. The selection of XML for <b>storing</b> <b>content</b> ensures content integrity and portability.|$|R
30|$|Information layer: The OpenACS system [71] {{provides}} a generic content repository infrastructure, {{which is used}} by practically all applications (forums, news, wiki, et cetera) to <b>store</b> <b>content</b> items (informational artifacts).|$|R
40|$|The Development Environment (DE) {{developed}} {{in this research}} includes a methodology and specification and implementation of a tool environment for Management Information Systems (IS) development. In the DE proposed, business IS development includes, and indeed hinges on, organizational modeling. Specifically, the objective and strategy, task, and agent structures are modeled and analyzed. This initial analysis uncovers incompletenesses and inconsistencies in the organizational model as well as allowing top-down prioritization of business areas for further IS development. For information required to support decision making, various information attributes [...] e. g., currency [...] are used in modeling the information requirements. These attributes represent variables of information benefit levels to the user and of cost factors in IS development and operations. In later stages of development, these attributes serve as inference parameters that dictate system aspects such as communication architectures, database design, and process scheduling. Information outputs are decomposed to form an information processing architecture [...] an architecture comprised of interlinked data and processes [...] that minimizes the redundancy of IS resources. A specification of computer-aided tools for this and all steps in IS development are given. Methods and tools are developed for the determination of data <b>store</b> <b>contents</b> and physical structure, information processing requirements, system input requirements, data/process distribution, and data acquisition, disposal, and information processing scheduling. The DE developed {{as a part of}} the dissertation research attempts to draw together and extend upon many notions and methods of system development, decision support mechanisms including artificial intelligence based systems, value of information, and organizational planning and modeling, to form an integrated system development environment...|$|E
40|$|The {{mammalian}} homologues of the Drosophila transient receptor potential (TRP) {{represent a}} superfamily of ion channels involved in Ca(2 +) homeostasis. Several {{members of this}} family are activated either by a depletion of the internal stores of Ca(2 +) or by stimulation of G protein-coupled receptors. In androgen responsive prostate cancer cell line LNCaP, TRPC 1, TRPC 4 and/or TRPV 6 {{have been reported to}} function as store-operated channels (SOCs) while TRPC 3 might be involved in the response to agonist stimulation, possibly through the induction of diacylglycerol production by phospholipase C. However, the control of expression of these TRP proteins is largely unknown. In the present study, we have investigated if the expression of the TRP proteins possibly involved in the capacitative influx of calcium is influenced by the contents of Ca(2 +) in the endoplasmic reticulum. Using real-time PCR and Western blot techniques, we show that the expression of TRPC 1, TRPC 3 and TRPV 6 proteins increases after a prolonged (24 - 48 h) depletion of the stores with thapsigargin. The upregulation of TRPC 1 and TRPC 3 depends on the <b>store</b> <b>contents</b> level and involves the activation of the Ca(2 +) /calmodulin/calcineurin/NFAT pathway. Functionally, cells overexpressing TRPC 1, TRPC 3 and TRPV 6 channels after a prolonged depletion of the stores showed an increased [Ca(2 +) ](i) response to alpha-adrenergic stimulation. However, the store-operated entry of calcium was unchanged. The isolated overexpression of TRPV 6 (without overexpression of TRPC 1 and TRPC 3) did not produce this increased response to agonists, therefore suggesting that TRPC 1 and/or TRPC 3 proteins are responsible for the response to alpha-adrenergic stimulation but that TRPC 1, TPRC 3 and TRPV 6 proteins, expressed alone or concomitantly, are not sufficient for SOC formation...|$|E
40|$|Le but de cet article est de présenter une méthodologie de mise à jour des paramètres de modèles pluie-débit en période de crue. Elle a été mise au point afin d'améliorer un des aspects de la gestion des réservoirs dans un contexte opérationnel de {{protection}} contre les crues: la réduction des incertitudes sur la prévision des débits. L'originalité de la méthode proposée réside dans le fait que l'on utilise non seulement une information sur les débits mais aussi une information sur l'humidité du sol. L'objectif de l'étude est d'évaluer l'intérêt de l'introduction de cette information supplémentaire. Pour cela, les données d'humidité du sol sont introduites au sein du modèle par l'intermédiaire d'une relation de passage établie entre l'humidité mesurée in situ et l'humidité calculée implicitement ou explicitement par les modèles. Cette méthodologie a été testée dans le cadre du projet européen AIMWATER sur quatre sous-bassins de la Seine {{en amont}} de Paris (France). Deux modèles pluie-débit sont utilisés dans cette étude, un modèle conceptuel semi-emprique et un modèle conceptuel couplé à un schéma de surface simulant une interface sol-végétation-atmosphère et permettant de calculer l'évolution de l'humidité du sol à différentes profondeurs. Cette approche comparative étudie l'intérêt d'un tel modèle couplé par rapport au modèle conceptuel semi-empirique sans représentation explicite des phénomènes se produisant à l'interface sol-végétation-atmosphère. Improving {{the accuracy of}} rainfall-runoff models and in particular their performances in flood prediction is a key point of continental hydrology. Methods {{have been developed to}} improve flood prediction in hydrology based on a better compliance of the model with current observations prior to its use in forecasting mode. This operation has been termed updating in hydrology and assimilation in meteorology. The fundamental idea is that if model predictions diverge from observations at a given time, there is little chance that future estimations will approach correct values. The improvement then comes from a correction of the trajectory of the model based on observations during the period preceding the day when a prediction into the immediate or long-term future is desired. This can be dealt with by a correction of model parameters, which is usually called "parameter updating". The inability of rainfall-runoff models to produce correct streamflow values generally translates into parameter uncertainty. Parameter calibration is the means used by a model structure to adjust to a given set of data. Therefore, a parameter updating methodology seems to be a natural way to amend errors in streamflow values. In this paper, a specific methodology of parameter updating is presented. The main feature of this method {{is that it does not}} carry out updating by reference only to recent streamflow observations, as classic procedures do, but also to soil moisture measurements, which can be retrieved daily from TDR probes. Indeed, it appears that the integration of soil moisture data allows better control of the evolution of the model and improves its performances, in particular in terms of forecasting. The aim of the research was to assess the usefulness of this additional soil moisture information. To this end, an approach has been suggested that gradually introduces additional information thanks to a constraint relationship between observed and modelled soil moisture. In fact, soil moisture can be calculated implicitly or explicitly by the model when extracting step-by-step the values of the model's <b>store</b> <b>contents.</b> This methodology was put forward for use in the European AIMWATER project on four catchments within the Seine River basin upstream of Paris (France). The other issue addressed in this paper was whether or not it is necessary to use a model that simulates explicitly the evolution of soil moisture at different depths. One can argue that if the model employed does not feature a store that can be identified closely to the observed soil moisture, there would be no possibility of benefiting from such measurements. On the other hand, it can be argued that if soil moisture is a model output, all the information drawn from soil moisture observations will be directed at improving this specific output at the expense of improving streamflow values. To answer this issue, two models were tested. The first model, GR 4 j, has no explicit counterpart for soil moisture measurements. The second one, GRHum, has been especially developed to introduce a two-layer soil reservoir that simulates the surface and sub-surface soil moisture. Since the aim of the present research was to analyse different ways of accounting for soil moisture, and to identify the one that offers the best prospects, several tests were carried out, using different relationships between observed and modelled soil moisture. Indeed, TDR probes give point measurements of soil moisture at several depths and several <b>store</b> <b>contents</b> can be taken into account in a constraint relationship. First, for both GR 4 j and GRHum models, tests showed that performances for flood forecasting are significantly improved when assimilating in situ measurements of soil moisture at a daily time-step, especially for the basins where poor simulations are obtained. It is also noteworthy that performances are very dependent on the items taken into account in a constraint relationship. Secondly, the GRHum model did not appear to be more efficient than the GR 4 j model when assimilating both streamflow and soil moisture data. However, the GRHum model gave the best results when assimilating only streamflow data, and superficial soil moisture seemed to fit the GRHum better than the GR 4 j model. Finally, although the tests required perfect foreknowledge of rainfall, the results of the research are encouraging from an operational point of view. Another interesting perspective is provided by the Earth Observation data. Indeed, previous studies have shown that soil moisture can be derived from EO data using, for example, microwave spaceborne Synthetic Aperture Radar (SAR) images (QUESNEY et al., 2000). This type of catchment-scale data could be more relevant than a local measure given by TDR probes (PAUWELS et al., 2002) ...|$|E
50|$|Distributed file systems {{differ in}} their performance, {{mutability}} of content, handling of concurrent writes, handling of permanent or temporary loss of nodes or storage, and their policy of <b>storing</b> <b>content.</b>|$|R
50|$|Forgetting {{can have}} very {{different}} causes than simply removal of <b>stored</b> <b>content.</b> Forgetting can mean access problems, availability problems, or can have other reasons such as amnesia caused by an accident.|$|R
5000|$|Mobile Digital Media Server (M-DMS): <b>store</b> <b>content</b> {{and make}} it {{available}} to wired/wireless networked mobile digital media players (M-DMP), and digital media renderers. Examples include mobile phones and portable music players.|$|R
50|$|Some {{proposal}} management software store {{documents in}} their native file format (Microsoft Word or Excel, PDF) while other web-based proposal management software are beginning to <b>store</b> <b>content</b> {{in the form of}} html.|$|R
50|$|Tiki can run on any {{computing}} {{platform that}} supports both a web server capable of running PHP 5 (including Apache HTTP Server, IIS, Lighttpd, Hiawatha, Cherokee, and nginx) and a MySQL database to <b>store</b> <b>content</b> and settings.|$|R
40|$|We {{take the}} {{position}} {{that it is time}} to revisit and challenge the dogma that TCP is undesirable for audio and video streaming. We contend that new compression practices and reduced storage costs make TCP a viable and attractive basis for streaming <b>stored</b> <b>content.</b> Our approach has much in common with the recent proliferation of work on TCP-friendly streaming, but using TCP itself poses distinct challenges and provides several advantages. To support our position, this paper describes an architecture for content preparation and delivery intended to demonstrate effective streaming of <b>stored</b> <b>content</b> over TCP. We present preliminary results from implementing a QoS-adaptive video system based on the proposed architecture, and describe our ongoing work on streaming. ...|$|R
30|$|As <b>storing</b> <b>content</b> locally leaves users {{vulnerable}} to device confiscation and search, SecurePost encrypts all data using an application-wide password. In {{the event that}} a user is under duress they can provide a false password that wipes the application data including posting keys.|$|R
30|$|Offer {{services}} for <b>storing</b> audiovisual <b>content.</b>|$|R
50|$|Exponent CMS can be {{installed}} in a Linux, Unix, Mac OS X or Windows environment, or any platform that supports the Apache web server and the PHP language (version 5.3.x+). Exponent CMS currently requires a MySQL (4.1+, 5+ Recommended) database to <b>store</b> <b>content</b> and settings.|$|R
50|$|Announced in 2012, the Redray Player was {{the first}} {{stand-alone}} device capable of providing 4K content to 4K displays. Using a 1TB internal drive to <b>store</b> <b>content,</b> Redray can play 4K, 3D or HD media. Additionally, the player supports 12-bit 4:2:2 precision at 4K resolution.|$|R
50|$|In May 2011, Playboy {{introduced}} i.playboy.com, a complete, uncensored {{version of}} its near 700 issue archive, targeting the Apple iPad. By launching the archive as a web app, Playboy was able to circumvent both Apple's App <b>Store</b> <b>content</b> restrictions and their 30% subscription fee.|$|R
50|$|The {{specification}} {{is aimed}} at the localization industry. It specifies elements and attributes to <b>store</b> <b>content</b> extracted from various original file formats and its corresponding translation. The goal was to abstract the localization skills from the engineering skills related to specific formats such as HTML.|$|R
50|$|The CMS {{framework}} then {{uses the}} ontology {{to create an}} object model that developers can work against. The built-in ORM automatically creates database tables and fields for <b>storing</b> <b>content</b> data. Web sites are created by defining the content, and {{the relationships between the}} different types of content.|$|R
50|$|This {{authoring}} {{approach is}} {{popular in the}} technical publications and documentation arenas, as it is adequate for technical documentation. Tools supporting this approach typically <b>store</b> <b>content</b> in XML document format {{in a way that}} facilitates content reuse, content management, and makes the dynamic assembly of personalized information possible.|$|R
30|$|By <b>storing</b> <b>content</b> like sediment, the {{stored data}} are never {{modified}} and {{any need for}} checking consistency is obviated. Changes of fact are written to the now front, like diary entries recording changes of temperature. Current data which refer to old ones use addresses as core values, implemented as pointers.|$|R
