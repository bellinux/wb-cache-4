0|10000|Public
40|$|This {{data set}} {{contains}} data {{created in the}} CLARIN GrNe project. In this project, we have curated part of the Ancient Greek - Dutch dictionary (during this project under construction at Leiden University) from MS Word to XML and created an online edition (www. woordenboekgrieks. nl). The data set consists of the XML of <b>the</b> <b>lemmata</b> starting with <b>the</b> Greek letter Pi, the source code of the software used to migrate <b>the</b> <b>lemmata</b> from MS Word to XML, the XML Schema, the webserver scripts used for the lemmatizer and configuration files to facilitate working with the XML in oXygen XML Editor...|$|R
5000|$|The Leiden Glossary is a {{glossary}} {{contained in}} a manuscript in Leiden University Library, Voss. Lat. Q. 69. <b>The</b> <b>lemmata</b> come from [...] "a range of biblical, grammatical, and patristic texts". It {{is based on an}} Anglo-Saxon exemplar, and was prepared c. 800 in the Abbey of Saint Gall.|$|R
30|$|<b>The</b> proofs of <b>Lemmata</b> 3.3 and 3.4 can {{be easily}} adapted from <b>the</b> proofs of <b>Lemmata</b> [8], Lemma  3.6] and [8], Lemma  3.5].|$|R
40|$|This {{technical}} report contains the proofs to <b>the</b> <b>lemmata</b> and theorems of [PN 12] {{as well as}} some additional material. As main contributions [PN 12] presents an encoding of mixed choice {{in the context of the}} pi-calculus and a criterion to measure whether the degree of distribution in process networks is preserved...|$|R
30|$|<b>The</b> {{following}} <b>lemmata</b> {{are useful}} {{in order to prove}} Theorem  4.1.|$|R
30|$|The {{proof of}} Proposition (3.1) is {{established}} through <b>the</b> following <b>lemmata.</b>|$|R
40|$|This {{technical}} report supplements the paper Geoadditive Survival Models (Hennerfeind, Brezger and Fahrmeir, 2005, Revised for JASA). In particular, {{we describe the}} simulation study of this paper in greater detail, present additional results for the application, and provide a complete proof of Theorem 1, Corollary 1, as well as <b>the</b> <b>lemmata</b> and corollaries in the appendix. ...|$|R
30|$|For the proofs {{of these}} results, we need <b>the</b> {{following}} <b>lemmata</b> of independent interest.|$|R
50|$|These {{dictionaries}} were alphabetized {{according to}} <b>the</b> Latin <b>lemmata</b> and lacked a German index, for which reason Jacob Grimm did not count them as German dictionaries. The first German dictionary which alphabetized according to <b>the</b> German <b>lemmata</b> was that by Josua Maaler, published in 1561. Later editions of Fries's dictionary did include German index; so Johann Kaspar Suicer, Joannis Frisii Dictionarium Latino-Germanicum Nec Non Germanico-Latinum (1701).|$|R
30|$|The {{proof of}} Theorem  2 {{is carried out}} by a {{suitable}} modification of that of [1], Theorem  6.5] and is based upon <b>the</b> following <b>lemmata.</b>|$|R
3000|$|... [...]. Algorithm 1 {{shows the}} steps of the pick-l {{algorithm}} in detail. <b>The</b> following <b>Lemmata</b> show that <b>the</b> pick-l algorithm provides an upper bound of α [...]...|$|R
40|$|Beinecke papyrus P. CtYBR inv. 5018 {{consists}} {{of a series of}} well-defined entries, each with three elements: (1) an ordinal number (surviving are " 10 th" through " 15 th"); (2) the lemma, no more than a phrase, apparently excerpted from an unknown prose text; (3) literary examples or verbatim quotations, presumably intended to illustrate the content of the lemma. Quoted are a passage from Odyssey 11 and two trimeter lines from an unknown tragedy or tragedies. The contents of the prose text from which <b>the</b> <b>lemmata</b> derive is not clear, but appears to regard poetics or poetic composition...|$|R
5000|$|... (c. 1011 {{possibly}} in Nasa, Khurasan [...] - [...] c. 1075 in Baghdad) was a Persian mathematician from Khurasan, Iran. He flourished under the Buwayhid sultan Majd al-dowleh, {{who died in}} 1029-30AD, and under his successor. He wrote a book on arithmetic in Persian, and then Arabic, entitled the [...] "Satisfying (or Convincing) on Hindu Calculation" [...] (al-muqni fi-l-hisab al Hindi). He also wrote on Archimedes's lemmata and Menelaus's theorem (Kitab al-ishba, or [...] "satiation"), where he made corrections to <b>The</b> <b>Lemmata</b> as translated into Arabic by Thabit ibn Qurra, which was last revised by Nasir al-Din al-Tusi.|$|R
40|$|We prove two lemmata about Schubert {{calculus}} on generalized flag manifolds G/B, and in {{the case}} of the ordinary flag manifold GL_n/B we interpret them combinatorially in terms of descents, and geometrically in terms of missing subspaces. One of them gives a symmetry of Schubert calculus that we christen_descent-cycling_. Computer experiment shows that these lemmata suffice to determine all of GL_n Schubert calculus through n= 5, and 99. 97 %+ at n= 6. We use them to give a quick proof of Monk's rule. <b>The</b> <b>lemmata</b> also hold in equivariant (``double'') Schubert calculus for Kac-Moody groups G. Comment: 10 pages, 2 figures, see also [URL]...|$|R
30|$|The {{proof of}} Theorem 1.1 {{consists}} of <b>the</b> following several <b>Lemmata</b> 4.1 - 4.6.|$|R
3000|$|... {{such that}} a node {{immediately}} adjusts its phase to a neighboring clock every time receiving a firing message from this clock. However, <b>the</b> following <b>lemmata</b> {{shows that there}} exists a basic upper bound which holds for every network.|$|R
50|$|The Third Cleopatra Glossary (folios 92r-117v) {{contains}} glosses to Aldhelm's Prosa de virginitate and Carmen de virginitate, with <b>the</b> <b>lemmata</b> in <b>the</b> {{same order}} {{as they appear}} in the text. It was presumably, therefore, based on a copy of Aldhelm's texts which had interlinear glosses. This glossary or one like it was influential, influencing Byrhtferth of Ramsey {{and at least one}} Anglo-Saxon medical text. Kittlick's linguistic investigation showed that some, at least, of the glosses in the Third Cleopatra Glossary are in the Anglian dialect of Old English, with later overlays from West Saxon and Kentish (probably in that order). The glossary--though not necessarily all its entries--must have originated in the eighth century.|$|R
40|$|AbstractTwo trisections of {{the angle}} were {{transmitted}} from Greek to Islamic geometry, {{one in the}} Arabic translation of <b>the</b> <b>Lemmata</b> of (pseudo-?) Archimedes, {{and the other in}} a hitherto unpublished 9 th-century treatise by Ahmad ibn Musa, which contains a translation from another Greek source. This paper presents an edition of the Arabic text of the latter treatise, as well as an English translation and a commentary, in which the text is compared with Propositions 36 – 42 of Book 4 of the Collection of Pappus of Alexandria (4 th century). Following this is a summary of the work of Thabit ibn Qurra on this trisection and an attempt to explain why some 10 th-century Islamic geometers thought that the ancients had not been successful in trisecting the angle...|$|R
40|$|This paper {{focuses on}} a group of some twenty-five Old English glosses from the Épinal {{glossary}} (c. 700). The Old English interpretations stand out due to one graphemic feature: they all use one of the runic characters wyn or thorn, which were adopted into the Old English alphabet to represent the phonemes /w/ and /θ/. A careful investigation of the sources of <b>the</b> <b>lemmata</b> reveals that, with three exceptions, these glosses stem from Isidore of Seville’s Etymologiae or from Paulus Orosius’ Historiae Adversum Paganos, but not from any of the other numerous sources of the glossary. This confirms the hypothesis that different orthographic systems were in use among the glossators contributing to the Épinal/Erfurt family of glossaries. The study thus uncovers one chapter in the earliest history of English spelling and contributes {{to a better understanding of}} the composition of Épinal/Erfurt...|$|R
40|$|We {{describe}} {{an approach to}} automatically invent/explore new mathematical theories, {{with the goal of}} producing results comparable to those produced by humans, as represented, for example, in the libraries of the Isabelle proof assistant. Our approach is based on ‘schemes’, which are terms in higher-order logic. We show {{that it is possible to}} automate the instantiation process of schemes to generate conjectures and definitions. We also show how the new definitions and <b>the</b> <b>lemmata</b> discovered during <b>the</b> exploration of a theory can be used, not only to help with the proof obligations during the exploration, but also to reduce redundancies inherent in most theory formation systems. We implemented our ideas in an automated tool, called IsaScheme, which employs Knuth-Bendix completion and recent automatic inductive proof tools. We have evaluated our system in a theory of natural numbers and a theory of lists. Keywords: mathematical theory exploration, schemes, theorem proving...|$|R
40|$|In this work, we {{investigate}} possible benefits of {{natural language processing}} tools, as means to support automated text categorization. Our corpus consists of a small collection of categorized Danish web pages {{in the fields of}} art, architecture, and design. The natural language processing techniques we examine are stop word removal, removal of functional words, and lemmatization. The tools are based on a stop word list, a part-of-speech tagger and a dictionary. We evaluate effects on a string matching classifier and a support vector machine. The classification accuracy increases when using <b>the</b> <b>lemmata,</b> either in addition to or replacing the original inflected words in the documents. Positive effects are seen on both precision and recall. In absence lemmatization, the removal of stop words increases classifier performance, although not as much. Results are valid both for support vector machine, and string matching categorization. Acknowledgments First of all, I would like to thank my main supervisor Pierre Nugues, fo...|$|R
40|$|In {{this thesis}} we {{describe}} {{an approach to}} automatically invent/explore new mathematical theories, {{with the goal of}} producing results comparable to those produced by humans, as represented, for example, in the libraries of the Isabelle proof assistant. Our approach is based on ‘schemes’, which are formulae in higher-order logic. We show {{that it is possible to}} automate the instantiation process of schemes to generate conjectures and definitions. We also show how the new definitions and <b>the</b> <b>lemmata</b> discovered during <b>the</b> exploration of a theory can be used, not only to help with the proof obligations during the exploration, but also to reduce redundancies inherent in most theory-formation systems. We exploit associative-commutative (AC) operators using ordered rewriting to avoid AC variations of the same instantiation. We implemented our ideas in an automated tool, called IsaScheme, which employs Knuth-Bendix completion and recent automatic inductive proof tools. We have evaluated our system in a theory of natural numbers and a theory of lists. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|This paper {{describes}} {{the development and}} use of a lexical semantic database for the Verbmobil speech [...] to [...] speech machine translation system. The motivation is to provide a common information source for the distributed development of the semantics, transfer and semantic evaluation modules and to store lexical semantic information application [...] independently. The database is organized around a set of abstract semantic classes and has been used to define the semantic contributions of <b>the</b> <b>lemmata</b> in <b>the</b> vocabulary of the system, to automatically create semantic lexica and to check the correctness of the semantic representations built up. The semantic classes are modelled using an inheritance hierarchy. The database is implemented using the lexicon formalism L E X 4 developed during the project. 1 Introduction The distributed development of the modules of a large natural language processing system at different sites makes interface definitions a vital issue. It becomes even more urgent when [...] ...|$|R
40|$|Aldred, the glossator of the Lindisfarne Gospels, {{presents}} {{himself as}} carefully rendering <b>the</b> Latin <b>lemmata</b> {{in front of}} him, {{in terms of both}} their internal structure and meaning. His work includes a very high number of multiple glosses, which often attempt to clarify the polysemous character of a lemma or to provide additional information. This paper explores the multiple glosses including different lexemes which Aldred added to lexical lemmata in Mark’s Gospel in an attempt to establish whether there is any correlation between Aldred’s ordering practices and the frequency with which he used the interpretamenta to render those <b>lemmata.</b> <b>The</b> results of the study show some preference for placing the interpretamentum which most commonly renders the Latin lemma in first position, although Aldred’s practice is not fully consistent...|$|R
40|$|Fully fledged {{automatic}} nominal lemmatizer for Portuguese. It {{takes as}} input: * A Portuguese nominal form A forms of a noun or an adjective, including adjectival forms of past participles, and * Inflectional feature values Intended values of inflectional features of Gender and Number for the output. It delivers: * Inflectional features The input form is {{returned with the}} corresponding values for the inflectional features of Gender and Number associated to it; * <b>Lemmata</b> <b>The</b> <b>lemmata</b> (singular and masculine forms when available) possibly corresponding to the input form; * Inflected forms The inflected forms (when available) of each lemmata {{in accordance with the}} values for inflectional features entered. LX-Inflector processes simple forms, both lexically known and unknown ones. It also processes compound forms. It handles nominal forms with prefixes as well. In sum, it lemmatizes and inflects: * Prefixed forms Nominal expressions integrating one or more prefixes, e. g. "anti-constitucional", "super-mega-fixe", etc; * Compounds Nominal expressions integrating more than one form, e. g. "trabalhador-estudante", "surdo-mudo", "lança-mísseis", etc; * Neologism...|$|R
40|$|In {{spite of}} the long-standing debate {{on the value of}} epigraphic data, {{especially}} in the studies aiming at reconstructing the sociolinguistic framework of Latin, scholars still disagree on the value to be assigned to abnormal (i. e. non classical) spellings occurring in inscriptions. Are they clues suggesting pronunciations re- flecting the social class of the reader/speaker? Are they simple mistakes in writ ing? Are they a sign of the archaizing style typical of the epigraphic register? The paper focuses on the graphemic alternations ˂I˃ ˂E˃ and ˂U˃ ˂O˃ occurring within CLaSSES I, a corpus of inscriptions of the Archaic and Early periods of the Latin language. The distribution of vowel alternations in spelling is not casual, but rather suggests a plausible correspondence in phonological variation. The fine-grained comparison carried out on <b>the</b> <b>lemmata</b> occurring in CLaSSES I reveals a quite complex orthographic picture, where vowel alternations cannot be as- cribable to archaism pure and simple but rather may be interpreted as evidence for a sociophonetic process sensitive to both lexical and prosodic constraints...|$|R
40|$|This study {{looks at}} a number of {{problems}} related to Greek ordering. It aims to derive the standard praxis from a representative samples culled from Greek dictionaries. To make comparison easier we have concentrated on a number of terms that bring {{one or more of the}} relevant problems into focus. An outline of the historical background of Greek lexicography will be added in an enlarged version of this study. − Ordering of diacritical marks. Directionality on level 2 − Word-by-word versus letter-by-letter: What is preferred? − Uppercase vs. lowercase on level 3. Clear rules? − Treatment of the final sigma ς in relation to the normal (closed) σ <b>The</b> <b>lemmata</b> (dictionary entries) selected for this exercise and the respective sorting issues are: − ασσα vs. ασσα and αττα vs. αττα 1 – treatment of breathing and accent mark combinations − Αιδης / αδης, 2 αδω and ραδιος – treatment of iota adscriptum and/or iota subscriptum in combination with upper-/lowercase distinction 3 − η, η, η, η, ηα, ηαται 4 – treatment of breathing and accent marks and iota subscriptum − ειµι vs. ειµι – treatment of breathing and accent mark...|$|R
30|$|This {{article is}} {{structured}} as follows. In Section 2, a triangularly divided area is introduced. This triangular double integral {{is defined by}} single limit of double sums of triangularly divided areas. In Section 3, the combination and <b>the</b> transformation <b>lemmata</b> are derived. In Section 4, the curl theorem on the 2 D plane is derived by substituting the total differentials in the transformation lemma. In Section 5, the curl theorems of a triangular integral in the 3 D space and the 4 D hyper-space are presented.|$|R
40|$|This thesis gives {{a survey}} of pumping lemmata from a linguist’s {{point of view and}} how they can be applied to natural languages. The survey {{includes}} definitions and proofs of pumping lemmata for regular lan-guages, context-free languages, tree adjoining languages, and multiple context-free languages. Also, the grammars that generate these classes of languages are briefly described. Using <b>the</b> pumping <b>lemmata,</b> a number of formal languages are classified. This systematization is used to describe the classifications of natural languages that have been argued by Chomsky (1957), Bar...|$|R
40|$|The {{invariant}} subspaces of the Hardy {{space on}} H^ 2 (D) {{of the unit}} disc are very well known however in several variables <b>the</b> <b>structure</b> of <b>the</b> invariant subspaces of the classical Hardy spaces is not yet fully understood. In this study we examine the invariant subspace problem for Poletsky-Stessin Hardy spaces which is a natural generalization of the classical Hardy spaces to hyperconvex domains in C^n. We showed that not all invariant subspaces of H^ 2 _ũ(D^ 2) are of Beurling-type. To characterize the Beurling-type invariant subspaces of this space we first generalized the Lax-Halmos theorem of vector valued Hardy spaces to the vector valued Poletsky-Stessin Hardy spaces and then we give a necessary and sufficient condition for the invariant subspaces of H^ 2 _ũ(D^ 2) to be of Beurling-type. Comment: One of <b>the</b> key <b>lemmata</b> referred to a different author in the previous version had some misleading points so in this new version it is proved differently by the authors for a much more clear representation of the main theore...|$|R
40|$|The Shortest Watchman Route Problem, namely ’Given a polygon {{find the}} {{shortest}} route in the polygon with the property that each {{point in the}} polygon is visible from at least one point along the route’, was solved in 1991. In order to model real-life situations, for example as encountered by a scout that must efficiently explore an unknown terrain, one must consider the online version of the problem. Exploration {{has been one of}} the major areas of research in Robotics but theoretical results on tractability, efficiency, and correctness of exploration algorithms are relatively recent. In this report, we will review the approaches to explore an unknown environment. We will first present some essential concepts and review <b>the</b> <b>lemmata</b> that have been used in all exploration algorithms. Then, we will describe the solution to the Watchman Route Problem, since this algorithm forms the basis of all successive exploration algorithms. The first solution for the unknown environments was for the rectilinear case. We also consider the situation faced by a robot with a finite line of sight, which also has to return to the starting point every so often to refuel. The existence of a competitive algorithm for a rectilinear environment with an arbitrary number of polygons can be constructively refuted. Finally, we present the algorithm for a simple polygon with no obstacles and we conclude by discussing the open problems and future areas of research. ...|$|R
40|$|Vance Ramsey, in his {{extended}} defense of Manly and Rickert's The Text of the Canterbury Tales, makes the following assertion: "Contrary to the absolutists, no intelligent person can doubt that learning, taste and judgment must be exercised by any editor {{worthy of the}} name, {{but they should be}} the very last resort of an editor and not the first. " I doubt whether anyone with experience of editing Middle English texts (as Ramsey had not) would formulate the problems of editorial intervention in such a categorical way. Nor is it clear what "absolutists" (whatever the term is intended to mean) he has in mind. And if "learning, taste and judgment" are to be the "last" resort of the editor, what is to be the first? It is worth applying this last question to the editorial activities of Manly and Rickert, that is, to those portions of their work specifically concerned with presenting a text of the Canterbury Tales, volumes 3 and 4. These volumes have important practical implications for their larger undertaking. One might reasonably assume that the very first procedural matter that any editor would wish to establish would be the choice of the base text, which will provide <b>the</b> <b>lemmata</b> for <b>the</b> edition, that is, the forms against which other witnesses will be collated. Normally such a base text would also provide, in general terms, the orthographic forms for the text, what are now termed its "accidentals. ...|$|R
40|$|The {{function}} {{theory of}} lexicography argues that specialised lexicographical products must help learners to transform their information needs into aspects {{of knowledge of}} the discipline, and of its discursive properties. Lexicographers, then, must combine information and data access with the user’s need for information and knowledge. To achieve this aim they need to devise theories providing solutions to different lexicographical problems. One such theory has recently been proposed by Tarp (2008), who claims that there are four categories which are central to a general theory of learner’s lexicography: users, user situation, user needs, and dictionary assistance. This paper focuses on dictionary assistance and addresses several lexicographical issues connected with polysemy: the selection of <b>the</b> <b>lemmata</b> of some printed English-Spanish/ Spanish-English business dictionaries, their entry structures, sense differentiation, and sense ordering. The analysis leads the author to discuss some proposals {{with the aim of}} making business dictionaries more pedagogically oriented, and to include a set of principles pedagogically-oriented business dictionaries must have. They are illustrated in a model entry which has been compiled by rearranging one of the entries studied according to the proposals and principles previously discussed. 1. The Function Theory of Lexicography Over the course of a number of years Bergenholtz/Tarp (2003, 2004) have defended a transformative view of lexicography, and have pre-sented lexicography as an area of social practice and independent sci-ence concerned with analysing and building dictionaries which can sat-isfy the needs of a specifi c type of user with specifi c types of problems related to a specifi c type of user situation (see Tarp 2008, for a review) ...|$|R
40|$|The article {{considers}} the contextual relationships and the semantic variation of 5 <b>lemmata</b> belonging to <b>the</b> semantic field of 'power'. It aims {{to demonstrate how}} their use in a diachronic corpus of U. S. Presidential speeches testifies {{to the development of}} the relationships between the executive, legislative and judicial branches of government. Furthermore, it shows how this is to relate to the emergence of a new conception of a new conception of the Presidency, one with a specific rhetorical focus and an attention to the persuasive effects of its communication strategies. The use of <b>the</b> 5 <b>lemmata</b> shows how, during the 20 th century, two strategies for the contruction of consensus around the Presidency have developed...|$|R
40|$|The {{manuscript}} Cambridge, Trinity College, 0. 1. 55 is one on {{the rare}} witnesses to the direct textual tradition of the scholia on Proverbs by Evagrius of Pontus and contains the first twenty-eight scholia. These must have been copied in the fourteenth century from a rather old model, which had itself been modified by <b>the</b> insertion of <b>lemmata</b> from a text of the Hexapla accompanied by kephalaia {{and a variety of}} additional glosses. The manuscript Cambridge, Trinity College, 0. 1. 55 is {{one on the}} rare witnesses to the direct textual tradition of the scholia on Proverbs by Evagrius of Pontus and contains the first twenty-eight scholia. These must have been copied in the fourteenth century from a rather old model, which had itself been modified by <b>the</b> insertion of <b>lemmata</b> from a text of the Hexapla accompanied by kephalaia and a variety of additional glosses. Bady Guillaume, Tchernetska Natalie. Un nouveau témoin direct des Scholies aux Proverbes d'Évagre le Pontique (Cambridge, Trinity Coll. O. 1. 55). In: Revue d'histoire des textes, bulletin n° 32 (2002), 2003. pp. 63 - 72...|$|R
40|$|Word {{recognition}} and generation {{is a fundamental}} part of the processing of natural language and it requires compu-tationally effective morphological processors, especially for languages with rich morphology such as Modern Greek. Various models have been proposed for developing com-puterized systems to accomplish the task of recognition of morphosyntactic features of words. In the work presented here, the lazy tagging approach was examined, in which taggers are expected {{to work in the}} simplest possible way. The model of functional decomposition was extended and adapted for Modern Greek as a target language, following the lazy word-parsing approach, in order to cover a number of morphological phenomena that are encountered in Modern Greek, namely inflection, affixation, and long-distance dependencies. To achieve a more efficient word recognition, several automata of different levels of comput-ing power, based on the original model, were introduced and evaluated according to the criteria of complexity, recognition speed, and accuracy of the results. The proposed system was used for processing a large-scale corpus, and the results are presented and discussed. To accomplish their task, taggers can rely upon large lexical databases, which are expected to be organized {{in such a way as}} to provide rapid access to the stored data and effi-cient memory management. Directed graphs can be used to describe and organize a lexical database of large magni-tude in a compact manner. These data structures are named here matrix lexica, where the letters are described as nodes of directed graphs and <b>the</b> <b>lemmata</b> as paths (set of edges). It is expected that matrix lexica will support a tagger efficiently by providing a high speed of resolution, sound mathematical foundation, low memory require-ments, and ability to handle distorted input in future de-velopments. 1...|$|R
