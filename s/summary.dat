10000|10000|Public
5|$|Chapter 17 <b>Summary</b> And Classification.|$|E
5|$|A <b>summary</b> of {{the book}} was {{published}} in the Arts & Leisure section of the Sunday Times over the course of three articles.|$|E
5|$|Bell, Bert (1957). The Story of Professional Football in <b>Summary.</b> Bala Cynwyd, PA: National Football League.|$|E
40|$|AbstractThe goal of {{this paper}} is to compare <b>summaries</b> {{generated}} by different automatic text summarization methods and those generated by human beings. To achieve this end, we did two series of experiments: in the first one, we employed automatically produced extractive summaries; in the second one, manually-produced <b>summaries</b> obtained by several English teachers were used. Our automatic <b>summaries</b> were obtained using Fuzzy method and Vector approach. Using Rouge evaluation system, we compared the manually-produced <b>summaries</b> and the automatically-produced ones. Rouge evaluation of generated <b>summaries</b> indicated the superiority of <b>summaries</b> produced by humans over the automatically produced <b>summaries.</b> On the other hand, the comparison between the generated <b>summaries</b> showed that <b>summaries</b> produced by Fuzzy method were much more acceptable and understandable compared to <b>summaries</b> produced by Vector approach. This can provide support for the replacement of manually generated <b>summaries</b> by <b>summaries</b> produced using Fuzzy method in certain cases where real time <b>summaries</b> are needed...|$|R
40|$|We {{describe}} a task-based evaluation {{to determine whether}} multi-document <b>summaries</b> measurably improve user performance when using online news browsing systems for directed research. We evaluated the multi-document <b>summaries</b> generated by Newsblaster, a robust news browsing system that clusters online news articles and summarizes multiple articles on each event. Four groups of {{subjects were asked to}} perform the same time-restricted fact-gathering tasks, reading news under di#erent conditions: no <b>summaries</b> at all, single sentence <b>summaries</b> drawn from one of the articles, Newsblaster multi-document <b>summaries,</b> and human <b>summaries.</b> Our results show that, in comparison to source documents only, the quality of reports assembled using Newsblaster <b>summaries</b> was significantly better and user satisfaction was higher with both Newsblaster and human <b>summaries...</b>|$|R
40|$|Generating query-biased <b>summaries</b> {{can take}} up {{a large part of}} the {{response}} time of interactive information retrieval (IIR) systems. This paper proposes to use document titles as an alternative to queries in the generation of <b>summaries.</b> The use of document titles allows us to pre-generate <b>summaries</b> statically, and thus, improve the response speed of IIR systems. Our experiments suggest that title-biased <b>summaries</b> are a promising alternative to query-biased <b>summaries...</b>|$|R
5|$|Below is a <b>summary</b> of {{the split}} number one selection, by {{respectively}} each country's jury and televoters in the Grand Final: San Marino and Slovenia voted only through juries.|$|E
5|$|Below is table <b>summary</b> of Wales Test matches {{up until}} 23 June 2017.|$|E
5|$|Campbell, K.E., Jr. 2004. The Santa Rosa local fauna: A <b>summary.</b> Science Series, Natural History Museum of Los Angeles County 40:155–163.|$|E
40|$|We {{present a}} novel {{graph-based}} summarization framework (Opinosis) that generates concise abstractive <b>summaries</b> of highly redundant opinions. Evaluation results on summarizing user reviews show that Opinosis <b>summaries</b> have better agreement with human <b>summaries</b> {{compared to the}} baseline extractive method. The <b>summaries</b> are readable, reasonably well-formed and are informative enough to convey the major opinions. ...|$|R
30|$|For the English evaluation, we {{used the}} {{reference}} update <b>summaries</b> provided in the TAC 2009 dataset, as it is usually done in the area. For Portuguese, however, as there are not update <b>summaries</b> made by humans in the CSTNews dataset, we applied the automatic evaluation approach that was proposed in [62], in which the produced <b>summaries</b> are compared to their respective source texts. Louis and Nenkova [62] {{have shown that the}} ROUGE evaluation of <b>summaries</b> based on their source texts is a good approximation of evaluations based on human <b>summaries.</b> Therefore, for ROUGE, we compared the produced <b>summaries</b> to their respective source texts that are labeled as collection B.|$|R
50|$|Fatality Data <b>Summaries</b> - <b>summaries</b> of the {{fatalities}} {{and a list}} of the incidents.|$|R
5|$|Certain things {{cannot be}} attempted. These include conspiracy, under section 1(4) of the 1981 Act, {{assisting}} a criminal, under section 4(1) of the Criminal Law Act 1967, aiding {{in the commission}} of an offence, or most <b>summary</b> offences, with the logic being that they are too minor for attempts to justify a criminal conviction. A small number of separate statutory offences have been created to cover attempts concerning individual <b>summary</b> offences, in which case, usual attempt law and procedure applies.|$|E
5|$|Data and {{conclusions}} from these tests {{were published in}} Informational Intelligence <b>Summary</b> 59, Technical Aviation Intelligence Brief #3, Tactical and Technical Trends #5 (published prior to the first test flight), and Informational Intelligence <b>Summary</b> 85. These results tend to somewhat understate the Zero's capabilities.|$|E
5|$|Hewitt, D.F., and Crickmay, G.W., 1937, The Warm Springs of Georgia, their {{relations}} and origins, a <b>summary</b> report, U.S. Geological Survey WSP 819, 40 p.|$|E
40|$|We {{introduce}} a technique for creating novel, enhanced thumbnails of Web pages. These thumbnails combine {{the advantages of}} plain thumbnails and text <b>summaries</b> to provide consistent performance {{on a variety of}} tasks. We conducted a study in which participants used three different types of <b>summaries</b> (enhanced thumbnails, plain thumbnails, and text <b>summaries)</b> to search Web pages to find several different types of information. Participants took an average of 67, 86, and 95 seconds to find the answer with enhanced thumbnails, plain thumbnails, and text <b>summaries,</b> respectively. As expected, there was a strong effect of question category. For some questions, text <b>summaries</b> outperformed plain thumbnails, while for other questions, plain thumbnails outperformed text <b>summaries.</b> Enhanced thumbnails (which combine the features of text <b>summaries</b> and plain thumbnails) had more consistent performance than either text <b>summaries</b> or plain thumbnails, having for all categories the best performance or performance that was statistically indistinguishable from the best...|$|R
40|$|We {{create and}} analyze {{two sets of}} {{reference}} <b>summaries</b> for discussion threads on a patient support forum: expert <b>summaries</b> and crowdsourced, non-expert <b>summaries.</b> Ideally, reference <b>summaries</b> for discussion forum threads are created by expert members of the forum community. When there are few or no expert members available, crowdsourcing the reference <b>summaries</b> is an alternative. In this paper we investigate whether domain-specific forum data requires the hiring of domain experts for creating reference <b>summaries.</b> We analyze the inter-rater agreement for both datasets and we train summarization models using {{the two types of}} reference <b>summaries.</b> The inter-rater agreement in crowdsourced reference <b>summaries</b> is low, close to random, while domain experts achieve a considerably higher, fair, agreement. The trained models however are similar to each other. We conclude {{that it is possible to}} train an extractive summarization model on crowdsourced data that is similar to an expert model, even if the inter-rater agreement for the crowdsourced data is low...|$|R
40|$|In this paper, {{we present}} a model for {{generating}} <b>summaries</b> of text documents {{with respect to a}} query. This is known as query-based summarization. We adapt an existing dataset of news article <b>summaries</b> for the task and train a pointer-generator model using this dataset. The generated <b>summaries</b> are evaluated by measuring similarity to reference <b>summaries.</b> Our results show that a neural network summarization model, similar to existing neural network models for abstractive summarization, can be constructed to make use of queries to produce targeted <b>summaries...</b>|$|R
5|$|Hitchcock, C.H., and Blake, W.P., 1873, <b>Summary</b> of the Geological Map of the U.S., Geological Magazine, 10:371-373.|$|E
5|$|In August 2016, the {{district}} court denied Trump's motion for <b>summary</b> judgment, ruling that there was sufficient evidence against Trump for the case {{to go to a}} jury.|$|E
5|$|International Energy Agency (2006). World Energy Outlook 2006: <b>Summary</b> and Conclusions, OECD, 11 pages.|$|E
5000|$|<b>Summaries</b> of PhD theses — <b>summaries</b> of PhD and DSc theses {{received}} by the Scientific Library; ...|$|R
40|$|In {{this paper}} {{we report on}} the {{effectiveness}} of query-biased <b>summaries</b> for a question-answering task. Our summarisation system presents searchers with short <b>summaries</b> of documents, composed of a series of highly matching sentences extracted from the documents. These <b>summaries</b> are also used as evidence for a query expansion algorithm to test the use of <b>summaries</b> as evidence for interactive and automatic query expansion. 1...|$|R
40|$|In this paper, we {{introduce}} a corpus of human-authored dialogue <b>summaries</b> collected through a web-experiment. The corpus features (i) {{one of the}} few existing corpora of written dialogue summaries; (ii) the only corpus available for dialogue <b>summaries</b> in Portuguese; and (iii) the only available corpus of <b>summaries</b> produced for dialogues whose participants’ politeness alignment was systematically varied. Comprising 1, 808 human-authored <b>summaries,</b> produced by 452 summarisers, for four different dialogues, this is, {{to the best of our}} knowledge, the largest individual corpus available for dialogue <b>summaries,</b> with the highest number of participants involved...|$|R
5|$|Note: Click on {{the points}} {{to see the}} <b>summary</b> for the match.|$|E
5|$|Pleas of the Crown. A Methodical <b>Summary</b> (1678).|$|E
5|$|<b>Summary</b> of 2006 IUCN Red List categories.|$|E
40|$|The SumNL <b>Summaries</b> Corpus {{consists}} {{mainly of}} <b>summaries</b> {{that were made}} for 30 clusters of texts Every cluster contains 5 - 25 texts and relates to 1 subject. For each cluster 5 different annotators have made two <b>summaries</b> of different sizes. Apart from the <b>summaries</b> in their own words, also extracts were made by choosing 10 sentences from the newspaper articles. Furthermore the annotators have given a score to the sentences of all texts which indicates {{the importance of the}} sentence. The SumNL <b>Summaries</b> Corpus was created and is owned by the University of Antwerp...|$|R
30|$|Discharge <b>summaries</b> {{were then}} scored {{according}} to whether the domains were satisfied or not. The total score for each domain was then divided {{by the total number}} of discharge <b>summaries,</b> in order to indicate the percentage of <b>summaries</b> that satisfied each domain.|$|R
40|$|Includes {{individual}} descriptive <b>summaries</b> {{of research}} underway by the Division of Biology and Medicine of the U. S. Atomic Energy Commission. The <b>summaries</b> {{were provided by}} the principal investigator of each project. "April 1961. "Includes individual descriptive <b>summaries</b> of research underway by the Division of Biology and Medicine of the U. S. Atomic Energy Commission. The <b>summaries</b> were provided by the principal investigator of each project. Mode of access: Internet...|$|R
5|$|<b>Summary</b> {{of battles}} of the 5th USCC. All except the October 2 & 21, 1864 battles had Carpenter present in the command structure.|$|E
5|$|The Natural History {{consists}} of 37 books. Pliny devised his own table of contents. The table {{below is a}} <b>summary</b> based on modern names for topics.|$|E
5|$|The United States' test series <b>summary</b> {{table is}} here: United States' nuclear testing series.|$|E
40|$|Several {{available}} <b>summaries</b> {{of research}} on coaching for the Scholastic Aptitude Test (SAT) are summarized and their principal findings discussed. Some additional studies, that have been completed since these <b>summaries</b> were reported, are considered and linked to the <b>summaries.</b> The four major meta-analyses considered ar...|$|R
40|$|Producing {{linguistic}} <b>summaries</b> {{of large}} databases or temporal sequences of measurements is an endeavor that is receiving increasing attention. These <b>summaries</b> {{can be used}} in a continuous monitoring situation, like eldercare, where it is important to ascertain if the current <b>summaries</b> represent an abnormal condition. It is therefore necessary to compute the distance between <b>summaries</b> as a basis for such a determination. In this paper, we propose a dissimilarity measure between <b>summaries</b> based on fuzzy protoforms, and prove that this measure is a metric. We take into account not only the linguistic meaning of the <b>summaries,</b> but also two quality evaluations, namely the truth values and the degrees of focus. We present examples of how the distance metric behaves and show that it corresponds with intuition...|$|R
25|$|Only {{their short}} <b>summaries</b> follow. Quotation marks refer not to literate citation, they just {{separate}} remarks from tale <b>summaries.</b>|$|R
