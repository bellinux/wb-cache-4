815|82|Public
25|$|What {{distinguished}} {{computer vision}} from the prevalent field of {{digital image processing}} {{at that time was}} a desire to extract three-dimensional structure from images with the goal of achieving full <b>scene</b> <b>understanding.</b> Studies in the 1970s formed the early foundations for many of the computer vision algorithms that exist today, including extraction of edges from images, labeling of lines, non-polyhedral and polyhedral modeling, representation of objects as interconnections of smaller structures, optical flow, and motion estimation.|$|E
2500|$|A new [...] "ARKit" [...] {{application}} programming interface (API) lets third-party developers build {{augmented reality}} apps, {{taking advantage of a}} device's camera, CPU, GPU, and motion sensors. The ARKit functionality is only available to users of devices with Apple A9 and later processors. According to Apple, this is because [...] "these processors deliver breakthrough performance that enables fast <b>scene</b> <b>understanding</b> and lets you build detailed and compelling virtual content on top of real-world scenes." ...|$|E
2500|$|The {{theory was}} also {{applied to a}} range of {{recognition}} tasks: from invariant single object recognition in clutter to multiclass categorization problems on publicly available data sets (CalTech5, CalTech101, MIT-CBCL) and complex (street) <b>scene</b> <b>understanding</b> tasks that requires the recognition of both shape-based as well as texture-based objects (on 	StreetScenes data set). The approach performs really well: It has the capability of learning from only a few training examples and was shown to outperform several more complex state-of-the-art systems [...] constellation models, the hierarchical SVM-based face- detection system). A key element in the approach is a new set of scale and position-tolerant feature detectors, which are biologically plausible and agree quantitatively with the tuning properties of cells along the ventral stream of visual cortex. These features are adaptive to the training set, though we also show that a universal feature set, learned from a set of natural images unrelated to any categorization task, likewise achieves good performance.|$|E
50|$|The Feng Shui Detective follows CF Wong, a {{feng shui}} master living in Singapore, who {{is forced to}} take on a young Australian {{assistant}} named Joyce McQuinnie. Expecting to be moving furniture, she discovers that Wong specializes in the feng shui of crime <b>scenes.</b> <b>Understanding</b> between the two is fraught - Wong speaks pidgin English, augmented by idioms learned from a textbook, and tends to be sexist, racist, money-oriented, and likes to eat small animals (alive if possible); Joyce, on the other hand, speaks in impenetrable youth argot and is a politically correct vegetarian.|$|R
50|$|Coronet {{was still}} very active during the 1973-4 school year, when it placed over 60 titles for {{evaluation}} with Project METRO of the Capitol Region Education Council (CREC), in central Connecticut. Titles included A Is For Alphabet, Color, Color Everywhere, Dating <b>Scene,</b> and <b>Understanding</b> Shakespeare: His Stagecraft.|$|R
40|$|Interpreting a <b>scene</b> {{requires}} <b>understanding</b> how its visual {{properties and}} context yield {{evidence about the}} spatial and conceptual properties of what it depicts. Depiction is intimately tied to spatial language, since describing a scene linguistically, or imagining a scene described in language, involves connecting linguistic and spatial knowledge. We focus here on scenes described via sketching...|$|R
5000|$|Content {{analysis}} {{consists of}} capabilities such as object detection and recognition, event detection and recognition, and <b>scene</b> <b>understanding.</b>|$|E
5000|$|Signal Processing Focus Areas: Compressed Sensing, Low rank matrix recovery, Biomedical Imaging, Information Hiding, Image & Video Forensics, Computer Vision, Image and <b>Scene</b> <b>Understanding,</b> Robust Statistical Methods, Pattern Recognition ...|$|E
5000|$|As visual cloud {{technology}} {{has become more}} capable, more demanding usages have begun to emerge, {{such as the use}} of visual cloud for virtual reality, augmented reality, 3D <b>scene</b> <b>understanding</b> and interactivity, and immersive live experiences. Visual cloud applications can be roughly divided into four primary domains: ...|$|E
40|$|Figure-ground {{organization}} and border-ownership assignment {{are essential for}} <b>understanding</b> natural <b>scenes.</b> It {{has been shown that}} many neurons in the macaque visual cortex signal border-ownership in displays of simple geometric shapes such as squares, but how well these neurons resolve border-ownership in natural scenes is not known. We studied area V 2 neurons in behaving macaques with static images of complex natural scenes. We found that about half of the neurons were border-ownership selective for contours in natural scenes, and this selectivity originated from the image context. The border-ownership signals emerged within 70 ms after stimulus onset, only ∼ 30 ms after response onset. A substantial fraction of neurons were highly consistent across scenes. Thus, the cortical mechanisms of figure-ground organization are fast and efficient even in images of complex natural <b>scenes.</b> <b>Understanding</b> how the brain performs this task so fast remains a challenge...|$|R
40|$|Object Proposals is {{a recent}} {{computer}} vision technique receiving increasing interest from the research community. Its main objective is to generate a relatively small set of bounding box proposals that {{are most likely to}} contain objects of interest. The use of Object Proposals techniques in the <b>scene</b> text <b>understanding</b> field is innovative. Motivated by the success of powerful while expensive techniques to recognize words in a holistic way, Object Proposals techniques emerge {{as an alternative to the}} traditional text detectors. In this paper we study to what extent the existing generic Object Proposals methods may be useful for <b>scene</b> text <b>understanding.</b> Also, we propose a new Object Proposals algorithm that is specifically designed for text and compare it with other generic methods in the state of the art. Experiments show that our proposal is superior in its ability of producing good quality word proposals in an efficient way. The source code of our method is made publicly available. Comment: 13 th International Conference on Document Analysis and Recognition (ICDAR 2015...|$|R
40|$|Goal {{events are}} {{important}} in automatic analysis of broadcast sports game videos, but previous approaches rely on visual or audio information which are hard to obtain. In this paper, we use superimposed texts to detect goals (both the occurrences of goal events and their types) for broadcast basketball video, and we propose a transition pattern based approach for both text extraction and goal detection. Our approach is lightweight and effectively handles main challenges in extracting superimposed texts: complex background, low-resolution and blur of the texts, which made standard localization and character recognition algorithms inaccurate. We focus on extracting superimposed game clock and game score texts in broadcast basketball video. We exploit transition patterns to develop a Hough transform for localization, and conditional random fields (CRFs) for both score digit recognition and goal detection. The experiments show that our transition pattern based approach leads to high accuracy for both superimposed text extraction and goal detection. Categories and Subject Descriptors I. 2. 10 [Artificial Intelligence]: Vision and <b>Scene</b> <b>Understandings</b> -Video Analysis. General Terms Algorithms. Copyright 2014 ACM...|$|R
50|$|Aerial videos are {{emerging}} Spatial Multimedia {{which can be}} used for <b>scene</b> <b>understanding</b> and object tracking. The input video is captured by low flying aerial platforms and typically consists of strong parallax from non-ground-plane structures. The integration of digital video, global positioning systems (GPS) and automated image processing will improve the accuracy and cost-effectiveness of data collection and reduction. Several different aerial platforms are under investigation for the data collection.|$|E
5000|$|A new [...] "ARKit" [...] {{application}} programming interface (API) lets third-party developers build {{augmented reality}} apps, {{taking advantage of a}} device's camera, CPU, GPU, and motion sensors. The ARKit functionality is only available to users of devices with Apple A9 or Apple A10 processors. According to Apple, this is because [...] "these processors deliver breakthrough performance that enables fast <b>scene</b> <b>understanding</b> and lets you build detailed and compelling virtual content on top of real-world scenes." ...|$|E
50|$|What {{distinguished}} {{computer vision}} from the prevalent field of {{digital image processing}} {{at that time was}} a desire to extract three-dimensional structure from images with the goal of achieving full <b>scene</b> <b>understanding.</b> Studies in the 1970s formed the early foundations for many of the computer vision algorithms that exist today, including extraction of edges from images, labeling of lines, non-polyhedral and polyhedral modeling, representation of objects as interconnections of smaller structures, optical flow, and motion estimation.|$|E
40|$|Scene {{classification}} is {{a fundamental}} process of human vision {{that allows us to}} efficiently and rapidly analyze our surroundings. Humans are able to recognize complex visual scenes at a single glance, despite the number of objects with different poses, colors, shadows and textures that may be contained in the <b>scenes.</b> <b>Understanding</b> the robustness and rapidness of this human ability has been a focus of investigation in the cognitive sciences over many years. These studies have stimulated researches in computer vision in building artificial scene recognition systems. Motivations beyond that of pure scientific curiosity are provided by several important computer vision applications in which scene classification can be exploited (e. g., robot navigation systems). Different methods have been proposed to model and to describe the content of a scene. Different machine learning procedures have been employed to automatically learn commonalities and differences between different classes. In this chapter we survey some {{of the state of the}} art approaches for scene classification. For each approach we report a description and a discussion of the most relevant peculiarities...|$|R
40|$|AbstractmA new {{simple and}} {{computationally}} efficient approach to image segmentation via recursive r gion splitting and merging is presented. Unlike other techniques {{the criterion for}} splitting {{is based on a}} generalization of a two-class gradient relaxation method and merging uses a test for mean gray level equivalency for adjacent regions. The technique is illustrated by providing results for both synthetic and natural <b>scenes.</b> Image <b>understanding</b> Multiclass images Outdoor natural scenes Relaxation Image segmentation Split and merge Thresholdin...|$|R
40|$|Visual {{patterns}} {{represent the}} discernible regularity {{in the visual}} world. They capture the essential nature of visual objects or <b>scenes.</b> <b>Understanding</b> and modeling visual patterns is a fundamental problem in visual recognition that has wide ranging applications. In this paper, we study the problem of visual pattern mining and propose a novel deep neural network architecture called PatternNet for discovering these patterns that are both discriminative and representative. The proposed PatternNet leverages the filters in the last convolution layer of a convolutional neural network to find locally consistent visual patches, and by combining these filters we can effectively discover unique visual patterns. In addition, PatternNet can discover visual patterns efficiently without performing expensive image patch sampling, and this advantage provides {{an order of magnitude}} speedup compared to most other approaches. We evaluate the proposed PatternNet subjectively by showing randomly selected visual patterns which are discovered by our method and quantitatively by performing image classification with the identified visual patterns and comparing our performance with the current state-of-the-art. We also directly evaluate the quality of the discovered visual patterns by leveraging the identified patterns as proposed objects in an image and compare with other relevant methods. Our proposed network and procedure, PatterNet, is able to outperform competing methods for the tasks described...|$|R
5000|$|Aerial {{video is}} an {{emerging}} form of data acquisition for <b>scene</b> <b>understanding</b> and object tracking. The video is captured by low flying aerial platforms that integrate Global Positioning Systems (GPS) and automated image processing {{to improve the}} accuracy and cost-effectiveness of data collection and reduction. Recorders can incorporate in-flight voice records from the cockpit intercom system. The addition of audio narration is an extremely valuable tool for documentation and communication. GPS data is incorporated with a text-captioning device on each video frame. Helicopter platforms enable [...] "low and slow" [...] flights, acquiring a continuous visual record without motion blur.|$|E
50|$|The {{theory was}} also {{applied to a}} range of {{recognition}} tasks: from invariant single object recognition in clutter to multiclass categorization problems on publicly available data sets (CalTech5, CalTech101, MIT-CBCL) and complex (street) <b>scene</b> <b>understanding</b> tasks that requires the recognition of both shape-based as well as texture-based objects (on StreetScenes data set). The approach performs really well: It has the capability of learning from only a few training examples and was shown to outperform several more complex state-of-the-art systems constellation models, the hierarchical SVM-based face- detection system). A key element in the approach is a new set of scale and position-tolerant feature detectors, which are biologically plausible and agree quantitatively with the tuning properties of cells along the ventral stream of visual cortex. These features are adaptive to the training set, though we also show that a universal feature set, learned from a set of natural images unrelated to any categorization task, likewise achieves good performance.|$|E
40|$|This paper {{presents}} a visual surveillance {{system for the}} automatic scene interpretation of airport aprons. The system comprises two modules — Scene Tracking and <b>Scene</b> <b>Understanding.</b> The Scene Tracking module, comprising a bottom-up methodology, and the <b>Scene</b> <b>Understanding</b> module, comprising a video event representation and recognition scheme, have been demonstated to be a valid approach for apron monitoring. 1...|$|E
40|$|My {{research}} interests are <b>scene</b> identification and <b>understanding.</b> This involves finding a robust {{representation of the}} scene, able to overcome the obstacles of variance in viewpoint, occlusions and image noise. RESEARCH EXPERIENCE 2008. 09 - present: PhD student at Intelligent Systems Lab Amsterdam Research topic: Identification of famous scenes. This typically means, given a set of already known scenes-locations {{and a picture of}} an unknown scene, find to which known scene this picture corresponds...|$|R
40|$|In {{the context}} of <b>scene</b> modelling, <b>understanding,</b> and landmark-based robot navigation, the {{knowledge}} of static scene parts and moving objects with their motion behaviours plays a vital role. We present a complete framework to detect and extract the moving objects to reconstruct a high quality static map. For a moving 3 D camera setup, we propose a novel 3 D Flow Field Analysis approach which accurately detects the moving objects using only 3 D point cloud information. Further, we introduce a Sparse Flow Clustering approach to effectively and robustly group the motion flow vectors. Experiments show that the proposed Flow Field Analysis algorithm and Sparse Flow Clustering approach are highly effective for motion detection and seg-mentation, and yield high quality reconstructed static maps as well as rigidly moving objects of real-world scenarios...|$|R
40|$|Sustainable {{development}} is multidisciplinary concept in its nature and {{is covered by}} various bodies of sciences. Yet, its literature is fragmented and each specific discipline of knowledge analyzes it and teaches it from its narrow perspective. Therefore, this paper suggests a new conceptual framework for teaching sustainability that assumes the multidisciplinary nature of sustainability. This framework is consists of ten concepts, a distinctive theme, and each one represents a specific domain or field that is related to sustainability. The themes represent the ethical, social, economic, ecological, spatial, design, and political aspects of sustainability. The ten conceptsare intertwined and interconnected and together they construct the holistic <b>scene</b> of <b>understanding</b> and teaching sustainability. These concepts are very useful for teaching sustainability. Moreover, each concept could be in-depth discussed individually in a specific class session. Each discipline {{could take advantage of}} this framework and may emphasize various aspects accordingly...|$|R
40|$|Abstract—Recent {{trends in}} image {{understanding}} have pushed for holistic <b>scene</b> <b>understanding</b> models that jointly reason about various {{tasks such as}} object detection, scene recognition, shape analysis, contextual reasoning, and local appearance based classifiers. In this work, {{we are interested in}} understanding the roles of these different tasks in improved <b>scene</b> <b>understanding,</b> in particular semantic segmentation, object detection and scene recognition. Towards this goal, we “plug-in ” human subjects for each of the various components in a state-of-the-art conditional random field model. Comparisons among various hybrid human-machine CRFs give us indications of how much “head room ” there is to improve <b>scene</b> <b>understanding</b> by focusing research efforts on various individual tasks...|$|E
40|$|Hierarchical {{methods have}} been widely {{explored}} for object recognition, which is {{a critical component of}} <b>scene</b> <b>understanding.</b> However, few existing works are able to model the contextual information (e. g., objects co-occurrence) explicitly within a sin-gle coherent framework for <b>scene</b> <b>understanding.</b> Towards this goal, in this paper we propose a novel three-level (superpixel level, object level and scene level) hierarchical model to address the scene categorization problem. Our proposed model is a coher-ent probabilistic graphical model that captures the object co-occurrence information for <b>scene</b> <b>understanding</b> with a probabilistic chain structure. The efficacy of the proposed model is demonstrated by conducting experiments on the LabelMe dataset. ...|$|E
40|$|Recent {{trends in}} image {{understanding}} have pushed for holistic <b>scene</b> <b>understanding</b> models that jointly reason about various {{tasks such as}} object detection, scene recognition, shape analysis, contextual reasoning, and local appearance based classifiers. In this work, {{we are interested in}} understanding the roles of these different tasks in improved <b>scene</b> <b>understanding,</b> in particular semantic segmentation, object detection and scene recognition. Towards this goal, we "plug-in" human subjects for each of the various components in a state-of-the-art conditional random field model. Comparisons among various hybrid human-machine CRFs give us indications of how much "head room" there is to improve <b>scene</b> <b>understanding</b> by focusing research efforts on various individual tasks...|$|E
40|$|Research in the Computer Vision Laboratory at the University of Maryland {{covers a}} broad range of topics and {{applications}} related to visual processing. This paper highlights some recent and ongoing {{research in the area of}} Document Understanding. The topics addressed include page segmentation, page decomposition, stroke interpretation, logo recognition, forms processing and the generation of synthetic data. 1 Introduction The goal of document understanding has evolved far beyond the age-old task of character recognition. The term understanding suggests that the problem involves a great deal more then the classification of a finite set of symbols. In many cases, the visual and perceptual processes required to understand complex documents can parallel those necessary to process more general classes of <b>scenes.</b> Document <b>understanding</b> research at the University of Maryland focuses on applying nontraditional approaches to the processing of handwritten as well as printed documents. Many of th [...] ...|$|R
6000|$|... "Cowards! {{you son of}} a dog!" [...] {{broke in}} Higgs in his high voice. [...] "How dare you talk to us like that? You see this man here"--and he pointed to Sergeant Quick, who, tall and upright, stood {{watching}} this <b>scene</b> grimly, and <b>understanding</b> most of what passed--"well, he is the lowest among us--a servant only" [...] (here the Sergeant saluted), [...] "but I tell you that there is more courage in his little finger than in your whole body, or in that of all the Abati people, so far as I can make out." ...|$|R
40|$|Automatic image {{orientation}} detection {{for natural}} images is a useful, yet challenging research area. Humans use scene context and semantic object recognition {{to identify the}} correct image orientation. However, {{it is difficult for}} a computer to perform the task in the same way because current object recognition algorithms are extremely limited in their scope and robustness. As a result, existing orientation detection methods were built upon lowlevel vision features such as spatial distributions of color and texture. In addition, discrepant detection rates have been reported. We have developed a probabilistic approach to image orientation detection via confidence-based integration of low-level and semantic cues within a Bayesian framework. Our current accuracy is approaching 90 % for unconstrained consumer photos, impressive given the findings of a psychophysical study conducted recently. The proposed framework is an attempt {{to bridge the gap between}} computer and human vision systems, and is applicable to other problems involving semantic <b>scene</b> content <b>understanding.</b> 1...|$|R
40|$|Abstract — This paper {{presents}} a complete visual surveillance {{system for the}} automatic scene interpretation of airport aprons. The system comprises two modules — Scene Tracking and <b>Scene</b> <b>Understanding.</b> The Scene Tracking module, comprising a bottom-up methodology, and the <b>Scene</b> <b>Understanding</b> module, comprising a video event representation and recognition scheme, have been demonstated to be a valid approach for apron monitoring. I...|$|E
40|$|Abstract. <b>Scene</b> <b>understanding</b> is an {{important}} problem in intelligent robotics. Since visual information is uncertain due to several reasons, we need a novel method that has robustness to the uncertainty. Bayesian probabilistic approach is robust to manage the uncertainty, and powerful to model high-level contexts like the relationship between places and objects. In this paper, we propose a context-based Bayesian method with SIFT for <b>scene</b> <b>understanding.</b> At first, image pre-processing extracts features from vision information and objects-existence information is extracted by SIFT that is rotation and scale invariant. This information is provided to Bayesian networks for robust inference in <b>scene</b> <b>understanding.</b> Experiments in complex real environments show that the pro-posed method is useful. ...|$|E
40|$|<b>Scene</b> <b>understanding</b> {{addresses}} {{the issue of}} “what a scene contains”. Existing research on <b>scene</b> <b>understanding</b> is typically focused on classifying a scene into classes that are of the same category type. These approaches, although they solve some scene-understanding tasks successfully, in general fail to address the semantics in <b>scene</b> <b>understanding.</b> For example, how does an agent learn the concept label “red ” and “ball ” without being told {{that it is a}} color or a shape label in advance? To cope with this problem, we have proposed a novel research called semantic scene concept learning. Our proposed approach models the task of <b>scene</b> <b>understanding</b> as a “multi-labeling ” classification problem. Each scene instance perceived by the agent may receive multiple labels coming from different concept categories, where the goal of learning is to let the agent discover the semantic meanings, i. e., the set of relevant visual features, of the scene labels received. Our preliminary experiments have shown the effectiveness of our proposed approach in solving this special intra- and intercategory mixing learning task. 1...|$|E
40|$|Abstract — We {{address the}} problem of <b>understanding</b> <b>scenes</b> from 3 -D laser scans via per-point {{assignment}} of semantic labels. In order to mitigate the difficulties of using a graphical model for modeling the contextual relationships among the 3 -D points, we instead propose a multi-stage inference procedure to capture these relationships. More specifically, we train this procedure to use point cloud statistics and learn relational information (e. g., tree-trunks are below vegetation) over fine (point-wise) and coarse (region-wise) scales. We evaluate our approach on three different datasets, that were obtained from different sensors, and demonstrate improved performance. I...|$|R
40|$|In visual {{imaging and}} {{processing}} research area unstructured arbitrary natural <b>scene</b> observation and <b>understanding</b> is a problem. Environmental perception and object recognition {{is an important}} part of the image processing. Research approach in image processing needs to proper effective abstraction low level features, so that primitive layer integration with output of preprocess always a simultaneous phenomena for content based CBIR system. Texture, color, and shape always a considerable points for extract but need a proper algorithm and model as per image database complexity increase. Paper work approach based to proposed algorithmic model for efficient and effective retrieval...|$|R
40|$|We {{address the}} problem of <b>understanding</b> <b>scenes</b> from 3 -D laser scans via per-point {{assignment}} of semantic labels. In order to mitigate the difficulties of using a graphical model for modeling the contextual relationships among the 3 -D points, we instead propose a multi-stage inference procedure to capture these relationships. More specifically, we train this procedure to use point cloud statistics and learn relational information (e. g., tree-trunks are below vegetation) over fine (point-wise) and coarse (region-wise) scales. We evaluate our approach on three different datasets, that were obtained from different sensors, and demonstrate improved performance...|$|R
