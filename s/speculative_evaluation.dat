20|3|Public
25|$|Eager Haskell, {{based on}} <b>speculative</b> <b>evaluation.</b>|$|E
40|$|A compile-time {{analysis}} {{technique is}} developed to derive the probability with which a user-defined function or a supercombinator requires {{each one of}} its arguments. This provides a basis for identifying useful speculative parallelism in a program. The performance of <b>speculative</b> <b>evaluation</b> is {{compared with that of}} lazy evaluation, and the necessary conditions under which <b>speculative</b> <b>evaluation</b> performs better are identified...|$|E
40|$|Optimisation of {{real world}} Variable Data {{printing}} (VDP) documents is a dicult problem because the interdependencies between layout functions may drastically {{reduce the number}} of invariant blocks that can be factored out for pre-rasterisation. This paper examines how <b>speculative</b> <b>evaluation</b> at an early stage in a document-preparation pipeline, provides a generic and effective method of optimising VDP documents that contain such interdependencies. <b>Speculative</b> <b>evaluation</b> will be at its most effective in speeding up print runs if sets of layout invariances can either be discovered automatically, or designed into the document at an early stage. In either case the expertise of the layout designer needs to be supplemented by expertise in exploiting potential invariances and also in predicting the effects of <b>speculative</b> <b>evaluation</b> on the caches used at various stages in the print production pipeline...|$|E
5000|$|... #243. <b>Speculative</b> <b>Evaluations.</b> Essays on a Pluralistic Universe. By Hugh P. McDonald. [...] E- ...|$|R
30|$|The {{potential}} impact of the instructors on the outcomes of this research, however, is less <b>speculative.</b> An <b>evaluation</b> of a National Science Foundation (NSF) Research Experience for Undergraduates at University of California—Irvine that focused on interdisciplinary teams in health-related areas found that {{the attitudes of the}} mentors predicted attitudes of participants towards interdisciplinary work (Misra et al. 2009). The team of instructors for this research has considerable overlap in their strengths (including strategic thinking and relationship building), as well as remarkable differences in areas such as executing and communicating. Each one highly values interprofessional work, and have all been through extensive professional development practices. Moreover, professional development has shown to have {{had a profound impact on}} the standard of “Facilitating Learning and Creativity” in a study of 32 teachers using the International Society for Technology Education National Educational Technology Standards (NETS•T) and Performance Indicators for Teachers (Fiala et al. 2015). Thus, as instructors in this study, we may have been especially well-positioned to facilitate student learning and promote the value of working in multidisciplinary teams.|$|R
40|$|Information and {{communication}} technologies (ICTs) are {{often cited as}} {{an important factor in}} firm location decisions. This is especially true for firms engaged in information intensive activities and those that have a strong need for advanced services such as broadband. Despite the suggested importance of these technologies, quantitative work evaluating the link between firm location and broadband provision is sparse and existing knowledge remains largely theoretical and <b>speculative.</b> However, theoretical <b>evaluations</b> of the impact of ICTs on firm location do provide a foundation for quantitative analyses of this relationship and may be grouped into three schools of thought: the deconcentration school, the concentration school, and the heterogeneous effects school. The predictions made by these three schools of thought will be analysed in an exploratory context to better understand the relationship between firm location and ICTs, with a focus on broadband service provision. A combination of basic spatial statistical analytical tools and geographic information systems (GIS) will be used in an exploratory spatial data analysis framework to evaluate the relationship between firm location and broadband provision trends from 1999 to 2004 in the state of Ohio. Results suggest that changes in broadband provision have no relationship with changes in firm location. However, a disaggregated, firm level analysis of this relationship does provide statistically significant results for a subset of industrial sectors. Firm size is also found to impact the correlation between firm presence and broadband provision. These results suggest that firm size and industry are perhaps critical components in determining the relative importance of ICTs, such as broadband, in firm location decisions. Copyright (c) 2008 by the Royal Dutch Geographical Society KNAG. ...|$|R
40|$|Abstract- A compile-time {{analysis}} {{technique is}} developed to derive the probability with which a user-defined function or a supercombinator requires {{each one of}} its arguments. This provides a basis for identifying useful speculative parallelism in a program. The performance of <b>speculative</b> <b>evaluation</b> is {{compared with that of}} lazy evaluation, and the necessary conditions under which <b>speculative</b> <b>evaluation</b> performs better are identified. Index Terms- Conservative parallelism, speculative parallel-ism, lazy evaluation, branch speculation, argument speculation, strictness analysis I...|$|E
40|$|<b>Speculative</b> <b>evaluation,</b> {{including}} leniency and futures, {{is often}} used to produce high degrees of parallelism. Existing speculative implementations, however, may serialize computation because of their implementation of queues of suspended threads. We give a provably efficient parallel implementation of a speculative functional language on various machine models. The implementation includes proper parallelization of the necessary queuing operations on suspended threads. Our target machine models are a butterfly network, hypercube, and PRAM. To prove the efficiency of our implementation, we provide a cost model using a profiling semantics and relate the cost model to implementations on the parallel machine models. 1 Introduction Futures, lenient languages, and several implementations of graph reduction for lazy languages all use <b>speculative</b> <b>evaluation</b> (call-by-speculation [15]) to expose parallelism. The basic idea of <b>speculative</b> <b>evaluation,</b> in this context, is that the evaluation of a [...] ...|$|E
40|$|This thesis {{considers}} how {{to speed}} up the execution of functional programs using parallel execution, load distribution, and <b>speculative</b> <b>evaluation.</b> This is an important challenge given the increasing complexity of software systems, the decreasing cost of individual processors, and the appropriateness of the functional paradigm for parallelisation. Processor speeds are continuing to climb â�� but the magnitudes of increase are overridden by both the increasing complexity of software and the escalating expectation of users. Future gains in speed are likely to occur through the combination of todayâ��s conventional uni-processors to form loosely-coupled multicomputers. Parallel program execution can theoretically provide linear speed-ups, but for this theoretical benefit to be realised two main hurdles must be overcome. The first of these is the identification and extraction of parallelism within the program to be executed. The second hurdle is the runtime management and scheduling of the parallel components to achieve the speed-up without slowing the execution of the program. Clearly a lot of work can be done by the programmer to â��paralleliseâ�� the algorithm. There is often, however, much parallelism available without significant {{effort on the part of}} the programmer. Functional programming languages and compilers have received much attention in the last decade for the contributions possible in parallel executions. Since the semantics of languages from the functional programming paradigm manifest the Church-Rosser property (that the order of evaluation of sub-expressions does not affect the result), sub-expressions may be executed in parallel. The absence of side-effects and the lack of state facilitate the availability of expressions suitable for concurrent evaluation. Unfortunately, such expressions may involve varying amounts of computation or require high amounts of data â�� both of which complicate the management of parallel execution. If the future of computation is through the formation of multicomputers, we are faced with the high probability that the number of available processing units will quickly outweigh the known parallelism of an algorithm at any given moment during execution. Intuitively this spare processing power should be utilised if possible. The premise of <b>speculative</b> <b>evaluation</b> is that it employs otherwise idle tasks on work that may prove beneficial. The more program components available for execution the greater the opportunity for speculation and potentially the quicker the programâ��s result may be obtained. The second impediment for the parallel execution of programs is the scheduling of program components for evaluation. Multicomputer execution of a program involves the allocation of program components among the available tasks to maximise throughput. We present a decentralised, speculation-cognate, load distribution algorithm that allocates and manages the distribution of program components among the tasks with the co-aim of minimising the impact on tasks executing program components known to be required. In this dissertation we present our implementation of minimal-impact <b>speculative</b> <b>evaluation</b> in the context of the functional programming language Haskell augmented with a number of primitives for the indication of useful parallelism. We expound four (two quantitative and two qualitative) novel schemes for expressing the initial speculative contribution of program components and provide a translation mechanism to illustrate the equivalence of the four. The implementation is based on the Glasgow Haskell Compiler (GHC) version 0 â�¢ 29 â�� the de facto standard for parallel functional programming research â�� and strives to minimise the runtime overhead of managing <b>speculative</b> <b>evaluation.</b> We have augmented the Graph reduction for a Unified Machine model (GUM) runtime system with our load distribution algorithm and <b>speculative</b> <b>evaluation</b> sub-system. Both are motivated by the need to facilitate <b>speculative</b> <b>evaluation</b> without adversely impacting on program components directly influencing the programâ��s result. Experiments have been undertaken using common benchmark programs. These programs have been executed under sequential, conservative parallel, and speculative parallel evaluation to study the overheads of the runtime system and to show the benefits of speculation. The results of the experiments conducted using an emulated multicomputer add evidence of the usefulness of <b>speculative</b> <b>evaluation</b> in general and effective <b>speculative</b> <b>evaluation</b> in particular...|$|E
40|$|<b>Speculative</b> <b>evaluation</b> {{can improve}} the {{performance}} of parallel graph reduction systems through increased parallelism. Although speculation is costly, much of the burden can be absorbed by processors which would otherwise be idle. Despite the overhead required for speculative task management, our prototype implementation achieves 70 % efficiency for speculative graph reduction, with little impact on mandatory tasks. Through <b>speculative</b> <b>evaluation,</b> some simple benchmarks exhibit nearly a factor of five speedup over their conservative counterparts. Keyword Codes: D. 1. 1; D. 1. 3. Keywords: Programming Techniques, Applicative (Functional) Programming; Concurrent Programming. 1 Motivation Graph reduction is a popular implementation technique for non-strict functional programming languages. Because graph reduction is free of side-effects, it is well-suited to parallel processing. With conservative evaluation, an expression is not evaluated until its results are actually required. Speculativ [...] ...|$|E
40|$|In a {{parallel}} graph reduction system, <b>speculative</b> <b>evaluation</b> can increase parallelism by performing potentially useful computations {{before they are}} known to be necessary. Speculative computations may be coded explicitly in a program, or they may be scheduled implicitly by the reduction system as idle processors become available. A general approach to both kinds of speculation incurs a great deal of overhead that may outweigh the benefits of <b>speculative</b> <b>evaluation</b> for fine-grain speculative tasks. The basic principle of local speculation is to permanently bind all implicit speculative computations to the sparking processor. Should all local mandatory tasks become blocked, local speculation offers a lowcost alternative to task migration. Restricting speculation to the local processor simplifies the problems of speculative task management, and opens the door for fine-grain speculative tasks. Though there are fewer opportunities for local speculation than for more general speculation, loca [...] ...|$|E
40|$|The article {{introduces}} theoretical {{external and}} internal migration insights {{in the context of}} globalization and research data on empirical migration attitude of higher-school graduates. <b>Speculative</b> <b>evaluation</b> of the problem as well as research, conducted in Lithuania, made it possible to identify the most vulnerable migration groups: IT and healthcare specialists, researchers and research workers, higher school students and graduates. Empirical part, using closed-type anonymous questionnaire reveals research target group‘s (Panevezys city higher-school graduates) preferences to {{external and internal}} migration. Using factorial research analysis reasons due to migration (cultural-political, economical, professional, personal and family) were identified...|$|E
40|$|Abstract <b>Speculative</b> <b>evaluation</b> {{relates to}} {{computing}} several (alternative) threads {{of control of}} large programs concurrently without knowing in advance which of them contribute to which extent to final results. This approach {{may be used to}} advantage to compute, at the expense of deploying considerable processing power, solutions of search problems on average decidedly faster than sequentially. The paper addresses, in the context of term rewrite systems, the measures necessary to organize speculative computations in multiprocessing environments. They primarily concern task management and scheduling, ensuring fair progress of all speculative tasks, and resolving the conflict between fairness and bounded numbers of speculative tasks...|$|E
40|$|Lazy {{programs}} are beautiful, {{but they are}} slow because they build many thunks. Simple measurements show {{that most of these}} thunks are unnecessary: they are in fact always evaluated, or are always cheap. In this paper we describe Optimistic Evaluation [...] - an evaluation strategy that exploits this observation. Optimistic Evaluation complements compile-time analyses with run-time experiments: it evaluates a thunk speculatively, but has an abortion mechanism to back out if it makes a bad choice. A run-time adaption mechanism records expressions found to be unsuitable for <b>speculative</b> <b>evaluation,</b> and arranges for them to be evaluated more lazily in the future...|$|E
40|$|The {{need for}} {{radical change in}} the economy, or more broadly, in the {{socioeconomic}} system, is acknowledged today. The implementation {{of a large number}} of major reforms, radical decisions, and capital projects is planned. The complexity of the interrelations between economic and social relations is so great that the <b>speculative</b> <b>evaluation</b> of socioeconomic decisions does not guarantee the logical correctness of the reforms under discussion. Even the most talented humanistic economists, who traditionally draw upon their limited experience and "common sense," are unable to analyze, compare, and evaluate avenues of >i>perestroika>/i>. History shows what insufficiently substantiated decisions not reinforced by a reliable forecast of their consequences can cost a country. ...|$|E
40|$|Pipelined {{wavefront}} computations are a ubiquitous {{class of}} parallel algorithm {{used for the}} solution {{of a number of}} scientific and engineering applications. This paper investigates three optimisations to the generic pipelined wavefront algorithm, which are investigated through the use of predictive analytic models. The modelling of potential optimisations is supported by a recently developed reusable LogGPbased analytic performance model, which allows the <b>speculative</b> <b>evaluation</b> of each optimisation within the context of an industrystrength pipelined wavefront benchmark developed and maintained by the United Kingdom Atomic Weapons Establishment (AWE). The paper details the quantitative and qualitative benefits of: (1) parallelising computation blocks of the wavefront algorithm using OpenMP; (2) a novel restructuring/shifting of computation within the wavefront code and, (3) performing simultaneous multiple sweeps through the data grid...|$|E
40|$|In {{this article}} we {{introduce}} a novel model for compilation and compiler construction, the CoSy(COmpiler SYstem) model. CoSy provides a framework for flexible combination and embedding of compiler phases [...] - called engines in the sequel [...] - such that the construction of parallel and (inter-procedural) optimizing compilers is facilitated. In CoSy a compiler writer may program some phase in a target language and embed it transparently [...] - without source code changes [...] - into different compiler contexts, such as with alternative phase order, <b>speculative</b> <b>evaluation,</b> parallel evaluation, and generate-and-test evaluation. Compilers constructed with CoSy can be tuned for different host systems (the system the compiler runs on, not the system it produces code for) and are transparently scalable for (shared memory) multiprocessor host configurations...|$|E
40|$|Parallel {{programs}} must describe both computation and coordination, i. e. what to compute {{and how to}} organize the computation. In functional languages equational reasoning {{is often used to}} reason about computation. In contrast, there have been many different coordination constructs for functional languages, and far less work on reasoning about coordination. We present an initial semantics for GpH, a small extension of the Haskell language, that allows us to reason about coordination. In particular we can reason about work, average parallelism and runtime. The semantics captures the notions of limited (physical) resources, the preservation of sharing, and <b>speculative</b> <b>evaluation.</b> We show a consistency result with Launchbury's well-known lazy semantics. 1 Introduction One of the advantages of declarative languages is that it is relatively easy to reason about the values computed by programs, this being attributable to their preservation of referential transparency. Indeed, within the fun [...] ...|$|E
40|$|The {{demands of}} future computing, {{as well as}} the {{challenges}} of nanometer-era VLSI design, require new digital logic techniques and styles that are simultaneously high performance, energy efficient, and robust to noise and variation. We propose a new family of logic styles called Preset Skewed Static Logic (PSSL). PSSL bridges the gap between the two main logic styles, static CMOS logic and domino logic, occupying an intermediate region in the energy-delay-robustness space between the two. PSSL is better than domino in terms of energy and robustness, and is better than static CMOS in terms of delay. PSSL works by partially overlapping the execution of consecutive iterations through <b>speculative</b> <b>evaluation.</b> This is accomplished by presetting nodes at register boundaries before input arrival. by Albert Ma. Thesis (Ph. D.) [...] Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2006. Includes bibliographical references (p. 85 - 88) ...|$|E
40|$|Parallel (lazy) {{functional}} {{programs must}} describe both computation and coordination, i. e., what to compute {{and how to}} arrange the computation in parallel. The formal manipulation of the behaviours of such programs requires a semantics which accurately captures lazy evaluation, and the dependence of execution {{on the availability of}} (physical) resources. In this paper we present a lockstep semantics as a first step towards this goal which, we hope, will allow us to reason about the coordination in a lazy setting. Version History Version 0. 44 (110998) : From a comment by David Crowe, changed normalisation so that use of new closure is correctly noted in seq and par Version 0. 43 : Included Clem's relationship to other work section. Otherwise, just a minor polish for IFL' 98 Version 0. 42 : Considered, but left unimplemented, Clem's tri-partite heap system. Random, <b>speculative</b> <b>evaluation</b> of dead threads remains a problem of the system. Partitioned rules into sequential and parallel rules. Hop [...] ...|$|E
40|$|Eduction often {{unnecessarily}} constrains evaluation {{speed to}} avoid any superfluous evaluation. We describe two simple {{ways in which the}} eduction model can be speeded up, through anticipatory and <b>speculative</b> <b>evaluation,</b> both of which avoid unbounded superfluous evaluation. We consider the effects of these extensions on a shared-memory multiprocessor implementation of GLU using idealized programs. 1 Introduction Eduction is a lazy intensional computing model for evaluating Lucid programs [1, 2]. It is lazy because output values are only produced when they are needed and it is intensional because the individual output values can be produced over time and not necessarily in order. Eduction has been the basis of all known implementations of various incarnations of Lucid (such as pLucid, fLucid, and indexical Lucid) as well as GLU, a hybrid of the latest Lucid and C[3, 2]. Eduction is usually implemented using tagged demand-driven execution and the two are used interchangeably. Demand f [...] ...|$|E
40|$|A {{checkpoint}} is {{a mechanism}} that allows program execution to be restarted from a previously saved state. Checkpoints {{can be used}} in conjunction with exception handling abstractions to recover from exceptional or erroneous events, to support debugging or replay mechanisms, or to facilitate algorithms that rely on <b>speculative</b> <b>evaluation.</b> While relatively straightforward in a sequential setting, for example through the capture and application of continuations, it is less clear how 6 ascribea meaningful semantics for lightweight and safe check~oints in the Dresence of concurrency. For a thread to correctly resume execution fmm a saved checkpoint, it must ensure that all other threads which have witnessed its unwanted effects after the checkpoint was established are also reverted to a meaningful earlier state. If this is not done, data inconsistencies and other undesirable behavior may result. However, automatically determining what constitutes a consistent global state is not straightforward since thread interactions are a dynamic property of the program; requiring applications to specify such states explicitly is not pragmatic if interactions are complex. In this paper, we present a safe and efficient on-the-fly checkpointing mechanism for concurrent programs. We introduce a new abstraction called stabilizers that permits the specification and restoration of globally consistent checkpoints. This state is computed through lightweight monitoring of communication events among threads (e. g., message-passing operations or updates to shared variables). Our implementation results show that the memory and computation overheads for using stabilizers on highlyconcurrent server applications is small, averaging roughly 4 to 6 %, leading us to conclude that stabilizers are a viable abstraction for defining restorable checkpoint state in complex concurrent programs...|$|E
40|$|In Ireland, the "home" {{has a long}} {{tradition}} as a powerful spatial signifier, capable of expressing a range of social, aesthetic, and political interactions. Historically, home ownership in Ireland was not only a measure of working-and-middle-class respectability, but also a symbol of national independence that stood in opposition to a colonial history of dispossession. Throughout the Celtic Tiger, however, the ideas of what constituted the "home" were transformed by a speculative form of capitalism that recreated domestic space as a fluctuating (and valuable) commodity. By centralizing the connection of urban space to capitalism, speculation opens the domestic spaces of the home to the processes of speculation and devaluation. Essentially, my argument analyzes how changing the economic parameters of domestic space creates a concomitant change in how homes are figured in the Irish cultural lexicon. This change represents a cognitive speculation where the speculative value of a home, as a commodity is what confers an authentic social status on the homeowner. Any ethical concerns related to domestic space, both its use and value, are superseded by its ability to generate capital via a <b>speculative</b> <b>evaluation.</b> Through the connection of spatial and cultural transformation, my paper furthers current discussions in Irish studies that analyze the dramatic impact, and aftermath, of the Tiger. The literary response to speculation, found in analyses of Deidre Madden and Anne Haverty, frame speculation as a parody of communal life that eliminates any real interpersonal relationships in a wave of postmodern alienation. Finally, my paper articulates how the lasting damage that followed the collapse of the Tiger altered the way the Irish understood the concepts and realities of the "home".   </div...|$|E

