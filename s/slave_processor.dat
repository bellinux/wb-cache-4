24|49|Public
50|$|The CA20K1 chip is a <b>slave</b> <b>processor</b> which {{requires}} a host CPU to control it. The CA20K2 adds an embedded RISC processor which controls the audio part; this configuration safeguards against the audio latency of its PCI Express interface. 20K2 also has more I/O ports, a DDR SDRAM memory interface, and a built-in Universal Audio Architecture component.|$|E
5000|$|In 1985, National Semi {{introduced}} the NS32332, a much improved {{version of the}} 32032. From the datasheet, the enhancements include [...] "the addition of new dedicated addressing hardware (consisting of a high speed ALU, a barrel shifter and an address register), a very efficient increased (20 bytes) instruction prefetch queue, a new system/memory bus interface/protocol, increased efficiency <b>slave</b> <b>processor</b> protocol and finally enhancements of microcode." [...] There was also a new NS32382 MMU, NS32381 FPU and the (very rare) NS32310 interface to a Weitek FPA.The aggregate performance boost of the NS32332 from these enhancements only made it 50 percent faster than the original NS32032, and therefore {{less than that of}} the main competitor, the MC68020.|$|E
30|$|Each node of the DFG {{represents}} a specific {{task in the}} application. For each task there can be up to three different implementations: Hardware implementations (HW) placed in the FPGA, Software implementations running on the master processor (MS), and another Software implementation running on the <b>slave</b> <b>processor</b> (SL).|$|E
40|$|A {{processor}} farm is a {{distributed system}} {{that consists of}} a unique master processor together {{with a number of}} identical <b>slave</b> <b>processors.</b> The master processor interacts with some environment that generates tasks to be solved by the <b>slave</b> <b>processors.</b> The processors are connected via some communication network that takes care of the communication of the tasks between the master and the slaves. We show ho...|$|R
40|$|Unique modular {{computer}} features compactness, low power, {{mass storage}} of data, multiprocessing, and choice of various input/output modes. Master processor communicates with user via usual keyboard and video display terminal. Coordinates operations {{of as many}} as 24 <b>slave</b> <b>processors,</b> each dedicated to different experiment. Each slave circuit card includes slave microprocessor and assortment of input/output circuits for communication with external equipment, with master processor, and with other <b>slave</b> <b>processors.</b> Adaptable to industrial process control with selectable degrees of automatic control, automatic and/or manual monitoring, and manual intervention...|$|R
40|$|Tremendous {{progress}} has been made at the level of sequential computation in phylogenetics. However, little {{attention has been paid to}} parallel computation. Parallel computing is particularly suited to phylogenetics because of the many ways large computational problems can be broken into parts that can be analyzed concurrently. In this paper, we investigate the scaling factors and efficiency of random addition and tree refinement strategies using the direct optimization software, POY, on a small (10 <b>slave</b> <b>processors)</b> and a large (256 <b>slave</b> <b>processors)</b> cluster of networked PCs running LINUX. These algorithms were tested on several data sets composed of DNA and morphology ranging from 40 to 500 taxa. Various algorithms in POY show fundamentally different properties within and between clusters. All algorithms are efficient on the small cluster for the 40 -taxon data set. On the large cluster, multibuilding exhibits excellent parallel efficiency, whereas parallel building is inefficient. These results are independent of data set size. Branch swapping in parallel shows excellent speed-up for 16 <b>slave</b> <b>processors</b> on the large cluster. However, there is no appreciable speed-up for branch swapping with the further addition of <b>slave</b> <b>processors</b> (> 16). This result is independent of data set size. Ratcheting in parallel is efficient with the addition of up to 32 processors in the large cluster. This result is independent of data set size. c 2001 The Willi Hennig Society...|$|R
40|$|In {{this paper}} we {{introduce}} the efficient load balancing algorithm which {{is helpful to}} distribute the packet of data {{to the group of}} <b>slave</b> <b>processor</b> for achieving the highest possible execution speed and proper utilization of system processor. We propose the load balancing algorithm in the cluster environment means group of <b>slave</b> <b>processor</b> make the cluster or communicator and master processor distribute work to the particular communicator. communicator have unique id which give identification to this. cluster computing provide various computing resource to facilitate execution of large scale task so select the proper node for execute a task able to enhance the performance of large scale cluster computing environment. The performance of the processor is calculated by the system throughput and response time. In our proposed algorithm one communicator do one type of work. It strike a good balancing throughput and output order of data unit and process Synchronization. this type of computing is very important in Engineering and scientific field where different different job come in the main processor...|$|E
40|$|Master {{processor}} supervises slave processors, {{each with}} its own memory. Computer with parallel processing serves as inexpensive tool for experimentation with parallel mathematical algorithms. Speed enhancement obtained depends on both nature of problem and structure of algorithm used. In parallel-processing architecture, "bank select" and control signals determine which one, if any, of N <b>slave</b> <b>processor</b> memories accessible to master processor at any given moment. When so selected, slave memory operates as part of master computer memory. When not selected, slave memory operates independently of main memory. Slave processors communicate with each other via input/output bus...|$|E
40|$|A general multi-tasking {{control system}} has been {{developed}} for real-time signal processing. This control system, written in the language PASCAL, enables tasks (expressed as PASCAL procedures) to be performed as separate, concurrent processes, with adjustable priority levels. Modifications of this system such as the addition of new processes and a change {{of the number of}} priority levels can be realised easily. The {{system has been}} used for the implementation of the real-time algorithms involved in monitoring exercise electrocardiograms. For this application an LSI 11 / 23 is used with the support of a <b>slave</b> <b>processor</b> for the calculation of inner products. The control system is also suitable for other real-time applications when process requirements are not too heavy...|$|E
50|$|There {{were also}} ambitious {{provisions}} for multiprocessing and either loosely or tightly coupled <b>slave</b> <b>processors,</b> {{with or without}} shared global memory. This {{was known as the}} extended processing architecture and extended processing units (EPU).|$|R
40|$|The FPGA (Field Programmable Gate Array) {{of recent}} years has opened newer design {{possibilities}} of moving software into hardware. This paper is studied at two cases of transferring functionality from software into hardware. The paper describes the VCB (Virtual Communication Bus) concept and hardware tasks. The approach with VCB is focused today on existing systems with a main <b>processor</b> and <b>slave</b> <b>processors</b> or DSP (Digital Signal Processors). The first approach {{is to reduce the}} system load from the VCB bus and the second phase will be to eliminate the <b>slave</b> <b>processors</b> and move them to hardware tasks implemented in FPGA. This means that a hardware task can consist of 1000 pages of C code. The hardware tasks will reduce the response time and make the system more time-deterministic...|$|R
40|$|Examines the {{computational}} {{efficiency of}} the master <b>slave</b> Multiple <b>processor</b> architectures system by considering a system consisting of a master M and p <b>slave</b> <b>processors.</b> The system performance is found by modelling it as a Markov process and a new method presented for computing the steady-state performance by dividing the state space into an interior and boundary space. The throughput {{of the system is}} then compared with that of a cost equivalent single processor using different values for the well-known Grosch parameter. It is demonstrated that the system is computationally efficient only for a sufficiently large number of jobs...|$|R
30|$|The {{second case}} study is a complex DFG which {{contains}} different classical structures (Fork, join, sequential). This DFG is depicted in Figure 12. It contains twenty tasks. Each task can be implemented on the Software computation unit (Master or <b>Slave</b> <b>processor)</b> or on the Reconfigurable RCU. The original DFG is the model of an image processing application: motion detection on a fixed image background. This application is composed of 10 sequential tasks (from ID 1 to ID 10 in Figure 12). We added 10 others virtual tasks to obtain a complex DFG containing the different possible parallel structures. This type of parallel program paradigm (Fork, join, etc.) arises in many application areas.|$|E
40|$|The {{master-slave}} paradigm finds important {{applications in}} parallel computer scheduling, semiconductor testing, machine scheduling, transportation, maintenance management and other industrial settings. In the master-slave model considered {{in this paper}} a set of jobs is to be processed by a system of processors. Each job consists of a preprocessing task, a slave task and a post-processing task that must be executed in this order. The pre- and post-processing tasks are to be processed by a master processor while the slave task is processed by a <b>slave</b> <b>processor.</b> In this paper, we motivate the master-slave model and develop bounded performance approximation algorithms for the unconstrained makespan minimization problem {{as well as for}} multiple master systems...|$|E
40|$|Includes bibliographical references. The {{purpose of}} this {{dissertation}} is to present the development and testing of the parallelisation of a Range-Doppler SAR processor. The inherent data parallelism found in SAR data lead to the choice of using master slave parallel processor, where copies of a slave task perform the same tasks on different sets of data. However, the SAR processor that was parallelised needed to implement a corner turn without saving data to disk keeping the data set being processed distributed in memory over the nodes in the cluster. This was successfully achieved using a in-place method, thus saving valuable memory resources. Once the parallel processor was implemented some timing tests were performed, yielding a maximum speedup factor of 6. 2 for an 8 <b>slave</b> <b>processor</b> system...|$|E
40|$|We {{propose a}} {{parallel}} algorithm for an evolutionary programming based two layer channel router (EPCHR). Channel routing {{is an important}} phase of the circuit layout in VLSI design. Channel routing {{can be done in}} two or more layers. The two layer channel routing is an NP complete combinatorial optimization problem. Evolutionary programming (EP) is a population based stochastic search technique. The proposed parallel EPCHR design takes care of the conditions imposed by the EPCHR algorithm. In the parallel EPCHR model we call one processor a master and the remaining <b>processors</b> are <b>slave</b> <b>processors.</b> Since most of the time in the EPCHR algorithm is spent in the generation of offspring and fitness evaluation, this part of the computation is distributed to the <b>slave</b> <b>processors.</b> The selection in EPCHR, needs to be done on the total of parents and offspring; hence, is done at the master processor. The parallel EPCHR algorithm is implemented in C, on CDAC's PARAM machine. The speedup obtained is good and encouraging...|$|R
40|$|Physical {{problems}} offer {{scope for}} macro level parallelization of solution by their essential structure. For parallelization of electrical network simulation, {{the most natural}} structure based method is that of multiport decomposition. In this paper this method {{is used for the}} simulation of electrical networks consisting of resistances, voltage and current sources using a distributed cluster of weakly coupled processors. At the two levels in which equations are solved in this method the authors have used sparse LU for both levels in the first scheme and sparse LU in the inner level and conjugate gradient in the outer level in the second scheme. Results are presented for planar networks, for the cases where the numbers of <b>slave</b> <b>processors</b> are 1 and 2, and for circuit sizes up to 8. 2 million nodes and 16. 4 million edges using 8 <b>slave</b> <b>processors.</b> The authors use a cluster of Pentium IV processors linked through a 10 / 100 MBPS Ethernet switch. © IEE...|$|R
40|$|We {{study the}} formal {{derivation}} of a distributed system that models the behaviour of an arbitrary processor farm. A processor farm is a reactive system {{that consists of}} a unique master processor together {{with a number of}} identical <b>slave</b> <b>processors</b> connected via a communication network. The system reacts with its environment that requires certain service from the farm through the master processor. We focus on the design of the communication network, which is stepwise brought about. We show how a combination of action systems and refinement calculus gives us a uniform method for deriving such a reactive system. 1 Introduction A processor farm [10, 15] is a distributed system that consists of a unique master processor together with an arbitrary number of identical <b>slave</b> <b>processors</b> connected via a communication network. We assume that there is an environment that generates some tasks that the farm is required to solve. The tasks are given to the master that distributes them to the slaves f [...] ...|$|R
40|$|Marionette is {{a system}} for {{distributed}} parallel programming {{in an environment of}} networked heterogeneous computer systems. It is based on a master/slave model. The master process can invoke worker operations (asynchronous remote procedure calls to single slaves) and context operations (updates to the state of all slaves). The master and slaves also interact through shared data structures that can be modified only by the master. The master and slave processes are pro-grammed in a sequential language. The Marionette runtime system manages slave process creation, propagates shared data structures to slaves as needed, queues and dispatches worker and context operations, and manages recovery from <b>slave</b> <b>processor</b> failures. The Marionette system also includes tools for automated compila-tion of program binaries for multiple architectures, and for distributing binaries to remote file sys-tems. A UNIX-based implementation of Marionette is described...|$|E
40|$|Abstract: In this research, a Parallel Two-Dimensional Sorting Algorithm (PTSA) is {{presented}} that has better performance than the classical Quicksort, to sort a data vector of size n = r (rows) × c(columns). PTSA algorithm divides the input vector into n/r sub-vectors, which represents towdimensional vector of <b>Slave</b> <b>Processor</b> Elements (VPE), {{the maximum number}} of VPE for parallel sorting is equal to r×c, VPE just the read,and write operations. The number of Master Processors (MP) which do the sort operation is equal to c, The time needed for PTSA algorithm is reduced by θ (n/r log n/r) with respect to the time needed by Quicksort θ (n log n) to sort the same vector. Simulation results show that the efficiency of sorting using PTSA algorithm is increased and the complexity is reduced significantly compared with classical Quicksort...|$|E
40|$|The {{purpose of}} this {{dissertation}} is to present the development and testing of the parallelisation of a Range-Doppler SAR processor. The inherent data parallelism found in SAR data lead to the choice of using master slave parallel processor, where copies of a slave task perform the same tasks on dierent sets of data. However, the SAR processor that was parallelised needed to implement a corner turn without saving data to disk keeping the data set being processed distributed in memory over the nodes in the cluster. This was successfully achieved using a in-place method, thus saving valuable memory resources. Once the parallel processor was implemented some timing tests where performed, yeilding a maximum speedup factor of 6. 2 for an 8 <b>slave</b> <b>processor</b> system. ii Dedication This thesis is dedicated {{to all those who}} encoraged me to nish it. ii...|$|E
40|$|Abstract:-This {{research}} {{presents a}} parallel simulation technique for large-scale or complex problems. Simulation of large scale problems {{is almost impossible}} to run on a single personal computer and cannot be solved in a reasonable time. Parallelization of problems is achieved by using horizontal domain decomposition while each decomposed domain is computed on each processor of personal computers, PC Cluster. We developed a PC Cluster, namely Walailak University Cluster of 16 <b>Slave</b> <b>processors</b> Pentium 4 (WAC 16 P 4 Cluster). A software of Parallel Computing Toolkit and the Server Message Block Protocol are applied with a new parallel computing technique, which can communicate with adjacent <b>slave</b> <b>processors</b> via the Server Message Block protocol. A WAC 16 P 4 Cluster is an efficient parallel computing facility. This WAC 16 P 4 Cluster is tested by running large-scale water flow simulation by Lattice Boltzmann Method to achieve faster speed. The results are comparable to a small-scale simulation of previous work and the speedup is linearly proportional to the number of processors...|$|R
30|$|The {{processors}} {{are assigned}} to several groups, and the master in the group communicates with the individual <b>slave</b> <b>processors,</b> receiving results and feedbacks on task assignments. Such a communications workload is within the master’s capacity. Therefore, the workload is balanced within groups. Moreover, {{because the number of}} slaves in a group is limited such that the number of requests being served by the master is limited as well, both workload scheduling and result collection will less degrade the performance.|$|R
40|$|The {{design of}} a dual-DSP {{microprocessor}} system and its application for parallel FFT and two-dimensional convolution are explained. The system {{is based on a}} master-salve configuration. Two ADSP- 2101 s are configured as <b>slave</b> <b>processors</b> and a PC/AT serves as the master. The master serves as a control processor to transfer the program code and data to the DSPs. The system architecture and the algorithms for the two applications, viz. FFT and two-dimensional convolutions, are discussed...|$|R
40|$|The Model SP- 320 {{device is}} a {{monolithic}} realization {{of a complex}} general purpose signal processor, incorporating such features as a 32 -bit ALU, a 16 -bit x 16 -bit combinatorial multiplier, and a 16 -bit barrel shifter. The SP- 320 is designed to operate as a <b>slave</b> <b>processor</b> to a host general purpose computer in applications such as coherent integration of a radar return signal in multiple ranges, or dedicated FFT processing. Presently available is an I/O module conforming to the Intel Multichannel interface standard; other I/O modules will be designed to meet specific user requirements. The main processor board includes input and output FIFO (First In First Out) memories, both with depths of 4096 W, to permit asynchronous operation between the source of data and the host computer. This design permits burst data rates in excess of 5 MW/s...|$|E
40|$|Known as an NP-Complete problem, {{the channel}} routing problem is very {{important}} in the automatic layout design of VLSI circuit and printed circuit boards. A distributed hybrid algorithm for this channel routing problem is presented in MPI environments. This system is implemented on a network of personal computers running Linux operating system connected via 100 Mbps Ethernet. Each <b>slave</b> <b>processor</b> generates its own sub-population using genetic operations and communicates with the master processor in an asynchronous manner to form the global population. The proposed hybrid algorithm of Mean Field Annealing and Simulated annealing-like Genetic Algorithm combines the benefit of rapid convergence property of MFA and the effective genetic operations of SGA. The experimental results show that the proposed algorithm maintains the convergence properties of sequential genetic algorithm while it achieves linear speedup as the nets of the channel routing and the number of computing processors increase...|$|E
40|$|Graduation date: 1999 An 8 bit {{microcontroller}} {{slave unit}} was designed, constructed, and tested to demonstrate advantages and feasibility of master/slave parallel processing using conventional processors and relatively slow inter-processor communications. An 8 bit ISA bus controlled by an 80 X 86 is interfaced to a logic block that controls data flow {{to and from}} the slave processors. The slave processors retrieve tasks sent by the master processor and once completed, return results to the master that are buffered for the master's retrieval. The task message sent to the slave processors has task description and task parameters. The master has access to the bi-directional buffer and a status byte for each <b>slave</b> <b>processor.</b> Considerable effort is made to allow the hardware and software architecture to be expandable such that the general design could be used on different master/slave targets. Attention is also given to cost effective solutions such that development and possible market production can be considered...|$|E
40|$|This paper {{describes}} {{the application of}} a master/slave configuration of processors to study a comparison of alternative material handling configurations for automated manufacturing. Such a study usually requires a large number of simulation replications, and carrying out those replications on a multi-processor platform yields significant savings in elapsed time. In the present application, a master processor carries out the statistical computations for a 2 k factorial design on up to eight <b>slave</b> <b>processors.</b> This paper will compare the results from using two, four and eight processors. ...|$|R
40|$|Master/Slave Speculative Parallelization (MSSP) is an {{execution}} paradigm {{for improving the}} execution rate of sequential programs by parallelizing them speculatively for execution on a multiprocessor. In MSSP, one processor [...] -the master [...] -executes an approximate version of the program to compute selected values that the full program's execution is expected to compute. The master's results are checked by <b>slave</b> <b>processors</b> that execute the original program. This validation is parallelized by cutting the program's execution into tasks. Each slave uses its predicted inputs (as computed by the master) to validate the input predictions of the next task, inductively validating the entire execution...|$|R
40|$|For {{parallel}} simulation of VLSI circuits on transistor level a sophisticated partitioning of the circuits into subcircuits is crucial. Each net connecting the subcircuits causes additional communication and computation effort. As the <b>slave</b> <b>processors</b> simulating the subcircuits advance synchronously in time, the computation effort for each subcircuit should be approximately the same. In this paper {{a new approach}} for partitioning VLSI circuits on transistor level yielding a low number of interconnects between the subcircuits and balanced subcircuit sizes is presented. Simulation of industrial circuits using this partitioning is up to 41 % faster than with other known partitioning approaches for parallel analog simulation...|$|R
40|$|The {{parallel}} computing {{based on the}} domain decomposition method is use for solving this problem. A number of PCs are used to form a network under the Linux operating system. A parallel algorithm of the domain decomposition is put forward {{by means of the}} physical conception, i. e., the continuous condition of heat flux in the interface. Another parallel algorithm is called as modified tridiagonal matrix algorithm (m-TDMA). In this algorithm, the equation set for a sub-region in the domain are eliminated to a single equation, and the collection of these equations (one per <b>slave</b> <b>processor)</b> form a small set of equations. This equation set is solved by using the classical tridiagonal matrix algorithm (TDMA), but by sending and receiving data among the master and slaves. The implementation of these domain decomposition parallel algorithm are made in PVM environment. (to be rewritten) 1. INTRODUCTION The heat transfer in the combustion chamber of the internal combustion engine is a complic [...] ...|$|E
40|$|Abstract. This paper {{presents}} {{design and}} manufacturing procedure of a tele-operative rescue robot. First, the general task {{to be performed}} by such a robot is defined, and variant kinematic mechanisms to form {{the basic structure of}} the robot will be discussed. Choosing an appropriate mechanism, geometric dimensions, and mass properties will be detailed to develop a dynamics model for the system. Next, the strength of each component is analyzed to finalize its shape. To complete the design procedure, Patran/Nastran was used to apply the finite element method for strength analysis of complicated parts. Also, ADAMS was used to model the mechanisms, where 3 D sketch of each component of the robot was generated by means of Solidworks, and several sets of equations governing the dimensions of system were solved using Matlab. Finally, the components are fabricated and assembled together with controlling hardware. Two main processors are used within the control system of the robot. The operator’s PC as the master processor and the laptop installed on the robot as the <b>slave</b> <b>processor.</b> The performance of the system was demonstrated i...|$|E
40|$|This thesis {{presents}} {{the design and}} implementation of a 3 D graphics pipeline, built {{on top of the}} "Raw" processor developed at MIT. The Raw processor consists of a tiled array of CPUs, caches, and routing processors connected by several high-speed networks, and can be treated as a coarse-grained reconfigurable architecture. The graphics pipeline has four stages, and four-way parallelism in each stage, and is mapped on to a 16 -tile Raw array. It supports basic rendering functions such as hardware transform and lighting, perspective correct texture mapping, and depth buffering, and is intended {{to be used as a}} <b>slave</b> <b>processor</b> receiving rendering commands from a host system. The design process is described in detail, along with difficulties encountered along the way, and a comprehensive performance evaluation is carried out. The paper concludes with many suggestions for architectural and performance improvements to be made over the initial design. by Kenneth William Taylor. Thesis (S. M.) [...] Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2004. Includes bibliographical references (p. 345 - 346) ...|$|E
40|$|Abstract. In {{the paper}} a new {{parallel}} method for learning decision rules is proposed. The method uses evolutionary algorithm to discover decision rules from datasets. We describe a parallelization of the algorithm based on master-slave model. In our approach the dataset is distributed among <b>slave</b> <b>processors</b> of a parallel system. The slave procesors compute fitness function of chromosomes in parallel. The remainder of evolutionary algorithm i. e. selection and genetic search operators is {{executed by the}} master processor. Our method was implemented on a cluster of SMP machines connected by Fast Ethernet. The experimental results show, that for large datasets {{it is possible to}} obtain a significant speedup. ...|$|R
40|$|This {{dissertation}} describes Master/Slave Speculative Parallelization (MSSP), a novel execution paradigm {{to improve}} the execution rate of sequential programs by parallelizing them speculatively for execution on a multiprocessor. In MSSP, one processor [...] -the master [...] -executes an approximate copy of the program to compute values the program's execution is expected to compute. The master's results are then checked by the <b>slave</b> <b>processors</b> by comparing them to the results computed by the original program. This validation is parallelized by cutting the program's execution into tasks. Each slave uses its predicted inputs (as computed by the master) to validate the input predictions of the next task, inductively validating the whole execution...|$|R
40|$|International audienceWe {{revisit the}} {{master-slave}} tasking paradigm {{in the context}} of heterogeneous processors. We assume that communications are handled by a bus and, therefore, at most one communication can take place at a given time step. We present a polynomial algorithm that gives the optimal solution when a single communication is needed before the execution of the tasks on the <b>slave</b> <b>processors.</b> When communications are required {{both before and after the}} processing of the tasks, we show that the problem is strongly NP-complete. In this case, we present a guaranteed approximation algorithm. Finally, we present asymptotically optimal algorithms when communications are required before the processing of each task, or both before and after the processing of each task...|$|R
