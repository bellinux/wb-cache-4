5|203|Public
40|$|Grounding {{autonomous}} {{behavior in}} the nervous system is a fundamental challenge for neuroscience. In particular, the self-organized behavioral development provides more questions than answers. Are there special functional units for curiosity, motivation, and creativity? This paper argues that these features can be grounded in synaptic plasticity itself, without requiring any higher level constructs. We propose differential extrinsic plasticity (DEP) as a new <b>synaptic</b> <b>rule</b> for self-learning systems {{and apply it to}} a number of complex robotic systems as a test case. Without specifying any purpose or goal, seemingly purposeful and adaptive behavior is developed, displaying a certain level of sensorimotor intelligence. These surprising results require no system specific modifications of the DEP rule but arise rather from the underlying mechanism of spontaneous symmetry breaking due to the tight brain-body-environment coupling. The new <b>synaptic</b> <b>rule</b> is biologically plausible and it would be an interesting target for a neurobiolocal investigation. We also argue that this neuronal mechanism may have been a catalyst in natural evolution. Comment: 18 pages, 5 figures, 7 video...|$|E
40|$|Abstract:-RetinotopicNET is an {{efficient}} simulator for neural networks with retinotopic-like receptive fields. The system has two main characteristics: it is event-driven {{and it takes}} advantage of the retinotopic arrangement in the receptive fields. The dynamics of the simulator are driven by the spiking events of the simple integrate-and-fire neurons. By using an implicit <b>synaptic</b> <b>rule</b> to represent the synapses, RetinotopicNET achieves a great reduction of memory requirement for simulation. We show that under such conditions the system is fit for the simulation of very large networks of integrate-and-fire neurons. Furthermore we test RetinotopicNET in the simulation of a complex neural architecture for the ventral visual pathway. We prove that the system is linearly scalable with respect to the number of neurons in the simulation. Key-words:- Neural simulator; Retinotopy; Receptive fields; Event-driven; Spikes. 1...|$|E
40|$|National audienceThe cortex must {{permanently}} {{deal with}} stimuli {{coming from the}} environment, perceived through different sensors spatially separated. These stimuli converge in the cortex to be processed together in order to construct a multisensory and coherent view of the world. When somebody is hearing /ba/ and is simultaneously viewing the lips movements corresponding to /ga/, he is perceiving /da/. This phenomenon, known as McGurk effect, reveals the cross correlation between different modalities. Some cortical areas are mainly dedicated to the processing of a specific perception. These areas are topographically organized, meaning that two spatially close neurons respond to close stimuli. The {{purpose of this paper}} is to modify the BCM <b>synaptic</b> <b>rule</b> to obtain a self organization of a neuronal map. We introduce a feedback modulation of the learning rule, representing multimodal constraints of the environment. This feedback will be obtained by using a multimap and multilevel architecture of modality assembling...|$|E
3000|$|... and <b>synaptic</b> {{plasticity}} <b>rules</b> can {{be obtained}} using stochastic gradient ascent procedures for this task.|$|R
40|$|We {{show how}} <b>synaptic</b> {{plasticity}} <b>rules</b> {{need to be}} matched to the statistics of stored patterns, and how recall dynamics need to be matched both to input statistics and to the plasticity rule itself {{in order to achieve}} optimal performance. In particular, for binary synapses with metastates we demonstrate {{for the first time that}} memories can be efficiently read out with biologically plausible network dynamics that we derive directly from the <b>synaptic</b> metaplasticity <b>rule</b> with virtually no free parameters...|$|R
40|$|STDP (spike-timing-dependent {{synaptic}} plasticity) {{is thought}} to be a <b>synaptic</b> learning <b>rule</b> that embeds spike-timing information into a specific pattern of synaptic strengths in neuronal circuits, resulting in a memory. STDP consists of bidirectional long-term changes in synaptic strengths. This process includes long-term potentiation and long-term depression, which are dependent on the timing of presynaptic and postsynaptic spikings. In this review, we focus on computational aspects of signaling mechanisms that induce and maintain STDP as a key step toward the definition of a general <b>synaptic</b> learning <b>rule.</b> In addition, we discuss the temporal and spatial aspects of STDP, and the requirement of a homeostatic mechanism of STDP in vivo...|$|R
40|$|Neural {{networks}} with synaptic weights constructed {{according to}} the weighted Hebb rule are studied {{in the presence of}} noise (finite temperature), when the number of stored patterns is finite. Although, for arbitrary weights not all of the stored patterns are global minima, there exists a temperature range in which only the stored patterns are minima of the free energy. In particular, a detailed analysis reveals that {{in the presence of a}} single extra pattern stored with an appropriate weight in the <b>synaptic</b> <b>rule,</b> the temperature at which the spurious minima of the free energy are eliminated is significantly lower than for a similar network without this extra pattern. The convergence time of the network, together with the overlaps of the equilibria of the network with the stored patterns, can thereby be improved considerably. 1 Introduction The statistical mechanics of large neural networks with the Hebb rule prescription for the synaptic weights has been studied in detail and is now w [...] ...|$|E
40|$|The {{ability to}} carry out signal processing, classification, recognition, and {{computation}} in artificial spiking neural networks (SNNs) is mediated by their synapses. In particular, through activity-dependent alteration of their efficacies, synapses play {{a fundamental role in}} learning. The mathematical prescriptions under which synapses modify their weights are termed <b>synaptic</b> plasticity <b>rules.</b> These learning rules can be based on abstract computational neuroscience models or on detailed biophysical ones. As these rules are being proposed and developed by experimental and computational neuroscientists, engineers strive to design and implement them in silicon and en masse in order to employ them in complex real-world applications. In this paper, we describe analog very large-scale integration (VLSI) circuit implementations of multiple <b>synaptic</b> plasticity <b>rules,</b> ranging from phenomenological ones (e. g., based on spike timing, mean firing rates, or both) to biophysically realistic ones (e. g., calcium-dependent models). We discuss the application domains, weaknesses, and strengths of various representative approaches proposed in the literature, and provide insight into the challenges that engineers face when designing and implementing <b>synaptic</b> plasticity <b>rules</b> in VLSI technology for utilizing them in real-world applications. Mostafa Rahimi Azghadi, Nicolangelo Iannella, Said F. Al-Sarawi, Giacomo Indiveri, and Derek Abbot...|$|R
40|$|Changes of {{synaptic}} {{connections between}} neurons {{are thought to}} be the physiological basis of learning. These changes can be gated by neuromodulators that encode the presence of reward. We study a family of reward-modulated <b>synaptic</b> learning <b>rules</b> for spiking neurons on a learning task in continuous space inspired by the Morris Water maze. The <b>synaptic</b> update <b>rule</b> modifies the release probability of synaptic transmission and depends on the timing of presynaptic spike arrival, postsynaptic action potentials, as well as the membrane potential of the postsynaptic neuron. The family of learning rules includes an optimal rule derived from policy gradient methods as well as reward modulated Hebbian learning. The <b>synaptic</b> update <b>rule</b> is implemented in a population of spiking neurons using a network architecture that combines feedforward input with lateral connections. Actions are represented by a population of hypothetical action cells with strong mexican-hat connectivity and are read out at theta frequency. We show that in this architecture, a standard policy gradient rule fails to solve the Morris watermaze task, whereas a variant with a Hebbian bias can learn the task within 20 trials, consistent with experiments. This result does not depend on implementation details such as the size of the neuronal populations. Our theoretical approach shows how learning new behaviors can be linked to reward modulated plasticit...|$|R
40|$|Maximization of {{information}} transmission by a spiking-neuron model predicts changes of synaptic connections {{that depend on}} timing of pre- and postsynaptic spikes and on the postsynaptic membrane potential. Under the assumption of Poisson firing statistics, the <b>synaptic</b> update <b>rule</b> exhibits all {{of the features of}} the Bienenstock–Cooper–Munro rule, in particular, regimes of synaptic potentiation and depression separated by a sliding threshold. Moreover, the learning rule is also applicable to the more realistic case of neuron models with refractoriness, and is sensitive to correlations between input spikes, {{even in the absence of}} presynaptic rate modulation. The learning rule is found by maximizing the mutual information between presynaptic and postsynaptic spike trains under the constraint that the postsynaptic firing rate stays close to some target firing rate. An interpretation of the <b>synaptic</b> update <b>rule</b> in terms of homeostatic synaptic processes and spike-timing-dependent plasticity is discussed...|$|R
30|$|Memory {{networks}} with variable sparseness {{have been}} studied by Amit and Huang [15, 16] under a different learning paradigm in which old memories are gradually overwritten by new memories, and for several more involved <b>synaptic</b> (meta-)plasticity <b>rules.</b> There, inhomogeneity in the pattern sizes was shown to decrease the signal-to-noise ratio during recall as well.|$|R
40|$|Infusion of a GABA agonist (Reiter & Stryker, 1988) and {{infusion}} of an NMDA receptor antagonist (Bear et al., 1990), {{in the primary}} visual cortex of kittens during monocular deprivation, shifts ocular dominance toward the closed eye, in the cortical region near the infusion site. This reverse ocular dominance shift has been previously modeled by variants of a covariance <b>synaptic</b> plasticity <b>rule</b> (Bear et al., 1990; Clothiaux et al., 1991; Miller et al., 1989; Reiter & Stryker, 1988). Kasamatsu et al. (1997, 1998) showed that {{infusion of}} an NMDA receptor antagonist in adult cat primary visual cortex changes ocular dominance distribution, reduces binocularity, and reduces orientation and direction selectivity. This paper presents a novel account {{of the effects of}} these pharmacological treatments, based on the EXIN <b>synaptic</b> plasticity <b>rules</b> (Marshall, 1995), which include both an instar afferent excitatory and an outstar lateral inhibitory rule. Functionally, the EXIN plasticity rules enha [...] ...|$|R
40|$|This thesis {{presents}} a versatile {{study on the}} design and Very Large Scale Integration(VLSI) implementation of various <b>synaptic</b> plasticity <b>rules</b> ranging from phenomenological rules, to biophysically realistic ones. In particular, the thesis aims at developing novel spike timing-based learning circuits that advance the current neuromorphic systems, in terms of power consumption, compactness and synaptic modification (learning) abilities. Furthermore, the thesis investigates {{the usefulness of the}} developed designs and algorithms in specific engineering tasks such as pattern classification. To follow the mentioned goals, this thesis makes several original contributions to the field of neuromorphic engineering, which are briefed in the following. First, a programmable multi-neuron neuromorphic chip is utilised to implement a number of desired rate- and timing-based <b>synaptic</b> plasticity <b>rules.</b> Specific software programs are developed to set up and program the neuromorphic chip, in a way to show the required neuronal behaviour for implementing various <b>synaptic</b> plasticity <b>rules.</b> The classical version of Spike Timing Dependent Plasticity (STDP), as well as the triplet-based STDP and the rate-based Bienenstock-Cooper-Munro (BCM) rules are implemented and successfully tested on this neuromorphic device. In addition, the implemented triplet STDP learning mechanism is utilised to train a feedforward spiking neural network to classify complex rate-based patterns, with a high classification performance. In the next stage, VLSI designs and implementations of a variety of <b>synaptic</b> plasticity <b>rules</b> are studied and weaknesses and strengths of these implementations are highlighted. In addition, the applications of these VLSI learning networks, which build upon various <b>synaptic</b> plasticity <b>rules</b> are discussed. Furthermore, challenges in the way of implementing these rules are investigated and effective ways to address those challenges are proposed and reviewed. This review provides us with deep insight into the design and application of <b>synaptic</b> plasticity <b>rules</b> in VLSI. Next, the first VLSI designs for the triplet STDP learning rule are developed, which significantly outperform all their pair-based STDP counterparts, in terms of learning capabilities. It is shown that a rate-based learning feature is also an emergent property of the new proposed designs. These primary designs are further developed to generate two different VLSI circuits with various design goals. One of these circuits that has been fabricated in VLSI as a proof of principle chip, aimed at maximising the learning performance—but this results in high power consumption and silicon real estate. The second design, however, slightly sacrifices the learning performance, while remarkably improves the silicon area, as well as the power consumption of the design, in comparison to all previous triplet STDP circuits, as well as many pair-based STDP circuits. Besides, it significantly outperforms other neuromorphic learning circuits with various biophysical as well as phenomenological plasticity rules, not only in learning but also in area and power consumption. Hence, the proposed designs in this thesis can play significant roles in future VLSI implementations of both spike timing and rate based neuromorphic learning systems with increased learning abilities. These systems offer promising solutions for a wide set of tasks, ranging from autonomous robotics to brain machine interfaces. Thesis (Ph. D.) [...] University of Adelaide, School of Electrical and Electronic Engineering, 201...|$|R
40|$|This paper {{presents}} {{a new approach}} to neural modeling based on the idea of using an automated method to optimize the parameters of a <b>synaptic</b> learning <b>rule.</b> The <b>synaptic</b> modification <b>rule</b> is considered as a parametric function. This function has local inputs and is the same in many neurons. We can use standard optimization methods to select appropriate parameters for a given type of task. We also present a theoretical analysis permitting to study the generalization property of such parametric learning rules. By generalization, we mean the possibility for the learning rule to learn to solve new tasks. Experiments were performed on three types of problems: a biologically inspired circuit (for conditioning in Aplysia), Boolean functions (linearly separable as well as non linearly separable) and classification tasks. The neural network architecture as well as the form and initial parameter values of the synaptic learning function can be designed using a priori knowledge...|$|R
40|$|This paper {{presents}} an original approach to neural modeling {{based on the}} idea of searching, with learning methods, for a <b>synaptic</b> learning <b>rule</b> which is biologically plausible, and yields networks that are able to learn to perform difficult tasks. The proposed method of automatically finding the learning rule relies on the idea of considering the <b>synaptic</b> modification <b>rule</b> as a parametric function. This function has local inputs and is the same in many neurons. The parameters that define this function can be estimated with known learning methods. For this optimization, we give particular attention to gradient descent and genetic algorithms. In both cases, estimation of this function consists of a joint global optimization of (a) the synaptic modification function, and (b) the networks that are learning to perform some tasks. The proposed methodology {{can be used as a}} tool to explore the missing pieces of the puzzle of neural networks learning. Both network architecture, and the learning function can be designed within constraints derived from biological knowledge. ...|$|R
40|$|The position, size, {{and shape}} of the {{receptive}} field (RF) of some cortical neurons change dynamically, in response to artificial scotoma conditioning (Pettet & Gilbert, 1992) and to retinal lesions (Chino et al., 1992; Darian-Smith & Gilbert, 1995) in adult animals. The RF dynamics are of interest because they show how visual systems may adaptively overcome damage (from lesions, scotomas, or other failures), may enhance processing efficiency by altering RF coverage in response to visual demand, and may perform perceptual learning. This paper presents an afferent excitatory <b>synaptic</b> plasticity <b>rule</b> and a lateral inhibitory <b>synaptic</b> plasticity <b>rule</b> [...] the EXIN rules (Marshall, 1995 a) [...] to model persistent RF changes after artificial scotoma conditioning and retinal lesions. The EXIN model is compared to the LISSOM model (Sirosh et al., 1996) and to a neuronal adaptation model (Xing & Gerstein, 1994). The rules within each model are isolated and are analyzed independently, to elucidate t [...] ...|$|R
40|$|The grid cells (GCs) of {{the medial}} entorhinal cortex (MEC) and place cells (PCs) of the {{hippocampus}} are {{key elements of}} the brain network for the metric representation of space. Currently, any of the existing theoretical models can explain all aspects of GC-specific spatially selective patterns of activity. It is also not clear how these patterns are formed during the network development. On the other hand, it was previously shown that the network that can learn to extract high principal components from {{the activity of the}} place cells could provide the basis for GC-like activity patterns development. Supporting this hypothesis is the finding that PC activity remains spatially stable after the disruption of the GC firing patterns and that grid patterns almost disappear when hippocampal cells are deactivated. Development of the early PCs before GCs formation also supports the role of PCs as the spatial information providers to GCs. Here we have developed a new theoretical model of grid fields formation based on synaptic plasticity in synapses connecting PCs in hippocampus and neurons in entorhinal cortex. Learning of hexagonally symmetric fields in the model is due to complex action of several simple biologicaly motivated <b>synaptic</b> plasticity <b>rules.</b> These <b>rules</b> include associative <b>synaptic</b> plasticity <b>rules</b> similar to BCM rule and homeostatic plasticity <b>rules</b> which restrict <b>synaptic</b> weigths. In contrast to previously described models, this network could learn GC patterns after a very short experience of navigation in a novel environment. In conclusion we suggest that learning on the basis of simple and biologically plausible associative <b>synaptic</b> plasticity <b>rules</b> could contribute to the formation of grid fields in early development and to maintainence of normal GCs activity patterns in familiar environments...|$|R
40|$|Networks of spiking neurons {{can be used}} {{not only}} for brain {{modeling}} but also to solve graph problems. With {{the use of a}} computationally efficient Izhikevich neuron model combined with plasticity rules, the networks possess self-organizing characteristics. Two different time-based <b>synaptic</b> plasticity <b>rules</b> are used to adjust weights among nodes in a graph resulting in solutions to graph prob- lems such as finding the shortest path and clustering...|$|R
40|$|Spike Timing Dependent Plasticity (STDP) is a Hebbian like <b>synaptic</b> {{learning}} <b>rule.</b> The {{basis of}} STDP has strong experimental evidences and {{it depends on}} precise input and output spike timings. In this paper we show that under biologically plausible spiking regime, slight variability in the spike timing leads to drastically different evolution of synaptic weights when its dynamics are governed by the additive STDP rule. Comment: On Computational Neuroscience/ 11 page...|$|R
40|$|Mechanisms of NMDA receptor-dependent {{synaptic}} plasticity {{contribute to the}} acquisition and retention of conditioned fear memory. However, <b>synaptic</b> <b>rules</b> which may {{determine the extent of}} NMDA receptor activation in the amygdala, a key structure implicated in fear learning, remain unknown. Here we show that the identity of the NMDAR glycine site agonist at synapses in the lateral nucleus of the amygdala may depend on the level of synaptic activation. Tonic activation of NMDARs at synapses in the amygdala under low-activity conditions is supported by ambient D-serine, whereas glycine may be released from astrocytes in response to afferent impulses. The release of glycine may decode the increases in afferent activity levels into enhanced NMDAR-mediated synaptic events, serving an essential function in the induction of NMDAR-dependent long-term potentiation in fear conditioning pathways. Activation of N-methyl-D-aspartate receptors (NMDARs) by glutamate in the brain defines many crucial biological processes, including learning, memory, and developmental plasticity 1, 2. Unlike other neurotransmitter receptors, activation of NMDARs requires simultaneous occupation of two different binding sites by glutamate and the glycine sit...|$|R
40|$|According to {{the theory}} of Melioration, organisms in {{repeated}} choice settings shift their choice preference in favor of the alternative that provides the highest return. The goal {{of this paper is to}} explain how this learning behavior can emerge from microscopic changes in the efficacies of synapses, in the context of a two-alternative repeated-choice experiment. I consider a large family of <b>synaptic</b> plasticity <b>rules</b> in which changes in synaptic efficacies are driven by the covariance between reward and neural activity. I construct a general framework that predicts the learning dynamics of any decision-making neural network that implements this <b>synaptic</b> plasticity <b>rule</b> and show that melioration naturally emerges in such networks. Moreover, the resultant learning dynamics follows the Replicator equation which is commonly used to phenomenologically describe changes in behavior in operant conditioning experiments. Several examples demonstrate how the learning rate of the network is affected by its properties and by the specifics of the plasticity rule. These results help bridge the gap between cellular physiology and learning behavior...|$|R
40|$|In {{the last}} decade {{dendrites}} of cortical neurons {{have been shown to}} nonlinearly combine synaptic inputs by evoking local dendritic spikes. It has been suggested that these nonlinearities raise the computational power of a single neuron, making it comparable to a 2 -layer network of point neurons. But how these nonlinearities can be incorporated into the synaptic plasticity to optimally support learning remains unclear. We present a theoretically derived <b>synaptic</b> plasticity <b>rule</b> for supervised and reinforcement learning that depends on the timing of the presynaptic, the dendritic and the postsynaptic spikes. For supervised learning, the rule {{can be seen as a}} biological version of the classical error-backpropagation algorithm applied to the dendritic case. When modulated by a delayed reward signal, the same plasticity is shown to maximize the expected reward in reinforcement learning for various coding scenarios. Our framework makes specific experimental predictions and highlights the unique advantage of active dendrites for implementing powerful <b>synaptic</b> plasticity <b>rules</b> that have access to downstream information via backpropagation of action potentials...|$|R
40|$|This paper {{studies the}} storage of memory {{patterns}} with varying coding levels (fraction of firing neurons within a pattern) in an associative memory network. It was previously shown that effective memory storage (that scales with the network’s size) requires that the <b>synaptic</b> modification <b>rule</b> used during learning explicitly depends on the coding level of the stored memory patterns. We show that the memory capacity of networks storing variably coded memory patterns is inherently bounded and does not scale with the network’s size. These results question the biological feasibility of associative memory learning that uses synaptic level information only. However, we show that applying a neuronal weight correction mechanism that uses local neuronal level information, provides effective memory capacity even when the coding levels vary considerably. Using neuronal weight correction yields near optimal memory performance even with non-optimal <b>synaptic</b> learning <b>rules.</b> These findings provide further support {{to the idea that}} neuronal level normalization processes {{play an important role in}} governing synaptic plasticity and may be crucial for learning in the nervous system. To whom correspondence should be addressed. 0...|$|R
40|$|A {{number of}} {{resistive}} switching memories exhibit activation-based dynamical behavior, {{which makes them}} suitable for neuromorphic and programmable analog filtering applications. Because the Boundary Condition Memristor (BCM) model accounts for tunable activation thresholds only at the on and off boundary states, it is not quantitatively accurate {{in the description of}} these kinds of memristors and in the investigation of their circuit applications. This paper introduces the Generalized Boundary Condition Memristor (GBCM) model, preserving the features of the BCM model while allowing, further, an ad-hoc tuning of activation-based dynamics, which enables an appropriate modulation of the conditions under which memristors may operate as storage elements or data processors. A simple circuit implementation of the novel model is presented, and time-efficient simulations confirming the improvement in modeling accuracy over the BCM model are shown. As a proof-of-concept for the suitability of the GBCM model in the exploration of the full potential of memristors in neuromorphic circuits and programmable analog filters, this paper adopts it to model fundamental <b>synaptic</b> <b>rules</b> governing the mechanisms of learning in neural systems and to gain some insight into key issues in the design of a couple of filter...|$|R
40|$|This paper {{studies the}} {{fundamental}} interplay between Hebbian synaptic changes and neuronally driven processes modifying synaptic efficacies, {{and its role}} in associative memory learning. The importance of neuronally driven normalization processes has already been demonstrated in the context of self-organization of cortical maps [1, 2] and in continuous unsupervised learning as in Principal-Component-Analysis networks [3]. In these scenarios such normalization was shown to be necessary to prevent the excessive growth of synaptic efficacies that occurs when learning and neuronal activity are strongly coupled. In this paper we focus on associative memory learning where this excessive synaptic runaway growth is mild [4], and show that even in this more simple learning paradigm, normalization processes are essential. Moreover, while various normalization procedures can prevent synaptic runaway, our analysis shows that a specific neuronally-driven correction procedure that preserves the total sum of synaptic efficacies is essential for effective memory storage. To this end we analyze the effectiveness of Hebbian <b>synaptic</b> learning <b>rules</b> and identify a critical constraint on effective learning. We then describe a neuronal procedure obtaining this constraint and show how it can be implemented via a biologically plausible mechanism. 2 The Space of <b>Synaptic</b> Learning <b>Rule...</b>|$|R
40|$|Controlling {{the flow}} and routing of data is a {{fundamental}} problem in many distributed networks, including transportation systems, integrated circuits, and the Internet. In the brain, <b>synaptic</b> plasticity <b>rules</b> have been discovered that regulate network activity in response to environmental inputs, which enable circuits to be stable yet flexible. Here, we develop a new neuro-inspired model for network flow control that only depends on modifying edge weights in an activity-dependent manner. We show how two fundamental plasticity rules (long-term potentiation and long-term depression) can be cast as a distributed gradient descent algorithm for regulating traffic flow in engineered networks. We then characterize, both via simulation and analytically, how different forms of edge-weight update rules affect network routing efficiency and robustness. We find a close correspondence between certain classes of <b>synaptic</b> weight update <b>rules</b> derived experimentally {{in the brain and}} rules commonly used in engineering, suggesting common principles to both. Comment: 43 pages, 5 Figures. Submitted to Neural Computatio...|$|R
40|$|We derive a {{plausible}} learning rule for feedforward, feedback and lateral connections in a recurrent network of spiking neurons. Operating {{in the context}} of a generative model for distributions of spike sequences, the learning mechanism is derived from variational inference principles. The <b>synaptic</b> plasticity <b>rules</b> found are interesting in that they are strongly reminiscent of experimental Spike Time Dependent Plasticity, and in that they differ for excitatory and inhibitory neurons. A simulation confirms the method’s applicability to learning both stationary and temporal spike patterns. ...|$|R
40|$|Self-organization {{provides}} a framework {{for the study of}} systems in which complex patterns emerge from simple rules, without the guidance of external agents or fine tuning of parameters. Within this framework, one can formulate a guiding principle for plasticity in the context of unsupervised learning, in terms of an objective function. In this work we derive Hebbian, self-limiting <b>synaptic</b> plasticity <b>rules</b> from such an objective function and then apply the rules to the non-linear bars problem. Comment: To appear in the Proceedings of ESANN 201...|$|R
40|$|<b>Synaptic</b> <b>rules</b> {{that may}} {{determine}} {{the interaction between}} coexisting forms of long-term potentiation (LTP) at glutamatergic central synapses remain unknown. Here, we show that two mechanistically distinct forms of LTP could be induced in thalamic input to the lateral nucleus of the amygdala (LA) with an identical presynaptic stimulation protocol, depending {{on the level of}} postsynaptic membrane polarization. One form of LTP, resulting from pairing of postsynaptic depolarization and low-frequency presynaptic stimulation, was both induced and expressed postsynaptically (“post-LTP”). The same stimulation in the absence of postsynaptic depolarization led to LTP, which was induced and expressed presynaptically (“pre-LTP”). The inducibility of coexisting pre- and postsynaptic forms of LTP at synapses in thalamic input followed a well-defined hierarchical order, such that pre-LTP was suppressed when post-LTP was induced. This interaction was mediated by activation of cannabinoid type 1 receptors by endogenous cannabinoids released in the lateral nucleus of the amygdala in response to activation of the type 1 metabotropic glutamate receptor. These results suggest a previously unknown mechanism by which the hierarchy of coexisting forms of long-term synaptic plasticity in the neural circuits of learned fear could be established, possibly reflecting the hierarchy of memories for the previously experienced fearful events according to their aversiveness level...|$|R
40|$|The {{formation}} of precise connections between retina and LGN involves the activity-dependent elimination of some synapses, with strengthening {{and retention of}} others. Here we show that the MHC Class I (MHCI) molecule H 2 -Db is necessary and sufficient for synapse elimination in the retinogeniculate system. In mice lacking both H 2 -Kb and H 2 -Db (KbDb−/−) despite intact retinal activity and basal synaptic transmission, the developmentally-regulated decrease in functional convergence of retinal ganglion cell synaptic inputs to LGN neurons fails and eye-specific layers do not form. Neuronal expression of just H 2 -Db in KbDb−/ − mice rescues both synapse elimination and eye specific segregation despite a compromised immune system. When patterns of stimulation mimicking endogenous retinal waves are used to probe <b>synaptic</b> learning <b>rules</b> at retinogeniculate synapses, LTP is intact but LTD is impaired in KbDb−/ − mice. This change is due {{to an increase in}} Ca 2 + permeable AMPA receptors. Restoring H 2 -Db to KbDb−/ − neurons renders AMPA receptors Ca 2 + impermeable and rescues LTD. These observations reveal an MHCI mediated link between developmental synapse pruning and balanced <b>synaptic</b> learning <b>rules</b> enabling both LTD and LTP, and demonstrate a direct requirement for H 2 -Db in functiona...|$|R
30|$|We have derived a {{class of}} <b>synaptic</b> {{plasticity}} <b>rules</b> for reinforcement learning in a complex neuronal cell model with NMDA-mediated dendritic nonlinearities. The novel feature of the rules is that the plasticity response to the external reward signal is shaped by the interaction of global somatic quantities with variables local to the dendritic zone where the nonlinear response to the synaptic release arises. Simulation results show that such so-called CR rules can strongly enhance learning performance compared to the case where the plasticity response is determined just from quantities local to the dendritic zone.|$|R
40|$|Abstract Linsker has {{reported}} {{the development of}} structured receptive fields in simulations using a Hebb-type <b>synaptic</b> plasticity <b>rule</b> in a feed-forward linear network. The synapses develop under dynamics determined by a matrix that {{is closely related to}} the covariance matrix of input cell activities. We analyse the dynamics of the learning rule in terms of the eigenvectors of this matrix. These eigenvectors represent independently evolving weight structures. Some general theorems are presented regarding the properties of these eigenvectors and their eigenvalues. For a general covariance matrix four principal parameter regimes are predicted...|$|R
40|$|Several {{observations}} {{suggest that}} the Ca 2 (+) -dependent postsynaptic release of nitric oxide (NO) may be important in the formation and function of the vertebrate nervous system. We explore here {{the hypothesis that the}} release of NO and its subsequent diffusion may be critically related to three aspects of nervous system function: (i) synaptic plasticity and long-term potentiation in certain regions of the adult nervous system, (ii) the control of cerebral blood flow in such regions, and (iii) the establishment and activity-dependent refinement of axonal projections during the later stages of development. In this paper, we detail and analyze the basic assumptions underlying this NO hypothesis and describe a computer simulation of a minimal version of the hypothesis. In the simulation, a 3 -dimensional volume of neuropil is presented with patterned afferent input; NO is produced, diffuses, and is destroyed; and synaptic strengths are determined by a set of <b>synaptic</b> <b>rules</b> based on the correlation of synaptic depolarization and NO levels. According to the hypothesis, voltage-dependent postsynaptic release of this rapidly diffusing substance links the activities of neurons in a local volume of tissue, regardless of whether the neurons are directly connected by synapses. This property is demonstrated in the simulation, and it is this property that is exploited in the hypothesis to account for certain aspects of long-term potentiation and activity-dependent sharpening of axonal arbors...|$|R
30|$|In cortex [7] {{draws the}} {{distinction}} between what they call synaptic vs. structural plasticity, focusing on structural plasticity, which directly effects the synaptic weights in a neural network model. They find that certain characteristics arise directly from the interaction of structural plasticity and <b>synaptic</b> plasticity <b>rules.</b> These characteristics in turn create a variety of stable synaptic weight distributions which could support information storage mechanisms. Neuromodulation can further alter the time-dependent characteristics of a synapse. Acting on the presynaptic side, many neuromodulators reduce the probability of release, protecting the synapse from depletion and therefore extending the duration or frequency sensitivity of the synapse overall.|$|R
40|$|We {{present a}} method of {{estimating}} the gradient of an objective function {{with respect to the}} synaptic weights of a spiking neural network. The method works by measuring the fluctuations in the objective function in response to dynamic perturbation of the membrane conductances of the neurons. It is compatible with recurrent networks of conductance-based model neurons with dynamic synapses. The method can be interpreted as a biologically plausible <b>synaptic</b> learning <b>rule,</b> if the dynamic perturbations are generated by a special class of ``empiric'' synapses driven by random spike trains from an external source. Comment: 5 pages; 1 figure; submitted to PR...|$|R
40|$|We {{show in a}} 4 -layer {{competitive}} neuronal {{network that}} continuous transformation learning, which uses spatial correlations and a purely associative (Hebbian) <b>synaptic</b> modification <b>rule,</b> can build view invariant representations of complex 3 D objects. This occurs even when views of the different objects are interleaved, a condition where temporal trace learning fails. Human psychophysical experiments showed that view invariant object learning can occur when spatial but not temporal continuity applies because of interleaving of stimuli, although sequential presentation, which produces temporal continuity, can facilitate learning. Thus continuous transformation learning is an important principle that may contribute to view invariant object recognition...|$|R
