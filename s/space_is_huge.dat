43|10000|Public
40|$|Background: In systems biology, {{the task}} of reverse {{engineering}} gene pathways from data has been limited not just by the curse of dimensionality (the interaction <b>space</b> <b>is</b> <b>huge)</b> but also by systematic error in the data. The gene expression barcode reduces spurious association driven by batch effects and probe effects. The binary nature of the resulting expression calls lends itself perfectly to modern regularization approaches that thrive in high-dimensional settings...|$|E
40|$|Abstract. This paper {{describes}} an evolutionary way to design behaviors of a mobile robot for recognizing environments. We have proposed an action-based approach (called AEM) for a mobile robot to recognize environments. In AEM, a behavior-based mobile robot acts in each environments and action sequences are obtained. The action sequences are transformed into vectors characterizing the environments, and the robot identifies the environments with the vectors. The design of suitable behaviors for AEM {{is very difficult}} for human because the search <b>space</b> <b>is</b> <b>huge</b> and intuitive understanding is hard. Thus we develop the evolutionary design of such behaviors using genetic algorithm. ...|$|E
40|$|The {{magnetic}} susceptibility, and low-temperature magnetization curve, of the [3 x 3] grid [Mn(III) 4 Mn(II) 5 (2 poap- 2 H) 6](ClO 4) 10. 10 H 2 O is analyzed {{within a}} spin Hamiltonian approach. The Hilbert <b>space</b> <b>is</b> <b>huge</b> (4, 860, 000 states), but the consequent {{use of all}} symmetries and a two-step fitting procedure nevertheless allows the best-fit determination of the magnetic exchange parameters in this system from complete quantum mechanical calculations. The cluster exhibits a total spin S = 1 / 2 ground state; the implications are discussed. Comment: 28 double-spaced pages, 7 figures, 2 table...|$|E
40|$|Abstract: This paper {{concerns}} the utilization parameters of reclaimed water, and evaluates the heat resource potential and suitable development area of reclaimed water. The heat energy from reclaimed water resources {{is expected to}} reach 1027 MW in 2020. The building area that is suitable for adopting reclaimed water heat pump system is about 140 million m 2. The potential of utilization and development <b>space</b> <b>are</b> <b>huge.</b> There <b>is</b> more than 30 % savings of operating cost by using reclaimed water heat energy for buildings with contrast to traditional supply mode, and can reduce more than 70 % of the exhaust heat at the same time. The economic and environmental benefit are great...|$|R
50|$|In Manggarai settlements, free <b>spaces</b> <b>are</b> {{paved with}} <b>huge</b> stones. In {{the city of}} Ende, the dead are buried in round holes, which are closed by stones placed on the grave.|$|R
40|$|This paper proposes an {{approach}} for learning call admission control (CAC) policies in a cellular network that handles several classes of traffic with different resource requirements. The performance measures in cellular networks are long term revenue, utility, call blocking rate (CBR) and handoff failure rate (CDR). Reinforcement Learning (RL) {{can be used}} to provide the optimal solution, however such method fails when the state space and action <b>space</b> <b>are</b> <b>huge.</b> We apply a form of NeuroEvolution (NE) algorithm to inductively learn the CAC policies, which is called CN (Call Admission Control scheme using NE). Comparing with the Q-Learning based CAC scheme in the constant traffic load shows that CN can not only approximate the optimal solution very well but also optimize the CBR and CDR in a more flexibility way. Additionally the simulation results demonstrate that the proposed scheme is capable of keeping the handoff dropping rate below a pre-specified value while still maintaining an acceptable CBR in the presence of smoothly varying arrival rates of traffic, in which the state <b>space</b> <b>is</b> too large for practical deployment of the other learning scheme. ...|$|R
40|$|Lots of {{professional}} collaborative work relies on shared net-worked file systems for easy collaboration, documentation, {{and as a}} joint workspace. We have found that in an engi-neering setting {{with tens of thousands}} of files, usual desktop search does not work as well, especially if the project <b>space</b> <b>is</b> <b>huge,</b> contains a large number of non-textual files that are difficult to search for, and is partially unknown by the users due to information needs reaching into previous years or projects. We therefore propose an approach that joins content and metadata analysis, link derivation, grouping, and other measures to arrive at high-level features suitable for semantic similarity and retrieval to improve information access for this case {{of professional}} search...|$|E
40|$|To {{cope with}} the {{widening}} design gap, the ever increasing impact of technology, reflected in increased interconnect delay and power consumption, and the time-consuming simulations needed to define the architecture of a microprocessor computer engineers need techniques to explore the design space efficiently in an early design stage. These techniques {{should be able to}} identify a region of interest with desirable characteristics in terms of performance, power consumption and cycle time. In addition, they should be fast since the design <b>space</b> <b>is</b> <b>huge</b> and the design time is limited. In this paper we demonstrate that statistical simulation is an early design stage technique capable of making viable microprocessor design decisions efficiently in early stages of the design flow while considering performance, power consumption and cycle time...|$|E
40|$|Users of {{database}} applications, {{especially in}} the e-commerce domain, often resort to exploratory “trial-anderror” queries since the underlying data <b>space</b> <b>is</b> <b>huge</b> and unfamiliar, {{and there are several}} alternatives for search attributes in this space. For example, scouting for cheap airfares typically involves posing multiple queries, varying flight times, dates, and airport locations. Exploratory queries are problematic from the perspective of both the user and the server. For the database server, it results in a drastic reduction in effective throughput since much of the processing is duplicated in each successive query. For the client, it results in a marked increase in response times, especially when accessing the service through wireless channels. In this paper, we investigate the design of automate...|$|E
40|$|According to [Liskov & Berzins 79], "every program {{performs}} some task correctly" {{and hence}} meets, at least, one specification. However, not every specification is satisfiable. In this proposal, we advance {{the hypothesis that}} an abduction mechanism, built upon the proof plans technique, can effectively correct some kinds of faults in formal specifications for both sequential and concurrent programs. Abduction aims to synthesise candidate hypotheses that account for given observations. It works by performing an analysis on unproved goals. Yet, most failed proof search <b>spaces</b> <b>are</b> <b>huge,</b> and special care {{has to be taken}} in order to tackle the combinatorial explosion in the process of identifying a candidate solution. Fortunately, the planing search space generated by proof plans are an exception [Bundy 88], they are moderately small. Furthermore, the meta-level reasoning which guides the plan formation provides us with a basis for analysing failure and partial success, and hence proposing corrections to faulty specifications. ...|$|R
40|$|Learning {{problem solving}} from {{examples}} suffers from three problems. First, {{there is a}} strong dependency on the order of the presented examples. Second, each example has its own peculiarities which must be overcome. Third, the size of the generalization <b>space</b> can <b>be</b> <b>huge,</b> even if the instance language is small. By adding perturbation operators to the concept tree each of these problems can be alleviated. This is demonstrated in a system which learns, through interaction with a teacher, to solve simultaneous linear equations. 1...|$|R
40|$|International audienceTraditional {{algorithms}} for sorting permutations by signed reversals output {{one solution}} while the solution <b>space</b> can <b>be</b> <b>huge.</b> The enumeration of traces of solutions for this {{problem can be}} a powerful tool to help the study of rearrangement scenarios which only include reversals. Through the analysis of the permutations of six members of the Rick-ettsia genus in relation with their common ancestral, we were able to produce all possible scenarios and infer some chronological order over the reversal events that occurred during the evolution of these species. Our results matched with the scenario proposed in the literature...|$|R
40|$|In {{this paper}} a {{statistical}} methodology for finding the optimal deployment of distributed software objects over computational nodes is presented. The optimal placement of a distributed software objects, from the performance viewpoint, {{has a significant}} impact on the performance of the software. In the proposed methodology, a performance predictor function is extracted from a dataset of simulation results using the regression analysis. This performance predictor function then is used by an optimization algorithm to find the optimal object deployment. The key advantage of the proposed methodology over using the traditional QN models is that solving the predictor model obtained from the QN approach during the optimization process many times, particularly when the search <b>space</b> <b>is</b> <b>huge,</b> is prohibiting due to its time complexity...|$|E
40|$|Abstract—Combining {{task and}} motion {{planning}} requires to interleave causal and geometric reasoning, {{in order to}} guarantee the plan to be executable in the real world. The resulting search space, which is the cross product of the symbolic search space and the geometric search <b>space,</b> <b>is</b> <b>huge.</b> Systematically calling a geometric reasoner while evaluating symbolic actions is costly. On the other hand, geometric reasoning can prune out large parts of this search space if geometrically infeasible actions are detected early. Hence, we hypothesized {{the existence of a}} search depth level, until which geometric reasoning can be interleaved with symbolic reasoning with tractable combinatorial explosion, while keeping the benefits of this pruning. In this paper, we propose a simple model that proves the existence of such search depth level, and validate it empirically through experiments in simulation. I. INTRODUCTION AND RELATED WOR...|$|E
40|$|We {{present an}} {{implementation}} of directed narrowing {{extended to the}} conditional framework, which is complete for two classes of conditional term rewrite systems : confluent and decreasing on one hand, level-confluent and terminating on the other hand. 1 Introduction Narrowing was originally introduced for solving unification of terms modulo a theory represented by a term rewrite system (E-unification). Next {{it has been used}} as the operational semantics of functional logic programming languages (see [5] for a recent survey). In this framework, the program is often represented by a set of equational Horn clauses, i. e. a conditional term rewrite system (CTRS). However, conditional narrowing is inefficient for practical applications because the search <b>space</b> <b>is</b> <b>huge</b> and contains infinite branches. This is why many authors have proposed optimizations to the narrowing procedure : innermost, basic innermost, LSE, lazy, approximations, etc [...] Many of them need some restrictions on the re [...] ...|$|E
40|$|The 3 -D {{password}} is a multifactor authentication scheme. For the authentication, it is require to {{presents a}} 3 -D virtual environment where the user navigates and interacts with various objects. The sequence of actions and interactions toward the objects inside the 3 -D environment constructs the user’s 3 -D password. The 3 -D password can combine most existing authentication schemes such as textual passwords, graphical passwords, and {{various types of}} biometrics into a 3 -D virtual environment. The design of the 3 -D virtual environment {{and the type of}} objects selected determine the 3 -D password key space. As per the reference[1] the author tells that the resulting 3 D password <b>space</b> <b>is</b> very <b>huge.</b> In this paper we shows that the <b>space</b> <b>is</b> reduced and increase the security...|$|R
40|$|In {{this paper}} we {{investigate}} {{the application of}} texture synthesis and image inpainting techniques for video applications. Working in the non-parametric framework, we use 3 D patches for matching and copying. This ensures temporal continuity to some extent which {{is not possible to}} obtain by working with individual frames. Since, in present application, patches might contain arbitrary shaped and multiple disconnected holes, fast fourier transform (FFT) and summed area table based sum of squared difference (SSD) calculation [1] cannot be used. We propose a modification of above scheme which allows its use in present application. This results in significant gain of efficiency since search <b>space</b> <b>is</b> typically <b>huge</b> for video applications. 1...|$|R
30|$|In this paper, {{the base}} station compares the MIMO {{channels}} of all users and selects the best user {{one at a time}} based on a certain criterion. In general, for MIMO multiuser scheduling, the best set of transmit antennas could be selected, and this set might belong to more than one user. However, this approach requires more feedback and synchronization than a single user selection constraint. In addition, for MIMO multiuser scheduling, the scheduler should select the best set of MT transmit antennas out of K MT antennas. Thus, the search <b>space</b> will <b>be</b> <b>huge,</b> and suboptimal search algorithms should be proposed. However, this is out of the scope of this paper where we are focusing on analyzing and comparing the performance of user selection criteria.|$|R
40|$|Abstract—System {{splitting}} problem, {{also known}} as controlled system separation problem, {{is to determine the}} proper splitting points for splitting the entire power network into islands when island operation of system is unavoidable. By “proper ” we mean that the splitting strategies should guarantee both the power balance and satisfaction to capacity constraints of transmission lines and other facilities in each island. The system splitting problem is very hard because the strategy <b>space</b> <b>is</b> <b>huge</b> for even middle-scale power networks. This paper proposes a two-phase method to search for proper splitting strategies in real-time. The method narrows down the strategy space using highly efficient OBDD-based algorithm in the first phase, then finds proper splitting strategies using power-flow analysis in the reduced strategy space in the second phase. Simulation with symbolic model checking tool SMV indicates that this method is very promising. Index Terms—Graph theory, island operation, OBDD, system splitting, splitting strategy...|$|E
40|$|High Level Synthesis tools {{have reduced}} {{accelerator}} design time. However, a complex scaling problem {{that remains is}} the data transfer bottleneck. Accelerators require huge amounts of data and are often limited by interconnect resources. Local buffers can reduce communication by exploiting data reuse, but the data access order has a substantial impact {{on the amount of}} reuse that can be utilized. With loop transformations such as interchange and tiling the data access order can be modified. However, for real applications the design <b>space</b> <b>is</b> <b>huge,</b> finding the best set of transformations is often intractable. Therefore, we present a new methodology that minimizes the data transfer by loop interchange and tiling. In contrast to other methods we take inter-tile reuse and loop bounds into account. For real-world applications we show buffer size trade-offs that can give speedups up to 14 x, alternatively these can reduce the required FPGA resources substantially...|$|E
40|$|For many {{real world}} problems, when the design <b>space</b> <b>is</b> <b>huge</b> and {{unstructured}} and time consuming simulation {{is needed to}} estimate the performance measure, {{it is important to}} decide how many designs should be sampled and how long the simulation should be run for each design alternative given that we only have a fixed amount of computing time. In this paper, we present a simulation study on how the distribution of the performance measure and the distribution of the estimation error/noise will affect the decision. From the analysis, it is observed that when the noise is bounded {{and if there is a}} high chance that we can get the smallest noise, then the decision will be to sample as many as possible, but if the noise is unbounded, then it will be important to reduce the level of the noise level by assigning more simulation time to each design alternative. ...|$|E
40|$|Abstract: Stochastic colored Petri nets are an {{established}} {{model for the}} specification and quantitative evaluation of com-plex systems. Automated design-space optimization for such models can help in the design phase to find good variants and parameter settings. However, since only indirect heuristic optimization based on simulation is usually possible, and the design <b>space</b> may <b>be</b> <b>huge,</b> the computational effort of such an algorithm is often prohibitively high. This paper extends earlier work on accuracy-adaptive simulation {{to speed up the}} overall optimization task. A local optimization heuristic in a “divide-and-conquer ” approach is combined with vary-ing simulation accuracy to save CPU time when the response surface contains local optima. An application example is analyzed with our recently implemented software tool to validate the advantages of the approach. ...|$|R
40|$|How {{to design}} {{automated}} procedures which (i) accurately assess {{the knowledge of}} a student, and (ii) efficiently provide advices for further study? To produce well-founded answers, Knowledge Space Theory relies on a combinatorial viewpoint on the assessment of knowledge, and thus departs from common, numerical evaluation. Its assessment procedures fundamentally differ from other current ones (such as those of S. A. T. and A. C. T.). They are adaptative (taking into account the possible correctness of previous answers from the student) and they produce an outcome which is far more informative than a crude numerical mark. This chapter recapitulates the main concepts underlying Knowledge Space Theory and its special case, Learning Space Theory. We begin by describing the combinatorial core of the theory, {{in the form of}} two basic axioms and the main ensuing results (most of which we give without proofs). In practical applications, learning <b>spaces</b> <b>are</b> <b>huge</b> combinatorial structures which may be difficult to manage. We outline methods providing efficient and comprehensive summaries of such large structures. We then describe the probabilistic part of the theory, especially the Markovian type processes which are instrumental in uncovering the knowledge states of individuals. In the guise of the ALEKS system, which includes a teaching component, these methods have been used by millions of students in schools and colleges, and by home schooled students. We summarize some of the results of these applications...|$|R
40|$|Abstract [...] Contemporary {{research}} in mining high utility itemsets from the databases faces two major challenges: exponential search space and database-dependent minimum utility threshold. The search <b>space</b> <b>is</b> very <b>huge</b> when number of distinct items {{and size of}} the database is very large. Data analysts must specify suitable minimum utility thresholds for their mining tasks though they may have no knowledge pertaining to their databases. To evade these problems, two approaches are presented to mine high utility itemsets from transaction databases with or without specifying minimum utility threshold by using genetic algorithm. To the best of our knowledge, this is the first work on mining high utility itemsets from transaction databases using Genetic Algorithm (GA). Experimental results show that below mentioned GA approaches achieve better performance in terms of scalability and efficiency...|$|R
40|$|Distributed systems {{usually have}} many {{configurable}} parameters {{such as those}} included in common configuration files. Performance of distributed systems is partially dependent on these system configurations. While operators may choose default settings or manually tune parameters based on their experience and intuition, the resulted settings {{may not be the}} optimal one for specific services running on the distributed system. In this paper, we formulate the problem of autotuning configurations as a black-box optimization problem. This problem becomes quite challenging since the joint parameter search <b>space</b> <b>is</b> <b>huge</b> and also no explicit relationship between performance and configurations exists. We propose to use a well known evolutionary algorithm called Covariance Matrix Adaptation (CMA) to automatically tune system parameters. We compare CMA algorithm to another existing techniques called Smart Hill Climbing (SHC) and demonstrate that CMA algorithm outperforms SHC algorithm both on synthetic data and in a real system. ...|$|E
40|$|This paper {{describes}} an evolutionary way to acquire behaviors of a mobile robot for recognizing environments. We have proposed AEM (Action-based Environment Modeling) approach {{for a simple}} mobile robot to recognize environments. In AEM, a behaviorbased mobile robot acts in each environments and action sequences are obtained. The action sequences are transformed into vectors characterizing the environments, and the robot identifies the environments with similarity between the vectors. The suitable behaviors like wall-following for AEM have been designed by a human. However the design is very di#cult for him/her because the search <b>space</b> <b>is</b> <b>huge</b> and intuitive understanding is hard. Thus we propose the evolutionary design of such behaviors using genetic algorithm and make simulations in which a robot recognizes the environments with different structures. As results, we find out suitable behaviors are learned even for environments in which human hardly designs them, a [...] ...|$|E
40|$|This paper {{describes}} an evolutionary way to design behaviors of a mobile robot for recognizing environments. We have proposed an action-based approach (called AEM) for a mobile robot to recognize environments. In AEM, a behavior-based mobile robot acts in each environments and action sequences are obtained. The action sequences are transformed into vectors characterizing the environments, and the robot identifies the environments with the vectors. The design of suitable behaviors for AEM is very di#cult for human because the search <b>space</b> <b>is</b> <b>huge</b> and intuitive understanding is hard. Thus we develop the evolutionary design of such behaviors using genetic algorithm. 1 Introduction The {{most studies to}} recognize environments have tried to build a precise geometric map using a robot with high-sensitive and global sensors. However, just to recognize environments, such a strict map may be unnecessary. Thus we have tried to build a mobile robot which recognizes environments only wi [...] ...|$|E
5000|$|Bischoff began {{recording}} the album Cistern {{in an empty}} two million gallon underground water tank under Fort Worden in Port Townsend, Washington. The size of the <b>space</b> <b>was</b> a <b>huge</b> factor {{in the development of}} the album. In an interview Bischoff described how [...] "the vast emptiness of the cistern generates a reverb decay that lasts 45 seconds. That means, if you snap your fingers, the sound lasts 45 seconds. That amount of reverberation is an absolutely wild environment to try to create music in". This led to [...] "a record intrinsically linked to the space in which it <b>was</b> conceived. A <b>space</b> which forced Bischoff to slow down, to reflect, to draw on his childhood growing up on a sailing boat - an unexpected journey of rediscovery, from the city back to the Pacific Ocean".|$|R
40|$|A {{management}} model {{with the economic}} objective of maximizing the benefit from pumped freshwater volume and minimizing the utility cost in coastal aquifers threatened by saltwater intrusion is developed. The saltwater intrusion model is based on sharp interface assumption and vertically integrated flow, namely the Strack model, reducing the problem to solving potential flow with a search in 2 -D space for the saltwater toe location. When a well is intruded, it is shut down. Since the search <b>space</b> <b>is</b> discontinuous, <b>huge,</b> and contains numerous local minima, a Simple Genetic Algorithm {{is used in the}} search of near-optimal solution. A few test cases are conducted and show that genetic algorithm can be effectively used to obtain nearoptimal global solutions to groundwater management problems. The developed model is then applied to the management of the coastal aquifer underlying the city of Miami Beach in the northeast of Spain. ...|$|R
40|$|In this paper, {{we study}} the {{structure}} of WiMAX mesh networks {{and the influence of}} tree structureon the performance of the network. From a given network graph, we search for trees, which fulfillsome network, QoS requirements. Since the searching <b>space</b> <b>is</b> very <b>huge,</b> we use geneticalgorithm in order to find solution in acceptable time. We use NetKey representation which is an unbiased representation with high locality, and due tohigh locality we expect standard genetic operators like n-point cross over and mutation workproperly and {{there is no need for}} problem specific operators. This encoding belongs to class ofweighted encoding family. In contrast to other representation such as characteristics vectorencoding which can only indicate whether a link is established or not, weighted encodings useweights for genotype and can thus encode the importance of links. Moreover, by using properfitness function we can search for any desired QoS constraint in the network...|$|R
40|$|International audienceReal time {{spectral}} imaging {{applications are}} data and communication intensive algorithms. SoC (System On Chip) architecture using NoC (Network On Chip) communication {{is the most}} appropriate solution as it brings flexibility, performances and immediate reuse of IPs cores. Tuning the SoC-based NoC architecture for a dedicated algorithm is a complex task as the design <b>space</b> <b>is</b> <b>huge</b> and experimentations take time. In this paper we propose an emulation platform for the evaluation and exploration of performances of the architecture on FPGA. The emulation platform {{is based on the}} NoC and parameterized emulation blocks. Emulating the communication inside the FPGA gives precise timing performances compared to simulation. The experiments are fast and reliable without hardware requirements. The designer can evaluate the timing of the application with a restricted number of implementations in a short time to experimentation. The evaluation is illustrated for a spectral imaging algorithm dedicated to art authentication on the architecture containing MIPS processors and the Hermes NoC...|$|E
40|$|International audienceUsing materialized views can highly {{speed up}} the query {{processing}} time. This paper deals with the view selection issue, which consists in finding a set of views to materialize that minimizes the expected cost of evaluating the query workload, given {{a limited amount of}} resource such as total view maintenance cost and/or storage space. However, the solution <b>space</b> <b>is</b> <b>huge</b> since it entails a large number of possible combinations of views. For this matter, we have designed a solution involving constraint programming, which {{has proven to be a}} powerful approach for modeling and solving combinatorial problems. The efficiency of our method is evaluated using workloads consisting of queries over the schema of the TPC-H benchmark. We show experimentally that our approach provides an improvement in the solution quality (i. e., the quality of the obtained set of materialized views) in term of cost saving compared to genetic algorithm in limited time. Furthermore, our approach scales well with the query workload size...|$|E
40|$|AbstractSince many {{desirable}} properties about finite-state {{model are}} {{expressed as a}} reachability problem, reachability algorithms have been extensively studied in model checking. On the other hand, reachability algorithms {{play an important role}} in game solving since reachability games are often described as a finite state model. In this sense, reachability algorithms are located in the intersection of the research areas of Model Checking and Artificial Intelligence. This paper interests in solving the reachability games called Push-Push. However, both exact and approximate reachability algorithms are not sufficient to the games since its state <b>space</b> <b>is</b> <b>huge</b> and requires lots of iterations such as 338 steps in the reachability computation. Thus we devise the new algorithm called relay reachability algorithm. It divides the global state space into several local ones. And exact reachability algorithm is applied on each local state space one by one. With these reachability algorithms, we solve all of the games...|$|E
40|$|Abstract—Fuzz testing or fuzzing is {{interface}} robustness testing {{by stressing}} the interface {{of a system}} under test (SUT) with invalid input data. It aims at finding security-relevant weaknesses in the implementation that {{may result in a}} crash of the system-under-test or anomalous behavior. Fuzzing means sending invalid input data to the SUT, the input <b>space</b> <b>is</b> usually <b>huge.</b> This <b>is</b> also true for behavioral fuzzing where invalid message sequences are submitted to the SUT. Because systems getting more and more complex, testing a single invalid message sequence becomes more and more time consuming due to startup and initialization of the SUT. We present an approach to make the test execution for behavioral fuzz testing more efficient by generating test cases at runtime instead of before execution, focusing on interesting regions of a message sequence based on a previously conducted risk analysis and reducing the test space by integrating already retrieved test results in the test generation process...|$|R
40|$|This paper {{presents}} an efficient technique {{for improving the}} efficiency of Thai error correction system by using memetic algorithm. In this paper, the token passing algorithm is used for constructing word graph and the language model is used checking the correct sentence. The correction process starts with word graph construction from token passing algorithm, then the correct sentence are searched by memetic algorithm with the best fitness function from language model. For a long sentence from the token passing algorithm, a search <b>space</b> <b>is</b> very <b>huge</b> which can <b>be</b> resolved by using memetic algorithm. The memetic algorithm is used for searching the correct sentence {{in order to reduce}} the analysis time. The performance of the proposed method are evaluated and compared to the full search and genetic algorithm. From the experimental results show that the proposed method performs very well and yields better performance more than the compared method. The proposed method can search the best sentence accurately and quickly. ...|$|R
40|$|Abstract—Future {{embedded}} system products, e. g. smart hand-held mobile terminals, will accommodate {{a large number}} of appli-cations that will partly run sequentially and independently, partly concurrently and interacting on massively parallel computing platforms. Already for systems of moderate complexity, the design <b>space</b> will <b>be</b> <b>huge</b> and its exploration requires that the system architect is able to quickly evaluate the performances of candidate architectures and application mappings. The mainstream evalu-ation technique today is the system-level performance simulation of the applications and platforms using abstracted workload and processing capacity models, respectively. These virtual system models allow fast simulation of large systems at an early phase of development with reasonable modeling effort and time. The accuracy of the performance results is dependent on how closely the models used reflect the actual system. This paper presents a compiler based technique for automatic generation of workload models for performance simulation, while exploiting an overall approach and platform performance capacity models developed previously. The resulting workload models are experimented using x 264 video and JPEG encoding application examples. I...|$|R
