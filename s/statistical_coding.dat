24|287|Public
5000|$|Moreover, the claimed symbol probabilities were , but {{the actual}} {{frequencies}} in this example are [...] If the intervals are readjusted for these frequencies, the entropy of the message would be 4.755 bits and the same NEUTRAL NEGATIVE ENDOFDATA message could be encoded as intervals [...] and a binary interval of [...] This is also {{an example of how}} <b>statistical</b> <b>coding</b> methods like arithmetic encoding can produce an output message that is larger than the input message, especially if the probability model is off.|$|E
40|$|<b>Statistical</b> <b>coding</b> {{techniques}} {{have been used}} for lossless statistical data compression, applying methods such as Ordinary, Shannon, Fano, Enhanced Fano, Huffman and Shannon-Fano-Elias coding methods. A new and improved coding method is presented, the Fano-Huffman Based <b>Statistical</b> <b>Coding</b> Method. It holds the advantages of both the Fano and Huffman coding methods. It is more easily applicable than the Huffman coding methods and it is more optimal than Fano coding method. The optimality with respect to the other methods is realized on the basis of English, German, Turkish, French, Russian and Spanish. Key words: Fano-Huffman based <b>statistical</b> <b>coding</b> method, probability distribution of language, entropy, information, optimal code...|$|E
40|$|Literate Statistical Practice (LSP) is an {{method for}} {{statistical}} practice {{which suggests that}} documentation and specification occur {{at the same time}} as <b>statistical</b> <b>coding.</b> It applies literate programming Knuth (1992) to the practice of statistics. We discuss 2 di#erent approaches for LSP, one currently implemented using Emacs with Noweb and Emacs Speaks Statistics (ESS), and the other developed based on eXtensible Markup Language (XML) tools...|$|E
50|$|Regarding {{even the}} fine {{measuring}} methods presented, {{there are still}} errors one may wish remove {{or at least to}} consider. Non-linearities of the time-to-digital conversion for example can be identified by taking a large number of measurements of a poissonian distributed source (<b>statistical</b> <b>code</b> density test). Small deviations from the uniform distribution reveal the non-linearities.Inconveniently the <b>statistical</b> <b>code</b> density method is quite sensitive to external temperature changes. Thus stabilizing delay or phase-locked loop (DLL or PLL) circuits are recommended.|$|R
50|$|Today their Native Title {{land rights}} {{interests}} {{are represented by}} the Goldfields Aboriginal Land and Sea Council Corporation. Australian Standard Classification of Languages (ASCL), 1997 give the Wongi language the <b>statistical</b> <b>code</b> 8503.|$|R
5000|$|Conditions and {{treatments}} are often captured using a medical classification system, such as ICD-10, {{in a process}} called clinical coding. The practice of coding, essentially groups patients using <b>statistical</b> <b>codes.</b> The coded data can be grouped further into Diagnosis-Related Groups (DRGs), which {{are used in the}} billing process by hospitals and practices; as the [...] "cost per item" [...] of healthcare is based on the casemix.|$|R
40|$|Simplified digital subband coders and decoders {{developed}} {{for use in}} converting digitized samples of source signals into compressed and encoded forms that maintain integrity of source signals while enabling transmission at low data rates. Examples of coding methods used in subbands include coarse quantization in high-frequency subbands, differential coding, predictive coding, vector quantization, and entropy or <b>statistical</b> <b>coding.</b> Encoders simpler, less expensive and operate rapidly enough to process video signals...|$|E
40|$|The {{purpose of}} this paper is to show that neural {{networks}} may be promising tools for data compression without loss of information. We combine predictive neural nets and <b>statistical</b> <b>coding</b> techniques to compress text les. We apply our methods to certain short newspaper articles and obtain compression ratios exceeding those of widely used Lempel-Ziv algorithms (which build the basis of the UNIX functions "compress" and "gzip"). The main disadvantage of our methods is that they are about three orders of magnitude slower than standard methods...|$|E
40|$|An {{approach}} to data compression {{is discussed in}} which the effect achieved by compression is evaluated by {{the closeness of the}} {{approach to}} the minimum possible volume. An attempt is made to systematize the known results on data compression. The review contains: description of methods of data compression based on <b>statistical</b> <b>coding</b> and information theory; application of methods of interpolation and extrapolation; a specific compression method (related to description of the histogram of a sample); some criteria of effectiveness and methods of service information representation; and discussion of models suggested for theoretical analysis...|$|E
40|$|Using {{the methods}} of cold neutron capture and photon {{scattering}} the electric dipole strength function and the level density of the nuclei 78 Se and 196 Pt are investigated. Considering that the deexcitation process could be described by the same strength functions one could describe both experiments in a <b>statistical</b> model <b>code.</b> The report shows the data analysis {{as well as a}} new very fast <b>statistical</b> <b>code,</b> which was used to get the complete strength information up to the neutron separation energy...|$|R
40|$|The two-step {{model for}} fusion {{reactions}} of massive systems is briefly reminded. By {{the use of}} fusion probabilities obtained by the model and of survival probabilities obtained by the new <b>statistical</b> <b>code,</b> we predict xn residue cross sections for 48 Ca+actinide systems leading to superheavy elements with Z= 114, 116 and 118. 1...|$|R
40|$|This report compares results {{computed}} by automatic differentiation (via ADIFOR) and by hand-coded derivatives for a numerically complicated <b>statistical</b> <b>code.</b> This report {{analyzes the}} types of discrepancies that were found and describes steps taken to mediate each of them. 1 Introduction This report describes {{the application of the}} ADIFOR 2. 0 automatic differentiation system[1, 2] to produce code to compute derivatives for a complicated statistical function: the log-likelihood for log-F distribution (LLDRLF) [3]. The ADIFOR system generated code to compute first and second derivative functions from the <b>statistical</b> function <b>code.</b> The authors of the <b>statistical</b> function <b>code</b> also "hand-coded" a version of the derivatives. The likelihood function calculation is exceptionally difficult to calculate numerically due to an exceptionally wide input domain[3, 4]. Consequently, the strategy for computing this function is complicated. One of five different methods (Extreme Value Method, Bratio M [...] ...|$|R
40|$|This work {{describes}} an FPGA {{implementation of a}} Microprogrammable Controller to perform lossless data compression based on the Huffman Algorithm. The Huffman Algorithm is a lossless <b>statistical</b> <b>coding</b> algorithm used in many modern compression systems, such as the JPEG and MPEG- 2 standards. The Microprogrammable Controller architecture was implemented in blocks, using the hardware description language VHDL and the Huffman microprogram was implemented using the microinstrutions of this architecture. The description and functionalities of the main blocks that compose its architecture are presented {{as well as the}} simulation and synthesis methodology. By changing the microcode, the controller can also implement other compression algorithms...|$|E
40|$|Binary {{arithmetic}} coding is, compression-wise, {{the most effective}} <b>statistical</b> <b>coding</b> method used in image and video compression. It is being used for compressing bi-level images (JBIG, JBIG 2, and MPEG- 4 shape coding) and is also being utilized (optionally) for coding of continuous-tone images (JPEG) and videos (H. 264). Despite its wide use, different arithmetic coders are incompatible {{with each other and}} application developers are faced with the difficult task of understanding and building each coder. We present a set of simple parameters {{that can be used to}} describe any binary arithmetic coder that is currently being deployed, and we also introduce a software tool for automatically generating C++/Java code for binary {{arithmetic coding}} according to the description. 1...|$|E
40|$|Existing lossy audio {{compression}} {{techniques such as}} MP 3, WMA and Ogg Vorbis, for example, demonstrate great success in providing compression ratios which successfully reduce the data size from the original sampled audio. These techniques employ psychoacoustic models and traditional <b>statistical</b> <b>coding</b> techniques to achieve data reduction. However, these methods do {{not take into account}} the perceived content of the audio, which is often particularly relevant in musical audio. In this paper, we present our research and development work completed to date, in producing a system for audio analysis, which will consider and exploit the repetitive nature of audio and the similarities which frequently occur in audio recordings. We demonstrate the feasibility and scope of the analysis system and consider the techniques and challenges that are employed to achieve data reduction...|$|E
5000|$|<b>Statistical</b> functions: <b>Coding</b> frequencies, cluster analysis, coding sequences, coding by variables.|$|R
40|$|Linear decompressors are the {{dominant}} methodology used in commercial test data compression tools. However, {{they are generally}} not able to exploit correlations in the test data, and thus the amount of compression {{that can be achieved}} with a linear decompressor is directly limited by the number of specified bits in the test data. The paper describes a scheme in which a non-linear decoder is placed between the linear decompressor and the scan chains. The nonlinear decoder uses statistical transformations that exploit correlations in the test data {{to reduce the number of}} specified bits that need to be produced by the linear decompressor. Given a test set, a procedure is presented for selecting a <b>statistical</b> <b>code</b> that effectively “compresses ” the number of specified bits (note that this is a novel and different application of <b>statistical</b> <b>codes</b> from what has been studied before and requires new algorithms). Results indicate that the overall compression can be increased significantly using a small non-linea...|$|R
40|$|We {{propose a}} new {{technique}} based on recursive finite state machines for tracking context {{to be used in}} a <b>statistical</b> <b>code</b> compression scheme for XML documents. We also study the tradeoffs between space and compression ratio, by observing the effects of either using or ignoring root to leaf contexts for textual content in the associated tree structures. The advantage of our scheme is that it is syntax aware and the compressor and decompressor can be generated automatically from the Document Type Definition(DTD). ...|$|R
40|$|We {{present a}} <b>statistical</b> <b>coding</b> {{framework}} that supports content analysis and retrieval in the compressed domain. An unsupervised learning approach based upon latent variable modeling is adopted {{to learn a}} collection, or mixture, of local linear subspaces that are designed for compression, while providing a probabilistic model of the source useful for inferring image content. The compressed bitstream is organized to enable the progressive decoding of the compressed data, such that the bitstream is only decompressed up to the level necessary to satisfy the query. We describe methods of extracting relevant features from the compressed representation that support query based on single and multiple example images, high level class categories such as people, and low-level features like particular colors and textures. Retrieval experiments have shown that this representation provides good inferencing with very little decompression. 1...|$|E
40|$|The {{components}} of most real-world patterns and pattern sequences carry redundant information. Most pattern classifiers (e. g. statistical classifiers and neural nets), however, work better if pattern components are non-redundant. Previous papers by this author introduced various unsupervised "neural" learning algorithms that transform patterns and pattern sequences into less redundant patterns without loss of information. Experiments with time series prediction tasks demonstrated that conventional gradient-based classification techniques can greatly {{benefit from this}} kind of unsupervised pre-processing. Encouraged by these earlier results, the paper at hand compares conventional data compression methods to a "neural" method. A neural net is {{used in conjunction with}} a <b>statistical</b> <b>coding</b> technique to compress text files without loss of information. The method is applied to short newspaper articles. The obtained compression ratios exceed those of the widely used asymptotically optimal L [...] ...|$|E
40|$|A new {{approach}} to data compression is developed and applied to mul-timedia content. This method separates messages into components suit-able for both lossless coding and ’lossy ’ or <b>statistical</b> <b>coding</b> techniques, compressing complex objects by separately encoding signals and noise. This is demonstrated by compressing the most significant bits of data exactly, since they are typically redundant and compressible, and either fitting a maximally likely noise function to the residual bits or compress-ing them using lossy methods. Upon decompression, the significant bits are decoded and added to a noise function, whether sampled from a noise model or decompressed from a lossy code. This results in compressed data similar to the original. Signals may be separated from noisy bits by considering derivatives of complexity in a manner akin to Kolmogorov’s approach or by empirical testing. The critical point separating the two represents the level beyond which compression using exact methods be...|$|E
40|$|The {{results are}} {{presented}} for a detailed study to determine a pseudo-optimum <b>statistical</b> <b>code</b> to be installed in a digital TV demonstration test set. Studies of source encoding were undertaken, using redundancy removal techniques in which the picture is reproduced within a preset tolerance. A method of source encoding, which preliminary studies show to be encouraging, is statistical encoding. A pseudo-optimum code was defined and the associated performance of the code was determined. The format was fixed at 525 lines per frame, 30 frames per second, as per commercial standards...|$|R
40|$|The 2 p − nd {{absorption}} {{structures in}} medium Z elements present a valuable benchmark for atomic models since they exhibit a complex dependence on temperature and density. For these transitions {{lying in the}} X-ray range, one observes a competition between the spin-orbit splitting and the broadening associated to the excitation of complex structures. Detailed opacity codes based on the HULLAC or FAC suites agree with the <b>statistical</b> <b>code</b> SCO; but in iron computations predict higher peak absorption than measured. An addition procedure on opacities calculated with detailed codes is proposed and successfully tested...|$|R
40|$|Abstract — Conventionally, the explore {{on quality}} {{attributes}} was set aside confidential inside the association that perform it {{by means of}} constricted black-box techniques. The appearance of open source software has transformed this image by permitting us to estimate both software products and procedures that yield them. This paper represent {{the results of a}} pilot case study intend to understand open source code evaluation and quality analysis by using statistical description of different source code metrics. Towards this finish we have measured quality characteristic of different versions of an application written in java. Index Terms — metrics, open source software, software measurement, <b>statistical</b> <b>code</b> evaluation and code qualit...|$|R
40|$|<b>Statistical</b> <b>coding</b> {{techniques}} {{have been used}} {{for a long time in}} lossless data compression, using methods such as Huffmans algorithm, arithmetic coding, Shannon’s method, Fano’s method, etc. Most of these methods can be implemented either statically or adaptively. In this paper, we show that although Fano coding is sub-optimal, it is possible to generate static Fano-based encoding schemes which are arbitrarily close to the optimal, i. e. those generated by Huffman’s algorithm. By taking advantage of the properties of the encoding schemes generated by this method, and the concept of code word arrangement, here present an enhanced version of the static Fano’s method, namely Fano. We formally analyze Fano by presenting some properties of the Fano tree, and the theory of list rearrangements. Our enhanced algorithm achieves compression ratios arbitrarily close to those of Huffman’s algorithm on files of the Calgary corpus and the Canterbury corpus. Chapter...|$|E
40|$|A compression/decompression scheme {{based on}} <b>statistical</b> <b>coding</b> is {{presented}} for {{reducing the amount of}} test data that must be stored on a tester and transferred to each core in a core-based design. The test vectors provided by the core vendor are stored in compressed form in the tester memory and transferred to the chip where they are decompressed and applied to the core. Given the set of test vectors for a core, a statistical code is carefully selected so that it satisfies certain properties. These properties guarantee that it can be decoded by a simple pipelined decoder (placed at the serial input of the core's scan chain) which requires very small area. Results indicate that the proposed scheme can use a simple decoder to provide test data compression near that of an optimal Huffman code. The compression results in a two-fold advantage since both test storage and test time are reduced. 1. Introduction One of the increasingly difficult challenges in testing systems-on-a-chip is dea [...] ...|$|E
40|$|A new {{lossless}} {{test vector}} compression scheme is presented which combines linear feedback shift register (LFSR) reseeding and <b>statistical</b> <b>coding</b> {{in a powerful}} way. Test vectors can be encoded as LFSR seeds by solving a system of linear equations. The solution space of the linear equations can be quite large. The proposed method takes advantage of this large solution space to find seeds that can be efficiently encoded using a statistical code. Two architectures for implementing LFSR reseeding with seed compression are described. One configures the scan cells themselves to perform the LFSR functionality while the other uses a new idea of "scan windows" to allow {{the use of a}} small separate LFSR whose size is independent of the number of scan cells. The proposed scheme can be used either for applying a fully deterministic test set or for mixed-mode built-in self-test (BIST), and it can be used in conjunction with other variations of LFSR reseeding that have been previously proposed to further improve the encoding efficiency...|$|E
40|$|In this paper, we {{describe}} Stat-Tool, a new object-oriented framework for statistical analysis. Stat-Tool {{can be seen}} as an independent stream-processor, dynamically performing analyses on sample-streams. This client/stream-processor architecture allows to easily extend the tool (adding a new statistical capability simply consists in developing and registering a separate module) and to clearly dissociate the <b>statistical</b> <b>code</b> from the simulation code. In order to cope with today's simulations, StatTool can dynamically distribute the analyses on heterogeneous computers. Performances are further increased with buffering and mapping techniques. Stat-Tool is implemented as a C or C++ library, and comes in 2 versions: one for local execution only and the distributed one. The distributed version uses PVM 3 for communication...|$|R
40|$|Freely {{available}} online. In {{a recent}} article Heinemann et al. (2013) focused “on the US staple crop agrobiodiversity, particularly maize” using {{the contrast between the}} yield of Western Europe and United States as a proxy for the comparison between genetically modified (GM) maize versus non-GM maize. They found no yield benefit from using GM maize when comparing the United States to Western Europe. In addition, Heinemann et al. contrasted wheat yields across United States and Western Europe to highlight the superiority of the European biotechnological package from a sustainability viewpoint. I am compelled to comment on two aspects that led the authors to draw incorrect conclusions on these issues. My <b>statistical</b> <b>code</b> and data are available as supplementary material...|$|R
40|$|Emission of a {{particles}} accompanying fusion-fission {{processes in}} the Ar- 40 + Th- 232 reaction at E(Ar- 40) = 365 MeV was studied {{in a wide range}} of in-fission-plane and out-of-plane angles. The exact determination of the emission angles of both fission fragments combined with the time-of-flight measurements allowed us to reconstruct the complete kinematics of each ternary event. The coincident energy spectra of a particles were analyzed by using predictions of the energy spectra of the <b>statistical</b> <b>code</b> CASCADE. The analysis clearly demonstrates emission from the composite system prior to fission, emission from fully accelerated fragments after fission, and also emission during scission. The analysis is presented for both symmetric and asymmetric fission. The results have been analyzed using a time-dependent <b>statistical</b> decay <b>code</b> and confronted with dynamical calculations based on a classical one-body dissipation model. The observed near-scission emission is consistent with evaporation from a dinuclear system just before scission and evaporation from separated fragments just after scission. The analysis suggests that the time scale of fission of the hot composite systems is long (about 7 X 10 (- 20) s) and the motion during the descent to scission almost completely damped...|$|R
40|$|Neural {{networks}} are promising tools for data compression without loss of information. We combine predictive neural nets and <b>statistical</b> <b>coding</b> techniques to compress text files. We apply our methods to certain short newspaper articles and obtain compression ratios exceeding those of widely used Lempel-Ziv algorithms (which {{are the basis}} of the UNIX functions "compress" and "gzip"). Our methods' main disadvantage is that they are about three orders of magnitude slower than standard methods. I. Introduction Text compression is important (e. g. [1]). It is cheaper to communicate compressed text files instead of original text files. Moreover, compressed files are cheaper to store. For such reasons various text encoding algorithms have been developed. A text encoding algorithm takes a text file and generates a shorter compressed file from it. The compressed file contains all the information necessary to restore the original file, which can be done by calling the corresponding decoding algor [...] ...|$|E
40|$|International audienceWe propose an {{efficient}} lossless compression scheme for still images based on arithmetic coding. The scheme presents a novel adaptive arithmetic coding that updates the probabilities of pixels only after detecting the last occurrence of each pixel and then removes the redundancy {{from the original}} image effectively. The proposed approach has interestingly low computational complexity. In addition, unlike other <b>statistical</b> <b>coding</b> techniques, arithmetic cod- ing in the proposed scheme is not solely dependent on the pixel probability distribution {{but also on the}} image block sorting. The pro- posed method is compared to both static and adaptive order- 0 mod- els while taking into account compression ratios and processing time. Experimental results, based on a set of 100 gray-level images, demonstrate that the proposed scheme gives mean compression ratios that are 5. 5 % higher than those by the conventional arithmetic encoders as well as significantly faster than the order- 0 adaptive arithmetic coding...|$|E
40|$|In {{a recent}} paper, {{it was shown}} in detail {{that in the case}} of {{orthonormal}} and biorthogonal filter banks we can convolve two signals by directly convolving the subband signals and combining the results. In this paper, we further generalize the result. We also derive the <b>statistical</b> <b>coding</b> gain for the generalized subband convolver. As an application, we derive a novel low sensitivity structure for FIR filters from the convolution theorem. We define and derive a deterministic coding gain of the subband convolver over direct convolution for a fixed wordlength implementation. This gain serves as a figure of merit for the low sensitivity structure. Several numerical examples are included to demonstrate the usefulness of these ideas. By using the generalized polyphase representation, we show that the subband convolvers, linear periodically time varying systems, and digital block filtering can be viewed in a unified manner. Furthermore, the scheme called IFIR filtering is shown to be a special case of the convolver...|$|E
40|$|Topics from {{contemporary}} mathematics, their development, applications, and role in society. Some typical topics, {{to be chosen}} by the instructor: graph theory, critical path analysis, <b>statistical</b> inference, <b>coding,</b> game theory, and symmetry. 4 lectures. Prerequisite: Passing score on ELM examination, or an ELM exemption, or credit in MATH 104...|$|R
40|$|Within the {{framework}} of the Lanzhou quantum molecular dynamics (LQMD) transport model, the nuclear fragmentation induced by low-energy antiprotons has been investigated thoroughly. A coalescence approach is developed for constructing the primary fragments in phase space. The secondary decay process of the fragments is described by the well-known <b>statistical</b> <b>code.</b> It is found that the localized energy released in antibaryon-baryon annihilation is deposited in a nucleus mainly via pion-nucleon collisions, which leads to the emissions of pre-equilibrium particles, fission, evaporation of nucleons and light fragments etc. The strangeness exchange reactions dominate the hyperon production. The averaged mass loss increases with the mass number of target nucleus. A bump structure in the domain of intermediate mass for heavy targets appears owing to the contribution of fission fragments. Comment: 9 pages, 12 figure...|$|R
40|$|The cross {{sections}} of 90 Zr(n, 2 n) 89 Zr {{reaction in the}} neutron energy range 14. 10 - 14. 71 MeV {{was measured by the}} activation technique. Neutrons were produced via 3 H(d, n) 4 He reaction using solid tritium target at J- 25 neutron generator. Activity of the reaction product was determined by measuring the gamma counts of the product nuclei using high resolution HPGe detector gamma ray spectrometry system. Neutron flux at each sample position was determined by using 27 Al(n, α) 24 Na monitor reaction. The measured cross section values along with the literature data were plotted as a function of neutron energy to get the excitation function of the reaction. A theoretical calculation was also performed to produce the excitation function of the investigated reaction using <b>statistical</b> <b>code</b> SINCROS-II...|$|R
