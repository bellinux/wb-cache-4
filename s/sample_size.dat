10000|10000|Public
5|$|Authors Lisa R. Baba, Jacqueline M. McGrath, and Jiexin Liu {{examined}} use of vibration {{delivery to}} infants to mitigate pain while doing heel stick procedures in a 2010 {{article for the}} Journal of Perinatal & Neonatal Nursing. Their <b>sample</b> <b>size</b> included 20 babies of age 35 weeks or more, and they tracked their subjects' levels of pain on the Neonatal Infant Pain Scale while giving mechanical vibration to alleviate discomfort for a heel stick procedure. Vibration was delivered using the Magic Wand, which they bought through Vibratex. They set the Magic Wand to the highest setting {{and placed it on}} the heel of the infant for five seconds before administering the heel stick procedure. Their results found that oscillation sensations delivered to newborns who had previously had discomfort from neonatal heel pricks were able to deliver some relief. They wrote that additional research was indicated in the form of a randomized clinical trial with a greater <b>sample</b> <b>size</b> of newborns.|$|E
5|$|Taloustutkimus {{carried out}} monthly {{telephone}} polls on party popularity for the Finnish Broadcasting Company. Since April 2007, the monthly <b>sample</b> <b>size</b> has varied between 2,900–3,900 with {{a margin of}} error of about ±1.8%. (Polling does not include Åland as it has its own party system.) However, there were also other less frequent opinion polls.|$|E
5|$|American Cocker Spaniels in UK and USA/Canada surveys had {{a median}} {{lifespan}} of about 10 to 11 years, {{which is on}} {{the low end of the}} typical range for purebred dogs, and one to two years less than other breeds of their size. The larger English Cocker Spaniel typically lives about a year longer than the American Cocker Spaniel. In a 2004 UK Kennel Club survey, the most common causes of death were cancer (23%), old age (20%), cardiac (8%), and immune-mediated (8%). In a 2003 USA/Canada Health Survey with a smaller <b>sample</b> <b>size,</b> the leading causes of death were cancer, hepatic disease, and immune-mediated.|$|E
40|$|We {{prove the}} {{following}} conjecture of Narayana: {{there are no}} nontrivial dominance refinements of the Smirnov two-sample test {{if and only if}} the two <b>sample</b> <b>sizes</b> are relatively prime. We also count the number of natural significance levels of the Smirnov two-sample test in terms of the <b>sample</b> <b>sizes</b> and relate this to the Narayana conjecture. In particular, Smirnov tests with relatively prime <b>sample</b> <b>sizes</b> turn out to have many more natural significance levels than do Smirnov tests whose <b>sample</b> <b>sizes</b> are not relatively prime (for example, equal <b>sample</b> <b>sizes)</b> ...|$|R
3000|$|... grow rapidly {{when the}} <b>sample</b> <b>sizes</b> increase. Therefore, we can, so far, only target small <b>sample</b> <b>sizes</b> in our work.|$|R
40|$|Locally optimal {{tests of}} {{homogeneity}} of a binomial series against a general class of alternatives are constructed for the cases (i) parameter specified, arbitrary <b>sample</b> <b>sizes,</b> (ii) parameter not specified, equal <b>sample</b> <b>sizes.</b> A test is proposed for unequal <b>samples</b> <b>sizes</b> with the parameter unspecified. 1...|$|R
5|$|Analysis of {{mitochondrial}} DNA, from {{a limited}} number of specimens, suggested that there was gene flow within the continental Asian populations until the 20th century reductions in range, and that Australia was colonized only in the Late Pleistocene, some 35,000 years ago. This has been corroborated by nDNA microsatellite analyses with four times the <b>sample</b> <b>size.</b> This study further suggests that the Australian population is quite inbred. As there exists the possibility of (limited) hybridization with the genetically distinct brolga, the Australian sarus crane can be expected to be an incipient species.|$|E
5|$|MacDougall stated his {{experiment}} {{would have}} to be repeated many times before any conclusion could be obtained. The experiment is widely regarded as flawed and unscientific due to the small <b>sample</b> <b>size,</b> the methods used, as well as the fact only one of the six subjects met the hypothesis. The case has been cited as an example of selective reporting. Despite its rejection within the scientific community, MacDougall's experiment popularized the concept that the soul has weight, and specifically that it weighs 21 grams.|$|E
5|$|The maximum spacing {{estimator}} is {{a consistent}} estimator {{in that it}} converges in probability to the true value of the parameter, θ0, as the <b>sample</b> <b>size</b> increases to infinity. The consistency of maximum spacing estimation holds under much more general conditions than for maximum likelihood estimators. In particular, {{in cases where the}} underlying distribution is J-shaped, maximum likelihood will fail where MSE succeeds. An example of a J-shaped density is the Weibull distribution, specifically a shifted Weibull, with a shape parameter less than 1. The density will tend to infinity as x approaches the location parameter rendering estimates of the other parameters inconsistent.|$|E
50|$|To {{improve the}} quality of replications, larger <b>sample</b> <b>sizes</b> than those used in the {{original}} study are often needed. Larger <b>sample</b> <b>sizes</b> are needed because estimates of effect sizes in published work are often exaggerated due to publication bias and large sampling variability associated with small <b>sample</b> <b>sizes</b> in an original study. Further, using significance thresholds usually leads to inflated effects, because particularly with small <b>sample</b> <b>sizes,</b> only the largest effects will become significant.|$|R
40|$|Abstract- The double {{sampling}} (DS) control chart can efficaciously reduce <b>sample</b> <b>sizes</b> and increase performance of process monitoring. Hsu [Int. j. prod. res., vol. 42, no. 5, pp. 1043 - 1047] {{mentioned that the}} statistical design model of DS X control chart could merely minimize <b>sample</b> <b>sizes</b> during in-control process monitoring but fail to decrease <b>sample</b> <b>sizes</b> during detection of process shifts. In this study, with minimization of <b>sample</b> <b>sizes</b> for both in-control process and out-of-control process, a multi-objective programming method and genetic algorithm are proposed for statistical designs of DS X control chart. In comparison with both statistical design models, it is quite obvious that our model can effectively lower <b>sample</b> <b>sizes</b> of two process situations...|$|R
5000|$|... and {{identifies}} {{any difference}} between two means that {{is greater than the}} expected standard error. The confidence coefficient for the set, when all <b>sample</b> <b>sizes</b> are equal, is exactly [...] for any [...] For unequal <b>sample</b> <b>sizes,</b> the confidence coefficient is greater than 1 − α. In other words, the Tukey method is conservative when there are unequal <b>sample</b> <b>sizes.</b>|$|R
5|$|This is {{believed}} to be due to the increased exposure of mucosal tissue to potential infection sites. Transmission risk from infected female to male is around 4–5% annually. Suppressive antiviral therapy reduces these risks by 50%. Antivirals also help prevent the development of symptomatic HSV in infection scenarios, meaning the infected partner will be seropositive but symptom-free by about 50%. Condom use also reduces the transmission risk significantly. Condom use is much more effective at preventing male-to-female transmission than vice versa. Previous HSV-1 infection may reduce the risk for acquisition of HSV-2 infection among women by a factor of three, although the one study that states this has a small <b>sample</b> <b>size</b> of 14 transmissions out of 214 couples.|$|E
25|$|The {{results of}} the INAA study were later defended in March 2006 in two {{articles}} in Latin American Antiquity, in particular contrasting the <b>sample</b> <b>size</b> of the INAA study (roughly 1000) with the <b>sample</b> <b>size</b> of the petrography analysis (20).|$|E
25|$|The United States {{government}} does not collect religious data in its census. The survey below, the American Religious Identification Survey (ARIS) 2008, was a random digit-dialed telephone survey of 54,461 American residential households in the contiguous United States. The 1990 <b>sample</b> <b>size</b> was 113,723; 2001 <b>sample</b> <b>size</b> was 50,281.|$|E
30|$|The <b>sample</b> <b>sizes</b> of lab {{experiments}} are criticized {{as being too}} small, although this is refuted such that <b>sample</b> <b>sizes</b> are stated to adequately correspond to this method and thus yield valid assertions.|$|R
30|$|Overall speaking, {{we would}} {{recommend}} {{the use of}} moment-based estimator β̃_ 1 for the estimation of the parameter β for any <b>sample</b> <b>sizes.</b> For small <b>sample</b> <b>sizes,</b> we would suggest the use of non-linear least-squared method by setting β = β̃_ 2 (i.e., Method 4) to estimate the parameters α and ν. For moderate to large <b>sample</b> <b>sizes,</b> the use of likelihood equations by setting β = β̃_ 2 (i.e., Method 2) is recommended.|$|R
50|$|A {{large amount}} of {{computing}} resources is required to compute exact probabilities for the Kruskal-Wallis test. Existing software only provides exact probabilities for <b>sample</b> <b>sizes</b> less than about 30 participants. These software programs rely on asymptotic approximation for larger <b>sample</b> <b>sizes.</b> Exact probability values for larger <b>sample</b> <b>sizes</b> are available. Spurrier (2003) published exact probability tables for samples as large as 45 participants. Meyer and Seaman (2006) produced exact probability distributions for samples as large as 105 participants.|$|R
25|$|This poll <b>sample</b> <b>size</b> is 15,330. The {{margin of}} error is 0.8%.|$|E
25|$|For large <b>sample</b> <b>size,</b> , {{there is}} little {{difference}} between these various expressions.|$|E
25|$|The {{intersection}} of the column and row is the minimum <b>sample</b> <b>size</b> required.|$|E
40|$|AbstractKiefer {{considered}} the asymptotics of q-sample Cramer-Von Mises statistics for a fixed q and <b>sample</b> <b>sizes</b> tending to infinity. For univariate observations, McDonald proved the asymptotic normality of these statistics when q goes to infinity while the <b>sample</b> <b>sizes</b> stay fixed. Here we define {{a class of}} multivariate randomness statistics that generalizes the class considered by McDonald. We also prove the asymptotic normality of such statistics when the <b>sample</b> <b>sizes</b> stay fixed while q tends to infinity...|$|R
40|$|Designing a {{tightened}} normal tightened {{sampling plan}} requires <b>sample</b> <b>sizes</b> and acceptance number with switching criterion. An evolutionary algorithm, the genetic algorithm, {{is designed to}} identify optimal <b>sample</b> <b>sizes</b> and acceptance number of a tightened normal tightened sampling plan for a specified consumer’s risk, producer’s risk, and switching criterion. Optimal <b>sample</b> <b>sizes</b> and acceptance number are obtained by implementing the genetic algorithm. Tables are reported for various choices of switching criterion, consumer’s quality level, and producer’s quality level...|$|R
40|$|Kiefer {{considered}} the asymptotics of q-sample Cramer-von Mises statistics for a fixed q and <b>sample</b> <b>sizes</b> tending to infinity. For univariate observations, McDonald proved the asymptotic normality of these statistics when q goes to infinity while the <b>sample</b> <b>sizes</b> stay fixed. Here we define {{a class of}} multivariate randomness statistics that generalizes the class considered by McDonald. We also prove the asymptotic normality of such statistics when the <b>sample</b> <b>sizes</b> stay fixed while q tends to infinity. ...|$|R
25|$|Formulas, tables, {{and power}} {{function}} charts {{are well known}} approaches to determine <b>sample</b> <b>size.</b>|$|E
25|$|In {{both cases}} the {{data suggest that the}} null {{hypothesis}} is false (that is, the coin is not fair somehow), but changing the <b>sample</b> <b>size</b> changes the p-value. In the first case, the <b>sample</b> <b>size</b> is not large enough to allow the null hypothesis to be rejected at the 0.05 level (in fact, the p-value can never be below 0.05 for the coin example).|$|E
25|$|This {{index is}} {{relatively}} {{independent of the}} population density but {{is affected by the}} <b>sample</b> <b>size.</b>|$|E
40|$|This paper {{investigates the}} utility of {{sampling}} as an evaluation-relaxation technique in genetic algorithms (GAs). In many real-world applications, sampling {{can be used to}} generate a less accurate, but computationally inexpensive fitness evaluator to speed GAs up. This pa-per focuses on the problem of polynomial regression as an example of problems with positive dependency among genes. Via statistical analysis of the noise introduced by sampling, this paper develops facet-wise models for the optimal <b>sampling</b> <b>size,</b> and these models are empir-ically verified. The results show that when the population is <b>sized</b> properly, small <b>sampling</b> <b>sizes</b> are preferred for most applications. When a fixed population size is adopted, which is usually the case in real-world applications, an optimal <b>sampling</b> <b>size</b> exists. If the <b>sampling</b> <b>size</b> is too small, the sampling noise increases, and GAs would perform poorly because of an insufficiently large population. If the <b>sampling</b> <b>size</b> is too large, the GA would spend too much time in fitness calculation and cannot perform well either within limited run duration. ...|$|R
50|$|Cohen's d is {{frequently}} used in estimating <b>sample</b> <b>sizes</b> for statistical testing. A lower Cohen's d indicates {{the necessity of}} larger <b>sample</b> <b>sizes,</b> and vice versa, as can subsequently be determined together with the additional parameters of desired significance level and statistical power.|$|R
5000|$|Also {{note that}} the <b>sample</b> <b>sizes</b> must be equal when using the studentized range approach. [...] is the {{standard}} deviation of the entire design, not just that {{of the two groups}} being compared. It is possible to work with unequal <b>sample</b> <b>sizes.</b> In this case, one has to calculate the estimated standard deviation for each pairwise comparison as formalized by Clyde Kramer in 1956, so the procedure for unequal <b>sample</b> <b>sizes</b> is sometimes referred to as the Tukey-Kramer method which is as follows: ...|$|R
25|$|This {{demonstrates}} that in interpreting p-values, one must also know the <b>sample</b> <b>size,</b> which complicates the analysis.|$|E
25|$|Important {{examples}} include the sample variance and sample standard deviation. Without Bessel's correction (using the <b>sample</b> <b>size</b> n instead of the degrees of freedom n−1), these are both negatively biased but consistent estimators. With the correction, the corrected sample variance is unbiased, while the corrected sample standard deviation is still biased, but less so, and both are still consistent: the correction factor converges to 1 as <b>sample</b> <b>size</b> grows.|$|E
25|$|This is {{the square}} of the {{coefficient}} of variation divided by N - 1 where N is the <b>sample</b> <b>size.</b>|$|E
40|$|Our {{research}} investigation {{focuses on}} the role of humans in supplying corrected examples in active learning cycles, an important aspect of deploying active learning in practice. In this paper, we dis-cuss sampling strategies and <b>sampling</b> <b>sizes</b> in set-ting up an active learning system for human ex-periments in the task of content analysis, which involves labeling concepts in large volumes of text. The cost of conducting comprehensive hu-man subject studies to experimentally determine the effects of <b>sampling</b> <b>sizes</b> and <b>sampling</b> <b>sizes</b> is high. To reduce those costs, we first applied an active learning simulation approach to test the ef-fect of different sampling strategies and <b>sampling</b> <b>sizes</b> on machine learning (ML) performance in order to select a smaller set of parameters to be evaluated in human subject studies. ...|$|R
3000|$|How {{can search}} {{algorithms}} be improved to incorporate {{different kinds of}} background knowledge, search over different classes of causal models, run faster, handle more variables and larger <b>sample</b> <b>sizes,</b> be more reliable at small <b>sample</b> <b>sizes,</b> and produced output that is as informative as possible? [...]...|$|R
40|$|Minimum <b>sample</b> <b>sizes</b> are {{recommended}} for conducting {{exploratory factor analysis}} on dichotomous data. A Monte Carlo simulation was conducted, varying the level of communalities, number of factors, variable-to-factor ratio and dichotomization threshold. <b>Sample</b> <b>sizes</b> were identified based on congruence between rotated population and sample factor loadings...|$|R
