12|97|Public
40|$|Abstract: This paper {{presents}} different {{concepts and}} procedures for ground measuring techniques. A few of the techniques that are discussed are the measuring of resistance to remote earth for a single earth electrode or for an entire earth electrode system, and also the resistivity of the local soil using an earth <b>system</b> <b>tester.</b> 1...|$|E
40|$|An {{investigation}} of the radiation from 5 different phones next to two head phantoms proposed for radiation measurements {{has been carried out}} and the radiation has been evaluated thru the Total Radiated Power, the Total Receiver Sensitivity and the Mean Effective Gain. The investigation is based on measurements of the farfield patterns measured in an anechoic room using a <b>system</b> <b>tester</b> to avoid any disturbance of the radiation patterns...|$|E
3000|$|According {{to these}} results, for the Canberra distance, the most {{critical}} feature (the one that most disapproved) was the Area, which significantly influenced the results when all three extractors were applied in unison. Note how in this result, in the graphic oracle approach, each extractor is individually important in the comparison. This means that the evaluator or <b>system</b> <b>tester</b> can set the characteristics that are important for a satisfactory segmentation and use them in the evaluation. In fact, the empirical results showed that, excluding only the Area extractor, the number of approved images increased to 21 when the reference set G [...]...|$|E
50|$|AAI’s test {{systems are}} used by every branch of the U.S. military. The Joint Systems Electronic Combat <b>Systems</b> <b>Tester</b> {{is part of the}} DoD’s family of testers and {{currently}} supports flight-line electronic warfare testing for several U.S. military aircraft.|$|R
40|$|Performance {{analysis}} tools for enterprise distributed real-time and embedded (DRE) systems require instrumenting heterogeneous sources (such as application- and system-level {{hardware and software}} resources). Traditional techniques for software instrumentation of such systems, however, are tightly coupled to system design and metrics of interest. It is therefore hard for <b>system</b> <b>testers</b> to increase their knowledge base and analytical capabilities for enterprise DRE system performance using existing instrumentation techniques when metrics of interest are not known during initial system design. This paper provides two contributions to research on software instrumentation for enterprise DRE systems. First, it presents OASIS, which is service-oriented middleware for instrumenting enterprise DRE systems to collect and extract metrics without design time knowledge of which metrics are collected. Second, this paper empirically evaluates OASIS {{in the context of}} a representative enterprise DRE system from the domain of shipboard computing. Results from applying OASIS to a representative enterprise DRE system show that its flexibility enables DRE <b>system</b> <b>testers</b> to precisely control the overhead incurred via instrumentation. 1...|$|R
5000|$|... "We don't want {{to become}} a <b>system</b> of <b>testers</b> and not be teachers." [...] December 4, 1988 ...|$|R
40|$|After over {{a decade}} of use, design {{patterns}} continue to find new areas of application. In previous work, we presented a contract formalism for specifying patterns precisely, and showed how {{the use of the}} formalism can amplify the benefits of patterns. In this paper, our goal is to enable practitioners to test whether their systems, as implemented, meet the requirements, as specified in the pattern contracts, corresponding to the correct usage of the patterns underlying the systems’ designs. In our testing approach, corresponding to each design pattern, there is a set of what we call pattern test case templates (PTCTs). A PTCT codifies a reusable test case structure designed to identify defects associated with applications of the particular pattern. The test assertions in the PTCT are based on the requirements specified in the appropriate pattern contract. Next we present a process using which, given any system designed using the pattern, the <b>system</b> <b>tester</b> can generate a test suite from the PTCTs for that pattern {{that can be used to}} test the system for bugs in the implementation of the particular pattern. The process allows the <b>system</b> <b>tester</b> to tailor the test suite the needs of the individual system by specifying a set of specialization rules that are designed to reflect the structure and the scenarios in which the defects codified in the PTCTs are likely to manifest themselves in the particular system...|$|E
40|$|Abstract Systems {{constructed}} from components, including distributed systems, {{consist of a}} number of elements that interact with each other. As the number of network elements or interchangeable components for each network element increases, the trade off that the <b>system</b> <b>tester</b> faces is the thoroughness of test configuration coverage, versus availability of limited resources (time and budget). An approach to resolving this trade off is to determine a minimal set of test configurations that test each pair-wise combination of components. This goal gives a well-defined, cost-effective level of test coverage, with a reduced number of system configurations. To select such a set of test configurations, we show how to apply the method of covering arrays, and improve on previous results. Keywords: System testing, Test coverage, Interactions, Component-based testing 1...|$|E
40|$|Verification and Validation (V&V) is {{a series}} of activities, {{technical}} and managerial, which performed by <b>system</b> <b>tester</b> not the system developer in order to improve the system quality, system reliability and assure that product satisfies the users operational needs. Verification is the assurance that the products of a particular development phase are consistent with the requirements of that phase and preceding phase(s), while validation is the assurance that the final product meets system requirements. an outside agency can be used to performed V&V, which is indicate by Independent V&V, or IV&V, or by a group within the organization but not the developer, referred to as Internal V&V. Use of V&V often accompanies testing, can improve quality assurance, and can reduce risk. This paper putting guidelines for performing V&V of Multi-Agent Systems (MAS). Comment: 10 pages, 3 figure...|$|E
40|$|Abstract: This paper {{presents}} a vulnerability analysis course developed for <b>system</b> <b>testers</b> and the experiences gained from it. The {{aim of this}} course is to educate testers {{in the process of}} finding security weaknesses in products. It covers the four steps of a vulnerability analysis: reconnaissance, research and planning, mounting attacks, and assessment. The paper describes in detail ten different laboratory assignments conducted within the course. For each experiment, an overview and a description on how to run the assignment together with the expected knowledge obtained are presented. In addition, a course evaluation and lessons learned are also provided...|$|R
40|$|Failures in Internet-based {{services}} often {{stem from}} {{problems in the}} underlying enterprise infrastructures that provide the services. We propose an environment called Hercules to test the reliability and performance of large-scale enterprise infrastructures. Hercules contains two major components: (1) a methodology to build a virtual testbed that can accurately emulate any infrastructure topology, as well as simulate failures, attacks {{and other types of}} stresses on the infrastructure to identify defects and bottlenecks; and (2) a realistic traffic model and a tool to automatically generate different traffic loads based on the model. <b>Systems</b> <b>testers</b> can use Hercules to evaluate whether an existing or proposed enterprise infrastructure provides adequate support to its targeted applications. ...|$|R
40|$|The main {{method of}} testing {{earthing}} system performance {{involves the use of}} an off-frequency injection current produced by a voltage source. While this provides a sufficient testing tool for a number of systems, deficiencies such as noise susceptibility, restricted power capabilities and unstable current present significant challenges to earthing <b>system</b> <b>testers.</b> This paper outlines a novel application of inverter technology to this field. A review of control schemes was undertaken which resulted in the design of a robust method which is stable over a range of system conditions. A hardware prototype design that combined the benefits of a voltage source converter and the aforementioned control scheme was constructed. Furthermore, simulation and testing, in both the laboratory and on-site, demonstrates that the prototype outperforms the existing industry standard...|$|R
40|$|Distributed systems {{consist of}} a number of network {{elements}} that interact with each other. As the number of network elements and interchangeable components for each network element increases, the trade off that the <b>system</b> <b>tester</b> faces is the thoroughness of test configuration coverage, versus limited resources of time and expense that are available. An approach to resolving this trade off is to determine a set of test configurations that test each pair-wise combination of network components. This goal gives a well-defined level of test coverage, with a reduced number of system configurations. To select such a set of test configurations, we show how to apply the method of orthogonal Latin squares, from the design of balanced statistical experiments. Since the theoretical treatment assumes constraints that may not be satisfied in practice, we then show how to adapt this approach to realistic application constraints. Motivation A common source of system faults are unexpected interaction [...] ...|$|E
40|$|IT {{companies}} {{are confronted with}} a huge, growing Off-the-Shelf (OTS) marketplace containing incredibly large amount of diverse, partial, ephemeral, sometimes tacit and not always trustable information about OTS components. Our empirical studies illustrate that different users, such as OTS evaluator, OTS integrator, <b>system</b> <b>tester,</b> and system maintainer, need different information and knowledge about OTS components when facing different scenarios. Although a few {{companies are}} using mini-wikis internally to manage and share OTS related knowledge, knowledge sharing across companies is rare. Searching existing web portals or sites of OTS components is time consuming and often does not bring systematic knowledge. This position paper proposes role-based wikis to organize OTS components related knowledge in a systematic way. One motivation of constructing such a wiki is to provide necessary knowledge for different users to help them reduce risks. Another motivation {{is to provide a}} global centralized platform, so that OTS users can systematically share their knowledge across organizations. 1...|$|E
40|$|Abstract—Measuring the {{software}} performance under load {{is an important}} task in both test and production of a software development. In large scale systems, {{a large amount of}} metrics and usage logs are analyzed to measure the performance of {{the software}}. Most of these metrics are analyzed by aggregating across all users to get general results for the scenario, i. e., how individual users have perceived the performance is typically not considered in software performance research and practice. To analyze a software’s performance, user’s perception of software performance metrics should be considered along with the scenario-centric perspective of <b>system</b> <b>tester</b> or operator. In our empirical study, we analyzed the impact of per-formance on individual users to see if performance analysis results based on the user’s perception is really different from the scenario-centric (aggregated) one. Case studies on common use case scenarios in two commercial large telecommunication systems and one open source performance benchmark show scenarios where user-centric software performance analysis was able to identify performance issues that would be in-visible in a scenario-centric analysis. We find that the user-centric approach does not replace the existing scenario-centric performance analysis approaches, but complements them by identifying more performance issues. Keywords-Software performance, user, metric, load test. I...|$|E
40|$|A {{letter report}} {{issued by the}} General Accounting Office with an {{abstract}} that begins "The armed services have had problems for years with their ability to adequately test their electronic combat systems. The success of the new Joint Service Electronic Combat <b>Systems</b> <b>Tester</b> Program in providing improved test capability is a positive development. Because the tester has identified many more faults in the F- 15 C and F/A- 18 C electronic combat systems than has the current test equipment, existing readiness, logistics, and maintenance problems with such systems could worsen. However, pilots would at least have greater knowledge about the readiness and reliability of their self-protection systems and their need for support from specialized aircraft designed to suppress enemy air defenses. GAO believes that {{it makes sense for}} the Air Force and Navy to consider using the new test equipment on their non-fighter aircraft. ...|$|R
40|$|Decomposition is used {{to manage}} system complexity, but is {{problematic}} for emergent properties such as system safety. Previously, we introduced Indirect Control Path Analysis (ICPA) for elaborating system safety goals in composite systems. We now provide mathematical definitions of emergent and composable system behaviors {{in the context of}} formal specifications and ICPA, and identify useful special cases in which partial decomposition of emergent safety goals is possible. We apply ICPA to a semi-autonomous automotive system to identify safety goals for key subsystems, and then monitor the system and subsystem goals at run-time in an implementation of the vehicle. Although false negatives at the subsystem level indicate the subgoals do not fully compose the original safety goal, some systemlevel goal violations are detected by subsystem monitors. In addition, monitoring at both the system and subsystem level has identified certain safety-related errors that may be imperceptible to <b>system</b> <b>testers.</b> 1...|$|R
40|$|Issues {{in testing}} {{distributed}} component-based systems are discussed. Differences in testing such systems and other systems are identified. Several limitations and {{shortcomings of the}} existing test methodologies are also identified and a new methodology proposed. Keywords: CORBA, DCOM, component-based distributed systems, fault-tolerance, Java RMI, test adequacy, test methodology. 1 Introduction Testing software systems is a complex problem in itself. With the increasing trend in using distributed software, the task of testing becomes even more complicated. The scalability of testing methodologies and development of testing tools need {{to keep up with}} new technologies such as CORBA, DCOM and Java RMI. The process of testing is further complicated by the use of COTS components in the <b>systems.</b> <b>Testers</b> need to test the behavior of such components in systems even if the components have been tested before. Sometimes the components that are reused may not have been designed for systems [...] ...|$|R
40|$|Design {{patterns}} are used {{extensively in the}} design of software systems. Patterns codify effective solutions for recurring design problems and allow software engineers to reuse these solutions, tailoring them appropriately to their particular applications, rather than reinventing them from scratch. In this paper, we consider the following question: How can system designers and implementers test whether their systems, as implemented, are faithful to the requirements of the patterns used in their design? A key consideration underlying our work is that the testing approach should enable us, in testing whether a particular pattern P has been correctly implemented in different systems designed using P, to reuse the common parts of this effort rather than having to do it from scratch for each system. Thus in the approach we present, corresponding to each pattern P, there is a set of pattern test case templates (PTCTs). A PTCT codifies a reusable test case structure designed to identify defects associated with applications of P in all systems designed using P. Next we present a process using which, given a system designed using P, the <b>system</b> <b>tester</b> can generate a test suite from the PTCTs for P {{that can be used to}} test the particular system for bugs in the implementation of P in that system. This allows the tester to tailor the PTCTs for P to the need...|$|E
40|$|Systems {{constructed}} from components, including distributed systems, {{consist of a}} number of elements that interact with each other. When a system is integrated, there may be undesired interactions among those components that cause system failures. There are two complementary problems in testing a software system. The first problem is to create a test suite, given a description of the expected behaviour of a system configuration. The second problem is to deal with a large number of distinct test configurations. We investigate the second problem in this thesis: the situation when there are various system parameters, each of which can take on a value from a discrete set. The trade-off that the <b>system</b> <b>tester</b> faces is the thoroughness of test configuration coverage, versus availability of limited resources. We introduce a coverage measure that can provide a basis for determining a set of configurations with "sufficient" coverage, or for evaluation of a set of test configurations that already exists. This thesis addresses the problem of testing interactions among components of a software system: the "interaction test coverage" problem. We formally define this problem, and give it a set-theoretic framework. This is done through the introduction of an "interaction element," which becomes the unit of test coverage. The problem is compared to, and distinguished from, the minimum set cover problem and the { 0, 1 } integer programming problem. As a result, the status of the NP-completeness of this problem remains open. Methods from statistical experimental design are introduced, and applied to the problem of generating a set of configurations that achieve coverage of all pair-wise combinations of parameter values. We present a fast, deterministic algorithm to generate such a set of test configurations. The method is compared with other methods, and shown to produce fewer configurations in most situations. The number of configurations generated is logarithmic in the number of parameters, and polynomial in the number of values per parameter. As a result, the number of configurations is usually feasible in practice, and is a significant reduction from the number of possible configurations...|$|E
40|$|At {{the same}} time as our {{dependence}} on IT systems increases, the number of reports of problems caused by failures of critical IT systems has also increased. Today almost every system or service, e. g., water, power supply, transportation, is dependent on IT systems, and failure of these systems has serious and negative effects on society. In general, governmental organizations are responsible for delivery of these services to society. The increasing dependence on critical IT systems also makes them more and more complex. Risk analysis is an important activity for the development and operation of critical IT systems, but the increased complexity and size put additional requirements on the effectiveness of risk analysis methods. Risk analysis of technical systems has a long history in mechanical and electrical engineering. Even if a number of methods for risk analysis of technical systems exist, the failure behavior of information systems is typically very different from mechanical systems. Therefore, risk analysis of IT systems requires different risk analysis techniques, or at least adaptations of traditional approaches. The research objective of this thesis is to improve the analysis process of risks pertaining to IT systems in governmental organizations. In this thesis the improvements in risk analysis processes are addressed in two different ways. First, by understanding what types of methods are available for IT systems and how they can be improved. Second, by developing new effective and efficient risk analysis methods that can be useful to analyze IT systems in governmental organizations. In this thesis work, a systematic mapping study was carried out to understand existing methods and techniques used for analyzing IT systems. It found very few empirical research papers about the evaluation of existing risk analysis methods. The results of the mapping study suggest to empirically investigate risk analysis methods for analyzing IT systems to conclude which methods are more effective than others. Based on the results of the mapping study a case study was carried out to evaluate the effectiveness and efficiency of an existing risk analysis method, System Theoretic Process Analysis (STPA). Based on the results of the mapping study a controlled experiment was carried out {{to evaluate the effectiveness of}} risk analysis methods. The effectiveness of risk analysis methods was evaluated by counting the number of relevant and non-relevant risks identified by the experiment participants. The difficulty level of risk analysis methods and the experiment participants’ confidence about the identified risks were also investigated. The work presented in this thesis also presents a new risk analysis method, Perspective Based Risk Analysis (PBRA), that uses different perspectives while analyzing IT systems. A perspective is a point of view or a specific role adopted by risk analyst while doing risk analysis, i. e., system engineer, <b>system</b> <b>tester,</b> or system user. A case study was carried out to save historical information about IT incidents to be used later for risk analysis. This study investigates how difficult it is to find relevant risks from the available sources and the effort required to set up such a system. It also investigates how accurate the found risks are. It is believed that this could be an important aid in the process of building a database of occurred IT incidents that later can be used as an input to improve the risk analysis process. The presented research work in this thesis provides research about methods and tools for governmental organizations to improve their risk analysis and management practices. Moreover, the presented work in this thesis is based on solid empirical studies...|$|E
40|$|Software {{plays an}} {{important}} role in complex systems, especially for complex applications such as Video surveillance application, transportation, financial management, communication, biomedical applications and so on. For these <b>systems,</b> <b>tester</b> needs to concentrate on performances such as efficient operation, fault tolerance, safety and security. The basic problem is the complexity of the task, which has to be performed on a software application. Unlike hardware, software cannot break or wear out, but can fail during its life cycle [1]. Software problems, essentially, have to be solved with quality assurance tools such as testing procedures, quality data reporting. In this context, the paper proposes a new approach to automate software testing process to cover maximum test plan time and also to increase software quality. In this paper a method explained to cover each module without missing any test scenario and guarantees software with high quality and decrease testing life cycle time by establishing automation life cycle to develop scripts...|$|R
40|$|AbstractHigh reliability, {{safety and}} {{professional}} software quality are compelling requirement to software applied in civil aviation field. In this article, {{the status and}} characteristic of software testing in civil aviation field are summarized and analyzed. On the basis of analysis to the personal <b>system</b> of <b>tester,</b> testing environment and guaranty system of software testing, some constructive suggestions are presented for building up the integrated and efficient software testing system...|$|R
40|$|Automated {{generation}} of test cases {{is a prerequisite}} for fast testing. Whereas the research in automated test data generation addressed the creation of individual test points, test trajectory generation has attracted limited attention. In simple terms, a test trajectory is dened as a series of data points, with each (possibly multidimensional) point relying upon the value(s) of previous point(s). Many embedded systems use data trajectories as inputs, including closed-loop process controllers, robotic manipulators, nuclear monitor-ing systems, and flight control systems. For these <b>systems,</b> <b>testers</b> can either handcraft test trajectories, use input trajectories from older versions of the system or, perhaps, collect test data in a high delity system simulator. While these are valid approaches, they are expensive and time-consuming, especially if the assessment goals require many tests. We developed a framework for expanding a small, conventionally developed set of test trajectories into a large set suitable, for example, for system safety assurance. Statis-tical regression is the core of this framework. The regression analysis builds a relationship between controllable independent variables and closely correlated dependent variables...|$|R
50|$|The WIA conducts {{training}} sessions and has training materials for people wishing to become licensed Amateur Radio operators, conducts the testing using a <b>system</b> of accredited <b>testers,</b> and issues the authorisations for the ACMA to issue licences.|$|R
50|$|She {{has also}} {{worked as a}} <b>systems</b> analyst, {{production}} <b>tester,</b> and project leader on Wall Street, as well as co-founding a specialized recruiting firm offering expertise in client/server architecture and graphical design. She also has her own blog.|$|R
40|$|Today’s {{enterprise}} applications can produce {{vast amounts of}} information both during system testing and in production. Correlation of this information can be difficult as it is generally stored {{in a range of}} different event logs, the format of which can be application or vendor specific. Furthermore these large logs can be physically distributed across a number of different locations. As a result {{it can be difficult to}} form a coherent understanding of the overall system behaviour. This has implications for a number of domains (e. g. autonomic computing, <b>system</b> <b>testers),</b> where an understanding of the system behaviour at run-time is required (e. g. for problem determination, autonomic management etc.) This paper presents an approach and implementation of run-time correlation of large volumes of log data and symptom matching of known issues in the context of large {{enterprise applications}}. Our solution provides for automatic data collection, data normalisation into a common format, run-time correlation and analysis of the data to give a coherent view of system behaviour at run-time and a symptom matching mechanism that can identify known errors in the correlated data on the fly...|$|R
40|$|This paper {{investigates the}} {{relationships}} between fabric formability (a fundamental measure of fabric tailorability), bias extension and shear resistance. The experimental investigation has been performed {{on a range of}} thirty-one (fifteen pure wool, twelve wooVpolyester blends, one wooVrayon blend and three pure linen) suiting and trousering materials varying in mass per unit area from 125 to 258 g/m 2. Low stress mechanical properties measurements of fabric bending, shear and tensile deformations were obtained using the KES (Kawabata Evaluation <b>System)</b> <b>testers.</b> Furthermore, the 45 -degree bias extension behaviour of these fabrics was measured using an Instron extensometer. Following Spivak and Treloar's analysis [12], the bias load-extension and recovery curves were analysed to obtain equivalent shear stress/strain hysteresis curves. The two measures of shear rigidity, one obtained from the KES shear hysteresis curves and the other calculated from the bias extension tests, have been compared for the series of 31 fabrics. Relationships between fabric formability (defined as the product of tensile extensibility under low load and the bending rigidity) and its shear resistance are analysed. In addition, the work also covers the investigation on {{the relationships between}} fabric shear properties and formability in bias direction. Institute of Textiles and Clothin...|$|R
40|$|Abstract—Instrumentation is a {{critical}} part of evaluating an enterprise distributed real-time and embedded (DRE) system’s performance. Traditional techniques for instrumenting enterprise DRE systems require DRE system developers to make design decisions regarding what metrics to collect during early phases of the software lifecycle so these needs can be factored into the system architecture. In many circumstances, however, it is hard for DRE system developers to know this information during early phases of the software lifecycle—especially when metrics come from many heterogeneous sources (such as application- and system-level hardware and software resources) and evaluating performance is traditionally an after-thought. To address these issues, this article presents the design and performance of OASIS, which is SOA-based middleware and tools that dynamically instruments enterprise DRE system without requiring design-time knowledge of which metrics to collect. This article also empirically evaluates OASIS {{in the context of a}} representative enterprise DRE system case study from the domain of shipboard computing. Results from applying OASIS to this case study show that its flexibility enables DRE <b>system</b> <b>testers</b> to precisely control instrumentation overhead. We also highlight open challenges in dynamic instrumentation for next-generation enterprise DRE systems. Keywords-dynamic instrumentation, enterprise DRE systems, service-oriented architecture, middleware, real-time instrumentation I...|$|R
40|$|Session Initiation Protocol (SIP) {{has been}} used for {{signaling}} in many Voice over IP (VoIP) applications. Being more cost-effective than conventional circuit-switched systems, IP-based telecommunication systems are extensively employed by many service providers. As these systems gain more popularity, the need for dimensioning of such systems grows correspondingly. Moreover, accurate information about system capacity is necessary for future improvements of the system, as well as service provision and implementation planning. For these reasons, a solution supporting system performance evaluation is useful and beneficial in several ways. The goal of this research was to develop a performance evaluation framework for a SIP-based telecommunication system. The developed framework facilitates measurements of the maximum number of requests which can be processed by a system, and the amount of time required for call session establishment. With a user-friendly interface, the framework enables <b>system</b> <b>testers</b> to perform experiments using simulated SIP traffics, as well as to deal with results interpretation easily. In order to achieve the objective, studies of related technologies and available tools for SIP traffic generation have been carried out. Afterwards, the performance evaluation framework is designed and implemented. Lastly, the developed framework is used for evaluating the performance of EasyVPaBX, a SIP-based call handling system, in various system configurations. Keywords: SIP, Performance, Evaluation, Dimensioning, Measuremen...|$|R
40|$|A test {{synthesis}} {{and analysis}} <b>system</b> Turbo <b>Tester</b> (TT) {{has been developed}} and installed on IBM PC/ATs for teaching graduate and undergraduate courses in integrated circuits design and test at the Technical University of Tallinn in Estonia. In TT, different methods for test pattern generation (random, deterministic, mixed), fault simulation (two- and multivalued), test quality evaluation (deductive and parallel critical path approaches) and testability analysis have been implemented. Different fault models (stuck-at faults, delay faults, stuckopens) are covered. Tools for BIST simulation and quality analysis have been implemented...|$|R
40|$|A set of E. coli strains was {{developed}} by Toman et al. (1985) to {{study the effects of}} chemical and physical agents on forward mutation, homologous recombination and induction of the SOS <b>system.</b> New <b>tester</b> strains have been constructed to improve this test system in order to explore quantitative genotoxicity spectra. Through the use of these strains: (i) SOS induction can be specifically detected without interference from mutagenesis; (ii) SOS-dependent and SOS-independent mutational events can be distinguished; (iii) the sensitivity of the recombination system has been considerably increased. © 1988. SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|R
40|$|Manual {{testing is}} hard and time {{consuming}} and it maybe impossible for large <b>systems</b> or <b>tester</b> {{mistake in the}} test. The software testing is {{the rising cost of}} activities in development software and the test case generation is important activity in software testing. Hence researches performed to automate testing such as automatic test case generation. This paper reports a survey of recent research to generate test case automatically. Those are presented from UML based, graph based, formal methods, web application, web service, and combined. Those needed future researches are presented...|$|R
40|$|International audienceTesting {{real-time}} {{embedded systems}} (RTES) {{is in many}} ways challenging. Thousands of test cases can be potentially executed on an industrial RTES. Given the magnitude of testing at the system level, only a fully automated approach can really scale up to test industrial RTES. In this paper we take a black-box approach and model the RTES environment using the UML/MARTE international standard. Our main motivation is to provide a more practical approach to the model-based testing of RTES by allowing <b>system</b> <b>testers,</b> who are often not familiar with the system design but know the application domain well-enough, to model the environment to enable test automation. Environment models can support the automation of three tasks: the code generation of an environment simulator, the selection of test cases, and the evaluation of their expected results (oracles). In this paper, we focus on the second task (test case selection) and investigate three test automation strategies using inputs from UML/MARTE environment models: Random Testing (baseline), Adaptive Random Testing, and Search-Based Testing (using Genetic Algorithms). Based on one industrial case study and three artificial systems, we show how, in general, no technique is better than the others. Which test selection technique to use is determined by the failure rate (testing stage) and the execution time of test cases. Finally, we propose a practical process to combine the use of all three test strategies...|$|R
40|$|Abstract. We {{present a}} new memory {{efficient}} algorithm for on-the-fly verification of labelled transition <b>systems</b> (LTSs) with <b>testers.</b> To our knowledge, {{this is the}} first thoroughly presented solution for verifying all properties specifiable with testers. The algorithm requires four passes of the state space of the composition of the LTS and the tester...|$|R
5000|$|ViTrox Corporation Berhad (...) is a Penang, Malaysia based {{company that}} specializes in {{designing}} and developing automated vision inspection <b>system</b> and equipment <b>testers</b> for the semiconductor and electronic packaging industries as well as electronic communications equipment. The name ViTrox supposedly reflects the core business of the company which is machine vision and electronics.|$|R
