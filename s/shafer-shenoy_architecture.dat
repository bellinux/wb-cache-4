2|2|Public
40|$|AbstractIn {{this paper}} we propose a framework, called {{mixtures}} of truncated basis functions (MoTBFs), for representing general hybrid Bayesian networks. The proposed framework generalizes both the mixture of truncated exponentials (MTEs) framework and the Mixture of Polynomials (MoPs) framework. Similar to MTEs and MoPs, MoTBFs are defined so that the potentials are closed under combination and marginalization, which ensures that inference in MoTBF networks can be performed efficiently using the <b>Shafer–Shenoy</b> <b>architecture.</b> Based on a generalized Fourier series approximation, we devise a method for efficiently approximating an arbitrary density function using the MoTBF framework. The translation method is more flexible than existing MTE or MoP-based methods, and it supports an online/anytime tradeoff between the accuracy and the complexity of the approximation. Experimental results show that the approximations obtained are either comparable or significantly better than the approximations obtained using existing methods...|$|E
40|$|AbstractIn {{this paper}} {{we present a}} {{junction}} tree based inference architecture exploiting {{the structure of the}} original Bayesian network and independence relations induced by evidence to improve the efficiency of inference. The efficiency improvements are obtained by maintaining a multiplicative decomposition of clique and separator potentials. Maintaining a multiplicative decomposition of clique and separator potentials offers a tradeoff between off-line constructed junction trees and on-line exploitation of barren variables and independence relations induced by evidence. We consider the impact of the proposed architecture on a number of commonly performed Bayesian network tasks. The tasks we consider include cautious propagation of evidence, determining a most probable configuration, and fast retraction of evidence a long with a number of other tasks. The general impression is that the proposed architecture increases the computational efficiency of performing these tasks. The efficiency improvement offered by the proposed architecture is emphasized through empirical evaluations involving large real-world Bayesian networks. We compare the time and space performance of the proposed architecture with non-optimized implementations of the Hugin and <b>Shafer–Shenoy</b> inference <b>architectures...</b>|$|R
40|$|The {{junction}} tree algorithm {{is a way}} of computing marginals of boolean multivariate probability distributions that factorise over sets of random variables. The {{junction tree}} algorithm first constructs a tree called a junction tree who's vertices are sets of random variables. The algorithm then performs a generalised version of belief propagation on the junction tree. The <b>Shafer-Shenoy</b> and Hugin <b>architectures</b> are two ways to perform this belief propagation that tradeoff time and space complexities in different ways: Hugin propagation is at least as fast as Shafer-Shenoy propagation and in the cases that we have large vertices of high degree is significantly faster. However, this speed increase comes at the cost of an increased space complexity. This paper first introduces a simple novel architecture, ARCH- 1, which has the best of both worlds: the speed of Hugin propagation and the low space requirements of Shafer-Shenoy propagation. A more complicated novel architecture, ARCH- 2, is then introduced which has, up to a factor only linear in the maximum cardinality of any vertex, time and space complexities at least as good as ARCH- 1 and in the cases that we have large vertices of high degree is significantly faster than ARCH- 1...|$|R

