85|87|Public
5000|$|Sound similarity: {{methods for}} {{comparison}} between sounds, <b>sound</b> <b>identification,</b> novelty detection, segmentation, and clustering.|$|E
50|$|The game {{features}} Elmo, Abby Cadabby, Count von Count and the Honkers {{and promotes}} various lessons around music, including instrument and <b>sound</b> <b>identification,</b> different music styles and counting.|$|E
40|$|The primary {{objective}} {{of this study was}} to draw mechanisms which have used to improve music student’s piano performing skills and <b>sound</b> <b>identification.</b> The problem that faces on the students was lack of practice because of having lack of piano in their class. In order to solve the existing problem the researcher have used three different mechanisms that is preparing handmade piano, adjusting their mobile to rehearse the sound with the position of their finger, adjusting additional class out of their credit hour and using computer rocket system was the main solution used to develop students skill of piano playing and <b>sound</b> <b>identification.</b> The following results was found due to my intervention that is increase <b>sound</b> <b>identification</b> abilities per two tetra chord and put press their finger position on piano. The results were evaluated by using tables ’ observations when they perform piano and using photograph to cheek changes in their finger position and analyze their change by using tables. For future practice the researcher have planed that searching another best solution for the unsolved problems like problem of identifying more than one octave...|$|E
40|$|This paper {{reports on}} a {{multilingual}} investigation into the effects of different masker types on native and non-native perception in a VCV consonant recognition task. Native listeners outperformed 7 other language groups, but all groups showed a similar ranking of maskers. Strong first language (L 1) interference was observed, both from the sound system and from the L 1 orthography. Universal acoustic-perceptual tendencies are also at work in both native and non-native <b>sound</b> <b>identifications</b> in noise. The effect of linguistic distance, however, was less clear: in large multilingual studies, listener variables may overpower other factors. Index Terms: speech perception, non-native, noise, masking 1...|$|R
40|$|Acoustical {{holography}} {{has been}} widely applied for noise sources location and sound field measurement. Performance of the microphones array directly determines the sound source recognition method. Therefore, research {{is very important to}} the performance of the microphone array, its array of applications, selection, and how to design instructive. In this paper, based on acoustic holography moving <b>sound</b> source <b>identification</b> theory, the optimization method is applied in design of the microphone array, we select the main side lobe ratio and the main lobe area as the optimization objective function and then put the optimization method use in the <b>sound</b> source <b>identification</b> based on holography, and finally we designed this paper to optimize microphone array and compare the original array of equally spaced array with optimization results; by analyzing the optimization results and objectives, we get that the array can be achieved which is optimized not only to reduce the microphone but also to change objective function results, while improving the far-field acoustic holography resolving effect. Validation experiments have showed that the optimization method is suitable for high speed trains <b>sound</b> source <b>identification</b> microphone array optimization...|$|R
40|$|EUROSPEECH 2001 : the 7 th European Conference on Speech Communication and Technology, September 3 - 7, 2001, Aalborg, Denmark. It is very {{important}} for a hands-free speech interface to capture distant talking speech with high quality. A microphone array is an ideal candidate for this purpose. However, this approach requires localizing the target talker. To cope with this problem, we propose a new talker localization method consisting of two algorithms. One algorithm is for multiple sound source localization based on CSP (Cross-power Spectrum Phase) analysis. The other algorithm is for <b>sound</b> source <b>identification</b> among localized multiple sound sources towards talker localization. In this paper, we particularly focus on the latter statistical <b>sound</b> source <b>identification</b> among localized multiple sound sources with statistical speech and environmental sound models based on GMMs (Gaussian Mixture Models) and a microphone array towards talker localization...|$|R
30|$|Again, the {{difficulty}} becomes apparent {{when we consider}} lower-level abstractions. For instance, tabla transcription [88] {{appears to be more}} difficult than general drum <b>sound</b> <b>identification.</b> An interesting study was performed on the focused but difficult problem of pandeiro sound classification [83, 89].|$|E
40|$|Many {{teachers}} of Head Start struggle {{to develop and}} implement a successful early literacy program in their classrooms. I n order to help the teachers, an integrated, inquiry-based Math, Science, and Literacy (MSL) unit was developed and implemented in the Head Start preschool classroom for the period of two weeks. The subjects were fifteen preschool age students and a teacher, a teacher assistant, and a teacher aide. Prior to the implementation of the unit teacher interviews and pre - tests were administered to teachers and students respective ly. Teacher interviews focused on teacher 2 ̆ 7 s background and knowledge of the early literacy instruction techniques, whereas the pretests were meant to determine student 2 ̆ 7 s prior knowledge in letter and <b>sound</b> <b>identification.</b> Following the implementation of the unit post - test were administered to students and compared with the pretests in order to discover any growth in letter and <b>sound</b> <b>identification.</b> The results indicated an average increase of four letters per student in upper and lower case letters, as well as, in letter <b>sound</b> <b>identification.</b> The increase in the results demonstrated that the MSL thematic unit was effective at promoting better literacy development in Head Start preschool children...|$|E
40|$|Until now little {{research}} has been done in the area of animal <b>sound</b> <b>identification.</b> Animal sound classification and retrieval is very helpful for bioacoustics and audio retrieval applications. This paper is a literature review of an animal identification and detection system based on animal voice pattern recognition. The system uses the Zero...|$|E
5000|$|The ABI was {{originally}} {{developed at the}} House Ear Institute in 1979 for NF2 patients who lost their VIIIn function bilaterally following surgery to remove vestibular schwannomas (VS). [...] The ABI has provided therapeutic benefit for NF2 patients in terms of <b>sound</b> awareness, <b>identification</b> of some environmental sounds and improved performance over lipreading alone when communicating face-to-face. However, speech understanding without visual cues (commonly called [...] "open-set" [...] speech recognition) was generally poor.|$|R
40|$|Abstract: Recently, {{numerous}} successful {{approaches have}} been developed for instrument recognition in monophonic sounds. Unfortunately, none of them can be successfully applied to polyphonic <b>sounds.</b> <b>Identification</b> of music instruments in polyphonic sounds is still difficult and challenging. This has stimulated a number of research projects on music sound separation and new features development for content-based automatic music information retrieval. The paper introduces several temporal features based on pitch to improve automatic music instrument recognition. The results from experiments show that these new features, with the pitch information removed from them, tend to provide less distraction for timber estimation. Sometime, the addition of new features to the database of music instruments does not help and related classifiers still do not perform well. One possibility to handle this problem is to build classifiers which learn not only the descriptions of music instruments but also their generalizations on different granularity levels. We show that by introducing several optional hierarchical classifications of musical instruments and constructing related classifiers, we increase a chance to build a system of good performance in terms of successful indexing of music by instruments and their types...|$|R
40|$|EUROSPEECH 2003 : 8 th European Conference on Speech Communication and Technology, September 1 - 4, 2003, Geneva, Switzerland. In real {{acoustic}} environments, humans {{communicate with}} each other through speech by focusing on the target speech among environmental sounds. We can easily identify the target sound from other environmental sounds. For hands-free speech recognition, the identification of the target speech from environmental sounds is imperative. This mechanism may also be important for a self-moving robot to sense the acoustic environments and communicate with humans. Therefore, this paper first proposes Hidden Markov Model (HMM) -based environmental <b>sound</b> source <b>identification.</b> Environmental <b>sounds</b> are modeled by three states of HMMs and evaluated using 92 kinds of environmental <b>sounds.</b> The <b>identification</b> accuracy was 95. 4 %. This paper also proposes a new HMM composition method that composes speech HMMs and an HMM of categorized environmental sounds for robust environmental sound-added speech recognition. As a result of the evaluation experiments, we confirmed that the proposed HMM composition outperforms the conventional HMM composition with speech HMMs and a noise (environmental sound) HMM trained using noise periods prior to the target speech in a captured signal...|$|R
40|$|The {{purpose of}} this study was to assess the model, lead, and test (MLT) {{procedure}} on the letter name and <b>sound</b> <b>identification</b> performance for two elementary students. The two participants were diagnosed with learning disabled in math, reading, writing and communication. One of the two students also had behavior goals. The study took place in a resource classroom located in a public school in the Pacific Northwest. A multiple-baseline across letter sets was employed to assess the effectiveness of the model, lead, and test procedure. The behavior measured was correct letter name and <b>sound</b> <b>identification.</b> The results showed mastery of all letters of the alphabet by the participants when the model, lead, and test procedure was employed. The present outcomes replicate those of previous research and were easy to implement and assess by the classroom personnel...|$|E
30|$|In {{terms of}} the first {{priority}} area, there is scant knowledge on {{the best way to}} expand the supply of housing for the poor and promote the rental market in LAC for low-income households. There is, however, a body of literature with <b>sound</b> <b>identification</b> strategies on formalizing urban poor by giving them land titles.|$|E
40|$|A central {{problem in}} {{automatic}} sound recognition is the mapping between low-level audio features and the meaningful content of an auditory scene. We propose a dynamic network model to perform this mapping. In acoustics, much research {{is devoted to}} low-level per-ceptual abilities such as audio feature extraction and grouping, which are translated into successful signal processing techniques. However, little work is done on modeling knowl-edge and context in sound recognition, although this information is necessary to identify a sound event rather than to separate its components from a scene. We first investigate the role of context in human <b>sound</b> <b>identification</b> in a simple experiment. Then we show {{that the use of}} knowledge in a dynamic network model can improve automatic <b>sound</b> <b>identification</b> by reducing the search space of the low-level audio features. Furthermore, context information dissolves ambiguities that arise from multiple interpretations of one sound event...|$|E
40|$|Progress {{was made}} in the {{following}} areas: development of a numerical/empirical noise source identification procedure using bondary element techniques; identification of structure-borne noise paths using structural intensity and finite element methods; development of a design optimization numerical procedure to be used to study active noise control in three-dimensional geometries; measurement of dynamic properties of acoustical foams and incorporation of these properties in models governing three-dimensional wave propagation in foams; and structure-borne <b>sound</b> path <b>identification</b> by use of the Wigner distribution...|$|R
40|$|The {{purpose of}} this {{research}} study is {{to determine if the}} DIBELS assessments and the TRC assessments can accurately identify students into a gifted language arts program. The results of this study demonstrate that DIBELS and TRC neither could accurately place students into a gifted program that uses a <b>sound,</b> researched <b>identification</b> process. One DIBELS subtest, Letter Naming Fluency (LNF) did show some promise, but even that subtest would not be effective enough to use for this purpose. Department of Educational LeadershipThesis (D. Ed. ...|$|R
40|$|Limited {{research}} has been done on the reading strategies of primary school children learning English as a foreign language, particularly in the Hong Kong context. To contribute much needed research in this area, this study uses miscue analysis and prompted think aloud procedures to investigate the reading strategies used by higher and lower proficiency native Cantonese speaking students learning English as a foreign language in primary 4. As part of the investigation the study focuses on the strategies that these students use to comprehend unfamiliar words in context and the contribution of graphophonic awareness skills to the reading process. The study found that (a) higher proficiency students have a clearer awareness of the reading process and the importance of overall coherence and comprehension than their lower proficiency counterparts; (b) higher proficiency students were able to make efficient and effective use of the syntactic and semantic cueing systems to reduce their reliance on graphophonic and pictorial cues; and (c) higher proficiency students focused on more sophisticated graphophonic stategies, such as blending chunks of <b>sounds,</b> <b>identification</b> of syllable and association with other words, while lower proficiency students were {{more likely to engage in}} lower level pronunciation strategies, such as blending single sounds, or attempting whole word visual recall. published_or_final_versionEducationMasterMaster of Educatio...|$|R
40|$|In this study, we {{realized}} acoustic sound field dictation which is effective {{for the security}} systems because it can quickly find an ab-normal sound {{on the basis of}} text information from a captured long signal. Environmental <b>sound</b> <b>identification</b> has been previously researched only with the method that individually models all sound sources. However, it is impossible to model the innumerable real world environmental sounds. Therefore in our research, we try {{to reduce the number of}} models by utilizing onomatopoeias because they can represent an acoustic sound as a word. We thus firstly aimed at the environmental <b>sound</b> <b>identification</b> with the hidden Markov model (HMM) on the basis of onomatopoeias for realizing acoustic sound dictation. We carried out the subjective evaluation experiment with identification results of real world acoustic sounds. The results confirmed that the proposed method can better use identification result to remind people of acoustic sound than the conventional method...|$|E
40|$|The full {{or partial}} {{recovery}} of cognitive functions following brain lesions {{is believed to}} rely on the recruitment of alternative neural networks. This has been shown anatomically for selective auditory cognitive functions (Adriani et al. 2003 b). We investigate here behavioral correlates that may accompany the use of alternative processing networks and in particular the resulting increase in response times. The performance of 5 patients with right or left unilateral hemispheric infarction and 6 normal control subjects in <b>sound</b> <b>identification,</b> asemantic sound recognition, sound localization, and sound motion perception was evaluated by the number of correct replies and response times for correct and wrong replies. Performance and response times were compared across patients and normal control subjects. Two patients with left lesions were deficient in <b>sound</b> <b>identification</b> and sound motion perception and normal in sound localization and asemantic sound recognition; one patient with right lesion was deficient in sound localization and sound motion perception and normal in <b>sound</b> <b>identification</b> and asemantic sound recognition; deficient performance was associated with increased response times. The remaining 2 patients (1 with left, 1 with right lesion) had normal performance in all 4 tasks but had significantly longer response times in some (but not all) tasks. Patients with normal or deficient performance tended more often than normal subjects to give faster correct than wrong replies. We propose that increased response time is an indication of processing within an alternative network...|$|E
40|$|International audienceTrevor Wishart is an electroacoustic {{composer}} who obtained his PhD at the University of York in 1973. books On Sonic Art (1984 and 1996) and Audible Design (1994), which {{present his}} ideas about sound treatment, perception and composition. All {{of the theories}} and ideas I talk about here are related to my research {{as a graduate student}} in music and musicology, entitled <b>Sound</b> <b>identification,</b> listening strategy and narrativity in Trevor Wishart’s Journey into Space – Agentization, objectization and narrativizations (translated from the French: Identification sonore, stratégie d’écoute et narrativité dans Journey into Space de Trevor Wishart – Agentisation, objétisation et narrativisations). The present essay is mainly the transcription of an interview with Wishart himself, in which we talked about Journey into Space, <b>sound</b> <b>identification</b> (“landscape”), voice (both recorded and improvised), symbolism and narrativity. I will add some comments and ideas throughout the essay; these will deal with music and meaning, the main subject of JMM, as well as with my own research on electroacoustic narrativity...|$|E
40|$|Abstract. With {{the fast}} booming of online music repositories, {{there is a}} need for {{content-based}} automatic indexing which will help users to find their favorite music objects in real time. Recently, numerous successful approaches on musical data feature extraction and selection have been proposed for instrument recognition in monophonic sounds. Unfortunately, none of these methods can be successfully applied to polyphonic <b>sounds.</b> <b>Identification</b> of music instruments in polyphonic sounds is still difficult and challenging, especially when harmonic partials are overlapping with each other. This has stimulated the research on music sound separation and new features development for content-based automatic music information retrieval. Our goal is to build a cooperative query answering system (QAS), for a musical database, retrieving from it all objects satisfying queries like ”find all musical pieces in pentatonic scale with a viola and piano where viola is playing for minimum 20 seconds and piano for minimum 10 seconds”. We use the database of musical sounds, containing almost 4000 sounds taken from the MUMs (McGill University Master Samples), as a vehicle to construct several classifiers for automatic instrument recognition. Classifiers showing the best performance are adopted for automatic indexing of musical pieces by instruments. Our musical database has an FS-tree (Frame Segment Tree) structure representation. The cooperativeness of QAS is driven by several hierarchical structures used for classifying musical instruments. ...|$|R
30|$|It is {{important}} to distinguish injuries that occur along the central auditory pathways before the decussation into the superior olivary nucleus {{that may lead to}} asymmetrical SNHL, from those potentially responsible for associated auditory agnosia [36]. This condition refers to impairments in <b>sound</b> perception and <b>identification</b> despite intact hearing, cognitive functioning, and language abilities [37].|$|R
40|$|A vowel {{recognition}} {{based on}} a pitch-synchronous signal processing is introduced in this paper. The investigation has been made within {{the development of a}} speaker independent system of automatic speech <b>sounds</b> <b>identification.</b> The length of a signal-processing window is equal to the pitch period. This makes the signal analysis more independent of a pitch value than in the case of using a fixed-window analysis. It is known that the most effective analysis could be made if a length of the analyzing window is divisible by the pitch period. Thus the smallest window, which provides the perfect effectiveness of the signal analysis, is used here. The patterns for vowels were generated with a help of the knowledge about the phonological system and phonetic rules of the Russian language. Conducted experiments have shown that phonetically-based patterns dictionary is not less effective for speaker-independent speech recognition than the one generated with a help of clustering analysis. The proposed vowels recognition method was tested on the following material: a set of isolated vowels manually extracted from phonetically representative text, read by a standard male speaker of Russian, and a set of isolated words, read by 10 male and 10 female speakers of Russian. Vowels were automatically extracted and then identified within a processing of {{the second part of the}} material. An average recognition accuracy of 85. 0 % was obtained. The achieved results seem to be quite successful. 1...|$|R
40|$|Automated {{acoustic}} {{monitoring of}} wild animal species at Hymettus {{would not be}} possible without a good knowledge of the biodiversity present there. Species have to be inventoried and recordings of their vocalizations have to be gathered, identified, tagged, and archived in order to calibrate the <b>sound</b> <b>identification</b> soft-ware. These tasks are the main responsi-bilities of the Zoological Research Mu-seum A. Koenig (ZFMK). Founded as...|$|E
40|$|The paper {{presents}} {{method used}} to creating patterns for hydroacoustics signals for necessity of <b>sound</b> <b>identification</b> or classification. First the mathematical fundamentals, with breaking to separate processed blocks, of proposed method were introduced. Next {{the description of}} realized research and discussion about some obtained results were presented. At {{the end of the}} paper the direction of development in creating patterns for hydroacoustics signals and its selectors were pointed. 1...|$|E
40|$|AbstractRecognition of nonverbal {{sounds in}} {{semantic}} dementia and other syndromes of anterior temporal lobe degeneration may determine clinical symptoms {{and help to}} define phenotypic profiles. However, nonverbal auditory semantic function has not been widely studied in these syndromes. Here we investigated semantic processing in two key nonverbal auditory domains – environmental sounds and melodies – in patients with semantic dementia (SD group; n= 9) and in patients with anterior temporal lobe atrophy presenting with behavioural decline (TL group; n= 7, including four cases with MAPT mutations) in relation to healthy older controls (n= 20). We assessed auditory semantic performance in each domain using novel, uniform within-modality neuropsychological procedures that determined <b>sound</b> <b>identification</b> based on semantic classification of sound pairs. Both the SD and TL groups showed comparable overall impairments of environmental sound and melody identification; individual patients generally showed superior identification of environmental sounds than melodies, however relative sparing of melody over environmental <b>sound</b> <b>identification</b> also occurred in both groups. Our findings suggest that nonverbal auditory semantic impairment is a common feature of neurodegenerative syndromes with anterior temporal lobe atrophy. However, the profile of auditory domain involvement varies substantially between individuals...|$|E
40|$|International audienceIn {{this paper}} the <b>sound</b> source <b>identification</b> problem is {{addressed}} {{with the use}} of the lattice Boltzmann method. To this aim, a time-reversed problem coupled to a complex differentiation method is used. In order to circumvent the inherent instability of the time-reversed lattice Boltzmann scheme, a method based on a split of the lattice Boltzmann equation into a mean and a perturbation component is used. Lattice Boltzmann method formulation around an arbitrary base flow is recalled and specific applications to acoustics are presented. The implementation of the noise source detection method for two-dimensional weakly compressible (low Mach number) flows is discussed, and the applicability of the method is demonstrated...|$|R
40|$|Abstract: This paper mainly explore {{application}} of neural network on self-adaptation system. An framework model of self-adaptation based on theory of artificial network. An active silencing control system is made which adopts a motional feedback loudspeaker as not a noise controlling source but a detecting sensor. The working fundamentals and {{the characteristics of}} the motional feedback loudspeaker are analyzed in detail. By analyzing each <b>sound</b> channel, <b>identification</b> of neural network is made. This kind of identifying method can be achieved conveniently, and it wouldn’t bring distribution to working environment. The estimated result of each sound channel matches well with its real sound character, respectively...|$|R
40|$|As a passive, harmless, and {{low-cost}} diagnosis tool, fetal {{heart rate}} (FHR) monitoring based on fetal phonocardiography (fPCG) signal is alternative to ultrasonographic cardiotocography. Previous fPCG-based methods commonly {{relied on the}} time difference of detected heart sound bursts. However, the performance is unavoidable to degrade due to missed heart sounds in very low signal-to-noise ratio environments. This paper proposes a FHR monitoring method using repetition frequency of heart sounds. The proposed method can track time-varying heart rate without both heart <b>sound</b> burst <b>identification</b> and denoising. The average accuracy rate comparison to benchmark is 88. 3 % as the SNR ranges from − 4. 4 [*]dB to − 26. 7 [*]dB...|$|R
40|$|Abstract. Natural {{audio-visual}} {{interface between}} human user and machine requires understanding of user’s audio-visual commands. This {{does not necessarily}} require full speech and image recognition. It does require, just as the interaction with any working animal does, that the machine is capable of reacting to certain particular sounds and/or gestures while ignoring the rest. Towards this end, {{we are working on}} <b>sound</b> <b>identification</b> and classification approaches that would ignore most of the acoustic input and react only to a particular sound (keyword). ...|$|E
40|$|International audienceClassical timbre {{studies have}} modeled timbre as the {{integration}} of {{a limited number of}} auditory dimensions and proposed acoustic correlates to these dimensions to explain <b>sound</b> <b>identification.</b> Here, the goal was to highlight time-frequency patterns subserving identification of musical voices and instruments, without making any assumption about these patterns. We adapted a “random search method” proposed in vision. The method consists of synthesizing sounds by randomly selecting “auditory bubbles” (small time-frequency glimpses) from the original sounds’ spectrograms, and then inverting the resulting sparsified representation. For each bubble selection, a decision procedure categorizes the resulting sound as a voice or an instrument. After hundreds of trials, the whole time-frequency space is explored, and adding together the correct answers reveals the relevant time-frequency patterns for each category. We used this method with two decision procedures: human listeners and a decision algorithm using auditory distances based on spectro-temporal excitation patterns (STEPs). The patterns were strikingly similar for the two procedures: they highlighted higher frequencies (i. e., formants) for the voices, whereas instrument identification was based on lower frequencies (particularly during the onset). Altogether these results show that timbre can be analyzed as time-frequency weighted patterns corresponding to the important cues subserving <b>sound</b> <b>identification...</b>|$|E
40|$|This paper {{describes}} {{a computer program}} designed to teach orientation and mobility skills to visually impaired persons. The system utilizes off-the-shelf computer hardware and a proprietary virtual reality authoring library to create 3 -D spatial audio environments. Simulation environments currently being developed emphasize <b>sound</b> <b>identification,</b> localization, and tracking skills which are requisite to effective orientation and mobility in real world settings. Data describing how well a blind individual is able to identify, localize, and track various sound sources within each virtual training environment will eventually be collected and correlated with actual orientation and mobility performance data. Keywords 3 D Sound, Blind Access, Education...|$|E
40|$|This paper investigates {{differences}} in the gestures people relate to pitched and non-pitched sounds respectively. An experiment {{has been carried out}} where participants were asked to move a rod in the air, pretending that moving it would create the sound they heard. By applying and interpreting the results from Canonical Correlation Analysis we are able to determine both simple and more complex correspondences between features of motion and features of sound in our data set. Particularly, the presence of a distinct pitch seems to influence how people relate gesture to <b>sound.</b> This <b>identification</b> of salient relationships between sounds and gestures contributes as a multi-modal approach to music information retrieval...|$|R
40|$|UnrestrictedIn 3 {{experiments}} 111 {{young adults}} (ages 18 - 30) performed a letter identification task {{to test the}} hypothesis that emotional arousal increases attention to high priority stimuli and inhibits attention to low priority stimuli. A circular array of letters was briefly presented to each subject following exposure to negative arousing or neutral sounds. Contrast level of the letters was manipulated to make some letters higher in priority. The results support the arousal-biased competition hypothesis, as exposure to emotionally arousing <b>sounds</b> increased <b>identification</b> rates for high priority letters and decreased identification rates for low priority letters. Yet this effect was limited to inter-stimulus intervals (ISI) from 750 to 3000 milliseconds (ms) ...|$|R
5000|$|WSB in Atlanta, Georgia {{claims it}} was the first station to <b>sound</b> a musical <b>identification</b> at the end of programs, using a four-bar {{xylophone}} given to station manager Lambdin Kay by a performer, Nell Pendly. A three note signature, E-G-C, was developed, based on the first three notes from the chorus of the World War One classic [...] "Over There".|$|R
