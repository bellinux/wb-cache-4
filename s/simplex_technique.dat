21|60|Public
30|$|To {{achieve the}} highest glucose recovery, <b>simplex</b> <b>technique</b> was employed. The extent of glucose {{absorption}} of extracted solution of MIPs prepared at predetermined condition by <b>simplex</b> <b>technique</b> was {{predicted by the}} optimized RBF and was used to search for optimum condition. In order to validate the empirical model, MIPs and the corresponding NIPs were prepared under optimum conditions and were evaluated for glucose recovery.|$|E
30|$|In {{the present}} study, {{nanoparticles}} of MIPs with narrow {{particle size distribution}} were prepared by precipitation polymerization. <b>Simplex</b> <b>technique</b> in conjunction with radial basis function neural network was demonstrated to be an effective and reliable approach in finding the optimal conditions for the preparation of MIPs for glucose. The optimal conditions were found to be as follows: 0.5 ml of MAA, 1.7 ml of EGDMA, 0.15 ml of hexadecane, 40 mg of AIBN, and 9 ml of DMF, at 65 °C for 4 h. The results showed that among the studied variables, time, temperature, and the amount of initiator showed the most significant impact on MIP nanoparticle performance. Comparison of experimental and predicted recoveries by RBF revealed that the results were closely in agreement with each other, indicating high capability of designed RBF to model process and subsequently <b>simplex</b> <b>technique</b> usefulness in optimizing MIP nanoparticle preparation parameters. The binding ability was also evaluated, and the findings demonstrated high glucose binding and selectivity of imprinted particles compared to NIPs. BET analysis also confirmed that the particle pores were mesopores and macropores around 40 nm and possessed higher volume, surface area, and uniform size compared to the NIPs. Hence, it is worthy to employ <b>simplex</b> <b>technique</b> in conjunction with RBF to optimize the parameters of polymerization process and obtain high efficiency MIPs for different purposes.|$|E
30|$|This paper {{describes}} {{a novel approach}} for the preparation and optimization of nanoparticles of MIPs through <b>simplex</b> <b>technique</b> in conjunction with RBF {{in order to obtain}} MIPs with high binding capacity. Glucose was used as a template molecule, and imprinted particles were prepared by non-covalent imprinting method. The glucose-imprinted nanoparticles under optimized condition were also synthesized and characterized.|$|E
40|$|AbstractThe {{computational}} {{difficulties that}} continue to plague decomposition algorithms, namely, “long-tail” convergence and numerical instabilities, have served to dampen enthusiasm about their computational effectiveness. The use of interior points of subproblems in decomposition procedures may have a significant {{role to play in}} alleviating such computational difficulties. Indeed, Dantzig-Wolfe decomposition provides the arena within which <b>simplex</b> <b>techniques</b> for master problems and interior-point techniques for subproblems complement one another in a useful way. In combination they could lead to more effective decomposition algorithms than we have today. We formulate a particular algorithm along these lines and illustrate its convergence and numerical characteristics through numerical experiments. We make these experiments the basis {{for a discussion of the}} merits of using interior points in decomposition...|$|R
40|$|The {{objective}} {{of this research is}} to determinate the optimum conditions for the alcoholic fermentation process of aqueous jerivá pulp extract using the response surface methodology and <b>simplex</b> optimisation <b>technique.</b> The incomplete factorial design 3 ³ was applied with the yeast extract, NH 4 H 2 PO 4  and yeast as the independent variables and the alcohol production yield as the response. The regression analysis indicated that the model is predictive, and the simplex optimisation generated a formulation containing 0. 35 g L- 1  yeast extract, 6. 33 g L- 1  yeast and 0. 30 g L- 1 NH 4 H 2 PO 4  for an optimum yield of 85. 40 % ethanol. To validate the predictive equation, the experiment was carried out in triplicate under optimum conditions, and an average yield of 87. 15 % was obtained. According to a t-test, no significant difference was observed (on the order of 5 %) between the average value obtained and the value indicated by the <b>simplex</b> optimisation <b>technique.</b> The {{objective of}} this research is to determinate the optimum conditions for the alcoholic fermentation process of aqueous jerivá pulp extract using the response surface methodology and <b>simplex</b> optimisation <b>technique.</b> The incomplete factorial design 3 ³ was applied with the yeast extract, NH 4 H 2 PO 4 and yeast as the independent variables and the alcohol production yield as the response. The regression analysis indicated that the model is predictive, and the simplex optimisation generated a formulation containing 0. 35 g L- 1 yeast extract, 6. 33 g L- 1 yeast and 0. 30 g L- 1 NH 4 H 2 PO 4 for an optimum yield of 85. 40 % ethanol. To validate the predictive equation, the experiment was carried out in triplicate under optimum conditions, and an average yield of 87. 15 % was obtained. According to a t-test, no significant difference was observed (on the order of 5 %) between the average value obtained and the value indicated by the <b>simplex</b> optimisation <b>technique.</b> ...|$|R
40|$|ABSTRACT V R Pratt {{has shown}} that the real and integer feastbdlty of sets of linear mequallUes ofthe form x _< y + c can be decided quickly by {{examining}} the loops m certain graphs Pratt's method is generahzed, first to real feaslbdlty of mequahues m two variables and arbitrary coefficients, and ultimately to real feaslbdlty of arbitrary sets of hnear mequahtles The method is well suited to apphcatlons m program verification KEY WORDS AND PHRASES theorem proving, decision procedures, program venficauon, linear programmmg CRCATEGORIES 3 15, 369, 521, 532, 541 1. lntroductton Procedures for deciding whether a given set of l inear inequalities has solutions often {{play an important role in}} deductive systems for program verification. Array bounds checks and tests on index variables are but two of the many common programming constructs that give rise to formulas involving inequalities. A number of approaches have been used to decide the feasibdity of sets of inequalities [3, 8, 9, 16, 22], ranging from goal-driven rewriting mechanisms [27] to the powerful <b>simplex</b> <b>techniques</b> [8...|$|R
40|$|The {{behaviour}} of the {{geomagnetic field}} as observed almost continuously at three European locations {{over the last}} 130 years is investigated {{by means of a}} non-linear forecasting approach. The analysis of the data in terms of first-differences (secular variation) of the horizontal magnetic components made in phase space with the <b>simplex</b> <b>technique</b> seems to exclude the pre-eminence of any stochastic or periodic behaviour. The dimensionality of the underlying non-linear process and the corresponding largest positive Lyapunov exponent are estimated. The results give some evidence that the geomagnetic field evolves as a non-linear chaotic system with unpredictable behaviour after times greater than a few years, confirming the common practice of updating global models of the geomagnetic field every 5 years...|$|E
40|$|Softcomputing {{techniques}} are receiving attention as optimisation techniques for many industrial applications. Although these techniques {{eliminate the need}} for derivatives computation, they require much work to adjust their parameters at the stage of research and development. Issues such as speed, stability, and parameters convergence remain much to be investigated. This paper discusses the application of the method of reference model to determine parameters of asynchronous machines using two optimisation techniques. Softcomputing techniques used in this paper are evolutionary strategy and the chemotaxis algorithm. Identification results using the two {{techniques are}} presented and compared with respect to the conventional <b>simplex</b> <b>technique</b> of Nelder and Mead. Discussion about the chemotaxis algorithm as the most promising optimisation technique is presented, giving its advantages and disadvantages...|$|E
40|$|An {{analytical}} model is described using linear programming for the optimum generation {{and distribution of}} energy demands among competing energy resources and different economic criteria. The model, which {{will be used as}} a general engineering tool in the analysis of the Deep Space Network ground facility, considers several essential decisions for better design and operation. The decisions sought for the particular energy application include: the optimum time to build an assembly of elements, inclusion of a storage medium of some type, and the size or capacity of the elements that will minimize the total life-cycle cost over a given number of years. The model, which is structured in multiple time divisions, employ the decomposition principle for large-size matrices, the branch-and-bound method in mixed-integer programming, and the revised <b>simplex</b> <b>technique</b> for efficient and economic computer use...|$|E
40|$|AbstractThe {{temperature}} distribution of steel coils during annealing process in hydrogen {{has a significant}} effect on their quality. It is approached by numerical modelling using the differential equations governing the heat transfers involved. The resolution is performed by the method of finite differences. Convergence standards of different numerical algorithms are adjusted {{on the basis of a}} compromise between precision and time by the method of experimental plan. Thermal coefficients used in the model are also adjusted by the <b>simplex</b> optimization <b>technique.</b> Modeling results show good agreement with experimental measurements...|$|R
50|$|The PTAAGMB model uses daily {{values of}} such balance {{variables}} as snowline altitude, zero balance altitude, glacier balance, balance flux and the accumulation area ratio are correlated throughout the ablation season using two-degree polynomial regressions {{to obtain the}} lowest fitting error. When the minimum average error (or maximum R2) is attained, the generated balances and other variables {{are considered to be}} real. A <b>simplex</b> optimization <b>technique</b> is used to determine the optimal coefficient values that are used in algorithms to convert meteorological observations to snow accumulation and snow and ice ablation.|$|R
40|$|Despite {{the wide}} use of partial {{differential}} equation (PDE) solvers, lack of automation still hinders realizing {{their full potential}} in assisting engineering analysis and design. In particular, the process of establishing a suitable mesh for a given problem often requires heavy person-in-the-loop involvement. This thesis presents work toward {{the development of a}} robust PDE solution framework that provides a reliable output prediction in a fully-automated manner. The framework consists of: a <b>simplex</b> cut-cell <b>technique</b> which allows the mesh generation process to be independent of the geometry of interest; a discontinuous Galerkin (DG) discretization which permits an easy extension to high-order accuracy; and an anisotropic output-based adaptation which improves the discretization mesh for an accurate output prediction in a fully-automated manner. Two issues are addressed that limit the automation and robustness of the existing <b>simplex</b> cut-cell <b>technique</b> in three dimensions. The first is the intersection ambiguity due to numerical precision. We introduce adaptive precision arithmetic that guarantees intersection correctness, and develop various techniques to improve the efficiency of using this arithmetic. The second is the poor quadrature quality for arbitrarily shaped elements. We propose a high-quality and efficient cut-cell quadrature rule that satisfies a quality measure we define, and demonstrate the improvement in nonlinear solver robustness using this quadrature rule. The robustness and automation of the solution framework is then demonstrated through a range of aerodynamics problems, including inviscid and laminar flows. We develop a high-order DG method with a dual-consistent output evaluation for elliptic interface problems, and extend the <b>simplex</b> cut-cell <b>technique</b> for these problems, together with a metric-optimization adaptation algorithm to handle cut elements. This solution strategy is further extended for multi-physics problems, governed by different PDEs across the interfaces. Through numerical examples, including elliptic interface problems and a conjugate heat transfer problem, high-order accuracy is demonstrated on non-interface-conforming meshes constructed by the cut-cell technique, and mesh element size and shape on each material are automatically adjusted for an accurate output prediction. by Huafei Sun. Thesis: Ph. D., Massachusetts Institute of Technology, Department of Aeronautics and Astronautics, 2013. Cataloged from PDF version of thesis. Includes bibliographical references (pages 189 - 199) ...|$|R
40|$|Trinder’s {{method for}} glucose has {{nearly all the}} attrib-utes of an ideal {{automated}} colorimetric glucose oxidase procedure. The chemicals used in the color reaction with peroxidase are readily available, the solutions are stable and can be prepared by the user, the method is highly specific and largely free of interferences, the sensitivity can. be adjusted by the user to cover {{a wide range of}} glucose concentrations, and the reagents are not hazardous. We found very good agreement be-tween results by this method and by the hexokinase and Beckman Glucose Analyzer methods. The method has been modified and adapted to the AutoAnalyzer I and SMA 6 / 60 (Technicon) with manifolds that give very lit-tle interaction between specimens. A study of the meth-od by the <b>simplex</b> <b>technique</b> revealed that the glucos...|$|E
30|$|The aim of {{nanoparticle}} preparation {{process was}} to obtain MIPs with high recovery {{which in turn}} leads to achieve high selectivity and binding capacity while keeping non-selective interactions as low as possible. The <b>simplex</b> <b>technique</b> was used to optimize the input space of RBF model {{with the objective of}} maximizing glucose recovery. Since when all the adjustable parameters had been optimized, the neural network showed high ability of generalization; therefore, the optimized neural network model put forward to predict glucose recovery at predetermined condition by the simplex method. Based on the simplex method, the optimal conditions were found to be as follows: 0.5 ml of MAA, 1.7 ml of EGDMA, and 0.15 ml of hexadecane, 40 mg of AIBN, and 9 ml of DMF, at 65 °C for 4 h which correspond to the maximum value of A (A = 0.568).|$|E
40|$|This paper {{describes}} a time-dependent transport model for carrier assisted cation transport through supported liquid membranes. The model describes the flux of salt {{as a function}} of time and two parameters viz. the diffusion coefficient of the cation complex (D), and the extraction constant (Kex). Simulations of transport experiments indicate that high extraction constants (Kex) are responsible for the decrease in the transport rate in time. A simulation program (CURSIM) was designed that is capable of determining the essential transport parameters D and Kex from only one time-dependent transport experiment. The non-linear curve-fitting Down-hill <b>Simplex</b> <b>technique,</b> gives directly the best-fit values of D and Kex. The CURSIM method was tested by evaluation of the transport of KClO 4 by different calix[4]arene-crown- 5 carriers and by valinomycin. Comparison of the results with those obtained previously by the initial flux approach, which requires a series of different experiments, shows a good agreement of both methods. ...|$|E
40|$|Barndorff-Nielsen and Jørgensen (1989) have {{introduced}} some parametric models {{on the unit}} simplex. The distributions associated with these models have been obtained by conditioning on the sum of d independent generalized inverse Gaussian random variables. We use a constructive approach to derive some of these models by first mapping the inverse Gaussian law on (0, 1) and formally extending it on the unit <b>simplex.</b> This <b>technique</b> is then applied to a mixture-inverse Gaussian distribution studied recently by Jørgensen, Seshadri and Whitmore (1991). The distributions are then retransformed to yield two versions of a multidimensional inverse Gaussian distribution. Dirichlet distribution general exponential families generalized inverse Gaussian distribution inverse Gaussian distribution mixture-inverse Gaussian distribution...|$|R
40|$|In this paper, a cut-cell {{method is}} {{considered}} in which (1) the background mesh {{is composed of}} simplices allowing arbitrary anisotropy and (2) a higher-order discontinuous Galerkin (DG) discretization is applied. This paper quantifies the impact of three limitations of this <b>simplex</b> cut-cell <b>technique,</b> specifically: the presence of small volume ratios (i. e. cut elements with volume that is arbitrarily small compared to the volume of its neighboring elements); the application of integration rules designed for arbitrarily cut elements; and, {{the use of a}} Cartesian solution space. Solutions to these issues are presented and the paper concludes by demonstrating the two-dimensional cut-cell method with simulations of high Reynolds number RANS flow over a complex geometry. I...|$|R
40|$|This chapter {{investigates the}} {{effectiveness}} of a number of objective functions used in conjunction with a novel technique to optimise the classification of objects based on a number of characteristic values, {{which may or may not}} be missing. The classification and ranking belief <b>simplex</b> (CaRBS) <b>technique</b> is based on Dempster-Shafer theory and, hence, operates in the presence of ignorance. The objective functions considered minimise the level of ambiguity and/or ignorance in the classification of companies to being either failed or not-failed. Further results are found when an incomplete version of the original data set is considered. The findings in this chapter demonstrate how techniques such as CaRBS, which operate in an uncertain reasoning based environment, offer a novel approach to object classification problem solving...|$|R
40|$|Abstract. In {{this paper}} we propose an {{efficient}} algorithm to align the face in real time, based on Active Appearance Model (AAM) in 2. 5 D. The main objective {{is to make a}} robust, rapid and memory efficient application suitable for embedded systems, so they could align the pose rapidly by using less memory. Classical AAM is a high memory consumer algorithm, consequently transfer of this stored memory in an embedded system makes it a time consuming algorithm as well. Our 2. 5 D AAM is generated by taking 3 D landmarks from frontal and profile view and 2 D texture only from frontal view of the face image. Moreover we propose Nelder Mead <b>Simplex</b> <b>technique</b> for face search. It does not require large memory, thus becoming suitable for embedded systems by eliminating the excess memory and access time requirements. We illustrate 2. 5 D AAM optimized by Simplex for pose estimation and test it on three databases: M 2 VTS, synthetic images and webcam images. Results validate our combination of simplex and AAM in 2. 5 D. ...|$|E
40|$|International audienceThe {{optimization}} {{and use of}} ICP-AES with slurry nebulization for {{the direct}} analysis of ZrO 2 -powder is described. The powder samples are dispersed in water, acidified to pH 2 and the slurry is fed into a Babington nebulizer. The effects of grain size, pH of the suspending medium and standing time on {{the stability of the}} slurry are discussed. For the optimization of the ICP operating conditions, a <b>simplex</b> <b>technique</b> is applied and for this purpose three types of objective functions were examined. Identical behaviour of slurries and solutions with the same matrix concentrations in the ICP-AES is achieved for powders with particle sizes lower than 10 ?m; in the latter case calibration can be performed by standard addition with aqueous solutions. The detection limits for Al, B, Ca, Cu, Fe, Mg, Mn, Na, Ti, V. Y are 0. 03 ?g/g to 10 ?g/g and the standard deviation is generally lower than 10 %. Six commercially available ZrO 2 powders are analyzed by slurry nebulization ICP-AES and the results were found to agree well with those obtained by ICP-AES after chemical decomposition of the samples. © 1992 Springer-Verlag...|$|E
40|$|Apparent dips from 3 -D reflectors are {{calculated}} by maximizing the semblance-based coherency (C 2) by numerical optimization techniques. This maximization {{was done by}} means of searching through a tessellation of the apparent dips domain, and by optimization algorithms. We applied the simplex and Levenberg-Marquardt, whose performance was compared with those from the direct search techniques. According to numerical experiments with real data, the simplex algorithm enables not just important computing time savings, but provides the highest values from the objective function under all circumstances. This result implies that simplex achieves the maximization process more efficiently, while the other analyzed techniques converge towards the solution region but fail attaining the maxima. This result translates into a better contrast bettween coherent and non coherent features which implies higher resolution. The Levenberg-Marquardt algorithm provides the lowest values for the coherency. These results also found application to the calculation from normal C 3 coherency (eigenstructure). This is achieved by slanting the traces with the apparent dips, previously obtained by optimizing the C 2 slanted semblance with the <b>simplex</b> <b>technique.</b> The obtained dip corrected coherency show partially an enhanced resolution...|$|E
40|$|The {{fundamental}} {{characteristics and}} applications of {{inductively coupled plasma}} - optical emission spectrometry (ICP-OES) for forensic science purposes have been evaluated. Optimisation of ICP-OES for single elements using <b>simplex</b> <b>techniques</b> identified an ICP torch fitted with a wide bore injector tube as most suitable for multielement analysis because of a compact analytical region in the plasma. A suitable objective function has been proposed for multielement simplex optimisation of ICP-OES and its effectiveness has been demonstrated. The effects of easily ionisable element (EIE) interferences have been studied and an interference minimisation simplex optimisation shown to be appropriate for the location of an interference free zone. Routine, interference free determinations (< 2 % for 0. 5 % Na) {{have been shown to}} be critically dependant on the stability of the injector gas flowrate and nebuliser derived pressure pulses. Discrete nebulisation has been investigated for the analysis of small fragments of a variety of metal alloys which could be encountered in casework investigations. External contamination together with alloy inhomogeneity have been shown to present some problems in the interpretation of the data. A compact, corrosion resistant recirculating nebuliser has been constructed and evaluated for the analysis of small fragments of shotgun steels. The stable aerosol production from this nebuliser allowed a set of element lines, free from iron interferences, to be monitored with a scanning monochromator. The analysis, classification and discrimination of casework sized fragments of brasses and sheet glasses have been performed and a method has been proposed for the analysis of white household gloss paints. The determination of metal traces on hands following the handling of a variety of metal alloys has been reported. The significance of the results from these evidential materials has been assessed for forensic science purposes...|$|R
40|$|In this study, {{time studies}} of the {{processes}} involved in custard production {{as well as the}} costs implications were analysed. The results obtained were transformed into a format which enabled <b>simplex</b> optimization <b>technique</b> to be performed on the system. Thereafter, the system was programmed using MATLAB Graphic Users Integrated Development Environment (GUIDE) for simplex optimization to take care of possible changes in future in both the objective function and the constraints and to also generate quicker results. Holistic sensitivity analysis was carried out using each of the sensitive parameters and the results obtained in each case were analysed. The insight from the study will significantly enhance the decision making process of custard production industries. The sensitivity analysis results from this study can serve as a pedagogical tool of system changes to parameter variations...|$|R
40|$|Recent {{advances}} in power electronics have meant that many loads now draw a distorted current {{from the power}} supply. For the same real power consumed, the apparent power for the distorted load {{is greater than the}} equivalent sinusoidal load. A real-time active filter optimization algorithm has been implemented in a TMS 32 OC 30 DSP, with the aim of maximizing the monetary savings from active filtering by reducing the apparent power consumed at the point of supply. As the basis for this optimization a savings function which takes into account active filter efficiency, the cost of energy, and the supply and load current distortion before and after filtering, has been derived. A <b>simplex</b> optimization <b>technique,</b> which is able to find the optimum operating point even under varying load conditions, is used to maximize these energy savings...|$|R
40|$|The optimum {{analytical}} Conditions {{with regard}} to two or more variables can be determined in an unambig-uous way by use of the simplex method. The method is particularly applicable to problems involving inter-acting variables. We describe {{the application of the}} <b>simplex</b> <b>technique</b> to the optimization of two proce-dures on a continuous-flow analyzer (AutoAnalyzer) and to the optimization of chemical variables in the lactatedehydrogenase-catalyzedpyruvate-to-lactate reaction and certain instrumental variables on a cen-trifugal analyzer (the ENI-GEMSAEC). Additional Keyphrases: experimental design #{ 149 }GEM-SAEC #{ 149 }centrifugal analyzer #{ 149 }lactate dehydrogenase #{ 149 }AutoAnalyzer #{ 149 }glucose analysis A clinical chemist is often faced with the task of determining the optimum handling of several vari-ables in a method, a particularly difficult task when the variables are known to interact. For example, in enzymatic analyses the optimum substrate concen-tration may change with changes in temperature, pH, concentration and type of buffer, and the like. For example, the pH optimum for measuring alka-line phosphatase activity in serum changes when the substrate concentration is altered (1). The traditional approach for optimizing experi-mental variables has been to vary one factor at a time and find its optimum while holding all other factors constant. This empirical approach may not provide the unique combined optimum of all van...|$|E
40|$|Abstract—Mine-like object {{classification}} from sidescan sonar images {{is of great}} interest for mine counter measure (MCM) operations. Because the shadow cast by an object is often the most distinct feature of a sidescan image, a standard procedure is to perform classification based on features extracted from the shadow. The classification can then be performed by extracting features from the shadow and comparing this to training data to determine the object. In this paper, a superellipse fitting ap-proach to classifying mine-like objects in sidescan sonar images is presented. Superellipses provide a compact and efficient way of representing different mine-like shapes. Through variation of a simple parameter of the superellipse function different shapes such as ellipses, rhomboids, and rectangles can be easily generated. This paper proposes a classification of the shape based directly on a parameter of the superellipse known as the squareness parameter. The first step in this procedure extracts the contour of the shadow given by an unsupervised Markovian segmentation algorithm. Afterwards, a superellipse is fitted by minimizing the Euclidean distance between points on the shadow contour and the superellipse. As the term being minimized is nonlinear, a closed-form solution is not available. Hence, {{the parameters of the}} superellipse are estimated by the Nelder–Mead <b>simplex</b> <b>technique.</b> The method was then applied to sidescan data to assess its ability to recover and classify objects. This resulted in a recovery rate of 70 % (34 of the 48 mine-like objects) and a classification rate of better than 80 % (39 of the 48 mine-like objects). Index Terms—Classification, mine-like objects, recovery, sidescan, sonar, superellipse fitting. I...|$|E
40|$|This paper {{deals with}} a hybrid {{optimization}} {{of the degree of}} cure and temperature field into the work piece during the pultrusion manufacturing process. The procedure is based on the consecutive application of a heuristic technique (Genetic Algorithms), with an analytical method (<b>simplex</b> <b>technique),</b> to minimize an opportunely defined fitness function. The objective function measures the uniformity of the degree of cure, with a satisfactory mean value, in the exit cross section of the forming die, taking into account constraints related to the degradation of the processing resin system, due to an exothermic peak temperature major than the carbonisation temperature of the polymeric matrix. Optimal values of the temperatures of the die heating platens have been evaluated by applying the proposed procedure to a three dimensional thermo-chemical numerical model, solved using a finite difference scheme. The robustness of the developed optimization procedure has been tested using several combinations of process parameters, such as temperature of the resin bath, temperature of the die cooler, and composite material pull speed. A finite element model is used to analyze temperature field and degree of cure profiles after the optimization of the temperatures of the heating platens. The proposed procedure {{can be used as a}} good tool to optimize part quality or productivity, for the conventional pultrusion process, as well as, for post die shaping pultrusion, in which the processing part is completely formed out of the heating die and material cure is then completed using U. V. rays or other heating sources...|$|E
40|$|There {{is little}} {{documentation}} {{with regards to}} some structural characteristics of masonry units. Inadequate information on these characteristics leaves room for much speculation, approximations and arbitrariness, which could be detrimental {{to the design of}} structures. Static modulus of elasticity of sand-laterite blocks was determined in this work. <b>Simplex</b> optimization <b>technique</b> was used to formulate mathematical model for optimization of static modulus of elasticity of the blocks. Statistical tools were used to verify the adequacy of the model. The model proved satisfactory. With the model, values of static modulus of elasticity can be obtained if the mix ratios are specified. It can also provide the mix proportions that will give a desired value of static modulus of elasticity. The maximum value of static modulus of elasticity of sand-laterite blocks obtainable from this model is 6. 414 GPa...|$|R
40|$|A {{technique}} is presented for planning the future reactive power {{requirements of a}} power system. The problem is formulated as a mathematical optimization problem based on a linearized power system model. An efficient dual <b>simplex</b> linear programming <b>technique</b> coupled with relaxation and contingency analysis is used for solution. The approach reduces the dimensionality of the problem significantly, thereby allowing for more rapid calculations and more case studies to be performed. The effectiveness of the scheme is demonstrated using the IEEE 30 -bus and IEEE 188 -bus system...|$|R
40|$|Genetic {{algorithms}} {{have been}} created as an optimization strategy to be used especially when complex response surfaces do not allow the use of better-known methods (<b>simplex,</b> experimental design <b>techniques,</b> etc.). This paper shows that these algorithms, conveniently modified, {{can also be a}} valuable tool in solving the feature selection problem. The subsets of variables selected by genetic algorithms are generally more efficient than those obtained by classical methods of feature selection, since they can produce a better result by using a lower number of features...|$|R
40|$|Water {{resources}} {{planning and management}} requires hydrologic models to estimate runoff from a catchment. For catchments with limited data, the choice of model and identification of its parameters {{is very important for}} development of a direct runoff hydrograph. A method is presented to determine a unique pair of hydrologic parameters of the Nash Model, number of linear cascade (n) and storage coefficient (k), using optimization based on Downhill <b>Simplex</b> <b>technique.</b> In this study physical parameters of the catchment are derived from (SPOT) satellite imageries of the basin using ERDAS software. Four different objective functions of varying complexity are tested to find the best solution. Weighted root mean square error (RMSE) and Model Efficiency (Nash-Sutcliffe coefficient) are used to evaluate the model performance. Using the NASH model, a direct surface runoff hydrograph (DSRH) is developed. Kaha catchment is part of Indus river system and is located in the semi-arid region of Pakistan. This catchment is dominated by hill torrent flows and is used in this work to demonstrate the applicability of the proposed method. Ten randomly selected rainfall-runoff events are used for calibration and five events are used for validation. Model results during validation are very promising with model efficiency exceeding 93 % and error in peak discharge under 8 %. The sensitivity of the Nash model output in response to variation in hydrologic parameters n and k is also investigated. When evaluating the hydrologic response of large catchments, model output is more sensitive to n as compared to k indicating that the runoff diffusion phenomenon is dominant compared to translation flow effects...|$|E
40|$|We {{present a}} method to {{estimate}} Time of Concentration (T c) and Storage Coefficient (R) to develop Clark’s Instantaneous Unit Hydrograph (CIUH). T c is estimated from Time Area Diagram of the catchment and R is determined using optimization approach based on Downhill <b>Simplex</b> <b>technique</b> (code written in FORTRAN). Four different objective functions are used in optimization to determine R. The sum of least squares objective function is used in a novel way by relating it to slope of a linear regression best fit line drawn between observed and simulated peak discharge values to find R. Physical parameters (delineation, land slope, stream lengths and associated drainage areas) of the catchment are derived from SPOT satellite imageries of the basin using ERDAS: Arc GIS is used for geographic data processing. Ten randomly selected rainfall–runoff events are used for calibration and five for validation. Using CIUH, a Direct surface runoff hydrograph (DSRH) is developed. Kaha catchment (5, 598 km 2), part of Indus river system, located in semi-arid region of Pakistan and dominated by hill torrent flows is used to demonstrate the applicability of proposed approach. Model results during validation are very good with model efficiency of more than 95 % and {{root mean square error}} of less than 6 %. Impact of variation in model parameters T c and R on DSRH is investigated. It is identified that DSRH is more sensitive to R compared to T c. Relatively equal values of R and T c reveal that shape of DSRH for a large catchment depends on both runoff diffusion and translation flow effects. The runoff diffusion effect is found to be dominant...|$|E
40|$|A {{sustainable}} {{transportation system}} is safe, affordable and accessible. It connects different modes of transportation {{in order to}} achieve efficient movement. A sustainable transport system is one that is economical and friendly to the environment. All trips in private and public transport begin and end with walking. Non-Motorized Transport (NMT) {{plays a key role in}} completion of any journey from origin to destination. However, this mode of transport is not used optimally to enhance sustainable transport, especially where public transport is concerned. The main objective of the research was to evaluate an optimal combination of transport modes for trips made by BMW employees residing in Ga-Rankuwa north of Pretoria, in order to reduce cost of transport. The travel behaviour of BMW employees was investigated by first studying secondary data on the Municipal Household Survey (MHS) and Integrated Transport Plan (ITP) of Tshwane City. The secondary data was supplemented with primary data which was sourced by interviewing the BMW’s human resources department and conducting trip studies between Rosslyn and Ga-Rankuwa. The trip pattern of workers of BMW was evaluated to know the origin-destination, journey period and transportation cost of the trips. The movement patterns of the BMW workers were grouped into four categories. The patterns were subjected to optimization using linear programming which included the subsisting movement pattern. The trip patterns developed were based on relevant literature. The BMW workers’ movement from home to work was optimized. Their movement to work constituted a transportation challenge or problem; and was solved using <b>simplex</b> <b>technique,</b> a linear programming approach. The model was subjected to a sensitivity analysis and the results were analysed. It was found that the most patronized transportation mode for the trips between Rosslyn and Ga-Rankuwa was taxi. It was further discovered that BMW workers expended R 18. 47 averagely on movement from home to work through taxis. The recognition of cycling, a means of movement within short distances, allows for additional trip patterns as available options for commuters. The integration of cycling as a feeder system to trains resulted in 36...|$|E
40|$|This paper {{proposes a}} new {{strategy}} for missile attitude control using a hybridization of Linear Quadratic Gaussian (LQG), Loop Transfer Recovery (LTR), and Linear Quadratic Integral (LQI) control techniques. The LQG control design is carried out in two steps i. e. firstly applying Kalman filter for state estimation in noisy environment and then using the estimated states for an optimal state feedback control via Linear Quadratic Regulator (LQR). As further steps of performance improvement of the missile attitude control system, the LTR and LQI schemes are applied to increase the stability margins and guarantee set-point tracking performance respectively, while also preserving the optimality of the LQG. The weighting matrix (Q) and weighting factor (R) of LQG and the LTR fictitious noise coefficient (q) are tuned using Nelder-Mead <b>Simplex</b> optimization <b>technique</b> to make the closed-loop system act faster. Simulations are given to illustrate {{the validity of the}} proposed technique...|$|R
40|$|Uncertain {{reasoning}} is {{closely associated with}} the pertinent analysis of data where there may be imprecision, inexactness, and uncertainty in its information content. In computer modelling, this should move any analysis to be inclusive of such potential uncertainty, away from the presumption of perfect data to be worked with. The nascent Classification and Ranking Belief <b>Simplex</b> (CaRBS) <b>technique</b> employed in this chapter enables analysis in the spirit of uncertain reasoning. The operational rudiments of the CaRBS technique are based on the Dempster-Shafer theory of evidence, affording the presence of ignorance in any analysis undertaken. An investigation of Total Hip Arthraplasty (THA), concerned with hip replacements, forms the applied problem around which the uncertain reasoning based analysis using CaRBS is exposited. The presented findings include the levels of fit in constructed models, and the contribution of features within the models. Where appropriate, numerical calculations are shown, to illustrate this novel form of analysis...|$|R
40|$|Heat {{transfer}} {{between a}} conductive solid and an adjacent convective fluid is prevalent in many aerospace systems. The {{ability to achieve}} accurate predictions of the coupled heat interaction is critical in advancing thermodynamic designs. Despite their growing use, coupled fluid-solid analyses known as conjugate heat transfer (CHT) are hindered {{by the lack of}} automation and robustness. The mesh generation process is still highly dependent on user experience and resources, requiring time-consuming involvement in the analysis cycle. This thesis presents work toward developing a robust PDE solution framework for CHT simulations that autonomously provides reliable output predictions. More specifically, the framework is comprised of the following compo-nents: a <b>simplex</b> cut-cell <b>technique</b> that generates multi-regioned meshes decoupled from the design geometry, a high-order discontinuous Galerkin (DG) discretization, and an anisotropic output-based adaptation method that autonomously adapts the mesh to minimize the error in an output of interest. An existing cut-cell technique is first extended to generate fully-embedded meshe...|$|R
