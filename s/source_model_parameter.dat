2|10000|Public
40|$|A new HBT {{current source}} {{model and the}} {{corresponding}} direct parameter extraction methods are presented. Exact analytical expressions for the current <b>source</b> <b>model</b> <b>parameter</b> are derived. This method is applied to scalable modeling of HBT. Some techniques to reduce redundancy of the parameters are introduced. The model based on this method can accurately predict the measured data for the change of ambient temperature, size, and bias...|$|E
40|$|We {{assess the}} {{reliability}} of space-borne InSAR-derived deformation source parameters for volcanic and seismic events, with special focus on the future L-band data of the proposed Tandem-L mission [1 - 3]. Using representative simulation cases, the influence of certain characteristics of the InSAR measurements on the <b>source</b> <b>model</b> <b>parameter</b> precision is quantified. The performance drivers are assessed from two aspects: the data acquisition geometry {{as well as the}} measurement noise; in particular governed by signal coherence and superposed atmospheric signal. The significance of each these governing noise components is shown to be dependent on the spatial scale of the geophysical signal of interest as well as the deformation source mechanism in question. Here, we estimate the error bounds for the inferred source parameters {{as a function of the}} signal coherence and atmospheric signal parameters...|$|E
40|$|We {{have already}} {{developed}} a speech analysis method {{based on the}} Glottal-ARMAX (Auto Regressive and Moving Average eXogenous) model, in which the speech production model {{is supposed to be}} an ARMAX vocal tract model and two kinds of excitation: glottal <b>source</b> <b>model</b> excitation and white Gaussian. The speech analysis method based on the Glottal-ARMAX model can estimate the glottal <b>source</b> and ARMAX <b>model</b> <b>parameters</b> simultaneously with pitch synchronous. In this paper, a subband processing with QMF lterbank or Haar lterbank is introduced to the Glottal-ARMAX method {{in order to reduce the}} computation. The introduction makes it possible to reduce computation since the orders of ARMAX identication can be set smaller than that of fullband analysis. The introduction also enables to improve an estimation accuracy of the glottal <b>source</b> <b>model</b> <b>parameters</b> owing to the improved resolution in the frequency domain. 1...|$|R
40|$|The vowels /a, i, u/ {{spoken by}} American English talkers with non-pathological voices are {{described}} {{by means of}} voice <b>source</b> <b>model</b> <b>parameters</b> using the Liljencrants-Fant (LF) model. The sampling frequency of the data is 8 kHz which matches approximately telephone bandwidth. After inverse filtering, trends of voice source characteristics depending on the LF parameters are analyzed and compared to literature and listening results. Keywords: voice <b>source,</b> LF <b>model,</b> LF <b>parameters.</b> 1. INTRODUCTION Non-pathological voice source characteristics have been studied by inverse filtering the speech waveform [11], analyzing the speech spectra [6], or by measuring the airflow at the mouth [10]. Knowing the voice source parameters can be beneficial for many speech processing applications, such as speaker identification [8], and speech synthesis. In [6], individual and gender variations in source parameters have been analyzed using measures from speech spectra and {{taking into account the}} influence o [...] ...|$|R
40|$|With {{the design}} and {{development}} of next-generation high-energy neutrino detectors, {{it is important to}} compare different detector designs to optimize detection probability and science reach. These comparisons are nevertheless difficult due to large uncertainties in current neutrino <b>source</b> <b>model</b> <b>parameters.</b> We examine the role of the most important characteristics of high-energy neutrino searches in the probability of discovering different sources types. We derive scaling relations for each considered source and search scenario, which can be used to compare different detector designs with respect to their utility in discovering different source populations. The recovered scaling relations are independent of source strengths, providing a model-independent comparison...|$|R
40|$|An {{improved}} SPICE based macromodels of monolithic instrumentation amplifiers (in-amps) {{is presented}} in which the {{power supply rejection ratio}} (PSRR) frequency effects are modelled. The simulation models are developed through modifying the existing macromodels employing the mechanism of controlled sources and subcircuits. The macromodels are independent from actual technical realizations and are based upon compromises regarding the representation of exact circuit structures in the models. The equivalent circuits of the models principally contains linear passive RLC elements and controlled voltage and current <b>sources.</b> <b>Model</b> <b>parameters</b> are extracted for the IC AD 8221 from Analog Devices and INA 114 from Texas Instruments as examples. Simulation results and selected diagrams are compared with the manufacturer’s data...|$|R
40|$|The optimal {{decision}} making problem in situations characterized by uncertainty {{and availability of}} information sources is considered in a general setting. This {{gives rise to the}} need for a quantitative framework for the description of information exchange between the decision maker and information sources. Two companion papers established the first two parts of such a framework: the concepts of question difficulty and answer depth. This paper explores the third component of the framework, i. e. quantitative <b>models</b> of information <b>sources.</b> The concept of a <b>source</b> <b>model</b> is introduced and several different models are proposed. The <b>source</b> <b>model</b> <b>parameters</b> and the pseudo-temperature function on the problem parameter space characterizing question difficulty and answer depth in the overall “ideal gas ” information exchange model can be estimated from the observed source performance on a set of sample questions. Optimization based methods for such estimation are discussed. ...|$|R
40|$|We propose {{and analyze}} {{a new model}} for Hyperspectral Images (HSI) based on the {{assumption}} that the whole signal is composed of a linear combination of few sources, each of which has a specific spectral signature, and that the spatial abundance maps of these sources are themselves piecewise smooth and therefore efficiently encoded via typical sparse models. We derive new sampling schemes exploiting this assumption and give theoretical lower bounds on the number of measurements required to reconstruct HSI data and recover their <b>source</b> <b>model</b> <b>parameters.</b> This allows us to segment hyperspectral images into their source abundance maps directly from compressed measurements. We also propose efficient optimization algorithms and perform extensive experimentation on synthetic and real datasets, which reveals that our approach can be used to encode HSI with far less measurements and computational effort than traditional CS methods...|$|R
40|$|Glottal {{waveform}} {{models have}} long been employed to improve quality in speech synthesis. In this paper the subjective importance of accuracy {{for some of the}} Rosenberg glottal <b>source</b> <b>model</b> <b>parameters</b> has been investigated through listener preference judgments. Two paired-comparison tests were conducted involving 14 and 13 listeners, respectively. The parameters, 1) relative opening duration, 2) relative closing duration and 3) pulse amplitude, were subjected to random variation, systematic compression/expansion and to quantization. The results show that if the pitch period is accurately conserved the other timing parameters and the pulse amplitude can be disturbed considerably without perceptible degradation of quality. 1. Introduction A prominent problem in the synthesis of speech, for example in text-to-speech systems and speech coding, is the quality or naturalness of the synthesized speech. Early LPC-based methods used impulse trains as excitation source for voiced speech and whit [...] ...|$|R
40|$|Here is {{presented}} a phonetic <b>source</b> <b>model</b> whose <b>parameters,</b> estimated from phonetically transcribed texts, reflect the non-stationary phoneme conditional probability which is proper {{of a given}} language. Such a model will give a priori knowledges about the allowed phonetic sequences probabilities for a very large vocabulary speech recognizer, where the lexical access is made after phonetic decoding. After {{a discussion about the}} probability estimation method, model features and performances are given...|$|R
30|$|The subband or {{transform}} audio coding schemes primarily {{exploit the}} destination (human ear) model; the psychoacoustic model tells us where signal distortions (quantization) are allowed such {{that these are}} inaudible or least annoying. In speech coding, on the other hand, <b>source</b> <b>models</b> are primarily used. The incoming signal is matched to {{the characteristics of a}} <b>source</b> <b>model</b> (the vocal tract <b>model),</b> and the <b>parameters</b> of this <b>source</b> <b>model</b> are transmitted. In the decoder, the <b>source</b> <b>model</b> and its <b>parameters</b> are used to reconstruct the signal. For an overview on speech coding, please refer to [21].|$|R
40|$|Control over voice quality, e. g. breathy {{and tense}} voice, is {{important}} for speech synthesis applications. For example, transformations {{can be used to}} modify aspects of the voice re- lated to speaker's identity and to improve expressiveness. How- ever, it is hard to modify voice characteristics of the synthetic speech, without degrading speech quality. State-of-the-art sta- tistical speech synthesisers, in particular, do not typically al- low control over parameters of the glottal source, which are strongly correlated with voice quality. Consequently, the con- trol of voice characteristics in these systems is limited. In con- trast, the HMM-based speech synthesiser proposed in this paper uses an acoustic glottal <b>source</b> <b>model.</b> The system passes the glottal signal through a whitening filter to obtain the excitation of voiced sounds. This technique, called glottal post-filtering, allows to transform voice characteristics of the synthetic speech by modifying the <b>source</b> <b>model</b> <b>parameters.</b> We evaluated the proposed synthesiser in a perceptual ex- periment, in terms of speech naturalness, intelligibility, and similarity to the original speaker's voice. The results show that it performed as well as a HMM-based synthesiser, which generates the speech signal with a commonly used high-quality speech vocoder...|$|R
30|$|In this section, {{we first}} define {{the problem of}} {{separating}} sound sources and the integrated tone model. This model {{is based on a}} previous study [19], and we improved implementation of the inharmonic models. We then derive an iterative algorithm that consists of two steps: sound <b>source</b> separation and <b>model</b> <b>parameter</b> estimation.|$|R
40|$|Catheter-related {{bloodstream}} {{infections are}} a serious problem. Many interventions reduce risk, {{and some have}} been evaluated in cost-effectiveness studies. We review the usefulness and quality of these economic studies. Evidence is incomplete, and data required to inform a coherent policy are missing. The cost-effectiveness studies are character-ized {{by a lack of}} transparency, short time-horizons, and nar-row economic perspectives. Data quality is low for some important <b>model</b> <b>parameters.</b> Authors of future economic evaluations should aim to model the complete policy and not just single interventions. They should be rigorous in de-veloping the structure of the economic model, include all relevant economic outcomes, use a systematic approach for selecting data <b>sources</b> for <b>model</b> <b>parameters,</b> and propa-gate the effect of uncertainty in <b>model</b> <b>parameters</b> on con...|$|R
40|$|Owing {{to a lack}} of robust principled methods, {{systematic}} instrumental uncertainties {{have generally}} been ignored in astrophysical data analysis despite wide recognition of the importance of including them. Ignoring calibration uncertainty can cause bias in the estimation of <b>source</b> <b>model</b> <b>parameters</b> and can lead to underestimation of the variance of these estimates. We previously introduced a pragmatic Bayesian method to address this problem. The method is “pragmatic ” in that it introduced an ad hoc – 2 – technique that simplified computation by neglecting the potential information in the data for narrowing the uncertainty for the calibration product. Following that work we use a principal component analysis to efficiently represent the uncertainty of the effective area of an X-ray (or γ-ray) telescope. Here, however, we leverage this representation to enable a principled, fully Bayesian method that coherently accounts for the calibration uncertainty in high-energy spectral analysis. In this setting, the method is compared with standard analysis techniques and the pragmatic Bayesian method. The advantage of the fully Bayesian method is that it allows the data to provide information not only for estimation of the source parameters but also for the calibration product—here the effective area, conditional on th...|$|R
40|$|Systematic {{instrumental}} uncertainties in astronomical analyses {{have been}} generally ignored {{due to the}} lack of robust principled method, though the importance of incorporating instrumental calibration uncertainty is widely realized by users and instrument builders. Ignoring calibration uncertainty can cause bias in the estimate of <b>source</b> <b>model</b> <b>parameters</b> and underestimate their variance. In this poster, we focus on incorporating uncertainty for the effective area curve into a principled fully Bayesian spectral analysis. A principle component analyses is explored to efficiently represent the variability of the effective area curve, enabling a fully Bayesian analysis of calibration uncertainty in spectral analysis of high-energy Chandra data. The method is compared with standard analysis techniques and the so-called“pragmatic ” Bayesian method of Lee et al(2011, ApJ). The advantage of the fully Bayesian method is that the data itself can provide information for both the source parameters and for the effective area curve. It is verified that implementing our fully Bayesian method can result in more accurate and efficient estimation of source parameters, and valid estimates of uncertainty. Calibration Products Analysis highly depende on Calibration Products: • Effective Area records sensitivity as a function of energ...|$|R
40|$|Abstract. We invoke an auto-regressive IIR inverse {{model for}} convolutive ICA and derive {{expressions}} for the likelihood and its gradient. We argue that optimization {{will give a}} stable inverse. When there are more sensors than <b>sources</b> the mixing <b>model</b> <b>parameters</b> are estimated in a second step by least squares estimation. We demonstrate the method on synthetic data and finally separate speech and music in a real room recording. ...|$|R
40|$|Abstract—This paper {{proposes a}} multipitch {{analyzer}} called the harmonic temporal structured clustering (HTC) method, that jointly estimates pitch, intensity, onset, duration, etc., of each underlying source in a multipitch audio signal. HTC decomposes the energy patterns diffused in time-frequency space, i. e., the power spectrum time series, into distinct clusters such that each has originated {{from a single}} source. The problem is equivalent to approximating the observed power spectrum time series by su-perimposed HTC <b>source</b> <b>models,</b> whose <b>parameters</b> {{are associated with the}} acoustic features that we wish to extract. The update equations of the HTC are explicitly derived by formulating the HTC <b>source</b> <b>model</b> with a Gaussian kernel representation. We verified through experiments the potential of the HTC method. Index Terms—Computational acoustic scene analysis, harmonic temporal structured clustering (HTC), multipitch analyzer. I...|$|R
40|$|We {{consider}} a central place forager with two qualitatively {{different types of}} food sources; type 1 sources are always available whereas type 2 sources become available intermittently and this availability is signalled by information present at the central place. <b>Source</b> 1 is <b>modelled</b> using a standard patch foraging <b>model</b> whereas <b>source</b> 2 is <b>modelled</b> somewhat schematically {{in terms of the}} presence of information, the time spent at the source and the average reward received. The only decision in the model is the time spent by the forager at source 1 on each trip. We characterise the optimal foraging time and the optimal overall reward rate under the two <b>source</b> <b>model</b> and compare it with the corresponding quantities for a single <b>source</b> <b>model.</b> We show that, in general, the potential for information transfer has a marked effect on the forager’s behaviour, and that a forager behaving optimally should return to check for new information with what might, under a single <b>source</b> <b>model,</b> seem to be a strictly submaximal load. We consider the dependence of the optimal foraging time and the optimal overall reward rate on the <b>source</b> 2 <b>model</b> <b>parameters,</b> and also show that our qualitative results hold for a variety of models for the time spent on source 2...|$|R
40|$|The {{theoretical}} {{aspects of}} an orbit determination filter that incorporates ground-system error <b>sources</b> as <b>model</b> <b>parameters</b> {{for use in}} interplanetary navigation are presented in this article. This filter, which is derived from sequential filtering theory, allows a systematic treatment of errors in calibrations of transmission media, station locations, and earth orientation models associated with ground-based radio metric data, {{in addition to the}} modeling of the spacecraft dynamics. The discussion includes a mathematical description of the filter and an analytical comparison of its characteristics with more traditional filtering techniques used in this application. The analysis in this article shows that this filter has the potential to generate navigation products of substantially greater accuracy than more traditional filtering procedures...|$|R
40|$|Sequence {{models are}} {{retrieved}} from a sequences index. The sequence models model DNA or RNA sequences {{stored in a}} database, and each comprises a finite memory tree <b>source</b> <b>model</b> and <b>parameters</b> for the finite memory tree <b>source</b> <b>model.</b> One or more DNA or RNA sequences stored in the database are identified as being most similar to a query DNA or RNA sequence based on fitting of the retrieved sequence models to the query DNA or RNA sequence. The sequence models may be context tree weighting (CTW) models {Sx, [theta]Sx} where Sx denotes the context tree model for the DNA or RNA sequence x stored in the database, and [theta]Sx denotes parameters of the context tree model Sx. The fitting may include, for each CTW model {Sx, [theta]Sx}, computing the codeword length for the query DNA or RNA sequence y using the CTW model {Sx, [theta]Sx}...|$|R
40|$|This study {{presents}} {{an application of}} the ETAS model to the first 20 days of the 2016 central Italy sequence. Despite of the provisional nature of data, the model is able to describe the occurrence rate, {{but for the first}} hours after the mainshock occurrence. A sensitivity analysis of the model to two uncertainty <b>sources,</b> the <b>model</b> <b>parameters</b> and the occurrence history, shows that the second has a main role in controlling the performance of the ETAS model, more than the uncertainty on parameters. Previous results, together with the clear inability of ETAS to forecast the occurrence of a sequence before its starting time, give important suggestions about possible improvements. Here, a very preliminary attempt in this sense is presented...|$|R
40|$|An {{innovative}} supercomputing grid services {{devoted to}} noise threat evaluation were presented. The services {{described in this}} paper concern two issues, first {{is related to the}} noise mapping, while the second one focuses on assessment of the noise dose and its influence on the human hearing system. The discussed serviceswere developed within the PL-Grid Plus Infrastructure which accumulates Polish academic supercomputer centers. Selected experimental results achieved by the usage of the services proposed were presented. The assessment of the environmental noise threats includes creation of the noise maps using either ofline or online data, acquired through a grid of the monitoring stations. A concept of estimation of the <b>source</b> <b>model</b> <b>parameters</b> based on the measured sound level for the purpose of creating frequently updated noise maps was presented. Connecting the noise mapping grid service with a distributed sensor network enables to automatically update noise maps for a specified time period. Moreover, a unique attribute of the developed software is the estimation of the auditory effects evoked by the exposure to noise. The estimation method uses a modified psychoacoustic model of hearing and is based on the calculated noise level values and on the given exposure period. Potential use scenarios of the grid services for research or educational purpose were introduced. Presentation of the results of predicted hearing threshold shift caused by exposure to excessive noise can raise the public awareness of the noise threats...|$|R
40|$|The {{origin of}} ultra-high energy cosmic rays {{is an open}} question. Detailed {{simulations}} of the propagation through extragalactic and galactic space are {{needed in order to}} connect theories about the cosmic-ray origin with the available measurements. For this purpose the simulation software CRPropa 3 was developed. The software is presented with its new modular simulation structure and the implementation of all physical processes relevant for the propagation of ultra-high energy cosmic-ray hadrons. This includes cosmological effects, deflections in magnetic fields, interactions with the photon background and the production of secondary particles. Using CRPropa 3 a simple astrophysical scenario of extragalactic sources accelerating a mixed composition of cosmic-rays with a power-law spectrum and a rigidity dependent cutoff is investigated. A method is presented to compare the predicted energy spectrum and the mass-sensitive Xmax distributions from these simulations to the measurements by the Pierre Auger Observatory for energies above 10 ^ 18. 7 eV. A Bayesian analysis is performed to calculate the probability distributions of the <b>source</b> <b>model</b> <b>parameters</b> given the measurements, and the impact of the main systematic uncertainties of simulation and measurement is investigated. The resulting scenarios are characterized by low maximum acceleration energies and by emission spectra that are significantly harder than expected for Fermi-type accelerators. The scenarios are discussed in terms of expected anisotropies in the arrival directions, and fluxes of cosmogenic photons and neutrinos, finding no additional constraints from these observables...|$|R
40|$|This paper {{presents}} a new method for reverberant speech separation, {{based on the}} combination of binaural cues and blind source separation (BSS) for the automatic classification of the time-frequency (T-F) units of the speech mixture spectrogram. The main idea is to model interaural phase difference, interaural level difference and frequency bin-wise mixing vectors by Gaussian mixture <b>models</b> for each <b>source</b> and then evaluate that model at each T-F point and assign the units with high probability to that <b>source.</b> The <b>model</b> <b>parameters</b> and the assigned regions are refined iteratively using the Expectation-Maximization (EM) algorithm. The proposed method also addresses the permutation problem of the frequency domain BSS by initializing the mixing vectors for each frequency channel. The EM algorithm starts with binaural cues {{and after a few}} iterations the estimated probabilistic mask is used to initialize and re-estimate the mix- ing vector <b>model</b> <b>parameters.</b> We performed experiments on speech mixtures, and showed an average of about 0. 8 dB improvement in signal-to-distortion (SDR) over the binaural-only baselin...|$|R
40|$|AbstractObjectivesAlthough {{there has}} {{recently}} been substantial interest in a Medicare drug benefit program, little attention has focused on ensuring improved access to medication monitoring for Medicare beneficiaries. Using a societal perspective, we evaluated the impact pharmacists could have on inappropriate prescribing, patient compliance, and medication-related morbidity and mortality within a Medicare drug benefits program. MethodsA cost-effectiveness analysis from a societal perspective was performed. A comprehensive MEDLINE search for relevant literature identified data <b>sources</b> and <b>model</b> <b>parameters.</b> ResultsIn the base case, a pharmaceutical care benefit in the elderly population would cost $ 2100 (year 2000 prices) per life-year saved, which is highly cost-effective. Reasonable changes in <b>model</b> <b>parameters</b> did not raise the cost-effectiveness ratio above $ 13, 000 per life-year saved. ConclusionDespite limitations in both the quantity and the specificity of data available, pharmaceutical care {{appears to be a}} highly cost-effective augmentation to a Medicare drug benefit program. This result is robust to <b>model</b> <b>parameter</b> changes. This <b>model</b> is conservative in that it does not include ongoing benefits from medication monitoring or increased elderly drug utilization and polypharmacy as the Medicare drug program is phased in...|$|R
40|$|Cone beam CT (CBCT) {{has been}} widely used for patient setup in image guided {{radiation}} therapy (IGRT). Radiation dose from CBCT scans has become a clinical concern. The {{purposes of this study}} are 1) to commission a GPU-based Monte Carlo (MC) dose calculation package gCTD for Varian On-Board Imaging (OBI) system and test the calculation accuracy, and 2) to quantitatively evaluate CBCT dose from the OBI system in typical IGRT scan protocols. We first conducted dose measurements in a water phantom. X-ray <b>source</b> <b>model</b> <b>parameters</b> used in gCTD are obtained through a commissioning process. gCTD accuracy is demonstrated by comparing calculations with measurements in water and in CTDI phantoms. 25 brain cancer patients are used to study dose in a standard-dose head protocol, and 25 prostate cancer patients are used to study dose in pelvis protocol and pelvis spotlight protocol. Mean dose to each organ is calculated. Mean dose to 2 % voxels that have the highest dose is also computed to quantify the maximum dose. It is found that the mean dose value to an organ varies largely among patients. Moreover, dose distribution is highly non-homogeneous inside an organ. The maximum dose is found to be 1 ~ 3 times higher than the mean dose depending on the organ, and is up to 8 times higher for the entire body due to the very high dose region in bony structures. High computational efficiency has also been observed in our studies, such that MC dose calculation time is less than 5 min for a typical case. Comment: 19 pages, 6 figures, 7 table...|$|R
40|$|International audienceThis paper studies <b>source</b> and {{correlation}} <b>models</b> for Distributed Video Coding (DVC). It first {{considers a}} twostates HMM, i. e. a Gilbert-Elliott process, {{to model the}} bit-planes produced by DVC schemes. A statistical analysis shows that this model allows us to accurately capture the memory present in the video bit-planes. The achievable rate bounds are derived for these ergodic sources, first assuming an additive binary symmetric correlation channel between the two sources. These bounds show that a rate gain {{can be achieved by}} exploiting the sources memory with the additive BSC model. A Slepian-Wolf (SW) decoding algorithm which jointly estimates the sources and the <b>source</b> <b>model</b> <b>parameters</b> is then described. Simulation results show that the additive correlation model does not always fit well with the correlation between the actual video bit-planes. This has led us to consider a second correlation model (the predictive model). The rate bounds are then derived for the predictive correlation model in the case of memory sources, showing that exploiting the source memory does not bring any rate gain and that the noise statistic is a sufficient statistic for the MAP decoder. We also evaluate the rate loss when the correlation model assumed by the decoder is not matched to the true one. An a posteriori estimation of the correlation channel has hence been added to the decoder in order to use the most appropriate correlation model for each bit-plane. The new decoding algorithm has been integrated in a DVC decoder, leading to a rate saving of up to 10. 14 % for the same PSNR, with respect to the case where the bit-planes are assumed to be memoryless uniform sources correlated with the SI via an additive channel model...|$|R
40|$|In this paper, an {{enhanced}} SPICE macromodel for Current-Feedback Amplifiers (CFA) is presented which {{takes into account}} the second-order effects such as the noise, the common-mode rejection ratio (CMRR), the positive and negative output voltage swing, and power-supply rejection ratio (PSRR) as a function of temperature. The simulation model is develop-ped through modifying the basic macromodel employing the mechanism of controlled <b>sources</b> and subcircuits. <b>Model</b> <b>parameters</b> are extracted for the integrated CFA AD 8001 as an example. The accuracy of the model is demonstrated by comparison between the data sheet parameters of the real IC and the simulation results...|$|R
40|$|Current {{statistical}} {{machine translation}} (SMT) systems are trained on sentencealigned and word-aligned parallel text collected from various <b>sources.</b> Translation <b>model</b> <b>parameters</b> are estimated from the word alignments, {{and the quality of}} the translations on a given test set depends on the parameter estimates. There are at least two factors affecting the parameter estimation: domain match and training data quality. This paper describes a novel approach for automatically detecting and down-weighing certain parts of the training corpus by assigning a weight to each sentence in the training bitext so as to optimize a discriminative objective function on a designated tuning set. This way, the proposed method can limit the negative effects of low quality training data, and can adapt the translation model to the domain of interest. It is shown that such discriminative corpus weights can provide significant improvements in Arabic-English translation on various conditions, using a state-of-the-art SMT system. ...|$|R
40|$|We {{report on}} our study of high-energy {{properties}} of two peculiar TeV emitters: the "extreme blazar" 1 ES 0347 – 121 and the "extreme blazar candidate" HESS J 1943 + 213 {{located near the}} Galactic plane. Both objects are characterized by quiescent synchrotron emission with flat spectra extending up to the hard X-ray range, and both were reported to be missing GeV counterparts in the Fermi Large Area Telescope (LAT) two-year Source Catalog. We analyze a 4. 5  yr accumulation of the Fermi -LAT data, resulting in the detection of 1 ES 0347 – 121 in the GeV band, {{as well as in}} improved upper limits for HESS J 1943 + 213. We also present the analysis results of newly acquired Suzaku data for HESS J 1943 + 213. The X-ray spectrum is well represented by a single power law extending up to 25  keV with photon index 2. 00 ± 0. 02 and a moderate absorption in excess of the Galactic value, which is in agreement with previous X-ray observations. No short-term X-ray variability was found over the 80  ks duration of the Suzaku exposure. Under the blazar hypothesis, we modeled the spectral energy distributions of 1 ES 0347 – 121 and HESS J 1943 + 213, and we derived constraints on the intergalactic magnetic field strength and source energetics. We conclude that although the classification of HESS J 1943 + 213 has not yet been determined, the blazar hypothesis remains the most plausible option since, in particular, the broadband spectra of the two analyzed sources along with the <b>source</b> <b>model</b> <b>parameters</b> closely resemble each other, and the newly available Wide-field Infrared Survey Explorer and UKIRT Infrared Deep Sky Survey data for HESS J 1943 + 213 are consistent with the presence of an elliptical host at the distance of approximately 600  Mpc...|$|R
40|$|The dosimetric {{measurement}} and modeling of small radiation treatment fields (< 2 x 2 cm(2)) {{are difficult to}} perform and prone to error. Measurements of small fields are often adversely influenced by {{the properties of the}} detectors used to make them. The dosimetric properties of small fields have been difficult to accurately model due to the effects of source occlusion caused by the collimating jaws. In this study, small longitudinal slice widths (SWs) of the TomoTherapy (R) Hi-Art (R) machine are characterized by performing dosimetric measurements topographically. By using a static gantry, opening the central 16 MLC leaves during the irradiations, and symmetrically scanning detectors 10 cm through each longitudinal SW, integral doses to a 'TomoTherapy equivalent' 10 x 10 cm(2) area are topographically measured. To quantify the effects of source occlusion for TomoTherapy, a quantity referred to as the integral scanned dose to slice width ratio (D/SW) is introduced. (D/SW) ratios are measured for SWs ranging from 0. 375 to 5 cm in size using ion chambers and a radiographic film. The measurements of the (D/SW) ratio are shown to be insensitive to the detectors used in this study. The (D/SW) ratios for TomoTherapy have values of unity in the range of SW sizes from 5 cm to approximately 2 cm. For SWs smaller than 2 cm in size, the source-occlusion effect substantially reduces the measured machine output and the value of the (D/SW) ratios. The topographic measurement method presented provides a way to directly evaluate the accuracy of the small-field <b>source</b> <b>model</b> <b>parameters</b> used in dose calculation algorithms. As an example, the electron source spot size of a Penelope Monte Carlo (MC) model of TomoTherapy was varied to match computed and measured (D/SW) ratios. It was shown that the MC results for small SW sizes were sensitive to that particular parameter...|$|R
40|$|Abstract A finite-difference {{modeling}} plus slowness {{analysis method}} is devel-oped to investigate near-source explosion energy partitioning and Lg-wave excita-tion. The finite-difference method {{is used to}} calculate seismic wave excitation and propagation, and an embedded array slowness analysis is used for quantifying how energy will be partitioned into the long-range propagation regime. Because of its high efficiency, the method can simulate near-source processes using very fine struc-tures. A large number of <b>source</b> and <b>model</b> <b>parameters</b> can be examined for broad-frequency ranges. As examples, P-pS-to-Lg and S*-to-Lg conversions in the presence of near-source scattering are tested as mechanisms for Lg-wave excitation. The nu-merical results reveal that the depth of the source and the depth of the scattering process have strong effects on P-to-S conversion and partitioning of energy into trapped or leaking signals. The Lg-wave excitation spectra from these mechanisms are also investigated. The modeling shows that S*-to-Lg excitation is generally stronger for low frequencies and shallow source depths whereas P-pS-to-Lg scatter-ing is stronger for high frequencies...|$|R
40|$|We {{present a}} {{realistic}} expanding <b>source</b> <b>model</b> with nine <b>parameters</b> {{that are necessary}} and sufficient to describe the main physics occuring during hydrodynamical freezeout of the excited hadronic matter produced in relativistic heavy-ion collisions. As a first test of the model, we compare it to data from central Si + Au collisions at p lab =A = 14 : 6 GeV/c measured in experiment E- 802 at the AGS. An overall 2 per degree of freedom of 1. 055 is achieved for a fit to 1416 data points involving invariant +, Γ, K +, and K Γ one-particle multiplicity distributions and + and K + two-particle correlations. The 99...|$|R
40|$|Various model {{equations}} {{are available}} for representing the excess Gibbs energy properties (osmotic and activity coefficients) of aqueous and other liquid mixed-electrolyte solutions. Scatchard's neutral-electrolyte model is among the simplest of these equations for ternary systems and contains terms that represent both symmetrical and asymmetric deviations from ideal mixing behavior when two single-electrolyte solutions are mixed in different proportions at constant ionic strengths. The usual form of this model allows from zero to six mixing parameters. In this report we present an analytical method for transforming the mixing <b>parameters</b> of neutral-electrolyte-type <b>models</b> with larger numbers of mixing parameters directly to those of models with fewer mixing parameters, without recourse to the source data used for evaluation of the original <b>model</b> <b>parameters.</b> The equations for this parameter conversion are based on an extension to ternary systems of the methodology of Rard and Wijesinghe [J. Chem. Thermodyn. 35, 439 - 473 (2003) ] and Wijesinghe and Rard [J. Chem. Thermodyn. 37, 1196 - 1218 (2005) ] that was applied by them to binary systems. It {{was found that the}} use of this approach with a constant ionic-strength cutoff of I {le} 6. 2 mol {center_dot} kg{sup - 1 } (the NaCl solubility limit) yielded parameters for the NaCl + SrCl{sub 2 } + H{sub 2 }O and NaCl + MgCl{sub 2 } + H{sub 2 }O systems that predicted osmotic coefficients {phi} in excellent agreement with those calculated using the same sets of parameters whose values were evaluated directly from the source data by least-squares, with root mean square differences of RMSE({phi}) = 0. 00006 to 0. 00062 for the first system and RMSE({phi}) = 0. 00014 to 0. 00042 for the second. If, however, the directly evaluated parameters were based on experimental data where the ionic strength cutoff varied with the ionic-strength fraction, i. e. because they were constrained by isopiestic ionic strengths (MgCl{sub 2 } + MgSO{sub 4 } + H{sub 2 }O) or solubility/oversaturation ionic strengths (NaCl + SrCl{sub 2 } + H{sub 2 }O and NaCl + MgCl{sub 2 } + H{sub 2 }O), then parameters converted by this approach assuming a constant ionic-strength cutoff yield RMSE({phi}) differences about an order of magnitude larger than the previous case. This indicates that for an accurate conversion of <b>model</b> <b>parameters</b> when the <b>source</b> <b>model</b> is constrained with variable ionic strength cutoffs, an extension of the parameter conversion method described herein will be required. However, when the <b>source</b> <b>model</b> <b>parameters</b> are evaluated at a constant ionic strength cutoff, such as when source isopiestic data are constrained to ionic strengths at or below the solubility limit of the less soluble component, or are Emf measurements that are commonly made at constant ionic strengths, then our method yields accurate converted models...|$|R
40|$|The {{estimation}} of finite fault earthquake <b>source</b> <b>models</b> is an inherently underdetermined problem: {{there is no}} unique solution to the inverse problem of determining the rupture history at depth {{as a function of}} time and space when our data are limited to observations at the Earth’s surface. Bayesian methods allow us to determine the set of all plausible <b>source</b> <b>model</b> <b>parameters</b> that are consistent with the observations, our a priori assumptions about the physics of the earthquake source and wave propagation, and models for the observation errors and the errors due to the limitations in our forward model. Because our inversion approach does not require inverting any matrices other than covariance matrices, we can restrict our ensemble of solutions to only those models that are physically defensible while avoiding the need to restrict our class of models based on considerations of numerical invertibility. We only use prior information that is consistent with the physics of the problem rather than some artefice (such as smoothing) needed to produce a unique optimal model estimate. Bayesian inference {{can also be used to}} estimate model-dependent and internally consistent effective errors due to shortcomings in the forward model or data interpretation, such as poor Green’s functions or extraneous signals recorded by our instruments. Until recently, Bayesian techniques have been of limited utility for earthquake source inversions because they are computationally intractable for problems with as many free parameters as typically used in kinematic finite fault models. Our algorithm, called cascading adaptive transitional metropolis in parallel (CATMIP), allows sampling of high-dimensional problems in a parallel computing framework. CATMIP combines the Metropolis algorithm with elements of simulated annealing and genetic algorithms to dynamically optimize the algorithm’s efficiency as it runs. The algorithm is a generic Bayesian Markov Chain Monte Carlo sampler; it works independently of the model design, a priori constraints and data under consideration, and so can be used for a wide variety of scientific problems. We compare CATMIP’s efficiency relative to several existing sampling algorithms and then present synthetic performance tests of finite fault earthquake rupture models computed using CATMIP...|$|R
