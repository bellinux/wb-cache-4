4541|10000|Public
25|$|Presents a <b>statistical</b> <b>test</b> to {{determine}} {{gender differences in}} toilet paper orientation.|$|E
25|$|<b>Statistical</b> <b>test</b> : A {{procedure}} whose inputs are {{samples and}} whose {{result is a}} hypothesis.|$|E
25|$|In 1994, J. A. Christen {{applied a}} strong <b>statistical</b> <b>test</b> to the {{radiocarbon}} data and {{concludes that the}} given age for the shroud is, from a statistical point of view, correct.|$|E
40|$|Faith {{community}} nurses need a {{basic understanding}} of common <b>statistical</b> <b>tests</b> and their interpretation to aid in the appraisal of research for evidence-based practice. The {{purpose of this article is}} to review statistical concepts, define common <b>statistical</b> <b>tests,</b> and interpret the results of <b>statistical</b> <b>tests.</b> Common <b>statistical</b> <b>tests</b> that measure differences in groups are independent samples t-test, paired sample t-tests, and analysis of variance. Common <b>statistical</b> <b>tests</b> that measure relationships are Pearson product moment correlation and chi-square. Knowledge of statistical concepts and common <b>statistical</b> <b>tests</b> assist in the appraisal of nursing research for evidence-based practice...|$|R
40|$|The {{applications}} of <b>statistical</b> <b>tests</b> often generate problems, {{because of their}} randomness. Here, stemming from the variability of <b>statistical</b> <b>tests,</b> the reproducibility probability of a statistically significant result () is estimated first. Then, <b>statistical</b> <b>tests</b> are defined {{on the basis of}} estimation. Lower bounds for, and then pointwise estimates are used. It is emphasized that the threshold for the pointwise estimator to define <b>statistical</b> <b>tests</b> turns out to be 1 / 2. Hence, taking the perspective of estimation, one can compute <b>statistical</b> <b>tests</b> and have suitable interpretations of their results. ...|$|R
50|$|Pseudorandom {{generators}} for low-degree polynomials are {{a particular}} instance of pseudorandom generators for <b>statistical</b> <b>tests,</b> where the <b>statistical</b> <b>tests</b> considered are evaluations of low-degree polynomials.|$|R
25|$|As {{an example}} of a <b>statistical</b> <b>test,</b> an {{experiment}} is performed to determine whether a coin flip is fair (equal chance of landing heads or tails) or unfairly biased (one outcome being more likely than the other).|$|E
25|$|There {{is great}} {{flexibility}} {{in the choice of}} <b>statistical</b> <b>test</b> (and thus the questions that an experiment can be designed to answer), and common choices include the Student's t test or linear regression. An important consideration with SPM, however, is that the large number of comparisons requires one to control the false positive rate with a more stringent significance threshold. This can be done either by modifying the initial <b>statistical</b> <b>test</b> to decrease the α value so as to make it harder for a particular voxel to exhibit a significant difference, or by modifying the clustering analysis in the second step by only considering a brain region's activation to be significant if it contains a certain number of voxels that exhibit a statistical difference.|$|E
25|$|A <b>statistical</b> <b>test</b> {{procedure}} {{is comparable to}} a criminal trial; a defendant is considered not guilty {{as long as his}} or her guilt is not proven. The prosecutor tries to prove the guilt of the defendant. Only when there is enough charging evidence the defendant is convicted.|$|E
40|$|Abstract. In {{this paper}} we {{demonstrate}} {{the effectiveness of}} <b>statistical</b> <b>testing</b> for error detection on {{the example of a}} Programmable Logic System (PLS). The introduction of <b>statistical</b> <b>testing</b> arose from the wish to quantify the PLS’s reliability. An appropriate <b>statistical</b> <b>testing</b> algorithm was devised and implemented, which is described in detail in this paper. We compare the results of <b>statistical</b> <b>testing</b> with those of a variety of other testing methods employed on the PLS. In terms of differences detected per number of <b>tests,</b> <b>statistical</b> <b>testing</b> showed an outstanding effectiveness. Furthermore, it detected a problem, which was missed by all other testing techniques. This together with its potential for reliability quantification illustrates its importance for system validation as part of a risk–based safety–case. ...|$|R
50|$|Before {{analyzing}} {{data for}} biological variation, QC steps must be performed {{to determine whether}} the data is fit for <b>statistical</b> <b>testing.</b> <b>Statistical</b> <b>tests</b> are sensitive {{to the nature of the}} input data.|$|R
5000|$|Let [...] be a {{class of}} {{functions}}.These functions are the <b>statistical</b> <b>tests</b> that the pseudorandom generator will try to fool, and they are usually algorithms.Sometimes the <b>statistical</b> <b>tests</b> are also called adversaries.|$|R
25|$|If {{experimental}} error follows a normal distribution, then, {{because of the}} linear relationship between residuals and observations, so should residuals, but since the observations are only {{a sample of the}} population of all possible observations, the residuals should belong to a Student's t-distribution. Studentized residuals are useful in making a <b>statistical</b> <b>test</b> for an outlier when a particular residual appears to be excessively large.|$|E
25|$|Statistical Parametric Mapping (SPM) is {{a method}} for {{determining}} whether the activation of a particular brain region changes between experimental conditions, stimuli, or over time. The essential idea is simple, and consists of two major steps: first, one performs a univariate <b>statistical</b> <b>test</b> on each individual voxel between each experimental condition. Second, one analyzes the clustering of the voxels that show statistically significant differences, and determines which brain regions exhibit different levels of activation under different experimental conditions.|$|E
25|$|The {{number of}} {{treatment}} units (subjects {{or groups of}} subjects) assigned to control and treatment groups, affects an RCT's reliability. If {{the effect of the}} treatment is small, the number of treatment units in either group may be insufficient for rejecting the null hypothesis in the respective <b>statistical</b> <b>test.</b> The failure to reject the null hypothesis would imply that the treatment shows no statistically significant effect on the treated in a given test. But as the sample size increases, the same RCT may be able to demonstrate a significant effect of the treatment, even if this effect is small.|$|E
40|$|Random {{sequences}} {{and random}} numbers constitute {{a necessary part}} of cryptography. Many cryptographic protocols depend on random values. Randomness is measured by <b>statistical</b> <b>tests</b> and hence security evaluation of a cryptographic algorithm deeply depends on <b>statistical</b> randomness <b>tests.</b> In this work we focus on statistical distributions of runs of lengths one, two, and three. Using these distributions we state three new <b>statistical</b> randomness <b>tests.</b> New tests use χ 2 distribution and, therefore, exact values of probabilities are needed. Probabilities associated runs of lengths one, two, and three are stated. Corresponding probabilities are divided into five subintervals of equal probabilities. Accordingly, three new <b>statistical</b> <b>tests</b> are defined and pseudocodes for these new <b>statistical</b> <b>tests</b> are given. New <b>statistical</b> <b>tests</b> are designed to detect the deviations in the number of runs of various lengths from a random sequence. Together with some other <b>statistical</b> <b>tests,</b> we analyse our tests’ results on outputs of well-known encryption algorithms and on binary expansions of e, π, and 2. Experimental results show the performance and sensitivity of our tests...|$|R
40|$|White and Gorard make {{important}} and relevant criticisms {{of some of}} the methods commonly used in social science research, but go further by criticising the logical basis for inferential <b>statistical</b> <b>tests.</b> This paper comments briefly on matters we broadly agree on with them and more fully on matters where we disagree. We agree that too little attention is paid to the assumptions underlying inferential <b>statistical</b> <b>tests,</b> to the design of studies, and that p-values are often misinterpreted. We show why we believe their argument concerning the logic of inferential <b>statistical</b> <b>tests</b> is flawed, and how White and Gorard misrepresent the protocols of inferential <b>statistical</b> <b>tests,</b> and make brief suggestions for rebalancing the statistics curriculum...|$|R
40|$|Abstract. Safety-critical {{software}} often {{includes some}} rarely executed critical operations {{that are often}} inadequately <b>tested</b> in <b>statistical</b> <b>testing</b> based reliability estimation. However, {{it is necessary to}} assess the reliability of safety-critical software to a high degree of confidence before it is deployed in the field. This paper discusses how to reduce safety-critical software <b>statistical</b> <b>testing</b> cost based on importance sampling technique. When both the critical operations and the entire software are adequately tested, the method can still compute the unbiased software reliability from the test results with much less test cases. Thus, the <b>statistical</b> <b>testing</b> cost of safety-critical software can be reduced effectively The simulated annealing algorithm for calculating optimum transition probabilities of the Markov chain usage model for reducing software <b>statistical</b> <b>testing</b> cost is also presented...|$|R
25|$|The LOD score (logarithm (base 10) of odds), {{developed}} by Newton Morton, is a <b>statistical</b> <b>test</b> {{often used for}} linkage analysis in human, animal, and plant populations. The LOD score compares the likelihood of obtaining the test data if the two loci are indeed linked, to the likelihood of observing the same data purely by chance. Positive LOD scores favour the presence of linkage, whereas negative LOD scores indicate that linkage is less likely. Computerised LOD score analysis is {{a simple way to}} analyse complex family pedigrees {{in order to determine the}} linkage between Mendelian traits (or between a trait and a marker, or two markers).|$|E
25|$|Statistics is {{increasingly}} {{being taught in}} schools with hypothesis testing {{being one of the}} elements taught. Many conclusions reported in the popular press (political opinion polls to medical studies) are based on statistics. An informed public should understand the limitations of statistical conclusions and many college fields of study require a course in statistics for the same reason. An introductory college statistics class places much emphasis on hypothesis testing – perhaps half of the course. Such fields as literature and divinity now include findings based on statistical analysis (see the Bible Analyzer). An introductory statistics class teaches hypothesis testing as a cookbook process. Hypothesis testing is also taught at the postgraduate level. Statisticians learn how to create good <b>statistical</b> <b>test</b> procedures (like z, Student's t, F and chi-squared). Statistical hypothesis testing is considered a mature area within statistics, but a limited amount of development continues.|$|E
2500|$|One of {{the people}} {{performs}} a <b>statistical</b> <b>test</b> (e.g. a Neyman-Pearson test or the comparison of the accumulated weight of evidence to a threshold) of the hypothesis that [...] "All ravens are black", while the other tests the hypothesis that [...] "All non-black objects ...|$|E
30|$|AIS 20 / 31 {{includes}} eight standard <b>statistical</b> <b>tests</b> such as {{the poker}} test, the long run test, and the uniform distribution test. We evaluated our TRNGs by using the <b>statistical</b> <b>tests</b> for test procedure A in AIS 20 / 31. In this procedure, five <b>statistical</b> <b>tests</b> out of eight are used. For an ideal random number, the probability of passing the tests is ≈ 0.9987, and that of failing more than two tests is ≈ 0. If one test is failed, a second run is done. If the second test is failed, test procedure A is failed [3].|$|R
40|$|Despite being a {{standard}} tool for data analysis in many scientific fields, <b>statistical</b> <b>testing</b> {{has always been}} associated to criticism and controversies. Recently, {{the debate on the}} role of <b>statistical</b> <b>testing</b> in science has been particularly vivid and coupled with the debate on the lack of reproducibility of scientific results in many disciplines. We review some recent critics and purported consequences of <b>statistical</b> <b>testing</b> use and abuse and discuss proposed remedies, which entails either improving testing or abandoning it for alternative, more appropriate, strategies...|$|R
50|$|In {{observational}} {{epidemiological studies}} where certain data is already known and certain effects are expected, <b>statistical</b> <b>tests</b> for {{significance of the}} results are not normally required. Yet the IAEA has challenged such papers that do not include <b>statistical</b> <b>tests</b> and confidence intervals, and questioned whether the observed effects are due to chance. Eastern scientists {{are faced with a}} catch-22 situation whereby they either leave out <b>statistical</b> <b>tests,</b> and are dismissed, or else apply the tests, leading western scientists to conclude that there is no real effect.|$|R
2500|$|In 2013 {{a team of}} {{physicists}} {{claimed that}} they had found mathematical and semiotic patterns in the genetic code which, they think is evidence for such a signature. This claim has been refuted by biologist PZ Myers who said, writing in Pharyngula: [...] In a later peer-reviewed article, the authors address the operation of natural law in an extensive <b>statistical</b> <b>test,</b> and draw the same conclusion {{as in the previous}} article. In special sections they also discuss methodological concerns raised by PZ Myers and some others.|$|E
2500|$|Several {{theorists have}} {{proposed}} {{the idea of}} the Markov chain <b>statistical</b> <b>test</b> (MCST), a method of conjoining Markov chains to form a [...] "Markov blanket", arranging these chains in several recursive layers ("wafering") and producing more efficient test sets—samples—as a replacement for exhaustive testing. MCSTs also have uses in temporal state-based networks; Chilukuri et al.'s paper entitled [...] "Temporal Uncertainty Reasoning Networks for Evidence Fusion with Applications to Object Detection and Tracking" [...] (ScienceDirect) gives a background and case study for applying MCSTs to a wider range of applications.|$|E
2500|$|In <b>{{statistical}}</b> <b>test</b> theory, {{the notion}} of statistical error {{is an integral part}} of hypothesis testing. The test requires an unambiguous statement of a null hypothesis, which usually corresponds to a default [...] "state of nature", for example [...] "this person is healthy", [...] "this accused is not guilty" [...] or [...] "this product is not broken". An alternative hypothesis is the negation of null hypothesis, for example, [...] "this person is not healthy", [...] "this accused is guilty" [...] or [...] "this product is broken". The result of the test may be negative, relative to the null hypothesis (not healthy, guilty, broken) or positive (healthy, not guilty, not broken). [...] If the result of the test corresponds with reality, then a correct decision has been made. However, if the result of the test does not correspond with reality, then an error has occurred. Due to the statistical nature of a test, the result is never, except in very rare cases, free of error. Two types of error are distinguished: ...|$|E
50|$|<b>Statistical</b> <b>tests</b> for the {{antimode}} are known.|$|R
40|$|Selecting loss {{distribution}} model by using <b>statistical</b> <b>tests</b> {{should be done}} to make sure it is accepted and the model selected can be applied in insurance. This study aims to select the best {{loss distribution}} model by applying <b>statistical</b> fitting <b>test</b> on seveal fitted Loss distribution model. The loss distribution model represents sample data from motor insurance claims in Malaysia. The model is obtained by using maximum likelihood estimation. The <b>statistical</b> <b>tests</b> used are Pearson: goodness-of fit statistic and Kolmogorov-Smirnov statistic. Based on results from maximum likelihood estimation and <b>statistical</b> <b>tests,</b> a single best model will be selected and the model will be applied in insurance...|$|R
50|$|Statsmodels is a Python {{package that}} {{allows users to}} explore data, {{estimate}} statistical models, and perform <b>statistical</b> <b>tests.</b> An extensive list of descriptive statistics, <b>statistical</b> <b>tests,</b> plotting functions, and result statistics are available for different types of data and each estimator. It complements SciPy's stats module.|$|R
2500|$|In 2010, {{based on}} [...] "the {{vast array of}} {{molecular}} sequences now available from all domains of life," [...] a formal test of universal common ancestry was published. The formal test favored {{the existence of a}} universal common ancestor over a wide class of alternative hypotheses that included horizontal gene transfer. While the formal test overwhelmingly favored the existence of a single LUCA, this does not imply that the LUCA was ever alone. Instead, it was one of several early microbes. [...] However, given that many other nucleotides are possible besides those that are actually used in DNA and RNA today, it is almost certain that all organisms do have a single common ancestor. This is because it is extremely unlikely that organisms which descended from separate incidents where organic molecules initially came together to form cell-like structures would be able to complete a horizontal gene transfer without garbling each other's genes, converting them into noncoding segments. Further, many more amino acids are chemically possible than the twenty found in modern protein molecules. These lines of chemical evidence, taken into account for the formal <b>statistical</b> <b>test</b> by Theobald (2010), point to a single cell having been the LUCA in that, although other early microbes probably existed, only the LUCA's descendents survived beyond the Paleoarchean Era. With a common framework in the AT/GC rule and the standard twenty amino acids, horizontal gene transfer would have been feasible and could have been very common later on among the progeny of that single cell.|$|E
50|$|Evolutionary {{algorithm}} {{based on}} NIST <b>Statistical</b> <b>Test</b> Suite.|$|E
5000|$|Jarque-Bera test, an <b>statistical</b> <b>test</b> {{named after}} Carlos Jarque and Anil K. Bera ...|$|E
5000|$|StatiBot, {{interactive}} online {{expert system}} on <b>statistical</b> <b>tests.</b>|$|R
5000|$|Multivariate <b>testing,</b> a <b>statistical</b> <b>testing</b> of user {{responses}} ...|$|R
40|$|Article aims of {{time series}} econometric model of {{macroeconomic}} variable GDP in the US economy. Because {{that is a}} nonstationary time series, there are used several <b>statistical</b> <b>tests</b> in order {{to turn into a}} stationary series. After applying these tests, the time series became stationary and integrated of order I; thus, we use Box-Jenkins procedure for the determination of ARMA. We estimate by OLS the parameters of various models. Performances chosen ARIMA model (1, 1, 1) are verified on the basis of classical <b>statistical</b> <b>tests</b> and forecasting. stationary time series; nonstationary time series; <b>statistical</b> <b>tests.</b> ...|$|R
