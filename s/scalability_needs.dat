22|120|Public
5000|$|Grid MP [...] - [...] a job {{scheduler}} and application provisioning platform. It is offered in various editions, {{depending on the}} <b>scalability</b> <b>needs</b> of the customer. The company publishes a list of high-profile organizations using Grid MP on the United Devices website.|$|E
50|$|Clusterpoint {{database}} eliminates customer integration efforts among database, {{search and}} analytics platforms. Clusterpoint database replaces integrated multi-platform solutions with a single-platform and one-API solution, typically, where SQL RDBMS data {{is used in}} combination with an enterprise search engine to address performance and <b>scalability</b> <b>needs</b> of web and mobile applications, or where Big data and analytics tools such as Hadoop might be needed due to sheer volume of data or large computing workloads.|$|E
40|$|International audienceIn {{order to}} {{overcome}} the myopia problem, routing strategies {{must be based on}} formal representations of flow that automatically account for modifications in the values of parameters of interest and in the model itself. This work addresses this problem and discusses how to automatically incorporate resources (e. g. workstations/ transportation devices/ storages) in a Petri Net derived model of flow that is modifiable at runtime to reflect and influence the routing in a manufacturing line. The modelling approach takes into consideration <b>scalability</b> <b>needs</b> and was experimentally validated. The applicability of the models is shown for PN-based dynamic scheduling...|$|E
40|$|We {{define a}} global routing {{mechanism}} for the NetInf protocol, part of the NetInf information-centric networking architecture. The mechanism makes use of two levels of aggregation {{in order to provide}} the <b>scalability</b> <b>needed</b> for a global network. An anticipated 10 ^ 15 number of individual named data objects are aggregated to in the order of 500 K routing hints which are very feasible to handle with existing routing technology. The hints are then used to forward requests for named data towards the publisher. ...|$|R
40|$|A popular {{approach}} to verification of software system correctness is model checking. To achieve <b>scalability</b> <b>needed</b> for large systems, model checking {{has to be}} augmented with abstraction. In this paper, we {{provide an overview of}} selected techniques of program verification based on predicate abstraction. We focus on techniques that advanced the state-of-the-art in a significant way, including counterexample-guided abstraction refinement, lazy abstraction, and current trends in the form of extensions targeting, for example, data structures and multi-threading. We discuss limitations of these techniques and present our plans for addressing some of them...|$|R
40|$|International audienceWe {{present a}} {{scalable}} encoding {{strategy for the}} 3 D facial data in various bandwidth scenarios. The <b>scalability,</b> <b>needed</b> to cater diverse clients, is achieved through the multiresolution characteristic of JPEG 2000. The disparate 3 D facial data is synchronously unified by the application of data hiding wherein the 2. 5 D facial model {{is embedded in the}} corresponding 2 D texture in the discrete wavelet transform (DWT) domain. The unified file conforms to the JPEG 2000 standard and thus no novel format is introduced. The method is effective and {{has the potential to be}} applied in videosurveillance and videoconference applications...|$|R
40|$|Abstract—We {{define a}} digital {{forensic}} investigative process as scalable {{if it can}} keep the average time per investigation constant {{in the face of}} growing target sizes and diversity. In technical terms, we consider scalability in terms of speed, cost, extensibility, and user interface abstractions. We argue that both commercial and open source products are showing a growing disconnect with actual <b>scalability</b> <b>needs</b> of digital forensic practice. In our view, the current technical approaches need to be rethought from the ground up. We put forward the idea that a new generation of technologies developed for the Internet should be adapted as the architectural basis for developing the new generation of open and scalable forensic tools. I...|$|E
40|$|This paper {{describes}} the system centric simulation methodology used for stress testing of Manufacturing Execution System (MES) in Intel. System centric simulation involves testing {{such that the}} system components (infrastructure stack and software) are characterized for the load they would experience in production, irrespective of how that load is exerted. A new manufacturing execution system software is introduced in Intel’s latest fabrication facility. Validation of the product under stress is vital to ensuring that the mission critical capability {{will be able to}} comply with Intel’s reliability, availability, performance, and <b>scalability</b> <b>needs.</b> The System centric simulation model allows for accurate reproduction of real-world scenarios while not requiring the expensive setup and execution of the complete set of defined use cases. ...|$|E
40|$|For their <b>scalability</b> <b>needs,</b> data-intensive Web {{applications}} {{can use a}} Database Scalability Service (DBSS), which caches applications ’ query {{results and}} answers queries on their behalf. One way for applications to address their security/privacy concerns when using a DBSS is to encrypt all data that passes through the DBSS. Doing so, however, causes the DBSS to invalidate large regions of its cache when data updates occur. To invalidate more precisely, the DBSS needs help in order to know which results to invalidate; such help inevitably reveals some properties about the data. In this paper, we present invalidation clues, a general technique that enables applications to reveal little data to the DBSS, yet {{limit the number of}} unnecessary invalidations. Compared wit...|$|E
40|$|In {{this paper}} we examine several access control {{problems}} {{that occur in}} an object-based distributed system that permits objects to be replicated on multiple machines. First, there is the classical access control problem, which relates to which users can execute which methods. Second, we identified a reverse access control problem, which concerns which replicas can execute which methods for authorized users. Finally, there {{is the issue of}} how updates are propagated securely from replica to replica. Our solution uses roles and preserves the <b>scalability</b> <b>needed</b> in a world-wide distributed system. 1 Security in Distributed Systems Security in distributed systems differs from operatin...|$|R
40|$|The paper {{describes}} {{the development and}} definition of Intel's new Internet Streaming SIMD Extensions introduced on the Pentium III processor. The extensions are divided into three categories: SIMD-FP, New Media, and Streaming Memory instructions. The new extensions accelerate the 3 D geometry pipeline by nearly 2 x that of the previous-generation processor while enabling new applications, such as real-time MPEG- 2 encode. The Pentium III processor implementations achieved the desired goal at a modest 10 % increase in die size. The definition achieved the short-term goal while still providing the performance <b>scalability</b> <b>needed</b> for future implementations...|$|R
40|$|Data items {{spread over}} {{a large number of}} nodes Which node stores which data item? A lookup {{mechanism}} needed � Centralized directory-> bottleneck/single point of failure Query Flooding-> <b>scalability</b> concerns <b>Need</b> more structure! Solution: Chord (a distributed lookup protocol) Chord supports only one operation: given key, maps key on to a nodeRelated Wor...|$|R
40|$|Communication {{subsystem}} plays {{a pivotal}} role in achieving scalable performance in clusters. The communication semantics employed are dictated by the programming model used by the application such as MPI, UPC, etc. Out of the gamut of communication primitives, collective and one-sided operations are especially significant and have to be designed harnessing the capabilities and features exposed by the underlying networks. In some cases, there is a direct match between the semantics of the operations and the underlying network primitives. InfiniBand provides two transport modes: (i) Connection-oriented Reliable connection (RC) supporting Memory and Channel semantics and (ii) Connection-less Unreliable Datagram (UD) supporting Channel semantics. Achieving good performance and <b>scalability</b> <b>needs</b> careful analysis and design of communication primitives based on these options. In this paper, we evaluate the scalability and performance tradeoff...|$|E
30|$|Clearly, future {{networks}} {{will have}} to support an explosive growth in traffic volume and connected devices, to provide exceptional capabilities for accessing and sharing information. The unprecedented scale and degree of uncertainty will amplify the complexity of traffic engineering tasks, such as congestion control, traffic prediction, classification, and routing, {{as well as the}} exposure to faults and security attacks. Although ML-based solutions have shown promising results to address many traffic engineering challenges, their <b>scalability</b> <b>needs</b> to be evaluated with the envisioned volume of data, number of devices and applications. On the other hand, existing ML-based approaches for fault and security management focus mostly on single-tenant and single-layer networks. To develop the fault and security management framework for future networks, existing ML approaches should be extended or re-architected {{to take into account the}} notion of multi tenancy in multi layer networks.|$|E
40|$|Online Social Networks (OSN) face serious {{scalability}} challenges due {{to their}} rapid growth and popularity. To address this issue we present a novel approach to scale up OSN called One Hop Replication (OHR). Our system combines partitioning and replication in a middleware to transparently scale up a centralized OSN design, and therefore, avoid the OSN application to undergo the costly transition to a fully distributed system to meet its <b>scalability</b> <b>needs.</b> OHR exploits some of the structural characteristics of Social Networks: 1) {{most of the information}} is one-hop away, and 2) the topology of the network of connections among people displays a strong community structure. We evaluate our system and its potential benefits and overheads using data from real OSNs: Twitter and Orkut. We show that OHR has the potential to provide out-of-the-box transparent scalability while maintaining the replication overhead costs in check...|$|E
40|$|Abstract — Energy {{efficiency}} is typically {{the most important}} requirement in wireless sensor networks (WSNs). However, in many application domains, additional requirements such as reliability, timeliness and <b>scalability</b> <b>need</b> to be considered as well. As emphasized in previous studies, the IEEE 802. 15. 4 MAC protocol has severe limitations in terms of performance and energy efficiency, which make it unsuitable for many application scenarios. In this paper we propose an asynchronous adaptive algorithm for periodic data reporting that leverages the 802. 15. 4 Beacon-Disabled Mode but desynchronizes the access times of different sensor nodes, so as to minimize the collision probability. Unlike previous desynchronization schemes, the proposed approach only relies on local information. Simulation results show that our asynchronous algorithm can provide a performance {{very similar to that}} of a TDMA scheme but, unlike TDMA, it does not requires any synchronization among nodes...|$|R
40|$|A global-scale {{low cost}} outdoor Internet access {{infrastructure}} is finally attainable. Emerging projects are leveraging {{the proliferation of}} private Wi-Fi networks to build a globalscale ubiquitous access infrastructure from autonomous, independently owned Internet connections at homes and other private properties. To ensure the traceability and accountability required by the broadband ISPs and private owners of these Wi-Fi networks, reliable authentication and authorization are needed. This paper describes authentication on the edge, a localized and distributed authentication method for open Wi-Fi networks. Three main ideas are used {{to adapt to the}} variability and unreliability of these networks: the use of certificate-based authentication, the distribution of a segmented certificate revocation list to all entities and the self organization of access points into a social lookup network. These methods achieve the <b>scalability</b> <b>needed</b> for the overwhelming size and volume of a global network and increase resiliency against temporary failures in the infrastructure...|$|R
40|$|Abstract—Important {{challenges}} in interoperability, reliability, and <b>scalability</b> <b>need</b> {{to be addressed}} before the Smart Grid vision can be fulfilled. The sheer scale of the electric grid and the criticality of the communication among its subsystems for proper management, demands a scalable and reliable communication framework able {{to work in an}} heterogeneous and dynamic environment. Moreover, the need to provide full interoperability between diverse current and future energy and non-energy systems, along with seamless discovery and configuration of a large variety of networked devices, ranging from the resource constrained sensing devices to servers in data centers, requires an implementation-agnostic Service Oriented Architecture. In this position paper we propose that this challenge can be addressed with a generic framework that reconciles the reliability and scalability of Peer-to-Peer systems, with the industrial standard interoperability of Web Services. We illustrate the flexibility of the proposed framework by showing how it can be used in two specific scenarios. I...|$|R
40|$|Abstract—Cloud {{computing}} {{represents a}} solution for applications with high <b>scalability</b> <b>needs</b> where usage patterns, and therefore resource requirements, may fluctuate based on external circumstances such as exposure or trending. However, {{in order to take}} advantage of the cloud’s benefits, software engineers need to be able to express the application’s needs in quantifiable terms. Additionally, cloud providers have to understand such requirements and offer methods to acquire the necessary infrastructure to fulfill the users ’ expectations. In this paper, we discuss the design and implementation of an Infrastructure as a Service cloud manager such that non-functional requirements determined during the requirements analysis phase can be mapped to properties for a group of Virtual Appliances running the application. The discussed management system ensures that expected Quality of Service is maintained during execution and can be considered during different development phases. I...|$|E
40|$|Abstract—Data {{storage in}} cloud {{computing}} centres is gaining popularity for personal and institutional data backups {{as well as}} for highly scalable access from software applications run-ning on attached compute servers. The data is usually access-protected, encrypted and replicated depending on the security and <b>scalability</b> <b>needs.</b> Despite the advances in technology, the practical usefulness and longevity of so-called cloud storage is limited in today’s systems which severely impacts the acceptance and adoption rates. Therefore, we introduce a novel cloud storage management architecture which combines storage resources from multiple providers so that redundancy, security and other non-functional properties can be adjusted adequately {{to the needs of the}} user. Furthermore, we present NubiSave, a user-friendly implementation with configurable adequate overhead which runs on and integrates into contemporary desktop systems. Finally, a brief analysis of measurements performed by us shows how well the system performs in a real distributed cloud storage environment. I...|$|E
40|$|A {{new type}} of {{manufacturing}} cell, with characteristics of reconfigurability, reusability and <b>scalability,</b> <b>needs</b> to be developed. To achieve the agile reconfiguration of a manufacturing cell, the cell control system must be rapidly and efficiently generated or modified. In this paper, a multi-agent based architecture is defined that supports the design and implementation of highly reconfigurable control systems for agile manufacturing cells, which are comprised of resource agents (material processing agents, material handling agents, and material storage agents), a control agent, and an information agent, {{in order to reduce}} costs and increase the control system's agility with respect to the changing environment. Different agents in the cell control system can be organized dynamically, communicate with each other through messages, and cooperate with each other to perform flexibly the task in the cell control system. The structure of the agents is proposed and the message-passing between agents is discussed in detail. Department of Industrial and Systems Engineerin...|$|E
40|$|Grid {{technology}} is evolving. Open Grid Service Architecture (OGSA) defines grid services based on Web services technology. The grids {{are starting to}} expand towards another popular resource-sharing technology: the peer-to-peer overlay networks. As grids are evolving, issues of <b>scalability</b> <b>need</b> to be addressed. P 2 P is a very scalable resource sharing technology. These two technologies are con-verging towards each other. The project includes study and a survey of Grid services (in the context of OGSA: Open Grid Service Architecture), technologies for Web services, P 2 P overlay networks, and related work. As a case study, the thesis presents design of a job meta-scheduling Grid service. The service {{is based on the}} peer-to-peer (P 2 P) technology, namely, on the DKS (Distributed K-ary search System) that is a structured P 2 P system. The service incorporates the properties of the system it is based on, such as decentralization, fault-tolerance, self-organization and scala-bility...|$|R
40|$|The {{overheads}} in {{a parallel}} system that limit its <b>scalability</b> <b>need</b> to be identified and separated in order to enable parallel algorithm design {{and the development of}} parallel machines. Such overheads may be broadly classified into two components. The first one is intrinsic to the algorithm and arises due to factors such as the work-imbalance and the serial fraction. The second one is due to the interaction between the algorithm and the architecture and arises due to latency and contention in the network. A top-down approach to scalability study of shared memory parallel systems is proposed in this research. We define the notion of overhead functions associated with the different algorithmic and architectural characteristics to quantify the scalability of parallel systems; we develop a method for separating the algorithmic overhead into a serial component and a work-imbalance component; we also develop a method for isolating the overheads due to network latency and contention from the [...] ...|$|R
30|$|Further studies {{regarding}} <b>scalability</b> are <b>needed</b> in literature. It {{is clear}} that NoSQL databases are scalable, {{but the question of}} which scale the most, or with the best performance, is still left unanswered. Nevertheless, we can conclude that popular choices for highly scalable systems are Cassandra and HBase. One must also take notice that scalability will be influenced by the particular choice of configuration parameters.|$|R
40|$|Cloud {{computing}} {{increases the}} level of connectivity between software applications. IT management applications delivered as a service may need to connect {{to tens of thousands}} of endpoint systems. In order to validate the application's reliability and performance at these very large scales, its <b>scalability</b> <b>needs</b> to be tested before being deployed in the cloud. We use an emulation approach, whereby endpoints are modelled and then executed in an emulation environment, which we call 'Kaluta'. The key aspect is to balance the modelling of the endpoint systems such that it is rich enough to 'fool' an unmodified application-under-test into thinking that it is talking to real systems, but lightweight enough such that tens of thousands of instances of model systems can be executed simultaneously in the emulation engine. We present an industry case study [...] -CA IdentityMinder(TM) -as-a-Service [...] -to demonstrate the effectiveness of using emulation to validate the scalability of a cloud hosted application...|$|E
40|$|Future {{warehouses}} {{will be made}} of modular embedded entities with communication {{ability and}} energy aware operation attached to the traditional materials handling and warehousing objects. This advancement is mainly to fulfill the flexibility and <b>scalability</b> <b>needs</b> of the emerging warehouses. However, it leads to a new layer of complexity during development and evaluation of such systems due to the multidisciplinarity in logistics, embedded systems, and wireless communications. Although each discipline provides theoretical approaches and simulations for these tasks, many issues are often discovered in a real deployment of the full system. In this paper we introduce PhyNetLab as a real scale warehouse testbed made of cyber physical objects (PhyNodes) developed {{for this type of}} application. The presented platform provides a possibility to check the industrial requirement of an IoT-based warehouse in addition to the typical wireless sensor networks tests. We describe the hardware and software components of the nodes in addition to the overall structure of the testbed. Finally, we will demonstrate the advantages of the testbed by evaluating the performance of the ETSI compliant radio channel access procedure for an IoT warehouse...|$|E
40|$|The publish/subscribe {{interaction}} {{paradigm is}} today becoming mainstream {{in a large}} number of very large scale applications like news syndication (with RSS) or massive multiplayer games. These applications are often still implemented by means of centralized services that will hardly scale with the user growth expected in the next years. Modern publish/subscribe systems are striving to address these <b>scalability</b> <b>needs</b> to play a dominant role in this future market. A very important contribution, on the road to reach this goal, is given by the interest clustering techniques adopted by these systems. Interest clustering aims at putting in close applicative relationship groups of users sharing similar interests {{in order to reduce the}} effort needed to dispatch a message to group. This technique can be applied to event dissemination mechanisms based on filtering to reduce the total amount of messages generated during event routing and, consequently, improve the overall system performance. In this paper we explore this topic to discover the potentialities of interest clustering, to understand how it can be implemented in a publish/subscribe system, and to study, through a small focussed survey, the central role played by this technique in modern systems. Copyright 2008 ACM...|$|E
50|$|The {{application}} proved hugely popular, and by late October, Line {{experienced an}} unexpected server overload. After {{concluding that the}} <b>scalability</b> process <b>needed</b> to be improved, NHN Japan chose to adopt HBase as the primary storage for user profiles, contacts, and groups. Within eighteen months of its release, Line reached 100 million users; six months later, it reached 200 million and by November 25, 2013, was used by 300 million users.|$|R
40|$|Distributed denial-of-service {{attacks on}} public servers have {{recently}} become a serious problem. To assure that network services {{will not be}} interrupted and more effective defense mechanisms to protect against malicious traffic, especially SYN floods. One problem in detecting SYN flood traffic is that server nodes or firewalls cannot distinguish the SYN packets of normal TCP connections from those of a SYN flood attack. Another problem is single-point defenses (e. g. firewalls) lack the <b>scalability</b> <b>needed</b> to handle {{an increase in the}} attack traffic. We have designed a new defense mechanism to detect the SYN flood attacks. First, we introduce a mechanism for detecting SYN flood traffic more accurately by taking into consideration the time variation of arrival traffic. We investigate the statistics regarding the arrival rates of both normal TCP SYN packets and SYN flood attack packets. We then describe a new detection mechanism based on these statistics. Through the trace driven approach defense nodes which receive the alert messages can identify legitimate traffic and block malicious traffic by delegating SYN/ACK packets...|$|R
40|$|The CMS {{experiment}} at LHC {{started using}} the Resource Broker (by the EDG and LCG projects) to submit Monte Carlo production and analysis jobs to distributed computing {{resources of the}} WLCG infrastructure over 6 years ago. Since 2006 the gLite Workload Management System (WMS) and Logging & Bookkeeping (LB) are used. The interaction with the gLite-WMS/LB happens through the CMS production and analysis frameworks, respectively ProdAgent and CRAB, through a common component, BOSSLite. The important improvements recently made in the gLite-WMS/LB {{as well as in}} the CMS tools and the intrinsic independence of different WMS/LB instances allow CMS to reach the stability and <b>scalability</b> <b>needed</b> for LHC operations. In particular the use of a multi-threaded approach in BOSSLite allowed to increase the scalability of the systems significantly. In this work we present the operational set up of CMS production and analysis based on the gLite-WMS and the performances obtained in the past data challenges and in the daily Monte Carlo productions and user analysis usage in the experiment...|$|R
40|$|As {{the vision}} of the Internet of Things (IoT) becomes a reality, {{thousands}} of devices will be connected to IoT platforms in smart cities and regions. These devices will actively send data updates to cloud-based platforms, as part of smart applications in domains like healthcare, traffic and pollution monitoring. Therefore, it is important to study the ability of modern IoT systems to handle high rates of data updates coming from devices. In this work we evaluated the per- formance of components of the Internet of Things Services Enablement Architecture of the European initiative FIWARE. We developed a testbed that is able to inject data updates using MQTT and the CoAP-based Lightweight M 2 M protocols, simulating large scale IoT deploy- ments. Our extensive tests considered the vertical and horizontal scalability of the components of the platform. Our results found the limits of the components when handling the load, and the scaling strategies that should be targeted by implementers. We found that vertical scaling is not an effective strategy in comparison to the gains achieved by horizontally scaling the database layer. We reflect about the load testing methodology for IoT systems, the <b>scalability</b> <b>needs</b> of different layers and conclude with future challenges in this topic...|$|E
40|$|Testing is {{the primary}} {{approach}} for detecting software defects. A major challenge faced by testers lies in crafting efficient test suites, able to detect a maximum number of bugs with manageable effort. To do so, they rely on coverage criteria, which define some precise test objectives to be covered. However, many common criteria specify {{a significant number of}} objectives that occur to be infeasible or redundant in practice, like covering dead code or semantically equal mutants. Such objectives are well-known to be harmful to the design of test suites, impacting both the efficiency and precision of testers' effort. This work introduces a sound and scalable formal technique able to prune out {{a significant part of the}} infeasible and redundant objectives produced by a large panel of white-box criteria. In a nutshell, we reduce this challenging problem to proving the validity of logical assertions in the code under test. This technique is implemented in a tool that relies on weakest-precondition calculus and SMT solving for proving the assertions. The tool is built on top of the Frama-C verification platform, which we carefully tune for our specific <b>scalability</b> <b>needs.</b> The experiments reveal that the tool can prune out up to 27 % of test objectives in a program and scale to applications of 200 K lines of code...|$|E
40|$|Abstract: To {{extend our}} research, {{in this paper}} we are taking review of process of {{localization}} and different methods used for localization in WBAN. The Mobile body area networks are considered as the ascendants of WBANs where the mobility plays a major role. In addition to mobility in WSN due to either becomes very important for two reasons 1) it may be accidental side effect and so may prove detrimental to network performance such as 2) this is a desirable property may prove to be and therefore can increase network performance. Robustness and <b>scalability</b> <b>needs</b> for a turn in sensor networks Is cost effective and accurate localization of the need {{in the past two decades}} from WBANs since one of the main research challenges. Since over the past decade, the system has a body area networks wireless indoor positioning of the environment like in real time are gaining more interest. System successfully inventory management as well as adopted in applications such as asset tracking. Localization in body area networks (WSN) is one of the key technologies indoor positioning is also referred to as indoor localization. location estimation technique is basically the source or target localization and node are divided in the review paper in different localization technique submitted by various writers for WBANs are presented. the basic benefit is necessary for localization methods which energy efficiency. This paper contains localization methods and concepts more detailed discussion of WBANs to our objective...|$|E
40|$|Processing {{and distributing}} quantum {{information}} using photons through fibre-optic or free-space links {{is essential for}} building future quantum networks. The <b>scalability</b> <b>needed</b> for such networks {{can be achieved by}} employing photonic quantum states that are multiplexed into time and/or frequency, and light-matter interfaces that are able to store and process such states with large time-bandwidth product and multimode capacities. Despite important progress in developing such devices, the demonstration of these capabilities using non-classical light remains challenging. Employing the atomic frequency comb quantum memory protocol in a cryogenically cooled erbium-doped optical fibre, we report the quantum storage of heralded single photons at a telecom-wavelength (1. 53 μm) with a time-bandwidth product approaching 800. Furthermore we demonstrate frequency-multimode storage as well as memory-based spectral-temporal photon manipulation. Notably, our demonstrations rely on fully integrated quantum technologies operating at telecommunication wavelengths, i. e. a fibre-pigtailed nonlinear waveguide for the generation of heralded single photons, an erbium-doped fibre for photon storage and manipulation, and fibre interfaced superconducting nanowire devices for efficient single photon detection. With improved storage efficiency, our light-matter interface may become a useful tool in future quantum networks...|$|R
40|$|Competing interests: {{the authors}} have {{declared}} that no competing interests exist. Next-generation sequencing (NGS) is becoming a standard method in modern life-science laboratories for studying biomacromolecules and their interactions. Methods such as RNA-Seq and DNA resequencing are replacing array-based methods that dominated the last decade. A sequencing facility needs {{to keep track of}} requests, requester details, reagent barcodes, sample tracing and monitoring, quality controls, data delivery, creation of workflows for customised data analysis, privileges of access to the data, customised reports etc. An integrated software tool to handle these tasks helps to troubleshoot problems quickly, to maintain a high quality standard, and to reduce time and costs needed for data production. Commercial and non-commercial tools called LIMS (Laboratory Information Management Systems) are available for this purpose. However, they often come at prohibitive cost and/or lack the flexibility and <b>scalability</b> <b>needed</b> to adjust seamlessly to the frequently changing protocols employed. In order to manage the flow of sequencing data produced at the IIT Genomic Unit, we developed SLIMS (Sequencing LIMS). Motivation and Objectives Next-generation sequencing is becoming a standard method in modern life science labo...|$|R
40|$|If a {{piece of}} {{information}} is released from a media site, can it spread, in 1 month, to a million web pages? This influence estimation problem is very challenging since both the time-sensitive {{nature of the problem}} and the issue of <b>scalability</b> <b>need</b> to be addressed simultaneously. In this paper, we propose a randomized algorithm for influence estimation in continuous-time diffusion networks. Our algorithm can estimate the influence of every node in a network with |V| nodes and |E| edges to an accuracy of ε using n=O(1 /ε^ 2) randomizations and up to logarithmic factors O(n|E|+n|V|) computations. When used as a subroutine in a greedy influence maximization algorithm, our proposed method is guaranteed to find a set of nodes with an influence of at least (1 - 1 /e) OPT- 2 ε, where OPT is the optimal value. Experiments on both synthetic and real-world data show that the proposed method can easily scale up to networks of millions of nodes while significantly improves over previous state-of-the-arts in terms of the accuracy of the estimated influence {{and the quality of the}} selected nodes in maximizing the influence. Comment: To appear in Advances in Neural Information Processing Systems (NIPS), 201...|$|R
