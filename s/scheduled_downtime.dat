17|38|Public
2500|$|... 99.9% {{guaranteed}} uptime {{with zero}} <b>scheduled</b> <b>downtime</b> for maintenance ...|$|E
5000|$|Operating Time = 480 Minutes Scheduled - 30 Minutes <b>Scheduled</b> <b>Downtime</b> - 60 Minutes Unscheduled Downtime = 390 Minutes ...|$|E
50|$|A {{distinction}} can be {{made between}} scheduled and unscheduled downtime. Typically, <b>scheduled</b> <b>downtime</b> {{is a result of}} maintenance that is disruptive to system operation and usually cannot be avoided with a currently installed system design. <b>Scheduled</b> <b>downtime</b> events might include patches to system software that require a reboot or system configuration changes that only take effect upon a reboot. In general, <b>scheduled</b> <b>downtime</b> is usually the result of some logical, management-initiated event. Unscheduled downtime events typically arise from some physical event, such as a hardware or software failure or environmental anomaly. Examples of unscheduled downtime events include power outages, failed CPU or RAM components (or possibly other failed hardware components), an over-temperature related shutdown, logically or physically severed network connections, security breaches, or various application, middleware, and operating system failures.|$|E
50|$|If {{users can}} be warned away from <b>scheduled</b> <b>downtimes,</b> then the {{distinction}} is useful. But if therequirement is for true high availability, then downtime is downtime {{whether or not it}} is scheduled.|$|R
5000|$|... kGraft is {{a feature}} of the Linux kernel that {{implements}} live patching of a running kernel, which allows kernel patches to be applied while the kernel is still running. By avoiding the need for rebooting the system with a new kernel that contains the desired patches, kGraft aims to maximize the system uptime and availability. At the same time, kGraft allows kernel-related security updates to be applied without deferring them to <b>scheduled</b> <b>downtimes.</b> [...] Internally, kGraft allows entire functions in a running kernel to be replaced with their patched versions, doing that safely by selectively using original versions of functions to ensure per-process consistency while the live patching is performed.|$|R
5000|$|... kpatch is {{a feature}} of the Linux kernel that {{implements}} live patching of a running kernel, which allows kernel patches to be applied while the kernel is still running. By avoiding the need for rebooting the system with a new kernel that contains the desired patches, kpatch aims to maximize the system uptime and availability. At the same time, kpatch allows kernel-related security updates to be applied without deferring them to <b>scheduled</b> <b>downtimes.</b> [...] Internally, kpatch allows entire functions in a running kernel to be replaced with their patched versions, doing that safely by stopping all running processes while the live patching is performed.|$|R
50|$|Many {{computing}} sites exclude <b>scheduled</b> <b>downtime</b> from availability calculations, {{assuming that}} it {{has little or no}} impact upon the computing user community. By doing this, they can claim to have phenomenally high availability, which might give the illusion of continuous availability. Systems that exhibit truly continuous availability are comparatively rare and higher priced, and most have carefully implemented specialty designs that eliminate any single point of failure and allow online hardware, network, operating system, middleware, and application upgrades, patches, and replacements. For certain systems, <b>scheduled</b> <b>downtime</b> does not matter, for example system downtime at an office building after everybody has gone home for the night.|$|E
50|$|The Availability {{portion of}} the OEE Metric {{represents}} the percentage of scheduled time that the operation is available to operate. The Availability Metric is a pure measurement of Uptime {{that is designed to}} exclude the effects of Quality, Performance, and <b>Scheduled</b> <b>Downtime</b> Events. The losses due to wasted availability are called availability losses.|$|E
50|$|In {{steam power}} plants, MJTs are used on {{boiler feed pump}} head and barrel casings, boiler circ pump main flanges, stop valves, control valves, turbine couplings, stay rods, manway doors, inlet flanges, and {{feedwater}} heaters. MJTs can {{save a lot of}} time during <b>scheduled</b> <b>downtime</b> or maintenance because they require less time to install and remove than other bolting methods.|$|E
5000|$|... {{increase}} annual {{electric power}} production (e.g. by ICUF increasing, <b>scheduled</b> and unplanned <b>downtime</b> reducing); ...|$|R
5000|$|... vPatch, is a {{solution}} for [...] "virtual patching" [...] of databases to overcome the problem that many customers are unable to apply security patches to their databases in a timely manner. This {{may be due to}} the inability to <b>schedule</b> <b>downtime</b> for a production system, the time lag for testing / 3rd party support for applications on top of the database, or numerous other reasons. Sentrigo vPatch includes a set of rules which generate alerts when known vulnerabilities are exploited, and can be used to terminate attackers' database sessions. vPatch rules are updated on a frequent basis as new security updates are issued by the DBMS vendor, or as new vulnerabilities are discovered by Sentrigo's research team or partners.|$|R
30|$|Migrating large {{relational}} databases {{from physical}} infrastructure into the cloud presents many significant challenges, e.g., managing system downtime, choosing suitable cloud instances, and choosing a cloud provider. The database could be deployed on a database-as-a-service offered {{by one of}} several public cloud providers, or installed and configured on a virtual machine(s). With either option, selecting the appropriate cloud resources requires knowledge of the database workload and size. The infrastructure of the source database may impact the migration duration; if it has limited available capacity or bandwidth, then it will take longer to extract the data. An organisation may wish to upgrade the existing database hardware to speed up migration, or <b>schedule</b> <b>downtime</b> to migrate the database while it is idle.|$|R
50|$|The {{availability}} of a website {{is measured by the}} percentage of a year in which the website is publicly accessible and reachable via the Internet. This is different from measuring the uptime of a system. Uptime refers to the system itself being online. Uptime {{does not take into account}} being able to reach it as in the event of a network outage. A hosting providerâ€™s Service Level Agreement (SLA) may include a certain amount of <b>scheduled</b> <b>downtime</b> per year in order to perform maintenance on the systems. This <b>scheduled</b> <b>downtime</b> is often excluded from the SLA timeframe, and needs to be subtracted from the Total Time when availability is calculated. Depending on the wording of an SLA, if the {{availability of}} a system drops below that in the signed SLA, a hosting provider often will provide a partial refund for time lost. How downtime is determined changes from provider to provider, therefore reading the SLA is imperative. Not all providers release uptime statistics. Most hosting providers will guarantee at least 99.9% uptime which will allow for 43m of downtime per month, or 8h 45m of downtime per year.|$|E
40|$|Introduction Accurate {{analyses}} of fault-tolerance and replication mechanisms depend on an accurate {{model of the}} reliability of the systems that make them up. The overall reliability of a replication protocol, for example, depends on the probability that some fraction of the replica sites are functioning when data must be read or written. There are several important measures used to quantify system reliability, including time-to-failure (TTF), time-to- repair (TTR), availability, and reliability. Throughout this study, "failure" is defined in a distributed-environment sense; that is, as an inability to access a host. The term encompasses both hardware and software faults attributable to the host, and can include power failures and <b>scheduled</b> <b>downtime.</b> It can also be caused by offsite communications failures, ranging from temporary routing failures to problems with the physical commu...|$|E
40|$|Internet servers {{need to be}} highly-available, inexpensive, and scalable. These {{goals are}} often {{conflicting}} and most designs meet, with limited success, only few of them. In this paper we describe the SunSCALR framework that achieves these goals by combining proven technologies, careful system design, and engineering trade-offs. It uses a distributed, self-stabilizing algorithm for status monitoring and failure detection, and IP failover for automatic reconfiguration. SunSCALR provides high-availability against message loss, host crashes, and <b>scheduled</b> <b>downtime,</b> and allows on-the-fly addition and removal of hosts. We present detailed performance of SunSCALR. It can provide 10 second failover latency (i. e., better than 99. 999 % availability if machines fail for 2 hours/month). SunSCALR based products have been in use within Sun and are also available in the market. 1 Introduction SunSCALR is currently deployed in the Netra Proxy Cache Array. It is a strategic technology belonging to [...] ...|$|E
40|$|Operating a {{plant at}} {{off-design}} conditions can incur many penalties, some immediate, some long range. An immediate penalty could be increased unit energy costs and possibly product quality deterioration. A long-range penalty {{will be possible}} exposure of machinery to possible overload, surge, or various system upsets which can deteriorate components, accelerate wear, and set up problems such as misalignments which may interrupt production months {{from the time of}} occurrence. Downtime, whether planned or unscheduled, results in increased costs and in lost production. All machinery inevitably develops problems. The basic issue is preventing these problems from destroying machinery, endangering personnel, or upsetting production schedules. Knowing the operating condition of plant machinery and demonstrating an ability to <b>schedule</b> <b>downtime</b> for periods of minimum penalty is a large measure of the maintenance planner's effectiveness...|$|R
5000|$|... isyVmon â€žFull Discovery (ANH)â€œ (Freeware / Enterprise Edition) Enhancements of isyVmon â€žSimple Discovery (ADI)â€œ (Freeware / Enterprise Edition) More than 300 Bugs {{fixed in}} the isyVmon Monitoring GUI (Freeware / Enterprise Edition) Simplified Setup for ESX(i) Monitoring (Freeware / Enterprise Edition) Improvements of the isyVmon worker (Freeware / Enterprise Edition) Improvements of the Commandline API (Freeware / Enterprise Edition) Simplified getting Support for isyVmon (Freeware / Enterprise Edition) Tactical Overview - Dashboard {{improvements}} (Freeware / Enterprise Edition) <b>Scheduled</b> periodic <b>downtimes</b> available (Freeware / Enterprise Edition) Secure LDAP (TLS) support (Freeware / Enterprise Edition) ...|$|R
40|$|The cost of poor Quality {{would help}} in {{analyzing}} the operating costs for effective and profitable business management. In the era of cut throat completion, success achieved by market leaders is credited to their improvement initiatives. A common element within many of these successful companies {{is the use of}} powerful cost of poor quality concepts in connecting improvement priorities to strategic objectives, assessing the financial impact of poor quality, understanding the root causes of poor quality, selecting high payback improvement projects and managing the Improvement initiative to simultaneously deliver improved financial performance and greater customer satisfaction. A widely used rule of thumb says if a defect costs Rs 100 to fix in the field it would only cost Rs 10 to fix in your facility and only Rs 1 to prevent, so in this case an ounce of prevention is definitely greater than the pound of cure. This means in the manufacturing process we want to stop defects before they are created. The six sigma capability and SPC tools can stop the defects before they are created and reduce the cost of poor quality (COPQ) by allowing maintenance to move toward a predictive model instead of a reactive one. The ability to <b>schedule</b> <b>downtime</b> and get to issues root causes allow for less production interruptions and better quality...|$|R
40|$|This paper {{analyzes}} the basic performance of 27 automobile engine lines operated by 18 companies on three continents, based on questionnaire data gathered in 1995. A composite cost comprising labor and amortization of capital, accounting for downtime, {{is used to}} compare plant performance. We find that performance varies widely, even after eliminating differences in number of cylinders, number of engine varieties, scheduled utilization, and currency translation effects on wages. Cost drivers comprise number of workers, capital invested, and efficiency (fraction of scheduled time actually used for production). The drivers are in turn driven by external factors out of the plant's control and internal factors that are under its control to some degree. About half the variance in cost {{is due to the}} external factors, such as number of cylinders, utilization of scheduled time, and number of variants of engine made (the last loosely related to age of the engine family). Internal factors include work in process inventory (strongly) and age of the workers (somewhat). <b>Scheduled</b> <b>downtime</b> is driven largely by number of variants while unschedule...|$|E
40|$|This project {{consisted}} {{of working with}} the Super Web, referred to as the printer, at the Avery Dennison facility located in Rochelle. There was a large amount of downtime associated with the printer. The <b>scheduled</b> <b>downtime</b> was the main cause, however the interest was in reducing the unscheduled downtime. The main causes of the unscheduled downtime were materials, unscheduled maintenance and not staffed. The problem associated with materials was not having mylar to run the printer. The unscheduled maintenance was mainly due to problems with the punches, plates, blankets and knives. Finally, there were occurrences in which the machine was not staffed due to operator absenteeism or the operator helping on another machine. The recommendations made to reduce the unscheduled downtime included having a safety stock of mylar, increasing the preventative maintenance before an order is run, determining how long the punches and blankets last, having two plates available and hiring parttime employees if needed. Other recommendations included doing a follow-up analysis and ensuring the work center monitor sheets are filled out consistently. B. S. (Bachelor of Science...|$|E
40|$|The ATLAS {{facility}} {{has provided a}} total of 5749 hours of beam for research in FY 1998. The accelerator operation had a very high 93 % reliability factor during that period. With the startup of Gammasphere in January, our schedule has attempted to minimize <b>scheduled</b> <b>downtime</b> and maximize beam-time for research. Our best performance so far occurred {{during the month of}} May when a total of 639 hours was provided for research. From the accelerator point-of-view, recent major highlights have included first operation of a new production configuration for our {sup 17 }F beams which increased the beam current on-target to 2 x 10 {sup 6 } {sup 17 }F ions/see. The {sup 17 }F production target was moved approximately 4 meters upstream and a new superconducting solenoid was added to the system to refocus the highly divergent secondary beam. This new location also places the target upstream of a new superconducting resonator which was used to reduce the energy spread of the beam delivered to the spectrograph to less than 300 keV (FWHM). An improved, liquid nitrogen cooled, multiple gas cell has also significantly contributed to better performance...|$|E
40|$|SAMGrid {{presently}} {{relies on}} the centralised database for pro-viding several services vital for the system operation. These ser-vices are all encapsulated in the SAMGrid Database Server, and include access to file metadata and replica catalogs, dataset and processing bookkeeping, {{as well as the}} runtime support for the SAMGrid station services. Access to the centralised database and DB Servers represents a single point of failure in the system and limits its scalability. In order to address this issue, we have created a prototype of a peer-to-peer information service that allows the system to operate during times when access to the central DB is not available for any reason (e. g., network failures, <b>scheduled</b> <b>downtimes,</b> etc.), as well as to improve the system performance during times of extremely high system load when the central DB access is slow and/or has a high failure rate. Our prototype uses Distributed Hash Tables to create a fault tolerant and self-healing service. We believe {{that this is the first}} peer-to-peer information service designed to become a part of an in-use grid system. We describe here the prototype architecture and its existing and planned functionality, as well as show how it can be inte-grated into the SAMGrid system. We also present a study of per-formance of our new service under different circumstances. Our results strongly demonstrate the feasibility and usefulness of the proposed architecture. ...|$|R
50|$|Engineering Announcements for the Radio and Television Trade, {{sometimes}} {{abbreviated to}} Engineering Announcements, was a weekly magazine {{of news and}} information intended for technicians and salespeople in the United Kingdom, produced and transmitted by the Independent Television Authority (and later the Independent Broadcasting Authority) from 23 November 1970 until 31 July 1990. It covered technical advances in the industry such as the launch of satellite television and NICAM stereo, along with details of new transmitters and the <b>scheduling</b> of transmitter <b>downtime.</b>|$|R
40|$|This {{report is}} a {{compilation}} of the notes from the ten meetings. The group charter is: (1) to identify and characterize the range of possibilities and necessities for keeping the HFIR operating {{for at least the}} next 15 years; (2) to identify and characterize the range of possibilities for enhancing the scientific and technical utility of the HFIR; (3) to evaluate the benefits or impacts of these possibilities on the various scientific fields that use the HFIR or its products; (4) to evaluate the benefits or impacts on the operation and maintenance of the HFIR facility and the regulatory requirements; (5) to estimate the costs, including operating costs, and the <b>schedules,</b> including <b>downtime,</b> for these various possibilities; and one possible impact of proposed changes may be to stimulate increased pressure for a reduced enrichment fuel for HFIR...|$|R
40|$|Maintenance and {{downtime}} {{costs can}} take {{a significant part of}} the cost of mining operation. Additionally, unplanned downtime could cost several times as much as <b>scheduled</b> <b>downtime.</b> Real time measurement of wear and corrosion can provide crucial information needed to determine an optimal schedule for maintenance to maximise equipment availability while ensuring its reliability. It also leads to savings from optimized spare parts handling and helps minimize costs associated with unnecessary preventative maintenance and negative impact on safety and the environment caused by unexpected equipment failures. This paper presents a technology of 2 Ì† 01 cpainted-on 2 Ì† 01 d ultrasonic transducers that can be integrated into structures to be monitored and accurately measure wear or corrosion induced structure material losses through ultrasonic thickness measurement. These transducers have a small footprint, performance comparable to other commercially available ultrasonic transducers, and can sustain temperatures as high as 400. They are part of the next generation condition-based maintenance that embraces new advanced sensors connected to wireless network and advanced algorithms to provide a powerful tool for maintenance scheduling and consequently an enormous potential for cost savings. The application of this technology to mining equipment and its synergy with other sensors are discussed. A vision of next generation assets management that includes predictive maintenance enabled by wide use of innovative, low cost and wireless sensors is also discussed. Peer reviewed: NoNRC publication: Ye...|$|E
40|$|LecturePg. 53 - 72 This paper {{presents}} a typical field problem that most maintenance engineers in any petrochemical plant could encounter. Two 4000 hp through-drive electric motors were totally destroyed when the inboard coupling failed. Market conditions dictated that the compressor {{be put back}} into service as expeditiously as possible. A 3600 rpm prototype 8000 hp motor which could be fitted on the existing foundation was acquired from a power company. The existing half shell journal bearings had to be converted to a full shell design in order to solve a vibration problem. After start-up, the motor was plagued with a random vibration problem which occurred during a load change. Vibration data taken during one of these excursions indicated sub-synchronous frequencies at halfspeed, which were equal in amplitude to the horizontal readings at synchronous speed. A computer model simulating the rotor and bearing systems indicated a whirl instability problem at 1800 rpm. The computer was used to design and optimize a "between the pad," four shoe tilting pad bearing which was installed during the December 1982 turnaround. The paper goes into details on the new bearing design and installation, motor mechanical and hot optical alignment, and start-up data. All of the design and fabrication of the bearings was done while the unit was running, and the bearings were installed during a normal <b>scheduled</b> <b>downtime.</b> The result was a "first try fix. " This example illustrates that the "trial and error" method of problem solving should be done on paper and not with hardware, particularly when {{the stakes are high}} and the time short. Equipment and techniques used to solve this problem are well established and used daily in most petrochemical plants...|$|E
40|$|A Dissertation {{submitted}} to the Department of Electrical Engineering for the MScMotors are the workhorses of the industry. Safety, reliability, efficiency, and performance {{are some of the}} major concerns and needs for motor system applications. The issue of preventive and condition-based maintenance, online monitoring, system fault detection, diagnosis, and prognosis are of increasing importance. The use of motors in today's industry is extensive and the motors can be exposed to different hostile environments, misoperations,manufacturing defect etc. Different internal motor faults (eg. inter-turn short circuits, short circuit of motor leads, ground faults, bearing and rotor faults) along with external motor faults are expected to happen sooner or later. // Early fault detection, diagnosis, and prognosis allow preventive condition based maintenance to be arranged for the motor system during <b>scheduled</b> <b>downtime</b> and prevent an extended period of downtime caused by system failures. // This thesis deals with the stator Faults and mainly for inter-turn short circuit fault. The faults related to the rotor and bearing also are considered in many research and developed successful fault diagnosis techniques. Literature survey revealed that Fast Fourier Transform (FFT) based current spectrum analysis can be successfully applied in rotor and bearing faults analysis. // FFT based Inter-turn short circuit analysis, Air-gap flux sensing by external coils and Partial Discharge (PD) analysis have been discussed. This research has been focused to the negative sequence current analysis, since the FFT augmentation due to inter- turn fault is marginal. // A Power Decomposition Technique (PDT) has been used to derive positive and negative sequence components of measured voltage and current. A multi-phase based motor model is developed to simulate the inter turn fault and the results are verified by practical testing. The practical current waveforms are subjected to power decomposition based sequence component analysis in MAT LAB calculation platform. iv The practical testing has been done for loaded machine and the machine under no load condition to prove no load machine is more suitable for applying this technique. Harmonic analysis also has been done for comparison. Simulation model is validated using the practical test results. Either novel methods of on line monitoring or off-line inter turn fault diagnosis as routing maintenance test scheme is presented...|$|E
40|$|Continuously running systems require kernel {{software}} updates {{applied to}} them without downtime. Facilitating fast reboots, or delaying an update {{may not be}} a suitable solution in many environments, especially in pay-per-use highperformance computing clusters and mission critical systems. Such systems will not reap the benefits of new kernel features, and will continue to operate with kernel security holes unpatched, at least until the next <b>scheduled</b> maintenance <b>downtime.</b> To address these problems we developed an on-the-fly kernel updating system that enables commodity operating systems to gain adaptive and mutative capabilities without kernel recompilation or reboot. Our system, DynAMOS, employs a novel and efficient dynamic code instrumentation technique termed adaptive function cloning. Execution flow can be switched adaptively among multiple editions of functions, possibly concurrently running. This approach becomes the foundation for dynamic replacement of non-quiescent kernel subsystems when the timeliness of an update depends on synchronization of multiple kernel paths. We illustrate our experience by dynamically updating core subsystems of the Linux kernel...|$|R
40|$|International audienceIn {{semiconductor}} manufacturing, {{machines are}} usually qualified to process {{a limited number}} of recipes related to products. It is possible to qualify recipes on machines to better balance the workload on machines in a given toolset. However, all machines of a toolset do not have equal uptimes and may further suffer from <b>scheduled</b> and unscheduled <b>downtimes.</b> This may heavily impact an efficient recipe-to-machine qualification configuration. In this paper, we propose indicators for recipe-to-machine qualification management based on the overall toolset workload balance under capacity constraints. The models, deployed in industry, demonstrate that the toolset capacity must be considered while managing qualifications. Industrial experiments show how capacity consideration leads to an optimal qualification configuration and therefore capacity utilization...|$|R
40|$|Existing {{applications}} often contain security {{holes that}} are not patched until after the system has already been compromised. Even when software updates are applied to address security issues, they often result in system services being unavailable for some time. To address these system security and availability issues, we have developed peas and pods. A pea provides a least privilege environment that can restrict processes to the minimal subset of system resources needed to run. This mechanism enables the creation of environments for privileged program execution that can help with intrusion prevention and containment. A pod provides a group of processes and associated users with a consistent, machine-independent virtualized environment. Pods are coupled with a novel checkpoint-restart mechanism which allows processes to be migrated across minor operating system kernel versions with different security patches. This mechanism allows system administrators the flexibility to patch their operating systems immediately without worrying over potential loss of data or needing to <b>schedule</b> system <b>downtime.</b> We have implemented peas and pods in Linux without requiring any application or operating system kernel changes. Our measurements on real world desktop and server applications demonstrate that peas and pods impose little overhead and enable secure isolation and migration of untrusted applications...|$|R
40|$|LecturePg. 9 - 20 This paper {{discusses}} the rotordynamic instability problems experienced with two separate centrifugal compressors. While {{the root causes}} of the instabilities are very different, the analysis methodology of reconciling the rotordynamic model with measured vibration data was the same. The first problem occurred with a propylene compressor in a Gulf Coast chemical plant that had experienced high journal bearing temperatures for several years. A modified bearing was installed to alleviate the temperature problem; however, a large subsynchronous vibration appeared after the new bearings were installed. A lateral stability analysis shows that the compressor with the modified bearings was very stable with the aerodynamic destabilizing effect predicted by the Alford and/or Wachel equation. A comparison analysis was made of the stability predicted with the original bearings (which were stable) as well as the modified bearings (which were not). This allowed the user to determine the magnitude of the destabilizing forces present in the compressor and design a new bearing that was both stable and would operate at an acceptable temperature. The new bearing was installed and the compressor has operated without the subsynchronous vibration for {{the past year and a}} half. The second problem occurred with a very similar ethylene compressor in a Midwest ethylene plant. The compressor had operated for over two years after an overhaul with low vibration. Then a subsynchronous vibration appeared that was very erratic, but was slowly increasing in amplitude over time. To solve the problem, a rotordynamic analysis was performed that suggested that replacing the bearing would solve the stability problem However, comparison between the measure field vibration and the rotordynamic model did not agree on all points. A more indepth look at the compressor revealed that the increase in subsynchronous vibration was tracking very closely with the balance line differential pressure. This fact, along with the characteristics of the balance piston seal, suggested that a bearing change alone may not completely address the problem A new balance piston seal was designed to reduce its destabilizing effects on the rotor. The compressor was inspected during the next <b>scheduled</b> <b>downtime</b> to determine the cause of the high vibration and install the new balance piston seal and bearings. Examination of the internals revealed a large seal rub in the compressor, but at the dry gas seals, not the balance piston. The rub was addressed and the subsynchronous vibration was eliminated. While the exact source of the destabilizing force was not correctly ?guessed? before disassembly, the indepth rotordynamic analysis did reveal that there was a large destabilizing force in the compressor, and a bearing change alone would not eliminate the vibration. The compressor has been operating without the subsynchronous problem for the past year since the modification...|$|E
40|$|There {{were several}} {{physical}} and operational changes {{made to the}} NASA Glenn Research Center 8 - by 6 -Foot Supersonic Wind Tunnel {{during the period of}} 1992 through 1996. Following each of these changes, a facility calibration was conducted to provide the required information to support the research test programs. Due to several factors (facility research test <b>schedule,</b> facility <b>downtime</b> and continued facility upgrades), a full test section calibration was not conducted until 1996. This calibration test incorporated all test section configurations and covered the existing operating range of the facility. However, near the end of that test entry, two of the vortex generators mounted on the compressor exit tailcone failed causing minor damage to the honeycomb flow straightener. The vortex generators were removed from the facility and calibration testing was terminated. A follow-up test entry was conducted in 1997 in order to fully calibrate the facility without the effects of the vortex generators and to provide a complete calibration of the newly expanded low speed operating range. During the 1997 tunnel entry, all planned test points required for a complete test section calibration were obtained. This data set included detailed in-plane and axial flow field distributions for use in quantifying the test section flow quality...|$|R
40|$|Sting {{applications}} often contain security {{holes that}} are not patched until after the system has already been compromised. Even when software updates are applied to address security issues, they often result in system services being unavailable for some time. To address these system security and availability issues, we have developed peas and pods. A pea provides a least privilege environment that can restrict processes to the minimal subset of system resources needed to run. This mechanism enables the creation of environments for privileged program execution that can help with intrusion prevention and containment. A pod provides a group of processes and associated users with a consistent, machine-independent virtualized environment. Pods are coupled with a novel checkpoint-restart mechanism which allows processes to be migrated across minor operating system kernel versions with different security patches. This mechanism allows system administrators the flexibility to patch their operating systems immediately without worrying over potential loss of data or needing to <b>schedule</b> system <b>downtime.</b> We have implemented peas and pods in Linux without requiring any application or operating system kernel changes. Our measurements on real world desktop and server applications demonstrate that peas and pods impose little overhead and enable secure isolation and migration of untrusted applications...|$|R
40|$|Abstractâ€”Prognostics is {{the process}} of {{predicting}} a systemâ€™s future states, health degradation/wear, and remaining useful life (RUL). This information {{plays an important role in}} prevent-ing failure, reducing <b>downtime,</b> <b>scheduling</b> maintenance, and improving system utility. Prognostics relies heavily on wear estimation. In some components, the sensors used to estimate wear may not be fast enough to capture brief transient states that are indicative of wear. For this reason it is beneficial to be capable of detecting and estimating the extent of component wear using steady-state measurements. This paper details a method for estimating component wear using steady-state mea-surements, describes how this is used to predict future states, and presents a case study of a current/pressure (I/P) Transducer. I/P Transducer nominal and off-nominal behaviors are char-acterized using a physics-based model, and validated agains...|$|R
40|$|Tribology, one of {{the applied}} {{sciences}} in mechanical engineering which studies friction, wear and lubrication, contributes in the effort of minimizing wear and loses due to contact between two surfaces. Tribology is an essential technology for most of industries due to the impacts on energy efficiency, machine life, engine design, maintenance <b>schedules</b> and machine <b>downtime.</b> In the medical area, tribology is also critical to the long term mobility, healthy {{and the quality of}} life of patients with for instance replacement joints. An energy efficiency which is related to green technology is an important issue nowadays. However, it appears that possibilities to save energy by reducing the losses due to wear and friction have not sufficiently been taken into account in a large number of countries, especially in the developed-countries. It can be said that through tribology strategies for energy conservation can be reached. Industry could savings 1. 5...|$|R
50|$|Optional {{breakfast is}} served on every school day {{including}} Saturdays, after {{which students are}} required to go to a chapel service in the morning before classes. Classes are 50 minutes long, with 15 minutes passing time between them with schedules varying by day of the week. Period breaks are allowed, depending on the students' <b>schedules,</b> giving occasional <b>downtime</b> to those students who are able to schedule one. In the middle of the school day, or {{the end of the day}} if it is Wednesday or Saturday, lunch is served in a formal community style with assigned seating, each teacher heading up a separate table, and eating with their students. Every two weeks the seating chart is re-done so that students eventually eat with nearly all of one another, as well as the teachers. The head table seats the Headmaster, and any students or faculty he has invited to sit with him for that meal.|$|R
