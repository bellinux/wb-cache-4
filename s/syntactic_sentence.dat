23|301|Public
50|$|Research {{eventually}} {{established that}} the two variables commonly used in readability formulas-a semantic (meaning) measure such as difficulty of vocabulary and a <b>syntactic</b> (<b>sentence</b> structure) measure such as average sentence length—are the best predictors of textual difficulty.|$|E
5000|$|Automatic {{syntactic}} analysis has recently become important for both natural language data processing and syntax-directed compilers. A formal parsing system G = (V, &mu;, T, R) {{consists of two}} finite disjoint vocabularies, V and T, a many-many map, &mu;, from V onto T, and a recursive set R of strings in T called <b>syntactic</b> <b>sentence</b> classes ...|$|E
50|$|For an individual, a Lexile {{measure is}} {{typically}} {{obtained from a}} reading comprehension assessment or program. These range from the adolescent level (DIBELS: Dynamic indicators of basic early literacy skills) to the adult level (TABE: Test of adult basic education). A Lexile text measure is obtained by evaluating the readability {{of a piece of}} text, such as a book or an article. The Lexile Analyzer, a software program specially designed to evaluate reading demand, analyzes the text's semantic (word frequency) and <b>syntactic</b> (<b>sentence</b> length) characteristics and assigns it a Lexile measure. Over 60,000 Web sites, 115,000 fiction and nonfiction books, and 80 million articles have Lexile measures, and these numbers continue to grow. Over 150 publishers including Capstone Publishers, Discovery Ed, Houghton Mifflin Harcourt, McGraw-Hill, Pearson PLC, Riverside Publishing, Scholastic Corporation, Simon & Schuster, Workman Publishing Company, and World Book offer certified Lexile text measures for their materials.|$|E
5000|$|Most {{researchers}} {{suppose that}} the faulty <b>syntactic</b> structure (<b>sentence</b> blends,contaminations, break-offs) results from a disturbance of the syntactic plan of the utterance(de Bleser/Bayer 1993:160f) ...|$|R
2500|$|The {{development}} of syntactic structures follows a particular pattern and reveals {{much on the}} nature of language acquisition, which has several stages. According to O'Grady and Cho (2011), the first stage, occurring between the ages of 12–18 months, is called [...] "one-word stage." [...] In this stage, children cannot form <b>syntactic</b> <b>sentences</b> and therefore use one-word utterances called [...] "holophrases" [...] that express an entire sentence. In addition, children's comprehension is more advanced than their production abilities. For example, a child who wants candy may say [...] "candy" [...] instead of expressing a full sentence.|$|R
40|$|In Dolakha Newar the {{boundaries}} of <b>syntactic</b> <b>sentences</b> are clearly demar-cated. However, each tightly-bounded sentence {{has the potential for}} infinite expansion and structural complexity. This results from the recursive interac-tion of two basic combinatorial structures: chaining and embedding. While these structures are basic to many of the world’s languages, in Dolakha Newar speakers combine them freely and frequently to spontaneously create sentences of remarkable intricacy. Additional structural nuance is provided by the chain-ing of constituents at levels below the clause and the sharing of core arguments. The result is a syntactic fabric of depth and complexity...|$|R
40|$|The present paper {{describes}} various <b>syntactic</b> <b>sentence</b> fusion {{techniques for}} Bengali language {{that belongs to}} the Indo-Aryan language family. Firstly a clause identification and classification system marks clause boundaries and classifies them as principle clause and subordinate clauses. A rule-based sentence classification system has been developed to categorize sentences as simple, complex and compound. The final <b>syntactic</b> <b>sentence</b> fusion system makes use of the sentence class and the clause types and finally fuses two textually entailed sentences using verb paradigm information and noun morphological information. The system outputs are compared with a gold standard data set using manual evaluation and BLEU techniques. The evaluation results yield good accuracy scores. The <b>syntactic</b> <b>sentence</b> fusion technique developed in the present work may be applied for other Indian languages...|$|E
40|$|Lexical ambiguities {{naturally}} {{arise in}} languages. We present Lamb, a lexical analyzer {{that produces a}} lexical analysis graph describing all the possible sequences of tokens {{that can be found}} within the input string. Parsers can process such lexical analysis graphs and discard any sequence of tokens that does not produce a valid <b>syntactic</b> <b>sentence,</b> therefore performing, together with Lamb, a context-sensitive lexical analysis in lexically-ambiguous language specifications...|$|E
40|$|This paper {{addresses}} the automatic transformation of financial statements into conceptual graph interchange format (CGIF). The method mainly involves extracting relevant financial performance indicators, parsing it to obtain <b>syntactic</b> <b>sentence</b> structure and {{to generate the}} CGIF for the extracted text. The required components for the transformation are detailed out with an illustrative example. The paper also discusses the potential manipulation of the resulting CGIF for knowledge discovery and more precisely for deviation detection...|$|E
40|$|Abstract: By {{means of}} fMRI measurements, {{the present study}} {{identifies}} brain regions in left and right peri-sylvian areas that subserve grammatical or prosodic processing. Normal volunteers heard 1) normal sentences; 2) so-called <b>syntactic</b> <b>sentences</b> comprising <b>syntactic,</b> but no lexical-semantic information; and 3) manipulated speech signals comprising only prosodic information, i. e., speech melody. For all conditions, significant blood oxygenation signals were recorded from the supratemporal plane bilaterally. Left hemisphere areas that surround Heschl gyrus responded more strongly during the two sentence conditions than to speech melody. This finding suggests that the anterior and posterior portions of the superior temporal region (STR) support lexical-semantic and <b>syntactic</b> aspects of <b>sentence</b> processing. In contrast, the right superior temporal region, in especially the planum temporale, responded more strongly to speech melody. Significant brain activation in the fronto-opercular cortices was observed when participants heard pseudo sentences and was strongest during the speech melody condition. In contrast, the fronto-opercular area is not prominently involved in listening to normal sentences. Thus, the functional activation in fronto-opercular regions increases as the grammatical information available in the sentence decreases. Generally, brain responses to speech melody were stronger in right than left hemisphere sites, suggesting aparticular role of right cortical areas in the processing of slow prosodic modulations. Hum...|$|R
50|$|Cross-Modal Priming Task. The Cross-Modal Priming Task (CMPT), {{developed}} by David Swinney, is an online measure {{used to detect}} activation of lexical and <b>syntactic</b> information during <b>sentence</b> comprehension.|$|R
50|$|Contrary {{to popular}} belief, Missingsch {{is not a}} dialect of Low German. Furthermore, it is also not simply High German with a Low German accent, as it is often described. Its Low German/Low Saxon {{influences}} are not restricted to its phonology but involve morphological and <b>syntactic</b> structures (<b>sentence</b> construction) and its lexicon (vocabulary) as well. It {{is a type of}} German variety with the minimally qualifying characteristic of a clearly noticeable Low German/Low Saxon substratum.|$|R
40|$|This paper {{provides}} {{evidence for}} theories of grounding and dialogue management in human conversation. For each utterance in a corpus of task-oriented dialogues, we calculated integration costs, {{which are based}} on <b>syntactic</b> <b>sentence</b> complexity. We compared the integration costs and grounding behavior under two conditions, namely face-to-face and a no-eye-contact condition. The results show that integration costs were significantly higher for explicitly grounded utterances in the no-eye-contact condition, but not in the face-to-face condition. Index Terms: dialogue, syntactic complexity, grounding 1...|$|E
40|$|Word-prediction is a {{technique}} commonly used {{to reduce the amount}} of keystrokes needed to input text by people with severe physical disabilities. Several methods based on word frequencies have been developed so far. Many of them do not take advantage of the information inherent to the <b>syntactic</b> <b>sentence</b> structure. This paper puts forth a word-prediction method based on the syntactical analysis of a sentence, carried out using the “chart ” parsing method proposed by Allen. This method also adapts its behaviour to the user’s lexicon. The obtained results are compared to others obtained from a pure statistical predictor...|$|E
40|$|This {{research}} {{belongs to}} qualitative research. The research purposes {{to identify the}} types of sense based sentence and illocutionary meaning or author‟s intention in the lyric of Adele‟s albums. This research is using scope of pragmatics {{to find out the}} author‟s intention. The objects of the research are lyrics in the Adele‟s album entitled 21 and 19 which consist of 21 songs. In collecting data, researcher uses the document. The data in this research are simple declarative sentences containing types of sense based sentence, they are analytic sentence, <b>syntactic</b> <b>sentence,</b> contradiction sentence and the illocutionary meaning or author‟s intentions, they are stating, informing, asserting, disclosing, offering, warning, threatening, affirming, promising, alleging, identifying, deploring, questioning, conjecturing, claiming and disputing. From the collected data, the researcher finds 50 data. The result of the research shows that: (a) There are three kinds of sense based sentences from 50 data found in Adele‟s album. They are analytic sentence (2 data/ 4, 0 %), <b>syntactic</b> <b>sentence</b> (29 data/ 58, 0 %) and contradiction sentence (19 data/ 38, 0 %). (b) There are 16 kinds of illocutionary meaning found in Adele‟s album. They are stating (5 data/ 10, 0 %), informing (21 data/ 42, 0 %), asserting (2 data/ 4, 0 %), disclosing (2 data/ 4, 0 %), offering(2 data/ 4, 0 %), warning(2 data/ 4, 0 %), threatening (2 data/ 4, 0 %), affirming (2 data/ 4, 0 %), promising (2 data/ 4, 0 %), alleging (1 datum/ 2, 0 %), identifying (1 datum/ 2, 0 %), deploring (4 data/ 8, 0 %), questioning (1 datum/ 2, 0 %), conjecturing (1 datum/ 2, 0 %), claiming (1 datum/ 2, 0 %) and disputing (1 datum/ 2, 0 %) ...|$|E
2500|$|<b>Syntactic</b> {{ambiguity}} characterizes <b>sentences</b> {{which can}} be interpreted in different ways depending solely on how one perceives syntactic connections between words and arranges them into phrases. Possible interpretations of the sentence They killed the man with a gun are: ...|$|R
50|$|Professor Friederici’s {{research}} centres {{on how the}} human brain processes language, examining both first and second language acquisition and use. She {{was the first to}} report the early left anterior negativity (ELAN), an EEG response to <b>syntactic</b> violations in <b>sentences.</b>|$|R
40|$|M. A. The {{purpose of}} this {{research}} is fourfold, namely to identify the different types of interrogative sentences used in Zulu; to describe the syntactic-semantic features of interrogative sentences; to point at the semantic interpretation which might be attached to specific <b>syntactic</b> interrogative <b>sentences</b> and to establish the pragmatic use of interrogative sentences. In the course of this study it became evident that the interpretation of interrogative sentences is bound to the pragmatic discourse context [...] ...|$|R
40|$|The {{present study}} explores {{the role of}} the word position-in-text in {{sentence}} and paragraph reading. Three eye-movement data sets based on the reading of Dutch and German unrelated sentences reveal a sizeable, replicable increase in reading times over several words at {{the beginning and the end}} of sentences. The data from the paragraph-based English-language Dundee corpus replicate the pattern and also indicate that the increase in inspection times is driven by the visual boundaries of the text organized in lines, rather than by <b>syntactic</b> <b>sentence</b> boundaries. We argue that this effect is independent of several established lexical, contextual, and oculomotor predictors of eye-movement behaviour. We also provide evidence that the effect of word position-in-text has two independent components: a start-up effect, arguably caused by a strategic oculomotor programme of saccade planning over the line of text, and a wrap-up effect, originating in cognitive processes of comprehension and semantic integration. Peer-reviewedPost-prin...|$|E
40|$|A {{system has}} been {{developed}} {{to find the most}} likely attachment for prepositional phrases in English sentences in a fairly unrestricted way. The system receives as input a <b>syntactic</b> <b>sentence</b> parse provided by a general-purpose computational grammar called PEG (PLNLP English Grammar) The semantic decision that is necessary to make the right attachments is made (a) by parsing (also with PEG) the natural language definitions of an online standard dictionary, in this case Webster's Seventh New Collegiate Dictionary; (b) by relating words to other words in the dictionary; and (c) by reasoning heuristically about the comparative likelihood of different possible attachments. The basic assumption of this research is that natural language itself is a knowledge representation language that can be conveniently accessed and richly exploited. Techniques such as those presented here offer hope for eliminating the timeconsuming and often incomplete hand coding of semantic information that has been conventional in natural language understanding systems...|$|E
40|$|The present {{thesis is}} a {{contribution}} to the widely discussed issue of how the syntactic structure of a sentence and the structure of discourse (text) are related. The <b>syntactic</b> <b>sentence</b> structure along with other language phenomena participates in building a coherent, comprehensible discourse. The author calls the syntactically motivated relations in discourse connective relations. These relations include coordinating relations and some of the subordinating relations within a sentence and, secondly, adjoining of discourse units across the sentence boundary. The explicit means of expressing connective relations are called discourse connectives. It is a group of language expressions that connect or adjoin discourse units while indicating the type of semantic relation between them, i. e. conjunctions, some subjunctions, particles and adverbials, and marginally also some other parts-of-speech. The present thesis describes the semantic category of discourse connectives in Czech on the basis of language data and their syntactic annotation in the Prague Dependency Treebank, and thus aims to contribute to the design of a language corpus annotation scenario capturing the discourse relations in Czech...|$|E
40|$|Lexical {{processing}} plays a transparently {{necessary and}} {{central role in}} language comprehension. In what follows, we discuss the connections between lexical activation and lexical integration {{in the construction of}} interpreted <b>syntactic</b> representations during <b>sentence</b> comprehension. We provide a basis for inferring how different brai...|$|R
40|$|The paper {{proposes a}} {{mathematical}} method of defining dependency and constituency provided linguistic criteria {{to characterize the}} acceptable fragments of an utterance have been put forward. The method {{can be used to}} define <b>syntactic</b> structures of <b>sentences,</b> as well as discourse structures for texts or morphematic structures for words...|$|R
40|$|Research {{has shown}} that {{cognates}} between Japanese and English {{have the potential to}} be a valuable learning tool (Daulton, 2008). Yet little is known on how Japanese learners of English produce cognates in context. Recently, studies have argued that cognates can cause a surprisingly high number of <b>syntactic</b> errors in <b>sentence</b> writing activities with Japanese learners (Rogers, Webb, 2 ̆ 6 Nakata, 2014; Masson, 2013). In the present study, I investigated how Japanese learners of English understood and used true cognates (words that have equivalent meanings in both languages) and non-true cognates (words where the Japanese meaning differs in various ways from their English source words). Via quasi-replication, I analyzed participantsâ€™ sentences to determine the interaction of true and non-true cognates on semantics and syntax. In an experimental study, twenty Japanese exchange students filled out a word knowledge scale of thirty target words (half true cognates and half non-true cognates) and wrote sentences for the words they indicated they knew. These sentences were analyzed quantitatively and qualitatively for both semantic and <b>syntactic</b> errors. <b>Sentences</b> with true cognates were semantically accurate 86...|$|R
40|$|Word sense {{disambiguation}} {{has been}} recognized as {{a major problem in}} natural language processing research for over forty years. Both quantitive and qualitative methods have been tried, but much of this work has been stymied by difficulties in acquiring appropriate lexical resources, such as semantic networks and annotated corpora. In particular, much of the work on qualitative methods has had to focus on ‘‘toy’’ domains since currently available semantic networks generally lack broad coverage. Similarly, much of the work on quantitative methods has had to depend on small amounts of hand-labeled text for testing and training. We have achieved considerable progress recently by taking advantage of a new source of testing and training materials. Rather than depending on small amounts of hand-labeled text, we have been making use of relatively large amounts of parallel text, text such as the Canadian Hansards, which are available in multiple languages. The translation can often be used in lieu of hand-labeling. For example, consider the polysemous word sentence, which has two major senses: (1) a judicial sentence, and (2), a <b>syntactic</b> <b>sentence.</b> We can collect a number of sense (1) examples by extracting instances that are translated as peine, and we can collect a number of sense (2) examples by extracting instances that are translated a...|$|E
40|$|At our {{institute}} a speech understanding and dialog system is developed. As an example we model an information system for timetables {{and other information}} about intercity trains. In understanding spoken utterances, additional problems arise due to pronunciation variabilities and vagueness of the word recognition process. Experiments so far have also shown that the syntactical analysis produces a lot more hypotheses instead of {{reducing the number of}} word hypotheses. The reason for that is the possibility o! combining nearly every group of word hypotheses which are adjacent with respect to the speech signal to a syntactically correct constituent. Also, the domain independent semantic analysis cannot be used for filtering, because a <b>syntactic</b> <b>sentence</b> hypothesis normally can be interp. reted in several different ways, respectively a set of syntactic hypotheses for constituents can be combined to a lot of semantically interpretible sentences. Because of this combinatorial explcaiun it seems to be reasonable to introduce domain dependent and contextual knowledge as early as possible, also for the semantic analysis. On the other hand it would be more efficient prior to the whole semantic interpretation of each syntactic hypothesis or combination of syntactic hypotheses to find possible candidates with less effort and interpret only the more probable Ones. 1...|$|E
40|$|A {{linguistic}} database is {{a collection}} of texts where sentences and words are annotated with linguistic information, such as part of speech, morphology, and <b>syntactic</b> <b>sentence</b> structure. While early linguistic databases focused on word annotations, and later also on parse-trees of sentences (so-called treebanks), the recent years have seen a growing interest in richly annotated corpora of historic texts that include not only syntactic annotations but further complex annotations, such as alignments between related text layers. This raises the issue of efficiently querying such complex structured linguistic databases. We present a generic approach for defining domain-specific query languages that we use in developing a query language for richly annotated historic corpora. In our approach, a query language is defined as a set of predicates. A query in form of a logic rule is translated by our LoToS query compiler into a single, possibly deeply nested SQL query. In contrast to previous approaches, the annotation structures that can be queried need not be trees but can also form DAGs, or, for a restricted class of recursive queries, arbitrary graphs. To this end, LoToS offers an operator for computing transitive closures using the recursive capabilities of modern database systems. We believe {{that this is the first}} approach to use modern SQL capabilities for evaluating recursive predicates in logic-based query languages...|$|E
30|$|The {{algorithm}} {{for generating}} the new reduced Vietnamese paragraph takes the input as two lists: AS_List contains predicates representing actions or states and O_List contains predicates representing instances (described in Sect. 4.1). The {{output of the}} algorithm is an ordered list S_StructureList containing <b>syntactic</b> structures of <b>sentences</b> in the new paragraph.|$|R
40|$|Interaction Grammars are a {{grammatical}} formalism {{based on}} the notion of polarities. Polarities express the resource sensitivity of natural languages by modelling the distinction between saturated and unsaturated <b>syntactic</b> structures. A <b>sentence’s</b> <b>syntactic</b> composition is represented as an “electrostatic” process guided by the neutralization of polarities. At the heart of the formalism, there is also the notion of underspecification: syntactic structures are not completely specified trees but tree descriptions. Then, parsing appears as a process of building tree description models. Semantics is represented at a level independent of the syntactic level, but {{based on the}} same notions of polarity and description with a difference: descriptions are not tree descriptions but directed acyclic graph (DAG) descriptions. The interface between the two levels is realized in a flexible way by a function that links every syntactic node to at most one semantic node...|$|R
40|$|Abstract We {{explore a}} <b>syntactic</b> {{approach}} to <b>sentence</b> compression in the biomedical domain, {{grounded in the}} context of result presentation for related article search in the PubMed search engine. By automatically trimming inessential fragments of article titles, a system can effectively display more results in the same amount of space. Our implemented prototype operates by applying a sequence of syntactic trimming rules over the parse trees of article titles. Two separate studies were conducted using a corpus of manually compressed examples from MEDLINE: an automatic evaluation using BLEU and a summative evaluation involving human assessors. Experiments show that a <b>syntactic</b> approach to <b>sentence</b> compression is effective in the biomedical domain and that the presentation of compressed article titles supports accurate ‘‘interest judgments’’, decisions by users as to whether an article is worth examining in more detail...|$|R
40|$|U radu će biti obrađeni prijedlozi koji se često pojavljuju u paru: od i do, iz i u, s i na. Učestalom uporabom u tim parovima navedeni prijedlozi nadilaze svoja pojedinačna primarna semantička i sintaktička obilježja. Na njihovu strukturnu i semantičku cjelovitost upućuje i frazeološka uporaba. In {{this paper}} the authors {{consider}} prepositions that often appear in pairs (e. g. od and do, iz and u, s and na), showing semantic and structural pair integrity (showing, namely, that such prepositions only become meaningful in pairs). This integrity {{can also be}} seen in their frequent idiomatic use. The paper provides a thorough outline of preposition pair analysis in dictionaries and of their status in literature. It is demonstrated that preposition pair analysis in Croatian monolingual dictionaries is inconsistent, i. e. nonsystematized. The following analyzes semantic features of such prepositions and classifies them into several groups according to their meaning. Furthermore, the paper considers syntactic features of such prepositions (they are used as adverbials, attributes or parts of nominal predicates). When it comes to identification of <b>syntactic</b> (<b>sentence)</b> idiom function, the authors point out that some structural types are predetermined for particular syntactic functions, e. g. all structurally verbal idioms with preposition pairs in this paper are of the verbal categorical meaning, and they will all have a predicative function in the sentence. In conclusion, the paper proposes a preposition pair methodology for monolingual dictionaries of the contemporary Croatian language, and provides a model of preposition pair idiom analysis...|$|E
40|$|Predictions {{allow for}} an {{efficient}} processing in communicative situations. In {{order to be}} efficient, predictions need to be adapted to characteristics of the environment. In a communicative situation, speaker-specific language use might shape a listener’s predictions about upcoming language stimuli. In the present experiment we asked whether listeners use speaker characteristics to generate predictions about syntactic structure and how these predictions might change over time. Twenty participants were presented with sentences which were spoken by two different speakers. Sentences had either a Subject-Object-Verb (SOV) structure or an Object- Subject-Verb (OSV) structure. Crucially, the two speakers differed {{with regards to the}} frequency by which they produced a particular syntactic structure. One of the speakers had a high probability to produce a SOV structure and a low probability to produce an OSV structure, and vice versa for the other speaker. Additionally, speakers produced sentences which were ambiguous towards their syntactic structure. For the ambiguous sentences, participants had to identify the subject or the object of the sentence. This allowed us to infer participants’ predictions regarding the <b>syntactic</b> <b>sentence</b> structure. Furthermore, in order to assess the significance of speaker-specific predictions, participants were invited to a follow-up study eight months after the initial exposure to the speakers. The data show that participants started with a strong bias towards the SOV structure, which is the canonical sentence structure in German. With increasing exposure to the speakers, however, participants developed predictions regarding the particular syntactic structure based on speaker identity. These predictions were still coupled to the speakers eight months after the initial exposure. This demonstrates that listeners are sensitive to speaker-specific syntactic preferences and use this information to generate predictions...|$|E
40|$|In {{the study}} of the {{relation}} between syntax and semantics, an important tradition of research has developed {{on the nature of the}} individual level and stage level distinction among predicates, first noted by Milsark (1977) and subsequently elaborated by Carlson (1977), Kratzer (1989), and Diesing (1990). This line of study has attempted to explain how the difference between the two types of predicates, generally construed as a semantic difference, manifests itself in different syntactic constructions which are allowed with the predicate (there-existential sentences and perceptual report sentences, to take two examples). The relationship between syntax and semantics has also been studied in depth by Kuroda (1972, 1992), in his examination of wa/ga sentences in Japanese. He has proposed that the different <b>syntactic</b> <b>sentence</b> constructions correspond with the two different types of logical judgment originally proposed by Brentano and Marty, the thetic and categorical judgments. Ladusaw (1994) attempts to bring these lines of study together in order to explain certain characteristics of the ILP/SLP distinction with Kuroda's distinction between judgment types. In this paper, I attempt to build upon Ladusaw's account, by providing a syntactic representation of the ILP/SLP and thetic/categorical distinctions. To do this I will examine the syntactic accounts of Kratzer and Diesing of the ILP/SLP distinction, and the account of Kuroda of the thetic/categorical distinction, in order to suggest how the distinctions interact on a syntactic level. I then hope to show how the semantic relations Ladusaw draws between the predicate distinction and judgment distinction are mirrored in the syntax. Finally, I hope to use Kuroda's logical notation (1992) to characterize these distinctions at the level of logical form, in order to explore some of the ramifications of the two distinctions...|$|E
50|$|In a {{study by}} Carvalho et al (2016), experimenters tested {{preschool}} children, where they showed that {{by the age of}} 4 prosody is used in real time to determine what kind of <b>syntactic</b> structure <b>sentences</b> could have. The children in the experiments were able to determine the target word as a noun when it was in a sentence with a prosodic structure typical for a noun and as a verb when it was in a sentence with a prosodic structure typical for a verb. Children by the age of 4 use phrasal prosody to determine the syntactic structure of different sentences.|$|R
40|$|<b>Syntactic</b> {{structure}} of <b>sentences</b> {{can be represented}} using constituency trees or dependency trees. Constituency trees indicate recursive “bracketting ” of the sentence–sequences of words are grouped together to form constituents: (1) John (loves Mary) Dependency trees indicate which words depend on which. Nivre (2005) gives a good review of dependency-based formalisms and dependency parsing...|$|R
40|$|In the {{proceedings}} of the conference, there is only an extended abstract of the paperInteraction Grammars are a grammatical formalism based on the notion of polarities. Polarities express the resource sensitivity of natural languages by modelling the distinction between saturated and unsaturated <b>syntactic</b> structures. A <b>sentence's</b> <b>syntactic</b> composition is represented as an ``electrostatic'' process guided by the neutralization of polarities. At the heart of the formalism, there is also the notion of underspecification: syntactic structures are not completely specified trees but tree descriptions. Then, parsing appears as a process of building tree description models. Semantics is represented at a level independent of the syntactic level, but based on the same notions of polarity and description with a difference: descriptions are not tree descriptions but directed acyclic graph (DAG) descriptions. The interface between the two levels is realized in a flexible way by a function that links every syntactic node to at most one semantic node...|$|R
