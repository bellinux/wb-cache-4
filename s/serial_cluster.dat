1|48|Public
40|$|This study {{looked at}} the {{relationship}} between the 27 indices on the California Verbal Learning Test - Children's Version (CVLT-C) and the 19 scores on the Wechsler Intelligence Test for Children - Third Edition (WISC-III). The sample consisted of 58 children, ages 6 - 16 from a clinical population. The subgroups with Attention Deficit Hyperactivity Disorder (ADHD), Learning Disabilities (LD), and High Ability (HA) were compared to children with no clinical diagnosis (ND) on WISC-III Full Scale IQ, Verbal IQ, and Performance IQ; on CVLT-C Total Recall Trials 1 - 5; and on CVLC factors of Attention Span, Learning Efficiency, Free Delayed Recall, Cued Delayed Recall, and Inaccurate Recall. Twelve significant correlations were found between the CVLT-C and WISC-III including Full Scale IQ with Total Recall Trials 1 - 5, Discriminability, and False Positives; Verbal IQ with False Positives; Vocabulary with Total Recall Trials 1 - 5; Information with <b>Serial</b> <b>Cluster</b> Ratio; Digit Span with Total Recall Trials 1 - 5 and Discriminability; Processing Speed with Discriminability and False Positives; and Symbol Search with False Positives, all within the moderate range. In comparing clinical subgroups, children with ADHD did not differ significantly from those without a clinical diagnosis (ND) on any selected measures. Children diagnosed with learning disabilities were significantly lower on Total Recall Trials 1 - 5, Attention Span, and Cued Delayed Recall. Children with high abilities (HA) were significantly higher on WISC-III Full Scale IQ, Verbal IQ, Performance IQ, CVLT-C Total Recall Trials 1 - 5, Learning Efficiency, and Free Delayed Recall. This study empirically supported a positive relationship between memory processes and cognitive abilities while also confirming that each are a part of a larger cognitive process. Department of Educational PsychologyThesis (Ph. D. ...|$|E
40|$|The aim of {{this study}} was to test the {{hypothesis}} that developmental differences exist in the use of learning strategies in primary school children. <b>Serial</b> and subjective <b>clustering</b> in a multitrial Pictorial Verbal Learning Test (PVLT) were compared in 79 children aged 6 - 12. Correlation analyses indicated that <b>serial</b> <b>clustering</b> yielded better performance when information was presented on the initial trials of the test. Subjective clustering was superior when information was presented repeatedly, i. e., after three or more trials. Analyses of variance indicated that subjective clustering was used more often in older children with repeated presentations. On the other hand, there was no increase in the use of <b>serial</b> <b>clustering</b> with age and with repeated presentations. The findings imply that training in the use of proper strategies could have benefit for children who use an inefficient learning strategy and/or have a learning problem. In addition, they point to the importance of the factor "age" in relation to the way information is presented to children...|$|R
40|$|Subjects with Alzheimer’s disease (AD) show {{impaired}} {{learning strategies}}. Whether impaired learning strategies are already present {{in subjects with}} prodromal AD remains unknown. The aim {{of the present study}} was to investigate the predictive accuracy of learning strategies for AD in subjects with Mild Cognitive Impairment (MCI). Subjects with MCI (n 202) were selected from the Maastricht Memory Clinic. Subjects were reassessed over a period of 10 years. Fifty-five of the 202 subjects converted to AD. Learning strategies investigated were subjective organization and <b>serial</b> <b>clustering.</b> Lower scores of subjective organization were associated with a higher risk for AD (OR 2. 1, p. 002). <b>Serial</b> <b>clustering</b> did not predict AD. Prodromal AD is characterized by a decreased use of effortful learning strategies. This finding may have implications for the early detection of AD in MCI subjects and for the development of cognitive training programs...|$|R
40|$|This study {{investigated}} which strategies children aged 5 - 15 years (N = 408) employ while performing a multitrial free recall test of semantically unrelated words. <b>Serial</b> <b>clustering</b> (i. e., a relatively passive strategy) is {{an index of}} the sequential consistency of recall order. Subjective clustering (i. e., a more active strategy) is based on similar word groupings in successive trials. Previously, Meijs et al. (2009) found {{that the level of}} (<b>serial</b> and subjective) <b>clustering</b> increases with age. At all ages, the level of <b>serial</b> <b>clustering</b> correlates positively with the ability to recall information on VLT trials. However, subjective clustering is more predictive of VLT performance than <b>serial</b> <b>clustering</b> after ≥ 3 trials, but only in children aged 8 +. Knowledge on how children organize words (based on, for example, sound or meaning) and how this relates to developmental stage is still lacking. This study revealed that the level of subjective clustering is primarily determined by the position of words in a VLT list. More specifically, primacy (i. e., recall of words 1 - 3 of the VLT list-whether recalled in the same order or reversed) and recency (i. e., recall of words 14 - 15) effects primarily determine level subjective organization over successive trials. Thus, older children still organize words based on the serial position of the VLT list and are much less likely to organize them based on any other feature of the words, for example, sound or meaning. This indicates that the most important information to be learned needs to be presented first or last, even in older children and even with repeated presentations. © 2013 Taylor & Francis Group, LLC...|$|R
40|$|Under {{particular}} large-scale atmospheric conditions, several windstorms {{may affect}} Europe {{within a short}} time period. The occurrence of such cyclone families leads to large socioeconomic impacts and cumulative losses. The <b>serial</b> <b>clustering</b> of windstorms is analyzed for the North Atlantic/western Europe. Clustering is quantified as the dispersion (ratio variance/mean) of cyclone passages over a certain area. Dispersion statistics are derived for three reanalysis data sets and a 20 -run European Centre Hamburg Version 5 /Max Planck Institute Version–Ocean Model Version 1 global climate model (ECHAM 5 /MPI-OM 1 GCM) ensemble. The dependence of the seriality on cyclone intensity is analyzed. Confirming previous studies, <b>serial</b> <b>clustering</b> is identified in reanalysis data sets primarily on both flanks and downstream regions of the North Atlantic storm track. This pattern is a robust feature in the reanalysis data sets. For the whole area, extreme cyclones cluster more than nonextreme cyclones. The ECHAM 5 /MPI-OM 1 GCM is generally able to reproduce the spatial patterns of clustering under recent climate conditions, but some biases are identified. Under future climate conditions (A 1 B scenario), the GCM ensemble indicates that <b>serial</b> <b>clustering</b> may decrease over the North Atlantic storm track area and parts of western Europe. This decrease is associated with an extension of the polar jet toward Europe, which implies a tendency to a more regular occurrence of cyclones over parts of the North Atlantic Basin poleward of 50 °N and western Europe. An increase of clustering of cyclones is projected south of Newfoundland. The detected shifts imply a change in the risk of occurrence of cumulative events over Europe under future climate conditions...|$|R
40|$|Information Retrieval Systems {{typically}} distinguish be-tween content bearing {{words and}} terms on a stop list. But “content-bearing “ is {{relative to a}} collection. For optimal retrieval efficiency, it is desirable to have au-tomated methods for custom building a stop list. This paper defines the notion of <b>serial</b> <b>clustering</b> of words in text, and explores the value of such clustering {{as an indicator of}} a word bearing cent ent. The numerical measures we propose may also be of value in assigning weights to terms in requests. Experimental support is obtained from natural text databases in three different languages. 1. Introduction an...|$|R
40|$|Society is {{increasingly}} impacted by natural hazards which cause significant damage in economic and human terms. Many of these natural hazards are weather and climate related. Here, {{we show that}} North Atlantic atmospheric circulation regimes affect the propensity of extreme wind speeds in Europe. We also show evidence that extreme wind speeds are long-range dependent, follow a generalized Pareto distribution and are serially <b>clustered.</b> <b>Serial</b> <b>clustering</b> means that storms come in bunches and, hence, do not occur independently. We discuss the use of waiting time distributions for extreme event recurrence estimation in serially dependent time series...|$|R
5000|$|It is {{suggested}} that a general verbal learning component consistently accounts for about 35-40% {{of the total variance}} and consists of total free recall over the five trials of list A, semantic clustering free and cued recall (both short- and long-delays), and recognition hits. A second, [...] "response discrimination" [...] component has also been found in most studies. It accounts for about 8-10% of the variance with loadings from free and cued recall intrusions and recognition false positives. The remaining components, learning strategy (semantic and <b>serial</b> <b>clustering),</b> <b>serial</b> position (primacy and recency) and proactive effect (List B recall) are inconsistent and account for little additional variance.|$|R
40|$|ArticleCyclone {{clusters}} are {{a frequent}} synoptic {{feature in the}} Euro-Atlantic area. Recent {{studies have shown that}} <b>serial</b> <b>clustering</b> of cyclones generally occurs on both flanks and downstream regions of the North Atlantic storm track, while cyclones tend to occur more regulary on the western side of the North Atlantic basin near Newfoundland. This study explores the sensitivity of <b>serial</b> <b>clustering</b> to the choice of cyclone tracking method using cyclone track data from 15 methods derived from ERA-Interim data (1979 – 2010). Clustering is estimated by the dispersion (ratio of variance to mean) of winter [December – February (DJF) ] cyclone passages near each grid point over the Euro-Atlantic area. The mean number of cyclone counts and their variance are compared between methods, revealing considerable differences, particularly for the latter. Results show that all different tracking methods qualitatively capture similar large-scale spatial patterns of underdispersion and overdispersion over the study region. The quantitative differences can primarily be attributed to the differences in the variance of cyclone counts between the methods. Nevertheless, overdispersion is statistically significant for almost all methods over parts of the eastern North Atlantic and Western Europe, and is therefore considered as a robust feature. The influence of the North Atlantic Oscillation (NAO) on cyclone clustering displays a similar pattern for all tracking methods, with one maximum near Iceland and another between the Azores and Iberia. The differences in variance between methods are not related with different sensitivities to the NAO, which can account to over 50...|$|R
30|$|In the {{standard}} <b>serial</b> spectral <b>clustering</b> algorithms, {{we know that}} algorithm computational complexity is mainly presented {{in the construction of}} similar matrix, calculation of k minimum feature vector(s) in Laplace matrix and k-means the clustering. The parallel design of spectral clustering algorithm is processed from the above three aspects.|$|R
3000|$|This metric {{represents}} the <b>serial</b> number of <b>cluster.</b> If {{it is not}} the first time for cognitive user to access cluster, C [...]...|$|R
40|$|Abstract Background There {{are many}} {{important}} clustering questions in computational biology {{for which no}} satisfactory method exists. Automated clustering algorithms, when applied to large, multidimensional datasets, such as flow cytometry data, prove unsatisfactory in terms of speed, problems with local minima or cluster shape bias. Model-based approaches are restricted by the assumptions of the fitting functions. Furthermore, model based <b>clustering</b> requires <b>serial</b> <b>clustering</b> for all cluster numbers within a user defined interval. The final cluster number is then selected by various criteria. These supervised <b>serial</b> <b>clustering</b> methods are time consuming and frequently different criteria result in different optimal cluster numbers. Various unsupervised heuristic approaches {{that have been developed}} such as affinity propagation are too expensive to be applied to datasets on the order of 10 6 points that are often generated by high throughput experiments. Results To circumvent these limitations, we developed a new, unsupervised density contour clustering algorithm, called Misty Mountain, that is based on percolation theory and that efficiently analyzes large data sets. The approach can be envisioned as a progressive top-down removal of clouds covering a data histogram relief map to identify clusters by the appearance of statistically distinct peaks and ridges. This is a parallel clustering method that finds every cluster after analyzing only once the cross sections of the histogram. The overall run time for the composite steps of the algorithm increases linearly by the number of data points. The clustering of 10 6 data points in 2 D data space takes place within about 15 seconds on a standard laptop PC. Comparison of the performance of this algorithm with other state of the art automated flow cytometry gating methods indicate that Misty Mountain provides substantial improvements in both run time and in the accuracy of cluster assignment. Conclusions Misty Mountain is fast, unbiased for cluster shape, identifies stable clusters and is robust to noise. It provides a useful, general solution for multidimensional clustering problems. We demonstrate its suitability for automated gating of flow cytometry data. </p...|$|R
40|$|Cyclone {{clusters}} are {{a frequent}} synoptic {{feature in the}} Euro-Atlantic area. Recent studies have 24 shown that <b>serial</b> <b>clustering</b> of cyclones generally occurs on both flanks and downstream 25 regions of the North Atlantic storm track, while cyclones tend to occur more regulary on the 26 {{eastern side of the}} North Atlantic basin near Newfoundland. This study explores the 27 sensitivity of <b>serial</b> <b>clustering</b> to the choice of cyclone tracking method using cyclone track 28 data from 15 methods derived from ERA-Interim data (1979 - 2010). Clustering is estimated by 29 the dispersion (ratio of variance to mean) of winter (DJF) cyclones passages near each grid 30 point over the Euro-Atlantic area. The mean number of cyclone counts and their variance are 31 compared between methods, revealing considerable differences, particularly for the latter. 32 Results show that all different tracking methods qualitatively capture similar large-scale 33 spatial patterns of underdispersion / overdispersion over the study region. The quantitative 34 differences can primarily be attributed to the differences in the variance of cyclone counts 35 between the methods. Nevertheless, overdispersion is statistically significant for almost all 36 methods over parts of the Eastern North Atlantic and Western Europe, and is therefore 37 considered as a robust feature. The influence of the North Atlantic Oscillation on cyclone 38 clustering displays a similar pattern for all tracking methods, with one maximum near Iceland 39 and another between the Azores and Iberia. The differences in variance between methods are 40 not related with different sensitivities to the NAO, which can account to over 50 % of the 41 clustering in some regions. We conclude that the general features of underdispersion / 42 overdispersion of extra-tropical cyclones over the North Atlantic and Western Europe is 43 robust to the choice of tracking method. The same is true for the influence of the North 44 Atlantic Oscillation on cyclone dispersion...|$|R
40|$|The {{clustering}} in time (seriality) of extratropical cyclones {{is responsible}} for large cumulative insured losses in western Europe, though surprisingly little scientific {{attention has been given}} to this important property. This study has investigated and quantified the seriality of extratropical cyclones in the Northern Hemisphere using a point-process approach. A possible mechanism for <b>serial</b> <b>clustering</b> is the time-varying effect of the large-scale flow on individual cyclone tracks. Another mechanism is the generation by one 'parent' cyclone" of one or more 'offspring through secondary cyclogenesis. A long cyclone-track database was constructed for extended October to March winters from 1950 until 2003 using 6 -hourly analyses of 850 -mb relative vorticity derived from the NCEP/NCAR reanalysis. A dispersion statistic based on the variance-to-mean ratio of monthly cyclone counts was used as a measure of clustering. It reveals extensive regions of statistically significant clustering in the European exit region of the North Atlantic storm track and over the central North Pacific. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
30|$|Most the {{proposed}} MapReduce-based clustering algorithms {{focused on the}} k-means and the DBSCAN methods which deal only with numerical data (points). It is therefore not obvious to compare the results produced by the PMR-Transitive, which operates on categorical data (records), with such methods. So in terms of quality, we suggest comparing the clustering results obtained by {{the proposed}} method with two <b>serial</b> <b>clustering</b> algorithms well-known for clustering categorical data, which are the MMR [8] and some enhanced versions of k-modes [9]. The MMR (Min-Min-Roughness) algorithm {{is based on the}} rough set theory, which requires the number of clusters as an input and uses a new similarity method based on the roughness concept to produce stable results. This algorithm is distinguished by the ability to handle uncertainty in the clustering process. Bai and Liang proposed to use the between-cluster information to improve the effectiveness of some existing versions of the k-modes algorithm. Clustering results of categorical data sets have demonstrated that the improvements brought to the k-modes algorithms are effective and scalable.|$|R
40|$|BACKGROUND: The study {{attempts}} to recruit well known 'cognitive' event related potential measures as an objective estimate of cognitive and specific memory impairment in schizophrenia. METHODS: We examined 19 schizophrenic patients and 28 healthy controls using an auditory discrimination task to elicit event related potentials, {{and a number}} of neuropsychological tests, including tests of general intellectual ability, putative frontal lobe function and verbal memory. RESULTS: The late positive deflection presumed to be associated with stimulus evaluation (P 300) was of lower amplitude and had a longer latency in the patients compared with controls of similar age and sex. We found correlations between P 300 amplitude and latency, and neuropsychological performance scores in patients. There were correlations between decreased P 300 amplitude and lower IQ and poorer memory performance, in particular, abnormal semantic clustering, discriminability and intrusion errors. Increased P 300 latency was correlated with lower pre-morbid IQ, poorer total memory scores and <b>serial</b> <b>clustering,</b> but paradoxically less relative retrieval deficit and fewer intrusion errors. CONCLUSIONS: These findings suggest that abnormal P 300 is generally more likely to occur in patients with memory impairment...|$|R
40|$|AcceptedArticleThis is {{the peer}} {{reviewed}} {{version of the}} following article: Economou, T., Stephenson, D. B., Pinto, J. G., Shaffrey, L. C. and Zappa, G. (2015), <b>Serial</b> <b>clustering</b> of extratropical cyclones in a multi-model ensemble of historical and future simulations. Q. J. R. Meteorol. Soc., which has been published in final form at [URL] This article {{may be used for}} non-commercial purposes in accordance with Wiley Terms and Conditions for Self-Archiving. This study has investigated <b>serial</b> (temporal) <b>clustering</b> of extra-tropical cyclones simulated by 17 climate models that participated in CMIP 5. Clustering was estimated by calculating the dispersion (ratio of variance to mean) of 30 December-February counts of Atlantic storm tracks passing nearby each grid point. Results from single historical simulations of 1975 - 2005 were compared to those from historical ERA 40 reanalyses from 1958 - 2001 ERA 40 and single future model projections of 2069 - 2099 under the RCP 4. 5 climate change scenario. Models were generally able to capture the broad features in reanalyses reported previously: underdispersion/regularity (i. e. variance less than mean) in the western core of the Atlantic storm track surrounded by overdispersion/clustering (i. e. variance greater than mean) to the north and south and over western Europe. Regression of counts onto North Atlantic Oscillation (NAO) indices revealed that much of the overdispersion in the historical reanalyses and model simulations can be accounted for by NAO variability. Future changes in dispersion were generally found to be small and not consistent across models. The overdispersion statistic, for any 30 year sample, is prone to large amounts of sampling uncertainty that obscures the climate change signal. For example, the projected increase in dispersion for storm counts near London in the CNRMCM 5 model is 0. 1 compared to a standard deviation of 0. 25. Projected changes in the mean and variance of NAO are insufficient to create changes in overdispersion that are discernible above natural sampling variations. Natural Environment Research Council (NERC) Consortium on Risk in the Environment: Diagnostics, Integration, Benchmarking, Learning, and Elicitation (CREDIBLE...|$|R
40|$|In this work, a <b>serial</b> on-line <b>cluster</b> {{reconstruction}} technique {{based on}} FPGA technology {{was developed to}} compress experiment data and reduce the dead time of data transmission and storage. At the same time, X-ray imaging experiment based on a two-dimensional positive sensitive triple GEM detector with an effective readout area of 10 cm* 10 cm was done to demonstrate this technique with FPGA development board. The result showed that the reconstruction technology was practicality and efficient. It provides a new idea for data compression of large spectrometers...|$|R
40|$|During {{the last}} decades, several {{windstorm}} series hit Europe leading to large aggregated losses. Such storm series {{are examples of}} <b>serial</b> <b>clustering</b> of extreme cyclones, presenting a considerable risk for the insurance industry. Clustering of events and return periods of storm series for Germany are quantified based on potential losses using empirical models. Two reanalysis data sets and observations from German weather stations are considered for 30 winters. Histograms of events exceeding selected return levels (1 -, 2 - and 5 -year) are derived. Return periods of historical storm series are estimated based on the Poisson and the negative binomial distributions. Over 4000 years of general circulation model (GCM) simulations forced with current climate conditions are analysed to provide a better assessment of historical return periods. Estimations differ between distributions, for example 40 to 65 years for the 1990 series. For such less frequent series, estimates obtained with the Poisson distribution clearly deviate from empirical data. The negative binomial distribution provides better estimates, even though a sensitivity to return level and data set is identified. The consideration of GCM data permits a strong reduction of uncertainties. The present results support the importance of considering explicitly clustering of losses for an adequate risk assessment for economical applications...|$|R
40|$|The {{state-of-the-art}} {{computer hardware}} is coming with multi-core processors. Even mobile phones are coming with dual-core processors. OpenMP is one technology supporting parallel programming on multi-core shared memory systems {{with the help}} of threads. In this paper, we observed the execution times <b>Serial</b> EM <b>Clustering</b> running on single-core and Parallel EM Clustering methods using OpenMP on I 3 system. Observations are made varying number of threads, samples, dimensions and clusters. The results show that OpenMP Lower Triangular Canonical Form with Forward Substitution and Winograd’s approach (OLFW) EM gives...|$|R
40|$|Researchers have {{analysed}} whether {{school and}} local knowledge complement or substitute each other, but have paid {{less attention to}} whether those two learning models use different cognitive strategies. In this study, we use data collected among three contemporary hunter-gatherer societies with relatively low levels of exposure to schooling yet {{with high levels of}} local ecological knowledge to test the association between i) schooling and ii) local ecological knowledge and verbal working memory. Participants include 94 people (24 Baka, 25 Punan, and 45 Tsimane') from whom we collected information on 1) schooling and school related skills (i. e., literacy and numeracy), 2) local knowledge and skills related to hunting and medicinal plants, and 3) working memory. To assess working memory, we applied a multi-trial free recall using words relevant to each cultural setting. People with and without schooling have similar levels of accurate and inaccurate recall, although they differ in their strategies to organize recall: people with schooling have higher results for <b>serial</b> <b>clustering,</b> suggesting better learning with repetition, whereas people without schooling have higher results for semantic clustering, suggesting they organize recall around semantically meaningful categories. Individual levels of local ecological knowledge are not related to accurate recall or organization recall, arguably due to overall high levels of local ecological knowledge. While schooling seems to favour some organization strategies this might {{come at the expense of}} some other organization strategies...|$|R
40|$|Online {{document}} clustering {{takes as}} its input {{a list of}} document vectors, ordered by time. A document vector consists of a list of K terms and their associated weights. The generation of terms and their weights from the document text may vary, but the TF-IDF (term frequency-inverse document frequency) method is popular for clustering applications [1]. The assumption is that the resulting document vector is a good overall representation of the original document. We note that the dimensionality of the document vectors is very high (potentially infinite), since a document could potentially contain any word (term). We also note that the vectors are sparse {{in the sense that}} most term weights have a zero value. We assume that each term not explicitly present in a particular document vector has a weight of zero. Document vectors are normalized. Clusters are also represented as a list of weighted terms. At any given time, a cluster’s term vector is equal to the average of all the document vector’s contained by the cluster. Cluster term vectors are truncated to the top K terms (those containing the highest term weights). Cluster term vectors are kept normalized. The objective of the algorithm is to partition the set of document vectors into a set of clusters, each cluster containing only those documents which are similar to each other with respect to some metric. For this paper, we consider the Euclidean dot product as the similarity metric, as it has been shown to provide good results with the TF-IDF metric [1]. The similarity between a cluster and a document is defined as the dot product between their term vectors. We first present serial a algorithm for online clustering. We then describe a PRAM algorithm for parallel online clustering, assuming a CRCW model. Finally, we present a practical implementation of an approximate parallel online clustering algorithm, suitable for the CUDA parallel computing architecture [2]. 1. <b>Serial</b> <b>Clustering</b> 1 The basic <b>serial</b> online <b>clustering</b> algorithm takes as input a list of n document vectors, as well as a clustering threshold T ranging between 0 and 1. Below is a high level overview of the algorithm...|$|R
40|$|Includes bibliographical {{references}} (p. 38 - 43) Alzheimer's Disease (AD) is {{a progressive}} and fatal brain disorder that {{is characterized by}} memory loss, behavioral change, and cognitive impairments resulting from neuronal degeneration. Declines in olfactory function occur early {{in the course of}} the disease and are evident before other cognitive impairments. Advancing age and possession of the [lower case epsilon] 4 isoform of the apolipoprotein E Gene (apoE) are risk factors for both AD and olfactory function impairment, yet it is likely that both environmental and genetic factors contribute to AD pathology. Stress is an environmental factor that impacts the aging process through physiological and psychological processes. Glucocorticoids (GCs) are hormones that are released during stress, and have been implicated as potentially important in the development of AD. Recent studies demonstrated an interactive impact of stress and apoE [lower case epsilon] 4 on cognitive impairment, but to date, no one has reported on the potential impact of stress and apoE [lower case epsilon] 4 on olfactory function. Elucidating the mechanisms of action that lead to impaired olfaction for those at risk for AD may help inform techniques for prevention and early detection of the disease. The current study examined the effect of stress and the apoE [lower case epsilon] 4 allele on olfactory learning and memory in 32 young and 32 older adults. Stress was operationally defined using the Penn State Worry Questionnaire (PSWQ; subjective stress) and Cortisol Awakening Response (CAR; physiological stress). It was hypothesized that there would be (1) a main effect of stress on odor memory and learning, (2) an apoE by stress interaction such that individuals with at least one apoE [lower case epsilon] 4 allele would have a greater decline in olfactory scores as stress scores increased than individuals with no apoE [lower case epsilon] 4 allele, and (3) an age effect such that the stress and apoE interaction would be evident in older but not younger adults. Subjective stress improved performance on immediate recall across trials, short and long delay free recall, and odor identification. When participants were given cues after a delay, [lower case epsilon] 4 - older and younger adults improved odor recall as PSWQ scores increased, while [lower case epsilon] 4 + younger adults did not improve, and older [lower case epsilon] 4 + adults recalled less as PSWQ scores increased. Use of <b>serial</b> <b>clustering</b> was affected by CAR in those individuals who were CAR responders such that increasing CAR appeared to be associated with increasing use of <b>serial</b> <b>clustering</b> for [lower case epsilon] 4 - individuals, and with decreasing use for [lower case epsilon] 4 + adults. Recognition scores were not significantly affected by either stress measure. Taken together, these findings suggest that stress may affect olfactory memory differentially depending on apoE [lower case epsilon] 4 status. Some support was found to suggest that CAR may be associated with use of certain learning techniques for CAR responders, and further studies are needed to confirm these findings in larger samples...|$|R
40|$|Motivated by genetic {{association}} {{studies of}} pleiotropy, we propose a Bayesian latent variable approach to jointly study multiple outcomes. The models studied here can in-corporate both continuous and binary responses, and {{can account for}} <b>serial</b> and <b>cluster</b> correlations. We consider Bayesian estimation for the model parameters, and we de-velop a novel MCMC algorithm that builds upon hierarchical centering and parameter expansion techniques to efficiently sample from the posterior distribution. We evalu-ate the proposed method via extensive simulations and demonstrate its utility with an application to an association study of various complication outcomes related to type 1 diabetes. This article has supplementary material online...|$|R
40|$|Evidence {{suggests}} that standard learning and recall indexes are sensitive markers of verbal declarative memory ability in bipolar disorder (BD), but no study has examined performance across {{the full range}} of component process measures on the Hopkins Verbal Learning Test (HVLT-R) in a BD cohort. As the HVLT-R is part of a widely used battery of cognitive functioning backed by the U. S. Federal Drug Administration as the accepted battery for use in pro-cognitive trials assessing cognitive-enhancing drugs in the related disorder schizophrenia, estimating the utility of its measures in BD is important. Forty-nine BD patients and 51 healthy controls completed the HVLT-R, which was scored for 13 variables of interest, across 4 indices: recall and learning, recognition, strategic organization, and errors. BD patients had greater difficulty in learning the HVLT-R word list compared to controls. They also demonstrated impairment in delayed recall/ recognition. There were no differences between the groups in terms of their slope of learning, retrieval index, retention percentage, semantic or <b>serial</b> <b>clustering,</b> errors, or level of retrieval. This pattern was consistent across symptomatic and euthymic patients. The HVLT-R has some utility in characterizing the component processes involved in memory function in BD, such that memory impairments appear to be attributable to deficient encoding processes during the acquisition phase of learning. In the case of planning pro-cognitive clinical trials, the encoding deficits in BD observed here may be sensitive enough to potentially respond to medications designed to enhance the verbal memory performance...|$|R
30|$|Those {{data on the}} Internet {{exist in}} vast scale and grow rapidly, so it is {{urgently}} required in technology to mine high-value information from the mass data. As a kind of unsupervised learning method, clustering algorithm is a technique commonly used in data statistics and analysis which contains data mining, machine learning, pattern recognition, image analysis, and many other areas. The traditional <b>serial</b> <b>clustering</b> algorithm has two problems {{and it is difficult}} to meet the needs of practical applications: the first one is that the speed of clustering is not fast enough and the efficiency is not high; the other one is that in the face of mass data, subject to the limits of memory capacity, it often cannot run effectively. This paper studied the traditional spectral clustering algorithm and designed efficient parallel spectral clustering algorithm. The strategy of parallel spectral clustering algorithm is to compute similar matrix and sparse according to data points segmentation; when computing eigenvectors, store the Laplacian matrix on the distributed file system HDFS, use distributed Lanczos to compute and get the eigenvectors by parallel computation; at last, in terms of the transposed matrix of eigenvectors, adopt the improved parallel K-Means cluster to obtain the clustering results. Through adopting different parallel strategies about each step of the algorithm, the whole algorithm gets linear growth in speed. The experimental results show that the proposed parallel spectral clustering algorithm is suitable for applying in mass data mining. We hope that the research achievements of this paper can provide inspiration and application value for subsequent research developers.|$|R
40|$|Given a {{very large}} moderate-to-high {{dimensionality}} dataset, how could one cluster its points? For datasets that don’t fit even on a single disk, parallelism is a first class option. In this paper we explore MapReduce for clustering this kind of data. The main questions are (a) how to minimize the I/O cost, {{taking into account the}} already existing data partition (e. g., on disks), and (b) how to minimize the network cost among processing nodes. Either of them may be a bottleneck. Thus, we propose the Best of both Worlds – BoW method, that automatically spots the bottleneck and chooses a good strategy. Our main contributions are: (1) We propose BoW and carefully derive its cost functions, which dynamically choose the best strategy; (2) We show that BoW has numerous desirable features: it can work with most <b>serial</b> <b>clustering</b> methods as a pluggedin clustering subroutine, it balances the cost for disk accesses and network accesses, achieving a very good tradeoff between the two, it uses no user-defined parameters (thanks to our reasonable defaults), it matches the clustering quality of the serial algorithm, and it has near-linear scale-up; and finally, (3) We report experiments on real and synthetic data with billions of points, using up to 1, 024 cores in parallel. To the best of our knowledge, our Yahoo! web is the largest real dataset ever reported in the database subspace clustering literature. Spanning 0. 2 TB of multi-dimensional data, it took only 8 minutes to be clustered, using 128 cores. 1...|$|R
5000|$|His output can {{be divided}} roughly into three {{creative}} periods. His early works from 1963 up {{to the beginning of}} the 1970s include several works for the stage as well as chamber music and three [...] "Essays" [...] for orchestra. In these he initially employed <b>serial</b> and <b>cluster</b> techniques, claiming later that he considered most of them [...] "to be thrown away." [...] Around 1969 Goldmann developed a technique of appropriating established musical forms (such as sonata, symphony, string quartet, etc.) and [...] "breaking them open from within", thereby changing their impact and meaning [...] Important examples of this phase are Bläsersonate (1969) and Symphony No. 1 (1971), both of which are major early examples of the deconstruction of the idea of linear progress in new music since the 1970s [...]|$|R
30|$|When {{the data}} volume {{is less than}} 50000, because in the {{parallel}} process, the data volume of each node is not big enough, the speed is smaller than the <b>serial</b> spectral <b>clustering</b> algorithm. However, {{with the increase of}} data volume, the speed of parallel algorithm is gradually increased, especially when the data volume is over 1000000, the speedup ratio grows significantly. The running time of stand-alone mode is 3.667 times as long as that of ten computers when dataset volume is 10000. However, it is 1014.39 times when dataset is 5000000. But, it can be seen from Figure  4, when the number of nodes increases to 8 or more, the increasing range of speed-up will narrow. It can be illustrated that the execution efficiency of the parallel spectral clustering algorithm based on Hadoop platform is higher than that of conventional spectral clustering algorithm.|$|R
40|$|In {{this paper}} we compare {{accelerated}} waveform relaxation algorithms to pointwise direct and iterative methods for the parallel transient simulation of semiconductor devices on parallel machines. Experimental {{results are presented}} for simulations on single (<b>serial)</b> workstations, <b>clusters</b> of workstations, and an IBM SP- 2. The results show that accelerated waveform methods are competitive with standard pointwise methods on serial machines, but that accelerated waveform methods are significantly faster in loosely coupled MIMD parallel environments. In particular, parallel accelerated waveform methods achieve significant speedup on workstation clusters and achieve nearly linear speed-up (with respect {{to the number of}} processors) on the IBM SP- 2. Experiments with parallel versions of standard pointwise methods exhibited limited or no parallel speedup. The strong implication of the results is that, as MIMD machines and cluster-based computing become more prevalent, accelerated waveform method [...] ...|$|R
40|$|Motivation: Large-scale Gene Discovery {{projects}} {{based on}} EST sequencing {{are important for}} providing catalogs of cDNAs for various purposes. To provide accurate estimates of gene discovery, the methods used must be robust {{in the presence of}} sequence errors. Efficient informatics systems {{play a vital role in}} a number of aspects of the process: <b>serial</b> subtraction, <b>clustering,</b> annotation, quality trimming, and clone verification. Subtracted cDNA libraries have been constructed to address the issues of high redundancy, and has implemented a sampling-based method for improving the quality of clone naming. Results: A large-scale EST-based gene discovery program at the University of Iowa has led to the discovery of more than 40, 000 previously undiscovered genes in three mammalian species in less than two years. The overall system of software is described, statistics concerning performance and throughput are given, and detailed methods of certain aspects of the pipeline are provided. In particular, our sequence editing, clustering, and clone verification methods are described in some detail. Availability: All of the originally produced software described in this paper is available from our project web server at...|$|R
40|$|We are {{concerned}} with the number and frequency of transactions on financial markets. Using transactions data on FTSE 100 index futures in 2002 from LIFFE, we illustrate the stylized features of transaction tick data. We find effects pointing to the presence of strong seasonal patterns, long-term <b>serial</b> dependence, <b>clustering</b> as well as strong interdependence between asks and trades and bids and trades, respectively. We also develop a model for the times and frequencies of transactions. We propose that transactions in individual assets form inhomogeneous Poisson processes, their intensities being generated by a continuous-time Markov chain describing the level of activity in the market. In addition, an iterative maximum likelihood estimation procedure is developed; we use it with success to model transactions in FTSE 100 index futures. Acknowledgements During the course of my practical project, I have received tremendous assistance from my supervisors. I am greatly indebted to my external supervisor, prof. L. C. G. Rogers for sharing his insights and experience on suitable models and estimation methodologies. Dr. J. R. Norris, my M. Phil. supervisor...|$|R
40|$|Abstract—During {{times of}} stock market turbulence, monitor-ing the intraday {{clustering}} behaviour of financial instruments {{allows one to}} better understand market characteristics and systemic risks. While genetic algorithms provide a versatile methodology for identifying such <b>clusters,</b> <b>serial</b> implementations are computationally intensive and can {{take a long time}} to converge to the global optimum. We implement a Master-Slave parallel genetic algorithm (PGA) with a Marsili and Giada log-likelihood fitness function to identify clusters within stock correlation matrices. We utilise the Nvidia Compute Unified Device Architecture (CUDA) programming model to implement a PGA and visualise the results using minimal spanning trees (MSTs). We demonstrate that the CUDA PGA implementation runs significantly faster than the test case implementation of a comparable serial genetic algorithm. This, combined with fast online intraday correlation matrix estimation from high frequency data for cluster identification, may enhance near-real-time risk assessment for financial practitioners. I...|$|R
5000|$|The first {{large-scale}} serial sessions using {{a single}} computer were STAR (based on Star Trek), OCEAN (a battle using ships, submarines and helicopters, with players divided between two combating cities) and 1975's CAVE (based on Dungeons and Dragons), created by Christopher Caldwell (with art work and suggestions by Roger Long and assembly coding by Robert Kenney) on the University of New Hampshire's DECsystem-1090. The university's computer system {{had hundreds of}} terminals, connected (via <b>serial</b> lines) through <b>cluster</b> PDP-11s for student, teacher and staff access. The games had a program running on each terminal (for each player), sharing a segment of shared memory (known as the [...] "high segment" [...] in the OS TOPS-10). The games became popular, and the university often banned them because of their RAM use. STAR was based on 1974's single-user, turn-oriented BASIC program STAR, written by Michael O'Shaughnessy at UNH.|$|R
40|$|Alternating Direction Implicit (ADI) {{methods have}} been in use {{for a long time}} for the {{solution}} of both parabolic and elliptic partial differential equations. In the case where good estimates of the eigenvalues of the operator are available, the convergence of these methods can be dramatically accelerated. However, in the case of computation on parallel computers, the solution of the tridiagonal system can impose an unreasonable overhead. We discuss methods to lower the overhead imposed by the solution of the corresponding tridiagonal systems. The proposed method has the same convergence properties as a standard ADI method, but all of the solves run in approximately {{the same time as the}} "fast" direction. Hence, this acts like a "transpose-free" method while still maintaining the smoothing properties of ADI. Algorithms are derived and convergence theory is provided. Numerical examples on <b>serial,</b> parallel, and <b>clusters</b> of processors are provided showing how much of a speed up can be gai [...] ...|$|R
40|$|In this paper, {{we observe}} that many Web pages contain ge-olocation {{information}} (address, zipcode, and telephone area code) {{and many of}} these geolocation items are directly re-lated to the locations of the IP addresses that host the Web pages. We then design Structon, a system that mines Web pages for IP address geolocations. In Structon, we first ex-tract geolocation information from every crawledWeb pages, we then devise a <b>serial</b> of information <b>clustering,</b> false-inform-ation filtering, error-correction, and location inferring algo-rithms to map IP addresses to geolocations. We have run our algorithms on top of a set of 74 M Chinese Web pages, from which we are able to identify the geolocations for 8. 2 M IP addresses, which contain addresses for not only Web servers but also client hosts. We have verified our result with an IP address location table of a major Chinese ISP, the ver-ification shows that the accuracy of Structon is 94. 4 % at province level. 1...|$|R
