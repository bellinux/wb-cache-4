382|815|Public
25|$|Dendritic spines {{were first}} {{described}} {{at the end}} of the 19th century by Santiago Ramón y Cajal on cerebellar neurons. Ramón y Cajal then proposed that dendritic spines could serve as contacting sites between neurons. This was demonstrated more than 50 years later thanks to the emergence of electron microscopy. Until the development of confocal microscopy on living tissues, it was commonly admitted that spines were formed during embryonic development and then would remain stable after birth. In this paradigm, variations of <b>synaptic</b> <b>weight</b> were considered as sufficient to explain memory processes at the cellular level. But since about a decade ago, new techniques of confocal microscopy demonstrated that dendritic spines are indeed motile and dynamic structures that undergo a constant turnover, even after birth.|$|E
25|$|One {{mechanism}} for preserving the dynamic {{range of a}} neuron is synaptic scaling, the homeostatic form of plasticity that restores neuronal activity to its normal 'baseline' levels by changing the postsynaptic response of all the synapses of a neuron {{as a function of}} activity. This means that the same scaling is done to each synapse, to either strengthen or weaken all of a neuron’s connections. Scaling can be multiplicative (multiplying or dividing the strength of each synapse by a constant number) or additive (adding or subtracting the same value from the <b>synaptic</b> <b>weight).</b> Homeostatic mechanisms go beyond the synapse. Modulation of the intrinsic excitability of a neuron is a way to maintain stability despite changing numbers and strengths of synapses. Cultured cortical pyramidal neurons maintain stability through the regulation of ionic conductances. The regulation of ionic conductances is achieved through the controlled release of brain-derived neurotrophic factor (BDNF). BDNF has also been found to influence synaptic scaling, suggesting that this neurotrophic factor may be responsible for the coordination of synaptic and nonsynaptic mechanisms in homeostatic plasticity.|$|E
2500|$|In neuroscience, the {{distribution}} of firing rates across a population of neurons is often approximately lognormal. This has been first observed in the cortex and striatum [...] and later in hippocampus and entorhinal cortex, {{and elsewhere in the}} brain.. Also, intrinsic gain distributions and <b>synaptic</b> <b>weight</b> distributions appear to be lognormalas well.|$|E
30|$|Initialize (init) the <b>synaptic</b> <b>weights</b> w_i randomly.|$|R
30|$|The {{coexistence}} {{of both the}} RS is useful for the neuromorphic applications. In such applications, analog switching {{with an increase in}} the <b>synaptic</b> <b>weights</b> can mimic through the homogeneous RS, whereas the better resistance retention properties can be achieved through filamentary RS [11]. To check the suitability of Al/MnO 2 /SS thin film MIM device for neuromorphic application, we have plotted the maximum positive (ISET) and negative current (IRESET) with different sweep rates at different resistive switching voltages and analog memory as shown in Fig.  5 a–d, respectively. It is observed that the current increases in both the bias regions with an increase in voltage sweeps and sweep rates. In the biological synapse, improvement in the strength of <b>synaptic</b> <b>weights</b> results in the better learning [33]. In many memristive-based electronic synapses, current (i) was considered as <b>synaptic</b> <b>weights</b> [33, 34, 35]. Considering the incremental ISET and IRESET or <b>synaptic</b> <b>weights,</b> the Al/MnO 2 /SS MIM device is suitable for the neuromorphic applications.|$|R
40|$|Generating functionals may {{guide the}} {{evolution}} of a dynamical system and constitute a possible route for handling the complexity of neural networks as relevant for computational intelligence. We propose and explore a new objective function which allows to obtain plasticity rules for the afferent <b>synaptic</b> <b>weights.</b> The adaption rules are Hebbian and self-limiting and result from the minimization of the the Fisher information with respect to the synaptic flux. We perform a series of simulations examining the behavior of the new learning rules in various circumstances. The vector of <b>synaptic</b> <b>weights</b> aligns with the principal direction of input activities, whenever one is present. A linear discrimination is performed when there are two or more principal directions; directions having bimodal firing-rate distributions, being characterized by a negative excess kurtosis, are preferred. We find robust performance and full homeostatic adaption of the <b>synaptic</b> <b>weights</b> results as a by-product of the synaptic flux minimization. This self-limiting behavior allows for stable online learning for arbitrary durations. The neuron acquires new information when the statistics of input activities is changed at a certain point of the simulation, showing however a distinct resilience to unlearn previously acquired knowledge. Learning is fast when starting with randomly drawn <b>synaptic</b> <b>weights</b> and substantially slower when the <b>synaptic</b> <b>weights</b> are already fully adapted. <br/...|$|R
2500|$|... where [...] is the <b>synaptic</b> <b>weight</b> of the th input axon, [...] {{is a time}} {{constant}} dependent on the insertion and removal rates of neurotransmitter receptors, which is dependent on , the concentration of calcium. [...] is also {{a function of the}} concentration of calcium that depends linearly on the number of receptors on the membrane of the neuron at some fixed point. Both [...] and [...] are found experimentally and agree on results from both hypotheses. The model makes important simplifications that make it unsuited for actual experimental predictions, but provides a significant basis for the hypothesis of a calcium-based synaptic plasticity dependence.|$|E
50|$|In rate coding, {{learning}} {{is based on}} activity-dependent <b>synaptic</b> <b>weight</b> modifications.|$|E
5000|$|The <b>synaptic</b> <b>weight</b> in {{this process}} is {{determined}} by several variable factors: ...|$|E
40|$|To {{understand}} {{the behavior of}} a neural circuit it is a presupposition that we have a model of the dynamical system describing this circuit. This model is determined by several parameters, including not only the <b>synaptic</b> <b>weights,</b> but also the parameters of each neuron. Existing works mainly concentrate on either the <b>synaptic</b> <b>weights</b> or the neural parameters. In this paper we present an algorithm to reconstruct all parameters including the <b>synaptic</b> <b>weights</b> of a spiking neuron model. The model based on works of Eugene M. Izhikevich (Izhikevich 2007) consists of two differential equations and covers different types of cortical neurons. It combines the dynamical properties of Hodgkin-Huxley-type dynamics with a high computational efficiency. The presented algorithm uses the recordings of the corresponding membrane potentials of the model for the reconstruction and consists of two main components. The first component is a rank based Genetic Algorithm (GA) which is used to find the neural parameters of the model. The second one is a Least Mean Squares approach which computes the <b>synaptic</b> <b>weights</b> of all interconnected neurons by minimizing the squared error between the calculated and the measured membrane potentials for each time step. In preparation for the reconstruction of the neural parameters and of the <b>synaptic</b> <b>weights</b> from real measured membrane potentials, promising results based on simulated data generated with a randomly parametrized Izhikevich model are presented. The reconstruction does not only converge to a global minimum of neural parameters, but also approximates the <b>synaptic</b> <b>weights</b> with high precision. Comment: 6 pages, 7 figure...|$|R
5000|$|... where w is a vector of <b>synaptic</b> <b>weights</b> and x is a vector of inputs.|$|R
3000|$|... are nonnegative {{constants}} {{and reflect}} the <b>synaptic</b> <b>weights</b> of intraarea connections and feedback connections, respectively, and [...]...|$|R
5000|$|The <b>synaptic</b> <b>weight</b> {{is changed}} {{by using a}} {{learning}} rule, {{the most basic of}} which is Hebb's rule, which is usually stated in biological terms as ...|$|E
50|$|A {{neural network}} that {{undergoes}} plastic changes between synapses must initiate normalization mechanisms {{in order to}} combat unrestrained potentiation or depression. One mechanism assures that the average firing rate of these neurons is kept at a reasonable rate through synaptic scaling. In this process, input levels are changed in cells to maintain average firing rate. For example, inhibitory synapses are strengthened or excitatory synapses are weakened to normalize the neural network and allow single neurons to regulate their firing rate.Another mechanism is the cell-wide redistribution of <b>synaptic</b> <b>weight.</b> This mechanism conserves the total <b>synaptic</b> <b>weight</b> across the cell by introducing competition between synapses. Thus, normalizing a single neuron after plasticity. During development, cells can be refined when some synapses are preserved and others are discarded to normalize total <b>synaptic</b> <b>weight.</b> In this way, homeostasis is conserved in cells that are undergoing plasticity and normal operation of learning networks is also preserved, allowing new information to be learned.|$|E
5000|$|... where [...] is a <b>synaptic</b> <b>weight,</b> {{describing}} {{the influence of}} neuron [...] on neuron , [...] expresses the leak, and [...] provides the spiking history of neuron [...] before , according to ...|$|E
40|$|Recent {{electrophysiological}} {{data show}} that <b>synaptic</b> <b>weights</b> are highly influenced by electrical activities displayed by neurons. Weights are not stable as assumed in classical neural network models. What {{is the nature of}} engrams, if not stored in <b>synaptic</b> <b>weights?</b> Adopting the theory of dynamical systems, which allows an implicit form of memory, we propose a new framework for learning, where <b>synaptic</b> <b>weights</b> are continuously adapted. Evolutionary computation has been applied to a population of dynamic neural networks evolving in a prey-predator environment. Each individual develops complex dynamic patterns of neuronal activity, underlied by multiple recurrent connections. We show that this method allows the emergence of learning capability through generations, as a by-product of evolution, since the behavioural performance of the network is not a priori based on this property...|$|R
40|$|This work {{shows how}} a {{modified}} Kohonen Self-Organizing Map with one dimensional neighborhood {{is used to}} approach the symmetrical Traveling Salesperson Problem (TSP). Solution generated by the Kohonen network is improved by the 2 opt algorithm. The paper describes briefly self-organization in neural networks, 2 opt algorithm and modifications applied to Self-Organizing Map. Finally, the algorithm is compared with Evolutionary Algorithm with Enhanced Edge Recombination operator and Lin-Kerninghan algorithm. Kohonen Self-Organizing Map basics In 1975 Teuvo Kohonen introduced new type of neural network that uses competitive, unsupervised learning [1]. This approach is based on WTA (Winner Takes All) and WTM (Winner Takes Most) algorithms. Therefore, these algorithms will explained here briefly. The most basic competitive learning algorithm is WTA. When input vector (a pattern) is presented, a distance to each neuron's <b>synaptic</b> <b>weights</b> is calculated. The neuron whose weights are most correlated to current input vector is the winner. Correlation is equal to scalar product of input vector and considered <b>synaptic</b> <b>weights.</b> Only the winning neuron modifies it's <b>synaptic</b> <b>weights</b> to the point presented by input pattern. <b>Synaptic</b> <b>weights</b> of other neurons do not change. The learning process can be described by the following equation...|$|R
40|$|Abstract- A novel {{algorithm}} named Spike-LMS {{is described}} that adapts the <b>synaptic</b> <b>weights</b> of an artificial spiking neuron {{to produce a}} desired response. The derivation of Spike-LMS follows from the derivation of the Least-Mean Squares (LMS) algorithm used in adaptive filter theory. Spike-LMS works directly {{in the domain of}} spike trains, and therefore makes no assumptions about any particular neural encoding method. This algorithm is able to identify the <b>synaptic</b> <b>weights</b> of a spiking neuron given the pre-synaptic and post-synaptic spike trains. ...|$|R
5000|$|... where [...] {{defines the}} <b>synaptic</b> <b>weight</b> or {{connection}} strength between the th input and th output neurons, [...] and [...] are the {{input and output}} vectors, respectively, and [...] is the learning rate parameter.|$|E
5000|$|... or {{the change}} in the th <b>synaptic</b> <b>weight</b> [...] is equal to a {{learning}} rate [...] times the th input [...] times the postsynaptic response [...] Often cited is the case of a linear neuron, ...|$|E
5000|$|For small , our higher-order terms [...] go to zero. We again {{make the}} {{specification}} of a linear neuron, that is, {{the output of}} the neuron is equal to the sum of the product of each input and its <b>synaptic</b> <b>weight,</b> or ...|$|E
40|$|Retention of {{parameters}} and learnt <b>synaptic</b> <b>weights</b> {{is a central}} problem {{in the construction of}} neural networks. We have applied analog oating gate technology to solve these problems in the context of biologically realistic `silicon neurons'. Parameters are stored on a novel oating gate array, and <b>synaptic</b> <b>weights</b> are retained by a oating gate learning synapse, that performs on-chip learning. The latter can emulate a form of long term potentiation (LTP) and long term depression (LTD) as observed in biological neurons...|$|R
40|$|The {{majority}} of distinct sensory and motor events occur as temporally ordered sequences with rich probabilistic structure. Sequences {{can be characterized}} by the probability of transitioning from the current state to upcoming states (forward probability), {{as well as the}} probability of having transitioned to the current state from previous states (backward probability). Despite the prevalence of probabilistic sequencing of both sensory and motor events, the Hebbian mechanisms that mold synapses to reflect the statistics of experienced probabilistic sequences are not well understood. Here, we show through analytic calculations and numerical simulations that Hebbian plasticity (correlation, covariance and STDP) with pre-synaptic competition can develop <b>synaptic</b> <b>weights</b> equal to the conditional forward transition probabilities present in the input sequence. In contrast, post-synaptic competition can develop <b>synaptic</b> <b>weights</b> proportional to the conditional backward probabilities of the same input sequence. We demonstrate that to stably reflect the conditional probability of a neuron's inputs and outputs, local Hebbian plasticity requires balance between competitive learning forces that promote synaptic differentiation and homogenizing learning forces that promote synaptic stabilization. The balance between these forces dictates a prior over the distribution of learned <b>synaptic</b> <b>weights,</b> strongly influencing both the rate at which structure emerges and the entropy of the final distribution of <b>synaptic</b> <b>weights.</b> Together, these results demonstrate a simple correspondence between the biophysical organization of neurons, the site of synaptic competition, and the temporal flow of information encoded in <b>synaptic</b> <b>weights</b> by Hebbian plasticity while highlighting the utility of balancing learning forces to accurately encode probability distributions, and prior expectations over such probability distributions...|$|R
3000|$|... 2 was {{measured}} after waiting for 5  min. According {{to the literature}} [10], the relative change of the <b>synaptic</b> <b>weights</b> (ΔW) is defined as (I [...]...|$|R
5000|$|The {{most basic}} {{model of a}} neuron {{consists}} of an input with some <b>synaptic</b> <b>weight</b> vector and an activation function or transfer function inside the neuron determining output. This is the basic structure used in artificial neurons, which in a neural network often looks like ...|$|E
50|$|In {{neuroscience}} {{and computer}} science, <b>synaptic</b> <b>weight</b> {{refers to the}} strength or amplitude of a connection between two nodes, corresponding in biology {{to the amount of}} influence the firing of one neuron has on another. The term is typically used in artificial and biological neural network research.|$|E
5000|$|... where [...] is <b>synaptic</b> <b>weight</b> that {{expresses the}} {{increase}} of membrane potential of neuron [...] because of neuron 's spike, [...] is a function that models the leak of potential and [...] is the most recent period of neuron 's spike before the given time period , considering the formula ...|$|E
40|$|Quantization of the <b>synaptic</b> <b>weights</b> is {{a central}} problem of {{hardware}} implementation of neural networks using 0 technology. In this paper, a particular linear threshold boolean function, called majority function is considered, whose <b>synaptic</b> <b>weights</b> are restricted to only three values: Γ 1, 0, + 1. Some results about {{the complexity of the}} circuits composed of such gates are reported. They show that this simple family of functions remains powerful in term of circuit complexity. The learning problem with this subclass of threshold function is also studied and numerical experiments of different algorithms are reported. Keywords: neural network, linear threshold function, circuit complexity, <b>synaptic</b> <b>weights</b> quantization, majority functions. 1 Introduction and Motivation The works reported in the literature on artificial neural nets can be subdivided in two classes. On one hand, theorists deal with the general issues of connexionism such as: machine learning, classification, optimiz [...] ...|$|R
5000|$|A set {{of neurons}} {{that are all}} the same except for some {{randomly}} distributed <b>synaptic</b> <b>weights,</b> and which therefore respond differently to a given set of input patterns ...|$|R
40|$|Abstract:- In {{this paper}} we study the {{mathematical}} {{foundations of the}} phenomenon of multiplicative scaling of <b>synaptic</b> <b>weights</b> (strengths). The Hebbian learning rule {{that gave rise to}} the entire neural network area is only an approximation of what happens in Central Nervous System synapses. Conditional probabilities are postulated to match biological synaptic strengthening in a more realistic way. Multiplicative scaling of <b>synaptic</b> <b>weights</b> is a consequence of conditional probabilities calculation taking place at the level of synapses. Normalization of post-synaptic activity is also involved in the scaling (or normalization) of synapses...|$|R
5000|$|... where [...] is the <b>synaptic</b> <b>weight</b> of the th synapse, [...] is that synapse's input current, [...] is the {{weighted}} presynaptic output vector, [...] is the postsynaptic activation function that changes sign at some output threshold , and [...] is the (often negligible) time constant of uniform decay of all synapses.|$|E
50|$|The {{changes in}} <b>{{synaptic}}</b> <b>weight</b> that occur {{is known as}} synaptic plasticity, and the process behind long-term changes (long-term potentiation and depression) is still poorly understood. Hebb's original learning rule was originally applied to biological systems, but has had to undergo many modifications {{as a number of}} theoretical and experimental problems came to light.|$|E
50|$|Computationally, {{this means}} that if a large signal {{from one of the}} input neurons results in a large signal from one of the output neurons, then the <b>synaptic</b> <b>weight</b> between those two neurons will increase. The rule is unstable, however, and is {{typically}} modified using such variations as Oja's rule, radial basis functions or the backpropagation algorithm.|$|E
40|$|In this paper, Miller's {{correlation}} based {{model of}} orientation selectivity is studied, in particular, a {{development of an}} oriented receptive eld is considered and analyzed mathematically. Our computer simulations show that (1) the nal shape of a receptive eld most likely appears just after the rst update of <b>synaptic</b> <b>weights,</b> and (2) <b>synaptic</b> <b>weights</b> are updated monotonously preserving {{the shape of a}} receptive eld that is already determined at the rst step. We try to explain these two properties and give some mathematical justications by considering a simplied model. As a result, we show that (1) if a learning eÆcacy is small enough, <b>synaptic</b> <b>weights</b> almost always grow monotonously, and (2) if the width of an arbor and the range of a correlation is similar, then the increment at the rst step forms three separated subregions of ON and OFF. Moreover, we also show that if the range of a correlation is much smaller than the width of an arbor, then the incremental forms several sm [...] ...|$|R
40|$|Sets of {{neuronal}} tuning curves, which {{describe the}} responses of neurons as functions of a stimulus, {{can serve as a}} basis for approximating other functions of stimulus parameters. In a function approximating network, <b>synaptic</b> <b>weights</b> determined by a correlation-based Hebbian rule are closely related to the coefficients that result when a function is expanded in an orthogonal basis. Although neuronal tuning curves typically are not orthogonal functions, the relationship between function approximation and correlation-based <b>synaptic</b> <b>weights</b> can be retained if the tuning curves satisfy the conditions of a tight frame. We examine whether the spatial receptive fields of simple cells in cat and monkey primary visual cortex (V 1) form a tight frame, allowing them to serve as a basis for constructing more complicated extrastriate receptive fields using correlation-based <b>synaptic</b> <b>weights.</b> Our calculations show that the set of V 1 simple cell receptive fields is not tight enough to account for the a [...] ...|$|R
40|$|Mean-field {{models of}} the cortex have been used {{successfully}} to interpret the origin of features on the electroencephalogram under situations such as sleep, anesthesia, and seizures. In a mean-field scheme, dynamic changes in <b>synaptic</b> <b>weights</b> can be considered through fluctuation-based Hebbian learning rules. However, because such implementations deal with population-averaged properties, they are not well suited to memory and learning applications where individual <b>synaptic</b> <b>weights</b> can be important. We demonstrate that, through an extended system of equations, the mean-field models can be developed further to look at higher-order statistics, in particular, the distribution of <b>synaptic</b> <b>weights</b> within a cortical column. This allows us to make some general conclusions on memory through a mean-field scheme. Specifically, we expect large changes in the standard deviation {{of the distribution of}} <b>synaptic</b> <b>weights</b> when fluctuation in the mean soma potentials are large, such as during the transitions between the “up” and “down” states of slow-wave sleep. Moreover, a cortex that has low structure in its neuronal connections is most likely to decrease its standard deviation in the weights of excitatory to excitatory synapses, relative to the square of the mean, whereas a cortex with strongly patterned connections is most likely to increase this measure. This suggests that fluctuations are used to condense the coding of strong (presumably useful) memories into fewer, but dynamic, neuron connections, {{while at the same time}} removing weaker (less useful) memories...|$|R
