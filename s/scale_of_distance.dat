13|10000|Public
50|$|The {{variation}} in altitude, {{from the ground}} level down to the sphere's or ellipsoid's surface, also changes the <b>scale</b> <b>of</b> <b>distance</b> measurements.|$|E
5000|$|Different {{factors are}} {{excluded}} or included {{that affect the}} <b>scale</b> <b>of</b> <b>distance.</b> The USDA maintains an online interactive mapping tool for the U.S., the [...] "Food Access Research Atlas," [...] which applies four different measurement standards to identify areas of low food access based on distance from the nearest supermarket.|$|E
5000|$|Of course, {{all this}} {{experience}} {{depends on the}} condition that the de Broglie wavelength is so small that on the ordinary <b>scale</b> <b>of</b> <b>distance</b> and time, the wave modulation in the detection can be neglected; this is equivalent to assuming an infinitely small wavelength of matter. When a finite de Broglie wavelength is taken into account, new problems of 'wave-particle duality' do in fact arise, which ran through the famous Bohr-Einstein debate and is still a key issue in recent discussions.|$|E
25|$|<b>Scaling</b> <b>of</b> <b>distances</b> {{while keeping}} the masses the same (in the case of point masses, or by {{reducing}} the densities) gives similar orbits; if distances are multiplied by 4, gravitational forces and accelerations are divided by 16, velocities are halved and orbital periods are multiplied by 8.|$|R
40|$|An {{analysis}} is presented for the bivariate parameter {{differences between the}} FPS- 16 Radar/Jimsphere and the Meteorological Sounding System (MSS) Windsonde. The Jimsphere is used as the standard to measure the ascent wind during the Space Shuttle launches at Kennedy Space Center, Florida, and the Windsonde is the backup system. In addition, {{a discussion of the}} terrestrial environment (below 20 km) and a description of the Jimsphere and Windsonde wind sensors are given. Computation of the wind statistics from 64 paired Jimsphere and Windsonde balloon releases in support of 14 Space Shuttle launches shows a good agreement between the two wind sensors. From the analysis of buildup and back-off data for various <b>scales</b> <b>of</b> <b>distance</b> and the comparison of the cumulative percent frequency (CPF) versus wind speed change, it is shown that the wind speed change for various <b>scales</b> <b>of</b> <b>distances</b> for the Jimsphere and Windsonde compare favorably...|$|R
5000|$|... 200 nm {{isolation}} pitch (91% of 65 nm generation) {{indicating a}} slowing <b>of</b> <b>scaling</b> <b>of</b> isolation <b>distance</b> between transistors ...|$|R
50|$|In {{quantum field}} theory, {{specifically}} {{the theory of}} renormalization, the bare mass of an elementary particle is the limit of its mass as the <b>scale</b> <b>of</b> <b>distance</b> approaches zero or, equivalently, as the energy of a particle collision approaches infinity. It differs from the invariant mass as usually understood because the latter includes the 'clothing' of the particle by pairs of virtual particles that are temporarily created by the force-fields around the particle. In some versions of field theory, the bare mass of some particles may be plus or minus infinity. In {{the theory of the}} electroweak interaction using the Higgs boson, all particles have a bare mass of zero.|$|E
5000|$|The {{second and}} third {{standards}} adjust the <b>scale</b> <b>of</b> <b>distance</b> and consider income to define a food desert. In the U.S., a food desert consists of a low income census tract residing at least 0.5 miles in urban areas (10 miles in rural areas) or 1 mile away in urban areas (20 miles in rural areas) from the large grocery store. [...] The availability of other fresh food sources like community gardens and food banks {{are not included in}} mapping and can change the number of communities that should be classified as food deserts. A 2014 geographical survey found that the average distance from a grocery store was 1.76 kilometers (1.09 miles) in Edmonton, but only 1.44 kilometers (0.89 miles) when farmers' markets and community gardens were included, 0.11 miles under the latter definition for an urban food desert.|$|E
40|$|In recent years, {{distance}} education has developed very quickly in China. With rapid {{expansion in the}} <b>scale</b> <b>of</b> <b>distance</b> education in China, quality assurance has become a topical point of concern. The approaches and experiences of quality management of {{distance education}} institutions in other countries are different. The paper examines the similaries and differences between CCRTVU and OUUK, comparing external and internal quality assurance in the two distance education institutions. Through this comparative research, the paper concludes {{with a list of}} recommendation for quality assurance in CCRTVU...|$|E
40|$|Ss judged "ratios <b>of</b> <b>distances</b> " and "differences <b>of</b> <b>distances</b> " between {{pairs of}} U. S. cities. Results fit {{the theory that}} Ss used two {{comparison}} processes as instructed. A ratio <b>scale</b> <b>of</b> <b>distances</b> between cities was constructed from the 2 rank orders. From this scale, an interval <b>scale</b> <b>of</b> the city locations on an east-west continuum was derived. This scale agrees with the subtractive model fit to "ratios " and "differences " of easterliness and westerliness, and it also agrees with multidimensional <b>scaling</b> <b>of</b> judged <b>distances</b> between cities. These {{findings are consistent with}} the theory that Ss use subtraction when instructed to judge either "ratios " or "differences, " but that they can use both ratio and difference operations when the stimuli (in this case, distances) constitute a ratio scale on the subjective continuum. For a variety of continua, judgments of "ratios " and "differences" of subjective magnitudes appear to be monotonicall...|$|R
25|$|<b>Scaling</b> <b>of</b> <b>{{distances}}</b> (including sizes <b>of</b> bodies, {{while keeping}} the densities the same) gives similar orbits without scaling the time: if for example distances are halved, masses are divided by 8, gravitational forces by 16 and gravitational accelerations by 2. Hence velocities are halved and orbital periods remain the same. Similarly, when an object is dropped from a tower, {{the time it takes}} to fall to the ground remains the same with a <b>scale</b> model <b>of</b> the tower on a <b>scale</b> model <b>of</b> the Earth.|$|R
60|$|The great {{gun in the}} bow of {{the vessel}} was loaded with {{one of the largest and}} most {{powerful}} motor-bombs, and the spot to be aimed at was selected. This was a point in the water just inside of the mouth of the harbour, and nearly a mile from the land on either side. The <b>distance</b> <b>of</b> this point from the vessel being calculated, the cannon was adjusted at the angle called for by the <b>scale</b> <b>of</b> <b>distances</b> and levels, and the instrument indicating rise, fall, and direction was then put in connection with it.|$|R
40|$|Alabama Four {{topics in}} wind {{analysis}} are briefly discussed. Research endeavors {{to treat the}} vertical variation of the wind profile as an entity considering simultaneous occurrences of the wind vector are described. This new approach, the so-called characteristics method, exhibits advantages over other methods. Profiles with maximum speed values in the frequency distribution of height levels are analyzed and two major types are discussed. Preliminary {{results of the investigation}} of the wind shear parameters {{as a function of the}} <b>scale</b> <b>of</b> <b>distance</b> delineate the deficiency of deriving shear parameters from smooth radiosonde records. The final topic deals with the analysis of the turbulence param-eters, and the separation of the wind profile from missile flight recordings into stationary and nonstationary parts...|$|E
40|$|A flame can be {{regarded}} as a flowing reaction system in which the time scale in the usual reaction rate expressions is replaced by a <b>scale</b> <b>of</b> <b>distance</b> along the flame. Flames provide a convenient system in which to study the kinetics of high temperature gas reactions because wall reactions are absent and the relatively high linear velocities enables quite rapid reactions to be followed; e. g. radical recombination reactions at atmospheric pressure. The reactions to be discussed in this thesis have been studied using the techniques of flame photometry. From measurements of light emission or absorption as a function of flame temperature, composition and time from primary combustion, it is possible to make important deductions concerning the rates of elementary reactions in the burnt gas {{and the nature of the}} equilibria set-up among the components of the burnt gas...|$|E
40|$|Abstract: It is {{hypothesized}} that the beginning formation of bedrock which encapsulates the cooling silicate magmas happens during late brown dwarf early blue dwarf stages of stellar metamorphosis. Bedrock is known to consist of feldspars, quartz, micas {{and a variety of}} basalts which are very dense in composition. Bedrock is composed of mostly neutralized aluminum, silicon, oxygen, calcium, sodium, manganese, chromium, titanium, lithium, barium, rubidium, and iron to name a few. These materials are also known to consist of mostly frozen in magnetic fields which allow them to form very stable, hard crystalline structure which does not conduct electricity over any <b>scale</b> <b>of</b> <b>distance.</b> This is understood in the science of stelithology [1] and the theory of stellar metamorphosis [2][3][4] as being evidence of a star’s stages of cooling and transition from plasmas to gases, liquids and solids. This is also consistent with the differen...|$|E
40|$|Universal <b>scaling</b> <b>of</b> <b>distances</b> between {{vertices}} <b>of</b> Erdos-Renyi random graphs, scale-free Barabasi-Albert models, science collaboration networks, biological networks, Internet Autonomous Systems {{and public}} transport networks are observed. A mean distance between two nodes of degrees k_i and k_j equals to =A-B log(k_i k_j). The scaling is valid over several decades. A simple theory for the appearance <b>of</b> this <b>scaling</b> is presented. Parameters A and B {{depend on the}} mean value of a node degree _nn calculated for the nearest neighbors and on network clustering coefficients. Comment: 4 pages, 3 figures, 1 tabl...|$|R
40|$|Abstract — This paper {{presents}} {{a case study}} on the frequency reuse using Hierarchical Radio Resource Management (HRRM) in an OFDMA system. In this HRRM, an Access Point Controller is developed to dynamically assign subchannels to the Access Points (APs) {{on the basis of}} interference measurements and traffic situation, and the APs allocate resource elements to the Mobile Terminals (MTs) using an OFDMA scheduler. Using this HRRM we focus on the effect of frequency reuse in the considered system. Three types of reuse constraints with different <b>scaling</b> <b>of</b> <b>distances</b> between the APs are used to analyze frequency reuse effect...|$|R
40|$|This paper {{provides}} {{evidence that}} <b>scales</b> <b>of</b> perceptual <b>distance</b> between sounds {{play a central}} role in both markedness and faithfulness constraints. Perceptual distance governs the markedness of contrasts: contrasts between sounds are more marked the smaller the distance between the sounds (Flemming 2002, 2004). Correspondenc...|$|R
40|$|Summary. The three- point {{moment of}} the spatial {{distribution}} of shallow earthquakes is determined for one local and one world-wide catalogue. We compare the numbers of hypocentre triplets forming a particular triangle in a real catalogue and in a randomized (simulated Poissonian) catalogue, which has the same boundaries and depth distribution of hypocentres as the real catalogue. The ratio of these quantities seems fairly well approximated by the reciprocal of the surface area of the triangle. There is good evidence that this ratio is independent of {{the form of the}} triangle once the surface area is fixed. These results, similar to those from the two-point spatial moments, indicate lack of any intrinsic <b>scale</b> <b>of</b> <b>distance</b> or triples configuration for distances between hypocentres ranging from a few kilometres up to 1000 km. The results, together with similar ones from two- and four-point moment studies, place limits on the possible models of earthquake fault geometries. ...|$|E
40|$|Though teleradiology and PACS use similar hardware, {{they are}} often thought of as very {{different}} systems. A closer examination, however, shows that a teleradiology system is usually a small, specialized PAC system. In teleradiology, the archival medium may be film, but {{there is at least}} some temporary electronic storage. The communications may employ anything from video channels to satellite links and the <b>scale</b> <b>of</b> <b>distance</b> is usually greater than for a PACS, but PACS also use a variety of media. The PACS communications environment is more like a local area network (LAN) in scale as compared to the wide-area network (WAN) nature of teleradiolgy. The advantage in thinking of the related nature of these systems is that proper planning will facilitate the growth of teleracliology into a PACS, or the smooth addition of teleradiology capability to an existing PAC system. This paper will focus on the Georgetown experience which included one teleradiology application in its initial plan, and added uses in a later phase of the project. These projects proved highly successful, both in terms of clinical use and technical performance...|$|E
40|$|It is {{practically}} {{shown that a}} pair of neutrinos from tau decay can form a flavor entangled state. With this kind of state we show that the locality constrains imposed by Bell inequality are violated by the quantum mechanics, and an experimental test of this effect is feasible within the earth’s length scale. Theoretically, the quantum entanglement of neutrino pairs can be employed {{to the use of}} long distance cryptography distribution in a protocol similar to the BB 84. Entanglement represents the essence of quantum mechanics (QM) and is the source of a number of paradoxial and counterintuitive phenomena [1]. The kernel of entanglement is the non-local correlation exhibit by QM. In 1964, Bell derived out a group of inequalities [2], that is a set of constraints that local hidden variable theory (LHVT) must satisfy whereas QM violates. Many experiments in regard of the Bell inequalities have been carried out by using entangled photons [3]. Great progress was made in this direction by using PDC technique in generating the entangled photon pairs [4]. More importantly quantum information theory has been developed along with the research of entanglement. The same effort has also been made in high energy physics [5, 6], where the mixing properties of neutral mesons can play a practical role [7, 8, 9]. People also attempted to use the entangled state in high energy physics to implement quantum information tasks [10], i. e. quantum teleportation. Neutrinos are one class of elementary particles known as having only weak interaction with matter. This nature enables them to transmit over a large <b>scale</b> <b>of</b> <b>distance</b> without significant attenuation. For this reason, thirty years ago it was proposed to be as an idea...|$|E
2500|$|If the {{vertices}} of {{a framework}} {{are in a}} motion, then that motion may be described over small <b>scales</b> <b>of</b> <b>distance</b> by its gradient, a vector for each vertex specifying its speed and direction. The gradient describes a linearized approximation to the actual motion of the points, in which each point moves at constant velocity in a straight line. The gradient may {{be described as a}} row vector that has one real number coordinate for each pair [...] where [...] is a vertex of the framework and [...] is the index of one of the Cartesian coordinates of -dimensional space; that is, the dimension of the gradient {{is the same as the}} width of the rigidity matrix.|$|R
3000|$|A {{distance}} matrix {{represents the}} closeness of data objects and this matrix is a square matrix and its dimension is N × N where N is number of data objects. Diagonal elements such as e 11, e 22, …, e [...] NN [...] are always zero. The <b>scaling</b> <b>of</b> the <b>distance</b> matrix is carried out by dividing all elements <b>of</b> <b>distance</b> matrix by a scaling factor if required.|$|R
50|$|Zhang’s <b>Scale</b> <b>of</b> Transactional <b>Distance</b> (2003): By {{the turn}} <b>of</b> the millennium, <b>distance</b> {{education}} had evolved from being synonymous with correspondence courses to being largely web-based. Aixu (Monica) Zhang built on Michael G. Moore’s theory by proposing that transactional distance {{could be viewed}} as a measure of the student’s difficulty in becoming actively engaged with their online learning environment. She defined transactional distance by four sets of variables (the Transactional Distance between Students and Students (TDSS), the Transactional Distance between Students and the Teacher (TDST), the Transactional Distance between the Student and the Content (TDSC), and the Transactional Distance between the Student and the Instructional Technology (TDSI). She found that these four sets of variables were highly and statistically correlated with Student Satisfaction, which she adopted as her surrogate measure for Transactional Distance. Her work lead to a 31 element statistically reliable and significant <b>Scale</b> <b>of</b> Transactional <b>Distance.</b>|$|R
40|$|Background: Machine {{learning}} and data mining techniques {{have been successfully}} applied on MRI images for detecting Alzheimer's disease (AD). But only {{a few studies have}} explored the possibility of AD detection from non-image data. These studies have applied traditional data visualization and classification algorithms. There is a need for new sophisticated learning algorithms, for detecting and quantifying the severity of AD by exploring the complex interactions between the features in AD subjects. Method: In this work, a supervised learning model to effectively capture the complex feature interactions, in the sample space of AD data, is presented for knowledge discovery. The discovered knowledge is further used to quantify the similarity of a test subject to the demented class. Results: Evaluation of the proposed model, on OASIS database of Alzheimer's subjects, validates the well established risk factors and identifiers for AD: Age, Socio-Economic Status, MMSE Score and Whole Brain Volume. The Test subjects are affiliated to either non-demented (ND) or AD class, with non-overlapping and measurable similarity indices: Female ND (CDR= 0) [0. 48 â 2. 90], Female AD (CDR= 0. 5) [90. 16 â 774. 51], Female AD (CDR= 1) [1633. 90 â 7182. 23], Female AD (CDR= 2) [55258. 51 â 66382. 44], Male ND (CDR= 0) [0. 69 â 3. 66], Male AD (CDR= 0. 5) [99. 18 â 647. 51] and Male AD (CDR= 1) [3880. 16 â 6519. 40]. Conclusion: The outcome of the work clearly demonstrates that, supervised learning model can be used effectively to quantify the severity of AD on a standard measurable scale. This <b>scale</b> <b>of</b> <b>distance</b> {{can be used as a}} supplement for clinical dementia rating. Keywords: Alzheimer's disease, Dementia, Multifactor dimensionality reduction, Knowledge discovery, Similarity measure, Affiliation analysi...|$|E
40|$|Persistent Homology {{is a tool}} {{to analyze}} and {{visualize}} the shape of data from a topological viewpoint. It computes persistence, which summarizes the evolution of topological and geometric information about metric spaces over multiple <b>scales</b> <b>of</b> <b>distances.</b> While computing persistence is quite efficient for low-dimensional topological features, it becomes overwhelmingly expensive for medium to high-dimensional features. In this thesis, we attack this computational problem from several different angles. We present efficient techniques to approximate the persistence of metric spaces. Three of our methods are tailored towards general point clouds in Euclidean spaces. We make use of high dimensional lattice geometry {{to reduce the cost}} of the approximations. In particular, we discover several properties of the Permutahedral lattice, whose Voronoi cell is well-known for its combinatorial properties. The last method is suitable for point clouds with low intrinsic dimension, where we exploit the structural properties of the point set to tame the complexity. In some cases, we achieve a reduction in size complexity by trading off the quality of the approximation. Two of our methods work particularly well in conjunction with dimension-reduction techniques: we arrive at the first approximation schemes whose complexities are only polynomial {{in the size of the}} point cloud, and independent of the ambient dimension. On the other hand, we provide a lower bound result: we construct a point cloud that requires super-polynomial complexity for a high-quality approximation of the persistence. Together with our approximation schemes, we show that polynomial complexity is achievable for rough approximations, but impossible for sufficiently fine approximations. For some metric spaces, the intrinsic dimension is low in small neighborhoods of the input points, but much higher for large <b>scales</b> <b>of</b> <b>distances.</b> We develop a concept of local intrinsic dimension to capture this property. We also present several applications of this concept, including an approximation method for persistence. This thesis is written in English...|$|R
40|$|It is {{presented}} an all sky search for short time emission (STE) in 1 to 10 minutes time windows above 20 TeV within the acceptance {{window of the}} wide angle air Cerenkov detector AIROBICC, {{which is part of}} the HEGRA experiment at La Palma. Data from March 1992 to March 1993 were analysed, corresponding to the first year of operation of AIROBICC. 1 Introduction STE of gamma rays or cosmic rays is expected from a wide variety of objects at different <b>scales</b> <b>of</b> <b>distances,</b> covering from nearby Galactic objects as pulsars and supernovas to cosmological distances as quasars and primordial black holes (PBH). Specially interesting objects are the gamma ray bursts (GRB) whose origin remains unclear although the cosmological hypothesis is the most accepted scenario [1]. We chose a time <b>scale</b> <b>of</b> minutes because it is sensitive to most interesting violent features of these objects while keeping enough statistics to apply classical methods of search. A search with the same detector for STE from PBH [...] ...|$|R
40|$|This paper {{examines}} how this mega-university offers increasing access to cost-effective, equitable and flexible higher education by analyzing data from {{primary and secondary}} sources, identifies challenges impacting the continued growth <b>of</b> enrollment in <b>distance</b> education, and outlines opportunities for increasing {{access to higher education}} through <b>scaling</b> <b>of</b> <b>distance</b> initiatives. For pedagogic delivery, BOU uses both the conventional face-to-face tutorial system based on print module and electronic learning technologies such as CD, audiovisual cassettes, and radio and TV broadcasts. It revealed that BOU education is flexible, cost-effective, and insensitive to gender and geography and of standard comparable to that of the conventional universities. The reasons behind the success and cost-effectiveness of BOU programs were; BOU has access to any government and non-government infrastructures and resources without any cost or with nominal costs and can engage specialists from any institution for tutoring with a small honorarium...|$|R
40|$|Four line pilots {{performed}} simulator {{flights of}} 747 s on different air routes in conditions of following other aircraft {{while maintaining a}} specified temporal flight interval separation. The flights {{were made in the}} heading-select mode of the autopilot, and the pitch wheel or altitude select/hold mode with the throttle on manual. A cockpit display of traffic information (CDTI) on a CRT was presented in front of the throttles. A VOR radial with dotted centerlines and sidebands representing 3 km intervals was displayed, along with <b>scales</b> <b>of</b> <b>distance</b> depending on altitude. Traffic information was updated every four seconds. Simulated flights began at cruise and followed a standard profile descent, and traffic following intervals of 60, 80, 120, and 140 seconds were presented in different trials. Results indicated that a CDTI was sufficient instrumentation for trailing other aircraft, with low error rates up to the point of landing preparations...|$|R
500|$|Geometry was {{essential}} to surveying and cartography. The earliest extant Chinese maps date to the 4th century BCE, yet {{it was not until}} the time of Pei Xiu (224–271) that topographical elevation, a formal rectangular grid system, and use of a standard graduated <b>scale</b> <b>of</b> <b>distances</b> was applied to terrain maps. Following a long tradition, Shen Kuo created a raised-relief map, while his other maps featured a uniform graduated <b>scale</b> <b>of</b> 1:900,000. A [...] squared map of 1137—carved into a stone block—followed a uniform grid <b>scale</b> <b>of</b> 100 li for each gridded square, and accurately mapped the outline of the coasts and river systems of China, extending all the way to India. Furthermore, the world's oldest known terrain map in printed form comes from the edited encyclopedia of Yang Jia in 1155, which displayed western China without the formal grid system that was characteristic of more professionally made Chinese maps. Although gazetteers had existed since 52 CE during the Han dynasty and gazetteers accompanied by illustrative maps (Chinese: tujing) since the Sui dynasty, the illustrated gazetteer became much more common in the Song dynasty, when the foremost concern was for illustrative gazetteers to serve political, administrative, and military purposes.|$|R
40|$|It {{is shown}} that small quantum {{effects of the}} model of {{low-energy}} quantum gravity by the author give a possibility of another interpretation of cosmological observations without {{an expansion of the}} Universe and dark energy. Twenty years ago it was difficult to suppose that cosmology may dictate fundamental tasks for other fields of physics, but now many people are searching for dark matter and dark energy or are inventing new scenarios of inflation. They admit that known matter amounts only about five per cent of a full matter content of the Universe, and that the Big Bang is a true event. But it is easy to note that deep roots of the problem lie in the assumed nature of redshifts of remote objects, such as galaxies. All other top-hampers of the current standard cosmological model- the Big Bang, inflation, dark matter and dark energy- are based on this assumption and on the hypothesis about the validity of general relativity on an arbitrary big <b>scale</b> <b>of</b> <b>distances.</b> Thes...|$|R
40|$|Advances in {{microscopy}} and genomic {{techniques have}} provided new insight into spatial chromatin organization {{inside of the}} nucleus. In particular, chromosome conformation capture data has highlighted the relevance of polymer physics for high-order chromatin organization. In this context, we review basic polymer states, discuss how an appropriate polymer model can be determined from experimental data, and examine the success and limitations of various polymer models of higher-order interphase chromatin organization. By taking into account topological constraints acting on the chromatin fiber, recently developed polymer models of interphase chromatin can reproduce the observed <b>scaling</b> <b>of</b> <b>distances</b> between genomic loci, chromosomal territories, and probabilities of contacts between loci measured by chromosome conformation capture methods. Polymer models {{provide a framework for}} the interpretation of experimental data as ensembles of conformations rather than collections of loops, and will be crucial for untangling functional implications of chromosomal organization. National Cancer Institute (U. S.). Physical Sciences-Oncology Center (MIT, (U 54 CA 143874) ...|$|R
40|$|In {{the last}} decade a major debate has emerged on the {{astrophysics}} community concerning the anomalous behaviour of the astronomical unit, the fundamental <b>scale</b> <b>of</b> <b>distances</b> in the Solar system. Several independent studies have combined radar ranging and optical data from {{the last four decades}} to {{come to the conclusion that}} the astronomical unit is increasing by several meters per century. It is abundantly clear that General Relativity cannot account for this new effect, although an still undefined angular momentum transfer mechanism could provide the simpler and more conventional explanation. Here we investigate several anomalous post-newtonian terms containing only the product of the mass and angular momentum of the Sun as well as its Schwarzschild radius in order to determine if they could explain the secular increase of the astronomical unit and the recently reported increase of Lunar eccentricity. If these anomalies are confirmed, searching for a modification of General Relativity predicting these terms could have far-reaching consequences...|$|R
50|$|The Imperium {{commands}} the largest {{military in the}} galaxy, honed in millennia of almost unending war (the prevalent theme of the Warhammer 40,000 universe): at any time, there are numerous conflicts engaging the Imperium, across the Imperial space and beyond. Because <b>of</b> the <b>scale</b> <b>of</b> the <b>distances</b> involved, and the number and severity of threats, Imperial military commanders have great autonomy into how and when they prosecute campaigns within their areas of responsibility.|$|R
5000|$|Usually, {{dimension}} is defined {{based on the}} <b>scaling</b> exponent <b>of</b> some property in the appropriate limit. One property one could use [...] is the <b>scaling</b> <b>of</b> volume with <b>distance.</b> For regular lattices [...] the number of nodes [...] within a <b>distance</b> [...] <b>of</b> node [...] <b>scales</b> as [...]|$|R
40|$|This paper aims {{to answer}} whether an ethnic {{distance}} in young children, age 11 - 12 (third and fourth year of primary school) s, {{as well as}} if their parents can be determined {{and what are the}} levels <b>of</b> that <b>distance.</b> Main techniques used were Bogardus` <b>scale</b> <b>of</b> social <b>distance</b> (somewhat modified for the children) and a questionnaire for the parents dealing with some aspects of knowledge about the culture. Results show that levels <b>of</b> ethnic <b>distance</b> are much higher in children, but also that this problem can be dealt with and in that spirit some directions on how to prevent the development <b>of</b> this <b>distance</b> were also given...|$|R
