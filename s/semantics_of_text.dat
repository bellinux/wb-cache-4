20|10000|Public
5000|$|W3C forum has {{undertaken}} {{the initiative of}} standardizing the ontology representation for web-based applications. The Web Ontology Language (OWL), standardized in 2004 after maturing through XML(S), RDF(S) and DAML+OIL {{is a result of}} that effort. Ontology in OWL (and some of its predecessor languages) has been successfully used in establishing <b>semantics</b> <b>of</b> <b>text</b> in specific application contexts.|$|E
40|$|Abstract. Clustering text data streams is an {{important}} issue in data mining community and has a number of applications such as news group filtering, text crawling, document organization and topic detection and tracing etc. However, most methods are similarity-based approaches and use the TF*IDF scheme to represent the <b>semantics</b> <b>of</b> <b>text</b> data and often lead to poor clustering quality. In this paper, we firstly give an improved semantic smoothing model for text data stream environment. Then we use the improved semantic model to improve the clustering quality and present an online clustering algorithm for clustering massive text data streams. In our algorithm, a new cluster statistics structure, cluster profile, is presented in which the <b>semantics</b> <b>of</b> <b>text</b> data streams are captured. We also present the experimental results illustrating the effectiveness of our technique...|$|E
40|$|In this paper, {{a formal}} {{semantic}} framework is developed {{in order to}} account for the temporal <b>semantics</b> <b>of</b> <b>text.</b> The theory is able to represent and reason about both semantic issues, which are independent of world knowledge (wk), and pragmatic issues, which are not, within a single logical framework. The theory will allow a text's semantic entailments to differ from its pragmatic ones, even though they are all derived within the same logic. I demonstrate that this feature of the theory gives rise to solutions to several puzzles concerning the temporal structure of text. 1 The Problem The {{purpose of this paper is}} to provide a formal account of the temporal <b>semantics</b> <b>of</b> <b>text.</b> The chief goal is to explain when a text is temporally coherent: it should not mislead the reader as to the order of the events reported. If John hits Max, causing Max to turn round (to face John), then text (1) reflects this while (2) distorts it: (1) John hit Max. Max turned round. (2) Max turned roun [...] ...|$|E
40|$|In {{this paper}} we are {{concerned}} with the special requirements that a <b>semantics</b> <b>of</b> <b>texts</b> should meet. It is argued that a <b>semantics</b> <b>of</b> <b>texts</b> should be incremental and should satisfy the break in principle. We develop a semantics for propositional texts that satisfies these constraints. We will see that our requirements do not only apply to the semantics but also have consequences for the syntax. The interaction between text structure and text meaning {{will turn out to be}} of crucial importance to the <b>semantics</b> <b>of</b> <b>texts.</b> We develop two versions of the semantics: one representational, one in update style. 1 Introduction Traditionally in formal semantics the attention has been focussed on the interpretation of sentences. But since it was argued 1 that the <b>semantics</b> <b>of</b> <b>texts</b> requires more than a straightforward extension of the techniques developed for sentences, text semantics has become a respectable topic of research. It is now quite generally recognised that special tools have to be dev [...] ...|$|R
40|$|Abstract. This paper {{discusses}} the semi-formal language {{of mathematics and}} presents the Naproche CNL, a controlled natural language for mathematical authoring. Proof Representation Structures, an adaptation of Discourse Representation Structures, are used to represent the <b>semantics</b> <b>of</b> <b>texts</b> written in the Naproche CNL. We discuss how the Naproche CNL {{can be used in}} formal mathematics, and present our prototypical Naproche system, a computer program for parsing texts in the Naproche CNL and checking the proofs in them for logical correctness...|$|R
5000|$|The PHP syntax and {{semantics}} are {{the format}} (syntax) andthe related meanings (<b>semantics)</b> <b>of</b> the <b>text</b> and symbols in the PHP programming language. They form {{a set of}} rules that define how a PHP program can be written and interpreted.|$|R
40|$|In this paper, a {{new model}} of {{representing}} semantics called the concept relational tree model is proposed. The model adapts the architecture of expression tree in providing a hierarchical organization to semantics. It is motivated by an approach known as the discourse structure tree that organizes semantics based on preposition and relies on rhetorical relations to define the connection between the prepositions. Comparatively, the concept relational tree uses concept and relation as its organizational unit instead. This allows the <b>semantics</b> <b>of</b> <b>text</b> to be defined at a finer level, which consequently enables a more flexible way of controlling the structure of semantics. Key words: Semantics, Parsing, Relation Extractio...|$|E
40|$|Human {{languages}} are naturally ambiguous, {{which makes it}} difficult to automatically understand the <b>semantics</b> <b>of</b> <b>text.</b> Most vector space models (VSM) treat all occurrences of a word as the same and build a single vector to represent the meaning of a word, which fails to capture any ambiguity. We present sense-aware semantic analysis (SaSA), a multi-prototype VSM for word representation based on Wikipedia, which could account for homonymy and polysemy. The “sense-specific ” prototypes of a word are produced by clustering Wikipedia pages based on both local and global contexts of the word in Wikipedia. Experimental evaluation on semantic relatedness for both isolated words and words in sentential contexts and word sense induction demonstrate its effectiveness...|$|E
40|$|In {{this paper}} we review the main {{intermediate}} forms proposed in text mining, and we briefly study some fuzzy counterparts. The concept of intermediate form applies to any knowledge representation employed to represent in a structured way the semantic {{content of a}} text corpus. Intermediate forms {{play a central role}} in the text mining process since it is necessary to transform plain text into a form in order to apply mining techniques. Since the <b>semantics</b> <b>of</b> <b>text</b> use to be imprecise, the use of fuzzy intermediate forms seems to be a natural solution in many cases. We discuss about fuzzy intermediate forms and the corresponding fuzzy text mining techniques that may be applicable on them...|$|E
30|$|The {{semantic}} meaning <b>of</b> <b>texts</b> {{is critical}} for correctly identifying sensitive data. To understand the <b>semantics</b> <b>of</b> a <b>text,</b> we need NLP techniques to process it. With advance of existing NLP techniques, the grammatical structure of a natural language sentence can be parsed accurately. We next briefly introduce the key NLP techniques used in our work.|$|R
50|$|A {{number of}} words have uneven {{distribution}} in the MT of the Hebrew Bible, if the indicators above (and internal evidence from the <b>semantics</b> <b>of</b> the <b>texts)</b> are used to identify which portions may have Israelite provenance. In many cases, these words are also attested in the languages of ancient Israel's northern neighbours, like Ugarit, Phoenicia and Aram.|$|R
50|$|A code browser is an editor, {{sometimes}} with folding or other advanced layout capabilities, designed to structure source code or, by extension, other kinds <b>of</b> <b>text</b> file. Since it is typically {{aware of the}} syntax (and, to some extent, the <b>semantics)</b> <b>of</b> the <b>text</b> it is displaying, {{it is able to}} use various techniques to make navigation and cross-referencing faster and easier; this allows it to present a good overview of the code of large projects.|$|R
40|$|Text {{clustering}} can {{be considered}} as a four step process consisting of feature extraction, text representation, document clustering and cluster interpretation. Most text clustering models consider text as an unordered collection of words. However the <b>semantics</b> <b>of</b> <b>text</b> would be better captured if word sequences are taken into account. In this paper we propose a sequence based text clustering model where four novel sequence based components are introduced {{in each of the four}} steps in the text clustering process. Experiments conducted on the Reuters dataset and Sydney Morning Herald (SMH) news archives demonstrate the advantage of the proposed sequence based model, in terms of capturing context with semantics, accuracy and speed, compared to clustering of documents based on single words and n-gram based models...|$|E
40|$|Abstract. In {{this work}} we present an {{approach}} to capture the total semantics in multimedia-multimodal web pages. Our research improves upon the state-of-the-art with two key features: (1) capturing the <b>semantics</b> <b>of</b> <b>text</b> and image-based media for static and dynamic web content; and (2) recognizing that information goals are defined by emergent user behavior and not statically declared by web design alone. Given a user session, the proposed method accurately predicts user information goals and presents them as a list of most relevant words and images. Conversely, given a set of information goals, the technique predicts possible user navigation patterns as network flow with a semantically-derived flow distribution. In the latter case, differences between predicted optimal and observed user navigation patterns highlight points of suboptimal website design. We compare this approach to other content-based techniques for modeling web-usage and demonstrate its effectiveness...|$|E
40|$|Compositional Distributional Semantics Models (CDSMs) are {{traditionally}} {{seen as an}} entire different world with respect to Tree Kernels (TKs). In this paper, we show that under a suitable regime these two approaches {{can be regarded as}} the same and, thus, structural information and distributional semantics can successfully cooperate in CSDMs for NLP tasks. Leveraging on distributed trees, we present a novel class of CDSMs that encode both structure and distributional meaning: the distributed smoothed trees (DSTs). By using DSTs to compute the similarity among sentences, we implicitly define the distributed smoothed tree kernels (DSTKs). Experiment with our DSTs show that DSTKs approximate the corresponding smoothed tree kernels (STKs). Thus, DSTs encode both structural and distributional <b>semantics</b> <b>of</b> <b>text</b> fragments as STKs do. Experiments on RTE and STS show that distributional semantics encoded in DSTKs increase performance over structure-only kernels...|$|E
50|$|Semantic folding theory {{describes}} a procedure for encoding the <b>semantics</b> <b>of</b> natural language <b>text</b> in a semantically grounded binary representation. This approach {{provides a framework}} for modelling how language data is processed by the neocortex.|$|R
40|$|In {{the school}} of basic {{education}} the presence of classic literature becomes necessary to endow the pupil with readings that substantiate and disclose the some possibilities of apprehension of the culture socially accumulated by the humanity {{since the advent of}} the writing, as well as, become necessary that the development of a good reader occurs. For this, to read the classics can consist in an argument when we think about the <b>semantics</b> <b>of</b> <b>texts</b> and contexts and the critical richness of Literature while first in rank object of cultural manifestation. Thinking about this, the article brings contributions to think a project that aims at in the basic education the development of the reading child, from the workmanships of fiction of Literature for children and the narrative considering myths, legends, stories, chronicles and fabulist until arriving in the romances, contrasting with the media contemporary...|$|R
40|$|Most <b>of</b> <b>text</b> mining {{techniques}} {{are based on}} word and/or phrase analysis <b>of</b> the <b>text.</b> The statistical analysis of a term (word or phrase) frequency captures {{the importance of the}} term within a document. However, to achieve a more accu-rate analysis, the underlying mining technique should indi-cate terms that capture the <b>semantics</b> <b>of</b> the <b>text</b> from which the importance of a term in a sentence and in the document can be derived. A new concept-based mining model that re-lies on the analysis of both the sentence and the document, rather than, the traditional analysis of the document dataset only is introduced. The proposed mining model consists of a concept-based analysis of terms and a concept-based similarity measure. The term which contributes to the sentence semantics is an-alyzed with respect to its importance at the sentence and document levels. The model can efficiently find significant matching terms, either words or phrases, of the documents according to the <b>semantics</b> <b>of</b> the <b>text.</b> The similarity be-tween documents relies on a new concept-based similarity measure which is applied to the matching terms between documents. Experiments using the proposed concept-based term analysis and similarity measure in text clustering are con-ducted. Experimental results demonstrate that the newly developed concept-based mining model enhances the clus-tering quality of sets of documents substantially...|$|R
40|$|The {{state-of-the-art}} {{solutions to}} the vocabulary mismatch in information retrieval (IR) mainly aim at leveraging either the relational semantics provided by external resources or the distributional semantics, recently investigated by deep neural approaches. Guided by the intuition that the relational semantics might improve the effectiveness of deep neural approaches, we propose the Deep Semantic Resource Inference Model (DSRIM) that relies on: 1) a representation of raw-data that models the relational <b>semantics</b> <b>of</b> <b>text</b> by jointly considering objects and relations expressed in a knowledge resource, and 2) an end-to-end neural architecture that learns the query-document relevance by leveraging the distributional and relational semantics of documents and queries. The experimental evaluation carried out on two TREC datasets from TREC Terabyte and TREC CDS tracks relying respectively on WordNet and MeSH resources, indicates that our model outperforms state-of-the-art semantic and deep neural IR models...|$|E
40|$|Text {{normalization}} transforms {{words into}} a base form so that terms from common equivalent classes match. Traditionally, information retrieval systems employ stemming techniques to remove derivational affixes. Depluralization, {{the transformation of}} plurals into singular forms, is also used as a low-level text normalization technique to preserve more precise lexical <b>semantics</b> <b>of</b> <b>text.</b> Experiment {{results suggest that the}} choice of text normalization technique should be made individually on each topic to enhance information retrieval accuracy. This paper proposes a hybrid approach, constructing a query-based selection model to select the appropriate text normalization technique (stemming, depluralization, or not doing any text normalization). The selection model utilized ambiguity properties extracted from queries to train a composite of Support Vector Regression (SVR) models to predict a text normalization technique that yields the highest Mean Average Precision (MAP). Based on our study, such a selection model holds promise in improving retrieval accuracy. ...|$|E
40|$|Although using ontologies {{to assist}} {{information}} retrieval and text document processing has recently attracted {{more and more}} attention, existing ontologybased approaches have not shown advantages over the traditional keywords-based Latent Semantic Indexing (LSI) method. This paper proposes an algorithm to extract a concept forest (CF) from a document {{with the assistance of}} a natural language ontology, the WordNet lexical database. Using concept forests to represent the <b>semantics</b> <b>of</b> <b>text</b> documents, the semantic similarities of these documents are then measured as the commonalities of their concept forests. Performance studies of text document clustering based on different document similarity measurement methods show that the CF-based similarity measurement is an effective alternative to the existing keywords-based methods. In particular, this CFbased approach has obvious advantages over the existing keywords-based methods, including LSI, in processing text abstracts or in P 2 P environments where it is impractical to collect the entire document corpus for analysis. 1...|$|E
40|$|This paper {{addresses}} the semantic issue of exceptions in natural language texts. After observing that first-order logic is not adequate for semantic representations of natural language, we {{make use of}} extended logic programs for meaning representation and inference engine in natural language processing. We show how to generate <b>semantics</b> <b>of</b> <b>texts</b> through deductive parsing by using definite clause grammars with focus on meanings of exceptions. Given a textual database, its semantics can be produced as an extended logic program, and the query against the textual database is thus reduced to a query in an extended logic program. The non-monotonicity of extended logic programs makes our method to be especially applicable to incomplete textual databases, {{but it is also}} generally applicable to the analysis of natural language descriptions in view of conceptual modelling and to natural language interfaces for database querying. Generally speaking, it also provides a method for conflict resoluti [...] ...|$|R
40|$|This paper {{presents}} an algorithm for aligning FrameNet lexical units to WordNet synsets. Both, FrameNet and WordNet, are well-known {{as well as}} widely-used resources by the entire research community. They help systems in the comprehension <b>of</b> the <b>semantics</b> <b>of</b> <b>texts,</b> and therefore, finding strategies to link FrameNet and WordNet involves challenges related {{to a better understanding}} of the human language. Such deep analysis is exploited by researchers to improve the performance of their applications. The alignment is achieved by exploiting the particular characteristics of each lexical-semantic resource, with special emphasis on the explicit, formal semantic relations in each. Semantic neighborhoods are computed for each alignment of lemmas, and the algorithm calculates correlation scores by comparing such neighborhoods. The results suggest that the proposed algorithm is appropriate for aligning the FrameNet and WordNet hierarchies. Furthermore, the algorithm can aid research on increasing the coverage of FrameNet, building FrameNets in other languages, and creating a system for querying a joint FrameNet-WordNet hierarchy. 1...|$|R
5000|$|Semantics is {{that part}} of {{linguistics}} that studies the meaning <b>of</b> words (lexical <b>semantics),</b> <b>of</b> the sets of words, <b>of</b> phrases (phrasal <b>semantics),</b> and <b>of</b> <b>texts.</b> Metasemantics, in the sense given by the Maraini, goes beyond the meaning of words and consists of the use, within the <b>text,</b> <b>of</b> words without Referent, but by the familiar sound to the language to which the text itself belongs, and which must still follow the syntactical and grammatical rules (in the case of Fosco Maraini, the Italian language). One can attribute more or less arbitrary meanings to these words by their sound and their position within the text [...]|$|R
30|$|While we {{have seen}} {{tremendous}} efforts in the past years to standardize and link lexical taxonomies and ontologies 2, {{there has not been}} a widespread use of such structured resources for the formal representation of the <b>semantics</b> <b>of</b> <b>text.</b> We attribute this to their excessive size and their author-centricity as outlined above, as well as to the lack of information for being able to assign their concepts to respective terms in unstructured text: just because e.g. all viruses are known in a database, it does not follow from this that it is possible to find their occurrences in text (e.g. because of ambiguous abbreviations, short forms and variants, idiosynchracies of the subfield etc.). Here, we propose a radical break with this traditional way of knowledge representation: instead, users should be able to choose their own set of categories per given task or problem, and thus should be able to grow their own local ontology without the need (but eventually with the possibility) of connecting it to existing upper ontologies, and users should ground their conceptualization in the respective texts of their current interest.|$|E
40|$|Seeking {{to enrich}} the search {{experience}} by allowing for extra time and alternate resources. words. ” The resulting fast, word-orient-ed matching ignores the rich <b>semantics</b> <b>of</b> <b>text</b> but is {{an efficient way to}} capture some aspects of the similarity between queries and documents. Time-saving mechanisms such as search-result caching and index tiering are also heavily exploited, despite the risk that such approaches may cause relevant content to be missed. Not All Searches Need to Be Fast Although searchers have grown ac-customed to rapid responses to their queries, recent advances in our under-standing of how people search suggest there are scenarios where a search en-gine could take significantly longer than {{a fraction of a second}} to return rel-evant content. 12 While someone search-W E L I V E I N a world where the pace of everything from communication to transportation is getting faster. In re-cent years a number of “slow move-ments ” have emerged that advocate for reducing speed in exchange for increasing quality. These include the slow food movement, slow parenting, slow travel, and even slow science. Building on these movements we pro-pose the concept of slow search, where search engines use additional time to provide a higher quality search expe-rience than is possible given conven-tional time constraints...|$|E
40|$|Most of the {{frequent}} techniques in text mining arebased on the arithmetic scrutiny of a idiom, either word orslogan. Arithmetical scrutiny of a term incidence captures theconsequence {{of the term}} within a manuscript only. However, twoprovisos can have the similar regularity in their documents, butone term contributes more to the connotation of its sentencesthan the further term. Thus, the essential text mining moldshould designate terms that incarcerate the <b>semantics</b> <b>of</b> <b>text.</b> Inthis case, the mining replica can incarcerate terms that currentthe concepts of the condemnation, which leads to innovation ofthe topic of the document. A novel concept-based mining replicathat analyzes terms on the condemnation, document, and corpuslevels is introduced. The concept-based mining replica canefficiently distinguish between non imperative terms with esteemto sentence semantics and terms which hold the concepts thatsymbolize the sentence connotation. The proposed mining replicaconsists of sentence-based impression scrutiny, document-basedperception analysis, corpus-based concept-analysis, and conceptbasedresemblance determine. The term which contributes to thecondemnation semantics is analyzed on the condemnation,document, and quantity levels relatively than the conventionalinvestigation of the manuscript only. The projected replica can proficiently find considerable toningconcepts between documents, according to the semantics of theirsentences. The comparison between documents is premeditatedbased on a new concept-based comparison assess. The proposedcorrespondence compute takes full improvement of using theperception investigation procedures on the judgment, document,and quantity levels in manipulative the comparison betweendocuments. Large sets of experiments using the projectedconcept-based mining replica on unusual data sets in textclustering are conducted. The experiments express generalcontrast between the concept-based investigation and thehabitual analysis. Tentative consequences reveal theconsiderable enrichment of the clustering superiority using thesentence-based, document-based, corpus-based, and united loomconcept analysis...|$|E
40|$|This paper {{addresses}} {{two different}} problems: (i) {{the meaning of}} Imperfeito in itself, i. e., what could be concluded from the individual analyses {{that made up the}} study, and (ii) the problems posed by a feature-based classification method for the <b>semantics</b> <b>of</b> real <b>text.</b> After {{a brief description of the}} study in Section 2, I devote Section 3 to the questions of classification, and Section 4 to the linguistic results concerning Imperfeito proper. In Section 5, I discuss the general description of a tense...|$|R
50|$|The <b>semantics</b> <b>of</b> what a <b>text</b> graph's {{nodes and}} edges {{represent}} can vary widely. Nodes for example can simply connect to tokenized words, or to domain-specific terms, or to entities {{mentioned in the}} text. The edges, on the other hand, can be between these text-based tokens or they can also link to a knowledge base.|$|R
40|$|Abstract: In {{order to}} reach the {{semantic}} Web, approaches to automatically extract semantic annotations from textual documents have been proposed. In this paper we propose an approach to automatically extract annotations by taking into account context {{in order to obtain}} a better representation of the document content. Our context is modelled by contextual relations built up from both the structure and the <b>semantics</b> <b>of</b> the <b>text.</b> Our approach requires text documents and a domain ontology as input. It automatically generates a set of contextual semantic annotations represented in RDF...|$|R
40|$|Most of {{the common}} {{techniques}} in text mining {{are based on the}} statistical analysis of a term, either word or phrase. Statistical analysis of a term frequency captures the importance of the term within a document only. However, two terms can have the same frequency in their documents, but one term contributes more to the meaning of its sentences than the other term. Thus, the underlying text mining model should indicate terms that capture the <b>semantics</b> <b>of</b> <b>text.</b> In this case, the mining model can capture terms that present the concepts of the sentence, which leads to discovery of the topic of the document. Now a day’s all the information’s are available with clear diagrammatic explanation or with related images. An image examination method can automate the recognition of landmarks and events in large image collections, significantly getting better Content utilization experience. The wide adoption of photo sharing applications and the enormous amounts of user-generated content uploaded to them raises an information overload issue for users. The concept-based mining model can effectively discriminate between non important terms with respect to sentence semantics and terms which hold the concepts that represent the sentence meaning. The proposed mining model consists of sentence-based concept analysis, document-based concept analysis, corpus-based concept-analysis, and concept-based similarity measure. The term which contributes to the sentence semantics is analyzed on the sentence, document, and corpus levels rather than the traditional analysis of the document only. An Automated Content Organization technique to defeat such an overload is to collect images into groups based on their similarity and then use the derived clusters to support navigation and browsing of the collection. In this paper, we present a community detection (i. e. graph-based clustering) approach that makes use of both visual and tagging features of images in order to efficiently extract groups of correlated images within large image collections. We perform clustering on such image similarity graphs by means of community detection, a process that identifies on the graph groups of nodes that are more closely associated to each other. We categorize the resultant image clusters as landmarks or events by use of features related to the temporal, community, and label characteristics of image clusters...|$|E
40|$|Ambiguity is {{inherent}} to human language. In particular, word sense ambiguity is prevalent in all natural languages, {{with a large}} number of the words in any given language carrying more than one meaning. Word sense disambiguation is the task of automatically assigning the most appropriate meaning to a polysemous word within a given context. Generally the problem of resolving ambiguity in literature has revolved around the famous quote “you shall know {{the meaning of the word}} by the company it keeps. ” In this thesis, we investigate the role of context for resolving ambiguity through three different approaches. Instead of using a predefined monolingual sense inventory such as WordNet, we use a language-independent framework where the word senses and sense-tagged data are derived automatically from Wikipedia. Using Wikipedia as a source of sense-annotations provides the much needed solution for knowledge acquisition bottleneck. In order to evaluate the viability of Wikipedia based sense-annotations, we cast the task of disambiguating polysemous nouns as a monolingual classification task and experimented on lexical samples from four different languages (viz. English, German, Italian and Spanish). The experiments confirm that the Wikipedia based sense annotations are reliable and can be used to construct accurate monolingual sense classifiers. It is a long belief that exploiting multiple languages helps in building accurate word sense disambiguation systems. Subsequently, we developed two approaches that recast the task of disambiguating polysemous nouns as a multilingual classification task. The first approach for multilingual word sense disambiguation attempts to effectively use a machine translation system to leverage two relevant multilingual aspects of the <b>semantics</b> <b>of</b> <b>text.</b> First, the various senses of a target word may be translated into different words, which constitute unique, yet highly salient signal that effectively expand the target word’s feature space. Second, the translated context words themselves embed co-occurrence information that a translation engine gathers from very large parallel corpora. The second approach for multlingual word sense disambiguation attempts to reduce the reliance on the machine translation system during training by using the multilingual knowledge available in Wikipedia through its interlingual links. Finally, the experiments on a lexical sample from four different languages confirm that the multilingual systems perform better than the monolingual system and significantly improve the disambiguation accuracy...|$|E
40|$|Exponential {{growth rates}} of {{learning}} materials and rapid distribution of those resources among e-learners via Internet have made it nearly infeasible to manually review each document and categorize it. In addition, the ability to classify objects into groups is of high importance in many applications like text retrieval, query search and learning recommender systems. This emerging need has re-emphasized the importance of automatic text classification systems in enabling us to classify semi-structured and unstructured text documents into predefined labeled groups. Specifically, if we can categorize learning resources based on their content (and not just based on subject topic or user declaration), this can help recommender systems and search engines to automatically build repositories of relevant documents and present to the users the most relevant ones based on query and/or user preferences. Text classification poses many challenges for learning systems which now deal with huge numbers of texts of highly variable length, structure and content. The representing features should be carefully selected to capture the important <b>semantics</b> <b>of</b> <b>text</b> to yield an acceptable classification performance while keeping the computational cost within a practically reasonable range. The features must also be useful across {{a wide range of}} class definitions. In this study, we investigate the applicability of text mining algorithms for categorizing different text-based educational resources into curriculum defined learning objectives. To do this, a variant of Term Frequency Inverse Document Frequency (TF-IDF) feature selection method along with a majority-voting-based classification system comprised of five different classical classifiers will be utilized. Three different knowledge domains with 65 learning objectives in total will be used in the experiments to evaluate the performance of the system. We will also study the effects of varying the number of features per each label on system performance. To deal with the rapid dimensionality rise of the feature vector as the system is being extended (which introduces more computational burden and tends to limit the capacity of the system to scale up), we will propose a hierarchical multitier classification architecture that can outperform single-layer single-node classification system in terms of computational cost and scalability. A simple version of this scheme will be implemented and analyzed. We will experimentally show that this architecture needs less number of features per label in comparison to the single node classification system. Despite other advantages like easier scalability, and lower maintenance cost, this multi-layer architecture could suffer from higher initial setup cost...|$|E
5000|$|Peyman is {{currently}} the founder of RobustLinks, funded by the National Science Foundation. The mission of RobustLinks is to use {{state of the art}} AI, NLP, and ML to compose Knowledge as a Service (KaaS). KaaS takes unstructured (textual) [...] "big data" [...] and reduces it to searchable knowledge. It integrates end-to-end technology stack that continuously gather, summarize and represent the <b>semantics</b> <b>of</b> unstructured <b>text,</b> in real-time and at scale. The derived semantics is then analyzed to condense the big data to a compact and searchable semantic graph.|$|R
40|$|Abstract: Most of {{the common}} {{techniques}} <b>of</b> <b>text</b> mining {{are based on the}} statistical analysis of the term frequency. The statistical analysis of the term frequency captures the importance of the term within the document only. An alternate approach would be to enhance the mining model to include the contribution of the term to the <b>semantics</b> <b>of</b> the <b>text</b> so that the terms that capture the concepts of the document and thereby the similarity between the documents may be found. The contribution of each term to the semantics at the sentence, document and corpus levels is determined using sentence-based concept-analysis, document-based concept-analysis and corpus-based concept-analysis respectively. A concept-based similarity measure is used to determine the similarity between the documents...|$|R
40|$|This paper {{presents}} a linguistic and discursive analysis of Getulio Vargas’ Testament Letter, focusing on aspects of genericity, compositional configurations and the <b>semantics</b> <b>of</b> the <b>text.</b> The theoretical-methodological support resides on traditional gender studies and textual linguistics, closely approaching the textual analysis of discourse. More specifically, the analysis seeks {{to describe and}} interpret the double genericity <b>of</b> the <b>text,</b> ingrained in the traditional nature of such document, the text plan, with a more detailed view of the opening stages and the causal explanation for the suicide, as well as semantic dimension aspects related to the discursive representation of the people component, {{in contrast to the}} statesman’s figurations and the political opposition...|$|R
