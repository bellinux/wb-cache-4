17|8|Public
50|$|WURFL solves this by {{allowing}} development of content pages using abstractions of page elements (buttons, links and textboxes for example). At run time, these are converted to the appropriate, <b>specific</b> <b>markup</b> types for each device. In addition, the developer can specify other content decisions {{be made at}} runtime based on device specific capabilities and features (which {{are all in the}} WURFL).|$|E
40|$|Resource level {{metadata}} markup alone cannot {{describe the}} rich, granular, associative and recombinant information objects potentially contained in modern digital libraries. Today, powerful mechanisms for content and structure {{description of documents}} exists {{in the form of}} domain <b>specific</b> <b>markup</b> languages such as MatML and MathML. Mechanisms for integrating resource level markup with domain <b>specific</b> <b>markup</b> documents in these languages are required. In the context of the NSDL GREEN digital library, issues and approaches to markup integration are critically discussed...|$|E
40|$|Recognising textual {{structures}} (paragraphs, sections, etc.) provides {{abstract and}} more general mechanisms for describ-ing documents {{independent of the}} particular semantics of <b>specific</b> <b>markup</b> schemas, tools and presentation stylesheets. In this paper we propose an algorithm {{that allows us to}} identify the structural role of each element in a set of homo-geneous scientific articles stored as XML files...|$|E
50|$|The {{gross margin}} is {{generally}} calculated {{as a percentage}} value of the pay rate. In certain circumstances, it may be calculated as a <b>specific</b> dollar <b>markup.</b>|$|R
50|$|Fuzzy Markup Language (FML) is a <b>specific</b> purpose <b>markup</b> {{language}} {{based on}} XML, used for describing {{the structure and}} behavior of a fuzzy system independently of the hardware architecture devoted to host and run it.|$|R
5000|$|Cost-plus pricing is a pricing {{strategy}} {{in which the}} selling price is determined by adding a <b>specific</b> dollar amount <b>markup</b> to a product's unit cost. An alternative pricing method is value-based pricing.|$|R
40|$|STMML is an XML-based markup {{language}} covering many generic aspects of scientific information. It {{has been developed}} as a re-usable core for more <b>specific</b> <b>markup</b> languages. It supports data structures, data types, metadata, scientific units and some basic components of scientific narrative. The central means of adding semantic information is through dictionaries. The specification is through an XML Schema {{which can be used}} to validate STMML documents or fragments. Many examples of the language are given...|$|E
40|$|This paper {{describes}} {{the beginning of}} an effort within the Linguist List's Electronic Metastructure for Endangered Languages Data (E-MELD) project to develop markup recommendations for representing the morphosyntactic structures of the world's endangered languages. Rather than proposing <b>specific</b> <b>markup</b> recommendations as in the Text Encoding Initiative (TEI), we propose to construct an environment for comparing data sets using possibly different markup schemes. The central feature of our proposed environment is an ontology of morphosyntactic terms with multiple inheritance and a variety of relations holding among the terms. We are developing our ontology using the Protg editor, and are extending an existing upper-level ontology known as SUMO...|$|E
40|$|Abstract. This paper {{presents}} ADDS, {{a systematic}} approach to sofware development using Domain-Specific Languages (DSLs) and markup technologies. XML {{is used as}} a common descriptive framework for DSLs formulation, obtaining Domain <b>Specific</b> <b>Markup</b> Languages (DSMLs). According to ADDS, the construction of applications in a domain starts with the provision of suitable DSMLs. Then, the applications in such a domain are described by means of sets of structured documents conforming these DSMLs. Finally, the application is produced by processing this documentation according to an operationalization model called OADDS. Hence ADDS provides a systematic approach to software development based on the processing of XMLdocumentation {{that can be used in}} a great variety of domains. ...|$|E
40|$|This paper tests whether {{economic}} liberalization in the 1980 s increased {{total factor productivity}} in Turkey’s manufacturing industries. The total factor productivity(TFP) measure here is the Solow residual. As is known, standard Solow residual assumes perfect competition and constant returns to scale. If the real industries under investigation show imperfect competition and non-constant returns to scale, then standard measure gives us biased estimates of the total factor productivity. Therefore, this paper modifies to include the effects of imperfect competition and non-constant returns to scale. After modifying the TFP growth, I test whether {{economic liberalization}} affects the TFP growth. Economic liberalization is captured by either an explicit measure of liberalization like the measures of trade liberalization such as reduced protection rates or by a dummy variable capturing {{a change in the}} economic policies. In this paper, industry specific total protection rates, total subsidy rates, and import penetration rates are used as explicit measures of economic liberalization. Our analysis is including both public and private sectors. For both sectors, I calculate, along the process of the effects of trade liberalization, sector <b>specific</b> <b>markups.</b> Markups that this paper comes up with are in general smaller than those of the US manufacturing industries. This point is interesting since one expects markups in the USA to be smaller than those in Turkey since the USA is mor...|$|R
40|$|Extensible Markup Language (XML) {{is quickly}} {{becoming}} the universal format for structured documents and {{data on the}} Web. XML is actually a metalanguage that has spawned the creation of numerous domain-specific, industrystandard markup languages, each having {{its own set of}} user-defined tags and attributes. XML is also a family of technologies such as the Extensible Style Sheet Transformation Language (XSLT) designed to transform an XML document into another XML document. In this paper, we survey <b>specific</b> XML-based <b>markup</b> languages for enterprise process modeling. We also discuss, {{in the context of an}} ongoing NSF-funded research project, the role of XML and related technologies in supporting a distributed, Web-based, collaborative framework for enterprise process modeling...|$|R
40|$|This paper {{outlines}} some design issues {{uncovered in}} our investigation of an XML mapping of the Institute for Interconnecting and Packaging Electronic Circuits GenCAM SM 1. 1 Standard. XML {{is an industry}} standard format developed by World Wide Web Consortium as an extensible replacement for HTML. Because it is flexible and concise, XML is rapidly being adopted as a standard mechanism for representing complex documents with industry specific content. GenCAM SM is an ANSI-approved PCB/A (Printed Circuit Board/Assembly) data standard sufficiently detailed for tooling, manufacturing, assembly, inspection and testing requirements. The shift from the current ASCII format of GenCAM SM to a <b>specific</b> XML <b>markup</b> language will not change the higher level, semantic objects represented by GenCAM SM. Only the instance file syntax will change to a newer and more widely used format. The results of our research to date support the utility of a convergence of XML and GenCAM technologies...|$|R
40|$|A geo-ontology has a {{key role}} to play in the {{development}} of a spatially aware search engine, with regard to providing support for query disambiguation, query term expansion, relevance ranking and web resource annotation. This paper reviews those functions and identifies the challenges arising in the construction and maintenance of such an ontology. Two current contenders for the representation of the geo-ontology are GML, a <b>specific</b> <b>markup</b> language for geographic domains and OWL, a generic ontology representation language. Both languages are used to model the geo-ontology designed for supporting web retrieval of geographic concepts. The powers and limitations of the languages are identified. In particular, the paper highlights the lack of representation and reasoning abilities for different types of rules needed for supporting the geo-ontology...|$|E
40|$|This paper {{describes}} DTC (Documents, Transformations and Components), {{our approach}} to the XML-based development of content-intensive applications. According to this approach, the contents of an application and other customizable features (e. g. the properties of its user interface) are represented in terms of XML documents. In DTC, the software of the application is organized in terms of reusable components capable of processing <b>specific</b> <b>markup</b> languages. In addition, we use document transformations to fit components and documents together, {{because they can be}} reused from pre-existing repositories. In this paper, we describe the DTC approach, illustrating its application in a case study. Because DTC encourages the explicit separation between the description of the application’s variability (contents and other customizable features) and the application’s operational support, the approach improves maintainability and reuse at both the information and software levels...|$|E
40|$|This paper {{describes}} the DTC (Documents, Transformations and Components) approach to XML-based development of software applications. According to this approach {{the content of}} an application, and key aspects of its structure and behavior, are represented in terms of XML documents. In DTC, software development is conceived as the process that gives operational support to these documents. DTC also encourages the organization of software in terms of reusable components, each of them able to process a <b>specific</b> <b>markup</b> language. Because documents and components could have been reused from pre-existing repositories, we use document transformations to fit them together. In this paper, we describe the DTC approach, illustrating its application in a case study. Because DTC encourages the explicit separation between application 9 ̆ 2 s content, software descriptions and operational support, we improve maintainability and reuse at both information and software levels...|$|E
40|$|I WikiMedia markup parser I Sentence {{normalization}} {{separated from}} query processing I Flexible ranking algorithm [Turpin et al., 2007] I Account for WikiMedia <b>markup</b> <b>specific</b> features I 42 unit tests I LOC: 575 (parser) + 392 (front-end) = 967 (cat *java | grep | grep-v / / | wc-l) Geller, Krahn, Krasnogolowy: Snippet Generation July 21, 2009 5 / 10 Lessons learned I WikiMedia markup {{is not that}} simple I Best to process it in multiple passes I Sentence normalization should be handled earlier in search engine architecture I Unicode is fu...|$|R
40|$|Abstract: This paper {{outlines}} {{an approach}} to XML-based software development. According to this method, applications are described using domain <b>specific,</b> XML based, <b>markup</b> languages. With these languages we structure a set of XML documents that are subsequently processed to yield the executable application. The approach also makes an explicit distinction between contents documents and documents describing other application aspects (e. g. interaction, presentation and process). Using a software process model based on markup languages and documents we obtain some benefits such as an important code reuse and a significant maintenance improvement. This paper describes our experiences applying this approach in the hypermedia domain and {{in the development of}} an application framework for supporting a broader range of information-based applications...|$|R
40|$|The {{usage of}} XML markup {{language}} and related technologies {{in the development}} of e-government services to improve codification, integration and interchange of electronic information is analysed. There are different alternatives to introduce XML technologies to e-government, namely: the usage of XML markup language, the usage of a language based on XML but oriented specifically to e-government, such as GOVML, or the usage of <b>specific</b> <b>markup</b> languages for specific purposes, such as: UBL and EML. It is concluded that XML based technologies have certain strategic application within an organization, among them can be pointed out the following ones: the improvement of work chain, the return of investments in a short term, the management of intangible assets of the organization, and the development of innovative e-gov services. Nevertheless, XML related technologies by themselves do not report any added value to an organization, their actual value depends on how efficiently and coherently XML technologies are applied {{in the context of an}} organization...|$|E
40|$|The {{lifecycle}} of Web-based applications {{is characterized}} by frequent changes to content, user interface, and functionality. Updating content, improving the services provided to users, drives further development of a Web-based application. The major goal {{for the success of}} a Web-based application becomes therefore its evolution. Though, development and maintenance of Web-based applications suffers from the underlying document-based implementation model. A disciplined evolution of Web based applications requires the application of software engineering practice for systematic further development and reuse of software artifacts. In this contribution we suggest to adopt the component paradigm to development and evolution of Web-based applications. The approach is based on a dedicated component technology and component-software architecture. It allows abstracting from many technical aspects related to the Web as an application platform by introducing domain <b>specific</b> <b>markup</b> languages. These languages allow the description of services, which represent domain components in our Web-component-software approach. Domain experts with limited knowledge of technical details can therefore describe application functionality and the evolution of orthogonal aspects of the application can be de-coupled. The whole approach is based on XML to achieve the necessary standardization and economic efficiency for the use in real world projects...|$|E
40|$|The Semantic Web uses formal {{distributed}} ontologies for representing {{relationships among}} concepts {{in the real}} world. A structured framework such as this allows agents to peruse and reason about published knowledge {{without the need for}} scrapers, information agents, and centralized ontologies. However, in order to process any information, an agent must be familiar with the underlying ontology used to markup that information. However, no single agent can be expected to be familiar with all possible ontologies that may be available on the Semantic Web. Therefore, translation services that transform concepts defined within previously unknown ontologies into known concepts allow agents to understand the available information and hence achieve their goal. This transformation may be achieved by invoking <b>specific</b> <b>markup</b> translation services or by logical reasoning through other shared ontologies. The RETSINA Calendar Agent (RCal) is a Distributed Meeting Scheduling Agent that processes schedules marked up on the Semantic Web, and imports them into the user’s Personal Information Manager. Translation services, which are used to translate unknown concepts into known concepts, which are located using a DAML-S based service discovery mechanisms. In this paper, we present RCal, and demonstrate how it extracts and uses meaningful knowledge from the semantic markup. In addition, we describe how web-service discovery mechanisms are used when new concepts are encountered...|$|E
40|$|One o f {{the most}} {{important}} stages in the localisation process is the provision of high quality help and documentation in the target languages. Translation of computer manuals and Help files consists of: (i) translating the text (ii) maintaining the formatting of the source. Although many tools are available to translators, the greatest need for standardised tools exists {{in the area of}} formatting and layout of documentation. At present a significant proportion of time allocated to this stage is spent checking that the formatting has not changed as a consequence of translation. For example, properties such as font type, font size and style (e. g. bold, italic) may accidentally be changed during translation. It is also possible that two paragraphs are combined into one, or even deleted altogether from the text. The aim of this research is to assess the viability of developing a generic comparison process for documentation files. This process should be able to take two text-based document files (e. g. TeX, MIF, RTF) and compare the underlying codes (called markup) that describe the format and structure o f the documents, where format is its physical appearance (e. g. underlined text, margins) and the structure is its composition (e. g. paragraphs, chapters, headings). Although the localised documents will usually use the same markup scheme as the original, the possibility of incorporating the comparison of different file types into the process is investigated, in keeping with the concept of generality. However, each markup scheme has its own set of codes. In addition to this, the format is described by <b>specific</b> <b>markup</b> and the structure by generalised markup. The vast differences between these schemes means it is not always possible to make a direct comparison, complicating the process...|$|E
40|$|Die Arbeit mit Texten im Computer hebt sich dann von der elektronischen Schreibmaschine ab und beginnt zur Datenverarbeitung zu werden, wenn im Mittelpunkt des Interesses nicht mehr der Ausdruck von Papier steht, sondern auch der Zugriff auf die Textdaten für Fragen der Analyse, Statistik und inhaltlichen Auswertung berücksichtigt wird. Der vorliegende Beitrag beabsichtigt eine praktische Einführung in die Idee und Anwendung der sachlich orientierten Textauszeichnung (generic markup), die die Grundlage für eine Textdatenverarbeitung bietet. Ausführlich wird das Programmpaket TUSTEP als Werkzeug für den Umgang mit dieser Art von Textdaten für Druckausgabe und Weiterverarbeitung dargestellt. " (Autorenreferat) "Working with {{computer}} becomes real data processing in opposite to simply {{working with a}} typewriter, when the user does not face only the printout but faces the data to be processed for several purposes: quantitative analysis, grammatical analysis, statistical approaches, retrieval for information. The success of this purposes needs not only powerful tools but a good method of data management. The paper gives a practical introduction into the generic text markup. Generic text markup means here markup {{with regard to the}} content of a text and not only to the typography in opposite to the <b>specific</b> <b>markup</b> for specific formatters. In a first part the paper describes the idea of generic markup. It stresses the advantages of generic markup in the daily work. The second part shows the four fundamental steps in working with generic markup. This steps are explained with regard to SGML. The third part gives examples. They present TUSTEP as a tool for working with SGML-like tags. TUSTEP-programs are demonstrated for parsing texts with generic markup. These examples include not only different layout programs, but also analytical work with tagged texts. " (author's abstract...|$|E
40|$|The ATLAS Technical Coordination disposes of 17 Web {{systems to}} support its operation. These applications, whilst ranging from {{supporting}} the process of publishing scientific papers to monitoring radiation levels in the equipment at the cave, are constantly prone to changes in requirements due to the collaborative nature of the experiment and its management. In this context, a Web framework is proposed to unify the generation of the supporting interfaces. Fence assembles classes to build applications by making extensive use of JSON configuration files. It relies vastly on Glance, a technology that was set forth in 2003 to create an abstraction layer {{on top of the}} heterogeneous sources that store the technical coordination data. Once Glance maps out the database modeling, records can be referenced in the configuration files by wrapping unique identifiers around double enclosing brackets. The deployed content can be individually secured by attaching clearance attributes to their description thus ensuring that view/edit privileges are granted to eligible users only. The framework also provides tools for securely writing data to a database. Fully HTML 5 -compliant multi-step forms can be generated from their JSON description to assure that the submitted data observe a series of constraints. Input validation is carried out primarily on the server side but, following progressive enhancement guidelines, verification might also be performed on the client side by enabling <b>specific</b> <b>markup</b> data attributes which are then handed over to the jQuery validation plug-in. User monitoring is accomplished by thoroughly logging user requests along with any POST data. Documentation is built from the source code using the phpDocumentor tool and made readily available for developers online. Fence, therefore, speeds up the implementation of Web interfaces and reduces the response time to requirement changes by minimizing maintenance overhead and facilitating the comprehension of embedded rules and requirements...|$|E
40|$|The Standard Generalized Markup Language (SGML) {{has been}} the International Organization of Standardization (ISO) {{published}} standard for text interchange for nearly a decade. Since 1986, SGML based publishing has been successfully implemented in many fields, notably those industries with massive and mission-critical publishing operations such as the military, legal, medical, and heavy industries. SGML based publishing differs from the WYSIWYG paradigm of desktop publishing in that an SGML document contains descriptive, structural markup rather than specific formatting markup. <b>Specific</b> <b>markup</b> describes {{the appearance of a}} document and is usually a proprietary code which makes the document difficult to re-use or interchange to different systems. The structurally generic markup codes in an SGML document allow the fullest exploitation of the information. An SGML document exhibits more re-usability than a document created and stored in a proprietary formatting code. In many cases, workflow and production are greatly improved by the implementation of SGML based publishing. Historical and anecdotal case studies of many applications clearly delineate the benefits of an SGML based publishing system. And certainly, the boom in Web publishing has spurred interest in enabling a publishing system with multi-output functionality. However, implementation is associated with high costs. The acquisition of new tools and new skills is a costly investment. A careful cost-benefit analysis must determine that the current publishing needs would be satisfied by moving to SGML. Increased productivity is the measure by which SGML is adopted. The purpose of this thesis project is to investigate the relative benefits and requirements of a simple SGML based publishing implementation. The graduate thesis for most of the School of Printing Management and Sciences at the Rochester Institute of Technology was used as an example. The author has expanded the requirements for the publication process of a graduate thesis with factors which do not exist in reality. The required output has been expanded from mere print output to include publishing on the World Wide Web (WWW) in the Hypertext Markup Language (HTML), and to some proprietary electronic browser such as Folio Views for inclusion in a searchable collection of graduate theses on CD-ROM. A proposed set of tools and methods are discussed in order to clarify the requirements of such an SGML implementation...|$|E
40|$|Today {{knowledge}} base authoring for the engineering of intelligent systems is performed mainly by using tools with graphical user interfaces. An alternative human-computer interaction para- digm is the maintenance and manipulation of electronic documents, which provides several ad- vantages {{with respect to}} the social aspects of knowledge acquisition. Until today it hardly has found any attention as a method for knowledge engineering. This thesis provides a comprehensive discussion of document-centered knowledge acquisition with knowledge markup languages. There, electronic documents are edited by the knowledge authors and the executable {{knowledge base}} entities are captured by markup language expressions within the documents. The analysis of this approach reveals significant advantages as well as new challenges when compared to the use of traditional GUI-based tools. Some advantages of the approach are the low barriers for domain expert participation, the simple integration of informal descriptions, and the possibility of incremental knowledge for- malization. It therefore provides good conditions for building up a knowledge acquisition pro- cess based on the mixed-initiative strategy, being a flexible combination of direct and indirect knowledge acquisition. Further it turns out that document-centered knowledge acquisition with knowledge markup languages provides high potential for creating customized knowledge au- thoring environments, tailored {{to the needs of the}} current knowledge engineering project and its participants. The thesis derives a process model to optimally exploit this customization po- tential, evolving a project specific authoring environment by an agile process on the meta level. This meta-engineering process continuously refines the three aspects of the document space: The employed markup languages, the scope of the informal knowledge, and the structuring and organization of the documents. The evolution of the first aspect, the markup languages, plays a key role, implying the design of project <b>specific</b> <b>markup</b> languages that are easily understood by the knowledge authors and that are suitable to capture the required formal knowledge precisely. The goal of the meta-engineering process is to create a knowledge authoring environment, where structure and presentation of the domain knowledge comply well to the users’ mental model of the domain. In that way, the approach can help to ease major issues of knowledge-based system development, such as high initial development costs and long-term maintenance problems. In practice, the application of the meta-engineering approach for document-centered knowl- edge acquisition poses several technical challenges that need to be coped with by appropriate tool support. In this thesis KnowWE, an extensible document-centered knowledge acquisition environment is presented. The system is designed to support the technical tasks implied by the meta-engineering approach, as for instance design and implementation of new markup lan- guages, content refactoring, and authoring support. It is used to evaluate the approach in several real-world case-studies from different domains, such as medicine or engineering for instance. We end the thesis by a summary and point out further interesting research questions consid- ering the document-centered knowledge acquisition approach. Ein Meta-Engineering Ansatz für dokumentenzentrierte Wissensakquisitio...|$|E

