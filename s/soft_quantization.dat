16|27|Public
30|$|The spatial {{relationship}} in an image exists between {{different regions of}} the image; however, spatial information is always neglected due to the orderless representation of the image using quantization-based methods [14]. Although methods based on query expansion [15], <b>soft</b> <b>quantization</b> [16], and large vocabulary size [17] can enhance the image retrieval performance, these methods lack spatial information that provides discriminating details.|$|E
40|$|Symbolic {{knowledge}} extraction from mapping/extrapolating {{neural networks}} {{is presented in}} the paper. An algorithm to obtain crisp rules in the form of logical implications which roughly describe the neural network mapping is introduced. The number of extracted rules can be selected using an uncertainty margin parameter as well as by changing the precision of the <b>soft</b> <b>quantization</b> of the inputs. A fuzzy decision system for a medication dosage problem has been developed and tested to demonstrate this approach...|$|E
40|$|A {{method of}} extracting {{intuitive}} knowledge from neural network classifiers {{is presented in the}} paper. An algorithm which obtains crisp rules in the form of logical implications which approximately describe the neural network mapping is introduced. The number of extracted rules can be selected using an uncertainty margin parameter as well as by changing the precision level of the <b>soft</b> <b>quantization</b> of inputs. A fuzzy decision system based on the iris database has been developed using this approach to produce linguistic rules for flower classification...|$|E
40|$|In this paper, {{we study}} the best rate {{distortion}} performance that an H. 264 encoder can possibly achieve. Using <b>soft</b> decision <b>quantization</b> {{rather than the}} standard hard decision quantization, we first establish a general framework for jointly designing motion compensation, quantization, and entropy coding in the hybrid coding structure of H. 264 to minimize a true rate distortion cost. We then propose three rate distortion optimization algorithms—a graph-based algorithm for optimal <b>soft</b> decision <b>quantization</b> in H. 264 baseline profile encoding given motion compensation and quantization step sizes, an iterative algorithm for optimal residual coding in H. 264 baseline profile encoding given motion compensation, and an iterative overall algorithm for optimal H. 264 baseline profile encoding—with them embedded in the indicated order. The graph-based algorithm for optimal <b>soft</b> decision <b>quantization</b> is the core; given motion compensation and quantization step sizes, it is guaranteed to perform optimal <b>soft</b> decision <b>quantization</b> to certain degree. The proposed iterative overall algorithm has been implemented based on the reference encoder JM 82 of H. 264. Comparative studies show that it achieves a significant performance gain, which can {{be as high as}} 25 % rate reduction at the same PSNR when compared to the reference encoder...|$|R
40|$|In this paper, we {{introduce}} a <b>soft</b> vector <b>quantization</b> scheme with inverse power-function distribution, and analytically derive an upper bound {{of the resulting}} quantization noise energy in comparison to that of typical (hard-deciding) vector quantization. We also discuss the positive impact {{of this kind of}} <b>soft</b> vector <b>quantization</b> on the performance of machine-learning systems that include one or more vector quantization modules. Moreover, we provide experimental evidence on the advantage of avoiding over-fitting and boosting the robustness of such systems in the presence of considerable parasitic variance; e. g. noise, in the runtime inputs. The experiments have been conducted with two versions of one of the best reported discrete HMM-based Arabic OCR systems; one version deploying hard vector quantization and the other deploying our herein presented <b>soft</b> vector <b>quantization.</b> Test samples of real-life scanned pages are used to challenge both versions; hence the recognition error margins are compare...|$|R
40|$|Abstract-In this paper, we {{introduce}} a <b>soft</b> vector <b>quantization</b> scheme with inverse power-function distribution, and analytically derive an upper bound {{of the resulting}} quantization noise energy in comparison to that of typical (hard-deciding) vector quantization. We also discuss the positive impact {{of this kind of}} <b>soft</b> vector <b>quantization</b> on the performance of machine-learning systems that include one or more vector quantization modules. Moreover, we provide experimental evidence on the advantage of avoiding over-fitting and boosting the robustness of such systems in the presence of considerable parasitic variance; e. g. noise, in the runtime inputs. The experiments have been conducted with two versions of one of the best reported discrete HMM-based Arabic OCR systems; one version deploying hard vector quantization and the other deploying our herei...|$|R
40|$|Convolutional coding and Viterbi decoding, {{along with}} binary {{phase-shift}} keyed modulation, {{is presented as}} an efficient system for reliable communication on power limited satellite and space channels. Performance results, obtained theoretically and through computer simulation, are given for optimum short constraint length codes {{for a range of}} code constraint lengths and code rates. System efficiency is compared for hard receiver quantization and 4 and 8 level <b>soft</b> <b>quantization.</b> The effects on performance of varying of certain parameters relevant to decoder complexity and cost are examined. Quantitative performance degradation due to imperfect carrier phase coherence is evaluated and compared to that of an uncoded system. As an example of decoder performance versus complexity, a recently implemented 2 -Mbit/sec constraint length 7 Viterbi decoder is discussed. Finally a comparison is made between Viterbi and sequential decoding in terms of suitability to various system requirements...|$|E
40|$|Abstract—The Bag-of-Words (BoW) {{model is}} prone to the {{deficiency}} of spatial constraints among visual words. The {{state of the art}} methods encode spatial information via visual phrases. However, these methods discard the spatial context among visual phrases instead. To address the problem, this letter introduces a novel visual concept, the Visual Phraselet, as a kind of similarity measurement between images. The visual phraselet refers to the spatial consistent group of visual phrases. In a simple yet effective manner, visual phraselet filters out false visual phrase matches, and is much more discriminative than both visual word and visual phrase. To boost the discovery of visual phraselets, we apply the <b>soft</b> <b>quantization</b> scheme. Our method is evaluated through extensive experiments on three benchmark datasets (Oxford 5 K, Paris 6 K and Flickr 1 M). We report significant improvements as large as 54. 6 % over the baseline approach, thus validating the concept of visual phraselet. Index Terms—Image search, spatial constraint, visual phrase, vi-sual phraselet. I...|$|E
40|$|A novel Sparsely Encoded Local Descriptor (SELD) is {{proposed}} for face verification. Different from traditional hard or <b>soft</b> <b>quantization</b> methods, we exploit linear regression (LR) model with sparsity and non-negativity constraints to extract more discriminative features (i. e. sparse codes) from local image patches sampled pixel-wisely. Sum-pooling is then imposed to integrate all the sparse codes within each block partitioned {{from the whole}} face image. Whitened Principal Component Analysis (WPCA) is finally used to suppress noises and reduce the dimensionality of the pooled features, which thus results in the so-called SELD. To validate the proposed method, comprehensive experiments are conducted on face verification task to compare SELD with the existing related methods in terms of three variable component modules: K-means or K-SVD for dictionary learning, hard/soft assignment or regression model for encoding, as well as sum-pooling or max-pooling for pooling. Experimental results show that our method achieves a competitive accuracy compared with the state-of-the-art methods on the challenging Labeled Faces in the Wild (LFW) database. Department of Computin...|$|E
40|$|In this work, we {{investigate}} the nearly lossless image compression technique, {{which provides a}} better compression ratio than purely lossless compression schemes and has a better reconstructed image quality than lossy ones. In particular, we introduce a new idea called the <b>soft</b> decision <b>quantization</b> and integrate it with the binary arithmetic QM coder. The superior performance of the developed algorithm is demonstrated with numerical experiments. Keywords: nearly lossless, semi-lossless, image compression, <b>soft</b> decision <b>quantization,</b> QM coder. 1 INTRODUCTION Image compression methods can be categorized into two classes: lossless and lossy schemes. Entropy encoding {{is an example of}} lossless compression which removes redundancy among symbols without actual loss of information so that the image can be reconstructed exactly as the original one. Quantization is applied in lossy compression schemes to achieve a better coding gain at the expense of information loss and, consequently, the rec [...] ...|$|R
40|$|In this thesis, {{we study}} {{the concept of}} {{scalable}} video coding as implemented in the extension to the H. 264 video coding standard. Specifically, for the spatial and quality scalability scenarios, we propose an optimization algorithm based on the <b>Soft</b> Decision <b>Quantization</b> (SDQ) concept, which aims at jointly optimizing all layers being encoded. The performance of the algorithm was assessed in an SVC implementation. Experimental results show, that the proposed method significantly improves the coding efficiency when compared to an unmodified SVC encoder...|$|R
40|$|Abstract: High-throughput genomic {{analysis}} provides insight into a complicated biological phenomena. However, the {{vast amount of}} data produced from upto-date biological experimental processes needs appropriate data mining techniques to extract useful information. In this paper, we propose a method based on cluster analysis and Bayesian network learning for the molecular pharmacology of cancer. Specifically, the NCI 60 dataset is analysed by <b>soft</b> topographic vector <b>quantization</b> (STVQ) for cluster analysis and by Bayesian network learning for dependency analysis. Our results of the cluster analysis show that gene expression profiles are more related {{to the kind of}} cancer than to drug activity patterns. Dependency analysis using Bayesian networks reveals some biologically meaningful relationships among gene expression levels, drug activities, and cancer types, suggesting the usefulness of Bayesian network learning as a method for exploratory analysis of high-throughput genomic data. Key words: Gene expression pattern, drug activity pattern, molecular pharmacology, <b>soft</b> topographic vector <b>quantization</b> (STVQ), Bayesian network...|$|R
40|$|We {{propose a}} new image {{representation}} for texture categorization and facial analysis, relying {{on the use of}} higher-order local differential statistics as features. It has been recently shown that small local pixel pattern distributions can be highly discriminative while being extremely efficient to compute, which is in contrast to the models based on the global structure of images. Motivated by such works, we propose to use higher-order statistics of local non-binarized pixel patterns for the image description. The proposed model does not require either (i) user specified quantization of the space (of pixel patterns) or (ii) any heuristics for discarding low occupancy volumes of the space. We propose to use a data driven <b>soft</b> <b>quantization</b> of the space, with parametric mixture models, combined with higher-order statistics, based on Fisher scores. We demonstrate that this leads to a more expressive representation which, when combined with discriminatively learned classifiers and metrics, achieves state-of-the-art performance on challenging texture and facial analysis datasets, in low complexity setup. Further, it is complementary to higher complexity features and when combined with them improves performance. Comment: CVIU preprin...|$|E
40|$|Includes bibliographical {{references}} (page 49) Communication satellite {{channels are}} characterized by limited transmission power and bandwidth. The primary disturbance on these channels can be quite accurately modeled as additive Gaussian noise which is ???white??? enough to be essentially independent from one bit interval to the next. Convolutional coding with soft-decision Viterbi decoding {{has proven to be}} a very cost-effective system for these channels. A coding system, consisting of a convolutional encoder and a Viterbi maximum likelihood decoder is developed utilizing a constraint length of 5 and a half rate code with 8 level <b>soft</b> <b>quantization.</b> The primary emphasis of this graduate project has been to develop simulations of the convolutional encoder and the Viterbi decoder that can be used in the coding system described above. The encoder simulation is used to develop an optimum coding structure and to generate test code sequences. The decoder simulation is used to test the performance of the Viterbi algorithm in the presence of white Gaussian noise and with the loss of either branch or node synchronization...|$|E
40|$|Accepted for {{publication}} in International Journal of Computer Vision and Image Understanding (CVIU) We propose a new image representation for texture categorization and facial analysis, relying {{on the use of}} higher-order local differential statistics as features. It has been recently shown that small local pixel pattern distributions can be highly discriminative while being extremely efficient to compute, which is in contrast to the models based on the global structure of images. Motivated by such works, we propose to use higher-order statistics of local non-binarized pixel patterns for the image description. The proposed model does not require either (i) user specified quantization of the space (of pixel patterns) or (ii) any heuristics for discarding low occupancy volumes of the space. We propose to use a data driven <b>soft</b> <b>quantization</b> of the space, with parametric mixture models, combined with higher-order statistics, based on Fisher scores. We demonstrate that this leads to a more expressive representation which, when combined with discriminatively learned classifiers and metrics, achieves state-of-the-art performance on challenging texture and facial analysis datasets, in low complexity setup. Further, it is complementary to higher complexity features and when combined with them improves performance...|$|E
40|$|In a {{move away}} from {{previous}} years’ participation in TRECVid ([1] [2] [3]), this year our team focused on the instance search task (INS). We improved our system from last year by applying large vocabulary <b>quantization,</b> <b>soft</b> assignment of visual words, spatial verifications and query expansion. Overall, four automatic runs have been submitted for evaluation. In this paper, we present first our system, then we discuss the results and findings of our experiments...|$|R
30|$|Although many {{photorealistic}} relighting methods {{provide a}} way to change the illumination of objects in a digital photograph, it is currently difficult to relight digital illustrations having a cartoon shading style. The main difference between photorealistic and cartoon shading styles is that cartoon shading is characterized by <b>soft</b> color <b>quantization</b> and nonlinear color variations that cause noticeable reconstruction errors under a physical reflectance assumption, such as Lambertian reflection. To handle this non-photorealistic shading property, we focus on shading analysis {{of the most fundamental}} cartoon shading technique. Based on the color map shading representation, we propose a simple method to determine the input shading as that of a smooth shape with a nonlinear reflectance property. We have conducted simple ground-truth evaluations to compare our results to those obtained by other approaches.|$|R
40|$|Abstract — Expression {{recognition}} from non-frontal faces is a challenging research area with growing interest. In this paper, we explore discriminative learning of Gaussian Mixture Models for multi-view facial expression recognition. Adopting the BoW model from image categorization, our image descriptors are computed using <b>Soft</b> Vector <b>Quantization</b> {{based on the}} Gaussian Mixture Model. We do extensive experiments on recognizing six universal facial expressions from face images {{with a range of}} seven pan angles (− 45 ◦ ∼ + 45 ◦) and five tilt angles (− 30 ◦ ∼ + 30 ◦) generated from the BU- 3 dFE facial expression database. Our results show that our approach not only significantly improves the resulting classification rate over unsupervised training but also outperforms the published state-of-the-art results, when combined with Spatial Pyramid Matching. I...|$|R
40|$|Face {{verification}} Labeled {{faces in}} the wild a b s t r a c t A novel Sparsely Encoded Local Descriptor (SELD) is proposed for face verification. Different from traditional hard or <b>soft</b> <b>quantization</b> methods, we exploit linear regression (LR) model with sparsity and non-negativity constraints to extract more discriminative features (i. e. sparse codes) from local image patches sampled pixel-wisely. Sum-pooling is then imposed to integrate all the sparse codes within each block partitioned from the whole face image. Whitened Principal Component Analysis (WPCA) is finally used to suppress noises and reduce the dimensionality of the pooled features, which thus results in the so-called SELD. To validate the proposed method, comprehensive experiments are conducted on face verification task to compare SELD with the existing related methods in terms of three variable component modules: K-means or K-SVD for dictionary learning, hard/soft assignment or regression model for encoding, as well as sum-pooling or max-pooling for pooling. Experimental results show that our method achieves a competitive accuracy compared with the state-of-the-art methods on the challenging Labeled Faces in the Wild (LFW) database. & 2014 Elsevier B. V. All rights reserved. 1...|$|E
40|$|This paper {{discusses}} the positive impact of soft vector quantization {{on the performance}} of machine-learning systems that include one or more vector quantization modules. The most impactful gains here are avoiding over-fitting and boosting the robustness of such systems in the presence of considerable parasitic variance; e. g. noise, in the runtime inputs. The paper then introduces a soft vector quantization scheme with inverse power-function distributions, and analytically derives an upper bound of its relative quantization noise energy to that of typical (hard-deciding) vector quantization. This relative noise is expressed as a closed-form function of the power in order to allow the selection of its optimal values of that compromise both a soft enough vector quantization with a stable performance via small enough relative quantization noise. Finally, we present empirical evidence obtained via experimenting with two versions of the best reported OCR system for cursive scripts - that happened to deploy discrete HMMs - one version with hard vector quantization and the other with our herein presented <b>soft</b> <b>quantization.</b> Test samples of real-life scanned Arabic text pages are used to challenge both versions; hence the recognition error margins are compared...|$|E
30|$|Of course, {{this type}} of feature {{quantization}} introduces the respective loss of the discriminative ability of the features. Thus, over the years, numerous improvements and alternatives have been proposed. The <b>soft</b> <b>quantization</b> and soft assignment techniques proposed in [18] and [19], respectively, reduce the quantization error of the original BOVW model, paying a price in terms of memory overload and higher searching time. Alternatively, the Fisher vector [20] uses the Gaussian mixture model to train the codebook and quantizes the features by calculating {{the probability of a}} feature falling into the Gaussian mixture. Different approaches like Hamming embedding [21] improve the model by generating binary signatures coupling visual words and thus providing additional information to filter false positives. Recently, an alternative to the BOVW model, the Vector of Locally Aggregated Descriptors (VLAD) [22] has gained the community’s attention. Given a codebook, instead of creating a vector of frequencies, the VLAD model creates a vector of differences, as distances, between a feature and the cluster’s centre. VLAD manages to speed up the aggregation step but leads to high-dimensional vector representations per image, which can affect the scalability of a method. Finally, authors in [23] focus on a multilayer deep learning architecture to represent high-level features in an effective compact manner, while [24 – 26] emphasize the need for domain-adaptive dictionary learning and the benefits of effectively fusing multiple information sources.|$|E
40|$|Abstract—This paper {{presents}} an axiomatic approach to <b>soft</b> learning vector <b>quantization</b> (LVQ) and clustering based on reformulation. The reformulation of the fuzzy ™-means (FCM) algorithm provides {{the basis for}} reformulating entropy-constrained fuzzy clustering (ECFC) algorithms. This analysis indicates that minimization of admissible reformulation functions using gradient descent leads to a broad variety of <b>soft</b> learning vector <b>quantization</b> and clustering algorithms. According to the proposed approach, the development of specific algorithms reduces to {{the selection of a}} generator function. Linear generator functions lead to the FCM and fuzzy learning vector quantization (FLVQ) algorithms while exponential generator functions lead to ECFC and entropy-constrained learning vector quantization (ECLVQ) algorithms. The reformulation of LVQ and clustering algorithms also provides the basis for developing uncertainty measures that can identify feature vectors equidistant from all prototypes. These measures are employed by a procedure developed to make soft LVQ and clustering algorithms capable of identifying outliers in the data set. This procedure is evaluated by testing the algorithms generated by linear and exponential generator functions on speech data. Index Terms — Fuzzy clustering, generator function, learning vector quantization, outlier identification, reformulation, reformulatio...|$|R
40|$|Abstract. We {{introduce}} a generalization of Robust <b>Soft</b> Learning Vector <b>Quantization</b> (RSLVQ). This algorithm for nearest prototype classification {{is derived from}} an explicit cost function and follows the dynamics of a stochastic gradient ascent. We generalize the RSLVQ cost function with respect to vectorial class labelings. This approach allows to realize multivariate class memberships for prototypes and training samples, and the prototype labels {{can be learned from}} the data during training. We present experiments to demonstrate the new algorithm in practice. ...|$|R
40|$|We {{study the}} problem of fast {{detection}} of dense, sufficiently separated clusters in (possibly) high dimensional spaces using self-organizing maps. To this end, we formulate the self-organizing map as a <b>soft</b> vector <b>quantization</b> procedure. Topological structure among representative vectors arises due to transition channel noise. We prove the convergence of this scheme and show that {{the introduction of a}} topology into the set of representative vectors results in a shift form Euclidean to a non-Euclidean dot product in attractive/repulsive terms of training equations. The non-Euclidean dot product is defined by the transition channel noise matrix and can be analyzed in terms of its eigenvector decomposition. In this context, we study the so called star topology of representative vectors as an appropriate topology for cluster detection applications and give an illustrative example. 1 Introduction An important subtask in data compression, signal coding and pattern classification is finding [...] ...|$|R
40|$|Recognizing the {{location}} and orientation of a mobile device from captured images is a promising application of image retrieval algorithms. Matching the query images to an existing georeferenced database like Google Street View enables mobile search for location related media, products, and services. Due to the rapidly changing field {{of view of the}} mobile device caused by constantly changing user attention, very low retrieval times are essential. These can be significantly reduced by performing the feature quantization on the handheld and transferring compressed Bag-of-Feature vectors to the server. To cope with the limited processing capabilities of handhelds, the quantization of high dimensional feature descriptors has to be performed at very low complexity. To this end, we introduce in this paper the novel Multiple Hypothesis Vocabulary Tree (MHVT) as a step towards real-time mobile location recognition. The MHVT increases the probability of assigning matching feature descriptors to the same visual word by introducing an overlapping buffer around the separating hyperplanes to allow for a <b>soft</b> <b>quantization</b> and an adaptive clustering approach. Further, a novel framework is introduced that allows us to integrate the probability of correct quantization in the distance calculation using an inverted file scheme. Our experiments demonstrate that our approach achieves query times reduced by up to a factor of 10 when compared to the state-of-the-art...|$|E
30|$|The {{key problem}} of image scene {{classification}} {{is how to}} bridge the “semantic gap” between the underlying features and the high-level semantics. It is an important research idea to solve the core problem of scene classification by extracting the local invariant features of images and constructing the local semantic concept representation of images. Sparse coding theory is introduced into the study of image local semantic concept representation and has achieved high accuracy in image scene classification. However, the existing sparse coding models aim at minimizing signal reconstruction error. For image scene classification, {{it is more important}} to find a discriminant representation than to minimize the reconstruction error. Therefore, this paper proposes an image classification method based on sparse coding multi-scale spatial latent semantic analysis. Spatial pyramid matching of image segmentation is used to extract the spatial position information of the target, and feature <b>soft</b> <b>quantization</b> based on sparse coding is used to form a co-occurrence matrix, which improves the accuracy of the original feature representation. Finally, the PLSA model is used to mine the local latent semantic information, and each local semantic information is concatenated to obtain the image multi-scale spatial latent semantic information. Experimental results show that the proposed method has higher classification accuracy than the existing better image classification methods, and the three modules of spatial pyramid matching, sparse coding to construct a co-occurrence matrix, and PLSA dimensionality reduction are indispensable in this method, so that the image can be more accurately represented and the performance of image classification can be improved together.|$|E
40|$|Abstract Image {{appearance}} descriptors {{are needed}} for different computer vision applications dealing with, for example, detection, recognition and classification of objects, textures, humans, etc. Typically, such descriptors should be discriminative to allow for making the distinction between different classes, yet still robust to intra-class variations due to imaging conditions, natural changes in appearance, noise, and other factors. The purpose of this thesis is the development and analysis of photometric descriptors for the appearance of real life images. The two application areas included in this thesis are face recognition and texture classification. To facilitate the development and analysis of descriptors, a general framework for image description using statistics of quantized filter bank responses modeling their joint distribution is introduced. Several texture and other image appearance descriptors, including the local binary pattern operator, can be presented using this model. This framework, within which the thesis is presented, enables experimental evaluation {{of the significance of}} each of the components of this three-part chain forming a descriptor from an input image. The main contribution of this thesis is a face representation method using distributions of local binary patterns computed in local rectangular regions. An important factor of this contribution is to view feature extraction from a face image as a texture description problem. This representation is further developed into a more precise model by estimating local distributions based on kernel density estimation. Furthermore, a face recognition method tolerant to image blur using local phase quantization is presented. The thesis presents three new approaches and extensions to texture analysis using quantized filter bank responses. The first two aim at increasing the robustness of the quantization process. The soft local binary pattern operator accomplishes this by making a <b>soft</b> <b>quantization</b> to several labels, whereas Bayesian local binary patterns make use of a prior distribution of labelings, and aim for the one maximizing the a posteriori probability. Third, a novel method for computing rotation invariant statistics from histograms of local binary pattern labels using the discrete Fourier transform is introduced. All the presented methods have been experimentally validated using publicly available image datasets and the results of experiments are presented in the thesis. The face description approach proposed in this thesis has been validated in several external studies, and it has been utilized and further developed by several research groups working on face analysis...|$|E
40|$|Abstract—Iterative {{decoding}} {{based on}} the sum-product algo-rithm (SPA) is examined for sending low-density parity check (LDPC) codes over a discrete non-binary queue-based Markovian burst noise channel. This channel model is adopted due to its analytically tractability and its recently demonstrated capability in accurately representing correlated flat Rayleigh fading chan-nels under antipodal signaling and either hard or <b>soft</b> output <b>quantization.</b> SPA equations are derived in closed-form for this model {{in terms of its}} parameters. It is then numerically observed that potentially large coding gains can be realized with respect to the Shannon limit by exploiting channel memory as opposed to ignoring it via interleaving. Finally, the LDPC decoding performance under both matched and mismatched decoding regimes is evaluated. It is shown that the Markovian model provides noticeable gains over channel interleaving and that it can effectively capture the underlying fading channels behavior when decoding LDPC codes. Index Terms—Burst Noise, finite-state Markov channels, modeling correlated Rayleigh fading channels, Shannon limit, matched and mismatched decoding, hard and soft-decision de-modulation, channel interleaving, low-density parity-check codes, iterative decoding. I...|$|R
40|$|Abstract. We {{present a}} {{technique}} to extend Robust <b>Soft</b> Learning Vector <b>Quantization</b> (RSLVQ). This algorithm is derived from an explicit cost function and follows the dynamics of a stochastic gradient ascent. The RSLVQ cost function involves a hyperparameter which is kept fixed during training. We propose to adapt the hyperparameter based on the gradient information. Experiments on artificial and real life data show that the hyperparameter crucially influences the performance of RSLVQ. However, {{it is not possible}} to estimate the best value from the data prior to learning. We show that the proposed variant of RSLVQ is very robust with respect to the choice of the hyperparameter. ...|$|R
40|$|Learning Vector Quantization (LVQ) is {{a popular}} method for multiclass classification. Several {{variants}} of LVQ have been developed recently, of which Robust <b>Soft</b> Learning Vector <b>Quantization</b> (RSLVQ) is a promising one. Although LVQ methods have an intuitive design with clear updating rules, their dynamics are not yet well understood. In simulations within a controlled environment RSLVQ performed very close to optimal. This controlled environment enabled us to perform a mathematical analysis {{as a first step}} in obtaining a better theoretical understanding of the learning dynamics. In this talk I will discuss the theoretical analysis and its results. Moreover, I will focus on the practical application of RSLVQ to a real world dataset containing extracted features from facial expression data...|$|R
40|$|This thesis {{addresses}} {{the problem of}} visual object retrieval, where a user formulates a query to an image database by providing one or multiple examples of an object of interest. The presented techniques aim both at finding those images in the database that contain the object as well as locating the object in the image and segmenting it from the background. Every considered image, both the ones used as queries and the ones contained in the target database, is represented as a Binary Partition Tree (BPT), the hierarchy of regions previously proposed by Salembier and Garrido (2000). This data structure offers multiple opportunities and challenges when applied to the object retrieval problem. A first application of BPTs appears during the formulation of the query, when the user must interactively segment the query object from the background. Firstly, the BPT can assist in adjusting an initial marker, such as a scribble or bounding box, to the object contours. Secondly, BPT can also define a navigation path for the user to adjust an initial selection to the appropriate spatial scale. The hierarchical structure of the BPT is also exploited to extract {{a new type of}} visual words named Hierarchical Bag of Regions (HBoR). Each region defined in the BPT is described with a feature vector that combines a <b>soft</b> <b>quantization</b> on a visual codebook with an efficient bottom-up computation through the BPT. These descriptors allow the definition of a novel feature space, the Parts Space, where each object is located according to the parts that compose it. HBoR descriptors have been applied to two scenarios for object retrieval, both of them solved by considering the decomposition of the objects in parts. In the first scenario, the query is formulated with a single object exemplar which is to be matched with each BPT in the target database. The matching problem is solved in two stages: an initial top-down one that assumes that the hierarchy from the query is respected in the target BPT, and a second bottom-up one that relaxes this condition and considers region merges which are not in the target BPT. The second scenario where HBoR descriptors are applied considers a query composed of several visual objects. In this case, the provided exemplars are considered as a training set to build a model of the query concept. This model is composed of two levels, a first one where each part is modelled and detected separately, and a second one that characterises the combinations of parts that describe the complete object. The analysis process exploits the hierarchical nature of the BPT by using a novel classifier that drives an efficient top-down analysis of the target BPTs. Postprint (published version...|$|E
40|$|Many {{successful}} {{models for}} scene or object recognition transform low-level descriptors (such as Gabor filter re-sponses, or SIFT descriptors) into richer representations of intermediate complexity. This process {{can often be}} bro-ken down into two steps: (1) a coding step, which per-forms a pointwise transformation of the descriptors into a representation better adapted to the task, and (2) a pool-ing step, which summarizes the coded features over larger neighborhoods. Several combinations of coding and pool-ing schemes have been proposed in the literature. The goal {{of this paper is}} threefold. We seek to establish the rela-tive importance of each step of mid-level feature extrac-tion through a comprehensive cross evaluation of several types of coding modules (hard and <b>soft</b> vector <b>quantization,</b> sparse coding) and pooling schemes (by taking the aver-age, or the maximum), which obtains state-of-the-art per-formance or better on several recognition benchmarks. We show how to improve the best performing coding scheme by learning a supervised discriminative dictionary for sparse coding. We provide theoretical and empirical insight into the remarkable performance of max pooling. By teasing apart components shared by modern mid-level feature ex-tractors, our approach aims to facilitate the design of better recognition architectures. 1...|$|R
40|$|One of {{the popular}} methods for multiclass {{classification}} is Learning Vector Quantization (LVQ). There have been developed several variants of LVQ lately, among which Robust <b>Soft</b> Learning Vector <b>Quantization,</b> or RSLVQ for short. An introductory study showed that RSLVQ performs better than other LVQ algorithms, even {{very close to the}} optimal linear classifier, within a controlled environment. In order to study its performance in detail, we performed a mathematical analysis of the algorithm, {{in the form of a}} system of coupled Ordinary Differential Equations (ODE's), which might also help development of an optimal LVQ algorithm. Following from our analysis, we compare the potential performance of RSLVQ in relation to other LVQ variants and present a guideline for settings of the control parameter, i. e. the softness parameter. ...|$|R
40|$|Abstract—A {{discrete}} (binary-input 2 q-ary output) {{communication channel}} with memory is introduced to judiciously capture both the statistical {{memory and the}} soft-decision information of a time-correlated discrete fading channel (DFC) used with antipodal signaling and <b>soft</b> output <b>quantization</b> of resolution q. The discrete channel, which can be explicitly described via its binary input process and a 2 q-ary noise process, is shown to be symmetric, thus admitting a simple expression for its capacity when its noise is stationary ergodic. It is observed that considerable capacity gains can be achieved due to the channel’s memory {{and the use of}} as few as 2 bits for softdecision over interleaving the channel (to render it memoryless) and hard-decision demodulation (q = 1). The 2 q-ary noise process is next modeled via a queue-based (QB) ball-sampling mechanism to produce a mathematically tractable stationary ergodic Markovian noise source. The DFC is fitted by the QB noise model via an iterative procedure that minimizes the Kullback-Leibler divergence rate between the DFC and QB noise sources. Modeling results, measured in terms of channel noise correlation function and capacity reveal a good agreement between the two channels for a broad range of fading conditions. Index Terms—Channel capacity, finite state Markov channels, queue-based Markovian noise, quantization, Rayleigh fading channels, soft-decision. I...|$|R
40|$|This paper {{presents}} {{a framework for}} developing <b>soft</b> learning vector <b>quantization</b> (LVQ) and clustering algorithms by minimizing reformulation functions based on aggregation operators. An axiomatic approach provides conditions for selecting the subset of all aggregation operators that lead to admissible reformulation functions. For mean-type aggregation operators, the construction of admissible reformulation functions reduces to the selection of admissible generator functions. Nonlinear generator functions result in a broad family of soft LVQ and clustering algorithms, which include fuzzy LVQ and clustering algorithms as special cases. The formulation considered in this paper also provides the basis for exploring {{the structure of the}} feature set by identifying outliers in the data. The procedure described in this paper for identifying outliers in the feature set is tested on a set of vowel data...|$|R
40|$|We derive and analyse robust {{optimization}} {{schemes for}} noisy vector quantization {{on the basis}} of deterministic annealing. Starting from a cost function for central clustering that incorporates distortions from channel noise we develop a <b>soft</b> topographic vector <b>quantization</b> algorithm (STVQ) which is based on the maximum entropy principle and which performs a maximum-likelihood estimate in an expectationmaximization (EM) fashion. Annealing in the temperature parameter fi leads to phase transitions in the existing code vector representation during the cooling process for which we calculate critical temperatures and modes as a function of eigenvectors and eigenvalues of the covariance matrix of the data and the transition matrix of the channel noise. A whole family of vector quantization algorithms is derived from STVQ, among them a deterministic annealing scheme for Kohonen's self-organizing map (SOM). This algorithm, which we call SSOM, is then applied to vector quantization of image dat [...] ...|$|R
