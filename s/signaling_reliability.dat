6|221|Public
40|$|The {{purpose of}} this study is to develop and test a model to explain why {{auditors}} issue going-concern reports to companies that subsequently do not fail. Given the auditor's access to records and unique interaction with management and legal council during the audit, it seems plausible that the auditor' s going concern report can serve as a useful indicator or "signal" of the company's potential inability to maintain itself as a going-concern. However, before one can even discuss the usefulness of the going-concern audit report, it must be shown to be reliable. Developing a model to explain why the auditor's signal of a client's going-concern status subsequently proves to be unreliable should help financial statement users better assess the information value or usefulness of the going-concern audit report. In this study the likelihood that the auditor has sent a reliable signal of the company's subsequent economic status to financial statement users was hypothesized to be a function of the client and auditor related factors which affect the auditor's judgements and/or reporting decisions. The tests showed that the company's estimated probability of bankruptcy, an indicator of ambiguity and complexity in the auditor's decision-making environment, was a significant determinant of <b>signaling</b> <b>reliability.</b> In addition, audit technology was found to be significantly related to <b>signaling</b> <b>reliability,</b> with more structured auditing firms issuing reports which appear to be more reliable than the audit reports issued by less structured auditing firms. Finally, subsequent mitigating actions or events which are reported between the date of the audit report and the subsequent financial statements are significantly negatively associated with <b>signaling</b> <b>reliability</b> of the going-concern audit report. The remaining client and auditor-related factors [...] audit/client tenure, auditor's industry concentration ratio, and client size relative to the auditor's total client base are not significantly associated with the report's <b>signaling</b> <b>reliability.</b> However, the results suggest that these factors should be further examined...|$|E
40|$|Spatial {{heterogeneity}} is {{a hallmark}} of living systems, even at the molecular scale in individual cells. A key example is the partitioning of membrane-bound proteins via lipid domain formation or cytoskeleton-induced corralling. Yet {{the impact of this}} spatial heterogeneity on biochemical signaling processes is poorly understood. Here we demonstrate that partitioning improves the reliability of biochemical signaling. We exactly solve a stochastic model describing a ubiquitous motif in membrane signaling. The solution reveals that partitioning improves <b>signaling</b> <b>reliability</b> via two effects: it moderates the non-linearity of the switching response, and it reduces noise in the response by suppressing correlations between molecules. An optimal partition size arises from a trade-off between minimizing the number of proteins per partition to improve <b>signaling</b> <b>reliability</b> and ensuring sufficient proteins per partition to maintain signal propagation. The predicted optimal partition size agrees quantitatively with experimentally observed systems. These results persist in spatial simulations with explicit diffusion barriers. Our findings suggest that molecular partitioning is not merely a consequence of the complexity of cellular substructures, but also plays an important functional role in cell signaling. Comment: 32 pages, 14 figure...|$|E
40|$|Due to its complex nature, {{after-sales service}} support for {{systems such as}} {{aircraft}} engines is commonly outsourced to the vendor. A key input to contracting for such support is system reliability. When producttechnology is new, the vendor often knows more about reliability than the buyer, potentially leading to contractual inefficiencies. This paper examines the role of after-sales contracts as a mechanism for <b>signaling</b> <b>reliability.</b> We focus on two commonly encountered categories of contracts, the (generalized) Time and Material contract and the performance-based contract (PBC). We find that they differ substantially {{in their ability to}} signal reliability, achieve system efficiency, and allocate rents in the supply chain. While the vendor of a reliable product is always better-off with PBC, the same is not true for the buyer or the vendor of an unreliable product. This sheds new light on the on-going debate regarding the relative merits of the two contracting strategies. Key words: signaling games; performance-based contracting; aerospace sector; aftermarket; service operation...|$|E
40|$|University of Minnesota Ph. D. dissertation. December 2015. Major: Ecology, Evolution and Behavior. Advisor: David Stephens. 1 {{computer}} file (PDF); v, 116 pages. This research centers on two themes fundamental to communication, <b>signal</b> <b>reliability</b> and receiver tolerance of imperfect reliability (abbreviated as receiver tolerance). Focus on <b>signal</b> <b>reliability</b> tends to dominate research on signaler-receiver interactions, but represents {{only half of}} the signaling dyad. Understanding why signals are reliable and why receivers follow imperfect reliability are equally important; I argue the combination of reliability and receiver tolerance to ultimately determines the form and stability of signaler-receiver interactions. To explore these themes, I first developed a model of signaling interactions that combines aspects of models of receiver choice and <b>signal</b> <b>reliability.</b> The results highlight the co-importance of receiver tolerance and reliability enforcement mechanisms (such as signal cost). To experimentally test the model predictions, I developed a novel laboratory signaling game that allows control over theoretically important variables (such as the level of conflict between the signaler and receiver). The game placed blue jay subjects (Cyanocitta cristata) in a signal-response game played for food rewards. A series of these signaling-game experiments demonstrate the effects of signal cost on <b>signal</b> <b>reliability</b> (or honesty) and show the extent to which uncertainty in the environment generates receiver tolerance. Signal cost is an important topic in signaling theory, but lacks direct empirical support. I show that high signal cost does increase honesty under conditions of conflict, but also that cost is unnecessary in mutualistic conditions. I also show that receiver tolerance increases when environments are uncertain (to the point that receivers are gullible), and that signalers are sensitive to the level of receiver tolerance – exploiting tolerance when signaler and receiver interests conflict. Taken together, these models and experiments establish the value of considering both <b>signal</b> <b>reliability</b> and receiver tolerance...|$|R
40|$|Communication is an {{indispensable}} component of animal societies, yet many open questions remain regarding the factors affecting the evolution and <b>reliability</b> of <b>signalling</b> systems. A potentially important {{factor is the}} level of genetic relatedness between signallers and receivers. To quantitatively explore the role of relatedness in the evolution of reliable signals, we conducted artificial evolution over 500 generations in a system of foraging robots that can emit and perceive light signals. By devising a quantitative measure of <b>signal</b> <b>reliability,</b> and comparing independently evolving populations differing in within-group relatedness, we show a strong positive correlation between relatedness and reliability. Unrelated robots produced unreliable signals, whereas highly related robots produced signals that reliably indicated the location of the food source and thereby increased performance. Comparisons across populations also revealed that the frequency for signal production-which is often used as a proxy of <b>signal</b> <b>reliability</b> in empirical studies on animal communication-is a poor predictor of <b>signal</b> <b>reliability</b> and, accordingly, is not consistently correlated with group performance. This has important implications for our understanding of signal evolution and the empirical tools that are used to investigate communication...|$|R
50|$|With {{multiple}} signals {{there is}} a greater processing demand placed on the receiver, which can lead to tighter design requirements of the base station. Typically, however, <b>signal</b> <b>reliability</b> is paramount and using multiple antennas is an effective way to decrease the number of drop-outs and lost connections.|$|R
40|$|The extant {{literature}} on after-sales service support suggests an implicit consensus around the (Pareto) superiority of performance-based contracts (PBC) over traditional time-and-material contracts. However, when product technology is new, a {{setting in which}} the vendor possesses superior information on product reliability, there is compelling {{evidence to suggest that}} buyers prefer the traditional contracts. We undertake the first investigation into the role of after-sales contracts as a mechanism for <b>signaling</b> <b>reliability.</b> We find that while both PBC and the traditional contracts allow perfect signaling and coordinate the after-sales supply chain, only PBC allow the vendor to appropriate all rents. When the choice of contract class is endogenous, we show that this observation provides a formal explanation for the buyers ’ observed contractual preference. We also analyze the interaction of asymmetric information with moral hazard and find that PBC lead to overinvestment in inventory; we propose a contractual innovation that recovers efficiency. Key words: signaling games; performance-based contracting; aerospace sector; aftermarket; service operation...|$|E
40|$|Hedgehog {{signaling}} plays conserved {{roles in}} controlling embryonic development; its dysregulation has {{been implicated in}} many human diseases including cancers. Hedgehog signaling has an unusual reception system consisting of two transmembrane proteins, Patched receptor and Smoothened signal transducer. Although activation of Smoothened and its downstream signal transduction have been intensively studied, less is known about how Patched receptor is regulated, and particularly how this regulation contributes to appropriate Hedgehog signal transduction. Here we identified a novel role of Smurf E 3 ligase in regulating Hedgehog signaling by controlling Patched ubiquitination and turnover. Moreover, we showed that Smurf-mediated Patched ubiquitination depends on Smo activity in wing discs. Mechanistically, we found that Smo interacts with Smurf and promotes it to mediate Patched ubiquitination by targeting the K 1261 site in Ptc. The further mathematic modeling analysis reveals that a bidirectional control of activation of Smo involving Smurf and Patched is important for signal-receiving cells to precisely interpret external signals, thereby maintaining Hedgehog <b>signaling</b> <b>reliability.</b> Finally, our data revealed an evolutionarily conserved role of Smurf proteins in controlling Hh signaling b...|$|E
40|$|Controlled {{improvement}} in the reliability and security of any system requires a comprehensive analysis. This requires the systematic identification of the fundamental underlying components of the system using a rigorous discipline. If successful, this process will illuminate areas for concern and identify areas for potential system enhancements. Such comprehensive analysis can be conducted for communications infrastructure using a framework of eight ingredients. This paper will explore these eight ingredients and identify their usage in vulnerability analysis and best practice identification for enhancing the reliability and security of communications infrastructure. © 2006 Lucent Technologies Inc. re-chartered NRIC to focus on various areas of concern, beginning with network reliability and subsequently on network <b>signaling</b> <b>reliability,</b> Y 2 K preparedness, packet-switched networks, homeland security, and emergency services. This is shown in Figure 2. Network reliability, interoperability, and security recommendations {{in the form of}} NRIC best practices (BPs) have been developed by communications ex-perts for use within the industry. Prior to NRIC V, best practices were developed from an historic analogy perspective. Analysis of previous network outages by industry experts was used to identify best practices to address these past events (i. e., network outages). Starting with NRIC V, development of BPs has been refined and extended, based on the NRIC charter, by leveraging a systematic and rigorous process that analyzes not only past events, but includes looking a...|$|E
40|$|We {{investigate}} the theoretically proposed link between judgmental overconfidence and trading activity. In addition to applying classical measures of miscalibration, we introduce {{a measure to}} capture misperception of <b>signal</b> <b>reliability,</b> which is the relevant bias in the theoretical overconfidence literature. We relate the obtained overconfidence measures to trading activity in call and continuous experimental asset markets. Our results confirm prior findings that classical miscalibration measures {{are not related to}} trading activity. However, misperception of <b>signal</b> <b>reliability</b> is significantly linked to trading volume, particularly in the continuous market. In addition, we find that men trade more than women at high levels of risk aversion, but the gender trading gap vanishes as risk aversion lessens. The reason is that the trading activity of women seems to be more sensitive to risk attitudes than that of men...|$|R
40|$|The paper {{discusses}} {{two methods}} {{to evaluate the}} <b>signal</b> <b>reliability</b> of the output of logical circuits. It is known that faults present in a circuit will not always cause {{the output of the}} circuit to be incorrect. The first method evaluates the contribution of each fault to the reliability of the circuit and requires the enumeration of the behavior of each fault in the entire fault set. The use of McCluskey and Clegg's characterization of faulty networks by evaluating the functional equivalence classes of the network is a way {{to reduce the amount of}} computation involved. The second method uses a probabilistic model of logical circuits and consists of straightforward operations which can easily be automated. The method also yields the <b>signal</b> <b>reliability</b> and has the capability of very easily specifying the individual fault probabilities of all the circuit lines independently...|$|R
40|$|To {{meet the}} demand for high speed data, {{wireless}} cellular system technology has grown in a steady pace. However, the wireless signals are still vulnerable to the multipath fading, shadowing and path loss, making the communication less reliable. Cooperative relay is a techniques to improve <b>signal</b> <b>reliability</b> by introducing a an additional node between source terminal and destination terminal to provide redundant path for data transmission. However, existing work of cooperative relay investigate performance through theoretically simulation only. The real world performance remains unknown because the lack of prototype for field testing and measurement. The focus of this work is therefore to implement the cooperative relay prototype using Universal Software Radio Peripheral (USRP) and LabVIEW platform. The relay prototype based on Amplify-and-Forward (AF) protocol has been developed. The performance in terms of bit error rate (BER) of the cooperative relay link is compared with the direct link without relay. The measurement is {{carried out in the}} indoor environment. Measurement results show that the cooperative relay significantly improves the <b>signal</b> <b>reliability</b> and extends the coverage distance if compared to direct communication without relay...|$|R
50|$|During {{practice}} missions, ground vans with telecommunication {{receiver equipment}} {{were used to}} test the <b>signal</b> and <b>reliability</b> of USCINCEUR ABNCP transmitting equipment.|$|R
50|$|Inherently {{an antenna}} {{diversity}} scheme requires additional hardware and integration versus a single antenna system {{but due to}} the commonality of the signal paths {{a fair amount of}} circuitry can be shared. Also with the multiple signals there is a greater processing demand placed on the receiver, which can lead to tighter design requirements. Typically, however, <b>signal</b> <b>reliability</b> is paramount and using multiple antennas is an effective way to decrease the number of drop-outs and lost connections.|$|R
40|$|The central {{question}} in communication theory is whether communication is reliable, and if so, which mechanisms select for reliability. The primary {{approach in the}} past has been to attribute reliability to strategic costs associated with signalling as predicted by the handicap principle. Yet, reliability can arise through other mechanisms, such as signal verification; but the theoretical understanding of such mechanisms has received relatively little attention. Here, we model whether verification can lead to reliability in repeated interactions that typically characterize mutualisms. Specifically, we model whether fruit consumers that discriminate among poor- and good-quality fruits within a population can select for reliable fruit signals. In our model, plants either signal or they do not; costs associated with signalling are fixed and independent of plant quality. We find parameter combinations where discriminating fruit consumers can select for <b>signal</b> <b>reliability</b> by abandoning unprofitable plants more quickly. This self-serving behaviour imposes costs upon plants as a by-product, rendering it unprofitable for unrewarding plants to signal. Thus, strategic costs to signalling are not a prerequisite for reliable communication. We expect verification to more generally explain <b>signal</b> <b>reliability</b> in repeated consumer-resource interactions that typify mutualisms but also in antagonistic interactions such as mimicry and aposematism...|$|R
40|$|One of {{the many}} {{questions}} to which John Maynard Smith contributed was that of why most animal signals are reliable. He initially rejected the handicap argument but gradually accepted it, a process I briefly describe. This episode illustrated his preference for mathematical models over verbal ones, and the generosity with which he could change his mind. Even after accepting that some signals are reliable because of their strategic costs, he argued for a pluralistic approach to <b>signal</b> <b>reliability.</b> <b>Signal</b> complexity was a developing interest when he died. Signals usually involve several components, some of which appear to amplify other signal components. The terms amplifier and index require more thought to reduce the scope for semantic confusion. I conclude by describing Maynard Smith's fascination with peacocks Pavo cristatus...|$|R
5000|$|DCTCP modifies the TCP {{receiver}} to always relay the exact ECN marking of incoming packets {{at the cost}} of ignoring a function that is meant to preserve <b>signalling</b> <b>reliability.</b> This makes a DCTCP sender vulnerable to loss of ACKs from the receiver, which it has no mechanism to detect or cope with. , algorithms that provide equivalent or better receiver feedback in a more reliable approach are an active research topic, and one experimental proposal is known as [...] "More accurate ECN feedback in TCP" [...] (Accurate ECN).|$|R
40|$|Abstract. There {{has been}} a rapid {{rise in the number}} of {{publications}} using functional near infrared spectroscopy (fNIRS) for human developmental research over the past decade. However test–retest reliability of this measure of brain activation in infants remains unknown. To assess this, we utilized data from a longitudinal cohort who participated in an fNIRS study on social perception at two age points. Thirteen infants had valid data from two sessions held 8. 5 months apart (4 to 8 months and 12 to 16 months). Inter- and intrasession fNIRS test–retest reliability was assessed at the individual and group levels using the oxyhemoglobin (HbO 2) signal. Infant compliance with the study was similar in both sessions (assessed by the proportion of time infants looked to the stimuli), and there was minimal discrepancy in sensor placement over the targeted area between sessions. At the group level, good spatial overlap of significant responses and <b>signal</b> <b>reliability</b> was seen (spatial overlap was 0. 941 and average signal change within an region of interest was r= 0. 896). At participant level, spatial overlap was acceptable (> 0. 5 on average across infants) although <b>signal</b> <b>reliability</b> varied between participants. This first study of test–retest reliability of fNIRS in infants shows encouraging results, particularly for group-based analysis...|$|R
40|$|In modern {{telecommunication}} technologies, {{the requirement}} for <b>signal</b> <b>reliability</b> is higher and higher but fading is the main challenge for <b>signal</b> <b>reliability.</b> Different types of techniques have been studied to mitigate this fading but MIMO (Multiple Input Multiple Output) has been studied extensively in wireless communication systems to overcome small-scale fading, which is {{an efficient way to}} improve signal-to-noise and bit error rates. In this thesis, all works were operated at 2. 45 GHz. Planar-Inverted F antenna (PIFA) is used for mobile phone due to its low profile and high gain. In this thesis, two PIFAs are used for antenna diversity. All the simulation of the antennas was performed in High Frequency Structure Simulator (HFSS). Advanced Design system (ADS) is used for Wilkinson combiner design and simulation and overall layout design for PCB fabrication. Phase shifters are used to change the phase of each input signals. All measurements have been done in both reverberation chamber and office environment and the two results are different. Office environment measurements have been done in PCB lab at Linköping University and reverberation measurements have been done at SP Technical Research Institute of Sweden. Finally a conclusion was drawn about the performance of this thesis...|$|R
40|$|LORAN-C {{a highly}} {{accurate}} radio navigation positioning system which operates at an assigned frequency of 10 kHz, and provides phase-coded pulses to develop hyperbolic time-difference lines-of-position (LOP's) was evaluated. LORAN-C provides precise {{time and time}} interval to within plus or minus 5 microseconds of UTC. The steps taken to plan, install, operate, and maintain the LORAN-C system up to the year 2000 are discussed. Topics included in the discussion were: theory of operation, timing, chain lanning, group repetition interval, coding delay versus emission delay, chain calibration, chart verification, system accuracy, <b>signal</b> <b>reliability,</b> and future developments...|$|R
25|$|Since around 2010 a {{nationwide}} digital radio communication standard is implemented. This {{is based on}} the Terrestrial Trunked Radio (TETRA) standards. Main advances over the old analog radio system are availability of far more channels and communication groups, encryption possibilities, noise filtering and enhanced <b>signal</b> <b>reliability.</b> To cover whole Germany about 4500 base stations are needed. As of August 2015 already 4338 of them are installed and 4323 working, thus about 97% of Germany is covered. Migration to the new radio standard is ongoing step by step, parallel use of the analog system is planned until around 2020.|$|R
2500|$|By way of explanation, it {{has been}} {{proposed}} that at a relatively late stage in human evolution, our ancestors' hands became so much in demand for making and using tools that the competing demands of manual gesturing became a hindrance. The transition to spoken language {{is said to have}} occurred only at that point. Since humans throughout evolution have been making and using tools, however, most scholars remain unconvinced by this argument. (For a different approach to this puzzle— one setting out from considerations of <b>signal</b> <b>reliability</b> and trust— see [...] "from pantomime to speech" [...] below).|$|R
40|$|International audienceA {{prerequisite}} to longitudinal fMRI studies in schizophrenia is the knowledge on fMRI <b>signal</b> <b>reliability</b> in schizophrenia patients. We assessed the reproducibility of activations elicited by two fMRI sessions, which were 21 months apart, {{of a story}} listening paradigm in 10 schizophrenia patients and 10 healthy subjects. In both groups, we observed {{a high degree of}} spatial overlap of activation maps as well as a good reproducibility of signal variations assessed on a voxel-wise basis in temporal areas underlying early stages of language processing. Task performance, assessed through a comprehension questionnaire, had no impact on the activation reproducibility...|$|R
40|$|Parametric {{variations}} in exploratory movements influence signal integration and <b>signal</b> <b>reliability</b> in active shape perception When sliding a finger across a bump on a surface, the finger follows {{the geometry of}} the bump (position signal). At the same time, forces related to {{the slope of the}} bump decelerate and accelerate the finger (force signal) [1]. Consistent with the Maximum Likelihood Estimate (MLE) model [2] haptically perceived shape can be described by the weighted average of the shape signaled by the force and the position signal [3, 4]. Here we investigated – for the haptic perception of bump amplitude – th...|$|R
40|$|Abstract. In {{this paper}} {{we argue that}} for brain-computer {{interfaces}} (BCIs) to be used reliably for extended periods of time, they {{must be able to}} adapt to the user’s evolving needs. This adaptation should not only be a function of the environmental (external) context, but should also consider the internal context, such as cognitive states and brain <b>signal</b> <b>reliability.</b> We demonstrate two successful approaches to modulating the level of assistance: by using online task performance metrics; and by monitoring the reliability of the BCI decoders. We then describe how these approaches could be fused together, resulting in a more user-centred solution...|$|R
5000|$|By way of explanation, it {{has been}} {{proposed}} that at a relatively late stage in human evolution, our ancestors' hands became so much in demand for making and using tools that the competing demands of manual gesturing became a hindrance. The transition to spoken language {{is said to have}} occurred only at that point. Since humans throughout evolution have been making and using tools, however, most scholars remain unconvinced by this argument. (For a different approach to this puzzle — one setting out from considerations of <b>signal</b> <b>reliability</b> and trust — see [...] "from pantomime to speech" [...] below).|$|R
50|$|Since around 2010 a {{nationwide}} digital radio communication standard is implemented. This {{is based on}} the Terrestrial Trunked Radio (TETRA) standards. Main advances over the old analog radio system are availability of far more channels and communication groups, encryption possibilities, noise filtering and enhanced <b>signal</b> <b>reliability.</b> To cover whole Germany about 4500 base stations are needed. As of August 2015 already 4338 of them are installed and 4323 working, thus about 97% of Germany is covered. Migration to the new radio standard is ongoing step by step, parallel use of the analog system is planned until around 2020.|$|R
40|$|Dedicated {{short-range}} communications (DSRC) are one-way or two-way from short-range to medium-range {{wireless communication}} channels {{specifically designed to}} push the intelligent transportation system into our daily life. The DSRC standard generally uses FM 0 and Manchester codes which to reach dc-balance, enhancing the <b>signal</b> <b>reliability.</b> Generally the code word structure of FM 0 encoding and Manchester encoding are different, thus limiting the hardware potential of existing DSRC systems. In this paper, the Finite State Machine (FSM) of FM 0 code is constructed and from the FSM, the architecture of FM 0 encoding is developed and then the combine hardware architecture of FMO and Manchester encoding is constructed...|$|R
40|$|It {{has been}} {{suggested}} that the first steps in visual processing strive to compress as much information as possible about the outside world into the limited dynamic range of the visual channels. Here I compare measured neural images with theoretical calculations based on maximizing information, taking into account the statistical structure of natural images. Neural images were obtained by scanning an image while recording from a second-order neuron in the fly visual system. Over a 5. 5 -log-units-wide range of mean intensities, experiment and theory correspond well. At high mean intensities, redundancy in the image is reduced by spatial and temporal antagonism. At low mean intensities, spatial and temporal low-pass filtering combat noise and increase <b>signal</b> <b>reliability.</b> ...|$|R
40|$|The {{wireless}} {{market has}} been growing at a tremendous speed and an ever-increasing number of radio-frequency systems are carried around by people. Therefore, the World Health Organization has indicated {{the need for an}} RF exposure assessment to describe the public electromagnetic environment. Current portable RF dosimeters suffer from a lack of wearability due to their size and rigidity. Moreover, they experience low <b>signal</b> <b>reliability</b> due to shadowing of the body. This paper introduces a personal distributed exposimeter, developed with off-the-shelf components. The system contains ten wearable nodes. Each node comprises RF transceivers that measure the received signal strength in the mobile communication and Wi-Fi channels. Two different topologies to acquire the radiation power are discussed. status: publishe...|$|R
40|$|PEGASO is a FP 7 -funded project whose goal is {{to develop}} an ICT and mobile-based {{platform}} together with an appropriate strategy to tackle the diffusion of obesity and other lifestyle-related illnesses among teenagers. Indeed, the design of an engaging strategy, leveraging a complementary set of technologies, is the approach proposed by the project to promote the adoption of healthy habits such as active lifestyle and balanced nutrition and to effectively counter-fight the emergence of overweight and obesity in the younger population. A technological key element of such a strategy sees the adoption of wearable sensors to monitor teenagers’ activities, {{which is at the}} basis of developing awareness about the current lifestyle. This paper describes the experience carried out in the framework of the PEGASO project in developing and evaluating wearable monitoring systems addressed to adolescents. The paper describes the methodological approach based on the co-designing of such a wearable system and the main results that, in the first phase, involved a total of 407 adolescents across Europe in a series of focus groups conducted in three countries for the requirements definition phase. Moreover, it describes an evaluation process of <b>signal</b> <b>reliability</b> during the usage of the wearable system. The main results described here are: (a) a prototype of the standardized experimental protocol that has been developed and applied to test <b>signal</b> <b>reliability</b> in smart garments; (b) the requirements definition methodology through a co-design activity and approach to address user requirements and preferences and not only technological specifications. Such co-design approach is able to support a higher system acceptance and usability together with a sustained adoption of the solution with respect to the traditional technology push system development strategy...|$|R
40|$|This report {{presents}} {{information from}} the ANS Criticality Alarm System Workshop relating to the consensus standard requirements and guidance. Topics presented include: definition; nomenclature; requirements and recommendations; purpose of criticality alarms; design criteria; <b>signal</b> characteristics; <b>reliability,</b> dependability and durability; tests; and emergency preparedness and planning...|$|R
40|$|We {{outline the}} notion of trust and two formal models {{developed}} to solve the trust problem in multi-agent systems. The limitations of these models, and their similarities with classical AI models (which stress centrally-stored representations of the world) are examined. We then consider trust as a Distributed Cognition problem, and suggest an agent design framework, inspired by Distributed Cognition. A distributed model of trust is then developed, extending work by Bacharach and Gambetta on trust in signs, and based on Zahavi’s Handicap Principle (a theory of animal signaling that emphasizes the role of costs in ensuring <b>signal</b> <b>reliability).</b> We apply this model to agent systems to suggest a programming language that can act as an institution to partially solve the trust problem...|$|R
40|$|I {{outline the}} notion of trust and the two major formal models {{developed}} to solve the trust problem in multi-agent systems. The limitations of these models, and their similarities with head-centered AI models are examined. Trust is then considered as a distributed cognition problem, and a design framework inspired by a facet of distributed cognition is suggested to tackle it. A distributed model of trust is then developed, extending work by Bacharach and Gambetta on trust in signs, and based on Zahavi’s Handicap Principle (a theory of animal signaling that emphasizes the role of costs in ensuring <b>signal</b> <b>reliability).</b> This model is then applied to agent systems to suggest a model programming language, which can act as an institution to partially solve the trust problem...|$|R
40|$|IT {{has been}} {{suggested}} 1 - 3 that the first steps in visual processing strive to compress {{as much information as}} possible about the outside world into the limited dynamic range of the visual channels. Here I compare measured neural images with theoretical calculations based on maximizing information, taking into account the statistical structure of natural images. Neural images were obtained by scanning an image while recording from a second-order neuron in the fly visual system. Over a 5. 5 -log-units-wide range of mean intensities, experiment and theory correspond well. At high mean intensities, redundancy in the image is reduced by spatial and temporal antagonism. At low mean intensities, spatial and temporal low-pass filtering combat noise and increase <b>signal</b> <b>reliability...</b>|$|R
40|$|This paper {{proposes a}} novel hybrid downlinkuplink {{cooperative}} NOMA (HDU-CNOMA) scheme {{to achieve a}} better tradeoff between spectral efficiency and <b>signal</b> reception <b>reliability</b> than the conventional cooperative NOMA schemes. In particular, the proposed scheme enables the strong user to perform a cooperative transmission and an interference-free uplink transmission simultaneously during the cooperative phase, {{at the expense of}} a slightly decrease in <b>signal</b> reception <b>reliability</b> at the weak user. We analyze the outage probability, diversity order, and outage throughput of the proposed scheme. Simulation results not only confirm the accuracy of the developed analytical results, but also unveil the spectral efficiency gains achieved by the proposed scheme over a baseline cooperative NOMA scheme and a non-cooperative NOMA scheme. Comment: 7 pages, accepted for presentation at the IEEE VTC 2017 Spring, Sydney, Australi...|$|R
