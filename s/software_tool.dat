8150|10000|Public
25|$|The Classification Tree Editor (CTE) is a <b>software</b> <b>tool</b> for {{test design}} that {{implements}} the classification tree method.|$|E
25|$|While {{the method}} {{can be applied}} using a pen and a paper, the usual way {{involves}} the usage of the Classification Tree Editor, a <b>software</b> <b>tool</b> implementing the classification tree method.|$|E
25|$|Mark S. Ghiorso (born October 21, 1954) is an American geochemist who {{resides in}} Seattle, Washington. He {{is best known}} for {{creating}} MELTS, a <b>software</b> <b>tool</b> for thermodynamic modeling of phase equilibria in magmatic systems.|$|E
50|$|IEC 61499-2 defines {{requirements}} for <b>software</b> <b>tools</b> to be compliant to IEC 61499. This includes {{requirements for}} the representation and the portability of IEC 61499 elements {{as well as a}} DTD format to exchange IEC 61499 elements between different software tools.There are already some IEC 61499 compliant <b>software</b> <b>tools</b> available. Among these are commercial <b>software</b> <b>tools,</b> open-source <b>software</b> <b>tools,</b> and academic and research developments. Usually an IEC 61499 compliant runtime environment and an IEC 61499 compliant development environment is needed.|$|R
40|$|The {{thesis is}} aimed at the {{modeling}} of releases of hazardous substances to the designated <b>software</b> <b>tools.</b> The work consists of three parts. The first part is theoretical, which deals with hazardous substances and their legislation, accidents in industry, physical models of leakage and dispersion of hazardous substances and factors influencing their spread {{in the air and}} finally <b>software</b> <b>tools.</b> The second part, experimental, describes some <b>software</b> <b>tools</b> and their subsequent modeling. The third part is devoted to the results of modeling and analysis of selected <b>software</b> <b>tools.</b> The aim of the thesis is to analyze selected <b>software</b> <b>tools</b> for modeling the release of hazardous substances, to compare their explanatory power and potential use...|$|R
40|$|Personal {{information}} management (PIM) raises some problems because the <b>software</b> <b>tools</b> {{that are used}} don’t adequately respond to user behaviour and activities. The concept and personal {{information management}} problems are reviewed, along with the <b>software</b> <b>tools</b> used for this task. A qualitative assessment explores whether an approach based on <b>software</b> <b>tools</b> for conceptual maps can address the problems discussed...|$|R
25|$|AnyLogic {{does not}} include a {{specific}} library for supply chain simulation, as The AnyLogic Company converted its development efforts for this domain in a separate <b>software</b> <b>tool</b> – anyLogistix. This spin-off product was introduced in 2014 as AnyLogic Logistics Network Manager and was renamed anyLogistix in 2015.|$|E
25|$|Research on U.S. roundabouts {{sponsored}} by the Transportation Research Board (TRB) and Federal Highway Administration (FHWA) culminated in a capacity model that {{was included in the}} Highway Capacity Manual (HCM) Edition 6 and the TRB-FHWA Roundabout Informational Guide (NCHRP Report 672). The HCM Edition 6 model is based on lane-based gap-acceptance theory. A recent NCHRP survey of US state transport agencies found that Sidra Intersection is the most widely used <b>software</b> <b>tool</b> in the USA for roundabout analysis.|$|E
25|$|SBML can encode models {{consisting}} of entities (called species in SBML) acted upon by processes (called reactions). An important principle is that models are decomposed into explicitly-labeled constituent elements, {{the set of}} which resembles a verbose rendition of chemical reaction equations (if the model uses reactions) together with optional explicit equations (again, if the model uses these); the SBML representation deliberately does not cast the model directly into a set of differential equations or other specific interpretation of the model. This explicit, modeling-framework-agnostic decomposition {{makes it easier for}} a <b>software</b> <b>tool</b> to interpret the model and translate the SBML form into whatever internal form the tool actually uses.|$|E
40|$|This {{bachelor}} thesis {{deals with}} a comparison between two selected data mining <b>software</b> <b>tools,</b> LISp-Miner, developed {{at the department of}} information and knowledge engineering at the faculty of informatics and statistics of the University of Economics, Prague, and Rapidminer, a globally popular software suite. The focus of the comparison is mining for association rules. The aim of this work is first to provide a user-oriented evaluation of how the <b>software</b> <b>tools</b> compare in the selected area and second to attempt to discover some interesting differences between the results of how the <b>software</b> <b>tools</b> implement association mining procedures. To reach these goals, the <b>software</b> <b>tools</b> will be tested and evaluated by the author according to a selected set of criteria, grouped into 3 categories: functionality, usability and performance. The work is structured as follows: chapter 1 sets out some theoretical background and introduces the <b>software</b> <b>tools,</b> chapter 2 evaluates the available functionality of the tools for various steps of the overall association mining procedure, chapter 3 rates the <b>software</b> <b>tools</b> based on their usability and user-friendliness, while chapter 4 summarises the results of testing the <b>software</b> <b>tools</b> on a selected data set...|$|R
40|$|Abstract—Using {{automated}} <b>software</b> <b>tools</b> {{is essential}} for successful planning and managing of projects. Many automated <b>software</b> <b>tools</b> have been developed in the industry. The literature on how to select the appropiate project management <b>software</b> <b>tools</b> is quite limited. This paper provides a comparison {{of a set of}} project management <b>software</b> <b>tools</b> (PMST). In this study, first, we developed criteria to determine which PMSTs would be subject to our analysis. Then, we developed criteria to compare and evaluate these PMSTs. Finally, we present our findings in a tabular format. Our findings will help project managers to assess {{the strengths and weaknesses of}} these tools...|$|R
5000|$|Moreover, a few <b>software</b> <b>tools</b> already exist, which aim for {{predicting}} the toxicity of a substance or {{even try to}} simulate the virtual patient (Entelos). A few of these <b>software</b> <b>tools</b> are listed below: ...|$|R
25|$|The APSF is a {{non-profit}} independent organisation founded in 1989 for anaesthesia error monitoring, and expanded to patient incident {{reporting and monitoring}} after results from the Quality in Australian Health Care Study (QAHCS) in 1995 prompted reaction from the public. Adverse medical events, both sentinel events (patient death and injury) and near misses (medical errors with potential harm), are reported and analyzed through its subsidiary, Patient Safety International (PSI), using a <b>software</b> <b>tool,</b> the Advanced Incident Management System (AIMS). AIMS is used in over half of Australia's hospitals, and was adopted in 2005 by the New Zealand Accident Compensation Corporation and the University of Miami Medical Group in Florida. Data remains confidential is protected from legal discovery under Australian Commonwealth Quality Assurance legislation. Patient safety information is provided by electronic newsletters.|$|E
500|$|Choosing a <b>software</b> <b>tool</b> for {{structural}} alignment {{can be a}} challenge due to the large variety of available packages that differ significantly in methodology and reliability. A partial solution to this problem was presented in [...] and made publicly accessible through the ProCKSI webserver. A more complete list of currently available and freely distributed structural alignment software can be found in structural alignment software.|$|E
500|$|Paxata {{refers to}} its suite of {{cloud-based}} data quality, integration, enrichment and governance products as [...] "Adaptive Data Preparation." [...] The software {{is intended for}} business analysts, who need to combine data {{from a variety of}} sources, then check the data for duplicates, empty fields, outliers, trends and integrity issues before conducting analysis or visualization in a third-party <b>software</b> <b>tool.</b> It uses algorithms and machine-learning to automate certain aspects of data preparation. For example, it may automatically detect records belonging to the same person or address, even if the information is formatted differently in each record in different data sets.|$|E
40|$|Abstract: The paper {{describes}} {{principles of}} the construction of <b>software</b> <b>tools</b> providing the control of streaming data in the environment of FLASH. These <b>software</b> <b>tools</b> are able to support virtual modification of FLV files broadcasted from different servers using the method of progressive downloading as well as using RTMP protocol. The questions of the creation the version of such <b>software</b> <b>tools</b> purposed for working with mobile devices are also discussed. Note: Publication language:russia...|$|R
40|$|It is {{increasingly}} important for researchers and practitioners {{to be familiar}} with methods and <b>software</b> <b>tools</b> for analyzing large data sets, formulating and solving large-scale mathematical optimization models, and sharing solutions using interactive media. Unfortunately, advanced <b>software</b> <b>tools</b> are seldom included in curricula of graduate-level operations research (OR) and analytics programs. We describe a course consisting of eight three-hour modules intended to introduce Master’s and PhD students to advanced <b>software</b> <b>tools</b> fo...|$|R
40|$|The {{development}} of applications for monitoring, control, simulation and diagnosis in the petroleum industry involves {{a multitude of}} complex <b>software</b> <b>tools.</b> These tools have their own formalisms, semantics and use different abstractions to represent the system under development. They use different data formats to represent data in the <b>software</b> <b>tools.</b> Each application requires coupling {{of two or more}} different such complex <b>software</b> <b>tools.</b> Providing efficient interaction between these complex <b>software</b> <b>tools</b> using different abstractions, formalisms, data formats, etc. becomes a mammoth task. Thus {{there is a need to}} provide a unified environment that allows capturing the desired application and provide a framework for interaction between the necessary <b>software</b> <b>tools.</b> This paper discusses the formal metamodels to describe the individual formalisms in the desired unified environment. These metamodels, created by the Generic Modeling Environment (GME), define the domain-specific modeling language for application development in the petroleum industry. ...|$|R
500|$|A {{technique}} {{common in}} cut scenes of video games, scripting consists of giving precise {{directions to the}} game engine. A filmmaker can work alone this way, as J. Thaddeus [...] "Mindcrime" [...] Skubis did in creating the nearly four-hour The Seal of Nehahra (2000), the longest work of machinima at the time. However, perfecting scripts can be time-consuming. Unless what-you-see-is-what-you-get (WYSIWYG) editing is available, as in , changes {{may need to be}} verified in additional runs, and non-linear editing may be difficult. In this respect, Kelland, Morris, and Lloyd compare scripting to stop-motion animation. Another disadvantage is that, depending on the game, scripting capabilities may be limited or unavailable. Matinee, a machinima <b>software</b> <b>tool</b> included with Unreal Tournament 2004, popularized scripting in machinima.|$|E
500|$|Timescales for ice cores {{from the}} same {{hemisphere}} can usually be synchronised using layers that include material from volcanic events. [...] It {{is more difficult to}} connect the timescales in different hemispheres. [...] The Laschamp event, a geomagnetic reversal about 40,000 years ago, can be identified in cores; away from that point, measurements of gases such as [...] (methane) can be used to connect the chronology of a Greenland core (for example) with an Antarctic core. [...] In cases where volcanic tephra is interspersed with ice, it can be dated using argon/argon dating and hence provide fixed points for dating the ice. [...] Uranium decay has also been used to date ice cores. [...] Another approach is to use Bayesian probability techniques to find the optimal combination of multiple independent records. [...] This approach was developed in 2010 and has since been turned into a <b>software</b> <b>tool,</b> DatIce.|$|E
500|$|Pacific City {{within the}} game {{consists}} of 495 [...] "city blocks" [...] which the player could travel among, according to Microsoft Game Studios' Jami Johns. Each block {{had to be}} tested separately, so Microsoft Game Studios designed a <b>software</b> <b>tool</b> to track issues when the game was in testing. For example, the tool was able to identify blocks where the performance dropped or the game crashed, allowing the developers to redesign the area to remove the issues. A further tool {{was used for the}} [...] "seams" [...] between city blocks, and included a screenshot just prior to any problem, which significantly reduced the debugging time for the game; this tool was further used with Forza Motorsport 2. However, the team had found some bugs during testing that actually worked well as game mechanics without throwing off the game balance. [...] For example, the ability to drive the Agency SUV up a vertical wall when the player has maxed out his driving skill was originally a bug within the game.|$|E
5000|$|SCCS (released in 1972) and DSEE (considered a {{predecessor}} of [...] Atria ClearCase) are {{two other}} relatively well-known ground-breaking VCS <b>software</b> <b>tools.</b> These tools are generally considered {{the first generation of}} VCS as automated <b>software</b> <b>tools</b> back in times.|$|R
40|$|MPLS. ?????????? ???????? ????????? ????????? ?? ???????????? ??????? ?? ??????????, ?????????????? ????????, ?? ????????? ????????? ??? ????????? ?????? ????? ????????????: ?????? ?????????? ????????????? ???????, ????????? ???????, ??????? ?? ??????????? ????????? ?? ???????????? ??????? ????? MPLS. ?????????? ?????? ?? ????????? ?????????? ????????? ?????????? MPLS. ???????? ?????????? ????????????????? ?????????? ??????????. <b>Software</b> <b>tools</b> for MPLS {{networks}} design are elaborated {{and presented}} in this work. The <b>software</b> <b>tools</b> are based on original methods and algorithms suggested by authors and include algorithms for solution of numerous tasks of channels capacities assignment, flows distribution, survivability analysis and structural synthesis. The elaborated models and algorithms {{take into account the}} specificity of MPLS technology. The results of experimental investigations and practical implementation of the suggested <b>software</b> <b>tools</b> are presented...|$|R
40|$|We {{describe}} new and freely available <b>software</b> <b>tools</b> {{for measuring}} volumes in subregions of the brain. The method is fast, flexible, and employs well-studied techniques {{based on the}} Talairach-Tournoux atlas. The <b>software</b> <b>tools</b> are released as plug-ins for MIPAV, a freely available and user-friendly image analysis software package developed by the National Institutes of Health. Our <b>software</b> <b>tools</b> include a digital Talairach atlas that consists of labels for 148 different substructures of the brain at various scales...|$|R
500|$|Concerning tracks {{related to}} {{times of day}} and {{elements}} such as battle themes, Shimomura allowed the flow of gameplay to dictate the flow of her music, which required putting {{in a number of}} natural transition points within pieces so the break would be smooth. To help with this, a special <b>software</b> <b>tool</b> called MAGI (Music API for Gaming Interaction) was developed to manage the tempo and create sync points that allowed the music's flow to be interactive while keeping its [...] "epic" [...] sound and scope. When the game began development as Versus XIII it used the same sound environment as Final Fantasy XIII, but with the move to new hardware the sound team needed to design a new sound environment. To develop the sound software, the team partnered with CRI Middleware to integrate the company's CRI ADX 2 middleware into the game's native sound environment. The amount of sound data, which also included 300 tracks of standalone music and 150 tracks of environmental music, required for the game was so large that there were fears it would not fit onto the game disc alongside the rest of the game data.|$|E
500|$|Johnson {{stated that}} Valve's aim {{was not to}} make Portal 2 more {{difficult}} than its predecessor, but instead to produce [...] "a game where you think your way through particular parts of the level, and feel really smart when you solve it." [...] Portal 2 allows the player to take incremental steps to understand the game's mechanics, an approach that led to two basic types of test chamber. The first type, which Valve calls [...] "checklisting", provides a relatively safe environment for player to experiment with a new gameplay concept; the second type combines elements in new ways to force the player to think laterally, providing challenging and rewarding puzzles. Chambers were first developed through whiteboard via isometric drawings. The developers performed a sanity check on the chamber before crafting simple levels with a <b>software</b> <b>tool</b> called the Hammer level editor. Extensive playtesting ensured the solutions to each chamber were neither too obvious nor too difficult, and observed alternative solutions discovered by playtesters. Based on play-testing results, the design team retained these alternative solutions or blocked them if they were considered too easy. These versions were sent back for further play-testing to verify that the new elements did not prevent players from finding solutions; further iterations between artists and playtesters occurred until such issues were resolved. Some elements from Portal were modified to suit Portal 2; whereas players of Portal would {{be familiar with the}} game mechanics, novice players required some training, as would players of Portal for some game elements. For example, the energy spheres used in the first game were replaced with lasers, which provided immediate feedback and reduced the in-game training time.|$|E
2500|$|A freely {{downloadable}} <b>software</b> <b>tool</b> {{for assessing}} the quality of 2D gel image analysis data.|$|E
40|$|The Master's Thesis {{deals with}} the usage of various <b>software</b> <b>tools</b> in {{emergency}} management and in environmental protection. Nowadays, on every step we make, we can face emergency situations which can cause great {{harm to the environment}} as well as to human life and property. The theoretical part provides an overview of <b>software</b> <b>tools.</b> Different <b>software</b> <b>tools,</b> their basic parameters, development and usage are described. There are also explained terms like geographic information system, etc. The practical part is focused on the instruments used in selected countries, including Czech Republic, Slovak Republic, Hungary, USA, Canada and the United Kingdom. In the conclusion is the comparison of the effectiveness of <b>software</b> <b>tools</b> in emergency management and in environmental protection...|$|R
5000|$|CANARIE’s Research Software Program funds {{development}} of <b>software</b> <b>tools</b> for research. <b>Software</b> created under CANARIE’s Research Software Program {{is designed to}} be modular and re-usable, [...]Reusable research <b>software</b> <b>tools</b> may be found at science.canarie.ca. These are available for use by any researcher.|$|R
40|$|With the {{development}} of next-generation sequencing (NGS) techniques, many <b>software</b> <b>tools</b> have emerged for the discovery of novel microRNAs (miRNAs) and for analyzing the miRNAs expression profiles. An overall evaluation of these diverse <b>software</b> <b>tools</b> is lacking. In this study, we evaluated eight <b>software</b> <b>tools</b> based on their common feature and key algorithms. Three deep-sequencing data sets were collected from different species and {{used to assess the}} computational time, sensitivity and accuracy of detecting known miRNAs as well as their capacity for predicting novel miRNAs. Our results provide useful information for researchers to facilitate their selection of the optimal <b>software</b> <b>tools</b> for miRNA analysis depending on their specific requirements, i. e. novel miRNAs discovery or miRNA expression profile analysis of sequencing data sets...|$|R
2500|$|... : Free online <b>software</b> <b>tool</b> {{to compute}} the {{radiation}} pattern {{of a variety of}} horn antennas.|$|E
2500|$|David M. Beazley – {{author of}} Python Essential Reference, the SWIG <b>software</b> <b>tool</b> for {{creating}} Python C extensions, and the PLY parsing tool; {{fellow of the}} Python Software Foundation and two-time awardee of the IEEE Gordon Bell Prize ...|$|E
2500|$|Information {{technology}} and changing business processes and management style {{can produce a}} work climate favorable to innovation. [...] For example, the <b>software</b> <b>tool</b> company Atlassian conducts quarterly [...] "ShipIt Days" [...] in which employees may work on anything related to the company's products. [...] Google employees work on self-directed projects for 20% of their time (known as Innovation Time Off). Both companies cite these bottom-up processes as major sources for new products and features.|$|E
40|$|The {{process of}} {{knowledge}} discovery in data (KDD) stored in computers in general requires iterations of three stages: data preparation, data mining, and results analysis. A variety of <b>software</b> <b>tools</b> {{are available for}} each of the stages. KDD environments, objectives of KDD, and types of data to be mined affect the choice of <b>software</b> <b>tools</b> in each stage. This article proposes a component-based architecture for an “end-to-end” integrated suite of KDD <b>software</b> <b>tools</b> that supports the entire KDD process. The architecture allows the configuring of an integrated <b>tool</b> suite with <b>software</b> <b>tools</b> appropriate for a given KDD environment and a given set of KDD objectives. The architecture {{is a part of the}} Chamois component-based knowledge-engineering framework under development at Ewha Women’s University in Korea. ...|$|R
25|$|InfraTools – <b>Software</b> <b>tools</b> for {{infrastructure}} management.|$|R
50|$|InfraTools - <b>Software</b> <b>tools</b> for {{infrastructure}} management.|$|R
