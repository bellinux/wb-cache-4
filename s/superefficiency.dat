45|0|Public
5000|$|... for any α ∈ R. Thus this {{estimator}} has {{the same}} asymptotic distribution as [...] for all θ ≠ 0, whereas for θ [...] 0 the rate of convergence becomes arbitrarily fast. This estimator is superefficient, as it surpasses the asymptotic behavior of the efficient estimator [...] at least at one point θ [...] 0. In general, <b>superefficiency</b> may only be attained on a subset of measure zero of the parameter space Θ.|$|E
40|$|In 1952 Lucien Le Cam {{announced}} his celebrated result that, for regular univariate statistical models, sets of points of <b>superefficiency</b> have Lebesgue measure zero. After reviewing the turbulent history of early studies of <b>superefficiency,</b> I suggest using {{the notion of}} computability {{as a tool for}} exploring the phenomenon of <b>superefficiency.</b> It turns out that only computable parameter points can be points of <b>superefficiency</b> for computable estimators. This algorithmic version of Le Cam's result implies, in particular, that sets of points of <b>superefficiency</b> not only have Lebesgue measure zero but are even countable. Comment: Published in at [URL] the Statistical Science ([URL] by the Institute of Mathematical Statistics ([URL]...|$|E
40|$|Fixed {{parameter}} asymptotic {{statements are}} often used {{in the context of}} nonparametric curve estimation problems (e. g., nonparametric density or regression estimation). In this context several forms of <b>superefficiency</b> can occur. In contrast to what can happen in regular parametric problems, here every parameter point (e. g., unknown density or regression function) can be a point of <b>superefficiency.</b> We begin with an example which shows how fixed parameter asymptotic statements have often appeared in the study of adaptive kernel estimators, and how <b>superefficiency</b> can occur in this context. We then carry out a more systematic study of such fixed parameter statements. It is shown in four general settings how the degree of <b>superefficiency</b> attainable depends on the structural assumptions in each case...|$|E
30|$|In [11, 12], {{the authors}} {{introduced}} many concepts of proper efficiency (tightly proper efficiency except) for normed spaces and for topological vector spaces, respectively. Furthermore, they discussed {{the relationships between}} <b>superefficiency</b> and other proper efficiencies. If {{we can get the}} relationship between tightly proper efficiency and <b>superefficiency,</b> then we can get the relationships between tightly proper efficiency and other proper efficiencies. So, in this section, the aim is to get the equivalent relationships between tightly proper efficiency and <b>superefficiency</b> under suitable assumption by virtue of strict efficiency.|$|E
40|$|Hajek's [17] {{convolution}} theorem {{was a major}} {{advance in}} understanding the classical information inequality. This re-examination of the convolution theorem discusses historical background to asymptotic estimation theory; the role of <b>superefficiency</b> in current esti-mation practice; the link between convergence of bootstrap distributions and convolution structure; and a dimensional asymptotics view of <b>superefficiency.</b> 1...|$|E
40|$|The {{connections}} between information pooling and adaptability {{as well as}} <b>superefficiency</b> are considered. Separable rules, which figure prominently in wavelet and other orthogonal series methods, are shown to lack adaptability; they are necessarily not rate-adaptive. A sharp lower bound {{on the cost of}} adaptation for separable rules is obtained. We show that adaptability is achieved through information pooling. A tight lower bound on the amount of information pooling required for achieving rate-optimal adaptation is given. Furthermore, in a sharp contrast to the separable rules, it is shown that adaptive non-separable estimators can be superefficient at every point in the parameter spaces. The results demonstrate that information pooling is the key to increasing estimation precision as well as achieving adaptability and even <b>superefficiency.</b> Adaptability Bayes rules Information pooling Minimax Minimum risk inequalities Nonparametric regression Orthogonal series Separable rules <b>Superefficiency</b> Wavelets White noise...|$|E
40|$|In his 1953 paper Lucien Le Cam proved {{for regular}} {{univariate}} statistical models that sets of points of <b>superefficiency</b> have Lebesgue measure zero (in fact, these sets are even countable). Considering only computable estimators, {{it is possible}} to show that no computable parameter point can be a point of <b>superefficiency.</b> This strengthens Le Cam’s result to a dichotomy: either a parameter point θ can be computably estimated with zero error, or no computable estimator is more efficient at θ than the maximum likelihood estimator. ...|$|E
40|$|Blind source {{separation}} extracts independent component {{signals from}} their mixtures {{without knowing the}} mixing coefficients nor the probability distributions of source signals. It is known that some algorithms work surprisingly well. The present paper elucidates the <b>superefficiency</b> of algorithms based on the statistical analysis. It is in general known from the asymptotic theory of statistical analysis that the covariance of any two extracted independent signals converges to 0 {{in the order of}} 1 =t in the case of statistical estimation by using t examples. In the case of on-line learning, the theory of on-line dynamics shows that the covariances converge to 0 in the order of j when the learning rate j is fixed to be a small constant. In contrast with the above general properties, the surprising <b>superefficiency</b> holds in blind source separation under a certain conditions. The <b>superefficiency</b> implies that the covariance decreases in the order of 1 =t 2 or of j 2. The present paper uses t [...] ...|$|E
40|$|Some {{models are}} {{presented}} in this paper which extend the concept of measuring <b>superefficiency</b> to the useful case of variable returns-to-scales (VRS), thus enabling the ranking of efficient as well as inefficient units. Two models, namely the Universal Radial Model and the Universal Additive Model, are presented that also have strong invariance properties (units and translation invariance). For both of these models a method for normalising the efficiency scores on a (0 - 1 +) scale is presented. These models have been implemented in a software package and applied to the ranking of units in an industrial context. Data envelopment analysis (DEA), <b>Superefficiency,</b> Universal models...|$|E
40|$|AbstractThe {{connections}} between information pooling and adaptability {{as well as}} <b>superefficiency</b> are considered. Separable rules, which figure prominently in wavelet and other orthogonal series methods, are shown to lack adaptability; they are necessarily not rate-adaptive. A sharp lower bound {{on the cost of}} adaptation for separable rules is obtained. We show that adaptability is achieved through information pooling. A tight lower bound on the amount of information pooling required for achieving rate-optimal adaptation is given. Furthermore, in a sharp contrast to the separable rules, it is shown that adaptive non-separable estimators can be superefficient at every point in the parameter spaces. The results demonstrate that information pooling is the key to increasing estimation precision as well as achieving adaptability and even <b>superefficiency...</b>|$|E
40|$|A {{theory of}} <b>superefficiency</b> and {{adaptation}} is developed under flexible performance measures which give a multiresolution view {{of risk and}} {{bridge the gap between}} pointwise and global estimation. This theory provides a useful benchmark for the evaluation of spatially adaptive estimators and shows that the possible degree of <b>superefficiency</b> for minimax rate optimal estimators critically depends {{on the size of the}} neighborhood over which the risk is measured. Wavelet procedures are given which adapt rate optimally for given shrinking neighborhoods including the extreme cases of mean squared error at a point and mean integrated squared error over the whole interval. These adaptive procedures are based on a new wavelet block thresholding scheme which combines both the commonly used horizontal blocking of wavelet coefficients (at the same resolution level) and vertical blocking of coefficients (across different resolution levels) ...|$|E
40|$|It {{is shown}} that superefficient Monte Carlo {{computations}} {{can be carried}} out by using chaotic dynamical systems as non-uniform random-number generators. Here <b>superefficiency</b> means that the expectation value of the square of the error decreases to 0 as 1 /N^{ 2 } with N successive observations for N-> infinity, whereas the conventional Monte Carlo simulation gives the square of the error in the order 1 /N. The order of N in the error convergence speed of superefficient Monte Carlo computations does not depend on the dimensionality of the problems. By deriving a necessary and sufficient condition for the <b>superefficiency,</b> it is shown that such high-performance Monte Carlo simulations {{can be carried out}} only if there exists a strong correlation of chaotic dynamical variables. Comment: 26 pages, 13 figures, typos in appendix A corrected, and title slightly change...|$|E
30|$|The {{paper is}} {{organized}} as follows. Some concepts about tightly proper efficiency, <b>superefficiency</b> and strict efficiency are introduced and a lemma {{is given in}} Section 2. In Section 3, the relationships among the concepts of tightly proper efficiency, strict efficiency and <b>superefficiency</b> in local convex topological vector spaces are clarified. In Section 4, the concept of tightly proper efficiency for set-valued vector optimization problem is introduced and a scalarization theorem for tightly proper efficiency in vector optimization problems involving nearly cone-subconvexlike set-valued maps is obtained. In Section 5, we establish two Lagrange multiplier theorems which show that tightly properly efficient solution of the constrained vector optimization problem is equivalent to tightly properly efficient solution of an appropriate unconstrained vector optimization problem. In Section 6, some results on tightly proper duality are given. Finally, a new concept of tightly proper saddle point for set-valued Lagrangian map is introduced and is then utilized to characterize tightly proper efficiency in Section 7. Section 8 contains some remarks and conclusions.|$|E
40|$|This {{last part}} states and proves {{the fact that}} the kernel type estima-tor defined and studied in Part I is optimal in sharp {{asymptotical}} min-imax sense on A simultaneously under the pointwise and the L 2 -risks. We also discuss some effects of dominating bias, such as <b>superefficiency</b> of minimax estimators. The notation is preserved and the numbering of sections, results and equations in Part I and II is continued...|$|E
3000|$|In this paper, we have {{extended}} {{the concept of}} tightly proper efficiency from normed linear spaces to locally convex topological vector spaces and got the equivalent relations among tightly proper efficiency, strict efficiency and <b>superefficiency.</b> We have also obtained a scalarization theorem and two Lagrange multiplier theorems for tightly proper efficiency in vector optimization involving nearly cone-subconvexlike set-valued maps. Then, we have introduced a Lagrange dual problem and got some duality results in terms of tightly properly efficient solutions. To characterize tightly proper efficiency, we have also introduced {{a new type of}} saddle point, which is called the tightly proper saddle point of an appropriate set-valued Lagrange map, and obtained its necessary and sufficient optimality conditions. Simultaneously, we have also given some examples to illustrate these concepts and results. On the other hand, by using the results of the Section 3 in this paper, we know that the above results hold for <b>superefficiency</b> and strict efficiency in vector optimization involving nearly cone-convexlike set-valued maps and, by virtue of [12, Theorem 3.11], all the above results also hold for positive proper efficiency, Hurwicz proper efficiency, global Henig proper efficiency and global Borwein proper efficiency in vector optimization with set-valued maps under the conditions that the set-valued [...]...|$|E
40|$|<b>Superefficiency</b> is an {{important}} extension of DEA that overcomes some limitations of the traditional models, specifically allowing ranking of efficient units and {{a unique set of}} weights for those units. Weights restriction is a well-known technique in the DEA field. When those techniques are applied, weights cluster around its new limits, making its evaluation dependent of its levels. This chapter introduces a new approach to weights adjustment by goal programming techniques, avoiding the imposition of hard restrictions that can even lead to unfeasibility. This method results in models that are more flexible...|$|E
40|$|Abstract: Some {{models are}} {{presented}} in this paper which extend the concept of measuring <b>superefficiency</b> to the useful case of variable returns-to-scales (VRS), thus enabling the ranking of efficient as well as inefficient units. Two models, namely the Universal Radial Model and the Universal Additive Model, are presented that also have strong invariance properties (units and translation invariance). For both of these models a method for normalising the efficiency scores on a (0 - 1 +) scale is presented. These models have been implemented in a software package and applied to the ranking of units in an industrial context. ...|$|E
40|$|Copyright © 2013 Springer Netherlands. <b>Superefficiency</b> is an {{important}} extension of DEA that overcomes some limitations of the traditional models, specifically allowing ranking of efficient units and {{a unique set of}} weights for those units. Weights restriction is a well-known technique in the DEA field. When those techniques are applied, weights cluster around its new limits, making its evaluation dependent of its levels. This chapter introduces a new approach to weights adjustment by goal programming techniques, avoiding the imposition of hard restrictions that can even lead to unfeasibility. This method results in models that are more flexible...|$|E
40|$|In some {{estimation}} problems, {{especially in}} applications dealing with information theory, signal processing and biology, theory {{provides us with}} additional information allowing us to restrict the parameter space to {{a finite number of}} points. In this case, we speak of discrete parameter models. Even though the problem is quite old and has interesting connections with testing and model selection, asymptotic theory for these models has hardly ever been studied. Therefore, we discuss consistency, asymptotic distribution theory, information inequalities and their relations with efficiency and <b>superefficiency</b> for a general class of $m$-estimators. Comment: Published in at [URL] the Statistical Science ([URL] by the Institute of Mathematical Statistics ([URL]...|$|E
40|$|It {{is shown}} (Proposition (3. 9)) that the {{asymptotic}} information bound which is valid for {{the estimation of}} a parameter in the structure (mixture) model remains valid in the functional model (incidental nuisance parameters) if only perturbation symmetric estimators (Definition (3. 6)) are admitted. Pertur- bation symmetry is a property which {{is closely related to}} permutation symmetry (Theorem (3. 4)). In particular, equicontinuous functions of empirical processes are perturbation symmetric (Theorem (3. 3)). Thus, the results of this paper continue a discussion initiated by Bickel and Klaassen (1986), Pfanzagl (1993) and Strasser (1996) on permutation symmetry of estimators and the exclusion of <b>superefficiency</b> in the functional model. (authors' abstract) Series: Forschungsberichte / Institut für Statisti...|$|E
30|$|One {{important}} {{problem in}} vector optimization {{is to find}} efficient points of a set. As observed by Kuhn, Tucker and later by Geoffrion, some efficient points exhibit certain abnormal properties. To eliminate such abnormal efficient points, there are many papers to introduce various concepts of proper efficiency; see [1 – 8]. Particularly, Zaffaroni [9] introduced the concept of tightly proper efficiency and used a special scalar function to characterize the tightly proper efficiency, and obtained some properties of tightly proper efficiency. Zheng [10] extended the concept of <b>superefficiency</b> from normed spaces to locally convex topological vector spaces. Guerraggio et al. [11] and Liu and Song [12] made a survey {{on a number of}} definitions of proper efficiency and discussed the relationships among these efficiencies, respectively.|$|E
40|$|Additional {{papers and}} lecture notes {{will be given}} later. The course begins with the {{classical}} asymptotic theory. Topics include Information Inequality, delta method, variance-stabilizing transformation, Edgeworth expansion, and their applications. Asymptotic properties of the MLE and <b>superefficiency</b> will be covered. The {{second part of the}} course is on high dimensional inference. Topics include detection of sparse signals, largest-scale multiple testing, compressed sensing/high-dimensional linear regression, and statistical inference on high-dimensional covariance structure. In particular, ` 1 minimization methods (Dantzig Selector and Lasso) and covariance ma-trix estimation are analyzed in detail. Both upper bound and lower bound techniques will be discussed. Homework: There will be occasional homework assignments. Presentation: Students are expected to give a presentation {{near the end of the}} semester...|$|E
40|$|Description of the mission, {{subsystems}} {{and communication}} capabilities {{of a joint}} Communications Technology Satellite (CTS) scheduled for launch in 1975 by the Canadian Department of Communications in cooperation with NASA. The principal objectives of the mission are TV transmission at 12 GHz to low-cost ground terminals, up-link TV transmission at 14 GHz transportable terminals, and flight tests of spacecraft subsystems and components for future communications satellites. The major advanced spacecraft subsystems are a novel <b>superefficiency</b> TWT design, a 0. 4 mlb Mercury Bombardment ion engine for north-south station keeping, a 3 -axis stabilization system to maintain a high antenna boresight pointing accuracy, a liquid metal slip ring experiment, and a lightweight extendible solar array with an initial power output greater than 1 kW...|$|E
40|$|This paper {{suggests}} {{a method of}} finding super-efficiency scores and modification of input-oriented models for sensitivity analysis of decision making units. First, by using DEA-R (ratiobased DEA) models in the input orientation, the models of <b>superefficiency</b> and also models of super-efficiency modification are suggested. Second, the worst-case scenarios are considered where {{the efficiency of the}} test DMU is deteriorating while the efficiencies of the other DMUs are improving. Then, by combining these two ideas, a model is suggested which increases the super-efficiency score and modifies the change ranges in order to preserve the performance class. In the end, the super-efficiency and change interval of efficient decision making units for 23 branches of Zone 1 of the Islamic Azad University are calculate...|$|E
40|$|The study applies a superefficiency-model to {{evaluate}} {{the efficiency of the}} 48 best selling compact cars in Germany. Efficiency is conceptualized from a customers' perspective as a ratio of outputs that customers obtain from a product relative to inputs (price, running costs) that customers have to invest. Extending Staat/Bauer/Hammerschmidt (2002), we integrate a multifaceted set of customer-relevant-attributes on the output side such as non-functional benefits and brand equity. More than 60 % of the cars are efficient but the analysis shows marked differences regarding their degree of <b>superefficiency.</b> Strongly superefficient products could demand a high increase in customers' inputs (price) preserving the provision of maximum customer value. Based on the parameter weights, we extract clusters of cars providing similar input-output-patterns and therefore belong to the same submarkets...|$|E
30|$|In many {{practical}} evaluation problems, {{efficiency of}} every evaluated DMU {{in a particular}} period may not be contrasted with the evaluated DMUs, but rather with sample standards determined by manufacturing parameters. The purpose of the contrast {{is not only to}} evaluate efficiency, but also to locate the standard with which the evaluated DMU has similar behavior. For instance, there are many grade standards for the evaluation of travel agencies. Travel agencies from the same region can be evaluated by the same standards separately, and those from different regions should not be evaluated by the same standards because of regional disparities. The standards should be formulated by the regional parameters. Taking outbound tourism as an example, it is an important part for travel agency business in developed regions, {{but it may not be}} contained in the travel agency business in some developing regions. Clearly, it is unreasonable that the outbound tourism is included in input measures to evaluate the travel agencies from different regions, and then grade standards in different regions should be formulated in terms of different manufacturing parameters. With these preparations, we then could use different standards to evaluate the level of travel agencies. However, in the existing DEA models, the constraint condition consists of the evaluated DMUs. Furthermore, we categorize DEA models into two types. The first type is the DEA models where the DMU under evaluation is included in the constraint condition [18 – 39]. The second type is the DEA models where the DMU under evaluation is not included in the constraint condition. For example, Andersen and Petersen [40] developed the <b>superefficiency</b> DEA model, which is identical to the BCC model, except that the DMU under evaluation is not included in the constraint condition. <b>Superefficiency</b> DEA model has been fully explored and applied [41 – 44].|$|E
30|$|Recently, {{several authors}} {{have turned their}} {{interests}} to vector optimization of set-valued maps, for instance, see [13 – 18]. Gong [19] discussed set-valued constrained vector optimization problems under the constraint ordering cone with empty interior. Sach [20] discussed the efficiency, weak efficiency and Benson proper efficiency in vector optimization problem involving ic-cone-convexlike set-valued maps. Li [21] extended the concept of Benson proper efficiency to set-valued maps and presented two scalarization theorems and Lagrange mulitplier theorems for set-valued vector optimization problem under cone-subconvexlikeness. Mehra [22], Xia and Qiu [23] discussed the <b>superefficiency</b> in vector optimization problem involving nearly cone-convexlike set-valued maps, nearly cone-subconvexlike set-valued maps, respectively. For other results for proper efficiencies in optimization problems with generalized convexity and generalized constraints, we refer to [24 – 26] and the references therein.|$|E
40|$|AbstractThis paper {{put forward}} a method to promote {{low-carbon}} development of electric power industry in China {{from the perspective of}} benchmarking circular economy efficiency of coal-fired power plants. The input-output index system that reflects reducing, reusing and recycling carbon dioxide, waste and pollution emissions for measuring the circular economy efficiency of coal-fired power plants is set up. With the survey data of 24 coal-fired power plants, the <b>superefficiency</b> data envelopment analysis (DEA) model is applied to ranking and improving the circular economy efficiency of these plants. And the benchmarking is carried out using the aforesaid DEA model. The benchmarking procedure proposed in this paper can be used to choose the best efficiency benchmark of circular economy in the environmental management of power plants and enhance the low-carbon development of coal-fired power industry...|$|E
40|$|Based {{on panel}} data {{covering}} {{the period from}} 2003 to 2012 in China’s 281 prefecture-level cities, we use <b>superefficiency</b> SBM model to measure regional financial efficiency and empirically test the spatial effects of fiscal decentralization on regional financial efficiency with SDM. The estimated results indicate that there exist significant spatial spillover effects among regional financial efficiency with the features of time inertia and spatial dependence. The positive promoting effect of fiscal decentralization on financial efficiency in local region depends on the symmetry between fiscal expenditure decentralization and revenue decentralization. Additionally, there exists inconsistency in the spatial effects of fiscal expenditure decentralization and revenue decentralization on financial efficiency in neighboring regions. The negative effect of fiscal revenue decentralization on financial efficiency in neighboring regions is more significant than that of fiscal expenditure decentralization...|$|E
40|$|We {{extend the}} {{traditional}} worst-case, minimax analysis of stochastic convex optimization by introducing a localized form of minimax complexity for individual functions. Our main result gives function-specific lower and upper bounds {{on the number}} of stochastic subgradient evaluations needed to optimize either the function or its "hardest local alternative" to a given numerical precision. The bounds are expressed in terms of a localized and computational analogue of the modulus of continuity that is central to statistical minimax analysis. We show how the computational modulus of continuity can be explicitly calculated in concrete cases, and relates to the curvature of the function at the optimum. We also prove a <b>superefficiency</b> result that demonstrates it is a meaningful benchmark, acting as a computational analogue of the Fisher information in statistical estimation. The nature and practical implications of the results are demonstrated in simulations...|$|E
40|$|It {{is shown}} (Proposition (3. 9)) that the {{asymptotic}} information bound which is valid for {{the estimation of}} a parameter in the structure (mixture) model remains valid in the functional model (incidental nuisance parameters) if only perturbation symmetric estimators (Definition (3. 6)) are admitted. Perturbation symmetry is a property which {{is closely related to}} permutation symmetry (Theorem (3. 4)). In particular, equicontinuous functions of empirical processes are perturbation symmetric (Theorem (3. 3)). Thus, the results of this paper continue a discussion initiated by Bickel and Klaassen, [2], Pfanzagl, [14], and Strasser, [21], on permutation symmetry of estimators and the exclusion of <b>superefficiency</b> in the functional model. 1 AMS 1991 subject classifications. Primary: 62 F 12; secondary: 62 B 15, 62 C 15 2 Key words and phrases. Asymptotic efficiency of estimates, incidental nuisance parameters, structure model, functional model, permutation symmetric estimates, perturbation symmetric [...] ...|$|E
40|$|It is {{well known}} that it is {{possible}} to achieve adaptation for “free” in function estimation under a global loss. It is unclear, however, why and how the adaptability is achieved. In this article we show that adaptability is achieved through information pooling. It is first shown that separable rules, which figure prominently in wavelet and other orthogonal series methods, lack adaptability; they are necessarily not rate-adaptive. A sharp lower bound on the cost of adaptation for separable rules is obtained. We then derive a tight lower bound on the amount of information pooling required for achieving global adaptability. Moreover, in a sharp contrast to the separable rules, it is shown that adaptive nonseparable estimators can be superefficient at every point in the parameter spaces. The results demonstate that information pooling is the key to increase estimation precision and achieve adaptability and even <b>superefficiency...</b>|$|E
40|$|Ergodic {{dynamical}} {{systems with}} absolutely continuous invariant probability measures are implemented as random-number generators for Monte Carlo computation. Such chaos-based Monte Carlo computation yields sometimes unexpected dynamical dependency behavior which cannot {{be explained by}} the usual statistical argument. We resolve the problem of its origin of this behavior by considering the effect of dynamical correlation on chaotic random-number generators. Furthermore, we find that superefficient Monte Carlo computation can be carried out by using chaotic dynamical systems as random-number generators. Here <b>superefficiency</b> means that the expectation value of the square of the error decreases to 0 in the order 1 N 2 with N successive observations for N ! 1, whereas the conventional Monte Carlo simulation gives the square of the error in the order 1 N. The computation speed of the superefficient case does not depend on the dimensionality s of the problems and, hence, it is superior to [...] ...|$|E
40|$|In {{this paper}} {{we show that}} the well­known {{asymptotic}} efficiency bounds for full mixture models remain valid if individual sequences of nuisance parameters are considered. This is made precise both for some classes of random (i. i. d.) and non­random nuisance parameters. For the random case it is shown that <b>superefficiency</b> of the kind given by an example of Pfanzagl (1993) can happen only with low probability. The non-random case deals with permutation invariant estimators under one­dimensional nuisance parameters. It is shown that the efficiency bounds remain valid for individual non­random arrays of nuisance parameters whose empirical process, if it is centered around its limit and standardized, satisfies a compactness condition. The compactness condition is satisfied in the random case with high probability. The results make use of basic LAN-theory. Regularity conditions are {{stated in terms of}} L^ 2 ­differentiability. (authors' abstract) Series: Forschungsberichte / Institut für Statisti...|$|E
40|$|The {{benefits}} of integration companies-suppliers top the strategic agendas of managers. Developing a system showing which suppliers merit continuing and deepening the partnership {{is difficult because}} of the large quantity of variables to be analyzed. The internationalized petroleum industry, requiring a large variety of materials, is no different. In this context, the Brazilian company PETROBRAS S. A. has a system to evaluate its suppliers based on a consensus panel formed by its managers. This paper shows a two phase methodology for classifying and awarding suppliers using the DEA model. Firstly, the suppliers are classified according to their efficiency based on commercial transactions realized. Secondly they are classified according to the opinions of the managers, using a DEA model for calculating votes, with the assurance regions and <b>superefficiency</b> defining the best suppliers. The paper presents a case study in the E&P segment of PETROBRAS and the results obtained with the methodology...|$|E
40|$|The {{concept of}} biased data {{is well known}} and its {{practical}} applications range from social sciences and biology to economics and quality control. These observations arise when a sampling procedure chooses an observation with probability {{that depends on the}} value of the observation. This is an interesting sampling procedure because it favors some observations and neglects others. It is known that biasing does not change rates of nonparametric density estimation, but no results are available about sharp constants. This article presents asymptotic results on sharp minimax density estimation. In particular, a coefficient of difficulty is introduced that shows the relationship between sample sizes of direct and biased samples that imply the same accuracy of estimation. The notion of the restricted local minimax, where a low-frequency part of the estimated density is known, is introduced; it sheds new light on the phenomenon of nonparametric <b>superefficiency.</b> Results of a numerical study are presented...|$|E
