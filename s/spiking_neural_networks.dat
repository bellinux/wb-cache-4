527|10000|Public
5000|$|TrueNorth - a {{processor}} architecture designed solely for <b>spiking</b> <b>neural</b> <b>networks.</b>|$|E
50|$|In practice, {{there is}} a major {{difference}} between the theoretical power of <b>spiking</b> <b>neural</b> <b>networks</b> and what has been demonstrated. They have proved useful in neuroscience, but not (yet) in engineering. Some large scale neural network models have been designed that take advantage of the pulse coding found in <b>spiking</b> <b>neural</b> <b>networks,</b> these networks mostly rely on the principles of reservoir computing. However, the real world application of large scale <b>spiking</b> <b>neural</b> <b>networks</b> has been limited because the increased computational costs associated with simulating realistic neural models have not been justified by commensurate benefits in computational power. As a result, there has been little application of large scale <b>spiking</b> <b>neural</b> <b>networks</b> to solve computational tasks of the order and complexity that are commonly addressed using rate coded (second generation) neural networks. In addition {{it can be difficult to}} adapt second generation neural network models into real time, <b>spiking</b> <b>neural</b> <b>networks</b> (especially if these network algorithms are defined in discrete time). It is relatively easy to construct a spiking neural network model and observe its dynamics. It is much harder to develop a model with stable behavior that computes a specific function.|$|E
50|$|Artificial neural networks, {{depending}} on type, {{may or may}} {{not take into account the}} timing of inputs. Those that do, such as <b>spiking</b> <b>neural</b> <b>networks,</b> fire only when the pooled inputs reach a membrane potential is reached. Because this mimics the firing of biological neurons, <b>spiking</b> <b>neural</b> <b>networks</b> are viewed as a more biologically accurate model of synaptic activity.|$|E
40|$|Architecture and {{learning}} algorithm of self-learning <b>spiking</b> <b>neural</b> <b>network</b> in fuzzy clustering task are outlined. Fuzzy receptive neurons for pulse-position transformation of input data are considered. It is proposed {{to treat a}} <b>spiking</b> <b>neural</b> <b>network</b> in terms of classical automatic control theory apparatus based on the Laplace transform. It is shown that synapse functioning can be easily modeled by a second order damped response unit. Spiking neuron soma {{is presented as a}} threshold detection unit. Thus, the proposed fuzzy <b>spiking</b> <b>neural</b> <b>network</b> is an analog-digital nonlinear pulse-position dynamic system. It is demonstrated how fuzzy probabilistic and possibilistic clustering approaches can be implemented on the base of the presented <b>spiking</b> <b>neural</b> <b>network...</b>|$|R
40|$|Abstract—The brain-inspired <b>neural</b> <b>networks</b> have {{demonstrated}} great potential in big data analysis. The <b>spiking</b> <b>neural</b> <b>network</b> (SNN), which encodes {{the real world}} data into spike trains, promises great performance in computational ability and energy efficiency. Moreover, {{it is much more}} biologically plausible than the traditional artificial <b>neural</b> <b>network</b> (ANN), which keeps the input data in its original form. In this paper, we introduce an RRAM-based energy efficient implementation of STDP-based <b>spiking</b> <b>neural</b> <b>network</b> cascaded with ANN classifier. The recognition accuracy and power consumption are compared between SNN and traditional three-layer ANN. The experiments on the MNIST database demonstrate that the proposed RRAM-based <b>spiking</b> <b>neural</b> <b>network</b> requires only 14 % of power consumption compared with RRAM-based artificial <b>neural</b> <b>network</b> with a slight accuracy decay (∼ 2 %). I...|$|R
5000|$|... #Caption: The insect is {{controlled}} by a <b>spiking</b> <b>neural</b> <b>network</b> to find a target in an unknown terrain.|$|R
50|$|<b>Spiking</b> <b>neural</b> <b>networks</b> with axonal {{conduction}} delays exhibit polychronization, {{and hence}} {{could have a}} very large memory capacity.|$|E
50|$|Bitplane formats may be {{used for}} passing images to <b>Spiking</b> <b>neural</b> <b>networks,</b> or low {{precision}} approximations to neural networks/convolutional neural networks.|$|E
5000|$|There is diverse {{range of}} {{application}} software to simulate <b>spiking</b> <b>neural</b> <b>networks.</b> This software can be classified according {{to the use of}} the simulation: ...|$|E
2500|$|... a C++/CUDA-based, {{high-performance}} <b>spiking</b> <b>neural</b> <b>network</b> simulator, {{intended for}} large-scale real-time simulations, and with APIs for C++, C, Python and Matlab.|$|R
40|$|In {{this article}} is {{presented}} a very simple and effective analog <b>spiking</b> <b>neural</b> <b>network</b> simulator, realized with an event-driven method, taking into account a basic biological neuron parameter: the spike latency. Also, other fundamentals biological parameters are considered, such as subthreshold decay and refractory period. This model allows to synthesize neural groups {{able to carry out}} some substantial functions. The proposed simulator is applied to elementary structures, in which some properties and interesting applications are discussed, such as the realization of a <b>Spiking</b> <b>Neural</b> <b>Network</b> Classifier...|$|R
40|$|Abstract — In this paper, {{we propose}} a biologically {{inspired}} <b>spiking</b> <b>neural</b> <b>network</b> approach to obtaining an opponent pair which is invariant to illumination variations {{and can be}} employed for colour discrimination. The model is motivated by the neural mechanisms involved in processing the visual stimulus starting from the cone photo receptors to the centresurround receptive fields present in the retinal ganglion cells and the striate cortex. For our <b>spiking</b> <b>neural</b> <b>network,</b> we have employed the excitatory and inhibitory lateral synaptic connections, the Spike-Timing Dependent Plasticity (STDP) and long term potentiation and depression (LTP/LTD). Here, we employ a feed-forward leaky integrate-and-fire <b>spiking</b> <b>neural</b> <b>network</b> trained using a dataset of Munsell spectra. We have performed tests on perceptually similar colours under large illuminant power variations and done experiments on colourbased object recognition. We have also compared our results to those yielded {{by a number of}} alternatives. I...|$|R
50|$|CoDi is a {{cellular}} automaton (CA) model for <b>spiking</b> <b>neural</b> <b>networks</b> (SNNs). CoDi is {{an acronym for}} Collect and Distribute, referring to the signals and spikes in a neural network.|$|E
50|$|Non-spiking neural {{networks}} are integrated with <b>spiking</b> <b>neural</b> <b>networks</b> {{to have a}} synergistic effect {{in being able to}} stimulate some sensory or motor response while also being able to modulate the response.|$|E
50|$|Neurogrid, {{built at}} Stanford University, is a board that can {{simulate}} <b>spiking</b> <b>neural</b> <b>networks</b> directly in hardware. SpiNNaker (Spiking Neural Network Architecture), designed at the University of Manchester, uses ARM processors as {{the building blocks}} of a massively parallel computing platform based on a six-layer thalamocortical model.|$|E
40|$|In this paper, {{a target}} {{tracking}} controller based on <b>spiking</b> <b>neural</b> <b>network</b> is proposed for autonomous robots. This controller encodes the preprocessed environmental and target {{information provided by}} CCD cameras, encoders and ultrasonic sensors into spike trains, which are integrated by a three-layer <b>spiking</b> <b>neural</b> <b>network</b> (SNN). The outputs of SNN are generated based on the competition between the forward/backward neuron pair corresponding to each motor, with the weights evolved by the Hebbian learning. The application to target tracking of a mobile robot in unknown environment verifies {{the validity of the}} proposed controller...|$|R
50|$|NEST is a {{simulator}} for <b>spiking</b> <b>neural</b> <b>network</b> {{models that}} focus on the dynamics, size, and structure of neural systems, rather than on the exact geometry of individual neurons.|$|R
50|$|NEST is a {{simulation}} software for <b>spiking</b> <b>neural</b> <b>network</b> models, including large-scale neuronal networks. NEST was initially developed by Markus Diesmann and Marc-Oliver Gewaltig {{and is now}} developed and maintained by the NEST Initiative.|$|R
50|$|<b>Spiking</b> <b>neural</b> <b>networks</b> (SNNs) {{explicitly}} {{consider the}} timing of inputs. The network input and output are usually represented {{as a series of}} spikes (delta function or more complex shapes). SNNs can process information in the time domain (signals that vary over time). They are often implemented as recurrent networks. SNNs are also a form of pulse computer.|$|E
5000|$|Myorobotics is a toolkit {{comprising}} muscles, tendons, joints, {{and bones}} to build diverse tendon-driven musculoskeletal robots, e.g. anthropomimetic arms with complex shoulder joints, quadrupeds, and hopping robots. Robots can be assembled, optimized, and simulated from primitives, then built and controlled {{either from the}} same software or from brain-like <b>spiking</b> <b>neural</b> <b>networks</b> simulated on a neuromorphic computer.|$|E
50|$|SpiNNaker (Spiking Neural Network Architecture) is a manycore {{computer}} architecture {{designed by the}} Advanced Processor Technologies Research Group (APT) at the School of Computer Science, University of Manchester, led by Steve Furber, to simulate the human brain (see Human Brain Project). It is planned to use 1 million ARM processors (currently 0.5 million) in a massively parallel computing platform based on <b>spiking</b> <b>neural</b> <b>networks.</b>|$|E
40|$|In {{this paper}} a novel {{application}} of {{a particular type of}} <b>spiking</b> <b>neural</b> <b>network,</b> a Polychronous <b>Spiking</b> Network, was used for financial time series prediction. It is argued that the inherent temporal capabilities of this type of network are suited to non-stationary data such as this. The performance of the <b>spiking</b> <b>neural</b> <b>network</b> was benchmarked against three systems: two "traditional", rate-encoded, neural networks; a Multi-Layer Perceptron <b>neural</b> <b>network</b> and a Dynamic Ridge Polynomial <b>neural</b> <b>network,</b> and a standard Linear Predictor Coefficients model. For this comparison three non-stationary and noisy time series were used: IBM stock data; US/Euro exchange rate data, and the price of Brent crude oil. The experiments demonstrated favourable prediction results for the <b>Spiking</b> <b>Neural</b> <b>Network</b> in terms of Annualised Return and prediction error for 5 -Step ahead predictions. These results were also supported by other relevant metrics such as Maximum Drawdown and Signal-To-Noise ratio. This work demonstrated the applicability of the Polychronous Spiking Network to financial data forecasting and this in turn indicates the potential of using such networks over traditional systems in difficult to manage non-stationary environments. © 2014 Reid et al...|$|R
40|$|Neuroscience {{study shows}} {{mammalian}} brain only use millisecond scale time window to process complicated real-life recognition scenarios. However, such speed cannot {{be achieved by}} traditional rate-based <b>spiking</b> <b>neural</b> <b>network</b> (SNN). Compared with spiking rate, the specific spiking timing (also called spiking pattern) may convey much more information. In this paper, by using modified rank order coding scheme, the generated absolute analog features have been encoded into the first spike wave with specific spatiotemporal structural information. An intuitive yet powerful feed-forward <b>spiking</b> <b>neural</b> <b>network</b> framework has been proposed, along with its own unsupervised spike-timing-dependent plasticity (STDP) learning rule with dynamic post-synaptic potential threshold. Compared with other state-of-art spiking algorithms, the proposed method uses biologically plausible STDP learning method to learn the selectivity while the dynamic post-synaptic potential threshold guarantees no training sample will be ignored during the learning procedure. Furthermore, unlike the complicated frameworks used in those state-of-art spiking algorithms, the proposed intuitive <b>spiking</b> <b>neural</b> <b>network</b> is not time-consuming and quite capable of on-line learning. A satisfactory experimental result has been achieved on classic MNIST handwritten character database...|$|R
40|$|Reinforcement {{learning}} is a theoretical framework for learning how to act in an unknown environment through trial and errors. One reinforcement learning framework proposed by Sallans and Hinton [1], which we call free-energy-based reinforcement learning (FERL), possesses many desirable characteristics such as an {{ability to deal with}} high-dimensional sensory inputs and goal-directed representation learning, and neurally plausible characteristics such as population coding of action-value and a Hebbian learning rule modulated by reward prediction errors. These characteristics imply that FERL is possibly implemented in the brain. In order to understand the neural implementation of the reinforcement learning and pursue the neural plausibility of FERL, we implemented FERL in a more realistic <b>spiking</b> <b>neural</b> <b>network</b> than binary stochastic neurons. An FERL framework uses a restricted Boltzmann machine (RBM) as a building block. The RBM is an energy-based statistical model with binary nodes separated in visible and hidden layers. In the RBM, due to its connectivity, the posterior distribution over hidden given visible nodes is statistically decoupled, yielding the simple computation of posterior distribution [2]. An RBM is implemented using a <b>spiking</b> <b>neural</b> <b>network</b> with leaky integrate- and-fire neurons. The network is composed of state, action, and hidden layers. The state Figure 1 Performance of the <b>spiking</b> <b>neural</b> <b>network.</b> A. The free-energies estimated by both the <b>spiking</b> <b>neural</b> <b>network</b> and the original RBM. They are highly correlated (correlation coefficient, r = 0. 9485) B. The hidden neurons activation on the two principal components. The hidden activation patterns are clustered by same optimal action...|$|R
50|$|In {{the context}} of <b>spiking</b> <b>neural</b> <b>networks,</b> the current {{activation}} level (modeled as some differential equation) is normally {{considered to be the}} neuron's state, with incoming spikes pushing this value higher, and then either firing or decaying over time. Various coding methods exist for interpreting the outgoing spike train as a real-value number, either relying on the frequency of spikes, or the timing between spikes, to encode information.|$|E
50|$|Neurorobotics, a {{combined}} study of neuroscience, robotics, and artificial intelligence, is {{the science and}} technology of embodied autonomous neural systems. Neural systems include brain-inspired algorithms (e.g. connectionist networks), computational models of biological neural networks (e.g. artificial <b>spiking</b> <b>neural</b> <b>networks,</b> large-scale simulations of neural microcircuits) and actual biological systems (e.g. in vivo and in vitro neural nets). Such neural systems can be embodied in machines with mechanic or any other forms of physical actuation. This includes robots, prosthetic or wearable systems but at also, at smaller scale, micro-machines and, at the larger scales, furniture and infrastructures.|$|E
50|$|The {{synaptic}} {{activity of}} individual neurons is modeled using equations {{to determine the}} temporal (and in some cases, spatial) summation of synaptic signals, membrane potential, threshold for action potential generation, the absolute and relative refractory period, and optionally ion receptor channel kinetics and Gaussian noise (to increase biological accuracy by incorporation of random elements). In addition to connectivity, some types of artificial neural networks, such as <b>spiking</b> <b>neural</b> <b>networks,</b> also model the distance between neurons, {{and its effect on}} the synaptic weight (the strength of a synaptic transmission).|$|E
40|$|This paper {{proposes a}} {{supervised}} delay-adaptation <b>spiking</b> <b>neural</b> <b>network</b> model to support {{decision making in}} orthodontic extraction. This temporal coding <b>spiking</b> <b>neural</b> <b>network</b> model employs a Hebbian-based rule to shift the connection delays instead of previous approaches to delay selection. Here, the tuned delays compensate for differences in the input firing times of temporal patterns and enable them to coincide. The coincidence detection capability of the spiking neuron has been utilised for detecting and classifying patterns. The structure of the network {{is similar to that}} of an LVQ network except that the output layer neurons are coincidence-detecting spiking neurons. An input pattern is represented by the group of the neuron that is the first to fire among all the competing spiking neurons. The proposed <b>spiking</b> <b>neural</b> <b>network</b> has been utilised to classify orthodontic data. The trained network obtained an average classification accuracy of 89. 8 % on previously unseen test data. This was achieved with a network of 2 x 4 spiking neurons trained for 40 epochs using 100 training examples. The classification accuracy of the proposed model was found to be better than that of a multilayer perceptron (MLP) network trained using the error back-propagation algorithm...|$|R
40|$|We {{propose a}} <b>spiking</b> <b>neural</b> <b>network</b> model that is {{inspired}} from an oversimplified general structure of layer IV of the cortex. The neuron model is an integrate-and-fire model whose threshold changes depending on its firing activity. Firing activity of neurons {{in combination with}} interactivity between them creates a highly dynamical self-organized process. The spiking activity, the neuron's threshold that changes depending on the firing activity {{and the time of}} stabilization of the proposed network can be used to represent its complex behavior. We experimentally show that this representation of the complex behavior of the network can be used to characterize spatio-temporal information of input patterns. Also, a new paradigm for novelty detection by the <b>spiking</b> <b>neural</b> <b>network</b> is proposed. The stabilization time of the <b>neural</b> <b>network</b> is used as a criterion for novelty detection...|$|R
40|$|Abstract: The Gustafson-Kessel fuzzy {{clustering}} {{algorithm is}} capable of detecting hyperellipsoidal clusters of different sizes and orientations by adjusting the covariance matrix of data, thus overcoming the drawbacks of conventional fuzzy c-means algorithm. In this paper, an adaptive version of the Gustafson-Kessel algorithm is proposed. The way to adjust the covariance matrix iteratively is introduced by applying the Sherman-Morrison matrix inversion procedure. The adaptive fuzzy clustering algorithm is implemented {{on the base of}} self-learning <b>spiking</b> <b>neural</b> <b>network</b> known as a realistic analog of biological neural systems that can perform fast data processing. Therefore, the proposed fuzzy <b>spiking</b> <b>neural</b> <b>network</b> that belongs to a new type of hybrid intelligent systems makes it possible both to perform fuzzy clustering tasks efficiently and to reduce data processing time considerably...|$|R
50|$|This kind {{of neural}} network can in {{principle}} {{be used for}} information processing applications {{the same way as}} traditional artificial neural networks. In addition, <b>spiking</b> <b>neural</b> <b>networks</b> can model the central nervous system of a virtual insect for seeking food without the prior knowledge of the environment. However, due to their more realistic properties, they {{can also be used to}} study the operation of biological neural circuits. Starting with a hypothesis about the topology of a biological neuronal circuit and its function, the electrophysiological recordings of this circuit can be compared to the output of the corresponding spiking artificial neural network simulated on computer, determining the plausibility of the starting hypothesis.|$|E
5000|$|Software used {{primarily}} to simulate <b>spiking</b> <b>neural</b> <b>networks</b> which {{are present in}} the biology to study their operation and characteristics. In this group we can find simulators such as GENESIS (the GEneral NEural SImulation System) developed in James Bower's laboratory at Caltech; NEURON, mainly developed by Michael Hines, John W. Moore and Ted Carnevale in Yale University and Duke University; Brian, developed by Romain Brette and Dan Goodman at the École Normale Supérieure; and NEST developed by the NEST Initiative. This type of application software usually supports the simulation of complex neural models {{with a high level of}} detail and accuracy. However large networks usually require very time-consuming simulations.|$|E
50|$|<b>Spiking</b> <b>neural</b> <b>networks</b> (SNNs) {{fall into}} the third {{generation}} of neural network models, increasing the level of realism in a neural simulation.In addition to neuronal and synaptic state, SNNs also incorporate the concept of time into their operating model. The idea is that neurons in the SNN do not fire at each propagation cycle (as it happens with typical multi-layer perceptron networks), but rather fire only when a membrane potential - an intrinsic quality of the neuron related to its membrane electrical charge - reaches a specific value. When a neuron fires, it generates a signal which travels to other neurons which, in turn, increase or decrease their potentials in accordance with this signal.|$|E
40|$|The {{design of}} <b>neural</b> <b>networks</b> that {{are able to}} e#ciently detect conjunctions of {{features}} is an important open challenge. We develop a feed-forward <b>spiking</b> <b>neural</b> <b>network</b> that requires a constant number of neurons for detecting a conjunction irrespective {{of the size of the}} retinal input field, and for up to four simultaneously present feature-conjunctions...|$|R
40|$|International audienceWe {{develop the}} major steps taken to map a {{realistic}} spike timing-dependent plasticity (STDP) model into digital hardware architecture. Several types of mappings are implemented and tested on FPGA device. We compare their applicability to a real-time <b>spiking</b> <b>neural</b> <b>network</b> (SNN) simulator running in biological time-scale...|$|R
40|$|This thesis {{presents}} <b>spiking</b> <b>neural</b> architectures which {{simulate the}} sound localisation {{capability of the}} mammalian auditory pathways. This localisation ability is achieved by exploiting important differences in the sound stimulus received by each ear, known as binaural cues. Interaural time difference and interaural intensity difference are the two binaural cues which play the most significant role in mammalian sound localisation. These cues are processed by different regions within the auditory pathways and enable the localisation of sounds at different frequency ranges; interaural time difference is used to localise low frequency sounds whereas interaural intensity difference localises high frequency sounds. Interaural time difference refers to the different points in time at which a sound from a single location arrives at each ear and interaural intensity difference refers to the difference in sound pressure levels of the sound at each ear, measured in decibels. Taking inspiration from the mammalian brain, two <b>spiking</b> <b>neural</b> <b>network</b> topologies were designed to extract each of these cues. The architecture of the <b>spiking</b> <b>neural</b> <b>network</b> designed to process the interaural time difference cue {{was inspired by the}} medial superior olive. The lateral superior olive was the inspiration for the architecture designed to process the interaural intensity difference cue. The development of these <b>spiking</b> <b>neural</b> <b>network</b> architectures required the integration of other biological models, such as an auditory periphery (cochlea) model, models of bushy cells and the medial nucleus of the trapezoid body, leaky integrate and fire spiking neurons, facilitating synapses, receptive fields and the appropriate use of excitatory and inhibitory neurons. Two biologically inspired learning algorithms were used to train the architectures to perform sound localisation. Experimentally derived HRTF acoustical data from adult domestic cats was employed to validate the localisation ability of the two architectures. The localisation abilities of the two models are comparable to other computational techniques employed in the literature. The experimental results demonstrate that the two SNN models behave in a similar way to the mammalian auditory system, i. e. the <b>spiking</b> <b>neural</b> <b>network</b> for interaural time difference extraction performs best when it is localising low frequency data, and the interaural intensity difference spiking neuron model performs best when it is localising high frequency data. Thus, the combined models form a duplex system of sound localisation. Additionally, both <b>spiking</b> <b>neural</b> <b>network</b> architectures show a high degree of robustness when the HRTF acoustical data is corrupted by noise...|$|R
