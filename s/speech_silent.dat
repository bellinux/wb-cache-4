13|166|Public
5000|$|In his <b>speech</b> <b>Silent</b> China, Lu Xun {{said that}} Chinese likes the reconcilable and the compromised. “For example, {{people will not}} allow you to add a window to a dark room, but when you {{threaten}} to uncover the roof, they would compromise {{to the idea of}} adding a window. Without a radical proposal pushing them, Chinese won’t permit even the mildest reform.” ...|$|E
50|$|Imagined <b>speech</b> (<b>silent</b> {{speech or}} covert speech) is {{thinking}} {{in the form of}} sound - “hearing” one’s own voice silently to oneself, without the intentional movement of any extremities such as the lips, tongue, or hands. Logically, imagined speech has been possible since the emergence of language, however, the phenomenon is most associated with the signal processing and detection within electroencephalograph (EEG) data as well as data obtained using alternative non-invasive, brain-computer interface (BCI) devices.|$|E
40|$|International audiencePhysical {{properties}} of speech articulators contribute to shape articulatory and formant trajectories. This study aims at evaluating {{the role of}} this shaping in speech perception. We conducted perception tests of synthetic stimuli generated with speech production models accounting for different degrees of physical complexity. Our results do not provide any support to {{the hypothesis that the}} degree of physical realism in the models influences the perception of naturalness. However for degraded <b>speech</b> (<b>silent</b> center speech), significant differences are observed...|$|E
40|$|In this paper, {{we review}} our {{proposed}} statistical voice conversion approaches to enhancing {{various types of}} body transmitted speech captured with non-audible murmur (NAM) microphone. Body transmitted speech conversion is a potential technique to bring a new paradigm to human-to-human speech communication. In addition to our previously proposed methods of enhancing body transmitted unvoiced <b>speech</b> for <b>silent</b> <b>speech</b> communication and of enhancing body transmitted artificial speech for speaking aid, we further propose conversion methods of enhancing body transmitted voiced speech for noise robust speech communication. An experimental result demonstrates that the proposed methods yield significant improvements in quality of body transmitted voiced speech. Index Terms — body transmitted speech, voice conversion, noise robust <b>speech</b> communication, <b>silent</b> <b>speech</b> communication, speaking aid 1...|$|R
40|$|We present {{our recent}} results on speech {{recognition}} by surface electromyography (EMG), which captures the electric potentials that are {{generated by the}} human articulatory muscles. This technique {{can be used to}} enable <b>Silent</b> <b>Speech</b> Interfaces, since EMG signals are generated even when people only articulate speech without producing any sound. Preliminary experiments have shown that the EMG signals created by audible and <b>silent</b> <b>speech</b> are quite distinct. In this paper we first compare various methods of initializing a <b>silent</b> <b>speech</b> EMG recognizer, showing that the performance of the recognizer substantially varies across different speakers. Based on this, we analyze EMG signals from audible and <b>silent</b> <b>speech,</b> present first results on how discrepancies between these speaking modes affect EMG recognizers, and suggest areas for future work. Index Terms: speech recognition, surface electromyography, <b>silent</b> <b>speech,</b> articulatio...|$|R
40|$|This paper {{presents}} {{our recent}} advances in speech recognition based on surface electromyography (EMG). This technology allows for <b>Silent</b> <b>Speech</b> Interfaces since EMG captures the electrical potentials of the human articulatory muscles rather than the acoustic speech signal. Our earlier experiments {{have shown that the}} EMG signal is greatly impacted by the mode of speaking. In this study we extend this line of research by comparing EMG signals from audible, whispered, and silent speaking mode. We distinguish between phonetic features like consonants and vowels and show that the lack of acoustic feedback in <b>silent</b> <b>speech</b> implies an increased focus on somatosensoric feedback, which is visible in the EMG signal. Based on this analysis we develop a spectral mapping method to compensate for these differences. Finally, we apply the spectral mapping to the front-end of our speech recognition system and show that recognition rates on <b>silent</b> <b>speech</b> improve by up to 11. 59 % relative. Index Terms: EMG, EMG-based <b>speech</b> recognition, <b>Silent</b> <b>Speech</b> Interfaces, somatosensoric feedbac...|$|R
40|$|International audienceWe {{examined}} lip and arm surface EMG {{activity in}} 23 healthy participants in tasks involving wilful inner <b>speech</b> (<b>silent</b> reading and silent generation of word definition) compared to relaxation and in 11 schizophrenia patients during auditory verbal hallucinations and rest periods. Results in healthy participants show that wilful inner speech {{corresponds to a}} significant increase in lip EMG activity compared with relaxation, while the activity in the arm muscle remains constant. Results in schizophrenia patients show {{a significant increase in}} lip EMG activity during AVHs relative to rest, without any increase of EMG activity in the arm muscle...|$|E
40|$|Children often talk {{themselves}} through their activities. They produce private speech (PS), which is internalized to form inner <b>speech</b> (<b>silent</b> verbal thought). Twenty-five 8 – 10 -year-olds completed four tasks {{in a laboratory}} context (Tower of London, digit span, and two measures of spatial IQ). PS production was recorded. Eleven months later, the same participants completed the Tower of London and academic numeracy tasks, again in a laboratory context, as well as numeracy tasks in a classroom context. Rates of PS production and its level of internalization showed large positive correlations across time, tasks, and contexts. The results are interpreted {{in terms of the}} psychometric properties of PS production and are taken as evidence {{for the development of a}} domain-general system for verbal self-regulation in childhood...|$|E
40|$|In this study, we {{evaluate}} our proposed {{methods for}} enhancing alaryngeal speech based on statistical voice conversion techniques. Voice conversion {{based on a}} Gaussian mixture model {{has been applied to}} the conversion of alaryngeal speech into normal speech (AL-to-Speech). Moreover, one-to-many eigenvoice conversion (EVC) has also been applied to AL-to-Speech to enable the recovery of the original voice quality of laryngectomees even if only one arbitrary utterance of the original voice is available. VC/EVC-based AL-to-Speech systems have been developed for several types of alaryngeal speech, such as esophageal speech (ES), electrolaryngeal speech (EL), and body-conducted silent electrolaryngeal <b>speech</b> (<b>silent</b> EL). These proposed systems are compared with each other from various perspectives. The experimental results demonstrate that our proposed systems yield significant enhancement effects on each type of alaryngeal speech. Index Terms — alaryngeal speech, speech enhancement, voice conversion, eigenvoice conversion, performance evaluation...|$|E
40|$|<b>Silent</b> <b>Speech</b> Interfaces {{have been}} {{proposed}} for communication in silent conditions or as a new means of restoring the voice of persons who have undergone a laryngectomy. To operate such a device, the user must articulate silently. Isolated word recognition tests performed with fixed and portable ultrasound based <b>silent</b> <b>speech</b> interface equipment show that systems trained on vocalized speech exhibit reduced performance when tested on silent articulation, but that training with silently articulated speech allows to recover much of this loss. Index Terms: <b>silent</b> <b>speech</b> interface, ultrasound, articulation 1...|$|R
40|$|The article {{describes}} a video-only speech recognition {{system for a}} “silent speech interface ” application, using ultrasound and optical images of the voice organ. A one-hour audio-visual speech corpus was phonetically labeled using an automatic speech alignment procedure and robust visual feature extraction techniques. HMM-based stochastic models were estimated separately on the visual and acoustic corpus. The performance of the visual speech recognition system is compared to a traditional acoustic-based recognizer. Index Terms: speech recognition, audio-visual <b>speech</b> description, <b>silent</b> <b>speech</b> interface, machine learnin...|$|R
5000|$|... (includes singles My Wonder Moon ,<b>Silent</b> <b>Speech</b> and [...] "Tao Perto Tao Longe") ...|$|R
40|$|ICASSP 2011 : The 36 th International Conference on Acoustics, Speech, and Signal Processing, May 22 - 27, 2011, Prague, Czech Republic. In this study, we {{evaluate}} our proposed {{methods for}} enhancing alaryngeal speech based on statistical voice conversion techniques. Voice conversion {{based on a}} Gaussian mixture model {{has been applied to}} the conversion of alaryngeal speech into normal speech (AL-to-Speech). Moreover, one-to-many eigenvoice conversion (EVC) has also been applied to AL-to-Speech to enable the recovery of the original voice quality of laryngectomees even if only one arbitrary utterance of the original voice is available. VC/EVC-based AL-to-Speech systems have been developed for several types of alaryngeal speech, such as esophageal speech (ES), electrolaryngeal speech (EL), and body-conducted silent electrolaryngeal <b>speech</b> (<b>silent</b> EL). These proposed systems are compared with each other from various perspectives. The experimental results demonstrate that our proposed systems yield significant enhancement effects on each type of alaryngeal speech...|$|E
40|$|Although the {{segmental}} intelligibility of converted {{speech from}} silent speech using direct signal-to-signal mapping proposed by Toda et al. [1] is quite acceptable, listeners have sometimes difficulty in chunking the speech continuum into meaningful words due to incomplete phonetic cues provided by output signals. This paper studies another approach consisting in combining HMM-based statistical speech recognition and synthesis techniques, {{as well as}} training on aligned corpora, to convert silent speech to audible voice. By introducing phonological constraints, such systems are expected to improve the phonetic consistency of output signals. Facial movements are used {{in order to improve}} the performance of both recognition and synthesis procedures. The results show that including these movements improves the recognition rate by 6. 2 % and a final improvement of the spectral distortion by 2. 7 % is observed. The comparison between direct signal-to-signal and phonetic-based mappings is finally commented in this paper. Index Terms: audiovisual voice conversion, non-audible murmur, whispered <b>speech,</b> <b>silent</b> speech interface, HMM...|$|E
40|$|Adults and infants {{were tested}} for the {{capacity}} to detect correspondences between nonspeech sounds and real vowels. The /i / and /a / vowels were presented in 3 different ways: auditory <b>speech,</b> <b>silent</b> visual faces articulating the vowels, or mentally imagined vowels. The nonspeech sounds were either pure tones or 3 -tone complexes that isolated a single feature of the vowel without allowing the vowel to be identified. Adults perceived an orderly relation between the nonspeech sounds and vowels. They matched high-pitched nonspeech sounds to /i / vowels and low-pitched nonspeech sounds to /a / vowels. In contrast, infants could not match nonspeech sounds to the visually presented vowels. Infants ' detection of correspondence between auditory and visual speech appears to require the whole speech signal; with development, an isolated feature of the vowel is sufficient for detection of the cross-modal correspondence. There is a long tradition in speech research of comparing the perception of speech and nonspeech events. A classic case was the attempt to determine whether nonspeech auditory signals were subject to the phenomenon of "categorical per-ception " (Liberman, Cooper, Shankweiler, & Studdert-Ken...|$|E
5000|$|Richards, Cecile: [...] "Banned in Boston: The <b>Silent</b> <b>Speech</b> {{of margaret}} Sanger" [...] (2008) ...|$|R
40|$|International audienceThis article {{describes}} a contour-based 3 D tongue deformation visualization framework using B-mode ultrasound image sequences. A robust, automatic tracking algorithm characterizes tongue motion via a contour, {{which is then}} used to drive a generic 3 D Finite Element Model (FEM). A novel contour-based 3 D dynamic modeling method is presented. Modal reduction and modal warping techniques are applied to model the deformation of the tongue physically and efficiently. This work {{can be helpful in}} a variety of fields, such as <b>speech</b> production, <b>silent</b> <b>speech</b> recognition, articulation training, speech disorder study, etc...|$|R
40|$|International audienceAlthough the {{segmental}} intelligibility of converted <b>speech</b> from <b>silent</b> <b>speech</b> using direct signal-to-signal mapping {{proposed by}} Toda et al. [1] is quite acceptable, listeners have sometimes difficulty in chunking the speech continuum into meaningful words due to incomplete phonetic cues provided by output signals. This paper studies another approach consisting in combining HMM-based statistical speech recognition and synthesis techniques, {{as well as}} training on aligned corpora, to convert <b>silent</b> <b>speech</b> to audible voice. By introducing phonological constraints, such systems are expected to improve the phonetic consistency of output signals. Facial movements are used {{in order to improve}} the performance of both recognition and synthesis procedures. The results show that including these movements improves the recognition rate by 6. 2 % and a final improvement of the spectral distortion by 2. 7 % is observed. The comparison between direct signal-to-signal and phonetic-based mappings is finally commented in this paper...|$|R
40|$|Abnormalities in the {{integration}} of auditory and visual language inputs could underlie many core psychotic features. Perceptual confusion may arise because of the normal propensity of visual speech perception to evoke auditory percepts. Recent functional neuroimaging studies of normal subjects have demonstrated activation in auditory-linguistic brain areas in response to silent lip-reading. Three {{functional magnetic resonance imaging}} experiments were carried out on seven normal volunteers, and 14 schizophrenia patients, half of whom were actively psychotic. The tasks involved listening to auditory <b>speech,</b> <b>silent</b> Lip-reading (visual speech), and perception of meaningless lip movements (visual non-speech). Subjects also undertook a behavioural study of audio-visual word identification designed to evoke perceptual fusions. Patients and controls both showed susceptibility to audio-visual fusions on the behavioural task. The patient group as a whole showed less activation relative to controls in superior and inferior posterior temporal areas while performing the silent lip-reading task. Attending to visual non-speech, the patients activated less posterior (occipito-temporal) and more anterior (frontal, insular and striatal) brain areas than controls. This difference was accounted for Largely by the psychotic subgroup. Insular and striatal areas were also activated in both subject groups in the auditory speech perception condition, thus demonstrating the bimodal sensitivity of these regions. The results suggest that schizophrenia patients with psychotic symptoms respond to visually ambiguous stimuli (non-speech) by activation of polysensory structures. This could reflect particular processing strategies and may increase susceptibility to certain paranoid and hallucinatory symptoms...|$|E
40|$|AbstractFunctional {{near-infrared}} spectroscopy (fNIRS) is a silent, non-invasive neuroimaging {{technique that}} is potentially {{well suited to}} auditory research. However, the reliability of auditory-evoked activation measured using fNIRS is largely unknown. The present study investigated the test-retest reliability of speech-evoked fNIRS responses in normally-hearing adults. Seventeen participants underwent fNIRS imaging in two sessions separated by three months. In a block design, participants were presented with auditory speech, visual <b>speech</b> (<b>silent</b> speechreading), and audiovisual speech conditions. Optode arrays were placed bilaterally over the temporal lobes, targeting auditory brain regions. A range of established metrics was used to quantify the reproducibility of cortical activation patterns, {{as well as the}} amplitude and time course of the haemodynamic response within predefined regions of interest. The use of a signal processing algorithm designed to reduce the influence of systemic physiological signals was found to be crucial to achieving reliable detection of significant activation at the group level. For auditory speech (with or without visual cues), reliability was good to excellent at the group level, but highly variable among individuals. Temporal-lobe activation in response to visual speech was less reliable, especially in the right hemisphere. Consistent with previous reports, fNIRS reliability was improved by averaging across a small number of channels overlying a cortical region of interest. Overall, the present results confirm that fNIRS can measure speech-evoked auditory responses in adults that are highly reliable at the group level, and indicate that signal processing to reduce physiological noise may substantially improve the reliability of fNIRS measurements...|$|E
40|$|Selected {{developments}} {{in the fields of}} verbal and nonverbal behaviour are reviewed, with especial reference to the study of speech in social context. Among features bordering on both language studies and social psychology are hesitations and disruptions in <b>speech.</b> <b>Silent</b> pauses have been found to result from need to plan verbal sequences and a miscellaneous group of speech disruptions known as NonAhs is a sign of topical anxiety. Filled pauses ('er', 'um' and variants, also called Ahs) were first thought to belong with NonAhs but have proved unrelated to anxiety and require a separate explanation. One hypothesis, that they have an interpersonal role in apportioning the conversational floor, has fared inconclusively under test and recent writers have written it off. In two experiments, filled pause rate was measured as a dependent variable. Mutual visibility in dyadic conversations was varied from zero through intermediate levels to normal, but no changes were observed. When an interviewer's tendency to interrupt was varied, again no significant differences in Ah rate were recorded. However, the filled pause as an independent variable elicited effects supportive of the floor control hypothesis. 'Matched guise' recordings of a speaker were heard by independent groups of undergraduates and presence of Ahs yielded ratings of speaker anxiety, caution and submissiveness, consistent with either the discredited anxiety hypothesis or that of floor control. In a final experiment, naive subjects each interviewed a person whose answers varied in grammatical completion and whether they terminated with Ahs. Either grammatical incompletion, an Ah or both prolonged latencies of subjects' next question substantially. The view that conversations are competitions for the floor is rejected for a broader outlook on interpersonal regulatory cues. Ahs probably do act in floor control, but less simply than previously thought and they may have other, non-regulatory roles besides. Avenues for future research on the topic are outlined with methodological suggestions and the work is presented within a suggested systematisation of accumulating knowledge about interrelationships between linguistic features at several structural levels and various aspects of social behaviour...|$|E
40|$|This article {{describes}} a contour-based 3 D tongue deformation visualization framework using B-mode ultrasound image sequences. A robust, automatic tracking algorithm characterizes tongue motion via a contour, {{which is then}} used to drive a generic 3 D Finite Element Model (FEM). A novel contour-based 3 D dynamic modeling method is presented. Modal reduction and modal warping techniques are applied to model the deformation of the tongue physically and efficiently. This work {{can be helpful in}} a variety of fields, such as <b>speech</b> production, <b>silent</b> <b>speech</b> recognition, articulation training, speech disorder study, etc. Comment: ICASSP 2016, Mar 2016, SHANGHAI, Chin...|$|R
40|$|Abstract. This paper {{reports on}} our latest study on speech {{recognition}} based on surface electromyography (EMG). This technology allows for <b>Silent</b> <b>Speech</b> Interfaces since EMG captures the electrical potentials {{of the human}} articulatory muscles rather than the acoustic speech signal. Therefore, our technology enables speech recognition {{to be applied to}} silently mouthed speech. Earlier experiments indicate that the EMG signal is greatly impacted by the mode of speaking. In this study we analyze and compare EMG signals from audible, whispered, and <b>silent</b> <b>speech.</b> We quantify the differences and develop a spectral mapping method to compensate for these differences. Finally, we apply the spectral mapping to the front-end of our speech recognition system and show that recognition rates on <b>silent</b> <b>speech</b> improve by up to 12. 3 % relative. ...|$|R
40|$|Abstract. The paper {{describes}} {{advances in}} the development of an ultrasound <b>silent</b> <b>speech</b> interface for use in silent communications applications or as a speaking aid for persons who have undergone a laryngectomy. It reports some first steps towards making such a device lightweight, portable, interactive, and practical to use. Simple experimental tests of an interactive <b>silent</b> <b>speech</b> interface for everyday applications are described. Possible future improvements including extension to continuous speech and real time operation are discussed. 1...|$|R
40|$|Although {{still in}} {{experimental}} stage, articulation-based <b>silent</b> <b>speech</b> interfaces may have significant potential for facilitating oral communication in persons with voice and speech problems. An articulation-based <b>silent</b> <b>speech</b> interface converts articulatory movement information to audible words. The complexity of speech production mechanism (e. g., coarticulation) makes the conversion a formidable problem. In this paper, we reported a novel, real-time algorithm for recognizing words from continuous articulatory movements. This approach differed from prior {{work in that}} (1) it focused on word-level, rather than phoneme-level; (2) online segmentation and recognition were conducted at the same time; and (3) a symbolic representation (SAX) was used for data reduction in the original articulatory movement timeseries. A data set of 5, 900 isolated word samples of tongue and lip movements was collected using electromagnetic articulograph from eleven English speakers. The average speaker-dependent recognition accuracy was up to 80. 00 %, with an average latency of 302 miliseconds for each word prediction. The results demonstrated the effectiveness of our approach and its potential for building a real-time articulationbased <b>silent</b> <b>speech</b> interface for clinical applications. The across-speaker variation of the recognition accuracy was discussed. Index Terms: <b>silent</b> <b>speech</b> recognition, laryngectomy, support vector machine, SAX, time-serie...|$|R
40|$|Recent {{research}} has demonstrated the potential of using an articulation-based <b>silent</b> <b>speech</b> interface for command-and-control systems. Such an interface converts articulation to words that can then drive a text-to-speech synthesizer. In this paper, we have proposed a novel near-time algorithm to recognize whole-sentences from continuous tongue and lip movements. Our goal is to assist persons who are aphonic or have a severe motor speech impairment to produce functional speech using their tongue and lips. Our algorithm was tested using a functional sentence data set collected from ten speakers (3012 utterances). The average accuracy was 94. 89 % with an average latency of 3. 11 seconds for each sentence prediction. The results indicate the effectiveness of our approach and its potential for building a real-time articulation-based <b>silent</b> <b>speech</b> interface for clinical applications. Index Terms — Sentence recognition, <b>silent</b> <b>speech</b> interface, support vector machine, laryngectom...|$|R
40|$|INTERSPEECH 2009 : 10 th Annual Conference of the International Speech Communication Association, September 6 - 10, 2009, Brighton, UK. Although the {{segmental}} intelligibility of converted <b>speech</b> from <b>silent</b> <b>speech</b> using direct signal-to-signal mapping {{proposed by}} Toda et al. [1] is quite acceptable, listeners have sometimes difficulty in chunking the speech continuum into meaningful words due to incomplete phonetic cues provided by output signals. This paper studies another approach consisting in combining HMM-based statistical speech recognition and synthesis techniques, {{as well as}} training on aligned corpora, to convert <b>silent</b> <b>speech</b> to audible voice. By introducing phonological constraints, such systems are expected to improve the phonetic consistency of output signals. Facial movements are used {{in order to improve}} the performance of both recognition and synthesis procedures. The results show that including these movements improves the recognition rate by 6. 2 % and a final improvement of the spectral distortion by 2. 7 % is observed. The comparison between direct signal-to-signal and phonetic-based mappings is finally commented in this paper...|$|R
40|$|In this study, {{the maximum}} {{speaking}} rates of 19 stutterers and 19 nonstutterers were mea-sured for three <b>speech</b> conditions: <b>silent,</b> lipped, and overt. Two types of stimulus sentences were used: tongue twisters and matched control sentences. The {{data show that}} stutterers are slower than nonstutterers for each combination of stimulus type and speech condition. The difference between stutterers and nonstutterers is larger for lipped <b>speech</b> than for <b>silent</b> <b>speech</b> and is strongest in the overt condition. These results suggest that speech planning is impaired in stutterers. Speech execution may be independently affected, or, alternatively, the planning impairment may have stronger repercussions with actual speech motor execution. A standard view divides the process of speech production in two terminal stages or levels of control (Kent, 1976; MacNeilage et al., 1981). First, there is the planning or premotor stage, at which speech segments, that is, phonemes, are selected and placed in proper order. The psychologic reality of this stage is evidenced by {{a large body of}} speech error data collected over the years (Dell, 1986; Fromkin, 1980; MacKay, 1970). Sec...|$|R
50|$|In digital telephony, a talkspurt is a {{continuous}} segment of <b>speech</b> between <b>silent</b> intervals where only background noise can be heard. Segmenting speech streams into talkspurts allows bandwidth to be conserved by not sending excess data in silent intervals, and also allows synchronization, buffering and other {{parameters of the}} communications system to be readjusted in the intervals between talkspurts.|$|R
40|$|ICASSP 2009 : IEEE International Conference on Acoustics, Speech, and Signal Processing, April 19 - 24, 2009, Taipei, Taiwan. In this paper, {{we review}} our {{proposed}} statistical voice conversion approaches to enhancing {{various types of}} body transmitted speech captured with non-audible murmur (NAM) microphone. Body transmitted speech conversion is a potential technique to bring a new paradigm to human-to-human speech communication. In addition to our previously proposed methods of enhancing body transmitted unvoiced <b>speech</b> for <b>silent</b> <b>speech</b> communication and of enhancing body transmitted artificial speech for speaking aid, we further propose conversion methods of enhancing body transmitted voiced speech for noise robust speech communication. An experimental result demonstrates that the proposed methods yield significant improvements in quality of body transmitted voiced speech...|$|R
40|$|This paper {{reports on}} our recent {{research}} in the feedback effects of <b>Silent</b> <b>Speech.</b> Our technology is based on surface electromyography (EMG) which captures the electrical potentials of the human articulatory muscles rather than the acoustic speech signal. While recognition results are good for loudly articulated speech and when experienced users speak silently, novice users usually achieve far worse results when speaking silently. Since there is no acoustic feedback when speaking silently, we investigate different kinds of feedback modes: no additional feedback except the natural somatosensory feedback (like the touching of the lips), visual feedback using a mirror and indirect acoustic feedback by speaking simultaneously to a previously recorded audio signal. In addition we examine recorded EMG data when the subject speaks audibly and silently in a loud environment {{to see if the}} Lombard effect can be observed in <b>Silent</b> <b>Speech,</b> too. Index Terms: <b>silent</b> <b>speech,</b> elecromyography, lack of acoustic feedback, EMG-based speech recognition, Lombard effec...|$|R
40|$|People with hearing or {{speaking}} {{disabilities are}} {{deprived of the}} benefits of conventional speech recognition technology because it is based on acoustic signals. Recent research has focused on <b>silent</b> <b>speech</b> recognition systems that are based on the motions of a speaker’s vocal tract and articulators. Because most <b>silent</b> <b>speech</b> recognition systems use contact sensors that are very inconvenient to users or optical systems that are susceptible to environmental interference, a contactless and robust solution is hence required. Toward this objective, this paper presents a series of signal processing algorithms for a contactless <b>silent</b> <b>speech</b> recognition system using an impulse radio ultra-wide band (IR-UWB) radar. The IR-UWB radar is used to remotely and wirelessly detect motions of the lips and jaw. In order to extract the necessary features of lip and jaw motions from the received radar signals, we propose a feature extraction algorithm. The proposed algorithm noticeably improved speech recognition performance compared to the existing algorithm during our word recognition test with five speakers. We also propose a speech activity detection algorithm to automatically select speech segments from continuous input signals. Thus, speech recognition processing is performed only when speech segments are detected. Our testbed consists of commercial off-the-shelf radar products, and the proposed algorithms are readily applicable without designing specialized radar hardware for <b>silent</b> <b>speech</b> processing...|$|R
50|$|The {{process for}} {{analyzing}} subjects' <b>silent</b> <b>speech</b> {{is composed of}} recording subjects’ brain waves, and then using a computer to process the data and determine {{the content of the}} subjects' covert speech.|$|R
60|$|After {{this strange}} <b>speech,</b> she lay <b>silent</b> for some time. Louisa, holding her hand, could feel no pulse; but kissing it, {{could see a}} slight thin thread of life in {{fluttering}} motion.|$|R
40|$|In {{the context}} of <b>Silent</b> <b>Speech</b> Communication (SSC) {{development}} after total laryngectomy rehabilitation, tongue and lip movements were recorded with a portable ultrasound transducer and a CCD video camera respectively. A list of 60 French minimal-pairs {{and a list of}} 50 most frequent French words were pronounced in vocalized and silent mode by one speaker. Amplitude and timing of the articulatory movements were measured and compared in the two modes. This study showed for <b>silent</b> <b>speech,</b> i) a reduced duration of words, ii) a general hypoarticulation for lips, but non significant changes for tongue movements {{depending on the type of}} vowel and consonant...|$|R
40|$|A <b>silent</b> <b>speech</b> {{interface}} (SSI) maps articula-tory movement data to speech output. Alt-hough {{still in}} experimental stages, <b>silent</b> <b>speech</b> interfaces hold significant potential for facilitating oral communication in persons after laryngectomy or with other severe voice impairments. Despite the recent efforts on si-lent speech recognition algorithm develop-ment using offline data analysis, online test of SSIs have rarely been conducted. In this paper, {{we present a}} preliminary, online test of a real-time, interactive SSI based on electro-magnetic motion tracking. The SSI played back synthesized speech sounds {{in response to the}} user’s tongue and lip movements. Three English talkers participated in this test, where they mouthed (silently articulated) phrases using the device to complete a phrase-reading task. Among the three partici-pants, 96. 67 % to 100 % of the mouthed phrases were correctly recognized and corre-sponding synthesized sounds were played af-ter a short delay. Furthermore, one participant demonstrated the feasibility of using the SSI for a short conversation. The experimental re-sults demonstrated the feasibility and poten-tial of <b>silent</b> <b>speech</b> interfaces based on elec-tromagnetic articulograph for future clinical applications. ...|$|R
