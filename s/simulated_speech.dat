34|131|Public
50|$|A {{wide range}} of {{scientists}} contribute to progress on the internal model hypothesis. Michael I. Jordan, Emmanuel Todorov and Daniel Wolpert {{contributed significantly to the}} mathematical formalization. Sandro Mussa-Ivaldi, Mitsuo Kawato, Claude Ghez, Reza Shadmehr, Randy Flanagan and Konrad Kording contributed with numerous behavioral experiments. The DIVA model of speech production developed by Frank H. Guenther and colleagues uses combined forward and inverse models to produce auditory trajectories with <b>simulated</b> <b>speech</b> articulators. Two interesting inverse internal models for the control of speech production were developed by Iaroslav Blagouchine & Eric Moreau. Both models combine the optimum principles and the equilibrium-point hypothesis (motor commands λ are taken as coordinates of the internal space). The input motor command λ is found by minimizing the length of the path traveled in the internal space, either under the acoustical constraint (the first model), or under the both acoustical and mechanical constraints (the second model). The acoustical constraint is related {{to the quality of the}} produced speech (measured in terms of formants), while the mechanical one is related to the stiffness of the tongue's body. The first model, in which the stiffness remains uncontrolled, is in agreement with the standard UCM hypothesis. In contrast, the second optimum internal model, in which the stiffness is prescribed, displays the good variability of speech (at least, in the reasonable range of stiffness) and is in agreement with the more recent versions of the uncontrolled manifold hypothesis (UCM). There is also a rich clinical literature on internal models including work from John Krakauer, Pietro Mazzoni, Maurice A. Smith, Kurt Thoroughman, Joern Diedrichsen, and Amy Bastian.|$|E
40|$|Speech transcriptions {{are very}} complex to process by taggers for {{conventional}} written language {{due to the}} lack of capitalization and punctuation marks. We report in this paper our first attempt toward named entity recognition on speech transcriptions. In this work, we experimented on <b>simulated</b> <b>speech</b> transcriptions. Our experimental results show that our method is robust and achieves high classification accuracies. 1...|$|E
30|$|It {{would be}} helpful to {{evaluate}} the accuracy of the fitting method to show that the proposed method works well. However, it is difficult to compare the simulated values with the actual values because sensors are not available to measure the actual values for human beings. In this paper, we calculate the error in acoustic features between real and <b>simulated</b> <b>speech</b> to describe the accuracy of the fitting method.|$|E
5000|$|... 2014. Co-authored by J. Kannampuzha and E. Kaufmann: Associative {{learning}} and self-organization as basic principles for <b>simulating</b> <b>speech</b> acquisition, speech production, and speech perception. EPJ Nonlinear Biomedical Physics 2:2 (Springer) ...|$|R
40|$|E. Testa {{examines}} in in two detailed chapters {{the stylistic}} techniques {{and methods of}} composition used by some short-story writers of the fifteen and sixteen century to <b>simulate</b> <b>speech</b> in the "dialoghi umili". In his conclusions, Testa connects their tendency toward realism with the tendency to reproduce speech "objectively"...|$|R
30|$|In this paper, we {{describe}} a neural model for <b>simulating</b> <b>speech</b> acquisition, speech production, and speech perception. The model {{is based on}} two important neural features: associative learning and self-organization. The model describes an SOM-based approach to speech acquisition, i.e. how speech knowledge and speaking skills are learned and stored {{in the context of}} self-organizing maps (SOMs).|$|R
30|$|Here, {{we use the}} {{residual}} signal from LPC analysis to estimate {{the parameters of the}} vocal folds. The LPC model is based on a mathematical approximation of the vocal tract. We use it to remove the effect of the vocal tract and obtain {{the residual}} signal to estimate the stiffness parameters with generated cost functions. In order to make a comparison with the spectrum of the residual signal from real speech, an LPC inverse filter is used for the <b>simulated</b> <b>speech</b> to obtain the residual signal. Our target here is to evaluate the similarity of the spectrums of residual signals both from real and <b>simulated</b> <b>speech</b> instead of representing the source wave. The aim {{of this paper is to}} classify speech under stress. It is believed that the main differences between neutral and stressed speech are focused on the harmonic structure of the spectrum of residual signal [11]. Thus, in this study, obtaining the residual signal using LPC can work well for showing the harmonic structure of the spectrum.|$|E
40|$|WESTPRAC VII 2000 : the 7 th West Pacific Regional Acoustics Conference, October 3 - 5, 2000, Kumamoto, Japan. This paper {{describes}} the continuous speech recognition {{performance in the}} car environments. Especially various kinds of phoneme models are evaluated. Since the speech recognition performance considerably degrades in the noisy environments, we must cope with this problem in the car environments. There are two primary factors which cause the degradation of the recognition performance. One is the additive noises such as the background noises, {{and the other is}} the multiplicative distortion such as the reverberation in the car cabin which is emphasized by the distance between a speaker and a microphone. In this paper, the phoneme models which take the additive noises and the multiplicative distortion into account are trained from the <b>simulated</b> <b>speech</b> data. When the car engine is off, the best word recognition rate is 98. 8 % for the multiplicative distortion phoneme model which is trained with the speech data generated by the multiplicative distortion simulation. When the car is in the running condition, the best recognition rate is 97. 2 % for the phoneme model which considers the multiplicative distortion and the additive noises. These results show the effectiveness of the phoneme models which are trained from the <b>simulated</b> <b>speech</b> database in the car environments...|$|E
40|$|The aim of {{this paper}} is to find a {{multiple}} window estimator that is mean square error optimal for cepstrum estimation. The estimator is compared with some known multiple window methods as well as with the parametric AR-estimator. The results show that the new estimator has high performance, especially for data with large spectral dynamics, and that it is also robust against parameter choices. <b>Simulated</b> <b>speech</b> data is used for the evaluation. It is also shown that the windows of the estimator can be approximated with the sinusoidal multiple windows and that the weighting factors of the different periodograms can be analytically computed...|$|E
50|$|Unlike RPGs, {{strategy}} {{video games}} make {{extensive use of}} sound files to create an immersive battle environment. Most games simply played a recorded audio track on cue with some games providing inanimate portraits to accompany the respective voice. StarCraft used full motion video character portraits with several generic speaking animations that did not synchronize with the lines spoken in the game. The game did, however, make extensive use of recorded speech to convey the game's plot, with the speaking animations providing {{a good idea of}} the flow of the conversation. Warcraft III used fully rendered 3D models to animate speech with generic mouth movements, both as character portraits as well as the in-game units. Like the FMV portraits, the 3D models did not synchronize with actual spoken text, while in-game models tended to <b>simulate</b> <b>speech</b> by moving their heads and arms rather than using actual lip synchronization. Similarly, the game Codename Panzers uses camera angles and hand movements to <b>simulate</b> <b>speech,</b> as the characters have no actual mouth movement. However, StarCraft II used fully synced unit portraits and cinematic sequences.|$|R
40|$|This paper {{presents}} {{a novel approach}} and interface for encouraging children to tell and act out original stories. "Dolltalk" is a toy that <b>simulates</b> <b>speech</b> recognition by capturing the gestures and speech of a child. The toy then plays back a child's pretend-play speech in altered voices representing the characters of the child's story. Dolltalk's tangible interface and ability to retell a child's story may enhance a child's creativity in narrative elaboration...|$|R
40|$|Background Quantitative neural {{models of}} speech {{acquisition}} and speech processing are rare. Methods In this paper, we describe a neural model for <b>simulating</b> <b>speech</b> acquisition, speech production, and speech perception. The model {{is based on}} two important neural features: associative learning and self-organization. The model describes an SOM-based approach to speech acquisition, i. e. how speech knowledge and speaking skills are learned and stored {{in the context of}} self-organizing maps (SOMs). Results The model elucidates that phonetic features, such as high-low, front-back in the case of vowels, place and manner or articulation in the case of consonants and stressed vs. unstressed for syllables, result from the ordering of syllabic states at the level of a supramodal phonetic self-organizing map. After learning, the speech production and speech perception of speech items results from the co-activation of neural states within different cognitive and sensorimotor neural maps. Conclusion This quantitative model gives an intuitive understanding of basic neurobiological principles from the viewpoint of speech acquisition and speech processing...|$|R
40|$|Computer-based {{interviewing}} {{systems could}} use models of respondent disfluency behaviors to predict {{a need for}} clarification of terms in survey questions. We compare <b>simulated</b> <b>speech</b> interfaces that use two such models - a generic model and a stereotyped model that distinguishes between the speech of younger and older speakers - to several non-modeling speech interfaces in a task where respondents provided answers to survey questions from fictional scenarios. Our modeling procedure found that {{the best predictor of}} conceptual misalignment was a critical Goldilocks range for response latency, outside of which responses {{are more likely to be}} conceptually misaligned. Different Goldilocks ranges are effective for younger and older speakers...|$|E
30|$|The aim of {{this paper}} is to find a multitaper-based {{spectrum}} estimator that is mean square error optimal for cepstrum coefficient estimation. The multitaper spectrum estimator consists of windowed periodograms which are weighted together, where the weights are optimized using the Taylor expansion of the log-spectrum variance and a novel approximation for the log-spectrum bias. A thorough discussion and evaluation are also made for different bias approximations for the log-spectrum of multitaper estimators. The optimized weights are applied together with the sinusoidal tapers as the multitaper estimator. Comparisons of the cepstrum mean square error are made of some known multitaper methods as well as with the parametric autoregressive estimator for <b>simulated</b> <b>speech</b> signals.|$|E
40|$|Mean square error optimal {{weighting}} for multitaper cepstrum estimation Maria Hansson-Sandsten The aim of {{this paper}} is to find a multitaper-based spectrum estimator that is mean square error optimal for cepstrum coefficient estimation. The multitaper spectrum estimator consists of windowed periodograms which are weighted together, where the weights are optimized using the Taylor expansion of the log-spectrum variance and a novel approximation for the log-spectrum bias. A thorough discussion and evaluation are also made for different bias approximations for the log-spectrum of multitaper estimators. The optimized weights are applied together with the sinusoidal tapers as the multitaper estimator. Comparisons of the cepstrum mean square error are made of some known multitaper methods as well as with the parametric autoregressive estimator for <b>simulated</b> <b>speech</b> signals...|$|E
40|$|This paper {{presents}} {{a framework for}} custom-tailoring voice font in data-driven TTS systems. Three criteria for unit pruning, the prosodic outlier criterion, the importance criterion and {{the combination of the}} two, are proposed. The performance of voice fonts in different sizes which are pruned with the three criteria is evaluated by <b>simulating</b> <b>speech</b> synthesis over large amount of texts and estimating the naturalness with an objective measure at the same time. The result shows that the combined criterion performs the best among the three. The pre-estimated curve for naturalness vs. database size might be used as a reference for custom-tailoring voice font. The naturalness remains almost unchanged when 50 % of instances are pruned off with the combined criterion. 1...|$|R
30|$|Note {{that the}} WSJ {{database}} {{was recorded in}} a noise-free condition. In order to <b>simulate</b> noise-corrupted <b>speech</b> signals, the DEMAND noise database ([URL] was used to sample noise segments. This database involves 18 types of noises, from which we selected 7 types in this work, including white noise and noises at cafeteria, car, restaurant, train station, bus and park.|$|R
40|$|Abstract. We {{describe}} {{the process of}} using ArtiSynth, a 3 D biomechanical simulation platform, to build models of the vocal tract and upper airway which are capable of <b>simulating</b> <b>speech</b> sounds. ArtiSynth allows mass-spring, finite element, and rigid body models of anatomical components (such as the face, jaw, tongue, and pharyngeal wall) {{to be connected to}} various acoustical models (including source filter and airflow models) to create an integrated model capable of sound production. The system is implemented in Java, and provides a class API for model creation, along with a graphical interface that permits the editing of models and their properties. Dynamical simulation can be interactively controlled through a “timeline ” interface that allows online adjustment of model inputs and logging of selected model outputs. ArtiSynth’s modeling capabilities, in combination with its interactive interface, allow for new ways to explore the complexities of articulatory speech synthesis. 1...|$|R
40|$|This paper {{describes}} a new simulation technique designed {{to support a}} wide spectrum of empirical studies on the characteristics of spoken, handwritten, and combined pen/voice input to future interactive systems. The simulatidn tech,fique alms: (1) to provide a tool for investigating interactive handwriting and pen systems, on which no simulation research currently is available, (2) to devise a technique appropriate for comparing people's use of speech and writing, such that differences be- tween these communication modalities and their related technologies can be better understood, and (3) to support a very rapid exchange with <b>simulated</b> <b>speech,</b> pen. and pen/voice sys- tems, such th,-tt interactions can be subject-paced. This paper outli,tes the pecifica,ions, general environment. and capabil- ities of a new semi-automatic simulation technique developed to achieve these goal...|$|E
40|$|The {{texts and}} the {{iconography}} of funerary monuments are a partial source {{of information on the}} professions of metics in Athens. Their references show that these professions did not correspond to a single pattern, but both prestige professions and lowly jobs are mentioned. Common to all of them is the pride manifested by the dead’s family (sometimes in a <b>simulated</b> <b>speech</b> of the dead) on the excellence in his/her profession. Such excellence is often the main topic in the epigrams. The epitaphs for nurses form a special group: their involvement in the familiar sphere and the close relation to the family they had served arise {{from the fact that the}} dedication comes from this family and not from their own’s...|$|E
40|$|Speech {{synthesis}} {{research has}} been transformed in recent years through the exploitation of speech corpora [...] both for statistical modelling and {{as a source of}} signals for concatenative synthesis. This revolution in methodology and the new techniques it brings calls into question the received wisdom that better computer voice output will come from {{a better understanding of how}} humans produce speech. This paper discusses the relationship between this new technology of <b>simulated</b> <b>speech</b> and the traditional aims of speech science. The paper suggests that the goal of speech simulation frees engineers from inadequate linguistic and physiological descriptions of speech. But at the same time, it leaves speech scientists free to return to their proper goal of building a computational model of human speech production...|$|E
40|$|A {{problem in}} {{developing}} and testing ubiquitous computing systems {{is the fact that}} they are environments and cannot be tested in conventional laboratory settings. Here we have addressed the problem of testing such an environment by applying the Wizard of Oz method to a ubiquitous computing system called Doorman. Doorman uses spoken language input and multimodal speech output (speech synthesis combined with pointing gestures) to control the access of incoming visitors and staff members to our office premises and to guide the visitors to find the people or the room they are seeking. The experiment was conducted by <b>simulating</b> <b>speech</b> recognition with a human wizard operating the otherwise fully working system. The user-initiated dialogue strategy was mostly successful, but did not meet the requirements in some cases. We also found that guiding visitors using multimodal spoken output was not successful and should be redesigned...|$|R
25|$|Computational {{modeling}} {{has also}} been used to <b>simulate</b> how <b>speech</b> may be processed by the brain to produce behaviors that are observed. Computer models {{have been used to}} address several questions in speech perception, including how the sound signal itself is processed to extract the acoustic cues used in speech, and how speech information is used for higher-level processes, such as word recognition.|$|R
40|$|Hearing loss afflicts {{as many as}} 10 % of our population. Fortunately, tech-nologies {{designed}} to alleviate the effects of hearing loss are improving rapidly, including cochlear implants and the increasing computing power of digital hearing aids. This thesis focuses on theoretically sound methods for improving hearing aid technology. The main contributions are documented in three research articles, which treat two separate topics: modelling of hu-man speech recognition (Papers A and B) and optimization of diagnostic methods for hearing loss (Paper C). Papers A and B present a hidden Markov model-based framework for <b>simulating</b> <b>speech</b> recognition in noisy conditions using auditory models and signal detection theory. In Paper A, a model of normal and impaired hear-ing is employed, in which a subject’s pure-tone hearing thresholds are used to adapt the model to the individual. In Paper B, the framework is modified to simulate hearing with a cochlear implant (CI). Two models of hearin...|$|R
40|$|This paper {{presents}} {{simulations of}} the impact of tongue surgery on tongue movements and on speech articulation. For this, a 3 D biomechanical Finite Element (FE) model of the tongue is used. Muscles are represented within the FE structure by specific subsets of elements. The tongue model is inserted in the upper airways including jaw, palate and pharyngeal walls. Two examples of tongue surgery, which are quite common in the treatment of cancers of the oral cavity are modelled: hemiglossectomy and large resection of the mouth floor. Three kinds of reconstruction are also modelled, assuming flaps with a low, medium or high stiffnesses. The impact of the surgery without any reconstruction and with the three different reconstructions is quantitatively measured and compared during <b>simulated</b> <b>speech</b> production sequences. More precisely, differences in global 3 D tongue shape and in velocity patterns during tongue displacements are evaluated...|$|E
40|$|Abstract. The quality {{assessment}} of multimodal conversational interactions {{is determined by}} many influence parameters. Stress and cognitive load are two of them. In order {{to assess the impact}} of stress and cognitive load on the perceived conversational quality it is essential to control their levels during the interaction. Therefore we present in this paper preliminary experiments carried out to determine the circumstances in which low/high levels of stress respectively cognitive load are achieved while interacting with the system. Different levels are manipulated by varying task difficulty (information complexity, task load, and <b>simulated</b> <b>speech</b> recognition errors), information presentation (modality usage, spatial organization and temporal order of information items) and time pressure. Heart rate variability (HRV) and galvanic skin response (GSR) as well as subjective judgments in the form of questionnaires are deployed to validate the induced stress and cognitive levels. Methods and preliminary results are presented...|$|E
40|$|Evaluations of {{algorithms}} for robust automatic {{speech recognition}} (ASR) are often based on artificial noisy speech instead of realistic noisy speech. In this paper we compare the ASR performance of speech with artificial additive noise {{to the performance of}} realistic noisy speech. All data was recorded during the same recording campaign and with nearly identical channel characteristics. The simulation process takes into account all major characteristics of the noisy reference data. Clean speech, noisy speech and <b>simulated</b> <b>speech</b> are compared for different aspects of robust ASR including noise reduction by Spectral Subtraction and the ETSI robust front end. The results show, that artificial noisy speech even in very controlled simulation environments is not very similar and not a full subst itute for realistic noisy data. While the tendencies of the improvement for artificial and realistic data are similar for the evaluated approaches, the magnitude can be quite different...|$|E
40|$|I {{would like}} to first gratefully {{acknowledge}} Benjamin Munson for his continued assistance and guidance, and motivation. I {{would also like to}} acknowledge Leslie Glaze and Peggy Nelson for their support as committee members. Acknowledgment is given to student research assistants who have contributed to completion of this thesis. Financial support was provided by the HSD Grant from the Department of Speech-Language-Hearing Sciences at the University of Minnesota. ii Perceptual judgment of hypernasality is a common and widely accepted practice among speech-language pathologists. However, because these judgments are somewhat subjective, reliability is an issue. This study examined the effect of auditory anchors on the validity of judgments of hypernasality in both natural and acoustically manipulated speech samples. In addition, this study investigated the effectiveness of auditory anchors developed using acoustic manipulation of first-formant bandwidth to <b>simulate</b> <b>speech</b> nasality. Anchors consisted of sentences of unprocessed speech and speech that had been acoustically altered by first formant bandwidth to 150 Hz, 300 Hz, and 500 Hz. Th...|$|R
50|$|Lip sync was {{for some}} time a minor focus in {{role-playing}} video games. Because {{of the amount of}} information conveyed through the game, the majority of communication uses of scrolling text. Older RPGs rely solely on text, using inanimate portraits to provide a sense of who is speaking. Some games make use of voice acting, such as Grandia II or Diablo, but due to simple character models, there is no mouth movement to <b>simulate</b> <b>speech.</b> RPGs for hand-held systems are still largely based on text, with the rare use of lip sync and voice files being reserved for full motion video cutscenes. Newer RPGs, have extensive audio dialogues. The Neverwinter Nights series are examples of transitional games where important dialogue and cutscenes are fully voiced, but less important information is still conveyed in text. In games such as Jade Empire and Knights of the Old Republic, developers created partial artificial languages to give the impression of full voice acting without having to actually voice all dialogue.|$|R
30|$|Note {{that when}} {{considering}} reverberant speech recognition, we may consider using a longer context to handle reverberation [3]. We also tested increasing the acoustic context to 15 frames for numbers of hidden layers between 5 and 8, but we observed no performance improvement for SimData, and for RealData, {{in fact the}} performance even degraded. This indicates over-fitting to the training conditions that consist of <b>simulated</b> reverberant <b>speech.</b>|$|R
40|$|This paper {{presents}} and discusses {{a strategy for}} mixed-initiative dialogue management within a home banking application. The strategy tries to utilise the guidance of system-directed dialogues, while accommodating user initiated focus shifts by the inclusion of short-cuts in the dialogue. The paper reports on two experiments, one with a <b>simulated</b> <b>speech</b> recogniser (WOZ), and the second with a fully automated system. Both experiments shows that users use the possibility for short-cuts, even when not instructed of their existence. A tendency towards user habituation is also demonstrated. The task structure {{is shown in figure}} 2. 1. Instead of building specific DTMF subtasks, all tasks accept spoken and DTMF key pad input in parallel. This is achieved by including two sets of prompts in the dialogue, and switching between them depending on which modality the user chooses. 3. DIALOGUE INITIATIVE The question of system directed vs. user-driven (or mixedinitiative) dialogue control strategi [...] ...|$|E
40|$|The {{application}} of machine learning methods to the {{dialogue management component}} of spoken dialogue systems is a growing research area. Whereas traditional methods use handcrafted rules to specify a dialogue policy, machine learning techniques seek to learn dialogue behaviours from a corpus of training data. In this paper, we identify the properties of a corpus suitable for training machine-learning techniques, and propose a framework for collecting dialogue data. The approach is akin to a "Wizard of Oz" set-up with a "wizard" and a "user", but introduces several novel variations to simulate the ASR communication-channel. Specifically, a turn-taking model common in spoken dialogue system is used, and rather than hearing the user directly, the wizard sees <b>simulated</b> <b>speech</b> recognition results on a screen. The simulated recognition results are produced with an error-generation algorithm which allows the target WER to be adjusted. An evaluation of the algorithm is presented...|$|E
40|$|The quality {{assessment}} of multimodal conversational interactions {{is determined by}} many influence parameters. Stress and cognitive load are two of them. In order {{to assess the impact}} of stress and cognitive load on the perceived conversational quality it is essential to control their levels during the interaction. Therefore we present in this paper preliminary experiments carried out to determine the circumstances in which low/high levels of stress respectively cognitive load are achieved while interacting with the system. Different levels are manipulated by varying task difficulty (information complexity, task load, and <b>simulated</b> <b>speech</b> recognition errors), information presentation (modality usage, spatial organization and temporal order of information items) and time pressure. Heart rate variability (HRV) and galvanic skin response (GSR) as well as subjective judgments in the form of questionnaires are deployed to validate the induced stress and cognitive levels. Methods and preliminary results are presented...|$|E
40|$|Magnetic {{resonance}} imaging (MRI) {{has been used}} for imaging the vocal tract (VT) for a long time [1]. Nowadays, the VT can be scanned in well under 30 s [2]. The anatomical data produced by MRI is suitable for generating the computational mesh for the finite element method (FEM). FEM solvers for the wave equation have been used for <b>simulating</b> normal <b>speech</b> production acoustics [3], the effects of anatomica...|$|R
50|$|FPS is a {{genre that}} {{generally}} places much {{more emphasis on}} graphical display, mainly due to the camera almost always being very close to character models. Due to increasingly detailed character models requiring animation, FPS developers assign many resources to create realistic lip synchronization with the many lines of speech used in most FPS games. Early 3D models used basic up-and-down jaw movements to <b>simulate</b> <b>speech.</b> As technology progressed, mouth movements began to closely resemble real human speech movements. Medal of Honor: Frontline dedicated a development team to lip sync alone, producing the most accurate lip synchronization for games at that time. Since then, games like Medal of Honor: Pacific Assault and Half-Life 2 have made use of coding that dynamically simulates mouth movements to produce sounds {{as if they were}} spoken by a live person, resulting in astoundingly lifelike characters. Gamers who create their own videos using character models with no lip movements, such as the helmeted Master Chief from Halo, improvise lip movements by moving the characters' arms, bodies and making a bobbing movement with the head (see Red vs. Blue).|$|R
3000|$|... where S(ω) and S*(ω) are {{the power}} spectrums of the {{residual}} signal for <b>simulated</b> and real <b>speech,</b> respectively. Here, we select the stiffness parameters k 1, k 2, and kc as variables for vocal tract fitting.|$|R
